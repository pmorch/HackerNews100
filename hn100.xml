<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 11 Aug 2023 13:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Fastest Branchless Binary Search (116 pts)]]></title>
            <link>https://mhdm.dev/posts/sb_lower_bound/</link>
            <guid>37086796</guid>
            <pubDate>Fri, 11 Aug 2023 09:38:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mhdm.dev/posts/sb_lower_bound/">https://mhdm.dev/posts/sb_lower_bound/</a>, See on <a href="https://news.ycombinator.com/item?id=37086796">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>You’re a busy person so I’ll first jump right to it. Here it is, the fastest general (and simple) binary search C++ implementation:</p><div><pre><code data-lang="C++"><span>template</span> <span>&lt;</span><span>class</span> <span>ForwardIt</span>, <span>class</span> <span>T</span>, <span>class</span> <span>Compare</span><span>&gt;</span>
<span>constexpr</span> ForwardIt sb_lower_bound(
      ForwardIt first, ForwardIt last, <span>const</span> T<span>&amp;</span> value, Compare comp) {
   <span>auto</span> length <span>=</span> last <span>-</span> first;
   <span>while</span> (length <span>&gt;</span> <span>0</span>) {
      <span>auto</span> rem <span>=</span> length <span>%</span> <span>2</span>;
      length <span>/=</span> <span>2</span>;
      <span>if</span> (comp(first[length], value)) {
         first <span>+=</span> length <span>+</span> rem;
      }
   }
   <span>return</span> first;
}
</code></pre></div><p>Same function interface as <code>std::lower_bound</code>, but <strong>2x</strong> faster, and shorter. “branchless” because the <code>if</code> compiles down to a conditional move instruction rather than a branch/conditional jump. We will explore compiler options, even faster versions, fully branchless, and caveats towards the end of the article. There’s a significant <strong>update</strong> too. Oh and I’m sorry about just thrusting C++ code on you. Rude, I know. You don’t really need to know C++ to understand this article, just iterators (<code>first</code> and <code>last</code>), basically pointers to elements in an array, though note they can point one past the last array entry. Ignore <code>template</code>, <code>class</code>, <code>constexpr</code> and <code>&amp;</code>. If only there was a clean fast bare-metal language to write all this in.. <sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> <sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></p><h2 id="binary-search-intro">Binary search intro</h2><p>You have a sorted list and want to find where a <code>value</code> would fit. In C++ you’d use <code>std::lower_bound</code> which returns the first position (iterator) for which the comparison (usually <code>&lt;</code>) fails, or <code>last</code> if the comparison is true for all elements. Let’s write that, so surprise coding interview question! (In C++. Good luck.)</p><p>The <code>binary</code> in binary search comes from splitting up the list into two at some middle item and doing a comparison of the middle against the given <code>value</code>. Based on the comparison result we pick which of the two lists to keep looking in. We start with a list of some <code>length = last - first</code> that starts at the given iterator <code>first</code>. We need to have some loop that keeps going until the list is empty, i.e. <code>while (length &gt; 0)</code>. Pick the/a middle, at <code>length / 2</code> do a comparison and update the current list, which we’ll do by updating <code>first</code> and <code>length</code>. Here goes:</p><div><pre><code data-lang="C++"><span>// std_lower_bound()
</span><span></span><span>auto</span> length <span>=</span> last <span>-</span> first;
<span>while</span> (length <span>&gt;</span> <span>0</span>) {
   <span>auto</span> half <span>=</span> length <span>/</span> <span>2</span>;
   <span>if</span> (comp(first[half], value)) {
      first <span>+=</span> half <span>+</span> <span>1</span>;
      length <span>-=</span> half <span>+</span> <span>1</span>;
   } <span>else</span> {
      length <span>=</span> half;
   }
}
<span>return</span> first;
</code></pre></div><p>That was straight-forward. What we got was a slightly refactored version of <a href="https://github.com/gcc-mirror/gcc/blob/releases/gcc-13/libstdc%2B%2B-v3/include/bits/stl_algobase.h#L1467">what gcc/libstdc++ uses</a> or <a href="https://github.com/llvm/llvm-project/blob/release/16.x/libcxx/include/__algorithm/lower_bound.h#L36">what llvm/libc++ uses</a>. Roughly the same speed, even (sometimes) compiles down to the same assembly in the loop.</p><h2 id="branch-prediction">Branch prediction</h2><p>“What’s slow about this?” Not much but glad you asked, great question. Processors have gotten faster and faster over the years, and part of the reason why is <em>branch prediction</em>. Short explanation: for speed, the CPU attempts to execute instructions in parallel by dividing each instruction execution into multiple stages, say F-D-E-W (fetch, decode, execute, writeback). With a careful design it can make progress on multiple stages of different instructions (Example: instruction 5 in stage F, 6 in D, 7E, 8F) at the same time. The complication comes from branches in the code, i.e. conditional jump instructions, where depending on some result the next instruction is either X or Y. The CPU <em>predicts</em> one option, X, and starts running through the stages, eventually including the stages of say X+1 and X+2. When the result becomes available and it turns out it should have been Y, all the work on X, X+1 and X+2 is thrown away. Branch mispredictions are expensive because the CPU could have already made progress on Y, Y+1 and Y+2 instead.</p><p>Branch prediction is great, but not for binary search. It’s a search, and you only search for something if you don’t know exactly where it is, otherwise you’d just get it. Which means that <code>if (comp(first[half], value))</code> is unpredictable in common use of <code>std::lower_bound</code>.</p><p><img src="https://mhdm.dev/posts/sb_lower_bound/predictions.png" alt="A happy-go-lucky processor saying “my predictions fail only half the time!" "="">
Credit <a href="https://www.instagram.com/practicemakespink">@practicemakespink</a></p><p>Let’s help the processor.</p><p>-We:</p><div><pre><code data-lang="C++"><span>// bstd_lower_bound()
</span><span></span><span>auto</span> length <span>=</span> last <span>-</span> first;
<span>while</span> (length <span>&gt;</span> <span>0</span>) {
   <span>auto</span> half <span>=</span> length <span>/</span> <span>2</span>;
   <span>bool</span> compare <span>=</span> comp(first[half], value);
   <span>// No more ifs
</span><span></span>   first <span>=</span> compare <span>?</span> first <span>+</span> half <span>+</span> <span>1</span> <span>:</span> first;
   length <span>=</span> compare <span>?</span> length <span>-</span> half <span>-</span> <span>1</span> <span>:</span> half;
}
<span>return</span> first;
</code></pre></div><p>-Clang compiler: “That’s not how this works!”</p><p>-We: <code>-mllvm -x86-cmov-converter=false</code></p><p>-Clang compiler: “Yes, boss."<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup></p><p>The result is 25% faster as it uses 2 conditional move instructions. Not bad. But turns out that <code>-mllvm -x86-cmov-converter=false</code>, which we’ll shorten to <code>-cmov</code>, speeds up <code>std::lower_bound</code> just as much because clang is smart enough to figure out how to convert the <code>if</code>/<code>else</code> to conditional moves. gcc doesn’t have such an option and generally just doesn’t care about what you want.</p><p>Overall, there’s currently no good way to tell either clang<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup> or gcc<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup> to use a conditional move in just a certain situation.I’m trying to not make this article about finicky compilers, so let’s move on.</p><h2 id="what-started-this">What started this</h2><p>Why are we even talking about speeding up binary searches anyway? Why am I roping you into this? Cause someone else roped me into this.</p><p><video muted="" autoplay="" loop="" src="https://mhdm.dev/posts/sb_lower_bound/rope.mp4" width="100%" title="Meeseeks blaming one another for being roped in | Rick and Morty" onclick="this.paused?this.play():this.pause()"></video>
<a href="https://www.adultswim.com/videos/rick-and-morty/meeseeks-and-destroy">Rick and Morty - Meeseeks and Destroy</a></p><p>I saw Malte Skarupke’s translation of “Shar’s algorithm” into a C++ binary search (<code>branchless_lower_bound</code>) and I couldn’t help but think “it’s not optimal”. Malte’s version sometimes compares <code>value</code> against the same element multiple times. So I wondered, what is the optimal ‘branchless’ binary search? That led to <code>sb_lower_bound</code> which is ~20% faster than <a href="https://probablydance.com/2023/04/27/beautiful-branchless-binary-search/">Malte’s version of lower_bound that he tested as 2x faster than GCC</a>.</p><p>“What’s an ‘optimal’ binary search anyway?” Good question. I think a binary search is ‘optimal’ if it completes by doing the minimum number of comparisons. This is very useful when you have a (relatively) slow comparison function. Malte noted his version is slower than <code>std::lower_bound</code> for binary searching a large number of strings.</p><p>Looking at <code>std::lower_bound</code> it returns an iterator, which can point to any of the list elements but also one past the end. For a list of size <code>n</code> there are <code>n+1</code> possible options. Thus for a list of size <code>2<sup>k</sup>-1</code> there are <code>2<sup>k</sup></code> possible options. In this case the optimal number of comparisons is <code>k</code>. Provably so as being able to distinguish between all <code>2<sup>k</sup></code> options requires <code>k</code> bits of information, and each comparison gives us 1 bit of information (true vs false). Translating this case into code, we get:</p><div><pre><code data-lang="C++"><span>// Really fast but only works when length is a power of 2 minus 1
</span><span>// When not, it can result in out of bounds accesses like for
</span><span>// array [0, 1, 2, 3, 4, 5] and value 4
</span><span></span><span>auto</span> length <span>=</span> last <span>-</span> first;
<span>while</span> (length <span>&gt;</span> <span>0</span>) {
   length <span>/=</span> <span>2</span>;
   <span>if</span> (comp(first[length], value)) {
      first <span>+=</span> length <span>+</span> <span>1</span>;
   }
}
<span>return</span> first;
</code></pre></div><p>With clang <code>-cmov</code> the loop compiles down to 6 instructions, one of which is <code>cmov</code>. The reason this (and Malte’s code) is so fast is that only updating <code>first</code> depends on the comparison result.</p><h2 id="sb_lower_bound"><code>sb_lower_bound</code></h2><p>Now let’s look at <code>sb_lower_bound</code> (named after <em>simple branchless</em>). It actually took me longer to stumble upon than faster versions (yet to be presented) as it’s not ‘optimal’.</p><div><pre><code data-lang="C++"><span>auto</span> length <span>=</span> last <span>-</span> first;
<span>while</span> (length <span>&gt;</span> <span>0</span>) {
   <span>auto</span> rem <span>=</span> length <span>%</span> <span>2</span>;
   length <span>/=</span> <span>2</span>;
   <span>if</span> (comp(first[length], value)) {
      first <span>+=</span> length <span>+</span> rem;
   }
}
<span>return</span> first;
</code></pre></div><p>Every time we split the list, the number of elements happens to be even, and comp returns true then we don’t skip over enough elements. For <code>n</code> elements, where <code>2<sup>k</sup> &lt;= n &lt; 2<sup>k+1</sup></code>, <code>sb_lower_bound</code> will always make <code>k+1</code> comparisons while <code>std::lower_bound</code> will either make <code>k</code> or <code>k+1</code> comparisons. On average <code>std::lower_bound</code> will make about <code>log2(n+1)</code> comparisons. Overall <code>sb_lower_bound</code> is faster as it has significantly fewer instructions in the loop. The comparison function has to be really slow for the difference between <code>k+1</code> and <code>log2(n+1)</code> number of comparisons to matter.</p><p>Second caveat is that currently, gcc does not emit branchless code for <code>sb_lower_bound</code> regardless of optimization level. It doesn’t emit branchless code for <code>std::lower_bound</code> either so they end up about as fast. We can try to write some inline assembly to force gcc to use <code>cmov</code> but there’s a tradeoff. The simple way results in more instructions than necessary. The alternative requires writing different assembly code for every possible type of <code>value</code> (int, float, etc.).</p><div><pre><code data-lang="C++"><span>// asm_lower_bound, works for x86 only
</span><span></span><span>auto</span> length <span>=</span> last <span>-</span> first;
<span>while</span> (length <span>&gt;</span> <span>0</span>) {
   <span>auto</span> rem <span>=</span> length <span>%</span> <span>2</span>;
   length <span>/=</span> <span>2</span>;
   <span>auto</span> firstplus <span>=</span> first <span>+</span> length <span>+</span> rem;
   <span>// Does a comparison which sets some x86 FLAGS like CF or ZF
</span><span></span>   <span>bool</span> compare <span>=</span> comp(first[length], value);
   <span>// Inline assembly doesn't support passing bools straight into FLAGS
</span><span></span>   <span>// so we ask the compiler to copy it from FLAGS into a register
</span><span></span>   __asm__(
         <span>// Then we test the register, which sets ZF=!compare and CF=0
</span><span></span>         <span>// Reference: https://www.felixcloutier.com/x86/test
</span><span></span>         <span>"test %[compare],%[compare]</span><span>\n</span><span>"</span>
         <span>// cmova copies firstplus into first if ZF=0 and CF=0
</span><span></span>         <span>// Reference: https://www.felixcloutier.com/x86/cmovv
</span><span></span>         <span>"cmova %[firstplus],%[first]</span><span>\n</span><span>"</span>
         <span>:</span> [first] <span>"+r"</span>(first)
         <span>:</span> [firstplus] <span>"r"</span>(firstplus), [compare]<span>"r"</span>(compare)
   );
}
<span>return</span> first;
</code></pre></div><p>2x faster than the gcc version of <code>std::lower_bound</code>, but slightly slower than <code>sb_lower_bound</code> with clang <code>-cmov</code>. Presented just for demonstration purposes.</p><h2 id="more-optimal">More optimal</h2><p>I promised an even faster version in the intro. Here it is:</p><div><pre><code data-lang="C++"><span>// bb_lower_bound
</span><span></span><span>auto</span> length <span>=</span> last <span>-</span> first;
<span>// while length isn't a power of 2 minus 1
</span><span></span><span>while</span> (length <span>&amp;</span> (length <span>+</span> <span>1</span>)) {
   <span>auto</span> step <span>=</span> length <span>/</span> <span>8</span> <span>*</span> <span>6</span> <span>+</span> <span>1</span>; <span>// MAGIC
</span><span></span>   <span>if</span> (comp(first[step], value)) {
      first <span>+=</span> step <span>+</span> <span>1</span>;
      length <span>-=</span> step <span>+</span> <span>1</span>;
   } <span>else</span> {
      length <span>=</span> step;
      <span>break</span>;
   }
}
<span>while</span> (length <span>!=</span> <span>0</span>) {
   length <span>/=</span> <span>2</span>;
   <span>if</span> (comp(first[length], value)) {
      first <span>+=</span> length <span>+</span> <span>1</span>;
   }
}
<span>return</span> first;
</code></pre></div><p>Let’s quickly go over <code>length &amp; (length + 1)</code>. Example: <code>length</code> is <code>110011</code>, <code>length+1</code> is <code>110100</code>, <code>length &amp; (length+1)</code> is <code>110000</code>. Note how they always share their most significant <code>1</code> except for when <code>length+1</code> carries over all set bits. That case is when <code>length</code> is of the form <code>11..1</code>, i.e. power of 2 minus 1, in which case <code>length &amp; (length + 1)</code> will be 0. Slightly adapted from one of the bit twiddling tricks I remembered and there’s a (warning!) <a href="https://graphics.stanford.edu/~seander/bithacks.html">rabbit hole full of awesome tricks here</a>.</p><p>Let us call lengths ‘nice’ if they are powers of 2 minus 1. Earlier we found that for nice lengths we can have optimal searching. The original idea was that for non-nice length we split the search list not in half but in a way that we’ll quickly end up only searching sub-lists of nice lengths. This idea was not quite fast enough as it meant spending significant time just splitting up the list. The first compromise is that early <code>break</code>, which means the second <code>while</code> loop may (non-optimally) search past its original length; no out of bounds accesses though as this sub-list isn’t at the end of the full list. This compromise leads to a second compromise, that bit of MAGIC in selecting the <code>step</code> size. To be fast we want a large <code>step</code>, definitely <code>&gt;= length/2</code>, so we more often break out to the faster <code>while</code> loop. But not so large a <code>step</code> that almost equals <code>length</code> because we lose on what makes a binary search fast. We’d also prefer that the <code>step</code> is nice or has many <code>1</code>s in its binary representation, which makes the second <code>while</code> loop more optimal. The many <code>1</code>s is why the <code>step</code> is forced to be odd. Last but not least, we’d want <code>length - step - 1</code> to be nice.</p><p>I’ve tried quite a few variations. The most bitter sweet was <code>auto step = length &gt;&gt; 1; step |= step &gt;&gt; 1;</code> which I thought to be a good fast heuristic that balances the listed compromises, is very close to optimal but ultimately ended up slightly slower than MAGIC. Another issue is the ‘break’ makes it branchy.</p><p>One unexplored avenue is exhaustively searching for the fastest (yet still optimal) <code>step</code> for every length and storing that into a precomputed table. Then either deriving a good heuristic from said table or using it outright. With <code>sb_lower_bound</code> I’ve reached my good-enough point but you’re welcome to explore further :).</p><div><pre><code data-lang="C++"><span>auto</span> length <span>=</span> last <span>-</span> first;
<span>while</span> (length <span>&amp;</span> (length <span>+</span> <span>1</span>)) {
   <span>auto</span> step <span>=</span> precomputed[length];
   <span>if</span> (comp(first[step], value)) {
      first <span>+=</span> step <span>+</span> <span>1</span>;
      length <span>-=</span> step <span>+</span> <span>1</span>;
   } <span>else</span> {
      length <span>=</span> step;
   }
}
<span>// ...
</span></code></pre></div><h2 id="more-branchless-more-better">More branchless, more better?</h2><p>Short answer: no. This section can be skipped but here’s the long answer: For <code>n</code> elements where <code>2<sup>k</sup> &lt;= n &lt; 2<sup>k+1</sup></code> <code>sb_lower_bound</code> will always do <code>k+1</code> comparisons. On a 64-bit machine that means at most 64 iterations of the <code>while (length &gt; 0)</code> loop. So it’s possible to write a “fully branchless” version that doesn’t have the <code>length</code> check by using a <code>switch</code> with intentional fall-through.</p><div><pre><code data-lang="C++">size_t length <span>=</span> last <span>-</span> first;
size_t rem;
<span>switch</span>(std<span>::</span>bit_width(length)) {
   <span>case</span> <span>64</span><span>:</span>
      rem <span>=</span> length <span>%</span> <span>2</span>;
      length <span>/=</span> <span>2</span>;
      first <span>+=</span> comp(first[length], value) <span>*</span> (length <span>+</span> rem);
   <span>case</span> <span>63</span><span>:</span>
      rem <span>=</span> length <span>%</span> <span>2</span>;
      length <span>/=</span> <span>2</span>;
      first <span>+=</span> comp(first[length], value) <span>*</span> (length <span>+</span> rem);
   <span>// ...
</span><span></span>   <span>case</span> <span>1</span><span>:</span>
      rem <span>=</span> length <span>%</span> <span>2</span>;
      length <span>/=</span> <span>2</span>;
      first <span>+=</span> comp(first[length], value) <span>*</span> (length <span>+</span> rem);
}
<span>return</span> first;
</code></pre></div><p>If you’re not familiar with <code>switch</code>, think of it as a jump into code. In our case to the exact place from which there are exactly the right number of comparisons left to do.</p><p>“Is it any faster?” No. Modern x86 processors handle loop conditions well as they’re predictable; we’re very likely to remain in the loop. And that’s good especially because it saves us from writing templates or macros or copy-paste-edit the 64 cases.</p><h2 id="spotlight-on-performance">Spotlight on Performance</h2><p><img src="https://mhdm.dev/posts/sb_lower_bound/performance.png" alt="Sketch of processor’s performance" title="Spotlight on Performance">
Credit <a href="https://www.instagram.com/practicemakespink">@practicemakespink</a></p><p>Average run time (ns):</p><table><thead><tr><th></th><th><code>std::lower_</code></th><th><code>branchless_lower_</code></th><th><code>asm_lower_</code></th><th><code>sb_lower_</code></th><th><code>sbm_lower_</code></th><th><code>bb_lower_</code></th></tr></thead><tbody><tr><td>gcc</td><td>80.68</td><td>43.65</td><td>54.92</td><td>77.22</td><td>34.19</td><td>75.64</td></tr><tr><td>clang</td><td>78.66</td><td>90.97</td><td>51.83</td><td>80.06</td><td>83.95</td><td>74.44</td></tr><tr><td>clang -cmov</td><td>61.30</td><td>43.43</td><td>54.32</td><td><strong>33.24</strong></td><td>35.54</td><td><strong>32.73</strong></td></tr></tbody></table><p>Geometric mean run time (ns):</p><table><thead><tr><th></th><th><code>std::lower_</code></th><th><code>branchless_lower_</code></th><th><code>asm_lower_</code></th><th><code>sb_lower_</code></th><th><code>sbm_lower_</code></th><th><code>bb_lower_</code></th></tr></thead><tbody><tr><td>gcc</td><td>62.44</td><td>25.55</td><td>32.35</td><td>59.67</td><td>20.62</td><td>57.93</td></tr><tr><td>clang</td><td>61.24</td><td>65.72</td><td>30.67</td><td>63.59</td><td>66.91</td><td>58.19</td></tr><tr><td>clang -cmov</td><td>39.17</td><td>25.14</td><td>31.21</td><td><strong>19.81</strong></td><td>20.91</td><td>21.33</td></tr></tbody></table><p>Runtimes in line chart form:</p><p><a href="https://mhdm.dev/posts/sb_lower_bound/runtimes.png"><img src="https://mhdm.dev/posts/sb_lower_bound/runtimes.png" alt="Runtimes"></a></p><p>“What’s <code>sbm_lower_bound</code>?” It’s basically <code>sb_lower_bound</code> but modified to trick gcc into generating a conditional move. The difference is switching from the <code>if</code> statement to <code>first += comp(first[length], value) * (length + rem)</code>. Use with comments and caution as the next version of gcc may undo this optimization.</p><p>Benchmarking commands showing compiler options:</p><div><pre><code data-lang="sh">g++-10 -std<span>=</span>c++20 -Wall -O2 -march<span>=</span>haswell test.cpp -o test <span>&amp;&amp;</span> ./test
clang++-10 -std<span>=</span>c++20 -Wall -O2 -march<span>=</span>haswell test.cpp -o test <span>&amp;&amp;</span> ./test
clang++-10 -std<span>=</span>c++20 -Wall -O2 -march<span>=</span>haswell -mllvm -x86-cmov-converter<span>=</span>false test.cpp -o test <span>&amp;&amp;</span> ./test
</code></pre></div><p><code>-march=native</code> or no <code>-march</code> did not significantly influence the rankings. Benchmarked on an intel i7 kaby lake.</p><h3 id="branch-mispredictions">Branch mispredictions</h3><p>We can look at branch mispredictions/misses using <code>perf</code>:</p><div><pre><code data-lang="sh"><span># clang</span>
perf stat ./test
<span># [..]</span>
         16,599.95 msec task-clock:u     <span>#    1.000 CPUs utilized</span>
<span># [..]</span>
    30,309,755,516      instructions:u   <span>#    0.53  insn per cycle</span>
     6,941,783,502      branches:u       <span>#  418.181 M/sec</span>
     1,203,569,540      branch-misses:u  <span>#   17.34% of all branches</span>
<span># clang -cmov</span>
perf stat ./test
<span># [..]</span>
         10,982.97 msec task-clock:u     <span>#    1.000 CPUs utilized</span>
<span># [..]</span>
    32,603,123,521      instructions:u   <span>#    0.90  insn per cycle</span>
     4,070,883,093      branches:u       <span>#  370.654 M/sec</span>
        35,954,999      branch-misses:u  <span>#    0.88% of all branches</span>
</code></pre></div><p><code>-cmov</code> removes ~2.9B branches and ~1.2B branch misses, so it removes branches that were mispredicted ~41% of the time. It’s close to the 50% we’d expect for purely unpredictable branches, if we had perfect randomness and benchmarking. In which case <code>-cmov</code> would result in an even higher improvement.</p><h3 id="performance-with-slower-comp">Performance with slower <code>comp()</code></h3><p>For somewhat realistic scenarios of binary searching with a slower <code>comp()</code> function I’ve thought of searching through ids, phone numbers, accounts and keywords. I’ve thus settled on testing searching 8-byte strings.</p><p>Average run time (ns):</p><table><thead><tr><th></th><th><code>std::lower_</code></th><th><code>branchless_lower_</code></th><th><code>sb_lower_</code></th><th><code>sbm_lower_</code></th><th><code>bb_lower_</code></th></tr></thead><tbody><tr><td>gcc</td><td><strong>160.01</strong></td><td>205.24</td><td>165.66</td><td>193.96</td><td>163.91</td></tr><tr><td>clang</td><td><strong>157.71</strong></td><td>178.77</td><td>162.68</td><td>166.00</td><td><strong>157.22</strong></td></tr><tr><td>clang -cmov</td><td><strong>156.06</strong></td><td>193.70</td><td>164.71</td><td>181.57</td><td>157.48</td></tr></tbody></table><p>In this case <code>std::lower_bound</code> is very slightly but consistently faster than <code>sb_lower_bound</code>. To always get the best performance it is possible for libraries to use <code>sb_lower_bound</code> whenever directly working on primitive types and <code>std::lower_bound</code> otherwise.</p><h2 id="assembly">Assembly</h2><p>No bare-metal code performance optimization is complete without looking at the generated assembly. However, feel free to skip this section.</p><div><pre><code data-lang="js"><span>Time</span><span>%</span> <span>|</span>      <span>instructions</span>

<span>// std::lower_bound, clang -cmov, hottest loop assembly
</span><span></span> <span>1.68</span> <span>│</span><span>20</span><span>:</span>   <span>mov</span>      <span>%</span><span>rsi</span>,<span>%</span><span>rcx</span>             <span>// rcx = length
</span><span></span> <span>3.20</span> <span>│</span>      <span>shr</span>      <span>%</span><span>rcx</span>                  <span>// rcx = length / 2
</span><span></span> <span>1.08</span> <span>│</span>      <span>mov</span>      <span>%</span><span>rcx</span>,<span>%</span><span>rdx</span>             <span>// rdx = length / 2
</span><span></span> <span>4.42</span> <span>│</span>      <span>not</span>      <span>%</span><span>rdx</span>                  <span>// rdx = binary_not(rdx) = -length / 2 - 1
</span><span></span> <span>2.85</span> <span>│</span>      <span>add</span>      <span>%</span><span>rsi</span>,<span>%</span><span>rdx</span>             <span>// rdx = length - length / 2 - 1
</span><span></span><span>63.41</span> <span>│</span>      <span>vucomiss</span> (<span>%</span><span>rax</span>,<span>%</span><span>rcx</span>,<span>4</span>),<span>%</span><span>xmm0</span>   <span>// Compare first[rcx] with value, sets FLAGS
</span><span></span> <span>0.21</span> <span>│</span>      <span>lea</span>      <span>0x4</span>(<span>%</span><span>rax</span>,<span>%</span><span>rcx</span>,<span>4</span>),<span>%</span><span>rsi</span> <span>// rsi = first + length / 2 + 1
</span><span></span><span>10.14</span> <span>│</span>      <span>cmova</span>    <span>%</span><span>rsi</span>,<span>%</span><span>rax</span>             <span>// first = compare_res ? first : rsi
</span><span></span> <span>4.87</span> <span>│</span>      <span>cmovbe</span>   <span>%</span><span>rcx</span>,<span>%</span><span>rdx</span>             <span>// rdx = not compare_res ? length / 2 : rdx
</span><span></span> <span>0.88</span> <span>│</span>      <span>mov</span>      <span>%</span><span>rdx</span>,<span>%</span><span>rsi</span>             <span>// rsi = rdx (new length)
</span><span></span> <span>0.33</span> <span>│</span>      <span>test</span>     <span>%</span><span>rdx</span>,<span>%</span><span>rdx</span>             <span>// Set FLAGS based on rdx
</span><span></span> <span>3.90</span> <span>│</span>    <span>↑</span> <span>jg</span>       <span>20</span>                    <span>// Jump to instruction 20 if rdx not zero
</span><span></span>
<span>// sb_lower_bound, clang -cmov, hottest loop assembly
</span><span></span> <span>4.21</span> <span>│</span><span>20</span><span>:</span>   <span>shr</span>      <span>%</span><span>rcx</span>                  <span>// rcx = length / 2
</span><span></span> <span>4.70</span> <span>│</span>      <span>and</span>      <span>$0x1</span>,<span>%</span><span>esi</span>             <span>// esi = length % 2
</span><span></span> <span>5.00</span> <span>│</span>      <span>add</span>      <span>%</span><span>rcx</span>,<span>%</span><span>rsi</span>             <span>// rsi = length / 2 + length % 2
</span><span></span><span>68.86</span> <span>│</span>      <span>vucomiss</span> (<span>%</span><span>rax</span>,<span>%</span><span>rcx</span>,<span>4</span>),<span>%</span><span>xmm0</span>   <span>// Compare first[rcx] with value, sets FLAGS
</span><span></span> <span>0.01</span> <span>│</span>      <span>lea</span>      (<span>%</span><span>rax</span>,<span>%</span><span>rsi</span>,<span>4</span>),<span>%</span><span>rdx</span>    <span>// rdx = first + length / 2 + length % 2
</span><span></span><span>11.69</span> <span>│</span>      <span>cmova</span>    <span>%</span><span>rdx</span>,<span>%</span><span>rax</span>             <span>// first = compare_res ? first : rdx
</span><span></span> <span>3.71</span> <span>│</span>      <span>mov</span>      <span>%</span><span>rcx</span>,<span>%</span><span>rsi</span>             <span>// rsi = length / 2
</span><span></span>      <span>│</span>      <span>test</span>     <span>%</span><span>rcx</span>,<span>%</span><span>rcx</span>             <span>// Set FLAGS based on rcx
</span><span></span> <span>0.02</span> <span>│</span>    <span>↑</span> <span>jne</span>      <span>20</span>                    <span>// Jump to instruction 20 if rcx not zero
</span><span></span>
<span>// branchless_lower_bound, clang -cmov, hottest loop assembly
</span><span></span> <span>3.04</span> <span>│</span><span>70</span><span>:</span>   <span>lea</span>      (<span>%</span><span>rdi</span>,<span>%</span><span>rcx</span>,<span>4</span>),<span>%</span><span>rax</span>    <span>// rax = first + length
</span><span></span><span>71.22</span> <span>│</span>      <span>vucomiss</span> (<span>%</span><span>rdi</span>,<span>%</span><span>rcx</span>,<span>4</span>),<span>%</span><span>xmm0</span>   <span>// Compare first[rcx] with value, sets FLAGS
</span><span></span><span>12.26</span> <span>│</span>      <span>cmova</span>    <span>%</span><span>rax</span>,<span>%</span><span>rdi</span>             <span>// first = compare_res ? first : rax
</span><span></span> <span>1.03</span> <span>│</span><span>7</span><span>d</span><span>:</span>   <span>shr</span>      <span>%</span><span>rcx</span>                  <span>// length /= 2, but note it also sets FLAGS
</span><span></span> <span>1.64</span> <span>│</span>    <span>↑</span> <span>jne</span>      <span>70</span>                    <span>// Jump to instruction 20 if length not zero
</span></code></pre></div><p>The <code>branchless_lower_bound</code> assembly is really short and clean. While that’s a good indicator of speed, <code>sb_lower_bound</code> wins out in the performance tests due to low overhead.</p><h2 id="conclusion">Conclusion</h2><p>If the slowest part of your program involves searching and/or comparisons that a processor would not be able to predict then try clang with <code>-mllvm -x86-cmov-converter=false</code> (if your processor is x86).</p><p>If you’d benefit from a faster binary search, try <code>sb_lower_bound</code> (or for gcc you could also try <a href="https://github.com/mh-dm/sb_lower_bound/blob/master/sbm_lower_bound.h"><code>sbm_lower_bound</code></a>). I’ve made it open source, MIT license.</p><p>If you want more articles like this, follow me <a href="https://twitter.com/_mhdm">@_mhdm</a> on Twitter (I don’t post often). Alternatively, you can <a href="https://mhdm.dev/index.xml">use RSS</a>.</p><p>Code, including benchmarking, is available at <a href="https://github.com/mh-dm/sb_lower_bound/">github.com/mh-dm/sb_lower_bound/</a>.</p><p>If you have ideas, thoughts, or something to add you can <a href="https://www.reddit.com/r/cpp/comments/14okto7/fastest_branchless_binary_search/">leave a comment here</a>.</p><h2 id="update">Update</h2><p>Following a fruitful comment by <a href="https://orlp.net/">orlp.net author</a>, <code>sb_lower_bound</code> can be refactored slightly to reduce the number of assembly instructions in the hot loop from 9 to 8.</p><div><pre><code data-lang="C++"><span>// sb_lower_bound refactored
</span><span></span><span>auto</span> length <span>=</span> last <span>-</span> first;
<span>while</span> (length <span>&gt;</span> <span>0</span>) {
   <span>auto</span> half <span>=</span> length <span>/</span> <span>2</span>;
   <span>if</span> (comp(first[half], value)) {
      <span>// length - half equals half + length % 2
</span><span></span>      first <span>+=</span> length <span>-</span> half;
   }
   length <span>=</span> half;
}
<span>return</span> first;
</code></pre></div><p>With <code>clang -cmov</code> there’s a slight speed up from ~33ns to ~32ns for the average run time.</p><h3 id="prefetching">Prefetching</h3><p>From comments there was a recommended method for a further speed-up, namely prefetching. Prefetching pulls particular parts of memory into cache (usually L1/L2) so that when prefetched data is actually needed the load only incurs L1/L2 cache latency (~4/12 cycles) vs slower cache (L3, ~40 cycles) or memory latency (~200 cycles). <a href="https://www.7-cpu.com/cpu/Skylake.html">Example timings</a>. For this there’s <code>__builtin_prefetch()</code> supported in both gcc and clang.</p><p>Say we’re going to check against the element at <code>length / 2</code>. We could prefetch at <code>length / 4</code> and <code>length * 3 / 4</code>. Or we could also prefetch at <code>length / 8</code> divisions, which is an extra 4 memory locations. For <code>length / 4</code> divisions, 1 out of every 2 prefetches will be wasted, doubling cache pressure. If also <code>length / 8</code> divisions, 5 out of every 6 prefetches will be wasted. There’s overhead in computing the locations and prefetching, overhead that will be significant in the hot loop we worked hard to make short.</p><p>Finally, if we’ve already made a few full searches the initial divisions are likely to be in the cache as many elements should fit in modern 256KB+ L2 caches.</p><p>When trying various prefetching strategies, none helped for under 256KB arrays, which is about what we expect. Long story short, here’s <code>sb_lower_bound</code> but with prefetching added in for 256KB+:</p><div><pre><code data-lang="C++"><span>// sbp_lower_bound
</span><span></span><span>auto</span> length <span>=</span> last <span>-</span> first;
<span>// Sized to roughly fit in L2 cache
</span><span></span><span>constexpr</span> <span>int</span> entries_per_256KB <span>=</span> <span>256</span> <span>*</span> <span>1024</span> <span>/</span> <span>sizeof</span>(T);
<span>if</span> (length <span>&gt;=</span> entries_per_256KB) {
   <span>constexpr</span> <span>int</span> num_per_cache_line <span>=</span> std<span>::</span>max(<span>64</span> <span>/</span> <span>int</span>(<span>sizeof</span>(T)), <span>1</span>);
   <span>while</span> (length <span>&gt;=</span> <span>3</span> <span>*</span> num_per_cache_line) {
      <span>auto</span> half <span>=</span> length <span>/</span> <span>2</span>;
      __builtin_prefetch(<span>&amp;</span>first[half <span>/</span> <span>2</span>]);
      <span>// length - half equals half + length % 2
</span><span></span>      <span>auto</span> first_half1 <span>=</span> first <span>+</span> (length <span>-</span> half);
      __builtin_prefetch(<span>&amp;</span>first_half1[half <span>/</span> <span>2</span>]);
      first <span>=</span> comp(first[half], value) <span>?</span> first_half1 : first;
      length <span>=</span> half;
   }
}
<span>while</span> (length <span>&gt;</span> <span>0</span>) {
   <span>auto</span> half <span>=</span> length <span>/</span> <span>2</span>;
   <span>auto</span> first_half1 <span>=</span> first <span>+</span> (length <span>-</span> half);
   first <span>=</span> comp(first[half], value) <span>?</span> first_half1 : first;
   length <span>=</span> half;
}
<span>return</span> first;
</code></pre></div><p>Tested in the same way as before, for sizes ranging up to ~4 million entries (or 16MB), there’s a further speed up from ~32ns to ~26ns average run time. At this point I should acknowledge that my original size stopping point happened to be too small. A case of a quick ‘should be larger than L3 cache size’ and a mistake in not following up. So let’s go much higher, now to ~128 million entries (or 512MB). We’re way past L3 but still in reasonable data set size.</p><p>Runtimes in line chart form:</p><p><a href="https://mhdm.dev/posts/sb_lower_bound/runtimes2.png"><img src="https://mhdm.dev/posts/sb_lower_bound/runtimes2.png" alt="Runtimes 2"></a></p><p>There’s some interesting stuff going on.</p><ul><li>Branchless <code>std::lower_bound</code> that’s generated with <code>clang -cmov</code> is slower than the branchy version at massive sizes. Modern cpus follow predicted branches including loading from memory (basically a prefetch) and speculatively executing on said data (basically a <a href="https://en.wikipedia.org/wiki/Transient_execution_CPU_vulnerability">security nightmare</a>).</li><li><code>sbpm_lower_bound</code> is the prefetching version of <code>sbm_lower_bound</code> which does a multiply with a boolean to trick gcc into generating branchless code.</li><li>We’re at ~2.3x faster average time comparing <code>std::lower_bound</code> (~161ns) to prefetched version (~71ns).</li></ul><h3 id="faster">Faster?</h3><h4 id="drop-in-replacement-for-stdlower_bound">Drop in replacement for <code>std::lower_bound</code></h4><p>There’s a performance graph bump/weirdness between 1-10 million elements so faster is possible in theory. In practice the prefetching code is getting messy and gaining magic constants. An exciting part for me in writing this post was the (small) probability of contributing back to <a href="https://gcc.gnu.org/contribute.html">gcc/libstdc++</a> and/or <a href="https://libcxx.llvm.org/Contributing.html">llvm/libc++</a>. I’ll stop here as that probability gets even smaller with added complexity.</p><h4 id="breaking-stdlower_bound-constraints">Breaking <code>std::lower_bound</code> constraints</h4><p>Comments also noted the interesting <a href="https://algorithmica.org/en/eytzinger">Eytzinger Binary Search</a> variant where the input array is reshaped (into a binary med-“heap”) for lookups to be cache friendly. I could ponder how fast a SIMD optimized K-ary tree could be, but no need to ponder: 7x to 15x faster than <code>std::lower_bound</code> (for 16-ary tree of ints) as presented by <a href="https://www.youtube.com/watch?v=1RIPMQQRBWk">Sergey Slotin at CppCon 2022</a>.</p><h4 id="footnotes">Footnotes</h4><section role="doc-endnotes"><hr><ol><li id="fn:1" role="doc-endnote"><p>BUT RUST.. Rust is fast but do you really want my first post to be <a href="https://doc.rust-lang.org/src/core/slice/mod.rs.html#2520">about unsafe code</a>? <a href="#fnref:1" role="doc-backlink">↩︎</a></p></li><li id="fn:2" role="doc-endnote"><p>BUT ZIG.. There’s no binary search implementation in Zig that I could find, rather it calls to C++. <a href="#fnref:2" role="doc-backlink">↩︎</a></p></li><li id="fn:3" role="doc-endnote"><p>There’s always a <a href="https://m.xkcd.com/149/">relevant XKCD</a>. <a href="#fnref:3" role="doc-backlink">↩︎</a></p></li><li id="fn:4" role="doc-endnote"><p>With clang we could do <a href="https://clang.llvm.org/docs/LanguageExtensions.html#builtin-unpredictable"><code>__builtin_unpredictable</code></a><code>(comp(first[half], value))</code> but it does nothing (tested v10-13). <a href="#fnref:4" role="doc-backlink">↩︎</a></p></li><li id="fn:5" role="doc-endnote"><p>gcc has <a href="https://gcc.gnu.org/onlinedocs/gcc/Other-Builtins.html#index-_005f_005fbuiltin_005fexpect_005fwith_005fprobability"><code>__builtin_expect_with_probability</code></a><code>(cond, 0, 0.5)</code> but it does nothing (tested v10). <a href="#fnref:5" role="doc-backlink">↩︎</a></p></li></ol></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[There is no hard takeoff (109 pts)]]></title>
            <link>https://geohot.github.io//blog/jekyll/update/2023/08/10/there-is-no-hard-takeoff.html</link>
            <guid>37086779</guid>
            <pubDate>Fri, 11 Aug 2023 09:34:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://geohot.github.io//blog/jekyll/update/2023/08/10/there-is-no-hard-takeoff.html">https://geohot.github.io//blog/jekyll/update/2023/08/10/there-is-no-hard-takeoff.html</a>, See on <a href="https://news.ycombinator.com/item?id=37086779">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>Back in 2014, Elon Musk referred to AI as <a href="https://www.washingtonpost.com/news/innovations/wp/2014/10/24/elon-musk-with-artificial-intelligence-we-are-summoning-the-demon/">summoning the demon</a>. And it wasn’t hard to see that view. Soon, <a href="https://en.wikipedia.org/wiki/AlphaGo">Go agents</a> would beat top humans learning from self play. By the end of 2017, the <a href="https://arxiv.org/abs/1712.01815">same algorithm</a> mastered Chess and Shogi. By 2020, it didn’t even need tons of calls to the simulator, and could <a href="https://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules">play Atari too</a>.</p>

<p>AI looked scary. It looked like it was one FOOM away from self playing and becoming superhuman at the universe. And yet, here we are in 2023 and self driving cars still don’t work.</p>

<p>Does becoming superhuman at the universe make any practical sense? The universe has so many orders of magnitude more states than any Go game. And I don’t even need math to illustrate this point, just imagine tiling the universe with as many very tiny Go boards as you can fit.</p>

<p>That’s a lot of Go boards. So many that the difference in complexity between Go and the Universe is not a matter of number, it’s a matter of kind. Every Go program operates at the stone level. Almost nothing predicting the world operates at the atom level.</p>

<hr>


<p>These modern self play systems like <a href="https://arxiv.org/pdf/1911.08265.pdf">MuZero</a> have some form of dynamics model, a function that given the current state of the world and an action, it predicts the next state. We also use these dynamics models <a href="https://twitter.com/comma_ai/status/1681491118536691712">at comma</a>. They are world models, and contain all the knowledge of how the world works inside of them.</p>

<p>GPT-4 is a dynamics model also, conditioned on the prior of the action space. And there’s an even simpler way to think about it. The loss function for dynamics is compression.</p>

<p><a href="http://www.hutter1.net/ai/">Compression is prediction is intelligence, intelligence is prediction is compression.</a> One of the coolest facts I ever learned. So, feed in the whole internet, build a compressive model, make it really really big, and you just won the universe?</p>

<hr>


<p>Not so fast. This approach can lead to <a href="https://chat.openai.com/">very neat applications</a>, but does it take over the world?</p>

<p>You want to get rich? Here’s an idea. Download all the historical stock market data. Get lots and lots of GPUs to train a huge model. Boom, predictor for the stock market. I look at prediction, I know which stocks go up! I buy the stocks that go up, I short the stocks that go down. With mega leverage, I am a billionaire by the end of the week! I can reinvest my billions in more GPUs for recursive self improvement. Nobody will stop me, I will take over the whole economy. How is nobody doing this?!?</p>

<p>Oh wait…every hedge fund bro is already doing this. And most of them aren’t billionaires. The problem is your model needs to include all the computers playing the market, and it also needs to include the other hedge fund bros themselves. This strategy only dominates if you have more compute than the whole market itself, which you don’t.</p>

<hr>


<p>In Go, your model doesn’t need to include other computers. Yes, you are playing against a computer that you are modeling, but the game itself is way too small to contain a Go playing computer. The other computer you are playing against is outside the universe. The rules of Go don’t include computers. The rules of Go are fixed in complexity.</p>

<p>On the other hand, your dynamics model of the universe must include computers. You’ll have to spend a lot of effort modeling them. Unless you have an absolutely staggering advantage, you aren’t going to figure them out from self play. The computers are active players, and they have a lot of compute. They might even be using it to try to model you!</p>

<p>Assuming <a href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/">no untoward market intervention</a>, why would any one system ever have a large majority of the compute? Compute will be distributed in a <a href="https://en.wikipedia.org/wiki/Power_law">power law</a>.</p>

<p>The smart regulation isn’t capping the FLOPS in training runs. That’s creating a powder keg. If the FLOPS are artificially restricted, and one person breaks the restriction, you could end up with a single dominant system.</p>

<p>If you don’t want FOOM, you just need to prevent a 51% attack on compute.</p>

<hr>


<p>Now, if there’s one weird trick to 1e20x your efficiency, and only one group gets it, all bets are off. But this is never how things happen. Nobody has a 1e20x more efficient steam engine, it isn’t even possible.</p>

<p>Nobody has a 1e20x more efficient Bitcoin miner either, and I also doubt that’s possible, we just <a href="https://en.wikipedia.org/wiki/Thermodynamics">understand</a> steam engines a lot better, so for steam engines we <em>know</em>. More intelligence leads to more new tricks, but the tricks get harder and harder to find.</p>

<p>There’s low hanging fruit, you pick it, then you build tools to get the higher hanging fruit. You spend money on ladders, electric crane thingies, more and more money to get less and less fruit. Oil <a href="https://en.wikipedia.org/wiki/Petroleum_seep">used to be</a> just pouring out of the ground, now we go <a href="https://www.indelac.com/blog/introduction-to-oil-gas-offshore-drilling">to the bottom</a> of the ocean.</p>

<hr>


<p>A revolution is coming. The information revolution will do for intelligence what the industrial revolution did for energy. Most work (force*distance) used to be done by muscles, now it’s not. Most thinking used to be done by brains, soon it won’t be.</p>

<p>But it’s not going to happen overnight. It’ll happen on a nice exponential, like it already is. The universe is an unfathomable number of orders of magnitude more complex than the Go game. The universe includes the player. The universe includes the other players. Games don’t.</p>

<p><img src="https://geohot.github.io/blog/assets/images/trolley.jpg" height="250"></p>

<p>Unless we build a terrifying powder keg, there is no FOOM. Let the markets cook.</p>

  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Artificial General Intelligence – A gentle introduction (103 pts)]]></title>
            <link>https://cis.temple.edu/~pwang/AGI-Intro.html</link>
            <guid>37086308</guid>
            <pubDate>Fri, 11 Aug 2023 08:15:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cis.temple.edu/~pwang/AGI-Intro.html">https://cis.temple.edu/~pwang/AGI-Intro.html</a>, See on <a href="https://news.ycombinator.com/item?id=37086308">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="ltr" id="sites-canvas-main"><center><span size="5"><b><br>
</b></span></center>
<center>
<span size="5"><b>Artificial General Intelligence</b></span></center>
<center>
<span size="4">— A gentle introduction</span></center>
<center><span size="4"><br>
</span></center>
<center>
<i><a href="http://www.cis.temple.edu/%7Epwang/" rel="nofollow">Pei Wang</a></i>
</center>
<p><span><span size="2">[This page contains up-to-date information about the field of Artificial General Intelligence (AGI), collected and organized according to my judgment, though efforts are made to avoid personal biases.] </span></span></p>
<div><p><span size="2"><div><p>Contents</p><ol><li><a href="#TOC-From-AI-to-AGI"><strong>1 </strong>From AI to AGI</a><ol><li><a href="#TOC-AI:-in-different-directions-and-through-seasonal-cycles"><strong>1.1 </strong>AI: in different directions, and through seasonal cycles</a></li><li><a href="#TOC-A-new-spring"><strong>1.2 </strong>A new spring</a></li><li><a href="#TOC-It-s-summer-again"><strong>1.3 </strong>It's summer again</a></li></ol></li><li><a href="#TOC-AGI-Basics"><strong>2 </strong>AGI Basics</a><ol><li><a href="#TOC-What-is-AGI"><strong>2.1 </strong>What is AGI</a></li><li><a href="#TOC-Limitations-and-objections"><strong>2.2 </strong>Limitations and objections</a></li><li><a href="#TOC-Strategies-and-techniques"><strong>2.3 </strong>Strategies and techniques</a></li><li><a href="#TOC-The-ethics-of-AGI"><strong>2.4 </strong>The ethics of AGI</a></li></ol></li><li><a href="#TOC-Representative-AGI-Projects"><strong>3 </strong>Representative AGI Projects</a></li><li><a href="#TOC-AGI-Literatures-and-Resources"><strong>4 </strong>AGI Literatures and Resources</a></li></ol></div></span></p></div>
<h2><a name="TOC-From-AI-to-AGI"></a>From AI to AGI</h2>
<h3><a name="TOC-AI:-in-different-directions-and-through-seasonal-cycles"></a>AI: in different directions, and through seasonal cycles</h3><p>
    Artificial Intelligence (AI) started with "thinking machine" or
    "human-comparable intelligence" as the ultimate goal, as documented by
    the following literature:
    
</p><ul>
<li><a href="http://cogprints.org/499/1/turing.html" rel="nofollow">Computing
          machinery and intelligence</a>, 1950</li>
<li><a href="http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html" rel="nofollow">Proposal of the Dartmouth Meeting</a>, 1956</li>
<li><a href="https://mitpress.mit.edu/books/computers-and-thought-1" rel="nofollow">Computers and Thought</a>, 1963</li>
</ul><p>
    In the past, there were some ambitious projects aiming at this goal,
    though they all failed. The best-known examples include the
    following ones:
</p><ul>
<li><a href="https://en.wikipedia.org/wiki/General_Problem_Solver" rel="nofollow">General Problem Solver</a> </li>
<li><a href="http://en.wikipedia.org/wiki/Fifth_generation_computer" rel="nofollow">Fifth
          Generation Computer Systems</a> </li>
<li><a href="https://en.wikipedia.org/wiki/Strategic_Computing_Initiative" rel="nofollow">DARPA's Strategic Computing Initiative</a></li>
</ul><p>
    Partly due to the recognized difficulty of the problem, in the
    1970s-1980s mainstream AI gradually moved away from general-purpose
    intelligent systems, and turned to domain-specific problems and
    special-purpose solutions, though there are opposite attitudes
    toward this change:
</p><ul>
<li>"<a href="https://www.americanscientist.org/article/the-manifest-destiny-of-artificial-intelligence" rel="nofollow">AI adopts
          the scientific method (1987-present): ... It is now more
        common to build on existing theories than to propose brand new
        ones, to base claims on rigorous theorems or hard experimental
        evidence rather than on intuition, and to show relevance to
        real-world applications rather than toy examples.</a>"</li>
<li>"<a href="https://www.wired.com/2003/08/why-a-i-is-brain-dead/" rel="nofollow">Only a small community has concentrated on general intelligence ... AI has been brain-dead since the 1970s.</a>" </li>
</ul><p>
    Consequently, the field currently called "AI" consists of many
    loosely related subfields without a common foundation or framework,
    and suffers from an identity crisis:
</p><ul>
<li><a href="http://en.wikipedia.org/wiki/AI_effect" rel="nofollow">External
          recognition</a>: As soon as a problem is solved, it is no
        longer considered as requiring "intelligence" anymore, so the AI community rarely gets credit. </li>
<li><a href="http://www.aaai.org/Library/President/Brachman.pdf" rel="nofollow">Internal
          fragmentation</a>: The subfields of AI become less and less
        associated to one another, even though their problems are closely
        related. </li>
</ul>
<h3><a name="TOC-A-new-spring"></a>A new spring</h3><p>
    Roughly in the period of 2004 to 2007, calls for research on
    general-purpose systems returned, both inside and outside mainstream
    AI.
    
</p><p> Anniversaries are good time to review the big picture of the
      field. In the following collections and events, many
      well-established AI researchers raised the topic of
      general-purpose and human-level intelligence: </p>
<ul>
<li><a href="http://www.aaai.org/ojs/index.php/aimagazine/issue/view/161/showToc" rel="nofollow">AI
          Magazine 26(4), Winter 2005</a>: for the 25th Anniversary of
        AAAI and AI Magazine </li>
<li><a href="http://www.aaai.org/ojs/index.php/aimagazine/issue/view/165/showToc" rel="nofollow">AI
          Magazine 27(4), Winter 2006</a>: for the 50th Anniversary of
        AI </li>
<li><a href="https://en.wikipedia.org/wiki/AI@50" rel="nofollow">AI@50</a>:
        2006 Dartmouth Artificial Intelligence Conference: The Next
        Fifty Years </li>
</ul><p>
    More or less coincidentally, from outside mainstream AI, there were several books with bold titles and novel technical approaches to produce
    intelligence as a whole in computers:
</p><ul>
<li>Eric Baum, <a href="http://www.whatisthought.com/" rel="nofollow">What is
          Thought?</a>, 2004 </li>
<li>Jeff Hawkins, <a href="https://numenta.com/resources/papers-videos-and-more/on-intelligence/" rel="nofollow">On Intelligence</a>, 2004 </li>
<li>Marcus Hutter, <a href="http://www.hutter1.net/ai/uaibook.htm" rel="nofollow">Universal
          Artificial Intelligence</a>, 2005 </li>
<li>Pei Wang, <a href="http://www.springer.com/west/home/computer/artificial?SGWID=4-147-22-173659733-0" rel="nofollow">Rigid
Flexibility:
          The Logic of Intelligence</a>, 2006 [The manuscript was finished in 2003.]</li>
<li>Ben Goertzel &amp; Cassio Pennachin (Editors), <a href="http://www.springer.com/sgw/cda/frontpage/0,11855,4-147-22-43950079-0,00.html" rel="nofollow">Artificial General Intelligence</a>, 2007 [The manuscript was finished in 2003.] </li>
</ul><p>
    There were also several less technical but more influential books,
    with the same optimism on the possibility of building
    general-purpose AI:
    
</p><ul>
<li>Ray Kurzweil, <a href="http://www.singularity.com/aboutthebook.html" rel="nofollow">The
          Singularity Is Near: When Humans Transcend Biology</a>, 2005 </li>
<li>Marvin Minsky, <a href="http://en.wikipedia.org/wiki/The_Emotion_Machine" rel="nofollow">The
          Emotion Machine: Commonsense Thinking, Artificial
          Intelligence, and the Future of the Human Mind</a>, 2006 </li>
<li>Ben Goertzel, <a href="http://www.brownwalker.com/book.php?method=ISBN&amp;book=1581129890" rel="nofollow">The
          Hidden Pattern: A Patternist Philosophy of Mind</a>, 2006 </li>
<li>J. Storrs Hall, <a href="https://www.google.com/books/edition/Beyond_AI/j6ofAQAAIAAJ?hl=en" rel="nofollow">Beyond AI: Creating the Conscience of the Machine</a>, 2007 </li>
</ul><p>
    So after several decades, "general-purpose system", "integrated AI",
    and "human-level AI" become less taboo (though still far from popular)
    topics, as shown by several related meetings:
    
</p><ul>
<li><a href="http://www.aaai.org/Library/Symposia/Fall/fs04-01.php" rel="nofollow">Achieving Human-Level Intelligence through Integrated Systems and Research, AAAI Fall Symposium (2004)</a>
</li>
<li><a href="http://www-cs.stanford.edu/groups/nips05-AI-Workshop/" rel="nofollow">Towards Human-Level AI?, NIPS Workshop (2005)</a>
</li>
<li><a href="http://www.aaai.org/Conferences/AAAI/2006/aaai06iictrack.php" rel="nofollow">AAAI conferences special track on Integrated Intelligent
        Capabilities (2006)</a>
</li>
<li><a href="http://www.iospress.nl/book/advances-in-artificial-general-intelligence-concepts-architectures-and-algorithms/" rel="nofollow">Artificial General Intelligence Workshop (2006)</a></li>
</ul>
<h3><a name="TOC-It-s-summer-again"></a>It's summer again</h3><p>

Since 2008, several research communities have emerged, with similar focuses and overlapping participants:
</p><ul>
<li>Artificial General Intelligence: <a href="http://www.agi-conf.org/" rel="nofollow">conferences</a>, <a href="https://sciendo.com/journal/JAGI" rel="nofollow">journal</a>, <a href="http://www.agi-society.org/" rel="nofollow">society</a> </li>
<li>Biologically Inspired Cognitive Architectures: <a href="https://bica.ai/" rel="nofollow">conferences</a>, <a href="http://www.sciencedirect.com/science/journal/2212683X" rel="nofollow">journal</a>,
        <a href="http://www.bicasociety.org/" rel="nofollow">society</a> </li>
<li>Advances in Cognitive Systems: <a href="http://www.cogsys.org/journal" rel="nofollow">journal</a>, <a href="http://www.cogsys.org/" rel="nofollow">conferences</a></li>
<li>IEEE Task Force on Towards Human-like Intelligence: <a href="http://www.mini.pw.edu.pl/%7Emandziuk/cis_tf_thli/" rel="nofollow">website</a>, <a href="http://www.mini.pw.edu.pl/~mandziuk/cis_tf_thli/?page=activities" rel="nofollow">conferences</a></li>
</ul><p>
More research books have been published:
</p><ul>
<li>Joscha Bach, <a href="https://global.oup.com/academic/product/principles-of-synthetic-intelligence-psi-an-architecture-of-motivated-cognition-9780195370676" rel="nofollow">Principles of Synthetic Intelligence PSI: An Architecture of Motivated Cognition</a>, 2009</li>
<li>John Laird, <a href="http://mitpress.mit.edu/books/soar-cognitive-architecture" rel="nofollow">The Soar Cognitive Architecture</a>, 2012</li>
<li>Pei Wang and Ben Goertzel (Editors), <a href="http://www.springer.com/computer/ai/book/978-94-91216-61-9" rel="nofollow">Theoretical
          Foundations of Artificial General Intelligence</a>, 2012</li>
<li>Pei Wang, <a href="http://www.worldscientific.com/worldscibooks/10.1142/8665" rel="nofollow">Non-Axiomatic
Logic: A Model of Intelligent Reasoning</a>, 2013</li>
<li>Ben Goertzel <i>et al.</i>, Engineering General Intelligence, <a href="http://www.springer.com/computer/ai/book/978-94-6239-026-3" rel="nofollow">Part 1</a> and <a href="http://www.springer.com/computer/ai/book/978-94-6239-029-4" rel="nofollow">Part 2</a>, 2014</li>
</ul>
<p>In mainstream AI, <a href="https://en.wikipedia.org/wiki/Deep_learning" rel="nofollow">deep learning</a> has made impressive progress in recent years, which raises many people's hope on "human-level" AI once again. The claim <a href="https://www.telegraph.co.uk/technology/news/10884839/Computer-passes-Turing-Test-for-the-first-time-after-convincing-users-it-is-human.html" rel="nofollow">"The Turing Test has been passed"</a> and the success of <a href="https://en.wikipedia.org/wiki/AlphaGo" rel="nofollow">AlphaGo</a> in the board game Go renewed the discussion on what "artificial intelligence" is really about, and how to reach it. There is still no consensus, and the <a href="https://content.sciendo.com/view/journals/jagi/11/2/article-p1.xml" rel="nofollow">opinions</a> are not even converging. Several large companies have labeled their results as "steps towards AGI", and their approaches are either extensions of deep learning or integrations of the existing AI techniques. This approach is exemplified by <a href="https://openai.com/research/gpt-4">GPT-4</a>, which is claimed by its creator as "a significant step towards AGI".
</p>
<p>
Partly triggered by the recent progresses, more and more people consider AGI, or whatever it is called, as really possible. As a consequence, the risk and safety of it becomes a hot topic:
</p><ul>
<li><a href="https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies" rel="nofollow">Superintelligence: Paths, Dangers, Strategies</a> by Nick Bostrom, 2014</li>
<li><a href="hhttps://futureoflife.org/open-letter/pause-giant-ai-experiments/" rel="nofollow">Pause Giant AI Experiments: An Open Letter</a> from the Future of Life Institute, 2023</li>
</ul>

<h2><a name="TOC-AGI-Basics"></a>AGI Basics</h2>
<div><p>The most general questions every AGI researcher needs to answer include:
  </p><ol><li>What is AGI, accurately specified?</li>
<li>Is it possible to build the AGI as specified?</li>
<li>If AGI is possible, what is the most plausible way to achieve it?</li>
<li>Even if we know how to achieve AGI, should we really do it?</li>
</ol><p>
[My own answers to these questions are <a href="http://www.iiim.is/2010/05/questions-about-artificial-intelligence/" rel="nofollow">here</a>.]</p></div>
<p>In the following the major answers in the field of AGI are summarized.</p>
<h3><a name="TOC-What-is-AGI"></a>What is AGI</h3><p>
Roughly speaking, Artificial General Intelligence (AGI) research has the following features:
</p><ul>
<li>Stressing on the <i>general-purpose</i> nature of intelligence,</li>
<li>Taking a <i>holistic or integrative</i> viewpoint on intelligence,</li>
<li>Believing the time has come to build an AI that is comparable to human intelligence.</li></ul><p>
Therefore, "AGI" is closer to the original meaning "AI", while very different from the current mainstream "AI research", which focuses on domain-specific and problem-specific methods. "AGI" is similar or related to notions like "<a href="http://en.wikipedia.org/wiki/Strong_AI" rel="nofollow">strong AI</a>", "<a href="http://www-formal.stanford.edu/jmc/human/human.html" rel="nofollow">human-level AI</a>", "<a href="http://en.wikipedia.org/wiki/AI-complete" rel="nofollow">complete AI</a>", "<a href="http://cogprints.org/499/1/turing.html" rel="nofollow">thinking machine</a>", "<a href="http://en.wikipedia.org/wiki/Cognitive_computing" rel="nofollow">cognitive computing</a>", and some others. <a href="https://goertzel.org/who-coined-the-term-agi/">Here</a> is an explanation about the selection of the term "AGI".
</p><p>
Even though there is a vague consensus on the objective of reproducing "intelligence" as a whole in computers, the current AGI projects are not aimed at exactly the same goal. Though every AGI approach gets its inspiration from the same source, that is, human intelligence, here "intelligence" is understood in several senses. Consequently, AGI projects attempt to duplicate human intelligence at different levels of abstraction:
</p><ul>
<li><b>Structure</b><br>
            Rationale: Intelligence is produced by the human brain. Therefore, to build an intelligent computer means to simulate the brain structure as faithfully as possible.<br>
            Background: Neuroscience, biology, etc.<br>
            Examples:&nbsp; <a href="http://en.wikipedia.org/wiki/Hierarchical_Temporal_Memory" rel="nofollow">HTM</a>,&nbsp;<a href="https://www.vicarious.com/" rel="nofollow">Vicarious</a><br>
            Challenge: There may be biological details that are neither
            possible nor necessary to be reproduced in AI systems.
      </li>
<li><b>Behavior</b><br>
            Rationale: Intelligence is displayed in how the human beings
            behave. Therefore, the goal should be to make a computer to behave exactly like a human.<br>
            Background: Psychology, linguistics, etc.<br>
            Examples: <a href="http://en.wikipedia.org/wiki/Turing_test" rel="nofollow">Turing
              Test</a>, <a href="https://openai.com/blog/chatgpt/" rel="nofollow">ChatGPT</a><br>
            Challenge: There may be psychological or social factors that are
            neither possible nor necessary to be reproduced in AI
            systems.
      </li>
<li><b>Capability</b><br>
            Rationale: Intelligence is evaluated by problem-solving capability. Therefore, an intelligent system should be able to solve certain practical problem that is currently solvable by humans only.<br>
            Background: Computer application guided by domain knowledge<br>
            Examples: <a href="https://en.wikipedia.org/wiki/IBM_Watson" rel="nofollow">IBM Watson</a>, <a href="https://en.wikipedia.org/wiki/AlphaGo" rel="nofollow">AlphaGo</a><br>
            Challenge: There is no defining problems of intelligence, and the
            special-purpose solutions lack generality and flexibility.
      </li>
<li><b>Function</b><br>
            Rationale: Intelligence is associated to a collection of cognitive
            functionality, such as perceiving, reasoning, learning, acting, communicating, problem solving, etc. Therefore the goal is to reproduce these functions in computers.<br>
            Background: Computer science<br>
            Examples: <a href="http://aima.cs.berkeley.edu/chapters.html" rel="nofollow">Mainstream
              AI textbooks</a>, <a href="http://soar.eecs.umich.edu/" rel="nofollow">Soar</a><br>
            Challenge: The AI techniques developed so far are highly
            fragmented and rigid, and it is hard for them to work together.
      </li>
<li><b>Principle</b><br>
            Rationale: Intelligence is a form of rationality or
            optimality. Therefore, an intelligent system should always "do the right thing" according to certain general principles.<br>
            Background: Logic, mathematics, etc.<br>
            Examples: <a href="http://www.hutter1.net/ai/uaibook.htm" rel="nofollow">AIXI</a>, <a href="https://cis.temple.edu/~pwang/NARS-Intro.html">NARS</a>
<br>
            Challenge: There are too many aspects in intelligence and cognition to be explained and reproduced by a
            simple theory.</li></ul>
<p>
From top to bottom, they correspond to descriptions of human intelligence in more and more general level, and to reproduce that description in computer systems. Since different descriptions have different granularity and scope, the above objectives are related, but still very different, and do not subsume each other. The best way to achieve one is usually not a good choice for the others. [A more detailed discussion of this issue can be found <a href="https://content.sciendo.com/view/journals/jagi/10/2/article-p1.xml" rel="nofollow">here</a>.]
</p><p>
The "general purpose" nature of AGI has also obtained different interpretations over the years, as meaning
</p>
<p><i><ol>
<li>Can solve all problems,</li>
<li>Can solve all human-solvable problems,</li>
<li>Can solve all computable problems,</li>
<li>Can try to solve all representable problems.</li>
</ol></i></p><p>
Because of this diversity in research goal, in AGI currently there is no commonly accepted evaluation criteria (such as milestones and benchmarks).

</p>
<h3><a name="TOC-Limitations-and-objections"></a>Limitations and objections</h3><p>

Since the idea of AI or "thinking machine" appeared, there have been various objections against its possibility. Some people claimed that they have proved that AGI, or whatever it is called, is theoretically impossible, due to certain fundamental limitations of computers.
</p><p>
Many researchers have argued against these objections. Classical arguments can be found in the following works:
</p>
<ul>
<li><a href="http://cogprints.org/499/1/turing.html" rel="nofollow">Computing machinery and
    intelligence</a>, Alan M. Turing</li>
<li><a href="http://en.wikipedia.org/wiki/G%C3%B6del%2C_Escher%2C_Bach" rel="nofollow">G<span color="#6a6a6a" face="Arial">ö</span>del, Escher, Bach: An Eternal Golden Braid</a>, Douglas R. Hofstadter</li>
</ul><p>
Obviously, all AGI researchers believe that AGI can be achieved (though they have different interpretations to the term). In the <a href="http://www.cis.temple.edu/~pwang/Publication/AGI_Aspects.pdf" rel="nofollow">introductory chapter of the AGI 2006 Workshop Proceedings</a>, I and Ben Goertzel responded to the following common doubts and objections of this research:
</p><ul><i>
<li>AGI is impossible.</li>
<li>There is no such a thing as general intelligence.</li>
<li>General-purpose systems are not as good as special-purpose ones.</li>
<li>AGI is already included in the current AI.</li>
<li>It is too early to work on AGI.</li>
<li>AGI is nothing but hype.</li>
<li>AGI research is not fruitful.</li>
<li>AGI is dangerous.</li>
</i></ul>
<p>Some of the doubts about the possibility of AGI come from misconceptions on what AGI attempts to achieve or what computers can do. The previous subsection has clarified the former issue, while an analysis of the latter issue can be found <a href="http://www.cis.temple.edu/~pwang/Publication/AI_Misconceptions.pdf" rel="nofollow">here</a>.</p>
<h3><a name="TOC-Strategies-and-techniques"></a>Strategies and techniques</h3><p>

On one hand, the ultimate goal of AGI is to reproduce intelligence as a whole, while on the other hand, engineering practice must be step-by-step. To resolve this dilemma, three overall strategies have been proposed:
    
</p><ul>
<li><b>Hybrid</b><br>
            Approach: To develop individual functions first (using
            different theories and techniques), then to connect them
            together.<br>
            Argument: <a href="http://www.aaai.org/Library/President/Brachman.pdf" rel="nofollow">(AA)AI:
              More than the Sum of Its Parts</a>, Ronald Brachman<br>
            Difficulty: Compatibility of the theories and techniques
      </li>
<li><b>Integrated</b><br>
            Approach: To design an architecture first, then to design
            its modules (using various techniques) accordingly.<br>
            Argument: <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.233.1596" rel="nofollow">Cognitive
              Synergy: A Universal Principle for Feasible General
              Intelligence?</a>, Ben Goertzel<br>
            Difficulty: Isolation, specification, and coordination of
            the functions
      </li>
<li><b>Unified</b><br>
            Approach: Using a single technique to start from a core
            system, then to extend and augment it incrementally.<br>
            Argument: <a href="http://www.cis.temple.edu/%7Epwang/Publication/unifiedAI.pdf" rel="nofollow">Toward
              a Unified Artificial Intelligence</a>, Pei Wang<br>
            Difficulty: Versatility and extensibility of the core technique
      </li>
</ul><p>
Obviously, the selection of development strategy partially depends on the selection of the research objective.
</p>
<div><p>At the current time, the major techniques used in AGI projects include, though are not limited to:
</p><ul><li>logic</li>
<li>probability theory</li>
<li>production system</li>
<li>graph theory</li>
<li>knowledge base</li>
<li>learning algorithm</li>
<li>neural network</li>
<li>evolutionary computation</li>
<li>robotics</li>
<li>multi-agent system</li></ul><p>
Though each of these techniques is also explored in mainstream AI, to use it in a general-purpose system leads to very different design decisions in technical details.</p><h3><a name="TOC-The-ethics-of-AGI"></a>The ethics of AGI</h3>
<p>Even if we have found out how to achieve AGI, it does not necessarily mean we really want to do it. Like all major scientific discoveries and technical breakthroughs, AGI has the potential to revolutionize our life and even the fate of the human species, either in a desired way or an undesired way — or, as things usually go, a mixture of the two.
</p>
<p>AGI researchers are aware of their responsibility on this topic, though most of them think that, according to the currently available evidence, progress in AGI research will benefit the human species, rather than to destroy it. Discussions on how to make AGI "safe" have existed in AGI meetings since the very beginning. Sample discussions include</p>
<ul>
<li><a href="http://agi-conf.org/2008/" rel="nofollow">AGI-08</a> had a workshop on <a href="http://agi-conf.org/2008/workshop/" rel="nofollow">The Sociocultural, Ethical and Futurological Implications of Artificial General Intelligence</a></li>
<li><a href="http://agi-conf.org/2012/" rel="nofollow">AGI-12</a> was jointly sponsored by Oxford's <a href="https://www.fhi.ox.ac.uk/" rel="nofollow">Future of Humanity Institute</a>. In this conference, many papers address ethical and moral issues</li>
</ul>
<p>Of course, many crucial problems remain open, but to find their solutions, the research of AGI should be speed up, rather than slowed down. Once again, some wide-spreading concerns and fears about AGI are based on misconceptions about the nature of AGI.</p>
	
<h2><a name="TOC-Representative-AGI-Projects"></a>Representative AGI Projects</h2><p>

The following projects are selected to represent the current AGI research, as for each of them, it can be said that</p></div>
<div>
<ol><li>It is clearly oriented to AGI (that is why IBM's Watson and DeepMind's AlphaGo are not included)</li>
<li>It is still very active (that is why Pollock's OSCAR and Brooks' Cog are no longer included)</li>
<li>It has ample publications on technical details (that is why many recent AGI projects are not included yet, except GPT-4 that is used to represent various deep learning projects toward AGI)</li>
</ol>
<p>The projects are listed in alphabetical order. Each project name is linked to the project website, where the following quotations are extracted. The focus of the quotations is on the research goal (the 1st question) and technical path (the 3rd question). Two publications on the project are selected, usually one brief introduction and one detailed description.</p>
<blockquote><i>
</i></blockquote>
<p><b><a href="http://act-r.psy.cmu.edu/" rel="nofollow">ACT-R</a></b>
[<a href="http://act-r.psy.cmu.edu/wordpress/wp-content/uploads/2012/12/526FSQUERY.pdf" rel="nofollow">An Integrated Theory of the Mind</a>; <a href="http://act-r.psy.cmu.edu/book/" rel="nofollow">The Atomic Components of Thought</a>]
    <i>
<blockquote>
    ACT-R is a cognitive architecture: a theory for simulating and
          understanding human cognition. Researchers working on ACT-R
          strive to understand how people organize knowledge and produce
          intelligent behavior. As the research continues, ACT-R evolves
          ever closer into a system which can perform the full range of
          human cognitive tasks: capturing in great detail the way we
          perceive, think about, and act on the world. 
<p>
On the exterior, ACT-R looks like a programming language;
          however, its constructs reflect assumptions about human
          cognition. These assumptions are based on numerous facts
          derived from psychology experiments. Like a programming
          language, ACT-R is a framework: for different tasks (e.g.,
          Tower of Hanoi, memory for text or for list of words, language
          comprehension, communication, aircraft controlling),
          researchers create models (aka programs) that are written in
          ACT-R and that, beside incorporating the ACT-R's view of
          cognition, add their own assumptions about the particular
          task. These assumptions can be tested by comparing the results
          of the model with the results of people doing the same tasks.
     </p>
<p>
     ACT-R is a hybrid cognitive
          architecture. Its symbolic structure is a production system;
          the subsymbolic structure is represented by a set of massively
          parallel processes that can be summarized by a number of
          mathematical equations. The subsymbolic equations control many
          of the symbolic processes. For instance, if several
          productions match the state of the buffers, a subsymbolic
          utility equation estimates the relative cost and benefit
          associated with each production and decides to select for
          execution the production with the highest utility. Similarly,
          whether (or how fast) a fact can be retrieved from declarative
          memory depends on subsymbolic retrieval equations, which take
          into account the context and the history of usage of that
          fact. Subsymbolic mechanisms are also responsible for most
          learning processes in ACT-R.
        </p>
</blockquote>
</i>
<b><a href="https://openaera.org/" rel="nofollow">AERA</a></b>
[<a href="https://alumni.media.mit.edu/~kris/ftp/AnytimeBoundedRationalityagi15_nivelEtAl.pdf" rel="nofollow">Anytime Bounded Rationality</a>; <a href="https://alumni.media.mit.edu/~kris/ftp/AERA-RUTR-SCS13002.pdf" rel="nofollow">Autocatalytic Endogenous Reflective Architecture</a>]
    <i>
<blockquote>
    AERA is a cognitive architecture - and a blueprint - for constructing agents with high levels of operational autonomy, starting from only a small amount of designer-specified code – a seed. Using a value-driven dynamic priority scheduling to control the parallel execution of a vast number of lines of reasoning, the system accumulates increasingly useful models of its experience, resulting in recursive self-improvement that can be autonomously sustained after the machine leaves the lab, within the boundaries imposed by its designers. 
<p>
AERA demonstrates domain-independent self-supervised cumulative learning of complex tasks. Unlike contemporary AI systems, AERA-based agents excel at handling novelty - situations, information, data, tasks - that their programmers could not anticipate. It is the only implementable / implemented system in existence for achieving bounded recursive self-improvement.
     </p>
<p>
     AERA-based agents learn cumulatively from experience by interacting with the world and generating compositional causal-relational micro-models of its experience. Using non-axiomatic abduction and deduction, it constantly predicts how to achieve its active goals and what the future may hold, generating a flexible opportunistically-interruptable plan for action.
        </p>
</blockquote>
</i>
<b><a href="http://www.hutter1.net/ai/index.htm" rel="nofollow">AIXI</a></b>  [<a href="http://www.hutter1.net/ai/aixigentle.htm" rel="nofollow">Universal Algorithmic Intelligence: A mathematical top-&gt;down approach</a>; <a href="http://www.hutter1.net/ai/uaibook.htm" rel="nofollow">Universal Artificial Intelligence</a><span>]</span><i>
<blockquote>An important observation is that most, if not all known facets of intelligence can be formulated as goal driven or, more precisely, as maximizing some utility function.
<p>Sequential decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental prior probability distribution is known. Solomonoff's theory of universal induction formally solves the problem of sequence prediction for unknown prior distribution. We combine both ideas and get a parameter-free theory of universal Artificial Intelligence. We give strong arguments that the resulting AIXI model is the most intelligent unbiased agent possible.</p>
<p>The major drawback of the AIXI model is that it is uncomputable, ... which makes an implementation impossible. To overcome this problem, we constructed a modified model AIXItl, which is still effectively more intelligent than any other time t and length l bounded algorithm.</p>
</blockquote>
</i>
<b><a href="http://www.cyc.com/" rel="nofollow">Cyc</a></b> [<a href="http://www.csee.umbc.edu/courses/471/papers/cyc95.pdf" rel="nofollow">Cyc: A Large-Scale Investment in Knowledge Infrastructure</a>; <a href="https://www.researchgate.net/publication/220545983_D_B_Lenat_and_R_V_Guha_Building_Large_Knowledge-Based_Systems_Representation_and_Inference_in_the_Cyc_Project" rel="nofollow">Building Large Knowledge-Based Systems</a>]<i>
<blockquote>Vast amounts of commonsense knowledge, representing human consensus reality, would need to be encoded to produce a general AI system. In order to mimic human reasoning, Cyc would require background knowledge regarding science, society and culture, climate and weather, money and financial systems, health care, history, politics, and many other domains of human experience. The Cyc Project team expected to encode at least a million facts spanning these and many other topic areas.
<p>The Cyc knowledge base (KB) is a formalized representation of a vast quantity of fundamental human knowledge: facts, rules of thumb, and heuristics for reasoning about the objects and events of everyday life. The medium of representation is the formal language CycL. The KB consists of terms -- which constitute the vocabulary of CycL -- and assertions which relate those terms. These assertions include both simple ground assertions and rules.</p>
</blockquote>
</i>
<b><a href="https://openai.com/research/gpt-4" rel="nofollow">GPT-4</a></b> [<a href="https://cdn.openai.com/papers/gpt-4.pdf" rel="nofollow">GPT-4 Technical Report</a>; <a href="https://arxiv.org/abs/2303.12712">Sparks of Artificial General Intelligence</a>]<i>
<blockquote>
We’ve created GPT-4, the latest milestone in OpenAI’s effort in scaling up deep learning. GPT-4 is a large multimodal model (accepting image and text inputs, emitting text outputs) that, while less capable than humans in many real-world scenarios, exhibits human-level performance on various professional and academic benchmarks. <p>
The combination of the generality of GPT-4's capabilities, with numerous abilities spanning a broad swath of domains, and its performance on a wide spectrum of tasks at or beyond human-level, makes us comfortable with saying that GPT-4 is a significant step towards AGI.</p>
</blockquote>
</i>
<b><a href="http://en.wikipedia.org/wiki/Hierarchical_Temporal_Memory" rel="nofollow">HTM</a></b> [<a href="http://numenta.com/assets/pdf/whitepapers/hierarchical-temporal-memory-cortical-learning-algorithm-0.2.1-en.pdf" rel="nofollow">Hierarchical Temporal Memory</a>; <a href="http://www.onintelligence.org/" rel="nofollow">On Intelligence</a>]<i>
<blockquote>At the core of every Grok model is the Cortical Learning Algorithm (CLA), a detailed and realistic model of a layer of cells in the neocortex. Contrary to popular belief, the neocortex is not a computing system, it is a memory system. When you are born, the neocortex has structure but virtually no knowledge. You learn about the world by building models of the world from streams of sensory input. From these models, we make predictions, detect anomalies, and take actions.
<p>In other words, the brain can best be described as a predictive modeling system that turns predictions into actions. Three key operating principles of the neocortex are described below: sparse distributed representations, sequence memory, and on-line learning.</p>
</blockquote>
</i><b><a href="http://ccrg.cs.memphis.edu/projects.html" rel="nofollow">LIDA</a></b>
[<a href="http://ccrg.cs.memphis.edu/assets/papers/zo-1010-lida-060403.pdf" rel="nofollow">The LIDA Architecture</a>;
<a href="http://ccrg.cs.memphis.edu/tutorial/tutorial.html" rel="nofollow">LIDA Tutorial</a>]
  <i>
  <blockquote>
Implementing and fleshing out
          a number of psychological and neuroscience theories of
          cognition, the LIDA conceptual model aims at being a cognitive
          "theory of everything." With modules or processes for
          perception, working memory, episodic memories,
          "consciousness," procedural memory, action selection,
          perceptual learning, episodic learning, deliberation,
          volition, and non-routine problem solving, the LIDA model is
          ideally suited to provide a working ontology that would allow
          for the discussion, design, and comparison of AGI systems. The
          LIDA technology is based on the LIDA cognitive cycle, a sort
          of "cognitive atom." The more elementary cognitive modules
          play a role in each cognitive cycle. Higher-level processes
          are performed over multiple cycles.
    
<p>
     The LIDA architecture
          represents perceptual entities, objects, categories,
          relations, etc., using nodes and links .... These serve as
          perceptual symbols acting as the common currency for
          information throughout the various modules of the LIDA
          architecture.</p>
</blockquote>
</i><b><a href="http://cognitive-ai.com/" rel="nofollow">MicroPsi</a></b><span>  [</span><a href="http://cognitive-ai.com/publications/assets/MicroPsiArchitectureICCM03.pdf" rel="nofollow">The MicroPsi Agent Architecture</a><span>;  </span><a href="https://global.oup.com/academic/product/principles-of-synthetic-intelligence-psi-an-architecture-of-motivated-cognition-9780195370676?cc=us&amp;lang=en&amp;" rel="nofollow">Principles of Synthetic Intelligence</a><span>]</span><i>
<blockquote>The MicroPsi agent architecture describes the interaction of emotion, 
motivation and cognition of situated agents, mainly based on the Psi 
theory of Dietrich Dorner. 
The Psi theory addresses emotion, perception, representation and bounded
 rationality, but being formulated within psychology, has had relatively
 little impact on the discussion of agents within computer science. 
MicroPsi is a formulation of the original theory in a more abstract and 
formal way, at the same time enhancing it with additional concepts for 
memory, building of ontological categories and attention.
<p>The agent framework uses semantic networks, called node nets, that are a
 unified representation for control structures, plans, sensory and 
action schemas, Bayesian networks and neural nets. Thus it is possible 
to set up different kinds of agents on the same framework.</p>
</blockquote>
</i><b><a href="http://opennars.org/">NARS</a></b><span>  [</span><a href="https://proceedings.mlr.press/v192/wang22a/wang22a.pdf">Intelligence: From Definition to Design</a><span>;  </span><a href="http://www.springer.com/west/home/computer/artificial?SGWID=4-147-22-173659733-0" rel="nofollow">Rigid Flexibility: The Logic of Intelligence</a><span>]</span><i>
<blockquote>What makes NARS different from conventional reasoning systems is its ability to learn from its experience and to work with insufficient knowledge and resources. NARS attempts to uniformly explain and reproduce many cognitive facilities, including reasoning, learning, planning, etc, so as to provide a unified theory, model, and system for AI as a whole. The ultimate goal of this research is to build a thinking machine.
<p>The development of NARS takes an incremental approach consisting four major stages. At each stage, the logic is extended to give the system a more expressive language, a richer semantics, and a larger set of inference rules; the memory and control mechanism are then adjusted accordingly to support the new logic.</p>
<p>In NARS the notion of "reasoning" is extended to represent a system's ability to predict the future according to the past, and to satisfy the unlimited resources demands using the limited resources supply, by flexibly combining justifiable micro steps into macro behaviors in a domain-independent manner.</p>
</blockquote>
</i><b><a href="http://opencog.org/" rel="nofollow">OpenCog</a> </b><span>[</span><a href="https://arxiv.org/abs/2103.15100v3" rel="nofollow">The General Theory of General Intelligence: A Pragmatic Patternist Perspective</a><span>; Engineering General Intelligence, </span><a href="http://www.springer.com/computer/ai/book/978-94-6239-026-3" rel="nofollow">Part 1</a><span> and </span><a href="http://www.springer.com/computer/ai/book/978-94-6239-029-4" rel="nofollow">Part 2</a><span>]</span><i>
<blockquote>OpenCog, as a software framework, aims to provide research scientists and software developers with a common platform to build and share artificial intelligence programs. The long-term goal of OpenCog is acceleration of the development of beneficial AGI.
<p>OpenCogPrime is a specific AGI design being constructed within the OpenCog framework. It comes with a fairly detailed, comprehensive design covering all aspects of intelligence. The hypothesis is that if this design is fully implemented and tested on a reasonably-sized distributed network, the result will be an AGI system with general intelligence at the human level and ultimately beyond.</p>
<p>While an OpenCogPrime based AGI system could do a lot of things, we are initially focusing on using OpenCogPrime to control simple virtual agents in virtual worlds. We are also experimenting with using it to control a Nao humanoid robot. See http://novamente.net/example for some illustrative videos.</p>
</blockquote>
</i><b><a href="http://cogarch.ict.usc.edu/" rel="nofollow">Sigma</a></b><span> [</span><a href="https://www.dropbox.com/s/bsfsyot89xl28zo/SM%20Symp%20Sigma%202017%20Revised%20D.pdf" rel="nofollow">Lessons from Mapping Sigma onto the Standard Model of the Mind</a><span>; </span><a href="https://sciendo.com/article/10.1515/jagi-2016-0001" rel="nofollow">The Sigma Cognitive Architecture and System</a><span>]</span><i>
<blockquote>The goal of this effort is to develop a sufficiently efficient, 
functionally elegant, generically cognitive, grand unified, cognitive 
architecture in support of virtual humans (and hopefully intelligent 
agents/robots – and even a new form of unified theory of human cognition
 – as well).<p>

Our focus is on the development of the <em>Sigma</em> (∑) architecture, which explores the <em>graphical architecture hypothesis</em>
 that progress at this point depends on blending what has been learned 
from over three decades worth of independent development of cognitive 
architectures and <em>graphical models</em>, a broadly applicable state-of-the-art formalism for constructing intelligent mechanisms.  The result is a <em>hybrid</em> (discrete+continuous) <em>mixed</em>
 (symbolic+probabilistic) approach that has yielded initial results 
across memory and learning, problem solving and decision making, mental 
imagery and perception, speech and natural language, and emotion and 
attention.</p></blockquote>
</i><b><a href="http://www.cse.buffalo.edu/sneps/" rel="nofollow">SNePS</a></b>
[<a href="http://www.cse.buffalo.edu/%7Eshapiro/Papers/shabon09a.pdf" rel="nofollow">The
GLAIR Cognitive Architecture</a>; <a href="http://www.cse.buffalo.edu/sneps/Tutorial/" rel="nofollow">SNePS Tutorial</a>]
<i>
<blockquote>
The long term goal of the SNePS Research Group is to understand the
          nature of intelligent cognitive processes by developing and
          experimenting with computational cognitive agents that are
          able to use and understand natural language, reason, act, and
          solve problems in a wide variety of domains.
          
<p>
          The SNePS knowledge representation, reasoning, and
          acting system has several features that facilitate
          metacognition in SNePS-based agents. The most prominent is the
          fact that propositions are represented in SNePS as terms
          rather than as logical sentences. The effect is that
          propositions can occur as arguments of propositions, acts, and
          policies without limit, and without leaving first-order logic.</p>
</blockquote>
</i></p><p><b><a href="http://soar.eecs.umich.edu/" rel="nofollow">Soar</a></b> [<a href="http://www.cse.msu.edu/~cse841/papers/Soar.pdf" rel="nofollow">A Gentle Introduction to Soar</a>; <a href="http://mitpress.mit.edu/books/soar-cognitive-architecture" rel="nofollow">The Soar Cognitive Architecture</a>]</p>
<blockquote><i>The ultimate in intelligence would be complete rationality which would imply the ability to use all available knowledge for every task that the system encounters. Unfortunately, the complexity of retrieving relevant knowledge puts this goal out of reach as the body of knowledge increases, the tasks are made more diverse, and the requirements in system response time more stringent. The best that can be obtained currently is an approximation of complete rationality. The design of Soar can be seen as an investigation of one such approximation.
<p>For many years, a secondary principle has been that the number of distinct architectural mechanisms should be minimized. Through Soar 8, there has been a single framework for all tasks and subtasks (problem spaces), a single representation of permanent knowledge (productions), a single representation of temporary knowledge (objects with attributes and values), a single mechanism for generating goals (automatic subgoaling), and a single learning mechanism (chunking). We have revisited this assumption as we attempt to ensure that all available knowledge can be captured at runtime without disrupting task performance. This is leading to multiple learning mechanisms (chunking, reinforcement learning, episodic learning, and semantic learning), and multiple representations of long-term knowledge (productions for procedural knowledge, semantic memory, and episodic memory).</p>
<p>Two additional principles that guide the design of Soar are functionality and performance. Functionality involves ensuring that Soar has all of the primitive capabilities necessary to realize the complete suite of cognitive capabilities used by humans, including, but not limited to reactive decision making, situational awareness, deliberate reasoning and comprehension, planning, and all forms of learning. Performance involves ensuring that there are computationally efficient algorithms for performing the primitive operations in Soar, from retrieving knowledge from long-term memories, to making decisions, to acquiring and storing new knowledge.</p>
</i></blockquote>
<p><b>A rough classification</b></p><p>
The above AGI projects are roughly classified in the following
        table, according to the type of their answers to the previously
        listed 1st question (on research goal) and 3rd question (on
        technical path).
</p>

<center>
<table>
<tbody>
<tr>
<td><i>goal  \  path</i></td>
<td><b>hybrid</b></td>
<td><b>integrated</b></td>
<td><b>unified</b></td>
</tr>
<tr>
<td><b>principle</b></td>
<td> </td>
<td><br>
</td>
<td>AERA, AIXI, NARS</td>
</tr>
<tr>
<td><b>function</b></td>
<td><br>
</td>
<td>OpenCog, Sigma, Soar</td>
<td>SNePS</td>
</tr>
<tr>
<td><b>capability</b></td>
<td><br>
</td>
<td><br>
</td>
<td>Cyc</td>
</tr>
<tr>
<td><b>behavior</b></td>
<td><br>
</td>
<td>ACT-R, LIDA, MicroPsi</td>
<td>GPT-4</td>
</tr>
<tr>
<td><b>structure</b></td>
<td><br>
</td>
<td><br>
</td>
<td>HTM</td>
</tr>
</tbody>
</table>
</center>
<p>
Since this classification is made at a high level, projects in the same entry of the table are still quite different in the details of their research goals and technical paths.
 </p>
<p>
In summary, the current AGI projects are based on very different theories and techniques.</p>
<h2><a name="TOC-AGI-Literatures-and-Resources"></a>AGI Literatures and Resources</h2>
<p>AGI collections:</p>
<div>
<ul><li>The earliest collection of AGI works is <a href="https://link.springer.com/book/10.1007/978-3-540-68677-4" rel="nofollow">Artificial General Intelligence</a>. Though this book was published in 2007, 
the manuscript was finished in 2003.</li>
<li><a href="https://www.iospress.com/catalog/books/advances-in-artificial-general-intelligence-concepts-architectures-and-algorithms" rel="nofollow">Advances in Artificial General Intelligence: Concepts, Architectures and Algorithms</a> is a post-conference proceedings of the 2006 AGI Workshop. The introductory chapter "<a href="https://cis.temple.edu/~pwang/Publication/AGI_Aspects.pdf" rel="nofollow">Aspects of Artificial General Intelligence</a>" clarified the notion of AGI and summarized the other chapters.</li>
<li><a href="http://www.springer.com/computer/ai/book/978-94-91216-61-9" rel="nofollow">Theoretical Foundations of Artificial General Intelligence</a> is a collection co-authored by active AGI researchers. Each chapter address a theoretical topic in AGI, and is written in a non-technical style, so as to provide information for readers who are not AGI researchers.</li>
<li><a href="https://www.amazon.com/Between-Ape-Artilect-Conversations-Transformative/dp/1496138171" rel="nofollow">Between Ape and Artilect: Conversations with Pioneers of Artificial General Intelligence and Other Transformative Technologies</a> contains some interviews of AGI researchers.</li></ul>
<p>
The <a href="http://agi-conf.org/" rel="nofollow">annual AGI international conference series</a> was started in 2008. The conference websites link to all accepted papers, plus additional materials like presentation files and video records.
</p>
<p>
<a href="https://sciendo.com/journal/JAGI" rel="nofollow">Journal of Artificial General Intelligence</a> (JAGI) is a peer-reviewed journal with open access, started in 2009.</p>
<p>The AGI conference and journal are managed by the <a href="http://www.agi-society.org/" rel="nofollow">Artificial General Intelligence Society</a> (AGIS). Everyone interested in AGI can become a member.
</p><p>
Communication venues and social media dedicated to AGI or related research:
</p><ul>
<li>AGI groups in Facebook: <a href="https://www.facebook.com/groups/propBitDev/" rel="nofollow">Artificial General Intelligence (AGI)</a>, <a href="https://www.facebook.com/groups/RealAGI/" rel="nofollow">Real AGI</a>,
<a href="https://www.facebook.com/groups/396475193810786/" rel="nofollow">Artificial General Intelligence</a>, <a href="https://www.facebook.com/groups/722892624534381/" rel="nofollow">Artificial Cognition</a></li>
<li><a href="https://www.linkedin.com/groups/1084997" rel="nofollow">AGI group</a> in LinkedIn</li>
<li><a href="http://groups.google.com/group/artificial-general-intelligence">AGI group</a> in Google Group</li>
<li><a href="https://agi.topicbox.com/" rel="nofollow">AGI mailing list</a> in Listbox</li>
</ul>
<p>Educational materials for students:</p>
<ul>
<li><a href="https://cis.temple.edu/~pwang/AGI-Curriculum.html">
Suggested Education for Future AGI Researchers</a>, by Pei Wang</li>
<li><a href="http://goertzel.org/agi-curriculum/" rel="nofollow">Sketch of an AGI Curriculum</a>, by Ben Goertzel</li>
<li><a href="http://www.hutter1.net/ai/introref.htm" rel="nofollow">AI Recommendations</a>, by Marcus Hutter</li>
<li><a href="http://www.agi-society.org/resources/" rel="nofollow">Videos of the past AGI Summer School lectures</a></li>
</ul>
<p>Other AGI resources:</p>
<div>
<ul><li>AGIS <a href="http://www.agi-society.org/resources/" rel="nofollow">resources page</a></li>
<li><a href="http://en.wikipedia.org/wiki/Artificial_General_Intelligence" rel="nofollow">Artificial general intelligence page</a> in Wikipedia</li>
<li><a href="http://www.scholarpedia.org/article/Artificial_General_Intelligence" rel="nofollow">Artificial general intelligence page</a> in Scholarpedia</li></ul>
</div>
</div>
</div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Firefox desktop extensions coming soon for the upcoming Android release (554 pts)]]></title>
            <link>https://blog.mozilla.org/addons/2023/08/10/prepare-your-firefox-desktop-extension-for-the-upcoming-android-release/</link>
            <guid>37084677</guid>
            <pubDate>Fri, 11 Aug 2023 03:09:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.mozilla.org/addons/2023/08/10/prepare-your-firefox-desktop-extension-for-the-upcoming-android-release/">https://blog.mozilla.org/addons/2023/08/10/prepare-your-firefox-desktop-extension-for-the-upcoming-android-release/</a>, See on <a href="https://news.ycombinator.com/item?id=37084677">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content" role="main">

    
      <article id="post-9099">
  <!-- .entry-header -->

  <div>
    <p><img width="160" height="160" src="https://blog.mozilla.org/addons/files/2019/10/Fx-Browser-icon-fullColor-160x160.png" alt="" decoding="async" title="" srcset="https://blog.mozilla.org/addons/files/2019/10/Fx-Browser-icon-fullColor-160x160.png 160w, https://blog.mozilla.org/addons/files/2019/10/Fx-Browser-icon-fullColor-252x252.png 252w, https://blog.mozilla.org/addons/files/2019/10/Fx-Browser-icon-fullColor-768x768.png 768w, https://blog.mozilla.org/addons/files/2019/10/Fx-Browser-icon-fullColor-600x600.png 600w, https://blog.mozilla.org/addons/files/2019/10/Fx-Browser-icon-fullColor.png 2048w" sizes="(max-width: 160px) 100vw, 160px"></p><p>In the coming months Mozilla will launch support for an open ecosystem of extensions on Firefox for Android on <i>addons.mozilla.org</i> (AMO). We’ll announce a definite launch date in early September, but it’s safe to expect a roll-out before the year’s end. Here’s everything developers need to know to get their Firefox desktop extensions ready for Android usage and discoverability on AMO…</p>
<h2>Firefox will become the only major Android browser to support an open extension ecosystem</h2>
<p>For the past few years Firefox for Android officially supported a small subset of extensions while we focused our efforts on strengthening core Firefox for Android functionality and understanding the unique needs of mobile browser users. Today, Mozilla has built the infrastructure necessary to support an open extension ecosystem on Firefox for Android. We anticipate considerable user demand for more extensions on Firefox for Android, so why not start optimizing your desktop extension for mobile-use right away?</p>
<blockquote><p><b>“There is so much creative potential to unlock within the mobile browser space. Mozilla wants to provide developers with the best support we can so they’re equipped and empowered to build modern mobile WebExtensions.”<i> — Giorgio Natili, Firefox Director of Engineering<br>
</i></b></p></blockquote>
<p>To support our ecosystem of extension developers, we will create additional guides, resources and host community events to support your transition to a managed multi-process environment like Android.</p>
<h2>Transition background scripts to non-persistent event pages</h2>
<p>We recently introduced support for multi-process in Firefox for Android Nightly. This means extensions are no longer hosted in the main process as Firefox’s user interface. This is a key consideration since Android is prone to shutting down resource-intensive processes, such as extensions. To mitigate the risk of unexpected extension termination, we’ve introduced event page architecture to be non-persistent and more resilient to process termination. Thus we strongly encourage developers to <a href="https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions/Background_scripts#convert_to_non-persistent">transition from persistent backgrounds to non-persistent Event pages</a> to improve their extension’s stability. In summary, this means:</p>
<ul>
<li>Update your manifest.json background key and add “persistent”: false.</li>
<li aria-level="1">Ensure listeners are registered synchronously at the top-level.</li>
<li aria-level="1">Record global state in the storage API, for example storage.session.</li>
<li aria-level="1">Change timers to alarms.</li>
<li aria-level="1">Switch from using<a href="https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions/API/extension/getBackgroundPage"> extension.getBackgroundPage</a> for calling a function from the background page, to extension messaging or<a href="https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions/API/runtime/getBackgroundPage"> runtime.getBackgroundPage</a>.</li>
</ul>
<p>Once you’re ready to test the mobile version of your extension, <a href="https://support.mozilla.org/en-US/kb/how-use-collections-addonsmozillaorg?utm_source=blog.mozilla.org&amp;utm_medium=post&amp;utm_content=expanded-extension-support-in-firefox-for-android-nightly#">create a collection</a> on AMO and test it on <a href="https://play.google.com/store/apps/details?id=org.mozilla.fenix">Firefox for Android Nightly</a>. If you’d prefer to polish your extension before publishing it on AMO, you can also <a href="https://extensionworkshop.com/documentation/develop/developing-extensions-for-firefox-for-android/#install-and-run-your-extension-in-firefox-for-android">debug and run the extension with web-ext</a>.</p>
<p>This is an exciting time for developers seeking to expand the reach of their desktop extensions into the mobile Android space. For community support and input, you’re welcome to join the conversation on <a href="https://discourse.mozilla.org/c/add-ons/35">Firefox Add-ons Discourse</a>.</p>
      </div><!-- .entry-content -->

  <!-- .entry-meta -->
</article><!-- #post -->

      



    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Git-appraise – Distributed Code Review for Git (196 pts)]]></title>
            <link>https://github.com/google/git-appraise</link>
            <guid>37084575</guid>
            <pubDate>Fri, 11 Aug 2023 02:52:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/google/git-appraise">https://github.com/google/git-appraise</a>, See on <a href="https://news.ycombinator.com/item?id=37084575">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Distributed Code Review For Git</h2>
<p dir="auto"><a href="https://travis-ci.org/google/git-appraise" rel="nofollow"><img src="https://camo.githubusercontent.com/aab84ff1822ea5220228493cad4a8d4b0174a0e7ab5404ddc125fe27f47bb3ca/68747470733a2f2f7472617669732d63692e6f72672f676f6f676c652f6769742d61707072616973652e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/google/git-appraise.svg?branch=master"></a></p>
<p dir="auto">This repo contains a command line tool for performing code reviews on git
repositories.</p>
<h2 tabindex="-1" dir="auto">Overview</h2>
<p dir="auto">This tool is a <em>distributed</em> code review system for git repos.</p>
<p dir="auto">By "distributed", we mean that code reviews are stored inside of the repository
as git objects. Every developer on your team has their own copy of the review
history that they can push or pull. When pulling, updates from the remote
repo are automatically merged by the tool.</p>
<p dir="auto">This design removes the need for any sort of server-side setup. As a result,
this tool can work with any git hosting provider, and the only setup required
is installing the client on your workstation.</p>
<h2 tabindex="-1" dir="auto">Installation</h2>
<p dir="auto">Assuming you have the <a href="https://golang.org/doc/install" rel="nofollow">Go tools installed</a>, run
the following command:</p>
<div data-snippet-clipboard-copy-content="go get github.com/google/git-appraise/git-appraise"><pre><code>go get github.com/google/git-appraise/git-appraise
</code></pre></div>
<p dir="auto">Then, either make sure that <code>${GOPATH}/bin</code> is in your PATH, or explicitly add the
"appraise" git alias by running the following command.</p>
<div data-snippet-clipboard-copy-content="git config --global alias.appraise '!'&quot;${GOPATH}/bin/git-appraise&quot;"><pre><code>git config --global alias.appraise '!'"${GOPATH}/bin/git-appraise"
</code></pre></div>
<h4 tabindex="-1" dir="auto">Windows:</h4>
<div data-snippet-clipboard-copy-content="git config --global alias.appraise &quot;!%GOPATH%/bin/git-appraise.exe&quot;"><pre><code>git config --global alias.appraise "!%GOPATH%/bin/git-appraise.exe"
</code></pre></div>
<h2 tabindex="-1" dir="auto">Requirements</h2>
<p dir="auto">This tool expects to run in an environment with the following attributes:</p>
<ol dir="auto">
<li>The git command line tool is installed, and included in the PATH.</li>
<li>The tool is run from within a git repo.</li>
<li>The git command line tool is configured with the credentials it needs to
push to and pull from the remote repos.</li>
</ol>
<h2 tabindex="-1" dir="auto">Usage</h2>
<p dir="auto">Requesting a code review:</p>

<p dir="auto">Pushing code reviews to a remote:</p>
<div data-snippet-clipboard-copy-content="git appraise push [<remote>]"><pre><code>git appraise push [&lt;remote&gt;]
</code></pre></div>
<p dir="auto">Pulling code reviews from a remote:</p>
<div data-snippet-clipboard-copy-content="git appraise pull [<remote>]"><pre><code>git appraise pull [&lt;remote&gt;]
</code></pre></div>
<p dir="auto">Listing open code reviews:</p>

<p dir="auto">Showing the status of the current review, including comments:</p>

<p dir="auto">Showing the diff of a review:</p>
<div data-snippet-clipboard-copy-content="git appraise show --diff [--diff-opts &quot;<diff-options>&quot;] [<review-hash>]"><pre><code>git appraise show --diff [--diff-opts "&lt;diff-options&gt;"] [&lt;review-hash&gt;]
</code></pre></div>
<p dir="auto">Commenting on a review:</p>
<div data-snippet-clipboard-copy-content="git appraise comment -m &quot;<message>&quot; [-f <file> [-l <line>]] [<review-hash>]"><pre><code>git appraise comment -m "&lt;message&gt;" [-f &lt;file&gt; [-l &lt;line&gt;]] [&lt;review-hash&gt;]
</code></pre></div>
<p dir="auto">Accepting the changes in a review:</p>
<div data-snippet-clipboard-copy-content="git appraise accept [-m &quot;<message>&quot;] [<review-hash>]"><pre><code>git appraise accept [-m "&lt;message&gt;"] [&lt;review-hash&gt;]
</code></pre></div>
<p dir="auto">Submitting the current review:</p>
<div data-snippet-clipboard-copy-content="git appraise submit [--merge | --rebase]"><pre><code>git appraise submit [--merge | --rebase]
</code></pre></div>
<p dir="auto">A more detailed getting started doc is available <a href="https://github.com/google/git-appraise/blob/master/docs/tutorial.md">here</a>.</p>
<h2 tabindex="-1" dir="auto">Metadata</h2>
<p dir="auto">The code review data is stored in <a href="https://git-scm.com/docs/git-notes" rel="nofollow">git-notes</a>,
using the formats described below. Each item stored is written as a single
line of JSON, and is written with at most one such item per line. This allows
the git notes to be automatically merged using the "cat_sort_uniq" strategy.</p>
<p dir="auto">Since these notes are not in a human-friendly form, all of the refs used to
track them start with the prefix "refs/notes/devtools". This helps make it
clear that these are meant to be read and written by automated tools.</p>
<p dir="auto">When a field named "v" appears in one of these notes, it is used to denote
the version of the metadata format being used. If that field is missing, then
it defaults to the value 0, which corresponds to this initial version of the
formats.</p>
<h3 tabindex="-1" dir="auto">Code Review Requests</h3>
<p dir="auto">Code review requests are stored in the "refs/notes/devtools/reviews" ref, and
annotate the first revision in a review. They must conform to the
<a href="https://github.com/google/git-appraise/blob/master/schema/request.json">request schema</a>.</p>
<p dir="auto">If there are multiple requests for a single commit, then they are sorted by
timestamp and the final request is treated as the current one. This sorting
should be done in a stable manner, so that if there are multiple requests
with the same timestamp, then the last such request in the note is treated
as the current one.</p>
<p dir="auto">This design allows a user to update a review request by re-running the
<code>git appraise request</code> command.</p>
<h3 tabindex="-1" dir="auto">Continuous Integration Status</h3>
<p dir="auto">Continuous integration build and test results are stored in the
"refs/notes/devtools/ci" ref, and annotate the revision that was built and
tested. They must conform to the <a href="https://github.com/google/git-appraise/blob/master/schema/ci.json">ci schema</a>.</p>
<h3 tabindex="-1" dir="auto">Robot Comments</h3>
<p dir="auto">Robot comments are comments generated by static analysis tools. These are
stored in the "refs/notes/devtools/analyses" ref, and annotate the revision.
They must conform to the <a href="https://github.com/google/git-appraise/blob/master/schema/analysis.json">analysis schema</a>.</p>
<h3 tabindex="-1" dir="auto">Review Comments</h3>
<p dir="auto">Review comments are comments that were written by a person rather than by a
machine. These are stored in the "refs/notes/devtools/discuss" ref, and
annotate the first revision in the review. They must conform to the
<a href="https://github.com/google/git-appraise/blob/master/schema/comment.json">comment schema</a>.</p>
<h2 tabindex="-1" dir="auto">Integrations</h2>
<h3 tabindex="-1" dir="auto">Libraries</h3>
<ul dir="auto">
<li><a href="https://github.com/google/git-appraise/blob/master/review/review.go">Go (use git-appraise itself)</a></li>
<li><a href="https://github.com/Nemo157/git-appraise-rs">Rust</a></li>
</ul>
<h3 tabindex="-1" dir="auto">Graphical User Interfaces</h3>
<ul dir="auto">
<li><a href="https://github.com/google/git-appraise-web">Git-Appraise-Web</a></li>
</ul>
<h3 tabindex="-1" dir="auto">Plugins</h3>
<ul dir="auto">
<li><a href="https://github.com/google/git-appraise-eclipse">Eclipse</a></li>
<li><a href="https://github.com/jenkinsci/google-git-notes-publisher-plugin">Jenkins</a></li>
</ul>
<h3 tabindex="-1" dir="auto">Mirrors to other systems</h3>
<ul dir="auto">
<li><a href="https://github.com/google/git-pull-request-mirror">GitHub Pull Requests</a></li>
<li><a href="https://github.com/google/git-phabricator-mirror">Phabricator Revisions</a></li>
</ul>
<h2 tabindex="-1" dir="auto">Contributing</h2>
<p dir="auto">Please see <a href="https://github.com/google/git-appraise/blob/master/CONTRIBUTING.md">the CONTRIBUTING file</a> for information on contributing to Git Appraise.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I fixed a bug the other day (102 pts)]]></title>
            <link>https://www.javiergonzalez.io/blog/i-fixed-a-bug/</link>
            <guid>37084262</guid>
            <pubDate>Fri, 11 Aug 2023 02:06:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.javiergonzalez.io/blog/i-fixed-a-bug/">https://www.javiergonzalez.io/blog/i-fixed-a-bug/</a>, See on <a href="https://news.ycombinator.com/item?id=37084262">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>I fixed a bug that was in production for 2 years the other day. My boss said thank you, the UX researcher that called it out two years ago and was told it was not possible to do on our end said thank you. The product manager that had tried to tackle the issue a few times but gotten nowhere with their devs said thank you. It turns out that it was one of the biggest customer complaints and for some reason it had been neglected? ignored? idk. The change was about 20 lines of code, as they usually are, but had an outsized impact on customer satisfaction ratings. Why?</p><div><h3>The Issue</h3>
<p>I work in supermarket systems. Think e-commerce on steroids. We do weekly ads (which are called circulars) and these have discounts, the app also handles enough work to employ hundreds of developers working on loyalty programs, personalized recommendations, organizing pick up, delivery, managing availability for orders that are fulfilled in a physical location (which is most orders) shopping lists, a custom cms..., you get the picture. The issue was that our system for weekly ad was having an issue where the ads needed to 'clip' a coupon (read hit an api) for the discounted price to reach our cart calculations. However, the api that we used for managing the weekly ads was managed by an external vendor, and often when we queried their endpoint, the coupon field was returning empty when it shouldn't. To the customer, this meant that they would click on a tile that says a discounted price in their description, but would hit their cart as full price. Really frustrating and a cause of many loud customer complaints.</p>
<h3>Why was it hard to fix</h3>
<p>It wasn't. It was, however an issue that was difficult to replicate in our lower test environments, as the data simply wasn't there. It was also an issue that had to do how the external provider formatted the data that they were passing to us. Because this data was missing a field, it required manual data entry to update it. However, this data changed in real time, so it was OFTEN out of date. Whenever this issue came up to developers, the reaction was always to blame the vendor. This is natural, as it technically was the vendor's fault. However, what became apparent is that people were not going below the surface level to get to the heart of what could be done.</p>
<p>It became clear that this was an issue in <strong>process</strong> not in <strong>execution</strong>. The inertia in the company was to find why it would be hard to do, or impossible to do, why the fault lay elsewhere. We had spikes, and backlogs with this issue, but no action. I had never looked at this code, but was told to take a look, given that we seemed to have the data coming back in one of our API calls. Here's a diagram of the flow of info that we already had on our site:
<img src="https://www.javiergonzalez.io/assets/i-fixed-a-bug-graphic.a273ed83.png"></p>
<p>That's right, for any developer, the answer here is simple: use the coupons that we are ALREADY receiving on the frontend to populate the fields that users expect so they can get their discounts. Products already had the associated coupons, but we were simply ignoring that. Not only where we ignoring that, we said it was not possible to do. This reminds me of <a href="https://badsoftwareadvice.substack.com/p/how-to-debug-software">this article</a> I read on how to report a bug:</p>
<ul>
<li>Deny that there is a problem.</li>
<li>Deny that it is your problem.</li>
<li>Ask for more information.</li>
<li>Complain.</li>
</ul>
<p>The real difficulty was in reproducing the data, so I completely hacked it, and replaced the api call service with a dumb function that returned hardcoded data copied from a production. As our principal engineer said to me when I told him about the hack to get the data in my local environment: "<strong>All data in lower environments is mock data</strong>".</p>
<h3>The Solution</h3>
<p>There is very little that is exciting about the bugfix. It was easy, a few array methods to ensure we were not searching for duplicates, and it even required one less API call than before. It worked nicely and the customer satisfaction scores went up as expected. What I did is come in as an outsider to the issue and this particular business concern, let my naivety guide my investigation and provided a solution within a few days. Why wasn't this done before is the more interesting question, and it seems to be related to a few things that are above my paygrade but are quite interesting to me.</p>
<h3>Closing Thoughts</h3>
<p>I like my company, and my direct managers, and several other people that I interact with in a regular basis. However, this was a case where the inertia of the status quo got the better of the organization at the expense of customer experience. I want to understand how this came to be, rather than to shift blame around, and to do there are a few things that I am paying attention to. The incentives and the feedback loops.</p>
<p>A company that is not a startup has strange incentives opposed to some of the original ones when it is getting started. Here's the cycle described by <a href="https://thenetworkstate.com/left-is-the-new-right-is-the-new-left#the-libertarian-cycle">Balaji Srinivasan</a>:</p>
<blockquote>
<p>First, a libertarian(ish) founder leaves the stifling bureaucracy of a big company to start their own. Most immediately fail, but through pure maneuver warfare and relentless execution, that founder might be able to make enough money to hire someone. In the early days the most important quantity is the burn rate. Every single person must be indispensable.</p>
<p>Eventually, if successful, the company starts building up some structure. Conservativism takes over. With the business growing consistently, the founder adds structure, career tracks, and a stable hierarchy. Now the most important quantity becomes the bus number, the number of people who can get hit by a bus such that the company is still functional. Suddenly every single person must now be&nbsp;<strong>dispensable</strong>.</p>
</blockquote>
<p><a href="https://medium.com/tech-tajawal/the-bus-factor-6ea1a3ede6bd">The bus factor</a> creates redundancy, and people being dispensable makes a strange feedback loop. When everyone is dispensable, as a good company should make their employees, there is a tension between owning your work and standing out, vs just doing the minimum. Where that line sits is not clear at all.</p>
<p>This pressure and quite frankly this contradiction is an extremely difficult one to solve. Harvard and others have written about it <a href="https://hbr.org/2012/09/why-big-companies-cant-innovate">Why Big Companies Can't Innovate</a>. I am interested in this, because I am interested in doing work, at scale, within a company that has a culture of solving problems, not of being a behemoth with enough inertia to not need to correct when there is a need. Luckily, my team within the larger org values this nimble attitude of getting sh(stuff)t done, and was able to in some sense circumvent this inertia and this extreme risk aversion.</p>
<p>I've also been thinking quite a bit of a company as a cybernetic system. You have a product, and that product has a shape which is shaped by the organizational structure (in Software Development, we call it <a href="https://en.wikipedia.org/wiki/Conway%27s_law">Conway's Law</a>). The product reflects the org, and the org reflects the product. This separation gets reinforced, and we were reinforcing invisible (and nonexistent to the customer) boundaries within ourselves and the vendors for no good reason. I want to investigate this notion of cybernetic systems and how to direct them in the context of a software company, so I'll be writing more about that.</p>
<p>In the end, this issue revealed a flaw in process more than a flaw in the technical execution of our product. Of course a <a href="https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/">leaky abstraction</a> showed to the customer, but it was a matter of understanding it and patching it rather than allowing that distinction to continue and reinforce itself. As I progress in software, I am very interested in thinking of steering the cybernetic system of a corporation in order to get self reinforcing outcomes in an intentional direction. I will be writing on that more soon.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Tetris, but the blocks are ARM instructions that execute in the browser (185 pts)]]></title>
            <link>https://ofrak.com/tetris/</link>
            <guid>37083309</guid>
            <pubDate>Thu, 10 Aug 2023 23:56:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ofrak.com/tetris/">https://ofrak.com/tetris/</a>, See on <a href="https://news.ycombinator.com/item?id=37083309">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="nowasm">
      
      <p>
        This game is Tetris, but the blocks are assembly instructions that run
        on a full, in-browser CPU emulator.
      </p>
      <p>This game requires WebAssembly to run.</p>
      <p>
        The game includes a WebAssembly-based emulator to execute CPU
        instructions. Please enable WebAssembly, or use a browser that supports
        it to play.
      </p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SQLite Functions for Working with JSON (148 pts)]]></title>
            <link>https://www.sqlite.org/json1.html</link>
            <guid>37082941</guid>
            <pubDate>Thu, 10 Aug 2023 23:14:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sqlite.org/json1.html">https://www.sqlite.org/json1.html</a>, See on <a href="https://news.ycombinator.com/item?id=37082941">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div>
<p>
JSON Functions And Operators
</p>


</div>





<h2 id="overview"><span>1. </span>Overview</h2>
<p>
By default, SQLite supports sixteen functions and two operators for
dealing with JSON values.  There are also two <a href="https://www.sqlite.org/vtab.html#tabfunc2">table-valued functions</a>
that can be used to decompose a JSON string.

</p><p>
There are 15 scalar functions and operators:

</p><ol>
<li value="1">
<a href="#jmini">json</a>(<i>json</i>)
</li>

<li value="2">
<a href="#jarray">json_array</a>(<i>value1</i>,<i>value2</i>,...)
</li>

<li value="3">
<a href="#jarraylen">json_array_length</a>(<i>json</i>)<br><a href="#jarraylen">json_array_length</a>(<i>json</i>,<i>path</i>)
</li>

<li value="4">
<a href="#jerr">json_error_position</a>(<i>json</i>)
</li>

<li value="5">
<a href="#jex">json_extract</a>(<i>json</i>,<i>path</i>,...)
</li>

<li value="6">
<i>json</i> <a href="#jptr">-&gt;</a> <i>path</i>
</li>

<li value="7">
<i>json</i> <a href="#jptr">-&gt;&gt;</a> <i>path</i>
</li>

<li value="8">
<a href="#jins">json_insert</a>(<i>json</i>,<i>path</i>,<i>value</i>,...)
</li>

<li value="9">
<a href="#jobj">json_object</a>(<i>label1</i>,<i>value1</i>,...)
</li>

<li value="10">
<a href="#jpatch">json_patch</a>(<i>json</i>1,json2)
</li>

<li value="11">
<a href="#jrm">json_remove</a>(<i>json</i>,<i>path</i>,...)
</li>

<li value="12">
<a href="#jrepl">json_replace</a>(<i>json</i>,<i>path</i>,<i>value</i>,...)
</li>

<li value="13">
<a href="#jset">json_set</a>(<i>json</i>,<i>path</i>,<i>value</i>,...)
</li>

<li value="14">
<a href="#jtype">json_type</a>(<i>json</i>)<br><a href="#jtype">json_type</a>(<i>json</i>,<i>path</i>)
</li>

<li value="15">
<a href="#jvalid">json_valid</a>(<i>json</i>)
</li>

<li value="16">
<a href="#jquote">json_quote</a>(<i>value</i>)
</li>


</ol>

<p>There are two aggregate SQL functions:

</p><ol>
<li value="17">
<a href="#jgrouparray">json_group_array</a>(<i>value</i>)
</li>

<li value="18">
<a href="#jgroupobject">json_group_object</a>(name,<i>value</i>)
</li>


</ol>

<p>The two <a href="https://www.sqlite.org/vtab.html#tabfunc2">table-valued functions</a> are:

</p><ol>
<li value="19">
<a href="#jeach">json_each</a>(<i>json</i>)<br><a href="#jeach">json_each</a>(<i>json</i>,<i>path</i>)
</li>

<li value="20">
<a href="#jtree">json_tree</a>(<i>json</i>)<br><a href="#jtree">json_tree</a>(<i>json</i>,<i>path</i>)
</li>


</ol>






<h2 id="compiling_in_json_support"><span>2. </span>Compiling in JSON Support</h2>

<p>
The JSON functions and operators are built into SQLite by default,
as of SQLite version 3.38.0 (2022-02-22).  They can be omitted
by adding the -DSQLITE_OMIT_JSON compile-time option.  Prior to
version 3.38.0, the JSON functions were an extension that would only
be included in builds if the -DSQLITE_ENABLE_JSON1 compile-time option
was included.  In other words, the JSON functions went from being
opt-in with SQLite version 3.37.2 and earlier to opt-out with
SQLite version 3.38.0 and later.

</p><h2 id="interface_overview"><span>3. </span>Interface Overview</h2>

<p>
SQLite stores JSON as ordinary text.
Backwards compatibility constraints mean that SQLite is only able to
store values that are NULL, integers, floating-point numbers, text,
and BLOBs.  It is not possible to add a sixth "JSON" type.

</p><p>
SQLite does not (currently) support a binary encoding
of JSON.  Experiments have been unable to find a binary encoding
that is smaller or faster than a plain text encoding.
(The present implementation parses JSON text at over 250 MB/s.)
All JSON functions currently throw an error if any of their
arguments are BLOBs because BLOBs are reserved
for a future enhancement in which BLOBs will store the binary encoding
for JSON.

</p><h2 id="json_arguments"><span>3.1. </span>JSON arguments</h2>

<p>
For functions that accept JSON as their first argument, that argument
can be a JSON object, array, number, string, or null.  SQLite numeric
values and NULL values are interpreted as JSON numbers and nulls, respectively.
SQLite text values can be understood as JSON objects, arrays, or strings.
If an SQLite text value that is not a well-formed JSON object, array, or
string is passed into JSON function, that function will usually throw
an error.  (Exceptions to this rule are <a href="https://www.sqlite.org/json1.html#jvalid">json_valid()</a>,
<a href="https://www.sqlite.org/json1.html#jquote">json_quote()</a>, and <a href="https://www.sqlite.org/json1.html#jerr">json_error_position()</a>.)

</p><p>
These routines understand all
<a href="https://www.rfc-editor.org/rfc/rfc7159.txt">rfc-7159 JSON syntax</a>
and also <a href="https://spec.json5.org/">JSON5 extensions</a>.  JSON text
generated by these routines always strictly conforms to the
<a href="https://json.org/">canonical JSON definition</a> and does not contain any JSON5
or other extensions.  The ability to read and understand JSON5 was added in
version 3.42.0 (2023-05-16).
Prior versions of SQLite would only read canonical JSON.


<a name="jsonpath"></a>

</p><h2 id="path_arguments"><span>3.2. </span>PATH arguments</h2>

<p>
For functions that accept PATH arguments, that PATH must be well-formed or
else the function will throw an error.
A well-formed PATH is a text value that begins with exactly one
'$' character followed by zero or more instances
of ".<i>objectlabel</i>" or "[<i>arrayindex</i>]".

</p><p>
The <i>arrayindex</i> is usually a non-negative integer <i>N</i>.  In
that case, the array element selected is the <i>N</i>-th element
of the array, starting with zero on the left.
The <i>arrayindex</i> can also be of the form "<b>#-</b><i>N</i>"
in which case the element selected is the <i>N</i>-th from the
right.  The last element of the array is "<b>#-1</b>".  Think of
the "#" characters as the "number of elements in the array".  Then
the expression "#-1" evaluates to the integer that corresponds to 
the last entry in the array.  It is sometimes useful for the array
index to be just the <b>#</b> character, for example when appending
a value to an existing JSON array:

</p><ul>
<li><span>json_set('[0,1,2]','$[#]','new')</span>
<span>→ '[0,1,2,"new"]'</span></li>

</ul>


<h2 id="value_arguments"><span>3.3. </span>VALUE arguments</h2>

<p>
For functions that accept "<i>value</i>" arguments (also shown as
"<i>value1</i>" and "<i>value2</i>"),
those arguments are usually understood
to be literal strings that are quoted and become JSON string values
in the result.  Even if the input <i>value</i> strings look like 
well-formed JSON, they are still interpreted as literal strings in the
result.

</p><p>
However, if a <i>value</i> argument comes directly from the result of another
JSON function or from <a href="https://www.sqlite.org/json1.html#jptr">the -&gt; operator</a> (but not <a href="https://www.sqlite.org/json1.html#jptr">the -&gt;&gt; operator</a>),
then the argument is understood to be actual JSON and
the complete JSON is inserted rather than a quoted string.

</p><p>
For example, in the following call to json_object(), the <i>value</i>
argument looks like a well-formed JSON array.  However, because it is just
ordinary SQL text, it is interpreted as a literal string and added to the
result as a quoted string:

</p><ul>
<li><span>json_object('ex','[52,3.14159]')</span>
<span>→ '{"ex":"[52,3.14159]"}'</span></li>

<li><span>json_object('ex',('52,3.14159]'-&gt;&gt;'$'))</span>
<span>→ '{"ex":"[52,3.14159]"}'</span></li>

</ul>


<p>
But if the <i>value</i> argument in the outer json_object() call is the
result of another JSON function like <a href="https://www.sqlite.org/json1.html#jmini">json()</a> or <a href="https://www.sqlite.org/json1.html#jarray">json_array()</a>, then
the value is understood to be actual JSON and is inserted as such:

</p><ul>
<li><span>json_object('ex',json('[52,3.14159]'))</span>
<span>→ '{"ex":[52,3.14159]}'</span></li>

<li><span>json_object('ex',json_array(52,3.14159))</span>
<span>→ '{"ex":[52,3.14159]}'</span></li>

<li><span>json_object('ex','[52,3.14159]'-&gt;'$')</span>
<span>→ '{"ex":[52,3.14159]}'</span></li>

</ul>


<p>
To be clear: "<i>json</i>" arguments are always interpreted as JSON
regardless of where the value for that argument comes from.  But
"<i>value</i>" arguments are only interpreted as JSON if those arguments
come directly from another JSON function or <a href="https://www.sqlite.org/json1.html#jptr">the -&gt; operator</a>.

</p><p>
Within JSON value arguments interpreted as JSON strings, Unicode escape
sequences are not treated as equivalent to the characters or escaped
control characters represented by the expressed Unicode code point.
Such escape sequences are not translated or specially treated; they
are treated as plain text by SQLite's JSON functions.

</p><h2 id="compatibility"><span>3.4. </span>Compatibility</h2>

<p>
The current implementation of this JSON library uses a recursive descent
parser.  In order to avoid using excess stack space, any JSON input that has
more than 1000 levels of nesting is considered invalid.   Limits on nesting
depth are allowed for compatible implementations of JSON by
<a href="https://tools.ietf.org/html/rfc7159#section-9">RFC-7159 section 9</a>.

<a name="json5"></a>

</p><h2 id="json5_extensions"><span>3.5. </span>JSON5 Extensions</h2>

<p>
Beginning in version 3.42.0 (2023-05-16), these routines will
read and interpret input JSON text that includes
<a href="https://spec.json5.org/">JSON5</a> extensions.  However, JSON text generated
by these routines will always be strictly conforming to the 
<a href="https://json.org/">canonical definition of JSON</a>.

</p><p>
Here is a synopsis of JSON5 extensions (adapted from the
<a href="https://spec.json5.org/#introduction">JSON5 specification</a>):

</p><ul>
<li> Object keys may be unquoted identifiers.
</li><li> Objects may have a single trailing comma.
</li><li> Arrays may have a single trailing comma.
</li><li> Strings may be single quoted.
</li><li> Strings may span multiple lines by escaping new line characters.
</li><li> Strings may include new character escapes.
</li><li> Numbers may be hexadecimal.
</li><li> Numbers may have a leading or trailing decimal point.
</li><li> Numbers may be "Infinity", "-Infinity", and "NaN".
</li><li> Numbers may begin with an explicit plus sign.
</li><li> Single (//...) and multi-line (/*...*/) comments are allowed.
</li><li> Additional white space characters are allowed.
</li></ul>

<p>
To convert string X from JSON5 into canonical JSON, invoke
"<a href="https://www.sqlite.org/json1.html#jmini">json(X)</a>".  The output of the "<a href="https://www.sqlite.org/json1.html#jmini">json()</a>" function will be canonical
JSON regardless of any JSON5 extensions that are present in the input.
For backwards compatibility, the <a href="https://www.sqlite.org/json1.html#jvalid">json_valid(X)</a> function continues
to report false for inputs that are not canonical JSON, even if the
input is JSON5 that the function is able to understand.  To determine
whether or not an input string is valid JSON5,
use the expression: "<a href="https://www.sqlite.org/json1.html#jerr">json_error_position(X)</a>==0".

</p><p>
These routines understand all of JSON5, plus a little more.
SQLite extends the JSON5 syntax in these two ways:

</p><ol>
<li><p>
Strict JSON5 requires that
unquoted object keys must be ECMAScript 5.1 IdentifierNames.  But large
unicode tables and lots of code is required in order to determine whether or
not a key is an ECMAScript 5.1 IdentifierName.  For this reason,
SQLite allows object keys to include any unicode characters
greater than U+007f that are not whitespace characters.  This relaxed
definition of "identifier" greatly simplifies the implementation and allows
the JSON parser to be smaller and run faster.

</p></li><li><p>
JSON5 allows floating-point infinities to be expressed as
"<tt>Infinity</tt>", "<tt>-Infinity</tt>", or "<tt>+Infinity</tt>"
in exactly that case - the initial "I" is capitalized and all other
characters are lower case.  SQLite also allows the abbreviation "<tt>Inf</tt>"
to be used in place of "<tt>Infinity</tt>" and it allows both keywords
to appear in any combination of upper and lower case letters.
Similarly,
JSON5 allows "NaN" for not-a-number.  SQLite extends this to also allow
"QNaN" and "SNaN" in any combination of upper and lower case letters.
Note that SQLite interprets NaN, QNaN, and SNaN as just an alternative
spellings for "null".
This extension has been added because (we are told) there exists a lot
of JSON in the wild that includes these non-standard representations
for infinity and not-a-number.
</p></li></ol>

<h2 id="function_details"><span>4. </span>Function Details</h2>

<p>The following sections provide additional detail on the operation of
the various JSON functions and operators:

<a name="jmini"></a>

</p><h2 id="the_json_function"><span>4.1. </span>The json() function</h2>

<p>The json(X) function verifies that its argument X is a valid
JSON string and returns a minified version of that JSON string
(with all unnecessary whitespace removed).  If X is not a well-formed
JSON string, then this routine throws an error.

</p><p>In other words, this function converts raw text that looks like
JSON into actual JSON so that it may be passed into the <a href="https://www.sqlite.org/json1.html#varg">value argument</a>
of some other json function and will be interpreted as JSON rather than
a string.  This function is not appropriate for testing whether or not
a particular string is well-formed JSON - use the <a href="https://www.sqlite.org/json1.html#jvalid">json_valid()</a> and/or
<a href="https://www.sqlite.org/json1.html#jerr">json_error_position()</a> routines below for that task.

</p><p>If the argument X to json(X) contains JSON objects with duplicate
labels, then it is undefined whether or not the duplicates are
preserved.  The current implementation preserves duplicates.
However, future enhancements
to this routine may choose to silently remove duplicates.

</p><p>
Example:

</p><ul>
<li><span>json(' { "this" : "is", "a": [ "test" ] } ')</span>
<span>→ '{"this":"is","a":["test"]}'</span></li>

</ul>


<h2 id="the_json_array_function"><span>4.2. </span>The json_array() function</h2>

<p>The json_array() SQL function accepts zero or more arguments and
returns a well-formed JSON array that is composed from those arguments.
If any argument to json_array() is a BLOB then an error is thrown.

</p><p>An argument with SQL type TEXT is normally converted into a quoted 
JSON string.  However, if the argument is the output from another json1
function, then it is stored as JSON.  This allows calls to json_array()
and <a href="https://www.sqlite.org/json1.html#jobj">json_object()</a> to be nested.  The <a href="https://www.sqlite.org/json1.html#jmini">json()</a> function can also
be used to force strings to be recognized as JSON.

</p><p>Examples:

</p><ul>
<li><span>json_array(1,2,'3',4)</span>
<span>→ '[1,2,"3",4]'</span></li>

<li><span>json_array('[1,2]')</span>
<span>→ '["[1,2]"]'</span></li>

<li><span>json_array(json_array(1,2))</span>
<span>→ '[[1,2]]'</span></li>

<li><span>json_array(1,null,'3','[4,5]','{"six":7.7}')</span>
<span>→ '[1,null,"3","[4,5]","{\"six\":7.7}"]'</span></li>

<li><span>json_array(1,null,'3',json('[4,5]'),json('{"six":7.7}'))</span>
<span>→ '[1,null,"3",[4,5],{"six":7.7}]'</span></li>

</ul>



<h2 id="the_json_array_length_function"><span>4.3. </span>The json_array_length() function</h2>

<p>The json_array_length(X) function returns the number of elements
in the JSON array X, or 0 if X is some kind of JSON value other
than an array.  The json_array_length(X,P) locates the array at path P
within X and returns the length of that array, or 0 if path P locates
an element in X that is not a JSON array, and NULL if path P does not
locate any element of X.  Errors are thrown if either X is not 
well-formed JSON or if P is not a well-formed path.

</p><p>Examples:

</p><ul>
<li><span>json_array_length('[1,2,3,4]')</span>
<span>→ 4</span></li>

<li><span>json_array_length('[1,2,3,4]', '$')</span>
<span>→ 4</span></li>

<li><span>json_array_length('[1,2,3,4]', '$[2]')</span>
<span>→ 0</span></li>

<li><span>json_array_length('{"one":[1,2,3]}')</span>
<span>→ 0</span></li>

<li><span>json_array_length('{"one":[1,2,3]}', '$.one')</span>
<span>→ 3</span></li>

<li><span>json_array_length('{"one":[1,2,3]}', '$.two')</span>
<span>→ NULL</span></li>

</ul>



<h2 id="the_json_error_position_function"><span>4.4. </span>The json_error_position() function</h2>

<p>The json_error_positionf(X) function returns 0 if the input X is a
well-formed JSON or JSON5 string.  If the input X contains one or more
syntax errors, then this function returns the character position of the
first syntax error.  The left-most character is position 1.

</p><p>This routine is useful for at least two purposes:

</p><ol>
<li>
<p> To determine is a text string X is valid JSON or JSON5 as understood
    by SQLite, run "<tt>json_error_position(X)==0</tt>".  This is similar
    to <a href="https://www.sqlite.org/json1.html#jvalid">json_valid()</a> except that json_valid(X) requires X to be strictly
    conforming canonical JSON whereas json_error_position() allows the
    input to contains <a href="https://www.sqlite.org/json1.html#json5">JSON5 extensions</a>.
</p></li><li>
<p> Use this routine to find the location of a syntax error in a large
    JSON string during interactive debugging, or to generate a better
    error messages for human users.
</p></li></ol>

<p>
The json_error_position() function was added with
SQLite version 3.42.0 (2023-05-16).


<a name="jex"></a>

</p>

<p>The json_extract(X,P1,P2,...) extracts and returns one or more 
values from the
well-formed JSON at X.  If only a single path P1 is provided, then the
SQL datatype of the result is NULL for a JSON null, INTEGER or REAL
for a JSON numeric value, an INTEGER zero for a JSON false value,
an INTEGER one for a JSON true value, the dequoted text for a 
JSON string value, and a text representation for JSON object and array values.
If there are multiple path arguments (P1, P2, and so forth) then this
routine returns SQLite text which is a well-formed JSON array holding
the various values.

</p><p>Examples:

</p><ul>
<li><span>json_extract('{"a":2,"c":[4,5,{"f":7}]}', '$')</span>
<span>→ '{"a":2,"c":[4,5,{"f":7}]}'</span></li>

<li><span>json_extract('{"a":2,"c":[4,5,{"f":7}]}', '$.c')</span>
<span>→ '[4,5,{"f":7}]'</span></li>

<li><span>json_extract('{"a":2,"c":[4,5,{"f":7}]}', '$.c[2]')</span>
<span>→ '{"f":7}'</span></li>

<li><span>json_extract('{"a":2,"c":[4,5,{"f":7}]}', '$.c[2].f')</span>
<span>→ 7</span></li>

<li><span>json_extract('{"a":2,"c":[4,5],"f":7}','$.c','$.a')</span>
<span>→ '[[4,5],2]'</span></li>

<li><span>json_extract('{"a":2,"c":[4,5],"f":7}','$.c[#-1]')</span>
<span>→ 5</span></li>

<li><span>json_extract('{"a":2,"c":[4,5,{"f":7}]}', '$.x')</span>
<span>→ NULL</span></li>

<li><span>json_extract('{"a":2,"c":[4,5,{"f":7}]}', '$.x', '$.a')</span>
<span>→ '[null,2]'</span></li>

<li><span>json_extract('{"a":"xyz"}', '$.a')</span>
<span>→ 'xyz'</span></li>

<li><span>json_extract('{"a":null}', '$.a')</span>
<span>→ NULL</span></li>

</ul>


<p>There is a subtle incompatibility between the json_extract() function
in SQLite and the json_extract() function in MySQL.  The MySQL version
of json_extract() always returns JSON.  The SQLite version of
json_extract() only returns JSON if there are two or more PATH arguments
(because the result is then a JSON array) or if the single PATH argument
references an array or object.  In SQLite, if json_extract() has only
a single PATH argument and that PATH references a JSON null or a string
or a numeric value, then json_extract() returns the corresponding SQL
NULL, TEXT, INTEGER, or REAL value.

</p><p>The difference between MySQL json_extract() and SQLite json_extract()
really only stands out when accessing individual values within the JSON
that are strings or NULLs.  The following table demonstrates the difference:

</p><center>
<table>
<tbody><tr><th>Operation</th><th>SQLite Result</th><th>MySQL Result
</th></tr><tr><td>json_extract('{"a":null,"b":"xyz"}','$.a')</td><td>NULL</td><td>'null'
</td></tr><tr><td>json_extract('{"a":null,"b":"xyz"}','$.b')</td><td>'xyz'</td><td>'"xyz"'
</td></tr></tbody></table></center>

<h2 id="the_and_operators"><span>4.6. </span>The -&gt; and -&gt;&gt; operators</h2>

<p>Beginning with SQLite version 3.38.0 (2022-02-22), the -&gt;
and -&gt;&gt; operators are available for extracting subcomponents of JSON.
The SQLite implementation of -&gt; and -&gt;&gt; strives to be
compatible with both MySQL and PostgreSQL.
The -&gt; and -&gt;&gt; operators take a JSON string
as their left operand and a PATH expression or object field
label or array index as their right operand.  The -&gt; operator
returns a JSON representation of the selected subcomponent or NULL if that
subcomponent does not exist.  The -&gt;&gt; operator returns an SQL TEXT,
INTEGER, REAL, or NULL value that represents the selected subcomponent,
or NULL if the subcomponent does not exist.

</p><p>Both the -&gt; and -&gt;&gt; operators select the same subcomponent
of the JSON to their left.  The difference is that -&gt; always returns a
JSON representation of that subcomponent and the -&gt;&gt; operator always
returns an SQL representation of that subcomponent.  Thus, these operators
are subtly different from a two-argument <a href="https://www.sqlite.org/json1.html#jex">json_extract()</a> function call.
A call to json_extract() with two arguments will return a JSON representation
of the subcomponent if and only if the subcomponent is a JSON array or
object, and will return an SQL representation of the subcomponent if the
subcomponent is a JSON null, string, or numeric value.

</p><p>The right-hand operand to the -&gt; and -&gt;&gt; operators can
be a well-formed JSON path expression.  This is the form used by MySQL.
For compatibility with PostgreSQL,
the -&gt; and -&gt;&gt; operators also accept a text label or
integer as their right-hand operand.  If the right operand is a text
label X, then it is interpreted as the JSON path '$.X'.  If the right
operand is an integer value N, then it is interpreted as the JSON path '$[N]'.

</p><p>Examples:

</p><ul>
<li><span>'{"a":2,"c":[4,5,{"f":7}]}' -&gt; '$'</span>
<span>→ '{"a":2,"c":[4,5,{"f":7}]}'</span></li>

<li><span>'{"a":2,"c":[4,5,{"f":7}]}' -&gt; '$.c'</span>
<span>→ '[4,5,{"f":7}]'</span></li>

<li><span>'{"a":2,"c":[4,5,{"f":7}]}' -&gt; 'c'</span>
<span>→ '[4,5,{"f":7}]'</span></li>

<li><span>'{"a":2,"c":[4,5,{"f":7}]}' -&gt; '$.c[2]'</span>
<span>→ '{"f":7}'</span></li>

<li><span>'{"a":2,"c":[4,5,{"f":7}]}' -&gt; '$.c[2].f'</span>
<span>→ '7'</span></li>

<li><span>'{"a":2,"c":[4,5],"f":7}' -&gt; '$.c[#-1]'</span>
<span>→ '5'</span></li>

<li><span>'{"a":2,"c":[4,5,{"f":7}]}' -&gt; '$.x'</span>
<span>→ NULL</span></li>

<li><span>'[11,22,33,44]' -&gt; 3</span>
<span>→ '44'</span></li>

<li><span>'[11,22,33,44]' -&gt;&gt; 3</span>
<span>→ 44</span></li>

<li><span>'{"a":"xyz"}' -&gt; '$.a'</span>
<span>→ '"xyz"'</span></li>

<li><span>'{"a":"xyz"}' -&gt;&gt; '$.a'</span>
<span>→ 'xyz'</span></li>

<li><span>'{"a":null}' -&gt; '$.a'</span>
<span>→ 'null'</span></li>

<li><span>'{"a":null}' -&gt;&gt; '$.a'</span>
<span>→ NULL</span></li>

</ul>


<h2 id="the_json_insert_json_replace_and_json_set_functions"><span>4.7. </span>The json_insert(), json_replace, and json_set() functions</h2>

<p>The json_insert(), json_replace, and json_set() functions all take
a single JSON value as their first argument followed by zero or more
pairs of path and value arguments, and return a new JSON string formed
by updating the input JSON by the path/value pairs.  The functions
differ only in how they deal with creating new values and overwriting
preexisting values.

</p><center>
<table>
<tbody><tr>
<th>Function</th><th>Overwrite if already exists?</th><th>Create if does not exist?
</th></tr><tr>
<td>json_insert()</td><td>No</td><td>Yes
</td></tr><tr>
<td>json_replace()</td><td>Yes</td><td>No
</td></tr><tr>
<td>json_set()</td><td>Yes</td><td>Yes
</td></tr></tbody></table></center>

<p>The json_insert(), json_replace(), and json_set() functions always
take an odd number of arguments.  The first argument is always the original
JSON to be edited.  Subsequent arguments occur in pairs with the first
element of each pair being a path and the second element being the value
to insert or replace or set on that path.

</p><p>Edits occur sequentially from left to right.  Changes caused by
prior edits can affect the path search for subsequent edits.

</p><p>If the value of a path/value pair is an SQLite TEXT value, then it
is normally inserted as a quoted JSON string, even if the string looks
like valid JSON.  However, if the value is the result of another
json function (such as <a href="https://www.sqlite.org/json1.html#jmini">json()</a> or <a href="https://www.sqlite.org/json1.html#jarray">json_array()</a> or <a href="https://www.sqlite.org/json1.html#jobj">json_object()</a>)
or if it is the result of <a href="https://www.sqlite.org/json1.html#jptr">the -&gt; operator</a>,
then it is interpreted as JSON and is inserted as JSON retaining all
of its substructure.  Values that are the result of <a href="https://www.sqlite.org/json1.html#jptr">the -&gt;&gt; operator</a>
are always interpreted as TEXT and are inserted as a JSON string even
if they look like valid JSON.

</p><p>These routines throw an error if the first JSON argument is not
well-formed or if any PATH argument is not well-formed or if any
argument is a BLOB.

</p><p>To append an element onto the end of an array, using json_insert()
with an array index of "#".  Examples:

</p><ul>
<li><span>json_insert('[1,2,3,4]','$[#]',99)</span>
<span>→ '[1,2,3,4,99]'</span></li>

<li><span>json_insert('[1,[2,3],4]','$[1][#]',99)</span>
<span>→ '[1,[2,3,99],4]'</span></li>

</ul>


<p>Other examples:

</p><ul>
<li><span>json_insert('{"a":2,"c":4}', '$.a', 99)</span>
<span>→ '{"a":2,"c":4}'</span></li>

<li><span>json_insert('{"a":2,"c":4}', '$.e', 99)</span>
<span>→ '{"a":2,"c":4,"e":99}'</span></li>

<li><span>json_replace('{"a":2,"c":4}', '$.a', 99)</span>
<span>→ '{"a":99,"c":4}'</span></li>

<li><span>json_replace('{"a":2,"c":4}', '$.e', 99)</span>
<span>→ '{"a":2,"c":4}'</span></li>

<li><span>json_set('{"a":2,"c":4}', '$.a', 99)</span>
<span>→ '{"a":99,"c":4}'</span></li>

<li><span>json_set('{"a":2,"c":4}', '$.e', 99)</span>
<span>→ '{"a":2,"c":4,"e":99}'</span></li>

<li><span>json_set('{"a":2,"c":4}', '$.c', '[97,96]')</span>
<span>→ '{"a":2,"c":"[97,96]"}'</span></li>

<li><span>json_set('{"a":2,"c":4}', '$.c', json('[97,96]'))</span>
<span>→ '{"a":2,"c":[97,96]}'</span></li>

<li><span>json_set('{"a":2,"c":4}', '$.c', json_array(97,96))</span>
<span>→ '{"a":2,"c":[97,96]}'</span></li>

</ul>


<h2 id="the_json_object_function"><span>4.8. </span>The json_object() function</h2>

<p>The json_object() SQL function accepts zero or more pairs of arguments
and returns a well-formed JSON object that is composed from those arguments.
The first argument of each pair is the label and the second argument of
each pair is the value.
If any argument to json_object() is a BLOB then an error is thrown.

</p><p>The json_object() function currently allows duplicate labels without
complaint, though this might change in a future enhancement.

</p><p>An argument with SQL type TEXT it is normally converted into a quoted 
JSON string even if the input text is well-formed JSON.  
However, if the argument is the direct result from another JSON
function or <a href="https://www.sqlite.org/json1.html#jptr">the -&gt; operator</a> (but not <a href="https://www.sqlite.org/json1.html#jptr">the -&gt;&gt; operator</a>), 
then it is treated as JSON and all of its JSON type information
and substructure is preserved.  This allows calls to json_object()
and <a href="https://www.sqlite.org/json1.html#jarray">json_array()</a> to be nested.  The <a href="https://www.sqlite.org/json1.html#jmini">json()</a> function can also
be used to force strings to be recognized as JSON.

</p><p>Examples:

</p><ul>
<li><span>json_object('a',2,'c',4)</span>
<span>→ '{"a":2,"c":4}'</span></li>

<li><span>json_object('a',2,'c','{e:5}')</span>
<span>→ '{"a":2,"c":"{e:5}"}'</span></li>

<li><span>json_object('a',2,'c',json_object('e',5))</span>
<span>→ '{"a":2,"c":{"e":5}}'</span></li>

</ul>


<h2 id="the_json_patch_function"><span>4.9. </span>The json_patch() function</h2>

<p>The json_patch(T,P) SQL function runs the
<a href="https://tools.ietf.org/html/rfc7396">RFC-7396</a> MergePatch algorithm
to apply patch P against input T.  The patched copy of T is returned.

</p><p>MergePatch can add, modify, or delete elements of a JSON Object,
and so for JSON Objects, the json_patch() routine is a generalized
replacement for <a href="https://www.sqlite.org/json1.html#jset">json_set()</a> and <a href="https://www.sqlite.org/json1.html#jrm">json_remove()</a>.  However, MergePatch
treats JSON Array objects as atomic.  MergePatch cannot append to an
Array nor modify individual elements of an Array.  It can only insert,
replace, or delete the whole Array as a single unit.  Hence, json_patch()
is not as useful when dealing with JSON that includes Arrays,
especially Arrays with lots of substructure.

</p><p>Examples:

</p><ul>
<li><span>json_patch('{"a":1,"b":2}','{"c":3,"d":4}')</span>
<span>→ '{"a":1,"b":2,"c":3,"d":4}'</span></li>

<li><span>json_patch('{"a":[1,2],"b":2}','{"a":9}')</span>
<span>→ '{"a":9,"b":2}'</span></li>

<li><span>json_patch('{"a":[1,2],"b":2}','{"a":null}')</span>
<span>→ '{"b":2}'</span></li>

<li><span>json_patch('{"a":1,"b":2}','{"a":9,"b":null,"c":8}')</span>
<span>→ '{"a":9,"c":8}'</span></li>

<li><span>json_patch('{"a":{"x":1,"y":2},"b":3}','{"a":{"y":9},"c":8}')</span>
<span>→ '{"a":{"x":1,"y":9},"b":3,"c":8}'</span></li>

</ul>


<h2 id="the_json_remove_function"><span>4.10. </span>The json_remove() function</h2>

<p>The json_remove(X,P,...) function takes a single JSON value as its
first argument followed by zero or more path arguments.
The json_remove(X,P,...) function returns
a copy of the X parameter with all the elements 
identified by path arguments removed.  Paths that select elements
not found in X are silently ignored.

</p><p>Removals occurs sequentially from left to right.  Changes caused by
prior removals can affect the path search for subsequent arguments.

</p><p>If the json_remove(X) function is called with no path arguments,
then it returns the input X reformatted, with excess whitespace
removed.

</p><p>The json_remove() function throws an error if the first argument
is not well-formed JSON or if any later argument is not a well-formed
path, or if any argument is a BLOB.

</p><p>Examples:

</p><ul>
<li><span>json_remove('[0,1,2,3,4]','$[2]')</span>
<span>→ '[0,1,3,4]'</span></li>

<li><span>json_remove('[0,1,2,3,4]','$[2]','$[0]')</span>
<span>→ '[1,3,4]'</span></li>

<li><span>json_remove('[0,1,2,3,4]','$[0]','$[2]')</span>
<span>→ '[1,2,4]'</span></li>

<li><span>json_remove('[0,1,2,3,4]','$[#-1]','$[0]')</span>
<span>→ '[1,2,3]'</span></li>

<li><span>json_remove('{"x":25,"y":42}')</span>
<span>→ '{"x":25,"y":42}'</span></li>

<li><span>json_remove('{"x":25,"y":42}','$.z')</span>
<span>→ '{"x":25,"y":42}'</span></li>

<li><span>json_remove('{"x":25,"y":42}','$.y')</span>
<span>→ '{"x":25}'</span></li>

<li><span>json_remove('{"x":25,"y":42}','$')</span>
<span>→ NULL</span></li>

</ul>


<h2 id="the_json_type_function"><span>4.11. </span>The json_type() function</h2>

<p>The json_type(X) function returns the "type" of the outermost element
of X.  The json_type(X,P) function returns the "type" of the element
in X that is selected by path P.  The "type" returned by json_type() is
one of the following SQL text values:
'null', 'true', 'false', 'integer', 'real', 'text', 'array', or 'object'.
If the path P in json_type(X,P) selects an element that does not exist
in X, then this function returns NULL.

</p><p>The json_type() function throws an error if any of its arguments is
not well-formed or is a BLOB.

</p><p>Examples:

</p><ul>
<li><span>json_type('{"a":[2,3.5,true,false,null,"x"]}')</span>
<span>→ 'object'</span></li>

<li><span>json_type('{"a":[2,3.5,true,false,null,"x"]}','$')</span>
<span>→ 'object'</span></li>

<li><span>json_type('{"a":[2,3.5,true,false,null,"x"]}','$.a')</span>
<span>→ 'array'</span></li>

<li><span>json_type('{"a":[2,3.5,true,false,null,"x"]}','$.a[0]')</span>
<span>→ 'integer'</span></li>

<li><span>json_type('{"a":[2,3.5,true,false,null,"x"]}','$.a[1]')</span>
<span>→ 'real'</span></li>

<li><span>json_type('{"a":[2,3.5,true,false,null,"x"]}','$.a[2]')</span>
<span>→ 'true'</span></li>

<li><span>json_type('{"a":[2,3.5,true,false,null,"x"]}','$.a[3]')</span>
<span>→ 'false'</span></li>

<li><span>json_type('{"a":[2,3.5,true,false,null,"x"]}','$.a[4]')</span>
<span>→ 'null'</span></li>

<li><span>json_type('{"a":[2,3.5,true,false,null,"x"]}','$.a[5]')</span>
<span>→ 'text'</span></li>

<li><span>json_type('{"a":[2,3.5,true,false,null,"x"]}','$.a[6]')</span>
<span>→ NULL</span></li>

</ul>


<h2 id="the_json_valid_function"><span>4.12. </span>The json_valid() function</h2>

<p>The json_valid(X) function return 1 if the argument X is well-formed
canonical RFC-7159 JSON without any extensions, or return 0 if the
argument X is not well-formed JSON or is JSON that includes
<a href="https://www.sqlite.org/json1.html#json5">JSON5 extensions</a>.

</p><p>Examples:

</p><ul>
<li><span>json_valid('{"x":35}')</span>
<span>→ 1</span></li>

<li><span>json_valid('{"x":35')</span>
<span>→ 0</span></li>

</ul>


<p>Use the expression "<a href="https://www.sqlite.org/json1.html#jerr">json_error_position(X)</a>==0" to determine if
a string is well-formed JSON5.  Use the "<a href="https://www.sqlite.org/json1.html#jmini">json(X)</a>" routine to convert
JSON5 into canonical JSON.

<a name="jquote"></a>

</p><h2 id="the_json_quote_function"><span>4.13. </span>The json_quote() function</h2>

<p>The json_quote(X) function converts the SQL value X (a number or a
string) into its corresponding JSON representation.  If X is a JSON value
returned by another JSON function, then this function is a no-op.

</p><p>Examples:

</p><ul>
<li><span>json_quote(3.14159)</span>
<span>→ 3.14159</span></li>

<li><span>json_quote('verdant')</span>
<span>→ '"verdant"'</span></li>

<li><span>json_quote('[1]')</span>
<span>→ '"[1]"'</span></li>

<li><span>json_quote(json('[1]'))</span>
<span>→ '[1]'</span></li>

<li><span>json_quote('[1,')</span>
<span>→ '"[1,"'</span></li>

</ul>


<h2 id="the_json_group_array_and_json_group_object_aggregate_sql_functions"><span>4.14. </span>The json_group_array() and json_group_object()
aggregate SQL functions</h2>

<p>The json_group_array(X) function is an
<a href="https://www.sqlite.org/lang_aggfunc.html">aggregate SQL function</a> that returns a JSON array
comprised of all X values in the aggregation.
Similarly, the json_group_object(NAME,VALUE) function returns a JSON object
comprised of all NAME/VALUE pairs in the aggregation.


<a name="jeach"></a>

<a name="jtree"></a>

</p><h2 id="the_json_each_and_json_tree_table_valued_functions"><span>4.15. </span>The json_each() and json_tree() table-valued functions</h2>

<p>The json_each(X) and json_tree(X) <a href="https://www.sqlite.org/vtab.html#tabfunc2">table-valued functions</a> walk the
JSON value provided as their first argument and return one row for each
element.  The json_each(X) function only walks the immediate children
of the top-level array or object,
or just the top-level element itself if the top-level
element is a primitive value.
The json_tree(X) function recursively walks through the
JSON substructure starting with the top-level element.  

</p><p>The json_each(X,P) and json_tree(X,P) functions work just like
their one-argument counterparts except that they treat the element
identified by path P as the top-level element.

</p><p>The schema for the table returned by json_each() and json_tree() is
as follows:

</p><blockquote><pre>CREATE TABLE json_tree(
    key ANY,             -- key for current element relative to its parent
    value ANY,           -- value for the current element
    type TEXT,           -- 'object','array','string','integer', etc.
    atom ANY,            -- value for primitive types, null for array &amp; object
    id INTEGER,          -- integer ID for this element
    parent INTEGER,      -- integer ID for the parent of this element
    fullkey TEXT,        -- full path describing the current element
    path TEXT,           -- path to the container of the current row
    json JSON HIDDEN,    -- 1st input parameter: the raw JSON
    root TEXT HIDDEN     -- 2nd input parameter: the PATH at which to start
);
</pre></blockquote>

<p>
The "key" column is the integer array index for elements of a JSON array 
and the text label for elements of a JSON object.  The key column is
NULL in all other cases.

</p><p>
The "atom" column is the SQL value corresponding to primitive elements - 
elements other than JSON arrays and objects.  The "atom" column is NULL
for a JSON array or object.  The "value" column is the same as the
"atom" column for primitive JSON elements but takes on the text JSON value
for arrays and objects.

</p><p>
The "type" column is an SQL text value taken from ('null', 'true', 'false',
'integer', 'real', 'text', 'array', 'object') according to the type of
the current JSON element.

</p><p>
The "id" column is an integer that identifies a specific JSON element
within the complete JSON string.  The "id" integer is an internal housekeeping
number, the computation of which might change in future releases.  The
only guarantee is that the "id" column will be different for every row.

</p><p>
The "parent" column is always NULL for json_each().
For json_tree(),
the "parent" column is the "id" integer for the parent of the current
element, or NULL for the top-level JSON element or the element identified
by the root path in the second argument.

</p><p>
The "fullkey" column is a text path that uniquely identifies the current
row element within the original JSON string.  The complete key to the
true top-level element is returned even if an alternative starting point
is provided by the "root" argument.

</p><p>
The "path" column is the path to the array or object container that holds 
the current row, or the path to the current row in the case where the 
iteration starts on a primitive type and thus only provides a single
row of output.

</p><h3 id="examples_using_json_each_and_json_tree_"><span>4.15.1. </span>Examples using json_each() and json_tree()</h3>

<p>Suppose the table "CREATE TABLE user(name,phone)" stores zero or
more phone numbers as a JSON array object in the user.phone field.
To find all users who have any phone number with a 704 area code:

</p><blockquote><pre>SELECT DISTINCT user.name
  FROM user, json_each(user.phone)
 WHERE json_each.value LIKE '704-%';
</pre></blockquote>

<p>Now suppose the user.phone field contains plain text if the user
has only a single phone number and a JSON array if the user has multiple
phone numbers.  The same question is posed: "Which users have a phone number
in the 704 area code?"  But now the json_each() function can only be called
for those users that have two or more phone numbers since json_each()
requires well-formed JSON as its first argument:

</p><blockquote><pre>SELECT name FROM user WHERE phone LIKE '704-%'
UNION
SELECT user.name
  FROM user, json_each(user.phone)
 WHERE json_valid(user.phone)
   AND json_each.value LIKE '704-%';
</pre></blockquote>

<p>Consider a different database with "CREATE TABLE big(json JSON)".
To see a complete line-by-line decomposition of the data:

</p><blockquote><pre>SELECT big.rowid, fullkey, value
  FROM big, json_tree(big.json)
 WHERE json_tree.type NOT IN ('object','array');
</pre></blockquote>

<p>In the previous, the "type NOT IN ('object','array')" term of the
WHERE clause suppresses containers and only lets through leaf elements.
The same effect could be achieved this way:

</p><blockquote><pre>SELECT big.rowid, fullkey, atom
  FROM big, json_tree(big.json)
 WHERE atom IS NOT NULL;
</pre></blockquote>

<p>Suppose each entry in the BIG table is a JSON object 
with a '$.id' field that is a unique identifier
and a '$.partlist' field that can be a deeply nested object.
You want to find the id of every entry that contains one
or more references to uuid '6fa5181e-5721-11e5-a04e-57f3d7b32808' anywhere
in its '$.partlist'.

</p><blockquote><pre>SELECT DISTINCT json_extract(big.json,'$.id')
  FROM big, json_tree(big.json, '$.partlist')
 WHERE json_tree.key='uuid'
   AND json_tree.value='6fa5181e-5721-11e5-a04e-57f3d7b32808';
</pre></blockquote>
<p><small><i>This page last modified on  <a href="https://sqlite.org/docsrc/honeypot" id="mtimelink" data-href="https://sqlite.org/docsrc/finfo/pages/json1.in?m=f2d9d092d1">2023-08-01 17:34:32</a> UTC </i></small></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Source code for Quake 2 rerelease (498 pts)]]></title>
            <link>https://github.com/id-Software/quake2-rerelease-dll</link>
            <guid>37082771</guid>
            <pubDate>Thu, 10 Aug 2023 22:57:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/id-Software/quake2-rerelease-dll">https://github.com/id-Software/quake2-rerelease-dll</a>, See on <a href="https://news.ycombinator.com/item?id=37082771">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Quake II Rerelease Game Source</h2>
<p dir="auto">This repository contains the game code for the 2023 rerelease of Quake II, for users who wish to mod the game, along with the original game code that was use for reference. Mods can be loaded into the rerelease the same way as the original game: launch the game with <code>+set game mymod</code> or type <code>game mymod</code> into the console while the game is running. We recommend installing mods into your <code>%USERPROFILE%\Saved Games\Nightdive Studios\Quake II</code> directory to ensure the original game files do not get modified.</p>
<p dir="auto">id Software is unable to provide support for this release, however we urge you to take advantage of the depth of community-driven resources already available.</p>
<p dir="auto">The rerelease of Quake II uses a new version of the API to communicate between the server &amp; the game module. It also introduces a very thin "client game" module, akin to Quake III Arena's cgame module, to allow for extended modding opportunities that change previously hardcoded client behavior. It also has a new network protocol, version 2023.</p>
<p dir="auto">This codebase is a combination of the separate game modules that were part of the original game: baseq2, ctf, rogue, and xatrix. It requires a C++17 compiler. In cases of conflicting spawnflags, maps were modified in order to resolve issues, so original expansion pack maps may not load correctly with this DLL. The combined FGD as used for development is also available for users wishing to make new maps. A modified TrenchBroom Quake II <code>gameconfig.cfg</code> is included as there are modified textureflags.</p>
<p dir="auto">Because the game export interface has changed, existing mods may be able to be moved over to support the API changes. However, in order to support all expansion packs under one codebase and new features in the rerelease, there have been some major changes to structure and layout, so old mods wishing to use the new codebase may need to be rewritten.</p>
<p dir="auto">The API changes discussed here are written from the perspective of the game DLL.</p>
<h2 tabindex="-1" dir="auto">Compiling</h2>
<p dir="auto">The game DLL has only been tested with Clang, VS2019 and VS2022.</p>
<p dir="auto">The code can compile under both C++17 and C++20. Using C++20 allows you to skip <code>fmtlib</code> as a dependency.</p>
<h3 tabindex="-1" dir="auto">Required preprocessor definitions</h3>
<ul dir="auto">
<li><code>GAME_INCLUDE</code> must be defined, tells <code>game.h</code> that this is the game DLL compiling it.</li>
<li><code>KEX_Q2GAME_EXPORTS</code> must be defined, tells <code>game.h</code> that we are exporting the <code>GetGameAPI</code> function.</li>
<li><code>KEX_Q2GAME_DYNAMIC</code> must be defined. The game DLL supports static linking on console platforms, but is always dynamic on PC.</li>
<li><code>NO_FMT_SOURCE</code>: this is only here because of a limitation in the internal build system. It must be defined.</li>
</ul>
<h3 tabindex="-1" dir="auto">Optional preprocessor definitions</h3>
<ul dir="auto">
<li><code>KEX_Q2_GAME</code>: must be defined if compiling for Kex. This changes the behavior of the <code>say</code> command to go through the lobby.</li>
<li><code>KEX_Q2GAME_IMPORTS</code>: only used by engine, tells <code>game.h</code> that we are importing <code>GetGameAPI</code>.</li>
<li><code>USE_CPP20_FORMAT</code>: if explicitly defined, C++20's <code>&lt;format&gt;</code> library will be used instead of <code>fmtlib</code>; otherwise <code>fmtlib</code> usage will be autodetected.</li>
</ul>
<h3 tabindex="-1" dir="auto">Dependencies</h3>
<ul dir="auto">
<li><a href="https://github.com/fmtlib/fmt">fmtlib</a>: If <code>USE_CPP20_FORMAT</code> is not set, the library needs to be available in the <code>fmt</code> subdirectory.</li>
<li><a href="https://github.com/open-source-parsers/jsoncpp">jsoncpp</a>: Must be placed inside <code>json</code> subdirectory.</li>
</ul>
<p dir="auto">Both of these can also be installed via vcpkg: <code>vcpkg install jsoncpp:x64-windows fmt:x64-windows</code></p>
<h3 tabindex="-1" dir="auto">Windows (Visual Studio 2019 / 2022):</h3>
<ul dir="auto">
<li>We recommend placing the source in a subfolder within a mod directory. For example, alongside <code>baseq2</code>, make a folder called <code>mymod</code>, enter that folder, make a folder called <code>src</code>, and copying the contents of the <code>rerelease</code> directory into the newly-created <code>src</code> subfolder.</li>
<li>Open <code>game.sln</code></li>
<li>Build solution</li>
</ul>
<p dir="auto">Debugging the DLL is possible when attaching to the engine EXE. Note that if you are using VS2022 Hot Reload, due to an internal Hot Reload issue, current edits will be lost when disconnecting from the server, or changing levels using the <code>map</code> command.</p>
<h2 tabindex="-1" dir="auto">40hz Tickrate Support</h2>
<p dir="auto">As part of this release, all internal logic in the game DLL has been adjusted to run at 40hz compared to the original 10hz of the original engine. This provides a better gameplay experience, and allows maps and game logic to run at more precise steps than the original 100ms. 40hz was chosen as it is a multiple of the original 10hz, operates at an integral 25ms step, and was the best balance between bandwidth and CPU concerns around the original tech.</p>
<h2 tabindex="-1" dir="auto">Print Adjustments</h2>
<p dir="auto">As part of the API cleanup, the game DLL no longer uses varargs in any of its functions. Varargs are compiler-specific and not very portable, so instead, the onus is on the caller to handle formatting. As a bonus, this allows the game DLL to more easily hook in modern formatting providers; the game DLL uses <code>fmt</code> almost exclusively. Several built-in types, like <code>edict_t</code> and <code>vec3_t</code>, can be formatted directly.</p>
<h2 tabindex="-1" dir="auto">Math Changes</h2>
<p dir="auto">Since C++ is now used in the game DLL, math functions were made constexpr where appropriate, and operator overloads are used to make math easier to work with and closer to QuakeC. For instance, <code>VectorMA(a, s, b, c)</code> can now be written as <code>c = a + (b * s)</code>, which expresses the operation better.</p>
<h2 tabindex="-1" dir="auto">Type Changes</h2>
<p dir="auto"><code>qboolean</code>, which was aliased to <code>int32_t</code>, is now instead aliased to <code>bool</code>. This type should be equivalent to C's <code>_Bool</code>.</p>
<h2 tabindex="-1" dir="auto">Info Keys</h2>
<p dir="auto">In the original Quake II, all infokey values are purely ASCII with upper bits stripped. Kex and the Quake II rerelease engine supports UTF-8 for things like player names, which necessated a change to the way info keys work. Instead of implementing a whole UTF-8 system in the game DLL, these functions are now imports, so the engine is in control of the parsing and string management.</p>
<p dir="auto">Userinfo variables are now suffixed with a number for split screen support. For instance, <code>name</code> and <code>skin</code> have become <code>name_0</code> and <code>skin_0</code> for the first player, <code>name_1</code> and <code>skin_1</code> for the second player, and so on.</p>
<h2 tabindex="-1" dir="auto">Extensions</h2>
<p dir="auto">In an attempt to remain compatible with future community engines, all engine interfaces contain stubbed functions for GetExtension. This is currently unused and will only return nullptr, however other engines may wish to support returning named structs containing extra function pointers based on string parameters. This is similar to <code>getextension</code> that has become standard in many QuakeC environments.</p>
<p dir="auto">Conforming engines should return nullptr if an extension is not supported, otherwise they should return a non null pointer value that is relevant to the requested feature. Supporting engines should use namespaces for features to prevent name collisions around common and possibly incompatible implementations.</p>
<h2 tabindex="-1" dir="auto">Player Movement</h2>
<p dir="auto">Player movement ("pmove") is now handled as an export in both <code>game_export_t</code> <em>and</em> <code>cgame_export_t</code>. This allows a game module to modify physics while still working with client prediction. Pmove also received several upgrades, such as more bits for flags and full float positioning.</p>
<p dir="auto">Because a lot of movement quirks in Quake II were indirectly caused by the compression, these behaviors were retained. Trick jumping is now an explicit movement type, to allow for things like the Mega Health box jumps to still work. Some fixes were made, like jumping higher below the 0 z point of the map.</p>
<h2 tabindex="-1" dir="auto">Frame visibility</h2>
<p dir="auto">As part of network improvements, some changes were made to the "entity is visible to client in frame" methods:</p>
<ul dir="auto">
<li>Split-screen support since all clients share a frame.</li>
<li>Entities with shadows will be visible if their shadows may be visible to a player.</li>
<li>Sound attenuation culling is now calculated formulaically to match the sound system, and takes loop_attenuation into account.</li>
</ul>
<h2 tabindex="-1" dir="auto">Entity linkage</h2>
<p dir="auto">To fix a legacy bug where lasers only relied on one position for culling, RF_BEAM entities now set their <code>absmin</code> &amp; <code>absmax</code> properly. This can cause them to be a bit inflated if they are angled, but should not cause any issues otherwise.</p>
<p dir="auto">In a similar vein, <code>gi.linkentity</code> no longer automatically sets <code>old_origin</code> for beams. This is to make it a bit easier to work with beams, as otherwise you'd be forced to link twice to get it linked into the world correctly. This <em>might</em> break old mods that depends on this behavior.</p>
<h2 tabindex="-1" dir="auto">Audio positioning</h2>
<p dir="auto">Entity spatialization underwent an overhaul (<code>CL_GetEntitySoundOrigin</code> mainly).</p>
<ul dir="auto">
<li>Brush models will use the closest point on the bmodel's absmin/absmax to the listener's origin. This allows moving brush models with sounds to make consistent sounds, and be full volume if you are inside of them.</li>
<li>Beams now support <code>s.sound</code>, and plays their sound on the nearest point between the two beam origins.</li>
</ul>
<p dir="auto">As a secondary fix to the above, <code>S_StartSound</code> has slightly different logic now surrounding what origin to pick when playing a sound:</p>
<div dir="auto" data-snippet-clipboard-copy-content="if (entnum < MAX_EDICTS &amp;&amp; (origin || fixed_origin))
{
	// [Paril-KEX] if we can currently see the entity in question
	// and it's a bmodel or beam, don't use a fixed origin so it sounds correct
	if (!fixed_origin &amp;&amp; entnum > 1 &amp;&amp; (cl_entities[entnum].serverframe == cl.frame.serverframe || 
		(cl_entities[entnum].current.solid == PACKED_SOLID_BSP || (cl_entities[entnum].current.renderfx &amp; RF_BEAM))))
	{
		ps->fixed_origin = false;
	}
	else
	{
		VectorCopy (origin, ps->origin);
		ps->fixed_origin = true;
	}
}
else
	ps->fixed_origin = false;"><pre><span>if</span> (entnum &lt; MAX_EDICTS &amp;&amp; (origin || fixed_origin))
{
	<span><span>//</span> [Paril-KEX] if we can currently see the entity in question</span>
	<span><span>//</span> and it's a bmodel or beam, don't use a fixed origin so it sounds correct</span>
	<span>if</span> (!fixed_origin &amp;&amp; entnum &gt; <span>1</span> &amp;&amp; (cl_entities[entnum].<span>serverframe</span> == cl.<span>frame</span>.<span>serverframe</span> || 
		(cl_entities[entnum].<span>current</span>.<span>solid</span> == PACKED_SOLID_BSP || (cl_entities[entnum].<span>current</span>.<span>renderfx</span> &amp; RF_BEAM))))
	{
		ps-&gt;<span>fixed_origin</span> = <span>false</span>;
	}
	<span>else</span>
	{
		<span>VectorCopy</span> (origin, ps-&gt;<span>origin</span>);
		ps-&gt;<span>fixed_origin</span> = <span>true</span>;
	}
}
<span>else</span>
	ps-&gt;fixed_origin = <span>false</span>;</pre></div>
<p dir="auto"><code>fixed_origin</code> is set to <code>flags &amp; SND_EXPLICIT_POS</code> for svc_sound packets, and is <code>false</code> otherwise. If the playsounds' <code>fixed_origin</code> field is set, then the <code>ps-&gt;origin</code> value will <em>always</em> be used over automatically trying to determine its position.</p>
<h2 tabindex="-1" dir="auto">Client entity adjustments</h2>
<ul dir="auto">
<li>Beam origin points are interpolated if they exist between frames.</li>
<li><code>TE_ELECTRIC_SPARKS</code> and <code>TE_SCREEN_SPARKS</code>/<code>TE_SHIELD_SPARKS</code> will only play sounds once on any given frame.</li>
<li>Entities will never play the same footstep sound twice in a row.</li>
<li>Beams now squash the ends of their beams so they don't intersect walls or end early.</li>
<li>Alpha and transparency settings now get copied over to all sub-modelindices.</li>
<li>Lightramps are now interpolated, which looks nicer and helps with epilepsy.</li>
<li><code>delta_angles</code> are interpolated now, although this is never used at all in the game.</li>
<li><code>screen_blend</code> and <code>damage_blend</code> are interpolated now, but only if the frame prior didn't have a clear color.</li>
</ul>
<h2 tabindex="-1" dir="auto">Configstrings</h2>
<p dir="auto">Configstrings have been overhauled. There is now a theoretical maximum of <code>32767</code> entries, although you are still bound by the game APIs value of <code>MAX_CONFIGSTRINGS</code>.</p>
<p dir="auto">The maximum length of a configstring has increased from <code>64</code> to <code>96</code>.</p>
<p dir="auto">The API now canonizes that certain spans (<code>CS_STATUSBAR</code> and <code>CS_GENERAL</code>) can span multiple lines. A <code>CS_SIZE</code> function is provided to calculate the total size (in bytes) that can be written to for a given configstring ID.</p>
<p dir="auto">A convenience function, <code>CS_REMAP</code>, is provided to help remap old configstring IDs to new ones. This is used in our engine to provide old demo support.</p>
<p dir="auto"><code>MAX_MODELS</code>, <code>MAX_SOUNDS</code> and <code>MAX_IMAGES</code> have been increased from 256 to 8192, 2048, and 512, respectively.</p>
<h3 tabindex="-1" dir="auto">CS_STATUSBAR</h3>
<p dir="auto">This entry now spans entries 5 to 58 instead of 5 to 28, giving you an effective size of 5184 bytes (minus one for the null terminator) for the statusbar strings, up from 1536 bytes.</p>
<h3 tabindex="-1" dir="auto">CS_SHADOWLIGHTS</h3>
<p dir="auto">This new span each consists of a shadow light entry. Its format is semicolon-separated numerical values, in the following type &amp; order:</p>
<ul dir="auto">
<li>int entity_order</li>
<li>int type (0 = point, 1 = cone)</li>
<li>float radius</li>
<li>int resolution</li>
<li>float intensity</li>
<li>float fade_start</li>
<li>float fade_end</li>
<li>int lightstyle</li>
<li>float coneangle</li>
<li>float direction_x</li>
<li>float direction_y</li>
<li>float direction_z</li>
</ul>
<h3 tabindex="-1" dir="auto">CS_WHEEL_WEAPONS</h3>
<p dir="auto">This new span consists of entries for the weapon wheel. It consists of pipe-separated integral values, in the following order:</p>
<ul dir="auto">
<li>CS_ITEMS item index</li>
<li>CS_IMAGES image index</li>
<li>CS_WHEEL_AMMO ammo index (or -1 for no ammo)</li>
<li>minimum ammo to use weapon</li>
<li>whether the weapon is on the powerup wheel or the weapon wheel</li>
<li>additional sort integer</li>
<li>quantity to warn on low ammo on</li>
<li>whether this weapon is droppable or not</li>
</ul>
<h3 tabindex="-1" dir="auto">CS_WHEEL_AMMO</h3>
<p dir="auto">This new span consists of entries for the weapon wheel ammo types. It consists of pipe-separated integral values, in the following order:</p>
<ul dir="auto">
<li>CS_ITEMS item index</li>
<li>CS_IMAGES image index</li>
</ul>
<h3 tabindex="-1" dir="auto">CS_WHEEL_POWERUPS</h3>
<p dir="auto">This new span consists of entries for the powerup wheel. It consists of pipe-separated integral values, in the following order:</p>
<ul dir="auto">
<li>CS_ITEMS image index</li>
<li>CS_IMAGES image index</li>
<li>if 1, it is a togglable powerup instead of having a count</li>
<li>additional sort integer</li>
<li>whether we can drop this powerup or not</li>
<li>CS_WHEEL_AMMO ammo index, if applicable (-1 for no ammo)</li>
</ul>
<h3 tabindex="-1" dir="auto">CS_CD_LOOP_COUNT</h3>
<p dir="auto">Integer which determines how many times to loop the music before switching to the ambient track. Leave blank to use the clients' preferred value, otherwise it is forced to this value (a value of zero means never switch to ambient track).</p>
<h3 tabindex="-1" dir="auto">CS_GAME_STYLE</h3>
<p dir="auto">Inform the client about the type of game being played.</p>
<h2 tabindex="-1" dir="auto">Structures</h2>
<p dir="auto">Quake II has two main shared structures: <code>gclient_t</code> and <code>edict_t</code>. These split off into various other shared structures that have to be the same between the game &amp; server.</p>
<p dir="auto">Like the original release, the "shared" aspects of these structs must be identical, and are stored in <code>game.h</code> as <code>edict_shared_t</code> and <code>gclient_shared_t</code> respectively.</p>
<p dir="auto">The structure changes will be listed from the bottom-up. Full listings of the structures can be found in the source.</p>
<h2 tabindex="-1" dir="auto">cvar_flags_t</h2>
<h3 tabindex="-1" dir="auto">CVAR_USER_PROFILE (bit 5)</h3>
<p dir="auto">This is a new flag that is solely for the client; it indicates that a cvar is treated like userinfo for the purposes of storage, but is not sent to the server like userinfo usually is. For example, this flag is applied to <code>cl_run_N</code>, which controls the individual Always Run flags for each split screen player.</p>
<h2 tabindex="-1" dir="auto">contents_t</h2>
<h3 tabindex="-1" dir="auto">CONTENTS_PROJECTILECLIP (bit 14)</h3>
<p dir="auto">This new content flag will be collided against by <code>CONTENTS_PROJECTILE</code> entities.</p>
<h3 tabindex="-1" dir="auto">CONTENTS_PLAYER (bit 30)</h3>
<p dir="auto">This special content type flag is set only on player entities, and allows tracing to exclude/include them.</p>
<h3 tabindex="-1" dir="auto">CONTENTS_PROJECTILE (bit 31)</h3>
<p dir="auto">This special content type flag is set only on projectiles, and allows tracing to exclude/include them.</p>
<h2 tabindex="-1" dir="auto">surfflags_t</h2>
<h3 tabindex="-1" dir="auto">SURF_ALPHATEST (bit 25)</h3>
<p dir="auto">This bit is widely supported by other engines and is supported in the rerelease.</p>
<h3 tabindex="-1" dir="auto">SURF_N64_UV (bit 28)</h3>
<p dir="auto">This flag is specific to N64, and halves texture sizes.</p>
<h3 tabindex="-1" dir="auto">SURF_N64_SCROLL_X (bit 29)</h3>
<p dir="auto">This flag is specific to N64, and causes textures to scroll in the X axis.</p>
<h3 tabindex="-1" dir="auto">SURF_N64_SCROLL_Y (bit 30)</h3>
<p dir="auto">This flag is specific to N64, and causes textures to scroll in the Y axis.</p>
<h3 tabindex="-1" dir="auto">SURF_N64_SCROLL_FLIP (bit 31)</h3>
<p dir="auto">This flag is specific to N64, and flips the scroll axis.</p>
<h2 tabindex="-1" dir="auto">csurface_t</h2>
<p dir="auto">This structure has undergone canonization of the 3.2x changes by Zoid.</p>
<h3 tabindex="-1" dir="auto">char[32] name</h3>
<p dir="auto">Uses the proper name length now.</p>
<h3 tabindex="-1" dir="auto">uint32_t id</h3>
<p dir="auto">This value must represent a unique ID that corresponds to its texinfo. The same ID must always reference the same texinfo, but they don't necessarily have to be sequential. Zero must always mean "no texinfo".</p>
<p dir="auto">This is used by the client for indexing footstep sounds.</p>
<h3 tabindex="-1" dir="auto">char[16] material</h3>
<p dir="auto">The material ID for this texinfo, from the corresponding <code>.mat</code> file.</p>
<h2 tabindex="-1" dir="auto">trace_t</h2>
<h3 tabindex="-1" dir="auto">csurface_t *surface</h3>
<p dir="auto">The only change is a contractual one: this value must never be null.</p>
<h3 tabindex="-1" dir="auto">cplane_t plane2 / csurface_t *surface2</h3>
<p dir="auto">When a trace impacts multiple planes at the destination, the collision system will now report both of them. The "second best" plane and surface are stored here. <code>surface2</code> <em>must</em> be null if a second surface was not hit.</p>
<p dir="auto">This is used to solve some epsilon issues with the player movement system.</p>
<h2 tabindex="-1" dir="auto">cvar_t</h2>
<h3 tabindex="-1" dir="auto">int32_t modified_count</h3>
<p dir="auto">The old <code>qboolean modified;</code> has been changed into an integral value. This value is increased when the cvar has been changed, but is <strong>never</strong> zero. The reason for this is so that "is cvar modified" checks always succeed on the first check, assuming you initialize the last modified value to 0.</p>
<p dir="auto">The function <code>Cvar_WasModified</code> is provided as a convenience function to perform this task for you.</p>
<h3 tabindex="-1" dir="auto">int32_t integer</h3>
<p dir="auto">A common extension to Quake II, the integral value is stored at the end of the struct for you to use.</p>
<h2 tabindex="-1" dir="auto">player_state_t</h2>
<h3 tabindex="-1" dir="auto">int32_t gunskin</h3>
<p dir="auto">This is a new value which sets the skin number used on the weapon.</p>
<h3 tabindex="-1" dir="auto">int32_t gunrate</h3>
<p dir="auto">This value sets the frame rate (in hz) of the players' weapon. For backwards compatibility, a value of 0 is equivalent to a value of 10. This ia mainly used for the Haste powerup, but in theory it could be used for future mods to have higher tickrate weapons in general.</p>
<h3 tabindex="-1" dir="auto">float[4] screen_blend / damage_blend</h3>
<p dir="auto">The full-screen <code>blend</code> value was split into two values: <code>screen_blend</code> and <code>damage_blend</code>.</p>
<p dir="auto"><code>screen_blend</code> is the same as the original one, and is a full-screen color change. It is mainly used now for full-screen fades. To reduce the amount of screen flashing, the base game avoids flashing the screen whenever possible.</p>
<p dir="auto"><code>damage_blend</code> is a new type of blend that occurs around the edge of the screen; this is used to replace many events that previously would flash the full screen.</p>
<h3 tabindex="-1" dir="auto">refdef_flags_t rdflags</h3>
<p dir="auto">The only adjustment to <code>rdflags</code> was the addition of a new flag: <code>RDF_NO_WEAPON_LERP</code>. This occupies bit 4, and can be used to temporarily disable interpolation on weapons.</p>
<h3 tabindex="-1" dir="auto">short[64] stats</h3>
<p dir="auto"><code>MAX_STATS</code> was increased from 32 to 64. Note that because stats are now handled by the game &amp; cgame modules, you are not limited to a short for the purposes of packing down data/</p>
<h3 tabindex="-1" dir="auto">uint8_t team_id</h3>
<p dir="auto">For teamplay-oriented games, the player's team is sent in player state. While the client could derive this from entity state in theory, in practice that's a bit ugly since the players' entity may not even be visible (for instance if you've been gibbed), so this was the cleaner approach.</p>
<h2 tabindex="-1" dir="auto">usercmd_t</h2>
<p dir="auto">The fields <code>upmove</code>, <code>impulse</code> and <code>lightlevel</code> have been removed.</p>
<h3 tabindex="-1" dir="auto">button_t buttons</h3>
<h4 tabindex="-1" dir="auto">BUTTON_HOLSTER (bit 2)</h4>
<p dir="auto">This button corresponds to the new <code>+holster</code> command, which will keep the weapon holstered until depressed. It is used by the weapon wheel to allow the player to start switching weapons before the weapon wheel is dismissed.</p>
<h4 tabindex="-1" dir="auto">BUTTON_JUMP (bit 3)</h4>
<h4 tabindex="-1" dir="auto">BUTTON_CROUCH (bit 4)</h4>
<p dir="auto">These two new bits replace <code>usercmd_t::upmove</code>, and determine the players' jumping and crouch states.</p>
<h3 tabindex="-1" dir="auto">vec3_t angles</h3>
<p dir="auto">These are now full float precision, allowing for players to aim more precisely.</p>
<h3 tabindex="-1" dir="auto">float forwardmove / sidemove</h3>
<p dir="auto">These are now full float, to allow controller inputs to be more precise.</p>
<h3 tabindex="-1" dir="auto">uint32_t server_frame</h3>
<p dir="auto">New entry sent along with every usercmd, which tells the server which server frame that the input was depressed on. This is used for integrity checks, as well as for anti-lag hitscan.</p>
<h2 tabindex="-1" dir="auto">pmove_state_t</h2>
<h3 tabindex="-1" dir="auto">pmtype_t pm_type</h3>
<p dir="auto">Two new pmove types have been added before <code>PM_SPECTATOR</code>, offsetting it and its subsequent entries by 2.</p>
<h4 tabindex="-1" dir="auto">PM_GRAPPLE (1)</h4>
<p dir="auto">Used for the grappling hook; it informs client prediction that you should be pulled towards <code>velocity</code> and are not affected by gravity.</p>
<h4 tabindex="-1" dir="auto">PM_NOCLIP (2)</h4>
<p dir="auto">This is what <code>PM_SPECTATOR</code> used to be, and prevents all clipping.</p>
<h4 tabindex="-1" dir="auto">PM_SPECTATOR (3)</h4>
<p dir="auto">This value now represents spectator mode; you cannot enter walls, but can go through brush entities.</p>
<h3 tabindex="-1" dir="auto">vec3_t origin / vec3_t velocity / vec3_t delta_angles</h3>
<p dir="auto">These fields now have full float precision versus the original release. See <a href="#pmove">Pmove</a> for more details.</p>
<h3 tabindex="-1" dir="auto">pmflags_t pm_flags</h3>
<p dir="auto">This type has had its capacity increased to <code>int16</code>. The following flags are new or adjusted:</p>
<h4 tabindex="-1" dir="auto">PMF_NO_POSITIONAL_PREDICTION</h4>
<p dir="auto">This flag was originally called <code>PMF_NO_PREDICTION</code>; it now only disables prediction on origin, allowing angles to be predicted. <strong>This is a backwards-incompatible change</strong>, but should have very minimal impact on running old mods. This improves the feeling of the grappling hook.</p>
<h4 tabindex="-1" dir="auto">PMF_ON_LADDER</h4>
<p dir="auto">This bit is used to signal back to the game that we are currently attached to a ladder.</p>
<h4 tabindex="-1" dir="auto">PMF_NO_ANGULAR_PREDICTION</h4>
<p dir="auto">The angular equivalent of <code>PMF_NO_POSITIONAL_PREDICTION</code>.</p>
<h4 tabindex="-1" dir="auto">PMF_IGNORE_PLAYER_COLLISION</h4>
<p dir="auto">This flag is input only, and tells Pmove to ignore <code>CONTENTS_PLAYER</code> contents.</p>
<h4 tabindex="-1" dir="auto">PMF_TIME_TRICK</h4>
<p dir="auto">If set, then <code>pm_time</code> is the time remaining to start a trick jump.</p>
<h3 tabindex="-1" dir="auto">uint16_t pm_time</h3>
<p dir="auto"><code>pm_time</code> is now expressed in milliseconds instead of 8 * ms; since the code clamped subtractions on this to 1, it meant that high framerate players experienced slightly different physics, and in the case of trick jumps, had a smaller time gap to perform them.</p>
<h3 tabindex="-1" dir="auto">int8_t viewheight</h3>
<p dir="auto">A new field describing the viewheight output; this is for crouch prediction.</p>
<h2 tabindex="-1" dir="auto">pmove_t</h2>
<p dir="auto">The field <code>viewheight</code> has been removed, since it is now part of <code>pmove_state_t</code>.</p>
<h3 tabindex="-1" dir="auto">touch_list_t touch</h3>
<p dir="auto">The list of touched entities has been replaced with a list of traces, allowing the game to react better to touches.</p>
<h3 tabindex="-1" dir="auto">cplane_t groundplane</h3>
<p dir="auto">The plane that you're standing on is now returned by pmove.</p>
<h3 tabindex="-1" dir="auto">edict_t *player</h3>
<p dir="auto">An opaque handle to the player object, passed back to <code>trace</code>.</p>
<h3 tabindex="-1" dir="auto">trace / clip</h3>
<p dir="auto"><code>trace</code> is now sent the <code>passent</code> and <code>contentmask</code>, so it can perform more complex tracing routines.</p>
<p dir="auto"><code>clip</code> is also now available to pmove, should you need it. It is currently only used in spectator movement, to clip solely against the world and nothing else.</p>
<h3 tabindex="-1" dir="auto">vec3_t viewoffset</h3>
<p dir="auto">The player's viewoffset is now passed in, to allow for accurate blending. Pmove is now semi-responsible for screen blends.</p>
<h3 tabindex="-1" dir="auto">vec3_t screen_blend</h3>
<p dir="auto">An output variable containing full-screen blends to apply to the view.</p>
<h3 tabindex="-1" dir="auto">refdef_flags_t rdflags</h3>
<p dir="auto">An output variable containing flags that should be merged with the server's representation.</p>
<h3 tabindex="-1" dir="auto">bool jump_sound</h3>
<p dir="auto">An output variable to tell the game to play a jumping sound.</p>
<h3 tabindex="-1" dir="auto">float impact_delta</h3>
<p dir="auto">When new ground is achieved, the impact is stored here for fall damage checks.</p>
<h2 tabindex="-1" dir="auto">edict_shared_t</h2>
<p dir="auto">NOTE: the following members of the old <code>edict_t</code> struct have been removed, and were moved server-side:</p>
<ul dir="auto">
<li><code>link_t area</code></li>
<li><code>int num_clusters</code></li>
<li><code>int clusternums[]</code></li>
<li><code>int headnode</code></li>
</ul>
<h3 tabindex="-1" dir="auto">sv_entity_t sv</h3>
<p dir="auto">Most of the meat of the bot system is contained in the server code, and doesn't have direct access to the games' representation of the state of the game.</p>
<p dir="auto">Bots use this thin interpretation of the game state data about entities to understand how to use entities to its advantage - similar to how the client receives a thin portion of entities to understand how to render them.</p>
<h3 tabindex="-1" dir="auto">bool linked</h3>
<p dir="auto">This boolean indicates whether the entity is currently linked into the world or not. It is the replacement of checking for <code>area.prev</code> being non-null.</p>
<h3 tabindex="-1" dir="auto">svflags_t svflags</h3>
<p dir="auto">For new functionality, some new flags were added to <code>svflags</code>. <strong>This may cause backwards-incompatibility in older mods that have modified this enum!</strong> This enum is server-specific, so it is always incorrect for mods to modify this.</p>
<h4 tabindex="-1" dir="auto">SVF_PLAYER</h4>
<p dir="auto">This flag causes the object to be treated as <code>CONTENTS_PLAYER</code> for collision. All players have this flag.</p>
<h4 tabindex="-1" dir="auto">SVF_BOT</h4>
<p dir="auto">This flag marks the entity as a bot.</p>
<h4 tabindex="-1" dir="auto">SVF_NOBOTS</h4>
<p dir="auto">This flag tells the bot subsystem to ignore this entity.</p>
<h4 tabindex="-1" dir="auto">SVF_RESPAWNING</h4>
<p dir="auto">This flag is a hint to the bot subsystem to inform it about how items respawn.</p>
<h4 tabindex="-1" dir="auto">SVF_PROJECTILE</h4>
<p dir="auto">This flag treats the entity as <code>CONTENTS_PROJECTILE</code> for collision.</p>
<h4 tabindex="-1" dir="auto">SVF_INSTANCED</h4>
<p dir="auto">This flag marks the entity as being instanced - it may be invisible or hidden for certain players.</p>
<h4 tabindex="-1" dir="auto">SVF_DOOR</h4>
<p dir="auto">This flag is for the bot subsystem, and informs it that this entity is a door.</p>
<h4 tabindex="-1" dir="auto">SVF_NOCULL</h4>
<p dir="auto">This flag overrides the client frame building culling routines, causing an entity to always be sent regardless of where it is (ignoring PVS/PHS, essentially). Its only use in our code is to keep no-attenuation looping speakers in frame always.</p>
<h4 tabindex="-1" dir="auto">SVF_HULL</h4>
<p dir="auto">This flag adjusts the servers' method of clipping movement to entities. Normally, only <code>SOLID_BSP</code> entities will use their proper clipping bounds for collision, but if this is set on a <code>SOLID_TRIGGER</code> brush entity, traces will have to collide with the actual BSP tree of the trigger instead of solely touching its bounding box.</p>
<p dir="auto">This is used in our game DLL to allow for certain triggers (like <code>trigger_gravity</code> or <code>trigger_flashlight</code>) to be activated when you are actually touching their brushes, allowing for angled triggers to finally exist.</p>
<h2 tabindex="-1" dir="auto">entity_state_t</h2>
<h3 tabindex="-1" dir="auto">uint32_t number</h3>
<p dir="auto">Number was changed to <code>uint32_t</code> (from <code>int32_t</code>) to better represent its use and to only have to catch out of bounds in one direction.</p>
<h3 tabindex="-1" dir="auto">int32_t skinnum</h3>
<p dir="auto">Skinnum now packs a bit more data into it.</p>
<div dir="auto" data-snippet-clipboard-copy-content="// [Paril-KEX] player s.skinnum's encode additional data
union player_skinnum_t
{
    int32_t         skinnum;
    struct {
        uint8_t     client_num; // client index
        uint8_t     vwep_index; // vwep index
        int8_t      viewheight; // viewheight
        uint8_t     team_index : 4; // team #; note that teams are 1-indexed here, with 0 meaning no team
                                    // (spectators in CTF would be 0, for instance)
        uint8_t     poi_icon : 4;   // poi icon; 0 default friendly, 1 dead, others unused
    };
};"><pre><span><span>//</span> [Paril-KEX] player s.skinnum's encode additional data</span>
<span>union</span> <span>player_skinnum_t</span>
{
    <span>int32_t</span>         skinnum;
    <span>struct</span> {
        <span>uint8_t</span>     client_num; <span><span>//</span> client index</span>
        <span>uint8_t</span>     vwep_index; <span><span>//</span> vwep index</span>
        <span>int8_t</span>      viewheight; <span><span>//</span> viewheight</span>
        <span>uint8_t</span>     team_index : <span>4</span>; <span><span>//</span> team #; note that teams are 1-indexed here, with 0 meaning no team</span>
                                    <span><span>//</span> (spectators in CTF would be 0, for instance)</span>
        <span>uint8_t</span>     poi_icon : <span>4</span>;   <span><span>//</span> poi icon; 0 default friendly, 1 dead, others unused</span>
    };
};</pre></div>
<h3 tabindex="-1" dir="auto">effects_t effects</h3>
<p dir="auto">The type <code>effects_t</code> was changed from <code>uint32_t</code> to <code>uint64_t</code> since we have way more effects to express.</p>
<h4 tabindex="-1" dir="auto">EF_BOB (bit 4)</h4>
<p dir="auto">Bit was unused in Quake II. This was repurposed into a weapon bobbing effect, similar to Quake III.</p>
<h4 tabindex="-1" dir="auto">EF_POWERSCREEN (bit 9)</h4>
<p dir="auto">This effect uses a different model that is scaled to the monster's size now.</p>
<h4 tabindex="-1" dir="auto">EF_DUALFIRE (bit 32)</h4>
<p dir="auto">This bit is used for a special effect, similar to <code>EF_QUAD</code>, but for Dualfire Damage.</p>
<h4 tabindex="-1" dir="auto">EF_HOLOGRAM (bit 33)</h4>
<p dir="auto">This bit is used for the N64 hologram effect; it adds a spinning ball of green particles around the object.</p>
<h4 tabindex="-1" dir="auto">EF_FLASHLIGHT (bit 34)</h4>
<p dir="auto">This bit marks a player entity as having a flashlight enabled. The effect itself is rendered separately by the client.</p>
<h4 tabindex="-1" dir="auto">EF_BARREL_EXPLODING (bit 35)</h4>
<p dir="auto">This effect is used before an explobox explodes; it emits steam particles from the barrel, as if it is experiencing a decompression event.</p>
<h4 tabindex="-1" dir="auto">EF_TELEPORTER2 (bit 36)</h4>
<p dir="auto">This effect is used for the teleporter FX in the N64.</p>
<h4 tabindex="-1" dir="auto">EF_GRENADE_LIGHT (bit 37)</h4>
<p dir="auto">This effect creates a small light on monster grenades, to make them slightly easier to track visually.</p>
<h3 tabindex="-1" dir="auto">EF_FIREBALL (EF_ROCKET | EF_GIB)</h3>
<p dir="auto">This mutually-exclusive bit combo did nothing in the original game, since these special trails could only render one or the other. In the rerelease, it will render a fireball trail that begins yellow and large, tapering off into an orange trail, to mimick the effect on N64.</p>
<h3 tabindex="-1" dir="auto">renderfx_t renderfx</h3>
<h4 tabindex="-1" dir="auto">RF_NO_ORIGIN_LERP (bit 6)</h4>
<p dir="auto">This effect had a confusing name originally. Its name now reflects what it does: it disables origin interpolation.</p>
<h4 tabindex="-1" dir="auto">RF_BEAM (bit 7)</h4>
<p dir="auto">You can now create custom segmented beams by setting a non-one modelindex on beams.</p>
<h4 tabindex="-1" dir="auto">RF_CUSTOMSKIN (bit 8)</h4>
<p dir="auto">This effect was unused originally. It is now implemented and works as intended: specifying a <code>skinnum</code> will change the skin on the model to the skin specified in <code>CS_IMAGES + skinnum</code>. For <code>RF_FLARE</code>, <code>frame</code> must be used instead however, as <code>skinnum</code> is used for color data.</p>
<h4 tabindex="-1" dir="auto">RF_NOSHADOW (bit 13)</h4>
<p dir="auto">This effect was client-only originally.</p>
<h4 tabindex="-1" dir="auto">RF_CASTSHADOW (bit 14)</h4>
<p dir="auto">This effect marks an entity that casts light in the world; it is only used by <code>dynamic_light</code> (or dynamic <code>light</code> entities), and should not be used otherwise.</p>
<h4 tabindex="-1" dir="auto">RF_SHELL_LITE_GREEN (bit 19)</h4>
<p dir="auto">This is the equivalent shell color for <code>EF_DUALFIRE</code>.</p>
<h4 tabindex="-1" dir="auto">RF_CUSTOM_LIGHT (bit 20)</h4>
<p dir="auto">This flag creates a custom dynamic light at the position of the object. Its used in the N64 campaign, as it has custom light entities (<code>target_light</code>). <code>s.frame</code> is the light's radius, and <code>s.skinnum</code> is the light's current color (packed RGB).</p>
<h4 tabindex="-1" dir="auto">RF_FLARE (bit 21)</h4>
<p dir="auto">This flag marks an entity as being rendered with a flare instead of the usual entity rendering. Flares overload some fields:</p>
<ul dir="auto">
<li><code>s.renderfx &amp; RF_SHELL_RED</code> causes the flare to have an outer red rim.</li>
<li><code>s.renderfx &amp; RF_SHELL_GREEN</code> causes the flare to have an outer green rim.</li>
<li><code>s.renderfx &amp; RF_SHELL_BLUE</code> causes the flare to have an outer blue rim.</li>
<li><code>s.renderfx &amp; RF_FLARE_LOCK_ANGLE</code> causes the flare to not rotate towards the viewer.</li>
<li><code>s.renderfx &amp; RF_CUSTOMSKIN</code> causes the flare to use the custom image index in <code>s.frame</code>.</li>
<li><code>s.modelindex2</code> is the start distance of fading the flare out.</li>
<li><code>s.modelindex3</code> is the end distance of fading the flare out.</li>
<li><code>s.skinnum</code> is the RGBA of the flare.</li>
</ul>
<h4 tabindex="-1" dir="auto">RF_OLD_FRAME_LERP (bit 22)</h4>
<p dir="auto">This flag signals that <code>s.old_frame</code> should be used for the next frame and respected by the client. This can be used for custom frame interpolation; its use in this engine is specific to fixing interpolation bugs on certain monster animations.</p>
<h4 tabindex="-1" dir="auto">RF_DOT_SHADOW (bit 23)</h4>
<p dir="auto">Draw a blob shadow underneath the entity.</p>
<h4 tabindex="-1" dir="auto">RF_LOW_PRIORITY (bit 24)</h4>
<p dir="auto">This flag marks an entity as low priority; if the renderer runs out of entity slots, these entities will be eligible for replacement. For instance, a monster is more important than a gib, so gibs are marked low priority so they can be replaced by a monster if the limit is reached.</p>
<h4 tabindex="-1" dir="auto">RF_NO_LOD (bit 25)</h4>
<p dir="auto">The original MD2 models will be used for LOD. Setting this bit prevents this behavior.</p>
<h4 tabindex="-1" dir="auto">RF_NO_STEREO (bit 2)</h4>
<p dir="auto">This is an overloaded flag that only applies to non-rendered entities that contain sounds. If set, stereo panning is disabled on this entity.</p>
<h4 tabindex="-1" dir="auto">RF_STAIR_STEP (bit 26)</h4>
<p dir="auto">The tick rate increase caused a bit of a visual bug with monsters and players: they now stepped up steps within 0.025 seconds instead of 0.1, causing jarring hitching. To fix this, entities set this flag when they detect they have stepped up a stair, and the client will interpolate their height difference over 0.1 seconds.</p>
<h4 tabindex="-1" dir="auto">RF_BEAM_LIGHTNING (RF_BEAM | RF_GLOW)</h4>
<p dir="auto">This mutually-exclusive bit combo causes a laser to become a lightning bolt, for N64 support.</p>
<h3 tabindex="-1" dir="auto">uint32_t solid</h3>
<p dir="auto">This was changed from <code>int32_t</code> to <code>uint32_t</code>, and now packs more data into it to better represent bounding boxes to clients.</p>
<p dir="auto">For backwards compatibility, 31 is still the magic value used for BSP entities. The actual packed data, however, is now as follows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="union solid_packed_t
{
    struct {
        uint8_t x;
        uint8_t y;
        uint8_t zd; // always negative
        uint8_t zu; // encoded as + 32
    } p;

    uint32_t u;
};

// packing:

packed.p.x = ent->maxs[0];
packed.p.y = ent->maxs[1];
packed.p.zd = -ent->mins[2];
packed.p.zu = ent->maxs[2] + 32;

// unpacking:
packed.u = state->solid;
ent->mins[0] = -packed.p.x;  ent->maxs[0] = packed.p.x;
ent->mins[1] = -packed.p.y;  ent->maxs[1] = packed.p.y;
ent->mins[2] = -packed.p.zd; ent->maxs[2] = packed.p.zu - 32;"><pre><span>union</span> <span>solid_packed_t</span>
{
    <span>struct</span> {
        <span>uint8_t</span> x;
        <span>uint8_t</span> y;
        <span>uint8_t</span> zd; <span><span>//</span> always negative</span>
        <span>uint8_t</span> zu; <span><span>//</span> encoded as + 32</span>
    } p;

    <span>uint32_t</span> u;
};

<span><span>//</span> packing:</span>

packed.p.x = ent-&gt;maxs[<span>0</span>];
packed.p.y = ent-&gt;maxs[<span>1</span>];
packed.p.zd = -ent-&gt;mins[<span>2</span>];
packed.p.zu = ent-&gt;maxs[<span>2</span>] + <span>32</span>;

<span><span>//</span> unpacking:</span>
packed.u = state-&gt;solid;
ent-&gt;mins[<span>0</span>] = -packed.p.x;  ent-&gt;maxs[<span>0</span>] = packed.p.x;
ent-&gt;mins[<span>1</span>] = -packed.p.y;  ent-&gt;maxs[<span>1</span>] = packed.p.y;
ent-&gt;mins[<span>2</span>] = -packed.p.zd; ent-&gt;maxs[<span>2</span>] = packed.p.zu - <span>32</span>;</pre></div>
<p dir="auto">This is similar to Quake III Arena, and essentially allows any integral bbox to make it to the clients unchanged.</p>
<h3 tabindex="-1" dir="auto">entity_event_t event</h3>
<p dir="auto">Two new events were added:</p>
<h4 tabindex="-1" dir="auto">EV_OTHER_FOOTSTEP (8)</h4>
<p dir="auto">Allows non-players to send footsteps. They have idle attenuation, whereas regular footsteps have normal attenuation.</p>
<h4 tabindex="-1" dir="auto">EV_LADDER_STEP (9)</h4>
<p dir="auto">Ladder climbing 'footstep' event.</p>
<h3 tabindex="-1" dir="auto">float alpha</h3>
<p dir="auto">This value allows you to specify exact transparency values for entities. For backwards compatibility, setting it to zero should be equivalent to an unchanged value, but any non-zero value should be respected as changed.</p>
<h3 tabindex="-1" dir="auto">float scale</h3>
<p dir="auto">This value allows you to scale an entity by the given amount. For backwards compatibility, setting it to zero should be equivalent to an unchanged value, but any non-zero value should be respected as changed.</p>
<h3 tabindex="-1" dir="auto">uint8_t instance_bits</h3>
<p dir="auto">This value is not meant to be set directly by the game code, but will have non-zero bits set for split-screen players that cannot see this entity.</p>
<h3 tabindex="-1" dir="auto">float loop_volume / loop_attenuation</h3>
<p dir="auto">Looping noises can now have volume and attenuation explicitly specified. For both, a value of zero indicates default/unchanged, for backwards compatibility. For <code>loop_attenuation</code>, a value of <code>-1</code> indicates full level audio (like <code>ATTN_NONE</code>).</p>
<h3 tabindex="-1" dir="auto">int32_t owner</h3>
<p dir="auto">An entity's owner is now networked, allowing for it to ignore collision properly.</p>
<h3 tabindex="-1" dir="auto">int32_t old_frame</h3>
<p dir="auto">Only sent when <code>renderfx &amp; RF_OLD_FRAME_LERP</code> - indicates that this frame is the frame to lerp from.</p>
<h2 tabindex="-1" dir="auto">Import/Exports</h2>
<h2 tabindex="-1" dir="auto">Game Import</h2>
<h3 tabindex="-1" dir="auto">(read-only) tick_rate / frame_time_s / frame_time_ms</h3>
<p dir="auto">This holds the server's tick variables. They will be set at the start of the server, before <a href="#preinit">PreInit</a>.
<code>tick_rate</code> stores the tick rate, in hz.
<code>frame_time_s</code> is the time a game frame will take in seconds.
<code>frame_time_ms</code> is the time a game frame will take in ms.</p>
<p dir="auto">These are provided pre-calculated for convenience.</p>
<h3 tabindex="-1" dir="auto">Broadcast_Print</h3>
<p dir="auto">This function writes <code>message</code> with the print type of <code>printlevel</code> to all players. See <a href="#print-adjustments">Print Adjustments</a>. This is kept for compatibility purposes, <a href="#loc_print">Loc_Print</a> replaces it.</p>
<h3 tabindex="-1" dir="auto">Com_Print</h3>
<p dir="auto">This function writes <code>message</code> to the server. See <a href="#print-adjustments">Print Adjustments</a>.</p>
<h3 tabindex="-1" dir="auto">Client_Print</h3>
<p dir="auto">This function writes out <code>message</code> with the print type of <code>printlevel</code> to the specified <code>ent</code> player. See <a href="#print-adjustments">Print Adjustments</a>. This is kept for compatibility purposes, <a href="#loc_print">Loc_Print</a> replaces it.</p>
<h3 tabindex="-1" dir="auto">Center_Print</h3>
<p dir="auto">This function writes <code>message</code> to the specified <code>ent</code> player in the center of their screen. See <a href="#print-adjustments">Print Adjustments</a>. This is kept for compatibility purposes, <a href="#loc_print">Loc_Print</a> replaces it.</p>
<h3 tabindex="-1" dir="auto">sound / positioned_sound</h3>
<p dir="auto">The <code>channel</code> enum has a single new flag:</p>
<h4 tabindex="-1" dir="auto">CHAN_FORCE_POS (bit 5)</h4>
<p dir="auto">If set (and an origin is <strong>not</strong> supplied), the entity's origin will be forced to be used as the origin point of the sound even if there is a better position available.</p>
<h3 tabindex="-1" dir="auto">local_sound</h3>
<p dir="auto">This function was introduced to deal with some split-screen issues that popped up. It's designed to mimick <code>localsound</code> of QuakeC; it will directly send a sound packet to the specified player, using a <code>dupe_key</code> if supplied (see <a href="#unicast">unicast</a>).</p>
<p dir="auto">See <a href="#sound--positioned_sound">sound</a> for info about the channels.</p>
<h3 tabindex="-1" dir="auto">get_configstring</h3>
<p dir="auto">This function fetches a configstring from the servers' current configstring data.</p>
<h3 tabindex="-1" dir="auto">Com_Error</h3>
<p dir="auto">See <a href="#print-adjustments">Print Adjustments</a>.</p>
<h3 tabindex="-1" dir="auto">clip</h3>
<p dir="auto">This is a new function designed to fit a specific purpose: it will test if the box specified by <code>mins</code> &amp; <code>maxs</code>, moved from <code>start</code> to <code>end</code>, will clip against the specified <code>entity</code> with the given <code>contentmask</code>. As an example, you could use this to detect if an entity is actually intersecting a brush in a trigger instead of just being within its bounding box.</p>
<h3 tabindex="-1" dir="auto">inPVS / inPHS</h3>
<p dir="auto">This function now accepts a boolean, <code>portals</code>, which changes whether or not it should ignore areaportals.</p>
<h3 tabindex="-1" dir="auto">BoxEdicts</h3>
<p dir="auto">This function was modified with a simple filtering callback, greatly extending its purpose and removing some limitations that would occur with previous uses. The filter callback is called for every entity discovered, and you can choose to include or skip entities that it finds, or even completely abort the search. In addition, you can now call the function with a 0 <code>maxcount</code>, and the function will still continue to filter and find entities, reporting the final count. To match the old behavior, if a non-zero <code>maxcount</code> is supplied, the return count will cap out at <code>maxcount</code>.</p>
<p dir="auto">Note that it is disallowed to modify world links (linkentity/unlinkentity, etc) in a filter callback, it can only be used for filtering.</p>
<h3 tabindex="-1" dir="auto">unicast</h3>
<p dir="auto">The <code>dupe_key</code> parameter is new, and is to solve a very peculiar issue with split screen players. When unicast is used to spawn effects or sounds, it may not be desirable to replay the same effect on multiple split screens, since split screen players are all the same client and share views. For example, if you do a unicast for a <code>TE_BLASTER</code> somewhere in the world for every player, for a split screen client with 4 players, that effect will play 4 times - even though all four players are viewing the same world. The game DLL also has no knowledge or understanding of split screen, so there's no way for the game to work around it.</p>
<p dir="auto">Instead of having the game need to know this kind of implementation detail and prevent double-sending, for effects that are going to potentially be sent to multiple players that <em>may</em> be on a split screen, you can specify a dupe key value. This value, when non-zero, will be marked as "already sent" for that client, and won't be sent again for the next packet if it was already tripped. The game DLL provides the <code>GetUnicastKey</code> global which will give you a rolling value to directly pass into unicast or local_sound.</p>
<h3 tabindex="-1" dir="auto">WriteFloat</h3>
<p dir="auto">Implemented; this was stubbed out of the old code.</p>
<h3 tabindex="-1" dir="auto">WritePosition</h3>
<p dir="auto">Now sends full float positions.</p>
<h3 tabindex="-1" dir="auto">WriteDir / WriteAngle</h3>
<p dir="auto">Unchanged - WriteAngle is for compressed angles, when high precision is not necessary.</p>
<h3 tabindex="-1" dir="auto">WriteEntity</h3>
<p dir="auto">New function to write an entity, to make it easier to write them without needing to WriteShort directly.</p>
<h3 tabindex="-1" dir="auto">GetExtension</h3>
<p dir="auto">See <a href="#extensions">Extensions</a>.</p>
<h3 tabindex="-1" dir="auto">Bot_RegisterEdict</h3>
<p dir="auto">Informs the bot subsystem that an entity needs to be registered.</p>
<h3 tabindex="-1" dir="auto">Bot_UnRegisterEdict</h3>
<p dir="auto">Informs the bot subsystem that an entity needs to be unregistered.</p>
<h3 tabindex="-1" dir="auto">Bot_MoveToPoint</h3>
<p dir="auto">Forces a bot to move to the specified point.</p>
<h3 tabindex="-1" dir="auto">Bot_FollowActor</h3>
<p dir="auto">Forces a bot to follow the specified actor.</p>
<h3 tabindex="-1" dir="auto">GetPathToGoal</h3>
<p dir="auto">The main pathfinding function; with the given pathfinding <code>request</code>, you'll be given <code>info</code> about the operation, the path, etc.</p>
<h3 tabindex="-1" dir="auto">Loc_Print</h3>
<p dir="auto">The new primary entry point for printing. This function replaces all of the others (except Com_Print).
For basic usage, it can be called on an entity (or nullptr for broadcasting) with the correct <code>level</code>, with the message to send in <code>base</code>, and nullptr <code>args</code> along with 0 <code>num_args</code>. For actual localized messages, however, you can send additional arguments via the <code>args</code>/<code>num_args</code> parameters which are sent to the client for further processing.</p>
<p dir="auto">In addition to localization, <code>level</code> now has new values and bit flags.</p>
<h4 tabindex="-1" dir="auto">PRINT_TYPEWRITER (4)</h4>
<p dir="auto">Causes the message to be printed out one at a time, like a typewriter. Used for objectives, similar to the N64 version.</p>
<h4 tabindex="-1" dir="auto">PRINT_CENTER (5)</h4>
<p dir="auto">An instant centerprint, like the legacy centerprints.</p>
<h4 tabindex="-1" dir="auto">PRINT_TTS (6)</h4>
<p dir="auto">Identical to <code>PRINT_HIGH</code> in importance, but additionally causes text to speech narration to activate if enabled on the client.</p>
<h4 tabindex="-1" dir="auto">PRINT_BROADCAST (bit 3)</h4>
<p dir="auto">Message will be sent to all players.</p>
<h4 tabindex="-1" dir="auto">PRINT_NO_NOTIFY (bit 4)</h4>
<p dir="auto">Message will not be sent to the notify system.</p>
<h3 tabindex="-1" dir="auto">Draw_Line / Draw_Point / Draw_Circle / Draw_Bounds / Draw_Sphere / Draw_OrientedWorldText / Draw_StaticWorldText / Draw_Cylinder / Draw_Ray</h3>
<p dir="auto">These functions are debugging aids that only render on the server.</p>
<h3 tabindex="-1" dir="auto">ReportMatchDetails_Multicast</h3>
<p dir="auto">This function is solely for platforms that need match result data.</p>
<h3 tabindex="-1" dir="auto">ServerFrame</h3>
<p dir="auto">Returns the server's frame number.</p>
<h3 tabindex="-1" dir="auto">SendToClipBoard</h3>
<p dir="auto">Copy data to the server's clipboard, useful for debugging.</p>
<h3 tabindex="-1" dir="auto">Info_ValueForKey / Info_RemoveKey / Info_SetValueForKey</h3>
<p dir="auto">See <a href="#info-keys">Info Keys</a>.</p>
<h2 tabindex="-1" dir="auto">Save Games</h2>
<p dir="auto">One of the major changes to this release of Quake II is the save system. Instead of storing pointer offsets and copies of memory, the level &amp; game data is written to UTF-8 JSON. This makes save data much easier to navigate for a human &amp; developer that wants to look into a bug, while also being quick and efficient for storage.</p>
<p dir="auto">The save system, as a result, no longer interfaces with the filesystem at all. Other mods are not required to use JSON, any text format will work as the server and client do not interact with the data.</p>
<h2 tabindex="-1" dir="auto">Game Export</h2>
<h3 tabindex="-1" dir="auto">(read-only) apiversion</h3>
<p dir="auto">The version # reported by the server.</p>
<h3 tabindex="-1" dir="auto">PreInit</h3>
<p dir="auto">This function is called before InitGame, and should be where you initialize your mod's latched cvars. This can be used to fix any conflicting latched cvars, which will be "locked in" after this is called.</p>
<h3 tabindex="-1" dir="auto">SpawnEntities</h3>
<p dir="auto">All three parameters are now properly marked const.</p>
<h3 tabindex="-1" dir="auto">WriteGameJson</h3>
<p dir="auto">See <a href="#save-games">Save Games</a>.</p>
<h3 tabindex="-1" dir="auto">ReadGameJson</h3>
<p dir="auto">See <a href="#save-games">Save Games</a>.</p>
<h3 tabindex="-1" dir="auto">WriteLevelJson</h3>
<p dir="auto">This function is now informed whether the level write is from a level transition or a manual save.
See <a href="#save-games">Save Games</a>.</p>
<h3 tabindex="-1" dir="auto">ReadLevelJson</h3>
<p dir="auto">See <a href="#save-games">Save Games</a>.</p>
<h3 tabindex="-1" dir="auto">CanSave</h3>
<p dir="auto">This new export now dictates whether the game is saveable or not.</p>
<h3 tabindex="-1" dir="auto">ClientChooseSlot</h3>
<p dir="auto">ClientChooseSlot is intended to take in a bunch of information about the client that is connecting, and choose which <code>edict_t</code> entity this player should occupy. It is used in the rerelease to reorder players consistently throughout coop games, and ensure that everybody always gets the correct slot.</p>
<p dir="auto">Callers are given the player's <code>userinfo</code> and <code>social_id</code> (the social ID is a unique value per player on certain platforms), which you can use to find the correct slot from the current saved client data. You're also told whether the client <code>isBot</code>, which should always use non-saved available slots first. The <code>ignore</code> field will give you a list of slots up to <code>num_ignore</code> entities that are already occupied or were reported as such, so they can be safely skipped over. Finally, the <code>cinematic</code> parameters will tell you whether the loaded map is a video, which in most cases reordering will not be necessary.</p>
<h3 tabindex="-1" dir="auto">ClientConnect</h3>
<p dir="auto">The function is now given the <code>social_id</code> and <code>isBot</code> state of the connecting client.</p>
<h3 tabindex="-1" dir="auto">RunFrame</h3>
<p dir="auto">This function now receives a boolean to tell whether the call is from the main game loop, or from some other source (the game is settling, or running frames to advance level transitions). If the latter is occurring, you can use this boolean to speed up level transitions by skipping logic that is not necessary but is CPU-intensive, such as enemies searching for players to attack.</p>
<h3 tabindex="-1" dir="auto">PrepFrame</h3>
<p dir="auto">This function used to be in the server, but is now controlled by the game DLL. It's ran after the game has execute a frame &amp; has sent the packet data over to all players. Things like hit markers and one-shot events are cleared in here.</p>
<h3 tabindex="-1" dir="auto">edict_size / num_edicts / max_edicts</h3>
<p dir="auto">These were changed to size_t and uint32_t/uint32_t respectively, to better represent their use.</p>
<h3 tabindex="-1" dir="auto">server_flags</h3>
<p dir="auto">This is an integer shared between server and game, which stores bits for special states that the server cares about.</p>
<h3 tabindex="-1" dir="auto">Pmove</h3>
<p dir="auto">See <a href="#pmove">Pmove</a>.</p>
<h3 tabindex="-1" dir="auto">GetExtension</h3>
<p dir="auto">See <a href="#extensions">Extensions</a>.</p>
<h3 tabindex="-1" dir="auto">Bot_SetWeapon</h3>
<p dir="auto">Called by the bot subsystem to switch weapons.</p>
<h3 tabindex="-1" dir="auto">Bot_TriggerEdict</h3>
<p dir="auto">Called by the bot subsystem to trigger an entity.</p>
<h3 tabindex="-1" dir="auto">Bot_UseItem</h3>
<p dir="auto">Called by the bot subsystem to use an item.</p>
<h3 tabindex="-1" dir="auto">Bot_GetItemID</h3>
<p dir="auto">Fetch an item ID by a classname; for the bot subsystem.</p>
<h3 tabindex="-1" dir="auto">Entity_IsVisibleToPlayer</h3>
<p dir="auto">This function is for item instancing; the rerelease of Quake II supports instanced items, which will display only for the players who haven't picked it up yet. For online players, this simply removes the item if you've gotten it, but for split screen players it will show a ghost where the entity was on players that have already picked it up.</p>
<h3 tabindex="-1" dir="auto">GetShadowLightData</h3>
<p dir="auto">This function fetches data for the given shadow light for building client frames.</p>
<h2 tabindex="-1" dir="auto">Player State</h2>
<p dir="auto">In the original client, player state was often accessed directly to perform various tasks or render things. Much of this has been moved into the cgame module to allow increased customization.</p>
<h2 tabindex="-1" dir="auto">Client Game Import</h2>
<h3 tabindex="-1" dir="auto">(read-only) tick_rate</h3>
<h3 tabindex="-1" dir="auto">(read-only) frame_time_s</h3>
<h3 tabindex="-1" dir="auto">(read-only) frame_time_ms</h3>
<p dir="auto">This holds the server's tick variables. They will be set at the start of the client, before Init.
<code>tick_rate</code> stores the tick rate, in hz.
<code>frame_time_s</code> is the time a game frame will take in seconds.
<code>frame_time_ms</code> is the time a game frame will take in ms.</p>
<p dir="auto">These are provided pre-calculated for convenience.</p>
<h3 tabindex="-1" dir="auto">Com_Print</h3>
<p dir="auto">Print a debug message to the client.</p>
<h3 tabindex="-1" dir="auto">get_configstring</h3>
<p dir="auto">Fetch the given configstring data from the client.</p>
<h3 tabindex="-1" dir="auto">Com_Error</h3>
<p dir="auto">Abort error for client.</p>
<h3 tabindex="-1" dir="auto">TagMalloc / TagFree / FreeTags</h3>
<p dir="auto">Same as server.</p>
<h3 tabindex="-1" dir="auto">cvar / cvar_set / cvar_forceset</h3>
<p dir="auto">Same as server.</p>
<h3 tabindex="-1" dir="auto">AddCommandString</h3>
<p dir="auto">Push command(s) into the command buffer on the client side.</p>
<h3 tabindex="-1" dir="auto">GetExtension</h3>
<p dir="auto">See <a href="#extensions">Extensions</a>.</p>
<h3 tabindex="-1" dir="auto">CL_FrameValid</h3>
<p dir="auto">Returns true if the current frame being rendered is valid.</p>
<h3 tabindex="-1" dir="auto">CL_FrameTime</h3>
<p dir="auto">Returns the current frame time delta.</p>
<h3 tabindex="-1" dir="auto">CL_ClientTime</h3>
<p dir="auto">Returns the client's current time (server-bound).</p>
<h3 tabindex="-1" dir="auto">CL_ClientRealTime</h3>
<p dir="auto">Returns the client's current real, unbound time.</p>
<h3 tabindex="-1" dir="auto">CL_ServerFrame</h3>
<p dir="auto">Returns the client's server frame.</p>
<h3 tabindex="-1" dir="auto">CL_ServerProtocol</h3>
<p dir="auto">Returns the client's connected server protocol.</p>
<h3 tabindex="-1" dir="auto">CL_GetClientName</h3>
<p dir="auto">Returns a UTF-8 string containing the givern player's name.</p>
<h3 tabindex="-1" dir="auto">CL_GetClientPic</h3>
<p dir="auto">Returns a string containing the given player's icon.</p>
<h3 tabindex="-1" dir="auto">CL_GetClientDogtag</h3>
<p dir="auto">Returns a string containing the given player's dogtag.</p>
<h3 tabindex="-1" dir="auto">CL_GetKeyBinding</h3>
<p dir="auto">Returns a key binding for the given key. Returns an empty string if the key is unbound.</p>
<h3 tabindex="-1" dir="auto">Draw_RegisterPic</h3>
<p dir="auto">Precache the given image.</p>
<h3 tabindex="-1" dir="auto">Draw_GetPicSize</h3>
<p dir="auto">Returns the size of the given image.</p>
<h3 tabindex="-1" dir="auto">SCR_DrawChar</h3>
<p dir="auto">Draw the given conchars char at the specified position. A <code>shadow</code> parameter has been added to draw a drop shadow.</p>
<h3 tabindex="-1" dir="auto">SCR_DrawPic</h3>
<p dir="auto">Draw the given pic at the specified position.</p>
<h3 tabindex="-1" dir="auto">SCR_DrawColorPic</h3>
<p dir="auto">Draw the given pic at the specified location, with the specified color.</p>
<h3 tabindex="-1" dir="auto">SCR_SetAltTypeface</h3>
<p dir="auto">Change whether the alternate (accessibility) typeface is in use or not.</p>
<h3 tabindex="-1" dir="auto">SCR_DrawFontString</h3>
<p dir="auto">Draw a string to the screen, using the Kex KFONT which includes non-latin characters.</p>
<h3 tabindex="-1" dir="auto">SCR_MeasureFontString</h3>
<p dir="auto">Measure the size of the string as it would be rendered.</p>
<h3 tabindex="-1" dir="auto">SCR_FontLineHeight</h3>
<p dir="auto">Returns the line height of the font.</p>
<h3 tabindex="-1" dir="auto">CL_GetTextInput</h3>
<p dir="auto">Returns a pointer to the current text input, and whether this input is for team say or not.</p>
<h3 tabindex="-1" dir="auto">CL_GetWarnAmmoCount</h3>
<p dir="auto">For the given weapon ID, get the amount that is considered to be low ammo.</p>
<h3 tabindex="-1" dir="auto">Localize</h3>
<p dir="auto">Localize the given string and arguments to an output buffer.</p>
<h3 tabindex="-1" dir="auto">SCR_DrawBind</h3>
<p dir="auto">Draw a user bind to the screen, returns the Y offset from rendering.</p>
<h3 tabindex="-1" dir="auto">CL_InAutoDemoLoop</h3>
<p dir="auto">Returns true if the engine is running the attract demo loop.</p>
<h2 tabindex="-1" dir="auto">Client Game Export</h2>
<h3 tabindex="-1" dir="auto">(read-only) apiversion</h3>
<p dir="auto">API version.</p>
<h3 tabindex="-1" dir="auto">Init / Shutdown</h3>
<p dir="auto">Lifecycle functions for the client game. Note that the cgame does not control UI, so the cgame only exists when you are connected and in-game.</p>
<h3 tabindex="-1" dir="auto">DrawHUD</h3>
<p dir="auto">This function is called by the client when their HUD needs to be rendered.</p>
<ul dir="auto">
<li><code>isplit</code> contains the split screen index of the player.</li>
<li><code>data</code> contains a pointer to some transient information from the server. This includes currently active layout, and the player's active inventory when the inventory is open.</li>
<li><code>hud_vrect</code> contains the unpadded rectangle of the HUD being rendered.</li>
<li><code>hud_safe</code> contains the size of the safe area. Only x and y are set, w and h are unused.</li>
<li><code>scale</code> is the integral scale of the HUD being rendered.</li>
<li><code>playernum</code> is the player's client index.</li>
<li><code>ps</code> is a pointer to the player's current player state.</li>
</ul>
<h3 tabindex="-1" dir="auto">TouchPics</h3>
<p dir="auto">Function called for precaching images used by the HUD.</p>
<h3 tabindex="-1" dir="auto">LayoutFlags</h3>
<p dir="auto">For the given player state, return the <code>layout_flags_t</code> that would match it.</p>
<h3 tabindex="-1" dir="auto">GetActiveWeaponWheelWeapon / GetOwnedWeaponWheelWeapons / GetWeaponWheelAmmoCount / GetPowerupWheelCount</h3>
<p dir="auto">The weapon wheel is in the client, but uses these callbacks to fetch data from <code>player_state_t</code>.</p>
<h3 tabindex="-1" dir="auto">GetHitMarkerDamage</h3>
<p dir="auto">Returns how much damage was done for this player.</p>
<h3 tabindex="-1" dir="auto">Pmove</h3>
<p dir="auto">See <a href="#pmove">Pmove</a>.</p>
<h3 tabindex="-1" dir="auto">ParseConfigString</h3>
<p dir="auto">When a configstring is received, the cgame is also notified of changes. The cgame module can react to configstring updates here.</p>
<h3 tabindex="-1" dir="auto">ParseCenterPrint</h3>
<p dir="auto">When a centerprint-like message is received by the client, it is sent to the cgame via this function.</p>
<ul dir="auto">
<li><code>isplit</code> is the split screen player it was sent to.</li>
<li><code>instant</code> is true if the message is a centerprint that is drawn without the typewriter effect.</li>
</ul>
<h3 tabindex="-1" dir="auto">ClearNotify</h3>
<p dir="auto">The client will call this when the notification area should be cleared.</p>
<h3 tabindex="-1" dir="auto">ClearCenterprint</h3>
<p dir="auto">The client will call this when centerprints should be cleared.</p>
<h3 tabindex="-1" dir="auto">NotifyMessage</h3>
<p dir="auto">When a notify message is received, the client will send it to this function.</p>
<ul dir="auto">
<li><code>isplit</code> is the split screen player it was sent to.</li>
<li><code>is_chat</code> is true if it was a chat-like message.</li>
</ul>
<h3 tabindex="-1" dir="auto">GetMonsterFlashOffset</h3>
<p dir="auto">To simplify the server to client muzzleflash communication, the cgame now exports muzzleflash origins via this function.</p>
<h3 tabindex="-1" dir="auto">GetExtension</h3>
<p dir="auto">See <a href="#extensions">Extensions</a>.</p>
<h2 tabindex="-1" dir="auto">Quake II server protocol - version 2023</h2>
<p dir="auto">The Quake II rerelease features an updated server protocol. Most of the messages are backwards compatible, but some needed adjustments to work with new or changed features, or raised limits.</p>
<p dir="auto">This document will only outline the changes since the original release, rather than the whole protocol.</p>
<h2 tabindex="-1" dir="auto">(out of band, client &lt;-&gt; server) challenges</h2>
<p dir="auto">The out of band challenges have been removed.</p>
<h2 tabindex="-1" dir="auto">(out of band, client -&gt; server) connect</h2>
<p dir="auto">The <code>connect</code> message is similar to the original, but has redundant information removed. Port and challenge are handled at a lower level, so that information is not included. The <code>connect</code> message is in the following format:</p>
<p dir="auto"><code>connect {protocol} {num split} {socials...} {userinfo...}</code></p>
<ul dir="auto">
<li><code>protocol</code> is 2023</li>
<li><code>num split</code> is the number of split screen players</li>
<li><code>socials</code> is <code>num split</code> number of arguments containing each players' social identifiers</li>
<li><code>userinfo</code> is the clients' userinfo string, split up by groups of 510 characters each (since command arguments have a maximum length). This can often span 2 or more arguments, since each userinfo var has a value per player. See <a href="#info-keys">Info Keys</a>.</li>
</ul>
<h2 tabindex="-1" dir="auto">(out of band, server -&gt; client) client_connect</h2>
<p dir="auto">This message is sent when the server accepts the connection. It is in the following format:</p>
<p dir="auto"><code>client_connect {protocol}</code></p>
<ul dir="auto">
<li><code>protocol</code> is 2023</li>
</ul>
<p dir="auto">The protocol version is sent mainly for backward compatibility with demos.</p>
<h2 tabindex="-1" dir="auto">(packet, server -&gt; client) svc_muzzleflash (1)</h2>
<p dir="auto">The following enum values are now accepted.</p>
<h3 tabindex="-1" dir="auto">MZ_BFG2 (19)</h3>
<p dir="auto">Secondary muzzleflash for the BFG, sent when the BFG actually fires.</p>
<h3 tabindex="-1" dir="auto">MZ_PHALANX2 (20)</h3>
<p dir="auto">Secondary muzzleflash for the Phalanx, sent for the second projectile.</p>
<h3 tabindex="-1" dir="auto">MZ_PROX (31)</h3>
<p dir="auto">Sent when the Prox Launcher is fired.</p>
<h3 tabindex="-1" dir="auto">MZ_ETF_RIFLE_2 (32)</h3>
<p dir="auto">Sent when the other barrel of the ETF Rifle is fired.</p>
<h2 tabindex="-1" dir="auto">(packet, server -&gt; client) svc_muzzleflash2 (2)</h2>
<h3 tabindex="-1" dir="auto">MZ2_BOSS2_MACHINEGUN_L2 / MZ2_BOSS2_MACHINEGUN_R2 (74 / 134)</h3>
<p dir="auto">These two values were just copies of L1/R1, but were repurposed to make Hyperblaster-specific sounds for the new Hornet.</p>
<p dir="auto">The following enum values are now accepted.</p>
<h3 tabindex="-1" dir="auto">MZ2_SOLDIER_RIPPER_1 - MZ2_SOLDIER_HYPERGUN_8 (211 - 226)</h3>
<p dir="auto">Muzzleflashes for the ripper &amp; blue hyperblaster guards.</p>
<h3 tabindex="-1" dir="auto">MZ2_GUARDIAN_BLASTER - MZ2_ARACHNID_RAIL_UP2 (227 - 231)</h3>
<p dir="auto">Muzzleflashes for the PSX monsters.</p>
<h3 tabindex="-1" dir="auto">MZ2_INFANTRY_MACHINEGUN_14 - MZ2_INFANTRY_MACHINEGUN_21 (232 - 239)</h3>
<p dir="auto">Muzzleflashes for the Infantry's run-attack animation.</p>
<h3 tabindex="-1" dir="auto">MZ2_GUNCMDR_CHAINGUN_1 - MZ2_GUNCMDR_GRENADE_CROUCH_3 (240 - 250)</h3>
<p dir="auto">Muzzleflashes for the Gunner Commander.</p>
<h3 tabindex="-1" dir="auto">MZ2_SOLDIER_BLASTER_9 - MZ2_SOLDIER_HYPERGUN_9 (251 - 255)</h3>
<p dir="auto">Muzzleflashes for the guards' new prone-firing animation.</p>
<h2 tabindex="-1" dir="auto">(packet, server -&gt; client) svc_muzzleflash3 (32)</h2>
<p dir="auto">This packet was necessitated from running out of bits in svc_muzzleflash2. The only difference is the byte for <code>id</code> is a ushort.</p>
<h3 tabindex="-1" dir="auto">MZ2_GUNNER_GRENADE2_1 - MZ2_GUNNER_GRENADE2_4 (256 - 259)</h3>
<p dir="auto">Alternate firing animation for the Gunner's grenade launcher.</p>
<h3 tabindex="-1" dir="auto">MZ2_INFANTRY_MACHINEGUN_22 (260)</h3>
<p dir="auto">Alternate firing animation for the Infantry.</p>
<h3 tabindex="-1" dir="auto">MZ2_SUPERTANK_GRENADE_1 (261 - 262)</h3>
<p dir="auto">Supertank's grenade launcher.</p>
<h3 tabindex="-1" dir="auto">MZ2_HOVER_BLASTER_2 / MZ2_DAEDALUS_BLASTER_2 (263 / 264)</h3>
<p dir="auto">The Icarus and Daedalus' opposite side blaster.</p>
<h3 tabindex="-1" dir="auto">MZ2_MEDIC_HYPERBLASTER1_1 - MZ2_MEDIC_HYPERBLASTER1_12 / MZ2_MEDIC_HYPERBLASTER2_1 - MZ2_MEDIC_HYPERBLASTER2_12 (265 - 276 / 277 - 288)</h3>
<p dir="auto">The Medic and Medic Commander's Hyperblaster firing animation sweep.</p>
<h2 tabindex="-1" dir="auto">(packet, server -&gt; client) svc_temp_entity (3)</h2>
<p dir="auto">As documented in <a href="#writeposition">WritePosition</a>, WritePos now writes full float precision, so ReadPos has to read full float.</p>
<h3 tabindex="-1" dir="auto">TE_SPLASH (10)</h3>
<p dir="auto">The "color/splash" enumeration accepts a new value:</p>
<h4 tabindex="-1" dir="auto">SPLASH_ELECTRIC (7)</h4>
<p dir="auto">A spark used exclusively in N64, which spawns blue/white particles and makes sparking noises.</p>
<p dir="auto">The following new enum values are accepted:</p>
<h3 tabindex="-1" dir="auto">TE_RAILTRAIL2 (31)</h3>
<p dir="auto">This effect was unused in Quake II, and was retooled to a lighter railgun effect used for Instagib mode.</p>
<h3 tabindex="-1" dir="auto">TE_BLUEHYPERBLASTER (56)</h3>
<p dir="auto">"Correct" version of the old buggy <code>TE_BLUEHYPERBLASTER</code>, which is now <code>TE_BLUEHYPERBLASTER_DUMMY</code>.</p>
<ul dir="auto">
<li>ReadPos</li>
<li>ReadDir</li>
</ul>
<h3 tabindex="-1" dir="auto">TE_BFG_ZAP (57)</h3>
<p dir="auto">Laser when an entity has been zapped by a BFG explosion.</p>
<ul dir="auto">
<li>ReadPos (start)</li>
<li>ReadPos (end)</li>
</ul>
<h3 tabindex="-1" dir="auto">TE_BERSERK_SLAM (58)</h3>
<p dir="auto">Large blue flash &amp; particles at impact point towards a direction.</p>
<ul dir="auto">
<li>ReadPos</li>
<li>ReadDir</li>
</ul>
<h3 tabindex="-1" dir="auto">TE_GRAPPLE_CABLE_2 (59)</h3>
<p dir="auto">The grappling hook in Quake II 3.20 used a larger message that didn't allow the cable to render like other player-derived beams.</p>
<ul dir="auto">
<li>ReadEntity</li>
<li>ReadPos (start)</li>
<li>ReadPos (end)</li>
</ul>
<h3 tabindex="-1" dir="auto">TE_POWER_SPLASH (60)</h3>
<p dir="auto">Effect sent when a power shield evaporates.</p>
<ul dir="auto">
<li>ReadEntity</li>
<li>ReadByte (1 for screen, 0 for armor)</li>
</ul>
<h3 tabindex="-1" dir="auto">TE_LIGHTNING_BEAM (61)</h3>
<p dir="auto">A lightning bolt that originates from the player, like the heat beam. Unused.</p>
<ul dir="auto">
<li>ReadEntity</li>
<li>ReadPos (start)</li>
<li>ReadPos (end)</li>
</ul>
<h3 tabindex="-1" dir="auto">TE_EXPLOSION1_NL / TE_EXPLOSION2_NL (62 / 63)</h3>
<p dir="auto">Variants of explosion that don't include any dynamic light.</p>
<ul dir="auto">
<li>ReadPos</li>
</ul>
<h2 tabindex="-1" dir="auto">(packet, server -&gt; client) svc_sound (9)</h2>
<p dir="auto">Since <code>MAX_EDICTS</code> is now 8192, this packet required changes to support higher entity numbers. <code>MAX_SOUNDS</code> being increased to 1024 also necessitated the sound index changing from byte to ushort.</p>
<ul dir="auto">
<li>ReadByte (flags)</li>
<li>ReadShort (soundindex)</li>
<li>[if flags &amp; SND_VOLUME] ReadByte (volume)</li>
<li>[if flags &amp; SND_ATTENUATION] ReadByte (attenuation)</li>
<li>[if flags &amp; SND_OFFSET] ReadByte (offset)</li>
<li>[if flags &amp; SND_ENT]
<ul dir="auto">
<li>[if flags &amp; SND_LARGE_ENT] ReadLong (entchan)</li>
<li>[if !(flags &amp; SND_LARGE_ENT)] ReadShort (entchan)</li>
</ul>
</li>
<li>[if flags &amp; SND_POS] ReadPos (origin)</li>
</ul>
<p dir="auto"><code>entchan</code> is encoded as such:</p>
<div data-snippet-clipboard-copy-content="struct sndchan_t
{
	uint8_t		channel : 3;
	uint32_t	entity : 29;
}"><pre><code>struct sndchan_t
{
	uint8_t		channel : 3;
	uint32_t	entity : 29;
}
</code></pre></div>
<p dir="auto"><code>flags</code> contains the following bits:</p>
<ul dir="auto">
<li>SND_VOLUME (bit 0)</li>
<li>SND_ATTENUATION (bit 1)</li>
<li>SND_POS (bit 2)</li>
<li>SND_ENT (bit 3)</li>
<li>SND_OFFSET (bit 4)</li>
<li>SND_EXPLICIT_POS (bit 5)</li>
<li>SND_LARGE_ENT (bit 6)</li>
</ul>
<p dir="auto">Note that <code>SND_POS</code> is <strong>always</strong> set. This is to fix a legacy bug where sounds played on entities outside of your PVS will play at the origin instead of their real location. The client should pick the real position if the entity is in their frame, but otherwise fall back to the sound packets' position.</p>
<h2 tabindex="-1" dir="auto">(packet, server -&gt; client) svc_print (10)</h2>
<p dir="auto">This packet now supports <code>PRINT_TYPEWRITER</code> and <code>PRINT_CENTER</code> values. See <a href="#loc_print">Loc_Print</a>.</p>
<h2 tabindex="-1" dir="auto">(packet, server -&gt; client) svc_stufftext (11)</h2>
<p dir="auto">For security reasons, this packet will only allow commands things to be executed.</p>
<h2 tabindex="-1" dir="auto">(packet, server -&gt; client) svc_serverdata (12)</h2>
<ul dir="auto">
<li>ReadLong (protocol)</li>
<li>ReadLong (spawncount)</li>
<li>ReadByte (0 = game, 1 = demo, 2 = server demo)</li>
<li>ReadByte (tickrate)</li>
<li>ReadString (gamedir)</li>
<li>ReadShort[N] (playernums; see below)</li>
<li>ReadString (level name)</li>
</ul>
<p dir="auto">To parse <code>playernums</code>, read the first short and check its value. If it is -2, then read an additional short, which is the number of split screen entities to follow. Read that number of shorts to get each entity number for each split screen player. Otherwise, the value returned by the initial ReadShort is the playernum of the client.</p>
<p dir="auto">The special value -1 will be used in cinematics, to indicate that the player has no entity.</p>
<h2 tabindex="-1" dir="auto">(packet, server -&gt; client) svc_frame (20)</h2>
<ul dir="auto">
<li>ReadLong (serverframe)</li>
<li>ReadLong (deltaframe)</li>
<li>ReadByte (surpressCount)</li>
</ul>
<p dir="auto">For each player in this client's <code>numSplit</code> the following data is parsed:</p>
<ul dir="auto">
<li>ReadByte (areabits length)</li>
<li>ReadData (using above byte)</li>
<li>ReadByte (value will be <code>svc_playerinfo</code>)</li>
<li>ParsePlayerState (see <a href="#svc_playerinfo-17">svc_playerinfo</a>)</li>
</ul>
<p dir="auto">Then, back to <code>svc_frame</code> data:</p>
<ul dir="auto">
<li>client entity <code>event</code>s should all be cleared back to <code>EV_NONE</code></li>
<li>ReadByte (value will be <code>svc_packetentities</code>)</li>
<li>ParsePacketEntities (see <a href="#svc_packetentities-18">svc_packetentities</a>)</li>
</ul>
<h3 tabindex="-1" dir="auto">svc_playerinfo (17)</h3>
<h4 tabindex="-1" dir="auto">Bits</h4>
<div dir="auto" data-snippet-clipboard-copy-content="#define PS_M_TYPE           (1<<0)
#define PS_M_ORIGIN         (1<<1)
#define PS_M_VELOCITY       (1<<2)
#define PS_M_TIME           (1<<3)
#define PS_M_FLAGS          (1<<4)
#define PS_M_GRAVITY        (1<<5)
#define PS_M_DELTA_ANGLES   (1<<6)

#define PS_VIEWOFFSET       (1<<7)
#define PS_VIEWANGLES       (1<<8)
#define PS_KICKANGLES       (1<<9)
#define PS_BLEND            (1<<10)
#define PS_FOV              (1<<11)
#define PS_WEAPONINDEX      (1<<12)
#define PS_WEAPONFRAME      (1<<13)
#define PS_RDFLAGS          (1<<14)

#define PS_MOREBITS         (1<<15)

// [Paril-KEX]
#define PS_DAMAGE_BLEND     (1<<16)
#define PS_TEAM_ID          (1<<17)"><pre><span>#define</span> <span>PS_M_TYPE</span>           (1&lt;&lt;0)
<span>#define</span> <span>PS_M_ORIGIN</span>         (1&lt;&lt;1)
<span>#define</span> <span>PS_M_VELOCITY</span>       (1&lt;&lt;2)
<span>#define</span> <span>PS_M_TIME</span>           (1&lt;&lt;3)
<span>#define</span> <span>PS_M_FLAGS</span>          (1&lt;&lt;4)
<span>#define</span> <span>PS_M_GRAVITY</span>        (1&lt;&lt;5)
<span>#define</span> <span>PS_M_DELTA_ANGLES</span>   (1&lt;&lt;6)

<span>#define</span> <span>PS_VIEWOFFSET</span>       (1&lt;&lt;7)
<span>#define</span> <span>PS_VIEWANGLES</span>       (1&lt;&lt;8)
<span>#define</span> <span>PS_KICKANGLES</span>       (1&lt;&lt;9)
<span>#define</span> <span>PS_BLEND</span>            (1&lt;&lt;10)
<span>#define</span> <span>PS_FOV</span>              (1&lt;&lt;11)
<span>#define</span> <span>PS_WEAPONINDEX</span>      (1&lt;&lt;12)
<span>#define</span> <span>PS_WEAPONFRAME</span>      (1&lt;&lt;13)
<span>#define</span> <span>PS_RDFLAGS</span>          (1&lt;&lt;14)

<span>#define</span> <span>PS_MOREBITS</span>         (1&lt;&lt;15)

<span>// [Paril-KEX]</span>
<span>#define</span> <span>PS_DAMAGE_BLEND</span>     (1&lt;&lt;16)
<span>#define</span> <span>PS_TEAM_ID</span>          (1&lt;&lt;17)</pre></div>
<h4 tabindex="-1" dir="auto">Data</h4>
<ul dir="auto">
<li>ReadUShort (flags)</li>
<li>[if flags &amp; PS_MOREBITS] flags |= ReadUShort &lt;&lt; 16</li>
<li>[if flags &amp; PS_M_TYPE] ReadByte (pm_type)</li>
<li>[if flags &amp; PS_M_ORIGIN] ReadPos (pm_origin)</li>
<li>[if flags &amp; PS_M_VELOCITY] ReadPos (pm_velocity)</li>
<li>[if flags &amp; PS_M_TIME] ReadUShort (pm_time)</li>
<li>[if flags &amp; PS_M_FLAGS] ReadUShort (pm_flags)</li>
<li>[if flags &amp; PS_M_GRAVITY] ReadShort (pm_gravity)</li>
<li>[if flags &amp; PS_M_DELTA_ANGLES] ReadPos (pm_delta_angles)</li>
<li>[if flags &amp; PS_VIEWOFFSET]:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="	viewoffset_x = ReadShort() * (1.f / 16.f)
	viewoffset_y = ReadShort() * (1.f / 16.f)
	viewoffset_z = ReadShort() * (1.f / 16.f)
	viewheight = ReadChar() // note: not in protocol 2022"><pre>	viewoffset_x = ReadShort() * (<span>1</span>.f / <span>16</span>.f)
	viewoffset_y = ReadShort() * (<span>1</span>.f / <span>16</span>.f)
	viewoffset_z = ReadShort() * (<span>1</span>.f / <span>16</span>.f)
	viewheight = ReadChar() <span><span>//</span> note: not in protocol 2022</span></pre></div>
<ul dir="auto">
<li>[if flags &amp; PS_VIEWANGLES] ReadPos (viewangles)</li>
<li>[if flags &amp; PS_KICKANGLES]</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="kick_angles_x = ReadShort() / 1024.f
kick_angles_y = ReadShort() / 1024.f
kick_angles_z = ReadShort() / 1024.f"><pre>kick_angles_x = ReadShort() / <span>1024</span>.f
kick_angles_y = ReadShort() / <span>1024</span>.f
kick_angles_z = ReadShort() / <span>1024</span>.f</pre></div>
<ul dir="auto">
<li>[if flags &amp; PS_WEAPONINDEX]</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="gunindex_temp = ReadUShort()
gunskin = (gunindex_temp &amp; 0xE000) >> 13
gunindex = gunindex_temp &amp; ~0xE000"><pre>gunindex_temp = ReadUShort()
gunskin = (gunindex_temp &amp; <span>0xE000</span>) &gt;&gt; <span>13</span>
gunindex = gunindex_temp &amp; ~<span>0xE000</span></pre></div>
<ul dir="auto">
<li>[if flags &amp; PS_WEAPONFRAME]</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="#define GUNBIT_OFFSET_X (1<<0)
#define GUNBIT_OFFSET_Y (1<<1)
#define GUNBIT_OFFSET_Z (1<<2)
#define GUNBIT_ANGLES_X (1<<3)
#define GUNBIT_ANGLES_Y (1<<4)
#define GUNBIT_ANGLES_Z (1<<5)
#define GUNBIT_GUNRATE (1<<6)"><pre>#<span>define</span> <span>GUNBIT_OFFSET_X</span> (<span>1</span>&lt;&lt;<span>0</span>)
#<span>define</span> <span>GUNBIT_OFFSET_Y</span> (<span>1</span>&lt;&lt;<span>1</span>)
#<span>define</span> <span>GUNBIT_OFFSET_Z</span> (<span>1</span>&lt;&lt;<span>2</span>)
#<span>define</span> <span>GUNBIT_ANGLES_X</span> (<span>1</span>&lt;&lt;<span>3</span>)
#<span>define</span> <span>GUNBIT_ANGLES_Y</span> (<span>1</span>&lt;&lt;<span>4</span>)
#<span>define</span> <span>GUNBIT_ANGLES_Z</span> (<span>1</span>&lt;&lt;<span>5</span>)
#<span>define</span> <span>GUNBIT_GUNRATE</span> (<span>1</span>&lt;&lt;<span>6</span>)</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="gunframe_temp = ReadUShort()
gun_bits = (gunframe_temp &amp; 0xFE00) >> 9
gunframe = (gunframe_temp &amp; ~0xFE00)

[if gun_bits &amp; GUNBIT_OFFSET_X] gunoffset_x = ReadFloat()
[if gun_bits &amp; GUNBIT_OFFSET_Y] gunoffset_y = ReadFloat()
[if gun_bits &amp; GUNBIT_OFFSET_Z] gunoffset_z = ReadFloat()
[if gun_bits &amp; GUNBIT_ANGLES_X] gunangles_x = ReadFloat()
[if gun_bits &amp; GUNBIT_ANGLES_Y] gunangles_y = ReadFloat()
[if gun_bits &amp; GUNBIT_ANGLES_Z] gunangles_z = ReadFloat()
[if gun_bits &amp; GUNBIT_GUNRATE] gunrate = ReadByte()"><pre>gunframe_temp = ReadUShort()
gun_bits = (gunframe_temp &amp; <span>0xFE00</span>) &gt;&gt; <span>9</span>
gunframe = (gunframe_temp &amp; ~<span>0xFE00</span>)

[<span>if</span> gun_bits &amp; GUNBIT_OFFSET_X] gunoffset_x = ReadFloat()
[<span>if</span> gun_bits &amp; GUNBIT_OFFSET_Y] gunoffset_y = ReadFloat()
[<span>if</span> gun_bits &amp; GUNBIT_OFFSET_Z] gunoffset_z = ReadFloat()
[<span>if</span> gun_bits &amp; GUNBIT_ANGLES_X] gunangles_x = ReadFloat()
[<span>if</span> gun_bits &amp; GUNBIT_ANGLES_Y] gunangles_y = ReadFloat()
[<span>if</span> gun_bits &amp; GUNBIT_ANGLES_Z] gunangles_z = ReadFloat()
[<span>if</span> gun_bits &amp; GUNBIT_GUNRATE] gunrate = ReadByte()</pre></div>
<ul dir="auto">
<li>[if flags &amp; PS_BLEND]</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="screen_blend_r = ReadByte() / 255.f
screen_blend_g = ReadByte() / 255.f
screen_blend_b = ReadByte() / 255.f
screen_blend_a = ReadByte() / 255.f"><pre>screen_blend_r = ReadByte() / <span>255</span>.f
screen_blend_g = ReadByte() / <span>255</span>.f
screen_blend_b = ReadByte() / <span>255</span>.f
screen_blend_a = ReadByte() / <span>255</span>.f</pre></div>
<ul dir="auto">
<li>[if flags &amp; PS_FOV] ReadByte(fov)</li>
<li>[if flags &amp; PS_RDFLAGS] ReadByte(rdflags)</li>
<li>ReadLong(statbits)</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="for (i = 0; i < 32; i++)
	if (statbits &amp; (1 << i))
		ReadShort(stats[i])"><pre><span>for</span> (i = <span>0</span>; i &lt; <span>32</span>; i++)
	<span>if</span> (statbits &amp; (<span>1</span> &lt;&lt; i))
		<span>ReadShort</span>(stats[i])</pre></div>
<ul dir="auto">
<li>ReadLong(morestatbits)</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="for (i = 32; i < 64; i++)
	if (morestatbits &amp; (1 << (i - 32)))
		ReadShort(stats[i])"><pre><span>for</span> (i = <span>32</span>; i &lt; <span>64</span>; i++)
	<span>if</span> (morestatbits &amp; (<span>1</span> &lt;&lt; (i - <span>32</span>)))
		<span>ReadShort</span>(stats[i])</pre></div>
<ul dir="auto">
<li>[if flags &amp; PS_DAMAGE_BLEND]</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="damage_blend_r = ReadByte() / 255.f
damage_blend_g = ReadByte() / 255.f
damage_blend_b = ReadByte() / 255.f
damage_blend_a = ReadByte() / 255.f"><pre>damage_blend_r = ReadByte() / <span>255</span>.f
damage_blend_g = ReadByte() / <span>255</span>.f
damage_blend_b = ReadByte() / <span>255</span>.f
damage_blend_a = ReadByte() / <span>255</span>.f</pre></div>
<ul dir="auto">
<li>[if flags &amp; PS_TEAM_ID]</li>
</ul>

<h3 tabindex="-1" dir="auto">svc_packetentities (18)</h3>
<h4 tabindex="-1" dir="auto">Bits</h4>
<div dir="auto" data-snippet-clipboard-copy-content="
// try to pack the common update flags into the first byte
#define U_ORIGIN1   (1<<0)
#define U_ORIGIN2   (1<<1)
#define U_ANGLE2    (1<<2)
#define U_ANGLE3    (1<<3)
#define U_FRAME8    (1<<4)      // frame is a byte
#define U_EVENT     (1<<5)
#define U_REMOVE    (1<<6)      // REMOVE this entity, don't add it
#define U_MOREBITS1 (1<<7)      // read one additional byte

// second byte
#define U_NUMBER16  (1<<8)      // NUMBER8 is implicit if not set
#define U_ORIGIN3   (1<<9)
#define U_ANGLE1    (1<<10)
#define U_MODEL     (1<<11)
#define U_RENDERFX8 (1<<12)     // fullbright, etc
#define U_EFFECTS8  (1<<14)     // autorotate, trails, etc
#define U_MOREBITS2 (1<<15)     // read one additional byte

// third byte
#define U_SKIN8     (1<<16)
#define U_FRAME16   (1<<17)     // frame is a short
#define U_RENDERFX16 (1<<18)    // 8 + 16 = 32
#define U_EFFECTS16 (1<<19)     // 8 + 16 = 32
#define U_MODEL2    (1<<20)     // weapons, flags, etc
#define U_MODEL3    (1<<21)
#define U_MODEL4    (1<<22)
#define U_MOREBITS3 (1<<23)     // read one additional byte

// fourth byte
#define U_OLDORIGIN (1<<24)     // FIXME: get rid of this
#define U_SKIN16    (1<<25)
#define U_SOUND     (1<<26)
#define U_SOLID     (1<<27)
#define U_MODEL16   (1<<28)
#define U_EFFECTS64 (1<<29) // [Edward-KEX]
#define U_ALPHA     (1<<30) // [Paril-KEX]
#define U_MOREBITS4 (1<<31) // [Paril-KEX] read one additional byte
#define U_SCALE     (1ull<<32ull) // [Paril-KEX]
#define U_INSTANCE  (1ull<<33ull) // [Paril-KEX]
#define U_OWNER     (1ull<<34ull) // [Paril-KEX]
#define U_OLDFRAME  (1ull<<35ull) // [Paril-KEX]"><pre><span>// try to pack the common update flags into the first byte</span>
<span>#define</span> <span>U_ORIGIN1</span>   (1&lt;&lt;0)
<span>#define</span> <span>U_ORIGIN2</span>   (1&lt;&lt;1)
<span>#define</span> <span>U_ANGLE2</span>    (1&lt;&lt;2)
<span>#define</span> <span>U_ANGLE3</span>    (1&lt;&lt;3)
<span>#define</span> <span>U_FRAME8</span>    (1&lt;&lt;4)      // frame is a byte
<span>#define</span> <span>U_EVENT</span>     (1&lt;&lt;5)
<span>#define</span> <span>U_REMOVE</span>    (1&lt;&lt;6)      // REMOVE this entity, don't add it
<span>#define</span> <span>U_MOREBITS1</span> (1&lt;&lt;7)      // read one additional byte

<span>// second byte</span>
<span>#define</span> <span>U_NUMBER16</span>  (1&lt;&lt;8)      // NUMBER8 is implicit if not set
<span>#define</span> <span>U_ORIGIN3</span>   (1&lt;&lt;9)
<span>#define</span> <span>U_ANGLE1</span>    (1&lt;&lt;10)
<span>#define</span> <span>U_MODEL</span>     (1&lt;&lt;11)
<span>#define</span> <span>U_RENDERFX8</span> (1&lt;&lt;12)     // fullbright, etc
<span>#define</span> <span>U_EFFECTS8</span>  (1&lt;&lt;14)     // autorotate, trails, etc
<span>#define</span> <span>U_MOREBITS2</span> (1&lt;&lt;15)     // read one additional byte

<span>// third byte</span>
<span>#define</span> <span>U_SKIN8</span>     (1&lt;&lt;16)
<span>#define</span> <span>U_FRAME16</span>   (1&lt;&lt;17)     // frame is a short
<span>#define</span> <span>U_RENDERFX16</span> (1&lt;&lt;18)    // 8 + 16 = 32
<span>#define</span> <span>U_EFFECTS16</span> (1&lt;&lt;19)     // 8 + 16 = 32
<span>#define</span> <span>U_MODEL2</span>    (1&lt;&lt;20)     // weapons, flags, etc
<span>#define</span> <span>U_MODEL3</span>    (1&lt;&lt;21)
<span>#define</span> <span>U_MODEL4</span>    (1&lt;&lt;22)
<span>#define</span> <span>U_MOREBITS3</span> (1&lt;&lt;23)     // read one additional byte

<span>// fourth byte</span>
<span>#define</span> <span>U_OLDORIGIN</span> (1&lt;&lt;24)     // FIXME: get rid of this
<span>#define</span> <span>U_SKIN16</span>    (1&lt;&lt;25)
<span>#define</span> <span>U_SOUND</span>     (1&lt;&lt;26)
<span>#define</span> <span>U_SOLID</span>     (1&lt;&lt;27)
<span>#define</span> <span>U_MODEL16</span>   (1&lt;&lt;28)
<span>#define</span> <span>U_EFFECTS64</span> (1&lt;&lt;29) // [Edward-KEX]
<span>#define</span> <span>U_ALPHA</span>     (1&lt;&lt;30) // [Paril-KEX]
<span>#define</span> <span>U_MOREBITS4</span> (1&lt;&lt;31) // [Paril-KEX] read one additional byte
<span>#define</span> <span>U_SCALE</span>     (1ull&lt;&lt;32ull) // [Paril-KEX]
<span>#define</span> <span>U_INSTANCE</span>  (1ull&lt;&lt;33ull) // [Paril-KEX]
<span>#define</span> <span>U_OWNER</span>     (1ull&lt;&lt;34ull) // [Paril-KEX]
<span>#define</span> <span>U_OLDFRAME</span>  (1ull&lt;&lt;35ull) // [Paril-KEX]</pre></div>
<h4 tabindex="-1" dir="auto">Data</h4>
<p dir="auto">The regular process for deltaing entities has not changed, but the data bits have.</p>
<p dir="auto">ParseEntityBits:</p>
<ul dir="auto">
<li>ReadByte(bits)</li>
<li>[if bits &amp; U_MOREBITS1] bits |= ReadByte() &lt;&lt; 8</li>
<li>[if bits &amp; U_MOREBITS2] bits |= ReadByte() &lt;&lt; 16</li>
<li>[if bits &amp; U_MOREBITS3] bits |= ReadByte() &lt;&lt; 24</li>
<li>[if bits &amp; U_MOREBITS4] bits |= ReadByte() &lt;&lt; 32</li>
<li>[if bits &amp; U_NUMBER16] ReadShort(number) [else] ReadByte(number)</li>
</ul>
<p dir="auto">ParseDelta:</p>
<ul dir="auto">
<li>[if bits &amp; U_MODEL16]</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="ReadShort(modelindex)
ReadShort(modelindex2)
ReadShort(modelindex3)
ReadShort(modelindex4)"><pre><span>ReadShort</span>(modelindex)
ReadShort(modelindex2)
ReadShort(modelindex3)
ReadShort(modelindex4)</pre></div>
<ul dir="auto">
<li>[else]</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="ReadByte(modelindex)
ReadByte(modelindex2)
ReadByte(modelindex3)
ReadByte(modelindex4)"><pre><span>ReadByte</span>(modelindex)
ReadByte(modelindex2)
ReadByte(modelindex3)
ReadByte(modelindex4)</pre></div>
<ul dir="auto">
<li>[if bits &amp; U_FRAME8] ReadByte(frame)</li>
<li>[if bits &amp; U_FRAME16] ReadShort(frame)</li>
<li>[if bits &amp; (U_SKIN8 | U_SKIN16) == (U_SKIN8 | U_SKIN16)] ReadLong(skinnum)</li>
<li>[elseif bits &amp; U_SKIN8] ReadByte(skinnum)</li>
<li>[elseif bits &amp; U_SKIN16] ReadUShort(skinnum)</li>
<li>[if bits &amp; (U_EFFECTS8 | U_EFFECTS16 | U_EFFECTS64)]</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="// if 64-bit effects are sent, the low bits are sent first
// and the high bits come after.
[if bits &amp; U_EFFECTS64] ReadULong(loeffects)

[if bits &amp; (U_EFFECTS8 | U_EFFECTS16) == (U_EFFECTS8 | U_EFFECTS16)] ReadULong(effects)
[elseif bits &amp; U_EFFECTS16] ReadUShort(effects)
[else] ReadByte(effects)

[if bits &amp; U_EFFECTS64] effects = (effects << 32) | loeffects "><pre><span><span>//</span> if 64-bit effects are sent, the low bits are sent first</span>
<span><span>//</span> and the high bits come after.</span>
[<span>if</span> bits &amp; U_EFFECTS64] ReadULong(loeffects)

[<span>if</span> bits &amp; (U_EFFECTS8 | U_EFFECTS16) == (U_EFFECTS8 | U_EFFECTS16)] ReadULong(effects)
[elseif bits &amp; U_EFFECTS16] ReadUShort(effects)
[<span>else</span>] ReadByte(effects)

[<span>if</span> bits &amp; U_EFFECTS64] effects = (effects &lt;&lt; <span>32</span>) | loeffects </pre></div>
<ul dir="auto">
<li>[if bits &amp; (U_RENDERFX8 | U_RENDERFX16) == (U_RENDERFX8 | U_RENDERFX16)] ReadLong(effects)</li>
<li>[elseif bits &amp; renderfx] ReadByte(renderfx)</li>
<li>[elseif bits &amp; U_RENDERFX16] ReadShort(renderfx)</li>
<li>[if bits &amp; U_SOLID] ReadULong(solid)</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="// note: for the protocol in the demos (2022), if `solid` is zero,
// then the following reads are lower precision, using ReadShort() * (1.f / 8.f)
[if bits &amp; U_ORIGIN1] ReadFloat(origin_x)
[if bits &amp; U_ORIGIN2] ReadFloat(origin_y)
[if bits &amp; U_ORIGIN3] ReadFloat(origin_z)
[if bits &amp; U_OLDORIGIN] ReadPos(oldorigin)"><pre><span><span>//</span> note: for the protocol in the demos (2022), if `solid` is zero,</span>
<span><span>//</span> then the following reads are lower precision, using ReadShort() * (1.f / 8.f)</span>
[<span>if</span> bits &amp; U_ORIGIN1] ReadFloat(origin_x)
[<span>if</span> bits &amp; U_ORIGIN2] ReadFloat(origin_y)
[<span>if</span> bits &amp; U_ORIGIN3] ReadFloat(origin_z)
[<span>if</span> bits &amp; U_OLDORIGIN] ReadPos(oldorigin)</pre></div>
<ul dir="auto">
<li>[if bits &amp; U_ANGLE1] ReadFloat(angle_x)</li>
<li>[if bits &amp; U_ANGLE2] ReadFloat(angle_y)</li>
<li>[if bits &amp; U_ANGLE3] ReadFloat(angle_z)</li>
<li>[if bits &amp; U_SOUND] ReadUShort(temp_sound)</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="bool has_volume = temp_sound &amp; 0x4000
bool has_attenuation = temp_sound &amp; 0x8000;

// the sound index takes up 14 bits
sound = temp_sound &amp; ~(0x4000 | 0x8000)

[if has_volume] loop_volume = ReadByte() / 255.f
[else] loop_volume = 1.f

[if has_attn] loop_attenuation = ReadByte()
[else] loop_attenuation = ATTN_STATIC"><pre><span>bool</span> has_volume = temp_sound &amp; <span>0x4000</span>
<span>bool</span> has_attenuation = temp_sound &amp; <span>0x8000</span>;

<span><span>//</span> the sound index takes up 14 bits</span>
sound = temp_sound &amp; ~(<span>0x4000</span> | <span>0x8000</span>)

[<span>if</span> has_volume] loop_volume = ReadByte() / <span>255</span>.f
[<span>else</span>] loop_volume = <span>1</span>.f

[<span>if</span> has_attn] loop_attenuation = ReadByte()
[<span>else</span>] loop_attenuation = ATTN_STATIC</pre></div>
<ul dir="auto">
<li>[if bits &amp; U_EVENT] ReadByte(event) [else] event = 0</li>
<li>[if bits &amp; U_ALPHA] alpha = ReadByte() / 255.f</li>
<li>[if bits &amp; U_SCALE] scale = ReadByte() / 16.f</li>
<li>[if bits &amp; U_INSTANCE] ReadByte(instance_bits)</li>
<li>[if bits &amp; U_OWNER] ReadShort(owner)</li>
<li>[if bits &amp; U_OLDFRAME] ReadUShort(old_frame)</li>
</ul>
<h2 tabindex="-1" dir="auto">(packet, server -&gt; client) svc_splitclient (21)</h2>
<p dir="auto">This packet indicates to the client which split screen player the next messages are directed towards, for unicast messages.</p>
<ul dir="auto">
<li>ReadByte (isplit)</li>
</ul>
<p dir="auto">Note that <code>isplit</code> will be offset by 1 (that is to say, a value of 1 indicates split screen client 0).</p>
<h2 tabindex="-1" dir="auto">(packet, server -&gt; client) svc_configblast (22)</h2>
<p dir="auto">Compressed configstring data. This is to make connection faster by sending fewer packets.</p>
<ul dir="auto">
<li>ReadShort (compressed size)</li>
<li>ReadShort (uncompressed size)</li>
<li>ReadByte[compressed size] (buffer)</li>
</ul>
<p dir="auto">The received <code>buffer</code> is directly passed through to zlib's <code>uncompress</code>. After decompression, until the buffer is exhausted, the following data repeats:</p>
<ul dir="auto">
<li>ReadUShort (index)</li>
<li>ReadString (str)</li>
</ul>
<h2 tabindex="-1" dir="auto">(packet, server -&gt; client) svc_spawnbaselineblast (23)</h2>
<p dir="auto">Compressed baseline data. This is to make connection faster by sending fewer packets.</p>
<ul dir="auto">
<li>ReadShort (compressed size)</li>
<li>ReadShort (uncompressed size)</li>
<li>ReadByte[compressed size] (buffer)</li>
</ul>
<p dir="auto">The received <code>buffer</code> is directly passed through to zlib's <code>uncompress</code>. After decompression, until the buffer is exhausted, read in the data contained in a <code>svc_spawnbaseline</code> packet.</p>
<h2 tabindex="-1" dir="auto">(packet, server -&gt; client) svc_level_restart (24)</h2>
<p dir="auto">Sent when the server executes a <code>restart_level</code> command. The client should be prepared to do a "soft wipe" of their state, but might want to defer it until the full frame is read since effects might come in after this command is executed.</p>
<p dir="auto">This message's data contains configstrings that were changed by restarting the level. The following should be repeated until an exit condition is met:</p>
<ul dir="auto">
<li>ReadShort (id)</li>
<li>[if id is -1, exit]</li>
<li>ReadString (str)</li>
</ul>
<h2 tabindex="-1" dir="auto">(packet, server -&gt; client) svc_damage (25)</h2>
<p dir="auto">This message is sent after accumulating damage on a player. It gives the player a rough idea of the damage they're receiving and from where.</p>
<ul dir="auto">
<li>ReadByte (count)</li>
</ul>
<p dir="auto">For <code>count</code> number of loops, read the following:</p>
<ul dir="auto">
<li>ReadByte (encoded)</li>
<li>ReadDir</li>
</ul>
<p dir="auto"><code>encoded</code> is in the following format:</p>
<div data-snippet-clipboard-copy-content="struct packed_damage_t
{
	uint8_t damage : 5;
	uint8_t health : 1;
	uint8_t armor : 1;
	uint8_t shield : 1;
}"><pre><code>struct packed_damage_t
{
	uint8_t damage : 5;
	uint8_t health : 1;
	uint8_t armor : 1;
	uint8_t shield : 1;
}
</code></pre></div>
<p dir="auto"><code>health</code> provides a <code>1,0,0</code> addition to color.
<code>armor</code> provides a <code>1,1,1</code> addition to color.
<code>shield</code> provides a <code>0,1,0</code> addition to color.</p>
<p dir="auto">The <code>damage</code> value is also divided by 3, so multiplying it by 3 will get you an approximation of the real damage amount.</p>
<p dir="auto">The color is then normalized.</p>
<h2 tabindex="-1" dir="auto">(packet, server -&gt; client) svc_locprint (26)</h2>
<p dir="auto">This packet is the new entry point for prints.</p>
<ul dir="auto">
<li>ReadByte (flags)</li>
<li>ReadString (base)</li>
<li>ReadByte (num args)</li>
<li>ReadString[num args] (args)</li>
</ul>
<p dir="auto">The <code>base</code> string is a <code>fmtlib</code> formatted string.</p>
<p dir="auto">The information in <a href="#print-adjustments">Print Adjustments</a> and <a href="#loc_print">Loc_Print</a> explains how formatting works.</p>
<h2 tabindex="-1" dir="auto">(packet, server -&gt; client) svc_fog (27)</h2>
<div dir="auto" data-snippet-clipboard-copy-content="enum bits_t : uint16_t
{
	// global fog
	BIT_DENSITY     = bit_v<0>,
	BIT_R           = bit_v<1>,
	BIT_G           = bit_v<2>,
	BIT_B           = bit_v<3>,
	BIT_TIME        = bit_v<4>, // if set, the transition takes place over N milliseconds

	// height fog
	BIT_HEIGHTFOG_FALLOFF   = bit_v<5>,
	BIT_HEIGHTFOG_DENSITY   = bit_v<6>,
	BIT_MORE_BITS           = bit_v<7>, // read additional bit
	BIT_HEIGHTFOG_START_R   = bit_v<8>,
	BIT_HEIGHTFOG_START_G   = bit_v<9>,
	BIT_HEIGHTFOG_START_B   = bit_v<10>,
	BIT_HEIGHTFOG_START_DIST= bit_v<11>,
	BIT_HEIGHTFOG_END_R     = bit_v<12>,
	BIT_HEIGHTFOG_END_G     = bit_v<13>,
	BIT_HEIGHTFOG_END_B     = bit_v<14>,
	BIT_HEIGHTFOG_END_DIST  = bit_v<15>
};"><pre><span>enum</span> <span>bits_t</span> : <span>uint16_t</span>
{
	<span><span>//</span> global fog</span>
	BIT_DENSITY     = bit_v&lt;<span>0</span>&gt;,
	BIT_R           = bit_v&lt;<span>1</span>&gt;,
	BIT_G           = bit_v&lt;<span>2</span>&gt;,
	BIT_B           = bit_v&lt;<span>3</span>&gt;,
	BIT_TIME        = bit_v&lt;<span>4</span>&gt;, <span><span>//</span> if set, the transition takes place over N milliseconds</span>

	<span><span>//</span> height fog</span>
	BIT_HEIGHTFOG_FALLOFF   = bit_v&lt;<span>5</span>&gt;,
	BIT_HEIGHTFOG_DENSITY   = bit_v&lt;<span>6</span>&gt;,
	BIT_MORE_BITS           = bit_v&lt;<span>7</span>&gt;, <span><span>//</span> read additional bit</span>
	BIT_HEIGHTFOG_START_R   = bit_v&lt;<span>8</span>&gt;,
	BIT_HEIGHTFOG_START_G   = bit_v&lt;<span>9</span>&gt;,
	BIT_HEIGHTFOG_START_B   = bit_v&lt;<span>10</span>&gt;,
	BIT_HEIGHTFOG_START_DIST= bit_v&lt;<span>11</span>&gt;,
	BIT_HEIGHTFOG_END_R     = bit_v&lt;<span>12</span>&gt;,
	BIT_HEIGHTFOG_END_G     = bit_v&lt;<span>13</span>&gt;,
	BIT_HEIGHTFOG_END_B     = bit_v&lt;<span>14</span>&gt;,
	BIT_HEIGHTFOG_END_DIST  = bit_v&lt;<span>15</span>&gt;
};</pre></div>
<ul dir="auto">
<li>ReadByte (bits)</li>
<li>[if bits &amp; BIT_MORE_BITS] ReadByte (morebits), bits |= (morebits &lt;&lt; 8)</li>
<li>[if bits &amp; BIT_DENSITY] ReadFloat (density)</li>
<li>[if bits &amp; BIT_DENSITY] ReadByte (skyfactor)</li>
<li>[if bits &amp; BIT_R] ReadByte (red)</li>
<li>[if bits &amp; BIT_G] ReadByte (green)</li>
<li>[if bits &amp; BIT_B] ReadByte (blue)</li>
<li>[if bits &amp; BIT_TIME] ReadUShort (time)</li>
<li>[if bits &amp; BIT_HEIGHTFOG_FALLOFF] ReadFloat (heightfog falloff)</li>
<li>[if bits &amp; BIT_HEIGHTFOG_DENSITY] ReadFloat (heightfog density)</li>
<li>[if bits &amp; BIT_HEIGHTFOG_START_R] ReadByte (heightfog start red)</li>
<li>[if bits &amp; BIT_HEIGHTFOG_START_G] ReadByte (heightfog start green)</li>
<li>[if bits &amp; BIT_HEIGHTFOG_START_B] ReadByte (heightfog start blue)</li>
<li>[if bits &amp; BIT_HEIGHTFOG_START_DIST] ReadLong (heightfog start distance)</li>
<li>[if bits &amp; BIT_HEIGHTFOG_END_R] ReadByte (heightfog end red)</li>
<li>[if bits &amp; BIT_HEIGHTFOG_END_G] ReadByte (heightfog end green)</li>
<li>[if bits &amp; BIT_HEIGHTFOG_END_B] ReadByte (heightfog end blue)</li>
<li>[if bits &amp; BIT_HEIGHTFOG_END_DIST] ReadLong (heightfog end distance)</li>
</ul>
<h2 tabindex="-1" dir="auto">(packet, server -&gt; client) svc_waitingforplayers (28)</h2>
<p dir="auto">Sent when there are players waiting to join before the game can start (or zero if all players are in).</p>
<ul dir="auto">
<li>ReadByte (count)</li>
</ul>
<h2 tabindex="-1" dir="auto">(packet, server -&gt; client) svc_bot_chat (29)</h2>
<p dir="auto">Bots talking to players.</p>
<ul dir="auto">
<li>ReadString (bot name)</li>
<li>ReadShort (client index, or 256 if no particular player)</li>
<li>ReadString (loc string)</li>
</ul>
<h2 tabindex="-1" dir="auto">(packet, server -&gt; client) svc_poi (30)</h2>
<p dir="auto">Spawn a POI.</p>
<div dir="auto" data-snippet-clipboard-copy-content="enum svc_poi_flags
{
    POI_FLAG_NONE = 0,
    POI_FLAG_HIDE_ON_AIM = 1, // hide the POI if we get close to it with our aim
};"><pre><span>enum</span> svc_poi_flags
{
    POI_FLAG_NONE = <span>0</span>,
    POI_FLAG_HIDE_ON_AIM = <span>1</span>, <span><span>//</span> hide the POI if we get close to it with our aim</span>
};</pre></div>
<ul dir="auto">
<li>ReadUShort (key)</li>
<li>ReadUShort (time)</li>
<li>ReadPos (pos)</li>
<li>ReadUShort (image index)</li>
<li>ReadByte (palette index)</li>
<li>ReadByte (flags)</li>
</ul>
<p dir="auto">If a non-zero <code>key</code> is specified, only one of that POI key can exist at any given time. If <code>time</code> is 0xFFFF, the POI that matches the key will be removed.</p>
<p dir="auto">If <code>time</code> is zero, the POI will last forever, <code>key</code> should be set in order to allow the POI to be cleaned up later.</p>
<h2 tabindex="-1" dir="auto">(packet, server -&gt; client) svc_help_path (31)</h2>
<p dir="auto">Spawns the Compass help path effect at the given location.</p>
<ul dir="auto">
<li>ReadByte (start)</li>
<li>ReadPos (pos)</li>
<li>ReadDir (dir)</li>
</ul>
<h2 tabindex="-1" dir="auto">(packet, server -&gt; client) svc_achievement (32)</h2>
<ul dir="auto">
<li>ReadString (id)</li>
</ul>
<h2 tabindex="-1" dir="auto">(packet, client -&gt; server) clc_stringcmd (4)</h2>
<ul dir="auto">
<li>ReadByte (isplit)</li>
<li>ReadString (s)</li>
</ul>
<p dir="auto">Note that <code>isplit</code> is offset by 1, so <code>1</code> is the first split screen client.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Git scraping: track changes over time by scraping to a Git repository (140 pts)]]></title>
            <link>https://simonwillison.net/2020/Oct/9/git-scraping/</link>
            <guid>37082289</guid>
            <pubDate>Thu, 10 Aug 2023 21:57:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2020/Oct/9/git-scraping/">https://simonwillison.net/2020/Oct/9/git-scraping/</a>, See on <a href="https://news.ycombinator.com/item?id=37082289">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>



<p>9th October 2020</p>

<p><strong>Git scraping</strong> is the name I’ve given a scraping technique that I’ve been experimenting with for a few years now. It’s really effective, and more people should use it.</p>
<p><em><strong>Update 5th March 2021:</strong> I presented a version of this post as <a href="https://simonwillison.net/2021/Mar/5/git-scraping/">a five minute lightning talk at NICAR 2021</a>, which includes a live coding demo of building a new git scraper.</em></p>
<p><em><strong>Update 5th January 2022:</strong> I released a tool called <a href="https://simonwillison.net/2021/Dec/7/git-history/">git-history</a> that helps analyze data that has been collected using this technique.</em></p>
<p>The internet is full of interesting data that changes over time. These changes can sometimes be more interesting than the underlying static data—The <a href="https://twitter.com/nyt_diff">@nyt_diff Twitter account</a> tracks changes made to New York Times headlines for example, which offers a fascinating insight into that publication’s editorial process.</p>
<p>We already have a great tool for efficiently tracking changes to text over time: <strong>Git</strong>. And <a href="https://github.com/features/actions">GitHub Actions</a> (and other CI systems) make it easy to create a scraper that runs every few minutes, records the current state of a resource and records changes to that resource over time in the commit history.</p>
<p>Here’s a recent example. Fires continue to rage in California, and the <a href="https://www.fire.ca.gov/">CAL FIRE website</a> offers an <a href="https://www.fire.ca.gov/incidents/">incident map</a> showing the latest fire activity around the state.</p>
<p>Firing up the Firefox Network pane, filtering to requests triggered by XHR and sorting by size, largest first reveals this endpoint:</p>
<p><a href="https://www.fire.ca.gov/umbraco/Api/IncidentApi/GetIncidents">https://www.fire.ca.gov/umbraco/Api/IncidentApi/GetIncidents</a></p>
<p>That’s a 241KB JSON endpoints with full details of the various fires around the state.</p>
<p>So... I started running a git scraper against it. My scraper lives in the <a href="https://github.com/simonw/ca-fires-history">simonw/ca-fires-history</a> repository on GitHub.</p>
<p>Every 20 minutes it grabs the latest copy of that JSON endpoint, pretty-prints it (for diff readability) using <code>jq</code> and commits it back to the repo if it has changed.</p>
<p>This means I now have a <a href="https://github.com/simonw/ca-fires-history/commits/main">commit log</a> of changes to that information about fires in California. Here’s an <a href="https://github.com/simonw/ca-fires-history/commit/7b0f42d4bf198885ab2b41a22a8da47157572d18">example commit</a> showing that last night the Zogg Fires percentage contained increased from 90% to 92%, the number of personnel involved dropped from 968 to 798 and the number of engines responding dropped from 82 to 59.</p>
<p><img src="https://static.simonwillison.net/static/2020/git-scraping.png" alt="Screenshot of a diff against the Zogg Fires, showing personnel involved dropping from 968 to 798, engines dropping 82 to 59, water tenders dropping 31 to 27 and percent contained increasing from 90 to 92."></p>
<p>The implementation of the scraper is entirely contained in a single GitHub Actions workflow. It’s in a file called <a href="https://github.com/simonw/ca-fires-history/blob/main/.github/workflows/scrape.yml">.github/workflows/scrape.yml</a> which looks like this:</p>
<div><pre><span>name</span>: <span>Scrape latest data</span>

<span>on</span>:
  <span>push</span>:
  <span>workflow_dispatch</span>:
  <span>schedule</span>:
    - <span>cron</span>:  <span><span>'</span>6,26,46 * * * *<span>'</span></span>

<span>jobs</span>:
  <span>scheduled</span>:
    <span>runs-on</span>: <span>ubuntu-latest</span>
    <span>steps</span>:
    - <span>name</span>: <span>Check out this repo</span>
      <span>uses</span>: <span>actions/checkout@v2</span>
    - <span>name</span>: <span>Fetch latest data</span>
      <span>run</span>: <span>|-</span>
<span>        curl https://www.fire.ca.gov/umbraco/Api/IncidentApi/GetIncidents | jq . &gt; incidents.json</span>
<span>    - <span>name</span>: <span>Commit and push if it changed</span>
      <span>run</span>: <span>|-</span>
<span>        git config user.name "Automated"</span>
<span>        git config user.email "actions@users.noreply.github.com"</span>
<span>        git add -A</span>
<span>        timestamp=$(date -u)</span>
<span>        git commit -m "Latest data: ${timestamp}" || exit 0</span>
<span>        git push</span></span></pre></div>
<p>That’s not a lot of code!</p>
<p>It runs on a schedule at 6, 26 and 46 minutes past the hour—I like to offset my cron times like this since I assume that the majority of crons run exactly on the hour, so running not-on-the-hour feels polite.</p>
<p>The scraper itself works by fetching the JSON using <code>curl</code>, piping it through <code>jq .</code> to pretty-print it and saving the result to <code>incidents.json</code>.</p>
<p>The “commit and push if it changed” block uses a pattern that commits and pushes only if the file has changed. I wrote about this pattern in <a href="https://til.simonwillison.net/til/til/github-actions_commit-if-file-changed.md">this TIL</a> a few months ago.</p>
<p>I have a whole bunch of repositories running git scrapers now. I’ve been labeling them with the <a href="https://github.com/topics/git-scraping">git-scraping topic</a> so they show up in one place on GitHub (other people have started using that topic as well).</p>
<p>I’ve written about some of these <a href="https://simonwillison.net/tags/gitscraping/">in the past</a>:</p>
<ul>
<li>
<a href="https://simonwillison.net/2017/Sep/10/scraping-irma/">Scraping hurricane Irma</a> back in September 2017 is when I first came up with the idea to use a Git repository in this way.</li>
<li>
<a href="https://simonwillison.net/2017/Oct/10/fires-in-the-north-bay/">Changelogs to help understand the fires in the North Bay</a> from October 2017 describes an early attempt at scraping fire-related information.</li>
<li>
<a href="https://simonwillison.net/2019/Mar/13/tree-history/">Generating a commit log for San Francisco’s official list of trees</a> remains my favourite application of this technique. The City of San Francisco maintains a frequently updated CSV file of 190,000 trees in the city, and I have <a href="https://github.com/simonw/sf-tree-history/find/master">a commit log</a> of changes to it stretching back over more than a year. This example uses my <a href="https://github.com/simonw/csv-diff">csv-diff</a> utility to generate human-readable commit messages.</li>
<li>
<a href="https://simonwillison.net/2019/Oct/10/pge-outages/">Tracking PG&amp;E outages by scraping to a git repo</a> documents my attempts to track the impact of PG&amp;E’s outages last year by scraping their outage map. I used the GitPython library to turn the values recorded in the commit history into a database that let me run visualizations of changes over time.</li>
<li>
<a href="https://simonwillison.net/2020/Jan/21/github-actions-cloud-run/">Tracking FARA by deploying a data API using GitHub Actions and Cloud Run</a> shows how I track new registrations for the US Foreign Agents Registration Act (FARA) in a repository and deploy the latest version of the data using Datasette.</li>
</ul>
<p>I hope that by giving this technique a name I can encourage more people to add it to their toolbox. It’s an extremely effective way of turning all sorts of interesting data sources into a changelog over time.</p>

<p><a href="https://news.ycombinator.com/item?id=24732943">Comment thread</a> on this post over on Hacker News.</p>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Llama 2 on ONNX runs locally (167 pts)]]></title>
            <link>https://github.com/microsoft/Llama-2-Onnx</link>
            <guid>37082117</guid>
            <pubDate>Thu, 10 Aug 2023 21:37:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/microsoft/Llama-2-Onnx">https://github.com/microsoft/Llama-2-Onnx</a>, See on <a href="https://news.ycombinator.com/item?id=37082117">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto"><strong>Llama 2 Powered By ONNX</strong></h2>
<p dir="auto"><strong>This is an optimized version of the Llama 2 model, available from Meta under the Llama Community License Agreement found on this repository. Microsoft permits you to use, modify, redistribute and create derivatives of Microsoft's contributions to the optimized version subject to the restrictions and disclaimers of warranty and liability in the Llama Community License agreement.</strong></p>
<h2 tabindex="-1" dir="auto"><strong>Before You Start</strong></h2>
<p dir="auto">The sub-modules that contain the ONNX files in this repository are access controlled.
To get access permissions to the Llama 2 model, please fill out the <a href="https://forms.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR1sq8AbaR35DlqQqW8HAxY1UQlU4UThHTlFWVUUwMzBXV1gxWENRTjRHRi4u" rel="nofollow">Llama 2 access request form</a>. If allowable, you will receive GitHub access in the next 48 hours, but usually much sooner.</p>
<h2 tabindex="-1" dir="auto"><strong>Cloning This Repository And The Submodules</strong></h2>
<p dir="auto">Chose from the following sub-modules:</p>
<ul dir="auto">
<li>7B_FT_float16</li>
<li>7B_FT_float32</li>
<li>7B_float16</li>
<li>7B_float32</li>
<li>13B_FT_float16</li>
<li>13B_FT_float32</li>
<li>13B_float16</li>
<li>13B_float32</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/microsoft/Llama-2-Onnx.git
cd Llama-2-Onnx
git submodule init <chosen_submodule> 
git submodule update"><pre>git clone https://github.com/microsoft/Llama-2-Onnx.git
<span>cd</span> Llama-2-Onnx
git submodule init <span>&lt;</span>chosen_submodule<span>&gt;</span> 
git submodule update</pre></div>
<p dir="auto">You can repeate the init command with a different submodule name to initialize multiple submodules. Be careful, the contained files are very large! (7B Float16 models are about 10GB)</p>
<h2 tabindex="-1" dir="auto"><strong>What is Llama 2?</strong></h2>
<p dir="auto">Llama 2 is a collection of pretrained and fine-tuned generative text models. To learn more about Llama 2, review the <a href="https://github.com/microsoft/Llama-2-Onnx/blob/main/MODEL-CARD-META-LLAMA-2.md">Llama 2 model card</a>.</p>
<h2 tabindex="-1" dir="auto"><strong>What Is The Structure Of Llama 2?</strong></h2>
<p dir="auto">Llama 2 model consists of a stack of decoder layers. Each decoder layer (or transformer block) is constructed from one self-attention layer and one feed-forward multi-layer perceptron. Llama models use different projection sizes compared with classic transformers in the feed-forward layer, for instance, both Llama 1 and Llama 2 projection use 2.7x hidden size rather than the standard 4x hidden size. A key difference between Llama 1 and Llama 2 is the architectural change of attention layer, in which Llama 2 takes advantage of Grouped Query Attention (GQA) mechanism to improve efficiency.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/Llama-2-Onnx/blob/main/Images/Llama2Model.png"><img src="https://github.com/microsoft/Llama-2-Onnx/raw/main/Images/Llama2Model.png" alt="Llama 2 Model"></a></p>
<h2 tabindex="-1" dir="auto"><strong>FAQ</strong></h2>
<h2 tabindex="-1" dir="auto"><strong>Is There A Simple Code Example Running Llama 2 With ONNX?</strong></h2>
<p dir="auto">There are two examples provided in this repository. There is a minimum working example shown in <a href="https://github.com/microsoft/Llama-2-Onnx/blob/main/MinimumExample/Example.md">Llama-2-Onnx/MinimumExample</a>. This is simply a command line program that will complete some text with the chosen version of Llama 2.</p>
<p dir="auto">Given the following input:</p>

<div dir="auto" data-snippet-clipboard-copy-content="python MinimumExample/Example_ONNX_LlamaV2.py --onnx_file 7B_FT_float16/ONNX/LlamaV2_7B_FT_float16.onnx --embedding_file 7B_FT_float16/embeddings.pth --tokenizer_path tokenizer.model --prompt &quot;What is the lightest element?&quot;"><pre>python MinimumExample/Example_ONNX_LlamaV2.py --onnx_file 7B_FT_float16/ONNX/LlamaV2_7B_FT_float16.onnx --embedding_file 7B_FT_float16/embeddings.pth --tokenizer_path tokenizer.model --prompt <span><span>"</span>What is the lightest element?<span>"</span></span></pre></div>
<p dir="auto">Output:</p>
<div dir="auto" data-snippet-clipboard-copy-content="The lightest element is hydrogen. Hydrogen is the lightest element on the periodic table, with an atomic mass of 1.00794 u (unified atomic mass units)."><pre>The lightest element is hydrogen. Hydrogen is the lightest element on the periodic table, with an atomic mass of 1.00794 u (unified atomic mass units).</pre></div>
<h2 tabindex="-1" dir="auto"><strong>Is There A More Complete Code Example Running Llama 2 With ONNX?</strong></h2>
<p dir="auto">There is a more complete chat bot interface that is available in <a href="https://github.com/microsoft/Llama-2-Onnx/blob/main/ChatApp/ChatApp.md">Llama-2-Onnx/ChatApp</a>. This is a python program based on the popular Gradio web interface. It will allow you to interact with the chosen version of Llama 2 in a chat bot interface.</p>
<p dir="auto">An example interaction can be seen here:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/Llama-2-Onnx/blob/main/Images/ChatAppExample.png"><img src="https://github.com/microsoft/Llama-2-Onnx/raw/main/Images/ChatAppExample.png" alt="Chat App"></a></p>
<h2 tabindex="-1" dir="auto"><strong>How Do I Use The Fine-tuned Models?</strong></h2>
<p dir="auto">The fine-tuned models were trained for dialogue applications.</p>
<p dir="auto">To get the expected features and performance for them, a specific formatting needs to be followed, including the <code>INST</code> tag, <code>BOS</code> and <code>EOS</code> tokens, and the whitespaces and breaklines in between (we recommend calling <code>strip()</code> on inputs to avoid double-spaces).</p>
<p dir="auto">This enables models in chat mode as well as additional safeguards  to reduce potentially undesirable output.</p>
<h2 tabindex="-1" dir="auto"><strong>Why Is The First Inference Session Slow?</strong></h2>
<p dir="auto">ONNX runtime execution provider might need to generate JIT binaries for the underlying hardware, typically the binary is cache and will be loaded directly in the subsequent runs to reduce the overhead.</p>
<h2 tabindex="-1" dir="auto"><strong>Why Is FP16 ONNX Slower Than ONNX FP32 On My Device?</strong></h2>
<p dir="auto">It is possible that your device does not support native FP16 math, therefore weights will be cast to FP32 at runtime. Using the FP32 version of the model will avoid the cast overhead.</p>
<h2 tabindex="-1" dir="auto"><strong>How Do I Get Better Inference Speed?</strong></h2>
<p dir="auto">It is recommended that inputs/outputs are put on target device to avoid expensive data copies, please refer to the following document for details.</p>
<p dir="auto"><a href="https://onnxruntime.ai/docs/performance/tune-performance/iobinding.html" rel="nofollow">I/O Binding | onnxruntime</a></p>
<h2 tabindex="-1" dir="auto"><strong>What Parameters Should I Test With?</strong></h2>
<p dir="auto">Users can perform temperature and top-p sampling using the model’s output logits. Please refer to Meta’s guidance for the best parameters combination; an example is located <a href="https://github.com/facebookresearch/llama/">here.</a></p>
<h2 tabindex="-1" dir="auto"><strong>How Can I Develop With Llama 2 Responsibly?</strong></h2>
<p dir="auto">In order to help developers innovate responsibly, Meta encourages you to review the <a href="https://ai.meta.com/llama/responsible-use-guide/" rel="nofollow">Responsible Use Guide</a> for the Llama 2 models.</p>
<p dir="auto">Microsoft encourages you to learn more about its <a href="https://aka.ms/rai" rel="nofollow">Responsible AI approach</a>, including many publicly available resources and tools for developers.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Modern C Development Environment (208 pts)]]></title>
            <link>https://interrupt.memfault.com/blog/a-modern-c-dev-env</link>
            <guid>37081833</guid>
            <pubDate>Thu, 10 Aug 2023 21:08:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://interrupt.memfault.com/blog/a-modern-c-dev-env">https://interrupt.memfault.com/blog/a-modern-c-dev-env</a>, See on <a href="https://news.ycombinator.com/item?id=37081833">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

        <p>Sometimes, <code>C/C++</code> projects have a long development cycle. When working on such a project, it can be easy to take our development environment for granted, and forget about the effort invested in its bring-up. The build environment works like <em>magic</em>, the test framework is neatly integrated, and the CI/CD pipeline relieves us of tedious, repetitive tasks.</p>

<p>For me, all it took was a simple thought: How do I best develop a <code>C</code> library, consisting of only a handful of files, but developed out of the context of my current, comfortable development environment? And there I was, back in the cold reality of <code>C/C++</code>, where you have the freedom - or obligation - to choose the entire development environment yourself.</p>

<!-- excerpt start -->
<p>In this article we’ll go over how to set up a containerized development environment for C projects. We’ll touch on setting up a build system using CMake, a testing environment using Unity, and even how to use our containerized environment in our CI pipeline!
<!-- excerpt end --></p>

<p>Maybe this little writeup helps the next person that is feeling a little overwhelmed when trying to start a new <code>C/C++</code> project from scratch, with their hands frozen over the keyboard, thinking about all those things that need to be done before even starting to write a single line of code…</p>

<h2 id="a-modern-development-environment">A “modern” development environment</h2>

<p>Throughout this article, we’ll set up a complete, containerized development environment for a <code>C</code> project:</p>

<ul>
  <li>We’ll create a <em>Docker</em> image that we can use as a development container with <a href="https://code.visualstudio.com/" target="_blank">vscode</a>
</li>
  <li>Based on a minimal <em>Dummy</em> library, we’ll set up the tools to build the library within the container.</li>
  <li>We’ll set up the static code analyzer <a href="https://clang.llvm.org/extra/clang-tidy/" target="_blank"><code>clang-tidy</code></a> to check our code for common mistakes.</li>
  <li>
<a href="https://clang.llvm.org/docs/ClangFormat.html" target="_blank"><code>clang-format</code></a> will help us to keep our code base nicely formatted and tidy.</li>
  <li>We’ll set up the <a href="https://www.throwtheswitch.org/unity" target="_blank">Unity</a>, executed on host via <a href="https://www.throwtheswitch.org/ceedling" target="_blank">Ceedling</a> to test our dummy function.</li>
  <li>And finally, we’ll set up a GitHub workflow to execute, build, and test our project using the same <em>Docker</em> image that we’re using locally.</li>
</ul>

<blockquote>
  <p><strong>Disclaimer:</strong> There are endless possibilities to set up environments and this is just one of them. Please be lenient in case some of my solutions are not ideal or could be solved differently.</p>
</blockquote>

<p>Throughout this article, I’ll make use of the <a href="https://www.docker.com/" target="_blank">Docker</a> command line interface. It is, however, out of the scope to discuss the basic concepts or the <em>Docker</em> command line parameters. In case you don’t understand why certain parameters are needed, I’d kindly ask you to refer to the online documentation.</p>

<p>The impatient reader can just skip ahead and open the <a href="https://github.com/lmapii/cproject" target="_blank">example project on GitHub</a>. If you’re happy to be walked through the steps, however, I’d be happy to guide you through the entire setup.</p>

<p>Like Interrupt? <a href="https://go.memfault.com/interrupt-subscribe" target="_blank">Subscribe</a> to get our latest posts straight to your mailbox.</p>

<div id="toc">

  <h2 id="table-of-contents">Table of Contents</h2>

  <!-- prettier-ignore -->
<ul id="markdown-toc">
  <li><a href="#a-modern-development-environment" id="markdown-toc-a-modern-development-environment">A “modern” development environment</a></li>
  <li>
<a href="#containers-for-development" id="markdown-toc-containers-for-development">Containers for development</a>    <ul>
      <li><a href="#why-docker-wheres-the-catch" id="markdown-toc-why-docker-wheres-the-catch">Why Docker? Where’s the catch?</a></li>
      <li><a href="#lets-do-this" id="markdown-toc-lets-do-this">Let’s do this</a></li>
      <li><a href="#a-shortcut-for-verbose-command-line-invocations" id="markdown-toc-a-shortcut-for-verbose-command-line-invocations">A shortcut for verbose command line invocations</a></li>
    </ul>
  </li>
  <li><a href="#visual-studio-code-dev-containers" id="markdown-toc-visual-studio-code-dev-containers">Visual Studio Code Dev Containers</a></li>
  <li><a href="#a-skeleton-subsystem" id="markdown-toc-a-skeleton-subsystem">A skeleton subsystem</a></li>
  <li>
<a href="#building-outside-of-vscode-and-mounting-volumes" id="markdown-toc-building-outside-of-vscode-and-mounting-volumes">Building outside of <code>vscode</code> and mounting volumes</a>    <ul>
      <li><a href="#a-note-on-docker-volumes" id="markdown-toc-a-note-on-docker-volumes">A note on Docker volumes</a></li>
      <li><a href="#running-the-development-container-manually" id="markdown-toc-running-the-development-container-manually">Running the development container manually</a></li>
    </ul>
  </li>
  <li>
<a href="#installing-clang-tools-for-formatting-and-static-code-analysis" id="markdown-toc-installing-clang-tools-for-formatting-and-static-code-analysis">Installing <code>clang</code> tools for formatting and static code analysis</a>    <ul>
      <li><a href="#installing-specific-in-the-builder-image" id="markdown-toc-installing-specific-in-the-builder-image">Installing specific in the builder image</a></li>
      <li><a href="#using-clang-format-in-the-development-container" id="markdown-toc-using-clang-format-in-the-development-container">Using <code>clang-format</code> in the Development Container</a></li>
      <li><a href="#wrapping-clang-format-and-clang-tidy-calls" id="markdown-toc-wrapping-clang-format-and-clang-tidy-calls">Wrapping <code>clang-format</code> and <code>clang-tidy</code> calls</a></li>
    </ul>
  </li>
  <li>
<a href="#adding-unit-tests" id="markdown-toc-adding-unit-tests">Adding unit tests</a>    <ul>
      <li><a href="#installing-unity-and-ceedling-in-the-builder-image" id="markdown-toc-installing-unity-and-ceedling-in-the-builder-image">Installing <code>Unity</code> and <code>Ceedling</code> in the builder image</a></li>
      <li><a href="#configuring-unity-and-running-unit-tests" id="markdown-toc-configuring-unity-and-running-unit-tests">Configuring <code>Unity</code> and running unit tests</a></li>
      <li><a href="#bonus-coverage-reports" id="markdown-toc-bonus-coverage-reports">Bonus: Coverage reports</a></li>
    </ul>
  </li>
  <li>
<a href="#getting-started-with-github-workflows" id="markdown-toc-getting-started-with-github-workflows">Getting started with GitHub workflows</a>    <ul>
      <li><a href="#adding-a-github-action" id="markdown-toc-adding-a-github-action">Adding a GitHub action</a></li>
      <li><a href="#building-an-image-in-a-github-action" id="markdown-toc-building-an-image-in-a-github-action">Building an image in a GitHub action</a></li>
      <li></li>
      <li><a href="#using-the-cached-image-to-build-the-project" id="markdown-toc-using-the-cached-image-to-build-the-project">Using the cached image to build the project</a></li>
      <li><a href="#running-the-tests-and-storing-the-coverage-report-as-an-artifact" id="markdown-toc-running-the-tests-and-storing-the-coverage-report-as-an-artifact">Running the tests and storing the coverage report as an artifact</a></li>
    </ul>
  </li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
</ul>

</div>

<h2 id="containers-for-development">Containers for development</h2>

<p>Working with embedded systems or C/C++ sometimes requires installing lots of specialized tools or compilers. If you are working on different projects at the same time, you’ll easily end up with conflicting versions. Therefore, whenever feasible, I tend to run everything within a <a href="https://www.docker.com/" target="_blank">Docker</a> <em>container</em>.</p>

<p>Another benefit is that you can also use your <em>Dockerfiles</em> as recipes in case you need to install tools locally. The real benefit, however, is that it makes it easy for anyone to join a project by using either a prebuilt image or by attempting to build the image locally. Also, your CI can use the same environment as you do!</p>

<h3 id="why-docker-wheres-the-catch">Why Docker? Where’s the catch?</h3>

<p>Don’t get me wrong, <em>Docker</em> is far from being perfect and I experienced plenty of issues personally, but it is a decent solution if you keep some limitations in mind. If you’re new to <em>Docker</em>, maybe consider the following:</p>

<ul>
  <li>
<em>Dockerfiles</em> are not stable. A <em>Dockerfile</em> that built just fine yesterday might fail to build today. There are simply too many external dependencies.</li>
  <li>
<em>Docker</em> is not platform-independent. Especially if you’re running a container on other CPU architectures, e.g., Apple ARM, you’ll notice that some things don’t run. We’ll see this later.</li>
  <li>Some <em>Docker</em> features are only supported in Linux or dedicated Windows containers. E.g., mounting a USB device into a Docker container is not supported on all platforms; a limitation known <a href="https://github.com/docker/for-mac/issues/900" target="_blank">since 2016</a>.</li>
  <li>
<em>Docker</em> - just like all the big companies out there - can suddenly deprecate even major features of their tools. I experienced this with <a href="https://github.com/docker/roadmap/issues/245" target="_blank">docker-machine</a>. It is always best not to bet 100% on one technology and you always need to be prepared to switch.</li>
  <li>
<em>Docker</em> is no longer entirely free to use. At the time of writing, <em>Docker</em> was still free to use for personal use and small businesses, but please make sure to check their latest <a href="https://docs.docker.com/subscription/desktop-license/" target="_blank">license agreement</a>.</li>
</ul>

<p>With all this in mind, <em>Docker</em> images are an elegant solution for development environments, especially if you have the chance to spin up your own registry. This sounds harder than it is since solutions already exist, e.g., <a href="https://cloud.google.com/" target="_blank">Google Cloud</a>, <a href="https://jfrog.com/artifactory/" target="_blank">JFrog Artifactory</a>, and other providers.</p>

<p>I have personally experienced the huge upside of this with my first dockerized environment. By pulling the images from a registry of a project that was parked for over four years, I had the development environment back up and running in under 10 minutes.</p>

<h3 id="lets-do-this">Let’s do this</h3>

<p>Let’s start from zero with an empty repository:</p>

<div><pre><code><span>mkdir </span>cproject
<span>cd </span>cproject
git init
</code></pre></div>

<p>Make sure you have <a href="https://www.docker.com/" target="_blank">Docker</a> installed and running. Create the following <a href="https://github.com/lmapii/cproject/blob/main/builder.Dockerfile" target="_blank"><code>builder.Dockerfile</code></a> in the root directory of the project:</p>

<div><pre><code><span>ARG</span><span> base_tag=bullseye</span>
<span>ARG</span><span> base_img=mcr.microsoft.com/vscode/devcontainers/base:dev-${base_tag}</span>

<span>FROM</span><span> --platform=linux/amd64 ${base_img} AS builder-install</span>

<span>RUN </span>apt-get update <span>--fix-missing</span> <span>&amp;&amp;</span> apt-get <span>-y</span> upgrade
<span>RUN </span>apt-get <span>install</span> <span>-y</span> <span>--no-install-recommends</span> <span>\
</span>    apt-utils <span>\
</span>    curl <span>\
</span>    cmake <span>\
</span>    build-essential <span>\
</span>    gcc <span>\
</span>    g++-multilib <span>\
</span>    locales <span>\
</span>    make <span>\
</span>    ruby <span>\
</span>    gcovr <span>\
</span>    wget <span>\
</span>    <span>&amp;&amp;</span> <span>rm</span> <span>-rf</span> /var/lib/apt/lists/<span>*</span>
</code></pre></div>

<blockquote>
  <p><strong>Note:</strong> Throughout this article, I’ll only show relevant parts of the <em>Dockerfile</em> and may omit some sections that you’ll find in the <a href="https://github.com/lmapii/cproject/blob/main/builder.Dockerfile" target="_blank"><em>Dockerfile</em> in the demo repository</a>.</p>
</blockquote>

<p>This <em>Dockerifle</em> specifies a base image and installs some packages that we’ll use in future steps. I won’t go into detail about each package: When creating your image you’ll soon notice if something is missing and you can just extend this list of packages. What is important is the following:</p>

<ul>
  <li>For the base image, I highly recommend using a specific <em>tag</em>. The <code>apt</code> packages vary wildly between base images, so choosing a tag buys you a bit more time until your <em>Dockerfiles</em> fail, e.g., when the <code>apt</code> registry changes.</li>
  <li>For development images, I tend to use a specific <strong>platform</strong>. This is important in later steps, where specific tools might not exist for the CPU architecture of your machine, e.g., if you’re using an Apple ARM-based computer.</li>
</ul>

<p>If you’re not using <a href="https://code.visualstudio.com/" target="_blank">vscode</a> you could also specify a different base image, e.g., <code>base_img=debian:${base_tag}</code>. In this article we’re spinning up a development environment with <code>vscode</code> and we’ll therefore stick to an image that is supported by its <a href="https://code.visualstudio.com/docs/devcontainers/containers" target="_blank">Dev Containers</a>.</p>

<p>The image is built with the following command, which might take a little time to execute:</p>

<div><pre><code>docker build <span>-f</span> builder.Dockerfile <span>-t</span> cproject-builder:latest <span>.</span>
</code></pre></div>

<blockquote>
  <p><strong>Note:</strong> Running this command again will execute much faster since all steps within a <em>Dockerfile</em> are cached! Only if, e.g., you add new packages to the list, the whole step will be re-executed.</p>
</blockquote>

<h3 id="a-shortcut-for-verbose-command-line-invocations">A shortcut for verbose command line invocations</h3>

<p>Docker commands are quite verbose, and they only tend to become even longer, which is why I typically park my most used commands in a <a href="https://github.com/lmapii/cproject/blob/main/makefile" target="_blank"><code>makefile</code></a> in the project root. Assuming you have <code>make</code> installed, you could do the following to make your life a little easier. Your future self will thank you for not having to remember all commands:</p>

<div><pre><code><span>project_name</span><span>=</span>cproject
<span>builder-build </span><span>:</span>
  <span>docker build -f builder.Dockerfile -t $(project_name)-builder</span><span>:</span><span>latest .</span>
</code></pre></div>

<p>Now, all you need to do is the following to rebuild your image:</p>



<blockquote>
  <p><strong>Note:</strong> A more elegant solution than <code>makefiles</code> is <a href="http://www.pyinvoke.org/" target="_blank">Invoke</a>, presented in a <a href="https://interrupt.memfault.com/blog/building-a-cli-for-firmware-projects">previous article</a>. Since I’m a dinosaur, however, and for the sake of simplicity, I’ll stick to <code>makefiles</code> in this article.</p>
</blockquote>

<p>Let’s spin up a container from our image and take it for a test drive:</p>

<div><pre><code><span>$ </span>docker run <span>--rm</span> <span>-it</span> <span>--platform</span> linux/amd64 cproject-builder:latest /bin/bash

<span>$ </span>gcc <span>--version</span>
gcc <span>(</span>Debian 10.2.1-6<span>)</span> 10.2.1 20210110
Copyright <span>(</span>C<span>)</span> 2020 Free Software Foundation, Inc.
This is free software<span>;</span> see the <span>source </span><span>for </span>copying conditions.  There is NO
warranty<span>;</span> not even <span>for </span>MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

<span>$ </span>cmake <span>--version</span>
cmake version 3.18.4
CMake suite maintained and supported by Kitware <span>(</span>kitware.com/cmake<span>)</span><span>.</span>

<span>$ </span><span>exit</span>
</code></pre></div>

<p>When using <code>apt</code>, the version of the tools that have been installed depends on the base image and therefore on the package registry. If you want to install specific versions, you can do this by executing custom <code>RUN</code> commands as we’ll see later.</p>

<h2 id="visual-studio-code-dev-containers">Visual Studio Code Dev Containers</h2>

<p>The downside of <strong>not</strong> having all tools installed locally is that those tools can not be leveraged by your IDE of choice. E.g., when using <code>vscode</code> you won’t be able to properly set up intellisense or any other helpers if you don’t have any compiler installed.</p>

<p><code>vscode</code> allows you to run an instance of the editor within a so-called <em>development container</em>. This is also the reason why we chose the <code>mcr.microsoft.com/vscode/devcontainers/base</code> image as the base image: We can connect <code>vscode</code> within our builder container and therefore have all the tools installed in our <em>Docker</em> image available! Notice, however, that this <code>vscode</code> instance does <strong>not</strong> match your local <code>vscode</code> installation. This <code>vscode</code> instance is created from scratch and works very similar to remote instances: E.g., you need to explicitly install all the extensions and provide all the settings that you have already set up in your local instance. Have a look at the <a href="https://code.visualstudio.com/docs/devcontainers/tutorial" target="_blank">tutorial</a> for the exact steps or in case anything fails whilst following along this article. For now, I assume that you have <code>vscode</code> and the <a href="https://code.visualstudio.com/docs/devcontainers/containers" target="_blank">Dev Containers</a> extension installed.</p>

<p>By creating the <a href="https://github.com/lmapii/cproject/blob/main/.devcontainer/devcontainer.json" target="_blank"><code>.devcontainer/devcontainer.json</code></a> file, we can tell <code>vscode</code> to use our newly built image for its development container. We’re also installing three extensions within this <code>vscode</code> instance by using the <code>customizations.extensions</code> field in the <code>devcontainer.json</code> configuration file:</p>

<div><pre><code><span>{</span><span>
  </span><span>"name"</span><span>:</span><span> </span><span>"C"</span><span>,</span><span>
  </span><span>"build"</span><span>:</span><span> </span><span>{</span><span>
    </span><span>"dockerfile"</span><span>:</span><span> </span><span>"../builder.Dockerfile"</span><span>
  </span><span>},</span><span>
  </span><span>"runArgs"</span><span>:</span><span> </span><span>[</span><span>"--platform=linux/amd64"</span><span>],</span><span>
  </span><span>"customizations"</span><span>:</span><span> </span><span>{</span><span>
    </span><span>"vscode"</span><span>:</span><span> </span><span>{</span><span>
      </span><span>"settings"</span><span>:</span><span> </span><span>{</span><span>
        </span><span>"terminal.integrated.defaultProfile.linux"</span><span>:</span><span> </span><span>"bash"</span><span>,</span><span>
        </span><span>"extensions.verifySignature"</span><span>:</span><span> </span><span>false</span><span>
      </span><span>},</span><span>
      </span><span>"extensions"</span><span>:</span><span> </span><span>[</span><span>"ms-vscode.cpptools"</span><span>,</span><span> </span><span>"ms-vscode.cmake-tools"</span><span>,</span><span> </span><span>"cschlosser.doxdocgen"</span><span>]</span><span>
    </span><span>}</span><span>
  </span><span>}</span><span>
</span><span>}</span><span>
</span></code></pre></div>

<p>If you now reload your window or reopen <code>vscode</code> (using the <code>cproject</code> folder as the root folder!) <code>vscode</code> should now ask you if it should use the detected development container.</p>

<p><img src="https://interrupt.memfault.com/img/c-dev-environment/vscode-reload.png" alt=""></p>

<p>It’ll take a while for <code>vscode</code> to set up your container and install the extensions, but in the end you should have <code>vscode</code> connected to your Linux container, with all previous tools installed.</p>

<p><img src="https://interrupt.memfault.com/img/c-dev-environment/vscode-remote.png" alt=""></p>

<h2 id="a-skeleton-subsystem">A skeleton subsystem</h2>

<p>For this article, we’ll create a single <code>.c</code> and <code>.h</code> pair, built using <a href="https://cmake.org/" target="_blank">cmake</a>, with a single random number generator function, as follows:</p>

<div><pre><code><span>$ </span>tree <span>--charset</span><span>=</span>utf-8 <span>--dirsfirst</span>
<span>.</span>
├── include
│   └── dummy
│       └── dummy.h
├── src
│   └── dummy.c
├── CMakeLists.txt
├── builder.Dockerfile
└── makefile &lt;- this is just our makefile containing the docker commands
</code></pre></div>

<div><pre><code><span>#include "dummy/dummy.h"
</span><span>uint8_t</span> <span>dummy_random</span><span>(</span><span>void</span><span>)</span>
<span>{</span>
    <span>return</span> <span>4U</span><span>;</span> <span>// determined by fair dice roll.</span>
<span>}</span>
</code></pre></div>

<p>I’ll spare you the details, the <code>CMakeLists.txt</code> simply defines a library named “Dummy” and adds the corresponding files to the library. Have a look at the <a href="https://github.com/lmapii/cproject" target="_blank">example project on GitHub</a> and its <a href="https://github.com/lmapii/cproject/blob/main/CMakeLists.txt" target="_blank"><code>CMakeLists.txt</code></a> file. What’s important is: This already builds within our development container and <code>vscode</code>! Open the integrated terminal in your remote instance and execute <code>cmake</code> as follows.</p>

<div><pre><code><span>$ </span>cmake <span>-B</span> build
<span>--</span> The C compiler identification is GNU 10.2.1
<span>--</span> Detecting C compiler ABI info
<span>--</span> Detecting C compiler ABI info - <span>done</span>
<span>--</span> Check <span>for </span>working C compiler: /usr/bin/cc - skipped
<span>--</span> Detecting C compile features
<span>--</span> Detecting C compile features - <span>done</span>
<span>--</span> Configuring <span>done</span>
<span>--</span> Generating <span>done</span>
<span>--</span> Build files have been written to: /workspaces/cproject/build
<span>$ </span>cmake <span>--build</span> build
Scanning dependencies of target Dummy
<span>[</span> 50%] Building C object CMakeFiles/Dummy.dir/src/dummy.c.o
<span>[</span>100%] Linking C static library libDummy.a
<span>[</span>100%] Built target Dummy
</code></pre></div>

<h2 id="building-outside-of-vscode-and-mounting-volumes">Building outside of <code>vscode</code> and mounting volumes</h2>

<p>Not everyone is a friend of <code>vscode</code>. All the magic happening behind the scenes in an IDE is not everyone’s favorite flavor and it is always good to know how to handle things without an IDE, so I’d like to provide an alternative setup that will also be important once we deal with the GitHub workflow, where we simply have to step out of the comforts of an IDE.</p>

<p>One such magic step that <code>vscode</code> does for you - and that we’ll now need to do ourselves - is managing the files within the root container. We’ll use a <a href="https://docs.docker.com/storage/volumes" target="_blank">volume</a> to make all files available within the container. At the same time, this makes all modifications that happen <em>inside</em> the container also visible <em>outside</em> of it. We update our <em>Dockerfile</em> with the following commands:</p>

<div><pre><code><span>VOLUME</span><span> ["/builder/mnt"]</span>
<span>WORKDIR</span><span> /builder/mnt</span>
</code></pre></div>

<p>This defines a volume that we can later use to mount our project directory when executing <code>docker run</code>, and the <code>workdir</code> instruction tells <em>Docker</em> to make this the default path for all future steps. Don’t forget to rebuild the image, otherwise, the changes will not be available!</p>



<h3 id="a-note-on-docker-volumes">A note on Docker volumes</h3>

<p>As mentioned in the introduction, <em>Docker</em> does not behave the same way on all platforms. Only on Linux or when using Windows containers on Windows, containers run “natively” and thus without major penalties. If you’re running a Linux container on macOS or Windows - in very simple words - they execute within a VM (though admittedly it is a bit more complicated than that).</p>

<p>The key takeaway is that since the container is executed in a VM, the I/O performance is significantly worse compared to a container that is run natively. For compiled languages or for any process that creates a lot of files, this impact can be significant since the overhead can be up to 100x of what you’re experiencing natively. This can lead to longer build or generation times.</p>

<p>Thankfully, <em>Docker</em> has provided a good solution for macOS with the <code>VirtioFS</code> file-sharing implementation. All you need to do is enable it in your <em>Docker</em> configuration:</p>

<p><img src="https://interrupt.memfault.com/img/c-dev-environment/docker-sharing.png" alt=""></p>

<p>For Windows, at the time of writing, I don’t know of a similar solution so you might have to dig a little deeper in case you run into performance problems. Personally, I’ve successfully used <a href="https://docker-sync.io/" target="_blank">docker-sync</a> before, but it is a bit harder to handle.</p>

<h3 id="running-the-development-container-manually">Running the development container manually</h3>

<p>Now we can run the following command to spin up the container with our project folder mounted as a volume:</p>

<div><pre><code><span>$ </span>docker run <span>\</span>
    <span>--rm</span> <span>\</span>
    <span>-it</span> <span>\</span>
    <span>--platform</span> linux/amd64 <span>\</span>
    <span>--workdir</span> /builder/mnt <span>\</span>
    <span>-v</span> .:/builder/mnt <span>\</span>
    cproject-builder:latest <span>\</span>
    /bin/bash
</code></pre></div>

<p>The current directory “<code>.</code>” is now available within the container as <code>/builder/mnt</code>. And - just like in our development container - we’re able to build the project:</p>

<div><pre><code><span>$ </span><span>rm</span> <span>-rf</span> build

<span>$ </span>cmake <span>-B</span> build
<span>--</span> The C compiler identification is GNU 10.2.1
<span>--</span> Detecting C compiler ABI info
<span>--</span> Detecting C compiler ABI info - <span>done</span>
<span>--</span> Check <span>for </span>working C compiler: /usr/bin/cc - skipped
<span>--</span> Detecting C compile features
<span>--</span> Detecting C compile features - <span>done</span>
<span>--</span> Configuring <span>done</span>
<span>--</span> Generating <span>done</span>
<span>--</span> Build files have been written to: /builder/mnt/build

<span>$ </span>cmake <span>--build</span> build
Scanning dependencies of target Dummy
<span>[</span> 50%] Building C object CMakeFiles/Dummy.dir/src/dummy.c.o
<span>[</span>100%] Linking C static library libDummy.a
<span>[</span>100%] Built target Dummy
</code></pre></div>

<p>Since the command to spin up the container is rather verbose, I tend to also use a <code>make</code> target in my <code>makefile</code> for that, such that all I need to type is <code>make builder-run</code>:</p>

<div><pre><code><span>builder-run </span><span>:</span>
  <span>docker</span> <span>run</span> <span>\</span>
    <span>--rm</span> <span>\</span>
    <span>-it</span> <span>\</span>
    <span>--platform</span> <span>linux/amd64</span> <span>\</span>
    <span>--workdir</span> <span>/builder/mnt</span> <span>\</span>
    <span>-v .</span><span>:</span><span>/builder/mnt </span>\
<span>    $(project_name)-builder:latest </span>\
<span>    /bin/bash</span>
</code></pre></div>



<p>The flexibility of C and C++ comes with plenty of massive footguns so, I try to add at least a minimal static code analysis to my project. This helps to catch the most obvious mistakes and gives some extra confidence in the codebase. There are plenty of tools out there, but my personal preference so far is <a href="https://clang.llvm.org/extra/clang-tidy/" target="_blank"><code>clang-tidy</code></a>.</p>

<p>Also, when collaborating on a codebase, a formatter is always a nice thing to have, and since we’re installing <code>clang-tidy</code> anyhow, we might as well go ahead and install <a href="https://clang.llvm.org/docs/ClangFormat.html" target="_blank"><code>clang-format</code></a>.</p>

<p>The two configuration files <a href="https://github.com/lmapii/cproject/blob/main/.clang-format" target="_blank"><code>.clang-format</code></a> and <a href="https://github.com/lmapii/cproject/blob/main/.clang-tidy" target="_blank"><code>.clang-tidy</code></a> are placed into the root directory of the project such that any IDE picks them up automatically. I won’t go into detail about the options of the two tools and will just assume that the reader follows the previous links for more details or copies the files from the <a href="https://github.com/lmapii/cproject" target="_blank">example project on GitHub</a>. So this is what we have now:</p>

<div><pre><code><span>$ </span>tree <span>--charset</span><span>=</span>utf-8 <span>--dirsfirst</span> <span>-a</span> <span>-I</span> build <span>-I</span> .git <span>-L</span> 1
<span>.</span>
├── .devcontainer
├── include
├── src
├── .clang-format
├── .clang-tidy
├── CMakeLists.txt
├── builder.Dockerfile
└── makefile
</code></pre></div>

<h3 id="installing-specific-in-the-builder-image">Installing specific in the builder image</h3>

<p>Here’s a catch with <code>clang-tidy</code> and <code>clang-format</code>: The configuration files must match the tool version, they are not necessarily backward compatible. It is therefore important to install compatible versions for your configuration files. Since we have a container we’ll always be using the correct version! The installation, however, can be tricky. Why? Running <code>apt-get</code> to install the tools does not always work since the package registries typically contain ancient tool versions. One way around this is to install them manually in our <em>Dockerfile</em>:</p>

<div><pre><code><span>ARG</span><span> base_tag=bullseye</span>
<span>ARG</span><span> llvm_version=16</span>
<span>RUN </span>apt-get update <span>--fix-missing</span> <span>&amp;&amp;</span> apt-get <span>-y</span> upgrade
<span>RUN </span>apt-get <span>install</span> <span>-y</span> <span>--no-install-recommends</span> <span>\
</span>    gnupg2 <span>\
</span>    gnupg-agent <span>\
</span>    ca-certificates <span>\
</span>    <span>&amp;&amp;</span> <span>rm</span> <span>-rf</span> /var/lib/apt/lists/<span>*</span>

<span>RUN </span>curl <span>--fail</span> <span>--silent</span> <span>--show-error</span> <span>--location</span> https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add -
<span>RUN </span><span>echo</span> <span>"deb http://apt.llvm.org/</span><span>$base_tag</span><span>/ llvm-toolchain-</span><span>$base_tag</span><span>-</span><span>$llvm_version</span><span> main"</span> <span>&gt;&gt;</span> /etc/apt/sources.list.d/llvm.list

<span>RUN </span>apt-get update <span>--fix-missing</span> <span>&amp;&amp;</span> apt-get <span>-y</span> upgrade
<span>RUN </span>apt-get <span>install</span> <span>-y</span> <span>--no-install-recommends</span> <span>\
</span>    clang-format-<span>${</span><span>llvm_version</span><span>}</span> <span>\
</span>    clang-tidy-<span>${</span><span>llvm_version</span><span>}</span> <span>\
</span>    <span>&amp;&amp;</span> <span>rm</span> <span>-rf</span> /var/lib/apt/lists/<span>*</span>

<span>RUN </span><span>ln</span> <span>-s</span> /usr/bin/clang-format-<span>${</span><span>llvm_version</span><span>}</span> /usr/local/bin/clang-format
<span>RUN </span><span>ln</span> <span>-s</span> /usr/bin/clang-tidy-<span>${</span><span>llvm_version</span><span>}</span> /usr/local/bin/clang-tidy
</code></pre></div>

<p>This is one of the steps in a <em>Dockerfile</em> that I’ve previously mentioned which can break quite easily when you’re trying to rebuild an image in a couple of years. In case you’re happy with the packages provided for your base images, you can also simply install the packages that are provided with the <code>apt</code> registry. Now both <code>clang-format</code> and <code>clang-tidy</code> are available in the image:</p>

<div><pre><code><span>$ </span>make builder-build
<span>$ </span>make builder-run

<span>$ </span>clang-format <span>--version</span>
Debian clang-format version 16.0.6 <span>(</span>++20230704084212+7cbf1a259152-1~exp1~20230704204302.101<span>)</span>

<span>$ </span>clang-tidy <span>--version</span>
Debian LLVM version 16.0.6
  Optimized build.
</code></pre></div>

<h3 id="using-clang-format-in-the-development-container">Using <code>clang-format</code> in the Development Container</h3>

<p><code>vscode</code> typically needs to be instructed to rebuild the development container from the <em>Dockerfile</em>. You can do so explicitly using the command prompt:</p>

<p><img src="https://interrupt.memfault.com/img/c-dev-environment/vscode-rebuild.png" alt=""></p>

<p>Now you have <code>clang-tidy</code> and <code>clang-format</code> available within your development container. You can now also instruct <code>vscode</code> to automatically format your files on save. For this, we create a workspace configuration <a href="https://github.com/lmapii/cproject/blob/main/.vscode/settings.json" target="_blank"><code>.vscode/settings.json</code></a> file:</p>

<div><pre><code><span>{</span><span>
  </span><span>"[c]"</span><span>:</span><span> </span><span>{</span><span>
    </span><span>"editor.formatOnSave"</span><span>:</span><span> </span><span>true</span><span>
  </span><span>},</span><span>
  </span><span>"[cpp]"</span><span>:</span><span> </span><span>{</span><span>
    </span><span>"editor.formatOnSave"</span><span>:</span><span> </span><span>true</span><span>
  </span><span>},</span><span>
  </span><span>"C_Cpp.default.compileCommands"</span><span>:</span><span> </span><span>"${workspaceFolder}/src/build/compile_commands.json"</span><span>,</span><span>
  </span><span>"cmake.configureOnOpen"</span><span>:</span><span> </span><span>true</span><span>
</span><span>}</span><span>
</span></code></pre></div>

<p>I’ve hidden a little detail here: <code>clang-tidy</code> operates on the <code>compile_commands.json</code> file that can be created when using <code>cmake</code> as a build system. I’ve enabled this option within the <a href="https://github.com/lmapii/cproject/blob/main/CMakeLists.txt" target="_blank"><code>CMakeLists.txt</code></a> file. At the same time, we’re allowing the <code>cmake</code> extension for <code>vscode</code> to configure the project when opening and therefore have all the fancy buttons and actions available to build the project in case you don’t like the command line. Personally, I still fall back to the command line. But then, I’m a dinosaur.</p>

<p>Now, if you open the <code>dummy.c</code> file in your editor it will automatically format it according to the rules in your <code>.clang-format</code> file whenever you save the file. But what if someone in your team doesn’t use <code>vscode</code> or forgot to enable the format-on-save feature?</p>

<h3 id="wrapping-clang-format-and-clang-tidy-calls">Wrapping <code>clang-format</code> and <code>clang-tidy</code> calls</h3>

<p>This section is a little self-promotion of two wrapper scripts for <code>clang-format</code> and <code>clang-tidy</code> that I am maintaining: <a href="https://github.com/lmapii/run-clang-tidy" target="_blank">run-clang-tidy</a> and <a href="https://github.com/lmapii/run-clang-format" target="_blank">run-clang-format</a>. Both tools are command line tools written in <a href="https://www.rust-lang.org/" target="_blank">Rust</a> that simplify and parallelize the execution of <code>clang-format</code> and <code>clang-tidy</code> from the command line.</p>

<p>In our <code>builder.dockerfile</code> we’ll now install the released, architecture-specific versions from the <em>GitHub</em> repositories of <a href="https://github.com/lmapii/run-clang-tidy" target="_blank">run-clang-tidy</a> and <a href="https://github.com/lmapii/run-clang-format" target="_blank">run-clang-format</a>.</p>

<div><pre><code><span>RUN </span><span>mkdir</span> <span>-p</span> /usr/local/run-clang-format
<span>RUN </span>wget <span>-O</span> clang-utils.tgz <span>"https://github.com/lmapii/run-clang-format/releases/download/v1.4.10/run-clang-format-v1.4.10-i686-unknown-linux-gnu.tar.gz"</span> <span>&amp;&amp;</span> <span>\
</span>    <span>tar</span> <span>-C</span> /usr/local/run-clang-format <span>-xzf</span> clang-utils.tgz <span>--strip-components</span> 1 <span>&amp;&amp;</span> <span>\
</span>    <span>rm </span>clang-utils.tgz

<span>ENV</span><span> PATH /usr/local/run-clang-format:$PATH</span>
<span>RUN </span>run-clang-format <span>--version</span>

<span>RUN </span><span>mkdir</span> <span>-p</span> /usr/local/run-clang-tidy
<span>RUN </span>wget <span>-O</span> clang-utils.tgz <span>"https://github.com/lmapii/run-clang-tidy/releases/download/v0.2.1/run-clang-tidy-v0.2.1-i686-unknown-linux-gnu.tar.gz"</span> <span>&amp;&amp;</span> <span>\
</span>    <span>tar</span> <span>-C</span> /usr/local/run-clang-tidy <span>-xzf</span> clang-utils.tgz <span>--strip-components</span> 1 <span>&amp;&amp;</span> <span>\
</span>    <span>rm </span>clang-utils.tgz

<span>ENV</span><span> PATH /usr/local/run-clang-tidy:$PATH</span>
<span>RUN </span>run-clang-format <span>--version</span>
</code></pre></div>

<p>Instead of just downloading the pre-built binaries, it is also possible to use the below steps to install <em>Rust</em> and its <a href="https://github.com/rust-lang/cargo" target="_blank">package manager <code>cargo</code></a> using the <a href="https://rustup.rs/" target="_blank">Rust toolchain installer <code>rustup</code></a>, and then install the two tools by running <code>cargo install</code>:</p>

<div><pre><code><span>RUN </span>curl https://sh.rustup.rs <span>-sSf</span> | sh <span>-s</span> <span>--</span> <span>--default-toolchain</span> stable <span>-y</span>
<span>ENV</span><span> PATH=/root/.cargo/bin:$PATH</span>

<span>RUN </span>cargo <span>install </span>run-clang-format
<span>RUN </span>cargo <span>install </span>run-clang-tidy
</code></pre></div>

<p>While this approach is less verbose, it significantly increases the image size and image build time: First of all, this installs the full Rust toolchain, which in my tests increased the image size by almost <em>1 GB</em>, and second the <code>cargo install</code> commands compile both tools and all of its dependencies from scratch, which - on my machine - took almost 4 minutes for each. When creating such builder images, it is therefore always important to find the right fit for your need: If you won’t be developing using <em>Rust</em> you might be better off <em>not</em> installing it in your builder. This way, you keep build times and the image size as low as possible - no matter how awesome <a href="https://www.rust-lang.org/" target="_blank">Rust</a> is.</p>

<p>But let’s keep on working on our development environment. Why do we need <a href="https://github.com/lmapii/run-clang-tidy" target="_blank">run-clang-tidy</a> and <a href="https://github.com/lmapii/run-clang-format" target="_blank">run-clang-format</a>? These wrapper scripts allow to define the execution of <code>clang-format</code> and <code>clang-tidy</code> based on <code>.json</code> input files. Within these files, it is possible to efficiently filter all files, e.g., third-party software for which the format-on-save would be more cumbersome than helpful. Since we’ll be adding unit tests later, we can prepare the configurations already:</p>

<p>For formatting, we simply need to provide the paths to the source files and filters for files that we want to exclude from formatting or from the glob expansion:</p>

<div><pre><code><span>{</span><span>
  </span><span>"paths"</span><span>:</span><span> </span><span>[</span><span>"./src/**/*.[ch]"</span><span>,</span><span> </span><span>"./include/**/*.[h]"</span><span>,</span><span> </span><span>"./tests/unittest/test/*.[ch]"</span><span>],</span><span>
  </span><span>"filterPre"</span><span>:</span><span> </span><span>[</span><span>".*"</span><span>],</span><span>
  </span><span>"filterPost"</span><span>:</span><span> </span><span>[</span><span>"./src/build/**"</span><span>,</span><span> </span><span>"./tests/unittest/build/**"</span><span>,</span><span> </span><span>"./tests/unittest/generated"</span><span>]</span><span>
</span><span>}</span><span>
</span></code></pre></div>

<p>For the static analysis, we also need to tell the tool where to find the <code>compile_commands.json</code>:</p>

<div><pre><code><span>{</span><span>
  </span><span>"paths"</span><span>:</span><span> </span><span>[</span><span>"./src/**/*.[ch]"</span><span>,</span><span> </span><span>"./include/**/*.[h]"</span><span>],</span><span>
  </span><span>"filterPre"</span><span>:</span><span> </span><span>[</span><span>".*"</span><span>],</span><span>
  </span><span>"filterPost"</span><span>:</span><span> </span><span>[</span><span>"./build/**"</span><span>],</span><span>
  </span><span>"buildRoot"</span><span>:</span><span> </span><span>"./build"</span><span>
</span><span>}</span><span>
</span></code></pre></div>

<p>Now we just need to update our image and run the commands. Whether you do this by spinning up your container or by rebuilding the <code>vscode</code> development container is up to you, it should run in both environments:</p>

<div><pre><code><span>$ </span>make builder-build
<span>$ </span>make builder-run

<span>$ </span>run-clang-format clang-format.json
 <span>[</span> 1/5 <span>]</span> No style file specified, assuming .clang-format exists <span>in </span>the project tree
 <span>[</span> 2/5 <span>]</span> Found 2 files <span>for </span>the provided path patterns
 <span>[</span> 3/5 <span>]</span> Found clang-format version 16.0.6 using <span>command </span>clang-format
 <span>[</span> 4/5 <span>]</span> Executing clang-format ...

  Formatting /builder/mnt/src/dummy.c
  Formatting /builder/mnt/include/dummy/dummy.h
     Running <span>[==========================]</span> 2/2
     Finished <span>in </span>0 seconds

<span>$ </span>run-clang-tidy clang-tidy.json
 <span>[</span> 1/6 <span>]</span> No tidy file specified, assuming .clang-tidy exists <span>in </span>the project tree
 <span>[</span> 2/6 <span>]</span> Using build root /workspaces/cproject/build
 <span>[</span> 3/6 <span>]</span> Found 1 files <span>for </span>the provided path patterns
 <span>[</span> 4/6 <span>]</span> Found clang-tidy version 16.0.6 using <span>command </span>clang-tidy
 <span>[</span> 5/6 <span>]</span> Executing clang-tidy ...

          Ok /workspaces/cproject/src/dummy.c
     Running <span>[==========================]</span> 1/1
     Finished <span>in </span>1 second
</code></pre></div>

<blockquote>
  <p><strong>Note:</strong> Instead of running <code>cmake --build build</code> every time you modify your files, you can now also simply execute the static code analysis instead. It also compiles your files, but at the same time points out some of the mistakes that you have enabled.</p>
</blockquote>

<h2 id="adding-unit-tests">Adding unit tests</h2>

<p>We’re already able to build our library, analyze it and have a formatter in place. One more thing that we need in our development environment is a unit test framework.</p>

<p>For this article, I’ve chosen the <a href="https://www.throwtheswitch.org/unity" target="_blank">Unity</a> test framework, executed on the host via <a href="https://www.throwtheswitch.org/ceedling" target="_blank">Ceedling</a>. I’m using this framework since it is quite easy to extend the integration to also execute tests on an actual target hardware in case you’re developing an embedded system.</p>

<h3 id="installing-unity-and-ceedling-in-the-builder-image">Installing <code>Unity</code> and <code>Ceedling</code> in the builder image</h3>

<p>In the first step of our builder image we already installed <code>ruby</code>, so installing our unit test tools is very straightforward:</p>

<div><pre><code><span># install unity cmock and ceedling (unit test environment)</span>
<span>RUN </span>gem <span>install </span>ceedling
<span># set standard encoding to UTF-8 for ruby (and thus ceedling)</span>
<span>ENV</span><span> RUBYOPT "-KU -E utf-8:utf-8"</span>
</code></pre></div>

<p>After rebuilding our image we’re good to go!</p>



<h3 id="configuring-unity-and-running-unit-tests">Configuring <code>Unity</code> and running unit tests</h3>

<p>Lots of resources exist on how to set up <code>Unity</code> and <code>Ceedling</code>, including an <a href="https://embeddedartistry.com/blog/2019/2/25/unit-testing-and-reporting-on-a-build-server-using-ceedling-and-unity/" target="_blank">excellent article on Embedded Artistry</a> or the <a href="https://www.throwtheswitch.org/articles" target="_blank"><em>ThrowTheSwitch’s</em> own articles</a>. In short, you set the configuration switches for <code>Unity</code> in a dedicated <a href="https://github.com/lmapii/cproject/blob/main/tests/unittest/support/unity_config.h" target="_blank"><code>unity_config.h</code></a> file, and configure <code>Ceedling</code> with the <a href="https://github.com/lmapii/cproject/blob/main/tests/unittest/project.yml" target="_blank"><code>project.yml</code></a>. <code>Ceedling</code> is a neat little helper that generates all the test runners for you, so all you need to do is add your test files and tell <code>Ceedling</code> how to detect them. Have a look at the files in the <a href="https://github.com/lmapii/cproject" target="_blank">example project on GitHub</a> in case you need details!</p>

<p>In the <a href="https://github.com/lmapii/cproject/blob/main/tests/unittest/project.yml" target="_blank"><code>project.yml</code></a> we told <code>Ceedling</code> that our tests will be located in the <code>tests/unittest/test</code> directory and all test file names have the prefix <code>test</code>. So all we need to do is to add our <a href="https://github.com/lmapii/cproject/blob/main/tests/unittest/test/test_dummy.c" target="_blank"><code>test_dummy.c</code></a> file to test our implementation, leading to the following files added:</p>

<div><pre><code><span>$ </span>tree <span>--charset</span><span>=</span>utf-8 <span>--dirsfirst</span> <span>--</span> tests
tests
└── unittest
    ├── support
    │   └── unity_config.h
    ├── <span>test</span>
    │   └── test_dummy.c
    └── project.yml
</code></pre></div>

<p>We can then create our first unit test <code>tests/unittest/test/test_dummy.c</code>:</p>

<div><pre><code><span>#include "dummy/dummy.h"
#include "unity.h"
</span>
<span>void</span> <span>setUp</span><span>(</span><span>void</span><span>)</span> <span>{}</span>
<span>void</span> <span>tearDown</span><span>(</span><span>void</span><span>)</span> <span>{}</span>

<span>void</span> <span>test_dummy</span><span>(</span><span>void</span><span>)</span>
<span>{</span>
    <span>TEST_ASSERT_EQUAL</span><span>(</span><span>4U</span><span>,</span> <span>dummy_random</span><span>());</span>
<span>}</span>
</code></pre></div>

<p>If you’re in your <code>vscode</code> development container, you can run the following straight from the integrated terminal. Otherwise, spin up the container using <code>make builder-run</code> to execute your tests:</p>

<div><pre><code><span>$ </span>ceedling <span>test</span>:all

Test <span>'test_dummy.c'</span>
<span>-------------------</span>
Generating runner <span>for </span>test_dummy.c...
Compiling test_dummy_runner.c...
Compiling test_dummy.c...
Compiling unity.c...
Compiling dummy.c...
Compiling cmock.c...
Linking test_dummy.out...
Running test_dummy.out...

<span>--------------------</span>
OVERALL TEST SUMMARY
<span>--------------------</span>
TESTED:  1
PASSED:  1
FAILED:  0
IGNORED: 0
</code></pre></div>

<h3 id="bonus-coverage-reports">Bonus: Coverage reports</h3>

<p>The previously mentioned <a href="https://embeddedartistry.com/blog/2019/2/25/unit-testing-and-reporting-on-a-build-server-using-ceedling-and-unity/" target="_blank">article on Embedded Artistry</a> also shows how to configure coverage reports. This is already set up in the <a href="https://github.com/lmapii/cproject/blob/main/tests/unittest/project.yml" target="_blank"><code>project.yml</code> of the GitHub example project</a>, and since we included <code>gcovr</code> in the <code>apt</code> packages of our image, we are good to run our unit tests with coverage:</p>

<div><pre><code><span>$ </span>ceedling gcov:all

Test <span>'test_dummy.c'</span>
<span>-------------------</span>
Compiling test_dummy_runner.c...
Compiling test_dummy.c...
Compiling dummy.c with coverage...
Compiling unity.c...
Compiling cmock.c...
Linking test_dummy.out...
Running test_dummy.out...

<span>--------------------------</span>
GCOV: OVERALL TEST SUMMARY
<span>--------------------------</span>
TESTED:  1
PASSED:  1
FAILED:  0
IGNORED: 0

<span>---------------------------</span>
GCOV: CODE COVERAGE SUMMARY
<span>---------------------------</span>
dummy.c Lines executed:100.00% of 2
dummy.c No branches
dummy.c No calls
</code></pre></div>

<p>The <a href="https://github.com/lmapii/cproject/blob/main/tests/unittest/project.yml" target="_blank"><code>project.yml</code></a> also sets up a utility for generating coverage reports. In this demo project we can generate a <code>gcovr</code> HTML report with the following command:</p>

<div><pre><code><span>$ </span>ceedling utils:gcov
Creating gcov results report<span>(</span>s<span>)</span> <span>in</span> <span>'build/artifacts/gcov'</span>... Done <span>in </span>1.359 seconds.
</code></pre></div>

<p>You’ll now find the HTML report in <code>tests/unittest/build/artifacts/gcov/GcovCoverageResults.html</code>.</p>

<p><img src="https://interrupt.memfault.com/img/c-dev-environment/gcovr-report.png" alt=""></p>

<h2 id="getting-started-with-github-workflows">Getting started with GitHub workflows</h2>

<p>We’ve gotten quite far already, and this setup is way beyond the “works on my machine” type of environment. All of your contributors can now build a development container and execute all steps to build or test your project. There is one more thing that we should do to keep the project stable even if we’re not actively working on it: setting up a pipeline that periodically ensures that the environment still works.</p>

<p>Continuously testing and checking your code is important. Pipelines are now available on all major platforms such as <a href="https://github.com/" target="_blank">GitHub</a>, <a href="https://about.gitlab.com/" target="_blank">GitLab</a>, or <a href="https://bitbucket.org/" target="_blank">Bitbucket</a>. In this article, we’ll set up a <a href="https://github.com/features/actions" target="_blank">GitHub action</a> which periodically builds our image and tests our code.</p>

<p>There’s yet another catch though: Ensuring that your Docker images build and using those Docker images in your pipeline can sometimes be a bit tedious. At the time of writing, GitHub has support for so-called <a href="https://docs.github.com/en/actions/creating-actions/creating-a-docker-container-action" target="_blank">container actions</a> that allow executing pipeline steps inside a Docker container. These steps, however, assume that the image exists on some registry, e.g., <a href="https://hub.docker.com/" target="_blank">dockerhub</a>, and is not built within the action itself.</p>

<p>Since nowadays <a href="https://hub.docker.com/" target="_blank">dockerhub</a> is no longer entirely free to use, we will still try to execute all steps within a Docker image that is built as part of the pipeline. This makes the pipeline steps less elegant and very verbose, but I’ll choose this approach over having to use multiple actions or a container registry.</p>

<h3 id="adding-a-github-action">Adding a GitHub action</h3>

<p>GitHub actions are added <code>.yml</code> workflow files in the <code>.github/workflows</code> directory of the repository. We’ll name our workflow file <a href="https://github.com/lmapii/cproject/blob/main/.github/workflows/ci.yml" target="_blank"><code>ci.yml</code></a>:</p>

<div><pre><code><span>name</span><span>:</span> <span>ci</span>
<span>on</span><span>:</span>
  <span>push</span><span>:</span>
    <span>branches</span><span>:</span>
    <span>-</span> <span>main</span>
  <span>schedule</span><span>:</span>
    <span>-</span> <span>cron</span><span>:</span> <span>'</span><span>0</span><span> </span><span>1</span><span> </span><span>*</span><span> </span><span>*</span><span> </span><span>0'</span>
<span>jobs</span><span>:</span>
  <span># empty</span>
</code></pre></div>

<p>With the above configuration, the jobs in this action are executed <a href="https://crontab.guru/#0_1_*_*_0" target="_blank">at 01:00 every Sunday</a> and on every push to the <code>main</code> branch.</p>

<h3 id="building-an-image-in-a-github-action">Building an image in a GitHub action</h3>

<p>Building a Docker image in your pipeline is quite straightforward due to the existing GitHub actions:</p>

<div><pre><code><span>jobs</span><span>:</span>
  <span>docker-build</span><span>:</span>
    <span>runs-on</span><span>:</span> <span>ubuntu-latest</span>
    <span>steps</span><span>:</span>
      <span>-</span> <span>uses</span><span>:</span> <span>actions/checkout@v3</span>
      <span>-</span> <span>uses</span><span>:</span> <span>docker/setup-buildx-action@v2</span>
        <span>name</span><span>:</span> <span>Build builder-image</span>
        <span>with</span><span>:</span>
          <span>context</span><span>:</span> <span>.</span>
          <span>file</span><span>:</span> <span>builder.Dockerfile</span>
          <span>tags</span><span>:</span> <span>cproject-builder:latest</span>
  <span>build-and-test</span><span>:</span>
    <span>needs</span><span>:</span> <span>docker-build</span>
    <span># TODO: needs cproject-builder:latest</span>
</code></pre></div>

<p>The problem is that all jobs are isolated and that data <em>including Docker images</em> must be shared explicitly between jobs. Without sharing, <code>cproject-builder:latest</code> is not available in the next job <code>build-and-test</code>.</p>



<p>The <a href="https://docs.docker.com/build/ci/github-actions/share-image-jobs/" target="_blank">proposed solution by Docker</a> is to share an image using <em>artifacts</em>. Artifacts are files that can be stored in steps in a GitHub action and thus be downloaded in a subsequent step. For our builder image this is very problematic since the builder image is very large:</p>

<ul>
  <li>Uploading the image takes a very long time.</li>
  <li>Artifact count towards the storage limit in your GitHub plan. A free plan only has 500 MB of storage.</li>
  <li>Downloading the image in subsequent steps also takes a very long time.</li>
</ul>

<p>My favorite solution for this problem is using <a href="https://github.com/actions/cache" target="_blank">cache dependencies</a>. Similar to the proposed solution by Docker, we build the image and store it in a file, which is then accessible using a <em>cache</em>. Another benefit of this approach is, that for subsequent actions and if the <em>Dockerfile</em> did not change, the image is not rebuilt!</p>

<div><pre><code><span>jobs</span><span>:</span>
  <span>docker-build</span><span>:</span>
    <span>runs-on</span><span>:</span> <span>ubuntu-latest</span>
    <span>steps</span><span>:</span>
      <span>-</span> <span>uses</span><span>:</span> <span>actions/checkout@v3</span>
      <span>-</span> <span>uses</span><span>:</span> <span>docker/setup-buildx-action@v2</span>
      <span>-</span>
        <span>name</span><span>:</span> <span>Create docker cache folder</span>
        <span>run</span><span>:</span> <span>mkdir -p /tmp/docker</span>
      <span>-</span>
        <span>name</span><span>:</span> <span>Restore docker image</span>
        <span>id</span><span>:</span> <span>cache-docker</span>
        <span>uses</span><span>:</span> <span>actions/cache@v3</span>
        <span>with</span><span>:</span>
          <span>path</span><span>:</span> <span>/tmp/docker</span>
          <span>key</span><span>:</span> <span>${{ runner.os }}-docker-${{ hashFiles('builder.Dockerfile') }}</span>
      <span>-</span>
        <span>name</span><span>:</span> <span>Build docker builder-image</span>
        <span>if</span><span>:</span> <span>steps.cache-docker.outputs.cache-hit != 'true'</span>
        <span>uses</span><span>:</span> <span>docker/build-push-action@v4</span>
        <span>with</span><span>:</span>
          <span>context</span><span>:</span> <span>.</span>
          <span>file</span><span>:</span> <span>builder.Dockerfile</span>
          <span>tags</span><span>:</span> <span>cproject-builder:latest</span>
          <span>outputs</span><span>:</span> <span>type=docker,dest=/tmp/docker/${{ runner.os }}-builder-image.tar</span>
</code></pre></div>

<p>At the time of writing caches do <strong>not</strong> count towards the storage limit of your GitHub account. The only downside that I’ve encountered, is that caches are not shared between actions of different branches and that you’ll need to manage your caches in case you have too many branches using images.</p>

<h3 id="using-the-cached-image-to-build-the-project">Using the cached image to build the project</h3>

<p>In the next job, we load the builder image and execute our actions. As mentioned before, the image is not available yet and therefore we cannot use the much more elegant GitHub actions for containers. It does, however, do its job and does so efficiently since restoring a cache takes much less time than downloading an artifact:</p>

<div><pre><code><span>jobs</span><span>:</span>
  <span>docker-build</span><span>:</span>
    <span># &lt;snip&gt;</span>
  <span>build-and-test</span><span>:</span>
    <span>runs-on</span><span>:</span> <span>ubuntu-latest</span>
    <span>needs</span><span>:</span> <span>docker-build</span>
    <span>steps</span><span>:</span>
      <span>-</span> <span>uses</span><span>:</span> <span>actions/checkout@v3</span>
      <span>-</span> <span>uses</span><span>:</span> <span>docker/setup-buildx-action@v2</span>
      <span>-</span>
        <span>name</span><span>:</span> <span>Restore docker image</span>
        <span>id</span><span>:</span> <span>cache-docker</span>
        <span>uses</span><span>:</span> <span>actions/cache@v3</span>
        <span>with</span><span>:</span>
          <span>path</span><span>:</span> <span>/tmp/docker</span>
          <span>key</span><span>:</span> <span>${{ runner.os }}-docker-${{ hashFiles('builder.Dockerfile') }}</span>
      <span>-</span>
        <span>name</span><span>:</span> <span>Load image</span>
        <span>run</span><span>:</span> <span>|</span>
          <span>docker load --input /tmp/docker/${{ runner.os }}-builder-image.tar</span>
          <span>docker image ls -a</span>
      <span>-</span>
        <span>name</span><span>:</span> <span>Build library</span>
        <span>run</span><span>:</span> <span>|</span>
          <span>docker run \</span>
            <span>--rm \</span>
            <span>--platform linux/amd64 \</span>
            <span>--workdir /builder/mnt \</span>
            <span>-v ${{ github.workspace }}:/builder/mnt \</span>
            <span>cproject-builder:latest \</span>
            <span>/bin/bash -c "rm -rf build; cmake -B build; cmake --build build"</span>
</code></pre></div>

<p>This job needs the previous <code>docker-build</code> to pass, then restores the docker image from the cache and builds the project within the image using <code>cmake</code>. Here the downside of this approach is visible: We always need to use the entire <code>docker run</code> command to execute our steps.</p>

<h3 id="running-the-tests-and-storing-the-coverage-report-as-an-artifact">Running the tests and storing the coverage report as an artifact</h3>

<p>We can now run the same commands that we used previously to run the unit tests with coverage and generate our coverage report. In addition, we can store this coverage report as an artifact in our pipeline such that we can download it after any successful run.</p>

<div><pre><code><span>jobs</span><span>:</span>
  <span>docker-build</span><span>:</span>
    <span># &lt;snip&gt;</span>
  <span>build-and-test</span><span>:</span>
    <span>runs-on</span><span>:</span> <span>ubuntu-latest</span>
    <span>needs</span><span>:</span> <span>docker-build</span>
    <span>steps</span><span>:</span>
      <span># &lt;snip&gt;</span>
      <span>-</span>
        <span>name</span><span>:</span> <span>Test library</span>
        <span>run</span><span>:</span> <span>|</span>
          <span>docker run \</span>
            <span>--rm \</span>
            <span>--platform linux/amd64 \</span>
            <span>--workdir /builder/mnt/tests/unittest \</span>
            <span>-v ${{ github.workspace }}:/builder/mnt \</span>
            <span>cproject-builder:latest \</span>
            <span>/bin/bash -c "ceedling clobber; ceedling gcov:all; ceedling utils:gcov"</span>
      <span>-</span>
        <span>name</span><span>:</span> <span>Archive coverage results</span>
        <span>shell</span><span>:</span> <span>bash</span>
        <span>run</span><span>:</span> <span>|</span>
          <span>staging="reports-${{github.run_number}}"</span>
          <span>mkdir -p "$staging"</span>
          <span>cp -r tests/unittest/build/artifacts/gcov "$staging"</span>
          <span>tar czf "$staging.tar.gz" "$staging"</span>
          <span>echo "ASSET=$staging.tar.gz" &gt;&gt; $GITHUB_ENV</span>
      <span>-</span>
        <span>name</span><span>:</span> <span>Archive artifacts</span>
        <span>uses</span><span>:</span> <span>actions/upload-artifact@v3</span>
        <span>with</span><span>:</span>
          <span>name</span><span>:</span> <span>reports-${{github.run_number}}</span>
          <span>path</span><span>:</span> <span>${{ env.ASSET }}</span>
          <span>retention-days</span><span>:</span> <span>3</span>
</code></pre></div>

<p>Never forget to specify a retention period for your artifacts, since otherwise, it is very easy to hit the storage limit of your GitHub plan.</p>

<h2 id="conclusion">Conclusion</h2>

<p>If you’ve managed to bear with me until this point, thank you for reading! Our job is done here, the library builds, the report is available as an artifact, and all of this without clogging our computer with tools.</p>

<p><img src="https://interrupt.memfault.com/img/c-dev-environment/github-action.png" alt=""></p>

<p>But are we really ever done? A development environment grows with the project, and so will your base image. Even at this point, it grew fairly large, and if that is a problem for you or your contributors, you should look into some of the <a href="https://www.docker.com/blog/reduce-your-image-size-with-the-dive-in-docker-extension/" target="_blank">strategies to reduce your image size</a>. Another strategy is to split images based on their usage, e.g., use a different image for building and another one for testing. This can greatly reduce your image size and allows you to use already stripped-down base images.</p>

<p>One last thing that is worth mentioning, is that with <a href="https://www.docker.com/products/docker-desktop/" target="_blank">Docker Desktop</a> you can now comfortably inspect your image for vulnerabilities, and also check the size impact of each step in your Dockerfile and even its base image:</p>

<p><img src="https://interrupt.memfault.com/img/c-dev-environment/docker-inspect.png" alt=""></p>

<p>This is a great starting point in case you want to reduce the image size or update packages. E.g., you could uninstall unused packages using <code>apt remove</code> as a last step in your <em>Dockerfile</em>, or install a more recent version of <code>ruby</code> using the <a href="https://rvm.io/" target="_blank">Ruby Version Manager</a> instead of relying on the outdated version that comes with <code>apt</code>. This discussion, however, is far beyond the scope of this article.</p>

<p>As mentioned throughout the article, the <a href="https://github.com/lmapii/cproject" target="_blank">example project is available on GitHub</a>. Feel free to point out my mistakes or add some improvements to the project yourself!</p>

<p>You have now all the skills to set up a containerized C/C++ project with a CI on GitHub. With this at hand, it should be easy to add a specific compiler to your <em>Dockerfile</em> or to clean up all the mistakes that I did when setting up the CI.</p>

<!-- Interrupt Keep START -->
<p>Like Interrupt? <a href="https://go.memfault.com/interrupt-subscribe" target="_blank">Subscribe</a> to get our latest posts straight to your mailbox.</p>

<p>See anything you'd like to change? Submit a pull request or open an issue at <a href="https://github.com/memfault/interrupt" target="_blank">GitHub</a></p>

<!-- Interrupt Keep END -->

<h2 id="references">References</h2>

<!-- prettier-ignore-start -->
<ul>
  <li><a href="https://code.visualstudio.com/docs/devcontainers/tutorial" target="_blank">Visual Studio Code Dev Containers tutorial</a></li>
  <li><a href="https://www.throwtheswitch.org/unity" target="_blank">The Unity test framework</a></li>
  <li><a href="https://embeddedartistry.com/blog/2019/2/25/unit-testing-and-reporting-on-a-build-server-using-ceedling-and-unity/" target="_blank">Embedded Artistry’s article on Ceedling and Unity on a build server</a></li>
  <li>
<a href="https://github.com/features/actions" target="_blank">GitHub actions documentation</a>
<!-- prettier-ignore-end -->
</li>
</ul>


    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My Overkill Home Network (463 pts)]]></title>
            <link>https://blog.networkprofile.org/my-home-network-complete-details-2023/</link>
            <guid>37081789</guid>
            <pubDate>Thu, 10 Aug 2023 21:05:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.networkprofile.org/my-home-network-complete-details-2023/">https://blog.networkprofile.org/my-home-network-complete-details-2023/</a>, See on <a href="https://news.ycombinator.com/item?id=37081789">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
        <p>In this post I will hopefully detail my entire home network. Some of this has been in separate posts explaining single items, but nowhere do I have all of the network in one post with all the changes since last year.</p><p>Here is a full shot of the rack in my house. Its in a centrally located closet which happens to have a 2ft x 2ft chase into the attic, which is very handy for running network cables. The rack itself is the 25u adjustable StarTech Rack which I've been quite pleased with.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-09-14.55.07.356.JPG" alt="" loading="lazy" width="2000" height="2667" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-09-14.55.07.356.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-09-14.55.07.356.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-09-14.55.07.356.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-09-14.55.07.356.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Starting at the top we have all of the networking gear and patch panels.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-09-14.56.14.527-1.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-09-14.56.14.527-1.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-09-14.56.14.527-1.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-09-14.56.14.527-1.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-09-14.56.14.527-1.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-09-14.56.25.766-1.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-09-14.56.25.766-1.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-09-14.56.25.766-1.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-09-14.56.25.766-1.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-09-14.56.25.766-1.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>At the very top are 2 x Monoprice keystone patch panels. They are mostly filled with Cat6 keystone jacks, but I also have a few LC-LC Keystone jacks for long fiber runs. All of these connections end up somewhere else in the house, either to cameras, AP's or to network jacks etc. Keystone patch panels are clearly the best, a you can easily add and remove jacks without moving the entire patch panel and possibly breaking other connections.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-11-12.36.30.879.JPG" alt="" loading="lazy" width="2000" height="2667" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-11-12.36.30.879.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-11-12.36.30.879.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-11-12.36.30.879.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-11-12.36.30.879.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>This is a generic 1u cable management device off of Amazon, which does a pretty good job along with these Monoprice SlimRun Cat6a cables I am using.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-11-12.36.39.284.JPG" alt="" loading="lazy" width="2000" height="2667" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-11-12.36.39.284.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-11-12.36.39.284.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-11-12.36.39.284.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-11-12.36.39.284.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Below the patch panels is a 1u Supermicro Server running PFSENSE</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-10-11.47.15.442.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-10-11.47.15.442.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-10-11.47.15.442.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-10-11.47.15.442.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-10-11.47.15.442.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-10-11.47.23.095.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-10-11.47.23.095.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-10-11.47.23.095.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-10-11.47.23.095.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-10-11.47.23.095.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>It has an Intel Pentium Gold G5500, 8GB of ECC RAM, Quad onboard Intel NIC's and a Mellonox Connect-X3 10Gb NIC for LAN + VLANS's. More details in this post</p><figure><a href="https://blog.networkprofile.org/updated-pfsense-build-for-gigabit-fiber/"><div><p>Updated Low Power PFSENSE Build for Gigabit Fiber</p><p>Since moving into a house I now pay for power, so I wanted to upgrade my PFSENSE box while at the same time saving power I also got gigabit fiber, so it needed to be able to handle that. This is the updated 2020 build of my pfSense Firewall. I</p><p><img src="https://blog.networkprofile.org/content/images/size/w256h256/2021/11/noun_Network_3683.png" alt=""><span>NetworkProfile.org</span><span>spookyghost</span></p></div><p><img src="https://blog.networkprofile.org/content/images/2020/01/2019-10-22-20.24.17-1.JPG" alt=""></p></a></figure><p>Around the back you can see the connections for my redundant internet. Ignore where it says T-Mobile, that should say Verizon!</p><p>I am using a 10Gb connection for the LAN and VLAN's because I do inter-VLAN routing on the firewall, so having that extra bandwidth helps. Also because my Internet is 1Gb symmetrical, it was very easy to saturate the 1Gb Link back to the switch.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-11-12.34.09.878.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-11-12.34.09.878.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-11-12.34.09.878.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-11-12.34.09.878.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-11-12.34.09.878.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Here is the Verizon Gateway sitting on top of the rack. I get pretty good signal here and get the rated speed, of around 300Mb/s down and 20Mb/s up. This costs $50/mo with no contract and no data cap, and serves as a backup to my main AT&amp;T Fiber connection.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/image-16.png" alt="" loading="lazy" width="750" height="400" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/image-16.png 600w, https://blog.networkprofile.org/content/images/2023/08/image-16.png 750w" sizes="(min-width: 720px) 720px"></figure><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-09-14.56.51.340-2.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-09-14.56.51.340-2.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-09-14.56.51.340-2.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-09-14.56.51.340-2.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-09-14.56.51.340-2.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Here is the AT&amp;T Fiber ONT, on the other side of the room. This is 1Gb symmetrical with no cap.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-09-15.11.32.796.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-09-15.11.32.796.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-09-15.11.32.796.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-09-15.11.32.796.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-09-15.11.32.796.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Both connections are in a Gateway Group in PFSENSE, so I always use the AT&amp;T Fiber, until there is an outage</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/image-6.png" alt="" loading="lazy" width="1255" height="408" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/image-6.png 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/image-6.png 1000w, https://blog.networkprofile.org/content/images/2023/08/image-6.png 1255w" sizes="(min-width: 1200px) 1200px"></figure><p>The AT&amp;T connection is in passthrough mode so I can do inbound connections, however the Verizon connection is not. I found that the IP changes very often, and PFSENSE can sometimes get hung up on that, and cause the connection to not work. So I left it in its default configuration which gives PFSENSE an internal IP. Inbound connections are not important on the Verizon connection, however I do still need inbound remote access VPN to work, as well as a connection for some security devices.</p><p>For that to work, I have a site-to-site Wireguard VPN setup to a VPS in Linode. Because my local PFSENSE box is the client, and the VPS is the server, I essentially bypass any NAT. This is very useful if you are stuck with an internet connection with CGNAT. On the VPS, I just have forwarding enabled so I can access my local network, and then I have NGINX Proxy Manager forwarding specific ports all the way through to my home network. This means I use the same IP or DNS record to get into my network no matter what goes on at home, as the tunnel just uses whatever gateway PFSENSE tells it to.</p><p>This is also how you are probably getting to this blog, which is hosted at home. More details here</p><figure><a href="https://blog.networkprofile.org/moving-my-blog-off-linode-and-back-home/"><div><p>Moving my Blog off of Linode and back Home (Sort of)</p><p>At the end of 2021, I moved my blog into Linode. The main goal for this was to keep it up in case of disaster or outage at home, and utilize the much better networking of Linode. Here is the post: Moving my Blog from Home to LinodeA while ago</p><p><img src="https://blog.networkprofile.org/content/images/size/w256h256/2021/11/noun_Network_3683.png" alt=""><span>NetworkProfile.org</span><span>spookyghost</span></p></div><p><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-01-21.50.41.248.JPG" alt=""></p></a></figure><p>You can read more about the 5G setup here</p><figure><a href="https://blog.networkprofile.org/redundant-wan-ditching-t-mobile-5g-for-verizon-5g/"><div><p>Redundant WAN - Ditching T-Mobile 5G for Verizon 5G</p><p>If you’ve been reading recent articles, you will know I got T-Mobile 5G Home internet as a secondary WAN connection. It is fantastic knowing I should always have internet. My main connection is AT&amp;T Fiber 1Gb/1Gb, but I like having a secondary connection. It started out GREAT. For</p><p><img src="https://blog.networkprofile.org/content/images/size/w256h256/2021/11/noun_Network_3683.png" alt=""><span>NetworkProfile.org</span><span>spookyghost</span></p></div><p><img src="https://blog.networkprofile.org/content/images/2022/10/2022-10-22-14.26.46.162.JPG" alt=""></p></a></figure><p>Below the firewall is my main 1Gb Switch. This is a Dell X1052P which handles the majority of the 1Gb connections in my house, and also all of the PoE in the house. This switch Does PoE and PoE+. I am using that to power AP's, some small network devices and a lot of IP cams.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-11-12.33.27.769.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-11-12.33.27.769.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-11-12.33.27.769.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-11-12.33.27.769.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-11-12.33.27.769.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>All of the blue connections are the IP Cameras on the house, connected in their own VLAN (I have more connected to the garage, coming up later)</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-11-12.33.32.458.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-11-12.33.32.458.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-11-12.33.32.458.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-11-12.33.32.458.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-11-12.33.32.458.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>This witch has a 20Gb LACP trunk that connects back to my main 10Gb Switch. I am using Cisco DAC's for this for no other reason than I ran out of spare transceivers when I setup the connection.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-11-12.33.37.019-1.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-11-12.33.37.019-1.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-11-12.33.37.019-1.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-11-12.33.37.019-1.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-11-12.33.37.019-1.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Overall the Dell X1052P is a nice switch in that it has 4 x 10Gb connections, PoE+, sliding rails, good quality fans etc, however the software is TERRIBLE. There is no way to configure via CLI, and the Web interface is the worst I have ever used.</p><p>Under that switch is a Cisco SG300-28 switch. This switch is here simply because I started running out of 1Gb connections on the Dell X1052P. I have a bunch of low bandwidth things connected here, such as IPMI for servers, Printers, etc. This is connected to the Dell switch with a single 1Gb connection. On this switch I also have the second port of the Verizon Gateway connected on a separate VLAN. This lets me connect a device DIRECTLY to the Verizon internet, bypassing PFSENSE. This is useful for a Ripe Atlas network probe that we will get to later, and makes troubleshooting very easy. Its also good for testing external connections, as it completely takes my internal DNS and firewall rules out of the question.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-11-12.33.15.776.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-11-12.33.15.776.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-11-12.33.15.776.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-11-12.33.15.776.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-11-12.33.15.776.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-10-15.46.40.950.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-10-15.46.40.950.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-10-15.46.40.950.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-10-15.46.40.950.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-10-15.46.40.950.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Below the SG300 is my favorite switch, the Cisco SX350-24F. This is a 24 Port 10Gb SFP+ switch, with 4 of the ports being combo ports for copper. This is my "Main" switch.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-09-15.23.50.958.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-09-15.23.50.958.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-09-15.23.50.958.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-09-15.23.50.958.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-09-15.23.50.958.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-10-11.51.36.524.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-10-11.51.36.524.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-10-11.51.36.524.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-10-11.51.36.524.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-10-11.51.36.524.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-11-12.33.42.491-1.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-11-12.33.42.491-1.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-11-12.33.42.491-1.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-11-12.33.42.491-1.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-11-12.33.42.491-1.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>This switch connects everything that needs 10Gb. There is Dual 10GB connections to my NAS, one of my ESXi server, single 10Gb connections to mine and my wife's desktop PC, the Firewall uplink, the 10Gb connection back to my garage rack and the 20GB LACP trunk to the Dell switch. I am also using some WiiTek Tranceivers to get 2.5Gb connections to my HTPC. Even though the switch only support 1Gb and 10GB, the Transceiver figures out the rest to let you have Multigig support. You can see that connection here on the far right.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-09-14.56.02.189.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-09-14.56.02.189.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-09-14.56.02.189.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-09-14.56.02.189.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-09-14.56.02.189.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Below the networking are 2 shelves, one with lower power devices, and below that my three VMware ESXi hosts.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-09-14.55.51.932.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-09-14.55.51.932.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-09-14.55.51.932.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-09-14.55.51.932.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-09-14.55.51.932.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Starting on the top row we have the following</p><ul><li>Hubitat Hub</li><li>Raspberry Pi 3B+ NTP server with GPS/PPS time source</li><li>A new Ripe Atlas Probe connected directly to the Verizon Connection</li><li>Another Raspberry Pi, this time a Pi W Zero with adapter board, also running NTP with GPS</li><li>An older Ripe Atlas Probe connected to the AT&amp;T Connection</li></ul><p>The Hubitat hub has now almost entirely been replaced by a HomeAssistant virtual machine. The only reason it is here is to connect the Z-Wave and Zigbee devices I have. However, I replace those devices with WiFi versions wherever possible. In general, I'm not a huge fan of the Hubitat Hub anymore, and I've had quite a few problems with it. Its configured to just pass the devices into HomeAssitant, so its just a radio at this point. The devices that I still need it for are important though, like Smoke Detectors and leak detectors.</p><p>The Ripe Atlas Probes are very cool, and generally just help the internet in general. Here is their website with more information</p><figure><a href="https://atlas.ripe.net/docs/getting-started/what-is-ripe-atlas.html?ref=blog.networkprofile.org"><div><p>RIPE Atlas docs | What is RIPE Atlas? | Docs</p><p>RIPE Atlas Docs</p><p><img src="https://t2.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;fallback_opts=TYPE,SIZE,URL&amp;url=https://atlas.ripe.net/docs/getting-started/what-is-ripe-atlas.html&amp;size=128" alt=""><span>Docs</span></p></div></a></figure><p>The NTP servers I have made an entire guide on, which you can read here. They give all the devices on my network super accurate time, even if there is no internet. I have one connected to the Dell Switch, and one to the Cisco switch. </p><figure><a href="https://blog.networkprofile.org/gps-backed-local-ntp-server/"><div><p>GPS Raspberry Pi NTP Server</p><p>This post details how to create a stratum-1 NTP Server using a Raspberry Pi utilizing GPS and PPS, and get time within 100 nanoseconds of real time, directly from the atomic clocks located in the GPS satellites above your head. The best part about this guide is that this will</p><p><img src="https://blog.networkprofile.org/content/images/size/w256h256/2021/11/noun_Network_3683.png" alt=""><span>NetworkProfile.org</span><span>spookyghost</span></p></div><p><img src="https://blog.networkprofile.org/content/images/2023/07/2023-07-10-14.16.26.786-2.JPG" alt=""></p></a></figure><p>And if you were wondering how I have a Pi Zero with ethernet, you can read about that also</p><figure><a href="https://blog.networkprofile.org/turning-a-raspberry-pi-zero-into-a-full-raspberry-pi-with-ethernet/"><div><p>Turn a Raspberry Pi Zero into a full Raspberry Pi with Ethernet</p><p>I needed another Raspberry Pi for a project, but with the limited availability I couldn’t find one. The project doesn’t need a very fast CPU, but it must have ethernet. I have an old Raspberry Pi Zero W, but being limited to wireless is a real problem. I also need</p><p><img src="https://blog.networkprofile.org/content/images/size/w256h256/2021/11/noun_Network_3683.png" alt=""><span>NetworkProfile.org</span><span>spookyghost</span></p></div><p><img src="https://blog.networkprofile.org/content/images/2023/07/2023-07-15-09.43.28.552.JPG" alt=""></p></a></figure><p>I have however put them in new cases, the HighPi Pro. These have plenty of space to fit the GPS modules.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-31-13.33.03.251.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-31-13.33.03.251.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-31-13.33.03.251.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-31-13.33.03.251.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-31-13.33.03.251.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Here you can see the antenna connection on the side</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-31-13.33.09.873.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-31-13.33.09.873.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-31-13.33.09.873.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-31-13.33.09.873.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-31-13.33.09.873.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-31-13.33.19.898.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-31-13.33.19.898.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-31-13.33.19.898.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-31-13.33.19.898.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-31-13.33.19.898.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>I just drilled a hole the same size as the connector and connected it. I put some hot glue on the back to stop it rotating when tightening also</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-31-13.33.59.927.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-31-13.33.59.927.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-31-13.33.59.927.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-31-13.33.59.927.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-31-13.33.59.927.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-31-13.33.40.118.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-31-13.33.40.118.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-31-13.33.40.118.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-31-13.33.40.118.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-31-13.33.40.118.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>For the Pi Zero and the other Pi's, I just used some generic standoffs to fit the GPS modules to a mounting hole where I could</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-31-14.03.53.401.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-31-14.03.53.401.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-31-14.03.53.401.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-31-14.03.53.401.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-31-14.03.53.401.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>The Pi Zero build has the wires soldered directly to the module saving space</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-31-12.11.53.074.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-31-12.11.53.074.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-31-12.11.53.074.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-31-12.11.53.074.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-31-12.11.53.074.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>But even the ones with the large connector cables fits well</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-31-14.03.37.768.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-31-14.03.37.768.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-31-14.03.37.768.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-31-14.03.37.768.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-31-14.03.37.768.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-31-14.03.41.633.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-31-14.03.41.633.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-31-14.03.41.633.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-31-14.03.41.633.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-31-14.03.41.633.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>The GPS antennas I am using are magnetic, so they just stick on top of the rack.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-09-14.56.38.144.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-09-14.56.38.144.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-09-14.56.38.144.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-09-14.56.38.144.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-09-14.56.38.144.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-09-14.56.42.621.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-09-14.56.42.621.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-09-14.56.42.621.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-09-14.56.42.621.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-09-14.56.42.621.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Almost everything on that top shelf including the Pi's are powered by this Anker USB power supply that I have had for years. </p><p><a href="https://www.amazon.com/gp/product/B00P936188/?ref=blog.networkprofile.org">https://www.amazon.com/gp/product/B00P936188/</a></p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-11-12.35.57.126.JPG" alt="" loading="lazy" width="2000" height="2667" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-11-12.35.57.126.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-11-12.35.57.126.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-11-12.35.57.126.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-11-12.35.57.126.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>There is one Pi on the shelf below, and that is an early TinyPilot device to give my NVR Machine remote access. Its a Raspberry Pi 4 with a custom USB interface and HDMI capture, so you can get BIOS level control of the machine. This is needed as my NVR Server (We'll get to that) does not have any remote management. </p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-10-11.57.56.429.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-10-11.57.56.429.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-10-11.57.56.429.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-10-11.57.56.429.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-10-11.57.56.429.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Moving over is the start of the VMware ESXi hosts. All are running ESXi 7.0 U3, as I have not yet made the jump to 8.0. The Move from 6.7 to 7.0 was full of bugs, so I am going to stick on 7.0 U3 for a while. I will detail all of the VM's I am running in another post as its quite extensive, however here is a quick list of services they host</p><p>Veeam Backup, Borg Backup, Arq Backup, HomeAssistant, HomeBridge, NGINX Proxy Manager, vCenter, Rekor Scout/OpenALPR, Grafana, InfluxDB, LibreNMS, General use VDI VM's, Test VM's, Windows Domain Lab, Portainer, Kiwix, NextCloud, Mealie, Navidrone, Solarr, PLEX, public SFTP Server, Netbox, qBitTorrent, Prowlarr, This blog, and a few other services.</p><p>The first is an ASUS PN50 with 64GB of DDR4, 2TB NVMe storage and an AMD Ryzen 4800U which has 8 cores and 16 threads. This is really a beast of a CPU for such a small box, and it handles all of the compute heavy VM's on my network that don't need fast networking. The main issue with this host is that the onboard NIC is not supported in ESXi, so I have a 1Gb Intel M.2 NIC sticking out the back. I tried to get a 2.5Gb NIC working, however I never could. </p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-09-15.00.36.501.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-09-15.00.36.501.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-09-15.00.36.501.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-09-15.00.36.501.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-09-15.00.36.501.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>To the left are my 2 Lenovo ESXi hosts. The bottom is an old M73 Tiny with just 16GB of RAM (The max it will take) and an old i5-4750T and a 1TB SATA SSD. This host handles some light VM's, however it needs to be replaced, as the NIC will no longer be supported by VMware in 8.0</p><p>Above is the Lenovo M720q Tiny, which has an i7-8700T, 64GB RAM, 2TB NVMe Disk, and best of all, an Intel X520 Dual 10GB NIC in the PCI-E Slot. This host handles all of the network heavy VM's. This is what I should have got instead of the Asus PN50.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-11-12.32.16.401.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-11-12.32.16.401.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-11-12.32.16.401.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-11-12.32.16.401.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-11-12.32.16.401.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-09-15.00.44.210.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-09-15.00.44.210.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-09-15.00.44.210.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-09-15.00.44.210.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-09-15.00.44.210.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Here is a shot of the NIC. Its a standard PCI-E NIC, with the IO cover removed. I put some Kapton tape on the NIC to make sure it doesn't touch anything it shouldn't. As you can expect a full size NIC in a system this small is quite a squeeze. Overall this small system has 21Gb/s of total bandwidth available to the network once you factor in the built in 1Gb NIC.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-10-12.00.01.310.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-10-12.00.01.310.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-10-12.00.01.310.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-10-12.00.01.310.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-10-12.00.01.310.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Below this we have 3 x 2u Supermicro chassis. The top used to my ESXi server, however now is just a disk shelf. This disk shelf contains 12 x 8TB SAS disks that are my main media storage and connects back to my NAS below it. Nothing important is stored on here, and its in a big RAIDZ2 array.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-09-14.55.42.565.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-09-14.55.42.565.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-09-14.55.42.565.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-09-14.55.42.565.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-09-14.55.42.565.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>You can see here that the control lights are not lit, and the power button doesn't even do anything.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-11-12.32.04.978.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-11-12.32.04.978.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-11-12.32.04.978.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-11-12.32.04.978.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-11-12.32.04.978.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>The IO shield is just a blank</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-11-12.34.23.139.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-11-12.34.23.139.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-11-12.34.23.139.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-11-12.34.23.139.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-11-12.34.23.139.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>And other than power cables, the only things connected are these Mini SAS SFF-8088 cables that run down to my TrueNAS box.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-10-12.02.29.771.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-10-12.02.29.771.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-10-12.02.29.771.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-10-12.02.29.771.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-10-12.02.29.771.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>To get the system to power on, I just jumpered the PSU connector with a paperclip and taped it in place. Power control is not important, as this will all stay on 24/7/365. There is a better way to do it, which is to get a Supermicro JBOD control board like the <strong>SuperMicro CSE-PTJBOD-CB2. </strong>However at the time of deploying this system, they were out of stock EVERYWHERE. Even now, they seem to only be available from China. So I just skipped it.</p><p>Below the JBOD is an almost identical Supermicro Chassis, but this one is my NAS. It runs TrueNAS on a Xeon E3-1270 V5, 64GB ECC RAM, a dual port 25Gb NIC and a few HBA's, here are the full specs</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/image-7.png" alt="" loading="lazy" width="466" height="319"></figure><p>For storage, the LSI 9207-8e (e for external) is connected to the 12 x 8TB SAS disks. The LSI 9300 is connected to 6 x 4TB SAS disks in striped mirrors for my important data. There is also 2 x Intel S3700 DC 800GB Enterprise SSD's for Metadata storage (Also called a Fusion Pool)</p><p>Since building it, its been completely stable, and I wouldn't change much about the build. Sadly I don't have many pictures of the inside, as its so important to my network I can't power it off to get some nice pictures!</p><p>You can read more about my storage config in this post. Nothing much has changed other than the replication destination. It now replicates to another NAS in the garage.</p><figure><a href="https://blog.networkprofile.org/my-data-backup-plan-2021/"><div><p>My Data Backup Plan - 2021</p><p>This post will detail how I am protecting my important data. The old post can be found here: My Backup Plan &amp; Lessons learnedI’ve posted about my backup plan before, but a lot of things have changed, and Ihave learned a lot about what works, and what doesn’t work</p><p><img src="https://blog.networkprofile.org/content/images/size/w256h256/2021/11/noun_Network_3683.png" alt=""><span>NetworkProfile.org</span><span>spookyghost</span></p></div><p><img src="https://blog.networkprofile.org/content/images/2021/08/2021-08-07-15.51.52.JPG" alt=""></p></a></figure><p>Below that server is my Blue Iris NVR running in an 8-bay Supermicro chassis. This has an i7 8700K, 16GB of DDR4, an NVIDIA Tesla P4 (For AI) and an LSI 9207-8i for the storage disks. I have around 48TB of storage for recording, and it does local AI to alert me to people walking on my property etc. Networking is handled by an Intel i350-T4 quad port gigabit NIC, with one port on the LAN, and one on the CCTV VLAN. This lets the server access the cameras directly, as the CCTV VLAN is completely firewalled off from everything and the internet.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2021-10-25-18.14.03.jpg" alt="" loading="lazy" width="1600" height="1200" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2021-10-25-18.14.03.jpg 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2021-10-25-18.14.03.jpg 1000w, https://blog.networkprofile.org/content/images/2023/08/2021-10-25-18.14.03.jpg 1600w" sizes="(min-width: 1200px) 1200px"></figure><p>This is the server that I use the TinyPilot for, as its a consumer board, it has no remote management.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/image-8.png" alt="" loading="lazy" width="1600" height="871" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/image-8.png 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/image-8.png 1000w, https://blog.networkprofile.org/content/images/2023/08/image-8.png 1600w" sizes="(min-width: 1200px) 1200px"></figure><p>Below that is my PDU's and my UPS. The PDU's are CyberPower PDU15M2F10R Metered PDU's, and I've been very happy with them. The front outlets are handy for quickly plugging something in that needs UPS power.</p><p>The UPS is an APC SRT3000RMXLA double conversion/online UPS powered from a 30a 120v receptacle. </p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-11-12.31.17.182.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-11-12.31.17.182.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-11-12.31.17.182.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-11-12.31.17.182.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-11-12.31.17.182.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Here you can see everything plugged in to the PDU's</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-11-12.35.45.562.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-11-12.35.45.562.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-11-12.35.45.562.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-11-12.35.45.562.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-11-12.35.45.562.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>The large orange cords actually go go back into the wall, where they power other parts of my house, like my desk, my wifes desk and the 2 locations we have TV's. This lets me use a single UPS for all of the critical devices in my house, and not have to worry about changing batteries in a bunch of different UPS's</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-11-12.35.34.613.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-11-12.35.34.613.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-11-12.35.34.613.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-11-12.35.34.613.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-11-12.35.34.613.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>I made a detailed post about that here</p><figure><a href="https://blog.networkprofile.org/home-server-room-power-upgrade-multi-room-ups/"><div><p>Home Server Room Power Upgrade + Multi-room UPS</p><p>This post shows how I upgraded the power delivery to my home server room, as well as installing the infrastructure to replace 5 UPS’s around my house with just one large UPS. Why Upgrade the power? The electrical in my home when I moved in was terrible, it was also</p><p><img src="https://blog.networkprofile.org/content/images/size/w256h256/2021/11/noun_Network_3683.png" alt=""><span>NetworkProfile.org</span><span>spookyghost</span></p></div><p><img src="https://blog.networkprofile.org/content/images/2020/09/2020-09-10-16.25.03-1.JPG" alt=""></p></a></figure><figure><a href="https://blog.networkprofile.org/ups-upgrade-apc/"><div><p>UPS Upgrade - APC SRT3000RMXLA Double Conversion</p><p>I have upgraded my lab power setup. The old setup can be found here My Lab Power SetupA lot of people wanted to know what UPS’s and PDU’s I am running, and where.Here are all the details I would highly suggest you read my post on setting</p><p><img src="https://blog.networkprofile.org/content/images/size/w256h256/2021/11/noun_Network_3683.png" alt=""><span>NetworkProfile.org</span><span>spookyghost</span></p></div><p><img src="https://blog.networkprofile.org/content/images/2021/07/2021-07-19-16.53.49.JPG" alt=""></p></a></figure><p>For a while I was able to run everything in here with no cooling, however adding more devices and more disks made the room way too hot. So I added this AC Infinity Fan which sucks the air out into a spare closet behind this one. The room its attached to we just use as storage, so the extra heat doesn't matter too much</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-09-14.56.59.802.JPG" alt="" loading="lazy" width="2000" height="2667" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-09-14.56.59.802.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-09-14.56.59.802.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-09-14.56.59.802.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-09-14.56.59.802.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Here is a closeup of the fan. I still need to neaten up the hole</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-09-14.57.07.044.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-09-14.57.07.044.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-09-14.57.07.044.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-09-14.57.07.044.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-09-14.57.07.044.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>The temp probe the controller are mounted in front of the rack</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-09-14.57.29.291.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-09-14.57.29.291.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-09-14.57.29.291.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-09-14.57.29.291.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-09-14.57.29.291.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Along with that is my Vertiv/Giest Watchdog 15-P, which is a PoE Powered enviromental moniotor with the ability to send alerts if the room gets above a certain temp or humidity level. I have it to alert on high temp, and low humidity </p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-09-14.57.17.723.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-09-14.57.17.723.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-09-14.57.17.723.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-09-14.57.17.723.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-09-14.57.17.723.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-09-14.57.25.796.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-09-14.57.25.796.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-09-14.57.25.796.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-09-14.57.25.796.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-09-14.57.25.796.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Here are some of the charts it can give you directly on the device. However, it also support SNMP so you can get the data into whatever monitoring solution you are using.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/image-15.png" alt="" loading="lazy" width="1016" height="727" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/image-15.png 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/image-15.png 1000w, https://blog.networkprofile.org/content/images/2023/08/image-15.png 1016w"></figure><p>Next we can move into the garage. In the patch panel you can see this run of SMF (Single Mode Fiber) This runs in conduit to the garage to provide networking there</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-09-15.17.09.707.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-09-15.17.09.707.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-09-15.17.09.707.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-09-15.17.09.707.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-09-15.17.09.707.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>It runs though this breezeway, along with the AT&amp;T Fiber to the garage on the right of the picture</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-09-15.03.22.815.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-09-15.03.22.815.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-09-15.03.22.815.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-09-15.03.22.815.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-09-15.03.22.815.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Here it exits the breezeway in the conduit. The AT&amp;T Fiber goes left, and the SMF keeps on going to the rack</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-11-13.17.37.320.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-11-13.17.37.320.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-11-13.17.37.320.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-11-13.17.37.320.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-11-13.17.37.320.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>The AT&amp;T Fiber goes down into the wall here on the other side</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-11-13.17.30.626.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-11-13.17.30.626.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-11-13.17.30.626.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-11-13.17.30.626.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-11-13.17.30.626.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>And into their box</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-11-13.18.08.243.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-11-13.18.08.243.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-11-13.18.08.243.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-11-13.18.08.243.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-11-13.18.08.243.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>And onto the pole</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-11-13.18.01.088.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-11-13.18.01.088.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-11-13.18.01.088.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-11-13.18.01.088.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-11-13.18.01.088.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>You can see here why I have the backup Verizon internet. As having that thin fiber run below 80ft oak trees, in a place that gets a lot of thunderstorms, isn't great. Knock on wood, I've never had a problem though (My neighbor did though!)</p><p>The SMF runs along the back of the garage to the rack, along with other ethernet runs that terminate in this rack for things like AP's and IP Cameras.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-10-12.23.24.279.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-10-12.23.24.279.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-10-12.23.24.279.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-10-12.23.24.279.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-10-12.23.24.279.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>I located the rack in front of my truck, so use some of the dead space</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-10-12.23.08.769.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-10-12.23.08.769.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-10-12.23.08.769.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-10-12.23.08.769.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-10-12.23.08.769.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>The rack is a small 12u rack I got for free, mounted to the studs. Sadly the bottom U isn't actually usable though for anything with depth, so its really 11u.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-11-12.42.39.036.JPG" alt="" loading="lazy" width="2000" height="2667" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-11-12.42.39.036.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-11-12.42.39.036.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-11-12.42.39.036.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-11-12.42.39.036.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>In this rack I have another patch panel, a Cisco WS-C2960S-48LPD-L switch which has 48 x PoE+ 1Gb ports, and 2 x 10Gb SFP+ Ports. </p><p>Below that is an APC Surge Protector PDU, and then my backup TrueNAS server built from scrap parts, and using 4 x 8TB SAS disk in RAIDZ2. The only thing this NAS does is receive replicated snapshots from my main NAS in the house. So if something were to happen to the NAS, I could instantly get the data from here instead. </p><p>There is more details on that NAS here</p><figure><a href="https://blog.networkprofile.org/deploying-a-truenas-backup-server-to-my-texas-garage/"><div><p>Deploying a TrueNAS Backup Server to my hot Texas Garage</p><p>If you’ve read my older posts, you know I have a rack in my unconditioned detached garage with a Cisco switch, an APC UPS, a couple of Raspberry Pi’s and some other misc items. Everyone told me it would all fail because it will get too hot in summer, too</p><p><img src="https://blog.networkprofile.org/content/images/size/w256h256/2021/11/noun_Network_3683.png" alt=""><span>NetworkProfile.org</span><span>spookyghost</span></p></div><p><img src="https://blog.networkprofile.org/content/images/2023/01/2023-01-27-16.54.38.152.JPG" alt=""></p></a></figure><p>Since the NAS is using an old consumer board, it has no remote management. So this new TinyPilot device comes in very handy here</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-30-13.41.06.889.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-30-13.41.06.889.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-30-13.41.06.889.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-30-13.41.06.889.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-30-13.41.06.889.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><figure><img src="https://blog.networkprofile.org/content/images/2023/08/image-9.png" alt="" loading="lazy" width="1514" height="1204" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/image-9.png 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/image-9.png 1000w, https://blog.networkprofile.org/content/images/2023/08/image-9.png 1514w" sizes="(min-width: 1200px) 1200px"></figure><p>I have another Raspberry Pi with GPS running here too, and the antenna is on top of the rack. </p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-10-14.40.59.384.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-10-14.40.59.384.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-10-14.40.59.384.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-10-14.40.59.384.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-10-14.40.59.384.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>The UPS is an old SMT1000RM2U which has plenty of capacity for this small rack and devices. It sits at about 20% load at most.</p><p>I am using the same tactic of giving UPS power to other devices through a power inlet here too. It powers my IoTaWatt Energy monitor, and a Raspberry Pi located inside my standby generator (Also where the run of MMF goes you can see in the above images)</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2022-01-28-13.21.43.jpg" alt="" loading="lazy" width="1600" height="1200" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2022-01-28-13.21.43.jpg 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2022-01-28-13.21.43.jpg 1000w, https://blog.networkprofile.org/content/images/2023/08/2022-01-28-13.21.43.jpg 1600w" sizes="(min-width: 1200px) 1200px"></figure><p>More details on those here</p><figure><a href="https://blog.networkprofile.org/power-monitoring-setup-iotawatt-grafana/"><div><p>Power Monitoring Setup (IoTaWatt + Grafana)</p><p>After getting solar panels I got used to checking the Enphase app for power details, but it’s not very reliable as sometimes the data is delayed, it requires internet access and it can’t get me any information other than solar and grid consumption. So if I want to see how</p><p><img src="https://blog.networkprofile.org/content/images/size/w256h256/2021/11/noun_Network_3683.png" alt=""><span>NetworkProfile.org</span><span>spookyghost</span></p></div><p><img src="https://blog.networkprofile.org/content/images/2022/03/Screenshot_2022-03-06-21.30.55_pvKHJR-1.png" alt=""></p></a></figure><figure><a href="https://blog.networkprofile.org/generac-maintenance-oil-report-and-more-genmon-changes/"><div><p>Generac Maintenance, Oil Report and more Genmon Changes</p><p>I could roll this into more than one post, but I’m not sure at what point there is too many generator posts. This post will go over some changes to Genmon (The FINAL CHANGES!) and some notes on maintenance, and the Blackstone Labs Oil Report. First, Genmon. I think I’ve</p><p><img src="https://blog.networkprofile.org/content/images/size/w256h256/2021/11/noun_Network_3683.png" alt=""><span>NetworkProfile.org</span><span>spookyghost</span></p></div><p><img src="https://blog.networkprofile.org/content/images/2022/09/2022-08-23-15.19.00.371-1.JPG" alt=""></p></a></figure><p>Up in the rafters there is also an antenna mounted for ADS-B. This connects back to a Pi with a SDR (Sofware Defined Radio) to feed ADS-B data from planes back to FlightAware, FlightRadar24, ADSB-X and a few others</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-07-11-12.42.54.440.JPG" alt="" loading="lazy" width="2000" height="2667" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-07-11-12.42.54.440.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-07-11-12.42.54.440.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-07-11-12.42.54.440.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-07-11-12.42.54.440.JPG 2400w" sizes="(min-width: 720px) 720px"></figure><p>Here is some live data. It would be better if I mounted the antenna outside, which is a project I will be taking on soon. The location of these planes is being reported entirely over ADS-B from the antenna, which is think is pretty neat.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/image-10.png" alt="" loading="lazy" width="1490" height="929" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/image-10.png 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/image-10.png 1000w, https://blog.networkprofile.org/content/images/2023/08/image-10.png 1490w" sizes="(min-width: 1200px) 1200px"></figure><p>There is also a WeatherFlow Tempest Receiver in the garage rack. This connects back to my Weather Station which is in the back yard on a pole up high</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/image-12.png" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/image-12.png 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/image-12.png 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/image-12.png 1600w, https://blog.networkprofile.org/content/images/2023/08/image-12.png 2000w" sizes="(min-width: 1200px) 1200px"></figure><p>Using MQTT I am able to get the data locally into HomeAssistant without using the internet, so I can have automations based on the sensors</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/image-14.png" alt="" loading="lazy" width="1484" height="1229" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/image-14.png 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/image-14.png 1000w, https://blog.networkprofile.org/content/images/2023/08/image-14.png 1484w" sizes="(min-width: 1200px) 1200px"></figure><figure><img src="https://blog.networkprofile.org/content/images/2023/08/image-13.png" alt="" loading="lazy" width="1472" height="583" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/image-13.png 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/image-13.png 1000w, https://blog.networkprofile.org/content/images/2023/08/image-13.png 1472w" sizes="(min-width: 1200px) 1200px"></figure><p>As for WiFi, I have access points spread around the house and garage. I am using 4 x Ruckus AP's. Here is an R510 on the ceiling in the house as an example. </p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-10-12.20.44.266.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-10-12.20.44.266.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-10-12.20.44.266.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-10-12.20.44.266.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-10-12.20.44.266.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>And here is an R320 mounted outside on the side of the garage (Note, this is not an outdoor AP, but its been there for over 3 years now working fine)</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-10-12.21.23.807.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-10-12.21.23.807.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-10-12.21.23.807.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-10-12.21.23.807.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-10-12.21.23.807.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>The Ruckus AP's don't look nearly as nice as the Ubiquiti AP's, but I've found they work MUCH better. Since I've installed these over 3 years ago, I've not had to tough the config at all apart from new SSID's etc. They just work. I am using them in Unleashed mode, which lets an AP act as the controller for all of the AP's, and doesn't require a controlled of any kind. </p><p>For Power, on the outside of the garage is the ATS for the generator which gives me backup power, and my main electrical panel(s)</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2021-09-30-17.59.47-1.jpg" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2021-09-30-17.59.47-1.jpg 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2021-09-30-17.59.47-1.jpg 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2021-09-30-17.59.47-1.jpg 1600w, https://blog.networkprofile.org/content/images/2023/08/2021-09-30-17.59.47-1.jpg 2000w" sizes="(min-width: 1200px) 1200px"></figure><p>The generator is a 27kw Generator which powers everything in the house. This means the UPS's only need at most 10 seconds of runtime, as that's how long it takes for the generator to start and switch</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-09-15.03.40.575.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-09-15.03.40.575.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-09-15.03.40.575.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-09-15.03.40.575.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-09-15.03.40.575.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>There are some future projects I am working on getting setup, like permanently installing one of these Meshtastic LoRa T-Beam's to get encrypted mesh communication even with no internet.</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-05-01-13.51.08.820.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-05-01-13.51.08.820.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-05-01-13.51.08.820.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-05-01-13.51.08.820.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-05-01-13.51.08.820.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>With one at home and one on me, I was able to communicate around 2 miles just with the stock antennas, and no height advantage. If I get a better antenna and mount it up high, I will be able to communicate throughout the entire neighborhood with no internet. </p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-10-15.11.00.792.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-10-15.11.00.792.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-10-15.11.00.792.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-10-15.11.00.792.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-10-15.11.00.792.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><figure><img src="https://blog.networkprofile.org/content/images/2023/08/2023-08-10-15.11.21.899.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/2023-08-10-15.11.21.899.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/2023-08-10-15.11.21.899.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2023/08/2023-08-10-15.11.21.899.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2023/08/2023-08-10-15.11.21.899.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>I currently have one setup as a repeater sitting on my desk. Configuration is much easier now they have made the WebUI 100x better. The device connects to my network over WiFi</p><figure><img src="https://blog.networkprofile.org/content/images/2023/08/image-11.png" alt="" loading="lazy" width="1500" height="500" srcset="https://blog.networkprofile.org/content/images/size/w600/2023/08/image-11.png 600w, https://blog.networkprofile.org/content/images/size/w1000/2023/08/image-11.png 1000w, https://blog.networkprofile.org/content/images/2023/08/image-11.png 1500w" sizes="(min-width: 1200px) 1200px"></figure><p>I think that's all! </p>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Grumman F11 Tiger Shot Itself Down (2021) (106 pts)]]></title>
            <link>https://www.planeandpilotmag.com/news/pilot-talk/grumman-f11-tiger-shoot-itself-down/</link>
            <guid>37081752</guid>
            <pubDate>Thu, 10 Aug 2023 21:02:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.planeandpilotmag.com/news/pilot-talk/grumman-f11-tiger-shoot-itself-down/">https://www.planeandpilotmag.com/news/pilot-talk/grumman-f11-tiger-shoot-itself-down/</a>, See on <a href="https://news.ycombinator.com/item?id=37081752">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<h2><b>The Aircraft</b></h2>
<p>In the early 1950s, aircraft designers began modernizing the F9F-6/7 Cougar—beefing it up with greatly reduced drag and supersonic speeds. When the redesign was completed in 1953, the result was a completely different aircraft than the Cougar. This new model was equipped with full-span leading-edge slats, trailing edge flaps with spoilers instead of ailerons for roll control, and wings that could fold down for easier storage on aircraft carriers. On its maiden flight in April of 1955, the now-complete Grumman F11F Tiger showed off its supersonic capabilities by nearing the speed of sound (Mach 1). Impressed, the Navy ordered the development of more than 400 for service, and it became the aircraft of its Blue Angels flight team.<span>&nbsp;</span></p>
<p>Despite its initial popularity, the Tiger quickly proved flawed: Its engine was unreliable, its range and endurance inadequate, and its performance inferior to other aircraft produced at the time, such as the Vought F-8 Crusader. By 1959, production ceased. The Blue Angels continued to fly it for another 10 years before it was switched out for the McDonnell Douglas F-4 Phantom II. While its time in service was short, its early-day supersonic speeds left a legacy—most famously because it was the first aircraft to be so fast that it shot itself down.</p>
				
<h2><b>The Incident</b></h2>
<p>On Sept. 21, 1956, young U.S. Navy test pilot Tom Attridge took off in an F11F Tiger (BuNo 138620) from Long Island, New York, for a weapons test over the Atlantic. He climbed to 20,000 feet, started a Mach 1 dive, and fired two bursts of rounds from his 20mm cannons until the ammunition was expended at 13,000 feet. He continued his dive, and around 7,000 feet something powerful struck his windshield. Thinking it must have been a bird, he quickly realized he had a big problem on his hands—his plane was losing power.<span>&nbsp;</span></p>
<p>Pulling up, he throttled back to 230 mph and began a return to base. Unable to maintain altitude, he attempted to apply more power, but the power would not exceed 78%. The plane went down into a sea of trees approximately a mile shy of the runway, traveling 300 feet and catching fire. It was a total loss. Attridge suffered a broken leg and several broken vertebrae but thankfully survived. As he later learned, it was not a bird that took him down. As it turned out, the crash was caused by a far more surprising source: his own rounds.</p>


<h2><b>Bullet Speed Vs. Supersonic Speed</b></h2>
<p>Many believed it was impossible for an aircraft, no matter how fast it could fly, to actually outrun its own bullets. After all, the speed of the average bullet is roughly around 1,700 mph. Mach 1, which Attridge had been traveling at, was 768 mph. That’s nearly a 1,000-mph difference. Clearly, this proved the damage had to have been caused by something like birds or even small meteorites. And, yet, that theory was wrong.</p>
<h2><b>How It Happened</b></h2>
<p>The rounds Attridge fired while traveling at 768 mph left their cannons at approximately 2,000 miles per hour. However, immediately after being fired, they encountered enough air resistance to produce significant drag. This drag resulted in a greatly reduced forward velocity, causing their trajectory to curve downward—directly into the flight path of the aircraft from which they had been fired. As the bullets descended and their speeds decreased to about 400 mph, the Tiger also descended but with an increased speed of 880 mph. Just as he began to pull out of his descent, Attridge was struck three times. The first bullet pierced his nose cone, the second went through his windshield, and the final one directly struck his right engine intake. The time between him firing the first rounds and taking the hits was a mere 11 seconds.</p>
<h2><b>A One-Time Thing?</b></h2>
<p>The Navy considered the incident a one-in-a-million fluke and was certain it would never happen again. Attridge was less convinced, however. “At the speeds we’re flying today,” he later said, “it could be duplicated any time.” He was right. In 1973, another Grumman test pilot, this one flying an <a href="https://nationalinterest.org/blog/buzz/how-f-14-tomcat-fighter-top-gun-shot-itself-down-130747" target="_blank" rel="noopener">F-14 Tomcat</a> out in California, was struck by his own missile. Luckily, it was a dummy missile, and the pilot was able to eject to safety. More recently, in 2019, a <a href="https://www.militarytimes.com/off-duty/military-culture/2019/04/08/dutch-f-16-makes-emergency-landing-after-plane-shoots-itself/" target="_blank" rel="noopener">Royal Netherlands Air Force F-16</a> accidentally shot itself from its 20mm rotary cannon. The pilot was able to land safely, uninjured.<span>&nbsp;</span></p>

<p><a href="https://www.planeandpilotmag.com/news/pilot-talk/2021/03/30/marine-fighter-damaged-by-extremely-close-friendly-fire/"><strong><em>Marine F-35 Fighter Jet Nearly Shoots Itself Down</em></strong></a></p>
<h2><b>Conclusion</b></h2>
<p>These days, aircraft weapon systems are primarily missile-based, not just bullet-based. Whether they are short-range heat-seeking or long-range radar-guided, missiles have many clear advantages, such as their speed, which can easily exceed any bullet or aircraft. In fact, in order to prevent them from being damaged, missiles are specifically designed to be faster than the aircraft from which they are deployed. Thankfully, protocols are now in place to avoid self-collision, so hopefully no more pilots will take themselves down.</p>

<p>As for Attridge, while he would always be known as “the pilot who shot himself down,” the incident cast little shadow on his career. He returned to service less than six months later and eventually went on to work on the Apollo Lunar Module. He flew west in 1997 at the age of 74.&nbsp;</p>
<!-- AI CONTENT END 1 -->
			
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Was Y Combinator worth it? (174 pts)]]></title>
            <link>https://tableflow.com/blog/was-y-combinator-worth-it</link>
            <guid>37081485</guid>
            <pubDate>Thu, 10 Aug 2023 20:41:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tableflow.com/blog/was-y-combinator-worth-it">https://tableflow.com/blog/was-y-combinator-worth-it</a>, See on <a href="https://news.ycombinator.com/item?id=37081485">Hacker News</a></p>
<div id="readability-page-1" class="page"><div fs-codehighlight-theme="base16/nebula" fs-codehighlight-element="code" id="content"><p>When people learn that we were part of a recent YC batch I usually get one of two reactions: "Cool… what is YC?" or "Was YC worth it?".</p><p>‍</p><p>Y Combinator (often just “YC”) is the top startup accelerator in the Bay Area, and arguably the world. It runs two “batches” each year – one in the Summer and one in the Winter. These batches last 3 months and consist of a kickoff retreat, meetups, talks from alumni, partner office hours, and regular group check-ins. (more about what happens <a href="https://www.ycombinator.com/about">here</a>)</p><p>They also provide funding. The <a href="https://www.ycombinator.com/deal">YC standard deal</a> usually equates to $500k for around 10% of the company. This is why people ask the second question.&nbsp;</p><p>So… was YC worth it? For us, absolutely!</p><p>I understand why people ask. 10% is a meaningful chunk of a company. While $500k sounds generous, many startups with traction could easily raise that money on much better terms. For companies in this position, there’s a difficult calculation to decide whether or not the benefits that come with YC are worth the less-favorable investment terms.</p><p>We, however, were not in this position. Eric and I still had full-time jobs when we were accepted. While we had been meeting regularly for a few months to discuss different ideas, we had absolutely zero traction, no working product, and very little validation. When we completed the application process, we thought of it more as a mental exercise than an actual chance at getting in. Luckily we did get in. This was much more due to our experience (technical background, worked at startups, friends/coworkers for 2+ years) than what we were working on (we pivoted anyway!).</p><p>So what did we get out of the experience?</p><h2>Ability to go full-time</h2><p>Starting a company takes a lot of time and energy. Doing it while having an existing job usually isn’t possible for any significant amount of time. So you’re kind of left choosing between two bad options: not making progress as quickly (working nights and weekends) OR not making any money while you figure out what you’re doing and attempt to fundraise. With YC, we were able to avoid this decision. When we applied and interviewed with YC, we still had our full-time jobs. Once we knew we’d have $500k to kickstart the company, the leap to founding was much less daunting.</p><h2>Overcoming imposter syndrome</h2><p>Even with a stereotypical founder background, I absolutely struggle with imposter syndrome. Am I really smart enough to be starting a company? Do I have the technical skills to hold my own with “10x engineers”? How am I going to figure out how to actually sell this thing? These are doubts I bet most founders experience when starting their journey.&nbsp;</p><p>YC is full of extremely bright founders, many of whom already have impressive achievements. That being said, they’re all trying to figure out the same things. Commiserating with batchmates on a weekly basis normalizes the challenges that you’re facing. Pivots no longer seem like a massive failure and instead feel like part of the process. Listening to raw, unfiltered stories directly from well-known alumni makes you realize that they faced similar challenges throughout their journey.</p><h2>Answering boring questions</h2><p>A critical part to founding and running a company is the business operations. This includes incorporating, tax filing, banking, working with lawyers, hiring contractors, configuring HR systems, and much, much more. With each one of these categories, there are likely dozens of decisions that will need to be made. Some of those are important decisions, others are easily updated in the future and don’t matter that much.</p><p>If you’re going at it alone, you’ll need to figure out which things matter and which don’t. And for the things that matter, you’ll need to figure out what the tradeoffs are between the available options. Most of these decisions are not exciting. They’re not fun to research. It probably ends up in most people Googling for hours and finding conflicting advice. With YC, all of these questions have come up before. After seeing thousands of companies, the YC partners typically know the tradeoffs and have clear recommendations. For the questions that aren’t already documented, you get access to the YC legal team for assistance.</p><h2>Adding legitimacy</h2><p>Attaching the YC brand to your name, both at a company and personal level, provides an immediate level of legitimacy to what you’re working on. Similar to how students from top universities get immediate recognition based on the strength of their program, YC founders also benefit greatly from their association with the accelerator.&nbsp;</p><p>It’s relatively easy to say you’re a “founder”. YC adds legitimacy by confirming that they interviewed you and believe your company is in the top percent of startups that applied. Additionally they have committed to providing $500k, a supportive community, and a wealth of other resources. For anyone looking to measure a company’s likelihood of success (investors, prospective employees, customers, concerned parents), having this additional support makes the company seem significantly more attractive.</p><h2>Founder community</h2><p>The YC community is often referenced as one of the program’s biggest selling points, and this is for good reason. During the batch, we forged strong friendships with many of our batchmates. We continue to meet on a regular basis to share updates, ideas, and provide support to each other. Having other people that can listen and empathize with the challenges you're experiencing goes a long way. While my family and friends have been extremely supportive, they can’t identify with many of the ups and downs of being a founder.</p><p>Not only are you kicking off the startup journey along with hundreds of other founders in your batch, you also have the support and contact information for thousands of alumni. This extremely valuable network provides an opportunity to connect with potential customers, investors, advisors, and future employees. Generally, YC founders are quick to give their time or respond to an email from another founder. This “pay it forward” mentality is part of the magic that makes the YC community so special.</p><p>‍</p><p>So, for us YC was a no-brainer. We were able to take the leap to founding our company, met brilliant batchmates and alumni, and we’ve already benefited greatly from the added legitimacy of being a “YC Company.” Beyond all of this, the founder experience has been more fun and less daunting by having the support of a world class community behind us.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HashiCorp Adopts Business Source License (521 pts)]]></title>
            <link>https://www.hashicorp.com/blog/hashicorp-adopts-business-source-license</link>
            <guid>37081306</guid>
            <pubDate>Thu, 10 Aug 2023 20:26:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.hashicorp.com/blog/hashicorp-adopts-business-source-license">https://www.hashicorp.com/blog/hashicorp-adopts-business-source-license</a>, See on <a href="https://news.ycombinator.com/item?id=37081306">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><section><p>When Mitchell and I founded HashiCorp, we made the decision to make our products open source because of a few key beliefs:</p>
<ul>
<li>We believe strongly in freely available source code to make it easy for practitioners to freely download, inspect source code, and solve their own problems.</li>
<li>We believe in building an ecosystem and community around our products to enable broad integrations.</li>
<li>We believe in the importance of transparency for our users.</li>
</ul>
<p>For more than a decade, we’ve continued to build new products and features provided to the community under a free open source license. This has led to a large community of users, contributors, partners, and customers who participate in and benefit from the work on the HashiCorp products.</p>
<p>Our open source model has been made possible by the thousands of commercial customers who partner with us on their mission-critical infrastructure. We invest tens of millions of dollars in research and development in our open source products annually, and our commercial efforts enable us to continue to support, and sponsor, our vibrant community of users.</p>
<p>Our approach has enabled us to partner closely with the cloud providers to enable tight integration for our joint users and customers, as well as hundreds of other technology partners we work closely with. However, there are other vendors who take advantage of pure OSS models, and the community work on OSS projects, for their own commercial goals, without providing material contributions back. We don’t believe this is in the spirit of open source.</p>
<p>As a result, we believe commercial open source models need to evolve for the ecosystem to continue providing open, freely available software. Open source has reduced the barrier to copying innovation and selling it through existing distribution channels. Many vendors have shifted increasingly to closed source for this reason, however we did not feel that would preserve our original goals in adopting open source.</p>
<p>That is why today we are announcing that HashiCorp is changing its source code license from Mozilla Public License v2.0 (MPL 2.0) to the Business Source License (BSL, also known as BUSL) v1.1 on all future releases of HashiCorp products. HashiCorp APIs, SDKs, and almost all other libraries will remain MPL 2.0.</p>
<p>BSL 1.1 is a source-available license that allows copying, modification, redistribution, non-commercial use, and commercial use under specific conditions. With this change we are following a path similar to other companies in recent years. These companies include Couchbase, Cockroach Labs, Sentry, and MariaDB, which developed this license in 2013. Companies including Confluent, MongoDB, Elastic, Redis Labs, and others have also adopted alternative licenses that include restrictions on commercial usage. In all these cases, the license enables the commercial sponsor to have more control around commercialization.</p>
<p>Our implementation of BSL includes additional usage grants that allow for broadly permissive use of our source code. We believe this will offer a fair and sustainable way for HashiCorp to share its source code widely, for free use. We consulted with OSS licensing experts and other industry stakeholders when developing our license, so that our efforts would be in line with industry practices.</p>
<p>Our first goal with this change is to minimize the impact to our community, partners, and customers. We will continue to publish source code and updates for HashiCorp products to our GitHub repository and distribution channels.</p>
<p>End users can continue to copy, modify, and redistribute the code for all non-commercial and commercial use, except where providing a competitive offering to HashiCorp. Partners can continue to build integrations for our joint customers. We will continue to work closely with the cloud service providers to ensure deep support for our mutual technologies. Customers of enterprise and cloud-managed HashiCorp products will see no change as well.</p>
<p>Vendors who provide competitive services built on our community products will no longer be able to incorporate future releases, bug fixes, or security patches contributed to our products.</p>
<p>Our commitment to our community, partners, and customers has not changed. We understand the trust the community places in us and we’ve worked carefully to preserve our original goals in adopting an open approach. We look forward to continuing to invest in the community and our products.</p>
<p>We know there will be additional questions, and we’ve assembled a set of <a href="https://hashi.co/license-faq">frequently asked questions</a> to help you better understand the changes. You can also watch the short video below for more information:</p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amiga runs Michigan schools' heating and air conditioning systems (2015) (146 pts)]]></title>
            <link>https://www.woodtv.com/news/grand-rapids/1980s-computer-controls-grps-heat-and-ac/</link>
            <guid>37081129</guid>
            <pubDate>Thu, 10 Aug 2023 20:12:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.woodtv.com/news/grand-rapids/1980s-computer-controls-grps-heat-and-ac/">https://www.woodtv.com/news/grand-rapids/1980s-computer-controls-grps-heat-and-ac/</a>, See on <a href="https://news.ycombinator.com/item?id=37081129">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
		
<p>GRAND RAPIDS, Mich. (WOOD) — A 30-year-old computer that has run day and night for decades is what controls the heat and air conditioning at 19 Grand Rapids Public Schools.</p>



<p>The Commodore Amiga was new to GRPS in the early 1980s and it has been working tirelessly ever since.&nbsp;GRPS Maintenance Supervisor Tim Hopkins said that the computer was purchased with money from an energy bond in the 1980s.&nbsp;It replaced a computer that was “about the size of a refrigerator.”</p>



<p>The computer is responsible for turning the heat and the air conditioners on and off for 19 school buildings.</p>



<p>“The system controls the start/stop of boilers, the start/stop of fans, pumps, [it] monitors space temperatures, and so on,” Hopkins explained.</p>



<p>A Kentwood High School student programmed it when it was installed in the 1980s. Whenever the district has a problem with it, they go back to the original programmer who still lives in the area.</p>



<p>Parts for the computer are difficult to find, Hopkins said. It is on its second mouse and third monitor.</p>





<p>“It’s a very unique product. It operates on a 1200-bit modem,” said Hopkins. “How it runs, the software that it’s running, is unique to Commodore.”</p>



<p>Hopkins said the system runs on a radio frequency that sends a signal to school buildings, which reply within a matter of seconds with the status of each building.&nbsp;The only problem is that the computer operates on the same frequency as some of the walkie-talkies used by the maintenance department.</p>



<p>“Because they share the same frequency as our maintenance communications radios and operations maintenance radios — it depends on what we’re doing — yes, they do interfere,” Hopkins said.</p>



<p>If that happens, “we have to clear the radio and get everyone off of it for up to 15 minutes.”</p>





<p>If the computer stopped working tomorrow, a staff person would have to turn each building’s climate control systems on and off by hand.</p>



<p>A new, more current system would cost between $1.5 and 2 million. If voters pass <a href="http://woodtv.com/2015/03/09/how-grps-plans-to-use-175-million-bond/" target="_blank" rel="noopener noreferrer"><strong>a $175 million bond proposal in November</strong></a>, the computer is on the list of things to be replaced.</p>



<p>It wasn’t replaced with money from the 2011 Warm Safe and Dry bond because it just didn’t rise to the top of the list.</p>



<p>“There’s a lot of projects, a lot of needs in the district, so there’s other priorities we have to put in place ahead of this,” Hopkins said. “This system is still running.”</p>





<p>Bringing Stocking Elementary out of moth balls, replacing boilers and roofs, and removing asbestos were just some of the projects GRPS put on the Warm, Safe and Dry list before the Commodore computer.</p>



<p>More about GRPS’s bond proposal <a href="https://youtu.be/06Z-sGL6ssw" target="_blank" rel="noopener noreferrer"><strong>via YouTube</strong></a>:</p>



		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Turns out lowly thymus may be saving your life (136 pts)]]></title>
            <link>https://news.harvard.edu/gazette/story/2023/08/turns-out-lowly-thymus-may-be-saving-your-life/</link>
            <guid>37080404</guid>
            <pubDate>Thu, 10 Aug 2023 19:07:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.harvard.edu/gazette/story/2023/08/turns-out-lowly-thymus-may-be-saving-your-life/">https://news.harvard.edu/gazette/story/2023/08/turns-out-lowly-thymus-may-be-saving-your-life/</a>, See on <a href="https://news.ycombinator.com/item?id=37080404">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-362162">
	
	

	
	<p>
		<h2>Study suggests organ plays vital role in immune health, particularly cancer prevention</h2>	</p>

	<div>

				<p>Many people couldn’t say where their thymus is, or what it does, and even doctors have long considered it expendable in adults. But new Harvard-led research suggests that the walnut-sized organ in the chest actually plays a vital role in immune health as we age, particularly in cancer prevention.</p>
<p>The study comparing data from patients who had their thymus removed with those who had not found that thymectomy patients had a nearly threefold higher risk of death from a variety of causes, including a twofold higher risk of cancer and a more modest increase in autoimmune diseases.</p>
<p>“The magnitude of risk was something we would have never expected,” said <a href="https://hsci.harvard.edu/people/david-scadden-md">David Scadden</a>, the Gerald and Darlene Jordan Professor of Medicine and professor in the <a href="https://hscrb.harvard.edu/">Department of Stem Cell and Regenerative Biology</a>, who led the <a href="https://www.nejm.org/doi/full/10.1056/NEJMoa2302892">study</a> published in The New England Journal of Medicine in collaboration with researchers at <a href="https://www.massgeneral.org/">Massachusetts General Hospital</a>.</p>
<p>“The primary reason why the thymus has an impact on overall health seems to be as a way to protect against the development of cancer.”</p>
<p>The thymus is the fastest-aging organ, according to Scadden. Most active in churning out T-cells during early childhood, it begins to atrophy into fatty tissue around puberty. That’s why, for many decades, scientists assumed it served a limited purpose in adulthood. It is typically removed due to issues with the organ itself, such as thymus cancer, or during other cardiothoracic surgeries because it’s located in front of the heart and is often in the surgeon’s way.</p>
<p>Yet in recent years scientists had started to suspect that the thymus plays an outsize role in our health as we age, by continuing to make T-cells that contribute to the diversity of the body’s overall T-cell population.</p>
<p>“This study demonstrates just how vital the thymus is to maintaining adult health,” Scadden said.</p>

		</div> <!-- article-wrap -->


<div>
	<figure>

		<p><img width="1350" height="900" src="https://news.harvard.edu/wp-content/uploads/2023/07/072723_David_Scadden_043-1350x900.jpg" alt="Researchers in lab." decoding="async" loading="lazy" sizes="(min-width: 1384px) 1224px, (min-width: 1070px) calc(100vw - 160px), (min-width: 600px) calc(100vw - 120px), calc(100vw - 50px)" srcset="https://news.harvard.edu/wp-content/uploads/2023/07/072723_David_Scadden_043-1350x900.jpg 1350w, https://news.harvard.edu/wp-content/uploads/2023/07/072723_David_Scadden_043-300x200.jpg 300w, https://news.harvard.edu/wp-content/uploads/2023/07/072723_David_Scadden_043-1024x683.jpg 1024w, https://news.harvard.edu/wp-content/uploads/2023/07/072723_David_Scadden_043-768x512.jpg 768w, https://news.harvard.edu/wp-content/uploads/2023/07/072723_David_Scadden_043-1536x1024.jpg 1536w, https://news.harvard.edu/wp-content/uploads/2023/07/072723_David_Scadden_043-2048x1366.jpg 2048w, https://news.harvard.edu/wp-content/uploads/2023/07/072723_David_Scadden_043-1200x800.jpg 1200w, https://news.harvard.edu/wp-content/uploads/2023/07/072723_David_Scadden_043-900x600.jpg 900w, https://news.harvard.edu/wp-content/uploads/2023/07/072723_David_Scadden_043-400x267.jpg 400w, https://news.harvard.edu/wp-content/uploads/2023/07/072723_David_Scadden_043-1500x1000.jpg 1500w">		</p>

					<figcaption>
									<p>Kameron Kooshesh, (from left), David Sykes, David Scadden, and Karin Gustafsson at work in their lab at MGH. </p>
													<p>Kris Snibbe/Harvard Staff Photographer</p>
							</figcaption>
		
	</figure>
</div>

<div>

			
<p>First author Kameron Kooshesh became intrigued by open questions about the adult thymus during a second-year neurology lecture at Harvard Medical School. He learned that surgical removal of the thymus is recommended in patients with the autoimmune disease myasthenia gravis as a way to halt T-cell-induced immune destruction of nerve endings. “And yet, clinical instruction on the surgery wards taught me that the thymus is thought to be vestigial in adults,” Kooshesh said. “These two philosophies seemed diametrically opposed, and I yearned to learn more.”</p>
<p>For the study, Kooshesh mined data from 1,146 adult patients who had undergone thymus removal, alongside demographically matched control patients who had undergone similar surgeries but kept their thymus. Kooshesh and Scadden worked in collaboration with Brody Foy, a biostatistician who helped direct the team’s statistical queries around the epidemiology of thymectomy patients, and Karin Gustafsson, an expert in T-cell biology. David Sykes at MGH helped the team facilitate patient blood draws.</p>
<p>In an analysis involving all patients with more than five years of follow-up, the rate of death was higher in the thymectomy group than in the general U.S. population ­— 9 percent vs. 5.2 percent, as was death due to cancer, or 2.3 percent vs. 1.5 percent.</p>
<p>In a subgroup of patients in whom T-cell production was measured, those who had had their thymus removed had less new production of T-cells, including both helper and cytotoxic T-cells. Those patients also had higher levels of pro-inflammatory cytokines, which are small signaling proteins associated with autoimmunity and cancer, in their blood.</p>
<p>“The magnitude of death and cancer in patients who had undergone thymectomy was the biggest surprise for me,” said Kooshesh, now an internal medicine resident at MGH. “The more we dug, the more we found: The results suggested to us that the lack of a thymus appears to perturb basic aspects of immune function.”</p>
<p>The analysis was facilitated by recent advances in rapid genetic sequencing of T-cell receptors (TCRs). The technology, called TCR sequencing, has enough resolution to allow scientists to not only identify different types of T cells, but also measure their diversity as a population overall.</p>

	


<div>
		<h3>The Daily Gazette</h3>
		<p>Sign up for daily emails to get the latest Harvard&nbsp;news.</p>
	</div>

				
			</div> <!--article-wrap -->

	
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Aurora I/O optimized config saved 90% DB cost (126 pts)]]></title>
            <link>https://graphite.dev/blog/how-an-aws-aurora-feature-cut-db-costs</link>
            <guid>37079909</guid>
            <pubDate>Thu, 10 Aug 2023 18:29:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://graphite.dev/blog/how-an-aws-aurora-feature-cut-db-costs">https://graphite.dev/blog/how-an-aws-aurora-feature-cut-db-costs</a>, See on <a href="https://news.ycombinator.com/item?id=37079909">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>All of Graphite’s data lives on Amazon Aurora Postgres.</p><p>Our database load is sizable — far larger than a typical startup of our size. This is because we sync bidirectionally with GitHub for <strong>everything</strong> a user does on Graphite, so Aurora plays a crucial role in helping us handle and scale massive amounts of data.</p><p>If Uber were to sign up for Graphite tomorrow, we could handle the countless webhook events triggered through their GitHub repos. But, our AWS bill would skyrocket due to a high volume of database operations.</p><p>Regardless of if one Uber engineer or a hundred sign up for Graphite, we’d still actively diff all of Uber’s GitHub activity against Graphite’s state in Aurora - mostly to power features like custom notifications. Our original implementation made our Aurora costs account for over 80% of our AWS bill, which paid for several thousand daily active users (roughly 4000 queries per second).</p><h3>Try serverless?</h3><p>Aurora prices on two things: storage rate and I/O rate. Our storage rate was affordable because our instance size was relatively cheap. But, our I/O rate was significantly more expensive because of our GitHub syncing.</p><p>We looked at Aurora Serverless to get our costs down. It sounded promising: Only pay for capacity that’s consumed. However, we constantly ingested so many updates via GitHub that it resulted in us needing such a large instance size on Aurora Serverless that it cost us <em>more</em> than if we chose a fixed instance size on Aurora. Furthermore, our traffic rarely experienced dramatic fluctuations, so we didn’t need the ability to upscale/downscale automatically.</p><p>But still, the issue with our high I/O costs on Aurora remained.</p><h3>Christmas came early in May: Aurora I/O Optimized</h3><p>On one foggy morning in late May, I opened the RDS console and found a pop-up that may as well have had a ribbon bow on top:</p><p>AWS <a href="https://aws.amazon.com/about-aws/whats-new/2023/05/amazon-aurora-i-o-optimized/">released</a> “Aurora I/O optimized”, a cost optimization feature that directly addressed our high I/O costs by offering up to 40% cost savings where I/O charges exceed 25% of the total Aurora database spend.</p><p>This feature would only charge us for a slightly higher storage rate but no I/O charges — making pricing more predictable. As CTO, I immediately began work to migrate our Aurora setup.</p><p>We hoped with I/O Optimized that we’d see those “40% cost savings”. After the migration, our cost savings were a whopping <strong>90%.</strong></p><p>This was insane. Also, the migration itself was so simple: you can convert existing clusters with either a few clicks in the console, using the CLI, or AWS’s SDK.</p><p>We reached out to some contacts at AWS to find out why the Aurora team built this. Did I/O Optimized do some clever engineering with sharding and storing data in S3? Were they just feeling generous?</p><p>The answer we got was more the latter. The Aurora team wanted to enable more customers to run heavier I/O workloads without worrying about heavier costs.</p><h3>Conclusion</h3><p>We faced a massive bill on AWS due to high I/O costs, and I/O Optimized rescued us. Serverless did not benefit us as much as we expected. Hopefully our story encourages you to monitor your AWS bill, experiment with cost optimization, and keep an eye out for new features/updates from your database provider.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Water Cooled Rooftop Solar Panels (118 pts)]]></title>
            <link>https://evergreenoffgrid.com/water-cooled-rooftop-solar-panels/</link>
            <guid>37079616</guid>
            <pubDate>Thu, 10 Aug 2023 18:05:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://evergreenoffgrid.com/water-cooled-rooftop-solar-panels/">https://evergreenoffgrid.com/water-cooled-rooftop-solar-panels/</a>, See on <a href="https://news.ycombinator.com/item?id=37079616">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
					
<p>Some combinations were just meant to be. Peanut butter and jelly, ice cream and spoons, <a href="https://electrek.co/2022/08/26/in-a-us-first-california-will-pilot-solar-panel-canopies-over-canals/">solar panels and canals</a>, and solar photovoltaic (PV) thermal integrated roofing. Who’da thunk. </p>



<figure>
<figure><img title="trac-home-bayview75 - Evergreen Off-Grid" decoding="async" fetchpriority="high" width="1024" height="585" data-id="1101" src="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/trac-home-bayview75.jpg?resize=1024%2C585&amp;ssl=1" data-src="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/trac-home-bayview75.jpg?resize=1024%2C585&amp;ssl=1" alt="" data-srcset="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/trac-home-bayview75.jpg?resize=1024%2C585&amp;ssl=1 1024w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/trac-home-bayview75.jpg?resize=300%2C171&amp;ssl=1 300w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/trac-home-bayview75.jpg?resize=768%2C439&amp;ssl=1 768w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/trac-home-bayview75.jpg?resize=1536%2C877&amp;ssl=1 1536w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/trac-home-bayview75.jpg?resize=600%2C343&amp;ssl=1 600w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/trac-home-bayview75.jpg?w=1600&amp;ssl=1 1600w" data-sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"></figure>



<figure><img title="Birdseye - Evergreen Off-Grid" decoding="async" width="1024" height="566" data-id="1102" src="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/Birdseye.png?resize=1024%2C566&amp;ssl=1" data-src="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/Birdseye.png?resize=1024%2C566&amp;ssl=1" alt="" data-srcset="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/Birdseye.png?resize=1024%2C566&amp;ssl=1 1024w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/Birdseye.png?resize=300%2C166&amp;ssl=1 300w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/Birdseye.png?resize=768%2C424&amp;ssl=1 768w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/Birdseye.png?resize=600%2C332&amp;ssl=1 600w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/Birdseye.png?w=1328&amp;ssl=1 1328w" data-sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"></figure>
<figcaption>Images Courtesy of Tractile</figcaption></figure>



<p>Of all the presentations I watched at the <a href="https://evergreenoffgrid.com/whats-new-with-evergreen-off-grid/">Energy Expo</a> one particularly stood out to me. The presentation was about a product called Eclipse Solar Roof Tiles. Designed and manufactured by a company called <a href="https://tractile.com.au/news/tractile-triumphs-in-the-desert/">Tractile</a> in Australia, this product has the potential to revolutionize rooftop solar.</p>





<h2>Roof Integrated Photovoltaic Thermal (RIPV-T)</h2>



<p>Introducing Roof Integrated Photovoltaic Thermal tiles… well that’s a mouth full, so let’s break it down. These tiles incorporate four features into one dynamic package. They’re a power generation plant, a complete roof system, a solar thermal water heater and an attic insulator. </p>



<h3>What is Solar Thermal Water Heating?</h3>



<p>In previous editions we’ve learned how <a href="https://evergreenoffgrid.com/learn-solar-from-sunlight-to-light-bulb/" title="Learn Solar: From Sunlight to Light Bulb">solar PV works</a>, we’ve also <a href="https://evergreenoffgrid.com/ready-for-a-new-roof-learn-about-integrated-solar/" title="Ready for a New Roof? Learn About Integrated Solar!">learned about integrated roofing</a> products like those offered by Tesla and GAF Energy. But we haven’t talked much about passive solar water heating yet. </p>


<!-- This site is converting visitors into subscribers and customers with OptinMonster - https://optinmonster.com :: Campaign Title: Inline Solar -->


<!-- / OptinMonster -->


<p>Passive solar water heaters work by passing water, or some other fluid, through small metal channels embedded in dark colored, heat absorbing material which is laid out in the sun. The heated fluid is then routed through a heat exchanger in your hot water tank or used directly. </p>



<p>By using the sun’s energy to to add heat to your water you reduce the amount of electricity needed to bring your tank to your desired temperature. Remember, heating and cooling are energy hungry endeavors.</p>



<p>Learn more about solar water heaters from the Department of Energy <a href="https://www.energy.gov/energysaver/solar-water-heaters">here</a>.</p>



<h2>Eclipse Solar Roof Tiles</h2>



<figure><img title="IMG_117E96437B17-1 - Evergreen Off-Grid" decoding="async" width="1024" height="967" src="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/IMG_117E96437B17-1.jpeg?resize=1024%2C967&amp;ssl=1" data-src="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/IMG_117E96437B17-1.jpeg?resize=1024%2C967&amp;ssl=1" alt="" data-srcset="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/IMG_117E96437B17-1.jpeg?resize=1024%2C967&amp;ssl=1 1024w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/IMG_117E96437B17-1.jpeg?resize=300%2C283&amp;ssl=1 300w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/IMG_117E96437B17-1.jpeg?resize=768%2C725&amp;ssl=1 768w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/IMG_117E96437B17-1.jpeg?resize=600%2C567&amp;ssl=1 600w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/IMG_117E96437B17-1.jpeg?w=1170&amp;ssl=1 1170w" data-sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"><figcaption>Image Courtesy of Tractile</figcaption></figure>



<h3>Synergy</h3>



<p>I apologize if you’ve been traumatized by managerial buzz word beat downs that included the word synergy. But I think it truly does apply here. </p>



<p>The synergy is what’s most exciting to me about this system. The combined features not only integrate, but they actually compliment each other. It’s one of those products that give you that duh factor the more you dig into it. That, why didn’t I think of that, feeling. Like putting wheels on luggage. </p>


<div><article>
			<a href="https://evergreenoffgrid.com/choosing-the-best-pure-sine-wave-inverter-a-comprehensive-guide-for-off-grid-enthusiasts/">
			<p><img width="568" height="568" src="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2023/07/evergreenoffgrid_small_offgrid_solar_setup_b0c5f9bf-3a57-4462-8a95-c3a9fd63e7e4.png?fit=568%2C568&amp;ssl=1" data-src="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2023/07/evergreenoffgrid_small_offgrid_solar_setup_b0c5f9bf-3a57-4462-8a95-c3a9fd63e7e4.png?fit=568%2C568&amp;ssl=1" alt="" decoding="async" data-srcset="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2023/07/evergreenoffgrid_small_offgrid_solar_setup_b0c5f9bf-3a57-4462-8a95-c3a9fd63e7e4.png?w=568&amp;ssl=1 568w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2023/07/evergreenoffgrid_small_offgrid_solar_setup_b0c5f9bf-3a57-4462-8a95-c3a9fd63e7e4.png?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2023/07/evergreenoffgrid_small_offgrid_solar_setup_b0c5f9bf-3a57-4462-8a95-c3a9fd63e7e4.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2023/07/evergreenoffgrid_small_offgrid_solar_setup_b0c5f9bf-3a57-4462-8a95-c3a9fd63e7e4.png?resize=100%2C100&amp;ssl=1 100w" data-sizes="(max-width: 568px) 100vw, 568px" title="evergreenoffgrid_small_offgrid_solar_setup_b0c5f9bf-3a57-4462-8a95-c3a9fd63e7e4 - Evergreen Off-Grid">			</p>
		</a><!-- .post-thumbnail -->
			
</article>
</div>


<h3>Water Cooled Solar Panels</h3>



<p>There’s a bit of a catch 22 when it comes to solar panels. They love the sun, but they aren’t too fond of heat. Solar panel manufacturers add a temperature coefficient to their specifications telling you exactly how much efficiency is lost as the temperature increases. </p>



<p>Learn more about the relationship between solar panels and heat from EnergySage <a href="https://news.energysage.com/solar-panel-temperature-overheating/?rc=p-evergreenoffgrid">here</a>.</p>



<p>The process of solar thermal requires absorbing heat and carrying it away to your hot water tank where you can make use of it. But absorbing heat from where? Well, from your solar panels of course. So water heating equals panel cooling. Pretty cool, right?</p>



<p>In hot climates this panel cooling can yield efficiency improvements in the neighborhood of 10%. That much improvement makes the panel cooling look more like a feature than a bonus.</p>



<h3>Water Cooled Roof</h3>



<p>Under the wrong conditions, <a href="https://www.hgtv.com/lifestyle/clean-and-organize/how-to-cool-a-hot-attic">your attic can reach temperatures as high as 150 degrees</a>. Not only is this detrimental to standard asphalt roofing materials, but that heat is cooking your house! You waste hard earned energy cycling cool air in and throughout your home just to fight back against this suffocating blanket of heat.</p>



<p>So in addition to carrying away heat from your panels, the system is carrying heat away from your attic as well. Once again, we’ve discovered an efficiency gain that which seems almost hidden by the primary features. So now, water heating equals panel cooling AND roof cooling, which of course means you require less energy for cooling your home. </p>



<h3>If you Can’t Stand the Heat Just Use it in the Kitchen, or in the Pool</h3>



<p>Now that you’ve absorbed the efficiency killing heat from your solar panels, and carried it away before allowing it to bake your roof we get to use that heat. In the spirit of zero waste, the thermal energy you’ve collected can now be used for a variety of purposes.</p>



<p>As the water travels to your hot water tank, why not take a circuitous route through channels in the floor for heated floors? Or perhaps you really want a heated pool, but don’t want a massive energy bill. Many processes can be made more efficient by supplementing with heat carried away from your roof. <a href="http://wepowr.com/technology/shw/winter">Even in cooler climates</a>.</p>



<figure><img title="Screen-Shot-2022-09-04-at-93130-PM - Evergreen Off-Grid" decoding="async" width="1024" height="664" src="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/Screen-Shot-2022-09-04-at-93130-PM.png?resize=1024%2C664&amp;ssl=1" data-src="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/Screen-Shot-2022-09-04-at-93130-PM.png?resize=1024%2C664&amp;ssl=1" alt="" data-srcset="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/Screen-Shot-2022-09-04-at-93130-PM.png?resize=1024%2C664&amp;ssl=1 1024w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/Screen-Shot-2022-09-04-at-93130-PM.png?resize=300%2C195&amp;ssl=1 300w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/Screen-Shot-2022-09-04-at-93130-PM.png?resize=768%2C498&amp;ssl=1 768w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/Screen-Shot-2022-09-04-at-93130-PM.png?resize=600%2C389&amp;ssl=1 600w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/Screen-Shot-2022-09-04-at-93130-PM.png?w=1058&amp;ssl=1 1058w" data-sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"><figcaption>Image Courtesy of Tractile</figcaption></figure>



<h3>What About the Big Bad Wolf?</h3>



<p>For those living under constant and rising threat of <a title="Complete Earthquake Bag - 3 Day Emergency kit for Earthquakes, Hurricanes, Wildfires, Floods + Other disasters" href="https://evergreenoffgrid.com/recommends/complete-earthquake-bag-3-day-emergency-kit-for-earthquakes-hurricanes-wildfires-floods-other-disasters/" data-shortcode="true">hurricanes</a> or cyclones, mounting solar racking and expensive panels to your roof may not be the best idea. Some places are so vulnerable to these threats that insurance companies refuse to offer rooftop solar coverage. With traditional rooftop solar racking the channel under the panels can create a wind tunnel that can tear the racking from your roof in a hurricane. </p>



<p><a href="https://evergreenoffgrid.com/ready-for-a-new-roof-learn-about-integrated-solar/" title="Ready for a New Roof? Learn About Integrated Solar!">Roof integrated solar</a> is much more secure. When Mother Nature comes huffing and puffing, the Eclipse Roof Tiles are prepared for Category 5 <a title="Complete Earthquake Bag - 3 Day Emergency kit for Earthquakes, Hurricanes, Wildfires, Floods + Other disasters" href="https://evergreenoffgrid.com/recommends/complete-earthquake-bag-3-day-emergency-kit-for-earthquakes-hurricanes-wildfires-floods-other-disasters/" data-shortcode="true">Hurricanes</a> and Cyclones. Eclipse Solar Tiles can withstand winds of up to 280km/hr (177mph). </p>



<p>In addition to being more stout than traditional asphalt shingles in the face of heavy winds, these tiles are resistant to large hail and also come with a BAL-40 rating, which means they’re well protected from brush fires. Learn more about BAL-40 rating <a href="https://www.no1roofing.com.au/bal-40/">here</a>.</p>



<figure>
<figure><img title="IMG_6655-7 - Evergreen Off-Grid" decoding="async" width="312" height="391" data-id="1111" src="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/IMG_6655-7.jpg?resize=312%2C391&amp;ssl=1" data-src="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/IMG_6655-7.jpg?resize=312%2C391&amp;ssl=1" alt="" data-srcset="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/IMG_6655-7.jpg?w=312&amp;ssl=1 312w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/IMG_6655-7.jpg?resize=239%2C300&amp;ssl=1 239w" data-sizes="(max-width: 312px) 100vw, 312px" data-recalc-dims="1"></figure>



<figure><img title="IMG_6655-4 - Evergreen Off-Grid" decoding="async" width="259" height="362" data-id="1108" src="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/IMG_6655-4.jpg?resize=259%2C362&amp;ssl=1" data-src="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/IMG_6655-4.jpg?resize=259%2C362&amp;ssl=1" alt="" data-srcset="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/IMG_6655-4.jpg?w=259&amp;ssl=1 259w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/IMG_6655-4.jpg?resize=215%2C300&amp;ssl=1 215w" data-sizes="(max-width: 259px) 100vw, 259px" data-recalc-dims="1"></figure>



<figure><img title="IMG_6655-6 - Evergreen Off-Grid" decoding="async" width="311" height="386" data-id="1110" src="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/IMG_6655-6.jpg?resize=311%2C386&amp;ssl=1" data-src="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/IMG_6655-6.jpg?resize=311%2C386&amp;ssl=1" alt="" data-srcset="https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/IMG_6655-6.jpg?w=311&amp;ssl=1 311w, https://i0.wp.com/evergreenoffgrid.com/wp-content/uploads/2022/09/IMG_6655-6.jpg?resize=242%2C300&amp;ssl=1 242w" data-sizes="(max-width: 311px) 100vw, 311px" data-recalc-dims="1"></figure>
<figcaption>Images Courtesy of Tractile</figcaption></figure>



<h2>So What’s the Price Tag?</h2>



<p>Tractile is not in the US market yet, but there are plans to do so. According to a company representative, once they do enter the US market they can supply the Eclipse Solar Roof Tile system at, “a comparable price to conventional roofing and solar.” If so, this will be inline with GAF’s Solar Shingle product and less costly than Tesla’s Solar Roof. The only difference is this product is better… way better. </p>



<p>If this product is available when it’s time for a new roof or when designing new construction the Eclipse Solar Roof Tile is definitely worth a look. Why add a dumb roof when you can get one that keeps you cool, heats your water, powers your home and keeps you secure under the heaviest of nature’s assaults.</p>



<h2>The Solar Decathlon </h2>



<p>In 2002, the US Department of Energy rolled out the <a href="https://www.solardecathlon.gov/about.html">Solar Decathlon</a>. The Solar Decathlon is an event where future building professionals from participating colleges compete and showcase their most effective and innovative solutions for addressing the world’s energy crisis. Since its inception, the decathlon has challenged more than 25,000 students representing over 40 countries in events held around the world. </p>



<h3>2018 Middle East Solar Decathlon</h3>



<p>The Eclipse Solar Roof Tiles were introduced to the world as a part of a net positive energy home called the Desert Rose House, sponsored by a school south of Sydney, Australia, called the University of Wollongong (UOW). </p>



<p>Among other awards, Team UOW took 1st in innovation, comfort and interior design, and 2nd in building efficiency and building integrated photovoltaics. Read more in the Tractile press release <a href="https://tractile.com.au/news/tractile-triumphs-in-the-desert/">here</a>.</p>



<p>And check out their video…</p>



<figure><p>
<iframe loading="lazy" title="The Desert Rose House Explained: BIPVT" width="1140" height="641" src="https://www.youtube.com/embed/zRroeLyBG1U?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</p></figure>



<h2>Thank You!</h2>



<p>Thank you for checking out our blog! If you want to read more like this please consider subscribing below. And, as always, please contribute your thoughts below.</p>



		

		
		

<!-- This site is converting visitors into subscribers and customers with OptinMonster - https://optinmonster.com :: Campaign Title: Subscribe Now! -->


<!-- / OptinMonster -->


<h3>Join the Conversation</h3>



<p>What do you think about the Eclipse Solar Roof Tiles? Let us know in the comments at the bottom of the page. Any thoughts on use in cooler climates? If the hot water heat exchanger can keep tiles from being covered with snow and frost that would be a huge benefit in some places. What other advantages or disadvantages do you see with this system. Let us know!</p>


<!-- This site is converting visitors into subscribers and customers with OptinMonster - https://optinmonster.com :: Campaign Title: Facebook Group: Solar DIY EOG -->


<!-- / OptinMonster -->
<!-- This site is converting visitors into subscribers and customers with OptinMonster - https://optinmonster.com :: Campaign Title: Subscribe Now! -->


<!-- / OptinMonster -->				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[eSignature for Google Docs and Google Drive (Beta) (399 pts)]]></title>
            <link>https://workspaceupdates.googleblog.com/2023/08/esignature-google-docs-google-drive.html</link>
            <guid>37079534</guid>
            <pubDate>Thu, 10 Aug 2023 17:59:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://workspaceupdates.googleblog.com/2023/08/esignature-google-docs-google-drive.html">https://workspaceupdates.googleblog.com/2023/08/esignature-google-docs-google-drive.html</a>, See on <a href="https://news.ycombinator.com/item?id=37079534">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<center>
			<h4>
				<a href="https://www.googlecloudcommunity.com/gc/Google-Workspace/ct-p/google-workspace" target="_blank">Join the official community for Google Workspace administrators</a>
			</h4>
			<p>
				In the Google Cloud Community, connect with Googlers and other Google Workspace admins like yourself. Participate in product discussions, check out the Community Articles, and learn tips and tricks that will make your work and life easier. Be the first to know what's happening with Google Workspace.
			</p>
<p>______________
			</p>            
            	<h4>
				<a href="https://support.google.com/a/go/whatsnew" target="_blank">Learn about more Google Workspace launches</a>
			</h4>
			<p>
				On the “What’s new in Google Workspace?” Help Center page, learn about new products and features launching in Google Workspace, including smaller changes that haven’t been announced on the Google Workspace Updates blog.
			</p>
<p>______________
			</p>            
            	</center>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon Cuts Dozens of House Brands as It Battles Costs, Regulators (102 pts)]]></title>
            <link>https://www.wsj.com/articles/amazon-cuts-dozens-of-house-brands-as-it-battles-costs-regulators-3f6ad56d</link>
            <guid>37079438</guid>
            <pubDate>Thu, 10 Aug 2023 17:53:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/articles/amazon-cuts-dozens-of-house-brands-as-it-battles-costs-regulators-3f6ad56d">https://www.wsj.com/articles/amazon-cuts-dozens-of-house-brands-as-it-battles-costs-regulators-3f6ad56d</a>, See on <a href="https://news.ycombinator.com/item?id=37079438">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
This copy is for your personal, non-commercial use only. Distribution and use of this material are governed by
our Subscriber Agreement and by copyright law. For non-personal use or to order multiple copies, please contact
Dow Jones Reprints at 1-800-843-0008 or visit www.djreprints.com.
</p><p>https://www.wsj.com/articles/amazon-cuts-dozens-of-house-brands-as-it-battles-costs-regulators-3f6ad56d</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vim Boss (1114 pts)]]></title>
            <link>https://neovim.io/news/2023/08</link>
            <guid>37078719</guid>
            <pubDate>Thu, 10 Aug 2023 17:02:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://neovim.io/news/2023/08">https://neovim.io/news/2023/08</a>, See on <a href="https://news.ycombinator.com/item?id=37078719">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      

      <p><i>August 2023</i>
      </p>

      <p>Bram is one of my heroes. That’s literal and recursive: when I say it,
internally I check before making a frivolous claim, which is a feature of this
particular role-model; “What would Bram do?” is a fixture in me which informs my
choices.</p>

<p>Those who studied <code>vim_dev</code> and the Vim source and docs, accumulated treasure
from a stream of copious messages and spare impressions. But also from what he
omitted: he never appealed to sensationalism or personal judgements.</p>

<p>Even when treated rudely, Bram usually responded only to advance his
understanding of a problem to solve. Bram was one of those humans quietly
providing deep value to the universe, but there was no parade and little
celebrity.</p>

<p>Bram was anchored to reality, directly interested in results and adjusting what
produced them. The “Problem/Solution” mantra in his commit messages is simple
yet profoundly effective. He used that approach to help <a href="https://www.moolenaar.net/albums.html">people in
Uganda</a>, managing resources directly
instead of abstractly.</p>

<p>Bram’s principles (as I observed them) extended beyond mere technical
craftsmanship. The ability to adopt a position of <em>modesty</em> is a mind-trick that
channels an endeavor through a “narrow waist”, a voluntary constraint. That lens
can create a more composable and powerful result. Plugins like
<a href="https://github.com/tpope/vim-unimpaired">unimpaired</a> riff on the theme. And
this touches on a central point: the main utility—not ideology, but <em>utility</em>—of
“lifestyle software” like Emacs and Vim, is that the ecosystem is alive, and has
escape velocity, so its momentum is self-perpetuated.</p>

<p>Neovim has always been intentionally positioned as a derivative of Vim, which
means simultaneously it both continues and diverges from Vim. I’m convinced that
<a href="https://twitter.com/justinmk/status/1671916719143526400">forks create energy</a>
rather than destroy it. So although we can’t deliver Vim without Bram, we can
continue some essential parts:</p>

<ul>
  <li>
<a href="https://neovim.io/doc/user/develop.html#design-maintain">Maintenance</a>:
Experimentation is good, and the world needs creative destruction and playful
failures. But Neovim does not represent lust for the new (“neomania”).</li>
  <li>
<a href="https://neovim.io/doc/user/develop.html#design-documented">Documentation</a>:
the habits of Vim documentation are obvious, this is one of the biggest gains
that Nvim acquired by building on vim.</li>
  <li>Extensibility: Bram’s own <a href="http://www.agide.org/">Agide</a> project aspired to
a similar sort of extensibility as Neovim:
    <blockquote>
      <p>Agide is not a monolitic application. Separate tools can be plugged in. Thus
you are not forced to use one editor. … Each tool implements part of the
plugin interface.</p>
    </blockquote>
  </li>
  <li>
<a href="https://neovim.io/doc/user/develop.html#design-not">Embedding</a>: Vim’s
<a href="https://github.com/vim/vim/blob/531da5955e03afadb2f0cf72264fe8deb4bf430e/runtime/doc/develop.txt#L145-L153">:help design-not</a>
for most of its life proclaimed this tenet of Neovim:
    <blockquote>
      <p>Vim is not a shell or an Operating System. …  This should work the other way
around: Use Vim as a component from a shell or in an IDE.</p>
    </blockquote>
  </li>
</ul>

<p>And another thing: Bram didn’t take himself too seriously. He had his own sense
of humor.</p>

<p>Neovim is a monument to Vim and Bram. We should be pragmatic, not dogmatic; we
should remember what the goal is, and compare our actions to the results.</p>

<hr>

<p><em>— Justin M. Keyes</em></p>

<p>P.S. Jan van den Berg wrote <a href="https://j11g.com/2023/08/07/the-legacy-of-bram-moolenaar/">a nice post on Bram’s legacy</a>.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CIQ, Oracle, SUSE Create Open Enterprise Linux Association (157 pts)]]></title>
            <link>https://www.suse.com/news/OpenELA-for-a-Collaborative-and-Open-Future/</link>
            <guid>37078423</guid>
            <pubDate>Thu, 10 Aug 2023 16:40:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.suse.com/news/OpenELA-for-a-Collaborative-and-Open-Future/">https://www.suse.com/news/OpenELA-for-a-Collaborative-and-Open-Future/</a>, See on <a href="https://news.ycombinator.com/item?id=37078423">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>RENO, Nev., AUSTIN, Texas, and LUXEMBOURG</p>

<p><span><span><span><span><span>CIQ, Oracle and SUSE today announced their intent to form the <a href="https://openela.org/">Open Enterprise Linux Association (OpenELA)</a>, a collaborative trade association to encourage the development of distributions compatible with Red Hat Enterprise Linux (RHEL) by providing open and free Enterprise Linux (EL) source code.</span></span></span></span></span></p>

<p><span><span><span><span><span><span>The formation of OpenELA arises from Red Hat’s recent changes to RHEL source code availability</span></span></span><span><span>. In response, CIQ, Oracle and SUSE are collaborating to deliver source code, tools and </span></span><span>systems through OpenELA for the community.</span></span></span></span></p>

<p><span><span><span><span><span><span>“Collaboration is critical to fostering innovation, which is why we welcome everyone to be part of this association and help us uphold open community standards,” said Thomas Di Giacomo, chief technology and product officer of SUSE. “SUSE is a strong believer in making choice happen. Together with the open source community we will redefine what it truly means to be open and deliver a stronger future for EL.”&nbsp;</span></span></span></span></span></span></p>

<p><span><span><span><span><span><span>“Today's announcement marks the beginning of a new era for EL,” said Gregory Kurtzer, CEO of CIQ. “With OpenELA, CIQ, Oracle and SUSE join forces with the open source community to ensure a stable and resilient future for both upstream and downstream communities to leverage Enterprise Linux."</span></span></span></span></span></span></p>

<p><span><span><span><span><span><span>“Many large organizations reached out to us to express the importance of community-driven source code for EL that can act as a starting point for compatible distributions,” said </span></span></span><span>Wim Coekaerts, head of Oracle Linux development, Oracle. “</span><span>OpenELA is our response to this need, and it represents a commitment to helping the open source community continue to develop compatible EL distributions.”</span></span></span></span></p>

<p><span><span><span><span><span><span>Starting later this year, OpenELA will provide sources necessary for downstreams compatible with RHEL to exist, with initial focus on RHEL versions EL8, EL9 and possibly EL7. The project is committed to ensuring the continued availability of OpenELA sources to the community indefinitely.</span></span></span></span></span></span></p>

<p><span><span><span><span><span><span>OpenELA's core tenets, reflecting the spirit of the project, include full compliance with this existing standard, swift updates and secure fixes, transparency, community, and ensuring the resource remains free and redistributable for all.&nbsp;</span></span></span></span></span></span></p>

<p><span><span><span><span><span><span>By welcoming other organizations and community members to join and contribute actively, OpenELA seeks to build a robust, community-driven standard that ensures impartiality and equilibrium in the EL ecosystem.</span></span></span></span></span></span></p>

<p><span><span><span><span><span><span>For more information about and to get involved with OpenELA, please visit <a href="https://openela.org/join/">openela.org/join/</a></span>.&nbsp;</span></span></span></span></span></p>

<p><span><span><span><span><span><img alt="openela-for-a-Collaborative-and-Open-Future" data-entity-type="" data-entity-uuid="" src="https://cms.suse.net/sites/default/files/openela-for-a-Collaborative-and-Open-Future.png"></span></span></span></span></span></p>

<p><span><span><strong><span><span><span>About CIQ</span></span></span></strong></span></span></p>

<p><span><span><span><span><span><span>CIQ powers the next generation of software infrastructure, leveraging capabilities from enterprise, cloud, hyperscale and HPC. From the base operating system, through containers, orchestration, provisioning, computing, and up to cloud applications, CIQ works with every part of the technology stack to drive solutions for customers and communities with stable, scalable, secure production environments. CIQ is the founding support and services partner of </span></span></span></span><a href="https://rockylinux.org/"><span><span><span><span><span><span>Rocky Linux</span></span></span></span></span></span></a><span><span><span>, and the creator of the next generation federated computing stack. For more information, please visit </span></span></span><a href="https://ciq.com/?utm_source=web&amp;utm_medium=press-release&amp;utm_campaign=CIQ&amp;utm_id=ciq-expands-leadership-team"><span><span><span><span><span><span>ciq.com</span></span></span></span></span></span></a><span><span><span>.</span></span></span></span></span></p>

<p><span><span><span><strong><span><span><span>About Oracle</span></span></span></strong></span></span></span></p>

<p><span><span><span><span><span><span>Oracle offers integrated suites of applications plus secure, autonomous infrastructure in the Oracle Cloud. For more information about Oracle (NYSE: ORCL), please visit us at </span></span></span></span><a href="https://www.oracle.com/apac/index.html"><span><span><span><span><span><span>www.oracle.com.</span></span></span></span></span></span></a></span></span></p>

<p><span><span><strong><span><span>Trademarks</span></span></strong></span></span></p>

<p><span><span><span><span><span><span><span><span>Oracle, Java, MySQL and NetSuite are registered trademarks of Oracle Corporation, NetSuite was the first cloud company – ushering in a new era of cloud computing. </span></span></span></span></span></span></span></span></p>

<p><span><span><span><strong><span><span><span>About SUSE</span></span></span></strong></span></span></span></p>

<p><span><span><span><span lang="EN"><span>SUSE is a global leader in innovative, reliable and secure enterprise-grade open source solutions, relied upon by more than 60% of the Fortune 500 to power their mission-critical workloads. The company behind Rancher, NeuVector and SUSE Linux Enterprise (SLE), SUSE collaborates with partners and communities to empower customers to innovate everywhere – from the data center to the cloud, to the edge and beyond. SUSE puts the “open” back in open source, giving customers the ability to tackle innovation challenges today and the freedom to evolve their strategy and solutions tomorrow. The company employs more than 2,400 people globally and is listed on the Frankfurt Stock Exchange. For more information, visit </span></span><a href="http://www.suse.com/"><span><span><span><span>www.suse.com</span></span></span></span></a><span>.</span></span></span></span></p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MIT Pirate Certificate (181 pts)]]></title>
            <link>https://physicaleducationandwellness.mit.edu/about/pirate-certificate/</link>
            <guid>37078047</guid>
            <pubDate>Thu, 10 Aug 2023 16:12:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://physicaleducationandwellness.mit.edu/about/pirate-certificate/">https://physicaleducationandwellness.mit.edu/about/pirate-certificate/</a>, See on <a href="https://news.ycombinator.com/item?id=37078047">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>The MIT Pirate Certificate became available in the Fall of 2011. Students who have completed Archery, Fencing, Pistol (Air Pistol or Rifle) and Sailing should send an email to <a href="mailto:ahoymitpe.mit.edu?subject=Arrr....I've%20completed%20the%20requirements%20of%20a%20pirate!">ahoymitpe@mit.edu</a> with name and MIT ID number once grades are posted for all four courses. Check <a href="https://eduapps.mit.edu/mitpe/student" target="_blank" rel="noopener">PE&amp;W history</a> after each quarter.</p>
<p>Follow us at <a href="https://www.facebook.com/mitpe" target="_blank" rel="noopener">https://www.facebook.com/mitpe</a> for updates.</p>
<p>The MIT Pirate Certificate is designed to recognize the completion of the undergraduate Physical Education &amp; Wellness General Institute Requirement.</p>
<p>The MIT Pirate Certificate is <strong>only made available to MIT students</strong> and is&nbsp;<strong>an incentive for undergraduate students to complete their Physical Education &amp; Wellness General Institute Requirement</strong> of 4 physical education &amp; wellness courses. <strong>It is not a stand-alone certificate</strong>. Non-MIT courses and life experience are not counted towards completing the certificate. <strong>The MIT Pirate Certificate is for entertainment purposes only</strong> and does not give the recipient license to engage in piracy or any pirate activities.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Virgin Galactic successfully flies tourists to space for first time (116 pts)]]></title>
            <link>https://www.theguardian.com/science/2023/aug/10/vigin-galactic-space-flight-vss-unity-landing</link>
            <guid>37077977</guid>
            <pubDate>Thu, 10 Aug 2023 16:07:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/science/2023/aug/10/vigin-galactic-space-flight-vss-unity-landing">https://www.theguardian.com/science/2023/aug/10/vigin-galactic-space-flight-vss-unity-landing</a>, See on <a href="https://news.ycombinator.com/item?id=37077977">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Virgin Galactic’s VSS Unity, the reusable <a href="https://www.theguardian.com/science/2023/jun/29/virgin-galactic-rocket-plane-commercial-space-flight" data-link-name="in body link">rocket-powered space plane</a> carrying the company’s first crew of tourists to space, successfully launched and landed on Thursday.</p><p>The mission, known as Galactic 02, took off shortly after 11am ET from Spaceport America in <a href="https://www.theguardian.com/us-news/newmexico" data-link-name="in body link" data-component="auto-linked-tag">New Mexico</a>.</p><p>Aboard the spacecraft were six individuals total – the space plane’s commander and former Nasa astronaut CJ Sturckow, the pilot Kelly Latimer, as well as Beth Moses, Virgin Galactic’s chief astronaut instructor who trained the crew before to the flight.</p><p>The spacecraft also carryied three private passengers, including the health and wellness coach Keisha Schahaff and her 18-year-old daughter, Anastasia Mayers, both of whom are Antiguan.</p><p><a href="https://www.space.com/virgin-galactic-galactic02-launch-time" data-link-name="in body link">According to</a> Space.com, Schahaff won her seat aboard the Galactic 02 as part of a fundraising competition by Space for Humanity, a non-profit organization seeking to democratize space travel. Mayers is studying philosophy and physics at Aberdeen University in Scotland. Together, Schahaff and Mayers are the first mother-daughter duo to venture to space together.</p><p>“When I was two years old, just looking up to the skies, I thought, ‘How can I get there?’ But, being from the Caribbean, I didn’t see how something like this would be possible. The fact that I am here, the first to travel to space from Antigua, shows that space really is becoming more accessible,” Schahaff said in a <a href="https://www.virgingalactic.com/news/virgin-galactic-broadens-access-to-space-with-first-private-astronaut" data-link-name="in body link">statement</a> last month.</p><p>The mission also marks the most women flown in a single mission to space.</p><figure id="e1ef61b3-5e26-46f1-b8b0-59a132e0f378" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-1"><picture><source srcset="https://i.guim.co.uk/img/media/e1b0c6c3de56683bdc4aaeb3c672258a503bb987/0_0_1913_1066/master/1913.jpg?width=620&amp;dpr=2&amp;s=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/e1b0c6c3de56683bdc4aaeb3c672258a503bb987/0_0_1913_1066/master/1913.jpg?width=620&amp;dpr=1&amp;s=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/e1b0c6c3de56683bdc4aaeb3c672258a503bb987/0_0_1913_1066/master/1913.jpg?width=605&amp;dpr=2&amp;s=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/e1b0c6c3de56683bdc4aaeb3c672258a503bb987/0_0_1913_1066/master/1913.jpg?width=605&amp;dpr=1&amp;s=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/e1b0c6c3de56683bdc4aaeb3c672258a503bb987/0_0_1913_1066/master/1913.jpg?width=445&amp;dpr=2&amp;s=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/e1b0c6c3de56683bdc4aaeb3c672258a503bb987/0_0_1913_1066/master/1913.jpg?width=445&amp;dpr=1&amp;s=none" media="(min-width: 320px)"><img alt="A still image taken from a video from Virgin Galactic shows the launch of Virgin Galactic’s private astronaut mission Galactic 02 on 10 August." src="https://i.guim.co.uk/img/media/e1b0c6c3de56683bdc4aaeb3c672258a503bb987/0_0_1913_1066/master/1913.jpg?width=445&amp;dpr=1&amp;s=none" width="445" height="247.9717720857292" loading="lazy"></picture></div><figcaption><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>A still image taken from a video from Virgin Galactic shows the launch of Virgin Galactic’s private astronaut mission Galactic 02 on 10 August.</span> Photograph: Virgin Galactic/AFP/Getty Images</figcaption></figure><p>Aboard the flight was also the former Olympian Jon Goodwin, who participated in the 1972 Olympics in Munich as a canoeist. At 80 years old, Goodwin was the second passenger with Parkinson’s disease and the first Olympian to embark on a trip to space.</p><p>“When I was diagnosed with Parkinson’s in 2014, I was determined not to let it stand in the way of living life to the fullest. And now for me to go to space with Parkinson’s is completely magical,” he said in a news release. “I hope this inspires all others facing adversity and shows them that challenges don’t have to inhibit or stop them from pursuing their dreams,” Goodwin <a href="https://www.virgingalactic.com/news/virgin-galactic-broadens-access-to-space-with-first-private-astronaut" data-link-name="in body link">said</a>.</p><p>Galactic 02 is a suborbital flight. However, despite VSS Unity not reaching orbit, the trajectory allows passengers to experience several minutes of weightlessness at an altitude high enough for them to see the Earth’s curvature, Space.com <a href="https://www.space.com/virgin-galactic-galactic02-launch-time" data-link-name="in body link">explains</a>.</p><p>Following liftoff, Virgin Galactic’s carrier plane VMS Eve transported VSS Unity to an altitude of about 44,300ft. Eve then dropped Unity which then fired its own rocket motor and ascended to suborbital space. Passengers aboard experienced approximately 3Gs.</p><figure id="05ba7fdf-3978-4d25-a7da-4cfa66759d31" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-2"><picture><source srcset="https://i.guim.co.uk/img/media/1f771bd32941f73789015b2bb12ca01deda83c90/62_0_1439_863/master/1439.jpg?width=620&amp;dpr=2&amp;s=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/1f771bd32941f73789015b2bb12ca01deda83c90/62_0_1439_863/master/1439.jpg?width=620&amp;dpr=1&amp;s=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/1f771bd32941f73789015b2bb12ca01deda83c90/62_0_1439_863/master/1439.jpg?width=605&amp;dpr=2&amp;s=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/1f771bd32941f73789015b2bb12ca01deda83c90/62_0_1439_863/master/1439.jpg?width=605&amp;dpr=1&amp;s=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/1f771bd32941f73789015b2bb12ca01deda83c90/62_0_1439_863/master/1439.jpg?width=445&amp;dpr=2&amp;s=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/1f771bd32941f73789015b2bb12ca01deda83c90/62_0_1439_863/master/1439.jpg?width=445&amp;dpr=1&amp;s=none" media="(min-width: 320px)"><img alt="Anastasia Mayers looks out of the windows while in space." src="https://i.guim.co.uk/img/media/1f771bd32941f73789015b2bb12ca01deda83c90/62_0_1439_863/master/1439.jpg?width=445&amp;dpr=1&amp;s=none" width="445" height="266.87630298818624" loading="lazy"></picture></div><figcaption><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>Anastasia Mayers looks out of the spacecraft’s windows while in space.</span> Photograph: Virgin Galactic/AFP/Getty Images</figcaption></figure><p><a href="https://twitter.com/virgingalactic/status/1689648950620598273?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1689648950620598273%7Ctwgr%5Ecd874548a3c4e92149a3ec218c0068228fb881ba%7Ctwcon%5Es1_c10&amp;ref_url=https%3A%2F%2Fnews.sky.com%2Fstory%2Fvirgin-galactic-space-launch-live-updates-british-man-80-and-mother-daughter-duo-on-first-spaceflight-for-tourists-12937126" data-link-name="in body link">Live footage</a> inside the spacecraft showed the passengers unstrapping themselves from their seats and peering out down to earth through the windows as they floated throughout the spacecraft.</p><p>In a press conference after the flight, Schahaff recounted her experience, saying: “Looking at Earth was the most amazing … It was so comfortable. It really was the best ride ever. I would love to do this again.”</p><p>“This experience has given me this beautiful feeling that if I can do this, I can do anything,” she added.</p><p>Mayers, who is the second-youngest person to go to space, said: “I was shocked at the things that you feel. You are so much more connected to everything than you would expect to be. You felt like a part of the team, a part of the ship, a part of the universe, a part of Earth. It was incredible and I’m still starstruck.”</p><figure data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.NewsletterSignupBlockElement"><a data-ignore="global-link-styling" href="#EmailSignup-skip-link-17">skip past newsletter promotion</a><p id="EmailSignup-skip-link-17" tabindex="0" aria-label="after newsletter promotion" role="note">after newsletter promotion</p></figure><p>To Goodwin, the experience was far more dramatic than he expected.</p><p>“The pure acceleration, Mach 3 [2,301 mph, 3,378 ft per second] in eight and a half seconds was completely surreal. The re-entry was a lot more dramatic than I imagined it would be. In fact, I would have said it was out of control if I didn’t know anything different,” he said.</p><p>“It was a completely surreal experience. But the most impressive thing was looking at Earth from <a href="https://www.theguardian.com/science/space" data-link-name="in body link" data-component="auto-linked-tag">Space</a>. The pure clarity was very moving, quite surreal. It was without a doubt the most exciting day of my life,” he added.</p><p>In a statement released following the flight, Sturckow said: “It is a surreal and humbling experience to have flown Unity today. The wonder and excitement of spaceflight never loses its magic.”</p><p>Latimer echoed similar sentiments, saying: “In my entire career, from the Air Force Academy to being a test pilot for Nasa, nothing tops what I have just experienced at the controls of VSS Unity. Going to space today fulfilled an ambition I’ve had since I was a child.”</p><p>Virgin Galactic founder Sir Richard Branson also <a href="https://twitter.com/richardbranson/status/1689679227325489155?s=20" data-link-name="in body link">hailed</a> the flight, tweeting: “Today we flew three incredible private passengers to space: Keisha Schahaff, Anastatia Mayers and Jon Goodwin. Congratulations Virgin Galactic commercial astronauts 011, 012 and 013 – welcome to the club!”</p><p>Despite Galactic 02 being Virgin Galactic’s second commercial spaceflight mission, it is the first flight to carry private customers. In June, Galactic 01 <a href="https://www.theguardian.com/science/2023/jun/29/virgin-galactic-rocket-plane-commercial-space-flight" data-link-name="in body link">carried</a> three crew members from the Italian air force and the National Research Council of Italy.</p><p>In July 2021, Branson <a href="https://www.theguardian.com/science/2021/jul/11/richard-branson-virgin-galactic-space" data-link-name="in body link">traveled</a> to space and back aboard the VSS Unity, a mission that marked the billionaire’s entry into the new era of space tourism helmed by other billionaires including SpaceX founder Elon Musk and Blue Origin founder Jeff Bezos.</p><p><a href="https://www.reuters.com/science/virgin-galactic-rocket-plane-carried-aloft-commercial-launch-space-2023-06-29/" data-link-name="in body link">According to</a> Virgin Galactic, the company has already booked a backlog of about 800 customers. Tickets have ranged from $250,000 to $450,000.</p><p>Galactic 03, the company’s third commercial spaceflight, is planned for September.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Maybe the problem is that Harvard exists (226 pts)]]></title>
            <link>https://dynomight.net/harvard/</link>
            <guid>37076968</guid>
            <pubDate>Thu, 10 Aug 2023 14:57:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dynomight.net/harvard/">https://dynomight.net/harvard/</a>, See on <a href="https://news.ycombinator.com/item?id=37076968">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <p>Here’s a rant from my <a href="https://marginalrevolution.com/?s=tyrone">evil twin</a> Tyromight, which I publish without endorsement as it appears to be an unhinged polemic with no constructive solutions.</p>

<hr>

<p>Say that when people apply for their first driver’s license, 1% get Executive Platinum licenses. For life, they get free use of toll roads and can drive 20% over the speed limit. People argue—<em>fiercely</em> argue—if these should be awarded based on the written test, the driving test, or based on personal essays on <em>What Driving Means to Me</em>.</p>

<p>That would be weird, right?</p>

<p>Or say there’s a school. When kids enter as five-year-olds, the school deems 5% of them to be Gold Elites. They get special lunches and when they graduate as ten-year-olds, get preferred admission to competitive middle schools.</p>

<p>The question is not if Gold Elites should be chosen based on finger-painting or kickball competitions. The question is, why do they exist at all?</p>

<p>What would happen in schools if we lived in a magical dreamworld?</p>

<p>I think the answer is: Each kid gets whatever experiences maximize their potential.</p>

<p>That’s not controversial, is it? <em>Ideally</em>, they’d learn whatever subjects, in whatever style would best help them flourish into rich, happy, successful adults. Alice might spend her first few years in immersive Czech-language math classes and postpone history and science until she’s older. Bob might study everything in parallel with teachers that use puppetry and interpretive dance.</p>

<p>Picture each student as a dot in the space of possible experiences.</p>

<p><img src="https://dynomight.net/img/harvard/experiences1small.svg" alt="student dots"></p>

<p>In the real world, there are only so many teachers and classrooms. So perhaps it’s necessary to carve up the space of experiences and create one class for each chunk.</p>

<p><img src="https://dynomight.net/img/harvard/experiences2small.svg" alt="student dots partitioned"></p>

<p>(Really, it’s harder than this picture suggests, because many experiences are based on other students. If I want you as my project partner but you want to forget I exist, then something has to give.)</p>

<p>So there are tough questions. What classes should exist? Where do you put the best teachers? Should there be a “gifted” program? Most people acknowledge some tension between what’s best for the “top” students and everyone else. Opinions differ on how to resolve that tension, ranging from “top students in best classrooms with best teachers” to “all students together, with faster students helping others”.</p>

<p>But we agree on the <em>ideal</em>, right? In dreamworld, every kid would follow their own path. There would be no “advanced classes” or “tracks” because those concepts wouldn’t exist.</p>

<p>Now, this dreamworld school would not be a rainbow utopia where all students emerge equal. It’s plausible there would be <em>more</em> variance in outcomes than we have now. But we should still do it if we could.</p>

<p>So if that’s the ideal, then what’s wrong with giving 5% of kids Gold Elite status? Well:</p>

<ul>
  <li>
    <p>It’s decided by a committee, not something that emerges organically.</p>
  </li>
  <li>
    <p>If you <em>must</em> have Gold Elites (why?) you should pick them when they are <em>graduating</em>, not when they start.</p>
  </li>
  <li>
    <p>If someone’s going to make Gold Elites, it damn well shouldn’t be the government or a tax-exempt nonprofit.</p>
  </li>
</ul>

<p>That’s pretty much what’s wrong with Harvard. (And high-stakes college admissions in general).</p>

<h2 id="problem-1-gating-mechanisms-are-bad">Problem #1: Gating mechanisms are bad</h2>

<p>In the limit, this is obvious. Imagine a society in which 18-year olds are assessed and then assigned to different career bands. “Alphas” could be senators or CEOs, while “gammas” could pursue dreams of carrying heavy rocks or carrying heavy pieces of wood. It doesn’t matter how the assessment is done, the idea is dystopic.</p>

<p>College isn’t nearly that consequential. But still, the effect of high-stakes college admissions is to make society a bit more like that.</p>

<p>I think that’s bad because I’ve internalized Western individualist values. (Haven’t you?) But there are practical reasons, too.</p>

<p>One is that when gates exist, the people who control them will put their fingers on the scales, creating all sorts of weird distortions and drama. (Witness: How college admissions currently creates all sorts of weird distortions and drama.)</p>

<p>But forget all that. The deeper reason is that <u>prediction is hard</u>.</p>

<p>Yes, grades and test scores and teacher evaluations are correlated with performance in college and beyond. But they are <em>only correlated</em>. When Carl Bernstein was a student at the University of Maryland, he was kicked off the school paper for bad grades. And yet, Carl Bernstein is extremely good at journalism.</p>

<p>Are there <em>any</em> criteria that would have identified what undergraduate Carl Bernstein was capable of? I doubt it. The only way to really know what a human can do is to let them try.</p>

<p>Yes, people will end up in different jobs somehow. But this should be a fluid process, not something controlled by any central authority. Your ability to become a famous journalist should be determined only by <em>how good you are at journalism</em>. Not by your grades or test scores or your ability to convince some committee of your <em>potential</em>.</p>

<p>And when gating functions exist, <em>cui bono</em>?</p>

<p>Well, who has parents to most help and support them in school? Who gets special tutors if they struggle? Who has school counselors trained to win the college admissions game? Who has the time and resources and connections to publish a book of nature photography or start a charity or volunteer in Guatemala or work in a cancer research lab?</p>

<p>We should have a Marxist version of <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law">Goodhart’s law</a>, something like:</p>

<blockquote>
  <p>When a measure becomes a target, the privileged always find a way to win.</p>
</blockquote>

<p>Sure, I think it’s bizarre that so many people have decided that standardized tests must be eliminated since they favor the rich. <em>Everything</em> favors the rich, but surely an SAT-prep course does less than a personal-essay consultant.</p>

<p>But that misses the point. Standardized tests are not the solution. If you eliminate all advantages for privileged students, you’re still handing out gold stars for no reason. High-stake college admissions based on <em>any</em> criteria make society less meritocratic.</p>

<h2 id="problem-2-if-there-must-be-gating-mechanisms-they-should-come-as-late-as-possible">Problem #2: If there must be gating mechanisms, they should come as late as possible</h2>

<p>Sometimes you gotta gate.</p>

<p>If a war started tomorrow and everyone was drafted, then the military would have to choose who to train to fly fighter jets and who to train to repair submarines. There’s no getting around it. Aren’t colleges like that?</p>

<p>No. The plain truth is that different colleges mostly teach the same stuff.</p>

<p>Don’t believe me? See for yourself. Here are practice exams for first-semester calculus at <a href="https://people.math.harvard.edu/~knill/teaching/math1a2020/final/practice1.pdf">Harvard</a> and <a href="https://www.siue.edu/artsandsciences/math/pdf/final1_scan.pdf">Southern Illinois University Edwardsville</a>. I’d say Harvard’s questions are a <em>little</em> harder, and a little “better” (more fun, better reflect core concepts). But mostly the difference is <em>number of pictures of Harvard</em>.</p>

<p>But isn’t the typical student at Harvard better prepared for calculus? Surely that has some impact?</p>

<p>It does! It has a huge impact: At SIU Edwardsville, <a href="https://nces.ed.gov/ipeds/dfr/2021/ReportPDF.aspx?unitId=149231">54%</a> of students graduate within 6 years. At Harvard, it’s <a href="https://college.harvard.edu/resources/faq/what-harvards-graduation-rate">98%</a>. Harvard does not do selective admissions so that it can push the most talented people to their limit.</p>

<p>The hardest part of Harvard is <em>getting in</em>. Do you think every legacy admit student athlete is a genius?</p>

<p>Say Alice wants to study math but she get rejected from Harvard so she goes to SIU Edwardsville. She struggles for a couple of semesters and then something clicks—she <em>gets</em> math and is at the top of the rest of her classes. She does a great research project and several professors say she’s the strongest undergrad to come through in years.</p>

<p>If Alice wants to be a professor, what are her odds of getting into a top program? (Of <em>course</em> a PhD from a top program is necessary to get a job as a professor!) The reality is: Low. Most PhD admissions committees will decide they don’t know what “strongest student at SIU” means, and pick someone “safer”.</p>

<p>If Alice wants to join the New York Times, or get a job at McKinsey, or go to a top law school so she could have a chance at the Supreme Court—same story. When Harvard rejected her, that made it harder for her to pass through <em>other</em> gating functions and many dreams became harder to reach.</p>

<p>High-stakes college admissions means that much of the value of a college degree is determined <em>before students even start college</em>. If you must mark and sort young people, gross, but OK. But why do it at 18 rather than 22? There’s no justification. No one even <em>suggests</em> a justification. Harvard just does it because it can.</p>

<h2 id="problem-3-gating-mechanisms-certainly-shouldnt-be-created-by-tax-exempt-nonprofits">Problem #3: Gating mechanisms certainly shouldn’t be created by tax-exempt nonprofits</h2>

<p>Some people say Harvard is a private institution and it can do what it wants. These people are wrong.</p>

<p>For one, companies can’t do whatever they want. Just try starting a restaurant that refuses entry to people over 50.</p>

<p>Anyway, private universities are <em>non-profits</em>. You can’t start a tax-exempt country club. If you could, then every business would be a non-profit and no one would pay taxes and the government would collapse.</p>

<p>When John Paulson donated $400 million to Harvard, that was <em>tax-deductible</em>. If we assume his marginal tax rate was 25%, that’s equivalent to him donating $300 million of after-tax money, and then having the government kick in an extra $100 million. Ivy-league universities also earn insane profits on endowments tax-free and are exempt from local property taxes.</p>

<p>Yes, I know Massachusetts has a weird system of passive-aggressive reduced-rate <a href="https://www.boston.gov/departments/assessing/payment-lieu-tax-pilot-program">“voluntary” taxes</a> and Harvard pays $4 million a year. Great! After 25 years, they’ll have repaid the subsidy on Paulson’s donation and can start working on the <em>billions</em> they earn each year from their endowment.</p>

<p>Harvard is clear about their educational mission. It is to create “citizen-leaders”.</p>

<p>Yes, it’s preposterous that they have legacy admissions and other criteria that might as well be designed to favor the rich. But even with the fairest possible admissions, Harvard would still be an organization <em>designed to reduce meritocracy</em>, one with the explicit goal of picking a subset of the population and labeling them as winners. And it would still do that while being subsidized by the rest of society.</p>

<p>When you argue about how it does admissions, you’re accepting the premise that it should exist at all. It only seems reasonable because we’ve been indoctrinated since birth with the idea that “Ivy League = prestige”, and humans are programmed to think prestigious things are good.</p>

<h2 id="does-this-matter">Does this matter?</h2>

<p>Everyone is talking about a <a href="https://opportunityinsights.org/wp-content/uploads/2023/07/CollegeAdmissions_Paper.pdf">recent paper</a> that looks at the impact of getting admitted to an Ivy+ (Ivy or similar) school. They looking at the ratios by which Ivy+ grads are represented in different groups relative to all college grads.</p>

<table>
  <thead>
    <tr>
      <th>Condition</th>
      <th>Ivy+ over-representation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Top 25% income</td>
      <td>1.8×</td>
    </tr>
    <tr>
      <td>Top 10% income</td>
      <td>3.3×</td>
    </tr>
    <tr>
      <td>Top 1% income</td>
      <td>10.1×</td>
    </tr>
    <tr>
      <td>Fortune 500 CEOs</td>
      <td>14.5×</td>
    </tr>
    <tr>
      <td>Top 0.1% income</td>
      <td>16.8×</td>
    </tr>
    <tr>
      <td>Senators</td>
      <td>31.2×</td>
    </tr>
    <tr>
      <td>Attend elite grad school</td>
      <td>32.4×</td>
    </tr>
    <tr>
      <td>Journalists at NYT / WSJ</td>
      <td>32.6×</td>
    </tr>
    <tr>
      <td>Presidents</td>
      <td>52.1×</td>
    </tr>
    <tr>
      <td>Supreme Court justices</td>
      <td>89.2×</td>
    </tr>
  </tbody>
</table>

<p>Of course, this isn’t causal. The people who get into Ivy+ schools are smart and rich and would do well no matter what. To estimate the <em>causal</em> impact, they compare two groups:</p>

<ol>
  <li>
    <p>Those who were put on a waitlist to an Ivy+ school and eventually accepted.</p>
  </li>
  <li>
    <p>Those who were put on a waitlist to an Ivy+ school but not accepted.</p>
  </li>
</ol>

<p>They make some statistical arguments that these groups are similar, so getting off the waitlist is effectively random. Comparing the two groups, here’s how much they find getting accepted increases your chances of various things.</p>

<table>
  <thead>
    <tr>
      <th>Condition</th>
      <th>Causal impact of Ivy+ admission</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>“Mean income rank”</td>
      <td>+1.8%</td>
    </tr>
    <tr>
      <td>Top 25% income</td>
      <td>+2.7%</td>
    </tr>
    <tr>
      <td>Top 10% income</td>
      <td>+8.2%</td>
    </tr>
    <tr>
      <td>Attend grad school</td>
      <td>+28%</td>
    </tr>
    <tr>
      <td>Top 1% income</td>
      <td>+44%</td>
    </tr>
    <tr>
      <td>Attend elite grad school</td>
      <td>+90%</td>
    </tr>
    <tr>
      <td>Work at place where many Ivy+ grads work</td>
      <td>+222%</td>
    </tr>
  </tbody>
</table>

<p>Some point out that Ivy+ admission has little income on mean income rank, and suggest that none of this matters. To this, I have three counterarguments:</p>

<p>First, note—as most journalists did not—that “mean earnings rank” contains the word <em>rank</em>. Your rank is your position on a list of people sorted by incomes. This—unlike mean income—is insensitive to high earners. Roughly speaking, what this says is that for the <em>median person</em>, the causal effect of Ivy+ admissions on income is small.</p>

<p>Second, we don’t have causal estimates for things like becoming a CEO or working at the NYT or becoming a senator. But come on—the causal impacts are strongly correlated with Ivy+ over-representation. And Ivy+ grads are just as over-represented among Senators / NYT journalists / presidents as they are among people who go to elite grad schools.</p>

<p>Third, and most import: These causal estimates are for attending an Ivy+ school, <em>compared to attending a highly selective state school</em> like the University of Michigan. Michigan rejects 80% of applicants! Getting in there is still passing a very difficult gating function. This is all comparing a gold star to a slightly less shiny gold star. Imagine the impact versus places lower down the greasy pole.</p>

<p>Here’s my explanation for what’s happening:</p>

<ul>
  <li>
    <p>Most people, even Ivy+ grads, end up in the world of normal jobs.</p>
  </li>
  <li>
    <p>That world is reasonably meritocratic. What matters is mostly your abilities, not your collection of gold stars. (At least, assuming you have  Michigan-tier gold star.)</p>
  </li>
  <li>
    <p>The “elite” world is an incestuous shell game.</p>
  </li>
</ul>

<p>Harvard <em>exists</em> to make society less meritocratic, and it does that while subsidized by everyone else. Give up. There’s a reasonable argument for putting the top <em>professors</em> together in one school, sure, and maybe even PhD students. But <em>undergraduates</em>? Please. We don’t need to sort and classify 18-year olds. It’s absurd. Stop trying to fix it and get rid of it.</p>

  


  

  

  
  
  

  
  

</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Students’ insight proves that the local-global conjecture doesn’t hold (232 pts)]]></title>
            <link>https://www.quantamagazine.org/two-students-shoot-down-a-widely-believed-math-conjecture-20230810/</link>
            <guid>37076933</guid>
            <pubDate>Thu, 10 Aug 2023 14:54:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/two-students-shoot-down-a-widely-believed-math-conjecture-20230810/">https://www.quantamagazine.org/two-students-shoot-down-a-widely-believed-math-conjecture-20230810/</a>, See on <a href="https://news.ycombinator.com/item?id=37076933">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>They looked exactly as expected: a wall of white, peppered with black specks for smaller integers. “We expected the black dots to peter out,” Stange said. Rickards added, “I thought maybe it would even be possible to prove they peter out.” He speculated that by looking at charts that synthesized many packings together, the team would be able to prove results that weren’t possible when they looked at any one packing on its own.</p>
<p>While Stange was away, Haag wound up plotting every pair of remainders — about 120. No surprises there. Then she went big.</p>
<p>Haag had been plotting how 1,000 integers interact. (The graph is bigger than it sounds, since it involves 1 million possible pairs.) Then she cranked the dial up to 10,000 times 10,000. In one graph, regular rows and columns of black specks refused to dissolve. It looked nothing like what the local-global conjecture would predict.</p>
<p>The team met on a Monday after Stange returned. Haag presented her graphs, and they all focused on the one with the weird dots. “It was just a continual pattern,” Haag said. “And that was when Kate said, ‘What if the local-global conjecture isn’t true?’”</p>
<p>“This looks like a pattern. It has to continue. So the local-global conjecture must be false,” Stange recalled thinking. “James was more skeptical.”</p>
<p>“My first thought was there must be a bug in my code,” Rickards said. “I mean, that was the only reasonable thing I could think of.”</p>
<p>Within half a day, Rickards came around. The pattern ruled out all pairs where the first number is of the form 8 × (3<em>n</em> ± 1)<sup>2</sup> and the second is 24 times any square. This means 24 and 8 never appear in the same packing. Numbers you’d expect to occur don’t.</p>
<p>“I was kind of giddy. It’s not very often that something really surprises you,” Stange said. “But that’s the magic of playing with data.”</p>
<p>The <a href="https://arxiv.org/abs/2307.02749">July paper</a> outlines a rigorous proof that the pattern they observed continues indefinitely, disproving the conjecture. The proof hinges on a centuries-old principle called quadratic reciprocity that involves the squares of two prime numbers. Stange’s team discovered how reciprocity applies to circle packings. It explains why certain curvatures can’t be tangent to each other. The rule, called an obstruction, propagates throughout the whole packing. “It’s just an entirely new thing,” said <a href="https://dept.math.lsa.umich.edu/~lagarias/">Jeffrey Lagarias</a>, a mathematician at the University of Michigan who was a co-author on the 2003 circle-packing paper. “They’ve found it ingeniously,” Sarnak said. “If these numbers did appear, they would violate reciprocity.”</p>
<h2><strong>The Fallout</strong></h2>
<p>A number of other conjectures in number theory may now be in doubt. Like the local-global conjecture, they are hard to prove but have already been shown to hold for virtually all cases and are generally assumed to be true.</p>

<p>For example, Fuchs studies Markov triples, sets of numbers that satisfy the equation <em>x</em><sup>2</sup> + <em>y</em><sup>2</sup> + <em>z</em><sup>2</sup> = 3<em>xyz</em>. She and others have shown that certain types of solutions are connected for prime numbers greater than 10<sup>392</sup>. Everyone believes the pattern should continue to infinity. But in light of the new result, Fuchs has allowed herself to feel a twinge of doubt. “Maybe I’m missing something,” she said. “Maybe everyone’s missing something.”</p>
<p>“Now that we have a single example where it’s false, the question is: Is it false for these other examples too?” Rickards said.</p>
<p>There’s also Zaremba’s conjecture. It says that a fraction with any denominator can be expressed as a continued fraction that uses only the numbers between 1 and 5. In 2014, Kontorovich and Bourgain showed that Zaremba’s conjecture holds for almost all numbers. But the surprise about circle packing has undermined confidence in Zaremba’s conjecture.</p>
<p>If the packing problem is a harbinger of things to come, computational data may be the tool of its undoing.</p>
<p>“I always find it fascinating when new mathematics is born out of just purely looking at data,” Fuchs said. “Without it, it’s really hard to imagine that [they] would have stumbled upon this.”</p>

<p>Stange added that none of this would have happened without the low-stakes summer project. “Serendipity and an attitude of playful exploration both have such a huge role in discovery,” she said.</p>
<p>“It was pure coincidence,” Haag said.&nbsp;“If I didn’t go big enough, we wouldn’t have noticed it.” The work bodes well for the future of number theory. “You can glean understanding of mathematics through your intuition, through proofs,” Stange said. “And you trust that a lot because you spent a lot of time thinking about it. But you can’t argue with the data.”</p>
<p><em>Editor’s note</em>: <em>Alex Kontorovich is a member of </em>Quanta Magazine<em>’s scientific advisory board. He was interviewed for this story but did not otherwise contribute to its production.</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MS Teams channels cannot contain MS-DOS device names (424 pts)]]></title>
            <link>https://learn.microsoft.com/en-us/microsoftteams/limits-specifications-teams</link>
            <guid>37076523</guid>
            <pubDate>Thu, 10 Aug 2023 14:21:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://learn.microsoft.com/en-us/microsoftteams/limits-specifications-teams">https://learn.microsoft.com/en-us/microsoftteams/limits-specifications-teams</a>, See on <a href="https://news.ycombinator.com/item?id=37076523">Hacker News</a></p>
<div id="readability-page-1" class="page">
	<div>
		<a href="#main" tabindex="1">Skip to main content</a>

		

		<div id="unsupported-browser" hidden="">
				<p>This browser is no longer supported.</p>
				<p>Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.</p>
				
			</div>
		<!-- liquid-tag banners global -->
		


			

		
	</div>

	<div data-bi-name="body">


					<div id="main-column">

						<main id="main" role="main" data-bi-name="content" lang="en-us" dir="ltr">
							<!-- article-header -->
							
							<!-- end article-header -->


							

							<!-- end mobile-contents button  -->

							<div>


								<h2 id="limits-and-specifications-for-microsoft-teams">Limits and specifications for Microsoft Teams</h2>


									<div>
											<ul data-bi-name="page info" lang="en-us" dir="ltr">
													<li>
Article													</li>

													<li>
														<time data-article-date="" aria-label="Article review date" datetime="2023-08-01T17:42:00Z" data-article-date-source="calculated">08/01/2023</time>
													</li>
														<li>
															
														</li>
													<li>
														<dl>
															<dt>Applies to:</dt>
															<dd>Microsoft Teams</dd>
														</dl>
													</li>
											</ul>
										</div>

									

										<nav id="center-doc-outline" data-bi-name="intopic toc" role="navigation" aria-label="In this article">
											<h2 id="ms--in-this-article">In this article</h2>
										</nav>

								<!-- <content> -->
									<p>This article describes some of the limits, specifications, and other requirements that apply to Teams.</p>
<h2 id="teams-and-channels">Teams and channels</h2>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Maximum limit</th>
</tr>
</thead>
<tbody>
<tr>
<td>Number of teams a user can create</td>
<td>Subject to a 250 object limit¹</td>
</tr>
<tr>
<td>Number of teams a user can be a member of</td>
<td>1,000²</td>
</tr>
<tr>
<td>Number of members in a team</td>
<td>25,000<sup>6</sup></td>
</tr>
<tr>
<td>Number of owners per team</td>
<td>100</td>
</tr>
<tr>
<td>Number of org-wide teams allowed in a tenant</td>
<td>5²</td>
</tr>
<tr>
<td>Number of members in an <a href="https://learn.microsoft.com/en-us/microsoftteams/create-an-org-wide-team" data-linktype="relative-path">org-wide team</a></td>
<td>10,000</td>
</tr>
<tr>
<td>Number of teams a global admin can create</td>
<td>500,000</td>
</tr>
<tr>
<td>Number of teams a Microsoft 365 or Office 365 organization can have</td>
<td>500,000³</td>
</tr>
<tr>
<td>Number of channels per team</td>
<td>200 (includes deleted channels)<sup>4</sup></td>
</tr>
<tr>
<td>Number of Private channels per team</td>
<td>30 (includes deleted channels)<sup>4</sup></td>
</tr>
<tr>
<td>Number of members in a Private channel</td>
<td>250</td>
</tr>
<tr>
<td>Maximum size of distribution list, security group or Microsoft 365 group that can be imported in to a team</td>
<td>3,500</td>
</tr>
<tr>
<td>Maximum number of members in a Microsoft 365 group that can be converted to a team</td>
<td>10,000<sup>6</sup></td>
</tr>
<tr>
<td>Channel conversation post size</td>
<td>Approximately 28 KB per post<sup>5</sup></td>
</tr>
</tbody>
</table>
<p><sup>1</sup> Any directory object in Azure Active Directory counts towards this limit. Global admins are exempt from this limit, as are apps calling Microsoft Graph using <a href="https://learn.microsoft.com/en-us/graph/permissions-reference" data-linktype="absolute-path">application permissions</a>.</p>
<p><sup>2</sup> This limit includes archived teams.</p>
<p><sup>3</sup> To further increase the number of teams, you must contact Microsoft support and request further increase to the number of Azure Active Directory objects in your tenant. Increase is only made for real-life production scenarios.</p>
<p><sup>4</sup> Deleted channels can be restored within 30 days. During these 30 days, a deleted channel continues to be counted towards the 200 channel or 30 private channel per team limit. After 30 days, a deleted channel and its content are permanently deleted and the channel no longer counts towards the per team limit.</p>
<p><sup>5</sup> 28 KB is an approximate limit because it includes the message itself (text, image links, etc.), @-mentions, number of connectors, and reactions.</p>
<p><sup>6</sup> Shared channels members from outside the team count toward this limit. Further note that teams/channel mentions are blocked in teams with over 10,000 members.</p>
<h3 id="limits-for-shared-channels">Limits for shared channels</h3>
<p>The following table describes the maximum number of channels and members.</p>
<table>
<thead>
<tr>
<th>Maximum...</th>
<th>Value</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Members in a team</td>
<td>25,000</td>
<td>Includes all users in the team and direct members in shared channels.</td>
</tr>
<tr>
<td>Shared channels per team</td>
<td>200</td>
<td>Hosted and shared with the team. (Includes deleted channels during their 30-day recovery window.)</td>
</tr>
<tr>
<td>Teams a channel can be shared with</td>
<td>50</td>
<td>Excluding parent team</td>
</tr>
<tr>
<td>Members in a shared channel</td>
<td>5,000 direct members, including up to 50 teams. (Each team the channel is shared with counts as one member for purposes of this limit.)</td>
<td>Real time updates are only available to 25,000 users at a time and only 25,000 users will appear in the channel list.</td>
</tr>
</tbody>
</table>
<p>The following limitations also apply:</p>
<ul>
<li><p>Only Azure AD work or school accounts are supported for external participants.</p>
</li>
<li><p>Shared channels support tabs except for Stream, Planner, and Forms.</p>
</li>
<li><p>Bots, connectors, and message extensions are not supported.</p>
</li>
<li><p>Org-wide teams are not supported to be added as members of a shared channel.</p>
</li>
<li><p>When you create a team from an existing team, any shared channels in the existing team won't be copied over.</p>
</li>
<li><p>Notifications from shared channels are not included in missed activity emails.</p>
</li>
<li><p>Shared channels are not supported in class teams.</p>
</li>
</ul>
<h2 id="messaging">Messaging</h2>
<h3 id="chat">Chat</h3>
<p>Users who participate in conversations that are part of the chat list in Teams must have an Exchange Online (cloud-based) mailbox for an admin to search chat conversations. That's because conversations that are part of the chat list are stored in the cloud-based mailboxes of the chat participants. If a chat participant doesn't have an Exchange Online mailbox, the admin won't be able to search or place a hold on chat conversations. For example, in an Exchange hybrid deployment, users with on-premises mailboxes might be able to participate in conversations that are part of the chat list in Teams. However, in this case, content from these conversations isn't searchable and can't be placed on hold because the users don't have cloud-based mailboxes. (For more, see <a href="https://learn.microsoft.com/en-us/microsoftteams/exchange-teams-interact" data-linktype="relative-path">How Exchange and Microsoft Teams interact</a>.)</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Maximum limit</th>
</tr>
</thead>
<tbody>
<tr>
<td>Number of people in a private chat<sup>1</sup></td>
<td>250<sup>2</sup></td>
</tr>
<tr>
<td>Number of people in a video or audio call from chat</td>
<td>20</td>
</tr>
<tr>
<td>Number of file attachments<sup>3</sup></td>
<td>10</td>
</tr>
<tr>
<td>Chat size</td>
<td>Approximately 28 KB per post<sup>4</sup></td>
</tr>
</tbody>
</table>
<p><sup>1</sup> If you have more than 20 people in a chat, the following chat features are turned off: Outlook automatic replies and Teams status messages; typing indicator; video and audio calling; sharing; read receipts. The "Set Delivery Options" button (!) is also removed when private group chats contain more than 20 members.</p>
<p><sup>2</sup> Only 200 members at a time can be added to a group chat. <a href="https://learn.microsoft.com/en-us/microsoftteams/troubleshoot/teams-administration/unable-send-message-group-chat" data-linktype="absolute-path">See this article for more information</a>.</p>
<p><sup>3</sup> If the number of attachments exceeds this limit, you'll see an error message.</p>
<p><sup>4</sup> 28 KB is an approximate limit because it includes the message itself (text, image links, etc.), @-mentions, and reactions.</p>
<h3 id="emailing-a-channel">Emailing a channel</h3>
<p>If users want to send an email to a channel in Teams, they use the channel email address. When an email is part of a channel, anyone can reply to it to start a conversation. Here are some of the applicable limits for sending email to a channel.</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Maximum limit</th>
</tr>
</thead>
<tbody>
<tr>
<td>Message size<sup>1<sup></sup></sup></td>
<td>24 KB</td>
</tr>
<tr>
<td>Number of file attachments<sup>2</sup></td>
<td>20</td>
</tr>
<tr>
<td>Size of each file attachment</td>
<td>Less than 10 MB</td>
</tr>
<tr>
<td>Number of inline images<sup>2</sup></td>
<td>50</td>
</tr>
</tbody>
</table>
<div>
<p>Note</p>
<p>There is a throttling limit on how many emails you can send to a channel. The limit is six emails per ten seconds per channel per user and eight emails per ten seconds per tenant per user.</p>
</div>
<p><sup>1</sup> If the message exceeds this limit, a preview message is generated and the user is asked to download and view the original email from the link provided.<br>
<sup>2</sup> If the number of attachments or images exceeds this limit, you'll see an error message.</p>
<p>For more information, see <a href="https://learn.microsoft.com/en-us/office365/servicedescriptions/exchange-online-service-description/exchange-online-limits" data-linktype="absolute-path">Exchange Online limits</a>.</p>
<div>
<p>Note</p>
<p>Message size, file attachments, and inline images limits are the same across all Microsoft 365 and Office 365 licenses. Emailing a channel is not available in Teams for Office GCC/GCCH/DOD organizations.</p>
</div>
<h2 id="channel-names">Channel names</h2>
<p>Channel names can't contain the following characters or words:</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Characters</td>
<td>~ # % &amp; * { } + / \ : &lt; &gt; ? | ' " , ..</td>
</tr>
<tr>
<td>Characters in these ranges</td>
<td>0 to 1F<br>80 to 9F</td>
</tr>
<tr>
<td>Words</td>
<td>forms, CON, CONIN$, CONOUT$, PRN, AUX, NUL, COM1 to COM9, LPT1 to LPT9, desktop.ini,  _vti_</td>
</tr>
</tbody>
</table>
<p>Channel names also can't start with an underscore (_) or period (.), or end with a period (.).</p>
<h2 id="meetings-and-calls">Meetings and calls</h2>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Maximum limit</th>
</tr>
</thead>
<tbody>
<tr>
<td>Number of people in a meeting (can chat and call in)</td>
<td>With Microsoft 365 Business Basic, Microsoft 365 Business Standard, Microsoft 365 Business Premium, and Microsoft 365 A1 plans, you can host online meetings and video calls for up to 300 people using Microsoft Teams. With Microsoft 365 E3/E5, Microsoft 365 A3/A5, and Microsoft 365 Government G3/G5 plans, this limit increases up to 1,000 people.</td>
</tr>
<tr>
<td>Number of people in a video or audio call from chat</td>
<td>20</td>
</tr>
<tr>
<td>Max PowerPoint File Size</td>
<td>2 GB</td>
</tr>
<tr>
<td>Teams keeps <a href="https://learn.microsoft.com/en-us/microsoftteams/meeting-recording" data-linktype="relative-path">meeting recordings</a> that don't get uploaded to Microsoft Stream, available for local download</td>
<td>20 days</td>
</tr>
<tr>
<td>Meeting recording maximum length</td>
<td>4 hours or 1.5 GB. When this limit is reached, the recording will end and automatically restart.</td>
</tr>
</tbody>
</table>
<p>For more information, see <a href="https://learn.microsoft.com/en-us/microsoftteams/quick-start-meetings-live-events" data-linktype="absolute-path">Meetings, webinars, and live events</a>.</p>
<div>
<p>Note</p>
<p>Breakout rooms can only be created in meetings that have fewer than 300 attendees. In addition, creating breakout rooms in a meeting automatically limits the number of meeting attendees to 300. Advise your end-users to not initiate breakout rooms in meetings where they expect more than 300 participants. For more information on large Team meetings, share the guidance <a href="https://support.microsoft.com/office/best-practices-for-a-large-teams-meeting-ce2cdb9a-0546-43a4-bb55-34ab98ab6b16" data-linktype="external">Best practices for a large Teams meeting</a> with your end-users.</p>
</div>
<h3 id="meeting-expiration">Meeting expiration</h3>
<div>
<p>Note</p>
<p>A meeting URL will never stop working. The expiry only relates to any PSTN dial-in numbers, CVI coordinates, and/or underlying meeting policies and settings.</p>
</div>
<table>
<thead>
<tr>
<th>Meeting type</th>
<th>Meeting expires after this much time</th>
<th>Each time you start or update a meeting, expiration extends by this much time</th>
</tr>
</thead>
<tbody>
<tr>
<td>Meet now</td>
<td>Start time + 8 hours</td>
<td>N/A</td>
</tr>
<tr>
<td>Regular with no end time</td>
<td>Start time + 60 days</td>
<td>60 days</td>
</tr>
<tr>
<td>Regular with end time</td>
<td>End time + 60 days</td>
<td>60 days</td>
</tr>
<tr>
<td>Recurring with no end time</td>
<td>Start time + 60 days</td>
<td>60 days</td>
</tr>
<tr>
<td>Recurring with end time</td>
<td>End time of last occurrence + 60 days</td>
<td>60 days</td>
</tr>
</tbody>
</table>
<div>
<p>Note</p>
<p>Microsoft Teams meetings have a time limit of 30 hours.</p>
</div>
<h2 id="live-events">Live Events</h2>
<p>Live events are structured meetings that enable your organization to schedule and produce events that stream to large online audiences—up to 20,000 people. With live events, the audience interaction is a managed Q&amp;A experience.</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Maximum limit</th>
</tr>
</thead>
<tbody>
<tr>
<td>Audience size</td>
<td>Up to 20,000 attendees <sup>1</sup></td>
</tr>
<tr>
<td>Duration of event</td>
<td>4 hours</td>
</tr>
<tr>
<td>Concurrent Live Events running in a Microsoft 365 or Office 365 organization <sup>2</sup></td>
<td>15</td>
</tr>
</tbody>
</table>
<p><sup>1</sup> The usual 10,000 is increased to 20,000 through December 31, 2023. You can schedule even greater numbers with live events in Viva Engage and/or Microsoft Stream. For more information, see <a href="https://learn.microsoft.com/en-us/stream/live-event-m365" data-linktype="absolute-path">Live events across Microsoft 365</a>. Note that events over 20,000 attendees require the <a href="https://learn.microsoft.com/en-us/stream/live-events-assistance" data-linktype="absolute-path">Live Events Assistance Program</a>.</p>
<p><sup>2</sup> You can schedule as many Live Events as you want, but you can only run 15 at a time. As soon as the producer joins a live event, it's considered to be running. The producer who attempts to join the 16th live event gets an error.</p>
<p>For more information about live events, go to <a href="https://learn.microsoft.com/en-us/microsoftteams/teams-live-events/plan-for-teams-live-events#teams-live-events" data-linktype="relative-path">Teams live events</a>. See also <a href="https://support.microsoft.com/office/schedule-a-teams-live-event-7a9ce97c-e1cd-470f-acaf-e6dfc179a0e2" data-linktype="external">Schedule a Teams live event</a>.</p>
<div>
<p>Important</p>
<p><strong>Microsoft 365 live event limit increases</strong></p>
<p><strong>To continue supporting our customers' needs, we will extend temporary limit increases for live events through December 31, 2023, including:</strong></p>
<ul>
<li>Event support for up to 20,000 attendees</li>
<li>50 events can be hosted simultaneously across a tenant</li>
<li>Event duration of 16 hours per broadcast</li>
</ul>
<p>Additionally, Live Events with up to 100,000 attendees can be planned through the Microsoft 365 assistance program. The team will assess each request and work with you to determine options that may be available. <a href="https://aka.ms/Stream/Blog/LiveEventOptions" data-linktype="external">Learn more</a>.</p>
</div>
<h2 id="presence-in-outlook">Presence in Outlook</h2>
<p>Teams presence in Outlook is supported on the Outlook 2013 desktop app and later. To learn more about presence in Teams, see <a href="https://learn.microsoft.com/en-us/microsoftteams/presence-admins" data-linktype="relative-path">User presence in Teams</a>.</p>
<h2 id="storage">Storage</h2>
<p>Each team in Microsoft Teams has a team site in SharePoint Online, and each channel in a team gets a folder within the default team site document library. Files shared within a conversation are automatically added to the document library, and permissions and file security options set in SharePoint are automatically reflected within Teams.</p>
<div>
<p>Note</p>
<p>Each <a href="https://learn.microsoft.com/en-us/microsoftteams/private-channels" data-linktype="relative-path">private channel</a> has its own SharePoint site (previously called "site collection").</p>
</div>
<p>If you don't have SharePoint Online enabled in your tenant, Microsoft Teams users cannot always share files in teams. Users in private chat also cannot share files because OneDrive for Business (which is tied to the SharePoint license) is required for that functionality.</p>
<p>By storing the files in the SharePoint Online document library and OneDrive for Business, all compliance rules configured at the tenant level will be followed. (For more, see <a href="https://learn.microsoft.com/en-us/microsoftteams/sharepoint-onedrive-interact" data-linktype="relative-path">How SharePoint Online and OneDrive for Business interact with Microsoft Teams</a>.)</p>
<p>Because Teams runs on a SharePoint Online backend for file sharing, SharePoint limitations apply to the Files section within a Team. Here are the applicable storage limits for SharePoint Online.</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Microsoft 365 Business Basic</th>
<th>Microsoft 365 Business Standard</th>
<th>Office 365 Enterprise E1</th>
<th>Office 365 Enterprise E3</th>
<th>Office 365 Enterprise E5</th>
<th>Office 365 Enterprise F1</th>
</tr>
</thead>
<tbody>
<tr>
<td>Storage</td>
<td>1 TB per organization plus 10 GB per license purchased</td>
<td>1 TB per organization plus 10 GB per license purchased</td>
<td>1 TB per organization plus 10 GB per license purchased</td>
<td>1 TB per organization plus 10 GB per license purchased</td>
<td>1 TB per organization plus 10 GB per license purchased</td>
<td>1 TB per organization</td>
</tr>
<tr>
<td>Storage for Teams Files</td>
<td>Up to 25 TB per site or group</td>
<td>Up to 25 TB per site or group</td>
<td>Up to 25 TB per site or group</td>
<td>Up to 25 TB per site or group</td>
<td>Up to 25 TB per site or group</td>
<td>Up to 25 TB per site or group</td>
</tr>
<tr>
<td>File upload limit  (per file)</td>
<td>250 GB</td>
<td>250 GB</td>
<td>250 GB</td>
<td>250 GB</td>
<td>250 GB</td>
<td>250 GB</td>
</tr>
</tbody>
</table>
<p>Channels are backed by folders within the SharePoint Online site (previously called "site collection") created for the team, so file tabs within Channels share the storage limits of the team they belong to.</p>
<p>For more information, see <a href="https://support.office.com/article/SharePoint-Online-limits-8f34ff47-b749-408b-abc0-b605e1f6d498" data-linktype="external">SharePoint Online limits</a>.</p>
<h2 id="class-teams">Class teams</h2>
<p>Microsoft Teams for Education provides templates designed for unique education scenarios, such as classroom teaching. More information about team types, including class teams, is available in <a href="https://support.microsoft.com/office/choose-a-team-type-to-collaborate-in-microsoft-teams-0a971053-d640-4555-9fd7-f785c2b99e67" data-linktype="external">Choose a team type to collaborate in Microsoft Teams</a>.</p>
<p>A class team is a template type with additional apps included, and with limits separate to the number of team members.</p>

<p>Limits for class teams are listed in the following table:</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Maximum limit</th>
</tr>
</thead>
<tbody>
<tr>
<td>Number of members in a team</td>
<td>See the <a href="#teams-and-channels" data-linktype="self-bookmark">Teams and channels</a> section of this article</td>
</tr>
<tr>
<td>Number of members to use Assignments in a class team</td>
<td>300</td>
</tr>
<tr>
<td>Number of members to use a OneNote Class Notebook in a class team</td>
<td>300</td>
</tr>
</tbody>
</table>
<p>A class team can support more than 300 members. However, if you plan to use either the Assignments app or Class Notebook app within your team, you will need to keep the number of members below the maximum limits above.</p>

<table>
<thead>
<tr>
<th>Feature</th>
<th>Maximum limit</th>
</tr>
</thead>
<tbody>
<tr>
<td>Number of tags per team</td>
<td>200</td>
</tr>
<tr>
<td>Number of suggested default tags per team</td>
<td>25</td>
</tr>
<tr>
<td>Number of team members assigned to a tag</td>
<td>200</td>
</tr>
<tr>
<td>Number of tags assigned to a user per team</td>
<td>25</td>
</tr>
</tbody>
</table>

<p>Teams uses these contacts:</p>
<ul>
<li>Contacts in your organization's Active Directory</li>
<li>Contacts added to the user's Outlook default folder</li>
</ul>
<p>Teams users can communicate with anyone in your organization's Active Directory and can add anyone in your organization's Active Directory as a contact and to their contact lists by going to <strong>Chat</strong> &gt; <strong>Contacts</strong> or <strong>Calls</strong> &gt; <strong>Contacts</strong>.</p>
<p>Teams users can also add a person who isn't in your organization's Active Directory as a contact by going to <strong>Calls</strong> &gt; <strong>Contacts</strong>.</p>
<h2 id="browsers">Browsers</h2>
<p>Teams fully supports the following Internet browsers, with noted exceptions for calling and meetings. This table applies to operating systems running on desktop computers.</p>
<table>
<thead>
<tr>
<th>Browser</th>
<th>Calling - audio, video, and sharing</th>
<th>Meetings - audio, video, and sharing<sup>1</sup> <sup>2</sup></th>
</tr>
</thead>
<tbody>
<tr>
<td>Internet Explorer 11</td>
<td>Not supported</td>
<td>Meetings are supported only if the meeting includes PSTN coordinates. To attend a meeting on IE11 without PSTN coordinates, users must download the Teams desktop client.<p>Video: Not supported</p><p>Sharing: Incoming sharing only (no outgoing)  </p><p> Microsoft 365 apps and services will not support Internet Explorer 11 starting August 17, 2021 (Microsoft Teams will not support Internet Explorer 11 earlier, starting November 30, 2020). <a href="https://www.microsoft.com/edge/business" data-linktype="external">Learn more</a>. Please note that Internet Explorer 11 will remain a supported browser. Internet Explorer 11 is a component of the Windows operating system and <a href="https://learn.microsoft.com/en-us/lifecycle/faq/internet-explorer-microsoft-edge" data-linktype="absolute-path">follows the Lifecycle Policy</a> for the product on which it is installed.</p></td>
</tr>
<tr>
<td>Microsoft Edge, RS2 or later</td>
<td>Fully supported, except no outgoing sharing<sup>3</sup></td>
<td>Fully supported, except no outgoing sharing</td>
</tr>
<tr>
<td>Microsoft Edge (Chromium-based), the latest version plus two previous versions</td>
<td>Fully supported</td>
<td>Fully supported</td>
</tr>
<tr>
<td>Google Chrome, the latest version plus two previous versions</td>
<td>Fully supported</td>
<td>Fully supported <p> Sharing is supported without any plug-ins or extensions on Chrome version 72 or later.</p></td>
</tr>
<tr>
<td>Safari 15+</td>
<td>1:1 calls fully supported.</td>
<td></td>
</tr>
<tr>
<td>Safari 14+</td>
<td>1:1 calls not supported. Group calls fully supported.<p>Video: Fully supported</p><p>Sharing: Fully supported</p></td>
<td>Meetings: Fully supported<p>Video: Fully supported</p><p>Sharing: Fully supported</p></td>
</tr>
<tr>
<td>Safari 13.1+</td>
<td>1:1 calls not supported. Group calls supported with full audio support.<p>Video: Incoming only</p><p>Sharing: Fully supported</p></td>
<td>Meetings are supported with full audio support.<p>Video: Incoming only</p><p>Sharing: Fully supported</p></td>
</tr>
<tr>
<td>Firefox, the latest version plus two previous versions</td>
<td>Not supported</td>
<td>Meetings: Fully supported<p>Video: Fully supported</p><p>Sharing: Fully supported</p><p>Note that users are required to have the OpenH264 plugin in Firefox for full support. Browsers without this plugin may see disruptions in the meeting, including in screen sharing activity. Learn more at <a href="https://support.mozilla.org/kb/open-h264-plugin-firefox" data-linktype="external">Mozilla Firefox Support</a>.</p></td>
</tr>
<tr>
<td>Safari versions before 13</td>
<td>Not supported</td>
<td>Meetings are supported only if the meeting includes PSTN coordinates. To attend a meeting on Safari without PSTN coordinates, users must download the Teams desktop client.<p>Video: Not supported</p><p>Sharing: Incoming sharing only (no outgoing)</p><p>Safari is enabled on versions higher than 11.1 in preview. While in preview, there are <a href="https://support.office.com/article/safari-browser-support-1aac0a7c-35a8-42c1-a7df-f674afe234df" data-linktype="external">known issues</a> with Safari's Intelligent Tracking Prevention.</p></td>
</tr>
</tbody>
</table>
<p><sup>1</sup> To <a href="https://learn.microsoft.com/en-us/microsoftteams/meeting-who-present-request-control" data-linktype="relative-path">give and take control of shared content during sharing</a>, both parties must be using the Teams desktop client. Control isn't supported when either party is running Teams in a browser. This is due to a technical limitation that we're planning to fix.</p>
<p><sup>2</sup> Teams meetings on browsers are limited to a single stream; either incoming video feed of the current speaker or screen sharing.</p>
<p><sup>3</sup> Edge RS2 or later doesn't support sending real-time audio and video traffic through HTTP proxies.</p>
<div>
<p>Note</p>
<p>Running Teams in a browser is supported on PCs and Macs that meet the minimum <a href="https://learn.microsoft.com/en-us/microsoftteams/hardware-requirements-for-the-teams-app" data-linktype="relative-path">Hardware requirements for Microsoft Teams</a>. For example, running Firefox on the Linux operating system is an option for using Teams.</p>
<p>On mobile devices we recommend that you use the Teams app. The Teams app is available from the Android and iOS stores.</p>
</div>
<h2 id="operating-systems">Operating systems</h2>
<p>For information about operating system requirements, see <a href="https://learn.microsoft.com/en-us/microsoftteams/get-clients" data-linktype="relative-path">Get clients for Microsoft Teams</a>.</p>

							</div>

							
							
							<!-- </content> -->

						</main>




						<!-- recommendations section -->
						<!-- end recommendations section -->

						<!-- feedback section -->
<section data-bi-name="feedback-section">
    <h2 id="feedback">Feedback</h2>

    <div>
        <p id="send-feedback-about">Submit and view feedback for</p>

        
    </div>

    
</section>
						<!-- end feedback section -->

						<!-- feedback report section -->
						<!-- end feedback report section -->

							<div id="ms--additional-resources-mobile" role="complementary" aria-label="Additional resources">
								<hr hidden="">
								<h2 id="ms--additional-resources-mobile-heading" hidden="">Additional resources</h2>
								
								
								
								
							</div>

						

					</div>

						<div id="ms--additional-resources" data-bi-name="pageactions" role="complementary" aria-label="Additional resources">
								<h2 id="ms--additional-resources-heading" hidden="">Additional resources</h2>
								
								
								
								<nav id="side-doc-outline" data-bi-name="intopic toc" role="navigation" aria-label="In this article">
									<h3>In this article</h3>
								</nav>
								
							</div>

				</div>
	<!--end of .mainContainer -->

	

	


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Do Machine Learning Models Memorize or Generalize? (402 pts)]]></title>
            <link>https://pair.withgoogle.com/explorables/grokking/</link>
            <guid>37076210</guid>
            <pubDate>Thu, 10 Aug 2023 13:56:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pair.withgoogle.com/explorables/grokking/">https://pair.withgoogle.com/explorables/grokking/</a>, See on <a href="https://news.ycombinator.com/item?id=37076210">Hacker News</a></p>
<div id="readability-page-1" class="page">
  
  
  

  <div>
    <p>
      By <b>
      Adam Pearce</b>, <b> Asma Ghandeharioun</b>, <b> Nada Hussein</b>, <b> Nithum Thain</b>, <b> Martin Wattenberg</b> and <b> Lucas Dixon
      </b>
    </p>
    <p>August 2023</p>
  </div>

  
  <p>In 2021, researchers made a striking discovery while training a series of tiny models on toy tasks <a key="Grokking"></a>. They found a set of models that suddenly flipped from memorizing their training data to correctly generalizing on unseen inputs after training for much longer. This phenomenon – where generalization seems to happen abruptly and long after fitting the training data – is called <em>grokking</em> and has sparked a flurry of interest <a key="Omnigrok Universality Zhong23 ProgressParity gromov"></a>.</p>
<div>


<p>Do more complex models also suddenly generalize after they’re trained longer? Large language models can certainly seem like they have a rich understanding of the world, but they might just be regurgitating memorized bits of the enormous amount of text they’ve been trained on <a key="Parrots Othello"></a>. How can we tell if they’re generalizing or memorizing? </p>
<p>In this article we’ll examine the training dynamics of a tiny model and reverse engineer the solution it finds – and in the process provide an illustration of the exciting emerging field of mechanistic interpretability <a key="MechInterp ProgressMeasures"></a>. While it isn’t yet clear how to apply these techniques to today’s largest models, starting small makes it easier to develop intuitions as we progress towards answering these critical questions about large language models. </p>
<h3 id="grokking-modular-addition">Grokking Modular Addition</h3>
<p>Modular addition is essentially the fruit fly of grokking.<a key="modular"></a> The above line chart comes from a model trained to predict <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>+</mo><mi>b</mi><mtext> </mtext><mo lspace="0.22em" rspace="0.22em"><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow></mo><mtext> </mtext><mn>67</mn></mrow><annotation encoding="application/x-tex">a + b \bmod 67</annotation></semantics></math></span></span>.<a key="67"></a> We start by randomly dividing all the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo separator="true">,</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">a, b</annotation></semantics></math></span></span> pairs into test and training datasets. Over thousands of training steps, the training data is used to adjust the model into outputting correct answers, while the test data is only used to check if the model has learned a general solution.</p>
<p>The model’s architecture is similarly simple: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>ReLU</mtext><mrow><mo fence="true">(</mo><msub><mi mathvariant="bold">a</mi><mtext>one-hot</mtext></msub><msub><mi mathvariant="bold">W</mi><mtext>input</mtext></msub><mo>+</mo><msub><mi mathvariant="bold">b</mi><mtext>one-hot</mtext></msub><msub><mi mathvariant="bold">W</mi><mtext>input</mtext></msub><mo fence="true">)</mo></mrow><msub><mi mathvariant="bold">W</mi><mtext>output</mtext></msub></mrow><annotation encoding="application/x-tex">\text{ReLU}\left(\mathbf{a}_{\text{one-hot}} \mathbf{W}_{\text{input}} + \mathbf{b}_{\text{one-hot}} \mathbf{W}_{\text{input}}\right) \mathbf{W}_{\text{output}}
</annotation></semantics></math></span></span> — a one-layer MLP with 24 neurons.<a key="playground"></a> All the weights of the model are shown in the heatmap below; you can see how they change during training by mousing over the line chart above. </p>
<div>


<p>The model makes a prediction by selecting the two columns of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">W</mi><mtext>input</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{W}_{\text{input}}</annotation></semantics></math></span></span> corresponding to inputs <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span></span> then adding them together to create a vector of 24 separate numbers. Next it sets all the negative numbers in the vector to 0 and finally outputs the column of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">W</mi><mtext>output</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{W}_{\text{output}}</annotation></semantics></math></span></span> that’s closest to the updated vector.</p>
<p>The weights of the model are initially quite noisy but start to exhibit periodic patterns as accuracy on the test data increases and the model <animate data-animate="top-switches">switches</animate> to generalizing. By the end of training, each neuron — each row of the heatmap — cycles through high and low values several times as the input number increases from 0 to 66.</p>
<p>This is easier to see if we group the neurons by how often they cycle at the end of training and chart each of them as a separate line:  </p>
</div>
</div>


<p>The periodic patterns suggest the model is learning some sort of mathematical structure; the fact that it happens when the model starts to solve the test examples hints that it’s related to the model generalizing. But <em>why</em> does the model move away from the memorizing solution? And <em>what</em> is the generalizing solution?  </p>
<h3 id="generalizing-with-1s-and-0s">Generalizing With 1s and 0s</h3>
<p>Figuring out both of these questions simultaneously is hard. Let’s make an even simpler task, one where we know what the generalizing solution should look like and try to understand why the model eventually learns it.</p>
<p>We’ll take random sequences of thirty 1s and 0s and train our model to predict if there is an odd number of 1s in the first three digits. e.g. <digits>000110010110001010111001001011</digits> is <digits>0</digits> while <digits>010110010110001010111001001011</digits> is <digits>1</digits> — basically a slightly trickier XOR with some distraction noise. A generalizing model should only use the first three digits of the sequence; if the model is memorizing the training data, it will also use the subsequent distracting digits <a key="ProgressParity TwoCircuits"></a>.   </p>
<p>Our model is again a one-layer MLP, trained on a fixed batch of 1,200 sequences.<a key="sp-model"></a> At first only training accuracy increases — the model is memorizing the training data. As with modular arithmetic, test accuracy is essentially random and then sharply rises as the model learns a general solution.</p>


<p>While <animate data-animate="sp-mem">memorizing</animate> , the model looks dense and noisy with lots of high magnitude weights (shown as dark red and blue squares) spread across the chart below – the model is using all the inputs to make a prediction. As the model <animate data-animate="sp-gen">generalizes</animate> and gets perfect test accuracy, we see all the weights connected to the distracting digits gray out with very low values and the model focusing on the first three digits — mirroring the generalized structure we expected!<a key="sp-solution"></a></p>


<p>With this simplified example it’s easier to see why this happens: we’re pushing our model to do two things during training — output a high probability for the correct label (called minimizing <em>loss</em> <a key="loss"></a>) and have weights with low magnitudes (known as <em>weight decay</em> <a key="sp-l2"></a>). <span>Train loss</span> actually slightly increases before the model generalizes as it exchanges loss related to outputting the correct label for having lower weights. </p>


<p>The sharp drop in <span>test loss</span> makes it appear like the model makes a sudden shift to generalization. But if we look at the weights of the model over training, most of them smoothly interpolate between the two solutions. The rapid generalization occurs when the last weights connected to the distracting digits are pruned by weight decay.</p>


<h3 id="when-does-grokking-happen-">When Does Grokking Happen?</h3>
<p>It’s important to note that grokking is a contingent phenomenon — it goes away if model size, weight decay, data size and other hyper parameters aren’t just right. With too little weight decay, the model can’t escape overfitting the training data.<a key="overfit"></a> Adding more weight decay pushes the model to generalize after memorizing. Increasing weight decay even more causes test and train loss to fall together; the model goes straight to generalizing. And with too much weight decay the model will fail to learn anything. </p>
<p>Below, we’ve trained over a thousand models on the 1s and 0s task with different hyperparameters. Training is noisy so nine models have been trained for each set of hyperparameters. </p>



<p>We can induce memorization and generalization on this somewhat contrived 1s and 0s task — but why does it happen with modular addition? Let’s first understand a little more about how a one-layer MLP can solve modular addition by constructing a generalizing solution that’s interpretable. </p>
<h3 id="modular-addition-with-five-neurons">Modular Addition With Five Neurons</h3>
<p>Recall that our modular arithmetic problem <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>+</mo><mi>b</mi><mtext> </mtext><mo lspace="0.22em" rspace="0.22em"><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow></mo><mtext> </mtext><mn>67</mn></mrow><annotation encoding="application/x-tex">a + b \bmod 67</annotation></semantics></math></span></span> is naturally periodic, with answers wrapping around if the sum ever passes 67. Mathematically, this can be mirrored by thinking of the sum as wrapping <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span></span> around a circle. The weights of the generalizing model also had periodic patterns, indicating that the solution might use this property. </p>
<p>We can train a simpler model with a head start on the problem, constructing an embedding matrix that places <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span></span> on a circle using <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>cos</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\cos</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>sin</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\sin</annotation></semantics></math></span></span>.<a key="unit-circle"></a></p>




<p><br>
Then we train <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">W</mi><mtext>in-proj</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{W}_{\text{in-proj}}</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">W</mi><mtext>out-proj</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{W}_{\text{out-proj}}</annotation></semantics></math></span></span> in this one-layer MLP:</p>


<p>With just five neurons the model finds a solution with perfect accuracy. </p>
<div>



<p>Eyeballing the trained parameters, all the neurons <animate data-animate="five-neuron-converge">converge</animate> to roughly equal norms. If we directly plot their <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>cos</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\cos</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>sin</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\sin</annotation></semantics></math></span></span> components, they’re essentially evenly distributed around a circle: </p>


<p>Connect the adjacent neurons on the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">W</mi><mtext>in-proj</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{W}_{\text{in-proj}}</annotation></semantics></math></span></span> circle and an intriguing pattern emerges: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">W</mi><mtext>out-proj</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{W}_{\text{out-proj}}</annotation></semantics></math></span></span> is rotating around the circle twice as fast as <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">W</mi><mtext>in-proj</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{W}_{\text{in-proj}}</annotation></semantics></math></span></span>. </p>
</div>


<p>The details of how this solution works aren’t essential —  check out <a href="#appendix-a-how-the-circular-construction-works">Appendix A</a> to see how the doubled rotation allows the model to map inputs like <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>+</mo><mn>0</mn><mtext> </mtext><mo lspace="0.22em" rspace="0.22em"><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow></mo><mtext> </mtext><mn>67</mn></mrow><annotation encoding="application/x-tex">1 + 0 \bmod 67</annotation></semantics></math></span></span>  and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>+</mo><mn>66</mn><mtext> </mtext><mo lspace="0.22em" rspace="0.22em"><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow></mo><mtext> </mtext><mn>67</mn></mrow><annotation encoding="application/x-tex">2 + 66 \bmod 67</annotation></semantics></math></span></span> to the same place — but we have found a 20 parameter construction that solves modular addition. Can we find the same algorithm hidden in the 3,216 parameter model we started with? And why does the larger model switch to the generalizing solution after memorizing? </p>
<h3 id="it-s-full-of-stars">It’s Full of Stars</h3>
<p>Here’s the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>+</mo><mi>b</mi><mtext> </mtext><mo lspace="0.22em" rspace="0.22em"><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow></mo><mtext> </mtext><mn>67</mn></mrow><annotation encoding="application/x-tex">a + b \bmod 67</annotation></semantics></math></span></span> model that we started with — it’s trained from scratch with no built-in periodicity. </p>
<div>

 

<p>Unlike the constructed solution, where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">W</mi><mtext>embed</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{W}_{\text{embed}}</annotation></semantics></math></span></span> rotates around the circle once, this model has many different frequencies. </p>
<p>Below, we’ve isolated the frequencies using the discrete Fourier transform (DFT).<a key="dft"></a>  This factors out the learned periodic patterns across inputs, leaving us with the equivalent of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">W</mi><mtext>in-proj</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{W}_{\text{in-proj}}</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">W</mi><mtext>out-proj</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{W}_{\text{out-proj}}</annotation></semantics></math></span></span> from the constructed solution. For each neuron, this gives a <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>cos</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\cos</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>sin</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\sin</annotation></semantics></math></span></span> value for every possible periodic frequency from 1 to 33. The wave charts we show above use this to group neurons into frequencies by finding their largest <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>cos</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\cos</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>sin</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\sin</annotation></semantics></math></span></span> value across all frequencies.<a key="dft-sort"></a> </p>


<p>Just like in the 1s and 0s task, weight decay encourages this representation to become much sparser as the model <animate data-animate="bot-gen">generalizes</animate> .</p>
<p>Grouping neurons by their final trained frequencies, and plotting the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>cos</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\cos</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>sin</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\sin</annotation></semantics></math></span></span> components of the DFT for each neuron, we see the same star shapes from the constructed solution appear.</p>



<p><strong>This trained model is using the same algorithm as our constructed solution!</strong> Below, the contribution to the output generated by the neurons in each frequency are shown and we can see them calculating  <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>cos</mi><mo>⁡</mo><mfrac><mrow><mn>2</mn><mi>π</mi><mo stretchy="false">(</mo><mi>a</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mi>f</mi><mi>r</mi><mi>e</mi><mi>q</mi></mrow><mn>67</mn></mfrac></mrow><annotation encoding="application/x-tex"> \cos\frac{2\pi (a +  b) freq}{67}</annotation></semantics></math></span></span> .<a key="logit-wave"></a>        </p>
<p>Notice what happens to the group of neurons with a frequency of 7 when test loss <animate data-animate="bot-improve">improves</animate> after the short plateau at 45,000 steps — they start to snap into a star shape and their outputs more closely approximate a wave.</p>



<p>To lower loss without using higher weights (which would be punished by weight decay), the model uses several frequencies, taking advantage of constructive interference.<a key="ProgressMeasures"></a> There’s nothing magical about the frequencies 4, 5, 7 and 26 — click through other training runs below to see variations of this algorithm get learned.  </p>
</div>


<h3 id="open-questions">Open Questions</h3>
<p>While we now have a solid understanding of the mechanisms a one-layer MLP uses to solve modular addition and why they emerge during training, there are still many interesting open questions about memorization and generalization. </p>
<h4 id="which-model-constraints-work-best-">Which Model Constraints Work Best?</h4>
<p>Directly training the model visualized above — <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>ReLU</mtext><mrow><mo fence="true">(</mo><msub><mi>a</mi><mtext>one-hot</mtext></msub><msub><mtext mathvariant="bold">W</mtext><mtext>input</mtext></msub><mo>+</mo><msub><mi>b</mi><mtext>one-hot</mtext></msub><msub><mtext mathvariant="bold">W</mtext><mtext>input</mtext></msub><mo fence="true">)</mo></mrow><msub><mtext mathvariant="bold">W</mtext><mtext>output</mtext></msub></mrow><annotation encoding="application/x-tex">\text{ReLU} \left(a_{\text{one-hot}}\textbf{W}_{\text{input}} + b_{\text{one-hot}}\textbf{W}_{\text{input}} \right) \textbf{W}_{\text{output}}</annotation></semantics></math></span></span> — does not actually result in generalization on modular arithmetic, even with the addition of weight decay. At least one of the matrices has to be factored:  </p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mtext mathvariant="bold">W</mtext><mtext>input</mtext></msub><mo>=</mo><msub><mi mathvariant="bold">W</mi><mtext>embed</mtext></msub><msub><mi mathvariant="bold">W</mi><mtext>in-proj</mtext></msub></mrow><annotation encoding="application/x-tex">
\textbf{W}_{\text{input}} = \mathbf{W}_{\text{embed}} \mathbf{W}_{\text{in-proj}}
</annotation></semantics></math></span></span></span></p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mtext mathvariant="bold">W</mtext><mtext>output</mtext></msub><mo>=</mo><msub><mtext mathvariant="bold">W</mtext><mtext>out-proj</mtext></msub><msubsup><mtext mathvariant="bold">W</mtext><mtext>embed</mtext><mi mathvariant="normal">⊤</mi></msubsup></mrow><annotation encoding="application/x-tex">
\textbf{W}_{\text{output}} = \textbf{W}_{\text{out-proj}} \textbf{W}_{\text{embed}}^{\top}
</annotation></semantics></math></span></span></span> </p>
<p>We observed that the generalizing solution is sparse after taking the discrete Fourier transformation, but the collapsed matrices have high norms. This suggests that direct weight decay on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">W</mtext><mtext>output</mtext></msub></mrow><annotation encoding="application/x-tex">\textbf{W}_\text{output}</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">W</mtext><mtext>input</mtext></msub></mrow><annotation encoding="application/x-tex">\textbf{W}_{\text{input}}</annotation></semantics></math></span></span> doesn’t provide the right inductive bias for the task. </p>
<p>Broadly speaking, weight decay does steer a wide variety of models away from memorizing their training data <a key="DoubleDescent double-demystified"></a>. Other techniques that help avoid overfitting include dropout, smaller models and even numerically unstable optimization algorithms <a key="Slingshot"></a>. These approaches interact in complex, non-linear ways, making it difficult to predict <em>a priori</em> which will ultimately induce generalization. Collapsing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">W</mi><mtext>embed</mtext></msub><msub><mi mathvariant="bold">W</mi><mtext>in-proj</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{W}_{\text{embed}} \mathbf{W}_{\text{in-proj}}</annotation></semantics></math></span></span> instead of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">W</mtext><mtext>out-proj</mtext></msub><msubsup><mtext mathvariant="bold">W</mtext><mtext>embed</mtext><mi mathvariant="normal">⊤</mi></msubsup></mrow><annotation encoding="application/x-tex">\textbf{W}_{\text{out-proj}} \textbf{W}_{\text{embed}}^{\top}</annotation></semantics></math></span></span>, for example, helps in some setups and hurts in others:</p>


<h4 id="why-is-memorization-easier-than-generalization-">Why Is Memorization Easier Than Generalization?</h4>
<p>One theory: there can be many more ways to memorize a training set than there are generalizing solutions. So statistically, memorization should be more likely to happen first, especially if we have no or little regularization. Regularization techniques, like weight decay, prioritize certain solutions over others, for example, preferring “sparse” solutions over “dense” ones.</p>
<p>Recent work suggests that generalization is associated with well-structured representations <a key="EffectiveTheory"></a>. However, it’s not a necessary condition; some MLP variations without symmetric inputs learn less “circular” representations when solving modular addition <a key="Zhong23"></a>. We also observed that well-structured representations are not a sufficient condition for generalization. This small model (trained with no weight decay) starts generalizing, then switches to memorizing with periodic embeddings. </p>



<p>It’s even possible to find hyperparameters where models start generalizing, then switch to memorizing, then switch back to generalizing! <a key="open-q-mem"></a> </p>



<h4 id="what-about-larger-models-">What About Larger Models?</h4>
<p>Does grokking happen in larger models trained on real world tasks? Earlier observations reported the grokking phenomenon in algorithmic tasks in small transformers and MLPs <a key="Grokking ProgressMeasures Zhong23"></a>. Grokking has subsequently been found in more complex tasks involving images, text, and tabular data within certain ranges of hyperparameters <a key="Omnigrok Goldilocks"></a>. It’s also possible that the largest models, which are able to do many types of tasks, may be grokking many things at different speeds during training <a key="quantization"></a>.</p>
<p>There have also been promising results in predicting grokking before it happens. Though some require knowledge of the generalizing solution <a key="ProgressMeasures"></a> or the overall data domain <a key="StructuralGrokking"></a>, some rely solely on the analysis of the training loss <a key="PredictingGrokking"></a> and might also apply to larger models — hopefully we’ll be able to build tools and techniques that can tell us when a model is parroting memorized information and when it’s using richer models.</p>
<p>Understanding the solution to modular addition wasn’t trivial. Do we have any hope of understanding larger models? One route forward — like our digression into the 20 parameter model and the even simpler boolean parity problem — is to: 1) train simpler models with more inductive biases and fewer moving parts, 2) use them to explain inscrutable parts of how a larger model works, 3) repeat as needed. We believe this could be a fruitful approach to better understanding larger models, and complementary to efforts that aim to use larger models to explain smaller ones and other work to disentangle internal representations <a key="explain multiple-choice TMOS"></a>. Moreover, this kind of mechanistic approach to interpretability, in time, may help identify patterns that themselves ease or automate the uncovering of algorithms learned by neural networks.</p>
<h3 id="credits">Credits</h3>
<p>Thanks to Ardavan Saeedi, Crystal Qian, Emily Reif, Fernanda Viégas, Kathy Meier-Hellstern, Mahima Pushkarna, Minsuk Chang, Neel Nanda and Ryan Mullins for their help with this piece. </p>
<h3 id="appendix-a-how-the-circular-construction-works">Appendix A: How the Circular Construction Works</h3>
<p>We can almost calculate <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>+</mo><mi>b</mi><mtext> </mtext><mo lspace="0.22em" rspace="0.22em"><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow></mo><mtext> </mtext><mn>67</mn></mrow><annotation encoding="application/x-tex">a + b  \bmod 67</annotation></semantics></math></span></span> using two circular embeddings and a completely linear model.</p>
<div>



<p>It works! But we’re cheating a bit, do you see how <strong>unembed</strong> loops around the circle twice? We need to output a single prediction for “<v></v>“ — not separate predictions for “<v></v>“ and “<v2></v2>“. Directly adding the two predictions for a number together won’t work since they’re on opposite sides of the circles and will cancel each other out. </p>
<p>Instead, let’s incorporate a <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>ReLU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{ReLU}(x)</annotation></semantics></math></span></span> to fix the repeated outputs. </p>


<p>We’ve essentially wrapped the circle around in on itself and the model outputs a single prediction for “<v></v>“.   </p>
<p>Formally, this is the constructed model: </p>


<p>With modulus <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> evenly spaced neurons/directions:  </p>



<p>Interestingly this circle has a few wrinkles: this construction doesn’t give an exact answer! </p>
</div>


<p><span>Neurons </span>
  <span>Modulus </span>
</p>


<p>Using <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">x^2</annotation></semantics></math></span></span> instead of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>ReLU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{ReLU}(x)</annotation></semantics></math></span></span> as the activation function, as suggested by <a key="gromov"></a> gives a provably exact solution! </p>
<p>For simplicity, let <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ω</mi><mo>:</mo><mo>=</mo><mfrac><mrow><mn>2</mn><mi>π</mi></mrow><mi>M</mi></mfrac></mrow><annotation encoding="application/x-tex">\omega:=\frac{2\pi}{M}</annotation></semantics></math></span></span> (the angle between numbers in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">W</mi><mtext>embed</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{W}_{\text{embed}}</annotation></semantics></math></span></span>) and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>:</mo><mo>=</mo><mfrac><mrow><mn>2</mn><mi>π</mi></mrow><mi>N</mi></mfrac></mrow><annotation encoding="application/x-tex">\theta := \frac{2\pi}{N}</annotation></semantics></math></span></span> (the angle between neurons in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="bold">W</mi><mtext>in-proj</mtext><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">\mathbf{W}_{\text{in-proj}}^T</annotation></semantics></math></span></span>). </p>
<p>Let’s rewrite <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext>logits</mtext><mrow><mi>a</mi><mo separator="true">,</mo><mi>b</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\text{logits}^{a, b}</annotation></semantics></math></span></span> as an <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span></span>-dimensional vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">∥</mo><mrow><mi>l</mi><msup><mo stretchy="false">∥</mo><mi>M</mi></msup></mrow></mrow><annotation encoding="application/x-tex"> \lVert \it{l} \rVert ^M </annotation></semantics></math></span></span> where: </p>
<p><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>l</mi><mi>j</mi></msub><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>N</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo fence="true" stretchy="true" minsize="2.4em" maxsize="2.4em">(</mo><mo fence="true" stretchy="true" minsize="1.2em" maxsize="1.2em">[</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>a</mi><mi>ω</mi><mo>−</mo><mi>i</mi><mi>θ</mi><mo stretchy="false">)</mo><mo>+</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>b</mi><mi>ω</mi><mo>−</mo><mi>i</mi><mi>θ</mi><mo stretchy="false">)</mo><msup><mo fence="true" stretchy="true" minsize="1.2em" maxsize="1.2em">]</mo><mn>2</mn></msup><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>j</mi><mi>ω</mi><mo>−</mo><mn>2</mn><mi>i</mi><mi>θ</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo fence="true" stretchy="true" minsize="2.4em" maxsize="2.4em">)</mo></mrow><annotation encoding="application/x-tex">l_{j} = \sum_{i=0}^{N-1} \biggl(\bigl[ \cos(a\omega-i\theta) + \cos(b\omega-i\theta) \bigl]^2\cos(j\omega-2i\theta)) \biggr) </annotation></semantics></math></span></span></p>
<p>This follows from the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext>logits</mtext><mrow><mi>a</mi><mo separator="true">,</mo><mi>b</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\text{logits}^{a,b}</annotation></semantics></math></span></span> equation above by plugging in the definitions of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">W</mtext><mtext>in-proj</mtext></msub></mrow><annotation encoding="application/x-tex">\textbf{W}_\text{in-proj}</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">W</mtext><mtext>out-proj</mtext></msub></mrow><annotation encoding="application/x-tex">\textbf{W}_\text{out-proj}</annotation></semantics></math></span></span> and applying the trigonometric identity that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo><mo>+</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\cos(x)\cos(y) + \sin(x)\sin(y) = \cos(x-y)</annotation></semantics></math></span></span>. </p>
<p>We can then prove the following: </p>
<p><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi mathvariant="normal">arg max</mi><mo>⁡</mo></mrow><mi>c</mi></msub><mtext> ⁣</mtext><msup><mtext>logits</mtext><mrow><mi>a</mi><mo separator="true">,</mo><mi>b</mi></mrow></msup><mo>=</mo><mi>a</mi><mo>+</mo><mi>b</mi><mtext> </mtext><mo lspace="0.22em" rspace="0.22em"><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow></mo><mtext> </mtext><mi>M</mi></mrow><annotation encoding="application/x-tex">
\argmax_c \! \text{logits}^{a,b} = a + b \bmod M
</annotation></semantics></math></span></span></p>
<p>Applying the two trigonometric identities of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mn>2</mn><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>y</mi></mrow><mn>2</mn></mfrac><mo stretchy="false">)</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mfrac><mrow><mi>x</mi><mo>+</mo><mi>y</mi></mrow><mn>2</mn></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\cos(x) + \cos(y) = 2 \cos(\frac{x-y}{2}) \cos(\frac{x+y}{2})</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mi>cos</mi><mo>⁡</mo></mrow><mn>2</mn></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mi>c</mi><mi>o</mi><mi>s</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mi mathvariant="normal">/</mi><mn>4</mn><mo fence="true" stretchy="true" minsize="1.2em" maxsize="1.2em">[</mo><mn>2</mn><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo><mo>+</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>2</mn><mi>x</mi><mo>−</mo><mi>y</mi><mo stretchy="false">)</mo><mo>+</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>2</mn><mi>x</mi><mo>+</mo><mi>y</mi><mo stretchy="false">)</mo><mo fence="true" stretchy="true" minsize="1.2em" maxsize="1.2em">]</mo></mrow><annotation encoding="application/x-tex">\cos^2(x)cos(y) = 1/4 \bigl[ 2\cos(y) + \cos(2x-y) + \cos (2x+y) \bigl] </annotation></semantics></math></span></span>, we have:</p>


<p>Note that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∑</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>γ</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\sum \cos(\gamma_{i})=0</annotation></semantics></math></span></span> where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>γ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\gamma_{i}</annotation></semantics></math></span></span> is equally spread around the circle. The first and the third sum terms wrap around the circle with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>θ</mi></mrow><annotation encoding="application/x-tex">2\theta</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mi>θ</mi></mrow><annotation encoding="application/x-tex">4\theta</annotation></semantics></math></span></span> increments respectively. The sum of the first terms equals zero for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>&gt;</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">N \gt 2</annotation></semantics></math></span></span> and the sum of the third terms equals zero for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>&gt;</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">N \gt 4</annotation></semantics></math></span></span>. Therefore, we have:</p>
<p><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext>logits</mtext><mrow><mi>a</mi><mo separator="true">,</mo><mi>b</mi></mrow></msup><mo>=</mo><msup><mrow><mi>cos</mi><mo>⁡</mo></mrow><mn>2</mn></msup><mo stretchy="false">(</mo><mfrac><mrow><mi>a</mi><mo>−</mo><mi>b</mi></mrow><mn>2</mn></mfrac><mi>ω</mi><mo stretchy="false">)</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mi>a</mi><mo>+</mo><mi>b</mi><mo>−</mo><mi>c</mi><mo stretchy="false">)</mo><mi>ω</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">
\text{logits}^{a,b} =  \cos^2(\frac{a-b}{2}\omega) \cos((a+b-c)\omega)
</annotation></semantics></math></span></span></p>
<p>Since the first term is a positive constant w.r.t inputs, the equation is maximized when <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mi>a</mi><mo>+</mo><mi>b</mi><mo>−</mo><mi>c</mi><mo stretchy="false">)</mo><mi>ω</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\cos((a+b-c)\omega)</annotation></semantics></math></span></span> is maximized, which is when <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mo>=</mo><mi>a</mi><mo>+</mo><mi>b</mi><mtext> </mtext><mo lspace="0.22em" rspace="0.22em"><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow></mo><mtext> </mtext><mi>M</mi></mrow><annotation encoding="application/x-tex">c = a + b \bmod M</annotation></semantics></math></span></span>.</p>
<p>Essentially <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>ReLU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{ReLU}(x)</annotation></semantics></math></span></span> activations with weight decay (a very typical model setup) gives the model an inductive bias that’s close enough to the exact generalizing solution of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">x^2</annotation></semantics></math></span></span> activations with a sparse discrete Fourier transform to push in the direction of generalization but not so close that it won’t also learn to fit the training data with memorization.  </p>

<h3 id="footnotes">Footnotes</h3>
<p><a key="modular"></a> In modular addition, we have two input numbers, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span></span>, and a modulus <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span></span>. We want to find the remainder of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">a + b</annotation></semantics></math></span></span> when divided by <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span></span>. 
<span></span>
This type of addition is often called clock-face addition, because when adding two times, we often report the result modulo 12 (i.e. 5 hours after 8 o’clock is 1 o’clock).
<span></span>
Modular addition sounds simple and it is. We can easily train 1,000s of models and treat them like fruit flies in neuroscience: small enough such that it is feasible to extract their <a href="https://www.science.org/doi/abs/10.1126/science.add9330">connectome</a> synapse-by-synapse, yet providing new interesting insights about the system more broadly. We can get a good understanding of the small models we’ve trained by visualizing all their internals.</p>
<p><a key="67"></a>67 isn’t a magic number – we could pick many numbers to illustrate grokking, but 67 is not so small that the task is trivial and also not so large that the visualizations are overwhelming. </p>
<p><a key="playground"></a>
The model is trained with cross-entropy loss, AdamW and full batches. The <a href="#which-model-constraints-work-best-">section on regularization</a> and <a href="https://colab.research.google.com/github/PAIR-code/ai-explorables/blob/master/server-side/grokking/MLP_Modular_Addition.ipynb">training colab</a> have additional details.<br><span></span>
If you’re not familiar with <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">MLPs</a>, <a href="http://playground.tensorflow.org/">playground.tensorflow.org</a> is a great place to start. 
<span></span>
A quick notation explanation: The columns of  <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">W</mi><mtext>input</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{W}_{\text{input}}</annotation></semantics></math></span></span> and  <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">W</mi><mtext>ouput</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{W}_{\text{ouput}}</annotation></semantics></math></span></span> represent the numbers from 0 to 66.  <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">a</mi><mtext>one-hot</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{a}_{\text{one-hot}}</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">b</mi><mtext>one-hot</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{b}_{\text{one-hot}}</annotation></semantics></math></span></span> are how we encode the model’s inputs; each pick a single column from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">W</mi><mtext>input</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{W}_{\text{input}}</annotation></semantics></math></span></span>. <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>ReLU</mtext></mrow><annotation encoding="application/x-tex">\text{ReLU}</annotation></semantics></math></span></span> replaces negative numbers with 0s; it is a <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks">fancy</a>) way of writing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mn>0</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\max(x, 0)</annotation></semantics></math></span></span>.</p>
<p><a key="sp-model"></a> With a small twist — we’re only outputting 1 or 0, so <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">W</mi><mtext>output</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{W}_{\text{output}}</annotation></semantics></math></span></span> can be a single column. In the modular addition task we needed a column for every output number.
<span></span>
The last column of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">W</mi><mtext>input</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{W}_{\text{input}}</annotation></semantics></math></span></span> is also fixed to 1 to provide a bias term.</p>
<p><a key="sp-solution"></a><a href="https://arxiv.org/pdf/2303.11873.pdf#page=8">Appendix D</a> of “A Tale of Two Circuits: Grokking as Competition of Sparse and Dense Subnetworks” has an explanation of the 4 neuron solution generalizing solution here</p>
<p><a key="loss"></a> So far we’ve been charting <a href="https://developers.google.com/machine-learning/crash-course/classification/accuracy">accuracy</a>, the percentage of sequences where the correct label is the most likely. Training typically instead optimizes a differentiable objective function. All the models in this post use <a href="https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html">cross entropy loss</a> which heavily penalizes incorrect predictions with high probabilities. 
<span></span>
Note that while some formulations of loss include a weight decay or regularization term, the loss plots here depict the cross entropy component alone.</p>
<p><a key="sp-l2"></a>
On the 1s and 0s task here, we use L1 weight decay <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mn>1</mn><mo stretchy="false">(</mo><mi mathvariant="bold">w</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mo>∑</mo><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">L1(\mathbf{w}) = \sum_{i} |w_i|</annotation></semantics></math></span></span>. 
<span></span>
L2 weight decay <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mn>2</mn><mo stretchy="false">(</mo><mi mathvariant="bold">w</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mo>∑</mo><mi>i</mi></msub><msubsup><mi>w</mi><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">L2(\mathbf{w}) = \sum_{i} w_i^2</annotation></semantics></math></span></span> is a more typical choice. It pushes for <a href="https://explained.ai/regularization/L1vsL2.html">lots of small weights</a> leading to redundant neurons on this task:
<span></span>
<img src="https://pair.withgoogle.com/explorables/grokking/img/sp-l2.gif"> </p>
<p><a key="overfit"></a> A model overfits the training data when it performs well on the training data but poorly on the test data — this is what we see with our memorizing models. In general, simpler models are less prone to overfitting as, due to their simplicity, decision rules are coarser and are required to make more generalizations. Of course, if a model is too simple for a task, it may not be able to learn good decision rules that capture the nuances of the task. Researchers force models to be simpler through a variety of techniques, including having models with fewer parameters or encouraging the parameters that the model does have to be small in size with weight decay.  </p>
<p><a key="unit-circle"></a> 
Here’s what <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">W</mtext><mtext>embed</mtext></msub></mrow><annotation encoding="application/x-tex">\textbf{W}_{\text{embed}}</annotation></semantics></math></span></span> looks like on the unit circle:
<span></span>
<img src="https://pair.withgoogle.com/explorables/grokking/img/w_embed.png" width="319"></p>
<p><a key="dft"></a> The <a href="https://www.youtube.com/watch?v=spUNpyF58BY">Discrete Fourier Transform</a> helps analyze the periodic nature of a sequence of values (in this case the <a href="https://colab.research.google.com/drive/1F6_1_cWXE5M7WocUcpQWp3v8z4b1jL20#scrollTo=iSPxi3ElsujY">weights for a particular neuron</a>) by breaking it down into sine and cosine functions. The more periodic a function is, the easier it is to represent with sine and cosines, and the sparser the output of the DFT.</p>
<p><a key="dft-sort"></a> We’ve reindexed the neurons by their final frequency and phase to make this grouping easier to see .</p>
<p><a key="logit-wave"></a>
The model generates probabilities by taking the dot product of the neuron activations for a given input with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">W</mtext><mtext>output</mtext></msub></mrow><annotation encoding="application/x-tex">\textbf{W}_{\text{output}}</annotation></semantics></math></span></span> and softmaxing. If we calculate the dot product using only the activations from neurons of a single frequency, we can see which outputs the frequency group is making more or less likely.<br><span></span>
<a href="#appendix-a-how-the-circular-construction-works">Appendix A</a> explains why these logits form a wave — each group of frequencies is essentially outputting how close the correct answer is to every number on a version of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">W</mtext><mtext>embed</mtext></msub></mrow><annotation encoding="application/x-tex">\textbf{W}_{\text{embed}}</annotation></semantics></math></span></span> with the group’s frequency. </p>
<p><a key="open-q-mem"></a>Both of these models are <a href="https://colab.sandbox.google.com/github/PAIR-code/ai-explorables/blob/master/server-side/grokking/MLP_Modular_Addition.ipynb#scrollTo=5hJqK4jx0vC7">quite small</a>. The bottom model has tweaked hyperparameters to encourage eventual generalization: it’s slightly larger to allow it to exit local minimums, it has more training data (making low loss memorizing solutions harder to find) and it has weight decay.    </p>
<h3 id="references">References</h3>
<p><a key="Grokking"></a> <a href="https://arxiv.org/pdf/2201.02177.pdf">Grokking: Generalization Beyond Overfitting On Small Algorithmic Datasets</a>
Power, A., Burda, Y., Edwards, H., Babuschkin, I., &amp; Misra, V. (2022). arXiv preprint arXiv:2201.02177.</p>
<p><a key="Omnigrok"></a> <a href="https://arxiv.org/pdf/2210.01117.pdf">Omnigrok: Grokking Beyond Algorithmic Data</a>
Liu, Z., Michaud, E. J., &amp; Tegmark, M. (2022, September). In The Eleventh International Conference on Learning Representations.</p>
<p><a key="Universality"></a> <a href="https://arxiv.org/abs/2302.03025">A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations</a>
Chughtai, B., Chan, L., Nanda, N.  (2023). International Conference on Machine Learning.</p>
<p><a key="Zhong23"></a><a href="https://arxiv.org/pdf/2306.17844.pdf">The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks</a>
Zhong, Z., Liu, Z., Tegmark, M., &amp; Andreas, J. (2023). arXiv preprint arXiv:2306.17844.</p>
<p><a key="ProgressParity"></a> <a href="https://arxiv.org/abs/2207.08799">Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit</a>
Boaz Barak, Benjamin L. Edelman, Surbhi Goel, Sham Kakade, Eran Malach, Cyril Zhang. (2022) Advances in Neural Information Processing Systems, 35, 21750-21764.</p>
<p><a key="gromov"></a><a href="https://arxiv.org/abs/2301.02679">Grokking modular arithmetic</a> Andrey Gromov (2023). arXiv preprint arXiv:2301.02679.</p>
<p><a key="Parrots"></a><a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445922?uuid=f2qngt2LcFCbgtaZ2024">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?🦜</a> Bender, E. M., Gebru, T., McMillan-Major, A., &amp; Shmitchell, S. (2021, March). <em>In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency</em> (pp. 610-623).</p>
<p><a key="Othello"></a> <a href="https://openreview.net/pdf?id=DeG07_TcZvT">Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task</a> Li, K., Hopkins, A. K., Bau, D., Viégas, F., Pfister, H., &amp; Wattenberg, M. (2022, September). <em>In The Eleventh International Conference on Learning Representations</em>.</p>
<p><a key="MechInterp"></a> <a href="https://transformer-circuits.pub/2022/mech-interp-essay/index.html">Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases</a>
Olah, C., 2022. Transformer Circuits Thread. </p>
<p><a key="ProgressMeasures"></a> <a href="https://openreview.net/pdf?id=9XFSbDPmdW">Progress Measures for Grokking via Mechanistic Interpretability</a>
Nanda, N., Chan, L., Lieberum, T., Smith, J., &amp; Steinhardt, J. (2022, September). In The Eleventh International Conference on Learning Representations.</p>
<p><a key="TwoCircuits"></a> <a href="https://arxiv.org/abs/2303.11873">A Tale of Two Circuits: Grokking as Competition of Sparse and Dense Subnetworks</a>
William Merrill, Nikolaos Tsilivis, Aman Shukla. (2023). arXiv preprint arXiv:2303.11873.</p>
<p><a key="DoubleDescent"></a><a href="https://arxiv.org/pdf/2303.06173.pdf">Unifying Grokking and Double Descent</a>
Davies, X., Langosco, L., &amp; Krueger, D. (2022, November). In NeurIPS ML Safety Workshop.</p>
<p><a key="double-demystified"></a><a href="https://arxiv.org/abs/2303.14151">Double Descent Demystified: Identifying, Interpreting &amp; Ablating the Sources of a Deep Learning Puzzle</a> Rylan Schaeffer, R., Khona, M., Robertson, Z., Boopathy, A., Pistunova, K., Rocks, J., Rani Fiete, I., &amp; Koyejo, O. (2023). arXiv preprint arXiv:2303.14151.</p>
<p><a key="Slingshot"></a> <a href="https://arxiv.org/pdf/2206.04817.pdf">The Slingshot Mechanism: An Empirical Study of Adaptive Optimizers and the Grokking Phenomenon</a>
Thilak, V., Littwin, E., Zhai, S., Saremi, O., Paiss, R., &amp; Susskind, J. (2022). arXiv preprint arXiv:2206.04817.</p>
<p><a key="EffectiveTheory"></a><a href="https://arxiv.org/pdf/2205.10343.pdf">Towards Understanding Grokking: An Effective Theory of Representation Learning</a>
Liu, Z., Kitouni, O., Nolte, N. S., Michaud, E., Tegmark, M., &amp; Williams, M. (2022). Advances in Neural Information Processing Systems, 35, 34651-34663.</p>
<p><a key="Goldilocks"></a><a href="https://arxiv.org/pdf/1807.02581.pdf">The Goldilocks Zone: Towards Better Understanding of Neural Network Loss Landscapes</a>
Fort, S., &amp; Scherlis, A. (2019, July). In Proceedings of the AAAI conference on artificial intelligence (Vol. 33, No. 01, pp. 3574-3581).</p>
<p><a key="quantization"></a><a href="https://arxiv.org/abs/2303.13506">The Quantization Model of Neural Scaling</a> Eric J. Michaud, Ziming Liu, Uzay Girit, Max Tegmark, O. (2023). arXiv preprint arXiv:2303.13506.</p>
<p><a key="StructuralGrokking"></a> <a href="https://arxiv.org/pdf/2305.18741.pdf">Grokking of Hierarchical Structure in Vanilla Transformers</a>
Murty, S., Sharma, P., Andreas, J., &amp; Manning, C. D. (2023). arXiv preprint arXiv:2305.18741.</p>
<p><a key="PredictingGrokking"></a> <a href="https://arxiv.org/pdf/2306.13253.pdf">Predicting Grokking Long Before it Happens: A Look Into the Loss Landscape of Models Which Grok</a>
Notsawo Jr, P., Zhou, H., Pezeshki, M., Rish, I., &amp; Dumas, G. (2023). arXiv preprint arXiv:2306.13253.</p>
<p><a key="explain"></a><a href="https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html">Language models can explain neurons in language models</a>
Bills, S., Cammarata, N., Mossing, D., Tillman, H., Gao, L., Goh, G., Sutskever, I., Leike, J., Wu, J., &amp; Saunders, W. 2023. OpenAI Blog</p>
<p><a key="multiple-choice"></a><a href="https://arxiv.org/abs/2307.09458">Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla</a> Tom Lieberum, Matthew Rahtz, János Kramár, Neel Nanda, Geoffrey Irving, Rohin Shah, Vladimir Mikulik (2023). arXiv preprint arXiv:2307.09458.</p>
<p><a key="TMOS"></a><a href="https://transformer-circuits.pub/2022/toy_model/index.html">Toy Models of Superposition</a>
Elhage, N., Hume, T., Olsson, C., Schiefer, N., Henighan, T., Kravec, S., Hatfield-Dodds, Z., Lasenby, R., Drain, D., Chen, C., Grosse, R., McCandlish, S., Kaplan, J., Amodei, D., Wattenberg, M. and Olah, C., 2022. Transformer Circuits Thread.</p>
<p><a key="Connectome"></a> <a href="https://www.science.org/doi/abs/10.1126/science.add9330">The Connectome of an Insect Brain</a> 
Winding, M., Pedigo, B. D., Barnes, C. L., Patsolic, H. G., Park, Y., Kazimiers, T., … &amp; Zlatic, M. (2023). Science, 379(6636), eadd9330.</p>
<p><a key="Multiscale"></a> <a href="https://proceedings.mlr.press/v162/pezeshki22a/pezeshki22a.pdf">Multi-Scale Feature Learning Dynamics: Insights for Double Descent</a>
Pezeshki, M., Mitra, A., Bengio, Y., &amp; Lajoie, G. (2022, June). In the International Conference on Machine Learning (pp. 17669-17690). PMLR.</p>
<p><a key="superposition"></a><a href="https://transformer-circuits.pub/2023/toy-double-descent/index.html">Superposition, Memorization, and Double Descent</a>
Henighan, T., Carter, S., Hume, T., Elhage, N., Lasenby, R., Fort, S., Schiefer, N., and Olah, C., 2023. Transformer Circuits Thread.</p>
<h3 id="more-explorables">More Explorables</h3>

















































































</div>]]></description>
        </item>
        <item>
            <title><![CDATA[MetaGPT: Meta Programming for Multi-Agent Collaborative Framework (138 pts)]]></title>
            <link>https://arxiv.org/abs/2308.00352</link>
            <guid>37076125</guid>
            <pubDate>Thu, 10 Aug 2023 13:48:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2308.00352">https://arxiv.org/abs/2308.00352</a>, See on <a href="https://news.ycombinator.com/item?id=37076125">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hong%2C+S">Sirui Hong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng%2C+X">Xiawu Zheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen%2C+J">Jonathan Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cheng%2C+Y">Yuheng Cheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+J">Jinlin Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang%2C+C">Ceyao Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+Z">Zili Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yau%2C+S+K+S">Steven Ka Shing Yau</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lin%2C+Z">Zijuan Lin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou%2C+L">Liyang Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ran%2C+C">Chenyu Ran</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiao%2C+L">Lingfeng Xiao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu%2C+C">Chenglin Wu</a></p></div>
      
    
  
  
  
    <p><a aria-describedby="download-button-info" href="https://arxiv.org/pdf/2308.00352">Download PDF</a></p><blockquote>
            <span>Abstract:</span>  Recently, remarkable progress has been made in automated task-solving through
the use of multi-agent driven by large language models (LLMs). However,
existing LLM-based multi-agent works primarily focus on solving simple dialogue
tasks, and complex tasks are rarely studied, mainly due to the LLM
hallucination problem. This type of hallucination becomes cascading when
naively chaining multiple intelligent agents, resulting in a failure to
effectively address complex problems. Therefore, we introduce MetaGPT, an
innovative framework that incorporates efficient human workflows as a meta
programming approach into LLM-based multi-agent collaboration. Specifically,
MetaGPT encodes Standardized Operating Procedures (SOPs) into prompts to
enhance structured coordination. Subsequently, it mandates modular outputs,
empowering agents with domain expertise comparable to human professionals, to
validate outputs and minimize compounded errors. In this way, MetaGPT leverages
the assembly line paradigm to assign diverse roles to various agents, thereby
establishing a framework that can effectively and cohesively deconstruct
complex multi-agent collaborative problems. Our experiments on collaborative
software engineering benchmarks demonstrate that MetaGPT generates more
coherent and correct solutions compared to existing chat-based multi-agent
systems. This highlights the potential of integrating human domain knowledge
into multi-agent systems, thereby creating new opportunities to tackle complex
real-world challenges. The GitHub repository of this project is publicly
available on:<a href="https://github.com/geekan/MetaGPT" rel="external noopener nofollow">this https URL</a>.

    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Sirui Hong [<a href="https://arxiv.org/show-email/5763f4d1/2308.00352">view email</a>]
      <br>
    
        <strong><a href="https://arxiv.org/abs/2308.00352v1">[v1]</a></strong>
    
        Tue, 1 Aug 2023 07:49:10 UTC (7,361 KB)<br>
    
    
        <strong><a href="https://arxiv.org/abs/2308.00352v2">[v2]</a></strong>
    
        Wed, 2 Aug 2023 04:11:02 UTC (7,362 KB)<br>
    
    <strong>[v3]</strong>
    
        Mon, 7 Aug 2023 19:20:19 UTC (12,029 KB)<br>
    </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Applite – Clean Homebrew front end app for macOS built with SwiftUI (230 pts)]]></title>
            <link>https://aerolite.dev/applite/index.html</link>
            <guid>37075730</guid>
            <pubDate>Thu, 10 Aug 2023 13:13:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aerolite.dev/applite/index.html">https://aerolite.dev/applite/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=37075730">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">

    <!-- ======= About Section ======= -->
    <div id="key-features">
          <p><img src="https://aerolite.dev/applite/assets/img/screenshots/productivity-sm.png" alt="">
          </p>

          <div data-aos="fade-left">
            <h3>Key Features</h3>
            <div data-aos="zoom-in" data-aos-delay="100">
              
              <h4>Download and manage with a single click</h4>
              <p>Download, update and uninstall apps with a single click</p>
            </div>

            <div data-aos="zoom-in" data-aos-delay="200">
              
              <h4>Clean and simple UI</h4>
              <p>Designed for non-technical users</p>
            </div>

            <div data-aos="zoom-in" data-aos-delay="300">
              
              <h4>Free and open-source</h4>
              <p>No costs, no tracking, fully transparent</p>
            </div>

            <p>Applite uses the <a href="https://brew.sh/" target="_blank">Homebrew</a> package manager under the hood. Homebrew is a free and open source project that
              makes it easy to install developer tools and desktop applications on macOS.</p>

          </div>
        </div><!-- End About Section -->

    <!-- ======= Details Section ======= -->
    <div id="details">

        <div>
          <p><img src="https://aerolite.dev/applite/assets/img/screenshots/installed-sm.png" alt="">
          </p>

          <div data-aos="fade-up">
            <h3>What apps are available on Applite?</h3>
            <p>
              Any application that can be found on the <a href="https://brew.sh/" target="_blank">Homebrew Catalog</a> is available on Applite.
            </p>

            <h3>Is it secure?</h3>
            <p>
              The macOS built-in protection (Gatekeeper and XProtect) will scan the application for potential malware
              the first time you open it and notify you if anything is suspicious. Also, most applications in the Homebrew
              Catalog are notarized, which means they come from a registered developer.
            </p>
            <p>
              The apps may not be sandboxed, which allows them to affect your system with elevated privileges.
              Nevertheless, be careful, as some applications may contain malware, especially those that have few downloads.
              Applite itself is also not sandboxed.
            </p>

            <h3>What information does Applite track?</h3>
            <p>
              None.
            </p>
          </div>
        </div>

        <div>
          <p><img src="https://aerolite.dev/applite/assets/img/screenshots/brew-path-selection.png" alt="">
          </p>

          <div data-aos="fade-up">
            <h2>Advanced FAQ</h2>

            <h3>Can I use Applite with my existing Homebrew installation?</h3>
            <p>
              Yes, you can. The first time you open the application, you will be asked if you want to use your own brew
              or if you want to create a new installation just for Applite.
            </p>
            <p>
              If you choose to create a new installation it will be stored at: <code>~/Library/Application Support/Applite/homebrew</code>.
            </p>

            <h3>Can I manage already installed apps with Applite?</h3>
            <p>
              If you chose to use your existing brew installation all casks installed with it will appear in Applite.
            </p>
            <p>
              Any other apps installed manually (e.g. from DMG or PKG files) will not show up as installed in Applite.
              You can add them by reinstalling them. See <a href="https://aerolite.dev/applite/troubleshooting.html">troubleshooting</a> page
              for more information on adding already installed apps.
            </p>
          </div>
        </div>
      </div><!-- End Details Section -->

    <!-- ======= Screenshots Section ======= -->
    <!-- End Gallery Section -->

    <!-- ======= Download Section ======= -->
    <div id="download">
        <p>Download</p>

        <div data-aos="fade-right" data-aos-delay="300">
              <p><code>brew install --cask applite</code>
                
              </p>
            </div>

        
      </div>
    <!-- End Gallery Section -->

    <!-- ======= Contact Section ======= -->
    <!-- End Contact Section -->

    

  </div></div>]]></description>
        </item>
    </channel>
</rss>