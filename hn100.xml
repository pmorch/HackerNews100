<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 30 Oct 2025 09:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Crunchyroll is destroying its subtitles for no good reason (318 pts)]]></title>
            <link>https://daiz.moe/crunchyroll-is-destroying-its-subtitles-for-no-good-reason/</link>
            <guid>45754509</guid>
            <pubDate>Wed, 29 Oct 2025 23:31:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daiz.moe/crunchyroll-is-destroying-its-subtitles-for-no-good-reason/">https://daiz.moe/crunchyroll-is-destroying-its-subtitles-for-no-good-reason/</a>, See on <a href="https://news.ycombinator.com/item?id=45754509">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <article>   <p>Since the beginning of the Fall 2025 anime season, a major change has started taking place at the anime streaming service <a href="https://en.wikipedia.org/wiki/Crunchyroll" rel="noopener" target="_blank">Crunchyroll</a>: <strong>the presentation quality for translations of on-screen text has taken a total nosedive</strong> compared to what has been on offer for many years, all the way up until the previous Summer 2025 season. Now, more and more subtitles on Crunchyroll are looking like this:</p>

<p>Poor presentation quality like this isn’t entirely new to Crunchyroll, as a portion of the subtitles on the site have always been of third-party origin — that is, provided by the licensor — and Crunchyroll just puts them up with zero oversight. This in itself has caused <a href="https://www.animenewsnetwork.com/news/2025-07-01/crunchyroll-german-necronomico-and-the-cosmic-horror-show-subtitles-listed-chatgpt/.226206" rel="noopener" target="_blank">numerous</a> <a href="https://www.animenewsnetwork.com/news/2023-10-05/the-yuzuki-family-four-sons-episode-1-briefly-inaccessible-on-crunchyroll-after-subtitle-quality-/.203183" rel="noopener" target="_blank">issues</a> over the years, but the pressing issue here is that <strong>low quality presentation like this can now be found even in first-party subtitles created by Crunchyroll’s own subtitling staff.</strong> For comparison, here’s the kind of presentation quality that first-party subtitles were providing just earlier this year:</p>

<p>Given the technical capabilities on display in the above screenshots, it should be clear that <strong>first-party subtitles for Fall 2025 shows shouldn’t look as bad as they do.</strong> Yet for some reason, what we’re getting is this low quality presentation reminiscent of third-party subtitles, where translations for dialogue and on-screen text aren’t even separated to different sides of the screen – everything is just bunched up together at either the top or the bottom. Lots of on-screen text is even left straight up untranslated.</p>

<h2 id="and-thats-destroying-subtitles">And that’s “destroying subtitles”?</h2>
<p>It sure is when it’s anime we’re talking about! <strong>Anime as a medium has made prominent use of on-screen text basically since its inception.</strong> The amount of it varies from series to series, but almost every anime out there makes use of on-screen text at one point or another, with some featuring downright ridiculous amounts of <b>signs</b> (what on-screen text is called for short). With all this on-screen text, it is also very common for there to be text visible on the screen potentially in multiple positions, even when characters are speaking.</p>
<p>As such, <strong>if you are in the business of localizing anime for non-Japanese audiences, you need to be able to deal with on-screen text.</strong> At bare minimum, when subtitling anime, you should be able to do <b>overlaps</b> (multiple lines of text on the screen at the same time) and <b>positioning</b> (the ability to freely place subtitles anywhere on the screen). Anything less and you are likely to run into trouble the moment you get to something as simple as a next episode preview:</p>
<figure id="typesetting-introduction" data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/saki-s3-fansub-1.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/saki-s3-fansub-1.jpg" alt="A screenshot from a next episode preview. It features a static sign saying 'Next episode preview' on top left, a similar sign for the title of the next episode on the bottom right, and you have dialogue running through the whole preview while these signs are visible, meaning you need at least three overlaps and positioning to handle it gracefully." loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>Multiple instances of on-screen text are running in parallel with dialogue. Screenshot from <em>Saki: The Nationals</em> (Winter&nbsp;2014, Underwater-FFF fansubs)</p></figcaption>  </figure>
<p>Overlaps and positioning are really just the bare necessities for dealing with on-screen text in anime though – ideally, you should also be able to use different fonts, colors, animate text in various ways, etc. Making use of all these possibilities is an art unto itself, and <strong>this art of on-screen text localization is commonly referred to as typesetting</strong>. Typesetting is important even when dubbing anime, as all that on-screen text is going to be there in the video all the same!</p>
<h2 id="so-why-would-crunchyroll-get-rid-of-typesetting">So why would Crunchyroll get rid of typesetting?</h2>
<p>That is a good question. It is no exaggeration to say that up to this point, Crunchyroll with its typesetting was the unambiguous market leader when it came to presentation quality for official anime subtitles… though for the most part, <strong>other services dealing in anime have never even bothered to try.</strong> Sentai Filmworks’ <a href="https://en.wikipedia.org/wiki/Hidive" rel="noopener" target="_blank">Hidive</a> is just about the only other anime service that even attempts to do typesetting, though they license so few shows per season that they are a tiny player compared to the Big Boys of anime streaming.</p>


<p>And it is very likely the existence of these Big Boys that has played a key part in Crunchyroll’s eradication of its typesetting. <a href="https://en.wikipedia.org/wiki/Netflix" rel="noopener" target="_blank">Netflix</a> and <a href="https://en.wikipedia.org/wiki/Amazon_Prime_Video" rel="noopener" target="_blank">Amazon Prime Video</a> probably need no introduction to anyone reading this – both are very popular general streaming services. <strong>Despite anime being only a minor part of their catalogs, a large chunk of today’s anime watching worldwide happens through said services</strong> thanks to their sheer user counts alone.</p>
<p>Crunchyroll clearly seems to know this, which is why it has been sublicensing its anime properties to both Amazon and Netflix for multiple years at this point. <strong>But with such sublicensing comes the matter of dealing with the subtitling standards of general streaming services.</strong> I’m not going to mince words: these standards are <em>awful,</em> at least as far as anime is concerned. Netflix for example insists that you stick to at most two lines of text on screen at once, which makes sense most of the time… if you’re talking about dialogue alone. Unfortunately, it becomes completely inadequate when dealing with anime’s plentiful on-screen text. Moreover, the standards of these services actively refuse to give you tools like positioning and overlaps, even though the <a href="https://en.wikipedia.org/wiki/Timed_Text_Markup_Language" rel="noopener" target="_blank">TTML subtitle format</a> they use supports said features!</p>

<p>With such typesetting-hostile standards to deal with, Crunchyroll had basically two choices for how to make sublicensing to Amazon and Netflix work with their existing subtitles that feature actual typesetting: Either 1) try to negotiate with the services for permission to make use of more TTML capabilities (that the subtitle renderers of said services should already support!) or 2) <strong>start mangling subtitles with typesetting into something compatible with the awful subtitling standards of the general streaming services.</strong> I am not aware if Crunchyroll ever attempted the former, but I can confirm that it eventually started doing the latter.</p>

<p>Editors among Crunchyroll’s subtitling staff were <strong>given an additional job</strong> to convert finished high quality subtitles with typesetting into limited low quality TTML subtitles without typesetting, compatible with Amazon &amp; Netflix subtitling standards. They got paid extra for the manual effort required by the process.</p>
<figure id="ttml-example" data-astro-cid-rkgwkvbt=""> <video poster="https://daiz.moe/content/crunchyroll/devilman-crybaby-netflix-1-poster.jpg" controls="" preload="metadata" data-astro-cid-rkgwkvbt=""> <source src="https://daiz.moe/content/crunchyroll/devilman-crybaby-netflix-1.mp4" type="video/mp4;codecs=avc1.640028,mp4a.40.2" data-astro-cid-rkgwkvbt=""> <span data-astro-cid-rkgwkvbt="">It appears that your browser does not support playing this video.
      However, you can always <a href="https://daiz.moe/content/crunchyroll/devilman-crybaby-netflix-1.mp4" data-astro-cid-rkgwkvbt="true" rel="noopener" target="_blank">download the video directly.</a></span> </video> <figcaption data-astro-cid-rkgwkvbt="">Overlapping on-screen text and dialogue makes for a miserable anime watching experience with limited TTML subtitles. Video clip from <em>Devilman Crybaby</em> (Winter&nbsp;2018, Netflix)</figcaption>  </figure>
<p>Unfortunately, after a couple years of this kind of manual conversion work, the Crunchyroll leadership seems to have decided that it isn’t enough, and that <strong>Crunchyroll must do away with high quality subtitles with typesetting entirely and only produce low quality TTML subtitles without typesetting from now on.</strong> But if they already had a working process for high quality subtitles at home and low quality TTML subtitles elsewhere, why would they just decide to give that up in order to produce exclusively low quality subtitles? It doesn’t seem to make very much sense, even as a cost-cutting measure. There should be so much value in being able to advertise <em>best viewed on Crunchyroll</em> to potential audiences for long-term growth, right?</p>
<p>To understand <em>how this is happening,</em> we need to look into some relevant history. <strong>Specifically, what happened after Sony bought Crunchyroll and merged it with Funimation,</strong> another US anime distributor that Sony had bought previously. But in order to also understand <em>why this is happening,</em> first we need to look at what both Crunchyroll and Funimation were like before this fateful merger happened, as well as how they approached anime subtitling over the years.</p>

<h2 id="a-short-history-of-crunchyroll-and-its-subtitling-standards">A short history of Crunchyroll and its subtitling standards</h2>
<p>Crunchyroll launched in 2006 as a pirate streaming site focused on East Asian media content, featuring <a href="https://en.wikipedia.org/wiki/Fansub" rel="noopener" target="_blank">fansubbed</a> anime, live action drama, music videos, and so on. There was nothing particularly remarkable about the site back then – as a rule of thumb, pirate streaming sites are always worse quality-wise than if you just directly downloaded the pirated releases they use as a base, and the sites mostly exist to make their admins illicit money through ads, begging for donations, and other shady crap. It is important to note though that <strong>legal anime streaming basically wasn’t a thing at this time.</strong></p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/crunchyroll-in-2007.png" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/crunchyroll-in-2007.png" alt="" loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>Crunchyroll in 2007. The “help out” message at the top is asking for donations.</p></figcaption>  </figure>
<p>Things started to change in 2008, <a href="https://www.animenewsnetwork.com/news/2008-03-21/gonzo-works-to-be-streamed-simultaneously-with-airing" rel="noopener" target="_blank">when the Japanese anime studio Gonzo started experimenting with legal internet distribution</a> for some of its titles. They struck deals with a couple companies for this, which is how Crunchyroll got its first few legitimate licenses. However, all the pirate material remained on the site while this was going on. Also in 2008: Crunchyroll managed to raise 4 million USD in venture capital funding while still operating as a pirate site, <a href="https://www.animenewsnetwork.com/news/2008-03-12/funimation-responds-to-crunchyroll-us$4m-funding" rel="noopener" target="_blank">which drew vocal criticism</a> from existing anime distributors at the time (for obvious reasons).</p>
<p>However, it was likely this exact venture capital funding that enabled Crunchyroll to negotiate a major deal with the Japanese broadcasting company TV Tokyo, which was announced at the start of 2009. <strong>This announcement brought with it the news that Crunchyroll was going full-time legitimate and getting rid of all its pirate content.</strong> With this move, Crunchyroll found itself in a position of having to start producing subtitles of its own (instead of just uploading fansubs) and somehow present said subtitles to its customers.</p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/aegisub.png" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/aegisub.png" alt="" loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>Aegisub is an advanced subtitling software built by fansubbers, for fansubbers.</p></figcaption>  </figure>
<p id="aegisub-ass-introduction">For the subtitle production part, Crunchyroll managed to strike a deal with a bunch of fansubbers to take on the job. This single decision was a fateful one, as it was the foundation for basically everything that came after – with former fansubbers on the job, the tools of the trade were set according to the standards of fansubbers: the subtitling software of choice was to be <a href="https://aegisub.org/" rel="noopener" target="_blank">Aegisub</a>, and <strong>the subtitle format of choice was to be Aegisub’s native format, Advanced SubStation Alpha, or ASS for short.</strong></p>

<p>ASS is an extremely powerful format in terms of formatting and styling capabilities, and with Aegisub, it is easy to produce ASS subtitles that make use of said capabilities. <strong>However, as a streaming site, Crunchyroll needed to be able to present these ASS subtitles in the browser somehow,</strong> and the only full-fledged ASS renderers that existed were only available in the traditional local media playback environments targeted by fansubbers, which meant that Crunchyroll couldn’t make use of said renderers on the web directly.</p>
<p>Now, there are two main ways to subtitle videos, with opposing pros and cons:</p>
<ul>
<li><b>Hardsubbing</b> – the subtitles are burned into the video itself. <strong>Simple to playback</strong> as you only need to be able to play video, <strong>but inflexible for updates and multiple languages</strong> as you have to recreate your video files over and over again with expensive processing called <b>encoding</b>.</li>
<li><b>Softsubbing</b> – the subtitles exist as their own separate media track that the video player renders on top of the video in realtime during playback, making softsubs <strong>complex to playback, but updates and multiple tracks are very cheap</strong> as you only need to deal with tiny subtitle files while the video files remain unchanged.</li>
</ul>
<p>As such, one way Crunchyroll could have solved the subtitle presentation problem would have been to simply hardsub its ASS subtitles, but despite the challenges it posed, Crunchyroll decided to go with softsubbing instead (which was also the fansub standard at the time). <strong>And so Crunchyroll set out to build its own ASS renderer in Flash,</strong> the primary technology used to play video on the web at the time. Here’s a screenshot of some of the first subtitles ever officially authored by the fully legitimate Crunchyroll, rendered in the current ASS renderer but adhering to the limits of the company’s very first Flash subtitle renderer:</p>
<figure id="cr-first-subs" data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/saki-cr-1.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/saki-cr-1.jpg" alt="A screenshot from a next episode preview. It features a static sign saying 'Next episode preview' on top left, a similar sign for the title of the next episode on the bottom right, and you have dialogue running through the whole preview while these signs are visible, meaning you need at least three overlaps and positioning to handle it gracefully." loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>Screenshot from <em>Saki</em> (Spring&nbsp;2009, Crunchyroll)</p></figcaption>  </figure>
<p>As can be seen, even the very first version was already capable of handling both overlaps and positioning. Now, the positioning was limited to the eight edges and the center of the screen, making for just nine possible positions total, but even that was enough to handle the humble next episode preview at the very least. Beyond these, the first version also supported fading animations. It wasn’t much, <strong>but it did cover the bare minimum for dealing with on-screen text in anime.</strong></p>

<p>Over the years, Crunchyroll managed to slowly improve its Flash subtitle renderer to enable the use of more ASS features. Custom colors, multiple fonts, multiple styles, rotation, and full positioning were implemented (albeit in somewhat hacky and unwieldy fashion). This went on until 2018, when Crunchyroll was faced with a major issue: <strong>Flash was seeing rapid decline in use, and web streaming was shifting over to HTML5-based technology.</strong> However, with a custom ASS renderer built in Flash, Crunchyroll couldn’t easily make the change, as it would mean having to essentially rebuild the custom subtitle renderer they had from scratch in HTML5 (as much like in the Flash days, there still were no solutions native to the web available for rendering ASS subtitles).</p>
<p id="cr-libass">However, Crunchyroll managed to come up with a way to solve the problem of moving from Flash to HTML5 with the help of another new web technology called <a href="https://webassembly.org/" rel="noopener" target="_blank">WebAssembly</a>, which allowed developers to take code that wasn’t developed for the web and compile it for use on the web. With WebAssembly, Crunchyroll could take <a href="https://github.com/libass/libass" rel="noopener" target="_blank">libass</a>, one of the few fully-featured ASS renderers out there, and <a href="https://github.com/libass/JavascriptSubtitlesOctopus" rel="noopener" target="_blank">use it for their new HTML5 player</a>. Now, not only did all their old ASS subtitles render nicely in HTML5, but <strong>the possibilities for typesetting at Crunchyroll had taken a huge leap forward.</strong> And the subtitling staff at Crunchyroll was more than happy to make use of this newfound power.</p>
<figure data-astro-cid-rkgwkvbt=""> <video poster="https://daiz.moe/content/crunchyroll/bocchi-cr-1-poster.jpg" controls="" preload="metadata" data-astro-cid-rkgwkvbt=""> <source src="https://daiz.moe/content/crunchyroll/bocchi-cr-1.mp4" type="video/mp4;codecs=avc1.640028,mp4a.40.2" data-astro-cid-rkgwkvbt=""> <span data-astro-cid-rkgwkvbt="">It appears that your browser does not support playing this video.
      However, you can always <a href="https://daiz.moe/content/crunchyroll/bocchi-cr-1.mp4" data-astro-cid-rkgwkvbt="true" rel="noopener" target="_blank">download the video directly.</a></span> </video> <figcaption data-astro-cid-rkgwkvbt="">You couldn’t see typesetting like this on Crunchyroll back in the days of Flash. Video clip from <em>Bocchi the Rock!</em> (Fall&nbsp;2022, Crunchyroll)</figcaption>  </figure>
<p>That said, despite having a technically fully-featured ASS renderer to work with, <strong>there were still limitations.</strong> Code compiled with WebAssembly runs worse compared to its original native counterpart, which limits how heavy the typesetting can be (with the flexible features of ASS, it is very easy to produce typesetting that simply cannot be rendered in realtime even on powerful computers, resulting in notable lag during playback). A commercial service like Crunchyroll will also generally want to keep its content watchable even on lower-end devices, which further reduces how complex any typesetting can be.</p>
<p>And this is the limited but functional standard of typesetting that Crunchyroll users got to enjoy (with first-party subtitles) up until the fateful season of Fall 2025 that prompted the creation of this article.</p>
<p>Before we move to the conclusions for this section, though, it is worth noting that while Crunchyroll currently uses softsubbed ASS subtitles whenever it can, there are platforms and devices (like various TVs) where this kind of ASS rendering simply isn’t possible to do. <strong>Crunchyroll is available on some platforms like this, which means it has been making additional hardsubbed versions of everything on top of the usual softsubbed ones.</strong></p>
<hr>
<p>So, what can we learn from all this? At least one thing is abundantly clear: for most of its existence, <strong>the leadership at Crunchyroll had at least some respect and understanding for anime as a medium.</strong> They understood that it was important to be able to deal with on-screen text in their subtitles, and allocated enough resources to make typesetting possible. The company even managed to improve in this regard over time, albeit very slowly.</p>
<p>That said, anyone familiar with anime fansubs of the 2010s and 2020s probably can’t help but feel disappointed that even the highest effort typesetting from Crunchyroll could only ever be on the level of fansub releases from around 2010 at best. Why 2010 specifically? <strong>Because from 2011 onwards, fansubbers started widely incorporating advanced motion tracking into their typesetting.</strong> Observe an example of such fansub typesetting from over a decade ago, the likes of which has never been seen on Crunchyroll:</p>
<figure data-astro-cid-rkgwkvbt=""> <video poster="https://daiz.moe/content/crunchyroll/klk-underwater-1-poster.jpg" controls="" preload="metadata" data-astro-cid-rkgwkvbt=""> <source src="https://daiz.moe/content/crunchyroll/klk-underwater-1.mp4" type="video/mp4;codecs=avc1.640028,mp4a.40.2" data-astro-cid-rkgwkvbt=""> <span data-astro-cid-rkgwkvbt="">It appears that your browser does not support playing this video.
      However, you can always <a href="https://daiz.moe/content/crunchyroll/klk-underwater-1.mp4" data-astro-cid-rkgwkvbt="true" rel="noopener" target="_blank">download the video directly.</a></span> </video> <figcaption data-astro-cid-rkgwkvbt="">Video clip from <em>Kill la Kill</em> (Fall&nbsp;2013, Underwater fansubs)</figcaption>  </figure>
<p>Now, while fansubbers giving away their work for free might get away with saying <q>just get a better computer</q> to anyone whose devices can’t render softsubbed typesetting like this in realtime, an official service that lots of people pay for doesn’t really have the same luxury, which is the main reason why you don’t see stuff like this softsubbed on Crunchyroll. But this is not an insurmountable problem, so make no mistake: <strong>official anime services could absolutely offer typesetting with similar level of quality to the best of fansubs.</strong> The basic solution to the performance problem is very simple, even: you simply hardsub the typesetting. This would work from streaming to physical disc releases and only the sky would be the limit in terms of the typesetting quality you could offer, as realtime rendering would no longer be a concern!</p>
<p id="best-of-both-worlds">Now, as mentioned earlier, hardsubbing does make things more complicated and expensive on the backend as you need to encode and store multiple copies of video. Crunchyroll is already dealing with this, though! But if costs are an issue, the system is pretty easy to improve in theory: if you keep the dialogue softsubbed, only the parts of the video that actually feature typesetting would be hardsubbed, and with some clever engineering and an understanding of how modern media formats work, you would only have to keep multiple copies of the typeset parts. <strong>And since the average anime episode has on-screen text only for a small percentage of its total runtime, combining softsubbed dialogue and hardsubbed typesetting like this would make for a highly cost-effective setup.</strong></p>
<figure data-astro-cid-rkgwkvbt=""> <video poster="https://daiz.moe/content/crunchyroll/komi-novaworks-1-poster.jpg" controls="" preload="metadata" data-astro-cid-rkgwkvbt=""> <source src="https://daiz.moe/content/crunchyroll/komi-novaworks-1.mp4" type="video/mp4;codecs=avc1.640028,mp4a.40.2" data-astro-cid-rkgwkvbt=""> <span data-astro-cid-rkgwkvbt="">It appears that your browser does not support playing this video.
      However, you can always <a href="https://daiz.moe/content/crunchyroll/komi-novaworks-1.mp4" data-astro-cid-rkgwkvbt="true" rel="noopener" target="_blank">download the video directly.</a></span> </video> <figcaption data-astro-cid-rkgwkvbt="">Typesetting like this would be possible to do even for official anime services. Video clip from <em>Komi Can’t Communicate</em> (Fall&nbsp;2021, NovaWorks fansubs)</figcaption>  </figure>
<p>And since with a mixed system like this you would only have softsubs for the technically simpler dialogue, you could even convert these dialogue-only ASS subtitles to a simpler but more widely supported subtitle format for playback, <strong>which theoretically should do away with the need to keep fully hardsubbed copies around entirely, without any real loss in quality!</strong> I actually built a minimal version of a mixed system like this myself when I was doing some anime streaming work a few years back and can confidently say that <strong>this would be extremely doable for any official anime service… as long as they just cared enough.</strong></p>
<figure data-astro-cid-rkgwkvbt=""> <video poster="https://daiz.moe/content/crunchyroll/danganronpa-utw-1-poster.jpg" controls="" preload="metadata" data-astro-cid-rkgwkvbt=""> <source src="https://daiz.moe/content/crunchyroll/danganronpa-utw-1.mp4" type="video/mp4;codecs=avc1.640028,mp4a.40.2" data-astro-cid-rkgwkvbt=""> <span data-astro-cid-rkgwkvbt="">It appears that your browser does not support playing this video.
      However, you can always <a href="https://daiz.moe/content/crunchyroll/danganronpa-utw-1.mp4" data-astro-cid-rkgwkvbt="true" rel="noopener" target="_blank">download the video directly.</a></span> </video> <figcaption data-astro-cid-rkgwkvbt="">Keep this example of fansub typesetting in mind for later. Video clip from <em>Danganronpa: The Animation</em> (Summer&nbsp;2013, UTW fansubs)</figcaption>  </figure>
<p>Unfortunately, any interest Crunchyroll had for improving their subtitle rendering for typesetting seemed to run out after the 2018 transition to WebAssembly libass. <strong>Not that it actually ever seemed to be all that high to begin with, though,</strong> as evident by some of the low-hanging fruit that Crunchyroll never bothered to pick in this regard; the most obvious of which would be Crunchyroll’s dogged insistence to restrict typesetting font choices to <a href="https://en.wikipedia.org/wiki/Core_fonts_for_the_Web" rel="noopener" target="_blank">Core Fonts for the Web</a>. Free for commercial use fonts have been plentily available since the Flash days, and custom fonts have been well supported on the web for a similarly long time.</p>

<p>Anyway, it would have never been all that hard for Crunchyroll to support custom fonts for typesetting, especially after the 2018 move to HTML5. The underlying technology was there and font files are tiny in size compared to the video files being streamed – this would have been an extremely simple and effective improvement for all typesetting efforts. Yet Crunchyroll never reached for this improvement, which is why <span>Comic Sans</span> has kept appearing in Crunchyroll typesetting with depressing regularity.</p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/kanokari-cr-1.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/kanokari-cr-1.jpg" alt="" loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>There it is again. <span>Taunting me.</span> Screenshot from <em>Rent-a-Girlfriend S3</em> (Summer&nbsp;2023, Crunchyroll)</p></figcaption>  </figure>
<p>It is also disappointing <strong>how regularly the anime staples of opening &amp; ending songs are still left untranslated on Crunchyroll,</strong> though this issue is admittedly <a href="https://www.animenewsnetwork.com/answerman/2017-09-01/.120752" rel="noopener" target="_blank">much harder to solve than you’d expect.</a> Still, it is possible to do so, especially with Sony’s resources behind the company today. That goes double when Sony is involved in anime production in any way, as then the songs being used should be well-known to all relevant parties well in advance of airing for timely rights-clearing. <strong>So if Crunchyroll/Sony is in any way involved with an anime’s production, it should basically always be possible for songs to be translated the moment the first episode is released.</strong></p>
<p>But that’s enough about Crunchyroll’s history. Now it’s time to look at the other company mentioned earlier and see how they’ve fared in comparison…</p>
<h2 id="a-short-history-of-funimation-and-its-subtitling-standards">A short history of Funimation and its subtitling standards</h2>
<p>In the early 90s, Japanese-American businessman Gen Fukunaga was approached by his uncle who was working as a producer for Toei. A proposal was made: if Fukunaga could start an anime company in US, Toei would license the rights to the <a href="https://en.wikipedia.org/wiki/Dragon_Ball" rel="noopener" target="_blank">Dragon Ball</a> franchise to it – a franchise that was already making mad cash in Japan. Sensing an opportunity, Fukunaga found investors, and thus in 1994 Funimation was born. A year later, Dragon Ball was on US TV, dubbed and edited to <a href="https://web.archive.org/web/20060518193827/http://www.animecauldron.com/dbzuncensored2/misc/footsteps.html" rel="noopener" target="_blank">“conform to American sensibilities and tastes”</a>.</p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/dbz-promo-art.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/dbz-promo-art.jpg" alt="Promotional art for the Dragon Ball Z anime, featuring some of its cast members." loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>It was especially <em>Dragon Ball Z</em> (1989-1996) that hit it big in the US.</p></figcaption>  </figure>
<p>In the early 2000s, fueled by Dragon Ball’s success, Funimation started expanding its business by getting home video distribution rights for <a href="https://en.wikipedia.org/wiki/4Kids_Entertainment" rel="noopener" target="_blank">4Kids Entertainment</a> licenses and non-Japanese kids’ cartoons, the latter eventually expanding into getting involved in production too. But beyond increased investment in kids’ cartoons, <strong>Funimation also started experimenting with more anime licenses of its own,</strong> the 2001 anime adaption for <a href="https://en.wikipedia.org/wiki/Fruits_Basket" rel="noopener" target="_blank">Fruits Basket</a> being one of its early standout releases.</p>
<p>Out of these various expansion attempts, <strong>“more anime” seemed to be the one to work out best,</strong> and towards the end of the 00s that became the main direction of Funimation’s business. This move was helped along by a bunch of licenses obtained from now-defunct US anime publishers Geneon USA and ADV. And in the spring of 2009, hot on the heels of Crunchyroll going legit, <a href="https://www.animenewsnetwork.com/news/2009-04-03/funimation-adds-toei-air-master-captain-harlock" rel="noopener" target="_blank">Funimation announced that they too were getting into the anime streaming business</a>. The resulting anime streams from Funimation were hardsubbed and looked like this:</p>
<div data-astro-cid-l2h4bdqm="">  <div id="view-inari-0" data-astro-cid-l2h4bdqm=""> <figure data-astro-cid-l2h4bdqm=""> <a href="https://daiz.moe/content/crunchyroll/inari-funi-1.jpg" target="_blank" rel="noopener" data-astro-cid-l2h4bdqm=""> <img src="https://daiz.moe/content/crunchyroll/inari-funi-1.jpg" alt="" loading="lazy" data-astro-cid-l2h4bdqm=""> </a> <figcaption data-astro-cid-l2h4bdqm="">Screenshot from <em>Inari, Konkon, Koi Iroha</em> (Winter 2014, Funimation)</figcaption> </figure> </div><div id="view-inari-1" data-astro-cid-l2h4bdqm=""> <figure data-astro-cid-l2h4bdqm=""> <a href="https://daiz.moe/content/crunchyroll/inari-funi-2.jpg" target="_blank" rel="noopener" data-astro-cid-l2h4bdqm=""> <img src="https://daiz.moe/content/crunchyroll/inari-funi-2.jpg" alt="" loading="lazy" data-astro-cid-l2h4bdqm=""> </a> <figcaption data-astro-cid-l2h4bdqm="">Screenshot from <em>Inari, Konkon, Koi Iroha</em> (Winter 2014, Funimation)</figcaption> </figure> </div><div id="view-inari-2" data-astro-cid-l2h4bdqm=""> <figure data-astro-cid-l2h4bdqm=""> <a href="https://daiz.moe/content/crunchyroll/inari-funi-3.jpg" target="_blank" rel="noopener" data-astro-cid-l2h4bdqm=""> <img src="https://daiz.moe/content/crunchyroll/inari-funi-3.jpg" alt="" loading="lazy" data-astro-cid-l2h4bdqm=""> </a> <figcaption data-astro-cid-l2h4bdqm="">Screenshot from <em>Inari, Konkon, Koi Iroha</em> (Winter 2014, Funimation)</figcaption> </figure> </div><div id="view-inari-3" data-astro-cid-l2h4bdqm=""> <figure data-astro-cid-l2h4bdqm=""> <a href="https://daiz.moe/content/crunchyroll/inari-funi-4.jpg" target="_blank" rel="noopener" data-astro-cid-l2h4bdqm=""> <img src="https://daiz.moe/content/crunchyroll/inari-funi-4.jpg" alt="" loading="lazy" data-astro-cid-l2h4bdqm=""> </a> <figcaption data-astro-cid-l2h4bdqm="">Screenshot from <em>Inari, Konkon, Koi Iroha</em> (Winter 2014, Funimation)</figcaption> </figure> </div>  </div>
<p>What you see here is exactly what you got: plain text at top center or bottom center, with dialogue on bottom, and translations for all on-screen text piled up top. <strong>So while overlaps were technically supported, full positioning did not seem to be possible,</strong> which made things quite awkward the moment there was more than one sign visible on the screen at the same time. <strong>This was also the standard you could expect from Funimation’s DVD and Blu-ray releases.</strong> And beyond the way too common dialogue three-liners (which are generally terrible for readability), sometimes you even saw <em>four-liners</em>:</p>
<figure id="maccaption-introduction" data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/d-frag-funi-1.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/d-frag-funi-1.jpg" alt="" loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>Screenshot from <em>D-Frag!</em> (Winter&nbsp;2014, Funimation)</p></figcaption>  </figure>
<p>The subtitling software that Funimation was using at the time was <a href="https://www.telestream.net/captioning/" rel="noopener" target="_blank">Telestream MacCaption</a>. In terms of usability and general authoring features, it was no match for Aegisub, although it was actually capable of doing <a href="https://www.youtube.com/watch?v=Vut8ucz_dYY" rel="noopener" target="_blank">some overlaps, positioning, and styling</a> – Funimation just never chose to make use of these capabilities for its anime subtitles.</p>

<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/maccaption.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/maccaption.jpg" alt="" loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>TeleStream stopped supporting MacCaption in 2023.</p></figcaption>  </figure>
<p>This remained the Funimation subtitle standard all the way until 2016, when Funimation struck a deal with Crunchyroll. <strong>Going forward, subtitled releases for Funimation licenses would be found on Crunchyroll,</strong> while dubbed releases for said titles would be on Funimation’s new streaming platform, <b>FunimationNow.</b></p>

<p>However, the only thing that really changed is that instead of Funimation content being hardsubbed on their website, <strong>it was now softsubbed on Crunchyroll to the exact same standard:</strong> plain text on top center or bottom center, often with three or more lines of dialogue at once, even.</p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/sakura-quest-funiroll-1.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/sakura-quest-funiroll-1.jpg" alt="" loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>Sometimes you could see sign translations on bottom too. Screenshot from <em>Sakura Quest</em> (Spring&nbsp;2017, Funimation/Crunchyroll)</p></figcaption>  </figure>
<p>Nothing else of particular note happened during this time period when it comes to Funimation’s subtitles. However, it is worth mentioning that <strong>Funimation dubs did have simple hardsubbed typesetting sometimes;</strong> this only seemed happen at the whim of the dubbing side of Funimation though, as these hardsubbed signs were never present in the subbed versions, nor were they a consistent feature of Funimation dubs in general.</p>

<p><strong>In 2017, Sony purchased Funimation</strong> as part of its growing collection of international anime distributors (Sony had previously bought Madman Anime and AnimeLab in Australia and Wakanim in Europe). As a result of this buyout, towards the end of 2018 the license sharing deal between Funimation and Crunchyroll was dissolved and soon after <strong>Funimation started serving new subtitled streams on FunimationNow, which were softsubbed</strong> and looked like this:</p>

<p><strong>No longer were the subtitles even making use of overlaps.</strong> Where dialogue translation used to go on bottom and sign translation on top when both were present, now all text was stuck on the same side of the screen together, either on top or bottom, but never both at the same time anymore.</p>
<p>How this further reduction in subtitling capabilities came about cannot be said for sure, but there are several possible explanations. For one, another major thing that happened at the end of 2018: <a href="https://www.animenewsnetwork.com/news/2018-12-04/funimation-hulu-sign-first-look-streaming-deal-for-new-anime/.140359" rel="noopener" target="_blank">Funimation signed a big sublicensing deal</a> with the general streaming service <a href="https://en.wikipedia.org/wiki/Hulu" rel="noopener" target="_blank">Hulu</a>, which meant <strong>dealing with Hulu’s subtitling standards and authoring accordingly limited subtitles</strong> – because as could be expected, the subtitling standards of a general streaming service did not account for the needs of anime in any real way.</p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/mha-funi-hulu.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/mha-funi-hulu.jpg" alt="" loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>Only the middle column of the blackboard is translated here. Good luck figuring that out with subtitles like these. Screenshot from <em>My Hero Academia 4</em> (Fall&nbsp;2019, Funimation/Hulu)</p></figcaption>  </figure>
<p id="ooona-introduction">Another possible reason for these less-than-great changes in Funimation’s subtitling standards was that around this time the company <strong>started using the cloud-based subtitling toolkit OOONA Tools</strong> by the localization service provider <a href="https://www.ooona.net/" rel="noopener" target="_blank">OOONA</a>. OOONA Tools, by default, <strong>do not allow for the creation of subtitles with overlaps.</strong> While it can be done in OOONA today by tweaking the options or by using OOONA’s track features (which are quite similar to those of MacCaption, incidentally), it is possible that at the time these features were either not available or that it wasn’t possible to correctly export subtitles with overlaps to the <a href="https://www.w3.org/TR/webvtt1/" rel="noopener" target="_blank">WebVTT subtitle format</a> that was being used on FunimationNow.</p>

<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/ooona-create.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/ooona-create.jpg" alt="" loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>Screenshot of <em>OOONA Create</em>, the primary subtitling software in OOONA Tools.</p></figcaption>  </figure>
<p>Regarding that last possibility in particular, there is <a href="https://archive.is/nRpg1" rel="noopener" target="_blank">this OOONA FAQ entry</a> that mentions how <q>not all formats support […] overlapping subtitles</q> and that <q>Currently, it’s supported in IMSC1.1, ITT and Videotron Lambda CAP exports</q>. However, based on my own testing, OOONA Tools can properly export subtitles with overlaps in more formats today than just the ones mentioned here (including WebVTT), meaning that the FAQ entry is in fact outdated – <strong>but it was likely true at some point.</strong></p>
<p>In any case, this was the extremely limited standard of subtitling that Funimation customers had to live with until the service was shut down in 2024 as a result of the Funimation-Crunchyroll merger.</p>
<hr>
<p>Now, what can we conclude from all this? If nothing else, one thing seems abundantly clear: <strong>the Funimation leadership never truly cared about or respected anime as a medium.</strong> From the very beginning, it’s clear that Gen Fukunaga (a businessman in his 30s at the time) got into the business <strong>with the mindset of making money with kids’ cartoons,</strong> and this only became more evident with how Funimation tried to expand into more types of kids’ cartoons before eventually realizing that anime is where the money was at.</p>
<p>But even with this eventual focus on more anime, <strong>no resources seem to have ever been dedicated to make typesetting an actual thing at Funimation,</strong> despite how obviously beneficial it would have been for their key product of localized anime. And the way Funimation never even bothered to figure out how to make the most of MacCaption, the expensive enterprise subtitling software they kept using for over a decade… while I speculated about possible technical reasons for Funimation abandoning even overlaps when they started producing softsubs for FunimationNow, there was always one possible additional reason: <strong>they just didn’t care at all.</strong> They ran into a problem, no resources were dedicated to fix the problem, and the subtitles got permanently worse as a result.</p>
<figure data-astro-cid-rkgwkvbt=""> <video poster="https://daiz.moe/content/crunchyroll/danganronpa-funi-1-poster.jpg" controls="" preload="metadata" data-astro-cid-rkgwkvbt=""> <source src="https://daiz.moe/content/crunchyroll/danganronpa-funi-1.mp4" type="video/mp4;codecs=avc1.640028,mp4a.40.2" data-astro-cid-rkgwkvbt=""> <span data-astro-cid-rkgwkvbt="">It appears that your browser does not support playing this video.
      However, you can always <a href="https://daiz.moe/content/crunchyroll/danganronpa-funi-1.mp4" data-astro-cid-rkgwkvbt="true" rel="noopener" target="_blank">download the video directly.</a></span> </video> <figcaption data-astro-cid-rkgwkvbt="">Remember the fansubbed version of this from earlier? Here’s Funimation in comparison. Video clip from <em>Danganronpa: The Animation</em> (Summer&nbsp;2013, Funimation)</figcaption>  </figure>
<p>The whole move to OOONA was questionable in itself, as while OOONA was capable of exporting subtitles to both WebVTT for FunimationNow and TTML (or <a href="https://en.wikipedia.org/wiki/SubRip" rel="noopener" target="_blank">SRT</a>, a very limited subtitle format) for Hulu in 2018, <em>so was MacCaption.</em> Why start paying for a monthly subscription service when your existing paid-for enterprise software should be able to deal with your needs just fine? I suspect the primary motivation behind the move (which could have even originated from the new parent company Sony) might have been the fact that it was trendy for companies at the time to move everything they possibly could to The Cloud™, regardless of how much sense it actually made… but that’s enough about OOONA for now.</p>
<p>Ultimately, <strong>Funimation’s subtitling standards were extremely poor to begin with, and they only managed to make them worse over time.</strong> That is something that only utter indifference or outright disdain for anime as a medium could bring about, which seems to have been the exact attitude that Gen Fukunaga cultivated at the executive levels of Funimation – <strong>and his followers appear to have carried the torch even after his departure from the company.</strong> But more on that in the next section, when we finally get to the Funimation-Crunchyroll merger.</p>

<h2 id="the-funimation-crunchyroll-merger-and-its-consequences">The Funimation-Crunchyroll merger and its consequences</h2>
<p>Following Sony’s 2017 purchase of Funimation, in 2019 Sony bought out Gen Fukunaga from the company entirely, which led to him stepping down as the General Manager, with Colin Decker taking his place. Soon after, Sony formed the Funimation Global Group to consolidate all the international anime publishing services it had bought, with Decker in charge of the joint venture as the CEO. Then, in late 2020, <strong>Sony announced that they were going to buy Crunchyroll, placing it under the executive control of the Funimation Global Group.</strong> The acquisition was completed in August 2021, coming with a statement from Sony that their goal is to <a href="https://www.sonypictures.com/corp/press_releases/2021/0809/sonysfunimationglobalgroupcompletesacquisitionofcrunchyrollfromatt" rel="noopener" target="_blank">“create a unified anime subscription experience as soon as possible”</a>.</p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/funiroll.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/funiroll.jpg" alt="Crunchyroll and Funimation logos side by side." loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>Soon, there would only be one.</p></figcaption>  </figure>
<p>Then, in March 2022, the news came that Funimation, Crunchyroll, Wakanim, and VRV (Crunchyroll’s more general streaming service) would all be merged together into a single streaming service that would exist under the name of Crunchyroll (as it had the strongest brand of the lot). <strong>Funimation Global Group LLC was renamed to Crunchyroll LLC, with Funimation executives remaining in charge.</strong> Soon after, Colin Decker stepped down as the CEO, with Rahul Purini (previously COO) taking his place. The merger was complete.</p>
<p>However, as is often the case with mergers &amp; acquisitions, layoffs were on the horizon. In 2023, <a href="https://www.animenewsnetwork.com/news/2023-02-21/crunchyroll-lays-off-approximately-85-employees-globally/.195161" rel="noopener" target="_blank">85 people were laid off globally</a> in the name of employee redundancy. More layoffs have happened since then, with the most recent one being from <a href="https://variety.com/2025/tv/news/crunchyroll-layoffs-restructuring-international-1236487273/" rel="noopener" target="_blank">just a couple months back in August 2025</a>.</p>

<p>Things weren’t much better for those left behind, as laid out in <a href="https://archive.ph/jxvFK" rel="noopener" target="_blank">this Bloomberg article</a> from 2024. Staff from Funimation was notably hostile towards those from Crunchyroll:</p>
<blockquote>
<p>Tension between the camps arose almost immediately. In a Zoom meeting announcing [Sony’s purchase of Crunchyroll], <strong>Funimation workers accused Crunchyroll of being pirates,</strong> alluding to the site’s history, according to two people who were present.</p>
</blockquote>
<p>While Crunchyroll workers were quickly frustrated with the new executives from Funimation:</p>
<blockquote>
<p>Current or former employees describe Crunchyroll’s new management–primarily from Funimation–as out-of-touch with employees and the anime fans the company once prioritized. <strong>Some executives write off anime as “kids’ cartoons,”</strong> they said, and resist hiring job candidates who describe themselves as fans.</p>
</blockquote>
<p>And while all these internal troubles were going on, Crunchyroll CEO Rahul Purini was excited to talk about <a href="https://archive.is/O1fjt" rel="noopener" target="_blank">how interested he is in AI-generated subtitles</a>.</p>

<h3 id="how-typesetting-gets-destroyed">How typesetting gets destroyed</h3>
<p>In 2025, the executives came up with an idea: Crunchyroll should move away from <a href="#aegisub-ass-introduction">Aegisub and ASS subtitles with typesetting</a> and start producing exclusively <a href="#ttml-example">limited TTML subtitles without typesetting</a> in <a href="#ooona-introduction">OOONA Tools</a>. <strong>The likely end goal of this is to get rid of Crunchyroll’s <a href="#cr-libass">unique ASS-based subtitle rendering</a> entirely in favor of something more “industry standard” like TTML-based subtitle rendering.</strong> This would mean no longer having to pay staff for manual ASS-to-TTML conversion, as well as being able to drop the relatively expensive fully hardsubbed encodes for limited playback environments where ASS rendering is not possible <em>(but some sort of TTML rendering usually is).</em></p>

<p><strong>However, a major change affecting all aspects of the company’s subtitling pipeline doesn’t happen overnight,</strong> especially considering Crunchyroll’s large back catalog of ASS subtitles with typesetting that couldn’t be automatically converted to limited TTML subtitles without typesetting. So while the subtitling staff was to be (begrudgingly) busy experimenting and onboarding with OOONA and doing manual ASS-to-TTML conversions for back catalog titles, <strong>technical work would also need to be done to prepare for this vision of a TTML-only future.</strong></p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/moneat-cr-1.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/moneat-cr-1.jpg" alt="" loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>And what an exciting future of not being able to read signs that would be! Screenshot from <em>This Monster Wants to Eat Me</em> (Fall&nbsp;2025, Crunchyroll)</p></figcaption>  </figure>
<p>For this purpose, Crunchyroll seems to have decided that it would take its existing manual ASS-to-TTML conversions produced by the subtitling staff and treat them as the new master subtitle files. These TTML “masters” would then be—for the time being—converted back to ASS with <a href="https://www.closedcaptionconverter.com/" rel="noopener" target="_blank">Closed Caption Converter</a> for use with the current ASS-based subtitle rendering. And so, with the start of the Fall 2025 anime season, a plan like this was pushed to production; while regular ASS subtitles were still being produced by Crunchyroll’s subtitling staff, <strong>these ASS subtitles with typesetting were generally left unused, while only limited ASS-to-TTML-to-ASS conversions without typesetting were being presented to customers on most shows.</strong></p>

<p>Implementing this interim pipeline with Closed Caption Converter didn’t seem to go exactly as planned, though, <strong>as some Fall 2025 shows on Crunchyroll ended up having no subtitles at all on release,</strong> including the premieres of the latest seasons of hit shows <a href="https://en.wikipedia.org/wiki/My_Hero_Academia" rel="noopener" target="_blank">My Hero Academia</a> and <a href="https://en.wikipedia.org/wiki/Spy_%C3%97_Family" rel="noopener" target="_blank">Spy × Family</a>.</p>

<p id="cr-statement-2025-10-09">With the internet taking note of all this, on the 9th of October 2025 <a href="https://www.animenewsnetwork.com/news/2025-10-09/crunchyroll-cites-internal-system-problems-regarding-subtitles-for-fall-2025-anime/.229669" rel="noopener" target="_blank">Crunchyroll responded to a press inquiry</a> by Anime News Network with the following statement:</p>
<blockquote>
<p>Over the past few days, some users experienced delays in accessing the content they wanted and subtitle issues across certain series. These were caused by internal system problems – not by any change in how we create subtitles, use of new vendors or AI. Those internal issues have now been fully resolved.</p>
<p>Quality subtitles are a core part of what makes watching anime on Crunchyroll so special. They connect global fans to the heart of every story, and we take that responsibility seriously.</p>
<p>Thank you for your patience. We’re committed to continuing to deliver the authenticity, quality, and care that fans deserve.</p>
</blockquote>
<p id="cr-actions">Following this statement, some of the new Fall 2025 shows have had their ASS-to-TTML-to-ASS subtitles switched out to the previously unused regular ASS subtitles. Other shows haven’t. <strong>And some shows in the Crunchyroll back catalog have been updated with ASS-to-TTML-to-ASS subtitles,</strong> though the exact timing of these back catalog updates is unknown.</p>

<p>With all of this, <strong>the future of typesetting on Crunchyroll is unclear.</strong></p>
<hr>
<p>And that’s how we’ve found ourselves in the situation we face today. Remember what the <a href="#cr-first-subs">first Crunchyroll subtitles from 2009</a> looked like? Yeah, these new subtitles adhering to limited TTML standards <em>are even worse than the subtitles from 2009</em> in terms of how on-screen text can be handled! In other words: <strong>The presentation quality of Crunchyroll’s first-party subtitles has reached an all-time low in 2025.</strong></p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/chitose-cr-1.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/chitose-cr-1.jpg" alt="" loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>Can’t even handle a next episode preview properly anymore. Screenshot from <em>Chitose Is in the Ramune Bottle</em> (Fall&nbsp;2025, Crunchyroll)</p></figcaption>  </figure>
<p>There is only one conclusion that can be drawn from that: <strong>the Funimation-turned-Crunchyroll executives still do not have any respect for anime as a medium.</strong> In addition, they seem to be treating Crunchyroll and its ways of doing things as the ways of <q>pirates</q> – which isn’t entirely incorrect, as Crunchyroll’s use of Aegisub and ASS <em>did</em> originate from the ways of pirate fansubbers. But fansubbers deeply care about anime as medium (they wouldn’t be illegally subtitling it for free as a hobby otherwise), which in turn means that <strong>the ways fansubbers have developed to subtitle anime are in fact extremely efficient for the job</strong> – much better than basically any “industry standards” for subtitling, even.</p>
<p>But that clearly doesn’t matter to the executives. <strong>The only thing that seems to be on their mind is how to best make money with kids’ cartoons</strong> that none of them personally watch, and what they seem to consider “best” is <em>getting rid of everything positively unique about Crunchyroll in favor of doing things the Funimation way,</em> even if that means ditching Aegisub and ASS in favor of OOONA Tools and TTML and getting rid of typesetting in the process. <span id="crunchyroll-maccaption">This</span> conclusion is further supported by the fact that <strong>Crunchyroll has kept Funimation’s old <a href="#maccaption-introduction">MacCaption-based workflow</a> around for its Blu-ray releases,</strong> with notable reduction in typesetting quality on Blu-ray as a result:</p>


<p>Then there’s the whole plan of moving to OOONA in general, which is even more questionable than it was back in the Funimation days. <strong>Crunchyroll has a lot more to lose in terms of subtitle quality than Funimation ever did,</strong> yet the executives seem to want to go back to their “old reliable” regardless. I can’t even see it saving them any money in the long run, considering that Aegisub is completely free software while OOONA will incur constant ongoing costs with its per-user subscription pricing. Rather than authoring limited TTML in OOONA directly, <strong>paying the subtitling staff to keep the manual ASS to TTML conversions going would likely be cheaper!</strong></p>

<p>Beyond that, there is also the thing about <strong>OOONA being an Israeli company.</strong> It is certainly a choice, not only in 2018 but most certainly in 2025, to heavily invest in the services of a company from a country that is <a href="https://en.wikipedia.org/wiki/Gaza_genocide" rel="noopener" target="_blank">actively committing genocide</a>. However, to quell some unsubstantiated internet discourse I have seen in relation to this, I do want to emphasize that <strong>OOONA being Israeli is not really directly relevant to the quality issues this article is about.</strong></p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/eztitles.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/eztitles.jpg" alt="" loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>EZTitles is another popular enterprise subtitling software. Notice how they mention AI directly in their navigation.</p></figcaption>  </figure>
<p>The reason for this lies in enterprise subtitling software (“industry standards”) being universally poor when it comes to producing high quality typesetting for anime, so it wouldn’t really matter which software suite a switch was being made to – <strong>no matter what, moving away from Aegisub would destroy typesetting as it currently exists on Crunchyroll.</strong> And while Crunchyroll’s CEO has expressed his interest in AI subtitles, at least currently there has been no signs of any kind of AI (Israeli or otherwise) being used to create first-party subtitles on Crunchyroll.</p>

<h3 id="why-crunchyroll-is-so-confident-it-will-get-away-with-this-or-how-capitalism-ruins-everything">Why Crunchyroll is so confident it will get away with this (or: how capitalism ruins everything)</h3>
<p>Finally, I want to talk about the possible reasons for Crunchyroll executives feeling so confident about getting away with making their own primary product so much worse. Ultimately, it comes down to the fact that <strong>international anime licensing operates primarily on an exclusive licensing model.</strong> This means that generally only one service will be able to offer a specific title in specific language(s) in specific region(s), unless the service voluntarily decides to sublicense it out to others. This in turn upends the assumption that the existence of multiple anime services would be beneficial to consumers, <strong>as the services don’t actually have to engage in competition on customer-beneficial factors like service quality almost at all</strong> – instead, they can just focus on hoarding as many exclusive licenses as possible.</p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/kun-gao-ama.png" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/kun-gao-ama.png" alt="Screenshot of three comments from a Reddit AMA. Check the link in the caption to read the contents in full." loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>I once asked former Crunchyroll CEO Kun Gao about “exclusivity or completeness” <a href="https://www.reddit.com/r/IAmA/comments/2b26ou/im_kun_gao_the_cofounder_and_ceo_of_crunchyroll/cj12zuo/?context=3" rel="noopener" target="_blank">in this Reddit AMA.</a> He dodged the question but basically said “exclusivity”.</p></figcaption>  </figure>
<p>This kind of “competition” twisted by exclusive licensing is more like a casino, where the customers might occasionally be thrown a bone, <strong>but at the end of the day, the house always wins.</strong> And the anime companies very much prefer to keep it that way, even if it means never being able to offer full coverage of new anime seasons – a limited amount of exclusives is much more important to them. Dreams of infinite growth are what drives the modern-day game of capitalism, and spending money to please customers rather than shareholders goes directly against said dreams. <strong>It’s all about spending as little money as possible to make as much money as possible.</strong></p>
<p>This is why the capitalists in charge of all the big companies these days are so excited about AI too: nothing gets them going more than the idea of not having for pay for those pesky human employees. <em>This is no doubt the actual reason why Crunchyroll CEO Rahul Purini is interested in AI subtitles.</em> It doesn’t matter that <em>anime localization costs are a drop in the bucket compared to the overall costs of anime production,</em> even if you were talking about super high quality work with fansub-level typesetting. <em>Any excuse to cut the wages of real human workers is one step closer to the next yacht purchase for the executive upper class.</em></p>
<p>…Whew, got a bit heated there. Anyway, the most likely reason why Crunchyroll executives believe they can get away with reducing the quality of their own service so much? <strong>Because Crunchyroll doesn’t have any meaningful competition thanks to the primarily-exclusive licensing model used by the international anime industry.</strong> Even if they make the service worse, what can you do about it? Cancel your subscription and not watch the new anime you’re excited about?</p>
<h2 id="what-you-can-do-about-it">What you can do about it</h2>
<p>If you are currently subscribed to Crunchyroll, <a href="https://www.crunchyroll.com/cancellation" rel="noopener" target="_blank">cancel your subscription.</a> <strong>When asked for a reason, mention the bad subtitle quality and lack of <a href="#typesetting-introduction">typesetting.</a></strong> <em>You could even link to this article.</em> Beyond that, and this applies to people who aren’t subscribed to Crunchyroll as well: <strong>spread the word!</strong> Share this article around, talk to people about how Crunchyroll is destroying its subtitles, make it so that Crunchyroll executives can’t ignore the issue. And the most important thing: <strong>Keep it up until Crunchyroll actually makes a clear public commitment to keep typesetting anime.</strong></p>

<p>Why ask for an explicit commitment? Because back in 2017 <a href="https://medium.com/@Daiz/crunchyrolls-reduced-video-quality-is-deliberate-cost-cutting-at-the-expense-of-paying-customers-c86c6899033b" rel="noopener" target="_blank">when Crunchyroll tried to drastically lower its video quality as a cost-cutting measure</a>, vocal user complaints and subscription cancellations forced them to backtrack on it, eventually leading the company to <a href="https://web.archive.org/web/20210622165834/http://www.crunchyroll.com/forumtopic-985103/our-statement-regarding-recent-video-quality-issues#expand" rel="noopener" target="_blank">make a statement</a> and not just <a href="https://medium.com/ellation-tech/improving-video-quality-for-crunchyroll-and-vrv-dd587261a364" rel="noopener" target="_blank">one</a> but <a href="https://medium.com/ellation-tech/stabilizing-improving-crunchyroll-service-c44c53789e86" rel="noopener" target="_blank">two</a> technical follow-up posts where it <em>explicitly promised</em> to do better, <strong>and in the end, video quality actually improved compared to what was previously available.</strong> Ideally, the same would happen with Crunchyroll’s typesetting here.</p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/cr-video-quality-promise.png" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/cr-video-quality-promise.png" alt="Screenshot of the first technical post from 2017 linked above, showing it's title of Improving Video Quality for Crunchyroll and VRV." loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>“Improving Subtitle Quality for Crunchyroll” is what we’d like to see here in 2025.</p></figcaption>  </figure>

<p>I also want to emphasize that the <a href="#cr-statement-2025-10-09">recent statement</a> Crunchyroll made about its Fall 2025 subtitles <strong>isn’t really worth anything.</strong> It’s worded in an intentionally obfuscated manner as to what actually has been <q>fixed</q> – is it the lack of typesetting or just the issues with subtitles not going up for new releases? Then it just outright lies about there being <q>no changes</q> with <a href="#how-typesetting-gets-destroyed">how subtitles are being handled</a>, before ending on empty platitudes about <q>quality subtitles</q> that mean nothing without concrete actions to back them up.</p>
<p>And so far, <strong><a href="#cr-actions">the actions of Crunchyroll</a> have made the future of typesetting on the service anything but clear.</strong> The lower quality subtitles in the back catalog are especially alarming, as the back catalog was exactly where Crunchyroll also started with its 2017 video quality reduction plans, all the while remaining careful with changes to simulcasts where people were paying closer attention – <strong>which is exactly what seems to be happening with subtitles on Crunchyroll right now.</strong></p>

<hr>
<p><b>To sum things up:</b> Without a clear public commitment to stick to higher subtitling standards that include typesetting, it is very likely that Crunchyroll executives will just delay their typesetting-killing plans and try again later. That’s why <em>you</em> need to <a href="#what-you-can-do-about-it">cancel your subscription</a>, encourage others to do so, and keep talking about this issue <em>until Crunchyroll explicitly promises to do better.</em></p>
<p><strong>Together, we can save Crunchyroll from itself!</strong></p>
<hr>
<h3 id="acknowledgements">Acknowledgements</h3>
<p>This article would have never been as thorough and detailed as it is without the assistance of the following people:</p>
<ul>
<li><strong>The multiple current and former Crunchyroll and Funimation workers</strong> who came forward to indepedently confirm the many previously unpublished details found in this article. <em>Huge thanks, all of you.</em></li>
<li><a href="https://bsky.app/profile/bigonanime.bsky.social" rel="noopener" target="_blank">BigOnAnime</a> – for his great help with researching the historical technical details of Funimation’s subtitling standards. <em>Thank you.</em></li>
<li><a href="https://bsky.app/profile/enonibobble.moe" rel="noopener" target="_blank">enonibobble</a> – for his help with various screenshots and technical analysis of Crunchyroll subtitles. <em>Thank you.</em></li>
<li><a href="https://bsky.app/profile/duxovni.com/post/3lydqz63sis2s" rel="noopener" target="_blank">Faye&nbsp;Duxovni</a> – for bringing Crunchyroll’s use of old Funimation workflows for Blu-rays to my attention and providing the screenshots of it that are used in the article. <em>Thank you.</em></li>
<li><a href="https://bsky.app/profile/rcombs.me" rel="noopener" target="_blank">Ridley</a>, <a href="https://bsky.app/profile/witchymary.fansubcar.tel" rel="noopener" target="_blank">witchymary</a>, <a href="https://bsky.app/profile/jhiday.bsky.social" rel="noopener" target="_blank">Jhiday</a> – for proofreading this article before release. <em>Thanks, all of you.</em></li>
<li><strong>People on social media</strong> who answered public questions I asked or otherwise helped with various small pieces of research. <em>Thanks, all of you.</em></li>
</ul>
<h3 id="external-coverage">External coverage</h3>
<p>I’m not the only one to have made note of Crunchyroll’s recent subtitle shenanigans, so here’s some additional reading/watching on the subject elsewhere:</p>
<ul>
<li><a href="https://animebythenumbers.substack.com/p/worse-crunchyroll-subtitles" rel="noopener" target="_blank">Why did Crunchyroll’s subtitles just get worse?</a> by <b>Miles Atherton</b> (former head of marketing for Crunchyroll), on the newsletter <b>Anime By The Numbers.</b> This includes some additional details (like numbers!) that I didn’t go over here (because this article was long enough as-is), so I can recommend giving it a read.</li>
<li><a href="https://www.youtube.com/watch?v=B-DX0Zolr6g" rel="noopener" target="_blank">The Absolute State of Crunchyroll</a> by YouTuber <b>Mother’s Basement.</b> This is a good watch just to see how bad the new Crunchyroll subtitles look like in action. Additionally, I didn’t really talk about how badly timing quality has been affected by the recent changes too, but this video has some good examples of that as well.</li>
<li><a href="https://www.animenewsnetwork.com/answerman/2025-10-27/.230367" rel="noopener" target="_blank">Are Subtitles Getting Smaller?</a> by <b>Jerome Mazandarani</b> on <b>Anime News Network</b>. This <b>Answerman</b> column is nominally about subtitles getting visually smaller, but most of it ends up being about the Crunchyroll subtitle situation. Jerome does keep incorrectly saying that general streaming services use the very bare-bones subtitle format SRT rather than TTML, though, and while these services do support SRT for ingestion (ie. content partners can deliver subtitles as SRT) and anime companies might even be making use of that, TTML is what the services actually use internally. SRT does not officially support any kind of positioning whatsoever, which means that even placing subtitles at the top of the screen would be impossible with it if the normal placement was on bottom.</li>
<li><a href="https://www.animenewsnetwork.com/this-week-in-anime/2025-10-14/.229891" rel="noopener" target="_blank">The Crunchyroll Sub Flub</a> by <b>Lucas DeRuyter</b> and <b>Coop Bicknell</b>, also on <b>Anime News Network</b>. Nothing particularly new in this one if you’re familiar with all the other coverage, but it’s nice to see this get discussed on the <b>This Week in Anime</b> column regardless. The more eyes on the subject, the better.</li>
</ul>
<hr>
<div id="about"><p><img src="https://daiz.moe/img/avatar-remilia-black-medium-3.jpg" alt="Daiz's avatar, featuring the female vampire Remilia Scarlet from Touhou Project" data-astro-cid-vjqwbl62=""></p><p>I’m <b>Daiz</b>, a digital distribution expert and high quality media enthusiast. I have over a decade of experience with Japanese-to-English media localization, including anime subtitling, and I also care deeply about consumer rights. <a href="https://bsky.app/profile/daiz.moe" rel="noopener" target="_blank">You can follow me on Bluesky</a>, or <a href="mailto:contact@daiz.moe">drop me a mail</a>.</p></div>
<hr>
<p>I’m working on getting Bluesky comments embedded at the end of the posts. For the time being though, <a href="https://bsky.app/profile/daiz.moe/post/3m4ejjogo5k2w" rel="noopener" target="_blank">you can read and join the discussion here!</a></p>  </article>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Raspberry Pi Pico Bit-Bangs 100 Mbit/S Ethernet (155 pts)]]></title>
            <link>https://www.elektormagazine.com/news/rp2350-bit-bangs-100-mbit-ethernet</link>
            <guid>45754439</guid>
            <pubDate>Wed, 29 Oct 2025 23:21:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.elektormagazine.com/news/rp2350-bit-bangs-100-mbit-ethernet">https://www.elektormagazine.com/news/rp2350-bit-bangs-100-mbit-ethernet</a>, See on <a href="https://news.ycombinator.com/item?id=45754439">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>         <div>   <p>Three years after the popular Pico-10BASE-T experiment and following bit-banged USB in 2023, developer Steve Markgraf now bit-bangs 100 Mbit/s Fast Ethernet on the Raspberry Pi Pico using only software and PIO. Let's take a look!</p>   <div itemprop="articleBody"> <p data-end="801" data-start="342">Three years ago, @<a href="https://github.com/kingyoPiyo" target="_blank">kingyoPiyo</a>’s Pico-10BASE-T project drew wide attention <a href="https://www.elektormagazine.com/news/10base-t-ethernet-bit-banged-on-a-raspberry-pi-pico" target="_blank">right here on Elektor</a> for implementing 10 Mbit/s Ethernet on the Raspberry Pi Pico using just a few resistors. In 2023, another milestone followed with <a href="https://www.elektormagazine.com/news/rp2040-bit-banged-usb-host" target="_blank">bit-banged USB</a>, showing how far the RP2040’s (and now RP2350) programmable I/O could be pushed.</p>

<h2 data-end="801" data-start="342">What Can an RP2350 Bit-Bang Next?</h2>

<div data-end="801" data-start="342"><p>Now, developer Steve Markgraf (GitHub <a href="https://github.com/steve-m" target="_blank">@steve-m</a>) has extended the concept with Pico-100BASE-TX — a 100 Mbit/s Fast Ethernet transmitter running entirely in software.</p><p>

Markgraf’s implementation uses the PIO and DMA to perform MLT-3 encoding, 4B5B line coding, and scrambling at a 125 MHz symbol rate. The result is a functioning 100 Mbit/s link capable of streaming about 11 Mbyte/s over UDP, demonstrated by real-time audio and ADC data streams.</p></div>

<div>
<figure><img loading="lazy" alt="Bit-Bangs 100 Mbit/s Ethernet on a Raspberry Pi Pico" height="471" src="https://cdn.xingosoftware.com/elektor/images/fetch/dpr_1,w_660,h_471,c_fit/https%3A%2F%2Fwww.elektormagazine.com%2Fassets%2Fupload%2Fimages%2F1%2F20251022042335_image.png" popup-src="https://cdn.xingosoftware.com/elektor/images/fetch/dpr_1,w_1000/https%3A%2F%2Fwww.elektormagazine.com%2Fassets%2Fupload%2Fimages%2F1%2F20251022042335_image.png" width="660">
<figcaption>Bit-banging 100 Mbps Ethernet on a Raspberry Pi Pico. Source:&nbsp;https://github.com/steve-m/Pico-100BASE-TX</figcaption>
</figure>
</div>

<p data-end="1271" data-start="1085">As before, this is a transmit-only proof of concept and must not be connected to PoE-enabled hardware. A pulse transformer or intermediary Ethernet switch is recommended for isolation.</p>

<h2 data-end="1271" data-start="1085">Check Out the Rest of His Repo</h2>

<p data-end="1516" data-start="1273">Example applications in the repository include a counter, internal-ADC streamer, and an audio demo using a PCM1802 converter at 75 kHz. The library supports both the RP2040 and the newer RP2350 (Pico 2) and builds with the standard Pico SDK.</p>

<p data-end="1599" data-start="1518">Source: <a data-end="1597" data-start="1526" href="https://github.com/steve-m/Pico-100BASE-TX" rel="noopener" target="_new">Pico-100BASE-TX on GitHub</a>&nbsp;— check it in action in the video there.<br>
&nbsp;</p>

<article data-scroll-anchor="true" data-testid="conversation-turn-14" data-turn="assistant" data-turn-id="e76bab7f-773d-4d99-a04d-24dc43cce9a4" dir="auto" tabindex="-1">
<p data-end="623" data-is-last-node="" data-is-only-node="" data-start="59">Beyond the technical achievement, projects like this hint at new possibilities for low-cost, high-speed data acquisition and streaming using microcontrollers that were never designed for it. A Pico capable of pushing 11 MB/s over Ethernet could form the basis of compact, inexpensive test instruments, remote sensors, or experimental network interfaces — all without a dedicated PHY chip. As these bit-banged interfaces become faster and more capable, the question naturally follows: how far can software-defined hardware really go on a two-dollar microcontroller?</p>
</article>

<div data-tag="300">  <p> Subscribe</p>  <p><b>Tag alert:</b> Subscribe to the tag <a href="https://www.elektormagazine.com/tags/raspberry-pi">Raspberry Pi</a> and you will receive an e-mail as soon as a new item about it is published on our website! </p></div>


 </div>   </div>       <p><span>
          <b>Add a rating to this article</b>
                       <b><i></i></b>
      </span>
</p>

     </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A century of reforestation helped keep the eastern US cool (2024) (119 pts)]]></title>
            <link>https://news.agu.org/press-release/a-century-of-reforestation-helped-keep-the-eastern-us-cool/</link>
            <guid>45754390</guid>
            <pubDate>Wed, 29 Oct 2025 23:17:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.agu.org/press-release/a-century-of-reforestation-helped-keep-the-eastern-us-cool/">https://news.agu.org/press-release/a-century-of-reforestation-helped-keep-the-eastern-us-cool/</a>, See on <a href="https://news.ycombinator.com/item?id=45754390">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
	
				<article id="post-24154">
			<header>
				
							</header>
			<div><h2><em>Much of the U.S. warmed during the 20th century, but the eastern part of the country remained mysteriously cool. The recovery of forests could explain why</em></h2>
<p>13 February 2024<br>


<!-- Go to www.addthis.com/dashboard to customize your tools -->
</p>
<br>
<div><div id="attachment_24155"><p><a href="https://news.agu.org/files/2024/02/wes-hicks-KS0qGHf9hXk-unsplash.jpg"><img fetchpriority="high" decoding="async" aria-describedby="caption-attachment-24155" src="https://news.agu.org/files/2024/02/wes-hicks-KS0qGHf9hXk-unsplash-1024x683.jpg" alt="" width="1024" height="683" srcset="https://news.agu.org/files/2024/02/wes-hicks-KS0qGHf9hXk-unsplash-1024x683.jpg 1024w, https://news.agu.org/files/2024/02/wes-hicks-KS0qGHf9hXk-unsplash-300x200.jpg 300w, https://news.agu.org/files/2024/02/wes-hicks-KS0qGHf9hXk-unsplash-768x512.jpg 768w, https://news.agu.org/files/2024/02/wes-hicks-KS0qGHf9hXk-unsplash-1536x1024.jpg 1536w, https://news.agu.org/files/2024/02/wes-hicks-KS0qGHf9hXk-unsplash.jpg 1920w" sizes="(max-width: 1024px) 100vw, 1024px"></a></p><p id="caption-attachment-24155">A century of forest growth likely helped keep the eastern United States cool as the rest of the country warmed, according to <a href="https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2023EF003663" target="_blank" rel="noopener">a new study</a> in <em>Earth’s Future</em>. <em>Credit: Wes Hicks/unsplash</em></p></div>
<p><strong>AGU press contact</strong>:<br>
Liza Lester, +1 (202) 777-7494, <a href="https://news.agu.org/cdn-cgi/l/email-protection#f29c978581b2939587dc9d8095"><span data-cfemail="2d43485a5e6d4c4a5803425f4a">[email&nbsp;protected]</span></a> (UTC-5 hours)</p>
<p><strong>Contact information for the researchers:</strong><br>
Kim Novick, Indiana University, <a href="https://news.agu.org/cdn-cgi/l/email-protection#f09b9e9f8699939bb09985de959485"><span data-cfemail="492227263f202a2209203c672c2d3c">[email&nbsp;protected]</span></a> (UTC-5 hours)<br>
Mallory Barnes, Indiana University, <a href="https://news.agu.org/cdn-cgi/l/email-protection#1f727e737d7e6d715f766a317a7b6a"><span data-cfemail="f59894999794879bb59c80db909180">[email&nbsp;protected]</span></a> (UTC-5 hours)</p>
<hr>
<p>WASHINGTON — Widespread 20<sup>th</sup>-century reforestation in the eastern United States helped counter rising temperatures due to climate change, according to <a href="https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2023EF003663">new research</a>. The authors highlight the potential of forests as regional climate adaptation tools, which are needed along with a decrease in carbon emissions.</p>
<p>“It’s all about figuring out how much forests can cool down our environment and the extent of the effect,” said Mallory Barnes, lead author of the study and an environmental scientist at Indiana University. “This knowledge is key not only for large-scale reforestation projections aimed at climate mitigation, but also for initiatives like urban tree planting.”</p>
<p>The study was published in the AGU journal <em>Earth’s Future</em>, which publishes interdisciplinary research on the past, present and future of our planet and its inhabitants.</p>
<h3><strong>Return of the trees</strong></h3>
<p>Before European colonization, the eastern United States was almost entirely covered in temperate forests. From the late 18<sup>th</sup> to early 20<sup>th</sup> centuries, timber harvests and clearing for agriculture led to forest losses exceeding 90% in some areas. In the 1930s, efforts to revive the forests, coupled with the abandonment and subsequent reforestation of subpar agricultural fields, kicked off an almost century-long comeback for eastern forests. About <a href="https://onlinelibrary.wiley.com/doi/full/10.1046/j.1365-2699.2000.00166.x">15 million hectares</a> of forest have since grown in these areas.</p>
<p>“The extent of the deforestation that happened in the eastern United States is remarkable, and the consequences were grave,” said Kim Novick, an environmental scientist at Indiana University and co-author of the new study. “It was a dramatic land cover change, and not that long ago.”</p>
<div id="attachment_24156"><p><a href="https://news.agu.org/files/2024/02/warming-hole.png"><img decoding="async" aria-describedby="caption-attachment-24156" src="https://news.agu.org/files/2024/02/warming-hole.png" alt="" width="255" height="131" srcset="https://news.agu.org/files/2024/02/warming-hole.png 729w, https://news.agu.org/files/2024/02/warming-hole-300x154.png 300w" sizes="(max-width: 255px) 100vw, 255px"></a></p><p id="caption-attachment-24156"><em>The “warming hole,” or anomalously cool region, is shown in blue. Credit: Barnes et al.</em></p></div>
<p>During the period of regrowth, global warming was well underway, with temperatures across North America <a href="https://www.epa.gov/climate-indicators/climate-change-indicators-us-and-global-temperature">rising</a> 0.7 degrees Celsius (1.23 degrees Fahrenheit) on average. In contrast, from 1900 to 2000, the East Coast and Southeast cooled by about 0.3 degrees Celsius (0.5 degrees Fahrenheit), with the strongest cooling in the southeast.</p>
<p>Previous studies suggested the cooling could be caused by aerosols, agricultural activity or increased precipitation, but many of these factors would only explain highly localized cooling. Despite known relationships between forests and cooling, studies had not considered forests as a possible explanation for the anomalous, widespread cooling.</p>
<p>“This widespread history of reforestation, a huge shift in land cover, hasn’t been widely studied for how it could’ve contributed to the anomalous lack of warming in the eastern U.S., which climate scientists call a ‘warming hole,’” Barnes said. “That’s why we initially set out to do this work.”</p>

<h3><strong>Trees are cool</strong></h3>
<p>Barnes, Novick and their team used a combination of data from satellites and 58 meteorological towers to compare forests to nearby grasslands and croplands, allowing an examination of how changes in forest cover can influence ground surface temperatures and in the few meters of air right above the surface.</p>
<p>The researchers found that forests in the eastern U.S. today cool the land’s surface by 1 to 2 degrees Celsius (1.8 to 3.6 degrees Fahrenheit) annually. The strongest cooling effect occurs at midday in the summer, when trees lower temperatures by 2 to 5 degrees Celsius (3.6 to 9 degrees Fahrenheit) — providing relief when it’s needed most.</p>
<p>Using data from a network of gas-measuring towers, the team showed that this cooling effect also extends to the air, with forests lowering the near-surface air temperature by up to 1 degree Celsius (1.8 degrees Fahrenheit) during midday. (Previous work on trees’ cooling effect has focused on land, not air, temperatures.)</p>

<p>The team then used historic land cover and daily weather data from 398 weather stations to track the relationship between forest cover and land and near-surface air temperatures from 1900 to 2010. They found that by the end of the 20<sup>th</sup> century, weather stations surrounded by forests were up to 1 degree Celsius (1.8 degrees Fahrenheit) cooler than locations that did not undergo reforestation. Spots up to 300 meters (984 feet) away were also cooled, suggesting the cooling effect of reforestation could have extended even to unforested parts of the landscape.</p>
<p>Other factors, such as changes in agricultural irrigation, may have also had a cooling effect on the study region. The reforestation of the eastern United States in the 20<sup>th</sup> century likely contributed to, but cannot fully explain, the cooling anomaly, the authors said.</p>
<p>“It’s exciting to be able to contribute additional information to the long-standing and perplexing question of, ‘Why hasn’t the eastern United States warmed at a rate commensurate with the rest of the world?’” Barnes said. “We can’t explain all of the cooling, but we propose that reforestation is an important part of the story.”</p>
<p>Reforestation in the eastern United States is generally regarded as a <a href="https://www.science.org/doi/full/10.1126/sciadv.aat1869">viable strategy</a> for climate mitigation due to the capacity of these forests to sequester and store carbon. The authors note that their work suggests that eastern United States reforestation also represents an important tool for climate adaptation. However, in different environments, such as snow-covered boreal regions, adding trees could have a warming effect. In some locations, reforestation can also affect precipitation, cloud cover, and other regional scale processes in ways that may or may not be beneficial. Land managers must therefore consider other environmental factors when evaluating the utility of forests as a climate adaptation tool.</p>
<p><em>***</em></p>
<h3><strong>Notes for Journalists:</strong></h3>
<p>This study is published in <em>Earth’s Future, </em>an open-access journal. Neither the paper nor this press release is under embargo. <a href="https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1029/2023EF003663">View and download a pdf of the study here</a>.</p>
<p>This study is part of an ongoing special collection, “<a href="https://agupubs.onlinelibrary.wiley.com/doi/toc/10.1002/(ISSN)2169-8961.NTRECLMTE">Quantifying Nature-based Climate Solutions</a>,” from AGU’s publications.</p>
<h3><strong>Paper title:</strong></h3>
<p>“A Century of Reforestation Reduced Anthropogenic Warming in the Eastern United States”</p>
<h3><strong>Authors:</strong></h3>
<ul>
<li>Mallory L. Barnes (corresponding author), Indiana University, Indiana, USA</li>
<li>Kim Novick (corresponding author), Indiana University, Indiana, USA</li>
<li>Quan Zhang, Wuhan University, Wuhan, China</li>
<li>Scott M. Robeson, Indiana University, Indiana, USA</li>
<li>Lily Young, Indiana University, Indiana, USA</li>
<li>Elizabeth A. Burakowski, University of New Hampshire, New Hampshire, USA</li>
<li>Christopher. Oishi, USDA Forest Service, North Carolina, USA</li>
<li>Paul C. Stoy, University of Wisconsin-Madison, USA</li>
<li>Gaby Katul, Duke University, North Carolina, USA</li>
</ul>
<hr>
<p><em>AGU (</em><a href="http://www.agu.org/"><em>www.agu.org</em></a><em>) is a global community supporting more than half a million advocates and professionals in Earth and space sciences. Through broad and inclusive partnerships, we advance discovery and solution science that accelerate knowledge and create solutions that are ethical, unbiased and respectful of communities and their values. Our programs include serving as a scholarly publisher, convening virtual and in-person events and providing career support. We live our values in everything we do, such as our net zero energy renovated building in Washington, D.C. and our Ethics and Equity Center, which fosters a diverse and inclusive geoscience community to ensure responsible conduct.</em></p>
<p>#</p>
<p>Contributed by Gabriella Lewis</p>
</div>

</div>
			
					</article>
	
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta and TikTok are obstructing researchers' access to data, EU commission rules (169 pts)]]></title>
            <link>https://www.science.org/content/article/meta-and-tiktok-are-obstructing-researchers-access-data-european-commission-rules</link>
            <guid>45754165</guid>
            <pubDate>Wed, 29 Oct 2025 22:54:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/content/article/meta-and-tiktok-are-obstructing-researchers-access-data-european-commission-rules">https://www.science.org/content/article/meta-and-tiktok-are-obstructing-researchers-access-data-european-commission-rules</a>, See on <a href="https://news.ycombinator.com/item?id=45754165">Hacker News</a></p>
Couldn't get https://www.science.org/content/article/meta-and-tiktok-are-obstructing-researchers-access-data-european-commission-rules: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Responses from LLMs are not facts (217 pts)]]></title>
            <link>https://stopcitingai.com/</link>
            <guid>45753422</guid>
            <pubDate>Wed, 29 Oct 2025 21:40:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stopcitingai.com/">https://stopcitingai.com/</a>, See on <a href="https://news.ycombinator.com/item?id=45753422">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
        <h2>Responses from Large Language Models like ChatGPT, Claude, or Gemini are not facts.</h2>
        <p>
        They’re predicting what words are most likely to come next in a sequence.
        </p>
        <p>
        They can produce convincing-sounding information, but that information may not be accurate or reliable.
        </p>
      </article><article>
        <h2>Imagine someone who has read thousands of books, but doesn’t remember where they read what.</h2>
        <hr>
        <p>
          What kinds of things might they be <mark>good</mark> at?
        </p>
        
        
        
        
        <hr>
        <p>
          What kinds of things might they be <mark>bad</mark> at?
        </p>
        
        
        
        
        <hr>
        <p>
          Sure, you <em>might</em> get an answer that’s right or advice that's good…
          but what “books” are it “remembering” when it gives that answer?
          That answer or advice is a common combination of words, not a fact.
        </p>
      </article><article>
        <h2>Don’t copy-paste something that a chatbot said and send it to someone as if that’s authoritative.</h2>
        <p>When you do that, you’re basically saying “here are a bunch of words that often go together in a sentence.”</p>
        <p>Sometimes that can be helpful or insightful. But it’s not a <strong>truth</strong>, and it’s certainly not the final say in a matter.</p>
      </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Uv is the best thing to happen to the Python ecosystem in a decade (1671 pts)]]></title>
            <link>https://emily.space/posts/251023-uv</link>
            <guid>45751400</guid>
            <pubDate>Wed, 29 Oct 2025 18:57:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://emily.space/posts/251023-uv">https://emily.space/posts/251023-uv</a>, See on <a href="https://news.ycombinator.com/item?id=45751400">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><!--[!--><!----><article><!--[--><!--]--> <div><h2>uv is the best thing to happen to the Python ecosystem in a decade</h2> <p><!--[--><!--[!--><!--]--> <a href="https://emily.space/blog/programming">Programming</a><!--]--></p> <p>23 October 2025 | Reading time: 6 minutes</p></div> <p>It’s 2025. Does installing Python, managing virtual environments, and synchronizing dependencies between your colleagues really have to be so difficult? Well… no! A <strong>brilliant</strong> new tool called <a href="https://docs.astral.sh/uv/" rel="nofollow">uv</a> came out recently that <strong>revolutionizes</strong> how easy installing and using Python can be.</p> <p>uv is a free, open-source tool built by Astral, a small startup that has been churning out Python tools (like the excellent linter <a href="https://docs.astral.sh/ruff/" rel="nofollow">Ruff</a>) for the past few years. uv can:</p> <ul><li>Install any Python version for you</li> <li>Install packages</li> <li>Manage virtual environments</li> <li>Solve dependency conflicts <em>extremely</em> quickly (<strong>very</strong> important for big projects.)</li></ul> <p>What’s best is that it can do all of the above <strong>better than any other tool</strong>, in my opinion. It’s <strong>shockingly fast</strong>, written in Rust, and works on almost any operating system or platform.</p> <h2>Installing uv</h2> <p>uv is <a href="https://docs.astral.sh/uv/#installation" rel="nofollow">straightforward to install</a>. There are a few ways, but the easiest (in my opinion) is this one-liner command — for Linux and Mac, it’s:</p> <pre><!----><code><span>curl</span> <span>-LsSf</span> https://astral.sh/uv/install.sh <span>|</span> <span>sh</span></code><!----></pre> <p>or on Windows in powershell:</p> <pre><!----><code>powershell <span>-ExecutionPolicy</span> ByPass <span>-c</span> <span>"irm https://astral.sh/uv/install.ps1 | iex"</span></code><!----></pre> <p>You can then access uv with the command <code>uv</code>. <strong>Installing uv will not mess up any of your existing Python installations</strong> — it’s a separate tool, so it’s safe to install it just to try it out.</p> <h2>Managing Python for a project</h2> <p>It’s always a good idea to work with <a href="https://stackoverflow.com/questions/41972261/what-is-a-virtualenv-and-why-should-i-use-one" rel="nofollow">virtual environments</a> for any Python project. It keeps different bits of code and dependencies ringfenced from one another, and in my experience, it can save <strong>a lot</strong> of hassle to get into the habit of using virtual environments as soon as you can. uv naturally uses virtual environments, so it’s very easy to start using them if you get into using uv.</p> <p>uv will build a Python environment for you based on what’s specified in a <code>pyproject.toml</code> file in the directory (or parent directories) you’re working in. <code>pyproject.toml</code> files are a <a href="https://stackoverflow.com/questions/62983756/what-is-pyproject-toml-file-for" rel="nofollow">standard, modern format</a> for specifying dependencies for a Python project. A barebones one might look a bit like this:</p> <pre><!----><code><span>[</span><span>project</span><span>]</span>
<span>name</span> <span>=</span> <span>"my_project"</span>
<span>version</span> <span>=</span> <span>"1.0.0"</span>
<span>requires-python</span> <span>=</span> <span>"&gt;=3.9,&lt;3.13"</span>
<span>dependencies</span> <span>=</span> <span>[</span>
  <span>"astropy&gt;=5.0.0"</span><span>,</span>
  <span>"pandas&gt;=1.0.0,&lt;2.0"</span><span>,</span>
<span>]</span></code><!----></pre> <p>In essence, it just has to specify <strong>which Python version to use</strong> and <strong>some dependencies.</strong> Adding a name and version number also aren’t a bad idea.</p> <p>(Sidenote: for projects that you publish as packages, such as to the Python Package Index that pip and uv use, <code>pyproject.toml</code> files are a modern way to specify <a href="https://packaging.python.org/en/latest/guides/writing-pyproject-toml/" rel="nofollow">everything you need</a> to publish your package.)</p> <h2>Making a new project with uv</h2> <p>To start a new Python project with uv, you can run</p> <pre><!----><code>uv init</code><!----></pre> <p>Which will create a new project for you, with a <code>pyproject.toml</code>, a <code>README.md</code>, and other important bits of boilerplate.</p> <p>There are a lot of different ways to run this command, like <code>uv init --bare</code> (which only creates a pyproject.toml), <code>uv init --package</code> (which sets up a new Python package), and more. I recommend running <code>uv init --help</code> to read about them.</p> <h2>Once you have/if you already have a <code>pyproject.toml</code> file</h2> <p>Once you initialize a project — or if you already have a <code>pyproject.toml</code> file in your project — it’s very easy to start using uv. You just need to do</p> <pre><!----><code>uv <span>sync</span></code><!----></pre> <p>in the directory that your <code>pyproject.toml</code> file is in. This command (and in fact, most uv commands if you haven’t ran it already) will:</p> <ol><li>Automatically install a valid version of Python</li> <li>Install all dependencies to a new virtual environment in the directory <code>.venv</code></li> <li>Create a <code>uv.lock</code> file in your directory, which saves the <strong>exact, platform-agnostic</strong> version of <strong>every</strong> package installed — meaning that other colleagues can replicate your Python environment exactly.</li></ol> <p>In principle, you can ‘activate’ this new virtual environment like any typical virtual environment that you may have seen in other tools, but the most ‘uv-onic’ way to use uv is simply to prepend any command with <code>uv run</code>. This command automatically picks up the correct virtual environment for you and runs your command with it. For instance, to run a script — instead of</p> <pre><!----><code><span>source</span> .venv/bin/activate
python myscript.py</code><!----></pre> <p>you can just do</p> <pre><!----><code>uv run myscript.py</code><!----></pre> <p>which will have the same effect. Likewise, to use a ‘tool’ like Jupyter Lab, you can just do</p> <pre><!----><code>uv run jupyter lab</code><!----></pre> <p>in your project’s directory, as opposed to first ‘activating’ the environment and then running <code>jupyter lab</code> separately.</p> <h2>Adding dependencies</h2> <p>You can always just edit your <code>pyproject.toml</code> file manually: uv will detect the changes and rebuild your project’s virtual environment. But uv also has easier ways to add dependencies — you can just do</p> <pre><!----><code>uv <span>add</span> numpy<span>&gt;=</span><span>2.0</span></code><!----></pre> <p>to add a package, including specifying version constraints (like the above.) This command automatically edits your <code>pyproject.toml</code> for you. <code>uv add</code> is also extremely powerful for adding remote dependencies from git or elsewhere on your computer (but I won’t get into that here.)</p> <h2>Pinning a Python version</h2> <p>Finally, I think that one of the most useful things uv can do is to pin a specific Python version for your project. Doing</p> <pre><!----><code>uv python pin <span>3.12</span>.9</code><!----></pre> <p>would pin the current project to <strong>exactly</strong> Python 3.12.9 for you, and anyone else using uv — meaning that you really can replicate the <strong>exact</strong> same Python install across multiple machines.</p> <h2>uvx: ignore all of the above and just run a tool, now!</h2> <p>But sometimes, you might just want to run a tool quickly — like using Ruff to lint code somewhere, or starting a Jupyter notebook server without an environment, or even just quickly starting an IPython session with pandas installed so you can open up a file. The <code>uv tool</code> command, which has a short alias <code>uvx</code>, makes this insanely easy. Running a command like</p> <pre><!----><code>uvx ruff</code><!----></pre> <p>will automatically download the tool you want to use and run it in a one-off virtual environment. Once the tool has been downloaded before, this is lightning-fast because of how uv uses caches.</p> <p>There are a lot of occasions when I might want to do this — a common one might be to quickly start an IPython session with pandas installed (using <code>--with</code> to add dependencies) so that I can quickly open &amp; look at a parquet file. For instance:</p> <pre><!----><code>uvx <span>--with</span> pandas,pyarrow ipython</code><!----></pre> <p>Or, maybe just starting a Jupyter Lab server so that I can quickly open a Jupyter notebook that a student sent me:</p> <pre><!----><code>uvx jupyter lab</code><!----></pre> <p>Or honestly just so many other weird, one-off use cases where <code>uvx</code> is really nice to have around. <strong>I don’t feel like I’m missing out by always using virtual environments</strong>, because <code>uvx</code> always gives you a ‘get out of jail free’ card whenever you need it.</p> <h2>If that hasn’t sold you: a personal note</h2> <p>I first discovered uv last year, while working together with our other lovely developers on building <a href="https://astrosky.eco/" rel="nofollow">The Astrosky Ecosystem</a> — a wonderful project to build open-source social media integrations for astronomers online. But with multiple developers all working asynchronously on multiple operating systems, managing Python installations quickly became a huge task.</p> <p>uv is an <strong>incredibly powerful simplification</strong> for us that we use across our entire tech stack. As developers, we can all work with identical Python installations, which is especially important given a number of semi-experimental dependencies that we use that have breaking changes with every version. On GitHub Actions, we’re planning to use uv to quickly build a Python environment and run our unit tests. In production, uv already manages Python for all of our servers.</p> <p>It’s just <em>so nice</em> to always know that Python and package installation will <strong>always</strong> be handled <strong>consistently and correctly</strong> across all of our machines. <strong>That’s why uv is the best thing to happen to the Python ecosystem in a decade.</strong></p> <h2>Find out more</h2> <p>There’s a lot more on <a href="https://docs.astral.sh/uv/" rel="nofollow">the uv docs</a>, including a <a href="https://docs.astral.sh/uv/getting-started/" rel="nofollow">getting started page</a>, more <a href="https://docs.astral.sh/uv/guides/" rel="nofollow">in-depth guides</a>, <a href="https://docs.astral.sh/uv/concepts/" rel="nofollow">explanations of important concepts</a>, and <a href="https://docs.astral.sh/uv/reference/" rel="nofollow">a full command reference</a>.</p><!----></article><!----><!--]--><!----></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Extropic is building thermodynamic computing hardware (123 pts)]]></title>
            <link>https://extropic.ai/</link>
            <guid>45750995</guid>
            <pubDate>Wed, 29 Oct 2025 18:25:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://extropic.ai/">https://extropic.ai/</a>, See on <a href="https://news.ycombinator.com/item?id=45750995">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content"><header></header><main><div data-sanity="id=7271adf3-ac3a-4622-a674-7cdec3c96b6e;type=page;path=pageBuilder;base=http%3A%2F%2Flocalhost%3A3333" id="home"><div data-sanity="id=7271adf3-ac3a-4622-a674-7cdec3c96b6e;type=page;path=pageBuilder:ifuyPKtWdk0v;base=http%3A%2F%2Flocalhost%3A3333" data-block-type="heroBlock"><p><h2><span><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></span> <!-- -->Computing</h2></p><p>Extropic is building thermodynamic computing hardware that is radically more energy efficient than GPUs.</p></div><div data-sanity="id=7271adf3-ac3a-4622-a674-7cdec3c96b6e;type=page;path=pageBuilder:tFMzoeT8oW5h;base=http%3A%2F%2Flocalhost%3A3333" data-block-type="introSlideshowBlock"><p><h2>Probabilistic software, meet probabilistic hardware.</h2></p><p>Our thermodynamic sampling units (TSUs) are inherently probabilistic,&nbsp;the perfect fit for probabilistic AI workloads.</p></div><section data-sanity="id=7271adf3-ac3a-4622-a674-7cdec3c96b6e;type=page;path=pageBuilder:k6BLgAS2tfUm;base=http%3A%2F%2Flocalhost%3A3333" data-block-type="mediaAndTextBlock"><p><img alt="" loading="lazy" sizes="100vw" srcset="https://cdn.sanity.io/images/otrk6k1t/production/e96c3fd186143852cb6a892db80327abb299860c-2904x1804.webp?auto=format&amp;fit=max&amp;q=100&amp;w=435 435w, https://cdn.sanity.io/images/otrk6k1t/production/e96c3fd186143852cb6a892db80327abb299860c-2904x1804.webp?auto=format&amp;fit=max&amp;q=100&amp;w=870 870w, https://cdn.sanity.io/images/otrk6k1t/production/e96c3fd186143852cb6a892db80327abb299860c-2904x1804.webp?auto=format&amp;fit=max&amp;q=100&amp;w=1304 1304w, https://cdn.sanity.io/images/otrk6k1t/production/e96c3fd186143852cb6a892db80327abb299860c-2904x1804.webp?auto=format&amp;fit=max&amp;q=100&amp;w=1739 1739w, https://cdn.sanity.io/images/otrk6k1t/production/e96c3fd186143852cb6a892db80327abb299860c-2904x1804.webp?auto=format&amp;fit=max&amp;q=100&amp;w=2174 2174w, https://cdn.sanity.io/images/otrk6k1t/production/e96c3fd186143852cb6a892db80327abb299860c-2904x1804.webp?auto=format&amp;fit=max&amp;q=100&amp;w=2609 2609w, https://cdn.sanity.io/images/otrk6k1t/production/e96c3fd186143852cb6a892db80327abb299860c-2904x1804.webp?auto=format&amp;fit=max&amp;q=100&amp;w=2904 2904w" src="https://cdn.sanity.io/images/otrk6k1t/production/e96c3fd186143852cb6a892db80327abb299860c-2904x1804.webp?auto=format&amp;fit=max&amp;q=100&amp;w=1739" width="1739" height="1080"></p><div><p>Software</p><h2>INTRODUCING <span>THRML</span></h2><div><p>Our open-source Python library that enables everyone to&nbsp;develop thermodynamic algorithms and simulate running them on TSUs</p></div></div></section><section data-sanity="id=7271adf3-ac3a-4622-a674-7cdec3c96b6e;type=page;path=pageBuilder:805c74f46afdb6a224af2731cef109b3;base=http%3A%2F%2Flocalhost%3A3333" data-block-type="thermodynamicsExplainerBlock"><img alt="Background" loading="lazy" decoding="async" data-nimg="fill" src="https://extropic.ai/_next/static/media/background.913b9d30.webp"></section><div data-sanity="id=7271adf3-ac3a-4622-a674-7cdec3c96b6e;type=page;path=pageBuilder:NhjiHwiaCNvj;base=http%3A%2F%2Flocalhost%3A3333" data-block-type="mediaCarouselBlock"><p><h2><span>Media</span></h2></p></div><div data-sanity="id=7271adf3-ac3a-4622-a674-7cdec3c96b6e;type=page;path=pageBuilder:c6HRmEIUF8zy;base=http%3A%2F%2Flocalhost%3A3333" data-block-type="mediaCarouselBlock"><p><h2><span>Writing</span></h2></p></div><div data-sanity="id=7271adf3-ac3a-4622-a674-7cdec3c96b6e;type=page;path=pageBuilder:aCBWSUHT0wOo;base=http%3A%2F%2Flocalhost%3A3333" data-block-type="callToActionBlock"><p>We are hiring engineers and scientists to help us pioneer a new form of computing.</p></div></div></main><div><p><img alt="Symbols Background" loading="lazy" decoding="async" data-nimg="fill" src="https://extropic.ai/_next/static/media/fallback-bg.4a136f71.webp"></p></div><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dithering – Part 1 (300 pts)]]></title>
            <link>https://visualrambling.space/dithering-part-1/</link>
            <guid>45750954</guid>
            <pubDate>Wed, 29 Oct 2025 18:21:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://visualrambling.space/dithering-part-1/">https://visualrambling.space/dithering-part-1/</a>, See on <a href="https://news.ycombinator.com/item?id=45750954">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>Loading assets, please wait...</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Internet runs on free and open source software and so does the DNS (156 pts)]]></title>
            <link>https://www.icann.org/en/blogs/details/the-internet-runs-on-free-and-open-source-softwareand-so-does-the-dns-23-10-2025-en</link>
            <guid>45750875</guid>
            <pubDate>Wed, 29 Oct 2025 18:16:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.icann.org/en/blogs/details/the-internet-runs-on-free-and-open-source-softwareand-so-does-the-dns-23-10-2025-en">https://www.icann.org/en/blogs/details/the-internet-runs-on-free-and-open-source-softwareand-so-does-the-dns-23-10-2025-en</a>, See on <a href="https://news.ycombinator.com/item?id=45750875">Hacker News</a></p>
<div id="readability-page-1" class="page"><div _ngcontent-ng-c1029541132="" itiimageurlhtml="" ititransformcontentlinks=""><p>Free and open-source software (FOSS) is not merely common on the Internet; it is a deeply embedded and essential foundation of the Domain Name System (DNS), the backbone of how we connect online.</p>
<p>The ICANN <a href="https://www.icann.org/en/ssac">Security and Stability Advisory Committee (SSAC)</a> is pleased to announce the publication of <a href="https://www.icann.org/en/ssac/publications/documents/sac132-executive-summary-for-the-domain-name-system-runs-on-free-and-open-source-software-foss-25-09-2025-en"><strong>SAC132</strong></a><strong>: <em>The Domain Name System Runs on Free and Open Source Software (FOSS)</em></strong>.</p>
<h3>Why This Matters Now</h3>
<p>As governments around the world explore new cybersecurity regulations, the ubiquity of FOSS in DNS operations—from domain registration to retrieval—means that policy decisions made today will have direct implications for the Internet's security and resilience tomorrow. SAC132 provides timely, nontechnical guidance to ensure that new policy and regulation serve to strengthen, rather than inadvertently weaken, this critical infrastructure.</p>
<h3>Key Insights for Policymakers</h3>
<p>SAC132 is a foundational guide designed to empower policymakers to strategically manage and sustain the FOSS ecosystem. The report provides:</p>
<ul>
<li><strong>Clear Foundations</strong> – An accessible overview of the DNS and the FOSS development model for nontechnical audiences.</li>
<li><strong>Policy Assessment</strong> – Analysis of cybersecurity regulations in the United States, United Kingdom, and European Union, with a focus on how they account for FOSS in the DNS ecosystem.</li>
<li><strong>Practical Guidance</strong> – Concrete findings and recommendations to help policymakers support and secure FOSS as a cornerstone of global connectivity.</li>
</ul>
<p>We invite all policymakers, technical experts, and stakeholders to read the full report.</p>
<h3>A Call to Engage</h3>
<p>By publishing SAC132, SSAC seeks to raise awareness of the indispensable role of FOSS in maintaining a secure, stable, and resilient Internet. We invite policymakers, technical experts, and all stakeholders to read the full report and join us in conversations about its findings.</p>
<p>You can engage with SSAC and the broader community at <a href="https://meetings.icann.org/en/meetings/icann84/"><strong>ICANN84</strong></a>, whether in Dublin or by participating remotely. Together, we can ensure that the FOSS ecosystem—and the Internet it supports—remains strong, sustainable, and open for all.</p>
<p>Finally, we thank all SSAC members and invited experts who contributed to this work, especially co-chairs <a href="https://www.icann.org/en/ssac/members?page=1&amp;ssac=3c06d027-70c7-48f4-b7f4-4e55805dca55" data-dmsid="3c06d027-70c7-48f4-b7f4-4e55805dca55">Maarten Aertsen</a> and <a href="https://www.icann.org/en/ssac/members?page=1&amp;ssac=f7490afd-0116-4a46-b724-dfc88f13671d" data-dmsid="f7490afd-0116-4a46-b724-dfc88f13671d">Barry Leiba</a>, for their leadership.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI’s promise to stay in California helped clear the path for its IPO (191 pts)]]></title>
            <link>https://www.wsj.com/tech/ai/openais-promise-to-stay-in-california-helped-clear-the-path-for-its-ipo-3af1c31c</link>
            <guid>45750425</guid>
            <pubDate>Wed, 29 Oct 2025 17:44:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/tech/ai/openais-promise-to-stay-in-california-helped-clear-the-path-for-its-ipo-3af1c31c">https://www.wsj.com/tech/ai/openais-promise-to-stay-in-california-helped-clear-the-path-for-its-ipo-3af1c31c</a>, See on <a href="https://news.ycombinator.com/item?id=45750425">Hacker News</a></p>
Couldn't get https://www.wsj.com/tech/ai/openais-promise-to-stay-in-california-helped-clear-the-path-for-its-ipo-3af1c31c: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[ICE and CBP Agents Are Scanning Faces on the Street to Verify Citizenship (337 pts)]]></title>
            <link>https://www.404media.co/ice-and-cbp-agents-are-scanning-peoples-faces-on-the-street-to-verify-citizenship/</link>
            <guid>45749781</guid>
            <pubDate>Wed, 29 Oct 2025 17:05:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/ice-and-cbp-agents-are-scanning-peoples-faces-on-the-street-to-verify-citizenship/">https://www.404media.co/ice-and-cbp-agents-are-scanning-peoples-faces-on-the-street-to-verify-citizenship/</a>, See on <a href="https://news.ycombinator.com/item?id=45749781">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
<!--kg-card-begin: html-->

<!--kg-card-end: html-->
<p>“You don’t got no ID?” a Border Patrol agent in a baseball cap, sunglasses, and neck gaiter asks a kid on a bike. The officer and three others had just stopped the two young men on their bikes during the day in what a video documenting the incident says is Chicago. One of the boys is filming the encounter on his phone. He says in the video he was born here, meaning he would be an American citizen.</p><p>When the boy says he doesn’t have ID on him, the Border Patrol officer has an alternative. He calls over to one of the other officers, “can you do facial?” The second officer then approaches the boy, gets him to turn around to face the sun, and points his own phone camera directly at him, hovering it over the boy’s face for a couple seconds. The officer then looks at his phone’s screen and asks for the boy to verify his name. The video stops.</p><div><p>💡</p><p><b><strong>Do you have any more videos of ICE or CBP using facial recognition? Do you work at those agencies or know more about Mobile Fortify? I would love to hear from you. Using a non-work device, you can message me securely on Signal at joseph.404 or send me an email at joseph@404media.co.</strong></b></p></div>
</div><div>
  <div>
    <h2>This post is for paid members only</h2>
    <p>Become a paid member for unlimited ad-free access to articles, bonus podcast content, and more.</p>
    <p><a href="https://www.404media.co/membership/">Subscribe</a>
  </p></div>
  <div>
    <h2>Sign up for free access to this post</h2>
    <p>Free members get access to posts like this one along with an email round-up of our week's stories.</p>
    <p><a href="https://www.404media.co/signup/">Subscribe</a>
  </p></div>
  <p>Already have an account? <a href="https://www.404media.co/signin/" data-portal="signin">Sign in</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Green Tea Garbage Collector (128 pts)]]></title>
            <link>https://go.dev/blog/greenteagc</link>
            <guid>45749746</guid>
            <pubDate>Wed, 29 Oct 2025 17:03:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://go.dev/blog/greenteagc">https://go.dev/blog/greenteagc</a>, See on <a href="https://news.ycombinator.com/item?id=45749746">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>Go 1.25 includes a new experimental garbage collector called Green Tea,
available by setting <code>GOEXPERIMENT=greenteagc</code> at build time.
Many workloads spend around 10% less time in the garbage collector, but some
workloads see a reduction of up to 40%!</p>
<p>It’s production-ready and already in use at Google, so we encourage you to
try it out.
We know some workloads don’t benefit as much, or even at all, so your feedback
is crucial to helping us move forward.
Based on the data we have now, we plan to make it the default in Go 1.26.</p>
<p>To report back with any problems, <a href="https://go.dev/issue/new">file a new issue</a>.</p>
<p>To report back with any successes, reply to <a href="https://go.dev/issue/73581">the existing Green Tea issue</a>.</p>
<p>What follows is a blog post based on Michael Knyszek’s GopherCon 2025 talk.
We’ll update this blog post with a link to the talk once it’s available online.</p>
<h2 id="tracing-garbage-collection">Tracing garbage collection</h2>
<p>Before we discuss Green Tea let’s get us all on the same page about garbage
collection.</p>
<h3 id="objects-and-pointers">Objects and pointers</h3>
<p>The purpose of garbage collection is to automatically reclaim and reuse memory
no longer used by the program.</p>
<p>To this end, the Go garbage collector concerns itself with <em>objects</em> and
<em>pointers</em>.</p>
<p>In the context of the Go runtime, <em>objects</em> are Go values whose underlying
memory is allocated from the heap.
Heap objects are created when the Go compiler can’t figure out how else to allocate
memory for a value.
For example, the following code snippet allocates a single heap object: the backing
store for a slice of pointers.</p>
<pre><code>var x = make([]*int, 10) // global
</code></pre>
<p>The Go compiler can’t allocate the slice backing store anywhere except the heap,
since it’s very hard, and maybe even impossible, for it to know how long <code>x</code> will
refer to the object for.</p>
<p><em>Pointers</em> are just numbers that indicate the location of a Go value in memory,
and they’re how a Go program references objects.
For example, to get the pointer to the beginning of the object allocated in the
last code snippet, we can write:</p>
<pre><code>&amp;x[0] // 0xc000104000
</code></pre>
<h3 id="the-mark-sweep-algorithm">The mark-sweep algorithm</h3>
<p>Go’s garbage collector follows a strategy broadly referred to as <em>tracing garbage
collection</em>, which just means that the garbage collector follows, or traces, the
pointers in the program to identify which objects the program is still using.</p>
<p>More specifically, the Go garbage collector implements the mark-sweep algorithm.
This is much simpler than it sounds.
Imagine objects and pointers as a sort of graph, in the computer science sense.
Objects are nodes, pointers are edges.</p>
<p>The mark-sweep algorithm operates on this graph, and as the name might suggest,
proceeds in two phases.</p>
<p>In the first phase, the mark phase, it walks the object graph from well-defined
source edges called <em>roots</em>.
Think global and local variables.
Then, it <em>marks</em> everything it finds along the way as <em>visited</em>, to avoid going in
circles.
This is analogous to your typical graph flood algorithm, like a depth-first or
breadth-first search.</p>
<p>Next is the sweep phase.
Whatever objects were not visited in our graph walk are unused, or <em>unreachable</em>,
by the program.
We call this state unreachable because it is impossible with normal safe Go code
to access that memory anymore, simply through the semantics of the language.
To complete the sweep phase, the algorithm simply iterates through all the
unvisited nodes and marks their memory as free, so the memory allocator can reuse
it.</p>
<h3 id="thats-it">That’s it?</h3>
<p>You may think I’m oversimplifying a bit here.
Garbage collectors are frequently referred to as <em>magic</em>, and <em>black boxes</em>.
And you’d be partially right, there are more complexities.</p>
<p>For example, this algorithm is, in practice, executed concurrently with your
regular Go code.
Walking a graph that’s mutating underneath you brings challenges.
We also parallelize this algorithm, which is a detail that’ll come up again
later.</p>
<p>But trust me when I tell you that these details are mostly separate from the
core algorithm.
It really is just a simple graph flood at the center.</p>
<h3 id="graph-flood-example">Graph flood example</h3>
<p>Let’s walk through an example.
Navigate through the slideshow below to follow along.</p>

<div id="marksweep">
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-007.png">
        <figcaption>
        Here we have a diagram of some global variables and Go heap.
        Let's break it down, piece by piece.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-008.png">
        <figcaption>
        On the left here we have our roots.
        These are global variables x and y.
        They will be the starting point of our graph walk.
        Since they're marked blue, according to our handy legend in the bottom left, they're currently on our work list.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-009.png">
        <figcaption>
        On the right side, we have our heap.
        Currently, everything in our heap is grayed out because we haven't visited any of it yet.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-010.png">
        <figcaption>
        Each one of these rectangles represents an object.
        Each object is labeled with its type.
        This object in particular is an object of type T, whose type definition is on the top left.
        It's got a pointer to an array of children, and some value.
        We can surmise that this is some kind of recursive tree data structure.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-011.png">
        <figcaption>
        In addition to the objects of type T, you'll also notice that we have array objects containing *Ts.
        These are pointed to by the "children" field of objects of type T.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-012.png">
        <figcaption>
        Each square inside of the rectangle represents 8 bytes of memory.
        A square with a dot is a pointer.
        If it has an arrow, it is a non-nil pointer pointing to some other object.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-013.png">
        <figcaption>
        And if it doesn't have a corresponding arrow, then it's a nil pointer.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-014.png">
        <figcaption>
        Next, these dotted rectangles represents free space, what I'll call a free "slot." We could put an object there, but there currently isn't one.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-015.png">
        <figcaption>
        You'll also notice that objects are grouped together by these labeled, dotted rounded rectangles.
        Each of these represents a page: a contiguous block of memory.
        These pages are labeled A, B, C, and D, and I'll refer to them that way.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-015.png">
        <figcaption>
        In this diagram, each object is allocated as part of some page.
        Like in the real implementation, each page here only contains objects of a certain size.
        This is just how the Go heap is organized.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-016.png">
        <figcaption>
        Pages are also how we organize per-object metadata.
        Here you can see seven boxes, each corresponding to one of the seven object slots in page A.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-016.png">
        <figcaption>
        Each box represents one bit of information: whether or not we have seen the object before.
        This is actually how the real runtime manages whether an object has been visited, and it'll be an important detail later.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-017.png">
        <figcaption>
        That was a lot of detail, so thanks for reading along.
        This will all come into play later.
        For now, let's just see how our graph flood applies to this picture.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-018.png">
        <figcaption>
        We start by taking a root off of the work list.
        We mark it red to indicate that it's now active.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-019.png">
        <figcaption>
        Following that root's pointer, we find an object of type T, which we add to our work list.
        Following our legend, we draw the object in blue to indicate that it's on our work list.
        Note also that we set the seen bit corresponding to this object in our metadata.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-020.png">
        <figcaption>
        Same goes for the next root.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-021.png">
        <figcaption>
        Now that we've taken care of all the roots, we're left with two objects on our work list.
        Let's take an object off the work list.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-022.png">
        <figcaption>
        What we're going to do now is walk the pointers of the objects, to find more objects.
        By the way, we call walking the pointers of an object "scanning" the object.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-023.png">
        <figcaption>
        We find this valid array object…
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-024.png">
        <figcaption>
        … and add it to our work list.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-025.png">
        <figcaption>
        From here, we proceed recursively.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-026.png">
        <figcaption>
        We walk the array's pointers.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-027.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-028.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-029.png">
        <figcaption>
        Find some more objects…
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-030.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-031.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-032.png">
        <figcaption>
        Then we walk the objects that the array object referred to!
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-033.png">
        <figcaption>
        And note that we still have to walk over all pointers, even if they're nil.
        We don't know ahead of time if they will be.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-034.png">
        <figcaption>
        One more object down this branch…
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-035.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-036.png">
        <figcaption>
        And now we've reached the other branch, starting from that object in page A we found much earlier from one of the roots.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-036.png">
        <figcaption>
        You may be noticing a last-in-first-out discipline for our work list here, indicating that our work list is a stack, and hence our graph flood is approximately depth-first.
        This is intentional, and reflects the actual graph flood algorithm in the Go runtime.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-037.png">
        <figcaption>
        Let's keep going…
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-038.png">
        <figcaption>
        Next we find another array object…
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-039.png">
        <figcaption>
        And walk it…
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-040.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-041.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-042.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-043.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-044.png">
        <figcaption>
        Just one more object left on our work list…
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-045.png">
        <figcaption>
        Let's scan it…
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-046.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-047.png">
        <figcaption>
        And we're done with the mark phase! There's nothing we're actively working on and there's nothing left on our work list.
        Every object drawn in black is reachable, and every object drawn in gray is unreachable.
        Let's sweep the unreachable objects, all in one go.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/marksweep-048.png">
        <figcaption>
        We've converted those objects into free slots, ready to hold new objects.
        </figcaption>
    </figure>
</div>
<h2 id="the-problem">The problem</h2>
<p>After all that, I think we have a handle on what the Go garbage collector is actually doing.
This process seems to work well enough today, so what’s the problem?</p>
<p>Well, it turns out we can spend <em>a lot</em> of time executing this particular algorithm in some
programs, and it adds substantial overhead to nearly every Go program.
It’s not that uncommon to see Go programs spending 20% or more of their CPU time in the
garbage collector.</p>
<p>Let’s break down where that time is being spent.</p>
<h3 id="garbage-collection-costs">Garbage collection costs</h3>
<p>At a high level, there are two parts to the cost of the garbage collector.
The first is how often it runs, and the second is how much work it does each time it runs.
Multiply those two together, and you get the total cost of the garbage collector.</p>
<figure>
    <figcaption>
    Total GC cost = Number of GC cycles × Average cost per GC cycle
    </figcaption>
</figure>
<p>Over the years we’ve tackled both terms in this equation, and for more on <em>how often</em> the garbage
collector runs, see <a href="https://www.youtube.com/watch?v=07wduWyWx8M" rel="noreferrer" target="_blank">Michael’s GopherCon EU talk from 2022</a>
about memory limits.
<a href="https://go.dev/doc/gc-guide">The guide to the Go garbage collector</a> also has a lot to say about this topic,
and is worth a look if you want to dive deeper.</p>
<p>But for now let’s focus only on the second part, the cost per cycle.</p>
<p>From years of poring over CPU profiles to try to improve performance, we know two big things
about Go’s garbage collector.</p>
<p>The first is that about 90% of the cost of the garbage collector is spent marking,
and only about 10% is sweeping.
Sweeping turns out to be much easier to optimize than marking,
and Go has had a very efficient sweeper for many years.</p>
<p>The second is that, of that time spent marking, a substantial portion, usually at least 35%, is
simply spent <em>stalled</em> on accessing heap memory.
This is bad enough on its own, but it completely gums up the works on what makes modern CPUs
actually fast.</p>
<h3 id="a-microarchitectural-disaster">“A microarchitectural disaster”</h3>
<p>What does “gum up the works” mean in this context?
The specifics of modern CPUs can get pretty complicated, so let’s use an analogy.</p>
<p>Imagine the CPU driving down a road, where that road is your program.
The CPU wants to ramp up to a high speed, and to do that it needs to be able to see far ahead of it,
and the way needs to be clear.
But the graph flood algorithm is like driving through city streets for the CPU.
The CPU can’t see around corners and it can’t predict what’s going to happen next.
To make progress, it constantly has to slow down to make turns, stop at traffic lights, and avoid
pedestrians.
It hardly matters how fast your engine is because you never get a chance to get going.</p>
<p>Let’s make that more concrete by looking at our example again.
I’ve overlaid the heap here with the path that we took.
Each left-to-right arrow represents a piece of scanning work that we did
and the dashed arrows show how we jumped around between bits of scanning work.</p>
<figure>
    <img src="https://go.dev/blog/greenteagc/graphflood-path.png">
    <figcaption>
    The path through the heap the garbage collector took in our graph flood example.
    </figcaption>
</figure>
<p>Notice that we were jumping all over memory doing tiny bits of work in each place.
In particular, we’re frequently jumping between pages, and between different parts of pages.</p>
<p>Modern CPUs do a lot of caching.
Going to main memory can be up to 100x slower than accessing memory that’s in our cache.
CPU caches are populated with memory that’s been recently accessed, and memory that’s nearby to
recently accessed memory.
But there’s no guarantee that any two objects that point to each other will <em>also</em> be close to each
other in memory.
The graph flood doesn’t take this into account.</p>
<p>Quick side note: if we were just stalling fetches to main memory, it might not be so bad.
CPUs issue memory requests asynchronously, so even slow ones could overlap if the CPU could see
far enough ahead.
But in the graph flood, every bit of work is small, unpredictable, and highly dependent on the
last, so the CPU is forced to wait on nearly every individual memory fetch.</p>
<p>And unfortunately for us, this problem is only getting worse.
There’s an adage in the industry of “wait two years and your code will get faster.”</p>
<p>But Go, as a garbage collected language that relies on the mark-sweep algorithm, risks the opposite.
“Wait two years and your code will get slower.”
The trends in modern CPU hardware are creating new challenges for garbage collector performance:</p>
<p><strong>Non-uniform memory access.</strong>
For one, memory now tends to be associated with subsets of CPU cores.
Accesses by <em>other</em> CPU cores to that memory are slower than before.
In other words, the cost of a main memory access <a href="https://jprahman.substack.com/p/sapphire-rapids-core-to-core-latency" rel="noreferrer" target="_blank">depends on which CPU core is accessing
it</a>.
It’s non-uniform, so we call this non-uniform memory access, or NUMA for short.</p>
<p><strong>Reduced memory bandwidth.</strong>
Available memory bandwidth per CPU is trending downward over time.
This just means that while we have more CPU cores, each core can submit relatively fewer
requests to main memory, forcing non-cached requests to wait longer than before.</p>
<p><strong>Ever more CPU cores.</strong>
Above, we looked at a sequential marking algorithm, but the real garbage collector performs this
algorithm in parallel.
This scales well to a limited number of CPU cores, but the shared queue of objects to scan becomes
a bottleneck, even with careful design.</p>
<p><strong>Modern hardware features.</strong>
New hardware has fancy features like vector instructions, which let us operate on a lot of data at once.
While this has the potential for big speedups, it’s not immediately clear how to make that work for
marking because marking does so much irregular and often small pieces of work.</p>
<h2 id="green-tea">Green Tea</h2>
<p>Finally, this brings us to Green Tea, our new approach to the mark-sweep algorithm.
The key idea behind Green Tea is astonishingly simple:</p>
<p><em>Work with pages, not objects.</em></p>
<p>Sounds trivial, right?
And yet, it took a lot of work to figure out how to order the object graph walk and what we needed to
track to make this work well in practice.</p>
<p>More concretely, this means:</p>
<ul>
<li>Instead of scanning objects we scan whole pages.</li>
<li>Instead of tracking objects on our work list, we track whole pages.</li>
<li>We still need to mark objects at the end of the day, but we’ll track marked objects locally to each
page, rather than across the whole heap.</li>
</ul>
<h3 id="green-tea-example">Green Tea example</h3>
<p>Let’s see what this means in practice by looking at our example heap again, but this time
running Green Tea instead of the straightforward graph flood.</p>
<p>As above, navigate through the annotated slideshow to follow along.</p>

<div id="greentea">
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-060.png">
        <figcaption>
        This is the same heap as before, but now with two bits of metadata per object rather than one.
        Again, each bit, or box, corresponds to one of the object slots in the page.
        In total, we now have fourteen bits that correspond to the seven slots in page A.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-060.png">
        <figcaption>
        The top bits represent the same thing as before: whether or not we've seen a pointer to the object.
        I'll call these the "seen" bits.
        The bottom set of bits are new.
        These "scanned" bits track whether or not we've <i>scanned</i> the object.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-060.png">
        <figcaption>
        This new piece of metadata is necessary because, in Green tea, <b>the work list tracks pages,
        not objects</b>.
        We still need to track objects at some level, and that's the purpose of these bits.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-062.png">
        <figcaption>
        We start off the same as before, walking objects from the roots.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-063.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-064.png">
        <figcaption>
        But this time, instead of putting an object on the work list,
        we put a whole page–in this case page A–on the work list,
        indicated by shading the whole page blue.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-066.png">
        <figcaption>
        The object we found is also blue to indicate that when we do take this page off of the work list, we will need to look at that object.
        Note that the object's blue hue directly reflects the metadata in page A.
        Its corresponding seen bit is set, but its scanned bit is not.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-069.png">
        <figcaption>
        We follow the next root, find another object, and again put the whole page–page C–on the work list and set the object's seen bit.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-071.png">
        <figcaption>
        We're done following roots, so we turn to the work list and take page A off the work list.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-072.png">
        <figcaption>
        Using the seen and scanned bits, we can tell there's one object to scan on page A.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-074.png">
        <figcaption>
        We scan that object, following its pointers.
        And as a result, we add page B to the work list, since the first object in page A points to an object in page B.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-075.png">
        <figcaption>
        We're done with page A.
        Next we take page C off the work list.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-076.png">
        <figcaption>
        Similar to page A, there's a single object on page C to scan.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-078.png">
        <figcaption>
        We found a pointer to another object in page B.
        Page B is already on the work list, so we don't need to add anything to the work list.
        We simply have to set the seen bit for the target object.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-079.png">
        <figcaption>
        Now it's page B's turn.
        We've accumulated two objects to scan on page B,
        and we can process both of these objects in a row, in memory order!
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-081.png">
        <figcaption>
        We walk the pointers of the first object…
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-082.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-083.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-084.png">
        <figcaption>
        We find a pointer to an object in page A.
        Page A was previously on the work list, but isn't at this point, so we put it back on the work list.
        Unlike the original mark-sweep algorithm, where any given object is only added to the work list at
        most once per whole mark phase, in Green Tea, a given page can reappear on the work list several times
        during a mark phase.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-085.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-086.png">
        <figcaption>
        We scan the second seen object in the page immediately after the first.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-087.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-088.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-089.png">
        <figcaption>
        We find a few more objects in page A…
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-090.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-091.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-092.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-093.png">
        <figcaption>
        We're done scanning page B, so we pull page A off the work list.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-094.png">
        <figcaption>
        This time we only need to scan three objects, not four,
        since we already scanned the first object.
        We know which objects to scan by looking at the difference between the "seen" and "scanned" bits.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-095.png">
        <figcaption>
        We'll scan these objects in sequence.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-096.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-097.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-098.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-099.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-100.png">
        <figcaption>
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-101.png">
        <figcaption>
        We're done! There are no more pages on the work list and there's nothing we're actively looking at.
        Notice that the metadata now all lines up nicely, since all reachable objects were both seen and scanned.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-101.png">
        <figcaption>
        You may have also noticed during our traversal that the work list order is a little different from the graph flood.
        Where the graph flood had a last-in-first-out, or stack-like, order, here we're using a first-in-first-out, or queue-like, order for the pages on our work list.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-101.png">
        <figcaption>
        This is intentional.
        We let seen objects accumulate on each page while the page sits on the queue, so we can process as many as we can at once.
        That's how we were able to hit so many objects on page A at once.
        Sometimes laziness is a virtue.
        </figcaption>
    </figure>
    <figure>
        <img src="https://go.dev/blog/greenteagc/greentea-102.png">
        <figcaption>
        And finally we can sweep away the unvisited objects, as before.
        </figcaption>
    </figure>
</div>
<h3 id="getting-on-the-highway">Getting on the highway</h3>
<p>Let’s come back around to our driving analogy.
Are we finally getting on the highway?</p>
<p>Let’s recall our graph flood picture before.</p>
<figure>
    <img src="https://go.dev/blog/greenteagc/graphflood-path2.png">
    <figcaption>
    The path the original graph flood took through the heap required 7 separate scans.
    </figcaption>
</figure>
<p>We jumped around a whole lot, doing little bits of work in different places.
The path taken by Green Tea looks very different.</p>
<figure>
    <img src="https://go.dev/blog/greenteagc/greentea-path.png">
    <figcaption>
    The path taken by Green Tea requires only 4 scans.
    </figcaption>
</figure>
<p>Green Tea, in contrast, makes fewer, longer left-to-right passes over pages A and B.
The longer these arrows, the better, and with bigger heaps, this effect can be much stronger.
<em>That’s</em> the magic of Green Tea.</p>
<p>It’s also our opportunity to ride the highway.</p>
<p>This all adds up to a better fit with the microarchitecture.
We can now scan objects closer together with much higher probability, so
there’s a better chance we can make use of our caches and avoid main memory.
Likewise, per-page metadata is more likely to be in cache.
Tracking pages instead of objects means work lists are smaller,
and less pressure on work lists means less contention and fewer CPU stalls.</p>
<p>And speaking of the highway, we can take our metaphorical engine into gears we’ve never been able to
before, since now we can use vector hardware!</p>
<h3 id="vector-acceleration">Vector acceleration</h3>
<p>If you’re only vaguely familiar with vector hardware, you might be confused as to how we can use it here.
But besides the usual arithmetic and trigonometric operations,
recent vector hardware supports two things that are valuable for Green Tea:
very wide registers, and sophisticated bit-wise operations.</p>
<p>Most modern x86 CPUs support AVX-512, which has 512-bit wide vector registers.
This is wide enough to hold all of the metadata for an entire page in just two registers,
right on the CPU, enabling Green Tea to work on an entire page in just a few straight-line
instructions.
Vector hardware has long supported basic bit-wise operations on whole vector registers, but starting
with AMD Zen 4 and Intel Ice Lake, it also supports a new bit vector “Swiss army knife” instruction
that enables a key step of the Green Tea scanning process to be done in just a few CPU cycles.
Together, these allow us to turbo-charge the Green Tea scan loop.</p>
<p>This wasn’t even an option for the graph flood, where we’d be jumping between scanning objects that
are all sorts of different sizes.
Sometimes you needed two bits of metadata and sometimes you needed ten thousand.
There simply wasn’t enough predictability or regularity to use vector hardware.</p>
<p>If you want to nerd out on some of the details, read along!
Otherwise, feel free to skip ahead to the <a href="#evaluation">evaluation</a>.</p>
<h4 id="avx-512-scanning-kernel">AVX-512 scanning kernel</h4>
<p>To get a sense of what AVX-512 GC scanning looks like, take a look at the diagram below.</p>
<figure>
    <img src="https://go.dev/blog/greenteagc/avx512.svg">
    <figcaption>
    The AVX-512 vector kernel for scanning.
    </figcaption>
</figure>
<p>There’s a lot going on here and we could probably fill an entire blog post just on how this works.
For now, let’s just break it down at a high level:</p>
<ol>
<li>
<p>First we fetch the “seen” and “scanned” bits for a page.
Recall, these are one bit per object in the page, and all objects in a page have the same size.</p>
</li>
<li>
<p>Next, we compare the two bit sets.
Their union becomes the new “scanned” bits, while their difference is the “active objects” bitmap,
which tells us which objects we need to scan in this pass over the page (versus previous passes).</p>
</li>
<li>
<p>We take the difference of the bitmaps and “expand” it, so that instead of one bit per object,
we have one bit per word (8 bytes) of the page.
We call this the “active words” bitmap.
For example, if the page stores 6-word (48-byte) objects, each bit in the active objects bitmap
will be copied to 6 bits in the active words bitmap.
Like so:</p>
</li>
</ol>
<figure>
    <div><pre>0 0 1 1 ...</pre><p> → </p><pre>000000 000000 111111 111111 ...</pre></div>
</figure>
<ol start="4">
<li>
<p>Next we fetch the pointer/scalar bitmap for the page.
Here, too, each bit corresponds to a word (8 bytes) of the page, and it tells us whether that word
stores a pointer.
This data is managed by the memory allocator.</p>
</li>
<li>
<p>Now, we take the intersection of the pointer/scalar bitmap and the active words bitmap.
The result is the “active pointer bitmap”: a bitmap that tells us the location of every
pointer in the entire page contained in any live object we haven’t scanned yet.</p>
</li>
<li>
<p>Finally, we can iterate over the memory of the page and collect all the pointers.
Logically, we iterate over each set bit in the active pointer bitmap,
load the pointer value at that word, and write it back to a buffer that
will later be used to mark objects seen and add pages to the work list.
Using vector instructions, we’re able to do this 64 bytes at a time,
in just a couple instructions.</p>
</li>
</ol>
<p>Part of what makes this fast is the <code>VGF2P8AFFINEQB</code> instruction,
part of the “Galios Field New Instructions” x86 extension,
and the bit manipulation Swiss army knife we referred to above.
It’s the real star of the show, since it lets us do step (3) in the scanning kernel very, very
efficiently.
It performs a bit-wise <a href="https://en.wikipedia.org/wiki/Affine_transformation" rel="noreferrer" target="_blank">affine
transformations</a>,
treating each byte in a vector as itself a mathematical vector of 8 bits
and multiplying it by an 8x8 bit matrix.
This is all done over the <a href="https://en.wikipedia.org/wiki/Finite_field" rel="noreferrer" target="_blank">Galois field</a> <code>GF(2)</code>,
which just means multiplication is AND and addition is XOR.
The upshot of this is that we can define a few 8x8 bit matrices for each
object size that perform exactly the 1:n bit expansion we need.</p>
<p>For the full assembly code, see <a href="https://cs.opensource.google/go/go/+/master:src/internal/runtime/gc/scan/scan_amd64.s;l=23;drc=041f564b3e6fa3f4af13a01b94db14c1ee8a42e0" rel="noreferrer" target="_blank">this
file</a>.
The “expanders” use different matrices and different permutations for each size class,
so they’re in a <a href="https://cs.opensource.google/go/go/+/master:src/internal/runtime/gc/scan/expand_amd64.s;drc=041f564b3e6fa3f4af13a01b94db14c1ee8a42e0" rel="noreferrer" target="_blank">separate file</a>
that’s written by a <a href="https://cs.opensource.google/go/go/+/master:src/internal/runtime/gc/scan/mkasm.go;drc=041f564b3e6fa3f4af13a01b94db14c1ee8a42e0" rel="noreferrer" target="_blank">code generator</a>.
Aside from the expansion functions, it’s really not a lot of code.
Most of it is dramatically simplified by the fact that we can perform most of the above
operations on data that sits purely in registers.
And, hopefully soon this assembly code <a href="https://go.dev/issue/73787">will be replaced with Go code</a>!</p>
<p>Credit to Austin Clements for devising this process.
It’s incredibly cool, and incredibly fast!</p>
<h3 id="evaluation">Evaluation</h3>
<p>So that’s it for how it works.
How much does it actually help?</p>
<p>It can be quite a lot.
Even without the vector enhancements, we see reductions in garbage collection CPU costs
between 10% and 40% in our benchmark suite.
For example, if an application spends 10% of its time in the garbage collector, then that
would translate to between a 1% and 4% overall CPU reduction, depending on the specifics of
the workload.
A 10% reduction in garbage collection CPU time is roughly the modal improvement.
(See the <a href="https://go.dev/issue/73581">GitHub issue</a> for some of these details.)</p>
<p>We’ve rolled Green Tea out inside Google, and we see similar results at scale.</p>
<p>We’re still rolling out the vector enhancements,
but benchmarks and early results suggest this will net an additional 10% GC CPU reduction.</p>
<p>While most workloads benefit to some degree, there are some that don’t.</p>
<p>Green Tea is based on the hypothesis that we can accumulate enough objects to scan on a
single page in one pass to counteract the costs of the accumulation process.
This is clearly the case if the heap has a very regular structure: objects of the same size at a
similar depth in the object graph.
But there are some workloads that often require us to scan only a single object per page at a time.
This is potentially worse than the graph flood because we might be doing more work than before while
trying to accumulate objects on pages and failing.</p>
<p>The implementation of Green Tea has a special case for pages that have only a single object to scan.
This helps reduce regressions, but doesn’t completely eliminate them.</p>
<p>However, it takes a lot less per-page accumulation to outperform the graph flood
than you might expect.
One surprise result of this work was that scanning a mere 2% of a page at a time
can yield improvements over the graph flood.</p>
<h3 id="availability">Availability</h3>
<p>Green Tea is already available as an experiment in the recent Go 1.25 release and can be enabled
by setting the environment variable <code>GOEXPERIMENT</code> to <code>greenteagc</code> at build time.
This doesn’t include the aforementioned vector acceleration.</p>
<p>We expect to make it the default garbage collector in Go 1.26, but you’ll still be able to opt-out
with <code>GOEXPERIMENT=nogreenteagc</code> at build time.
Go 1.26 will also add vector acceleration on newer x86 hardware, and include a whole bunch of
tweaks and improvements based on feedback we’ve collected so far.</p>
<p>If you can, we encourage you to try at Go tip-of-tree!
If you prefer to use Go 1.25, we’d still love your feedback.
See <a href="https://go.dev/issue/73581#issuecomment-2847696497">this GitHub
comment</a> with some details on
what diagnostics we’d be interested in seeing, if you can share, and the preferred channels for
reporting feedback.</p>
<h2 id="the-journey">The journey</h2>
<p>Before we wrap up this blog post, let’s take a moment to talk about the journey that got us here.
The human element of the technology.</p>
<p>The core of Green Tea may seem like a single, simple idea.
Like the spark of inspiration that just one single person had.</p>
<p>But that’s not true at all.
Green Tea is the result of work and ideas from many people over several years.
Several people on the Go team contributed to the ideas, including Michael Pratt, Cherry Mui, David
Chase, and Keith Randall.
Microarchitectural insights from Yves Vandriessche, who was at Intel at the time, also really helped
direct the design exploration.
There were a lot of ideas that didn’t work, and there were a lot of details that needed figuring out.
Just to make this single, simple idea viable.</p>
<figure>
    <img src="https://go.dev/blog/greenteagc/timeline.png">
    <figcaption>
    A timeline depicting a subset of the ideas we tried in this vein before getting to
    where we are today.
    </figcaption>
</figure>
<p>The seeds of this idea go all the way back to 2018.
What’s funny is that everyone on the team thinks someone else thought of this initial idea.</p>
<p>Green Tea got its name in 2024 when Austin worked out a prototype of an earlier version while cafe
crawling in Japan and drinking LOTS of matcha!
This prototype showed that the core idea of Green Tea was viable.
And from there we were off to the races.</p>
<p>Throughout 2025, as Michael implemented and productionized Green Tea, the ideas evolved and changed even
further.</p>
<p>This took so much collaborative exploration because Green Tea is not just an algorithm, but an entire
design space.
One that we don’t think any of us could’ve navigated alone.
It’s not enough to just have the idea, but you need to figure out the details and prove it.
And now that we’ve done it, we can finally iterate.</p>
<p>The future of Green Tea is bright.</p>
<p>Once again, please try it out by setting <code>GOEXPERIMENT=greenteagc</code> and let us know how it goes!
We’re really excited about this work and want to hear from you!</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AOL to be sold to Bending Spoons for $1.5B (234 pts)]]></title>
            <link>https://www.axios.com/2025/10/29/aol-bending-spoons-deal</link>
            <guid>45749161</guid>
            <pubDate>Wed, 29 Oct 2025 16:28:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.axios.com/2025/10/29/aol-bending-spoons-deal">https://www.axios.com/2025/10/29/aol-bending-spoons-deal</a>, See on <a href="https://news.ycombinator.com/item?id=45749161">Hacker News</a></p>
Couldn't get https://www.axios.com/2025/10/29/aol-bending-spoons-deal: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Tailscale Peer Relays (307 pts)]]></title>
            <link>https://tailscale.com/blog/peer-relays-beta</link>
            <guid>45749017</guid>
            <pubDate>Wed, 29 Oct 2025 16:21:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tailscale.com/blog/peer-relays-beta">https://tailscale.com/blog/peer-relays-beta</a>, See on <a href="https://news.ycombinator.com/item?id=45749017">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><span>Today we’re excited to announce public availability of Tailscale Peer Relays, a traffic relaying alternative to Tailscale’s managed DERP servers that can be enabled on any Tailscale node.</span></p><div><p>Tailscale Peer Relays provides a customer-deployed and managed traffic relaying mechanism. By advertising itself as a peer relay, a Tailscale node can relay traffic for any peer nodes on the tailnet, even for traffic bound to itself. Tailscale Peer Relays can only relay traffic for nodes on your tailnet, and only for nodes that have access to the peer relay. Because they’re managed entirely by the customer, peer relays are less throughput-constrained than Tailscale’s managed DERP relays, and can provide higher throughput connections for traffic to and from locked-down cloud infrastructure, or behind strict network firewalls.</p><p>In testing with early design partners, we’ve seen throughputs nearing that of a direct connection; often multiple orders of magnitude higher than Tailscale’s managed DERP fleet.</p><h2 id="moving-past-hard-nat"><a href="#moving-past-hard-nat">Moving past hard NAT<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2><p>Over the past few weeks, you’ve heard us talk about <a target="" rel="noreferrer" href="https://tailscale.com/blog/nat-traversal-improvements-pt-1">improvements we’ve made to</a> Network Address Translation (NAT) traversal techniques, so that Tailscale can establish direct connections wherever possible (<em>hint: it’s over 90% of the time</em>). However, we’ve also outlined places where this isn’t possible or desirable today for a variety of reasons, <a target="" rel="noreferrer" href="https://tailscale.com/blog/nat-traversal-improvements-pt-2-cloud-environments">especially in cloud environments</a>. And, we’ve postulated a bit about <a target="" rel="noreferrer" href="https://tailscale.com/blog/nat-traversal-improvements-pt3-looking-ahead">where we think the industry is going</a>.</p><p>While we’ve been keeping your network reliably connected for years with <a target="" rel="noreferrer" href="https://tailscale.com/kb/1232/derp-servers">DERP</a>, we’ve heard from customers that the throughput and performance aspects of a QoS-aware managed relay fleet makes deployments in certain environments difficult or untenable. Furthermore, customers have noted that it’s non-trivial to deploy and manage <a target="" rel="noreferrer" href="https://tailscale.com/kb/1232/derp-servers#custom-derp-servers">custom DERP fleets</a> (which run as a separate service and binary).</p><p>DERP provides an incredibly valuable service, setting up reliable connections between Tailscale clients anywhere in the world (including negotiating connections through peer relays). But often, DERP’s focus as a reliability and NAT traversal tool results in performance tradeoffs.</p><p>By contrast, Tailscale Peer Relays is designed as a performant connectivity tool, and can perform at a level rivaling direct connections. Peer relays can be placed right next to the resources they serve, and peer relays also run on top of UDP, both characteristics beneficial to lower latency and resource overhead. And, they are built into the Tailscale client itself for ease of deployment.</p><p>We want to move past even more hard NATs, and put Tailscale’s relaying technology in our customers’ hands, so they can use Tailscale at scale, anywhere, with ease. We believe our new Tailscale Peer Relays connectivity option—unique to Tailscale—gives customers the best performance and flexibility.</p><h2 id="how-it-works"><a href="#how-it-works">How it works<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2><p>Peer relays are configured with a single UDP port that must be available to both sides of a connection. Tailscale Peer Relays is built right into the Tailscale client, and can be enabled with a simple command, using the <code>tailscale set --relay-server-port</code> flag from the Tailscale CLI. Once enabled via <a target="" rel="noreferrer" href="https://tailscale.com/kb/1591/peer-relays?utm_source=blog&amp;utm_medium=content&amp;utm_campaign=fall-update-2025">the steps in our documentation</a>, clients can connect to infrastructure in hard NAT environments over the peer relay.</p><p><strong>And don’t worry, we still prefer to fly direct.</strong> Tailscale prefers direct connections wherever possible. Clients can then fall back to available peer relays, and finally leverage Tailscale’s managed DERP fleet, or any customer-deployed custom DERPs, to ensure you have connectivity wherever you need it. All of this traffic, over any connection, is still end-to-end encrypted via WireGuard®.</p><p>Tailscale Peer Relays is designed for the real world, based on the feedback we’ve received from customers and our own hard-earned networking expertise. It allows customers to make just one firewall exception for connections only coming from their tailnet. Peer relays scale across regions, are resilient to real-world network conditions, and graciously fall back to DERP (Tailscale’s or custom). Your network maintains its shape, but gains all kinds of flexibility.</p><h2 id="connectivity-everywhere-at-warp-speed"><a href="#connectivity-everywhere-at-warp-speed">Connectivity, everywhere, at warp speed<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2><p>Customers can now maintain performance benchmarks even where direct connections aren’t possible, by enabling Tailscale Peer Relays to build a deterministic and high-throughput relay topology.</p><figure id=""><img _type="asset" video="[object Object]" alt="Green checked background. On top, a left node, &quot;Your Network,&quot; with &quot;Your Resource&quot; (database) pointing into &quot;Peer relay,&quot; then into a checkmark box on the &quot;Network Firewall,&quot; with &quot;Customer Network&quot; inside, and &quot;Resource&quot; (database) inside that." loading="lazy" width="1152" height="688" decoding="async" data-nimg="1" srcset="https://cdn.sanity.io/images/w77i7m8x/production/d160d9fa1e9477445b543c120b310e5086325245-1152x688.svg?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1x, https://cdn.sanity.io/images/w77i7m8x/production/d160d9fa1e9477445b543c120b310e5086325245-1152x688.svg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 2x" src="https://cdn.sanity.io/images/w77i7m8x/production/d160d9fa1e9477445b543c120b310e5086325245-1152x688.svg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format"></figure><p>We’ve had customers use peer relays to provide access into unmanaged networks, allowing their partners or customers to provide a controllable and auditable connectivity path without sacrificing performance.</p><figure id=""><img _type="asset" video="[object Object]" alt="A node, User, into a block in a Network Firewall square, &quot;Peer relay ip:port exception,&quot; into &quot;Peer relay,&quot; and then to &quot;Customer Network,&quot; with three &quot;Resource&quot; (database) nodes inside." loading="lazy" width="1152" height="688" decoding="async" data-nimg="1" srcset="https://cdn.sanity.io/images/w77i7m8x/production/8c2d19093b0e49986ad91123e2178ef49f8df4b1-1152x688.svg?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1x, https://cdn.sanity.io/images/w77i7m8x/production/8c2d19093b0e49986ad91123e2178ef49f8df4b1-1152x688.svg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 2x" src="https://cdn.sanity.io/images/w77i7m8x/production/8c2d19093b0e49986ad91123e2178ef49f8df4b1-1152x688.svg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format"></figure><p>In strict private networks, customers can build predictable access paths. Tailscale Peer Relays can be placed in public subnets with VPC peering to private subnets, allowing security teams to efficiently segment their private network infrastructure, while enabling networking teams to roll Tailscale out in full-mesh mode across the subnet.</p><p>Today, customers are using peer relays to establish relayed connections at near-direct speeds across a variety of environments and settings.</p><ul><li><strong>Enable high-throughput traffic through cloud NATs, like AWS Managed NAT Gateways:</strong> Applications and services behind a Managed NAT Gateway can leverage peer relays to relay traffic that can’t establish direct connections.</li><li><strong>Relay through network firewalls:</strong> Workloads running in strictly firewalled environments can predictably expose a single UDP port, limiting the Tailscale surface area and fast-tracking the approval process for firewall exceptions.</li><li><strong>Offload from Custom and Managed DERP: </strong>Minimize data-in-transit through Tailscale‘s managed DERP network, and remove the need for customer-maintained DERP servers.</li><li><strong>Provide access to locked down customer networks: </strong>Data plane traffic can be relayed through predictable endpoints in customer networks, so that they only need to open minimal numbers of ports to facilitate cross network connections.</li></ul><h2 id="its-not-perfect-but-were-getting-there"><a href="#its-not-perfect-but-were-getting-there">It’s not perfect, but we’re getting there<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2><p>Tailscale Peer Relays is available today as a public beta. We’ve yet to establish all the connectivity paths we want to, and there’s still visibility and debugging improvements to work through. However, we’ve reliably seen our early design partners move to peer relay deployments with relative ease, and we’re ready for you to give it a try on your tailnet.</p><p>Tailscale Peer Relays can be enabled on all plans, including free (it’s our little way of working through the kinks of the modern internet with our customers). All customers can use two peer relays, for free, forever. As your needs scale, so will the number of available peer relays. To add even more peer relays to your tailnet, <a target="_blank" rel="noreferrer" href="https://tailscale.com/contact/sales?utm_source=blog&amp;utm_medium=content&amp;utm_campaign=fall-update-2025P">come have a chat with us</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Minecraft removing obfuscation in Java Edition (745 pts)]]></title>
            <link>https://www.minecraft.net/en-us/article/removing-obfuscation-in-java-edition</link>
            <guid>45748879</guid>
            <pubDate>Wed, 29 Oct 2025 16:12:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.minecraft.net/en-us/article/removing-obfuscation-in-java-edition">https://www.minecraft.net/en-us/article/removing-obfuscation-in-java-edition</a>, See on <a href="https://news.ycombinator.com/item?id=45748879">Hacker News</a></p>
Couldn't get https://www.minecraft.net/en-us/article/removing-obfuscation-in-java-edition: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Azure Outage (436 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=45748799</link>
            <guid>45748799</guid>
            <pubDate>Wed, 29 Oct 2025 16:08:35 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=45748799">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><tbody><tr id="45749790"><td></td></tr><tr id="45750104"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750104" href="https://news.ycombinator.com/vote?id=45750104&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>This is funny but also possibly true because: business/MBA types see these outages as a way to prove how critical some services are, leading to investors deciding to load up on the vendor's stock.</p></div></td></tr></tbody></table></td></tr><tr id="45750140"><td></td></tr><tr id="45749044"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749044" href="https://news.ycombinator.com/vote?id=45749044&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Update 16:57 UTC:</p><p>Azure Portal Access Issues</p><p>Starting at approximately 16:00 UTC, we began experiencing Azure Front Door issues resulting in a loss of availability of some services. In addition. customers may experience issues accessing the Azure Portal. Customers can attempt to use programmatic methods (PowerShell, CLI, etc.) to access/utilize resources if they are unable to access the portal directly. We have failed the portal away from Azure Front Door (AFD) to attempt to mitigate the portal access issues and are continuing to assess the situation.</p><p>We are actively assessing failover options of internal services from our AFD infrastructure. Our investigation into the contributing factors and additional recovery workstreams continues. More information will be provided within 60 minutes or sooner.</p><p>This message was last updated at 16:57 UTC on 29 October 2025</p><p>---</p><p>Update: 16:35 UTC:</p><p>Azure Portal Access Issues</p><p>Starting at approximately 16:00 UTC, we began experiencing DNS issues resulting in availability degradation of some services. Customers may experience issues accessing the Azure Portal. We have taken action that is expected to address the portal access issues here shortly. We are actively investigating the underlying issue and additional mitigation actions. More information will be provided within 60 minutes or sooner.</p><p>This message was last updated at 16:35 UTC on 29 October 2025</p><p>---</p><p>Azure Portal Access Issues</p><p>We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly.</p><p>This message was last updated at 16:18 UTC on 29 October 2025</p><p>---</p><p>Message from the Azure Status Page: <a href="https://azure.status.microsoft/en-gb/status" rel="nofollow">https://azure.status.microsoft/en-gb/status</a></p></div></td></tr></tbody></table></td></tr><tr id="45749996"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749996" href="https://news.ycombinator.com/vote?id=45749996&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Azure Network Availability Issues</p><p>Starting at approximately 16:00 UTC, we began experiencing Azure Front Door issues resulting in a loss of availability of some services. We suspect that an inadvertent configuration change as the trigger event for this issue. We are taking two concurrent actions where we are blocking all changes to the AFD services and at the same time rolling back to our last known good state.</p><p>We have failed the portal away from Azure Front Door (AFD) to mitigate the portal access issues. Customers should be able to access the Azure management portal directly.</p><p>We do not have an ETA for when the rollback will be completed, but we will update this communication within 30 minutes or when we have an update.</p><p>This message was last updated at 17:17 UTC on 29 October 2025</p></div></td></tr></tbody></table></td></tr><tr id="45750154"><td></td></tr><tr id="45750015"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750015" href="https://news.ycombinator.com/vote?id=45750015&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>AFD is down quite often regionally in Europe for our services. In 50%+ the cases they just don‘t report it anywhere, even if its for 2h+.</p></div></td></tr></tbody></table></td></tr><tr id="45750063"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45750063" href="https://news.ycombinator.com/vote?id=45750063&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Spam those Azure tickets.  If you have a CSAM, build them a nice powerpoint telling the story of all your AFD issues (that's what they are there for).</p><p>&gt; In 50%+ the cases they just don‘t report it anywhere, even if its for 2h+.</p><p>I assume you mean publicly.  Are you getting the service health alerts?</p></div></td></tr></tbody></table></td></tr><tr id="45750117"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45750117" href="https://news.ycombinator.com/vote?id=45750117&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>I got a service health alert an hour after it started, saying the portal was having issues. Pretty useless and misleading.</p></div></td></tr></tbody></table></td></tr><tr id="45749919"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749919" href="https://news.ycombinator.com/vote?id=45749919&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Whilst the status message acknowledge's the issue with Front Door (AFD), it seems as though the rest of the actions are about how to get Portal/internal services working without relying on AFD. For those of us using Front Door does that mean we're in for a long haul?</p></div></td></tr></tbody></table></td></tr><tr id="45749370"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749370" href="https://news.ycombinator.com/vote?id=45749370&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>I'll be interested in the incident writeup since DNS is mentioned. It will be interesting in a way if it is similar to what happened at AWS.</p></div></td></tr></tbody></table></td></tr><tr id="45749618"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749618" href="https://news.ycombinator.com/vote?id=45749618&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>It's pretty unlikely. AWS published a public 'RCA' <a href="https://aws.amazon.com/message/101925/" rel="nofollow">https://aws.amazon.com/message/101925/</a>. A race condition in a DNS 'record allocator' causing all DNS records for DDB to be wiped out.</p><p>I'm simplifying a bit, but I don't think it's likely that Azure has a similar race condition wiping out DNS records on _one_ system than then propagates to all others. The similarity might just end at "it was DNS".</p></div></td></tr></tbody></table></td></tr><tr id="45749762"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45749762" href="https://news.ycombinator.com/vote?id=45749762&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>That RCA was fun. A distributed system with members that don't know about each other, don't bother with leader elections, and basically all stomp all over each other updating the records. It "worked fine" until one of the members had slightly increased latency and everything cascade-failed down from there. I'm sure there was missing (internal) context but it did not sound like a well-architected system at all.</p></div></td></tr></tbody></table></td></tr><tr id="45750112"><td></td></tr><tr id="45749776"><td></td></tr><tr id="45749640"><td></td></tr><tr id="45749055"><td></td></tr><tr id="45750056"><td></td></tr><tr id="45749250"><td></td></tr><tr id="45749704"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45749704" href="https://news.ycombinator.com/vote?id=45749704&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>From what I can tell, Downdetector just tracks traffic to their pages without actually checking if the site is down.</p><p>The other day during the AWS outage they "reported" OVH down too.</p></div></td></tr></tbody></table></td></tr><tr id="45749294"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45749294" href="https://news.ycombinator.com/vote?id=45749294&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>yea I saw that, but im not sure on how accurate that is. a few large apps/companies I know to be 100% on AWS in us-east-1 are cranking along just fine.</p></div></td></tr></tbody></table></td></tr><tr id="45749673"><td></td></tr><tr id="45749939"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749939" href="https://news.ycombinator.com/vote?id=45749939&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>yes, and it seems that at least for some login.microsoftonline.com is down too, which is part of the Entra login / SSO  flow.</p></div></td></tr></tbody></table></td></tr><tr id="45749144"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749144" href="https://news.ycombinator.com/vote?id=45749144&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Yeah, I am guessing it's just a placeholder till they get more info. I thought I saw somewhere that internally within Microsoft it's seen as a "Sev 1" with "all hands on deck" - Annoyingly I can't remember where I saw it, so if someone spots it before I do, please credit that person :D</p><p>Edit: Typo!</p></div></td></tr></tbody></table></td></tr><tr id="45749309"><td></td></tr><tr id="45750017"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750017" href="https://news.ycombinator.com/vote?id=45750017&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Yet another reason to move away from Front Door.</p><p>We already had to do it for large files served from Blob Storage since they would cap out at 2MB/s when not in cache of the nearest PoP. If you’ve ever experienced slow Windows Store or X-Box downloads it’s probably the same problem.</p><p>I had a support ticket open for months about this and in the end the agent said “this is to be expected and we don’t plan on doing anything about it”.</p><p>We’ve moved to Cloudflare and not only is the performance great, but it costs less.</p><p>Only thing I need to move off Front Door is a static website for our docs served from Blob Storage, this incident will  make us do it sooner rather than later.</p></div></td></tr></tbody></table></td></tr><tr id="45750144"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45750144" href="https://news.ycombinator.com/vote?id=45750144&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>we are considering the same but because our website uses APEX domain we would need to move all DNS resolver to cloudfront right ? Does it have as a nice "rule set builder" as azure ?</p></div></td></tr></tbody></table></td></tr><tr id="45749332"><td></td></tr><tr id="45749788"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749788" href="https://news.ycombinator.com/vote?id=45749788&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>The Internet is supposed to be decentralized. The big three seem to have all the power now (Amazon, Microsoft, and Google) plus Cloudflare/Oracle.</p><p>How did we get here? Is it because of scale? Going to market in minutes by using someone else's computers instead of building out your own, like co-location or dedicated servers, like back in the day.</p></div></td></tr></tbody></table></td></tr><tr id="45749826"><td></td></tr><tr id="45749905"><td></td></tr><tr id="45749946"><td></td></tr><tr id="45749866"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749866" href="https://news.ycombinator.com/vote?id=45749866&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>A lot of money and years of marketing the cloud as the responsible business decision led us here. Now that the cloud providers have vendor lock-in, few will leave, and customers will continue to wildly overpay for cloud services.</p></div></td></tr></tbody></table></td></tr><tr id="45749985"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749985" href="https://news.ycombinator.com/vote?id=45749985&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Ahh, but you forget what it <i>used</i> to be like. Sites used to go down <i>all the time.</i></p><p>Now, they go down a lot less frequently, but when they do, it's more widespread.</p></div></td></tr></tbody></table></td></tr><tr id="45750078"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750078" href="https://news.ycombinator.com/vote?id=45750078&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>&gt; How did we get here?</p><p>I think the response lies in the surrounding ecosystem.</p><p>If you have a company it's easier to scale your team if you use AWS (or any other established ecosystem). It's way easier to hire 10 engineers that are competent with AWS tools than it is to hire 10 engineers that are competent with the IBM tools.</p><p>And from the individuals perspective it also make sense to bet on larger platforms. If you want to increase your odds of getting a new job, learning the AWS tools gives you a better ROI than learning the IBM tools.</p></div></td></tr></tbody></table></td></tr><tr id="45749853"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749853" href="https://news.ycombinator.com/vote?id=45749853&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Thats the whole point, big players like AWS and MS can go down, but here we are still talking on the internet.</p><p>Decentralisation is winning it seems.</p></div></td></tr></tbody></table></td></tr><tr id="45750084"><td></td></tr><tr id="45749871"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749871" href="https://news.ycombinator.com/vote?id=45749871&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Consolidation is the inevitable outcome of free unregulated markets.</p><p>In our highly interconnected world, decentralization paradoxically requires a central authority to enforce decentralization by restricting M&amp;A, cartels, etc.</p></div></td></tr></tbody></table></td></tr><tr id="45750150"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45750150" href="https://news.ycombinator.com/vote?id=45750150&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>I’ve been migrating our services off of Azure slowly for the past couple of years. The last internet facing things remaining are a static assets bucket and an analytics VM running Matomo. Working with Front Door has been an abysmal experience, and today was the push I needed to finally migrate our assets to Cloudflare.</p><p>I feel pretty justified in my previous decisions to move away from Azure. Using it feels like building on quicksand…</p></div></td></tr></tbody></table></td></tr><tr id="45749410"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749410" href="https://news.ycombinator.com/vote?id=45749410&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>I noticed that Starbucks mobile ordering was down and thought “welp, I guess I’ll order a bagel and coffee on Grubhub”, then GrubHub was down. My next stop was HN to find the common denominator, and y’all did not disappoint.</p></div></td></tr></tbody></table></td></tr><tr id="45749715"><td></td></tr><tr id="45749980"><td></td></tr><tr id="45749813"><td></td></tr><tr id="45749639"><td></td></tr><tr id="45749688"><td></td></tr><tr id="45749903"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45749903" href="https://news.ycombinator.com/vote?id=45749903&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>you wouldn't believe some of the crap enterprise bigco mgmt put in place for disaster recovery.</p><p>they think that they are 'eliminating a single point of failure', but in reality, they end up adding multiple, complicated points of mostly failure.</p></div></td></tr></tbody></table></td></tr><tr id="45749680"><td></td></tr><tr id="45749659"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749659" href="https://news.ycombinator.com/vote?id=45749659&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Gonna build my application to be multicloud so that it requires multiple cloud platforms to be online at the same time. The RAID 0 of cloud computing.</p></div></td></tr></tbody></table></td></tr><tr id="45749773"><td></td></tr><tr id="45750148"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45750148" href="https://news.ycombinator.com/vote?id=45750148&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>All of my employers things are hosted on Azure and running just fine and didn't go down at all. Portal access has been fixed.</p><p>Doesn't seem to be too bad of an outage unless you were relying on Azure Front Door.</p></div></td></tr></tbody></table></td></tr><tr id="45749346"><td></td></tr><tr id="45749554"><td></td></tr><tr id="45749814"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749814" href="https://news.ycombinator.com/vote?id=45749814&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>&gt; [Satya Nadella] said that the company’s future opportunity was to bring AI to all eight billion people on the planet.</p><p>But what if I don't want AI brought to me?</p></div></td></tr></tbody></table></td></tr><tr id="45750147"><td></td></tr><tr id="45749924"><td></td></tr><tr id="45749992"><td></td></tr><tr id="45749909"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749909" href="https://news.ycombinator.com/vote?id=45749909&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Like most technology initiative these tech CEOs dream up: You're going to get it and swallow it, whether you want it or not.</p></div></td></tr></tbody></table></td></tr><tr id="45749977"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749977" href="https://news.ycombinator.com/vote?id=45749977&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>I especially like how Nadella speaks of layoffs as some kind of uncontrollable natural disaster, like a hurricane, caused by no-one in particular. A kind of "God works in mysterious ways".</p><pre><code>    &gt; “Microsoft is being recognized and rewarded at levels never seen before,” Nadella wrote. “And yet, at the same time, we’ve undergone layoffs. This is the enigma of success in an industry that has no franchise value.”
     
    &gt; Nadella explained the disconnect between thriving financials and layoffs by stating that “progress isn’t linear” and that it is “sometimes dissonant, and always demanding.”
</code></pre><p>
I've read the whole memo and it's actually worse than those excerpts. Nadella doesn't even claim these were low performers:</p><pre><code>    &gt; These decisions are among the most difficult we have to make. They affect people we’ve worked alongside, learned from, and shared countless moments with—our colleagues, teammates, and friends.
</code></pre><p>
Ok, so Microsoft is thriving, these were friends and people "we've learned from", but they must go because... uh... "progress isn't linear". Well, thanks Nadella! That explains so much!</p></div></td></tr></tbody></table></td></tr><tr id="45749820"><td></td></tr><tr id="45749957"><td></td></tr><tr id="45749162"><td></td></tr><tr id="45749765"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749765" href="https://news.ycombinator.com/vote?id=45749765&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>There's a Family Dollar by my house that is down at least 2 full days per month because of bad inet connectivity. I live close enough that with a small tower on my roof i can get line of sight to theirs. I've thought about offering them a backup link off my home inet if they give me 50% of sales whenever its in use. It would be a pretty good deal for them, better some sales when their inet is down vs none.</p></div></td></tr></tbody></table></td></tr><tr id="45749986"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749986" href="https://news.ycombinator.com/vote?id=45749986&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>You'd think any SeriousBusiness would have a backup way to take customers' money. This is the one thing you always want to be able to do: accept payment. If they made it so they can't do that, they deserve the hit to their revenue. People should just walk out of the store with the goods if they're not being charged.</p><p>Why doesn't someone in the store at least have one of those manual kachunk-kachunk carbon copy card readers in the back that they can resuscitate for a few days until the technology is turned back on? Did they throw them all away?</p></div></td></tr></tbody></table></td></tr><tr id="45750073"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45750073" href="https://news.ycombinator.com/vote?id=45750073&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>The kachunk-kachunk credit card machines need raised digits on the cards, and I don't think most banks have been issuing those for years at this point. Mine have been smooth for at least 10 years.</p></div></td></tr></tbody></table></td></tr><tr id="45750027"><td></td></tr><tr id="45749805"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749805" href="https://news.ycombinator.com/vote?id=45749805&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>IIRC, the grocery chain I worked for used to have an offline mode to move customers out the door. But it meant that when the system came back online, if the customers card was denied, the customer got free groceries.</p></div></td></tr></tbody></table></td></tr><tr id="45749516"><td></td></tr><tr id="45749620"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749620" href="https://news.ycombinator.com/vote?id=45749620&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>I knew an old guy in the '00s who specialized in cobal/fortran for working on tiller software. Guess he retired and they couldn't maintain it</p></div></td></tr></tbody></table></td></tr><tr id="45749752"><td></td></tr><tr id="45750088"><td></td></tr><tr id="45750057"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45750057" href="https://news.ycombinator.com/vote?id=45750057&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>I absolutely love the utility aspect of LLMs but part of me is curious if moving faster by using AI is going to make these sorts of failure more and more often.</p></div></td></tr></tbody></table></td></tr><tr id="45750162"><td></td></tr><tr id="45749961"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749961" href="https://news.ycombinator.com/vote?id=45749961&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>They admit in their update blurb azure front door is having issues but still report azure front door as having no issues on their status page.</p><p>And it's very clear from these updates that they're more focused on the portal than the product, their updates haven't even mentioned fixing it yet, just moving off of it, as if it's some third party service that's down.</p></div></td></tr></tbody></table></td></tr><tr id="45750107"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750107" href="https://news.ycombinator.com/vote?id=45750107&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>&gt; as having no issues on their status page</p><p>Unsubstantiated idea: So the support contract likely says there is a window between each reporting step and the status page is the last one and the one in the legal documents giving them several more hours before the clauses trigger.</p></div></td></tr></tbody></table></td></tr><tr id="45748903"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45748903" href="https://news.ycombinator.com/vote?id=45748903&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Ouch, and login.microsoftonline.com too - i.e. SSO using MS accounts. We'd just rolled that out across most (all?) of our internal systems...</p><p>And microsoft.com too - that's gotta hurt</p></div></td></tr></tbody></table></td></tr><tr id="45749902"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749902" href="https://news.ycombinator.com/vote?id=45749902&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>It is interesting to see the differential across different tenants in different geographies:</p><p>- on a US tenant I am unable to access login.microsoftonline.com and the login flow stalls on any SSO authentication attempt.</p><p>- on a European tenant, probably germany-west, I am able to login and access the Azure portal.</p></div></td></tr></tbody></table></td></tr><tr id="45749907"><td></td></tr><tr id="45749507"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749507" href="https://news.ycombinator.com/vote?id=45749507&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>SSO and 365 are working fine for us, but admin portals for Azure/365 are down. Our workloads in Azure don't seem to be impacted.</p></div></td></tr></tbody></table></td></tr><tr id="45749484"><td></td></tr><tr id="45749727"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749727" href="https://news.ycombinator.com/vote?id=45749727&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>We are very dependent on Azure and Microsoft Authentication and Microsoft 365 and haven’t had weekly or even monthly issues. I can think of maybe three issues this year.</p></div></td></tr></tbody></table></td></tr><tr id="45749468"><td></td></tr><tr id="45749906"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749906" href="https://news.ycombinator.com/vote?id=45749906&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>This is because Azure just copies everything AWS does. Google is a bit more innovative, they will have something else unexpected happen.</p></div></td></tr></tbody></table></td></tr><tr id="45750118"><td></td></tr><tr id="45749678"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749678" href="https://news.ycombinator.com/vote?id=45749678&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Maybe they are and no one realized yet.. :P</p><p>That said, I don't hear about GCP outages all that often. I do think AWS might be leading in outages, but that's a gut feeling, I didn't look up numbers.</p></div></td></tr></tbody></table></td></tr><tr id="45750070"><td></td></tr><tr id="45749885"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749885" href="https://news.ycombinator.com/vote?id=45749885&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>fairly certain they had a significant multi region outage within the past few years. I'll try to find some details to link.</p><p>Few customers....few voices to complain as well.</p></div></td></tr></tbody></table></td></tr><tr id="45749967"><td></td></tr><tr id="45749818"><td></td></tr><tr id="45749536"><td></td></tr><tr id="45749691"><td></td></tr><tr id="45749553"><td></td></tr><tr id="45750014"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45750014" href="https://news.ycombinator.com/vote?id=45750014&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>pretty interesting how datadog's uptime tracker (<a href="https://updog.ai/" rel="nofollow">https://updog.ai/</a>) says all the sites are fully available.</p><p>if that's true then it's a sign that Azure's control / data plane separation is doing it's job! at least for now</p></div></td></tr></tbody></table></td></tr><tr id="45749237"><td></td></tr><tr id="45749738"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749738" href="https://news.ycombinator.com/vote?id=45749738&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>I've been doing it since 1998 in my bedroom with a dual T1 (and on to real DCs later). While I've had some outages for sure it makes me feel better I am not that divergent in uptime in the long run vs big clouds.</p></div></td></tr></tbody></table></td></tr><tr id="45749988"><td></td></tr><tr id="45749477"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749477" href="https://news.ycombinator.com/vote?id=45749477&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Pretty much all Azure services seem to be down. Their status page says it's only the portal since 16:00. It would be nice if these mega-companies could update their status page when they take down a large fraction of the Internet and thousands of services that use them.</p></div></td></tr></tbody></table></td></tr><tr id="45749505"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749505" href="https://news.ycombinator.com/vote?id=45749505&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>FWIW, all of our databases, VMs, AKS clusters, services, jobs etc - are all working fine. Which services are down for you, maybe we can build a list?</p></div></td></tr></tbody></table></td></tr><tr id="45749567"><td></td></tr><tr id="45749621"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749621" href="https://news.ycombinator.com/vote?id=45749621&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>All of our Azure workloads are up, but we don't use Azure Front Door. That seems to be the only impacted product, apart from the management portal.</p></div></td></tr></tbody></table></td></tr><tr id="45749993"><td></td></tr><tr id="45749913"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749913" href="https://news.ycombinator.com/vote?id=45749913&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Same playbook for AWS. When they admitted that Dynamo was inaccessible, they failed to provide context that their internal services are heavily dependent on Dynamo</p><p>It's only after the fact they are transparent about the impact</p></div></td></tr></tbody></table></td></tr><tr id="45749481"><td></td></tr><tr id="45749627"><td></td></tr><tr id="45749881"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749881" href="https://news.ycombinator.com/vote?id=45749881&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>LinkedIn has been acting funny for an hour or so, and some pages in the learn.microsoft.com domain have been failing for me too...</p></div></td></tr></tbody></table></td></tr><tr id="45749082"><td></td></tr><tr id="45749888"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749888" href="https://news.ycombinator.com/vote?id=45749888&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>&gt; Even the national digital id service is down.</p><p>Can't help but smirk as my country is ramming through "Digital ID" right now</p></div></td></tr></tbody></table></td></tr><tr id="45749875"><td></td></tr><tr id="45749736"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749736" href="https://news.ycombinator.com/vote?id=45749736&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Does (should, could) DownDetector also say what customer-facing services are down, when some infrastructure is unworking?  Or is that the info that the malefactors are seeking?</p></div></td></tr></tbody></table></td></tr><tr id="45749413"><td></td></tr><tr id="45749438"><td></td></tr><tr id="45749612"><td></td></tr><tr id="45749721"><td></td></tr><tr id="45749449"><td></td></tr><tr id="45749204"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749204" href="https://news.ycombinator.com/vote?id=45749204&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>i guess folks in azure wanted to show some solidarity with aws brethren</p><p>(couldn't resist adding it. i acknowledge this comment adds no value to the discussion)</p></div></td></tr></tbody></table></td></tr><tr id="45749613"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749613" href="https://news.ycombinator.com/vote?id=45749613&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Azure goes down all the time. On Friday we had an entire regional service down all day. Two weeks ago same thing different region. You only hear about it when it's something everyone uses like the portal, because in general nobody uses Azure unless they're held hostage.</p></div></td></tr></tbody></table></td></tr><tr id="45749672"><td></td></tr><tr id="45748863"><td></td></tr><tr id="45749701"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749701" href="https://news.ycombinator.com/vote?id=45749701&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>It is much more than azure. One of my kids needs a key for their laptop and can't reach that either. Great excuse though, 'Azure ate my homework'. What a ridiculous world we are building. Fuck MS and their account requirements for windows.</p></div></td></tr></tbody></table></td></tr><tr id="45749922"><td></td></tr><tr id="45749769"><td></td></tr><tr id="45749835"><td></td></tr><tr id="45749411"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749411" href="https://news.ycombinator.com/vote?id=45749411&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>I am having a bunch of issues. It looks like their sites and azure are both affected.</p><p>I also got weird notification in VS2022 that my license key was upgraded to Enterprise, but we did not purchase anything.</p></div></td></tr></tbody></table></td></tr><tr id="45749648"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749648" href="https://news.ycombinator.com/vote?id=45749648&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Might be a failsafe, if you cant get a license status, and you're aware that MS is down, just default to the highest tier.</p></div></td></tr></tbody></table></td></tr><tr id="45749524"><td></td></tr><tr id="45748826"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45748826" href="https://news.ycombinator.com/vote?id=45748826&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>This is impacting the Azure CDN at azureedge.net. DNS A records for azureedge.net tenants are taking 2-6 seconds and often return nothing.</p></div></td></tr></tbody></table></td></tr><tr id="45749205"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749205" href="https://news.ycombinator.com/vote?id=45749205&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Quite close to the recent AWS outage. Let me take a look if its a major one similar to AWS.</p><p>Any guess on what's causing it?</p><p>In hindsight, I guess the foresight of some organizations to go multi-cloud was correct after all.</p></div></td></tr></tbody></table></td></tr><tr id="45749784"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749784" href="https://news.ycombinator.com/vote?id=45749784&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>We're multi-cloud and it really saved a few workloads last week with the AWS issue.</p><p>It's not easy though.</p></div></td></tr></tbody></table></td></tr><tr id="45749302"><td></td></tr><tr id="45749493"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749493" href="https://news.ycombinator.com/vote?id=45749493&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Yeah, these things never happened when humans were trusted without sufficient review and oversight of changes to production.</p></div></td></tr></tbody></table></td></tr><tr id="45749386"><td></td></tr><tr id="45749434"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45749434" href="https://news.ycombinator.com/vote?id=45749434&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>I don't think it's meant to be serious. It's a comment on Microsoft laying off their staff and stuffing their Azure and Dotnet teams with AI product managers.</p></div></td></tr></tbody></table></td></tr><tr id="45749472"><td></td></tr><tr id="45749228"><td></td></tr><tr id="45749053"><td></td></tr><tr id="45749070"><td></td></tr><tr id="45749846"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749846" href="https://news.ycombinator.com/vote?id=45749846&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Anyone have betting odds on when Google will go down next? Are we looking at all 3 providers having outages in the span of 3 weeks?</p></div></td></tr></tbody></table></td></tr><tr id="45749500"><td></td></tr><tr id="45749768"><td></td></tr><tr id="45749018"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749018" href="https://news.ycombinator.com/vote?id=45749018&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Azure portal currently mostly not working (UK)... Downdetector reporting various Microsoft linked services are out (Minecraft, Microsoft 365, Xbox...)</p></div></td></tr></tbody></table></td></tr><tr id="45749512"><td></td></tr><tr id="45749186"><td></td></tr><tr id="45749266"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749266" href="https://news.ycombinator.com/vote?id=45749266&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>Yeah, I have non prod environments that don't use FD that are functioning. Routing through FD does not work. And a different app, nonprod doesn't use FD (and is working) but loads assets from the CDN (which is not working).</p><p>FD and CDN are global resources and are experiencing issues. Probably some other global resources as well.</p><p>Hate to say it, but DNS is looking like it's still the undisputed champ.</p></div></td></tr></tbody></table></td></tr><tr id="45749215"><td></td></tr><tr id="45749369"><td></td></tr><tr id="45749571"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749571" href="https://news.ycombinator.com/vote?id=45749571&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>microsoft.com is back -</p><p>edit: it worked once, then died again. So I guess - some resolvers, or FD servers may be working!</p></div></td></tr></tbody></table></td></tr><tr id="45748991"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45748991" href="https://news.ycombinator.com/vote?id=45748991&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>downdetector reports coincident cloudflare outage. is microsoft using cloudflare for management plane, or is there common infra? data center problem somewhere, maybe fiber backbone? BGP?</p></div></td></tr></tbody></table></td></tr><tr id="45749742"><td></td></tr><tr id="45749048"><td></td></tr><tr id="45749390"><td></td></tr><tr id="45749406"><td></td></tr><tr id="45749351"><td></td></tr><tr id="45749395"><td></td></tr><tr id="45749921"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749921" href="https://news.ycombinator.com/vote?id=45749921&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>You just paste the outage error codes back to the LLM and pray it's still working and can fix whatever went wrong!</p></div></td></tr></tbody></table></td></tr><tr id="45750109"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45750109" href="https://news.ycombinator.com/vote?id=45750109&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>When all the people forget to code for themselves, every LLM will code itself out of existence with that one last bug. One, after another.</p></div></td></tr></tbody></table></td></tr><tr id="45749713"><td></td></tr><tr id="45749037"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749037" href="https://news.ycombinator.com/vote?id=45749037&amp;how=up&amp;goto=item%3Fid%3D45748799"></a></center></td><td><br>
<div><p>My best guess at the moment is something global like the CDN is having problems affecting things everywhere. I'm able to use a legacy application we have that goes directly to resources in uswest3, but I'm not able to use our more modern application which uses APIM/CDN networks at all.</p></div></td></tr></tbody></table></td></tr><tr id="45749158"><td></td></tr><tr id="45749474"><td></td></tr><tr id="45748888"><td></td></tr></tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cursor Composer: Building a fast frontier model with RL (194 pts)]]></title>
            <link>https://cursor.com/blog/composer</link>
            <guid>45748725</guid>
            <pubDate>Wed, 29 Oct 2025 16:04:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cursor.com/blog/composer">https://cursor.com/blog/composer</a>, See on <a href="https://news.ycombinator.com/item?id=45748725">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-sanity="id=a852e721-560c-440e-be81-44a95e28da2d;type=post;path=content;base=%2F"><p>Composer is our new agent model designed for software engineering intelligence and speed. On our benchmarks, the model achieves frontier coding results with generation speed four times faster than similar models.</p><p>We achieve these results by training the model to complete real-world software engineering challenges in large codebases. During training, Composer is given access to a set of production search and editing tools and tasked with efficiently solving a diverse range of difficult problems. The final result is a large-scale model optimized for high-speed use as an agent in Cursor.</p><figure></figure><p>Our motivation comes from our experience developing Cursor Tab, our custom completion model. We found that often developers want the smartest model that can support interactive use, keeping them in the flow of coding. In our development process, we experimented with a prototype agent model, codenamed Cheetah, to better understand the impact of faster agent models. Composer is a smarter version of this model that keeps coding delightful by being fast enough for an interactive experience.</p><p>Composer is a mixture-of-experts (MoE) language model supporting long-context generation and understanding. It is specialized for software engineering through reinforcement learning (RL) in a diverse range of development environments. At each iteration of training, the model is given a problem description and instructed to produce the best response, be it a code edit, a plan, or an informative answer. The model has access to simple tools, like reading and editing files, and also more powerful ones like terminal commands and codebase-wide semantic search.</p><p>To measure progress, we constructed an evaluation that measures a model's usefulness to a software developer as faithfully as possible. Our benchmark, Cursor Bench, consists of real agent requests from engineers and researchers at Cursor, along with hand-curated optimal solutions to these requests. The resulting evaluation measures not just the agent’s correctness, but also its adherence to a codebase's existing abstractions and software engineering practices.</p><figure></figure><p>Reinforcement learning allows us to actively specialize the model for effective software engineering. Since response speed is a critical component for interactive development, we incentivize the model to make efficient choices in tool use and to maximize parallelism whenever possible. In addition, we train the model to be a helpful assistant by minimizing unnecessary responses and claims made without evidence. We also find that during RL, the model learns useful behaviors on its own like performing complex searches, fixing linter errors, and writing and executing unit tests.</p><figure></figure><p>Efficient training of large MoE models requires significant investment into building infrastructure and systems research. We built custom training infrastructure leveraging PyTorch and Ray to power asynchronous reinforcement learning at scale. We natively train our models at low precision by combining our <a href="https://cursor.com/blog/kernels">MXFP8 MoE kernels</a> with expert parallelism and hybrid sharded data parallelism, allowing us to scale training to thousands of NVIDIA GPUs with minimal communication cost. Additionally, training with MXFP8 allows us to deliver faster inference speeds without requiring post-training quantization.</p><p>During RL, we want our model to be able to call any tool in the Cursor Agent harness. These tools allow editing code, using semantic search, grepping strings, and running terminal commands. At our scale, teaching the model to effectively call these tools requires running hundreds of thousands of concurrent sandboxed coding environments in the cloud. To support this workload, we adapted existing infrastructure we built for Background Agents, rewriting our virtual machine scheduler to support the bursty nature and scale of training runs. This enabled seamless unification of RL environments with production environments.</p><p>Cursor builds tools for software engineering, and we make heavy use of the tools we develop. A motivation of Composer development has been developing an agent we would reach for in our own work. In recent weeks, we have found that many of our colleagues were using Composer for their day-to-day software development. With this release, we hope that you also find it to be a valuable tool.</p><p>—</p><p><em>¹ Benchmarked on an internal benchmark in the Cursor tool harness. We group models into classes based on score and report the best model in each class. "Fast Frontier" includes models designed for efficient inference such as Haiku 4.5 and Gemini Flash 2.5. "Best Open" includes recent open weight model releases such as Qwen Coder and GLM 4.6. "Frontier 7/2025" is the best model available in July of this year.  "Best Frontier" includes GPT-5 and Sonnet 4.5, which both outperform Composer. For the Tokens per Second calculation, tokens are standardized across models to the latest Anthropic tokenizer.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Tell HN: Azure outage (765 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=45748661</link>
            <guid>45748661</guid>
            <pubDate>Wed, 29 Oct 2025 16:01:18 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=45748661">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><tbody><tr id="45750953"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45750953" href="https://news.ycombinator.com/vote?id=45750953&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>For some reason an Azure outage does not faze me in the same way that an AWS outage does.</p><p>I have never had much confidence in Azure as a cloud provider. The vertical integration of all the things for a Microsoft shop was initially <i>very</i> compelling. I was ready to fight that battle. But, this fantasy was quickly ruined by poor execution on Microsoft's part. They were able to convince me to move back to AWS by simply making it difficult to provision compute resources. Their quota system &amp; availability issues are a nightmare to deal with compared to EC2.</p><p>At this point I'd rather use GCP over Azure and I have zero seconds of experience with it. The number of things Microsoft gets right in 2025 can be counted single-handedly. The things they do get right are quite good, but everything else tends to be extremely awful.</p></div></td></tr></tbody></table></td></tr><tr id="45749044"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749044" href="https://news.ycombinator.com/vote?id=45749044&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Update 16:57 UTC:</p><p>Azure Portal Access Issues</p><p>Starting at approximately 16:00 UTC, we began experiencing Azure Front Door issues resulting in a loss of availability of some services. In addition. customers may experience issues accessing the Azure Portal. Customers can attempt to use programmatic methods (PowerShell, CLI, etc.) to access/utilize resources if they are unable to access the portal directly. We have failed the portal away from Azure Front Door (AFD) to attempt to mitigate the portal access issues and are continuing to assess the situation.</p><p>We are actively assessing failover options of internal services from our AFD infrastructure. Our investigation into the contributing factors and additional recovery workstreams continues. More information will be provided within 60 minutes or sooner.</p><p>This message was last updated at 16:57 UTC on 29 October 2025</p><p>---</p><p>Update: 16:35 UTC:</p><p>Azure Portal Access Issues</p><p>Starting at approximately 16:00 UTC, we began experiencing DNS issues resulting in availability degradation of some services. Customers may experience issues accessing the Azure Portal. We have taken action that is expected to address the portal access issues here shortly. We are actively investigating the underlying issue and additional mitigation actions. More information will be provided within 60 minutes or sooner.</p><p>This message was last updated at 16:35 UTC on 29 October 2025</p><p>---</p><p>Azure Portal Access Issues</p><p>We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly.</p><p>This message was last updated at 16:18 UTC on 29 October 2025</p><p>---</p><p>Message from the Azure Status Page: <a href="https://azure.status.microsoft/en-gb/status" rel="nofollow">https://azure.status.microsoft/en-gb/status</a></p></div></td></tr></tbody></table></td></tr><tr id="45749996"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749996" href="https://news.ycombinator.com/vote?id=45749996&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Azure Network Availability Issues</p><p>Starting at approximately 16:00 UTC, we began experiencing Azure Front Door issues resulting in a loss of availability of some services. We suspect that an inadvertent configuration change as the trigger event for this issue. We are taking two concurrent actions where we are blocking all changes to the AFD services and at the same time rolling back to our last known good state.</p><p>We have failed the portal away from Azure Front Door (AFD) to mitigate the portal access issues. Customers should be able to access the Azure management portal directly.</p><p>We do not have an ETA for when the rollback will be completed, but we will update this communication within 30 minutes or when we have an update.</p><p>This message was last updated at 17:17 UTC on 29 October 2025</p></div></td></tr></tbody></table></td></tr><tr id="45750984"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45750984" href="https://news.ycombinator.com/vote?id=45750984&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>"We have initiated the deployment of our 'last known good' configuration. This is expected to be fully deployed in about 30 minutes from which point customers will start to see initial signs of recovery. Once this is completed, the next stage is to start to recover nodes while we route traffic through these healthy nodes."</p><p>"This message was last updated at 18:11 UTC on 29 October 2025"</p></div></td></tr></tbody></table></td></tr><tr id="45750015"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750015" href="https://news.ycombinator.com/vote?id=45750015&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>AFD is down quite often regionally in Europe for our services. In 50%+ the cases they just don‘t report it anywhere, even if its for 2h+.</p></div></td></tr></tbody></table></td></tr><tr id="45750063"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45750063" href="https://news.ycombinator.com/vote?id=45750063&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Spam those Azure tickets.  If you have a CSAM, build them a nice powerpoint telling the story of all your AFD issues (that's what they are there for).</p><p>&gt; In 50%+ the cases they just don‘t report it anywhere, even if its for 2h+.</p><p>I assume you mean publicly.  Are you getting the service health alerts?</p></div></td></tr></tbody></table></td></tr><tr id="45750560"><td></td></tr><tr id="45750621"><td></td></tr><tr id="45750504"><td></td></tr><tr id="45750525"><td></td></tr><tr id="45750372"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45750372" href="https://news.ycombinator.com/vote?id=45750372&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>in many cases: no service health alerts, no status page updates and no confirmations from the support team in tickets.
still we can confirm these issues from different customers accross europe. Mostly the issues are regional dependent.</p></div></td></tr></tbody></table></td></tr><tr id="45750631"><td></td></tr><tr id="45751016"><td></td></tr><tr id="45750812"><td></td></tr><tr id="45750117"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45750117" href="https://news.ycombinator.com/vote?id=45750117&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>I got a service health alert an hour after it started, saying the portal was having issues. Pretty useless and misleading.</p></div></td></tr></tbody></table></td></tr><tr id="45750192"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_45750192" href="https://news.ycombinator.com/vote?id=45750192&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>That should go into the presentation you provide your CSAM with as well.</p><p>Storytelling is how issues get addressed.  Help the CSAM tell the story to the higher ups.</p></div></td></tr></tbody></table></td></tr><tr id="45750175"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45750175" href="https://news.ycombinator.com/vote?id=45750175&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Same experience. We've recently migrated fully away from AFD due to how unreliable it is.</p></div></td></tr></tbody></table></td></tr><tr id="45749370"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749370" href="https://news.ycombinator.com/vote?id=45749370&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>I'll be interested in the incident writeup since DNS is mentioned. It will be interesting in a way if it is similar to what happened at AWS.</p></div></td></tr></tbody></table></td></tr><tr id="45749618"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749618" href="https://news.ycombinator.com/vote?id=45749618&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>It's pretty unlikely. AWS published a public 'RCA' <a href="https://aws.amazon.com/message/101925/" rel="nofollow">https://aws.amazon.com/message/101925/</a>. A race condition in a DNS 'record allocator' causing all DNS records for DDB to be wiped out.</p><p>I'm simplifying a bit, but I don't think it's likely that Azure has a similar race condition wiping out DNS records on _one_ system than then propagates to all others. The similarity might just end at "it was DNS".</p></div></td></tr></tbody></table></td></tr><tr id="45749762"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45749762" href="https://news.ycombinator.com/vote?id=45749762&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>That RCA was fun. A distributed system with members that don't know about each other, don't bother with leader elections, and basically all stomp all over each other updating the records. It "worked fine" until one of the members had slightly increased latency and everything cascade-failed down from there. I'm sure there was missing (internal) context but it did not sound like a well-architected system at all.</p></div></td></tr></tbody></table></td></tr><tr id="45750112"><td></td></tr><tr id="45749640"><td></td></tr><tr id="45749776"><td></td></tr><tr id="45750867"><td></td></tr><tr id="45750385"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45750385" href="https://news.ycombinator.com/vote?id=45750385&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>DNS has both naming and cache invalidation, so no surprise it’s among the hardest things to get right. ;)</p></div></td></tr></tbody></table></td></tr><tr id="45749919"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749919" href="https://news.ycombinator.com/vote?id=45749919&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Whilst the status message acknowledge's the issue with Front Door (AFD), it seems as though the rest of the actions are about how to get Portal/internal services working without relying on AFD. For those of us using Front Door does that mean we're in for a long haul?</p></div></td></tr></tbody></table></td></tr><tr id="45750179"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45750179" href="https://news.ycombinator.com/vote?id=45750179&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Please migrate off of front door. It's been a failure mode since it came out historically. Anything else is better at this point</p></div></td></tr></tbody></table></td></tr><tr id="45750440"><td></td></tr><tr id="45749055"><td></td></tr><tr id="45750056"><td></td></tr><tr id="45749250"><td></td></tr><tr id="45749704"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45749704" href="https://news.ycombinator.com/vote?id=45749704&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>From what I can tell, Downdetector just tracks traffic to their pages without actually checking if the site is down.</p><p>The other day during the AWS outage they "reported" OVH down too.</p></div></td></tr></tbody></table></td></tr><tr id="45750814"><td></td></tr><tr id="45749294"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45749294" href="https://news.ycombinator.com/vote?id=45749294&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>yea I saw that, but im not sure on how accurate that is. a few large apps/companies I know to be 100% on AWS in us-east-1 are cranking along just fine.</p></div></td></tr></tbody></table></td></tr><tr id="45749673"><td></td></tr><tr id="45749144"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749144" href="https://news.ycombinator.com/vote?id=45749144&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Yeah, I am guessing it's just a placeholder till they get more info. I thought I saw somewhere that internally within Microsoft it's seen as a "Sev 1" with "all hands on deck" - Annoyingly I can't remember where I saw it, so if someone spots it before I do, please credit that person :D</p><p>Edit: Typo!</p></div></td></tr></tbody></table></td></tr><tr id="45749309"><td></td></tr><tr id="45750565"><td></td></tr><tr id="45749939"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749939" href="https://news.ycombinator.com/vote?id=45749939&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>yes, and it seems that at least for some login.microsoftonline.com is down too, which is part of the Entra login / SSO  flow.</p></div></td></tr></tbody></table></td></tr><tr id="45750841"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750841" href="https://news.ycombinator.com/vote?id=45750841&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>They briefly had a statement about using Traffic Manager to work with your AFD to work around this issue, with a link to learn.microsoft.com/...traffic-manager, and the link didn't work. Due to the same issue affecting everyone right now.</p><p>They quickly updated the message to REMOVE the link. Comical at this point.</p></div></td></tr></tbody></table></td></tr><tr id="45750154"><td></td></tr><tr id="45750017"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750017" href="https://news.ycombinator.com/vote?id=45750017&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Yet another reason to move away from Front Door.</p><p>We already had to do it for large files served from Blob Storage since they would cap out at 2MB/s when not in cache of the nearest PoP. If you’ve ever experienced slow Windows Store or Xbox downloads it’s probably the same problem.</p><p>I had a support ticket open for months about this and in the end the agent said “this is to be expected and we don’t plan on doing anything about it”.</p><p>We’ve moved to Cloudflare and not only is the performance great, but it costs less.</p><p>Only thing I need to move off Front Door is a static website for our docs served from Blob Storage, this incident will  make us do it sooner rather than later.</p></div></td></tr></tbody></table></td></tr><tr id="45750144"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45750144" href="https://news.ycombinator.com/vote?id=45750144&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>we are considering the same but because our website uses APEX domain we would need to move all DNS resolver to cloudfront right ? Does it have as a nice "rule set builder" as azure ?</p></div></td></tr></tbody></table></td></tr><tr id="45750531"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45750531" href="https://news.ycombinator.com/vote?id=45750531&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Unless you pay for CloudFlare’s Enterpise plan, you’re required to have them host your DNS zone, you can use a different registrar as long as you just point your NS records to Cloudflare.</p><p>Be aware that if you’re using Azure as your registrar, it’s (probably still) impossible to change your NS records to point to CloudFlare’s DNS server, at least it was for me about 6 months ago.</p><p>This also makes it impossible to transfer your domain to them either, as CloudFlare’s domain transfer flow requires you set your NS records to point to them before their interface shows a transfer option.</p><p>In our case we had to transfer to a different registrar, we used Namecheap.</p><p>However, transferring a domain from Azure was also a nightmare. Their UI doesn’t have any kind of transfer option, I eventually found an obscure document (not on their Learn website) which had an az command which would let you get a transfer code which I could give to Namecheap.</p><p>Then I had to wait over a week for the transfer timeout to occur because there is no way on Azure side that I could find to accept the transfer immediately.</p><p>I found CloudFlare’s way of building rules quite easy to use, different from Front Door but I’m not doing anything more complex than some redirects and reverse proxying.</p><p>I will say that Cloudflare’s UI is super fast, with Front Door I always found it painfully slow when trying to do any kind of configuration.</p><p>Cloudflare also doesn’t have the problem that Front Door has where it requires a manual process every 6 months or so to renew the APEX certificate.</p></div></td></tr></tbody></table></td></tr><tr id="45750733"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_45750733" href="https://news.ycombinator.com/vote?id=45750733&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Thanks :). We don't use Azure as our registrar. It seems I'll have to plan for this then, we also had another issue, AFD has a hard 500ms tls handshake timeout (doesn't matter how much you put on the origin timeout settings) which means if our server was slow for some reason we would get 504 origin timeout.</p></div></td></tr></tbody></table></td></tr><tr id="45750780"><td></td></tr><tr id="45750987"><td></td></tr><tr id="45749332"><td></td></tr><tr id="45750573"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750573" href="https://news.ycombinator.com/vote?id=45750573&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Sounds like they need to move their portal to a region with more capacity for the desired instance type.  /s</p></div></td></tr></tbody></table></td></tr><tr id="45749410"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749410" href="https://news.ycombinator.com/vote?id=45749410&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>I noticed that Starbucks mobile ordering was down and thought “welp, I guess I’ll order a bagel and coffee on Grubhub”, then GrubHub was down. My next stop was HN to find the common denominator, and y’all did not disappoint.</p></div></td></tr></tbody></table></td></tr><tr id="45749715"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749715" href="https://news.ycombinator.com/vote?id=45749715&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Good thing HN is hosted on a couple servers in a basement. Much more reliable than cloud, it seems!</p></div></td></tr></tbody></table></td></tr><tr id="45750608"><td></td></tr><tr id="45750648"><td></td></tr><tr id="45749980"><td></td></tr><tr id="45750668"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45750668" href="https://news.ycombinator.com/vote?id=45750668&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Yeah looks like they're back on M5.</p><p>dang saying it's temporary: <a href="https://news.ycombinator.com/item?id=32031136">https://news.ycombinator.com/item?id=32031136</a></p><pre><code>    $ dig news.ycombinator.com

    ; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; news.ycombinator.com
    ;; global options: +cmd
    ;; Got answer:
    ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 54819
    ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

    ;; OPT PSEUDOSECTION:
    ; EDNS: version: 0, flags:; udp: 512
    ;; QUESTION SECTION:
    ;news.ycombinator.com.  IN A

    ;; ANSWER SECTION:
    news.ycombinator.com. 1 IN A 209.216.230.207

    ;; Query time: 79 msec
    ;; SERVER: 100.100.100.100#53(100.100.100.100)
    ;; WHEN: Wed Oct 29 13:59:29 EDT 2025
    ;; MSG SIZE  rcvd: 65
</code></pre><p>
And that IP says it's with M5 again.</p></div></td></tr></tbody></table></td></tr><tr id="45749813"><td></td></tr><tr id="45751017"><td></td></tr><tr id="45749639"><td></td></tr><tr id="45749688"><td></td></tr><tr id="45749903"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45749903" href="https://news.ycombinator.com/vote?id=45749903&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>you wouldn't believe some of the crap enterprise bigco mgmt put in place for disaster recovery.</p><p>they think that they are 'eliminating a single point of failure', but in reality, they end up adding multiple, complicated points of mostly failure.</p></div></td></tr></tbody></table></td></tr><tr id="45749680"><td></td></tr><tr id="45749659"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749659" href="https://news.ycombinator.com/vote?id=45749659&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Gonna build my application to be multicloud so that it requires multiple cloud platforms to be online at the same time. The RAID 0 of cloud computing.</p></div></td></tr></tbody></table></td></tr><tr id="45749773"><td></td></tr><tr id="45750795"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45750795" href="https://news.ycombinator.com/vote?id=45750795&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>The sad thing is - $MSFT isn't even down by 1%. And IIRC, $AMZN actually went up during their previous outage.</p><p>So if we look at these companies' bottom lines, all those big wigs are actually doing something right. Sales and lobbying capacity is way more effective than reliability or good engineering (at least in the short term).</p></div></td></tr></tbody></table></td></tr><tr id="45750926"><td></td></tr><tr id="45749957"><td></td></tr><tr id="45750140"><td></td></tr><tr id="45750576"><td></td></tr><tr id="45749162"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749162" href="https://news.ycombinator.com/vote?id=45749162&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Currently standing in a half closed supermarket because the tills are down and they cant take payments</p></div></td></tr></tbody></table></td></tr><tr id="45749765"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749765" href="https://news.ycombinator.com/vote?id=45749765&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>There's a Family Dollar by my house that is down at least 2 full days per month because of bad inet connectivity. I live close enough that with a small tower on my roof i can get line of sight to theirs. I've thought about offering them a backup link off my home inet if they give me 50% of sales whenever its in use. It would be a pretty good deal for them, better some sales when their inet is down vs none.</p></div></td></tr></tbody></table></td></tr><tr id="45750027"><td></td></tr><tr id="45750202"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45750202" href="https://news.ycombinator.com/vote?id=45750202&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>2-3%, bit higher on perishables. Though i'd just ask lump sum payments in cash since it likely has to no go through corporate (as in, avoid the corporation).</p></div></td></tr></tbody></table></td></tr><tr id="45749986"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749986" href="https://news.ycombinator.com/vote?id=45749986&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>You'd think any SeriousBusiness would have a backup way to take customers' money. This is the one thing you always want to be able to do: accept payment. If they made it so they can't do that, they deserve the hit to their revenue. People should just walk out of the store with the goods if they're not being charged.</p><p>Why doesn't someone in the store at least have one of those manual kachunk-kachunk carbon copy card readers in the back that they can resuscitate for a few days until the technology is turned back on? Did they throw them all away?</p></div></td></tr></tbody></table></td></tr><tr id="45750494"><td></td></tr><tr id="45750371"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45750371" href="https://news.ycombinator.com/vote?id=45750371&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>I think a lot of payment terminals have an option to record transactions offline and upload them later, but apparently it's not enabled by default - probably because it increases your risk that someone pays with a bad card.</p></div></td></tr></tbody></table></td></tr><tr id="45750073"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45750073" href="https://news.ycombinator.com/vote?id=45750073&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>The kachunk-kachunk credit card machines need raised digits on the cards, and I don't think most banks have been issuing those for years at this point. Mine have been smooth for at least 10 years.</p></div></td></tr></tbody></table></td></tr><tr id="45750394"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45750394" href="https://news.ycombinator.com/vote?id=45750394&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Pretty sure it'd be a lot better deal for them to have no sales than to pay out 50% of sales on stuff with single digit margins.</p></div></td></tr></tbody></table></td></tr><tr id="45749805"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749805" href="https://news.ycombinator.com/vote?id=45749805&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>IIRC, the grocery chain I worked for used to have an offline mode to move customers out the door. But it meant that when the system came back online, if the customers card was denied, the customer got free groceries.</p></div></td></tr></tbody></table></td></tr><tr id="45750721"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45750721" href="https://news.ycombinator.com/vote?id=45750721&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>I remember that banks will try to honor the transactions, even if the customer's balance/credit limit is exhausted. It doesn't apply only to some gift cards.</p></div></td></tr></tbody></table></td></tr><tr id="45749516"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749516" href="https://news.ycombinator.com/vote?id=45749516&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Mind-boggling that any retailer would not have the capability to at least run the checkout stations offline.</p></div></td></tr></tbody></table></td></tr><tr id="45749620"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749620" href="https://news.ycombinator.com/vote?id=45749620&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>I knew an old guy in the '00s who specialized in cobal/fortran for working on tiller software. Guess he retired and they couldn't maintain it</p></div></td></tr></tbody></table></td></tr><tr id="45749752"><td></td></tr><tr id="45749820"><td></td></tr><tr id="45750048"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750048" href="https://news.ycombinator.com/vote?id=45750048&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>That specific subdomain has issues with propagation: <a href="https://dnschecker.org/#A/answers.microsoft.com" rel="nofollow">https://dnschecker.org/#A/answers.microsoft.com</a> (only four resolvers return records)</p><p>The root zone and www. do not: <a href="https://dnschecker.org/#A/microsoft.com" rel="nofollow">https://dnschecker.org/#A/microsoft.com</a> (all resolvers return records)</p><p>And querying <a href="https://www.microsoft.com/" rel="nofollow">https://www.microsoft.com/</a> results in HTTP 200 on the root document, but the page elements return errors (a 504 on the .css/.js documents, a 404 on some fonts, Name Not Resolved on scripts.clarity.ms, Connection Timed Out on wcpstatic.microsoft.com and mem.gfx.ms). That many different kinds of errors is actually kind of impressive.</p><p>I'm gonna say this was a networking/routing issue. The CDN stayed up, but everything else non-CDN became unroutable, and different requests traveled through different paths/services, but each eventually hit the bad network path, and that's what created all the different responses. Could also have been a bad deploy or a service stopped running and there's different things trying to access that service in different ways, leading to the weird responses... but that wouldn't explain the failed DNS propagation.</p></div></td></tr></tbody></table></td></tr><tr id="45750405"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750405" href="https://news.ycombinator.com/vote?id=45750405&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>wow, right after AWS suffered a similar thing.</p><p>I wonder if this is microsoft "learning" to "prevent" such an issue and instead triggered it...</p><p>"One often meets his destiny on the path he takes to avoid it" -- Master Oogway</p></div></td></tr></tbody></table></td></tr><tr id="45750309"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45750309" href="https://news.ycombinator.com/vote?id=45750309&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Yeah just took down the prod site for one of our clients since we host the front-end out of their CDN. Just got wrapped up panic hosting it somewhere else for the past hour, very quickly reminds you about the pain of cookies...</p></div></td></tr></tbody></table></td></tr><tr id="45750535"><td></td></tr><tr id="45748903"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45748903" href="https://news.ycombinator.com/vote?id=45748903&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Ouch, and login.microsoftonline.com too - i.e. SSO using MS accounts. We'd just rolled that out across most (all?) of our internal systems...</p><p>And microsoft.com too - that's gotta hurt</p></div></td></tr></tbody></table></td></tr><tr id="45749507"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749507" href="https://news.ycombinator.com/vote?id=45749507&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>SSO and 365 are working fine for us, but admin portals for Azure/365 are down. Our workloads in Azure don't seem to be impacted.</p></div></td></tr></tbody></table></td></tr><tr id="45749902"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749902" href="https://news.ycombinator.com/vote?id=45749902&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>It is interesting to see the differential across different tenants in different geographies:</p><p>- on a US tenant I am unable to access login.microsoftonline.com and the login flow stalls on any SSO authentication attempt.</p><p>- on a European tenant, probably germany-west, I am able to login and access the Azure portal.</p></div></td></tr></tbody></table></td></tr><tr id="45749907"><td></td></tr><tr id="45749484"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749484" href="https://news.ycombinator.com/vote?id=45749484&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>I am still stunned people choose to do this, considering major Office 365 outages are basically a weekly thing now.</p></div></td></tr></tbody></table></td></tr><tr id="45749727"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749727" href="https://news.ycombinator.com/vote?id=45749727&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>We are very dependent on Azure and Microsoft Authentication and Microsoft 365 and haven’t had weekly or even monthly issues. I can think of maybe three issues this year.</p></div></td></tr></tbody></table></td></tr><tr id="45750741"><td></td></tr><tr id="45750150"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45750150" href="https://news.ycombinator.com/vote?id=45750150&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>I’ve been migrating our services off of Azure slowly for the past couple of years. The last internet facing things remaining are a static assets bucket and an analytics VM running Matomo. Working with Front Door has been an abysmal experience, and today was the push I needed to finally migrate our assets to Cloudflare.</p><p>I feel pretty justified in my previous decisions to move away from Azure. Using it feels like building on quicksand…</p></div></td></tr></tbody></table></td></tr><tr id="45750589"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750589" href="https://news.ycombinator.com/vote?id=45750589&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>All the clouds hav had major outages this year.</p><p>At this point I dont believe that any one of them is any better or reliable than the others.</p></div></td></tr></tbody></table></td></tr><tr id="45748806"><td></td></tr><tr id="45748839"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45748839" href="https://news.ycombinator.com/vote?id=45748839&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>That status page is never red. Absolutely useless.</p><p>&gt; There are currently no active events. Use <i>Azure Service Health</i> to view other issues that may be impacting your services.</p><p>Links to a page on Azure Portal which is down...</p></div></td></tr></tbody></table></td></tr><tr id="45749482"><td></td></tr><tr id="45749792"><td></td></tr><tr id="45750821"><td></td></tr><tr id="45749163"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749163" href="https://news.ycombinator.com/vote?id=45749163&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>They added a message at the same time as your comment:</p><p>"We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly."</p></div></td></tr></tbody></table></td></tr><tr id="45750892"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45750892" href="https://news.ycombinator.com/vote?id=45750892&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>I was working when I saw the portal page showing only resource groups and lots of items missing. I thought it was a weird browser cache issue.</p><p>The actual stuff I was working on (App Insights, Function App) that was still open was operational.</p></div></td></tr></tbody></table></td></tr><tr id="45749347"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749347" href="https://news.ycombinator.com/vote?id=45749347&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>For us, it looks like most services are still working (eastus and eastus2). Our AKS cluster is still running and taking requests. Failures seem limited to management portal.</p></div></td></tr></tbody></table></td></tr><tr id="45750652"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45750652" href="https://news.ycombinator.com/vote?id=45750652&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>So much of Belgium runs on Azure… it's honestly baffling how many services are down, there's no resilience built into (even large) companies anymore.</p></div></td></tr></tbody></table></td></tr><tr id="45749788"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749788" href="https://news.ycombinator.com/vote?id=45749788&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>The Internet is supposed to be decentralized. The big three seem to have all the power now (Amazon, Microsoft, and Google) plus Cloudflare/Oracle.</p><p>How did we get here? Is it because of scale? Going to market in minutes by using someone else's computers instead of building out your own, like co-location or dedicated servers, like back in the day.</p></div></td></tr></tbody></table></td></tr><tr id="45749826"><td></td></tr><tr id="45749905"><td></td></tr><tr id="45749946"><td></td></tr><tr id="45750949"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750949" href="https://news.ycombinator.com/vote?id=45750949&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>From today [0].</p><p>&gt; Big Tech lobbying is riding the EU’s deregulation wave by spending more, hiring more, and pushing more, according to a new report by NGO’s Corporate Europe Observatory and LobbyControl on Wednesday (29 October).</p><p>&gt; Based on data from the EU’s transparency register, the NGOs found that tech companies spend the most on lobbying of any sector, spending €151m a year on lobbying — a 33 percent increase from €113m in 2023.</p><p>Gee whizz, I really do wonder how they end up having all the power!</p><p>[0] <a href="https://news.ycombinator.com/item?id=45744973">https://news.ycombinator.com/item?id=45744973</a></p></div></td></tr></tbody></table></td></tr><tr id="45749866"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749866" href="https://news.ycombinator.com/vote?id=45749866&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>A lot of money and years of marketing the cloud as the responsible business decision led us here. Now that the cloud providers have vendor lock-in, few will leave, and customers will continue to wildly overpay for cloud services.</p></div></td></tr></tbody></table></td></tr><tr id="45749985"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749985" href="https://news.ycombinator.com/vote?id=45749985&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Ahh, but you forget what it <i>used</i> to be like. Sites used to go down <i>all the time.</i></p><p>Now, they go down a lot less frequently, but when they do, it's more widespread.</p></div></td></tr></tbody></table></td></tr><tr id="45750549"><td></td></tr><tr id="45750629"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45750629" href="https://news.ycombinator.com/vote?id=45750629&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Not sure how the current situation is better. Being stranded with no way whatsoever to access most/all of your services sounds way more terrifying than regular issues limited to a couple of services at a time</p></div></td></tr></tbody></table></td></tr><tr id="45750937"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_45750937" href="https://news.ycombinator.com/vote?id=45750937&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>&gt; no way whatsoever to access most/all of your services</p><p>I work on a product hosted on Azure. That's not the case. Except for front door, everything else is running fine. (Front door is a reverse proxy for static web sites.)</p><p>The product itself (an iot stormwater management system) is running, but our customers just can't access the website. If they need to do something, they can go out to the sites or call us and we can "rub two sticks together" and bypass the website. (We could also bypass front door if someone twisted our arms.)</p><p>Most customers only look at the website a few times a year.</p><p>---</p><p>That being said, our biggest point of failure is a completely different iot vendor who you probably won't hear about on Hacker News when they, or their data networks, have downtime.</p></div></td></tr></tbody></table></td></tr><tr id="45749853"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749853" href="https://news.ycombinator.com/vote?id=45749853&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Thats the whole point, big players like AWS and MS can go down, but here we are still talking on the internet.</p><p>Decentralisation is winning it seems.</p></div></td></tr></tbody></table></td></tr><tr id="45750570"><td></td></tr><tr id="45750078"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750078" href="https://news.ycombinator.com/vote?id=45750078&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>&gt; How did we get here?</p><p>I think the response lies in the surrounding ecosystem.</p><p>If you have a company it's easier to scale your team if you use AWS (or any other established ecosystem). It's way easier to hire 10 engineers that are competent with AWS tools than it is to hire 10 engineers that are competent with the IBM tools.</p><p>And from the individuals perspective it also make sense to bet on larger platforms. If you want to increase your odds of getting a new job, learning the AWS tools gives you a better ROI than learning the IBM tools.</p></div></td></tr></tbody></table></td></tr><tr id="45750500"><td></td></tr><tr id="45750326"><td></td></tr><tr id="45750317"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750317" href="https://news.ycombinator.com/vote?id=45750317&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>A natural monopoly is a monopoly in an industry in which high infrastructure costs and other barriers to entry relative to the size of the market give the largest supplier in an industry, often the first supplier in a market, an overwhelming advantage over potential competitors. Specifically, an industry is a natural monopoly if a single firm can supply the entire market at a lower long-run average cost than if multiple firms were to operate within it. In that case, it is very probable that a company (monopoly) or a minimal number of companies (oligopoly) will form, providing all or most of the relevant products and/or services.</p><p><a href="https://en.wikipedia.org/wiki/Natural_monopoly" rel="nofollow">https://en.wikipedia.org/wiki/Natural_monopoly</a></p></div></td></tr></tbody></table></td></tr><tr id="45749871"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749871" href="https://news.ycombinator.com/vote?id=45749871&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Consolidation is the inevitable outcome of free unregulated markets.</p><p>In our highly interconnected world, decentralization paradoxically requires a central authority to enforce decentralization by restricting M&amp;A, cartels, etc.</p></div></td></tr></tbody></table></td></tr><tr id="45750198"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45750198" href="https://news.ycombinator.com/vote?id=45750198&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Is there a theorem that models this behavior? Capital feels like a mass that attracts more mass the larger it becomes, like gravity.</p></div></td></tr></tbody></table></td></tr><tr id="45750084"><td></td></tr><tr id="45751023"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45751023" href="https://news.ycombinator.com/vote?id=45751023&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>An important quality of the cloud is that it is always available.</p><p>Except that it is not!</p><p>Interesting times...</p></div></td></tr></tbody></table></td></tr><tr id="45749237"><td></td></tr><tr id="45749738"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749738" href="https://news.ycombinator.com/vote?id=45749738&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>I've been doing it since 1998 in my bedroom with a dual T1 (and on to real DCs later). While I've had some outages for sure it makes me feel better I am not that divergent in uptime in the long run vs big clouds.</p></div></td></tr></tbody></table></td></tr><tr id="45749988"><td></td></tr><tr id="45750287"><td></td></tr><tr id="45750406"><td></td></tr><tr id="45748997"><td></td></tr><tr id="45749477"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749477" href="https://news.ycombinator.com/vote?id=45749477&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Pretty much all Azure services seem to be down. Their status page says it's only the portal since 16:00. It would be nice if these mega-companies could update their status page when they take down a large fraction of the Internet and thousands of services that use them.</p></div></td></tr></tbody></table></td></tr><tr id="45749505"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749505" href="https://news.ycombinator.com/vote?id=45749505&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>FWIW, all of our databases, VMs, AKS clusters, services, jobs etc - are all working fine. Which services are down for you, maybe we can build a list?</p></div></td></tr></tbody></table></td></tr><tr id="45749567"><td></td></tr><tr id="45749621"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749621" href="https://news.ycombinator.com/vote?id=45749621&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>All of our Azure workloads are up, but we don't use Azure Front Door. That seems to be the only impacted product, apart from the management portal.</p></div></td></tr></tbody></table></td></tr><tr id="45749993"><td></td></tr><tr id="45749913"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749913" href="https://news.ycombinator.com/vote?id=45749913&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Same playbook for AWS. When they admitted that Dynamo was inaccessible, they failed to provide context that their internal services are heavily dependent on Dynamo</p><p>It's only after the fact they are transparent about the impact</p></div></td></tr></tbody></table></td></tr><tr id="45749437"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749437" href="https://news.ycombinator.com/vote?id=45749437&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>High availability is touted as a reason for their high prices, but I swear I read about major cloud outages far more than I experience any outages at Hetzner.</p></div></td></tr></tbody></table></td></tr><tr id="45750524"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750524" href="https://news.ycombinator.com/vote?id=45750524&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>I think the biggest features of the big cloud vendors is that when they are down, not only you but your customers and your competitors usually have issues at the same time so everybody just shrug and have a lazy/off day at the same time. Even on call teams reall just have to wait and stay on standby because there is very little they can do. Doing a failover can be slower than waiting for the recovery, not help at all if outage is spanned accross several region, or bring aditional risks.</p><p>And more importantly nobody lose any reputation except AWS/Azure/Google.</p></div></td></tr></tbody></table></td></tr><tr id="45750555"><td></td></tr><tr id="45749998"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749998" href="https://news.ycombinator.com/vote?id=45749998&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Ostensible reason.</p><p>The real reason is that outages are not your fault. Its the new version of "nobody ever got fired for buying IBM" - later it became MS, and now its any big cloud provider.</p></div></td></tr></tbody></table></td></tr><tr id="45750439"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750439" href="https://news.ycombinator.com/vote?id=45750439&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>For one it’s statistics - Hetzner simply runs far fewer major services than hyperscalers. And the services they run are also more affluent, with larger customer bases, so downtimes are systemically critical. Therefore it’s louder.</p><p>On the merits though, I agree, haven’t had any serious issues with Hetzner.</p></div></td></tr></tbody></table></td></tr><tr id="45749453"><td></td></tr><tr id="45750156"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45750156" href="https://news.ycombinator.com/vote?id=45750156&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>DO has been shockingly reliable for me. I shut down a neglected box almost 900 days uptime the other day. In that time AWS has randomly dropped many of my boxes with no warning requiring a manual stop/start action to recover them... But everybody keeps telling me that DO isn't "as reliable" as the big three are.</p></div></td></tr></tbody></table></td></tr><tr id="45750548"><td></td></tr><tr id="45749488"><td></td></tr><tr id="45750700"><td></td></tr><tr id="45750321"><td></td></tr><tr id="45750673"><td></td></tr><tr id="45750732"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45750732" href="https://news.ycombinator.com/vote?id=45750732&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>It looks like it is just the 365 admin panels for us. Admittedly, we don't currently host any other services on Azure though.</p></div></td></tr></tbody></table></td></tr><tr id="45749504"><td></td></tr><tr id="45749575"><td></td></tr><tr id="45749859"><td></td></tr><tr id="45749979"><td><table><tbody><tr><td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td><center><a id="up_45749979" href="https://news.ycombinator.com/vote?id=45749979&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>we use front door (as does miccrosoft.com) and our website was down, I was able to change the DNS records to point directly to our server and will leave it like that for a few hours until everything is green</p></div></td></tr></tbody></table></td></tr><tr id="45750088"><td></td></tr><tr id="45750625"><td></td></tr><tr id="45749790"><td></td></tr><tr id="45750178"><td></td></tr><tr id="45750313"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45750313" href="https://news.ycombinator.com/vote?id=45750313&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Do Microsoft still say "If the government has a broader voluntary national security program to gather customer data, we don't participate in it" today (which PRISM proved very false), or are they at least acknowledging they're participating in whatever NSA has deployed today?</p></div></td></tr></tbody></table></td></tr><tr id="45750437"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45750437" href="https://news.ycombinator.com/vote?id=45750437&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>PRISM wasn't voluntary.  Also there are 3 levels here:</p><p>1. Mandatory</p><p>2. "Voluntary"</p><p>3. Voluntary</p><p>And I suspect that very little of what the NSA does falls into category 3.  As Sen Chuck Schumer put it "you take on the intelligence community, they have six ways from Sunday at getting back at you"</p></div></td></tr></tbody></table></td></tr><tr id="45750754"><td></td></tr><tr id="45750200"><td></td></tr><tr id="45750104"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750104" href="https://news.ycombinator.com/vote?id=45750104&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>This is funny but also possibly true because: business/MBA types see these outages as a way to prove how critical some services are, leading to investors deciding to load up on the vendor's stock.</p></div></td></tr></tbody></table></td></tr><tr id="45750196"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45750196" href="https://news.ycombinator.com/vote?id=45750196&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>I may or may not have been known to temporarily take a database down in the past to make a point to management about how unreliable some old software is.</p></div></td></tr></tbody></table></td></tr><tr id="45750749"><td></td></tr><tr id="45749553"><td></td></tr><tr id="45749027"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749027" href="https://news.ycombinator.com/vote?id=45749027&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Updated 16:35 UTC</p><p>Azure Portal Access Issues</p><p>Starting at approximately 16:00 UTC, we began experiencing DNS issues resulting in availability degradation of some services. Customers may experience issues accessing the Azure Portal. We have taken action that is expected to address the portal access issues here shortly. We are actively investigating the underlying issue and additional mitigation actions. More information will be provided within 60 minutes or sooner.</p><p>This message was last updated at 16:35 UTC on 29 October 2025</p><p>----</p><p>Azure Portal Access Issues</p><p>We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly.</p><p>This message was last updated at 16:18 UTC on 29 October 2025</p><p>-- From the Azure status page</p></div></td></tr></tbody></table></td></tr><tr id="45751001"><td></td></tr><tr id="45748946"><td></td></tr><tr id="45748965"><td></td></tr><tr id="45749033"><td></td></tr><tr id="45749056"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749056" href="https://news.ycombinator.com/vote?id=45749056&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>now there is an information about "Azure Portal Access Issues". No word about front door being down.</p></div></td></tr></tbody></table></td></tr><tr id="45749082"><td></td></tr><tr id="45749888"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749888" href="https://news.ycombinator.com/vote?id=45749888&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>&gt; Even the national digital id service is down.</p><p>Can't help but smirk as my country is ramming through "Digital ID" right now</p></div></td></tr></tbody></table></td></tr><tr id="45749206"><td></td></tr><tr id="45749292"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749292" href="https://news.ycombinator.com/vote?id=45749292&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Always fun when you can't trust the main status page but have to go to some opinionated social medial website to see the actual problem.</p></div></td></tr></tbody></table></td></tr><tr id="45750220"><td></td></tr><tr id="45750367"><td></td></tr><tr id="45750613"><td></td></tr><tr id="45750357"><td></td></tr><tr id="45750583"><td></td></tr><tr id="45749269"><td></td></tr><tr id="45749501"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749501" href="https://news.ycombinator.com/vote?id=45749501&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>On our end, our VMs are still working, so our gitlab instance is still up. Our services using Azure App Services are available through their provided url. However, Front Door is failing to resolve any domains that it was responsible for.</p></div></td></tr></tbody></table></td></tr><tr id="45749961"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749961" href="https://news.ycombinator.com/vote?id=45749961&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>They admit in their update blurb azure front door is having issues but still report azure front door as having no issues on their status page.</p><p>And it's very clear from these updates that they're more focused on the portal than the product, their updates haven't even mentioned fixing it yet, just moving off of it, as if it's some third party service that's down.</p></div></td></tr></tbody></table></td></tr><tr id="45750107"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750107" href="https://news.ycombinator.com/vote?id=45750107&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>&gt; as having no issues on their status page</p><p>Unsubstantiated idea: So the support contract likely says there is a window between each reporting step and the status page is the last one and the one in the legal documents giving them several more hours before the clauses trigger.</p></div></td></tr></tbody></table></td></tr><tr id="45749521"><td></td></tr><tr id="45749963"><td></td></tr><tr id="45749660"><td></td></tr><tr id="45749728"><td></td></tr><tr id="45749657"><td></td></tr><tr id="45749901"><td></td></tr><tr id="45749481"><td></td></tr><tr id="45749627"><td></td></tr><tr id="45748933"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45748933" href="https://news.ycombinator.com/vote?id=45748933&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Portal and Azure CDN are down here in the SF Bay Area. Tenant azureedge.net DNS A queries are taking 2-6 seconds and most often return nothing. I got a couple successful A response in the last 10 minutes.</p><p>Edit: As of 9:19 AM Pacific time, I'm now getting successful A responses but they can take several seconds. The web server at that address is not responding.</p></div></td></tr></tbody></table></td></tr><tr id="45749229"><td></td></tr><tr id="45749280"><td></td></tr><tr id="45750254"><td></td></tr><tr id="45749404"><td></td></tr><tr id="45749552"><td></td></tr><tr id="45749606"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_45749606" href="https://news.ycombinator.com/vote?id=45749606&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>I'd say DNS/Front Door (or some carrier interconnect) is the thing affected, since I can auth just fine in a few places. (I'm at MS, but not looped into anything operational these days, so I'm checking my personal subscription).</p></div></td></tr></tbody></table></td></tr><tr id="45749398"><td></td></tr><tr id="45750764"><td></td></tr><tr id="45750148"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45750148" href="https://news.ycombinator.com/vote?id=45750148&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>All of my employers things are hosted on Azure and running just fine and didn't go down at all. Portal access has been fixed.</p><p>Doesn't seem to be too bad of an outage unless you were relying on Azure Front Door.</p></div></td></tr></tbody></table></td></tr><tr id="45750194"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750194" href="https://news.ycombinator.com/vote?id=45750194&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>SSO is down, Azure Portal Down and more, seems like a major outage. Already a lot of services seem to be affected: banks, airlines, consumer apps, etc.</p></div></td></tr></tbody></table></td></tr><tr id="45750226"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45750226" href="https://news.ycombinator.com/vote?id=45750226&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>The portal is up for me and their status page confirms they did a failover for it. Definitely not disputing that its reach is wide, but a lot of smaller setups probably aren't using Front Door.</p></div></td></tr></tbody></table></td></tr><tr id="45750256"><td></td></tr><tr id="45750403"><td></td></tr><tr id="45748842"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45748842" href="https://news.ycombinator.com/vote?id=45748842&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>We saw all incoming traffic to our app drop to zero at about 15:45. I wonder how long this one will take to fix.</p></div></td></tr></tbody></table></td></tr><tr id="45748897"><td></td></tr><tr id="45748863"><td></td></tr><tr id="45750778"><td></td></tr><tr id="45751027"><td></td></tr><tr id="45748746"><td></td></tr><tr id="45749374"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749374" href="https://news.ycombinator.com/vote?id=45749374&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>“ Starting at approximately 16:00 UTC, we began experiencing DNS issues resulting in availability degradation of some services. Customers may experience issues accessing the Azure Portal. We have taken action that is expected to address the portal access issues here shortly. We are actively investigating the underlying issue and additional mitigation actions. More information will be provided within 60 minutes or sooner.</p><p>This message was last updated at 16:35 UTC on 29 October 2025”</p></div></td></tr></tbody></table></td></tr><tr id="45749068"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749068" href="https://news.ycombinator.com/vote?id=45749068&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Looks like MyGet is impacted too. Seems like they use Azure:</p><p>&gt;What is required to be able to use MyGet? ... MyGet runs its operations from the Microsoft Azure in the West Europe region, near Amsterdam, the Netherlands.</p></div></td></tr></tbody></table></td></tr><tr id="45749204"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749204" href="https://news.ycombinator.com/vote?id=45749204&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>i guess folks in azure wanted to show some solidarity with aws brethren</p><p>(couldn't resist adding it. i acknowledge this comment adds no value to the discussion)</p></div></td></tr></tbody></table></td></tr><tr id="45749613"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749613" href="https://news.ycombinator.com/vote?id=45749613&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Azure goes down all the time. On Friday we had an entire regional service down all day. Two weeks ago same thing different region. You only hear about it when it's something everyone uses like the portal, because in general nobody uses Azure unless they're held hostage.</p></div></td></tr></tbody></table></td></tr><tr id="45749672"><td></td></tr><tr id="45750217"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45750217" href="https://news.ycombinator.com/vote?id=45750217&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Wasn't the saying "It's always DNS" floating around somewhere?</p><p>Be interesting to understand cause here. Pretty big impact on services we use</p></div></td></tr></tbody></table></td></tr><tr id="45749543"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749543" href="https://news.ycombinator.com/vote?id=45749543&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>&gt;Last week AWS, now this.</p><p>This is not the first or second time this happened, multiple Hyperscaler failed one by one.</p></div></td></tr></tbody></table></td></tr><tr id="45750057"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45750057" href="https://news.ycombinator.com/vote?id=45750057&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>I absolutely love the utility aspect of LLMs but part of me is curious if moving faster by using AI is going to make these sorts of failure more and more often.</p></div></td></tr></tbody></table></td></tr><tr id="45750162"><td></td></tr><tr id="45751013"><td></td></tr><tr id="45749413"><td></td></tr><tr id="45749438"><td></td></tr><tr id="45749612"><td></td></tr><tr id="45749721"><td></td></tr><tr id="45750452"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_45750452" href="https://news.ycombinator.com/vote?id=45750452&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>It begs the question from a noob like me... Where should they host the status page? Surely it shouldn't be on the same infra that it's supposed to be monitoring. Am I correct in thinking that?</p></div></td></tr></tbody></table></td></tr><tr id="45751025"><td><table><tbody><tr><td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td><center><a id="up_45751025" href="https://news.ycombinator.com/vote?id=45751025&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>My old company used to host it at AWS. There was no other reliance on AWS besides the outage page and internal outage communication, everything else was self-hosted.</p></div></td></tr></tbody></table></td></tr><tr id="45749449"><td></td></tr><tr id="45749077"><td></td></tr><tr id="45749701"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749701" href="https://news.ycombinator.com/vote?id=45749701&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>It is much more than azure. One of my kids needs a key for their laptop and can't reach that either. Great excuse though, 'Azure ate my homework'. What a ridiculous world we are building. Fuck MS and their account requirements for windows.</p></div></td></tr></tbody></table></td></tr><tr id="45750236"><td></td></tr><tr id="45748826"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45748826" href="https://news.ycombinator.com/vote?id=45748826&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>This is impacting the Azure CDN at azureedge.net. DNS A records for azureedge.net tenants are taking 2-6 seconds and often return nothing.</p></div></td></tr></tbody></table></td></tr><tr id="45750594"><td></td></tr><tr id="45749142"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749142" href="https://news.ycombinator.com/vote?id=45749142&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>I'm mid-deployment, but thankfully it seems to be running ok so far. Just the portal is not working so my visibility is not good.</p></div></td></tr></tbody></table></td></tr><tr id="45749747"><td></td></tr><tr id="45750904"><td></td></tr><tr id="45749736"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749736" href="https://news.ycombinator.com/vote?id=45749736&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Does (should, could) DownDetector also say what customer-facing services are down, when some infrastructure is unworking?  Or is that the info that the malefactors are seeking?</p></div></td></tr></tbody></table></td></tr><tr id="45748937"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45748937" href="https://news.ycombinator.com/vote?id=45748937&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Unable to access the portal and any hit to SSO for other corporate accesses is also broken. Seems like there's something wrong in their Identity services.</p></div></td></tr></tbody></table></td></tr><tr id="45749093"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749093" href="https://news.ycombinator.com/vote?id=45749093&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Oh, well, I'm <i>sure</i> Azure will be given the same pass that AWS got here recently when they had their 12-hour outage...</p></div></td></tr></tbody></table></td></tr><tr id="45749286"><td></td></tr><tr id="45750012"><td></td></tr><tr id="45750264"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45750264" href="https://news.ycombinator.com/vote?id=45750264&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Apologies, but this just reads like a low effort critique of big things.</p><p>To be clear, they should get criticism.  They should be held liable for any damage they cause.</p><p>But that they remain the biggest cloud offering out there isn't something you'd expect to change from a few outages that, by most all evidence, potential replacements have, as well?  More, a lot of the outages potential replacements have are often more global in nature.</p></div></td></tr></tbody></table></td></tr><tr id="45750614"><td></td></tr><tr id="45750554"><td></td></tr><tr id="45750014"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45750014" href="https://news.ycombinator.com/vote?id=45750014&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>pretty interesting how datadog's uptime tracker (<a href="https://updog.ai/" rel="nofollow">https://updog.ai/</a>) says all the sites are fully available.</p><p>if that's true then it's a sign that Azure's control / data plane separation is doing it's job! at least for now</p></div></td></tr></tbody></table></td></tr><tr id="45750266"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750266" href="https://news.ycombinator.com/vote?id=45750266&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Our Azure hosted dotnet App Service is working fine, but our docs site served via Front Door went down. Can’t access anything through the Portal.</p></div></td></tr></tbody></table></td></tr><tr id="45750335"><td></td></tr><tr id="45749881"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749881" href="https://news.ycombinator.com/vote?id=45749881&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>LinkedIn has been acting funny for an hour or so, and some pages in the learn.microsoft.com domain have been failing for me too...</p></div></td></tr></tbody></table></td></tr><tr id="45749080"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749080" href="https://news.ycombinator.com/vote?id=45749080&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Our Azure DevOps site is still functioning and our Azure hosted databases are accessible. Everything else is cooked.</p></div></td></tr></tbody></table></td></tr><tr id="45749468"><td></td></tr><tr id="45749906"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749906" href="https://news.ycombinator.com/vote?id=45749906&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>This is because Azure just copies everything AWS does. Google is a bit more innovative, they will have something else unexpected happen.</p></div></td></tr></tbody></table></td></tr><tr id="45750118"><td></td></tr><tr id="45749678"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749678" href="https://news.ycombinator.com/vote?id=45749678&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Maybe they are and no one realized yet.. :P</p><p>That said, I don't hear about GCP outages all that often. I do think AWS might be leading in outages, but that's a gut feeling, I didn't look up numbers.</p></div></td></tr></tbody></table></td></tr><tr id="45750070"><td></td></tr><tr id="45749885"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749885" href="https://news.ycombinator.com/vote?id=45749885&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>fairly certain they had a significant multi region outage within the past few years. I'll try to find some details to link.</p><p>Few customers....few voices to complain as well.</p></div></td></tr></tbody></table></td></tr><tr id="45749967"><td></td></tr><tr id="45749818"><td></td></tr><tr id="45749536"><td></td></tr><tr id="45749691"><td></td></tr><tr id="45749411"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749411" href="https://news.ycombinator.com/vote?id=45749411&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>I am having a bunch of issues. It looks like their sites and azure are both affected.</p><p>I also got weird notification in VS2022 that my license key was upgraded to Enterprise, but we did not purchase anything.</p></div></td></tr></tbody></table></td></tr><tr id="45749648"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749648" href="https://news.ycombinator.com/vote?id=45749648&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Might be a failsafe, if you cant get a license status, and you're aware that MS is down, just default to the highest tier.</p></div></td></tr></tbody></table></td></tr><tr id="45749524"><td></td></tr><tr id="45750183"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45750183" href="https://news.ycombinator.com/vote?id=45750183&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>vscode.dev appears to be down.  I think this will be my excuse to find an alternative -- I never really liked vscode.dev anyway.</p><p>(Coder is currently at the top of the experiment list.  Any other suggestions?)</p></div></td></tr></tbody></table></td></tr><tr id="45749878"><td></td></tr><tr id="45749875"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749875" href="https://news.ycombinator.com/vote?id=45749875&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>MS website seems to be up but really slow. Think xbox might still be down, Bing works for some reason tho!?</p></div></td></tr></tbody></table></td></tr><tr id="45749053"><td></td></tr><tr id="45749205"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749205" href="https://news.ycombinator.com/vote?id=45749205&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Quite close to the recent AWS outage. Let me take a look if its a major one similar to AWS.</p><p>Any guess on what's causing it?</p><p>In hindsight, I guess the foresight of some organizations to go multi-cloud was correct after all.</p></div></td></tr></tbody></table></td></tr><tr id="45749784"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749784" href="https://news.ycombinator.com/vote?id=45749784&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>We're multi-cloud and it really saved a few workloads last week with the AWS issue.</p><p>It's not easy though.</p></div></td></tr></tbody></table></td></tr><tr id="45749472"><td></td></tr><tr id="45749228"><td></td></tr><tr id="45749302"><td></td></tr><tr id="45749493"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749493" href="https://news.ycombinator.com/vote?id=45749493&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Yeah, these things never happened when humans were trusted without sufficient review and oversight of changes to production.</p></div></td></tr></tbody></table></td></tr><tr id="45749386"><td></td></tr><tr id="45749434"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45749434" href="https://news.ycombinator.com/vote?id=45749434&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>I don't think it's meant to be serious. It's a comment on Microsoft laying off their staff and stuffing their Azure and Dotnet teams with AI product managers.</p></div></td></tr></tbody></table></td></tr><tr id="45749070"><td></td></tr><tr id="45748968"><td></td></tr><tr id="45750360"><td></td></tr><tr id="45749018"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749018" href="https://news.ycombinator.com/vote?id=45749018&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Azure portal currently mostly not working (UK)... Downdetector reporting various Microsoft linked services are out (Minecraft, Microsoft 365, Xbox...)</p></div></td></tr></tbody></table></td></tr><tr id="45750209"><td></td></tr><tr id="45750805"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45750805" href="https://news.ycombinator.com/vote?id=45750805&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>So that's why all of our municipality's digital services are down ... utter chaos at the political meeting I attended just now.</p></div></td></tr></tbody></table></td></tr><tr id="45748961"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45748961" href="https://news.ycombinator.com/vote?id=45748961&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>downdetector reports coincident cloudflare outage. is microsoft using cloudflare for management plane, or is there common infra? data center problem somewhere, maybe fiber backbone? BGP?</p></div></td></tr></tbody></table></td></tr><tr id="45750186"><td></td></tr><tr id="45749817"><td></td></tr><tr id="45750935"><td></td></tr><tr id="45750587"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750587" href="https://news.ycombinator.com/vote?id=45750587&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p><i>Reports of Azure and AWS down on the same day? Infrastructure terrorism?</i></p><p>&gt; We have confirmed that an inadvertent configuration change as the trigger event for this issue.</p><p>Save the speculation for Reddit.  HN is better than that.</p></div></td></tr></tbody></table></td></tr><tr id="45749500"><td></td></tr><tr id="45749768"><td></td></tr><tr id="45749186"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749186" href="https://news.ycombinator.com/vote?id=45749186&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Appears to be an issue in Front Door. Our back end stuff is fine but FD is bouncing everything.</p></div></td></tr></tbody></table></td></tr><tr id="45749266"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749266" href="https://news.ycombinator.com/vote?id=45749266&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Yeah, I have non prod environments that don't use FD that are functioning. Routing through FD does not work. And a different app, nonprod doesn't use FD (and is working) but loads assets from the CDN (which is not working).</p><p>FD and CDN are global resources and are experiencing issues. Probably some other global resources as well.</p><p>Hate to say it, but DNS is looking like it's still the undisputed champ.</p></div></td></tr></tbody></table></td></tr><tr id="45749215"><td></td></tr><tr id="45749369"><td></td></tr><tr id="45749351"><td></td></tr><tr id="45749395"><td></td></tr><tr id="45749921"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749921" href="https://news.ycombinator.com/vote?id=45749921&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>You just paste the outage error codes back to the LLM and pray it's still working and can fix whatever went wrong!</p></div></td></tr></tbody></table></td></tr><tr id="45750109"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45750109" href="https://news.ycombinator.com/vote?id=45750109&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>When all the people forget to code for themselves, every LLM will code itself out of existence with that one last bug. One, after another.</p></div></td></tr></tbody></table></td></tr><tr id="45749512"><td></td></tr><tr id="45749922"><td></td></tr><tr id="45748680"><td></td></tr><tr id="45748699"><td></td></tr><tr id="45749494"><td></td></tr><tr id="45750066"><td></td></tr><tr id="45750590"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45750590" href="https://news.ycombinator.com/vote?id=45750590&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>When you look at the scale of the reports, you find they are much lower than Azure's.  seeing a bunch of 24-hour sparkline type graphs next to each other can make it look like they are equally impacted, but AWS has 500 reports and Azure has 20,000.  The scale is hidden by the choice of graph.</p><p>In other words, people reporting outages at AWS are probably having trouble with microsoft-run DNS services or caching proxies.  It's not that the issues aren't there, it's that the internet is full of intermingled complexity.  Just that amount of organic false-positives can make it look like an unrelated major service is impacted.</p></div></td></tr></tbody></table></td></tr><tr id="45750071"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750071" href="https://news.ycombinator.com/vote?id=45750071&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Multi cloud is really hard to get right at scale, and honestly not worth the effort for the majority of companies and use-case.</p></div></td></tr></tbody></table></td></tr><tr id="45750415"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45750415" href="https://news.ycombinator.com/vote?id=45750415&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>on the line with msft, they said 4 hours is what they are thinking. a workaround they are saying is to use traffic manager,</p></div></td></tr></tbody></table></td></tr><tr id="45749846"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749846" href="https://news.ycombinator.com/vote?id=45749846&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Anyone have betting odds on when Google will go down next? Are we looking at all 3 providers having outages in the span of 3 weeks?</p></div></td></tr></tbody></table></td></tr><tr id="45748840"><td></td></tr><tr id="45748991"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45748991" href="https://news.ycombinator.com/vote?id=45748991&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>downdetector reports coincident cloudflare outage. is microsoft using cloudflare for management plane, or is there common infra? data center problem somewhere, maybe fiber backbone? BGP?</p></div></td></tr></tbody></table></td></tr><tr id="45749742"><td></td></tr><tr id="45748927"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45748927" href="https://news.ycombinator.com/vote?id=45748927&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>As of now Azure Status page still shows no incident. It must be manually updated, someone has to actively decide to acknowledge an issue, and they're just... not. It undermines confidence in that status page.</p></div></td></tr></tbody></table></td></tr><tr id="45749177"><td></td></tr><tr id="45749126"><td></td></tr><tr id="45749130"><td></td></tr><tr id="45748979"><td></td></tr><tr id="45749377"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749377" href="https://news.ycombinator.com/vote?id=45749377&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Yeah the graph for that one looks exactly the same shape. I wonder if they were depending on some azure component somehow, or maybe there were things hosted on both and the azure failure made enough things failover to AWS that AWS couldn't cope? If that was the case I'd expect to see something similar with GCP too though.</p><p>Edit: nope looks like there's actually a spike on GCP as well</p></div></td></tr></tbody></table></td></tr><tr id="45749506"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749506" href="https://news.ycombinator.com/vote?id=45749506&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>It's possibly more likely that people mis-attribute the cause of an outage to the wrong providers when they use downdetector.</p></div></td></tr></tbody></table></td></tr><tr id="45749671"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45749671" href="https://news.ycombinator.com/vote?id=45749671&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Definitely also a strong possibility. I wish I had paid more attention during the AWS one earlier to see what other things looked like on there at the time.</p></div></td></tr></tbody></table></td></tr><tr id="45749048"><td></td></tr><tr id="45749571"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749571" href="https://news.ycombinator.com/vote?id=45749571&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>microsoft.com is back -</p><p>edit: it worked once, then died again. So I guess - some resolvers, or FD servers may be working!</p></div></td></tr></tbody></table></td></tr><tr id="45750143"><td></td></tr><tr id="45749855"><td></td></tr><tr id="45748792"><td></td></tr><tr id="45750975"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45750975" href="https://news.ycombinator.com/vote?id=45750975&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>From Azure status page: "Customers can consider implementing failover strategies with Azure Traffic Manager, to fail over from Azure Front Door to your origins".</p><p>What a terrible advise.</p></div></td></tr></tbody></table></td></tr><tr id="45750401"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45750401" href="https://news.ycombinator.com/vote?id=45750401&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>I noticed issues on Azure so I went to the status page. It said everything was fine even though the Azure Portal was down. It took more than 10 minutes for that status page to update.</p><p>How can one of the richest companies in the world not offer a better service?</p></div></td></tr></tbody></table></td></tr><tr id="45748797"><td></td></tr><tr id="45749259"><td></td></tr><tr id="45749390"><td></td></tr><tr id="45748789"><td></td></tr><tr id="45749406"><td></td></tr><tr id="45749778"><td></td></tr><tr id="45750916"><td></td></tr><tr id="45749713"><td></td></tr><tr id="45749037"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749037" href="https://news.ycombinator.com/vote?id=45749037&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>My best guess at the moment is something global like the CDN is having problems affecting things everywhere. I'm able to use a legacy application we have that goes directly to resources in uswest3, but I'm not able to use our more modern application which uses APIM/CDN networks at all.</p></div></td></tr></tbody></table></td></tr><tr id="45749158"><td></td></tr><tr id="45749474"><td></td></tr><tr id="45750455"><td></td></tr><tr id="45749086"><td></td></tr><tr id="45749770"><td></td></tr><tr id="45749346"><td></td></tr><tr id="45749554"><td></td></tr><tr id="45750580"><td></td></tr><tr id="45749814"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749814" href="https://news.ycombinator.com/vote?id=45749814&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>&gt; [Satya Nadella] said that the company’s future opportunity was to bring AI to all eight billion people on the planet.</p><p>But what if I don't want AI brought to me?</p></div></td></tr></tbody></table></td></tr><tr id="45749924"><td></td></tr><tr id="45749992"><td></td></tr><tr id="45750316"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45750316" href="https://news.ycombinator.com/vote?id=45750316&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>You'll have to find another planet.</p><p>Although judging by the available transports it will likely be colonized by nazis.</p></div></td></tr></tbody></table></td></tr><tr id="45750147"><td></td></tr><tr id="45749909"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749909" href="https://news.ycombinator.com/vote?id=45749909&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>Like most technology initiative these tech CEOs dream up: You're going to get it and swallow it, whether you want it or not.</p></div></td></tr></tbody></table></td></tr><tr id="45749977"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749977" href="https://news.ycombinator.com/vote?id=45749977&amp;how=up&amp;goto=item%3Fid%3D45748661"></a></center></td><td><br>
<div><p>I especially like how Nadella speaks of layoffs as some kind of uncontrollable natural disaster, like a hurricane, caused by no-one in particular. A kind of "God works in mysterious ways".</p><pre><code>    &gt; “Microsoft is being recognized and rewarded at levels never seen before,” Nadella wrote. “And yet, at the same time, we’ve undergone layoffs. This is the enigma of success in an industry that has no franchise value.”
     
    &gt; Nadella explained the disconnect between thriving financials and layoffs by stating that “progress isn’t linear” and that it is “sometimes dissonant, and always demanding.”
</code></pre><p>
I've read the whole memo and it's actually worse than those excerpts. Nadella doesn't even claim these were low performers:</p><pre><code>    &gt; These decisions are among the most difficult we have to make. They affect people we’ve worked alongside, learned from, and shared countless moments with—our colleagues, teammates, and friends.
</code></pre><p>
Ok, so Microsoft is thriving, these were friends and people "we've learned from", but they must go because... uh... "progress isn't linear". Well, thanks Nadella! That explains so much!</p></div></td></tr></tbody></table></td></tr><tr id="45748662"><td></td></tr><tr id="45749769"><td></td></tr><tr id="45749835"><td></td></tr></tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Tell HN: Twilio support replies with hallucinated features (137 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=45748570</link>
            <guid>45748570</guid>
            <pubDate>Wed, 29 Oct 2025 15:54:36 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=45748570">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><tbody><tr id="45751442"><td></td></tr><tr id="45749497"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749497" href="https://news.ycombinator.com/vote?id=45749497&amp;how=up&amp;goto=item%3Fid%3D45748570"></a></center></td><td><br>
<div><p>There used to be a contract that a business had something to lose by providing bad service, that customers would leave and seek better service elsewhere.</p><p>I believe the most important and least discussed phenomenon of modern consumer culture is that consumers have passed a threshold of passive and docile behavior such that businesses no longer fear losing customers. Partly because the customers have shown willingness to eat shit, partly because there's a new understanding that all businesses will adopt the same customer-hostile behaviors (AI customer service in this case) so consumers don't have significant choice anyway.</p></div></td></tr></tbody></table></td></tr><tr id="45749669"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749669" href="https://news.ycombinator.com/vote?id=45749669&amp;how=up&amp;goto=item%3Fid%3D45748570"></a></center></td><td><br>
<div><p>It's not so much the willingness to eat shit, but that no matter the service I use, I will have to eat shit, so who's shit tastes least bad for the benefits.</p><p>A lot of VoIP/SMS providers exist, but compared to Twilio, they are just DIY API and SIP providers, which might be what we as developers want, but not what a business "needs".</p></div></td></tr></tbody></table></td></tr><tr id="45749840"><td></td></tr><tr id="45751254"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45751254" href="https://news.ycombinator.com/vote?id=45751254&amp;how=up&amp;goto=item%3Fid%3D45748570"></a></center></td><td><br>
<div><p>The conspiracy of service degradation: as long as every other provider sucks, the consumer never can expect better.</p><p>Alpha tested with lightbulbs but is now a clear strategy taught to MBAs</p></div></td></tr></tbody></table></td></tr><tr id="45751437"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45751437" href="https://news.ycombinator.com/vote?id=45751437&amp;how=up&amp;goto=item%3Fid%3D45748570"></a></center></td><td><br>
<div><p>The light bulb thing is actually the opposite of what you imply.</p><p>It was an open agreement to avoid misleading marketing that would have (presumably) caused a "race to the bottom".</p></div></td></tr></tbody></table></td></tr><tr id="45749579"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45749579" href="https://news.ycombinator.com/vote?id=45749579&amp;how=up&amp;goto=item%3Fid%3D45748570"></a></center></td><td><br>
<div><p>This is mostly due to not trust busting enough in society. If there were actual competitive markets, not monopolies/oligarchies/monopsonies/cartels, the business world would be completely different.</p><p>Either that, or legislate workplace democracy.</p></div></td></tr></tbody></table></td></tr><tr id="45749838"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45749838" href="https://news.ycombinator.com/vote?id=45749838&amp;how=up&amp;goto=item%3Fid%3D45748570"></a></center></td><td><br>
<div><p>I tend to disagree. While there are definitely monopolies/oligopoly for every domain, I'm actually constantly impressed with the very long tail of other providers available in that area.</p><p>Whenever I am looking for a new solution to a need at work, I would go to sites like g2.com to look at the lists of the most popular ones, and would then typically skim reviews of the top ~10, and more fully evaluate the top ~3. But there are often hundreds of alternatives that I haven't given a chance to, and I know that it's my &lt;s&gt;laziness&lt;/s&gt; need to manage my limited time that's promoting this oligopoly, rather than any particular issue with all of those other providers down the list.</p><p>I don't see how legislation can help here, other than picking a provider for me. If anything, this is actually a place where I feel that AI tools, and particularly ChatGPT's Deep Research can research a lot more of the alternatives than I as a human would have time for. But that of course has its own set of issues, and I really don't know what the solution is. We no longer live in that world where you just use that provider who lives down the street.</p></div></td></tr></tbody></table></td></tr><tr id="45750206"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45750206" href="https://news.ycombinator.com/vote?id=45750206&amp;how=up&amp;goto=item%3Fid%3D45748570"></a></center></td><td><br>
<div><p>Legislation can help in a variety of ways, like taxing digital goods to provide work grants for open source developers. The federal government could create a public payment processor.</p><p>There are many things that can be done to help the public flourish, it's very easy if you open up your imagination.</p></div></td></tr></tbody></table></td></tr><tr id="45750352"><td></td></tr><tr id="45750526"><td></td></tr><tr id="45750879"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45750879" href="https://news.ycombinator.com/vote?id=45750879&amp;how=up&amp;goto=item%3Fid%3D45748570"></a></center></td><td><br>
<div><p>It was back when militant labor presented an actual threat to the owning class. Now they know they can act with impunity, and they do.</p></div></td></tr></tbody></table></td></tr><tr id="45750061"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750061" href="https://news.ycombinator.com/vote?id=45750061&amp;how=up&amp;goto=item%3Fid%3D45748570"></a></center></td><td><br>
<div><p>It's interesting that you bring that up because I was just thinking about this concept in an undeveloped form. Egregious salesmanship is to sell an inferior or poor product while bolstering the overall brand reputation. How could that even be possible? With lies. You're absolutely right, <i>the salesman</i> in our world is in his purest and most demonic form.</p><p>With Brand management specifically, they specialize in servicing an ornate roof on a house so as to distract from the rest of the house. The ornate roof can be seen from miles away, and so it is the greatest ad you can buy in terms of reach.</p><p>I <i>think</i> I was thinking about this because of all the AI startup ads I've been seeing on Youtube. You wouldn't ever know how unworthy their product is based on how much branding and marketing they do. But that is the dance they do, the managing of the delta between product quality and brand quality, the management being the logistics of <i>veiling</i> that delta (not actually closing it).</p><p>Taking down a brand means to be diligent and aggressive in exposing that delta. Seems like common sense, but I'd urge you to consider it as more a "classical" formalization of what it is and what needs to be done. There is a terrible phenomenon within the human experience that results in humans trying to lie to each other for money.</p><p>It's the classical Theory on Being a Piece of Shit.</p></div></td></tr></tbody></table></td></tr><tr id="45751247"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45751247" href="https://news.ycombinator.com/vote?id=45751247&amp;how=up&amp;goto=item%3Fid%3D45748570"></a></center></td><td><br>
<div><p>Amen! This touches on my biggest frustration.  Product marketing doesn't market anymore.  For so many products you can't find any specific information unless you go look at reddit or reviews or something.  Half the time you can't even see a real picture of the item in a room or serving its purpose because listings are so filled with photoshopped garbage.  They want you to spend sometimes hundreds or thousands on something without even being told an accurate set of dimensions or ever seeing it actually in use.</p><p>It's really disgusting.  The problem is sometimes you need these things.  We were recently shopping for an oven and it was like that.  Lots of photoshopped images, it says "5 burner" but doesn't actually mention the 5th burner is just a warming burner except if you can see the one picture where the dial looks different from the others.</p><p>It's just ok for corporations to scam people now it seems.  I don't know what to do about it but I'm very sick of dealing with it.</p></div></td></tr></tbody></table></td></tr><tr id="45751243"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45751243" href="https://news.ycombinator.com/vote?id=45751243&amp;how=up&amp;goto=item%3Fid%3D45748570"></a></center></td><td><br>
<div><p>The vending machine mention is about this paper from Anthropic: <a href="https://www.anthropic.com/research/project-vend-1" rel="nofollow">https://www.anthropic.com/research/project-vend-1</a></p><p>The gist is:
Claude AI successfully ran a shop by itself!
- Actually a vending machine
- Actually a mini-fridge in our office
- Actually it gave lots of discounts and free products on our slack
- Actually it hallucinated a Venmo account and people sent payments to God-knows-who</p></div></td></tr></tbody></table></td></tr><tr id="45749111"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749111" href="https://news.ycombinator.com/vote?id=45749111&amp;how=up&amp;goto=item%3Fid%3D45748570"></a></center></td><td><br>
<div><p>These tools are perfect for deployment where providing plausible-but-incorrect info is aligned with business outcomes, like cutting your support staff and giving disgruntled customers fake information.</p><p>I’ve seen most of the frontier models hallucinate their capabilities, not surprising they might do so for api completions regarding a product they barely know about.</p><p>Unless they lose more money from cancelled subscriptions than they saved on cutting support staff, it’s probably the new normal.</p></div></td></tr></tbody></table></td></tr><tr id="45750777"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45750777" href="https://news.ycombinator.com/vote?id=45750777&amp;how=up&amp;goto=item%3Fid%3D45748570"></a></center></td><td><br>
<div><p>Twilio registered my business name as “My Twilio Account” and is unable to change it. My application for 10DLC also got rejected since I wanted to do something other than send marketing messages with it and I can’t figure out how to describe an opt-in only service that is strictly for employees, to their provided phone number, with a signature opting in to get payroll information texted to them.</p><p>As a test, I set up something to send junk quality marketing texts. Was approved.</p></div></td></tr></tbody></table></td></tr><tr id="45749224"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749224" href="https://news.ycombinator.com/vote?id=45749224&amp;how=up&amp;goto=item%3Fid%3D45748570"></a></center></td><td><br>
<div><p>None of them know what they're doing. Even Google's own AI integrated into their own apps, hallucinates about those very apps, e.g. asking Gemini in Docs about how to do something in Docs. It's laughable. LLM have great utility but this is not it.</p></div></td></tr></tbody></table></td></tr><tr id="45751004"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45751004" href="https://news.ycombinator.com/vote?id=45751004&amp;how=up&amp;goto=item%3Fid%3D45748570"></a></center></td><td><br>
<div><p>What distinguishes AI slop customer support from the previous enshittification of customer service is that previously if you wanted to avoid the garbage chat support you could get on the phone and -- even if you had to go through a phone tree -- you could at least eventually ask a person about the problem.</p><p>But now, even if it's possible to get a person on the phone, THAT PERSON is just doing the AI chatbot on their end. By talking to a human, you're just adding a middleman who is accessing the same incorrect chatbot that's available to you.</p></div></td></tr></tbody></table></td></tr><tr id="45749692"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45749692" href="https://news.ycombinator.com/vote?id=45749692&amp;how=up&amp;goto=item%3Fid%3D45748570"></a></center></td><td><br>
<div><p>"Hallucination machine, responds with hallucinations".</p><p>But seriously, entreprise customers (and any big spender account) usually get access to a dedicated (human) account rep and private support channels in Slack, so they never really interact with this.</p></div></td></tr></tbody></table></td></tr><tr id="45750891"><td></td></tr><tr id="45750959"><td></td></tr></tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The end of the rip-off economy: consumers use LLMs against information asymmetry (227 pts)]]></title>
            <link>https://www.economist.com/finance-and-economics/2025/10/27/the-end-of-the-rip-off-economy</link>
            <guid>45748195</guid>
            <pubDate>Wed, 29 Oct 2025 15:32:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/finance-and-economics/2025/10/27/the-end-of-the-rip-off-economy">https://www.economist.com/finance-and-economics/2025/10/27/the-end-of-the-rip-off-economy</a>, See on <a href="https://news.ycombinator.com/item?id=45748195">Hacker News</a></p>
Couldn't get https://www.economist.com/finance-and-economics/2025/10/27/the-end-of-the-rip-off-economy: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[I made a 10¢ MCU Talk (175 pts)]]></title>
            <link>https://www.atomic14.com/2025/10/29/CH32V003-talking</link>
            <guid>45747112</guid>
            <pubDate>Wed, 29 Oct 2025 14:12:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.atomic14.com/2025/10/29/CH32V003-talking">https://www.atomic14.com/2025/10/29/CH32V003-talking</a>, See on <a href="https://news.ycombinator.com/item?id=45747112">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
      <article>
        
        <section>
          
          <p><span></span> read
          </p>
          <a name="topofpage"></a>
          
          




          
          <div>
            <p><small>
                HELP <a href="https://www.atomic14.com/support/index.html">SUPPORT</a> MY WORK: If you're feeling flush then please stop by <a href="https://www.patreon.com/atomic14">Patreon</a> Or you can make a one off donation via <a href="https://ko-fi.com/atomic14">ko-fi</a>
              </small>
            </p>
            
            
            <div data-pagefind-body="">
            
            <blockquote>
  <p>TLDR: Yes, you can fit about 7 seconds of audio into 16K of flash and still have room for code. And you can even play LPC encoded audio on a 10 cent MCU.</p>
</blockquote>

<p>There’s quite a lot more detail in this video (and of course you can hear the audio!).</p>

<lite-youtube videoid="RZvX95aXSdM" playlabel="Talking 10 Cent MCU"></lite-youtube>

<p>In the <a href="https://www.atomic14.com/2025/10/12/CH32V003-music">previous project</a>, I had this ultra-cheap CH32V003 microcontroller playing simple tunes on a tiny SMD buzzer. It was just toggling a GPIO pin at musical note frequencies – 1-bit audio output – and it sounded surprisingly decent. That was a fun start, but now it’s time to push this little $0.10 MCU even further: can we make it actually talk?</p>

<p><img src="https://www.atomic14.com/assets/article_images/2025-11-01/ic.webp" alt="CH32V003"></p>

<p>Spoiler: Yes, we can! (well, there wouldn’t be much of a blog post if we couldn’t) This 8-pin RISC-V chip is now producing sampled audio data and spoken words. We’re really stretching the limits of what you can fit in 16 KB of flash.</p>

<p><img src="https://www.atomic14.com/assets/article_images/2025-11-01/16k.webp" alt="16K Flash, 2K RAM"></p>

<h2 id="from-beeps-to-actual-audio">From Beeps to Actual Audio</h2>

<p>Moving from simple beeps to real audio meant using the microcontroller’s PWM output as a rudimentary DAC. Instead of just on/off beeping, I’m driving a waveform at an 8 kHz sample rate using a high-frequency PWM on the output pin. The hardware is the same tiny board as before
 – but I’ve swapped the small SMD buzzer for a small speaker. The buzer works too, but it’s quieter and very tinny.</p>

<p><img src="https://www.atomic14.com/assets/article_images/2025-11-01/speaker.webp" alt="New Speaker"></p>

<p>The sample I wanted to test with is just over 6 seconds in length - it’s the iconic “Open the pod bay doors HAL…” sequence from 2001.</p>

<p><img src="https://www.atomic14.com/assets/article_images/2025-11-01/pod-bay-doors.webp" alt="Open the pod bay doors"></p>

<p>If we keep this audio at 16-bit PCM, 8kHZ, we’d need about 96KB – way beyond our 16 KB flash! And remember, that 16 KB has to hold both the audio data and our playback code. Clearly some aggressive compression is required.</p>

<table>
  <thead>
    <tr>
      <th>Format</th>
      <th>Sample Rate</th>
      <th>Bits/Sample</th>
      <th>Size</th>
      <th><strong>Fits in 16KB?</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CD Quality</td>
      <td>44.1 kHz</td>
      <td>16-bit</td>
      <td>529 KB</td>
      <td>❌ 33× too big!</td>
    </tr>
    <tr>
      <td>Phone Quality</td>
      <td>16 kHz</td>
      <td>16-bit</td>
      <td>192 KB</td>
      <td>❌ 12× too big!</td>
    </tr>
    <tr>
      <td>Basic PCM</td>
      <td>8 kHz</td>
      <td>8-bit</td>
      <td>48 KB</td>
      <td>❌ 3× too big!</td>
    </tr>
    <tr>
      <td><strong>4-bit ADPCM (IMA)</strong></td>
      <td><strong>8 kHz</strong></td>
      <td><strong>4-bit</strong></td>
      <td><strong>24 KB</strong></td>
      <td>❌ <strong>1.5× too big</strong></td>
    </tr>
    <tr>
      <td><strong>QOA (Quite OK Audio)</strong></td>
      <td><strong>8 kHz</strong></td>
      <td><strong>3.2-bit</strong></td>
      <td><strong>19 KB</strong></td>
      <td>❌ <strong>Still too big!</strong></td>
    </tr>
    <tr>
      <td><strong>2-bit ADPCM</strong></td>
      <td><strong>8 kHz</strong></td>
      <td><strong>2-bit</strong></td>
      <td><strong>12 KB</strong></td>
      <td>✅ <strong>Fits!</strong></td>
    </tr>
  </tbody>
</table>

<p>I considered a few encoding options for compressing the audio.</p>

<ul>
  <li><strong>8-bit PCM:</strong> Simply using 8-bit samples at 8 kHz cuts size in half (to ~47 KB for 6s), but that’s still about 3× too large for our flash.</li>
  <li><strong>4-bit ADPCM:</strong> Adaptive Differential PCM is a simple lossy compression that could quarter the size. In theory 6 seconds would be ~24 KB – much closer to fitting,</li>
  <li><strong><a href="https://qoaformat.org/">“Quite OK Audio” (QOA)</a>:</strong> This is nice codec that packs audio into about 3.2 bits per sample (roughly 1/5 the size of 16-bit PCM)</li>
  <li><strong>2-bit ADPCM:</strong> Going even further with ADPCM, using only 2 bits per sample gives a 4:1 compression relative to 8-bit audio – that’s 75% storage savings.</li>
</ul>

<p>2-bit ADPCM is definitely the winner here. Our 6-second clip shrinks to under 12 KB, which comfortably fits in flash with room for code. This looked like the winner, provided the audio quality was acceptable. The decoder for 2-bit ADPCM is also very lightweight (my implementation compiled to under just over 1K of code - 1340 bytes!). It’s definitely low quality - but it actually sounds surprisingly ok.</p>

<h2 id="how-does-2-bit-adpcm-work">How does 2-bit ADPCM work?</h2>

<p>It’s actually a very simple algorithm. Both the encoder and decoder maintain a predicted signal value and a step size index into a predefined table. Each 2-bit code tells the decoder how to adjust the current prediction and the step size index. In essence, we’re coding the difference between the real audio and our prediction, with only four possible levels (since 2 bits gives 4 values). After each sample, the algorithm adapts: if the prediction error was large, we move to a bigger step size (to allow larger changes); if the error was small, we use a smaller step size for finer resolution. This adaptive step is what makes it ADPCM (Adaptive Differential PCM).</p>

<p>Our codes are as follows:</p>

<ul>
  <li><code>00</code> (0): Go down by 1 step - subtract the step size from our current prediction</li>
  <li><code>01</code> (1): Go up by 1 step - add the step size to our current prediction</li>
  <li><code>10</code> (2): Go down by 2 steps - subtract the 2 x step size from our current prediction</li>
  <li><code>11</code> (3): Go up by 2 steps - add the 2 x step size to our current prediction</li>
</ul>

<p><img src="https://www.atomic14.com/assets/article_images/2025-11-01/audio.webp" alt="2-bit ADPCM Compression"></p>

<p>Even with this very high level of compression, the predicted waveform manages to track the original audio surprisingly well. The above graph shows a small snippet of the audio: the blue line is the original waveform and the yellow line is the ADPCM decoder’s output.</p>

<p>They’re not identical (and we wouldn’t expect them to be), but the general shape is preserved. When you play it back through the little speaker, it’s recognizable and surprisingly good.</p>

<p>To make my life easier, I built a quick conversion <a href="https://buzzer-studio.atomic14.com/">tool</a> to encode WAV files into this 2-bit ADPCM format. The tool lets me drag-and-drop a WAV, and it gives you the files with the data that can ve dropped into the firmware code.</p>

<p><img src="https://www.atomic14.com/assets/article_images/2025-11-01/buzzer-2-adpcm.webp" alt="2-bit ADPCM Buzzer Studio"></p>

<h2 id="lpc-speech-synthesis">LPC Speech Synthesis</h2>

<p>Six seconds of audio is cool, but what about longer phrases or even arbitrary speech? Storing anything much longer with raw or ADPCM audio would quickly fill the 16K of flash.</p>

<p>For my second experiment, I tried something different: instead of recorded waveform audio, I used an <a href="https://en.wikipedia.org/wiki/Texas_Instruments_LPC_Speech_Chips">old-school speech synthesis approach</a>. This leverages the fact that spoken language can be encoded very compactly by modeling the human voice, rather than storing the raw sound. Specifically, I integrated a library called <a href="https://github.com/going-digital/Talkie/tree/master/Talkie">Talkie</a>.</p>

<p>Talkie is a software implementation of the Texas Instruments LPC speech synthesis architecture from the late 1970s. This was implemented in a variety of chips, most commonly the TMS5220 and TMS5100 speech chips.</p>

<p><img src="https://www.atomic14.com/assets/article_images/2025-11-01/TMS_types.webp" alt="TMS5220 and TMS5100 Variants"></p>

<p>These were used in things like the original Speak &amp; Spell, arcade games like early Star Wars, and speech add-ons for home computers (e.g. the BBC Micro).</p>

<p><img src="https://www.atomic14.com/assets/article_images/2025-11-01/speak-spell.webp" alt="Speak and Spell"></p>

<p>The Talkie library (originally by <a href="https://github.com/going-digital/Talkie/tree/master/Talkie">Peter Knight</a>, later added to by <a href="https://learn.adafruit.com/bringing-back-the-voice-of-speak-spell/software">Adafruit</a>) comes with a big set of examples and vocabulary. It’s also possible to extract examples from old ROMs from arcade games.</p>

<p>Each phrase or word only takes a few hundred bytes or even less, so you can fit quite a lot of speech into a few kilobytes of flash. The trade-off is that the voice has a very computer-esque timbre – think of the Speak &amp; Spell’s voice. It’s clearly synthetic, but still understandable.</p>

<p>To say custom sentences not in the library, you either concatenate the available words/phonemes (which can be clunky), or you need to generate new LPC data. The original tools for this are a bit obscure – there’s BlueWizard (a classic Mac app) and PythonWizard (a command-line tool with TK GUI) which can analyze WAV files and produce LPC data.</p>

<p>I gave both a try with some success (and a few headaches setting them up). In the end, I cheated a bit and used an AI coding assistant to help me create a streamlined online tool for this.</p>

<p>The result is a little <a href="http://buzzer-studio.atomic14.com/">web app</a> where I can upload a recording of, say, my own voice, and it outputs the LPC data. It even lets me play back the synthesized voice in-browser to check it.</p>

<p><img src="https://www.atomic14.com/assets/article_images/2025-11-01/buzzer-lpc.webp" alt="LPC Encoder"></p>

<p>So there we have it – our 10¢ microcontroller now has a voice! By using 2-bit ADPCM compression, we can store short audio clips (up to around 8 seconds) even in 16 KB of flash, and play them back via PWM with decent fidelity.</p>

<p>And with the Talkie LPC speech synthesis, we can make the device “speak” lots of words and phrases with only a tiny memory footprint.</p>

<p>If you want to hear it for yourself, check out the video demo linked at the top of this post. In the video, you’ll see (and hear) the WarGames clip and the Star Wars quotes running on the hardware. It’s honestly amazing what these cheap little MCUs can do. We’re really pushing the boundaries of cheap hardware here.</p>

<p>You can find all my code on GitHub in this <a href="https://github.com/atomic14/ch32v003-audio">repository</a>.</p>

<lite-youtube videoid="RZvX95aXSdM" playlabel="Talking 10 Cent MCU"></lite-youtube>

            </div>
            
            
            
              <h2>Related Posts</h2>
                              
                
                <a href="https://www.atomic14.com/2025/10/12/CH32V003-music.html">
                
                </a>
                              
                
                <a href="https://www.atomic14.com/2024/01/05/esp32-s3-no-pins.html">
                
                </a>
                              
                
                <a href="https://www.atomic14.com/2024/01/26/16bit-handheld-teardown.html">
                
                </a>
                              
                
                <a href="https://www.atomic14.com/2023/09/26/decoding-avi-files-for-fun-and-0-profit.html">
                
                </a>
                              
                
                <a href="https://www.atomic14.com/2022/08/19/a-life-in-tech-part1.html">
                
                </a>
              
            
            
              <h2>Related Videos</h2>
                              
                
                <a href="https://www.atomic14.com/videos/posts/RZvX95aXSdM.html">
                
                </a>
                              
                
                <a href="https://www.atomic14.com/videos/posts/RiiS4jjG6ME.html">
                
                </a>
                              
                
                <a href="https://www.atomic14.com/videos/posts/LhZ9pWwnn6s.html">
                
                </a>
                              
                
                <a href="https://www.atomic14.com/videos/posts/oZ39VCUvKjw.html">
                
                </a>
                              
                
                <a href="https://www.atomic14.com/videos/posts/G6MROvlLeKE.html">
                
                </a>
              
            
            <p><small>
                HELP <a href="https://www.atomic14.com/support/index.html">SUPPORT</a> MY WORK: If you're feeling flush then please stop by <a href="https://www.patreon.com/atomic14">Patreon</a> Or you can make a one off donation via <a href="https://ko-fi.com/atomic14">ko-fi</a>
              </small>
            </p>
          </div>
          
        </section>
        
        <div>
          <div>
            <h5><span>Written by</span></h5>
            <section>
              
              <h4>Chris Greening</h4>
              
              <hr>
              <p>Published <time datetime="2025-10-29 00:00">29 Oct 2025</time></p>
            </section>
          </div>
          
          <div>
            <h5><span>Supported by</span></h5>
            
          </div>
        </div>
      </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kafka is Fast – I'll use Postgres (328 pts)]]></title>
            <link>https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks</link>
            <guid>45747018</guid>
            <pubDate>Wed, 29 Oct 2025 14:06:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks">https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks</a>, See on <a href="https://news.ycombinator.com/item?id=45747018">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><h2 id="intro">Intro</h2>
<p>I feel like the tech world lives in two camps.</p>
<ol>
<li>One camp chases buzzwords.</li>
</ol>
<p>This camp tends to adopt whatever’s popular without thinking hard about whether it’s appropriate. They tend to fall for all the purported benefits the sales pitch gives them - real-time, infinitely scale, cutting-edge, cloud-native, serverless, zero-trust, AI-powered, etc.</p>
<p>You see this everywhere in the Kafka world: Streaming Lakehouse™️, Kappa™️ Architecture, Streaming AI Agents<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup>.</p>
<p>This phenomenon is sometimes known as <em>resume-driven design</em>. Modern practices actively encourage this. Consultants push “innovative architectures” stuffed with vendor tech via “insight” reports<sup><a href="#user-content-fn-5" id="user-content-fnref-5" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup>. System design interviews expect you to design Google-scale architectures that are inevitably at a scale 100x higher than the company you’re interviewing for would ever need. Career progression rewards you for replatforming to the Hot New Stack™️, not for being resourceful.</p>
<ol start="2">
<li>The other camp chases common sense</li>
</ol>
<p>This camp is far more pragmatic. They strip away unnecessary complexity and steer clear of overengineered solutions. They reason from first principles before making technology choices. They resist marketing hype and approach vendor claims with healthy skepticism.</p>
<p>Historically, it has felt like Camp 1 definitively held the upper hand in sheer numbers and noise. Today, it feels like the pendulum may be beginning to swing back, at least a tiny bit. Two recent trends are on the side of Camp 2:</p>
<p>Trend 1 - the “<a href="https://topicpartition.io/Small-Data" data-slug="Small-Data">Small Data</a>” movement. People are realizing two things - their data isn’t that big and their computers are becoming big too. You can rent a <a href="https://instances.vantage.sh/aws/ec2/x1e.xlarge">128-core, 4 TB of RAM instance</a> from AWS. AMD just released 192-core CPUs this summer. That ought to be enough for anybody.<sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup></p>
<p>Trend 2 - the Postgres Renaissance. The space is seeing incredible growth and investment<sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup>. In the last 2 years, the phrase <a href="https://github.com/Olshansk/postgres_for_everything">“Just Use Postgres (for everything)”</a> has gained a ton of popularity. The basic premise is that you shouldn’t complicate things with new tech when you don’t need to, and that Postgres alone solves most problems pretty well. Postgres competes with purpose-built solutions like:</p>
<ul>
<li>Elasticsearch (functionality supported by Postgres’ <code>tsvector</code>/<code>tsquery</code>)</li>
<li>MongoDB (<code>jsonb</code>)</li>
<li>Redis (<code>CREATE UNLOGGED TABLE</code>)</li>
<li>AI Vector Databases (<code>pgvector</code>, <code>pgai</code>)</li>
<li>Snowflake (<code>pg_mooncake</code>, <code>pg_duckdb</code>)</li>
</ul>
<p>and… Kafka (this blog).</p>
<p>The claim isn’t that Postgres is functionally equivalent to any of these specialized systems. The claim is that it handles 80%+ of their use cases with 20% of the development effort. (Pareto Principle)</p>
<p>When you combine the two trends, the appeal becomes obvious. Postgres is a battle-tested, well-known system that is simple, scalable and reliable. Pair it with today’s powerful hardware and you quickly begin to realize that, more often than not, you do not need the state-of-the-art highly optimized and complex distributed system in order to handle your organization’s scale.</p>
<p><a href="https://bento.me/stanislavkozlovski">Despite being somebody who is biased towards Kafka</a>, I tend to agree. Kafka is similar to Postgres in that it’s stable, mature, battle-tested and boasts a strong community. It also scales a lot further. Despite that, I don’t think it’s the right choice for a lot of cases. Very often I see it get adopted where <a href="https://www.reddit.com/r/apachekafka/comments/1o7gbyg/controlling_llm_outputs_with_kafka_schema/">it doesn’t make sense</a>.</p>
<p><strong>A 500 KB/s workload should not use Kafka.</strong> There is a scalability cargo cult in tech that always wants to choose “the best possible” tech for a problem - but this misses the forest for the trees. The “best possible” solution frequently isn’t a technical question - it’s a practical one. Adriano makes an airtight case for why you should opt for <strong>simple tech</strong> in his <a href="https://adriano.fyi/posts/2023-09-24-choose-postgres-queue-technology">PG as Queue blog</a> (2023) that originally inspired me to write this.</p>
<p>Enough background. In this article, we will do three simple things:</p>
<ol>
<li>Benchmark how far Postgres can scale for pub/sub messaging - <a href="#pg-as-a-pubsub"># PG as a Pub/Sub</a></li>
<li>Benchmark how far Postgres can scale for queueing - <a href="#pg-as-a-queue"># PG as a Queue</a></li>
<li>Concisely touch upon when Postgres can be a fit for these use cases - <a href="#should-you-use-postgres"># Should You Use Postgres?</a></li>
</ol>
<p>I am not aiming for an exhaustive in-depth evaluation. Benchmarks are messy af. Rather, my goal is to publish some reasonable data points which can start a discussion.</p>
<p><em>(while this article is for Postgres, feel free to replace it with your database of choice)</em></p>
<hr>
<h2 id="results-tldr">Results TL;DR</h2>
<p>If you’d like to skip straight to the results, here they are:</p>
<details>
  <summary>🔥 The Benchmark Results</summary>
<h3 id="pub-sub-results">Pub-Sub Results</h3>

































<div><table><thead><tr><th>Setup</th><th>✍️ <strong>Write</strong></th><th>📖 <strong>Read</strong></th><th>🔭 <strong>e2e Latency<sup><a href="#user-content-fn-9" id="user-content-fnref-9" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup> (p99)</strong></th><th>Notes</th></tr></thead><tbody><tr><td><strong>1× c7i.xlarge</strong></td><td><strong>4.8 MiB/s<br>5036 msg/s</strong></td><td><strong>24.6 MiB/s<br>25 183 msg/s</strong> (5x fanout)</td><td><strong>60 ms</strong></td><td>~60 % CPU; 4 partitions</td></tr><tr><td><strong>3× c7i.xlarge (replicated)</strong></td><td><strong>4.9 MiB/s<br>5015 msg/s</strong></td><td><strong>24.5 MiB/s<br>25 073 msg/s</strong> (5x fanout)</td><td><strong>186 ms</strong></td><td>~65 % CPU; cross-AZ RF≈2.5; 4 partitions</td></tr><tr><td><strong>1× c7i.24xlarge</strong></td><td><strong>238 MiB/s<br>243,000 msg/s</strong></td><td><strong>1.16 GiB/s<br>1,200,000 msg/s</strong> (5x fanout)</td><td><strong>853 ms</strong></td><td>~10 % CPU (idle); 30 partitions</td></tr></tbody></table></div>
<h3 id="queue-results">Queue Results</h3>





























<div><table><thead><tr><th>Setup</th><th>📬 <strong>Throughput (read + write)</strong></th><th>🔭 <strong>e2e Latency<sup><a href="#user-content-fn-9" id="user-content-fnref-9-2" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup> (p99)</strong></th><th>Notes</th></tr></thead><tbody><tr><td><strong>1× c7i.xlarge</strong></td><td><strong>2.81 MiB/s<br>2885 msg/s</strong></td><td><strong>17.7 ms</strong></td><td>~60 % CPU; read-client bottleneck</td></tr><tr><td><strong>3× c7i.xlarge (replicated)</strong></td><td><strong>2.34 MiB/s<br>2397 msg/s</strong></td><td><strong>920 ms ⚠️<sup><a href="#user-content-fn-19" id="user-content-fnref-19" data-footnote-ref="" aria-describedby="footnote-label">6</a></sup></strong></td><td>replication lag inflated E2E latency</td></tr><tr><td><strong>1× c7i.24xlarge</strong></td><td><strong>19.7 MiB/s<br>20,144 msg/s</strong></td><td><strong>930 ms ⚠️<sup><a href="#user-content-fn-19" id="user-content-fnref-19-2" data-footnote-ref="" aria-describedby="footnote-label">6</a></sup></strong></td><td>~50 % CPU; single-table bottleneck</td></tr></tbody></table></div>
<p>Make sure to at least read the last section of the article where we philosophize - <a href="#should-you-use-postgres"># Should You Use Postgres?</a></p>
</details>
<hr>
<h2 id="pg-as-a-pubsub">PG as a Pub/Sub</h2>
<p>There are dozens of blogs out there using Postgres as a <u>queue</u>, but interestingly enough I haven’t seen one use it as a pub-sub messaging system.</p>
<p>A quick distinction between the two because I often see them get confused:</p>
<ol>
<li>
<p><strong>Queues</strong> are meant for point-to-point communication. They’re widely used for asynchronous background jobs: worker apps (clients) process a task in the queue like sending an e-mail or pushing a notification. The event is consumed once and it’s done with. A message is immediately deleted (popped) off the queue once it’s consumed. Queues do not have strict ordering guarantees<sup><a href="#user-content-fn-6" id="user-content-fnref-6" data-footnote-ref="" aria-describedby="footnote-label">7</a></sup>.</p>
</li>
<li>
<p><strong>Pub-sub</strong> messaging differs from the queue in that it is meant for one-to-many communication. This inherently means there is a large read fanout - more than one reader client is interested in any given message. Good pub-sub systems decouple readers from writers by storing data on disks. This allows them to not impose a max queue depth limit - something in-memory queues need to do in order to prevent them from going OOM.</p>
<p>There is also a general expectation that there is strict order - events should be read in the same order that they arrived in the system.</p>
</li>
</ol>
<p>Postgres’ main competitor here is Kafka, which is the standard in pub-sub today. Various (mostly-proprietary) alternatives exist.<sup><a href="#user-content-fn-13" id="user-content-fnref-13" data-footnote-ref="" aria-describedby="footnote-label">8</a></sup></p>
<p>Kafka uses the Log data structure to hold messages. You’ll see my benchmark basically reconstructs a log from Postgres primitives.</p>
<p>Postgres doesn’t seem to have any popular libraries for pub-sub<sup><a href="#user-content-fn-27" id="user-content-fnref-27" data-footnote-ref="" aria-describedby="footnote-label">9</a></sup> use cases, so I had to write my own. The Kafka-inspired workflow I opted for is this:</p>
<ol>
<li>Writers produce batches of messages per statement<sup><a href="#user-content-fn-20" id="user-content-fnref-20" data-footnote-ref="" aria-describedby="footnote-label">10</a></sup> (<code>INSERT INTO</code>). Each transaction carries one batch insert and targets a single <code>topicpartition</code> table<sup><a href="#user-content-fn-14" id="user-content-fnref-14" data-footnote-ref="" aria-describedby="footnote-label">11</a></sup></li>
<li>Each writer is sticky to one table, but in aggregate they produce to multiple tables.</li>
<li>Each message has a unique monotonically-increasing offset number. A specific row in a special <code>log_counter</code> table denotes the latest offset for a given <code>topicpartition</code> table.</li>
<li>Write transactions atomically update both the <code>topicpartition</code> data and the <code>log_counter</code> row. This ensures consistent offset tracking across concurrent writers.</li>
<li>Readers poll for new messages. They consume the <code>topicpartition</code> table(s) sequentially, starting from the lowest offset and progressively reading up.</li>
<li>Readers are split into consumer groups. Each group performs separate, independent reads and makes progress on the <code>topicpartition</code> tables.</li>
<li>Each group contains 1 reader per <code>topicpartition</code> table.</li>
<li>Readers store their progress in a <code>consumer_offsets</code> table, with a row for each <code>topicpartition,group</code> pair.</li>
<li>Each reader updates the latest processed offset (claiming the records), selects the records and processes them inside a single transaction.</li>
</ol>
<p>This ensures Kafka-like semantics - gapless, monotonically-increasing offsets and at-least-once/at-most-once processing. This test in particular uses at-least-once semantics, but neither choice should impact the benchmark results.</p>
<h2 id="pub-sub-setup">Pub-Sub Setup</h2>
<h4 id="table">Table</h4>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="sql" data-theme="github-light github-dark"><code data-language="sql" data-theme="github-light github-dark"><span data-line=""><span>CREATE</span><span> TABLE</span><span> log_counter</span><span> (</span></span>
<span data-line=""><span>  id           </span><span>INT</span><span> PRIMARY KEY</span><span>, </span><span>-- topicpartition table name id</span></span>
<span data-line=""><span>  next_offset  </span><span>BIGINT</span><span> NOT NULL</span><span>  -- next offset to assign</span></span>
<span data-line=""><span>);</span></span>
<span data-line=""> </span>
<span data-line=""><span>for</span><span> i </span><span>in</span><span> NUM_PARTITIONS:</span></span>
<span data-line=""><span>  CREATE</span><span> TABLE</span><span> topicpartition</span><span>%d (</span></span>
<span data-line=""><span>    id          </span><span>BIGSERIAL</span><span> PRIMARY KEY</span><span>,</span></span>
<span data-line=""><span>    -- strictly increasing offset (indexed by UNIQUE)</span></span>
<span data-line=""><span>    c_offset    </span><span>BIGINT</span><span> UNIQUE</span><span> NOT NULL</span><span>,</span></span>
<span data-line=""><span>    payload     </span><span>BYTEA</span><span> NOT NULL</span><span>,</span></span>
<span data-line=""><span>    created_at  </span><span>TIMESTAMPTZ</span><span> NOT NULL</span><span> DEFAULT</span><span> now</span><span>()</span></span>
<span data-line=""><span>  );</span></span>
<span data-line=""><span>  INSERT INTO</span><span> log_counter(id, next_offset) </span><span>VALUES</span><span> (%d, </span><span>1</span><span>);</span></span>
<span data-line=""> </span>
<span data-line=""><span>CREATE</span><span> TABLE</span><span> consumer_offsets</span><span> (</span></span>
<span data-line=""><span>  group_id     </span><span>TEXT</span><span> NOT NULL</span><span>,     </span><span>-- consumer group identifier</span></span>
<span data-line=""><span>  -- topic-partition id (matches log_counter.id / topicpartitionN)</span></span>
<span data-line=""><span>  topic_id     </span><span>INT</span><span>  NOT NULL</span><span>,</span></span>
<span data-line=""><span>  -- next offset the consumer group should claim</span></span>
<span data-line=""><span>  next_offset  </span><span>BIGINT</span><span> NOT NULL</span><span> DEFAULT</span><span> 1</span><span>,</span></span>
<span data-line=""><span>  PRIMARY KEY</span><span> (group_id, topic_id)</span></span>
<span data-line=""><span>);</span></span></code></pre></figure>
<h4 id="writes">Writes</h4>
<p>The benchmark runs <code>N</code> writer goroutines. These represent writer clients.
Each one loops and atomically inserts <code>$BATCH_SIZE</code> records while updating the latest offset:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="sql" data-theme="github-light github-dark"><code data-language="sql" data-theme="github-light github-dark"><span data-line=""><span>WITH</span><span> reserve </span><span>AS</span><span> (</span></span>
<span data-line=""><span>  UPDATE</span><span> log_counter</span></span>
<span data-line=""><span>  SET</span><span> next_offset </span><span>=</span><span> next_offset </span><span>+</span><span> $</span><span>1</span></span>
<span data-line=""><span>  WHERE</span><span> id </span><span>=</span><span> $</span><span>3</span><span>::</span><span>int</span></span>
<span data-line=""><span>  RETURNING (next_offset </span><span>-</span><span> $</span><span>1</span><span>) </span><span>AS</span><span> first_off</span></span>
<span data-line=""><span>)</span></span>
<span data-line=""> </span>
<span data-line=""><span>INSERT INTO</span><span> topicpartition%d(c_offset, payload)</span></span>
<span data-line=""><span>SELECT</span><span> r</span><span>.</span><span>first_off</span><span> +</span><span> p</span><span>.</span><span>ord</span><span> -</span><span> 1</span><span>, </span><span>p</span><span>.</span><span>payload</span></span>
<span data-line=""><span>FROM</span><span> reserve r,</span></span>
<span data-line=""><span>     unnest($</span><span>2</span><span>::</span><span>bytea</span><span>[]) </span><span>WITH</span><span> ORDINALITY </span><span>AS</span><span> p(payload, ord);</span></span></code></pre></figure>
<h4 id="reads">Reads</h4>
<p>The benchmark also runs <code>N</code> reader goroutines. Each reader is assigned a particular consumer group and partition. The group as a whole reads all partitions while each reader in the group reads only one partition at a time.</p>
<p>The reader loops, opens a transaction, optimistically claims <code>$BATCH_SIZE</code> records (by advancing the offset mark beyond them), selects them and processes the records.
If successful, it commits the transaction and through that advances the offset for the group.</p>
<p>It is a pull-based read (just like Kafka), rather than push-based. If the reader has no records to poll, it sleeps for a bit.</p>
<p>First it opens a transaction:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="sql" data-theme="github-light github-dark"><code data-language="sql" data-theme="github-light github-dark"><span data-line=""><span>BEGIN</span><span> TRANSACTION</span></span></code></pre></figure>
<p>Then it claims the offsets:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="sql" data-theme="github-light github-dark"><code data-language="sql" data-theme="github-light github-dark"><span data-line=""><span>WITH</span><span> counter_tip </span><span>AS</span><span> (</span></span>
<span data-line=""><span>  SELECT</span><span> (next_offset </span><span>-</span><span> 1</span><span>) </span><span>AS</span><span> highest_committed_offset</span></span>
<span data-line=""><span>  FROM</span><span> log_counter</span></span>
<span data-line=""><span>  WHERE</span><span> id </span><span>=</span><span> $</span><span>3</span><span>::</span><span>int</span><span> -- partition id</span></span>
<span data-line=""><span>),</span></span>
<span data-line=""> </span>
<span data-line=""><span>-- select &amp; lock the particular group&lt;-&gt;topic_partition&lt;-&gt;offset pair</span></span>
<span data-line=""><span>to_claim </span><span>AS</span><span> (</span></span>
<span data-line=""><span>  SELECT</span></span>
<span data-line=""><span>    c</span><span>.</span><span>group_id</span><span>,</span></span>
<span data-line=""><span>    c</span><span>.</span><span>next_offset</span><span> AS</span><span> n0, </span><span>-- old start offset pointer before update</span></span>
<span data-line=""><span>    -- takes the min of the batch size</span></span>
<span data-line=""><span>    -- or the current offset delta w.r.t the tip of the log</span></span>
<span data-line=""><span>    LEAST</span><span>(</span></span>
<span data-line=""><span>      $</span><span>2</span><span>::</span><span>bigint</span><span>, </span><span>-- BATCH_SIZE</span></span>
<span data-line=""><span>      GREATEST</span><span>(</span><span>0</span><span>,</span></span>
<span data-line=""><span>        (</span><span>SELECT</span><span> highest_committed_offset </span><span>FROM</span><span> counter_tip) </span><span>-</span><span> c</span><span>.</span><span>next_offset</span><span> +</span><span> 1</span><span>)</span></span>
<span data-line=""><span>    ) </span><span>AS</span><span> delta</span></span>
<span data-line=""><span>  FROM</span><span> consumer_offsets c</span></span>
<span data-line=""><span>  WHERE</span><span> c</span><span>.</span><span>group_id</span><span> =</span><span> $</span><span>1</span><span>::</span><span>text</span><span> AND</span><span> c</span><span>.</span><span>topic_id</span><span> =</span><span> $</span><span>3</span><span>::</span><span>int</span></span>
<span data-line=""><span>  FOR</span><span> UPDATE</span></span>
<span data-line=""><span>),</span></span>
<span data-line=""> </span>
<span data-line=""><span>-- atomically select + update the offset</span></span>
<span data-line=""><span>upd </span><span>AS</span><span> (</span></span>
<span data-line=""><span>  UPDATE</span><span> consumer_offsets c</span></span>
<span data-line=""><span>  SET</span><span> next_offset </span><span>=</span><span> c</span><span>.</span><span>next_offset</span><span> +</span><span> t</span><span>.</span><span>delta</span></span>
<span data-line=""><span>  FROM</span><span> to_claim t</span></span>
<span data-line=""><span>  WHERE</span><span> c</span><span>.</span><span>group_id</span><span> =</span><span> t</span><span>.</span><span>group_id</span><span> AND</span><span> c</span><span>.</span><span>topic_id</span><span> =</span><span> $</span><span>3</span><span>::</span><span>int</span></span>
<span data-line=""><span>  RETURNING</span></span>
<span data-line=""><span>    t</span><span>.</span><span>n0</span><span> AS</span><span> claimed_start_offset, </span><span>-- start = the old next_offset</span></span>
<span data-line=""><span>    (</span><span>c</span><span>.</span><span>next_offset</span><span> -</span><span> 1</span><span>) </span><span>AS</span><span> claimed_end_offset </span><span>-- end   = new pointer - 1</span></span>
<span data-line=""><span>)</span></span>
<span data-line=""> </span>
<span data-line=""><span>SELECT</span><span> claimed_start_offset, claimed_end_offset</span></span>
<span data-line=""><span>FROM</span><span> upd;</span></span></code></pre></figure>
<p>Followed by selecting the claimed records:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="sql" data-theme="github-light github-dark"><code data-language="sql" data-theme="github-light github-dark"><span data-line=""><span>SELECT</span><span> c_offset, payload, created_at</span></span>
<span data-line=""><span>  FROM</span><span> topicpartition%d</span></span>
<span data-line=""><span>  WHERE</span><span> c_offset </span><span>BETWEEN</span><span> $</span><span>1</span><span> AND</span><span> $</span><span>2</span></span>
<span data-line=""><span>  ORDER BY</span><span> c_offset</span></span></code></pre></figure>
<p>Finally, the data gets processed by the business logic (no-op in our benchmark) and the transaction is closed:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="sql" data-theme="github-light github-dark"><code data-language="sql" data-theme="github-light github-dark"><span data-line=""><span>COMMIT</span><span>;</span></span></code></pre></figure>
<p>If you’re wondering <em>“why no <code>NOTIFY/LISTEN</code>?”</em> - my understanding of that feature is that it’s an optimization and cannot be fully relied upon, so polling is required either way<sup><a href="#user-content-fn-21" id="user-content-fnref-21" data-footnote-ref="" aria-describedby="footnote-label">12</a></sup>. Given that, I just copied Kafka’s relatively simple design.</p>
<h2 id="pub-sub-results-1">Pub-Sub Results</h2>
<p>The full code and detailed results are all published on GitHub at <a href="https://github.com/stanislavkozlovski/pg-queue-pubsub-benchmark">stanislavkozlovski/pg-queue-pubsub-benchmark</a>.
I ran three setups - a single-node 4 vCPU, a 3-node replicated 4 vCPU and a single-node 96 vCPU setup. Here are the summarized results for each:</p>
<h3 id="4-vcpu-single-node">4 vCPU Single Node</h3>
<p><small><em>The results are the average of three 2-minute tests.</em></small>
<small><em><a href="https://github.com/stanislavkozlovski/pg-queue-pubsub-benchmark/blob/e6ccbd9a3dd7eb64e6498fcccc251095584ea0cc/results/pubsub/4vcpu/single_node/4vcpu.md">[full results link]</a></em></small></p>
<p><strong>Setup:</strong></p>
<ul>
<li><a href="https://instances.vantage.sh/aws/ec2/c7i.xlarge">c7i.xlarge</a> Postgres server /w 25GB gp3 9000 IOPS EBS volume</li>
<li>mostly default Postgres settings (synchronous commit, fsync);
<ul>
<li><code>autovacuum_analyze_scale_factor = 0.05</code> set on the partition tables too (unclear if it has an effect)</li>
</ul>
</li>
<li>each row’s payload is 1 KiB (1024 bytes)</li>
<li>4 topicpartition tables</li>
<li>10 writers (2 writers per partition on average)</li>
<li>5x read fanout via 5 consumer groups</li>
<li>20 reader clients total (4 readers per group)</li>
<li>write batch size: 100 records</li>
<li>read batch size: 200 records</li>
</ul>
<p><strong>Results:</strong></p>
<ul>
<li>
<p>write message rate: <strong>5036 msg/s</strong></p>
</li>
<li>
<p>write throughput: <strong>4.8 MiB/s</strong></p>
</li>
<li>
<p>write latency: 38.7ms p99 / 6.2ms p95</p>
</li>
<li>
<p>read message rate: <strong>25,183 msg/s</strong></p>
</li>
<li>
<p>read message throughput: <strong>24.6 MiB/s</strong></p>
</li>
<li>
<p>read latency: 27.3ms p99 (varied 8.9ms-47ms b/w runs); 4.67ms p95</p>
</li>
<li>
<p>end-to-end latency<sup><a href="#user-content-fn-9" id="user-content-fnref-9-3" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup>: <strong>60ms p99</strong> / 10.6ms p95</p>
</li>
<li>
<p>server kept at ~60% CPU;</p>
</li>
<li>
<p>disk was at ~1200 writes/s with iostat claiming 46 MiB/s</p>
</li>
</ul>
<p>These are pretty good results. It’s funny to think that the majority of people run a complex distributed system like Kafka for similar workloads<sup><a href="#user-content-fn-24" id="user-content-fnref-24" data-footnote-ref="" aria-describedby="footnote-label">13</a></sup>.</p>
<h3 id="4-vcpu-tri-node">4 vCPU Tri-Node</h3>
<p>Now, a replicated setup to more accurately mimic the durability and availability guarantees of Kafka.</p>
<p><small><em>The average of two 5-minute tests.</em></small>
<small><em><a href="https://github.com/stanislavkozlovski/pg-queue-pubsub-benchmark/blob/8164907ba0f1afa6bfec3b402950217f31952d2a/results/pubsub/4vcpu/three_node/4vcpu_replicated.md">[full results link]</a></em></small></p>
<p><strong>Setup:</strong></p>
<ul>
<li>3x <a href="https://instances.vantage.sh/aws/ec2/c7i.xlarge">c7i.xlarge</a> Postgres servers /w 25GB gp3 9000 IOPS EBS volume
<ul>
<li>each on a separate AZ (us-east-1a, us-east-1b, us-east-1c)</li>
<li>one <code>sync</code> replica and one <code>potential</code><sup><a href="#user-content-fn-22" id="user-content-fnref-22" data-footnote-ref="" aria-describedby="footnote-label">14</a></sup> replica</li>
</ul>
</li>
<li>a few custom Postgres settings like <code>wal_compression</code>, <code>max_worker_processes</code>, <code>max_parallel_workers</code>, <code>max_parallel_workers_per_gather</code> and of course - <code>hot_standby</code>
<ul>
<li><code>autovacuum_analyze_scale_factor = 0.05</code> set on the partition tables too (unclear if it has an effect)</li>
</ul>
</li>
<li>each row’s payload is 1 KiB (1024 bytes)</li>
<li>4 topicpartition tables</li>
<li>10 writers (2 writers per partition on average)</li>
<li>5x read fanout via 5 consumer groups</li>
<li>readers only access the primary DB<sup><a href="#user-content-fn-25" id="user-content-fnref-25" data-footnote-ref="" aria-describedby="footnote-label">15</a></sup>; readers are in the same AZ as the primary;</li>
<li>20 reader clients total (4 readers per group)</li>
<li>write batch size: 100 records</li>
<li>read batch size: 200 records</li>
</ul>
<p><strong>Results:</strong></p>
<ul>
<li>
<p>write message rate: <strong>5015 msg/s</strong></p>
</li>
<li>
<p>write throughput: <strong>4.9 MiB/s</strong></p>
</li>
<li>
<p>write latency: 153.45ms p99 / 6.8ms p95</p>
</li>
<li>
<p>read message rate: <strong>25,073 msg/s</strong></p>
</li>
<li>
<p>read message throughput: <strong>24.5 MiB/s</strong></p>
</li>
<li>
<p>read latency: 57ms p99; 4.91ms p95</p>
</li>
<li>
<p>end-to-end latency<sup><a href="#user-content-fn-9" id="user-content-fnref-9-4" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup>: <strong>186ms p99</strong> / 12ms p95</p>
</li>
<li>
<p>server kept at ~65% CPU;</p>
</li>
<li>
<p>disk was at ~1200 writes/s with iostat claiming 46 MiB/s</p>
</li>
</ul>
<p>Now these are astonishing results! Throughput was not impacted at all. Latency increased but not extremely. Our p99 e2e latency 3x’d (60ms vs 185ms), but the p95 barely moved from 10.6ms to 12ms.</p>
<p>This shows that a simple 3-node Postgres cluster can pretty easily sustain what is a very common Kafka workload - 5 MB/s ingest and 25 MB/s egress. Not only that, but for a cheap cost too. Just $11,514 per year.<sup><a href="#user-content-fn-26" id="user-content-fnref-26" data-footnote-ref="" aria-describedby="footnote-label">16</a></sup></p>
<p>Typically, you’d expect Postgres to run more expensive than Kafka at a certain scale, simply because it wasn’t designed to be efficient for this use case.
Not here though. Running Kafka yourself would cost the same. Running the same workload through a Kafka vendor will cost you at least $50,000 a year. 🤯</p>
<p>By the way, in Kafka it’s customary to apply client-side compression on your data. If we assume your messages were 5 KB in size and your clients applied a pretty regular compression ratio of 4x<sup><a href="#user-content-fn-28" id="user-content-fnref-28" data-footnote-ref="" aria-describedby="footnote-label">17</a></sup> - Postgres is actually handling 20 MB/s ingress and 100 MB/s egress.</p>
<h3 id="96-vcpu-single-node">96 vCPU Single Node</h3>
<p>Ok, let’s see how far Postgres will go.</p>
<p><small><em>The results are the average of three 2-minute tests.</em></small>
<small><em><a href="https://github.com/stanislavkozlovski/pg-queue-pubsub-benchmark/blob/8164907ba0f1afa6bfec3b402950217f31952d2a/results/pubsub/96vcpu/single_node/96vcpu.md">[full results link]</a></em></small></p>
<p><strong>Setup:</strong></p>
<ul>
<li><a href="https://instances.vantage.sh/aws/ec2/c7i.24xlarge">c7i.24xlarge</a> (96 vCPU, 192 GiB RAM) Postgres server instance /w 250GB io2 12,000 IOPS EBS volume</li>
<li>modified Postgres settings (<code>huge_pages</code> on, other settings scaled to match the machine);
<ul>
<li>still kept fsync &amp; synchronous_commit on for durability.</li>
<li><code>autovacuum_analyze_scale_factor = 0.05</code> set on the partition tables too (unclear if it has an effect)</li>
</ul>
</li>
<li>each row’s payload is 1 KiB (1024 bytes)</li>
<li>30 topicpartition tables</li>
<li>100 writers (~3.33 writers per partition on average)</li>
<li>5x read fanout via 5 consumer groups</li>
<li>150 reader clients total (5 readers per group)</li>
<li>write batch size: 200 records</li>
<li>read batch size: 200 records</li>
</ul>
<p><strong>Results:</strong></p>
<ul>
<li>
<p>write message rate: <strong>243,000 msg/s</strong></p>
</li>
<li>
<p>write throughput: <strong>238 MiB/s</strong></p>
</li>
<li>
<p>write latency: 138ms p99 / 47ms p95</p>
</li>
<li>
<p>read message rate: <strong>1,200,000 msg/s</strong></p>
</li>
<li>
<p>read message throughput: <strong>1.16 GiB/s</strong></p>
</li>
<li>
<p>read latency: 24.6ms p99</p>
</li>
<li>
<p>end-to-end latency<sup><a href="#user-content-fn-9" id="user-content-fnref-9-5" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup>: <strong>853ms p99</strong> / 242ms p95 / 23.4ms p50</p>
</li>
<li>
<p>server kept at <strong>~10%</strong> CPU (basically idle);</p>
</li>
<li>
<p>bottleneck: The bottleneck was the write rate per partition. It seems like the test wasn’t able to write at a higher rate than 8 MiB/s (8k msg/s) per table with this design. I didn’t push further, but I do wonder now as I write this - how far would writes have scaled?</p>
<ul>
<li>Reads were trivial to scale. Adding more consumer groups was trivial - I tried with 10x fanout and still ran at low CPU. I didn’t include it because I didn’t feel the need to push to an unrealistic read-fanout extreme.</li>
</ul>
</li>
</ul>
<p>240 MiB/s ingress and 1.16 GiB/s egress are pretty good! The 96 vCPU machine was overkill for this test - it could have done a lot more, or we could have simply opted for a smaller machine. For what it’s worth, I do think it’s worth it to deploy a separate Kafka cluster at this scale. Kafka can save you a lot of money here because it can be more efficient in how it handles cross-zone network traffic with features like <a href="https://blog.2minutestreaming.com/p/diskless-kafka-topics-kip-1150">Diskless Kafka</a>.</p>
<h3 id="pub-sub-test-summary">Pub-Sub Test Summary</h3>
<p><small><em>The summarized table with the three test results can be found here <span>→</span> 👉 <a href="https://github.com/stanislavkozlovski/pg-queue-pubsub-benchmark/tree/main/results#pubsub-results">stanislavkozlovski/pg-queue-pubsub-benchmark</a></em></small></p>
<p>These tests seem to show that Postgres is pretty competitive with Kafka at low scale.</p>
<p>You may have noticed none of these tests were particularly long-running. From my understanding, the value in longer-running tests is to test table vacuuming in Postgres, as that can have negative performance effects. In the pub-sub section, vacuuming doesn’t apply because the tables are append-only. My other reasoning for running shorter tests was to keep costs in check and not spend too much time<sup><a href="#user-content-fn-10" id="user-content-fnref-10" data-footnote-ref="" aria-describedby="footnote-label">18</a></sup>.</p>
<p>In any case, no benchmark is perfect. My goal wasn’t to indisputably prove <code>$MY_CLAIM</code>. Rather, I want to start a discussion by showing that what’s possible is likely larger than what most people assume. I certainly didn’t assume I’d get such good numbers, especially with the pub-sub part.</p>
<hr>
<h2 id="pg-as-a-queue">PG as a Queue</h2>
<p>In Postgres, a queue can be implemented with <code>SELECT FOR UPDATE SKIP LOCKED</code>. This command selects an unlocked row and locks it. It also skips reading already-locked rows. That’s how mutual exclusion is achieved - a worker can’t get other workers’ jobs.</p>
<p>Postgres has a very popular <a href="https://github.com/pgmq/pgmq">pgmq</a> library that offers a slick queue API. To keep it simple and understand the end-to-end flow better, I decided to write my own queue. The basic version of it is very easy. My workflow is:</p>
<ol>
<li>add job (<code>INSERT</code>)</li>
<li>lock row &amp; take job (<code>SELECT FOR UPDATE SKIP LOCKED</code>)</li>
<li>process job (<code>{your business logic}</code>)</li>
<li>mark job as “done” (<code>UPDATE</code> a field or <code>DELETE &amp; INSERT</code> the row into a separate table)</li>
</ol>
<p>Postgres competes with RabbitMQ, AWS SQS, NATS, Redis<sup><a href="#user-content-fn-7" id="user-content-fnref-7" data-footnote-ref="" aria-describedby="footnote-label">19</a></sup> and to an extent Kafka<sup><a href="#user-content-fn-8" id="user-content-fnref-8" data-footnote-ref="" aria-describedby="footnote-label">20</a></sup> here.</p>
<h2 id="queue-setup">Queue Setup</h2>
<h4 id="table-1">Table</h4>
<p>We use a simple <code>queue</code> table. When an element is processed off the queue, it’s moved into the archive table.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="sql" data-theme="github-light github-dark"><code data-language="sql" data-theme="github-light github-dark"><span data-line=""><span>CREATE</span><span> TABLE</span><span> queue</span><span> (</span></span>
<span data-line=""><span>  id </span><span>BIGSERIAL</span><span> PRIMARY KEY</span><span>,</span></span>
<span data-line=""><span>  payload </span><span>BYTEA</span><span> NOT NULL</span><span>,</span></span>
<span data-line=""><span>	created_at </span><span>TIMESTAMP</span><span> NOT NULL</span><span> DEFAULT</span><span> NOW</span><span>()</span></span>
<span data-line=""><span>)</span></span>
<span data-line=""> </span>
<span data-line=""><span>CREATE</span><span> TABLE</span><span> queue_archive</span><span> (</span></span>
<span data-line=""><span>  id </span><span>BIGINT</span><span>,</span></span>
<span data-line=""><span>  payload </span><span>BYTEA</span><span> NOT NULL</span><span>,</span></span>
<span data-line=""><span>  created_at </span><span>TIMESTAMP</span><span> NOT NULL</span><span>, </span><span>-- ts the event was originally created at</span></span>
<span data-line=""><span>  processed_at </span><span>TIMESTAMP</span><span> NOT NULL</span><span> DEFAULT</span><span> NOW</span><span>() </span><span>-- ts the event was processed at</span></span>
<span data-line=""><span>)</span></span></code></pre></figure>
<h4 id="writes-1">Writes</h4>
<p>We again run <code>N</code> writer client goroutines.
Each one simply loops and sequentially inserts a single random item into the table:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="sql" data-theme="github-light github-dark"><code data-language="sql" data-theme="github-light github-dark"><span data-line=""><span>INSERT INTO</span><span> queue</span><span> (payload) </span><span>VALUES</span><span> ($</span><span>1</span><span>)</span></span></code></pre></figure>
<p>It only inserts one message per statement, which is pretty inefficient at scale.</p>
<h4 id="reads-1">Reads</h4>
<p>We again run <code>M</code> reader client goroutines. Each reader loops and processes one message.
The processing is done inside a database transaction.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="sql" data-theme="github-light github-dark"><code data-language="sql" data-theme="github-light github-dark"><span data-line=""><span>BEGIN</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>SELECT</span><span> id, payload, created_at</span></span>
<span data-line=""><span>  FROM</span><span> queue</span></span>
<span data-line=""><span>  ORDER BY</span><span> id</span></span>
<span data-line=""><span>  FOR</span><span> UPDATE</span><span> SKIP</span><span> LOCKED</span></span>
<span data-line=""><span>  LIMIT</span><span> 1</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>-- Your business code "processes" the message. In the benchmark, it's a no-op.</span></span>
<span data-line=""> </span>
<span data-line=""><span>DELETE</span><span> FROM</span><span> queue</span><span> WHERE</span><span> id </span><span>=</span><span> $</span><span>1</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>INSERT INTO</span><span> queue_archive (id, payload, created_at, processed_at)</span></span>
<span data-line=""><span>  VALUES</span><span> ($</span><span>1</span><span>,$</span><span>2</span><span>,$</span><span>3</span><span>,</span><span>NOW</span><span>());</span></span>
<span data-line=""> </span>
<span data-line=""><span>COMMIT</span><span>;</span></span></code></pre></figure>
<p>Each reader again only works with one message at a time per transaction.</p>
<h2 id="queue-results-1">Queue Results</h2>
<p>I again ran the same three setups - a single-node 4 vCPU, a 3-node replicated 4 vCPU and a single-node 96 vCPU setup. Here are the summarized results for each:</p>
<h3 id="4-vcpu-single-node-1">4 vCPU Single Node</h3>
<p><small><em>The results are the average of two 15-minute tests. I also ran three 2-minute tests. They all performed similarly.</em></small>
<small><em><a href="https://github.com/stanislavkozlovski/pg-queue-pubsub-benchmark/blob/e6ccbd9a3dd7eb64e6498fcccc251095584ea0cc/results/queue/4vcpu/single_node/4vcpu.md">[full results link]</a></em></small></p>
<p><strong>Setup:</strong></p>
<ul>
<li><a href="https://instances.vantage.sh/aws/ec2/c7i.xlarge">c7i.xlarge</a> Postgres server /w 25GB gp3 9000 IOPS EBS volume</li>
<li>all default Postgres settings<sup><a href="#user-content-fn-16" id="user-content-fnref-16" data-footnote-ref="" aria-describedby="footnote-label">21</a></sup></li>
<li>each row’s payload is 1 KiB (1024 bytes)</li>
<li>10 writer clients, 15 reader clients</li>
</ul>
<p><strong>Results:</strong></p>
<ul>
<li>message rate: <strong>2885 msg/s</strong></li>
<li>throughput: <strong>2.81 MiB/s</strong></li>
<li>write latency: 2.46ms p99</li>
<li>read latency: 4.2ms p99</li>
<li>end-to-end latency<sup><a href="#user-content-fn-9" id="user-content-fnref-9-6" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup>: 17.72ms p99</li>
<li>server kept at ~60% CPU;</li>
</ul>
<p>What I found Postgres wasn’t good at was handling client count. The bottleneck in this setup was the read clients. Each client could not read more than ~192 messages a second because of its median read latency and sequential read nature.</p>
<p>Increasing client count boosted throughput but violated my ~60% CPU target. Trying to run 50 write and 50 read clients got to 4000 msg/s without increasing the queue depth but pegged the server’s CPU to 100%. I wanted to keep the benchmarks realistic for what you may run in production, rather than maxing out what a machine can do. This would be easily alleviated with a connection pooler (standard across all prod PG deployments) or a larger machine.</p>
<p>Another thing worth mentioning is that the workload could sustain a lot more writes than reads. If I didn’t throttle the benchmark, it would write at 12,000 msg/s and read at 2,800 msg/s. In the spirit of simplicity, I didn’t debug further and instead throttled my writes to see at what point I could get a stable 1:1 workload.</p>
<h3 id="4-vcpu-tri-node-1">4 vCPU Tri-Node</h3>
<p><small><em>A single 10-minute test.</em></small>
<small><em><a href="https://github.com/stanislavkozlovski/pg-queue-pubsub-benchmark/blob/e6ccbd9a3dd7eb64e6498fcccc251095584ea0cc/results/queue/4vcpu/three_node/4vcpu_replicated.md">[full results link]</a></em></small></p>
<p><strong>Setup:</strong></p>
<ul>
<li>3x <a href="https://instances.vantage.sh/aws/ec2/c7i.xlarge">c7i.xlarge</a> Postgres servers /w 25GB gp3 9000 IOPS EBS volume
<ul>
<li>each on a separate AZ (us-east-1a, us-east-1b, us-east-1c)</li>
<li>one <code>sync</code> replica and one <code>potential</code><sup><a href="#user-content-fn-22" id="user-content-fnref-22-2" data-footnote-ref="" aria-describedby="footnote-label">14</a></sup> replica</li>
</ul>
</li>
<li>a few custom Postgres settings like <code>wal_compression</code>, <code>max_worker_processes</code>, <code>max_parallel_workers</code>, <code>max_parallel_workers_per_gather</code> and of course - <code>hot_standby</code></li>
<li>each row’s payload is 1 KiB (1024 bytes)</li>
<li>10 writer clients, 15 reader clients</li>
<li>readers only access the primary DB<sup><a href="#user-content-fn-25" id="user-content-fnref-25-2" data-footnote-ref="" aria-describedby="footnote-label">15</a></sup>; readers are in the same AZ as the primary;</li>
</ul>
<p><strong>Results:</strong></p>
<ul>
<li>message rate: <strong>2397 msg/s</strong></li>
<li>throughput: <strong>2.34 MiB/s</strong></li>
<li>write latency: 3.3ms p99</li>
<li>read latency: 7.6ms p99</li>
<li>end-to-end latency<sup><a href="#user-content-fn-9" id="user-content-fnref-9-7" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup>: 920ms p99 ⚠️<sup><a href="#user-content-fn-19" id="user-content-fnref-19-3" data-footnote-ref="" aria-describedby="footnote-label">6</a></sup>; 536ms p95; 7ms p50</li>
<li>server kept at ~60% CPU;</li>
</ul>
<p>As expected, throughput and latency were impacted somewhat. But not that much. It’s still over 2000 messages a second, which is pretty good for an HA queue!</p>
<h3 id="96-vcpu-single-node-1">96 vCPU Single Node</h3>
<p><small><em>The average of three 2-minute tests.</em></small>
<small><em><a href="https://github.com/stanislavkozlovski/pg-queue-pubsub-benchmark/blob/e6ccbd9a3dd7eb64e6498fcccc251095584ea0cc/results/queue/96vcpu/single_node/96vcpu.md">[full results link]</a></em></small></p>
<p><strong>Setup:</strong></p>
<ul>
<li><a href="https://instances.vantage.sh/aws/ec2/c7i.24xlarge">c7i.24xlarge</a> Postgres server instance /w 250GB io2 12,000 IOPS EBS volume</li>
<li>modified Postgres settings (<code>huge_pages</code> on, other settings scaled to match the machine);
<ul>
<li>still kept fsync &amp; synchronous_commit on for durability.</li>
</ul>
</li>
<li>each row’s payload is 1 KiB (1024 bytes)</li>
<li>100 writer clients, 200 reader clients</li>
</ul>
<p><strong>Results:</strong></p>
<ul>
<li>message rate: <strong>20,144 msg/s</strong></li>
<li>throughput: <strong>19.67 MiB/s</strong></li>
<li>write latency: 9.42ms p99</li>
<li>read latency: 22.6ms p99</li>
<li>end-to-end latency: 930ms p99 ⚠️<sup><a href="#user-content-fn-19" id="user-content-fnref-19-4" data-footnote-ref="" aria-describedby="footnote-label">6</a></sup>; 709ms p95; 12.6ms p50</li>
<li>server at 40-60% CPU;</li>
</ul>
<p>This run wasn’t that impressive. There is some bottleneck in the single-table queue approach at this scale which I didn’t bother figuring out. I figured that it wasn’t important to reach absurd numbers on a single table, since all realistic scenarios would have multiple queues and never reach 20,000 msg/s on a single one. The 96 vCPU instance would likely scale far further were we to run a few separate queue tables in parallel.</p>
<h3 id="queue-test-summary">Queue Test Summary</h3>
<p><small><em>The summarized table with the three test results can be found here <span>→</span> 👉 <a href="https://github.com/stanislavkozlovski/pg-queue-pubsub-benchmark/tree/main/results#queue-results">stanislavkozlovski/pg-queue-pubsub-benchmark</a></em></small></p>
<p>Even a modest Postgres node can durably push thousands of queue ops/sec, which already covers the scale 99% of companies ever hit with a single queue.
As I said earlier, the last 2 years have seen the Just Use Postgres slogan become mainstream. The <code>pgmq</code> <a href="https://github.com/pgmq/pgmq">library</a>’s star history captures this trend perfectly:
<img src="https://topicpartition.io/blog/images/pgmq_star_history.png" alt="pgmq"></p>
<hr>
<h2 id="should-you-use-postgres">Should You Use Postgres?</h2>
<p>Most of the time - <strong>yes</strong>. You should always default to Postgres until the constraints prove you wrong.</p>
<p>Kafka is obviously better optimized for pub-sub workloads. Queue systems are obviously better optimized for queue workloads. The point is that <strong>picking a technology based on technical optimization alone is a flawed approach</strong>. To throw an analogy:</p>
<blockquote>
<p>a Formula One car is optimized to drive faster, but I still use a sedan to go to work. I am way more comfortable driving my sedan than an F1 car.</p>
<p><small><em>(seriously, see <a href="https://www.youtube.com/watch?v=nsup4xKpb20">the steering wheel</a> on these things)</em></small></p>
</blockquote>
<p>The Postgres sedan comes with many quality-of-life comforts that the F1 Kafka does not:</p>
<ul>
<li>ability to debug messages with regular SQL</li>
<li>ability to delete, re-order or edit messages in place</li>
<li>ability to join pub-sub data with regular tables</li>
<li>ability to trivially read specific data via rich SQL queries (<code>ID=54</code>, <code>name="John"</code>, <code>cost&gt;1000</code>)</li>
</ul>
<p>Giving up these comforts is a justified sacrifice for your F1 car to go at 378 kmh (235 mph), but masochistic if you plan on driving at 25kmh (15 mph).</p>
<p>Donald Knuth warned us in 1974 - <strong>premature optimization</strong> is the root of all evil. Deploying Kafka at small scale is premature optimization.
The point of this article is to show you that this “small scale” number has grown further than what people remember it to be - it can comfortably mean many megabytes per second.</p>
<p>We are in a Postgres Renaissance for a reason: Postgres is <strong>frequently</strong> good enough. Modern NVMEs and cheap RAM allow it to scale absurdly high.</p>
<p>What’s the alternative?</p>
<h2 id="custom-solutions-for-everything">Custom Solutions for Everything?</h2>
<p>Naive engineers tend to adopt a specialized technology at the slightest hint of a need:</p>
<ul>
<li><em>Need a cache?</em> Redis, of course!</li>
<li><em>Search?</em> Let’s deploy Elasticsearch!</li>
<li><em>Offline data analysis?</em> BigQuery or Snowflake - that’s what our data analysts used at their last job.</li>
<li><em>No schemas?</em> We need a NoSQL database like MongoDB.</li>
<li><em>Have to crunch some numbers on S3?</em> Let’s use Spark!</li>
</ul>
<p>A good engineer thinks through the bigger picture.</p>
<ul>
<li><em>Does this new technology move the needle?</em></li>
<li><em>Is shaving a few milliseconds off our query worth the extra organizational complexity introduced with the change?</em></li>
<li><em>Will our users notice?</em></li>
</ul>
<p>At small scale, these systems hurt you more than they benefit you. Distributed systems - both in terms of node count and system cardinality - should be respected, feared, avoided and employed only as a weapon of last resort against particularly gnarly problems. Everything with a distributed system becomes more challenging and time-consuming.</p>
<p>The problem is <strong>the organizational overhead</strong>. The organizational overhead of adopting a new system, learning its nuances, configs, establishing monitoring, establishing processes around deployments and upgrades, attaining operational expertise on how to manage it, creating runbooks, testing it, debugging it, adopting its clients and API, using its UI, keeping up with its ecosystem, etc.</p>
<p>All of these are real organizational costs that can take months to get right, even if the system in question isn’t difficult (a lot are). Managed SaaS offerings trade off some of the organizational overhead for greater financial costs - but they still don’t remove it all. And until you reach the scale where the technology is necessary, you pay these extra <em>{financial, organizational}</em> costs for zero significant gain.</p>
<p>If the same can be done with tech for which you’ve already paid the organizational costs for (e.g Postgres), adopting something else prematurely is most definitely an anti-pattern. You don’t need web-scale technologies when you don’t have web-scale problems.</p>
<h2 id="mvi-a-better-alternative">MVI (a better alternative)</h2>
<p>What I think is a better approach is to search for the <strong>minimum viable infrastructure</strong> (MVI): build the smallest amount of system while still providing value.</p>
<ol>
<li>choose <strong>good-enough</strong> technology your org is already <strong>familiar</strong> with
<ul>
<li><em>good-enough</em> == meets your users’ needs without being too slow/expensive/insecure</li>
<li><em>familiar</em> == your org has prior experience, has runbooks/ops setups, monitoring, UI, etc</li>
</ul>
</li>
<li>solve a real problem with it</li>
<li>use the minimum set of features
<ul>
<li>the fewer features you use, the more flexibility you have to move off the infra in question in the future (e.g if locked in with a vendor)</li>
</ul>
</li>
</ol>
<p>Bonus points if that technology:</p>
<ul>
<li>is widely adopted so finding good engineers for it is trivial (Postgres - check)</li>
<li>has a strong and growing network effect (Postgres - check)</li>
</ul>
<p>The MVI approach reduces the surface area of your infra. The fewer moving parts you have, the fewer failure modes you worry about and the less glue code you have to maintain.</p>
<p>Unfortunately, it’s human nature to go against this. Just like startups suffer due to <a href="https://en.wikipedia.org/wiki/Minimum_viable_product">MVP</a> bloat <em>(one more feature!)</em>, infra teams suffer due to MVI bloat <em>(one more system!)</em></p>
<h2 id="why-are-we-like-this">Why are we like this?</h2>
<p>I won’t pretend to be able to map out the exact path-dependent outcome, but my guess is this:</p>
<ol start="0">
<li>the zero interest rate era gave us abundant speculative money that was invested in any company that could grow fast</li>
<li>a lot of viral internet companies were growing at speeds that led old infra to become obsolete fast</li>
<li>this prompted the next wave of ZIRP investment - specialized data infrastructure companies (in a gold rush, sell shovels!); some of these data infra startups spun off directly from the high-growth companies themselves</li>
<li>each well-funded data infra vendor is financially motivated to evangelize their product and have you adopt it even when you don’t need to (<a href="https://topicpartition.io/blog/everyone-is-talking-their-book" data-slug="blog/everyone-is-talking-their-book">Everyone is Talking Their Book</a>). They had deep pockets for marketing and used them.</li>
<li>innovative infrastructure software got engineered. It was exciting - so engineers got <a href="https://xkcd.com/356/">nerd-sniped</a> into it</li>
<li>a <a href="https://www.youtube.com/watch?v=b2F-DItXtZs">web-scale</a> craze/cargo cult developed, where everybody believed they need to be able to scale from zero to millions of RPS because they may go viral any day.</li>
<li>a trend developed to copy whatever solutions the most successful, largest digital-native companies were using (Amazon, Google, Uber, etc.)</li>
<li>the trend became a self-perpetuating prophecy: these technologies became a sought-after skill on resumes
<ul>
<li>system design interview questions were adapted to test for knowledge of these systems</li>
<li>within an organization, engineers (knowingly or not) pushed for projects that are exciting and helped build their resumes;</li>
</ul>
</li>
</ol>
<p>This trend continues to grow while there is no strong competing force that is sufficiently motivated to push the opposite view. Even engineers inside a company, who ought to be motivated to keep things simple, have strong incentives to pursue extra complexity. It benefits their career by giving them a project to use as ammo for their next promotion and improves their resume (cool tech/story on there) for their next job-hop. Plus it’s simply more fun.</p>
<p>This is why I think we, as an industry, don’t always use the simplest solution available.</p>
<p>In most cases, Postgres is that simplest solution that is available.</p>
<h2 id="but-it-wont-scale">But It Won’t Scale!</h2>
<p>I want to wrap this article up, but one rebuttal I can’t miss addressing is the “it won’t scale argument”.</p>
<p>The argument goes something like this: “in today’s age we can go viral at a moment’s notice; these viral moments are very valuable for our business so we need to aggressively design in a way that keeps our app stable under traffic spikes”</p>
<p>I have three arguments against this:</p>
<h3 id="1-postgres-scales">1. Postgres Scales</h3>
<p>As of 2025, OpenAI <a href="https://news.ycombinator.com/item?id=44074702">still uses</a> an unsharded Postgres architecture with only one primary instance for writes<sup><a href="#user-content-fn-17" id="user-content-fnref-17" data-footnote-ref="" aria-describedby="footnote-label">22</a></sup>. OpenAI is <em><strong>the</strong></em> poster-child of rapid viral growth. They hold the record for <a href="https://www.researchgate.net/figure/Time-to-reach-100-million-users-for-different-technologies-in-months-after-initial_fig1_372212988">the fastest startup to reach 100 million users</a>.</p>
<p><a href="https://bohanzhang.me/">Bohan Zhang</a>, a member of OpenAI’s infrastructure team and co-founder of <a href="https://ottertune.com/">OtterTune</a> (a Postgres tuning service), can be quoted as saying<sup><a href="#user-content-fn-29" id="user-content-fnref-29" data-footnote-ref="" aria-describedby="footnote-label">23</a></sup>:</p>
<blockquote>
<p><em>“At OpenAI, we utilize an unsharded architecture with one writer and multiple readers, demonstrating that PostgreSQL can scale gracefully under massive read loads.”</em></p>
<p><em>“The main message of my talk was that if you are not too write heavy, you can scale Postgres to a very high read throughput with read replicas using only a single master! That is exactly the message that needs to be spelled out as that covers <strong>the vast majority</strong> of apps.”</em></p>
<p><em>“Postgres is probably the default choice for developers right now. You can use Postgres for a very long time. If you are building a startup with read-heavy workloads, just start with Postgres. If you hit a scalability issue, increase the instance size. You can scale it to a very large scale. If in the future the database becomes a bottleneck, congratulations. You have built a successful startup. It’s a good problem to have.”</em></p>
<p>(slightly edited for clarity and grammar)</p>
</blockquote>
<p>Despite their rapid growth to a user base of more than 800 million, OpenAI has still NOT opted for a web-scale distributed database. If they haven’t… why does your unproven project need to?</p>
<h3 id="2-you-have-more-time-to-scale-than-you-think">2. You Have More Time To Scale Than You Think</h3>
<p>Let’s say it’s a good principle to design/test for ~10x your scale. Here are the years of <em>consistent</em> growth rate it takes to get to 10x your current scale:</p>





































<div><table><thead><tr><th>annual growth</th><th>years to hit 10× scale</th></tr></thead><tbody><tr><td>10 %</td><td>24.16 y</td></tr><tr><td>25 %</td><td>10.32 y</td></tr><tr><td>50 %</td><td>5.68 y</td></tr><tr><td>75 %</td><td>4.11 y</td></tr><tr><td>100 %</td><td>3.32 y</td></tr><tr><td>150 %</td><td>2.51 y</td></tr><tr><td>200 %</td><td>2.10 y</td></tr></tbody></table></div>
<p>It goes to show that even at extreme growth levels, you still have years to migrate between solutions.
The majority of developers, though, work at companies in the 0-50% growth rate. They are more likely to have moved on to another job by the time the solution needs to change (if ever).</p>
<h3 id="3-its-overdesign">3. It’s Overdesign</h3>
<p>In an ideal world, you <em>would</em> build for scale and any other future problem you may hit in 10 years.</p>
<p>In the real world, you have finite bandwidth, so you have to build for the most immediate, highest ROI problem.</p>
<p><a href="https://lobste.rs/s/wshruu/small_data#c_kjygo0">Commenter snej on lobste.rs</a> captured it well:</p>
<blockquote>
<p>Planning your infrastructure around being able to handle that is sort of like buying a huge Marshall stack as your first guitar amp because your garage band might get invited to open for Coldplay.</p>
</blockquote>
<h2 id="conclusion">Conclusion</h2>
<p>Just use Postgres until it breaks.</p>
<hr>
<h3 id="disclaimers"><em>Disclaimers</em></h3>
<ul>
<li>
<p><em>Title inspiration comes from a great recent piece - <a href="https://dizzy.zone/2025/09/24/Redis-is-fast-Ill-cache-in-Postgres/">“Redis is fast - I’ll cache in Postgres”</a></em></p>
</li>
<li>
<p><em>I’m a complete Postgres noob. You may see a lot of dumb mistakes here. Feel free to call me out on them - I’m happy to learn. I used AI to help a lot with some of the PG tools to use. This both shows how inexperienced I am in the context and how easy it is to start. I am generally skeptical of AI’s promise (in the short-term), but there’s no denying it has made a large dent in democratizing niche/low-level knowledge.</em></p>
</li>
</ul>
<p>If you’d like to reach out to me, you can find me on <a href="https://www.linkedin.com/in/stanislavkozlovski/">LinkedIn</a> or <a href="https://x.com/BdKozlovski">X (Twitter)</a>.</p>
<section data-footnotes="">
<ol>
<li id="user-content-fn-1">
<p>Don’t worry if you don’t fully understand these terms. I work full-time in the industry that spews these things and I don’t have a great grasp either. It’s marketing slop. <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="user-content-fn-5">
<p>Gartner and others push embarrassing recommendations that aren’t tech driven. It’s frequently the opposite - they’re profit driven. Gartner makes $6.72B purely off a consulting service that charges organizations $50k <em>per seat</em> solely for access to reports that recommend these slop architectures. It’s not crazy to believe, hence many people are converging with the idea that it is a <a href="https://www.linkedin.com/posts/gergelyorosz_gartner-has-been-out-of-touch-with-tech-analysis-activity-7374374378240786432-9WsQ">pay-to-win racket</a> model. <a href="#user-content-fnref-5" data-footnote-backref="" aria-label="Back to reference 2">↩</a></p>
</li>
<li id="user-content-fn-4">
<p>Seriously, the improvement in hardware is something I find most senior engineers haven’t properly appreciated. Newest gen AMD CPUs boast <a href="https://www.amd.com/en/products/processors/server/epyc/9005-series.html">192 cores</a>. Modern SSDs can do <strong>5.5 million</strong> random reads a second, or ~28GB/s sequential reads. Both are a 10-20x improvement over the last 10 years alone. Single nodes are more powerful than ever. <a href="#user-content-fnref-4" data-footnote-backref="" aria-label="Back to reference 3">↩</a></p>
</li>
<li id="user-content-fn-3">
<p>Just in the last 6 months - Snowflake acquired Crunchy Data for ~$250M, Databricks acquired Neon for ~$1 <strong>b</strong>illion;
In the last 12 months, Supabase more than <strong>5x’d</strong> its valuation from ($900M to $5B), raising $380M across <strong>three</strong> series (!!!). Within a single year! <a href="#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 4">↩</a></p>
</li>
<li id="user-content-fn-9">
<p>End-to-end latency here is defined as <code>now() - event_create_time</code>; In essence, it tracks how long a brand new persisted event takes to get consumed. It helps show cases where queue lag spikes like when consumers temporarily fall behind the write rate. <a href="#user-content-fnref-9" data-footnote-backref="" aria-label="Back to reference 5">↩</a> <a href="#user-content-fnref-9-2" data-footnote-backref="" aria-label="Back to reference 5-2">↩<sup>2</sup></a> <a href="#user-content-fnref-9-3" data-footnote-backref="" aria-label="Back to reference 5-3">↩<sup>3</sup></a> <a href="#user-content-fnref-9-4" data-footnote-backref="" aria-label="Back to reference 5-4">↩<sup>4</sup></a> <a href="#user-content-fnref-9-5" data-footnote-backref="" aria-label="Back to reference 5-5">↩<sup>5</sup></a> <a href="#user-content-fnref-9-6" data-footnote-backref="" aria-label="Back to reference 5-6">↩<sup>6</sup></a> <a href="#user-content-fnref-9-7" data-footnote-backref="" aria-label="Back to reference 5-7">↩<sup>7</sup></a></p>
</li>
<li id="user-content-fn-19">
<p>Some queue tests showed higher E2E latencies which I believe was due to a bug. In the pub-sub tests, I ensured readers start <em>before</em> the writers via a 1000ms sleep. For the queue tests, though, I didn’t do this. The result is that queue tests immediately spike queue depth at startup because the writers manage to get a head start before the readers. I believe the E2E latency is artificially high because of this flaw in the test. <a href="#user-content-fnref-19" data-footnote-backref="" aria-label="Back to reference 6">↩</a> <a href="#user-content-fnref-19-2" data-footnote-backref="" aria-label="Back to reference 6-2">↩<sup>2</sup></a> <a href="#user-content-fnref-19-3" data-footnote-backref="" aria-label="Back to reference 6-3">↩<sup>3</sup></a> <a href="#user-content-fnref-19-4" data-footnote-backref="" aria-label="Back to reference 6-4">↩<sup>4</sup></a></p>
</li>
<li id="user-content-fn-6">
<p>Actually, things are ordered in the happy path. Only during retries can you get out of order processing. e.g at t=0, client A reads task N; At t=1, client B reads task N+1 and processes it successfully; At t=2, A fails and is unable to process task N; At t=3, client B takes the next available task - which is N. B therefore executes the tasks in order [N+1, N], whereas proper order would have been [N, N+1] <a href="#user-content-fnref-6" data-footnote-backref="" aria-label="Back to reference 7">↩</a></p>
</li>
<li id="user-content-fn-13">
<p>Open-source projects include <a href="https://pulsar.apache.org/">Apache Pulsar</a> (open source), <a href="https://github.com/redpanda-data/redpanda/">RedPanda</a> (source-available), <a href="https://github.com/AutoMQ">AutoMQ</a> (a fork of Kafka) and a lot of proprietary offerings - <a href="https://aws.amazon.com/kinesis/">AWS Kinesis</a>, <a href="https://cloud.google.com/pubsub">Google Pub/Sub</a>, <a href="https://azure.microsoft.com/products/event-hubs">Azure Event Hubs</a>, <a href="https://www.confluent.io/confluent-cloud/kora/">Confluent Kora</a>, <a href="https://www.warpstream.com/">Confluent WarpStream</a>, <a href="https://buf.build/product/bufstream">Bufstream</a> to name a few. What’s common in 90% of these projects is that they all <strong><em>implement</em></strong> the Apache Kafka API, making Kafka undoubtedly the protocol standard in the space. There’s also an open-source project which exposes a Kafka API on top of a pluggable Postgres or S3 backend - <a href="https://github.com/tansu-io/tansu">Tansu</a> (Rust, btw :] ) <a href="#user-content-fnref-13" data-footnote-backref="" aria-label="Back to reference 8">↩</a></p>
</li>
<li id="user-content-fn-27">
<p>The most popular library I could find is <a href="https://github.com/imqueue/pg-pubsub">pg-pubsub</a> with 106 stars as of writing (Oct 2025). Its last commit was 3 months ago. <a href="#user-content-fnref-27" data-footnote-backref="" aria-label="Back to reference 9">↩</a></p>
</li>
<li id="user-content-fn-20">
<p>Batching messages per client is very important for scalability here. It is one of Kafka’s least-talked-about performance “tricks”. <a href="#user-content-fnref-20" data-footnote-backref="" aria-label="Back to reference 10">↩</a></p>
</li>
<li id="user-content-fn-14">
<p>These tables act as different log data structures. You can see them as separate <strong>topics</strong>, or <strong>partitions</strong> of one topic (shards). <a href="#user-content-fnref-14" data-footnote-backref="" aria-label="Back to reference 11">↩</a></p>
</li>
<li id="user-content-fn-21">
<p>Postgres stores all <code>NOTIFY</code> events in a single, global queue. If this queue becomes full, transactions calling <code>NOTIFY</code> will fail when committing. (<a href="https://lobste.rs/c/iix4ph">src</a>) <a href="#user-content-fnref-21" data-footnote-backref="" aria-label="Back to reference 12">↩</a></p>
</li>
<li id="user-content-fn-24">
<p>A <a href="https://cdn.prod.website-files.com/6659da8aecd70e0898c0d7ed/672fb52dc29ab4cdf8a59525_The-State-of-Streaming-Data_Report-by-Redpanda.pdf">report by RedPanda</a> found that ~55% of respondents use Kafka for &lt; 1 MB/s. Kafka-vendor Aiven <a href="https://aiven.io/blog/apache-kafkas-80-percent-problem">similarly shared</a> that 50% of their Kafka deployments have an ingest rate of below 10 MB/s. <a href="#user-content-fnref-24" data-footnote-backref="" aria-label="Back to reference 13">↩</a></p>
</li>
<li id="user-content-fn-22">
<p>This replication is equivalent to RF=2 in Kafka with one extra non-synchronous replica. Call it RF=2.5. The client receives a response when the one <code>sync</code> replica confirms the change. The other <code>potential</code> replica is replicating asynchronously without blocking the write path. It will become promoted to <code>sync</code> if the other one was to die. <a href="#user-content-fnref-22" data-footnote-backref="" aria-label="Back to reference 14">↩</a> <a href="#user-content-fnref-22-2" data-footnote-backref="" aria-label="Back to reference 14-2">↩<sup>2</sup></a></p>
</li>
<li id="user-content-fn-25">
<p>The tests didn’t direct any read traffic to the standbys. This caused extra load on the primary - most production workloads would read from the standbys. Despite that, the results were still good! In my tests, I found that the extra read workload didn’t seem to have a negative effect on the database - it seems such tail reads were served exclusively from cache. <a href="#user-content-fnref-25" data-footnote-backref="" aria-label="Back to reference 15">↩</a> <a href="#user-content-fnref-25-2" data-footnote-backref="" aria-label="Back to reference 15-2">↩<sup>2</sup></a></p>
</li>
<li id="user-content-fn-26">
<p>The node and its disk cost <a href="https://github.com/stanislavkozlovski/pg-queue-pubsub-benchmark/blob/main/results/pubsub/4vcpu/single_node/4vcpu.md#-cost">$1826 per year</a>. Since we run three of those, it’s $5478/yr. The networking in AWS costs $0.02/GB and our setup is replicating 4.9MB/s across two instances - that results in 294.74TB cross-zone networking per year. That’s $6036 per year in replication networking. Assuming your clients are in the same zone as the database they’re writing to / reading from, that networking is free. That results in an annual cost of $11,514. <a href="#user-content-fnref-26" data-footnote-backref="" aria-label="Back to reference 16">↩</a></p>
</li>
<li id="user-content-fn-28">
<p>We can realistically achieve a <a href="https://www.linkedin.com/pulse/subtle-art-cost-comparisons-tristan-stevens-tqtue">10x+ compression ratio</a> if operating on compressible data like logs (something Kafka is used for frequently). The only gotcha is that we need to compress larger batches - eg 25KB+ - so that requires a bit of a re-design in the pub-sub data model. <a href="#user-content-fnref-28" data-footnote-backref="" aria-label="Back to reference 17">↩</a></p>
</li>
<li id="user-content-fn-10">
<p>I had already spent enough business days working on this benchmark and re-running tests numerous, numerous times as I iterated on the benchmark and the methodology. On the larger instances, the cost accumulates fast and running longer tests at high MB/s rates requires deploying much larger and more expensive disks in order to store all the accumulated data. The effort spent matches the goal I have with the article. If any Postgres vendor wants to sponsor a more thorough investigation - let me know! <a href="#user-content-fnref-10" data-footnote-backref="" aria-label="Back to reference 18">↩</a></p>
</li>
<li id="user-content-fn-7">
<p>Surprisingly (to me), Redis is a very popular queue-like backend choice for background jobs. <a href="https://github.com/topics/background-jobs">Most popular open-source libraries</a> use it. Although I’m sure Postgres can do just as good a job, many devs will prefer to use an established library rather than build one from scratch or use something less well-maintained. I do think PG-backed libraries should get developed though! <a href="#user-content-fnref-7" data-footnote-backref="" aria-label="Back to reference 19">↩</a></p>
</li>
<li id="user-content-fn-8">
<p>Kafka has historically never been a queue. To use it as one, you had to develop some difficult workarounds. Today, however, it is in the middle of implementing a first-class Queue-like interface (currently in <a href="https://cwiki.apache.org/confluence/display/KAFKA/Queues+for+Kafka+%28KIP-932%29+-+Preview+Release+Notes">Preview</a>) <a href="#user-content-fnref-8" data-footnote-backref="" aria-label="Back to reference 20">↩</a></p>
</li>
<li id="user-content-fn-16">
<p>Most importantly, synchronous commit and fsync are both on. This means every write is durably persisted to disk. <a href="#user-content-fnref-16" data-footnote-backref="" aria-label="Back to reference 21">↩</a></p>
</li>
<li id="user-content-fn-17">
<p>The optimizations they did to support this scale are cool, but not novel. See these two talks at a) <a href="https://www.youtube.com/watch?v=Ni1SGhNu-Q4">PGConf.dev 2025</a> (<a href="https://gist.github.com/stanislavkozlovski/d1283b784eed03a8dfe126297c11b7e6">my transcript</a>) and b) <a href="https://www.youtube.com/watch?v=NvY2kvi1Fa0">POSETTE</a> (<a href="https://gist.github.com/stanislavkozlovski/7853a1c0a73caba81de53f4e36c618f5">my transcript</a>) <a href="#user-content-fnref-17" data-footnote-backref="" aria-label="Back to reference 22">↩</a></p>
</li>
<li id="user-content-fn-29">
<p>From the talks <a href="https://www.youtube.com/watch?v=Ni1SGhNu-Q4">PGConf.dev 2025</a> (<a href="https://gist.github.com/stanislavkozlovski/d1283b784eed03a8dfe126297c11b7e6">my transcript</a>) and <a href="https://www.youtube.com/watch?v=NvY2kvi1Fa0">POSETTE</a> (<a href="https://gist.github.com/stanislavkozlovski/7853a1c0a73caba81de53f4e36c618f5">my transcript</a>) <a href="#user-content-fnref-29" data-footnote-backref="" aria-label="Back to reference 23">↩</a></p>
</li>
</ol>
</section></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Israel demanded Google and Amazon use secret 'wink' to sidestep legal orders (130 pts)]]></title>
            <link>https://www.theguardian.com/us-news/2025/oct/29/google-amazon-israel-contract-secret-code</link>
            <guid>45746482</guid>
            <pubDate>Wed, 29 Oct 2025 13:20:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/us-news/2025/oct/29/google-amazon-israel-contract-secret-code">https://www.theguardian.com/us-news/2025/oct/29/google-amazon-israel-contract-secret-code</a>, See on <a href="https://news.ycombinator.com/item?id=45746482">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p><span>W</span>hen <a href="https://www.theguardian.com/technology/google" data-link-name="in body link">Google</a> and <a href="https://www.theguardian.com/technology/amazon" data-link-name="in body link">Amazon</a> negotiated a major $1.2bn cloud-computing deal in 2021, their customer – the Israeli government – had an unusual demand: agree to use a secret code as part of an arrangement that would become known as the “winking mechanism”.</p><p>The demand, which would require Google and Amazon to effectively sidestep legal obligations in countries around the world, was born out of Israel’s concerns that data it moves into the global corporations’ cloud platforms could end up in the hands of foreign law enforcement authorities.</p><p>Like other big tech companies, Google and Amazon’s cloud businesses routinely comply with requests from police, prosecutors and security services to hand over customer data to assist investigations.</p><p>This process is often cloaked in secrecy. The companies are frequently gagged from alerting the affected customer their information has been turned over. This is either because the law enforcement agency has the power to demand this or a court has ordered them to stay silent.</p><p>For Israel, losing control of its data to authorities overseas was a significant concern. So to deal with the threat,<strong> </strong>officials created<strong> </strong>a secret warning system: the companies must send signals hidden in payments to the Israeli government, tipping it off when it has disclosed Israeli data to foreign courts or investigators.</p><p>To clinch the lucrative contract, Google and Amazon agreed to the so-called winking mechanism, according to leaked documents seen by the Guardian, as part of a joint investigation with Israeli-Palestinian publication <a href="https://www.972mag.com/project-nimbus-contract-google-amazon-israel/" data-link-name="in body link">+972 Magazine</a> and Hebrew-language outlet <a href="https://www.mekomit.co.il/ps/157869/" data-link-name="in body link">Local Call</a>.</p><p>Based on the documents and descriptions of the contract by Israeli officials, the investigation reveals how the companies bowed to a series of stringent and unorthodox “controls” contained within the 2021 deal, known as Project Nimbus. Both <a href="https://www.theguardian.com/technology/google" data-link-name="in body link" data-component="auto-linked-tag">Google</a> and Amazon’s cloud businesses have denied evading any legal obligations.</p><p>The strict controls include measures that prohibit the US companies from restricting how an array of Israeli government agencies, security services and military units use their cloud services. According to the deal’s terms, the companies cannot suspend or withdraw Israel’s access to its technology, even if it’s found to have violated their terms of service.</p><p>Israeli officials inserted the controls to counter a series of anticipated threats. They feared Google or Amazon might bow to employee or shareholder pressure and withdraw Israel’s access to its products and services if linked to human rights abuses in the occupied Palestinian territories.</p><p>They were also concerned the companies could be vulnerable to overseas legal action, particularly in cases relating to the use of the technology in the military occupation of the West Bank and Gaza.</p><p>The terms of the Nimbus deal would appear to prohibit Google and Amazon from the kind of unilateral <a href="https://www.theguardian.com/world/2025/sep/25/microsoft-blocks-israels-use-of-its-technology-in-mass-surveillance-of-palestinians" data-link-name="in body link">action taken by Microsoft last month</a>, when it disabled the Israeli military’s access to technology used to operate an indiscriminate surveillance system monitoring Palestinian phone calls.</p><p>Microsoft, which provides a range of cloud services to Israel’s military and public sector, bid for the Nimbus contract but was beaten by its rivals. According to sources familiar with negotiations, Microsoft’s bid suffered as it refused to accept some of Israel’s demands.</p><p>As with Microsoft, Google and Amazon’s cloud businesses have faced scrutiny in recent years over the role of their technology – and the Nimbus contract in particular – in Israel’s two-year war on Gaza.</p><figure id="e561623b-754a-4fc5-9bae-9f58879387dc" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-2"><picture><source srcset="https://i.guim.co.uk/img/media/bd7b3c030de0aa3bd917b98569f8c13f04c10350/0_0_6000_4000/master/6000.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/bd7b3c030de0aa3bd917b98569f8c13f04c10350/0_0_6000_4000/master/6000.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/bd7b3c030de0aa3bd917b98569f8c13f04c10350/0_0_6000_4000/master/6000.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/bd7b3c030de0aa3bd917b98569f8c13f04c10350/0_0_6000_4000/master/6000.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/bd7b3c030de0aa3bd917b98569f8c13f04c10350/0_0_6000_4000/master/6000.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/bd7b3c030de0aa3bd917b98569f8c13f04c10350/0_0_6000_4000/master/6000.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="people wear shirts that read ‘Google workers say drop project nimbus’" src="https://i.guim.co.uk/img/media/bd7b3c030de0aa3bd917b98569f8c13f04c10350/0_0_6000_4000/master/6000.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="296.66666666666663" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>Ex-Google employees speak about Google’s Project Nimbus at the University of California, Berkeley, in Berkeley, California, on 25 April 2024.</span> Photograph: Tayfun Coskun/Anadolu via Getty Images</figcaption></figure><p>During its offensive in the territory, where a <a href="https://www.theguardian.com/world/2025/sep/16/israel-committed-genocide-in-gaza-says-un-inquiry" data-link-name="in body link">UN commission of inquiry concluded</a> that Israel has committed genocide, the Israeli military has <a href="https://www.972mag.com/cloud-israeli-army-gaza-amazon-google-microsoft/" data-link-name="in body link">relied heavily on cloud providers</a> to store and analyse large volumes of data and intelligence information.</p><p>One such dataset was <a href="https://www.theguardian.com/world/2025/aug/06/microsoft-israeli-military-palestinian-phone-calls-cloud" data-link-name="in body link">the vast collection of intercepted Palestinian calls</a> that until August was stored on Microsoft’s cloud platform. According to intelligence sources, the Israeli military planned to move the data to Amazon Web Services (AWS) datacentres.</p><p>Amazon did not respond to the Guardian’s questions about whether it knew of Israel’s plan to migrate the mass surveillance data to its cloud platform. A spokesperson for the company said it respected “the privacy of our customers and we do not discuss our relationship without their consent, or have visibility into their workloads” stored in the cloud.</p><p>Asked about the winking mechanism, both Amazon and Google denied circumventing legally binding orders. “The idea that we would evade our legal obligations to the US government as a US company, or in any other country, is categorically wrong,” a Google spokesperson said.</p><figure id="34c35196-1993-4a21-86e5-7ce6bcd19c78" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.GuideAtomBlockElement"><gu-island name="GuideAtomWrapper" priority="feature" deferuntil="visible" props="{&quot;id&quot;:&quot;567057a3-5948-48a4-adbd-7698fd9a54e8&quot;,&quot;title&quot;:&quot;Contact Harry Davies and Yuval Abraham about this story&quot;,&quot;html&quot;:&quot;<p><strong></strong></p><p>If you have something to share about  this story, you can contact Harry Davies and Yuval Abraham using one of the following methods.</p><p><strong>Secure Messaging in the Guardian app</strong></p><p>The Guardian app has a tool to send tips about stories. Messages are end to end encrypted and concealed within the routine activity that every Guardian mobile app performs. This prevents an observer from knowing that you are communicating with us at all, let alone what is being said.</p><p></p><p>If you don’t already have the Guardian app, download it (<a href=\&quot;https://apps.apple.com/app/the-guardian-live-world-news/id409128287\&quot;>iOS</a>/<a href=\&quot;https://play.google.com/store/apps/details?id=com.guardian\&quot;>Android</a>) and go to the menu. Select ‘Secure Messaging’.</p><p></p><p>To send a message to Harry and Yuval please choose the ‘UK Investigations’ team.</p><p> </p><p><strong>Signal Messenger</strong></p><p>You can message Harry using the Signal Messenger app. Use the ‘find by username’ option and type&amp;nbsp;<b>hfd.90</b></p><p><strong>Email (not secure)</strong></p><p>If you don’t need a high level of security or confidentiality you can email  <a href=\&quot;mailto:harry.davies@theguardian.com\&quot;>harry.davies@theguardian.com</a></p><p><strong>SecureDrop and other secure methods</strong></p><p>If you can safely use the tor network without being observed or monitored you can send messages and documents to the Guardian via our <a href=\&quot;https://www.theguardian.com/securedrop\&quot;>SecureDrop platform</a>.</p><p></p><p>Finally, our guide at <a href=\&quot;https://www.theguardian.com/tips\&quot;>theguardian.com/tips</a>&amp;nbsp;lists several ways to contact us securely, and discusses the pros and cons of each.&amp;nbsp;</p>&quot;,&quot;image&quot;:&quot;https://i.guim.co.uk/img/media/ae475ccca7c94a4565f6b500a485479f08098383/788_0_4000_4000/4000.jpg?width=620&amp;quality=85&amp;auto=format&amp;fit=max&amp;s=45fd162100b331bf1618e364c5c69452&quot;,&quot;credit&quot;:&quot;Illustration: Guardian Design / Rich Cousins&quot;}"><div data-atom-id="567057a3-5948-48a4-adbd-7698fd9a54e8" data-atom-type="guide"><details data-atom-id="567057a3-5948-48a4-adbd-7698fd9a54e8" data-snippet-type="guide"><summary><span>Quick Guide</span><h4>Contact Harry Davies and Yuval Abraham about this story</h4><span><span><span></span>Show</span></span></summary><div><p><img src="https://i.guim.co.uk/img/media/ae475ccca7c94a4565f6b500a485479f08098383/788_0_4000_4000/4000.jpg?width=620&amp;quality=85&amp;auto=format&amp;fit=max&amp;s=45fd162100b331bf1618e364c5c69452" alt=""></p><div><p>If you have something to share about  this story, you can contact Harry Davies and Yuval Abraham using one of the following methods.</p><p><strong>Secure Messaging in the Guardian app</strong></p><p>The Guardian app has a tool to send tips about stories. Messages are end to end encrypted and concealed within the routine activity that every Guardian mobile app performs. This prevents an observer from knowing that you are communicating with us at all, let alone what is being said.</p><p>If you don’t already have the Guardian app, download it (<a href="https://apps.apple.com/app/the-guardian-live-world-news/id409128287">iOS</a>/<a href="https://play.google.com/store/apps/details?id=com.guardian">Android</a>) and go to the menu. Select ‘Secure Messaging’.</p><p>To send a message to Harry and Yuval please choose the ‘UK Investigations’ team.</p><p><strong>Signal Messenger</strong></p><p>You can message Harry using the Signal Messenger app. Use the ‘find by username’ option and type&nbsp;<b>hfd.90</b></p><p><strong>Email (not secure)</strong></p><p>If you don’t need a high level of security or confidentiality you can email  <a href="mailto:harry.davies@theguardian.com">harry.davies@theguardian.com</a></p><p><strong>SecureDrop and other secure methods</strong></p><p>If you can safely use the tor network without being observed or monitored you can send messages and documents to the Guardian via our <a href="https://www.theguardian.com/securedrop">SecureDrop platform</a>.</p><p>Finally, our guide at <a href="https://www.theguardian.com/tips">theguardian.com/tips</a>&nbsp;lists several ways to contact us securely, and discusses the pros and cons of each.&nbsp;</p></div><div><p>Illustration: Guardian Design / Rich Cousins</p></div></div></details></div></gu-island></figure><p>Referring to statements <a href="https://www.theguardian.com/technology/2024/apr/27/google-project-nimbus-israel" data-link-name="in body link">Google has previously made</a> claiming Israel had agreed to abide by Google policies, the spokesperson added: “We’ve been very clear about the Nimbus contract, what it’s directed to, and the terms of service and acceptable use policy that govern it. Nothing has changed. This appears to be yet another attempt to falsely imply otherwise.”</p><p>However, according to the<strong> </strong>Israeli government documents detailing the controls inserted into the Nimbus agreement, officials concluded they had extracted important concessions from Google and Amazon after the companies agreed to adapt internal processes and “subordinate” their standard contractual terms in favour of Israel’s demands.</p><p>A government memo circulated several months after the deal was signed stated: “[The companies] understand the sensitivities of the Israeli government and are willing to accept our requirements.”</p><h2 id="how-the-secret-code-works">How the secret code works</h2><p>Named after the towering cloud formations, the Nimbus contract – which runs for an initial seven years with the possibility of extension – is a flagship Israeli government initiative to store information from across the public sector and military in commercially owned datacentres.</p><p>Even though its data would be stored in Google and Amazon’s newly built Israel-based datacentres, Israeli officials feared developments in US and European laws could create more direct routes for law enforcement agencies to obtain it via direct requests or court-issued subpoenas.</p><figure id="c9443020-7c9f-4e7e-a5ec-0f918e2075a3" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-3"><picture><source srcset="https://i.guim.co.uk/img/media/4266a95890ac90566270619766b2b770d9025b7c/0_0_4427_2952/master/4427.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/4266a95890ac90566270619766b2b770d9025b7c/0_0_4427_2952/master/4427.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/4266a95890ac90566270619766b2b770d9025b7c/0_0_4427_2952/master/4427.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/4266a95890ac90566270619766b2b770d9025b7c/0_0_4427_2952/master/4427.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/4266a95890ac90566270619766b2b770d9025b7c/0_0_4427_2952/master/4427.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/4266a95890ac90566270619766b2b770d9025b7c/0_0_4427_2952/master/4427.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="An aerial view of a five very long, two-story buildings alongside what looks like a human-made lake." src="https://i.guim.co.uk/img/media/4266a95890ac90566270619766b2b770d9025b7c/0_0_4427_2952/master/4427.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="296.7336796927942" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>An Amazon Web Services datacentre in Stone Ridge, Virginia, on 28 July 2024.</span> Photograph: Nathan Howard/Bloomberg via Getty Images</figcaption></figure><p>With this threat in mind, Israeli officials inserted into the Nimbus deal a requirement for the companies to a send coded message – a “wink” – to its government, revealing the identity of the country they had been compelled to hand over Israeli data to, but were gagged from saying so.</p><p>Leaked documents from Israel’s finance ministry, which include a finalised version of the Nimbus agreement, suggest the secret code would take the form of payments – referred to as “special compensation” – made by the companies to the Israeli government.</p><p>According to the documents, the payments must be made “within 24 hours of the information being transferred” and correspond to the telephone dialing code of the foreign country, amounting to sums between 1,000 and 9,999 shekels.</p><p>Under the terms of the deal, the mechanism works like this:</p><ul>
 <li>
  <p>If either Google or Amazon provides information to authorities in the US, where the dialing code is +1, and they are prevented from disclosing their cooperation, they must send the Israeli government 1,000 shekels.</p></li>
 <li>
  <p>If, for example, the companies receive a request for Israeli data from authorities in Italy, where the dialing code is +39, they must send 3,900 shekels.</p></li>
 <li>
  <p>If the companies conclude the terms of a gag order prevent them from even signaling which country has received the data, there is a backstop: the companies must pay 100,000 shekels ($30,000) to the Israeli government.</p></li>
</ul><p>Legal experts, including several former US prosecutors, said the arrangement was highly unusual and carried risks for the companies as the coded messages could violate legal obligations in the US, where the companies are headquartered, to keep a subpoena secret.</p><p>“It seems awfully cute and something that if the US government or, more to the point, a court were to understand, I don’t think they would be particularly sympathetic,” a former US government lawyer said.</p><p>Several experts described the mechanism as a “clever” workaround that could comply with the letter of the law but not its spirit. “It’s kind of brilliant, but it’s risky,” said a former senior US security official.</p><p>Israeli officials appear to have acknowledged this, documents suggest. Their demands about how Google and Amazon respond to a US-issued order “might collide” with US law, they noted, and the companies would have to make a choice between “violating the contract or violating their legal obligations”.</p><p>Neither Google nor Amazon responded to the Guardian’s questions about whether they had used the secret code since the Nimbus contract came into effect.</p><p>“We have a rigorous global process for responding to lawful and binding orders for requests related to customer data,” Amazon’s spokesperson said. “We do not have any processes in place to circumvent our confidentiality obligations on lawfully binding orders.”</p><p>Google declined to comment on which of Israel’s stringent demands it had accepted in the completed Nimbus deal, but said it was “false” to “imply that we somehow were involved in illegal activity, which is absurd”.</p><p>A spokesperson for Israel’s finance ministry said: “The article’s insinuation that Israel compels companies to breach the law is baseless.”</p><h2 id="no-restrictions">‘No restrictions’</h2><p>Israeli officials also feared a scenario in which its access to the cloud providers’ technology could be blocked or restricted.</p><p>In particular, officials worried that activists and rights groups could place pressure on Google and Amazon, or seek court orders in several European countries, to force them to terminate or limit their business with Israel if their technology were linked to human rights violations.</p><p>To counter the risks, Israel inserted controls into the Nimbus agreement which Google and Amazon appear to have accepted, according to government documents prepared after the deal was signed.</p><p>The documents state that the agreement prohibits the companies from revoking or restricting Israel’s access to their cloud platforms, either due to changes in company policy or because they find Israel’s use of their technology violates their terms of service.</p><p>Provided Israel does not infringe on copyright or resell the companies’ technology, “the government is permitted to make use of any service that is permitted by Israeli law”, according to a finance ministry analysis of the deal.</p><p>Both companies’ standard “acceptable use” policies state their cloud platforms should not be used to violate the legal rights of others, nor should they be used to engage in or encourage activities that cause “serious harm” to people.</p><p>However, according to an Israeli official familiar with the Nimbus project, there can be “no restrictions” on the kind of information moved into Google and Amazon’s cloud platforms, including military and intelligence data. The terms of the deal seen by the Guardian state that Israel is “entitled to migrate to the cloud or generate in the cloud any content data they wish”.</p><p>Israel inserted the provisions into the deal to avoid a situation in which the companies “decide that a certain customer is causing them damage, and therefore cease to sell them services”, one document noted.</p><p>The Intercept <a href="https://theintercept.com/2025/05/12/google-nimbus-israel-military-ai-human-rights/" data-link-name="in body link">reported last year</a> the Nimbus project was governed by an “amended” set of confidential policies, and cited a leaked internal report suggesting Google understood it would not be permitted to restrict the types of services used by Israel.</p><p>Last month, when Microsoft <a href="https://www.theguardian.com/world/2025/sep/25/microsoft-blocks-israels-use-of-its-technology-in-mass-surveillance-of-palestinians" data-link-name="in body link">cut off Israeli access</a> to some cloud and artificial intelligence services, it did so after confirming reporting by the Guardian and its partners, +972 and Local Call, that the military had stored a vast trove of intercepted Palestinian calls in the company’s Azure cloud platform.</p><p>Notifying the Israeli military of its decision, Microsoft said that using Azure in this way violated its terms of service and it was “not in the business of facilitating the mass surveillance of civilians”.</p><p>Under the terms of the Nimbus deal, Google and Amazon are prohibited from taking such action as it would “discriminate” against the Israeli government. Doing so would incur financial penalties for the companies, as well as legal action for breach of contract.</p><p>The Israeli finance ministry spokesperson said Google and Amazon are “bound by stringent contractual obligations that safeguard Israel’s vital interests”. They added: “These agreements are confidential and we will not legitimise the article’s claims by disclosing private commercial terms.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[From VS Code to Helix (231 pts)]]></title>
            <link>https://ergaster.org/posts/2025/10/29-vscode-to-helix/</link>
            <guid>45746478</guid>
            <pubDate>Wed, 29 Oct 2025 13:19:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ergaster.org/posts/2025/10/29-vscode-to-helix/">https://ergaster.org/posts/2025/10/29-vscode-to-helix/</a>, See on <a href="https://news.ycombinator.com/item?id=45746478">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-2q5oecfc=""> <nav><ol><li><a href="#automation-is-a-double-edged-sword">Automation is a double-edged sword</a></li><li><a href="#why-i-feared-using-helix">Why I feared using Helix</a></li><li><a href="#what-helped">What Helped</a><ol><li><a href="#just-do-it">Just Do It</a></li><li><a href="#better-docs">Better docs</a></li></ol></li><li><a href="#getting-the-most-of-markdown-and-astro-in-helix">Getting the most of Markdown and Astro in Helix</a><ol><li><a href="#markdown">Markdown</a></li><li><a href="#astro">Astro</a></li><li><a href="#yaml">YAML</a></li></ol></li><li><a href="#is-it-worth-it">Is it worth it?</a></li></ol></nav><p>I created the website you’re reading with VS Code. Behind the scenes I use Astro, a static site generator that gets out of the way while providing nice conveniences.</p>
<p>Using VS Code was a no-brainer: everyone in the industry seems to at least be familiar with it, every project can be opened with it, and most projects can get enhancements and syntactic helpers in a few clicks. In short: VS Code is free, easy to use, and widely adopted.</p>
<p>A Rustacean colleague kept singing <a href="https://helix-editor.com/">Helix</a>’s praises. I discarded it because he’s much smarter than I am, and I only ever use vim when I need to fiddle with files on a server. I like when things “Just Work” and didn’t want to bother learning how to use Helix nor how to configure it.</p>
<p>Today it has become my daily driver. Why did I change my mind? What was preventing me from using it before? And how difficult was it to get there?</p>
<h2 id="automation-is-a-double-edged-sword"><a href="#automation-is-a-double-edged-sword">Automation is a double-edged sword</a></h2>
<p>Automation and technology make work easier, this is why we produce technology in the first place. But it also means you grow more dependent on the tech you use. If the tech is produced transparently by an international team or a team you trust, it’s fine. But if it’s produced by a single large entity that can screw you over, it’s dangerous.</p>
<p>VS Code might be open source, but in practice it’s produced by Microsoft. Microsoft has a problematic relationship to consent and is shoving AI products down everyone’s throat. I’d rather use tools that respect me and my decisions, and I’d rather not get my tools produced by already monopolistic organizations.</p>
<p>Microsoft is also based in the USA, and the political climate over there makes me want to depend as little as possible on American tools. I know that’s a long, uphill battle, but we have to start somewhere.</p>
<p>I’m not advocating for a ban against American tech in general, but for more balance in our supply chain. I’m also not advocating for European tech either: I’d rather get open source tools from international teams competing in a race to the top, rather than from teams in a single jurisdiction. What is happening in the USA could happen in Europe too.</p>
<h2 id="why-i-feared-using-helix"><a href="#why-i-feared-using-helix">Why I feared using Helix</a></h2>
<p>I’ve never found vim particularly pleasant to use but it’s everywhere, so I figured I might just get used to it. But one of the things I never liked about vim is the number of moving pieces. By default, vim and neovim are very bare bones. They can be extended and completely modified with plugins, but I really don’t like the idea of having extremely customize tools.</p>
<p>I’d rather have the same editor as everyone else, with a few knobs for minor preferences. I am subject to choice paralysis, so making me configure an editor before I’ve even started editing is the best way to tank my productivity.</p>
<p>When my colleague told me about Helix, two things struck me as improvements over vim.</p>
<ol>
<li><strong>Helix’s philosophy is that everything should work out of the box.</strong> There are a few configs and themes, but everything should work similarly from one Helix to another. All the language-specific logic is handled in Language Servers that implement the <a href="https://en.wikipedia.org/wiki/Language_Server_Protocol">Language Server Protocol</a> standard.</li>
<li><strong>In Helix, first you select text, and then you perform operations onto it.</strong> So you can visually tell what is going to be changed before you apply the change. It fits my mental model much better.</li>
</ol>
<p>But there are major drawbacks to Helix too:</p>
<ol>
<li><strong>After decades of vim, I was scared to re-learn everything.</strong> In practice this wasn’t a problem at all because of the very visual way Helix works.</li>
<li><strong>VS Code “Just Works”, and Helix sounded like more work than the few clicks from VS Code’s extension store.</strong> This is true, but not as bad as I had anticipated.</li>
</ol>
<p>After a single week of usage, Helix was already very comfortable to navigate. After a few weeks, most of the wrinkles have been ironed out and I use it as my primary editor. So how did I overcome those fears?</p>
<h2 id="what-helped"><a href="#what-helped">What Helped</a></h2>
<h3 id="just-do-it"><a href="#just-do-it">Just Do It</a></h3>
<p><strong>I tried Helix.</strong> It can sound silly, but the very first step to get into Helix was not to overthink it. I just installed it on my mac with <code>brew install helix</code> and gave it a go. I was not too familiar with it, so I looked up <a href="https://docs.helix-editor.com/usage.html">the official documentation</a> and noticed there was a tutorial.</p>
<p>This tutorial alone is what convinced me to try harder. It’s an interactive and well written way to learn how to move and perform basic operations in Helix. I quickly learned how to move around, select things, surround them with braces or parenthesis. I could <em>see</em> what I was about to do before doing it. This has been epiphany. Helix just worked the way I wanted.</p>
<p><img alt="A screenshot of the Helix tutorial open in a terminal. It mostly consists of text in a 80 character wide column. As the user reads the text, they learn new commands to move around and edit text." loading="lazy" decoding="async" fetchpriority="auto" sizes="(min-width: 1520px) 1520px, 100vw" data-astro-image="constrained" width="1520" height="1044" src="https://ergaster.org/_astro/helix-tutor.ke2dVx7w_WV66I.webp" srcset="https://ergaster.org/_astro/helix-tutor.ke2dVx7w_1YfIWi.webp 640w, https://ergaster.org/_astro/helix-tutor.ke2dVx7w_Z1tbanv.webp 750w, https://ergaster.org/_astro/helix-tutor.ke2dVx7w_ZUIAxz.webp 828w, https://ergaster.org/_astro/helix-tutor.ke2dVx7w_1n4RrQ.webp 1080w, https://ergaster.org/_astro/helix-tutor.ke2dVx7w_1WMsvp.webp 1280w, https://ergaster.org/_astro/helix-tutor.ke2dVx7w_WV66I.webp 1520w"></p>
<p>Better: I could get things done faster than in VS Code after a few minutes of learning. Being a lazy person, I never bothered looking up VS Code shortcuts. Because the learning curve for Helix is slightly steeper, you <em>have</em> to learn those shortcuts that make moving around feel so easy.</p>
<p>Not only did I quickly get used to Helix key bindings: my vim muscle-memory didn’t get in the way at all!</p>
<h3 id="better-docs"><a href="#better-docs">Better docs</a></h3>
<p>The built-in tutorial is a very pragmatic way to get started. You get results fast, you learn hands on, and it’s not that long. But if you want to go further, you have to look for docs. Helix <a href="https://docs.helix-editor.com/">has officials docs</a>. They seem to be fairly complete, but they’re also impenetrable as a new user. They focus on what the editor supports and not on what I will want to do with it.</p>
<p>After a bit of browsing online, I’ve stumbled upon <a href="https://helix-nikita-revencos-projects.vercel.app/start-here/basics">this third-party documentation website</a>. The domain didn’t inspire me a lot of confidence, but the docs are really good. They are clearly laid out, use-case oriented, and they make the most of Astro Starlight to provide a great reading experience. The author <a href="https://github.com/helix-editor/helix/pull/12127#issuecomment-2525902615">tried to upstream these docs, but that won’t happen</a>. It looks like they are upstreaming their docs to the current website. I hope this will improve the quality of upstream docs eventually.</p>
<p>After learning the basics and finding my way through the docs, it was time to ensure Helix was set up to help me where I needed it most.</p>
<h2 id="getting-the-most-of-markdown-and-astro-in-helix"><a href="#getting-the-most-of-markdown-and-astro-in-helix">Getting the most of Markdown and Astro in Helix</a></h2>
<p>In my free time, I mostly use my editor for three things:</p>
<ol>
<li>Write notes in markdown</li>
<li>Tweak my website with Astro</li>
<li>Edit yaml to faff around my Kubernetes cluster</li>
</ol>
<p>Helix is a “stupid” text editor. It doesn’t know much about what you’re typing. But it supports Language Servers that implement the Language Server Protocol. Language Servers understand the document you’re editing. They explain to Helix what you’re editing, whether you’re in a TypeScript function, typing a markdown link, etc. With that information, Helix and the Language Server can provide code completion hints, errors &amp; warnings, and easier navigation in your code.</p>
<p>In addition to Language Servers, Helix also supports plugging code formatters. Those are pieces of software that will read the document and ensure that it is consistently formatted. It will check that all indentations use spaces and not tabs, that there is a consistent number of space when indenting, that brackets are on the same line as the function, etc. In short: it will make the code pretty.</p>
<h3 id="markdown"><a href="#markdown">Markdown</a></h3>
<p>Markdown is not really a programming language, so it might seem surprising to configure a Language Server for it. But if you remember what we said earlier, Language Servers can provide code completion, which is useful when creating links for example. <a href="https://github.com/artempyanykh/marksman">Marksman</a> does exactly that!</p>
<p>Since Helix <a href="https://docs.helix-editor.com/lang-support.html">is pre-configured to use marksman for markdown files</a> we only need to install marksman and make sure it’s in our <code>PATH</code>. Installing it with homebrew is enough.</p>

<p>We can check that Helix is happy with it with the following command</p>
<div><figure><pre data-language="console"><code><div><p><span>$ hx --health markdown</span></p></div><div><p><span>Configured language servers:</span></p></div><div><p><span><span>  </span></span><span>✓ marksman: /opt/homebrew/bin/marksman</span></p></div><div><p><span>Configured debug adapter: None</span></p></div><div><p><span>Configured formatter: None</span></p></div><div><p><span>Tree-sitter parser: ✓</span></p></div><div><p><span>Highlight queries: ✓</span></p></div><div><p><span>Textobject queries: ✘</span></p></div><div><p><span>Indent queries: ✘</span></p></div></code></pre></figure></div>
<p>But Language Servers can also help Helix display errors and warnings, and “code suggestions” to help fix the issues. It means Language Servers are a perfect fit for… grammar checkers! Several grammar checkers exist. The most notable are:</p>
<ul>
<li><a href="https://ltex-plus.github.io/ltex-plus/">LTEX+</a>, the Language Server used by <a href="https://languagetool.org/">Language Tool</a>. It supports several languages must is quite resource hungry.</li>
<li><a href="https://writewithharper.com/">Harper</a>, a grammar checker Language Server developed by Automattic, the people behind WordPress, Tumblr, WooCommerce, Beeper and more. Harper only support English and its variants, but they intend to support more languages in the future.</li>
</ul>
<p>I mostly write in English and want to keep a minimalistic setup. Automattic is well funded, and I’m confident they will keep working on Harper to improve it. Since grammar checker LSPs can easily be changed, I’ve decided to go with Harper for now.</p>
<p><img alt="A screenshot of Helix. The cursors is on the word &quot;place&quot; with a typo. The word is underlined. On the top right there is a warning message asking if the user intended to write the word like this." loading="lazy" decoding="async" fetchpriority="auto" sizes="(min-width: 1520px) 1520px, 100vw" data-astro-image="constrained" width="1520" height="1044" src="https://ergaster.org/_astro/helix-grammar-error.QcORNFVo_25IkSu.webp" srcset="https://ergaster.org/_astro/helix-grammar-error.QcORNFVo_VCrV1.webp 640w, https://ergaster.org/_astro/helix-grammar-error.QcORNFVo_Z2hdkPy.webp 750w, https://ergaster.org/_astro/helix-grammar-error.QcORNFVo_u8pfW.webp 828w, https://ergaster.org/_astro/helix-grammar-error.QcORNFVo_q8NWf.webp 1080w, https://ergaster.org/_astro/helix-grammar-error.QcORNFVo_ZtntcY.webp 1280w, https://ergaster.org/_astro/helix-grammar-error.QcORNFVo_25IkSu.webp 1520w"></p>
<p>To install it, homebrew does the job as always:</p>

<p>Then I edited my <code>~/.config/helix/languages.toml</code> to add Harper as a secondary Language Server in addition to marksman</p>
<div><figure><pre data-language="toml"><code><div><p><span>[</span><span>language-server</span><span>.</span><span>harper-ls</span><span>]</span></p></div><div><p><span>command = </span><span>"harper-ls"</span></p></div><div><p><span>args = [</span><span>"--stdio"</span><span>]</span></p></div><div><p><span>[[</span><span>language</span><span>]]</span></p></div><div><p><span>name = </span><span>"markdown"</span></p></div><div><p><span>language-servers = [</span><span>"marksman"</span><span>, </span><span>"harper-ls"</span><span>]</span></p></div></code></pre></figure></div>
<p>Finally I can add a markdown linter to ensure my markdown is formatted properly. Several options exist, and <a href="https://github.com/DavidAnson/markdownlint">markdownlint</a> is one of the most popular. My colleagues recommended the new kid on the block, a <em>Blazing Fast</em> equivalent: <a href="https://github.com/rvben/rumdl">rumdl</a>.</p>
<p>Installing rumdl was pretty simple on my mac. I only had to add the repository of the maintainer, and install rumdl from it.</p>
<div><figure><pre data-language="console"><code><div><p><span>$ brew tap rvben/rumdl</span></p></div><div><p><span>$ brew install rumdl</span></p></div></code></pre></figure></div>
<p>After that I added a new <code>language-server</code> to my <code>~/.config/helix/languages.toml</code> and added it to the language servers to use for the markdown <code>language</code>.</p>
<div><figure><pre data-language="toml"><code><div><p><span>[</span><span>language-server</span><span>.</span><span>rumdl</span><span>]</span></p></div><div><p><span>command = </span><span>"rumdl"</span></p></div><div><p><span>args = [</span><span>"server"</span><span>]</span></p></div><div><p><span>[...]</span></p></div><div><p><span>[[</span><span>language</span><span>]]</span></p></div><div><p><span>name = </span><span>"markdown"</span></p></div><div><p><span>language-servers = [</span><span>"marksman"</span><span>, </span><span>"harper-ls"</span><span>, </span><span>"rumdl"</span><span>]</span></p></div><div><p><span>soft-wrap.enable = </span><span>true</span></p></div><div><p><span>text-width = </span><span>80</span></p></div><div><p><span>soft-wrap.wrap-at-text-width = </span><span>true</span></p></div></code></pre></figure></div>
<p>Since my website already contained a <code>.markdownlint.yaml</code> I could import it to the rumdl format with</p>
<div><figure><pre data-language="console"><code><div><p><span>$ rumdl import .markdownlint.yaml</span></p></div><div><p><span>Converted markdownlint config from '.markdownlint.yaml' to '.rumdl.toml'</span></p></div><div><p><span>You can now use: rumdl check --config .rumdl.toml .</span></p></div></code></pre></figure></div>
<p>You might have noticed that I’ve added a little quality of life improvement: soft-wrap at 80 characters.</p>
<p>Now if you add this to your own <code>config.toml</code> you will notice that the text is completely left aligned. This is not a problem on small screens, but it rapidly gets annoying on wider screens.</p>
<p>Helix doesn’t support centering the editor. There is <a href="https://github.com/helix-editor/helix/pull/9838">a PR tackling the problem</a> but it has been stale for most of the year. The maintainers are overwhelmed by the number of PRs making it their way, and it’s not clear if or when this PR will be merged.</p>
<p>In the meantime, a workaround exists, with a few caveats. It is possible to add spaces to the left gutter (the column with the line numbers) so it pushes the content towards the center of the screen.</p>
<p><img alt="A screenshot of Helix editing markdown. The column is 80 character wide and centered in the screen." loading="lazy" decoding="async" fetchpriority="auto" sizes="(min-width: 3408px) 3408px, 100vw" data-astro-image="constrained" width="3408" height="2110" src="https://ergaster.org/_astro/helix-zen-mode.Aup4KvDq_1tEuBf.webp" srcset="https://ergaster.org/_astro/helix-zen-mode.Aup4KvDq_dgVBO.webp 640w, https://ergaster.org/_astro/helix-zen-mode.Aup4KvDq_1AMJeD.webp 750w, https://ergaster.org/_astro/helix-zen-mode.Aup4KvDq_Z17PUS3.webp 828w, https://ergaster.org/_astro/helix-zen-mode.Aup4KvDq_1iPz5z.webp 1080w, https://ergaster.org/_astro/helix-zen-mode.Aup4KvDq_1q994v.webp 1280w, https://ergaster.org/_astro/helix-zen-mode.Aup4KvDq_Z1SYODT.webp 1668w, https://ergaster.org/_astro/helix-zen-mode.Aup4KvDq_1D8XxE.webp 2048w, https://ergaster.org/_astro/helix-zen-mode.Aup4KvDq_Z2fWAsQ.webp 2560w, https://ergaster.org/_astro/helix-zen-mode.Aup4KvDq_1tEuBf.webp 3408w"></p>
<p>To figure out how many spaces are needed, you need to get your terminal width with <code>stty</code></p>

<p>In my case, when in full screen, my terminal is 243 characters wide. I need to remove the content column with from it, and divide everything by 2 to get the space needed on each side. In my case for a 243 character wide terminal with a text width of 80 characters:</p>

<p>As is, I would add 203 spaces to my left gutter to push the rest of the gutter and the content to the right. But the gutter itself has a width of 4 characters, that I need to remove from the total. So I need to subtract them from the total, which leaves me with <code>76</code> characters to add.</p>
<p>I can open my <code>~/.config/helix/config.toml</code> to add a new key binding that will automatically add or remove those spaces from the left gutter when needed, to shift the content towards the center.</p>
<div><figure><pre data-language="toml"><code><div><p><span>[</span><span>keys</span><span>.</span><span>normal</span><span>.</span><span>space</span><span>.</span><span>t</span><span>]</span></p></div><div><p><span>z = </span><span>":toggle gutters.line-numbers.min-width 76 3"</span></p></div></code></pre></figure></div>
<p>Now when in normal mode, pressing <kbd>Space</kbd> then <kbd>t</kbd> then <kbd>z</kbd> will add/remove the spaces. Of course this workaround only works when the terminal runs in full screen mode.</p>
<h3 id="astro"><a href="#astro">Astro</a></h3>
<p>Astro works like a charm in VS Code. The team behind it provides <a href="https://github.com/withastro/language-tools?tab=readme-ov-file#astrojslanguage-server">a Language Server</a> and a <a href="https://github.com/withastro/language-tools?tab=readme-ov-file#astrojsts-plugin">TypeScript plugin</a> to enable code completion and syntax highlighting.</p>
<p>I only had to install those globally with</p>
<div><figure><pre data-language="console"><code><div><p><span>$ pnpm install -g @astrojs/language-server typescript @astrojs/ts-plugin</span></p></div></code></pre></figure></div>
<p>Now we need to add a few lines to our <code>~/.config/helix/languages.toml</code> to tell it how to use the language server</p>
<div><figure><pre data-language="toml"><code><div><p><span>[</span><span>language-server</span><span>.</span><span>astro-ls</span><span>]</span></p></div><div><p><span>command = </span><span>"astro-ls"</span></p></div><div><p><span>args = [</span><span>"--stdio"</span><span>]</span></p></div><div><p><span>config = { typescript = { tsdk = </span><span>"/Users/thibaultmartin/Library/pnpm/global/5/node_modules/typescript/lib"</span><span> }}</span></p></div><div><p><span>[[</span><span>language</span><span>]]</span></p></div><div><p><span>name = </span><span>"astro"</span></p></div><div><p><span>scope = </span><span>"source.astro"</span></p></div><div><p><span>injection-regex = </span><span>"astro"</span></p></div><div><p><span>file-types = [</span><span>"astro"</span><span>]</span></p></div><div><p><span>language-servers = [</span><span>"astro-ls"</span><span>]</span></p></div></code></pre></figure></div>
<p>We can check that the Astro Language Server can be used by helix with</p>
<div><figure><pre data-language="console"><code><div><p><span>$ hx --health astro</span></p></div><div><p><span>Configured language servers:</span></p></div><div><p><span><span>  </span></span><span>✓ astro-ls: /Users/thibaultmartin/Library/pnpm/astro-ls</span></p></div><div><p><span>Configured debug adapter: None</span></p></div><div><p><span>Configured formatter: None</span></p></div><div><p><span>Tree-sitter parser: ✓</span></p></div><div><p><span>Highlight queries: ✓</span></p></div><div><p><span>Textobject queries: ✘</span></p></div><div><p><span>Indent queries: ✘</span></p></div></code></pre></figure></div>
<p>I also like to get a formatter to automatically make my code consistent and pretty for me when I save a file. One of the most popular code formaters out there is <a href="https://prettier.io/">Prettier</a>. I’ve decided to go with the fast and easy formatter <a href="https://dprint.dev/">dprint</a> instead.</p>
<p>I installed it with</p>

<p>Then in the projects I want to use dprint in, I do</p>

<p>I might edit the <code>dprint.json</code> file to my liking. Finally, I configure Helix to use dprint globally for all Astro projects by appending a few lines in my <code>~/.config/helix/languages.toml</code>.</p>
<div><figure><pre data-language="toml"><code><div><p><span>[[</span><span>language</span><span>]]</span></p></div><div><p><span>name = </span><span>"astro"</span></p></div><div><p><span>scope = </span><span>"source.astro"</span></p></div><div><p><span>injection-regex = </span><span>"astro"</span></p></div><div><p><span>file-types = [</span><span>"astro"</span><span>]</span></p></div><div><p><span>language-servers = [</span><span>"astro-ls"</span><span>]</span></p></div><div><p><span>formatter = { command = </span><span>"dprint"</span><span>, args = [</span><span>"fmt"</span><span>, </span><span>"--stdin"</span><span>, </span><span>"astro"</span><span>]}</span></p></div><div><p><span>auto-format = </span><span>true</span></p></div></code></pre></figure></div>
<p>One final check, and I can see that Helix is ready to use the formatter as well</p>
<div><figure><pre data-language="console"><code><div><p><span>$ hx --health astro</span></p></div><div><p><span>Configured language servers:</span></p></div><div><p><span><span>  </span></span><span>✓ astro-ls: /Users/thibaultmartin/Library/pnpm/astro-ls</span></p></div><div><p><span>Configured debug adapter: None</span></p></div><div><p><span>Configured formatter:</span></p></div><div><p><span><span>  </span></span><span>✓ /opt/homebrew/bin/dprint</span></p></div><div><p><span>Tree-sitter parser: ✓</span></p></div><div><p><span>Highlight queries: ✓</span></p></div><div><p><span>Textobject queries: ✘</span></p></div><div><p><span>Indent queries: ✘</span></p></div></code></pre></figure></div>
<h3 id="yaml"><a href="#yaml">YAML</a></h3>
<p>For yaml, it’s simple and straightforward: Helix is preconfigured to use <code>yaml-language-server</code> as soon as it’s in the PATH. I just need to install it with</p>
<div><figure><pre data-language="console"><code><div><p><span>$ brew install yaml-language-server</span></p></div></code></pre></figure></div>
<h2 id="is-it-worth-it"><a href="#is-it-worth-it">Is it worth it?</a></h2>
<p><strong>Helix really grew on me. I find it particularly easy and fast to edit code with it.</strong> It takes a tiny bit more work to get the language support than it does in VS Code, but it’s nothing insurmountable. There is a slightly steeper learning curve than for VS Code, but I consider it to be a good thing. It forced me to learn how to move around and edit efficiently, because there is no way to do it inefficiently. Helix remains intuitive once you’ve learned the basics.</p>
<p>I am a GNOME enthusiast, and I adhere to the same principles: <strong>I like when my apps work out of the box, and when I have little to do to configure them.</strong> This is a strong stance that often attracts a vocal opposition. I like products that follow those principles better than those who don’t.</p>
<p>With that said, Helix sometimes feels like it is maintained by one or two people who have a strong vision, but who struggle to onboard more maintainers. As of writing, Helix has more than 350 PRs open. Quite a few bring interesting features, but the maintainers don’t have enough time to review them.</p>
<p>Those 350 PRs mean there is a lot of energy and goodwill around the project. <strong>People are willing to contribute. Right now, all that energy is gated, resulting in frustration</strong> both from the contributors who feel like they’re working in the void, and the maintainers who feel like there at the receiving end of a fire hose.</p>
<p><strong>A solution to make everyone happier without sacrificing the quality of the project would be to work on a Contributor Ladder.</strong> CHAOSS’ Dr Dawn Foster published <a href="https://fastwonderblog.com/2025/08/12/governance-part-3-new-contributors-and-pathways-to-leadership/">a blog post about it</a>, listing interesting resources at the end.</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Grammarly rebrands to 'Superhuman,' launches a new AI assistant (112 pts)]]></title>
            <link>https://techcrunch.com/2025/10/29/grammarly-rebrands-to-superhuman-launches-a-new-ai-assistant/</link>
            <guid>45746401</guid>
            <pubDate>Wed, 29 Oct 2025 13:12:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/10/29/grammarly-rebrands-to-superhuman-launches-a-new-ai-assistant/">https://techcrunch.com/2025/10/29/grammarly-rebrands-to-superhuman-launches-a-new-ai-assistant/</a>, See on <a href="https://news.ycombinator.com/item?id=45746401">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">Typically, when a company acquires another, it will absorb the new company’s branding or integrate it with its own identity. Grammarly is doing something different: After <a href="https://techcrunch.com/2025/07/01/grammarly-acquires-ai-email-client-superhuman/">acquiring email client Superhuman in July</a>, the company is renaming itself “Superhuman.” </p>

<p>Despite the branding change, Grammarly, the product, will continue to be known as it has. However, the company says it is thinking about rebranding products like Coda, <a href="https://techcrunch.com/2024/12/17/grammarly-acquires-productivity-startup-coda-brings-on-new-ceo/">a productivity platform it acquired last year</a>, in the long run.</p>







<p>The company is also launching an AI assistant called Superhuman Go that’s built into Grammarly’s existing extension. The assistant can provide writing suggestions, give feedback on emails, and you can even connect it with other apps like Jira, Gmail, Google Drive, and Google Calendar to arm it with more context. The assistant can use these connections to do tasks like logging tickets or fetching your availability when you’re scheduling a meeting.</p>

<figure><img loading="lazy" decoding="async" height="510" width="680" src="https://techcrunch.com/wp-content/uploads/2025/10/GIF_Superhuman-Go-use-case.gif?w=680" alt=""><figcaption><span><strong>Image Credits:</strong>Superhuman</span></figcaption></figure>

<p>Superhuman said it plans to add functionality to enable the assistant to fetch data from sources like CRMs and internal systems to suggest changes to your emails. </p>

<p>Users can try Superhuman Go by turning on a toggle in the Grammarly extension, which will let them connect it to different apps. Users can also try out different agents in the company’s agent store, which include a <a href="https://techcrunch.com/2025/08/18/grammarly-gets-a-design-overhaul-multiple-ai-features/">plagiarism checker and a proofreader</a>, launched in August.</p>

<p>All Grammarly users can try out Superhuman Go right now, though the company is also selling product bundles. Its Pro subscription plan will cost $12 per month (billed annually) and will enable grammar and tone support in multiple languages. The Business plan will cost $33 per month (billed annually) and will give users access to Superhuman Mail.</p>

<figure><img loading="lazy" decoding="async" height="383" width="680" src="https://techcrunch.com/wp-content/uploads/2025/10/Static_Superhuman-Go-2.jpeg?w=680" alt="" srcset="https://techcrunch.com/wp-content/uploads/2025/10/Static_Superhuman-Go-2.jpeg 1920w, https://techcrunch.com/wp-content/uploads/2025/10/Static_Superhuman-Go-2.jpeg?resize=150,84 150w, https://techcrunch.com/wp-content/uploads/2025/10/Static_Superhuman-Go-2.jpeg?resize=300,169 300w, https://techcrunch.com/wp-content/uploads/2025/10/Static_Superhuman-Go-2.jpeg?resize=768,432 768w, https://techcrunch.com/wp-content/uploads/2025/10/Static_Superhuman-Go-2.jpeg?resize=680,383 680w, https://techcrunch.com/wp-content/uploads/2025/10/Static_Superhuman-Go-2.jpeg?resize=1200,675 1200w, https://techcrunch.com/wp-content/uploads/2025/10/Static_Superhuman-Go-2.jpeg?resize=1280,720 1280w, https://techcrunch.com/wp-content/uploads/2025/10/Static_Superhuman-Go-2.jpeg?resize=430,242 430w, https://techcrunch.com/wp-content/uploads/2025/10/Static_Superhuman-Go-2.jpeg?resize=720,405 720w, https://techcrunch.com/wp-content/uploads/2025/10/Static_Superhuman-Go-2.jpeg?resize=900,506 900w, https://techcrunch.com/wp-content/uploads/2025/10/Static_Superhuman-Go-2.jpeg?resize=800,450 800w, https://techcrunch.com/wp-content/uploads/2025/10/Static_Superhuman-Go-2.jpeg?resize=1536,864 1536w, https://techcrunch.com/wp-content/uploads/2025/10/Static_Superhuman-Go-2.jpeg?resize=668,375 668w, https://techcrunch.com/wp-content/uploads/2025/10/Static_Superhuman-Go-2.jpeg?resize=1097,617 1097w, https://techcrunch.com/wp-content/uploads/2025/10/Static_Superhuman-Go-2.jpeg?resize=708,398 708w, https://techcrunch.com/wp-content/uploads/2025/10/Static_Superhuman-Go-2.jpeg?resize=50,28 50w" sizes="auto, (max-width: 680px) 100vw, 680px"><figcaption><span><strong>Image Credits:</strong>Superhuman</span></figcaption></figure>

<p>Superhuman said it also wants to add more AI-powered features to the Coda document suite and Superhuman email clients, such as fetching details from external and internal sources to create additional details in documents and email drafts automatically.</p>
<div>
		
		<p>Techcrunch event</p>
		<div>
			
			<p><span>San Francisco</span>
													<span>|</span>
													<span>October 27-29, 2025</span>
							</p>
			
		</div>
	</div>

<p>Grammarly has for the past few years made a concerted effort to increase its viability as a productivity suite, exemplified through its acquisitions of Coda and Superhuman. With this AI assistant, the company is positioning itself to compete better with the likes of <a href="https://techcrunch.com/2025/04/15/notion-releases-its-ai-driven-email-inbox/">Notion</a>, <a href="https://techcrunch.com/2025/03/13/clickup-is-launching-a-revamped-calendar-tool-for-task-and-meeting-management/">ClickUp</a>, and Google Workspace, which have launched multiple AI-powered features in the past few years.</p>
</div><div>
	
	
	
	

	
<div>
		<p>Ivan covers global consumer tech developments at TechCrunch. He is based out of India and has previously worked at publications including Huffington Post and The Next Web.</p>
<p>You can contact or verify outreach from Ivan by emailing <a href="mailto:im@ivanmehta.com">im@ivanmehta.com</a> or via encrypted message at ivan.42 on Signal.</p>	</div>


	
	<p>
		<a data-ctatext="View Bio" data-destinationlink="https://techcrunch.com/author/ivan-mehta/" data-event="button" href="https://techcrunch.com/author/ivan-mehta/">View Bio <svg style="width: 1em;" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"><path fill="var(--c-svg, currentColor)" d="M16.5 12 9 19.5l-1.05-1.05L14.4 12 7.95 5.55 9 4.5z"></path></svg></a>
	</p>
	
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AWS to bare metal two years later: Answering your questions about leaving AWS (574 pts)]]></title>
            <link>https://oneuptime.com/blog/post/2025-10-29-aws-to-bare-metal-two-years-later/view</link>
            <guid>45745281</guid>
            <pubDate>Wed, 29 Oct 2025 11:14:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://oneuptime.com/blog/post/2025-10-29-aws-to-bare-metal-two-years-later/view">https://oneuptime.com/blog/post/2025-10-29-aws-to-bare-metal-two-years-later/view</a>, See on <a href="https://news.ycombinator.com/item?id=45745281">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                <p>When we published <a href="https://oneuptime.com/blog/post/2023-10-30-moving-from-aws-to-bare-metal/view">How moving from AWS to Bare-Metal saved us $230,000 /yr.</a> in 2023, the story travelled far beyond our usual readership. The discussion threads on <a href="https://news.ycombinator.com/item?id=38294569" target="_blank" rel="noopener noreferrer">Hacker News</a> and <a href="https://www.reddit.com/r/sysadmin/comments/17y6zbi/moving_from_aws_to_baremetal_saved_us_230000_yr/" target="_blank" rel="noopener noreferrer">Reddit</a> were packed with sharp questions: did we skip Reserved Instances, how do we fail over a single rack, what about the people cost, and when is cloud still the better answer? This follow-up is our long-form reply.</p><p>Over the last twenty-four months we:</p><ul><li>Ran the MicroK8s + Ceph stack in production for 730+ days with 99.993% measured availability.</li><li>Added a second rack in Frankfurt, joined to our primary Paris cage over redundant DWDM, to kill the “single rack” concern.</li><li>Cut average customer-facing latency by 19% thanks to local NVMe and eliminating noisy neighbours.</li><li>Reinvested the savings into buying bare metal AI servers to expand LLM-based alert / incident summarisation and auto code fixes based on log / traces and metrics in OneUptime.</li></ul><p>Below we tackle the recurring themes from the community feedback, complete with the numbers we use internally.</p><h2>$230,000 / yr savings? That is just an engineers salary.</h2><p>In the US, it is. In the rest of the world. That's 2-5x engineers salary. We <em>used</em> to save $230,000 / yr but now the savings have exponentially grown. We now save over $1.2M / yr and we expect this to grow, as we grow as a business.</p><h2>“Why not just buy Savings Plans or Reserved Instances?”</h2><p>We tried. Long answer: the maths still favoured bare metal once we priced everything in. We see a savings of over 76% if you compare our bare metal setup to AWS. </p><p>A few clarifications:</p><ul><li>Savings Plans <strong>do not</strong> reduce S3, egress, or Direct Connect. 37% off instances still leaves you paying list price for bandwidth, which was 22% of our AWS bill.</li><li>EKS had an extra $1,260/month control-plane fee plus $600/month for NAT gateways. Those costs disappear once you run Kubernetes yourself.</li><li>Our workload is 24/7 steady. We were already at &gt;90% reservation coverage; there was no idle burst capacity to “right size” away. If we had the kind of bursty compute profile many commenters referenced, the choice would be different.</li></ul><h2>“How much did migration and ongoing ops really cost?”</h2><p>We spent a week of engineers time (and that is the worst case estimate) on the initial migration, spread across SRE, platform, and database owners. Most of that time was work we needed anyway—formalising infrastructure-as-code, smoke testing charts, tightening backup policies. The incremental work that existed purely <em>because</em> of bare metal was roughly one week.</p><p>Ongoing run-cost looks like this:</p><ul><li><strong>Hands-on keyboard:</strong> ~24 engineer-hours/quarter across the entire platform team, including routine patching and firmware updates. That is comparable to the AWS time we used to burn on cost optimisation, IAM policy churn, and chasing deprecations and updating our VM's on AWS. </li><li><strong>Remote hands:</strong> 2 interventions in 24 months (mainly disks). Mean response time: 27 minutes. We do not staff an on-site team. We rely on co-location provider to physically manage our rack. This means no traditional hardware admins. </li><li><strong>Automation:</strong> We're now moving to Talos. We PXE boot with Tinkerbell, image with Talos, manage configs through Flux and Terraform, and run conformance suites before each Kubernetes upgrade. All of those tools also hardened our AWS estate, so they were not net-new effort.</li></ul><p>The opportunity cost question from is fair. We track it the same way we track feature velocity: did the infra team ship less? The answer was “no”—our release cadence increased because we reclaimed few hours/month we used to spend in AWS “cost council” meetings.</p><h2>“Isn’t a single rack a single point of failure?”</h2><p>We have multiple racks across two different DC / providers. We:</p><ul><li>Leased a secondary quarter rack in Frankfurt with a different provider and power utility.</li><li>Currently: Deployed a second MicroK8s control plane, mirrored Ceph pools with asynchronous replication. Future: We're moving to Talos. Nothing against Microk8s, but we like the Talos way of managing the k8s cluster.</li><li>Added isolated out-of-band management paths (4G / satellite) so we can reach the gear even during metro fibre events.</li></ul><p>The AWS failover cluster we mentioned in 2023 still exists. We rehearse a full cutover quarterly using the same Helm releases we ship to customers. DNS failover remains the slowest leg (resolver caches can ignore TTL), so we added Anycast ingress via BGP with our transit provider to cut traffic shifting to sub-minute.</p><h2>“What about hardware lifecycle and surprise CapEx?”</h2><p>We amortise servers over five years, but we sized them with 2 × AMD EPYC 9654 CPUs, 1 TB RAM, and NVMe sleds. At our current growth rate the boxes will hit CPU saturation before we hit year five. When that happens, the plan is to cascade the older gear into our regional analytics cluster (we use Posthog + Metabase for this) and buy a new batch. Thanks to the savings delta, we can refresh 40% of the fleet every 24 months and still spend less annually than the optimised AWS bill above.</p><p>We also buy extended warranties from the OEM (Supermicro) and keep three cold spares in the cage. The hardware lasts 7-8 years and not 5, but we wtill count it as 5 to be very conservative. </p><h2>“Are you reinventing managed services?”</h2><p>Another strong Reddit critique: why rebuild services AWS already offers? Three reasons we are comfortable with the trade:</p><ol><li><strong>Portability is part of our product promise.</strong> OneUptime customers self-host in their own environments. Running the same open stack we ship (Postgres, Redis, ClickHouse, etc.) keeps us honest. We eun on Kubernetes and self-hosted customers run on Kubernetes as well. </li><li><strong>Tooling maturity.</strong> Two years ago we relied on Terraform + EKS + RDS. Today we run MicroK8s (Talos in the future), Argo Rollouts, OpenTelemetry Collector, and Ceph dashboards. None of that is bespoke. We do not maintain a fork of anything.</li><li><strong>Selective cloud use.</strong> We still pay AWS for Glacier backups, CloudFront for edge caching, and short-lived burst capacity for load tests. Cloud makes sense when elasticity matters; bare metal wins when baseload dominates.</li></ol><p>Managed services are phenomenal when you are short on expertise or need features beyond commodity compute. If we were all-in on DynamoDB streams or Step Functions we would almost certainly still be on AWS.</p><h2>“How do bandwidth and DoS scenarios work now?”</h2><p>We committed to 5 Gbps 95th percentile across two carriers.  The same traffic on AWS egress would be 8x expensive in eu-west-1. For DDoS protection we front our ingress with Cloudflare. </p><h2>“Has reliability suffered?”</h2><p>Short answer: No. Infact it was better than AWS (compared to recent AWS downtimes)</p><p>We have 730+ days with 99.993% measured availability and we also escaped AWS region wide downtime that happened a week ago. </p><h2>“How do audits and compliance work off-cloud now?”</h2><p>We stayed SOC 2 Type II and ISO 27001 certified through the transition. The biggest deltas auditors cared about:</p><ul><li>Physical controls: We provide badge logs from the colo, camera footage on request, and quarterly access reviews. The colo already meets Tier III redundancy, so their reports roll into ours.</li><li>Change management: Terraform plans, and now Talos machine configs give us immutable evidence of change. Auditors liked that more than AWS Console screenshots.</li><li>Business continuity: We prove failover by moving workload to other DC.</li></ul><p>If you are in a regulated space (HIPAA for instance), expect the paperwork to grow a little. We worked it in by leaning on the colo providers’ standard compliance packets—they slotted straight into our risk register.</p><h2>“Why not stay in the cloud but switch providers?”</h2><p>We priced Hetzner, OVH, Leaseweb, Equinix Metal, and AWS Outposts. The short version:</p><ul><li>Hyperscaler alternatives were cheaper on compute but still expensive on egress once you hit petabytes/month. Outposts also carried minimum commits that exceeded our needs.</li><li>European dedicated hosts (Hetzner, OVH) are fantastic for lab clusters. The challenge was multi-100 TB Ceph clusters with redundant uplinks and smart-hands SLAs. Once we priced that tier, the savings narrowed.</li><li>Equinix Metal got the closest, but bare metal on-demand still carried a 25-30% premium over our CapEx plan. Their global footprint is tempting; we may still use them for short-lived expansion.</li></ul><p>Owning the hardware also let us plan power density (we run 15 kW racks) and reuse components. For our steady-state footprint, colocation won by a long shot.</p><h2>“What does day-to-day toil look like now?”</h2><p>We put real numbers to it because Reddit kept us honest:</p><ul><li>Weekly: Kernel and firmware patches (Talos makes this a redeploy), Ceph health checks,  Total time averages 1 hour/week on average over months. </li><li>Monthly: Kubernetes control plane upgrades in canary fashion. About 2 engineer-hours. We expect this to reduce when Talos kicks in.</li><li>Quarterly: Disaster recovery drills, capacity planning, and contract audits with carriers. Roughly 12 hours across three engineers.</li></ul><p>Total toil is ~14 engineer-hours/month, including prep. The AWS era had us spending similar time but on different work: chasing cost anomalies, expanding Security Hub exceptions, and mapping breaking changes in managed services. The toil moved; it did not multiply.</p><h2>“Do you still use the cloud for anything substantial?”</h2><p>Absolutely. Cloud still solves problems we would rather not own:</p><ul><li>Glacier keeps long-term log archives at a price point local object storage cannot match.</li><li>CloudFront handles 14 edge PoPs we do not want to build. We terminate TLS at the edge for marketing assets and docs. We will soon move this to Cloudflare as they are cheaper.</li><li>We spin up short-lived AWS environments for load testing.</li></ul><p>So yes, we left AWS for the base workload, but we still swipe the corporate card when elasticity or geography outweighs fixed-cost savings.</p><h2>When the cloud is still the right answer</h2><p><strong>It depends on your workload</strong>. We still recommend staying put if:</p><ul><li>Your usage pattern is spiky or seasonal and you can auto-scale to near zero between peaks.</li><li>You lean heavily on managed services (Aurora Serverless, Kinesis, Step Functions) where the operational load is the value prop.</li><li>You do not have the appetite to build a platform team comfortable with Kubernetes, Ceph, observability, and incident response.</li></ul><p>Cloud-first was the right call for our first five years. Bare metal became the right call once our compute footprint, data gravity, and independence requirements stabilised.</p><h2>What is next</h2><ul><li>We are working on a detailed runbook + Terraform module to help teams do <em>capex forecasting</em> for colo moves. Expect that on the blog later this year.</li><li>A deep dive on Talos is in the queue, as requested by multiple folks in the HN thread.</li></ul><p>Questions we did not cover? Let us know in the discussion threads—we are happy to keep sharing the gritty details.</p><p><strong>Related Reading:</strong></p>                            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Aggressive bots ruined my weekend (186 pts)]]></title>
            <link>https://herman.bearblog.dev/agressive-bots/</link>
            <guid>45745072</guid>
            <pubDate>Wed, 29 Oct 2025 10:47:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://herman.bearblog.dev/agressive-bots/">https://herman.bearblog.dev/agressive-bots/</a>, See on <a href="https://news.ycombinator.com/item?id=45745072">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2025-10-29T09:43Z">
                    29 Oct, 2025
                </time>
            </i>
        </p>
    

    <p>On the 25th of October Bear had its first major outage. Specifically, the reverse proxy which handles custom domains went down, causing custom domains to time out.</p>
<p>Unfortunately my monitoring tool failed to notify me, and it being a Saturday, I didn't notice the outage for longer than is reasonable. I apologise to everyone who was affected by it.</p>
<p>First, I want to dissect the root cause, exactly what went wrong, and then provide the steps I've taken to mitigate this in the future.</p>
<p>I wrote about <a href="https://herman.bearblog.dev/the-great-scrape/">The Great Scrape</a> at the beginning of this year. The vast majority of web traffic is now bots, and it is becoming increasingly more hostile to have publicly available resources on the internet.</p>
<p>There are 3 major kinds of bots currently flooding the internet: AI scrapers, malicious scrapers, and unchecked automations/scrapers.</p>
<p>The first has been discussed at length. Data is <em>worth something</em> now that it is used as fodder to train LLMs, and there is a financial incentive to scrape, so scrape they will. They've depleted all human-created writing on the internet, and are becoming increasingly ravenous for new wells of content. I've seen this compared to the search for <a href="https://en.wikipedia.org/wiki/Low-background_steel" target="_blank">low-background-radiation steel</a>, which is, itself, very interesting.</p>
<p>These scrapers, however, are the easiest to deal with since they tend to identify themselves as ChatGPT, Anthropic, XAI, et cetera. They also tend to specify whether they are from user-initiated searches (think all the sites that get scraped when you make a request with ChatGPT), or data mining (data used to train models). On Bear Blog I allow the first kinds, but block the second, since bloggers want discoverability, but usually don't want their writing used to train the next big model.</p>
<p>The next two kinds of scraper are more insidious. The malicious scrapers are bots that systematically scrape and re-scrape websites, sometimes every few minutes, looking for vulnerabilities such as misconfigured Wordpress instances, or <code>.env</code> and <code>.aws</code> files, among other things, accidentally left lying around.</p>
<p>It's more dangerous than ever to self-host, since simple mistakes in configurations will likely be found and exploited. In the last 24 hours I've blocked close to 2 million malicious requests across several hundred blogs.</p>
<p>What's wild is that these scrapers rotate through thousands of IP addresses during their scrapes, which leads me to suspect that the requests are being tunnelled through apps on mobile devices, since the ASNs tend to be cellular networks. I'm still speculating here, but I think app developers have found another way to monetise their apps by offering them for free, and selling tunnel access to scrapers.</p>
<p>Now, on to the unchecked automations. Vibe coding has made web-scraping easier than ever. Any script-kiddie can easily build a functional scraper in a single prompt and have it run all day from their home computer, and if the dramatic rise in scraping is anything to go by, many do. Tens of thousands of new scrapers have cropped up over the past few months, accidentally DDoSing website after website in their wake. The average consumer-grade computer is significantly more powerful than a VPS, so these machines can easily cause a lot of damage without noticing.</p>
<p>I've managed to keep all these scrapers at bay using a combination of web application firewall (WAF) rules and rate limiting provided by Cloudflare, as well as some custom code which finds and quarantines bad bots based on their activity.</p>
<p>I've played around with serving <a href="https://en.wikipedia.org/wiki/Zip_bomb" target="_blank">Zip Bombs</a>, which was quite satisfying, but I stopped for fear of accidentally bombing a legitimate user. Another thing I've played around with is Proof of Work validation, making it expensive for bots to scrape, as well as serving endless junk data to keep the bots busy. Both of these are <em>interesting</em>, but ultimately are just as effective as simply blocking those requests, without the increased complexity.</p>
<p>With that context, here's exactly went wrong on Saturday.</p>
<p>Previously, the bottleneck for page requests was the web-server itself, since it does the heavy lifting. It automatically scales horizontally by up to a factor of 10, if necessary, but bot requests can scale by significantly more than that, so having strong bot detection and mitigation, as well as serving highly-requested endpoints via a CDN is necessary. This is a solved problem, as outlined in my Great Scrape post, but worth restating.</p>
<p>On Saturday morning a few hundred blogs were DDoSed, with tens of thousands of pages requested per minute (from the logs it's hard to say whether they were malicious, or just very aggressive scrapers). The above-mentioned mitigations worked as expected, however the reverse-proxy—which sits up-stream of most of these mitigations—became saturated with requests and decided it needed to take a little nap.</p>
<p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/herman/page-requests.webp" alt="page-requests"></p>
<p><small>The big blue spike is what toppled the server. It's so big it makes the rest of the graph look flat.</small></p>
<p>This server had been running with zero downtime for 5 years up until this point.</p>
<p>Unfortunately my uptime monitor failed to alert me via the push notifications I'd set up, even though it's the only app I have that not only has notifications enabled (see my <a href="https://herman.bearblog.dev/notifications/">post on notifications</a>), but even has critical alerts enabled, so it'll wake me up in the middle of the night if necessary. I still have no idea why this alert didn't come through, and I have ruled out misconfiguration through various tests.</p>
<p>This brings me to how I will prevent this from happening in the future.</p>
<ol>
<li>Redundancy in monitoring. I now have a second monitoring service running alongside my uptime monitor which will give me a phone call, email, and text message in the event of any downtime.</li>
<li>More aggressive rate-limiting and bot mitigation on the reverse proxy. This already reduces the server load by about half.</li>
<li>I've bumped up the size of the reverse proxy, which can now handle about 5 times the load. This is overkill, but compute is cheap, and certainly worth the stress-mitigation. I'm already bald. I don't need to go balder.</li>
<li>Auto-restart the reverse-proxy if bandwidth usage drops to zero for more than 2 minutes.</li>
<li>Added a status page, available at <a href="https://status.bearblog.dev/">https://status.bearblog.dev</a> for better visibility and transparency. Hopefully those bars stay solid green forever.</li>
</ol>
<p>This should be enough to keep everything healthy. If you have any suggestions, or need help with your own bot issues, <a href="https://herman.bearblog.dev/contact/">send me an email</a>.</p>
<p>The public internet is mostly bots, many of whom are bad netizens. It's the most hostile it's ever been, and it is because of this that I feel it's more important than ever to take good care of the spaces that make the internet worth visiting.</p>
<p>The arms race continues...</p>


    

    
        

        
            


        
    


  </div></div>]]></description>
        </item>
    </channel>
</rss>