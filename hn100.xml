<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 29 Jun 2024 07:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[The XAES-256-GCM extended-nonce AEAD (105 pts)]]></title>
            <link>https://words.filippo.io/dispatches/xaes-256-gcm/</link>
            <guid>40826683</guid>
            <pubDate>Sat, 29 Jun 2024 00:01:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://words.filippo.io/dispatches/xaes-256-gcm/">https://words.filippo.io/dispatches/xaes-256-gcm/</a>, See on <a href="https://news.ycombinator.com/item?id=40826683">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <p>About a year ago <a href="https://words.filippo.io/dispatches/xaes-256-gcm-11/">I wrote</a> that “I want to use XAES-256-GCM/11, which has a number of nice properties and only the annoying defect of not existing.” Well, there is now <a href="https://c2sp.org/XAES-256-GCM?ref=words.filippo.io">an XAES-256-GCM specification</a>. (Had to give up on the /11 part, but that was just a performance optimization.)</p>
<p>XAES-256-GCM is an <em>authenticated encryption with additional data</em> (AEAD) algorithm with 256-bit keys and <strong>192-bit nonces</strong>. It was designed with the following goals:</p>
<ol>
<li>supporting a nonce large enough to be safe to generate randomly for a virtually unlimited number of messages (2⁸⁰ messages with collision risk 2⁻³²);</li>
<li>full, straightforward FIPS 140 compliance; and</li>
<li>trivial implementation on top of common cryptographic libraries.</li>
</ol>
<p>The large nonce enables safer and more friendly APIs that automatically read a fresh nonce from the operating system’s CSPRNG for every message, without burdening the user with any <a href="https://en.wikipedia.org/wiki/Birthday_attack?ref=words.filippo.io">birthday bound</a> calculations. Compliance and compatibility make it available anywhere an AEAD might be needed, including in settings where alternative large-nonce AEADs are not an option.</p>
<p>Like XChaCha20Poly1305, XAES-256-GCM is an extended-nonce construction on top of AES-256-GCM. That is, it uses the key and the large nonce to compute a derived key for the underlying AEAD.</p>
<p>It’s simple enough to fit inline in this newsletter. Here we go. <em>K</em> and <em>N</em> are the input key and nonce, <em>Kₓ</em> and <em>Nₓ</em> are the derived AES-256-GCM key and nonce.</p>
<ol>
<li><em>L</em> = AES-256ₖ(0¹²⁸)</li>
<li>If MSB₁(<em>L</em>) = 0, then <em>K1</em> = <em>L</em> &lt;&lt; 1;<br>
Else <em>K1</em> = (<em>L</em> &lt;&lt; 1) ⊕ 0¹²⁰10000111</li>
<li><em>M1</em> = 0x00 || 0x01 || <code>X</code> || 0x00 || <em>N</em>[:12]</li>
<li><em>M2</em> = 0x00 || 0x02 || <code>X</code> || 0x00 || <em>N</em>[:12]</li>
<li><em>Kₓ</em> = AES-256ₖ(<em>M1</em> ⊕ <em>K1</em>) || AES-256ₖ(<em>M2</em> ⊕ <em>K1</em>)</li>
<li><em>Nₓ</em> = <em>N</em>[12:]</li>
</ol>
<p>As you can see, it costs three AES-256ₖ calls per message, although one can be precomputed for a given key, and the other two can reuse its key schedule.</p>
<p>The <a href="https://github.com/C2SP/C2SP/blob/main/XAES-256-GCM/go/XAES-256-GCM.go?ref=words.filippo.io">Go reference implementation</a> fits in less than 100 lines of mostly boilerplate, including the precomputation optimization, and only uses the standard library’s crypto/cipher and crypto/aes.</p>
<p>Importantly, you could also describe XAES-256-GCM entirely in terms of a standard <a href="https://csrc.nist.gov/publications/detail/sp/800-108/rev-1/final?ref=words.filippo.io">NIST SP 800-108r1</a> KDF and the standard NIST AES-256-GCM AEAD (<a href="https://csrc.nist.gov/pubs/sp/800/38/d/final?ref=words.filippo.io">NIST SP 800-38D</a>, <a href="https://csrc.nist.gov/pubs/fips/197/final?ref=words.filippo.io">FIPS 197</a>).</p>
<blockquote>
<p>Instantiate a counter-based KDF (<a href="https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-108r1.pdf?ref=words.filippo.io#%5B%7B%22num%22%3A79%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C70%2C300%2C0%5D">NIST SP 800-108r1, Section 4.1</a>) with CMAC-AES256 (<a href="https://csrc.nist.gov/publications/detail/sp/800-38b/final?ref=words.filippo.io">NIST SP 800-38B</a>) and the input key as <em>Kin</em>, the ASCII letter <code>X</code> (0x58) as <em>Label</em>, the first 96 bits of the input nonce as <em>Context</em> (as recommended by <a href="https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-108r1.pdf?ref=words.filippo.io#%5B%7B%22num%22%3A71%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C70%2C720%2C0%5D">NIST SP 800-108r1, Section 4</a>, point 4), a counter (<em>i</em>) size of 16 bits, and omitting the optional <em>L</em> field, and produce a 256-bit derived key. Use that derived key and the last 96 bits of the input nonce with AES-256-GCM.</p>
</blockquote>
<p>Thanks to the choice of parameters, if we peel off the KDF and CMAC abstractions, the result is barely slower and more complex than straightforwardly invoking AES-256 on a counter. In exchange, we get a vetted and compliant solution. The parameters <a href="https://github.com/C2SP/C2SP/blob/main/XAES-256-GCM/openssl/openssl.c?ref=words.filippo.io">are supported by the high-level OpenSSL API</a>, too.</p>
<p>Why no more “/11”? Well, half the point of using AES-GCM is FIPS 140 compliance. (The other half being hardware acceleration.) If we mucked with the rounds number the design wouldn’t be compliant.</p>
<p>Indeed, if compliance is not a goal there are a number of alternatives, from AES-GCM-SIV to modern AEAD constructions based on the AES core. The specification has <a href="https://c2sp.org/XAES-256-GCM?ref=words.filippo.io#alternatives">an extensive Alternatives section</a> that compares each of them to XAES-256-GCM.</p>
<p>Also included in the specification are <a href="https://c2sp.org/XAES-256-GCM?ref=words.filippo.io#test-vectors">test vectors</a> for the two main code paths (MSB₁(<em>L</em>) = 0 and 1), and <a href="https://c2sp.org/XAES-256-GCM?ref=words.filippo.io#accumulated-randomized-tests">accumulated test vectors</a> that compress 10 000 or 1 000 000 random iterations.</p>
<p>To sum up, XAES-256-GCM is designed to be a safe, boring, compliant, and interoperable AEAD that can fit high-level APIs, the kind we’d like to add to Go. It’s designed to complement XChaCha20Poly1305 and AES-GCM-SIV as implementations of a hypothetical <a href="https://github.com/golang/go/issues/54364?ref=words.filippo.io#issuecomment-1642676993">nonce-less AEAD API</a>. If other cryptography library maintainers like it (or don’t), I would love to hear about it, because we are not big fans of adding Go-specific constructions to the standard library.</p>
<p>By the way, I have an exciting update about my professional open source maintainer effort coming in less than two weeks! Make sure to subscribe to <a href="https://filippo.io/newsletter?ref=words.filippo.io">Maintainer Dispatches</a> or to follow me on Bluesky at <a href="https://bsky.app/profile/filippo.abyssdomain.expert?ref=words.filippo.io">@filippo.abyssdomain.expert</a> or on Mastodon at <a href="https://abyssdomain.expert/@filippo?ref=words.filippo.io">@filippo@abyssdomain.expert</a>. (Or, see you at <a href="https://www.gophercon.com/?ref=words.filippo.io">GopherCon</a> in Chicago!)</p>
<h2 id="the-picture">The picture</h2>
<p>Earlier this year I ran in the <a href="https://www.centopassi.net/?ref=words.filippo.io">Centopassi</a> motorcycle competition. It involves driving more than 1600km on mountain roads, through one hundred GPS coordinates you select in advance from a long list, in three days and a half. It’s been fantastic. It took me to corners of Italy I would have never seen, and I had a lot of fun. This picture is taken at our 100th location, after a couple kilometers of unpaved hairpins on the side of the hill. The finish line was at the lake you can see in the distance. I was ecstatic.</p>
<p>That’s my 2014 KTM Duke 690, a single-cylinder “naked” from before KTM knew how to make larger street bikes. It’s weird and I love it.</p>
<p><img src="https://words.filippo.io/content/images/2024/06/IMG_1921.jpeg" alt="A black motorcycle with saddlebags and a race plate, parked on a dirt road overlooking a vast, scenic valley with green hills, a lake in the distance, and mountains under a bright blue sky with scattered white clouds." loading="lazy"></p>
<p>My awesome clients—<a href="https://www.sigsum.org/?ref=words.filippo.io">Sigsum</a>, <a href="https://www.latacora.com/?ref=words.filippo.io">Latacora</a>, <a href="https://interchain.io/?ref=words.filippo.io">Interchain</a>, <a href="https://smallstep.com/?ref=words.filippo.io">Smallstep</a>, <a href="https://www.avalabs.org/?ref=words.filippo.io">Ava Labs</a>, <a href="https://goteleport.com/?ref=words.filippo.io">Teleport</a>, <a href="https://www.sandboxaq.com/?ref=words.filippo.io">SandboxAQ</a>, <a href="https://charm.sh/?ref=words.filippo.io">Charm</a>, and <a href="https://tailscale.com/?ref=words.filippo.io">Tailscale</a>—are funding all my work for the community and through our retainer contracts they get face time and unlimited access to advice on Go and cryptography.</p>
<p>Here are a few words from some of them!</p>
<p>Latacora — <a href="https://www.latacora.com/?ref=words.filippo.io">Latacora</a> bootstraps security practices for startups. Instead of wasting your time trying to hire a security person who is good at everything from Android security to AWS IAM strategies to SOC2 and apparently has the time to answer all your security questionnaires plus never gets sick or takes a day off, you hire us. We provide a crack team of professionals prepped with processes and power tools, coupling individual security capabilities with strategic program management and tactical project management.</p>
<p>Teleport — For the past five years, attacks and compromises have been shifting from traditional malware and security breaches to identifying and compromising valid user accounts and credentials with social engineering, credential theft, or phishing. <a href="https://goteleport.com/identity-governance-security/?utm=filippo&amp;ref=words.filippo.io">Teleport Identity Governance &amp; Security</a> is designed to eliminate weak access patterns through access monitoring, minimize attack surface with access requests, and purge unused permissions via mandatory access reviews.</p>
<p>Ava Labs — We at <a href="https://www.avalabs.org/?ref=words.filippo.io">Ava Labs</a>, maintainer of <a href="https://github.com/ava-labs/avalanchego?ref=words.filippo.io">AvalancheGo</a> (the most widely used client for interacting with the <a href="https://www.avax.network/?ref=words.filippo.io">Avalanche Network</a>), believe the sustainable maintenance and development of open source cryptographic protocols is critical to the broad adoption of blockchain technology. We are proud to support this necessary and impactful work through our ongoing sponsorship of Filippo and his team.</p>
<p>SandboxAQ — <a href="https://www.sandboxaq.com/?ref=words.filippo.io">SandboxAQ</a>’s <a href="https://www.sandboxaq.com/solutions/aqtive-guard?ref=words.filippo.io">AQtive Guard</a> is a unified cryptographic management software platform that helps protect sensitive data and ensures compliance with authorities and customers. It provides a full range of capabilities to achieve cryptographic agility, acting as an essential cryptography inventory and data aggregation platform that applies current and future standardization organizations mandates. AQtive Guard automatically analyzes and reports on your cryptographic security posture and policy management, enabling your team to deploy and enforce new protocols, including quantum-resistant cryptography, without re-writing code or modifying your IT infrastructure.</p>

        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Eulogy for DevOps (168 pts)]]></title>
            <link>https://matduggan.com/a-eulogy-for-devops/</link>
            <guid>40826236</guid>
            <pubDate>Fri, 28 Jun 2024 22:59:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matduggan.com/a-eulogy-for-devops/">https://matduggan.com/a-eulogy-for-devops/</a>, See on <a href="https://news.ycombinator.com/item?id=40826236">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
<p>We hardly knew ye. </p><p>DevOps, like many trendy technology terms, has gone from the peak of optimism to the depths of exhaustion. While many of the fundamental ideas behind the concept have become second-nature for organizations, proving it did in fact have a measurable outcome, the difference between the initial intent and where we ended up is vast. For most organizations this didn't result in a wave of safer, easier to use software but instead encouraged new patterns of work that centralized risk and introduced delays and irritations that didn't exist before. We can move faster than before, but that didn't magically fix all our problems. </p><p>The cause of its death was a critical misunderstanding over what was causing software to be hard to write. The belief was by removing barriers to deployment, more software would get deployed and things would be easier and better. Effectively that the issue was that developers and operations teams were being held back by ridiculous process and coordination. In reality these "soft problems" of communication and coordination are much more difficult to solve than the technical problems around pushing more code out into the world more often. </p><h3 id="what-is-devops">What is DevOps?</h3><p>DevOps, when it was introduced around 2007, was a pretty radical concept of removing the divisions between people who ran the hardware and people who wrote the software. Organizations still had giant silos between teams, with myself experiencing a lot of that workflow. </p><p>Since all computer nerds also love space, it was basically us cosplaying as NASA. Copying a lot of the procedures and ideas from NASA to try and increase the safety around pushing code out into the world. Different organizations would copy and paste different parts, but the basic premise was every release was as close to bug free as time allowed. You were typically shooting for zero exceptions.</p><p>When I worked for a legacy company around that time, the flow for releasing software looked as follows. </p><ul><li>Development team would cut a release of the server software with a release number in conjunction with the frontend team typically packaged together as a full entity. They would test this locally on their machines, then it would go to dev for QA to test, then finally out to customers once the QA checks were cleared. </li><li>Operations teams would receive a playbook of effectively what the software was changing and what to do if it broke. This would include how it was supposed to be installed, if it did anything to the database, it was a whole living document. The idea was the people managing the servers, networking equipment and SANs had no idea what the software did or how to fix it so they needed what were effectively step by step instructions. Sometimes you would even get this as a paper document. </li><li>Since these happened often inside of your datacenter, you didn't have unlimited elasticity for growth. So, if possible, you would slowly roll out the update and stop to monitor at intervals. But you couldn't do what people see now as a blue/green deployment because rarely did you have enough excess server capacity to run two versions at the same time for all users. Some orgs did do different datacenters at different times and cut between them (which was considered to be sort of the highest tier of safety). </li><li>You'd pick a deployment day, typically middle of the week around 10 AM local time and then would monitor whatever metrics you had to see if the release was successful or not. These were often pretty basic metrics of success, including some real eyebrow raising stuff like "is support getting more tickets" and "are we getting more hits to our uptime website". Effectively "is the load balancer happy" and "are customers actively screaming at us". </li><li>You'd finish the deployment and then the on-call team would monitor the progress as you went. </li></ul><h3 id="why-didnt-this-work">Why Didn't This Work</h3><p>Part of the issue was this design was very labor-intensive. You needed enough developers coordinating together to put together a release. Then you needed a staffed QA team to actually take that software and ensure, on top of automated testing which was jusssttttt starting to become a thing, that the software actually worked. Finally you needed a technical writer working with the development team to walk through what does a release playbook look like and then finally have the Operations team receive, review the book and then implement the plan. </p><p>It was also slow. Features would often be pushed for months even when they were done just because a more important feature had to go out first. Or this update was making major changes to the database and we didn't want to bundle in six things with the one possibly catastrophic change. It's effectively the Agile vs Waterfall design broken out to practical steps. </p><figure><img src="https://kruschecompany.com/wp-content/uploads/2021/09/Waterfall-vs-Agile-in-software-development-infographic.jpg" alt="Waterfall vs Agile in software development infographic" loading="lazy" width="1000" height="2000"></figure><p>A lot of the lip service around this time that was given as to why organizations were changing was, frankly, bullshit. The real reason companies were so desperate to change was the following:</p><ul><li>Having lots of mandatory technical employees they couldn't easily replace was a bummer</li><li>Recruitment was hard and expensive. </li><li>Sales couldn't easily inject whatever last-minute deal requirement they had into the release cycle since that was often set it stone. </li><li>It provided an amazing opportunity for SaaS vendors to inject themselves into the process by offloading complexity into their stack so they pushed it hard.</li><li>The change also emphasized the strengths of cloud platforms at the time when they were starting to gobble market share. You didn't need lots of discipline, just allocate more servers. </li><li>Money was (effectively) free so it was better to increase speed regardless of monthly bills. </li><li>Developers were understandably frustrated that minor changes could take weeks to get out the door while they were being blamed for customer complaints.</li></ul><p>So executives went to a few conferences and someone asked them if they were "doing DevOps" and so we all changed our entire lives so they didn't feel like they weren't part of the cool club. </p><h3 id="what-was-devops">What Was DevOps?</h3><p>Often this image is used to sum it up:</p><figure><img src="https://wac-cdn.atlassian.com/dam/jcr:ef9fe684-c6dc-4ba0-a636-4ef7bcfa11f1/New%20DevOps%20Loop%20image.png?cdnVersion=1833" alt="DevOps Infinity Wheel" loading="lazy" width="2240" height="1090"></figure><p>In a nutshell, the basic premise was that development teams and operations teams were now one team. QA was fired and replaced with this idea that because you could very quickly deploy new releases and get feedback on those releases, you didn't need a lengthy internal test period where every piece of functionality was retested and determined to still be relevant. </p><p>Often this is <em>conflated</em> with the concept of SRE from Google, which I will argue until I die is a giant mistake. SRE is in the same genre but a very different tune, with a much more disciplined and structured approach to this problem. DevOps instead is about the simplification of the stack such that any developer on your team can deploy to production as many times in a day as they wish with only the minimal amounts of control on that deployment to ensure it had a reasonably high chance of working. </p><p>In reality DevOps as a practice looks much more like how Facebook operated, with employees committing to production on their first day and relying extensively on real-world signals to determine success or failure vs QA and tightly controlled releases. </p><p>In practice it looks like this:</p><ul><li>Development makes a branch in git and adds a feature, fix, change, etc. </li><li>They open up a PR and then someone else on that team looks at it, sees it passes their internal tests, approves it and then it gets merged into main. This is effectively the only safety step, relying on the reviewer to have perfect knowledge of all systems. </li><li>This triggers a webhook to the CI/CD system which starts the build (often of an entire container with this code inside) and then once the container is built, it's pushed to a container registry. </li><li>The CD system tells the servers that the new release exists, often through a Kubernetes deployment or pushing a new version of an internal package or using the internal CLI of the cloud providers specific "run a container as a service" platform. It then monitors and tells you about the success or failure of that deployment. </li><li>Finally there are release-aware metrics which allow that same team, who is on-call for their application, to see if something has changed since they released it. Is latency up, error count up, etc. This is often just a line in a graph saying this was old and this is new. </li><li>Depending on the system, this can either be something where every time the container is deployed it is on brand-new VMs or it is using some system like Kubernetes to deploy "the right number" of containers. </li></ul><p>The sales pitch was simple. Everyone can do everything so teams no longer need as many specialized people. Frameworks like Rails made database operations less dangerous, so we don't need a team of DBAs. Hell, use something like Mongo and you never need a DBA! </p><p>DevOps combined with Agile ended up with a very different philosophy of programming which had the following conceits:</p><ul><li>The User is the Tester</li><li>Every System Is Your Specialization </li><li>Speed Of Shipping Above All</li><li>Catch It In Metrics</li><li>Uptime Is Free, SSO Costs Money (free features were premium, expensive availability wasn't charged for)</li><li>Logs Are Business Intelligence</li></ul><h3 id="what-didnt-work">What Didn't Work</h3><p>The first cracks in this model emerged pretty early on. Developers were testing on their local Mac and Windows machines and then deploying code to Linux servers configured from Ansible playbooks and left running for months, sometimes years. Inevitably small differences in the running fleet of production servers emerged, either from package upgrades for security reasons or just from random configuration events. This could be mitigated by frequently rotating the running servers by destroying and rebuilding them as fresh VMs, but in practice this wasn't done as often as it should have been. </p><p>Soon you would see things like "it's running fine on box 1,2, 4, 5, but 3 seems to be having problems". It wasn't clear in the DevOps model <em>who</em> exactly was supposed to go figure out what was happening or how. In the previous design someone who worked with Linux for years and with these specific servers would be monitoring the release, but now those team members often wouldn't even know a deployment was happening. Telling someone who is amazing at writing great Javascript to go "find the problem with a Linux box" turned out to be easier said than done. </p><p>Quickly feedback from developers started to pile up. They didn't want to have to spend all this time figuring out what Debian package they wanted for this or that dependency. It wasn't what they were interested in doing and also they weren't being rewarded for that work, since they were almost exclusively being measured for promotions by the software they shipped. This left the Operations folks in charge of "smoothing out" this process, which in practice often meant really wasteful practices. </p><p>You'd see really strange workflows around this time of doubling the number of production servers you were paying for by the hour during a deployment and then slowly scaling them down, all relying on the same AMI (server image) to ensure some baseline level of consistency. However since any update to the AMI required a full dev-stage-prod check, things like security upgrades took <em>a very long time</em>. </p><p>Soon you had just a pile of issues that became difficult to assign. Who "owned" platform errors that didn't result in problems for users? When a build worked locally but failed inside of Jenkins, what team needed to check that? The idea of we're all working on the same team broke down when it came to assigning ownership of annoying issues because <em>someone</em> had to own them or they'd just sit there forever untouched. </p><h3 id="enter-containers">Enter Containers</h3><p>DevOps got a real shot in the arm with the popularization of containers, which allowed the movement to progress past its awkward teenage years. Not only did this (mostly) solve the "it worked on my machine" thing but it also allowed for a massive simplification of the Linux server component part. Now servers were effectively dumb boxes running containers, either on their own with Docker compose or as part of a fleet with Kubernetes/ECS/App Engine/Nomad/whatever new thing that has been invented in the last two weeks. </p><p>Combined with you could move almost everything that might previous be a networking team problem or a SAN problem to configuration inside of the cloud provider through tools like Terraform and you saw a real flattening of the skill curve. This greatly reduced the expertise required to operate these platforms and allowed for more automation. Soon you started to see what we now recognize as the current standard for development which is "I push out a bajillion changes a day to production". </p><h3 id="what-containers-didnt-fix">What Containers Didn't Fix</h3><p>So there's a lot of other shit in that DevOps model we haven't talked about. </p><figure><img src="https://matduggan.com/content/images/2024/06/New-DevOps-Loop-image.png" alt="" loading="lazy" width="2000" height="973" srcset="https://matduggan.com/content/images/size/w600/2024/06/New-DevOps-Loop-image.png 600w, https://matduggan.com/content/images/size/w1000/2024/06/New-DevOps-Loop-image.png 1000w, https://matduggan.com/content/images/size/w1600/2024/06/New-DevOps-Loop-image.png 1600w, https://matduggan.com/content/images/2024/06/New-DevOps-Loop-image.png 2240w" sizes="(min-width: 720px) 720px"></figure><p>So far teams had improved the "build, test and deploy" parts. However operating the crap was still very hard. Observability was <em>really really</em> hard and expensive. Discoverability was actually harder than ever because stuff was constantly changing beneath your feet and finally the Planning part immediately collapsed into the ocean because now teams could do whatever they wanted all the time. </p><p><strong>Operate</strong></p><p>This meant someone going through and doing all the boring stuff. Upgrading Kubernetes, upgrading the host operating system, making firewall rules, setting up service meshes, enforcing network policies, running the bastion host, configuring the SSH keys, etc. What organizations quickly discovered was that this stuff was very time consuming to do and often required <em>more</em> specialization than the roles they had previously gotten rid of. </p><p>Before you needed a DBA, a sysadmin, a network engineer and some general Operations folks. Now you needed someone who not only understood databases but understood <em>your specific cloud providers</em> version of that database. You still needed someone with the sysadmin skills, but in addition they needed to be experts in your cloud platform in order to ensure you weren't exposing your database to the internet. Networking was still critical but now it all existed at a level outside of your control, meaning weird issues would sometimes have to get explained as "well that sometimes happens". </p><p>Often teams would delay maintenance tasks out of a fear of breaking something like k8s or their hosted database, but that resulted in delaying the pain and making their lives more difficult. This was the era where every startup I interviewed with basically just wanted someone to update all the stuff in their stack "safely". Every system was well past EOL and nobody knew how to Jenga it all together. </p><p><strong>Observe</strong></p><p>As applications shipped more often, knowing they worked became more important so you could roll back if it blew up in your face. However replacing simple uptime checks with detailed traces, metrics and logs was hard. These technologies are specialized and require detailed understanding of what they do and how they work. A syslog centralized box lasts <em>to a point</em> and then it doesn't. Prometheus scales to x amount of metrics and then no longer works on a single box. You needed someone who had a detailed understanding of how metrics, logs and traces worked and how to work with development teams in getting them sending the correct signal to the right places at the right amount of fidelity. </p><p>Or you could pay a SaaS a <em>shocking amount</em> to do it for you. The rise of companies like Datadog and the eye-watering bills that followed was proof that they understood how important what they were providing was. You quickly saw Observability bills exceed CPU and networking costs for organizations as one team would misconfigure their application logs and suddenly you have blown through your monthly quota in a week. </p><p>Developers were being expected to monitor with detailed precision what was happening with their applications without a full understanding of what they were seeing. How many metrics and logs were being dropped on the floor or sampled away, how did the platform work in displaying these logs to them, how do you write an query for terabytes of logs so that you can surface what you need quickly, all of this was being passed around in Confluence pages being written by desperate developers who were learning as they were getting paged at 2AM how all this shit works together. </p><p><strong>Continuous Feedback</strong></p><p>This to me is the same problem as Observe. It's about whether your deployment worked or not and whether you had signal from internal tests if it was likely to work. It's also about feedback from the team on what in this process worked and what didn't, but because nobody ever did anything with that internal feedback we can just throw that one directly in the trash. </p><p>I guess in theory this would be retros where we all complain about the same six things every sprint and then continue with our lives. I'm not an Agile Karate Master so you'll need to talk to the experts. </p><p><strong>Discover</strong></p><p>A big pitch of combining these teams was the idea of more knowledge sharing. Development teams and Operation teams would be able to cross-share more about what things did and how they worked. Again it's an interesting idea and there was some improvement to discoverability, but in practice that isn't how the incentives were aligned. </p><p>Developers weren't rewarded for discovering more about how the platform operated and Operations didn't have any incentive to sit down and figure out how the frontend was built. It's not a lack of intellectual curiosity by either party, just the finite amount of time we all have before we die and what we get rewarded for doing. Being surprised that this didn't work is like being surprised a mouse didn't go down the tunnel with no cheese just for the experience. </p><p>In practice I "discovered" that if NPM was down nothing worked and the frontend team "discovered" that troubleshooting Kubernetes was a bit like Warhammer 40k Adeptus Mechanicus waving incense in front of machines they didn't understand in the hopes that it would make the problem go away. </p><figure><img src="https://warhammeruniverse.com/wp-content/uploads/2023/12/00007-2552221792-1024x569.png" alt="The Adeptus Mechanicus - Warhammer Universe (2024)" loading="lazy" width="1024" height="569"><figcaption><span>Try restarting the Holy Deployment</span></figcaption></figure><p><strong>Plan</strong></p><p>Maybe more than anything else, this lack of centralization impacted planning. Since teams weren't syncing on a regular basis anymore, things could continue in crazy directions unchecked. In theory PMs were syncing with each other to try and ensure there were railroad tracks in front of the train before it plowed into the ground at 100 MPH, but that was a lot to put on a small cadre of people. </p><p>We see this especially in large orgs with microservices where it is easier to write a new microservice to do something rather than figure out which existing microservice does the thing you are trying to do. This model was sustainable when money was free and cloud budgets were unlimited, but once that gravy train crashed into the mountain of "businesses need to be profitable and pay taxes" that stopped making sense. </p><h3 id="the-part-where-we-all-gave-up">The Part Where We All Gave Up</h3><p>A lot of orgs solved the problems above by simply throwing bodies into the mix. More developers meant it was possible for teams to have someone (anyone) learn more about the systems and how to fix them. Adding more levels of PMs and overall planning staff meant even with the frantic pace of change it was...more possible to keep an eye on what was happening. While cloud bills continued to go unbounded, for the most part these services worked and allowed people to do the things they wanted to do. </p><p>Then layoffs started and budget cuts. Suddenly it wasn't acceptable to spend unlimited money with your logging platform and your cloud provider as well as having a full team. Almost instantly I saw the shift as organizations started talking about "going back to basics". Among this was a hard turn in the narrative around Kubernetes where it went from an amazing technology that lets you grow to Google-scale to a weight around an organizations neck nobody understood. </p><p><strong>Platform Engineering</strong></p><p>Since there are no new ideas, just new terms, a successor to the throne has emerged. No longer are development teams expected to understand and troubleshoot the platforms that run their software, instead the idea is that the entire process is completely abstracted away from them. They provide the container and that is the end of the relationship. </p><p>From a certain perspective this makes more sense since it places the ownership for the operation of the platform with the people who should have owned it from the beginning. It also removes some of the ambiguity over what is whose problem. The development teams are still on-call for their specific application errors, but platform teams are allowed to enforce more global rules. </p><p>Well at least in theory. In practice this is another expansion of roles. You went from needing to be a Linux sysadmin to being a cloud-certified Linux sysadmin to being a Kubernetes-certified multicloud Linux sysadmin to finally being an application developer who can create a useful webUI for deploying applications on top of a multicloud stack that runs on Kubernetes in multiple regions with perfect uptime and observability that doesn't blow the budget. I guess at some point between learning the difference between AWS and GCP we were all supposed to go out and learn how to make useful websites. </p><figure><img src="https://cdn.prod.website-files.com/622b2fcc29fc56492b771cb8/637d1841fc5e9a6a942d5a02_1.png" alt="" loading="lazy" width="1600" height="683"></figure><p>This division of labor makes no sense but at least it's something I guess. Feels like somehow Developers got stuck with a lot more work and Operation teams now need to learn 600 technologies a week. Surprisingly tech executives didn't get any additional work with this system. I'm sure the next reorg they'll chip in more. </p><h3 id="conclusion">Conclusion</h3><p>We are now seeing a massive contraction of the Infrastructure space. Teams are increasingly looking for simple, less platform specific tooling. In my own personal circles it feels like a real return to basics, as small and medium organizations abandon technology like Kubernetes and adopt much more simple and easy-to-troubleshoot workflows like "a bash script that pulls a new container". </p><p>In some respects it's a positive change, as organizations stop pretending they needed a "global scale" and can focus on actually servicing the users and developers they have. In reality a lot of this technology was adopted by organizations who weren't ready for it and didn't have a great plan for how to use it. </p><p>However Platform Engineering is not a magical solution to the problem. It is instead another fabrication of an industry desperate to show monthly growth in cloud providers who know teams lack the expertise to create the kinds of tooling described by such practices. In reality organizations need to be more brutally honest about what they <em>actually need</em> vs what bullshit they've been led to believe they need. </p><p>My hope is that we keep the gains from the DevOps approach and focus on simplification and stability over rapid transformation in the Infrastructure space. I think we desperately need a return to basics ideology that encourages teams to stop designing with the expectation that endless growth is the only possible outcome of every product launch. </p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Open source 'Eclipse Theia IDE' exits beta to challenge Visual Studio Code (141 pts)]]></title>
            <link>https://visualstudiomagazine.com/Articles/2024/06/27/eclipse-theia-ide.aspx</link>
            <guid>40825146</guid>
            <pubDate>Fri, 28 Jun 2024 20:49:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://visualstudiomagazine.com/Articles/2024/06/27/eclipse-theia-ide.aspx">https://visualstudiomagazine.com/Articles/2024/06/27/eclipse-theia-ide.aspx</a>, See on <a href="https://news.ycombinator.com/item?id=40825146">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="level0"> 
        
        <p id="ph_pcontent2_0_KickerText"><a href="https://visualstudiomagazine.com/Articles/List/News.aspx">News</a></p>
        
        <h3 id="ph_pcontent2_0_MainHeading">Open Source 'Eclipse Theia IDE' Exits Beta to Challenge Visual Studio Code</h3>
        
        
        

        <p>
  Some seven years in the making, the Eclipse Foundation's Theia IDE project is now generally available, emerging from beta to challenge Microsoft's similar Visual Studio Code editor, with which it shares much tech.
  
</p>



<p>
   The <a href="https://theia-ide.org/#theiaide" target="_blank">Eclipse Theia IDE</a>, part of the <a href="https://ecdtools.eclipse.org/" target="_blank">Eclipse Cloud DevTools ecosystem</a>, primarily differs from VS Code in licensing and governance. Open-source champion Eclipse Foundation calls it a "true open-source alternative" to VS Code, which Microsoft has <a href="https://code.visualstudio.com/docs/supporting/faq" target="_blank">described</a> as being "built" on open source but with proprietary elements like default telemetry with which usage data is collected.
 
</p>



<div><figure> <a href="https://visualstudiomagazine.com/Articles/2024/06/27/~/media/ECG/visualstudiomagazine/Images/2024/06/theia_desktop.ashx" target="_blank">
<img alt="Eclipse Theia IDE in on Windows" src="https://visualstudiomagazine.com/Articles/2024/06/27/~/media/ECG/visualstudiomagazine/Images/2024/06/theia_desktop_s.ashx" height="160" width="300"> </a>
<figcaption> <b>[Click on image for larger view.]</b> Eclipse Theia IDE in on Windows <em>(source: Screenshot).</em></figcaption>
</figure></div>




<p>
  Note that Eclipse Theia IDE is a separate component from the overall Theia project's related <a href="https://projects.eclipse.org/projects/ecd.theia" target="_blank">Eclipse Theia Platform</a>, used to build IDEs and tools based on modern web technologies.
  
</p>






<p>
  As far as the similarities with VS Code, Theia is built on the same <a href="https://microsoft.github.io/monaco-editor/" target="_blank">Monaco editor</a> that powers VS Code, and it supports the same Language Server Protocol (LSP) and Debug Adapter Protocol (DAP) that provide IntelliSense code completions, error checking and other features.
  
</p>



<p>
  Eclipse Theia IDE also supports the same extensions as VS Code (via the <a href="https://open-vsx.org/" target="_blank">Open VSX Registry</a> instead of Microsoft's Visual Studio Code Marketplace), which are typically written in TypeScript and JavaScript. There are many, many more extensions available for VS Code in Microsoft's marketplace, while "Extensions for VS Code Compatible Editors" in the Open VSX Registry number 3,784 at the time of this writing. 
</p>

    

<br>



<div><figure> <a href="https://visualstudiomagazine.com/Articles/2024/06/27/~/media/ECG/visualstudiomagazine/Images/2024/06/open_vsx.ashx" target="_blank">
<img alt="Open VSX Registry" src="https://visualstudiomagazine.com/Articles/2024/06/27/~/media/ECG/visualstudiomagazine/Images/2024/06/open_vsx_s.ashx" height="159" width="300"> </a>
<figcaption> <b>[Click on image for larger view.]</b> Open VSX Registry <em>(source: Open VSX Registry).</em></figcaption>
</figure></div>




<p>
Eclipse Foundation <a href="https://eclipsesource.com/blogs/2019/12/06/the-eclipse-theia-ide-vs-vs-code/" target="_blank">compared the two tools</a> in 2019, when it said to make a good decision between using VS Code or Eclipse Theia as a platform for a tool, an organization will need to evaluate custom project requirements, noting that as a general direction:
</p>




<ul>
<li>If you want to provide some tooling, which is focussed on code and want as many developers as possible to use it in their existing IDE, providing an extension for VS Code seems like a valid choice.
</li>
<li>If you want to provide a white-labeled product for customers or your own developers, which is tailored to a specific use case and possibly contains more features than code editing, you might be better served with Eclipse Theia.</li>
</ul>




<p>
  A somewhat more recent <a href="https://blogs.eclipse.org/post/mike-milinkovich/eclipse-theia-and-vs-code-differences-explained" target="_blank">post</a> from 2020 exploring the differences between the Eclipse Theia Platform (not IDE) and VS Code noted two primary ways in which the projects' architectures differ:
  
</p>



<ul>
<li>Eclipse Theia allows developers to create desktop and cloud IDEs using a single, open source technology stack. Microsoft now offers VS Online for cloud development environments, but like VS Code, it cannot be used in open source initiatives such as Gitpod.
</li>
<li>Eclipse Theia allows developers to customize every aspect of the IDE without forking or patching the code. This means they can easily use Theia as a base to develop desktop and cloud IDEs that are fully tailored for the needs of internal company projects or for commercial resale as a branded product. VS Code is a developer IDE only. It was never intended to be used as the base for other IDEs, extended, or further distributed.</li>
</ul>




<p>
  For developers just wanting to pick a tool to write apps with, an Eclipse Foundation blog <a href="https://eclipsesource.com/blogs/2024/06/27/introducing-the-theia-ide/" target="_blank">post</a> today said: "For developers in search of an IDE that combines flexibility, openness, and cutting-edge technology, the Theia IDE is a compelling choice. Distinctive features like an adaptable toolbar, detachable views, remote development support, and the forthcoming live collaboration mode set Theia apart from other open-source IDEs. Moreover, its commitment to privacy and its stance against incorporating telemetry by default reflect its respect for user preferences."
</p>






<p>
  Eclipse Foundation today emphasized another difference between its Theia IDE and VS Code: the surrounding ecosystem/community.
</p>




<div><figure> <a href="https://visualstudiomagazine.com/Articles/2024/06/27/~/media/ECG/visualstudiomagazine/Images/2024/06/theia_community.ashx" target="_blank">
<img alt="Eclipse Theia Community" src="https://visualstudiomagazine.com/Articles/2024/06/27/~/media/ECG/visualstudiomagazine/Images/2024/06/theia_community_s.ashx" height="220" width="300"> </a>
<figcaption> <b>[Click on image for larger view.]</b> Eclipse Theia Community <em>(source: Eclipse).</em></figcaption>
</figure></div>




<p>
  "At the core of Theia IDE is its vibrant open source community hosted by the Eclipse Foundation," the organization <a href="https://newsroom.eclipse.org/news/announcements/eclipse-foundation-introduces-theia-ide-elevate-modern-developer-experience" target="_blank">said</a> in a news release. "This ensures freedom for commercial use without proprietary constraints and fosters innovation and reliability through contributions from companies like Ericsson, EclipseSource, STMicroelectronics, TypeFox, and more. The community-driven model encourages participation and adaptation according to user needs and feedback."
</p>




<p>
  Indeed, the list of contributors to and adopters of the platform is extensive, also featuring Broadcom, Arm, IBM, Red Hat, SAP, Samsung, Google, Gitpod, Huawei and many others.
</p>




<p>
  "The Theia IDE's open-source foundation, supported by a vibrant community and underpinned by a license that champions commercial use, sets the stage for a development environment that is not only powerful and flexible but also inclusive and forward-looking," Eclipse Foundation concluded in its announcement today. "By choosing the Theia IDE, developers and organizations are not just adopting an IDE; they are joining a movement that values collaboration, freedom, and the collective pursuit of excellence in software development."
</p>
<br>
        
        
        
        
        
        
        
        <!-- pager start -->
        
        <!-- pager end -->
        
        
            
        

        
                <div>
                    <p id="ph_pcontent2_0_AuthorInfo_AboutAuthor">About the Author</p>
                    
                <p>
                    <strong></strong>
                    David Ramel is an editor and writer for Converge360.
                    <br>
                    
                    <a id="ph_pcontent2_0_AuthorInfo_AuthorEmail_0"></a>
                </p>
            
                </div>
            
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The story, as best I can remember, of the origin of Mosaic and Netscape [video] (223 pts)]]></title>
            <link>https://pmarca.substack.com/p/the-true-story-as-best-i-can-remember</link>
            <guid>40825033</guid>
            <pubDate>Fri, 28 Jun 2024 20:39:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pmarca.substack.com/p/the-true-story-as-best-i-can-remember">https://pmarca.substack.com/p/the-true-story-as-best-i-can-remember</a>, See on <a href="https://news.ycombinator.com/item?id=40825033">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main"><div data-testid="navbar"><p><a href="https://pmarca.substack.com/" native=""><img src="https://substackcdn.com/image/fetch/w_96,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg"></a></p><h2><a href="https://pmarca.substack.com/" native="">Marc Andreessen Substack</a></h2></div><div><div><article><div><div><div><div><div><div><div><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="white" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="m12 14 4-4"></path><path d="M3.34 19a10 10 0 1 1 17.32 0"></path></svg><p>Playback speed</p></div><div><p>×</p></div></div><div><div><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="white" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8"></path><polyline points="16 6 12 2 8 6"></polyline><line x1="12" x2="12" y1="2" y2="15"></line></svg><p>Share post</p></div></div><div><div><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="white" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8"></path><polyline points="16 6 12 2 8 6"></polyline><line x1="12" x2="12" y1="2" y2="15"></line></svg><p>Share post at current time</p></div></div></div><div><div><div><div id="trigger27149" aria-expanded="false" aria-haspopup="dialog" aria-controls="dialog27150" arialabel="View more"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="white" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8"></path><polyline points="16 6 12 2 8 6"></polyline><line x1="12" x2="12" y1="2" y2="15"></line></svg><p>Share from 0:00</p></div></div></div><div><div><p>0:00</p><p>/</p><p>0:00</p></div></div></div></div><div><div><p>Transcript</p></div></div></div></div><div><div><div><div><p>Enjoy!</p></div><div><div><a href="https://substack.com/profile/22353-marc-andreessen" target="_blank" rel="noopener"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_80,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg"><img src="https://substackcdn.com/image/fetch/w_80,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg" sizes="100vw" alt="" width="80"></picture></a></div><div><div><p><a href="https://substack.com/@pmarca">Marc Andreessen</a></p></div><div><p>Jun 28, 2024</p></div></div></div><div><span><p>Share</p></span><a role="button"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="var(--color-secondary-themed)" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="21" x2="3" y1="6" y2="6"></line><line x1="15" x2="3" y1="12" y2="12"></line><line x1="17" x2="3" y1="18" y2="18"></line></svg><p>Transcript</p></a></div></div></div><div><div><div><div><a href="https://pmarca.substack.com/" native=""><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_48,h_48,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg 48w, https://substackcdn.com/image/fetch/w_96,h_96,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg 96w, https://substackcdn.com/image/fetch/w_144,h_144,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg 144w" sizes="48px"><img src="https://substackcdn.com/image/fetch/w_48,h_48,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg" sizes="48px" alt="Marc Andreessen Substack" srcset="https://substackcdn.com/image/fetch/w_48,h_48,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg 48w, https://substackcdn.com/image/fetch/w_96,h_96,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg 96w, https://substackcdn.com/image/fetch/w_144,h_144,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg 144w" width="48" height="48"></picture></a></div><p>Marc Andreessen Substack</p></div><div data-component-name="SubscribeWidget"><form action="/api/v1/free?nojs=true" method="post" novalidate=""></form></div></div><div><div><p>Authors</p><div><div><a href="https://substack.com/profile/22353-marc-andreessen?utm_source=author-byline-face-podcast" target="_blank" rel="noopener"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_64,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg"><img src="https://substackcdn.com/image/fetch/w_64,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg" sizes="100vw" alt="" width="64"></picture></a></div><div><p>Marc Andreessen</p></div></div></div><div><p>Recent Posts</p><div><div><p><img type="image/gif" src="https://pmarca.substack.com/api/v1/video/upload/b33e4bc3-2428-4b56-8367-cd1d85a680cc/preview.gif?height=480"></p><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_150,h_150,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-video.s3.amazonaws.com%2Fvideo_upload%2Fpost%2F143254349%2Fb33e4bc3-2428-4b56-8367-cd1d85a680cc%2Ftranscoded-00001.png"><img src="https://substackcdn.com/image/fetch/w_150,h_150,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-video.s3.amazonaws.com%2Fvideo_upload%2Fpost%2F143254349%2Fb33e4bc3-2428-4b56-8367-cd1d85a680cc%2Ftranscoded-00001.png" sizes="(min-width:768px) 50vw, 100vw" alt="" width="150" height="150"></picture></div><div><div><p><a href="https://pmarca.substack.com/p/on-tech-politicspolicy-2-hour-video" data-testid="post-preview-title">On Tech Politics/Policy -- 2 hour video discussion</a></p></div><div><p><time datetime="2024-04-04T05:37:52.095Z">Apr 4</time>&nbsp;<span>•</span>&nbsp;<span><a href="https://substack.com/@pmarca">Marc Andreessen</a></span></p></div></div></div></div></div></div></div></div></article></div><div><p>Ready for more?</p><div><form action="/api/v1/free?nojs=true" method="post" novalidate=""></form></div></div></div><div><div><p>© 2024 Marc Andreessen</p><div><p><a href="https://substack.com/privacy" target="_blank" rel="noopener noreferrer">Privacy</a><span> ∙ </span><a href="https://substack.com/tos" target="_blank" rel="noopener noreferrer">Terms</a><span> ∙ </span><a href="https://substack.com/ccpa#personal-data-collected" target="_blank" rel="noopener noreferrer">Collection notice</a></p></div></div><div><a native="" href="https://substack.com/signup?utm_source=substack&amp;utm_medium=web&amp;utm_content=footer"><svg role="img" width="1000" height="1000" viewBox="0 0 1000 1000" fill="#FF6719" stroke-width="1.8" stroke="none" xmlns="http://www.w3.org/2000/svg"><g><title></title><path d="M764.166 348.371H236.319V419.402H764.166V348.371Z"></path><path d="M236.319 483.752V813.999L500.231 666.512L764.19 813.999V483.752H236.319Z"></path><path d="M764.166 213H236.319V284.019H764.166V213Z"></path></g></svg> Start Writing</a><p><a native="" href="https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&amp;utm_content=web-footer-button">Get the app</a></p></div><p><a href="https://substack.com/" native="">Substack</a> is the home for great culture</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US Supreme Court allows cities to ban homeless camps (105 pts)]]></title>
            <link>https://www.bbc.com/news/articles/cj774nxrpy7o</link>
            <guid>40823850</guid>
            <pubDate>Fri, 28 Jun 2024 18:54:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/cj774nxrpy7o">https://www.bbc.com/news/articles/cj774nxrpy7o</a>, See on <a href="https://news.ycombinator.com/item?id=40823850">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="byline" data-component="byline-block"><p><time>10 hours ago</time></p><div><p><span data-testid="byline-name">By&nbsp;<!-- -->Samantha Granville<!-- -->,&nbsp;<!-- --></span><span>BBC News, Los Angeles</span></p></div></div><div data-component="text-block"><p>The US Supreme Court has ruled in a 6-3 vote along ideological lines that cities can ban homeless people from sleeping rough.<!-- --></p><p>It is the court's most significant decision on homelessness since at least the 1980s, when many experts say the modern US homeless crisis began.<!-- --></p><p>The ruling says that local governments can enforce laws against people sleeping in public places without being in violation of the US constitution's limits on cruel and unusual punishment.<!-- --></p><p>The case started in the small city of Grants Pass, Oregon, where three homeless people sued after receiving citations for sleeping and camping outside.<!-- --></p><p>At a Supreme Court hearing in April, the city argued that criminal penalties were necessary to enforce local laws banning homeless people from public spaces for "reasons of cleanliness and safety".<!-- --></p><p>The homeless residents said those penalties violated the Eighth Amendment of the US Constitution because the city did not have any public shelters.<!-- --></p><p>Writing for the conservative majority in an opinion issued on Friday, Justice Neil Gorsuch wrote that the city's regulations on camping do not inflict “terror, pain or disgrace”.<!-- --></p><p>He added that the law does not criminalise the “mere status” of being homeless, and that the ban focuses more on the actions taken by individuals rather than their status alone.<!-- --></p><p>“Under the city’s laws, it makes no difference whether the charged defendant is homeless, a backpacker on vacation passing through town, or a student who abandons his dorm room to camp out in protest on the lawn of a municipal building,” Justice Gorsuch wrote.<!-- --></p><p>Justice Sonia Sotomayor, writing on behalf of the three dissenting liberal justices, wrote: “Sleep is a biological necessity, not a crime. Homelessness is a reality for so many Americans.”<!-- --></p><p>Several cities issued statements welcoming the ruling. San Francisco said it would help cities "manage our public spaces more effectively and efficiently," and the city of Grants Pass, the centre of the legal dispute, said that city leaders would meet with their lawyers to discuss next steps. <!-- --></p><p>Homelessness is on the rise in the US, fuelled in part by chronic shortages of affordable housing. Around 653,000 people did not have homes in 2023, the largest number since tracking began in 2007, according to US government figures. <!-- --></p><p>There were also an estimated 256,000 people living without shelter on a given night across the country last year, according to the <!-- --><a target="_blank" href="https://www.huduser.gov/portal/sites/default/files/pdf/2023-AHAR-Part-1.pdf">Department of Housing and Urban Development.<!-- --></a></p><p>Reacting to the ruling, the National Alliance to End Homelessness said it "sets a dangerous precedent that will cause undue harm to people experiencing homelessness and give free reign to local officials who prefer pointless and expensive arrests and imprisonment, rather than real solutions".<!-- --></p><p>Grants Pass's population has doubled to 40,000 in the last 20 years, but its supply of affordable or public housing has not.<!-- --></p><p>Soaring housing costs led to a sizeable number of people losing their homes.<!-- --></p><p>Town officials responded by passing laws that fined people for sleeping or camping in public. Over time, those fines stacked up, reaching thousands of dollars for some.<!-- --></p><p>Unable to pay for multiple citations, three homeless people sued the city.<!-- --></p><p>Their lawsuit reached the 9th Circuit Court of Appeals, which decided in 2022 that the restrictions in Grants Pass were so tight that they amounted to an effective ban on being homeless within city limits.<!-- --></p><p>The court had determined four years earlier in a similar case in Idaho that the constitution “bars a city from prosecuting people criminally for sleeping outside on public property when those people have no home or other shelter to go to".<!-- --></p></div><div data-component="text-block"><p>Meanwhile, the homeless crisis has continued to worsen.<!-- --></p><p>Jennifer Friedenbach, of the Coalition on Homelessness in San Francisco, said that money and resources should "go towards getting folks off the streets".<!-- --></p><p>“What we know is that arresting and fining people for being homeless doesn't work," she said. "It doesn't get anybody off the streets. It wastes municipal resources, and it exacerbates homelessness."<!-- --></p><p>The Supreme Court's Grants Pass decision will now allow cities to take more severe measures without the fear of legal recourse.<!-- --></p><p>The first problem with putting homeless people in jail is that it is extremely expensive, and when they get out, the person is still homeless and now even less apt to finding employment with a criminal record, says Elizabeth Funk, founder of DignityMoves, a nonprofit dedicated to ending unsheltered homelessness.<!-- --></p><p>“We need to be thinking about how to get this problem solved," she says. "It's not going to be fining people for doing something they can't avoid. It's helping them.”<!-- --></p><p>Some of the highest concentrations of homeless are on the West Coast. <!-- --></p><p>California, with its moderate temperatures, accounts for nearly half of all homeless people who live outside and has a total of 123,423 homeless, according to data from the US Department of Housing and Urban Development.<!-- --></p><p>Cities across the country have been wrestling with how to combat the growing crisis. <!-- --></p><p>The issue has been at the heart of recent election cycles in West Coast cities, including Los Angeles, where officials have poured record amounts of money into creating shelters and affordable housing while homelessness has still increased.<!-- --></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft informs customers that Russian hackers spied on emails (107 pts)]]></title>
            <link>https://www.reuters.com/technology/cybersecurity/microsoft-tells-clients-russian-hackers-viewed-emails-bloomberg-news-reports-2024-06-27/</link>
            <guid>40821994</guid>
            <pubDate>Fri, 28 Jun 2024 16:01:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/technology/cybersecurity/microsoft-tells-clients-russian-hackers-viewed-emails-bloomberg-news-reports-2024-06-27/">https://www.reuters.com/technology/cybersecurity/microsoft-tells-clients-russian-hackers-viewed-emails-bloomberg-news-reports-2024-06-27/</a>, See on <a href="https://news.ycombinator.com/item?id=40821994">Hacker News</a></p>
Couldn't get https://www.reuters.com/technology/cybersecurity/microsoft-tells-clients-russian-hackers-viewed-emails-bloomberg-news-reports-2024-06-27/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Supreme Court overrules Chevron deference [pdf] (111 pts)]]></title>
            <link>https://www.supremecourt.gov/opinions/23pdf/22-451_7m58.pdf</link>
            <guid>40821007</guid>
            <pubDate>Fri, 28 Jun 2024 14:36:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.supremecourt.gov/opinions/23pdf/22-451_7m58.pdf">https://www.supremecourt.gov/opinions/23pdf/22-451_7m58.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=40821007">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Supreme Court overturns 40-year-old "Chevron deference" doctrine (641 pts)]]></title>
            <link>https://www.axios.com/2024/06/28/supreme-court-chevron-doctrine-ruling</link>
            <guid>40820949</guid>
            <pubDate>Fri, 28 Jun 2024 14:31:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.axios.com/2024/06/28/supreme-court-chevron-doctrine-ruling">https://www.axios.com/2024/06/28/supreme-court-chevron-doctrine-ruling</a>, See on <a href="https://news.ycombinator.com/item?id=40820949">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-cy="au-image" data-chromatic="ignore"><img data-cy="StoryImage" alt="The U.S. Supreme Court in April 2024." fetchpriority="high" width="1920" height="1080" decoding="async" data-nimg="1" sizes="100vw" srcset="https://images.axios.com/0HWtMwcrXgCb_5lFNMsNTUU6Yy8=/0x900:8640x5760/320x180/2024/05/16/1715879188297.jpg?w=320 320w, https://images.axios.com/0HWtMwcrXgCb_5lFNMsNTUU6Yy8=/0x900:8640x5760/320x180/2024/05/16/1715879188297.jpg?w=320 320w, https://images.axios.com/7ZxL6bKpkNxWL6ubjcOmQOyctKo=/0x900:8640x5760/640x360/2024/05/16/1715879188297.jpg?w=640 640w, https://images.axios.com/7ZxL6bKpkNxWL6ubjcOmQOyctKo=/0x900:8640x5760/640x360/2024/05/16/1715879188297.jpg?w=640 640w, https://images.axios.com/lSDvbXXvNLIXeF2xPC0G-oouy-c=/0x900:8640x5760/768x432/2024/05/16/1715879188297.jpg?w=768 768w, https://images.axios.com/lSDvbXXvNLIXeF2xPC0G-oouy-c=/0x900:8640x5760/768x432/2024/05/16/1715879188297.jpg?w=768 768w, https://images.axios.com/hXC9UdkVp4dGz5KhHQvsDxPQfXw=/0x900:8640x5760/1024x576/2024/05/16/1715879188297.jpg?w=1024 1024w, https://images.axios.com/hXC9UdkVp4dGz5KhHQvsDxPQfXw=/0x900:8640x5760/1024x576/2024/05/16/1715879188297.jpg?w=1024 1024w, https://images.axios.com/Fqe49_mapPRZ-bwhEVixaLwVxSg=/0x900:8640x5760/1366x768/2024/05/16/1715879188297.jpg?w=1366 1366w, https://images.axios.com/Fqe49_mapPRZ-bwhEVixaLwVxSg=/0x900:8640x5760/1366x768/2024/05/16/1715879188297.jpg?w=1366 1366w, https://images.axios.com/99g12vnypLK_H6s5a3GwwpWuoss=/0x900:8640x5760/1600x900/2024/05/16/1715879188297.jpg?w=1600 1600w, https://images.axios.com/99g12vnypLK_H6s5a3GwwpWuoss=/0x900:8640x5760/1600x900/2024/05/16/1715879188297.jpg?w=1600 1600w, https://images.axios.com/9RMqkLL-qeuwtkEPKrMjkj_KObk=/0x900:8640x5760/1920x1080/2024/05/16/1715879188297.jpg?w=1920 1920w, https://images.axios.com/9RMqkLL-qeuwtkEPKrMjkj_KObk=/0x900:8640x5760/1920x1080/2024/05/16/1715879188297.jpg?w=1920 1920w" src="https://images.axios.com/9RMqkLL-qeuwtkEPKrMjkj_KObk=/0x900:8640x5760/1920x1080/2024/05/16/1715879188297.jpg?w=1920"><figcaption><p>The U.S. Supreme Court in April 2024. Photo; Bill Clark/CQ-Roll Call, Inc via Getty Images</p></figcaption></div><div data-chromatic="ignore"><p><span data-schema="smart-brevity"><p>The <a data-vars-link-text="Supreme Court" data-vars-click-url="https://www.axios.com/2024/06/20/supreme-court-rulings-trump-abortion" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2024/06/20/supreme-court-rulings-trump-abortion" target="_self">Supreme Court</a> on Friday curtailed the executive branch's ability to interpret laws it's charged with implementing, giving the judiciary more say in what federal agencies can do.</p><p><strong>Why it matters:</strong> The<strong> </strong><a data-vars-link-text="landmark 6-3 ruling" data-vars-click-url="https://www.supremecourt.gov/opinions/23pdf/22-451_7m58.pdf" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.supremecourt.gov/opinions/23pdf/22-451_7m58.pdf" target="_blank">landmark 6-3 ruling</a> along ideological lines overturns<strong> </strong>the court's 40-year-old "Chevron deference" doctrine. It could make it harder for executive agencies to tackle<strong> </strong>a wide array of policy areas, including <a data-vars-link-text="environmental" data-vars-click-url="https://www.axios.com/2024/01/17/supreme-court-chevron-environment" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2024/01/17/supreme-court-chevron-environment" target="_self">environmental</a> and <a data-vars-link-text="health" data-vars-click-url="https://www.axios.com/2024/01/17/supreme-court-chevron-deference-medicare-medicaid" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2024/01/17/supreme-court-chevron-deference-medicare-medicaid" target="_self">health</a> regulations and <a data-vars-link-text="labor and employment laws" data-vars-click-url="https://www.bloomberglaw.com/external/document/X1N1TE8K000000/labor-relations-professional-perspective-apres-moi-le-deluge-big" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.bloomberglaw.com/external/document/X1N1TE8K000000/labor-relations-professional-perspective-apres-moi-le-deluge-big" target="_blank">labor and employment laws</a>.</p></span></p><p><strong>Driving the news:</strong> Chief Justice John Roberts, writing the opinion of the court, argued Chevron "defies the command of" the Administrative Procedure Act, which governs federal administrative agencies.</p><ul><li>He said it "requires a court to ignore, not follow, 'the reading the court would have reached had it exercised its independent judgment as required by the APA.'"</li><li>Further, he said it "is misguided" because "agencies have no special competence in resolving statutory ambiguities. Courts do."</li></ul><p><strong>Roberts noted </strong>the court's decision did not call into question prior cases that relied on Chevron, including holdings pertaining to the Clean Air Act, because they "are still subject to statutory stare decisis despite our change in interpretive methodology."</p><ul><li>"Mere reliance on Chevron cannot constitute a 'special justification' for overruling such a holding," he said.</li></ul><p><strong>Justice Elena Kagan,</strong> in a dissenting opinion, wrote that the ruling Friday was "yet another example of the Court's resolve to roll back agency authority, despite congressional direction to the contrary."</p><ul><li>"Congress knows that it does not — in fact cannot — write perfectly complete regulatory statutes," she wrote. "It knows that those statutes will inevitably contain ambiguities that some other actor will have to resolve, and gaps that some other actor will have to fill. And it would usually prefer that actor to be the responsible agency, not a court.<strong>"</strong></li><li>She warned the decision "is likely to produce large-scale disruption." </li><li>"In one fell swoop, the majority today gives itself exclusive power over every open issue — no matter how expertise-driven or policy-laden — involving the meaning of regulatory law."</li><li>"The majority disdains restraint, and grasps for power."</li></ul><p><strong>Context: </strong>The ruling <a data-vars-link-text="marks another major victory" data-vars-click-url="https://www.axios.com/2023/05/25/supreme-court-epa-wetlands-clean-water-act" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2023/05/25/supreme-court-epa-wetlands-clean-water-act" target="_self">marks another major victory</a> for conservatives, who for decades have sought to limit the federal government's ability to regulate businesses.</p><ul><li>In the wake of the court's ruling, it's expected that more federal rules will be challenged in the courts and judges will have greater discretion to invalidate agency actions.</li><li>The decision comes one day after the Supreme Court <a data-vars-link-text="curtailed federal agencies' use of administrative law judges" data-vars-click-url="https://www.axios.com/2024/06/27/scotus-sec-jarkesy-decision" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2024/06/27/scotus-sec-jarkesy-decision" target="_self">curtailed federal agencies' use of administrative law judges</a> in another blow to the administrative state.</li></ul><p><strong>How it works:</strong> <a data-vars-link-text="The doctrine" data-vars-click-url="https://sgp.fas.org/crs/misc/R44954.pdf" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://sgp.fas.org/crs/misc/R44954.pdf" target="_blank">The doctrine</a> was created by the Reagan-era Supreme Court in Chevron U.S.A. v. Natural Resources Defense Council in 1984 and has since become <a data-vars-link-text="the most cited" data-vars-click-url="https://www.yalejreg.com/nc/most-cited-supreme-court-administrative-law-decisions-by-chris-walker/" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.yalejreg.com/nc/most-cited-supreme-court-administrative-law-decisions-by-chris-walker/" target="_blank">the most cited</a> Supreme Court decision in administrative law.</p><ul><li>Under Chevron deference, courts would defer to how to expert federal agencies interpret the laws they are charged with implementing provided their reading is reasonable — even if it's not the only way the law can be interpreted.</li><li>It allowed Congress to rely on the expertise within the federal government when implementing everything from health and safety regulations to environmental and financial laws.</li></ul><p><strong>Zoom in: </strong>However, Chevron was challenged in two separate cases over a National Marine Fisheries Service regulation meant to prevent overfishing on commercial fishing vessels.</p><ul><li>Fishing companies challenging the regulation claimed the doctrine violated Article III of the Constitution by shifting the authority to interpret federal law from the courts to the executive branch.</li><li> They also claimed it violated Article I by allowing agencies to formulate policy when only Congress should have lawmaking power.</li></ul><p><strong>The other side:</strong> The government argued that the doctrine had  safeguards within it that prevented agencies from usurping Congress's lawmaking authority.</p><ul><li>It noted that Chevron only applied to ambiguous text in laws passed by Congress and instances in which lawmakers had given interpretive authority to an agency.</li><li>The doctrine was also necessary to limit federal judges' abilities to make public policy when they may not have the expertise to do so and aren't subject to democratic accountability, the government said.</li></ul><p><strong>Between the lines: </strong>Lawyers who worked pro bono to represent fishing companies involved in the cases are also staff attorneys for Americans for Prosperity, a libertarian political advocacy group funded by billionaire Charles Koch, the <a data-vars-link-text="New York Times" data-vars-click-url="https://www.nytimes.com/2024/01/16/climate/koch-chevron-deference-supreme-court.html" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.nytimes.com/2024/01/16/climate/koch-chevron-deference-supreme-court.html" target="_blank">New York Times</a> reported earlier this year.</p><ul><li>The political network associated with Charles Koch and his late brother, David Koch, have long championed efforts to get cases before the Supreme Court that, if decided in their favor, would roll back the federal government's regulatory powers.</li><li>The Koch network also successfully attracted Supreme Court Justice Clarence Thomas, who voted against the doctrine, to speak at at least one of its <a data-vars-link-text="donor events" data-vars-click-url="https://www.axios.com/2023/09/22/clarence-thomas-koch-brothers-donor-events" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2023/09/22/clarence-thomas-koch-brothers-donor-events" target="_self">donor events</a> in 2018, <a data-vars-link-text="ProPublica" data-vars-click-url="https://www.propublica.org/article/clarence-thomas-secretly-attended-koch-brothers-donor-events-scotus" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.propublica.org/article/clarence-thomas-secretly-attended-koch-brothers-donor-events-scotus" target="_blank">ProPublica</a> reported last year.</li><li>It's unclear who purchased Thomas' flight to the 2018 event, as he never  reported it in his annual financial disclosure form. Thomas has attended at least two of such events in past years.</li></ul><p><strong>The big picture: </strong>In recent years, Chevron had fallen out of favor of the <a data-vars-link-text="conservative-majority Supreme Court" data-vars-click-url="https://crsreports.congress.gov/product/pdf/LSB/LSB11061" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://crsreports.congress.gov/product/pdf/LSB/LSB11061" target="_blank">conservative-majority Supreme Court</a>, which had declined to apply it or cite it in cases which it may once have applied.</p><ul><li>The ruling comes as some federal judges have taken a more active role in overruling agency expertise.</li><li>For example, Texas District Judge Matthew Kacsmaryk last April <a data-vars-link-text="paused" data-vars-click-url="https://www.axios.com/2024/03/23/abortion-pill-access-supreme-court-mifepristone-fda" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2024/03/23/abortion-pill-access-supreme-court-mifepristone-fda" target="_self">paused</a> the FDA's original 23-year-old approval of the abortion pill mifepristone in a case that's now to be decided by the Supreme Court.</li></ul><p><strong>Go deeper: </strong><a data-vars-link-text="Supreme Court brushes off payday lenders' challenge to consumer watchdog's funding" data-vars-click-url="https://www.axios.com/2024/05/16/supreme-court-cfpb-funding" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2024/05/16/supreme-court-cfpb-funding" target="_self">Supreme Court brushes off payday lenders' challenge to consumer watchdog's funding</a></p><p><em>Editor's note: This story was updated with details from the court's ruling.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple II graphics: More than you wanted to know (152 pts)]]></title>
            <link>https://nicole.express/2024/phasing-in-and-out-of-existence.html</link>
            <guid>40820311</guid>
            <pubDate>Fri, 28 Jun 2024 13:14:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nicole.express/2024/phasing-in-and-out-of-existence.html">https://nicole.express/2024/phasing-in-and-out-of-existence.html</a>, See on <a href="https://news.ycombinator.com/item?id=40820311">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>The Apple ][ is one of the most iconic vintage computers of all time. But since Wozniak’s monster lasted all the way until 1993 (1995 if you could the IIe card, which I won’t count until I get one), it can be easy to forget that in 1977, it was a video <em>extravaganza</em>. The competitors– even much bigger and established companies like Commodore and Tandy– generally only had text modes, let alone pixel-addressable graphics, and they certainly didn’t have sixteen colors. (Gray and grey are different colors, right?)</p>

<h2 id="preliminary">Preliminary</h2>

<p>My main source here is going to be <em>Understanding the Apple II</em> by Jim Sather. You might say, why should I read this post then, when I can go to the source? And honestly <a href="https://archive.org/details/utaii">yeah go do that</a>. What I’ll do here is try to digest it for myself by writing it in a form I find understandable, focused on details I find interesting. I’ll also throw together some looks at my own personal Apple II, maybe an oscilloscope, that sort of thing.</p>

<p><img src="https://nicole.express/assets/a2p-2.JPG" title="This photo is from one of the earliest posts on this blog" alt="The Apple II, plugged into a greenscreen monitor"></p>

<p>Now, my personal Apple ][<sub><i>plus</i></sub> and the book have something in common: they predate the Apple IIe. So this blog post will focus on the original Apple ][ designed machines. So when I talk about graphics mode, you won’t see the 80 column or double-width graphics modes. Those were IIe features; there were no provisions for such things on the original models. If this post proves interesting maybe I’ll dig into them later; I have an Apple IIgs, so certainly I <em>can</em> explore IIe exclusives. (And Jim Sather even wrote a follow-up book, <em>Understanding the Apple IIe</em>)</p>



<p><img src="https://nicole.express/assets/img/2apple2furious/lang.jpeg" title="taking out the language card is a pain so I left it in" alt="Apple II motherboard"></p>

<p>Here is the motherboard of my Apple II plus. It’s serial number 820-0044-01, which despite the 1979 copyright date, implies it’s definitely one of the later Apple IIs of its type– in 1981, the 820-0044-XX motherboard series was created by Apple in order to try to reduce radio-frequency interference (RFI), so this is known as the “RFI” motherboard. Go dig into <a href="https://archive.org/details/apple-ii-circuit-description/page/n157/mode/2up"><em>The Apple II Circuit Description</em></a> for all the nitty-gritty on motherboard variants.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/serial.jpeg" title="I want a Rev 0 so bad" alt="Apple II motherboard serial number"></p>

<p>It’s worth noting for those not familiar with the Apple II that the “Apple II” and the “Apple II plus” are the same system, whose major difference is just the ROM. After the introduction of the new ROMs in 1979, there was a period where the same motherboard, when sold with the original Integer BASIC ROM set, it was an Apple II; when Applesoft (Microsoft BASIC for the Apple) was baked in instead, the badge was changed to II plus. Eventually all Apples shipped with Applesoft and the original II badges stopped being used. The internal capabilities are identical, including all the graphics modes I’ll talk about.</p>

<h2 id="everything-but-the-kitchen-sync">Everything but the kitchen sync</h2>

<p>Television video systems predate computers wanting to use them. Therefore, they are greedy– a video signal must produce the expected signals at the expected times, or your television will lose synchronization with the signal. Regaining synchronization will likely result in a delay, and definitely a loss of visual signals.</p>

<p>So when the video signal is being drawn, everything else has to bow to the video system’s will. The <a href="https://nicole.express/2022/the-nes-as-an-artifact.html">Nintendo Entertainment System</a> creates a separate world, the PPU bus, for the video system to inhabit, and the developer should avoid touching it unless it’s convenient. Other machine did things differently– the Atari 130XE I recently <a href="https://nicole.express/2024/have-you-typed-atari-today.html">upgraded the keyboard</a> on uses a variant of the 6502 processor with an extra pin whose sole purpose is to halt the CPU whenever the video chip needs extra time to access RAM. Both the 130XE and the NES have a 1.7MHz CPU, but the NES can run its just a little faster. (The 130XE has literally 64 times the CPU-accessible RAM, it’ll be fine)</p>



<p>The Apple II is a little bit different than that. The CPU can access RAM whenever it wants. The graphics system, known as the video scanner (it’s not one chip), can also access the RAM it needs whenever it wants. How did Woz do this? A clue is hidden in the <a href="http://www.6502.org/documents/datasheets/synertek/">6502 datasheets</a>– specifically, this diagram is from the August 1976 “SY6500/MCS6500 Microcomputer Family Hardware Manual”.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/book.png" title="This diagram is vintage" alt="Timing diagram for the 6502, showing that the address and data only both need be valid when the clock is down"></p>

<p>The important thing to note is to look at φ<sub>2</sub>, the input clock. The data output of external memory only needs to be valid at the very end of the period in which φ<sub>2</sub> is low. The <em>entire</em> period when φ<sub>2</sub> is high, the 6502 is doing internal stuff, and you don’t actually want its signals to show up on the data bus. So if you have sufficiently fast memory, you can have your memory off doing something else while φ<sub>2</sub> is low, and the 6502 will never know the difference.</p>



<p>Now, my understanding is that the Commodore VIC-20 interlaces its memory accesses the same way. But there are three major differences between the Apple II and the Commodore VIC-20. Well, okay, there’s more than that. But there are a few particularly relevant ones:</p>

<ol>
  <li>The VIC-20 uses static RAM. The Apple II’s video scanner also handles DRAM refresh, while the Commodore doesn’t need to worry about that.</li>
  <li>The VIC-20 has no directly-accessible-pixel screen modes.</li>
  <li>The VIC-20’s Video Interface Chip is, well, a chip. A highly integrated circuit that is opaque to exterior analysis, and with room for extra logic to simplify the external interface. The Apple II video scanner is constructed out of easily-analyzable discrete logic, and wears its implementation details on its sleeve.</li>
</ol>

<p>It’s that last one that I think is really important here. A lot of the fiddly details of the Apple II’s video that a programmer has to put up with could have been papered over with a few extra logic gates, internal registers, and buffers. On an integrated circuit this wouldn’t be a big deal as long as everything still fit within the planned mask size. But Steve Wozniak was building the Apple II out of discrete logic, and Apple paid for each one of those chips. So it was in the interest of cost-effectiveness that Apple offloaded some complexity to the programmer.</p>

<h3 id="what-frequency-is-it-anyways">What frequency is it anyways?</h3>

<p>Let’s talk about pixels. The Apple II has a core oscillator at 14.318180MHz (“14M”), which is divided by two to create a 7.15909MHz (“7M”) signal, and then divided again to create a 3.579545MHz signal. This latter suspiciously-specific frequency is the NTSC “colorburst” frequency. 7M is our pixel clock; during active display, a pixel is output every (1 / 7.15909MHz). A division of 7M by 7 gives you 1.0227MHz, which sounds like the 1MHz CPU clock. <em>But is it?</em></p>

<p>The horizontal scanning rate of NTSC television is 15.734kHz. PAL is 15.625kHz, but we’ll ignore that which challenges us. That means we have 63.56μs to finish a line, or a quick trip to <a href="https://www.wolframalpha.com/input?i=%281+%2F+15.734kHz%29+%2F+%281+%2F+14.318180MHz%29">Wolfram Alpha</a> says that’s 910 clicks of our 14M clock. 65 clicks of 1.0227MHz, 455 pixels (including in blanking periods; 280 pixels the screen actually draws), and therefore 227.5 ticks of our color reference. Which isn’t evenly divided.</p>

<p>That’s actually correct and how the spec expects it, however, we need to keep all the accesses perfectly synchronized, and we want the color reference to also be constant relative to the CPU. (Ever wonder why systems like PC clones don’t always have consistent artifact colors?) So the Apple II lengthens that last 65th clock– it’s the “long cycle”, taking an extra tick. Now the scanline frequency is dropped to 15.700kHz (fine for most TVs), but also now the Apple II CPU clock is <strong>not constant</strong>, it varies based off of where the screen is drawing. It’s 1.0205MHz on average, but only on average.</p>

<p>Combine that with the knowledge of the Apple II’s <a href="https://nicole.express/2021/stop-mocking-me.html">audio system</a> and the stock Apple II’s lack of any hardware timers, and this is actually worth knowing. Unfortunately, the Apple II doesn’t give the programmer any ability to know where in its cycle the video scanner is at any given time. (<i>Understanding the Apple II</i> has a few possible mods you can do to your computer to let it know, though!)</p>

<h2 id="text-mode">TEXT mode</h2>

<p>Many vintage computers are defined by their fonts. The PET’s PETSCII is iconic, of course (though probably moreso for its use on the Commodore 64). The TRS-80 had its “pseudographics” characters allowing for very blocky pixel graphics despite only having text mode. And I’ve always enjoyed the thick letters of the Atari 8-bit font, which not only look nice, but also help readability on a system whose text mode is single-color (varying only in luminance) and would be viewed by most users over a noisy RF modulator.</p>

<p><img src="https://nicole.express/assets/img/dasatari/name-intro.jpg" title="Please exclusively refer to me as (5-note tune) from now on" alt="The point in the software described above"></p>

<p>And then we get to the Apple II, with its stark simple character forms, and absolutely no special characters to speak of. (Though at least it has built in inverse and flashing text modes, always useful?) One can easily imagine Steve Jobs in the mindset that would lead to the lack of arrow keys on the first Macintosh keyboard, insisting that since the Apple II’s standout feature was its graphics modes, there would be no special incentives to create pseudographics with text mode.</p>

<p><video src="https://nicole.express/assets/img/2apple2furious/txt.mp4" width="640px" autoplay="" loop="" muted=""> You don't have a video tag support or something? So you can't see this footage of The Apple II, showing its narrow font? Ah too bad.</video></p>

<p>You could think that, but you’d be wrong. Early revisions of the Apple II used the Signetics 2513 character generator ROM. This was a commercial, off the shelf part. You can go find its datasheet online.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/sign.png" title="Understanding the Apple II says it was a GI version but everything was second and third sourced those days" alt="Signetics logo advertising a 2513 HIGH SPEED 64x7x5 CHARACTER GENERATOR"></p>

<p>This was a popular part that Woz had used earlier in the Apple 1, and was a popular use for hobbyist projects like the famous 1973 <a href="https://en.wikipedia.org/wiki/TV_Typewriter">TV Typewriter</a>. So the message the Apple II font actually sends is “hey tinkerers, this is for you”. Now later models of the Apple II, like my II plus, use a more standard mask ROM instead of this weird 5-bit character-specific ROM; you can even mod it to put your own EEPROM in. On my RFI board, it’s “ROM SPCL” deep under the keyboard.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/spcl.jpeg" title="a special rom for a special computer <3" alt="ROM SPCL is underneath the keyboard PCB. It is only visible by the edge of its socket"></p>

<p>Despite the Signetics ROM being only 5 pixels wide, text characters on the Apple II are 7 pixels wide. But why 7? Well, the reason is all that screen math again. The memory access clock is 7M divided by 7 to get the memory timing; so you have one memory cycle to get 7 pixels. We have 40 columns in the visible area, and 24 rows, fine for low resolution text mode. Pretty basic, right?</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/steve.jpg" title="I swear this won't become a habit but I couldn't resist" alt="Steve Jobs saying 'One More Thing'"></p>

<h3 id="memory-layout">Memory layout</h3>

<p>The official Apple <em>Apple II Reference Manual</em>, signed by Woz on the cover, provides some detail on the memory layout that starts to be a bit concerning.</p>

<blockquote>
  <p>The area of memory which is used for the primary text page starts at location number 1024 and extends to location number 2047. The secondary screen begins at location number 2048 and extends to location 3071. In machine language, the primary page is from hexadecimal address $400 to address $7FF; the secondary page is from $800 to $BFF. Each of these pages is 1,024 bytes long. Those of you intrepid enough to do the multiplication will realize that there are only 960 characters displayed on the screen. The remaining 64 bytes in each page which are not displayed on the screen are used as temporary storage locations by programs stored in PROM on Apple Intelligent Interface (r) peripheral boards (see page 82).</p>
</blockquote>

<p>You might wonder why they’re giving memory addresses in decimal– well, that was pretty normal for 70’s and 80’s computer manuals. You might also wonder why they’re so desperate for RAM that such a small amount of extra RAM would be in demand for peripheral cards– well, the original Apple II was sold with a base RAM configuration of 4kiB, so no addresses above <code>0x1000</code> would exist.</p>

<p>But the real question is how that memory is laid out. The Reference Manual just gives a screen map, but it doesn’t tell you <em>why</em> it is the way it is. My scan of this isn’t great, but you can see that the rows are very much not sequential.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/text.png" title="and you thought microsoft was bad at counting to ten" alt="Screen layout diagram"></p>

<p>I’ve called the Apple II screen memory layout bizarre before, and from a programmer’s perspective, it really is. But I was also criticized for that– because Steve Wozniak is really doing something quite impressive here. You have to understand the constraints he was under.</p>

<ol>
  <li><strong>Use as few chips as possible</strong>. Each piece of discrete logic costs money. So wherever possible work with the signals you have– the binary counters that are used for all that counting logic, for example.</li>
  <li><strong>But don’t waste RAM</strong>. The constraints of the television standard give us 40 columns wide. That’s not a simple binary number; you could have a gap after every text line, but that’d give 24 small areas of wasted RAM.</li>
  <li><strong>Refresh DRAM</strong>. DRAM addressing is pretty complicated, relying on “row” and “column” signals. But long story short, when going through the screen to display video, you also need to access every “row” address every 2ms. A 60Hz frame is 16.67ms. (Note that each chip is either 4kiB or 16kiB, so if you refresh the right range in one chip you can simultaneously refresh the rest)</li>
</ol>

<p>I’m not going to go into the full detail of the design because I think I’d just be repeating Jim Sather’s book in full here. But more-or-less, the screen is divided into three areas: top, middle, and bottom, each of eight rows. Then, the memory page, let’s use the primary text page <code>0x400</code>, is divided into eight subsections of 128 bytes each– 128 bytes gives us something our binary counters can easily catch. Each of these 128 byte sections is as follows:</p>

<ul>
  <li>One row of 40 characters for the top area</li>
  <li>One row of 40 characters for the middle area</li>
  <li>One row of 40 characters for the bottom area</li>
  <li>One 8 byte “screen hole” given to the Apple Intelligent Interface (r) peripheral boards</li>
</ul>

<p>In order to refresh a larger part of the screen during TEXT and LORES modes, the video scanner actually accesses different addresses during the horizontal blanking period, which allows it to refresh a wider range. These are wrong for video, but there’s no video during the blanking period. It doesn’t need to do this in HIRES mode, so it doesn’t.</p>



<p>From the programmer’s perspective, this usually just is papered over with a lookup table, and isn’t a big deal in the end.</p>

<h2 id="hires-mode">HIRES mode</h2>

<p>The Apple II offers no ability to customize the blocks in text mode; that Signetics ROM could not be replaced with RAM. This was the case for all three machines of the 1977 “Trinity”, but later Commodore machines would allow it. Unlike the other two “Trinity” machines, though, Apple lets you address the screen pixels directly.</p>

<p>With 40 bytes per row, and 24 * 8 = 192 rows in the visible area, you’d need at least 7.5kiB for just one screen. So we’ve abandoned the 4kiB Apple II users here– the 4kiB Apple II was not on the market very long though, and the upgraded 16kiB was the low-end model for most of the late 70’s. By the time of later models like my plus, 48kiB was more or less assumed anyways. With 16kiB you get one HIRES page at <code>0x2000</code>, with 48kiB you can get a second one at <code>0x4000</code>. (Double-buffering in 1977?) Of course, that’s a lot of RAM, so if you don’t need it a program will probably use it for something else.</p>

<p>Now, there’s an interesting problem that HIRES memory has to handle– the addresses and layout for text mode are very carefully chosen and set up to allow DRAM refresh. But now we need to get eight times as many addresses, with as few changes as possible. How do we do it? HIRES mode uses <em>higher</em> address bits, which are mapped to the DRAM “columns”, mostly not impacting the careful dance of refresh. But this creates a pretty wild memory layout.</p>

<p>
<svg width="640" height="860" viewBox="-10 0 320 420" xmlns="http://www.w3.org/2000/svg" title="oh gods please age well SVG" alt="Memory map">
    
    <defs>
        <pattern id="striped" viewBox="0,0,7,8" width="4.16%" height="4.16%">
            <rect fill="#0000ff" x="-2" y="0" width="40" height="1"></rect>
            <rect fill="#00a0a0" x="-2" y="1" width="40" height="1"></rect>
            <rect fill="#00ff00" x="-2" y="2" width="40" height="1"></rect>
            <rect fill="#a0a000" x="-2" y="3" width="40" height="1"></rect>
            <rect fill="#ff0000" x="-2" y="4" width="40" height="1"></rect>
            <rect fill="#a0a0ff" x="-2" y="5" width="40" height="1"></rect>
            <rect fill="#ffa0a0" x="-2" y="6" width="40" height="1"></rect>
            <rect fill="#a000a0" x="-2" y="7" width="40" height="1"></rect>
        </pattern>
    </defs>
    <rect fill="#fff" x="-20" y="-20" width="320" height="450"></rect>
    <text x="0" y="12">Screen</text>
    <rect fill="url(#striped)" height="192" width="240" x="0" y="20"></rect>
    <text x="0" y="227">Memory</text>
    <rect fill="#0000ff" x="0" y="230" width="240" height="24"></rect>
    <rect fill="#00a0a0" x="0" y="254" width="240" height="24"></rect>
    <rect fill="#00ff00" x="0" y="278" width="240" height="24"></rect>
    <rect fill="#a0a000" x="0" y="302" width="240" height="24"></rect>
    <rect fill="#ff0000" x="0" y="326" width="240" height="24"></rect>
    <rect fill="#a0a0ff" x="0" y="350" width="240" height="24"></rect>
    <rect fill="#ffa0a0" x="0" y="374" width="240" height="24"></rect>
    <rect fill="#a000a0" x="0" y="398" width="240" height="24"></rect>
</svg>
</p>

<p>Apologies to the colorblind for the graph above! In fact, maybe I should just apologize to everyone with eyes. The SVGs are an experiment, we’ll see how they go.</p>

<p>Essentially, the HIRES memory space is divided into eight sections. The first section is the top row of pixels for each 7x8 text mode tile, the second section the second row, etc. etc. Each section (the large colored blocks in memory above) is itself laid out the same way as TEXT mode, complete with some screen holes. Confusing? Sure, but again, most programmers made a lookup table or two and called it a day. Each byte has seven pixels, the first bit being ignored. (The bits are also pushed to the screen in <em>opposite</em> order to how they’re usually written, but this is all just convention anyway)</p>

<h3 id="color">Color</h3>

<p><img src="https://nicole.express/assets/img/softcard/monitor3.jpg" title="Probably because it looks awesome" alt="The Monitor III sitting on the Apple II plus"></p>

<p>Everything I just described is good enough for business software and users of the monochrome Monitor ///. But this is an Apple II, the ultimate gaming PC of 1977. We want <em>color</em>. You probably know about “NTSC artifacts”, but what does that mean? And that’s where all our timing synchronization comes into play.</p>

<p>Imagine the Apple II drawing an alternating pixel pattern, <code>0101</code>. It draws those pixels at the rate of its pixel clock, <code>7M</code>. An important thing to remember about square waves is that the frequency is the frequency of a <em>complete cycle</em>, both the “up” and “down” of the wave.</p>

<p>
<svg width="500" height="325" viewBox="0 0 100 65" xmlns="http://www.w3.org/2000/svg" title="oh gods please age well SVG" alt="Pixel timing diagram">
    
    <rect fill="#fff" x="0" y="0" width="100" height="100"></rect>
    <text x="5" y="7">7M (pixel clock)</text>
    <polyline points="10,10 10,20 20,20 20,10 30,10 30,20 40,20 40,10 50,10 50,20 60,20 60,10 70,10 70,20 80,20 80,10 90,10 90,20" stroke="red" stroke-width="2" fill="none"></polyline>
    <text x="5" y="27">3.5M (color signal)</text>
    <polyline points="10,30 10,40 30,40 30,30 50,30 50,40 70,40 70,30 90,30" stroke="blue" stroke-width="2" fill="none"></polyline>
    <text x="5" y="47">Pixels</text>
    <rect fill="#000" x="10" y="50" width="20" height="10" rx="4"></rect>
    <rect fill="#000" x="50" y="50" width="20" height="10" rx="4"></rect>
</svg>
</p>

<p>That is to say, if you alternate pixels, you’re creating a signal that repeats at the colorburst frequency! This is a real color signal, just like you’d generate if you had one of those fancy TMS9918As or something, but it’s being generated using the same mechanism that generates the pixels. (Sure, it’s a square wave here, but that’s what signal filters and such are for) Also, as the programmer, you get to control it directly.</p>

<p><img src="https://nicole.express/assets/img/pang-sparts/rf2av.jpg" title="Yep it's a rainbow" alt="Pong with rainbow backgrounds and art"></p>

<p>This is a screenshot from <a href="https://nicole.express/2024/super-duper-rainbow-pong.html">Atari’s <em>Pong Sports IV</em></a>, which like Atari’s other <em>Home Pong</em> series of consoles, uses a slightly out of phase crystal to create a cool rainbow effect. Obviously that isn’t possible here– with such strict pixel timing, we can only create two color phase shifts. And now you know why the “long cycle” that keeps the memory accesses in sync with the colorburst frequency is so important, or the phase shifts would also be different on each scanline.</p>

<p>
<svg width="500" height="400" viewBox="0 0 100 80" xmlns="http://www.w3.org/2000/svg" title="oh gods please age well SVG" alt="Pixel timing diagram">
    
    <rect fill="#fff" x="0" y="0" width="100" height="100"></rect>
    <text x="5" y="7">7M (pixel clock)</text>
    <polyline points="10,10 10,20 20,20 20,10 30,10 30,20 40,20 40,10 50,10 50,20 60,20 60,10 70,10 70,20 80,20 80,10 90,10 90,20" stroke="red" stroke-width="2" fill="none"></polyline>
    <text x="5" y="27">3.5M (color signal)</text>
    <polyline points="10,30 10,40 30,40 30,30 50,30 50,40 70,40 70,30 90,30" stroke="blue" stroke-width="2" fill="none"></polyline>
    <text x="5" y="47">Patterns</text>
    <polyline points="10,50 10,60 30,60 30,50 50,50 50,60 70,60 70,50 90,50" stroke="orange" stroke-width="2" fill="none"></polyline>
    <polyline points="10,75 10,65 30,65 30,75 50,75 50,65 70,65 70,75 90,75" stroke="orange" stroke-width="2" fill="none"></polyline>
    <rect fill="rgba(0, 0, 0, 0.5)" x="10" y="50" width="20" height="10" rx="4"></rect>
    <rect fill="rgba(0, 0, 0, 0.5)" x="50" y="50" width="20" height="10" rx="4"></rect>
    <rect fill="rgba(0, 0, 0, 0.5)" x="30" y="65" width="20" height="10" rx="4"></rect>
    <rect fill="rgba(0, 0, 0, 0.5)" x="70" y="65" width="20" height="10" rx="4"></rect>
</svg>
</p>

<p>I deliberately used the chosen colors above because they are <em>not</em> the colors generated, because I don’t necessarily have the phase relationships perfectly correct. Take the diagrams above as basic conceptual scribbles, not necessarily oscilloscope traces– the point is, there are two signals: one in phase with the colorburst, one 180° out of phase with the colorburst. But the colorburst is itself defined as 180° out of phase with the color carrier, so this is a bit complex.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/snek.png" title="no step on snek" alt="Ultima II showing a green dorky-looking snake in a dungeon"></p>

<p>Anyway long story short, as <em>Ultima II</em> shows us above, it’s pink and green, the colors are pink and green. Well, pink is looking awfully purplish today, but that’s the wonder of NTSC. (And the horizontal lines visible on the green snake are the wonder of using square waves and this particular upscaler-capture combo) Notice those horizontal lines also picked up some color– if you want to guarantee white, you’ll need to create a signal with a frequency that <em>isn’t</em> a color carrier. The line above is just one pixel, but two pixels next to each other will do it.</p>

<p>
<svg width="500" height="325" viewBox="0 0 100 65" xmlns="http://www.w3.org/2000/svg" title="oh gods please age well SVG" alt="Pixel timing diagram">
    
    <rect fill="#fff" x="0" y="0" width="100" height="100"></rect>
    <text x="5" y="7">7M (pixel clock)</text>
    <polyline points="10,10 10,20 20,20 20,10 30,10 30,20 40,20 40,10 50,10 50,20 60,20 60,10 70,10 70,20 80,20 80,10 90,10 90,20" stroke="red" stroke-width="2" fill="none"></polyline>
    <text x="5" y="27">3.5M (color signal)</text>
    <polyline points="10,30 10,40 30,40 30,30 50,30 50,40 70,40 70,30 90,30" stroke="blue" stroke-width="2" fill="none"></polyline>
    <text x="5" y="47">Pixels</text>
    <polyline points="10,50 10,60 50,60 50,50 90,50 90,60" stroke="orange" stroke-width="2" fill="none"></polyline>
    <rect fill="rgba(0, 0, 0, 0.5)" x="10" y="50" width="20" height="10" rx="4"></rect>
    <rect fill="rgba(0, 0, 0, 0.5)" x="30" y="50" width="20" height="10" rx="4"></rect>
</svg>
</p>

<p>Even if you alternate groups of two pixels, that signal isn’t at 3.579545MHz, so your television won’t be able to pull out any color information from it– it’ll just be treated as monochrome white. Modulo some higher-frequency fringes, after all, this <em>is</em> still good old-fashioned <a href="https://nicole.express/2021/shouldve-had-field-sequential.html">composite video</a> and no filter is perfect. Apple II users got used to some color fringing, or used a monochrome monitor.</p>

<p>Now, unfortunately, there is another catch here. Our addressable pixel areas are 7 pixels wide, which is not evenly divisible by two.<sup><i>citation needed</i></sup> This means that if you have a pattern, <code>x0101010</code>, whether it’s pink or green will depend on its position relative to the beginning of the line. Odd addresses will have one color, even addresses the other. This is why many Apple II games, like <em>Ultima III: Exodus</em>, create a grid of 14-pixel wide tiles that things move about on.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/exy.png" title="let my people go?" alt="Exodus, showing a tiled area"></p>

<p>This is still HIRES graphics mode, not a tile-based mode– the developer is just implementing tiles to make their lives easier. And on this title screen they only do so where things will move around in the bottom half.</p>

<h3 id="whats-the-big-deal">What’s the big deal?</h3>

<p>Now you might have noticed something about the screenshot above. It’s got colors that aren’t pink and green– it’s got blue water, and red lava. That’s true, but it’s only true because my Apple II isn’t a Revision 0. Those early adopters only have a three-color HIRES mode. The rest of us have something better.</p>

<p>Remember that first bit? It’d be pretty wasteful to just leave that unused. This is especially true from the perspective of the video scanner– this bit is fully decoded and just sitting there, waiting to have a use applied for it. What Woz did was have it delay the output of pixels by one cycle of the <em>14M</em> master clock, breaking the 7M pixel clock, but creating new phase shifts.</p>

<p>
<svg width="500" height="400" viewBox="0 0 100 80" xmlns="http://www.w3.org/2000/svg" title="oh gods please age well SVG" alt="Pixel timing diagram">
    
    <rect fill="#fff" x="0" y="0" width="100" height="100"></rect>
    <text x="5" y="7">7M (pixel-ish clock)</text>
    <polyline points="10,10 10,20 20,20 20,10 30,10 30,20 40,20 40,10 50,10 50,20 60,20 60,10 70,10 70,20 80,20 80,10 90,10 90,20" stroke="red" stroke-width="2" fill="none"></polyline>
    <text x="5" y="27">3.5M (color signal)</text>
    <polyline points="10,30 10,40 30,40 30,30 50,30 50,40 70,40 70,30 90,30" stroke="blue" stroke-width="2" fill="none"></polyline>
    <text x="5" y="47">Patterns (offset)</text>
    <polyline points="9,50 20,50 20,60 40,60 40,50 60,50 60,60 80,60 80,50 90,50" stroke="#0d0" stroke-width="2" fill="none"></polyline>
    <polyline points="9,75 20,75 20,65 40,65 40,75 60,75 60,65 80,65 80,75 90,75" stroke="#0d0" stroke-width="2" fill="none"></polyline>
    <polyline points="10,50 10,60 30,60 30,50 50,50 50,60 70,60 70,50 90,50" stroke="rgba(0,0,0,0.2)" stroke-width="2" fill="none"></polyline>
    <polyline points="10,75 10,75 10,65 30,65 30,75 50,75 50,65 70,65 70,75 90,75" stroke="rgba(0,0,0,0.2)" stroke-width="2" fill="none"></polyline>
</svg>
</p>

<p>This gives us two more phases to work with, 90° out of phase with the pink and green colors we had before– blue and red-orange. But remember, we’re limited to using them within groups of 7 pixels. You can almost think of this as like being able to choose a palette for a 1-pixel high and 7-pixel wide area, except for…</p>

<h3 id="the-boundaries">The boundaries</h3>

<p>An interesting thing can be seen in this screenshot from Sega’s <em>Frogger</em>. You should now understand why Frogger is white, and why the colored areas are laid out the way they are. But take a look at where the coast meets the water. The green areas and the blue areas don’t quite line up.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/f1.png" title="Frogger on the TRS-80 has horizontal scrolling" alt="Frogger port"></p>

<p><img src="https://nicole.express/assets/img/2apple2furious/zoom.png" title="plus turtles arent as big as trucks, inaccurate" alt="Zoomed in to show misaligned blocks"></p>

<p>Why don’t they line up? It’s not the developer’s fault, it’s because they can’t. Take a look at what happens if I plug the Apple II’s output into the component luma input on the OSSC. (This is actually how I got most of the text mode captures too, to avoid unnecessary color noise)</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/f2.jpeg" title="the wormhole aliens HATE the apple ii because it is linear" alt="Frogger port all made out of lines"></p>

<p>The lines just don’t line up. The Apple II can get you in a mindset trap; of <em>course</em> it can’t line up, you might start to think. But on a system where the luminance and chrominance are set separately, and where it can output analog values, not just 0 or 1, of course it can. On the Apple II, all sorts of weirdness will happen where non-delayed pixels interact with delayed pixels. Say, at the edge of the screen, when we enter the screen border, the seven pixels will be abruptly cut off, leaving a half-pixel at the edge.</p>

<p>
<svg width="500" height="250" viewBox="0 40 100 50" xmlns="http://www.w3.org/2000/svg" title="oh gods please age well SVG" alt="Pixel timing diagram">
    
    <rect fill="#fff" x="0" y="0" width="100" height="100"></rect>
    <text x="30" y="45">Screen boundary</text>
    <line stroke-width="2" x1="70" y1="0" x2="70" y2="100" stroke="#f00"></line>
    <polyline points="9,50 20,50 20,60 40,60 40,50 60,50 60,60 90,60" stroke="#0d0" stroke-width="2" fill="none"></polyline>
    <polyline points="9,75 20,75 20,65 40,65 40,75 60,75 60,65 70,65 70,75 90,75" stroke="#0d0" stroke-width="2" fill="none"></polyline>
</svg>
</p>

<p>Now, do these effects matter? On a CRT, probably not, but you might see some fun color fringes here and there. But if you’re wondering why your Apple II image capture is looking so much worse than your other systems, even over composite? Well, stuff like this doesn’t help.</p>

<p>HIRES is by far the most important graphics mode on the Apple II; more games used it than any of the other options, and even business software used it to do things like implement 80-column text in software. A bit awkward and weird? People got over it. I’ll end this discussion with one of my more nostalgic vintage HIRES intro sequences, from <em>Ultima II</em>. Sorry about the lack of audio; I kept the internal speaker wired up separately when I installed the Mockingboard, and this game only uses the internal beeper.</p>

<p><video src="https://nicole.express/assets/img/2apple2furious/u2.mp4" width="640px" controls="">You can't see this due to lacking video tag support. It's pretty cool, a dragon shows up and breathes fire.</video></p>

<p>I especially like the color animation on the “II” from toggling that seventh bit.</p>

<h2 id="kill-it-with-fire-or-a-transistor-will-do">Kill it with fire, or a transistor will do</h2>

<p>With all the above in mind, text mode <em>should</em> have the same color fringing everywhere that you see in HIRES graphics mode. The pixels are the same size, and Apple didn’t even design the font, so it’d be pretty impressive if it had been optimized to not fringe. But on most displays you’ll see nice pure white in Apple II text mode. How come?</p>

<p>Well, take a look at a screenshot of <em>Mission Asteroid</em> by Sierra. This uses a mixed mode, which I won’t really go into detail on how it works, but basically has four lines worth of text mode at the bottom of the screen underneath the graphics mode. And in this mode, the text fringes quite a bit. All those single-pixel horizontal lines suffer the same problem as the horizontal lines the snake was hanging out in in <em>Ultima II</em>.</p>

<p><img src="https://nicole.express/assets/img/yellow/mission.jpg" title="this is art hang it in the louvre" alt="A secretary sits at a desk. The game is a text adventure with a prompt at the bottom of the screen."></p>

<p>If I had a Revision 0 Apple II, I’d have the same experience with the pure text mode. But I don’t– that’s because later revisions of the Apple II like mine added a circuit called the “color killer”, which removes the color burst when in text mode. In theory, a signal without a color burst should always be interpreted by the TV as a monochrome signal, because NTSC is backwards-compatible. The problem is, the color killer isn’t great– it merely reduces the color burst.</p>

<p>Here’s the signal with the color burst present, which I obtained by booting BASIC and typing <code>HGR</code>.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/color1.png" title="doctor please my color's burst" alt="Oscilloscope trace showing color burst"></p>

<p>And here it is with the color burst on, which I obtained by typing <code>TEXT</code> with the same trace showing. I’m kind of surprised this ever doesn’t work, honestly– I guess that little bit of a cycle must be enough to confuse a sufficiently sensitive detector. Interesting, <em>Understanding the Apple II</em> suggests you mod your TV to detect the color burst, only suggesting modding the computer as a last resort, despite the many other computer mods recommended.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/color2.png" title="dont like all that noise" alt="Oscilloscope trace showing color burst gone, except for a cycle"></p>

<p>This impact for me has always shown up on higher-quality scalers, which are desperate to try to extract a color signal. The color killer works fine on a cheap AV2HD box, but the Micromsoft Framemeister ends up with a fringy mess. Check out my <a href="https://nicole.express/2021/composite-conflict-completed.html#test-10-the-apple-">composite scaler competition</a> post for more details on that.</p>

<p><img src="https://nicole.express/assets/img/composhoot/apii-meister.gif" title="I talk about the color killer way too much" alt="Poorly color killed signal"></p>

<h2 id="lores-mode">LORES mode</h2>

<p>The Apple II’s LORES graphics mode is very impressive. It can display any pixel on screen in any of 16 colors. Well, 15. More or less. There’s two greys that are usually the same. But still, far more colors than HIRES, and with pure pixel-level color selection. So what’s the catch? The resolution is a whopping 40x48. When even <a href="https://nicole.express/2024/radio-keith-orpheum.html">RCA Studio II</a> fans think you could use a few more pixels, you’re in trouble.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/duke1.png" title="Duke Nukem accepts all babes, regardless of how few pixels" alt="Duke Nukem port in LORES mode"></p>

<p>That’s not to say LORES mode is useless. <a href="http://deater.net/weave/vmwprod/duke/">Deater</a> has done quite a few demakes into LORES mode; such a low resolution makes it fast to update the whole screen even doing things like parallax, and the mixed text/graphics mode means you can use text mode for things that absolutely have to be readable, like scores and such.</p>

<p><img src="https://nicole.express/assets/img/mock-me/little-brick-out.png" title="cuz she's a brick... OUT" alt="Game Over screen in Little Brick Out. Only a few bricks are broken and the game is telling me my score is not too good."></p>

<p>And honestly LORES mode is part of the heart of the Apple II. It’s the mode that Wozniak created so that, having done <em>Breakout</em> in hardware for Atari, he could now do it in software with <em>Little Brick Out</em>. It’s the mode classic business simulation <em>Lemonade Stand</em> used. It justified the rainbow Apple logo. So how does it work?</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/lores.png" title="I took a new version of this picture, because I love you" alt="LORES color palette from the Diagonstics II plus disk"></p>

<p>The Koryuu I’m using here has a filter option that kind of blurs everything horizontally; I generally keep it disabled, but it does at least blur the lines together and gets rid of the high frequency noise. Of course, as we’ll see, that noise is the point. By the way, these color bars are from the Apple Diagnostics II+ disk.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/blurry.png" title="fuzzy" alt="LORES color palette from the Diagonstics II plus disk, blurred a bit"></p>

<p>The LORES mode is twice the height of TEXT mode, and that’s no coincidence. The same screen data as text mode is used, with the same layout– the difference is, the two “nybbles” of each byte each become one of 16 colors, stacked on top of each other. A LORES pixel is 7 HIRES pixels wide, and four HIRES pixels tall.</p>

<p>But how does LORES get so many colors?</p>

<p>
<svg width="500" height="325" viewBox="0 0 100 65" xmlns="http://www.w3.org/2000/svg" title="oh gods please age well SVG" alt="Pixel timing diagram">
    
    <rect fill="#fff" x="0" y="0" width="100" height="100"></rect>
    <line stroke-width="2" x1="50" y1="0" x2="50" y2="100" stroke="#ccc"></line>
    <text x="5" y="07">14M (oscillator)</text>
    <polyline points="10,10 10,20 15,20 15,10 20,10 20,20 25,20 25,10 30,10 30,20 35,20 35,10 40,10 40,20 45,20 45,10 50,10 50,20 55,20 55,10 60,10 60,20 65,20 65,10 70,10 70,20 75,20 75,10 80,10 80,20 85,20 85,10 90,10 90,20" stroke="green" stroke-width="2" fill="none"></polyline>
    <text x="5" y="27">7M (TEXT pixel clock)</text>
    <polyline points="10,30 10,40 20,40 20,30 30,30 30,40 40,40 40,30 50,30 50,40 60,40 60,30 70,30 70,40 80,40 80,30 90,30 90,40" stroke="red" stroke-width="2" fill="none"></polyline>
    <text x="5" y="47">3.5M (color signal)</text>
    <polyline points="10,50 10,60 30,60 30,50 50,50 50,60 70,60 70,50 90,50" stroke="blue" stroke-width="2" fill="none"></polyline>
</svg>
</p>

<p>Notice that for every four cycles of the <code>14M</code> master oscillator, there’s one cycle of the 3.5MHz color burst signal. So if you repeat a four-bit pattern at the rate of the 14MHz clock, you’ll create a signal with a period of 3.5MHz. That’s all <code>LORES</code> mode is doing to create its colors. Sure, there will be components of the signal at other frequencies– you can see the OSSC trying to show the high-frequency lines, while other devices like an RF modulator might blur the high-frequency signal out into a flat luminance.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/lores2.png" title="i am the lores, i speak for the pixels" alt="LORES color palette in monochrome, showing lines"></p>

<p>And the repeating patterns? That’s the genius part– they <em>are</em> the nybble in question. Take a look at the chart again. Repeating <code>0000</code> over and over again? Of course that’s a pure black. Repeating <code>1111</code> again and again? That’s pure white. What are the grey patterns? <code>0101</code> (5) and <code>1010</code> (10), which alternate fast enough that they don’t really have a low-frequency component, so no color to pick up on. There are two alternating patterns, so two greys. (For homework, consider what happens when those two greys are next to each other)</p>

<p>There is a bit more to it; for example, the phase inversion caused by having 7-pixel-wide slots is compensated for in LORES, but in general this is really a very clever graphics mode. Double HIRES mode on the 80-column Apple IIe uses the same pixel patterns, but that’s a story for another time.</p>

<h2 id="apple-ii-forever">Apple II Forever</h2>

<p><img src="https://nicole.express/assets/img/2apple2furious/oregon.png" title="not going to lie I have more nostalgia for the monochrome Mac oregon trail" alt="Oregon Trail: NICOLE has cholera"></p>

<p>The Apple II is one of the oldest computers I recall using; my kindergarten in 1995 had an old machine they let the kids bang on to avoid them breaking anything new. And yet that computer still fascinates me to this day. I think it’s because it’s not only a useful machine, with a lot of history, but also that despite its rougher edges, it’s something that beckons to be understood. And is well documented to boot. Definitely beats the arcade boards for that.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/timezone.png" title="this is the opening to TIME ZONE, the biggest Apple II game of all time, and the most expensive at release. if you can afford this game, you can afford this big house" alt="YOU ARE IN FRONT OF YOUR OWN HOUSE. Underneath an image of a house, from the Apple II game TIME ZONE."></p>

<p>Since Jim Sather’s book was crucial to the completion of this blog post, I think it’s only fair that we end with a quote from <em>Understanding the Apple 2</em>.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/suzy.png" title="what are you talking about" alt="...bus system. This means that a peripheral card can control all hardware features of the Apple. It is as if you could plug a Suzy brain into Johnny and have the Suzy brain control Johnny's body, a concept much in vogue in some circles."></p>

<p>Jim Sather is in way cooler circles than me.</p>

  </div>

  
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New ways to catch gravitational waves (206 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-024-02003-6</link>
            <guid>40820063</guid>
            <pubDate>Fri, 28 Jun 2024 12:41:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-024-02003-6">https://www.nature.com/articles/d41586-024-02003-6</a>, See on <a href="https://news.ycombinator.com/item?id=40820063">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>In September 2015, a vibration lasting just one-fifth of a second changed the history of physics. It was the <a href="https://www.nature.com/articles/530261a" data-track="click" data-label="https://www.nature.com/articles/530261a" data-track-category="body text link">first direct detection of gravitational waves</a> — perturbations in the geometry of space-time that move across the Universe at the speed of light.</p><p>Astronomers say it was like gaining a new sense — as if, until 2015, they had only been able to ‘see’ cosmic events, and now could ‘hear’ them, too. Since then, it has become almost a matter of daily routine to record the passage of gravitational waves at the two massive facilities of the Laser Interferometer Gravitational-wave Observatory (LIGO) in Louisiana and Washington state, along with their sibling Virgo observatory near Pisa, Italy.</p><p>The detection of gravitational waves has provided <a href="https://www.nature.com/articles/nature.2016.19337" data-track="click" data-label="https://www.nature.com/articles/nature.2016.19337" data-track-category="body text link">new ways to explore the laws of nature and the history of the Universe</a>, including clues about the life story of black holes and the large stars they originated from. For many physicists, the birth of gravitational-wave science was a rare bright spot in the past decade, says Chiara Caprini, a theoretical physicist at the University of Geneva in Switzerland. Other promising fields of exploration have disappointed: <a href="https://www.nature.com/articles/d41586-020-02741-3" data-track="click" data-label="https://www.nature.com/articles/d41586-020-02741-3" data-track-category="body text link">dark-matter searches</a> have kept coming up empty handed; the Large Hadron Collider near Geneva has found nothing beyond the Higgs boson; and even some promising hints of <a href="https://www.nature.com/articles/d41586-023-02532-6" data-track="click" data-label="https://www.nature.com/articles/d41586-023-02532-6" data-track-category="body text link">new physics</a> seem to be fading. “In this rather flat landscape, the arrival of gravitational waves was a breath of fresh air,” says Caprini.</p><p>That rare bright spot looks set to become brighter.</p><p>All of the more than 100 gravitational-wave events spotted so far have been just a tiny sample of what physicists think is out there. The window opened by LIGO and Virgo was rather narrow, limited mostly to frequencies in the range 100–1,000 hertz. As pairs of heavy stars or black holes slowly spiral towards each other, over millions of years, they produce gravitational waves of slowly increasing frequency, until, in the final moments before the objects collide, the waves ripple into this detectable range. But this is only one of <a href="https://www.nature.com/articles/nature.2016.19337" data-track="click" data-label="https://www.nature.com/articles/nature.2016.19337" data-track-category="body text link">many kinds of phenomenon</a> that are expected to produce gravitational waves.</p><p>LIGO and Virgo are laser interferometers: they work by detecting small differences in travel time for lasers fired along perpendicular arms, each a few kilometres long. The arms expand and contract by minuscule amounts as gravitational waves wash over them. Researchers are now working on several next-generation LIGO-type observatories, both on Earth and, in space, the Laser Interferometer Space Antenna; some have even proposed building one on the Moon<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup>. Some of these could be sensitive to gravitational waves at frequencies as low as 1 Hz.</p><p>But physicists are also exploring entirely different techniques to detect gravitational waves. These strategies, which range from watching pulsars to measuring quantum fluctuations, hope to catch a much wider variety of gravitational waves, with frequencies in the megahertz to nanohertz range (see ‘Opening the window on gravitational waves’).</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27257006.png?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27257006.png?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Opening the window on gravitational waves: graphic that shows a range of new detectors, and the range of frequencies from different sources that they will be able to detect." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27257006.png">
  <figcaption>
   
  </figcaption>
 </picture>
</figure><p>By broadening their observational window, astronomers should be able to watch black holes circling each other for days, weeks or even years, rather than just catching the last few seconds before collision. And they’ll be able to spot waves made by totally different cosmic phenomena — including mega black holes and even the start of the Universe itself. All this, they say, will crack open many remaining secrets of the cosmos.</p><h2>Pulsar timing array: catching waves that last a decade</h2><p>Last year, one viable alternative to interferometers entered the game.</p><p>Since the early 2000s, radio astronomers have been attempting to use the entire Galaxy as a gravitational-wave detector. The trick is to monitor dozens of neutron stars called pulsars. These spin on their axis hundreds of times per second while emitting a radio-frequency beam, producing what looks like a pulse of light on each turn.</p><p>Gravitational waves sweeping the Galaxy would change the distance between Earth and each pulsar, creating anomalies in detected pulsar frequencies from one year to the next. Observations of a collection or array of pulsars — called a pulsar timing array (PTA) — should be able to detect changes induced by gravitational waves with frequencies of just nanohertz, as might be produced by pairs of supermassive black holes, for example. It takes tens of years for successive crests of such waves to pass a given vantage point, meaning that tens of years of observations are needed to spot them.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-023-02203-6" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_25559284.jpg"><p>Giant gravitational waves: why scientists are so excited</p></a>
 </article><p>In 2023, the PTA technique <a href="https://www.nature.com/articles/d41586-023-02203-6" data-track="click" data-label="https://www.nature.com/articles/d41586-023-02203-6" data-track-category="body text link">began to bear fruit</a>. Four separate collaborations, in North America, Europe, Australia and China, <a href="https://www.nature.com/articles/d41586-023-02167-7" data-track="click" data-label="https://www.nature.com/articles/d41586-023-02167-7" data-track-category="body text link">unveiled tantalizing hints</a> of a pattern expected from a random ‘stochastic background’ of gravitational waves that make Earth slosh around, probably caused by a cacophony of supermassive black-hole binaries, says astrophysicist Chiara Mingarelli at Yale University in New Haven, Connecticut.</p><p>The teams have not yet used the word ‘discovery’, because the evidence that each collaboration unveiled is not yet rock solid. But three of the groups — all but the Chinese one — are now pooling their data and conducting a joint analysis in the hope of getting to the ‘D’ word. This requires painstaking work, because each group processed its raw data in slightly different ways, and so it could take at least another year to get to publication, says Scott Ransom, an astrophysicist at the US National Radio Astronomy Observatory in Charlottesville, Virginia, and a senior member of the North American collaboration.</p><p>“In our current data, we almost certainly have the hints of individual supermassive black-hole binaries out there,” says Ransom. With each extra year of observation, they should get closer to resolving single black-hole pairs out of the cacophony, he adds. “Things are just going to get better and better.”</p><h2>Microwave telescopes: spotting waves from the Big Bang</h2><p>A year before LIGO’s 2015 detection, a team of cosmologists using a South Pole telescope called BICEP2 claimed to have spotted gravitational waves — not directly, but in the pattern of light called the cosmic microwave background (CMB), sometimes described as the afterglow of the Big Bang.</p><p>The <a href="https://www.nature.com/articles/nature.2014.15440" data-track="click" data-label="https://www.nature.com/articles/nature.2014.15440" data-track-category="body text link">BICEP2 claim turned out to be premature</a>, but cosmologists are now doubling down on this idea. An array of telescopes much more powerful than BICEP2, called the <a href="https://www.nature.com/articles/d41586-024-00333-z" data-track="click" data-label="https://www.nature.com/articles/d41586-024-00333-z" data-track-category="body text link">Simons Observatory</a>, is being set up on a mountaintop in northern Chile’s Atacama Desert. Some researchers are holding out hope for an even more powerful array called CMB-S4 (originally proposed to include 12 telescopes in Chile and at the South Pole) — although in May, plans for that project were put on hold because of the disrepair of the US South Pole base.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/nature.2016.19337" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27257160.jpg"><p>Gravitational waves: 6 cosmic questions they can tackle</p></a>
 </article><p>What cosmologists are looking for in the CMB is a specific ‘B mode’ pattern in the swirls of its polarization — the preferential directions in which the microwaves wiggle — that would have been imprinted by the passage of gravitational waves. The theory is that such waves should have been produced by inflation, a quick burst of exponential cosmic expansion thought to have happened around the time of the Big Bang<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup>. Inflation would explain many of the Universe’s most striking properties, such as its flatness and how mass is distributed. The gravitational waves that inflation produced would have started at high frequencies, but would by now be at incredibly low frequencies of around 10<sup>−14</sup> Hz.</p><p>Although inflation is a cornerstone of accepted cosmological theory, there’s no proof of it yet. The B-mode pattern would be the smoking gun and, moreover, would reveal the energy scales involved, which would be a first step towards understanding what powered inflation.</p><p>The problem is, no one knows whether that energy scale was large enough to have left a noticeable mark. “Inflation predicts the B modes, but we don’t know if it’s big enough to be detected,” says Marc Kamionkowski, a theoretical astrophysicist at Johns Hopkins University in Baltimore, Maryland. But if the leading models are right, either the Simons Observatory or CMB-S4 should eventually find it, he says.</p><h2>Atom interferometry: closing the gap</h2><p>Although many of these projects push gravitational-wave science towards lower frequencies, they leave a crucial gap just below 1 Hz.</p><p>Detecting such frequencies could reveal mergers of black holes much more massive than those seen by LIGO (which spots waves from collapsing stars that weigh at most a few tens of solar masses). “This is an unexplored region, but it could be populated with lots of black holes,” says Caprini.</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27252636.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27252636.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Physicists Jason Hogan and Mark Kasevich pictured next to equipment they are developing for measuring gravitational waves." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27252636.jpg">
  <figcaption>
   <p><span>Jason Hogan (left) and Mark Kasevich work on an atom interferometer — a device that could reveal mergers of black holes much more massive than those seen by current laser interferometers.</span><span>Credit: L.A. Cicero and Stanford University</span></p>
  </figcaption>
 </picture>
</figure><p>A nascent technique could come to the rescue, according to physicist Oliver Buchmüller at Imperial College London. “Atom interferometry sits in that gap which we currently cannot explore with any other technology,” he says. An atom interferometer is a vertical high-vacuum pipe in which atoms can be released and allowed to fall under gravity. As they do so, physicists tickle the atoms with laser light to toggle them between an excited and a relaxed state — the same principle used by atomic clocks. “We’re trying to push this atomic-clock technique to what’s ultimately possible,” says Jason Hogan, a physicist at Stanford University in California.</p><p>To detect gravitational waves, physicists plan to drop two or more sets of atoms at different heights inside the same vertical pipe, and to measure the time it takes for a laser pulse to travel from one set of atoms to the next, says Hogan. The passage of gravitational waves would result in light spending either slightly less or slightly more time travelling between them — a variation smaller than one part in 100 billion billion.</p><p>Pioneering experiments at Stanford University have developed atom interferometers with 10-metre drops, but detecting gravitational waves would require devices at least 1 kilometre in height, which could be installed in a mine shaft, say, or even in space. As a first step, several groups around the world are planning to build 100-m atom interferometers as test beds. One such facility, called MAGIS-100, is already under construction in an existing shaft at the Fermi National Accelerator Laboratory outside Chicago, Illinois, and is scheduled for completion in 2027.</p><h2>Desktop detectors: pushing the frequency up</h2><p>Other researchers are exploring ways of detecting gravitational waves with much, much smaller (and cheaper) detectors — including some that could fit on a desktop. These are designed to watch for extremely high-frequency gravity waves. Known phenomena probably don’t produce such waves, but some speculative theories do predict them.</p><p>The Levitated Sensor Detector (LSD) at Northwestern University in Evanston, Illinois looks like a toy LIGO: it bounces lasers between pairs of mirrors just 1 metre apart. The LSD is a prototype for a new type of instrument designed to sense gravitational waves using resonance: the same principle by which even little pushes can make a child on a swing go higher and higher if they are timed just right<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup>.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d43978-024-00069-4" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27257164.jpg"><p>Will the Einstein Telescope be split in two?</p></a>
 </article><p>In a vacuum inside each of the LSD’s arms, laser light suspends a particle just micrometres wide. As with an interferometer, the passage of gravitational waves will alternately elongate and compress the length of each arm. If the frequency of the gravitational waves resonates with that of the device, the lasers will then give many tiny kicks to the particle. The LSD can track the particle’s motion with a precision of femtometres, says Northwestern physicist Andrew Geraci, who is leading the project.</p><p>The LSD is designed to be sensitive to gravitational waves with frequencies of around 100 kHz. This prototype might already have a shot at detecting some, if the team can keep experimental noise in check — and provided that such waves exist. “Depending how optimistic you are, we may be able to measure a real signal in that band even with a 1-m instrument,” Geraci says. Future versions could be scaled up to 100-m-long arms, he adds, which would increase their sensitivity.</p><p>Theoretical physicist Ivette Fuentes at the University of Southampton, UK, has an idea for making an even smaller resonant detector. She aims to exploit sound waves in an exotic state of matter called a Bose–Einstein condensate (BEC) — a cloud of atoms kept at temperatures as low as a few millionths of a degree above absolute zero. If a gravitational wave passes through at a frequency that resonates with the sound wave, it can be detected. Because the act of looking for this signal destroys the BEC, a new flood of atoms needs to be released every second. The process might need to be repeated for months for a successful detection, Fuentes says.</p><p>In principle, a BEC-based detector could expand the search for gravitational waves to extremely high frequencies of 1 MHz or more — again, provided they exist. Fuentes says her scheme would require pushing BEC techniques just a little beyond the current state of the art. “I think the idea is very bold,” she says. Physicists have posited that high-frequency gravitational waves could reveal exotic physics that went on in the first second or so after the Big Bang. “We could use it to study the state of the Universe at very high energies,” says Caprini.</p><h2>Quantum crystal: only takes a second</h2><p>A final, more radical proposal for detecting gravitational waves involves putting objects in two places at once.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-024-00333-z" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27257162.jpg"><p>‘Best view ever’: observatory will map Big Bang’s afterglow in new detail</p></a>
 </article><p>Sougato Bose, a physicist at University College London, has proposed a device in which a micrometre-sized diamond crystal is put in a superposition of two quantum states. In his scheme, the crystal’s two ‘personas’ would be pushed apart by as much as 1 metre and then brought together again — an extremely delicate procedure that has been compared to putting the nursery-rhyme character Humpty Dumpty back together after a fall. The passage of gravitational waves would make one persona travel further than the other when apart, putting them out of sync — in a measurable way — when reunited. The whole process would take around one second to complete, which would make the device sensitive to gravitational waves of around 1 Hz.</p><p>This idea is extremely ambitious: such quantum tricks have so far been shown to work only for objects the size of molecules, and no one has ever tested whether quantum weirdness can be pushed to such extremes. “Putting Humpty Dumpty back together has never been demonstrated for crystals,” says Bose.</p><p>But if the technique can be perfected, then table-top experiments such as this one could take gravitational-wave detection out of the hands of just a few large-scale labs. Together, these techniques could blow open the window on what can be seen. “The outlook is very positive,” says Caprini.</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is Clear Air Turbulence becoming more common? (225 pts)]]></title>
            <link>https://www.flightradar24.com/blog/is-cat-more-common/</link>
            <guid>40819784</guid>
            <pubDate>Fri, 28 Jun 2024 12:04:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.flightradar24.com/blog/is-cat-more-common/">https://www.flightradar24.com/blog/is-cat-more-common/</a>, See on <a href="https://news.ycombinator.com/item?id=40819784">Hacker News</a></p>
Couldn't get https://www.flightradar24.com/blog/is-cat-more-common/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[A Novel way to curb poaching, injecting radioisotopes into 20 live rhinoceros (127 pts)]]></title>
            <link>https://www.wits.ac.za/news/latest-news/general-news/2024/2024-06/a-novel-way-to-save-rhinos-.html</link>
            <guid>40819617</guid>
            <pubDate>Fri, 28 Jun 2024 11:35:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wits.ac.za/news/latest-news/general-news/2024/2024-06/a-novel-way-to-save-rhinos-.html">https://www.wits.ac.za/news/latest-news/general-news/2024/2024-06/a-novel-way-to-save-rhinos-.html</a>, See on <a href="https://news.ycombinator.com/item?id=40819617">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
          <article data-contentid="3353674">
  <span id="d.en.3353674"></span>
  
  <p>
    <time datetime="2024-06-25 08:54">25 June 2024</time> - Wits University 
  </p>
  <p>The Rhisotope Project at Wits is entering a new testing phase with the insertion of radioisotopes into 20 live rhinoceros.</p>
  <p>After three years of meticulous and dedicated hard work, the Rhisotope Project at Wits University has successfully inserted low doses of radioisotopes into 20 live rhinoceros.</p>
<p>In this final phase of the research project, Professor James Larkin from the University of the Witwatersrand’s Radiation and Health Physics Unit (RHPU) in collaboration with a team of experts who are leaders in the world of rhino conservation and veterinary work, will closely monitor the health and vital statistics of the rhinos over a period of six months, in order to determine the viability of this approach. .</p>
<p>The Rhisotope Project’s intention is to use nuclear technology in the form of small, measured quantities of radioisotopes and to insert these into the horns of rhinoceros, which can be picked up by radiation detection portal monitors at international borders, including at harbours, airports and land-crossings. These radioisotopes will provide an affordable, safe and easily applicable method to create long-lasting and detectable horn markers that cause no harm to the animals and environment. At a later stage, the work will expand to elephants, pangolins and other fauna and flora.</p>
<p>Being pioneered in the UNESCO Waterberg Biosphere Reserve, the Project aims to benefit from existing, sophisticated multi-billion-dollar nuclear security infrastructure that already exists throughout the world. Over 11 000 radiation detection portal monitors are installed at airports, harbours and other ports of entry, including thousands of trained personnel equipped with radiation detectors, all of which can detect the smallest radioactive particles. In contrast to this, the infrastructure and number of trained officials to detect wildlife trafficking at ports of entry internationally is extremely limited.</p>
  <section>
  <article>
    <iframe width="100%" height="315" src="//www.youtube.com/embed/Wy3hqdexWy0" frameborder="0" allowfullscreen=""></iframe>
  </article>
</section>
  
  <p>“Every 20 hours in South Africa a rhino dies for its horn. These poached horns are then trafficked across the world and used for traditional medicines, or as status symbols. This has led to their horns currently being the most valuable false commodity in the black-market trade, with a higher value even than gold, platinum, diamonds and cocaine. Sadly, rhino horns play a large role in funding a wide variety of criminal activities globally,” says Professor James Larkin. “Ultimately, the aim is to try to devalue rhinoceros horn in the eyes of the end users, while at the same time making the horns easier to detect as they are being smuggled across borders.”</p>
<p>Starting on Monday, 24 June 2004, Professor Larkin and his team carefully sedated the 20 rhinos &nbsp;and drilled a small hole into each of their horns to insert the non-toxic radioisotopes. The rhinos were then released under the care of a highly qualified crew that will monitor the animals on a 24-hour basis for the next six months. “Each insertion was closely monitored by expert veterinarians and extreme care was taken to prevent any harm to the animals,” says Larkin. “Over months of research and testing we have also ensured that the inserted radioisotopes hold no health or any other risk for the animals or those who care for them.”&nbsp;&nbsp;&nbsp;&nbsp;</p>
<p>The development and application of the Rhisotope Project nuclear technology has the capacity to help deter poaching, increase the detection capabilities of smuggled horns, increase prosecution success, reveal smuggling routes and deter end-user markets.</p>
<p>Rhino poaching reached crisis levels since 2008 where close to 10 000 rhinos were lost to poaching in South Africa, with wildlife trafficking being the third biggest organised crime globally.</p>
<p>Professor Lynn Morris, the Deputy Vice-Chancellor: Research and Innovation at Wits University says: “This is an example of how cross-disciplinary research and innovation makes a real difference. This novel approach pioneered by Prof Larkin and his colleagues has the potential to eradicate the threat of extinction our unique wild-life species&nbsp;&nbsp; , especially in South Africa and on the continent. This is one of many projects at Wits that demonstrates research with impact, and which helps to address some of the local and global challenges of the 21<sup>st</sup> Century.”</p>
<p>The Rhisotope Project at Wits was set up by a small team of likeminded individuals as a South African-based conservation initiative in January 2021 with the intention of becoming a global leader in harnessing nuclear technology to protect threatened and endangered species of fauna and flora as well as communities of people.</p>
<p>Aside from developing a solution to combat the illicit trade and trafficking of wildlife products, the Rhisotope Project seeks to provide education and social upliftment to empower people and local communities. A special focus is aimed at uplifting the girls and women of rural communities, who are often the backbone of these communities in the remote areas where endangered species are found and are the greatest components of success in changing the hearts and minds of local communities thereby creating rhino ambassadors and champions.</p>
</article>
<h5>Share</h5>


<span displaytext="Facebook"></span>
<span displaytext="LinkedIn"></span>

</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta LLM Compiler: neural optimizer and disassembler (188 pts)]]></title>
            <link>https://twitter.com/AIatMeta/status/1806361623831171318/photo/1</link>
            <guid>40819479</guid>
            <pubDate>Fri, 28 Jun 2024 11:12:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/AIatMeta/status/1806361623831171318/photo/1">https://twitter.com/AIatMeta/status/1806361623831171318/photo/1</a>, See on <a href="https://news.ycombinator.com/item?id=40819479">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Understanding React Compiler (106 pts)]]></title>
            <link>https://tonyalicea.dev/blog/understanding-react-compiler/</link>
            <guid>40819440</guid>
            <pubDate>Fri, 28 Jun 2024 11:05:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tonyalicea.dev/blog/understanding-react-compiler/">https://tonyalicea.dev/blog/understanding-react-compiler/</a>, See on <a href="https://news.ycombinator.com/item?id=40819440">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  
  
<p>React's core architecture calls the functions you give it (i.e. your components) over and over. This fact both contributed to its popularity by simplifying its mental model, and created a point of possible performance issues. In general, if your functions do expensive things, then your app will be slow.</p>
<p>Performance tuning, therefore, became a pain point for devs, as they had to manually tell React which functions should be re-run and when. The React team has now provided a tool called the React Compiler to automate that manual work performance tuning for devs, by rewriting their code.</p>
<p>What does React Compiler do to your code? How does it work under-the-hood? Should you use it? Let's dive in.</p>
<p><small>To gain a complete, accurate mental model of React by deep diving into its internals, check out my new course <b><a href="https://understandingreact.com/">Understanding React</a></b> where we dig into React's source code. I've found a deep understanding of React's internals greatly helps even devs with years of React experience.</small></p>
<h2>Compilers, Transpiler, and Optimizers</h2>
<p>We hear the terms compiler, transpiler, and optimizer thrown about the modern JavaScript ecosystem. What are they?</p>
<h3>Transpilation</h3>
<p>A transpiler is a program that analyzes your code and outputs functionally equivalent code in a different programming language, or an adjusted version of your code in the same programming language.</p>
<p>React devs have been using a transpiler for years to convert JSX to the code that is actually run by the JavaScript engine. JSX is essentially shorthand for building trees of nested function calls (which then build trees of nested objects).</p>
<p>Writing nested function calls is cumbersome and error-prone, so JSX makes the developer's life easier, and a transpiler is needed to analyze the JSX and convert it into those function calls.</p>
<p>For example, if you wrote the following React code using JSX:</p>
<p><small>Note that, for ease of reading, all code in this blog post is intentionally oversimplified.</small></p>
<pre><code><span>function</span> <span>App</span><span>(</span><span>)</span> <span>{</span><br>    <span>return</span> <span>&lt;</span>Item item<span>=</span><span>{</span>item<span>}</span> <span>/</span><span>&gt;</span><span>;</span><br><span>}</span><p><span>function</span> <span>Item</span><span>(</span><span><span>{</span> item <span>}</span></span><span>)</span> <span>{</span><br>    <span>return</span> <span>(</span><br>        <span>&lt;</span>ul<span>&gt;</span><br>            <span>&lt;</span>li<span>&gt;</span><span>{</span> item<span>.</span>desc <span>}</span><span>&lt;</span><span>/</span>li<span>&gt;</span><br>        <span>&lt;</span><span>/</span>ul<span>&gt;</span><br>    <span>)</span><br><span>}</span></p></code></pre>
<p>it becomes, after transpilation:</p>
<pre><code><span>function</span> <span>App</span><span>(</span><span>)</span> <span>{</span><br>  <span>return</span> <span>_jsx</span><span>(</span>Item<span>,</span> <span>{</span><br>    <span>item</span><span>:</span> item<br>  <span>}</span><span>)</span><span>;</span><br><span>}</span><p><span>function</span> <span>Item</span><span>(</span><span><span>{</span> item <span>}</span></span><span>)</span> <span>{</span><br>  <span>return</span> <span>_jsx</span><span>(</span><span>"ul"</span><span>,</span> <span>{</span><br>    <span>children</span><span>:</span> <span>_jsx</span><span>(</span><span>"li"</span><span>,</span> <span>{</span><br>      <span>children</span><span>:</span> item<span>.</span>desc<br>    <span>}</span><span>)</span><br>  <span>}</span><span>)</span><span>;</span><br><span>}</span></p></code></pre>
<p>This is the code that is actually sent to the browser. Not HTML-like syntax, but nested function calls passing plain JavaScript objects that React calls 'props'.</p>
<p><small>The result of transpilation shows why you can't use if-statements easily inside JSX. You can't use if-statements inside function calls.</small></p>
<p>You can quickly generate and examine the output of transpiled JSX using <a href="https://babeljs.io/repl">Babel</a>.</p>
<h3>Compilation and Optimization</h3>
<p>So what's the difference between a transpiler and a compiler? It depends on who you ask, and what their education and experience is. If you come from a computer science background you might have mostly been exposed to compilers as a program that converts the code you write into machine language (the binary code that a processor actually understands).</p>
<p>However, "transpilers" are also called "source-to-source compilers". Optimizers are also called "optimizing compilers". Transpilers and optimizers are types of compilers!</p>
<p>Naming things is hard, so there will be disagreement about what constitutes a transpiler, compiler, or optimizer. The important thing to understand is that transpilers, compilers, and optimizers are programs that take a text file containing your code, analyze it, and produce a new text file of different but functionally equivalent code. They may make your code better, or add abilities that it didn't have before by wrapping bits of your code in calls to other people's code.</p>
<blockquote>Compilers, transpilers, and optimizers are programs that take a text file containing your code, analyze it, and produce different but functionally equivalent code.</blockquote>
<p>That last part is what React Compiler does. It creates code functionally equivalent to what you wrote, but wraps bits of it in calls to code the React folks wrote. In that way, your code is rewritten into something that does what you intended, plus more. We'll see exactly what the "more" is in a bit.</p>
<h3>Abstract Syntax Trees</h3>
<p>When we say your code is "analyzed", we mean the text of your code is parsed character-by-character and algorithms are run against it to figure out how to adjust it, rewrite it, add features to it, etc. The parsing usually results in an abstract syntax tree (or AST).</p>
<p>While that sounds fancy, it really is just a tree of data that represents your code. It is then easier to analyze the tree, rather than the code you wrote.</p>
<p>For example, let's suppose you have a line in your code that looks like this:</p>
<pre><code><span>const</span> item <span>=</span> <span>{</span> <span>id</span><span>:</span> <span>0</span><span>,</span> <span>desc</span><span>:</span> <span>'Hi'</span> <span>}</span><span>;</span></code></pre>
<p>the abstract syntax tree for that line of code might end up looking something like this:</p>
<pre><code><span>{</span><br>    <span>type</span><span>:</span> VariableDeclarator<span>,</span><br>    <span>id</span><span>:</span> <span>{</span><br>        <span>type</span><span>:</span> Identifier<span>,</span><br>        <span>name</span><span>:</span> Item<br>    <span>}</span><span>,</span><br>    <span>init</span><span>:</span> <span>{</span><br>        <span>type</span><span>:</span> ObjectExpression<span>,</span><br>        <span>properties</span><span>:</span> <span>[</span><br>            <span>{</span><br>                <span>type</span><span>:</span> ObjectProperty<span>,</span><br>                <span>key</span><span>:</span> id<span>,</span><br>                <span>value</span><span>:</span> <span>0</span><br>            <span>}</span><span>,</span><br>            <span>{</span><br>                <span>type</span><span>:</span> ObjectProperty<span>,</span><br>                <span>key</span><span>:</span> desc<span>,</span><br>                <span>value</span><span>:</span> <span>'Hi'</span><br>            <span>}</span><br>        <span>]</span><br>    <span>}</span><br><span>}</span></code></pre>
<p>The generated data structure describes your code as you wrote it, breaking it down into small defined pieces containing both what type of thing the piece is and any values associated with it. For example <code>desc: 'Hi'</code> is an <code>ObjectProperty</code> with a <code>key</code> called 'desc' and a <code>value</code> of 'Hi'.</p>
<p>This is the mental model you should have when you imagine what is happening to your code in a transpiler/compiler/etc. People wrote a program that takes your code (the text itself), converts it into a data structure, and performs analysis and work on it.</p>
<p>The code that is generated ultimately comes from this AST as well as perhaps some other intermediate languages. You can imagine looping over this data structure and outputting text (new code in the same language or a different one, or adjusting it in some way).</p>
<p>In the case of React Compiler it utilizes both an AST and an intermediate language to generate new React code from the code you write. It's important to remember that React Compiler, like React itself, is just <em>other people's code</em>.</p>
<p>When it comes to compilers, transpilers, optimizers, and the like, don't think of these tools as mysterious black boxes. Think of them as things that you could build, if you had the time.</p>
<h2>React's Core Architecture</h2>
<p>Before we move on to React Compiler itself, there's a few more concepts we need to have clear.</p>
<p>Remember that we said React's core architecture is both a source of its popularity, but also a potential performance issues? We saw that when you write JSX, you're actually writing nested function calls. But you are giving your functions to React, and it will decide when to call them.</p>
<p>Let's take the beginnings of a React app for dealing with a large list of items. Let's suppose our <code>App</code> function gets some items, and our <code>List</code> function processes and shows them.</p>
<pre><code><span>function</span> <span>App</span><span>(</span><span>)</span> <span>{</span><br>    <span>// TODO: fetch some items here</span><br>    <span>return</span> <span>&lt;</span>List items<span>=</span><span>{</span>items<span>}</span> <span>/</span><span>&gt;</span><span>;</span><br><span>}</span><p><span>function</span> <span>List</span><span>(</span><span><span>{</span> items <span>}</span></span><span>)</span> <span>{</span><br>    <span>const</span> pItems <span>=</span> <span>processItems</span><span>(</span>items<span>)</span><span>;</span><br>    <span>const</span> listItems <span>=</span> pItems<span>.</span><span>map</span><span>(</span><span>(</span><span>item</span><span>)</span> <span>=&gt;</span> <span>&lt;</span>li<span>&gt;</span><span>{</span> item <span>}</span><span>&lt;</span><span>/</span>li<span>&gt;</span><span>)</span><span>;</span><br>    <span>return</span> <span>(</span><br>        <span>&lt;</span>ul<span>&gt;</span><span>{</span> listItems <span>}</span><span>&lt;</span><span>/</span>ul<span>&gt;</span><br>    <span>)</span><br><span>}</span></p></code></pre>
<p>Our functions return plain JavaScript objects, like a <code>ul</code> object which contains its children (which here will end up being multiple <code>li</code> objects). Some of these objects like <code>ul</code> and <code>li</code> are built-in to React. Others are the ones we create, like <code>List</code>.</p>
<p>Ultimately, React will build a tree from all these objects called the Fiber tree. Each node in the tree is called a Fiber or Fiber node. The idea of creating our own JavaScript object tree of nodes to describe a UI is called creating a "Virtual DOM".</p>
<p><img src="https://tonyalicea.dev/assets/blogimages/ReactCompiler_FiberTree.png" alt="React Fiber Tree"></p>
<p>React actually keeps two branches that can fork out from each node of the tree. One branch is called is of the "current" state of that branch of the tree (which matches the DOM), and the other the "work-in-progress" state of that branch of the tree which matches the tree created from what our functions returned when they were re-run.</p>
<p><img src="https://tonyalicea.dev/assets/blogimages/ReactCompiler_Reconciliation.png" alt="Reconciliation"></p>
<p>React will then compare those two trees to decide what changes need to made to the actual DOM, so that the DOM matches the work-in-progress side of the tree. This process is called "reconciliation".</p>
<p>Thus, depending on what other functionality we add to our app, React may choose to call our <code>List</code> function over and over, whenever it thinks the UI might need to be updated. This makes our mental model fairly straightforward. Whenever the UI might need to be updated (for example, in response to a user action like clicking a button), the functions that define the UI will be called again, and React will figure out how to update the actual DOM in the browser to match how our functions say the UI should look.</p>
<p>But if the <code>processItems</code> function is slow, then every call to <code>List</code> will be slow, and our whole app will be slow as we interact with it!</p>
<h2>Memoization</h2>
<p>A solution in programming to deal with repeated calls to expensive functions is to cache the results of the function. This process is called memoization.</p>
<p>For memoization to work, the function must be "pure". That means that if you pass the same inputs to the function, you <em>always</em> get the same output. If that's true, then you can take the output and store it in a way that it's related to the set of inputs.</p>
<p>The next time you call the expensive function, we can write code to look at the inputs, check the cache to see if we've already run the function with those inputs, and if we have, then grab the stored output from cache rather than calling the function again. No need to call the function again since we know the output will be the same as the last time those inputs were used.</p>
<p>If the <code>processItems</code> function from the previous used implemented memoization it might look something like:</p>
<pre><code><span>function</span> <span>processItems</span><span>(</span><span>items</span><span>)</span> <span>{</span><br>    <span>const</span> memOutput <span>=</span> <span>getItemsOutput</span><span>(</span>items<span>)</span><span>;</span><br>    <span>if</span> <span>(</span>memOutput<span>)</span> <span>{</span><br>        <span>return</span> memOutput<span>;</span><br>    <span>}</span> <span>else</span> <span>{</span><br>        <span>// ...run expensive processing</span><br>        <span>saveItemsOutput</span><span>(</span>items<span>,</span> output<span>)</span><span>;</span><br>        <span>return</span> output<span>;</span><br>    <span>}</span><br><span>}</span></code></pre>
<p>We can imagine that the <code>saveItemsOutput</code> function stores an object that saves both items and the associated output from the function. The <code>getItemsOutput</code> will look to see if <code>items</code> is already stored, and if it is we return the related cached output without doing any more work.</p>
<p>For React's architecture of calling functions over and over, memoization becomes a vital technique for helping to keep apps from becoming slow.</p>
<h2>Hook Storage</h2>
<p>There's one more piece of React's architecture to understand in order to understand React Compiler.</p>
<p>React will look at calling your functions again if the "state" of the app changes, meaning the data that the creation of the UI is dependent on. For example a piece of data might be "showButton" which is true or false, and the UI should show or hide the button based on the value of that data.</p>
<p>React stores state on the client's device. How? Let's take the React app that will render and interact with a list of items. Suppose we will eventually store a selected item, process the items client side for rendering, handle events, and sort the list. Our app might start to look something like below.</p>
<pre><code><span>function</span> <span>App</span><span>(</span><span>)</span> <span>{</span><br>    <span>// TODO: fetch some items here</span><br>    <span>return</span> <span>&lt;</span>List items<span>=</span><span>{</span>items<span>}</span> <span>/</span><span>&gt;</span><span>;</span><br><span>}</span><p><span>function</span> <span>List</span><span>(</span><span><span>{</span> items <span>}</span></span><span>)</span> <span>{</span><br>    <span>const</span> <span>[</span>selItem<span>,</span> setSelItem<span>]</span> <span>=</span> <span>useState</span><span>(</span><span>null</span><span>)</span><span>;</span><br>    <span>const</span> <span>[</span>itemEvent<span>,</span> dispatcher<span>]</span> <span>=</span> <span>useReducer</span><span>(</span>reducer<span>,</span> <span>{</span><span>}</span><span>)</span><span>;</span><br>    <span>const</span> <span>[</span>sort<span>,</span> setSort<span>]</span> <span>=</span> <span>useState</span><span>(</span><span>0</span><span>)</span><span>;</span></p><p>    <span>const</span> pItems <span>=</span> <span>processItems</span><span>(</span>items<span>)</span><span>;</span><br>    <span>const</span> listItems <span>=</span> pItems<span>.</span><span>map</span><span>(</span><span>(</span><span>item</span><span>)</span> <span>=&gt;</span> <span>&lt;</span>li<span>&gt;</span><span>{</span> item <span>}</span><span>&lt;</span><span>/</span>li<span>&gt;</span><span>)</span><span>;</span><br>    <span>return</span> <span>(</span><br>        <span>&lt;</span>ul<span>&gt;</span><span>{</span> listItems <span>}</span><span>&lt;</span><span>/</span>ul<span>&gt;</span><br>    <span>)</span><br><span>}</span></p></code></pre>
<p>What is really happening here when the <code>useState</code> and <code>useReducer</code> lines are executed by the JavaScript engine? The node of the Fiber tree created from our <code>List</code> component has some more JavaScript objects attached to it to store our data. Each of those objects is connected to each other in a data structure called a linked list.</p>
<p><small>By the way, a lot of devs think <code>useState</code> is the core unit of state management in React. But it isn't! It's actually a wrapper for a simple call to <code>useReducer</code>.</small></p>

<p>So, when you call <code>useState</code> and <code>useReducer</code>, React will attach the state to the Fiber tree that sits around while our app runs. Thus state remains available as our functions keep re-running.</p>
<p><small>How hooks are stored also explains the "rule of hooks" that you can't call a hook inside a loop or an if-statement. Every time you call a hook, React moves to the next item in the linked list. Thus, the number of times you call hooks must be consistent, or React would sometimes be pointing at the wrong item in the linked list.</small></p>
<p>Ultimately, hooks are just objects designed to hold data (and functions) in the user's device memory. This is key to understanding what React Compiler really does. But there's more.</p>
<h2>Memoization in React</h2>
<p>React combines the idea of memoization and its idea of hook storage. You can memoize the results of entire functions you give React that are part of the Fiber Tree (like <code>List</code>), or individual functions you call within them (like <code>processItems</code>).</p>
<p>Where is the cache stored? On the Fiber tree, just like state! For example the <code>useMemo</code> hook stores the inputs and outputs on the node that calls <code>useMemo</code>.</p>
<p>So, React already has the idea of storing the results of expensive functions in linked lists of JavaScript objects that are part of the Fiber Tree. That's great, except for one thing: maintenance.</p>
<p>Memoization in React can be cumbersome, because you have to explicitly tell React what inputs the memoization depends on. Our call to <code>processItems</code> becomes:</p>
<pre><code><span>const</span> pItems <span>=</span> <span>useMemo</span><span>(</span><span>processItems</span><span>(</span>items<span>)</span><span>,</span> <span>[</span>items<span>]</span><span>)</span><span>;</span></code></pre>
<p>The array at the end being the list of 'dependencies', that is the inputs that, if changed, tell React the function should be called again. You have to make sure you get those inputs right, or memoization won't work properly. It becomes a clerical chore to keep up with.</p>
<h2>React Compiler</h2>
<p>Enter React Compiler. A program that analyzes the text of your React code, and produces new code ready for JSX transpilation. But that new code has some extra things added to it.</p>
<p>Let's look at what React Compiler does to our app in this case. Before compilation it was:</p>
<pre><code><span>function</span> <span>App</span><span>(</span><span>)</span> <span>{</span><br>    <span>// TODO: fetch some items here</span><br>    <span>return</span> <span>&lt;</span>List items<span>=</span><span>{</span>items<span>}</span> <span>/</span><span>&gt;</span><span>;</span><br><span>}</span><p><span>function</span> <span>List</span><span>(</span><span><span>{</span> items <span>}</span></span><span>)</span> <span>{</span><br>    <span>const</span> <span>[</span>selItem<span>,</span> setSelItem<span>]</span> <span>=</span> <span>useState</span><span>(</span><span>null</span><span>)</span><span>;</span><br>    <span>const</span> <span>[</span>itemEvent<span>,</span> dispatcher<span>]</span> <span>=</span> <span>useReducer</span><span>(</span>reducer<span>,</span> <span>{</span><span>}</span><span>)</span><span>;</span><br>    <span>const</span> <span>[</span>sort<span>,</span> setSort<span>]</span> <span>=</span> <span>useState</span><span>(</span><span>0</span><span>)</span><span>;</span></p><p>    <span>const</span> pItems <span>=</span> <span>processItems</span><span>(</span>items<span>)</span><span>;</span><br>    <span>const</span> listItems <span>=</span> pItems<span>.</span><span>map</span><span>(</span><span>(</span><span>item</span><span>)</span> <span>=&gt;</span> <span>&lt;</span>li<span>&gt;</span><span>{</span> item <span>}</span><span>&lt;</span><span>/</span>li<span>&gt;</span><span>)</span><span>;</span><br>    <span>return</span> <span>(</span><br>        <span>&lt;</span>ul<span>&gt;</span><span>{</span> listItems <span>}</span><span>&lt;</span><span>/</span>ul<span>&gt;</span><br>    <span>)</span><br><span>}</span></p></code></pre>
<p>after compilation it becomes:</p>
<pre><code><span>function</span> <span>App</span><span>(</span><span>)</span> <span>{</span><br>  <span>const</span> $ <span>=</span> <span>_c</span><span>(</span><span>1</span><span>)</span><span>;</span><p>  <span>let</span> t0<span>;</span></p><p>  <span>if</span> <span>(</span>$<span>[</span><span>0</span><span>]</span> <span>===</span> Symbol<span>.</span><span>for</span><span>(</span><span>"react.memo_cache_sentinel"</span><span>)</span><span>)</span> <span>{</span><br>    t0 <span>=</span> <span>&lt;</span>List items<span>=</span><span>{</span>items<span>}</span> <span>/</span><span>&gt;</span><span>;</span><br>    $<span>[</span><span>0</span><span>]</span> <span>=</span> t0<span>;</span><br>  <span>}</span> <span>else</span> <span>{</span><br>    t0 <span>=</span> $<span>[</span><span>0</span><span>]</span><span>;</span><br>  <span>}</span></p><p>  <span>return</span> t0<span>;</span><br><span>}</span></p><p><span>function</span> <span>List</span><span>(</span><span>t0</span><span>)</span> <span>{</span><br>  <span>const</span> $ <span>=</span> <span>_c</span><span>(</span><span>6</span><span>)</span><span>;</span></p><p>  <span>const</span> <span>{</span> items <span>}</span> <span>=</span> t0<span>;</span><br>  <span>useState</span><span>(</span><span>null</span><span>)</span><span>;</span><br>  <span>let</span> t1<span>;</span></p><p>  <span>if</span> <span>(</span>$<span>[</span><span>0</span><span>]</span> <span>===</span> Symbol<span>.</span><span>for</span><span>(</span><span>"react.memo_cache_sentinel"</span><span>)</span><span>)</span> <span>{</span><br>    t1 <span>=</span> <span>{</span><span>}</span><span>;</span><br>    $<span>[</span><span>0</span><span>]</span> <span>=</span> t1<span>;</span><br>  <span>}</span> <span>else</span> <span>{</span><br>    t1 <span>=</span> $<span>[</span><span>0</span><span>]</span><span>;</span><br>  <span>}</span></p><p>  <span>useReducer</span><span>(</span>reducer<span>,</span> t1<span>)</span><span>;</span><br>  <span>useState</span><span>(</span><span>0</span><span>)</span><span>;</span><br>  <span>let</span> t2<span>;</span></p><p>  <span>if</span> <span>(</span>$<span>[</span><span>1</span><span>]</span> <span>!==</span> items<span>)</span> <span>{</span><br>    <span>const</span> pItems <span>=</span> <span>processItems</span><span>(</span>items<span>)</span><span>;</span><br>    <span>let</span> t3<span>;</span></p><p>    <span>if</span> <span>(</span>$<span>[</span><span>3</span><span>]</span> <span>===</span> Symbol<span>.</span><span>for</span><span>(</span><span>"react.memo_cache_sentinel"</span><span>)</span><span>)</span> <span>{</span><br>      <span>t3</span> <span>=</span> <span>(</span><span>item</span><span>)</span> <span>=&gt;</span> <span>&lt;</span>li<span>&gt;</span><span>{</span>item<span>}</span><span>&lt;</span><span>/</span>li<span>&gt;</span><span>;</span></p><p>      $<span>[</span><span>3</span><span>]</span> <span>=</span> t3<span>;</span><br>    <span>}</span> <span>else</span> <span>{</span><br>      t3 <span>=</span> $<span>[</span><span>3</span><span>]</span><span>;</span><br>    <span>}</span></p><p>    t2 <span>=</span> pItems<span>.</span><span>map</span><span>(</span>t3<span>)</span><span>;</span><br>    $<span>[</span><span>1</span><span>]</span> <span>=</span> items<span>;</span><br>    $<span>[</span><span>2</span><span>]</span> <span>=</span> t2<span>;</span><br>  <span>}</span> <span>else</span> <span>{</span><br>    t2 <span>=</span> $<span>[</span><span>2</span><span>]</span><span>;</span><br>  <span>}</span></p><p>  <span>const</span> listItems <span>=</span> t2<span>;</span><br>  <span>let</span> t3<span>;</span></p><p>  <span>if</span> <span>(</span>$<span>[</span><span>4</span><span>]</span> <span>!==</span> listItems<span>)</span> <span>{</span><br>    t3 <span>=</span> <span>&lt;</span>ul<span>&gt;</span><span>{</span>listItems<span>}</span><span>&lt;</span><span>/</span>ul<span>&gt;</span><span>;</span><br>    $<span>[</span><span>4</span><span>]</span> <span>=</span> listItems<span>;</span><br>    $<span>[</span><span>5</span><span>]</span> <span>=</span> t3<span>;</span><br>  <span>}</span> <span>else</span> <span>{</span><br>    t3 <span>=</span> $<span>[</span><span>5</span><span>]</span><span>;</span><br>  <span>}</span></p><p>  <span>return</span> t3<span>;</span><br><span>}</span></p></code></pre>
<p>That's a lot! Let's break down a bit of the now rewritten <code>List</code> function to understand it.</p>
<p>It starts off with:</p>
<pre><code><span>const</span> $ <span>=</span> <span>_c</span><span>(</span><span>6</span><span>)</span><span>;</span></code></pre>
<p>That <code>_c</code> function (think "c" for "cache") creates an array that's stored using a hook. React Compiler analyzed our <code>Link</code> function and decided, to maximize performance, we need to store six things. When our function is first called, it stores the results of each of those six things in that array.</p>
<p>It's the subsequent calls to our function where we the cache in action. For example, just looking at the area where we call <code>processItems</code>:</p>
<pre><code><span>if</span> <span>(</span>$<span>[</span><span>1</span><span>]</span> <span>!==</span> items<span>)</span> <span>{</span><br>    <span>const</span> pItems <span>=</span> <span>processItems</span><span>(</span>items<span>)</span><span>;</span><br>    <span>let</span> t3<span>;</span><p>    <span>if</span> <span>(</span>$<span>[</span><span>3</span><span>]</span> <span>===</span> Symbol<span>.</span><span>for</span><span>(</span><span>"react.memo_cache_sentinel"</span><span>)</span><span>)</span> <span>{</span><br>        <span>t3</span> <span>=</span> <span>(</span><span>item</span><span>)</span> <span>=&gt;</span> <span>&lt;</span>li<span>&gt;</span><span>{</span>item<span>}</span><span>&lt;</span><span>/</span>li<span>&gt;</span><span>;</span><br>        $<span>[</span><span>3</span><span>]</span> <span>=</span> t3<span>;</span><br>    <span>}</span> <span>else</span> <span>{</span><br>        t3 <span>=</span> $<span>[</span><span>3</span><span>]</span><span>;</span><br>    <span>}</span></p><p>    t2 <span>=</span> pItems<span>.</span><span>map</span><span>(</span>t3<span>)</span><span>;</span><br>    $<span>[</span><span>1</span><span>]</span> <span>=</span> items<span>;</span><br>    $<span>[</span><span>2</span><span>]</span> <span>=</span> t2<span>;</span><br><span>}</span> <span>else</span> <span>{</span><br>    t2 <span>=</span> $<span>[</span><span>2</span><span>]</span><span>;</span><br><span>}</span></p></code></pre>
<p>The entire work around <code>processItems</code>, both calling the function and generating the <code>li</code>s, is wrapped in a check to see if the cache in the second position of the array (<code>$[1]</code>) is the same input as the last time the function was called (the value of <code>items</code> which is passed to <code>List</code>).</p>
<p>If they are equal, then the third position in the cache array (<code>$[2]</code>) is used. That stores the generated list of all the <code>li</code>s when <code>items</code> is mapped over. React Compiler's code says "if you give me the same list of items as the last time you called this function, I will give you the list of <code>li</code>s that I stored in cache the last time".</p>
<p>If the <code>items</code> passed is different, then it calls <code>processItems</code>. Even then, it uses the cache to store what <em>one</em> list item looks like.</p>
<pre><code><span>if</span> <span>(</span>$<span>[</span><span>3</span><span>]</span> <span>===</span> Symbol<span>.</span><span>for</span><span>(</span><span>"react.memo_cache_sentinel"</span><span>)</span><span>)</span> <span>{</span><br>    <span>t3</span> <span>=</span> <span>(</span><span>item</span><span>)</span> <span>=&gt;</span> <span>&lt;</span>li<span>&gt;</span><span>{</span>item<span>}</span><span>&lt;</span><span>/</span>li<span>&gt;</span><span>;</span><br>    $<span>[</span><span>3</span><span>]</span> <span>=</span> t3<span>;</span><br><span>}</span> <span>else</span> <span>{</span><br>    t3 <span>=</span> $<span>[</span><span>3</span><span>]</span><span>;</span><br><span>}</span></code></pre>
<p>See the <code>t3 =</code> line? Rather than recreating the arrow function that returns the <code>li</code>, it stores the <em>function itself</em> in the fourth position in the cache array (<code>$[3]</code>). This saves the JavaScript engine the work of creating that small function the next time <code>List</code> is called. Since that function never changes, the initial if-statement is basically saying "if this spot in the cache array is empty, cache it, otherwise get it from cache".</p>
<p>In this way, React caches values and memoizes the results of function calls automatically. The code it outputs is functionally equivalent to the code we wrote, but includes code to cache these values, saving performance hits when our functions are called over and over by React.</p>
<p>React Compiler is caching, though, at a more granular level than what a dev typically does with memoization, and is doing so automatically. This means devs don't have to manually manage dependencies, or memoization. They can just write code, and from it React Compiler will generate new code that utilizes caching to make it faster.</p>
<p>It's worth noting that React Compiler is still producing JSX. The code that is <em>actually</em> run is the result of React Compiler after JSX transpilation.</p>
<p>The <code>List</code> function actually run in the JavaScript engine (sent to the browser or on the server) looks like this:</p>
<pre><code><span>function</span> <span>List</span><span>(</span><span>t0</span><span>)</span> <span>{</span><br>  <span>const</span> $ <span>=</span> <span>_c</span><span>(</span><span>6</span><span>)</span><span>;</span><br>  <span>const</span> <span>{</span><br>    items<br>  <span>}</span> <span>=</span> t0<span>;</span><br>  <span>useState</span><span>(</span><span>null</span><span>)</span><span>;</span><br>  <span>let</span> t1<span>;</span><br>  <span>if</span> <span>(</span>$<span>[</span><span>0</span><span>]</span> <span>===</span> Symbol<span>.</span><span>for</span><span>(</span><span>"react.memo_cache_sentinel"</span><span>)</span><span>)</span> <span>{</span><br>    t1 <span>=</span> <span>{</span><span>}</span><span>;</span><br>    $<span>[</span><span>0</span><span>]</span> <span>=</span> t1<span>;</span><br>  <span>}</span> <span>else</span> <span>{</span><br>    t1 <span>=</span> $<span>[</span><span>0</span><span>]</span><span>;</span><br>  <span>}</span><br>  <span>useReducer</span><span>(</span>reducer<span>,</span> t1<span>)</span><span>;</span><br>  <span>useState</span><span>(</span><span>0</span><span>)</span><span>;</span><br>  <span>let</span> t2<span>;</span><br>  <span>if</span> <span>(</span>$<span>[</span><span>1</span><span>]</span> <span>!==</span> items<span>)</span> <span>{</span><br>    <span>const</span> pItems <span>=</span> <span>processItems</span><span>(</span>items<span>)</span><span>;</span><br>    <span>let</span> t3<span>;</span><br>    <span>if</span> <span>(</span>$<span>[</span><span>3</span><span>]</span> <span>===</span> Symbol<span>.</span><span>for</span><span>(</span><span>"react.memo_cache_sentinel"</span><span>)</span><span>)</span> <span>{</span><br>      <span>t3</span> <span>=</span> <span>item</span> <span>=&gt;</span> <span>_jsx</span><span>(</span><span>"li"</span><span>,</span> <span>{</span><br>        <span>children</span><span>:</span> item<br>      <span>}</span><span>)</span><span>;</span><br>      $<span>[</span><span>3</span><span>]</span> <span>=</span> t3<span>;</span><br>    <span>}</span> <span>else</span> <span>{</span><br>      t3 <span>=</span> $<span>[</span><span>3</span><span>]</span><span>;</span><br>    <span>}</span><br>    t2 <span>=</span> pItems<span>.</span><span>map</span><span>(</span>t3<span>)</span><span>;</span><br>    $<span>[</span><span>1</span><span>]</span> <span>=</span> items<span>;</span><br>    $<span>[</span><span>2</span><span>]</span> <span>=</span> t2<span>;</span><br>  <span>}</span> <span>else</span> <span>{</span><br>    t2 <span>=</span> $<span>[</span><span>2</span><span>]</span><span>;</span><br>  <span>}</span><br>  <span>const</span> listItems <span>=</span> t2<span>;</span><br>  <span>let</span> t3<span>;</span><br>  <span>if</span> <span>(</span>$<span>[</span><span>4</span><span>]</span> <span>!==</span> listItems<span>)</span> <span>{</span><br>    t3 <span>=</span> <span>_jsx</span><span>(</span><span>"ul"</span><span>,</span> <span>{</span><br>      <span>children</span><span>:</span> listItems<br>    <span>}</span><span>)</span><span>;</span><br>    $<span>[</span><span>4</span><span>]</span> <span>=</span> listItems<span>;</span><br>    $<span>[</span><span>5</span><span>]</span> <span>=</span> t3<span>;</span><br>  <span>}</span> <span>else</span> <span>{</span><br>    t3 <span>=</span> $<span>[</span><span>5</span><span>]</span><span>;</span><br>  <span>}</span><br>  <span>return</span> t3<span>;</span><br><span>}</span></code></pre>
<p>React Compiler added an array for caching values, and all the needed if-statements to do so. The JSX transpiler converted the JSX into nested function calls. There is a not-insignificant difference between what you wrote and what the JavaScript engine runs. We are trusting other people's code to produce something that matches our original intent.</p>
<h2>Trading Processor Cycles for Device Memory</h2>
<p>Memoization and caching in general means trading processing for memory. You save on the processor having to execute expensive operations, but you avoid that by using up space to store things in memory.</p>
<p>If you use React Compiler, that means you are saying "store as much as you can" in the device's memory. If the code is running on the user's device in the browser, that's an architectural consideration to keep in mind.</p>
<p>Likely, this won't be a real problem for many React apps. But if you are dealing with large amounts of data in your apps, then device memory usage is something you should at least be aware of and keep an eye on if you use React Compiler once it leaves the experimental stage.</p>
<h2>Abstractions and Debugging</h2>
<p>Compilation in all its forms amounts to a layer of abstraction between the code you write and the code that is actually being run.</p>
<p>As we saw, in the case of React Compiler, to understand what is actually sent to the browser you need to take your code and run it through React Compiler, and then take <em>that</em> code and run it through a JSX transpiler.</p>
<p>There is a downside to adding layers of abstraction to our code. They can make our code harder to debug. That doesn't mean we shouldn't use them. But you should keep clearly in mind that the code you need to debug isn't just yours, but the code the tool is generating.</p>
<p>What makes a real difference in your ability to debug code generated from an abstraction layer, is to have an accurate mental model of the abstraction. Fully understanding how React Compiler works will give you the ability to debug the code it writes, improving your dev experience and lowering the stress your dev life.</p>
<h2>Dive Deeper</h2>
<p>If you found this blog post helpful, you might be interested in doing a similar deep dive across all of React's features in my 16.5 hour course <strong><a href="https://understandingreact.com/">Understanding React</a></strong>. You get lifetime access, all source code, and a certificate of completion.</p>
<p>I read every line of React's source code, and then every line of React Compiler's source code. Why? So I could explain React from the internals level, under-the-hood.</p>
<p>React itself is a massive abstraction layer on top of web fundamentals. As is the case with so many abstractions, I find that most devs using React have an inaccurate mental model of how it works, which greatly impacts how they build and debug React-based applications. But you <em>can</em> understand React deeply.</p>
<p>New content on React 19 features and React Compiler is coming to the course, free to students who are already enrolled. <strong><a href="https://understandingreact.com/">Check the course out</a></strong>, you can watch a lot for free. I hope you'll join me on a journey of, not just imitating someone else writing code, but truly understanding what you're doing.</p>
<p>-- Tony</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Two million checkboxes (written in Elixir) (114 pts)]]></title>
            <link>https://twomillioncheckboxes.com</link>
            <guid>40819184</guid>
            <pubDate>Fri, 28 Jun 2024 10:05:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twomillioncheckboxes.com">https://twomillioncheckboxes.com</a>, See on <a href="https://news.ycombinator.com/item?id=40819184">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p><a href="https://twomillioncheckboxes.com/" data-phx-link="redirect" data-phx-link-state="push">
      <span>Two</span> Million Checkboxes
    </a></p><p>(checking a box checks it for everyone)</p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Parkinson's Link to Gut Bacteria Suggests Unexpected, Simple Treatment (129 pts)]]></title>
            <link>https://www.sciencealert.com/parkinsons-link-to-gut-bacteria-suggests-unexpected-simple-treatment</link>
            <guid>40818987</guid>
            <pubDate>Fri, 28 Jun 2024 09:23:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sciencealert.com/parkinsons-link-to-gut-bacteria-suggests-unexpected-simple-treatment">https://www.sciencealert.com/parkinsons-link-to-gut-bacteria-suggests-unexpected-simple-treatment</a>, See on <a href="https://news.ycombinator.com/item?id=40818987">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><img width="642" height="260" src="https://www.sciencealert.com/images/2023/04/microbes_epithelium_gut_AD-642x260.jpg" alt="Artist's impression of microbes on the gut lining." loading="eager" decoding="async" fetchpriority="high" srcset="https://www.sciencealert.com/images/2023/04/microbes_epithelium_gut_AD-642x260.jpg 642w, https://www.sciencealert.com/images/2023/04/microbes_epithelium_gut_AD.jpg 1024w, https://www.sciencealert.com/images/2023/04/microbes_epithelium_gut_AD-768x311.jpg 768w, https://www.sciencealert.com/images/2023/04/microbes_epithelium_gut_AD-600x243.jpg 600w" sizes="(-webkit-min-device-pixel-ratio: 2) 50vw,
			(min-resolution: 192dpi) 50vw,
			(min-resolution: 2dppx) 50vw,
			(-webkit-min-device-pixel-ratio: 3) 33.33vw,
			(min-resolution: 288dpi) 33.33vw,
			(min-resolution: 3dppx) 33.33vw"> <span>
<span>Illustration of bacteria on the colon epithelium.</span> <span>(Nanoclustering/Science Photo Library/Getty Images)</span> </span>
</p><div>
<p>Researchers have suspected for<a href="https://www.sciencealert.com/new-evidence-suggests-parkinson-s-might-start-in-the-gut-before-spreading-to-the-brain"> some time</a> that the link between our gut and brain plays a role in the development of <a href="https://www.sciencealert.com/go/IYl" data-linkid="73029" data-postid="130757" rel="nofollow" target="_self">Parkinson's</a> disease.</p><p>A new study just identified gut microbes likely to be involved and linked them with decreased <a href="https://en.wikipedia.org/wiki/Riboflavin">riboflavin</a> ( <a href="https://www.sciencealert.com/what-are-vitamins-and-do-we-really-need-to-take-them" data-linkid="73116" data-postid="130757" rel="nofollow" target="_self">vitamin</a> B2) and <a href="https://en.wikipedia.org/wiki/Biotin">biotin</a> (vitamin B7), pointing the way to an unexpectedly simple treatment that may help: B <a href="https://www.sciencealert.com/what-are-vitamins-and-do-we-really-need-to-take-them" data-linkid="73116" data-postid="130757" rel="nofollow" target="_self">vitamins</a>.</p>
<p>"Supplementation of riboflavin and/or biotin is likely to be beneficial in a subset of Parkinson's disease patients, in which gut dysbiosis plays pivotal roles," Nagoya University medical researcher Hiroshi Nishiwaki and colleagues <a href="https://doi.org/10.1038/s41531-024-00724-z">write</a> in their published paper.</p>
<p>The neurodegenerative disease impacts <a href="https://www.who.int/news-room/fact-sheets/detail/parkinson-disease">almost 10 million people globally</a>, who at best can hope for <a href="https://www.sciencealert.com/this-is-the-first-time-drug-shows-signs-of-slowing-parkinsons-disease">therapies that slow and alleviate symptoms</a>.</p>
<p>Symptoms typically begin with constipation and sleep problems, up to 20 years before progressing into dementia and the debilitating loss of muscle control.</p><figure id="attachment_131231" aria-describedby="caption-attachment-131231"><img decoding="async" src="https://www.sciencealert.com/images/2024/06/SHakingHandPIckingUpGlass-1.jpg" alt="Hands picking up a glass" width="642" height="500" srcset="https://www.sciencealert.com/images/2024/06/SHakingHandPIckingUpGlass-1.jpg 642w, https://www.sciencealert.com/images/2024/06/SHakingHandPIckingUpGlass-1-533x415.jpg 533w, https://www.sciencealert.com/images/2024/06/SHakingHandPIckingUpGlass-1-600x467.jpg 600w" sizes="(max-width: 642px) 100vw, 642px" loading="lazy"><figcaption id="caption-attachment-131231">(<a href="https://www.canva.com/photos/MAEGCRiWJIk/">pixelshot/Canva Pro</a>)</figcaption></figure><p><a href="https://www.sciencealert.com/changes-in-gut-bacteria-are-present-long-before-signs-of-parkinsons-appear">Previous research</a> found people with Parkinson's disease also experience changes in their microbiome long before other signs appear.</p>
<p>Analyzing fecal samples from 94 patients with Parkinson's disease and 73 relatively healthy controls in Japan, Nishiwaki and team compared their results with data from China, Taiwan, Germany, and the US.</p>
<p>While different groups of bacteria were involved in the different countries examined, they all influenced pathways that synthesize B vitamins in the body. The researchers found the changes in gut bacteria communities were associated with a decrease in <a href="https://en.wikipedia.org/wiki/Riboflavin">riboflavin</a> and <a href="https://en.wikipedia.org/wiki/Biotin">biotin</a> in people with Parkinson's disease.</p>
<p>Nishiwaki and colleagues then showed the lack of B vitamins was linked to a decrease in <a href="https://en.wikipedia.org/wiki/Short-chain_fatty_acid">short-chain fatty acids</a> (SCFAs) and <a href="https://en.wikipedia.org/wiki/Polyamine">polyamines</a>: molecules that help create a healthy mucus layer in the intestines.</p>
<p>"Deficiencies in polyamines and SCFAs could lead to thinning of the intestinal mucus layer, increasing intestinal permeability, both of which have been observed in Parkinson's disease," Nishiwaki <a href="https://www.nagoya-u.ac.jp/researchinfo/result-en/2024/06/20240618-01.html">explains</a>.</p><figure id="attachment_131227" aria-describedby="caption-attachment-131227"><img decoding="async" src="https://www.sciencealert.com/images/2024/06/GraphicOfParkinsonsVitaminBConnection642.jpg" alt="A graphic depicting the process of gut bacteria depleting B vitamins and leading to symptoms of Parkinson's disease" width="642" height="950" srcset="https://www.sciencealert.com/images/2024/06/GraphicOfParkinsonsVitaminBConnection642.jpg 642w, https://www.sciencealert.com/images/2024/06/GraphicOfParkinsonsVitaminBConnection642-280x415.jpg 280w, https://www.sciencealert.com/images/2024/06/GraphicOfParkinsonsVitaminBConnection642-600x888.jpg 600w" sizes="(max-width: 642px) 100vw, 642px" loading="lazy"><figcaption id="caption-attachment-131227">Summary of findings from the study and speculations from previous research. (Nishiwaki et al., <a href="https://doi.org/10.1038/s41531-024-00724-z"><em>npj Parkinson's Disease</em></a>, 2024)</figcaption></figure><p>They suspect the weakened protective layer exposes the intestinal nervous system to more of the toxins we now encounter more regularly. These include <a href="https://www.sciencealert.com/dry-cleaning-chemical-could-be-major-cause-of-parkinsons-scientists-warn">cleaning chemicals,</a> <a href="https://www.sciencealert.com/two-pathways-to-parkinsons-could-point-to-a-single-way-to-prevent-it">pesticides</a>, and herbicides.</p>
<p>Such toxins lead to the overproduction of α-synuclein fibrils – molecules <a href="https://doi.org/10.1016/j.nbd.2024.106411">known to amass</a> in <a href="https://www.sciencealert.com/new-test-detects-parkinsons-7-years-before-most-symptoms-show">dopamine-producing cells</a> in the <a href="https://www.sciencealert.com/scientists-pinpoint-exactly-which-brain-cells-die-in-parkinson-s-disease">substantia nigra</a> part of our brains, and increased nervous system inflammation, eventually leading to the more debilitating motor and dementia symptoms of Parkinson's.</p>
<p>A <a href="https://www.scielo.br/j/bjmbr/a/BM4WLJBtjxF8Cx3wFsjFhKb/?lang=en">2003 study</a> found high doses of riboflavin can assist in recovering some motor functions in patients who also eliminated red meat from their diets.</p>
<p>So it's possible that high doses of vitamin B may prevent some of the damage, Nishiwaki and team propose.</p><figure id="attachment_131228" aria-describedby="caption-attachment-131228"><img decoding="async" src="https://www.sciencealert.com/images/2024/06/VitaminBInBlood642.jpg" alt="Illustration of a vitamin B2 molecule in the blood. " width="642" height="424" srcset="https://www.sciencealert.com/images/2024/06/VitaminBInBlood642.jpg 642w, https://www.sciencealert.com/images/2024/06/VitaminBInBlood642-628x415.jpg 628w, https://www.sciencealert.com/images/2024/06/VitaminBInBlood642-600x396.jpg 600w" sizes="(max-width: 642px) 100vw, 642px" loading="lazy"><figcaption id="caption-attachment-131228">Illustration of a riboflavin (B2) molecule in the blood. (Nemes Laszlo/Science Photo Library/Getty Images)</figcaption></figure><p>This all suggests ensuring patients have healthy gut microbiomes may also prove protective, as would <a href="https://www.hopkinsmedicine.org/health/conditions-and-diseases/parkinsons-disease/can-environmental-toxins-cause-parkinson-disease">reducing the toxic pollutants</a> in our environment.</p>
<p>Of course, with such a complicated chain of events involved in Parkinson's disease, not all patients likely experience the same causes, so each individual would need to be assessed.</p>
<p>"We could perform gut microbiota analysis on patients or conduct fecal metabolite analysis," <a href="https://www.nagoya-u.ac.jp/researchinfo/result-en/2024/06/20240618-01.html">explains</a> Nishiwak.</p>
<p>"Using these findings, we could identify individuals with specific deficiencies and administer oral riboflavin and biotin supplements to those with decreased levels, potentially creating an effective treatment."</p><p>This research was published in <a href="https://doi.org/10.1038/s41531-024-00724-z"><em>npj Parkinson's Disease</em></a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: What is the best code base you ever worked on? (395 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=40818809</link>
            <guid>40818809</guid>
            <pubDate>Fri, 28 Jun 2024 08:40:44 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=40818809">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="40818809">
      <td><span></span></td>      <td><center><a id="up_40818809" href="https://news.ycombinator.com/vote?id=40818809&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=40818809">Ask HN: What is the best code base you ever worked on?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_40818809">109 points</span> by <a href="https://news.ycombinator.com/user?id=pcatach">pcatach</a> <span title="2024-06-28T08:40:44"><a href="https://news.ycombinator.com/item?id=40818809">9 hours ago</a></span> <span id="unv_40818809"></span> | <a href="https://news.ycombinator.com/hide?id=40818809&amp;goto=item%3Fid%3D40818809">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20What%20is%20the%20best%20code%20base%20you%20ever%20worked%20on%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=40818809&amp;auth=5ee72ab5243c5242c20ff4d4f59b5526ab0dd94a">favorite</a> | <a href="https://news.ycombinator.com/item?id=40818809">96&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>And what made it so good?</p><p>Was there someone enforcing good practices top down? Just being in a group of great engineers? Or something else?</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table><table>
            <tbody><tr id="40823142"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40823142" href="https://news.ycombinator.com/vote?id=40823142&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Google's monorepo, and it's not even close - primarily for the tooling:</p><p>* Creating a mutable snapshot of the entire codebase takes a second or two.</p><p>* Builds are perfectly reproducible, and happen on build clusters. Entire C++ servers with hundreds of lines of code can be built from scratch in a minute or two tops.</p><p>* The build config language is really simple and concise.</p><p>* Code search across the entire codebase is instant.</p><p>* File history loads in an instant.</p><p>* Line-by-line blame loads in a few seconds.</p><p>* Nearly all files in supported languages have instant symbol lookup.</p><p>* There's a consistent style enforced by a shared culture, auto-linters, and presubmits.</p><p>* Shortcuts for deep-linking to a file/version/line make sharing code easy-peasy.</p><p>* A ton of presubmit checks ensure uniform code/test quality.</p><p>* Code reviews are required, and so is pairing tests with code changes.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40823346"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40823346" href="https://news.ycombinator.com/vote?id=40823346&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>&gt; Entire C++ servers with hundreds of lines of code can be built from scratch in a minute or two tops.</p><p>Hundreds, huh? Is this a typo? It makes me wonder if the whole comment is facetious. Or do C++ programmers just have very low expectations for build time?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40823383"><td></td></tr>
                        <tr id="40819222"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819222" href="https://news.ycombinator.com/vote?id=40819222&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>The one in my previous job, which was an admin board for a market intelligence application. Ultimately, the reason it was good was because the engineers had zero ego on top of having excellent skills. The team that set the codebase were basically 4 seniors and 3 principals (the client actually did pay for top talent in this case) so not only everything was based on industry standards, written elegantly and organized perfectly, but every time some new requirement came up, these senior / principal engineers would discuss it in the most civilized matter I have ever seen.</p><p>E.g, "we need to come up with a way to implement X". Person A gives their idea, person B gives another idea and so on until everybody shared their thoughts. Then someone would say "I think what person C said makes the most sense" and everybody would agree and that was it. 30 minutes to hear everybody out, 3 minutes to discuss who will do it and when and the meeting was over.</p><p>I think the biggest testament to this code base was that when junior members joined the team, they were able to follow the existing code for adding new features. It was that easy to navigate and understand the big picture of.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40823111"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40823111" href="https://news.ycombinator.com/vote?id=40823111&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I've seen a few people say 'google3'.</p><p>Q: is it actually the code that you loved, or simply the tooling that exists?</p><p>(and if it's tooling, why can't that type of tooling be replicated for other codebases outside of google?)</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40823510"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40823510" href="https://news.ycombinator.com/vote?id=40823510&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Its both. The tooling has a very direct impact on the quality of the code.</p><p>I think the reason its not easy replicable is:</p><p>1. It takes a ton of initial investment and ongoing maintenance but its worth it when your code base is gigantic.</p><p>2. There is a consistent set of top down enforced rules. With the consistency it becomes much, much easier to build tight integrations between tools.</p><p>(almost?) everything is buildable by a single build system (blaze). When anyone can consistently build/test/run anything in your codebase it becomes a lot easier to build a whole host of potential tools like code search.</p><p>Probably someone can dive deeper than I can. But one thing I learned the most important property for a code base to be maintainable/scalable is consistency.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40823459"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40823459" href="https://news.ycombinator.com/vote?id=40823459&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>&gt; (and if it's tooling, why can't that type of tooling be replicated for other codebases outside of google?)</p><p>The elegance of the tooling from what I hear is that there's tons of different tools maintained by different teams that work seamlessly (and fast) together to produce google3 and all of its supporting pieces.</p><p>But to answer your question, sure it can. But good luck building your own. Google has been doing this since the 2000s.</p><p>And if you're a big company already, you've already bought into your existing patterns &amp; design choices; things like that are VERY hard to change.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40822429"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40822429" href="https://news.ycombinator.com/vote?id=40822429&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Postgres. I don't code in C if I can avoid it, since it often feels like an awful lot of extra typing while still having to worry about memory safety. But the Postgres codebase is extraordinarily well organized and respects the humans that work with it with its intelligent handling of memory and judicious use of macros.</p><p>I consider the core Postgres codebase to be the gold standard in development even though it's in a language I do not prefer to write in if given the choice.</p><p>Shout out to the pgrx folks. You're awesome! <a href="https://github.com/pgcentralfoundation/pgrx">https://github.com/pgcentralfoundation/pgrx</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819157"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819157" href="https://news.ycombinator.com/vote?id=40819157&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Google3 codebase. It's just so vast and it works. It's a miracle. I feel lucky to have seen it. Everytime you change it, it reruns the dependencies. Everyone has different view concurrently. Commits are efficient immutable snapshots. It's just incredible multiplayer. So massively beyond what can be done with GitHub. Really I feel it's peak codebase. I've not seen what the other big techs do but it blew my mind</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40823120"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40823120" href="https://news.ycombinator.com/vote?id=40823120&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>+1</p><p>I can not understate how much I agree with parent comment.</p><p>The opposite of move fast, build a shitty prototype and iterate is a deliberate problem solving approach  undertaken by the highest caliber of engineers. The actual challenges to be addressed are effectively addressed right at the design stage.</p><p>The result is a thing of immense beauty and elegance.</p><p>I will forever be grateful for the opportunity I had to see this magnificent piece of engineering in action.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819316"><td></td></tr>
            <tr id="40822973"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40822973" href="https://news.ycombinator.com/vote?id=40822973&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Google3 codebase very consistently has <i>clean</i> code, but some of the architecture there is very much not great.</p><p>Some is great, some not so much.</p><p>Some of Verizon's code was <i>much</i> more elegant (though much smaller scope) from an API perspective, and really leaned into advanced type systems in a way Google has not.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819202"><td></td></tr>
                <tr id="40819223"><td></td></tr>
                <tr id="40819247"><td></td></tr>
                              <tr id="40819146"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819146" href="https://news.ycombinator.com/vote?id=40819146&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>For me, the most eye-opening codebase of my career was Cocotron, around 2007:</p><p><a href="https://github.com/cjwl/cocotron">https://github.com/cjwl/cocotron</a></p><p>I was looking for a way to port my native Mac Cocoa apps to Windows. I had been already disappointed by the aimless sprawl of GNUstep.</p><p>This one-person project implemented all the essential APIs for both Foundation and AppKit. Reading the code was a revelation: can it really be this simple and at the same time this effortlessly modular for cross-platform support?</p><p>I contributed a few missing classes, and successfully used Cocotron for some complex custom GUI apps that needed the dual-platform support.</p><p>Cocotron showed me that one person with a vision can build something that will rival or even outgun large teams at the big tech companies. But focus is essential. Architecture astronauts usually never get down from their high orbits to ship something.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819140"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819140" href="https://news.ycombinator.com/vote?id=40819140&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Worked on a codebase for a large safety-critical system where everything was 100% documented, and the development guide for the project was followed so closely that you couldn't tell, across millions of lines of code, that the whole thing wasn't written by one person. Absolutely impressive levels of attention to detail everywhere, down to not even being able to find typographical errors in comments or documentation (a typo in a comment was treated just as seriously as any other bug).</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819164"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819164" href="https://news.ycombinator.com/vote?id=40819164&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Let me guess, it was very well funded and there were no fake deadlines and cross-team dependencies, am I correct or am I very correct?</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40823140"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40823140" href="https://news.ycombinator.com/vote?id=40823140&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>I am almost sure this is because the system would have to pass a certification procedure somewhere, and for that they would need this level of clarity. Am I right?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40819254"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819254" href="https://news.ycombinator.com/vote?id=40819254&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Last year I worked for a client that gave me a lot of time, money and autonomy to lead dev on a critical software rewrite.</p><p>We got a small team of competent people, with domain experts to peer code with the devs.</p><p>It was wonderful. We could test, document and clean up. Having people who knew the trade and users at hand removed second guessing.</p><p>The result was so good we found bugs even in competitors' implementations.</p><p>We also got x5 in perfs compared to the system it was replacing and more features.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40823353"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40823353" href="https://news.ycombinator.com/vote?id=40823353&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Similar thing.</p><p>Had time and autonomy from a client, so took sweet time examining the domain, the existing systems et al. Spent a few months writing the basis and the framework around what will be done, based on years and years of experience I had with bad frameworks and codebases, combined with working on the same domain for their parent company years ago.</p><p>And it worked. We delivered features insanely fast, hundreds of forms were migrated, feature generators would create 90% of the boilerplate code and the code was small, readable and neatly clustered. Maintaining it was a piece of cake, leading to us not having enough work after a while so we I negotiated our time to half a week for the same money.</p><p>After a while, client deemed us too expensive to pay for only 2.5 days of work - after all, how does it make sense - if we are paying them that much, they should work 5 days!</p><p>So they cut us out. Two things happened:</p><p>1. Devs that got moved to other projects in the company told me they didn't know development could be so smooth and tried to replicate it in future projects, even tho they say it failed a lot of lessons they picked up from the framework were highly relevant in their future careers.</p><p>2. The company found a cheaper developer, he said "this is unusable and has to be rewritten" and rewrote it into "clean code", taking longer than the original project took. At least he works 5 days a week now.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819277"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819277" href="https://news.ycombinator.com/vote?id=40819277&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Good for you. Magical moments in careers are hard to find in my experience but they are so satisfying when you get there.</p><p>Glad whomever was over this didnt just drop the "dont rewrite" joel spolsky article and fight making it happen.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40822182"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40822182" href="https://news.ycombinator.com/vote?id=40822182&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Joel has since said that that he doesn't really agree with that advice anymore, at least not in the same way. Super annoying that it gets parroted over and over again as though it's the word of the lord.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819354"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40819354" href="https://news.ycombinator.com/vote?id=40819354&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I actually was the one telling them not to rewrite, lol.</p><p>But the original code was a mess of matlab spaghetti, they couldn't find a way to hire for that. Not to mention turning it into a web service was already a big hack of java parsing a raw dump of matlab datastructures that nobody dared to touch.</p><p>I had to read the matlab code, and it tooks hours to decypher a few lines. Plus the language doesn't have great debugging and error handling capabilities and the tooling is quite terrible.</p><p>So rewriting to python won, and for once, I must say it was a good call.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40819274"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819274" href="https://news.ycombinator.com/vote?id=40819274&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>One of them stands out, due to being super-productive, over years, and then decades.</p><p>A large system that was originally written by only two super-productive engineers (I mean real engineers, both with PhDs in an area of Engineering).  And a comparably capable and essential IT person.</p><p>The reasons for the super-productivity include one of the developers choosing great technology and using it really well, to build a foundation with "force multiplier" effects, and the other developer able to build out bulk with that, while understanding the application domain.</p><p>Another reason was understanding and being pretty fully in control of the code base, so that, as needs grew and changed, over years, someone could figure out how to do whatever was needed.</p><p>One of the costs was that most things had to be built from scratch.  Over time that also proved to be an advantage, because whenever they needed (put loosely) a "framework" to something it couldn't do, they effectively owned the framework, and could make dramatic changes.</p><p>When I said "costs", I mean things like, many times they needed to make a component from scratch that would be an off-the-shelf component in some other ecosystem.  So if someone looked closely at how time was sometimes spent, without really understanding it or knowing how that panned out, it would look like a cost that they could optimize away.  But if they looked at the bigger picture, they'd see a few people consistently, again and again, accomplishing what you'd think would take a lot more people to do.</p><p>It helped that the first programmer also became the director for that area of business, and made sure that smart engineering kept happening.</p><p>Someone might look for a reason this couldn't work, and think of bus factor.  What I think helped there was the fact that the work involved one of those niche languages that attract way more super programmers than there are jobs.  "Gosh, if only we had access to a secret hiring pool of super programmers who were capable of figuring out how to take up where the other person left off, and we had a way to get them to talk with us...")</p><p>It was easy to imagine a competitor with 100 developers, not able to keep up, and at many points getting stuck with a problem that none of them were able to solve.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40823146"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40823146" href="https://news.ycombinator.com/vote?id=40823146&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>I assume you avoided identifying (or even hinting at) this "great technology" on purpose, but could you persuaded to divulge what it was?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40819718"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819718" href="https://news.ycombinator.com/vote?id=40819718&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Pretty much any internal tool/TUI/CLI/library I've created. If I had to guess I'd say at most 25% of the company projects I've worked on have launched AND have consistent usage. Working hard on something just for it to wither crushes my soul but internal projects are different. They're all skunk works projects. No tickets. No project/board. No PM pushing back on how many points (read: hours) something should be. I'm solving real problems that directly impact the quality of life for myself and my coworkers. The best part is getting real, genuine, feedback. If something sucks they'll tell you and they won't sugarcoat it.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819727"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819727" href="https://news.ycombinator.com/vote?id=40819727&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>I love this take.  What language(s) do you typically use to write CLI programs?  I'm also interested in learning about what types of internal TUI tools you have created.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40822493"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40822493" href="https://news.ycombinator.com/vote?id=40822493&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>The run time library for Turbo Pascal/Delphi for Windows was completely documented, sane, and very easy to work with. The working examples really helped.</p><p>The free Pascal RTL seems opaque in comparison. Their reliance on and archaic help file build system keeps contributors away. Thus it's poorly documented at best.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819043"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819043" href="https://news.ycombinator.com/vote?id=40819043&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>My favourite projects are small, with very focused goals and features.</p><p>I have a Laravel project that I have maintained for a customer for seven years.
The app is straightforward and allows users to create portals that list files and metadata, such as expiration dates and tags.</p><p>Every other year, they ask me to add a new batch of features or update the UI to reflect the business's branding.
As the app is so small, I have the opportunity to review every part of the app and refactor or completely rewrite parts I am not happy with.</p><p>It is a joy to work on and I always welcome new requests.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40823296"><td></td></tr>
            <tr id="40819262"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819262" href="https://news.ycombinator.com/vote?id=40819262&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>My past three employers code bases: mono-repos, Bazel, lots ot C++ and Python, thousands of libraries and tools, code generation and modeling tools that are fully integrated into the build, easy cross compilation, large integration tests just one bazel test invocation away, hermetic and uniform dependencies...</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819125"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819125" href="https://news.ycombinator.com/vote?id=40819125&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>The latest Go micro-service I have built.</p><p>About once a year roughly, for the last couple years, the opportunity has arisen to greenfield a Go micro-service with pretty loose deadlines.</p><p>Each time I have come into it with more knowledge about what went well and what I wasn't particularly happy with the last time. Each one has been better than the last.</p><p>I've been building software professionally for twenty years, and these micro-services have been one of the few projects in that time that have had clear unified vision and time to build with constant adjustments in the name of code quality.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819013"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819013" href="https://news.ycombinator.com/vote?id=40819013&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Although I'm only on job three and have not had that much involvement with open source, I think my current employer (Attio) has one of the best codebases I've seen.</p><p>Qualitatively, I experience this in a few ways:
* Codebase quality improves over time, even as codebase and team size rapidly increase
* Everything is easy to find. Sub-packages are well-organised. Files are easy to search for
* Scaling is now essentially solved and engineers can put 90% of their time into feature-focused work instead of load concerns</p><p>I think there are a few reasons for this:</p><p>* We have standard patterns for our common use cases
* Our hiring bar is high and everyone is expected to improve code quality over time
* Critical engineering decisions have been consistently well-made. For example, we are very happy to have chosen our current DB architecture, avoided GraphQL and used Rust for some performance-critical areas
* A TypeScript monorepo means code quality spreads across web/mobile/backend
* Doing good migrations has become a core competency. Old systems get migrated out and replaced by better, newer ones
* GCP makes infra easy
* All the standard best practices: code review, appropriate unit testing, feature flagging, ...</p><p>Of course, there are still some holes. We have one or two dark forest features that will eventually need refactoring/rebuilding; testing needs a little more work. But overall, I'm confident these things will get fixed and the trajectory is very good.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819307"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819307" href="https://news.ycombinator.com/vote?id=40819307&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>One that had a sort of improvised facade/adapter pattern (it didn't really follow either) in a clearly cut multilayered and pipelined structure, with actor model bits where it made sense.</p><p>The code wasn't simple, at all. It took active training of new arrivals for them to understand it. But it was very well thought out, with very few warts given the complexity, and extremely easy to extend (that was the main requirement, given constant changes in APIs and clients).</p><p>We had an API, with multiple concurrent versions, that transformed requests into an intermediate model, on which our business logic operated, later targetted external APIs (dozens of them, some REST, some SOAP, some under NDAs, some also with multiple versions), whose responses turned again into the intermediate model, with more business logic on our end, and a final response through our API. Each transaction got its context serialized so we could effectively have what was an, again improvised, "async/await"-like syntax in what was (trigger warning) C++03 code.</p><p>The person who engineered it didn't have formal CS background.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819022"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819022" href="https://news.ycombinator.com/vote?id=40819022&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>google3, all the devex tooling was taken care of by other teams. Tons of useful library functions available to import, accumulated over decades.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40820893"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40820893" href="https://news.ycombinator.com/vote?id=40820893&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Probably the microservices-based one for async video messaging (i.e. Slack for video) for workers in the field. Each service was small enough that we could do a blue-green deploy to prod in about 2 minutes running only the service's tests and a tiny (intentionally limited) set of about 8 system journey tests <i>(can onboard a new user, user can create a contact/group, user can send a common content-type message, user can receive messages, user can react/respond to a message)</i>. Every commit to main/master automatically either deployed to prod or broke the CI/CD pipeline and needed to be fixed ASAP. Each service was also well-known by team members that it literally could be rewritten in a week or two if desired to change a key part of its design.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40821122"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40821122" href="https://news.ycombinator.com/vote?id=40821122&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Thinking about what I wrote, I suppose my criteria for a good codebase is one that has the lowest friction to change: a fast edit/run/test/debug loop, a meaningful sense of security (without dogma), and fast automated deployment/revert (via blue/green). Given those a bad codebase can become a good one again (by uncoordinated action of individuals) without <i>everyone having to buy into</i> a large investment.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40820818"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40820818" href="https://news.ycombinator.com/vote?id=40820818&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Realistically any code base where the engineers had at least a basic understanding of programming. You do not know suffering until you've seen someone hard code basic variables, we're talking about strings all over the place, and then they just copy the function again to replace the strings .</p><p>I've legitimately left jobs over bad code. We're talking about code that did nothing in reality.
The best code bases have been ones where I've been able to lead the direction. I get to know exactly how things work. I'm privileged to have a job where I essentially created the initial framework right now .</p><p>Plus I'm fully remote, life is pretty good.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819061"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819061" href="https://news.ycombinator.com/vote?id=40819061&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Considering the biases, the one I wrote for the company I created.</p><p>When we have the opportunity to be in this context, keeping in mind what bothered us in the codebases with which we were able to work in the past, we can force ourselves not to reproduce the same errors. Like the unmaintained unit and integration tests, the lack of refactoring, other developers that use fancy technologies instead of simpler concepts more for the opportunity to play with technologies than real need..</p><p>And also, I guess, because we are more aware that the code is a reflection of the company that we want to have, that the simpler the better is a key point when we need to debug.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819304"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819304" href="https://news.ycombinator.com/vote?id=40819304&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>The best codebase is the one you fully understand. I prefer codebases that are small enough to understand within a week. This is why I like Microservices. Large codebases can be overwhelming and even senior developers working a decade in the company of might not fully understand them. Instead, I prefer maintaining a few Microservices that our team fully comprehends, where the entire codebase fits into a clear mental model. We then interact with other codebases, that have active mental models in other teams, via APIs.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819115"><td></td></tr>
            <tr id="40819057"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819057" href="https://news.ycombinator.com/vote?id=40819057&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Any codebase that I had complete control over.</p><p>No, but more seriously, I've found that familiarity with the codebase is more important than having it be perfectly engineered. Once you're really familiar with the codebase, you know where dragons be, and you can make changes more easily. And God (PM) forbid, if you ever find yourself with some extra free time you might even reduce the size of dragons over time.</p><p>This brings me to my final point. Any codebase that I really enjoyed working with was the one that was constantly evolving. I don't mean rewriting everything from scratch every few months, but as long as I have permission (and time) to refactor the things that have been bothering me for months as patterns emerge, I'm a happy bee.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819086"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819086" href="https://news.ycombinator.com/vote?id=40819086&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>&gt; Any codebase that I had complete control over.</p><p>No excuses. Code ownership is important. Sometimes it works for a team, sometimes only for individuals.</p><p>But not having to submit to core teams, architects and self-proclaimed experts of all kinds is a blessing.</p><p>I now work for an organization that discourages code ownership, and it struggles  on many fronts:</p><pre><code>        1. core teams are dysfunctional
        2. people find niches and stick to them
        3. top talent is leaving, although pay is good and business creates real value for citizens
        4. there is virtually no horizontal communication
        5. mediocre ones rise to the level of their incompetence and  infest the lives of others
        6. and so on and so forth...
</code></pre><p>
And I think the root cause of all this is lack of individual (code) ownership.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819367"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40819367" href="https://news.ycombinator.com/vote?id=40819367&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>I’ve had exactly the same issues but because I couldn’t change anything without getting approval from 16.5 code owners on every PR submitted. It’s a real pain if you start modifying your coding for ‘least code owners hit’ instead of ‘best architecture’.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819893"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40819893" href="https://news.ycombinator.com/vote?id=40819893&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>I like how my workplace does it -- there are rigorous codeowners and usually you only need approval from 1-2. if you do need approval from 5+, you can request a select 'super' codeowner review which will approve it for all.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40819644"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40819644" href="https://news.ycombinator.com/vote?id=40819644&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I think core teams being helpful or harmful really comes down to the individuals.</p><p>The problem is at this level, most orgs don't have anyone to really judge or contest competency, so they hire the salesmen rather than the doers and when they don't, they tend to cheap out and just get inexperienced people.</p><p>Logically it makes a bunch of sense, though.</p><p>Why rebuild yet another platform?  Why is your central platform bad?  Usually it's not self-service, sometimes it's because it's built in cumbersome ways, other times its because it actually enforces good standards on you rather than just giving app your apps admin.</p><p>It's difficult for the person who hires the core team to differentiate between those complaints, unless they themselves both have the technical competency and the empathy to really understand the problem.  They usually don't.</p><p>Point being, done well, it's great, but most folks can't do it well.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40821957"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40821957" href="https://news.ycombinator.com/vote?id=40821957&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>&gt; And God (PM) forbid, if you ever find yourself with some extra free time you might even reduce the size of dragons over time.</p><p>Honest question, what is the company like where you can do that? Everywhere I've worked (only been working in industry for 6 years) has had such rigid agile development that even when I do find myself with free time, there's no flexibility to work on things that haven't been assigned to you and the best I can do is work on profiling/debugging tools.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819258"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819258" href="https://news.ycombinator.com/vote?id=40819258&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I also echo the code ownership part.</p><p>I have a part-time gig where I maintain accounting software for a former client of mine. It takes up a few months' weekends a year.</p><p>I wrote about 60-70% of it when I was working for the owner of the software. It's something where as long as the client's happy, and they get new integrations and updates on time, they could keep using it for a decade longer.</p><p>I had almost complete ownership of the architecting of the software. It's broken down into a few microservices (think database, core business logic, reporting, auth, logging etc).
The best thing I did at the time was pushing to use gRPC even though management felt it was too new tech.</p><p>The UI is in Angular, pain-free periodic upgrades. I've even rewritten some perf-sensitive code in Rust, and everyone's happy with snappier calculations.</p><p>The code hygiene is relatively good.</p><p>The only downside's that if someone else were to take over the code, they'd struggle (it's one of those things where I'm wearing many hats). I've been fortunate to be a professional accountant who moved into software engineering, so everything makes sense to me.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819251"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819251" href="https://news.ycombinator.com/vote?id=40819251&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>&gt; Once you're really familiar with the codebase, you know where dragons be, and you can make changes more easily.</p><p>This is an interesting, and often overlooked, point. A month or two ago someone asked us, the Fossil SCM maintainers, if we'd be open to them refactoring the tree to something which better matches modern sensibilities (i'm paraphrasing here). Despite its many proverbial dragons, the long-time maintainers are comfortable with it and know where those dragons are and how to defeat them (or, in some cases, sneak around them), so, despite our collective geek tendencies to prefer "perfect code," we're happier with the dragons we know than those we don't. (That's not to say that fossil's code is awful, but its structure varies significantly from what one might write today if one was to start such a project from the ground up.)</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819200"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819200" href="https://news.ycombinator.com/vote?id=40819200&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>This is very much true. Initially when I joined the industry I used to work for a product from its inception. So I was aware of what section of the code affects what part of the product. Although I hadn't worked on all of those, I kept an eye for the all the changes that were coming in. I knew where I should look immediately a bug is reported even if it is not something related to my line of work.</p><p>Recently I switched teams and now I find myself taking up bugs that are only related to my line of work. Not being familiar with the codebase decreases productivity and wants you to rely on other people in the team for most of the time.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819174"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819174" href="https://news.ycombinator.com/vote?id=40819174&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>&gt; Any codebase that I had complete control over.</p><p>Anyone other than myself would instantly observe it as the the worst codebase they have ever seen.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40820078"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40820078" href="https://news.ycombinator.com/vote?id=40820078&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>For that, there is a saying: "code as if your kids will maintain it".</p><p>But I think it does not convey the right meaning. When I code something I will have to maintain for a long time, I try to make it as simple as possible for my future, older, less motivated and weary self.</p><p>The worst codebases are written by people who landed the gig a few months before and do not expect to stay around longer than a year or two.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40819097"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819097" href="https://news.ycombinator.com/vote?id=40819097&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Also nice is when an entire community has agreed to architect a codebase more or less the same. You're basically psychic when that happens.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819275"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819275" href="https://news.ycombinator.com/vote?id=40819275&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>&gt; Any codebase that I had complete control over.</p><p>This is probably one of the pillars of good codebases, or at least decoupling the bits that you don't control as well as you can (this includes external services). I remember needing to write a wrapper around another JWT library, but because it was quite important, I aimed for &gt;95% test coverage, some of the tests acted as documentation for how to use the code, there was also a nice README, there was CI configuration for pushing the build artifacts to Maven and suddenly even managing dependency updates became easy because of the test suite. Years later, it's been integrated in a bunch of services and "just works".</p><p>Come to think of it, things always get less pleasant once you add a bunch of complex dependencies and libraries/frameworks. Need to make a few RESTful Web APIs in Java? Something like Dropwizard will probably give you fewer headaches than Spring (or Spring Boot) and all of its inherent complexity, in the case of the former you might even need to do configuration in XML and that has honestly never been pleasant. If you need to integrate with a bunch of other stuff and want something opinionated, going for Spring Boot will make sense then, but for simple use cases it's overkill. Same for ASP.NET, Ruby on Rails, Laravel and many others, while they might be easier to use, updates (especially across major versions) and breaking changes will give you a headache and just add a bunch of churn to the project.</p><p>Similarly, if you need a message queue, externalizing that into RabbitMQ might make a lot of sense, same with storing files in an S3 compatible store, using something like Redis or Valkey for key-value storage, as well as not trying to shove too much logic into your RDBMS. Just pick whatever tool feels best for the given task at hand, instead of shaping things into what they're not (using the database for blob storage, for example), unless you have a whole bunch of constraints to contend with. Otherwise, sometimes you just get the worst of both worlds, like needing to use Oracle for a project, not having easily launchable local environments (because Oracle XE doesn't support some features), having to share DB instances with others during development and also running into weird obscure issues like DATABASE LINK queries taking 100 times longer in some cases, even when executing the same SELECT query on the remote DB works without issues.</p><p>To not go into a rant, I'd sum it up like this: be in control, isolate the things that you cannot control, pick the correct technologies, do the simplest thing that you can get away with without overengineering and think about the person who'll have to maintain, debug and grow the system in the following months and years (which might also be yourself, sans knowledge about what the code did, if you don't make it explain itself or don't comment the non-trivial stuff).</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40819736"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819736" href="https://news.ycombinator.com/vote?id=40819736&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>The ones that were straightforward and close to the business. It starts at the obvious, works in an obvious way and has comment blocks at hard parts.</p><p>For this reason I despise most modern [web] projects, which have a weak start, immediately drop into “services” and “components”, do one action per source file per 30-50 lines of code, which are mostly imports and boilerplate, and have hundreds of these files. You can never tell what it does because it does almost nothing except formalities.</p><p>I also noted a tendency to use wrong paradigms for a language. E.g. it may have no closures (imagine that in 202x) so they use events as continuations for asynchronicity, which results in a mess. Or it isn’t immutable/functional, but they pretend it is, which results in fragility.</p><p>The best projects are both close to their business and written in a paradigm of the language used.</p><p><i>Was there someone enforcing good practices top down?</i></p><p>Natural time pressure is the best bs cleaner, imo. You write effing code, maybe have few hours a week to refactor strange parts. With no time pressure a project naturally becomes massaged by all members into the “likeable” form of their age.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819135"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819135" href="https://news.ycombinator.com/vote?id=40819135&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>My open source projects. One in particular I've been working on for about 10 years. The code is consistent and always getting better, even though there is a lot more work that could be done.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819550"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819550" href="https://news.ycombinator.com/vote?id=40819550&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I was debugging some issues with Thanos and had pretty good success tweaking the codebase to add additional telemetry.</p><p>The code was fairly well organized and more importantly worked out of the box with a Makefile and IDE integration (GoLand). All it took was `git clone` and opening GoLand to get started.</p><p>For C (maybe it's C++), fluentbit seemed pretty straight forward (I don't have much experience in C though)</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819194"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819194" href="https://news.ycombinator.com/vote?id=40819194&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I haven’t contributed code to it, but I’ve read lots of code in it ( for work reasons ) - I really like the golang compiler and library codebase.</p><p>About codebases I’ve written code for, the best one strived for simplicity, and was driven by very strong engineers who actively considered code hygiene ( in the broadest possible sense ) a first class citizen.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40820282"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40820282" href="https://news.ycombinator.com/vote?id=40820282&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>I can't name a good one, but i have strong
 dislike for big project codebases where
 even finding one file out of thousands where
the relevant code resides is a challenge:
its never something isolated but acts 
like some "component".
The best i can think of is one-person projects
where organization is streamlined as its
actually has to be used by the author,
not like a cog in a giant project.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819305"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819305" href="https://news.ycombinator.com/vote?id=40819305&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I don't think the 'code' side of code base can be considered in isolation.</p><p>What makes a <i>project</i> objectively good (from subjective experience) is a combination of code, design, documentation, and often the humans involved.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819110"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819110" href="https://news.ycombinator.com/vote?id=40819110&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Any code base that doesn't use the advanced features of it's language(s) are always better in my experience. Heavy usage of e.g. meta-programming in Python or perhaps uber's fx (dependency injection) in Go makes projects infinitely harder to get into.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819409"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819409" href="https://news.ycombinator.com/vote?id=40819409&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I worked at GOV.UK for a few years on what was effectively specialised CMSs all written in Rails. Mostly basic CRUD stuff. A contractor came along and built the most insane CMS I've ever seen. I found out later it was flavor of the Java Repository Pattern using dependency injection. It became so difficult to work with that the last thing I worked on there was to delete it and rebuild it using standard Rails patterns.</p><p>The KISS philosophy exists for a reason and that includes over-using advanced language features just to show off.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819510"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40819510" href="https://news.ycombinator.com/vote?id=40819510&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Besides just KISS, a lot of messes I've seen have been implementing patterns outside the framework or implementing complex patterns that didn't add value.</p><p>Besides KISS (or maybe as an extensive), try to keep framework-based codebases as close to the official documented setup as possible. You automatically get to s of free, high-quality documentation available on the Internet.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40822224"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40822224" href="https://news.ycombinator.com/vote?id=40822224&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Props for gov.uk! I’ve looked at its documentation and design system and see both as peak user experience and clarity.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40822758"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40822758" href="https://news.ycombinator.com/vote?id=40822758&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I think one should not use advanced language features just because, but I also think one should not avoid using advanced language features where it is useful.</p><p>Why would the code base be worse when advanced language features are used?</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819218"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819218" href="https://news.ycombinator.com/vote?id=40819218&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Depends who is providing it.</p><p>Django and pydantic meta programming usually make the code easier to deal with.</p><p>In shop written meta programming usually sucks.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819168"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819168" href="https://news.ycombinator.com/vote?id=40819168&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Such a great point. I audibly groan when I come across Python meta-programming.</p><p>While not an advanced feature, I have a similar response when I see lots of decorators in Python. They quickly become a debugging nightmare.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819211"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819211" href="https://news.ycombinator.com/vote?id=40819211&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I can see a few cases where that depends...</p><p>Really simple languages: Ruling out meta-programming is really going to limit you in Lua for example. Just being able to do `mySocket:close()` instead of `Socket.close(mySocket)` involves meta-programming.</p><p>Older languages: For C++ the "simple" features are going to include raw pointers and macros. Maybe it's not so bad to allow smart pointers and templates to avoid those</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819783"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40819783" href="https://news.ycombinator.com/vote?id=40819783&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Both of these examples are examples of an under-programmed core though. Lua is notorious for lacking batteries, so everyone has to reinvent their own. There’s literally no serious Lua program without some sort of classes, but they still resist adding them into lauxlib.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40819457"><td></td></tr>
                  <tr id="40819080"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819080" href="https://news.ycombinator.com/vote?id=40819080&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Generally code that hasn't been tested commercially. Unencumbered by pesky client driven features, just code for dreamt up features that are fun to code but perhaps will never be used.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819010"><td></td></tr>
                <tr id="40819324"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819324" href="https://news.ycombinator.com/vote?id=40819324&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>So true!</p><p>I spend so much time obsessing over how what I am about to write ties in with what has already been written; or fuming over the stupidity of earlier decisions (usually made by myself). A blank slate is incredibly refreshing.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819098"><td></td></tr>
                <tr id="40819327"><td></td></tr>
                        <tr id="40819453"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819453" href="https://news.ycombinator.com/vote?id=40819453&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Ruby on Rails.</p><p>It is the only framework I have read top to bottom.</p><p>Also the FreeBSD kernel, if you want to see a C code base that's quite beautiful (for C).</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819569"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819569" href="https://news.ycombinator.com/vote?id=40819569&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>In this category I would nominate Django as well. It's very well designed (opinionated, but usually for good reasons).</p><p>In terms of large C code bases I enjoy reading the PostgreSQL source code.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40820203"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40820203" href="https://news.ycombinator.com/vote?id=40820203&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>whenever I'm stuck on how to structure some code, I ask myself how would Laravel do it? and look up their code and structure mine similarly</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40819297"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819297" href="https://news.ycombinator.com/vote?id=40819297&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>No name because the project code name changes with each new contract, but the first-level ground processing system for the NRO that turns raw satellite downlinks into some kind of human-intelligible files.</p><p>I think a lot was just the restrictions built into the way development happens required a high level of discipline, care, and planning. Also, requirements were pretty tightly coupled to sensor platform capabilities, which are known well in advance and don't change unexpectedly, so waterfall development actually works and you don't have to deal with the chaos of not really knowing what will and won't work and customers constantly changing their minds.</p><p>Code base was overwhelmingly C++, some Fortran, a lot of it was very old. It was all developed on airgapped networks, and the difficulty of bringing in external dependencies meant there largely were not any. All of the library functionality we required was mostly developed in-house, so we had extremely well-documented and stable functions available to do damn near anything you could want, with a good chance that whoever first wrote it was still there. All development had always been tied to a ticketing system of some sort that included all of the original discussion, design documents, and that kind of thing might add process overhead upfront, but it means that forever new developers can simply read the history and learn exactly why everything works the way it works.</p><p>The system itself was very Unixy. In production, it was meant to be run as a server with many instances striped across high-performance compute nodes, but it did not have to be run that way. Every individual product flow could also be built as its own transient executable, so that working on a single component could easily be done locally. You didn't have to rebuild the world or spin up your own server. Performance requirements were enough that we had our own customized networking stack and filesystem in production, but nothing depended on this for function, so you could still develop and test on plain Linux with ext4.</p><p>The culture was terrific. We were part of one of the big five defense contractors, but an acquisition and this program was largely still staffed by employees of the original company that had been acquired. We were de facto exempted from most of the bullshit any other program had to deal with. I don't know if that was part of the original terms of being acquired or just a consequence of having so many long-time developers that couldn't afford to be lost if you subjected them to nonsense. This was the kind of project that people intentionally stayed on and retired from because the experience was so much better than any other project you could get assigned to.</p><p>Ironically, it had none of the characteristics that high-performing companies often tout. You work in private. The rest of the company, including your own management chain, doesn't even know what you're working on. You'll never get any recognition or publicity. The pay is mediocre. We weren't attracting the best and brightest from all of the world. You had to be American, have a top-secret clearance, and be geographically close enough to the facility to get there every day, so this was a pretty constrained hiring pool. I still worked with some of the smartest people and best engineers I've ever known. The upside of this kind of environment is you have no mercenaries or publicity hounds. Everyone who sticks around is a person who really loves and cares about what they're working on, and a lot of people did stick around. The sanity and organization of the code was heavily facilitated by having a whole lot of people working on it who'd been working on it for 30+ years.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819180"><td></td></tr>
            <tr id="40819267"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819267" href="https://news.ycombinator.com/vote?id=40819267&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Laravel, once you get the hang of it everything just works, and using a debug bar to optimize database calls is very satisfying.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40821046"><td></td></tr>
            <tr id="40819100"><td></td></tr>
            <tr id="40819131"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819131" href="https://news.ycombinator.com/vote?id=40819131&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Every code base I have ever worked on was a legacy nightmare. Every "greenfield" project I joined turned into a legacy nightmare within weeks. I have never encountered enjoyable code. I had the displeasure of wading through Spring, Hibernate and Apache HTTP client code before and they were all an incomprehensible mess.</p><p>My conclusion: You know the claim "any medication that really has an effect must also have side effects". I would like to adapt that for code: Any code that does a lot of useful and complex things must be an arcane, barely maintainable mess that can only be understood by deep study.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819485"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819485" href="https://news.ycombinator.com/vote?id=40819485&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Something in your answer triggered a flash back to when I worked for a phone company, 20 years ago. One team, under the leadership of our questionable chief architect, had produced our new, all encompassing backend system. This was to be the corner stone of all future development and integration, dog slow and complicated as it where. I worked as a .Net developer and had the misfortune to be among the first to integrate with this monster. Try as I might, I could not get .Net to interoperate with these services. Finally I figured out that the SOAP services was using some old deprecated versions. Going back to the architect, I ask "did you build this on Apache Axis, and please say Axis2", but no, it was just Axis, a deprecated version, that would generate webservices not supported by newer .Net version. That wasn't an problem, because: "None of our project have upgraded to those .Net versions yet"... DUDE, we've launched a brand new system based on that .Net version a year ago, and what was you f-ing plan for the future, to redo every single service using Axis2?</p><p>This guy had based a brand new system on a framework/library that was no longer maintained, even before the system was launched.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819334"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819334" href="https://news.ycombinator.com/vote?id=40819334&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>IMHO library code is especially challenging, as cruft has a tendency to accumulate, historical behavior needs to be preserved, and APIs are set in stone once they’re built.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819418"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819418" href="https://news.ycombinator.com/vote?id=40819418&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>"Any code that does a lot of useful and complex things must be an arcane, barely maintainable mess that can only be understood by deep study."</p><p>"Every code base I have ever worked on was a legacy nightmare. Every "greenfield" project I joined turned into a legacy nightmare within weeks"</p><p>I am sorry to say this, but it really sounds like you were either really, really unlucky, or part of the reason why it became a mess.</p><p>Complicated things are complicated. Nothing can ever change that and it requires study to understand it.</p><p>But it still does matter a lot, how one organizes the whole thing. How it is structured, documented(!), refactored. Are there competent people in charge who understand it all and kick peoples asses if they make even a temporary mess or forget to document, or do random people make changes wherever they see fit, because a deadline is ticking?</p><p>Modularisation is usually the key. Small modules do one thing and are as seperated as much as possible with as little side effects as possible.</p><p>And if one has to ship things, it is not always possible to keep it pure and if the code is not intended to live forever then this is often fine. But if the codebase is supposed to stay, then there needs to be the time to clean up the hacks. Or don't and then you end up scared touching anything.</p><p>That being said, the technologies you mentioned, I would not like to touch either..</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819494"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40819494" href="https://news.ycombinator.com/vote?id=40819494&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I think that's a bit cruel. Usually mess happens because lots of people with different ideas about the future needs and best structure meet up in the code - and it's hard to develop a consistent culture.</p><p>Also codebases get too large for any one person to refactor them into shape in the time they have each day. So you end up needing people who are responsible just for keeping things in shape.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819611"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40819611" href="https://news.ycombinator.com/vote?id=40819611&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Well, that is why I said with those words it <i>sounds</i> like this to me, not that he or she <i>was</i> in fact responsible. (at least this is what I meant)</p><p>"Also codebases get too large for any one person to refactor them into shape in the time they have each day."</p><p>Which is why modules, or subprojects were invented. Or however you want to call it, if one person is only responsible for a small part and not everyone for everything. And yes, there is also the non trivial problem of time management.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819704"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_40819704" href="https://news.ycombinator.com/vote?id=40819704&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>yup, but the style of the modules starts to diverge as different people maintain them.</p><p>IMO one does want people whose fulltime job is to looking at a codebase  orthogonally to those who are just implementing a feature. People who make sure it builds fast, is secure tries to be consistent to some degree etc.</p><p>Many companies call teams/indivuduals like this a "cost centre" and disparage it because they are dolts.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40823366"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_40823366" href="https://news.ycombinator.com/vote?id=40823366&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>If those code checkers are experienced programmers, who do not annoy people with arbitary guidelines and standards, then yes, this might also work. But I do not think they are always necessary as a seperate full time role, if everything works normal otherwise. And with working normal, I mean there is enough time to refactor and document and clean up and is actually done.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                                          <tr id="40819108"><td></td></tr>
            <tr id="40819477"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819477" href="https://news.ycombinator.com/vote?id=40819477&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I just cannot face the thought of calling any of them "best".</p><p>Every one had good and bad features though. One or two were OS-sized and I think a codebase that compiles and links to 85GB of output for 20+ devices without being a total disaster inside is harder to do than a neat small python module or whatever.</p><p>GOOD FEATURES:</p><p>Maintenance of the build and test: 
I worked on tools that helped builds go faster so I saw a lot of codebases where people were not maintaining the build partly because nobody had that a s a responsibility. There was bad management of dependencies leading to build failures, poor performance, incorrect build output.  Android would be a counter example to that - I don't know if people like developing in it but it was always hard to accelerate it as the maintainers fixed performance problems regularly leaving our tools with little to improve.</p><p>Using appropriate languages.  Writing everything in C++ was a fad at one time. All projects work better, port better, have faster build times, are easier to test etc if they use memory safe "build once" languages to a maximum (e.g. java) and unsafe ones (e.g. C/C++ which have to be rebuilt and tested for each device/os) to a minimum. IMO Android beat Symbian amongst other reasons because it wasn't all C/C++ and that meant a lot of code didn't have to be rebuilt for different devices.  This made builds faster and fast builds lead to better quality because of a short dev-test cycle.</p><p>Use of faster compilers over "better" compilers. Ultimate code performance and quality depends on a fast development cycle more (IMO) than on having the most optimizing compiler. GCC versus the older ARM compilers for example.  Now the ARM compiler is based on LLVM and I know that happened indirectly from a suggestion I made to someone who then made it to ARM who then did it.</p><p>The setup and build of one codebase I worked on was as easy as one could expect, the build errored out if you tried to use the wrong tools so you never ended up debugging weird failures because of an incorrect tool in your PATH somewhere. I made this feature happen :-D. With big codebases the tools could be included in the version control system so you knew you had the right compiler, right object dumper etc.  This is another strength of Android and yet I was in a project for Symbian to do the opposite because of some utter bonehead who never touched a build in his life who was trying to make a name for himself with his slimy bosses as a "doer" and "reformer."</p><p>Codebases (especially big ones) benefit a lot from some kind of codesearch/index where you could find out where some function/class/variable was defined and what version of the source base it was introduced in.</p><p>BAD FEATURES:</p><p>Exclusively Owned code - we need to know who understands code best and who is best to review it but I don't think anyone should have totally exclusive control. It was a nightmare for me at one job - trying to get another team to make some needed change (like fixing their stupid makefiles to work properly in parallel).  We (build team) should have been able to do it ourselves - maybe including them in the PR.  Sometimes ownership is entirely theoretical - nobody who wrote it is still employed and nobody among the notional owners understands it and none of them want to approach it within 100 metres in case it blows up and becomes their problem.  I simply <i>had</i> to approach such code - no choice - but I kept having to send diffs to people who didn't want to bother to look at them. It was a case of pushing wet spaghetti and took forever to do very simple changes.</p><p>Insufficient tests that run infrequently. What else is there to say?</p><p>Complicated code with no "why" or "what this is for" type comments.   The kind of thing you trawl around in for weeks and cannot make head nor tail of what is going on overall.</p><p>Code with so much dependency injection and general SOLID that you have to bounce all over the place to understand a very simple action.</p><p>Code where writing tests is an enormous ballache.  In one Go codebase the reason was because somone decided that the standard Go practise of an array of test data being run through a kind of "test engine" was the only way anyone should be allowed to write tests.  Hence you had to do lots of weird things to make your test cases into data.  Generally we use a kind of "religious" approach to try to get consistency out of a group of people but then take it much too far.</p><p>codebases without automated reformatting - so everyone wastes time arguing about line spacing or camel-case names or whatever in their PRs.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40820509"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40820509" href="https://news.ycombinator.com/vote?id=40820509&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>&gt; Code with so much dependency injection and general SOLID that you have to bounce all over the place to understand a very simple action.</p><p>I find that happens when people get religious about patterns and methodology; without understanding the "why", the language, and how a computer works.</p><p>Case in point: I once worked on a C# project that used a port of Spring for dependency injection: Ultimately, it was near impossible to know when something was constructed, and what was calling what. There were classes that couldn't call themselves through "this" because of certain weird dependency injection features used.</p><p>Later, I decided to use dependency injection as a design pattern: Instead of a complicated DI framework, there was just a few files of code. It was very easy for newcomers to understand. It was also easy to swap in mock objects, and easy to swap dependencies based on the target platform. It was also easy to see when a dependency was constructed; because it wasn't hidden behind a giant framework.</p></div></td></tr>
        </tbody></table></td></tr>
                            <tr id="40819191"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819191" href="https://news.ycombinator.com/vote?id=40819191&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>I worked with a very experienced engineer who created his own WAF (software-based) using Java.
From a purely architectural perspective, it might not be the best (I might have used an ISAPI filter back then), but the code itself was very efficient, well-written and documented. I used it several times as a teaching example.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            </tbody></table>
  </td></div></div>]]></description>
        </item>
    </channel>
</rss>