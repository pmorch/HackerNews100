<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 18 Jun 2025 21:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Andrej Karpathy's YC AI SUS talk on the future of the industry (191 pts)]]></title>
            <link>https://www.donnamagi.com/articles/karpathy-yc-talk</link>
            <guid>44311509</guid>
            <pubDate>Wed, 18 Jun 2025 16:56:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.donnamagi.com/articles/karpathy-yc-talk">https://www.donnamagi.com/articles/karpathy-yc-talk</a>, See on <a href="https://news.ycombinator.com/item?id=44311509">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://www.donnamagi.com/">back</a></p><p>This is a transcript of Andrej Karpathy's talk on Software 3.0 on June 17th. I was in the audience, and it was recorded in a noisy environment - set your expectations accordingly.</p><p>YC has said the official video will take a few weeks to release, by which Karpathy himself agrees the talk will be deprecated.<!-- --> <a href="https://x.com/karpathy/status/1935077692258558443" target="_blank" referrerpolicy="origin">https://x.com/karpathy/status/1935077692258558443</a></p><p>and ok wow, this is going viral. let's<!-- --> <a href="https://x.com/DonnaMagi" target="_blank" referrerpolicy="origin">keep in touch</a>?</p><p>I think it's actually an extremely unique and very interesting time to enter the industry right now. And I think fundamentally the reason for that is that software is changing. Again. And I say again because I actually gave this talk already. But the problem is that software keeps changing, so I actually have a lot of material to create new talks. And I think it's changing quite fundamentally. I think broadly speaking, software has not changed much at such a fundamental level for 70 years. And then it's changed, I think, about twice quite rapidly in the last few years. And so there's just a huge amount of work to do, a huge amount of software to write and rewrite.</p><p>So let's take a look at maybe the realm of software. So if we're going to think of this as like a map of software, this is a really cool tool called Map. This is kind of like all the software that's written. These aren't instructions. Computer for clearing out tasks in digital space. So if you zoom in here, these are all different kinds of repositories. And this is all the code that has been written. And a few years ago, I kind of observed that software was kind of changing and there was kind of like a new type of software around. And I called this Software 2.0 at the time. And the idea here was that Software 1.0 is the code you write on the computer. Software 2.0 are basically neural networks. In particular, the weights of the neural network. And you're not writing this code correctly. You're more like tuning the data sets and then you're running an optimizer to create the parameters. And I think at the time, neural networks were kind of seen as just a different kind of classifier, like a decision tree or something like that. And so I think this training was a lot more appropriate.</p><p>And now actually what we have is kind of like an equivalent of GitHub in the realm of Software 2.0. And I think the hugging face is basically an equivalent of GitHub in Software 2.0. And there's also a model atlas and you can visualize all the code written there. In case you're curious, by the way, the giant circle, the point in the middle, these are the parameters of Flux, the image generator. And so any time someone tunes a lower on top of a Flux model, you basically create a GitHub event in this space and create a different kind of image generator. So basically what we have is Software 1.0 is the computer code that programs the computer. Software 2.0 are the weights which program neural networks. And here's an example of AlexNet image recognizer neural network. Now so far, all of the neural networks that we've been familiar with until recently were kind of like fixed-functional computers. Image categories or something like that. And I think what's changed, and I think it's a fundamental change, is that neural networks became programmable with large libraries. And so I see this as quite new, unique. It's a new kind of computer. And in my mind, it's worth giving it the designation of a Software 3.0. And basically your products are now programs that program people all over. And remarkably, these products are written in English. So it's kind of a very interesting programming language.</p><p>So maybe to summarize the business, if you're doing central classification, for example, you can imagine writing some Python to basically do central classification. Or you can train neural networks. Or you can drop a large amount of code. So here I'm just using SoftPrompt, and you can imagine changing it and programming the computer's life. So basically we have Software 1.0, Software 2.0. And I think we're seeing, I mean, you've seen a lot of GitHub code is not just like code anymore. There's a bunch of English interspersed with code. And so I think there's a growing category of that kind of code. So not only is it a new programming paradigm, it's also remarkable to me that it's in our native language of English. And so this blew my mind a few years ago now. I tweeted this, and I think it captured the attention of a lot of people. And one of the things that I currently pinpoint to them is that arguably we're not programming computers in English.</p><p>Now when I was at Tesla, we were working on the Autopilot, and we were trying to get the car to drive. And I sort of showed this slide at the time where you can imagine that the inputs for the car are on the bottom, and they're going through the software stack to produce the steering and acceleration. And I made the observation at the time that there was a ton of C++ code around in the Autopilot, which was the software model development. And that there were some neural nets in there doing the interactions. And I kind of observed that over time as we made the Autopilot better, basically the neural network grew in capability and size. And in addition to that, all this C++ code was being completed. And a lot of the capabilities and functionality that was originally in 1.0 was migrated to 2.0. So as an example, a lot of the stitching up of information across images from the different cameras across time was done by neural network, and we were able to delete a lot of code. And so the software development stack was quite literally made through the software stack of the Autopilot. So I thought this was a brilliant model at the time. And I think we're seeing the same thing again, where basically we have a new kind of software, and it's being through the stack, made through a completely different programming paradigm.</p><p>And I think if you're entering the industry, it's a very good idea to be fluent in all of them. Because they all have slight pros and cons, and you may want to program some functionality in 1.0 or 2.0, or 3.0, or you're going to train in LLM, or you're going to just run from LLM, that shouldn't be any software that's explicit, etc. So we don't have to make these decisions to actually potentially fluidly transition to LLMs.</p><p>So what I want to get into now is, first I want to, in the first part, talk about LLMs, and what I think of this new paradigm in the ecosystem, and what that looks like. What is this new computer? What does it look like? And what does the ecosystem look like? I was struck by this quote from Andrew, actually many years ago now, I think. And I think Andrew is going to speak right after me. But he said that the term AI is probably not his thing. And I do think that it captures something very interesting, in that LLMs certainly feel like they have properties of utilities, right? So, LLM labs, like OpenAI, Gemini, Fungi, etc, they spend time to train the LLMs, and this is kind of equivalent to a built-in AI algorithm, and then there's op-ecs to serve them intelligence over APIs to all of us. And this is done through internet access, where we pay per million tokens or something like that, and we have a lot of demands that are very utility-like demands out of this API. We demand low latency, high uptime, etc.</p><p>In electricity, you would have a transfer switch, so you can transfer your electricity source from, like, grid, solar, or battery, or generator. In LLMs, we have maybe open router, and easily switch between the different types of LLMs that exist. Because the LLMs are software, they don't need more physical space, so it's okay to have, basically, like six electricity providers that you can switch between, right? Because they don't need this directly. And I think what's also really fascinating, and we saw this in the last few days, actually, a lot of the LLMs went down, and people were kind of stuck and unable to work.</p><p>And I think it's kind of fascinating to me that when the state-of-the-art LLMs go down, it's actually kind of like an intelligence brownout in the world. It's kind of like when the voltage is unreliable on the grid, and the planet just gets smaller. The more reliance we have on these models, which already is, like, really dramatic, and I think will continue to grow. But LLMs don't only have case of utilities. I think it's also fair to say that they have some power tools called CAPs. And the reason for this is that the CAPs required for building LLMs is actually quite large. It's not just like building some power station or something like that, right? You're investing a huge amount of money, and I think the technology is growing quite rapidly. So we're in a world where we have, sort of, deep tech trees, research and development, secrets, that are centralizing our shiny LLM labs. But I think the analogy varies a little bit also, because, as I mentioned, this is software. And software is a bit less expensive, but because it is so valuable. And so I think that's just an interesting kind of thing to think about.</p><p>There's many analogies you can make, like a formatted data process, and maybe, for instance, something like a cluster with certain max-ops. And you can think about, when you're using it, you actually use it only through the software, not through the hardware. That's kind of what the CAPs are. But if you're actually also building it on hardware, and you're sharing it using Google, that's kind of like an integral on your platform. So I think there's analogies here that make sense. But actually, I think the analogy that makes the most sense, perhaps, is that, in my mind, LLM's have very strong analogies to operating systems, in that this is not just electricity or power. It's not something that comes automatically to happen. It's a commodity. These are now increasingly complex software ecosystems. So they're not just simple commodities like electricity. And it's kind of interesting to me that the ecosystem is shaping in a very similar kind of way, where you have a few closed-source providers, like Windows and macOS, and then you have an open-source alternative, like Linux. And I think for LLM's as well, we have a few competing closed-source providers. And then maybe the LLM ecosystem is currently maybe a close approximation to something that may grow into something like Linux. Again, I think it's still very early, because these are just simple LLMs. And I'm starting to see that these are going to get a lot more complicated. It's not just about the LLM itself, but it's about the tool use, the full-time analogies, how all that works. And so when I sort of have this realization about that, I try to sketch it out. And it kind of seems to me like LLMs are kind of like a new operating system, right? So the LLM is a new kind of computer. It's kind of like a CPU equivalent. The context windows are kind of like the memory. And then the LLM is orchestrating memory and compute for problem solving using all these capabilities. And so definitely, if you look at it, it looks very much like software. from that perspective.</p><p>A few more analogies, for example, if you want to download an app, say I go to VS Code and I go to Download, you can download VS Code and you can run it on Windows, Linux, or Mac, in the same way as you can take an LLM app, like a cursor, and you can run it on GBT, or Blob, or JPEG streams, right? It's just a drop-in. So it's kind of like similar in that way as well. The more analogies that I can describe to you is that we're kind of like in this 1960s-ish era, where LLM compute is still very expensive for this new kind of a computer, and that forces the LLMs to be centralized in the cloud, and we're all just sort of fake clients that interact with it over the network, and none of us have full utilization of these computers. And therefore, it makes sense to use timesharing, where we're all just, you know, at the mission of the batch, when they're running the computer in the cloud. And this is very much what the computers used to look like. During this time, the operating systems were in the cloud, everything was streamed around them, and they were batching. And so the personal computing revolution hasn't happened yet, because it's just not that common, and it doesn't make sense, but I think some people are trying. And it turns out that Mac minis, for example, are a very good fit for some of the LLMs, because it's all purely batch-run inference, this is all super-memory-bound, so this actually works. And I think these are some early indications that you have personal computing, but this hasn't really happened yet, and it's not clear what this looks like. Maybe some of you guys are interested in talking about what this is, or how it works, or what it should be.</p><p>Maybe one more analogy that I'll mention is, whenever I talk to ChartsQt or some LLM directly in text, I feel like I'm talking to an operating system through the terminal. Like, it's text, it's direct access to the operating system, and I think a GUI hasn't yet really been invented in a general way, like ChartsQt had a GUI, but different than just a text box. Certainly some of the apps that we're going to go into in a bit have GUI, but there's no GUI across all the tasks and devices. There are some ways in which LLMs are different from operating systems in some kind of unique way, and from early computing. And I wrote about this one particular property that strikes me as very different. It's that LLMs, they flip the direction of technology that is usually present in technology. So for example, with electricity, the economy is getting quite expensive, and lots of new transformative technologies have not been around, particularly in government-owned corporations that are the first officers, because it's new and expensive, etc. And only later did they fix it to consumers. But I feel like LLMs are kind of what flipped around. So maybe with early computers, it was all about ballistics and military use, but with LLMs, it's all about up the oil bank or something like that. This is certainly like a lot of minds.</p><p>And so it's really fascinating to me that we have a new magical computer, and it's like helping the oil bank. It's not helping the government to do something really crazy like some military ballistics or some special technology. Indeed, corporations are now getting the lagging, apparently not, of all of us, of all these technologies. So it's just backwards. And I think it informs me in some of the uses of how we want to use this technology, or like where I saw the first apps in the store.</p><p>So in summary so far, LLMs, app LLMs, I think it's accurate language to use. LLMs are complicated operating systems. They're circa 1960s computing, or we do computing already. And they're currently available via time-sharing and distributed like a utility. What is new and unprecedented is that they're not in the hands of a few governments and corporations. They're in the hands of all of us, because we all have a computer, and it's all just software. And Chachapitibos leans down to our computers that collect billions of people like this and play it overnight. And this is insane. And it's kind of insane to me that this is the case. And now it is our time to enter the industry and program these computers. It's crazy.</p><p>So I think this is a better part. Before we program LLMs, we have to kind of like spend some time to think about what these things are. And I especially like to talk about their psychology. So the way I like to think about LLMs is that they're kind of like little spirits. They are stochastic simulations of, and the simulator in this case happens to be an autoregressive transformer. So a transformer is a neural network. And it just kind of like goes from level of focus, it goes chunk, chunk, chunk, chunk, chunk. And there's an almost equal amount of compute for every single chunk. And this simulator, of course, is just, it's basically there's some weights involved and we fit it to all the text that we have on the internet and so on. And you end up with this kind of a simulator. And because it is trained in humans, it's got this emergent psychology that is human-like.</p><p>So the first thing you'll notice is, of course, LLMs have this type of deep knowledge and memory. And they can remember lots of things, a lot more than any single individual can because they've read so many things. It actually kind of reminds me of this movie Rain Man, which I actually really recommend people watch. It's an amazing movie. I love this movie. Dustin Hoffman here is an autistic sponge who has almost perfect memory. So he can read like a notebook and remember all of the names and phone numbers. And I kind of feel like LLMs are kind of like very similar. They can remember shock hashes and lots of different kinds of things, very, very easily. So they certainly have superpowers and stuff in some respect, but they also have a bunch of, I would say, cognitive deficits. So they hallucinate quite a bit and they kind of make up stuff and don't have a very good sort of internal model of self-knowledge, but not sufficient at least. And this has gotten better, but not perfect. They display jagged intelligence. So they're going to be superhuman in some problem-solving ways, and then they're going to make mistakes that basically no human will make, like, you know, it will insist that 9.11 is greater than 9.9, or that there are two bars of strawberry. These are some famous examples. But basically there are rough edges that you can trip on. So that's kind of, I think, also kind of cool. They also kind of suffer from internal brain amnesia. So, and I think alluding to the fact that if you have a coworker, which is your organization, this coworker will, over time, learn your organization and they will understand and gain, like, a huge amount of context on the organization and they go home and they sleep and they consolidate knowledge and they build expertise over time. LLMs don't natively do this. This is not something that has really been solved in R&amp;D with LLMs by them. And so context windows are really kind of like a working memory that you have to sort of program the working memory quite directly because they don't just kind of, like, get borrowed by people. And I think a lot of people get tripped up by analogies in this way. In popular culture, I recommend people watch these two movies, Memento and First Dates. In both of these movies, the protagonists, their ways are mixed, and their context windows get wiped every single morning. And it's really problematic to go to work or have relationships when this happens. And this happens to a lot of us all the time.</p><p>I guess one more thing I would point to is security-related limitations on the use of LLMs. So, for example, LLMs are quite vulnerable. They are susceptible to conflict-injection risks. They might leak your data, etc. And there's many other considerations security-related. So, basically, long story short, you have to simultaneously think through this superhuman thing that has a bunch of cognitive deficits and issues. How do we enhance them? They are extremely likely usable. And so how do we program them? And how do we work around their deficits and toy with their superhuman powers?</p><p>So what I'm going to switch to now is talk about the operators. How do we use these models? And what are some of the biggest operators? This is not a comprehensive list of some of the things that I thought were interesting in this stuff. The first thing I'm kind of excited about is what I would call partial autonomy apps. So, for example, let's work with the example of coding. You can certainly go to chat.gt directly, and you can start copy-pasting code around, and copy-pasting awkward words and stuff around, and then code, and copy-pasting everything around. Why would you do that? Why would you go directly to the operators? It makes a lot more sense to have an app dedicated for this. And so I think many of you use Cursor. I do as well. And Cursor is kind of like the thing you want instead. You don't want to just directly go to the chat.gt. And I think Cursor is a very good example of an early LLM app that has a bunch of properties that I think are useful across all the LLMs. So in particular, you will notice that we have a traditional interface that allows a human to go in and do all the work manually, just as before. But in addition to that, we now have this LLM integration that allows us to go in bigger chunks. And so some of the properties of LLM apps that I think are shared are useful. Number one, the LLMs basically do a ton of good context management. Number two, they orchestrate multiple calls to LLMs. So in the case of Cursor, there's under-the-hood eventing models for all your files, the actual chat models, models that apply ifs to the code, and this whole orchestra is for you. A really big one that I think also may be not fully appreciated in all this is application-specific and the importance of it. Because you don't just want to talk to the operating system directly in text. Text is very hard to read, interpret, understand, and also you don't want to take some of these actions natively in text. So it's much better to just see the bit as like red and green change, and you can see what's the matter of subtracting. It's much easier to just do command Y to accept or command N to reject. You don't have to type it in text. So it really allows the human to audit the work of these fabulous systems and to grow faster. We're going to come back to this in a little bit later as well. And the last kind of feature I want to point out is that there's what I call the autonomous slider. So for example, in Cursor, you can just do calculation. You're mostly in charge. You can select a chunk of code and command K to change a static chunk of code. You can do Command L to change this entire file, or you can do Command I, which just, you know, for better or worse, it packages up a lot of things, makes sure that it orchestrates people all at once. It's got a GUI that allows you to audit some of its work. So, for example, it will cite sources that you can imagine inspecting them, and it's got an autonomy slider. You can either just do a quick search, or you can do research, or you can do deep research and come back to all this later. So this is all just very well-structured, automated, and optimized.</p><p>So I guess my question is, I feel like a lot of software will become partially autonomous. And I'm trying to think through, like, what does that look like? In fact, many of you maintain products and services. How are you going to make your products and services partially autonomous? Can an LLM see everything that a human can see? Can an LLM act in all the ways that a human can act? And can humans supervise and stay in the loop of this activity? Because, again, these are allowable systems that aren't yet perfect. And what does the diff look like on Photoshop? There's a lot of things we don't know. And also, a lot of the traditional software right now has all these switches and all this kind of stuff. It's all designed for people. All this has to change and become accessible to LLMs.</p><p>So one thing I wanted to stress with a lot of these LLM apps that I'm not sure gets as much attention as it should, is LLM. We're now kind of, like, cooperating with the apps. And usually, they are doing a generation, and we assume this sort of verification. It is in our interest to make this move as fast as possible, so we're getting a lot of work. There are two major ways that I think this can be done. Number one, you can speed up verification a lot. And I think GUIs, for example, are extremely important for this, because a GUI utilizes your computer vision GPU in all of our head. Reading text is effortful, and it's not looking at stuff. It's on a headset. It's just a, like, a highway to your brain. So I think GUIs are very useful for auditing systems and visual representations in general. And number two, I would say is, we have to keep the AI in a leash. I think a lot of people are getting way overexcited with AI engines, and it's not useful to me to get the diff of 1,000 lines of code into my repo. Like, I have to, I'm still the bottom line, right? Even though the 1,000 lines come out instantly, I have to make sure that this thing is not introducing bugs. It's just like, and that it's doing the correct thing, right, and that there's no security issues and so on. So I think that, yeah, basically, you have to sort of, like, it's in our interest to make the flow of these two go very, very fast, and we have to somehow keep the AI in a leash because it gets way too overactive. It's kind of like this. This is how I feel when I do AI-assisted coding. If I'm just live coding, everything is nice and great, but if I'm actually trying to get work done, it's not so great to have an overactive engine doing all this kind of stuff. So this slide is not very good, I'm sorry, but I guess I'm trying to develop, like many of you, some ways of utilizing these engines in my career to look at AI-assisted coding, and if I don't work, I'm always scared to get way too big bits. I always go with small incremental chunks. I want to make sure that everything is good. I want to spin this thing very, very fast, and I sort of work on small chunks of single-token thing, and so I think many of you probably have a little bit similar ways to work with algorithms.</p><p>I also saw a number of blog posts that try to develop these best practices for working with algorithms, and here's one that I recently came across that's quite good, and it kind of discusses some techniques, and some of them have to do with how you keep the AI on a leash. And so as an example, if you're on prompting, if your prompt is vague, then the AI might not do exactly what you want it, and in that case, the verification will fail. You're gonna ask for something else. If the verification fails, then you're gonna start spinning. So it makes a lot more sense to spend a bit more time to be more complete in your prompts, which increases the probability of a successful verification, and you can move forward. And so I think a lot of us are gonna end up finding techniques like this.</p><p>I think in my own work as well, I'm very interested in what education looks like, and together with, now that we have an AI, and a lens for what does education look like, and I think a large amount of thought for me goes into how we keep AI on a leash. I don't think it just works to go through trashy ATM, you know, like the aging business. I don't think this works, because the AI is like, it gets lost in the woods. And so for me, this is actually two separate apps, for example, there's an app for a teacher that creates courses, and then there's an app that takes courses and serves them to students. And in both cases, we now have this intermediate artifact of a course that is auditable, we can make sure it's good, we can make sure it's consistent, and the AI is kept on a leash with respect to a certain syllabus, a certain progression of projects, and so on. And so this is one way of keeping AI on a leash that I think has a much higher likelihood of working. And the AI is not getting lost in the woods.</p><p>One more kind of methodology I wanted to sort of allude to is I'm not a most major to partial autonomy, I've kind of worked on this, I think, for five years for Tesla, and this is also a partial autonomy product that shares a lot of features, like for example, right there, the instrument panel has the weight of the product, so it's showing me what the neuron sees, and so on. And then the autonomy slide, where over the course of my tenure there, we did more and more autonomous tasks for Google News. And maybe the story that I wanted to tell very briefly is, actually the first time I drove a self-driving vehicle was in 2013, and I had a friend who works at Rainbow, and he offered to give me a drive around Palo Alto. I took this picture using a moving bus at the time, and many of you are so young that you might not even know what that is, but yeah, this was like all the rage at the time. And we got into this car, and we went for about a 40-minute drive around Palo Alto, and the highways, the streets, and so on, and this drive was perfect. There was zero traffic issues. And this was in 2013, which is now 12 years ago. And it kind of struck me, because at the time when I had this perfect drive, it was a perfect gift, I felt like, wow, self-driving was imminent, because this just worked, this is incredible. But here we are 12 years later, and we are still working on the tunnel. We are still working on the driving engines. And even now, we haven't actually like fully solved the problem. Like, you may see way-bos going around, and they look driverless, but there's still a lot of tool operation, and a lot of human input of what was driving. So we still haven't even like declared success, but I think it's definitely like going to succeed at this, but it just took a long time. And so I think, to me, what I found there is that there's a very large, what I call network-to-product gap that people don't intuitively always understand very well. And look, if you imagine works as like this binary array of a different situation, of like what works and doesn't work, then that is worse than many in products, in like works at all. In the sense that, if anything works, you can make demos. But in many cases, lots of things must work, but very few people are new to the product. And this is especially the case in high-reliability areas. Not all the areas are like this, but for sure in high-reliability cases, it means. And I would say autonomy, of course, because like this thing is going to crash, which would be bad. But I would also say software is like this. If you make a single mistake in software, we know that there could be a code path that's going to break, or it's going to introduce a security issue, or a zero-day, or something like that. Like, look, software is really tricky, I think, in the same way that driving is tricky. And so when I see things like, oh, 2035 is the year of agents, I get very concerned, and I kind of feel like, you know, this is the decade of agents, and this is going to be, by some time, you do this carefully, and this is software. Let's be serious here, okay?</p><p>One more kind of analogy that I always think through is the Iron Man suit. I think this is, I always love Iron Man, I think it's like so correct in a bunch of ways, with respect to technology and how it will play out. And what I love about the Iron Man suit is that it's both an augmentation, and twin-star mechanic driver. And it's also an agent, and in some of the movies, the Iron Man suit is quite autonomous, and you can fly around, climb trees, and all this kind of stuff. And so this is the autonomy sluggish, it can be, we can build augmentations, or we can build agents, and we kind of want to do a bit of both, but at this stage, I would say, working with Palo Alto LMS itself, I would say, you know, it's less Iron Man robots, and more Iron Man suits that you want to build. It's less, like, building flashy demos of autonomous agents, and more, building partial autonomy products. And these products affect who you speak, and we're trying to, and this is done so that the generation verification of the human is very, very fast. But we are not losing the sight of the fact that it is, in principle, possible to automate this work, and there should be an autonomy slider in your product, and you should be thinking about how you can slide that autonomy slider, and make your product sort of more autonomous over time. But this is kind of how, I think there's lots of opportunities in these kinds of products.</p><p>I want to now switch gears a little bit, and talk about one other dimension that I think is very important. Not only is there a new type of programmer language that allows for autonomy in software, but also, as I mentioned, it's programmed in English, which is this natural interface. And suddenly, everyone is a programmer, because everyone speaks natural language, like English. So this is extremely bullish, and very interesting to me, and also completely unprecedented, I would say. It used to be the case that you need to spend five to 10 years studying something to be able to do something that software can do. This is not the case anymore.</p><p>So I think that, by chance, anyone has heard of IBM? Okay. This is the tweet that introduced this, but I'm told that this is now a major meme. A story about this is that, I've been on Twitter for 15 years, or something like that, at this point, and I still have no clue which tweet will become viral, and which tweet this will send over the years. And I thought that this tweet was going to be, I'm going to just have a shower of thoughts, but this became a total meme, and I really just can't tell, but I guess it's working. Or even name something that everyone is calling, but can apply it to the same words. Now they're speaking Yiddish and everything. This is life. Yeah, this is like a major contribution now or something like that, so.</p><p>So Tom Wolfe from Hugging Feet shared this beautiful video that I really love. These are kids vibe coding. And I find that this is such a wholesome video, like, I love this video. Like, how can you look at this video and feel bad about the future? The future is great. I think this will end up being like a gateway drug to software development. I'm not a doer about the future of the generation, and I think, yeah, I love this video. So, I tried vibe coding a little bit as well, because it's so fun. So, vibe coding is so great when you want to build something super duper custom that doesn't appear to exist, and you just want to wing it because it's a Saturday or something like that. So, I built this iOS app, and I built into the, I can't actually program it in Swift, but I was really shocked that I was able to build a super basic app, and I'm not going to explain it, that's really dumb. But, I kind of like, this was just like a day of work, and this was running on my phone like later that day, and I was like, wow, this is amazing. I didn't have to like leave from Swift for like five days or something like that to like get started. I also vibe coded this app with a menu gem, and this is a lot, you can try a menu gem on that. And, I basically have this problem where I show up at a restaurant, I reach for the menu, and I have no idea what any of the things are, and I need pictures. So, this doesn't exist, so I was like, hey, I'm going to vibe code it. So, this is what it looks like. You go to menu gem, that app, and take a picture of the menu, and then menu gem generates the images. And, everyone gets five dollars in credits for free when you sign up, and therefore, this is a major cost center in my life. So, this is a negative revenue app for me right now. I lost a huge amount of money on menu gem. Okay.</p><p>But, the fascinating thing about menu gem for me is that the code, well, the vibe coding part, the code was actually an easy part of vibe coding menu gem. And, most of it actually was when I tried to make it real so that you can actually have authentication, payments, domain name, and personal deployment. This was really hard, and all of this was not code. All of this DevOps stuff was me and the browser clicking stuff. And, this was an extreme spot into another room. So, it was really fascinating that I had the menu gem basically demo working on my laptop in a few hours, and then it took me a week because I was trying to make it do it. And, the reason for this is this was just really annoying. So, for example, if you try to add Google log into your webpage, I know this is very small, but just a huge amount of instructions of this important library telling you how to integrate this, and this is crazy, like it's telling me, go to this URL, click on this dropdown, choose this, go to this, and click on that, and it's like telling me what to do. Like, the computer is telling me the actions I should be taking, like, you do it. What do I do? What the hell? I had to follow all these instructions. This was crazy.</p><p>So, I think the last part of my talk, therefore, focuses on can we just build for agents? I don't want to do this work anymore. Thank you. Okay.</p><p>So, roughly speaking, I think there's a new category of consumer and manipulator of digital information. It used to be just humans through GUIs, or computers, or APIs. And now it's a completely new thing. And agents are computers, but they are human-like. Kind of, right? They're people spirits. There's people spirits on the internet, and they need to interact with our software infrastructure. Can we build for them? It's a new thing. So, as an example, you can have robots.txt in your domain, and you can instruct, or advise, I suppose, web crawlers on how to behave with your website. In the same way, you can have maybe LLMs.txt file, which is just a simple markdown that's telling LLMs what this domain is about. And this is very readable to an LLM. If it had to be said, get the HTML of your webpage and try to parse it. This is very error-prone and difficult, and it will screw it up, and it's not going to work. So, we can just directly speak to the LLM, and it's worth it. 5.1. I see some of the services now are transitioning a lot of their docs to be specifically for LLMs. So, for Cell and Stripe, as an example, are early users here. But there are a few more that I've seen before. And they offer their documentation in Markdown. Markdown is crazy for LLMs to understand. This is great. Maybe one simple example from my experience as well. Maybe someone you know from Google Brown who makes beautiful animation videos. Yeah, I love this library that he wrote, Mavic. And I wanted to make my own. And there's extensive documentation on how to use M. So, I didn't want to actually read through it. So, I copy-pasted the whole thing to an LLM. And I just grabbed what I wanted, and it just worked out of the box. But LLM just byte-coded the animation exactly what I wanted. And I was like, wow, this is amazing. So, if we can make docs legible to LLMs, it's going to unlock a huge amount of objectives. And I think this is one of the core issues that should happen.</p><p>The other thing I wanted to point out is that we do unfortunately have to. It's not just about taking your docs and making them appear in Markdown. That's the easy part. We actually have to change the docs. Because any time your docs say, like, this is bad. And LLM will not be able to agent-maintain this action right now. So, Purcell, for example, is replacing every occurrence of play with an equivalent program that your LLM agent could take on your behalf. And so, I think this is very interesting. And then, of course, there's a model context protocol from them. And this is also another way. It's a protocol that's given directly to agents. It's a new consumer and particular commercial application. So, I'm very bullish on this.</p><p>The other thing I really like is the number of little tools here and there that are helping ingest data in very LLM-friendly formats. So, for example, when I go to a GitHub repo, I can't feed this to an LLM and ask questions about it. Because this is a human interface from GitHub. So, when you just change the URL from GitHub to GitHub Ingest, then this will actually concatenate all the files into a single giant text. And it will create a directory structure. It's ready to copy-paste it into a favorite LLM. And you can use it. Maybe even more of a dramatic example of this is Eclipse, where it's not just the raw content of its files. This is from Devon. But also, they have Devon basically do analysis of the GitHub repo. Devon basically builds up a whole box of pages just for your repo. And you can imagine that this is even more helpful to copy-paste into your LLM. So, I love all the little tools that basically just change the URL and make something accessible to an LLM. So, this is all well and great. And I think there's more to come.</p><p>One little note I wanted to make is that it is absolutely possible that in the future, LLMs will be able to, it's not even future, this is today, they'll be able to go around and they'll be able to put stuff at home. But I still think it's very worth basically eating LLM's pathway and making it easier for them to access all this information. Because this is still fairly expensive, I would say, to do this. And a lot more difficult. And so, I do think that lots of software that are being long-tailed, where it won't let them gap. Because these are not live layers or repositories or traditional infrastructure. And we will need these tools. But I think for everyone else, I think it's very worth it. So, I'm bullish on both.</p><p>So, in summary, what an amazing time to give to the industry. We need to rewrite a ton of code. A ton of code will be written by professionals and by players. These LLM's are kind of like utilities, kind of like labs, but they're kind of, especially, like operating systems. But it's so early. It's like 1960s operating systems. And I think a lot of the analogies cross over. And these LLM's are kind of like these fallible people spirits that we have to learn to work with. And in order to do that properly, we need to adjust our infrastructure. So, when you're building these LLM's, it's practical ways of working effectively with these LLM's and some of the tools that make that possible. And how you can spin this very, very quickly and basically create partial time for products. And then, yeah, a lot of code has to be written for the agents. But, in any case, going back to the Iron Man suit analogy, I think what we'll see over the next decade, roughly, is we're going to take the slider from left to right. Very interesting. It's going to be very interesting to see what that looks like. So, with all of you. Thank you.</p><p>that's all. let's keep in touch?</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The unreasonable effectiveness of fuzzing for porting programs (119 pts)]]></title>
            <link>https://rjp.io/blog/2025-06-17-unreasonable-effectiveness-of-fuzzing</link>
            <guid>44311241</guid>
            <pubDate>Wed, 18 Jun 2025 16:26:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rjp.io/blog/2025-06-17-unreasonable-effectiveness-of-fuzzing">https://rjp.io/blog/2025-06-17-unreasonable-effectiveness-of-fuzzing</a>, See on <a href="https://news.ycombinator.com/item?id=44311241">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
        

<section>
<p><em>A simple strategy of having LLMs write fuzz tests and build up a port in topological order seems effective at automating porting from C to Rust.</em></p>
<h2>Agents are starting to produce more and more code</h2>
<p>A week or 2 back, I was reflecting on some code Claude had generated for me and
I had a sort of moment of clarity. "Clarity" might be overstating it; more
like the type of thought you have in the shower or after a few beers. Anyway.</p>
<p>The thought was: LLMs produce more and more code, and they'll eventually be
producing more code than people. We can imagine at some point we're going to
fork from mostly people writing code to mostly computers. What does this mean
for how we're going to treat our libraries and code maintenance in the future?</p>
<p>LLMs make it easier to deal with API issues or inconsistencies, they're dealing
with it, not us. Will this continue, leaving us with a <a href="https://en.wikipedia.org/wiki/The_Princess_and_the_Pea">The
Princess and the Pea</a> situation where we deal with piles of leaking
abstractions? Or will we use LLMs to radically change our APIs when needed - will
things get cleaner and better and faster as we go?</p>
<p>LLMs open up the door to performing radical updates that we'd
never really consider in the past. We can port our libraries from one language
to another. We can change our APIs to fix issues, and give downstream users an
LLM prompt to migrate over to the new version automatically, instead of
rewriting their code themselves. We can make massive internal refactorings.
These are types of tasks that in the past, <em>rightly</em>, a senior engineer would
reject in a project until its the last possibly option. Breaking customers
almost never pays off, and its hard to justify refactoring on a "maintenance
mode" project.</p>
<p>But if its more about finding the right prompt and letting an
LLM do the work, maybe that changes our decision process.</p>
<h2>Maintaining big important libraries is no fun</h2>
<p>I used to work on
<a href="https://www.tensorflow.org/">TensorFlow</a>. (TensorFlow is a system to develop
gradient based models, like PyTorch. It's not used so much anymore outside of
Google.) Now TensorFlow had some design flaws in the core language, and a
botched version 1 -&gt; version 2 migration didn't help matters too much. But as a
maintainer, the biggest issue was the enormous technical debt. It suffered from
a sort of "career advancement syndrome" <label for="sn-sad-9e298f0f"></label><span>I sadly missed this period and only was around for the aftermath where we had to deal with the cruft. </span>: TensorFlow was popular and you got credit for
contributing to it, so there was a huge incentive to add some feature as quickly
as you could and then get away.</p>
<p>As a result of a few years of this style of development, a huge surface of
Python code had been cobbled together on top of the C++ core. The complexity
only spiraled over time: engineers came into the project and needed to get
something done. Increasingly the easiest thing to do was to add some Python global or
context manager, shove something in it, then grab it later. Do this over and
over and eventually it becomes impossibly to figure out what's happening. It
also ended up being incredibly slow. It would take 10s of minutes to build some
graphs (TensorFlow's equivalent of a program).</p>
<p>As a result of this over time TensorFlow became harder and harder to maintain,
much less evolve <label for="sn-drag-2cb313ba"></label><span>It was also hard to keep good engineers working on the project. If
you're a good engineer, other teams want you and you could choose to work on
less crufty, more exciting projects like <a href="https://docs.jax.dev/">Jax</a> or
<a href="https://rjp.io/blog/openxla.org">XLA</a>, or you know, Ads. </span>. One idea we discussed internally to try to fix this was
porting the Python code, bit by bit, into a more comprehensible C++ layer, and
then re-exporting it via PyBind. At first you'd still call everything through
the Python interface, but as you ported code over, you could start to use the
internal C++ interfaces more and more, getting faster and more stable as you
went. This would incrementally improve the performance and consistency of the
system, or that was the hope.</p>
<p>But it would have been an enormous effort to do this all. Each time you tried to
port a module you'd reveal subtle errors or assumptions that you'd need to debug
and revisit, update users etc. Given our staffing, we couldn't really justify
the time versus the real work of making sure Ads didn't break.</p>
<p>Even though TensorFlow was moving to maintenance mode, if we could have made
this type of change it would have improved the maintainer and users' experience
for the years we need to keep the lights on. But because the cleanup was thorny
and slow, we just couldn't justify the cost.</p>
<p>This is by no means limited to TensorFlow. If you're working on something like
<a href="https://github.com/libexpat/libexpat">libexpat</a> its hard to justify
refactoring your code or rewriting it in Rust, no matter how potentially useful
it would be, when you've got this at the top of your page:</p>
<p><img alt="libexpat staffing warning" src="https://rjp.io/blog/libexpat-warning.png"></p>
<p>But what if we could make this type of refactoring much, much cheaper?</p>
<h2>Infinite patience versus really hard problems</h2>
<p>My experience with coding agents like <a href="https://github.com/anthropics/claude-code">Claude
Code</a> and <a href="https://aider.chat/">Aider</a>
makes me think this could be different today. I've found that agents are really
good when you set them up against a concrete obstacle: "I changed this API, now
fix everything until the tests pass". They can show real ingenuity in finding
solutions to subtle problems. I've had the LLM diagnose subtle bugs in a
program, write test cases, write minimizers, and probe for solutions, where my
contribution to this conversation largely consisted of "keep going until the
test passes". Not always, of course. Good luck getting an agent to produce
anything other than React &amp; Tailwind on your new project, for instance.</p>
<p>My hunch is that the stronger the test case, the more concrete the objective the
LLM has to fulfill, the less likely it is to try to sabotage the problem. e.g. "I made
an empty test and it passed", or to misinterpret the request "you asked for a
Ruby on Rails project, so here's NextJS, like you asked for".</p>
<p>You're putting the agent between a rock and a hard place, as it were, and this
reduces the easy solutions it could use, and thus it works harder to find a real
solution.</p>
<p>In general its really hard to define test cases, a priori, that are specific
enough to constrain the LLM to do exactly what you want. But what about when
we're changing an existing project? What if we could constrain the output before
and after the changes to be the same? This could be as simple as running all of
our tests for our project, if we're confident they're very thorough. For most
complex changes, it might be hard to keep both versions aligned.</p>
<p>What happens if we take a very specific and important type of refactoring:
moving from an unsafe language like C to a safe language like Rust? "I did something with memory I shouldn't have" are
still in the <a href="https://www.cvedetails.com/vulnerabilities-by-types.php">top 3 for CVEs</a><label for="sn-cve-f8b1dde5"></label><span>No longer #1, not because the number of memory CVEs is going down, but because XSS vulnerabilities have grown so quickly . </span>. This is exactly the
type of thing we might <em>like</em> to do, but hard to justify.  Can we make porting more of a prompt engineering problem instead of a coding
one? Let's find out...</p>
<h2>Going from C to Rust</h2>
<p>Porting C to Rust with LLMs isn't a brand new idea:
<a href="https://github.com/immunant/c2rust">c2rust</a> can produce a mechanical
translation of C code to Rust, though the result is intentionally "C in Rust
syntax", with the idea that a person uses it as a starting point to then Rustify
the project.</p>
<p>More recently researchers have started throwing LLMs at the problem.
<a href="https://arxiv.org/pdf/2412.14234">Syzygy</a> is the only paper I found which produced a complete working
implementation of a library, the other papers were more like "we threw this at
the wall and it compiled". Syzygy leverages a bunch of machinery to mechanize
parts of the porting process:</p>
<p><img alt="syzygy" src="https://rjp.io/blog/syzygy.png"></p>
<p>What's cool is their approach <em>works</em>: they get a compression program out of it, and its <a href="https://github.com/syzygy-project/Syzygy_Zopfli/blob/main/rust_code/src/main.rs">all safe Rust</a>. That's a non-trivial achievement! That said, it runs 3x slower than the C version, and its not
<em>identical</em>: how much this detail matters depends on the users of your API and how much
you care about <a href="https://en.wikipedia.org/wiki/Hyrum%27s_Law">Hyrum's Law</a>.</p>
<p>Syzygy puts a lot of work into developing test cases for symbols as they port,
but because the tests are against inferred properties of the C program, instead
of directly comparing against the C behavior, you can end up with subtle changes
in behavior. This is hard to avoid with their approach: if the interfaces
differ, it becomes hard to perform a direct comparison between your C and Rust
programs.</p>
<p>I wanted to test if we could do something radically simpler and more direct: <em>what if we just
randomly compared the C and Rust output as we ported</em>? Would that be sufficient
to port an entire library? Or would our tests be ineffectual and we'd get stuck
halfway through. In effect we'd be performing a specific type of fuzz or
<a href="https://en.wikipedia.org/wiki/Property_testing">property testing</a> to our
program as we ported it.</p>
<h2>Property testing</h2>
<p>Property testing has an interesting supposition: we can avoid the drudge work of
writing tests by having the computer build out the test cases for us, and just
check if a property holds for our system.  Trivia item: it is a fundamental law
of all property testing frameworks that they start with one of 2 examples:</p>
<ul>
<li>reverse is its own inverse, so <code>x = reverse(reverse(x))</code>.</li>
<li>the elements of a sorted list are in order</li>
</ul>
<p>Don't believe me? Check it out:</p>
<ul>
<li><a href="https://github.com/BurntSushi/quickcheck">https://github.com/BurntSushi/quickcheck</a></li>
<li><a href="https://github.com/HypothesisWorks/hypothesis">https://github.com/HypothesisWorks/hypothesis</a></li>
<li><a href="https://hackage-content.haskell.org/package/QuickCheck-2.16.0.0/docs/Test-QuickCheck.html">Haskell Quickcheck</a></li>
<li><a href="https://github.com/emil-e/rapidcheck">https://github.com/emil-e/rapidcheck</a></li>
</ul>
<p>These examples are common for a reason. They're the perfect fit for property
testing: you just feed random lists in and check your condition holds. A common
critique of property testing is that it doesn't really extend beyond these
examples. Either its too hard to generate meaningful inputs, or
properties to test, and so we're better off writing individual test
cases. My experience is that there's some narrow cases where property testing is
great, but I still end up writing plenty of individual tests.</p>
<p>Sometimes determining the property that holds is hard: I can
test individual cases and validate them, but I don't know how to generalize.  Or
generating interesting inputs might be hard. If I want to test a C parser,
sampling random bit strings isn't going to help me very much.  Entire projects
like <a href="https://github.com/csmith-project/csmith">Csmith</a> are dedicated to fuzzing C compilers.</p>
<p>But in our case we've solved at least the property test is solved for us: we
have the <em>perfect</em> output property to test: for a given input X, does our Rust
library produce the exact same output as our C library? If we can test this over
an interesting part of our input space, then we can be confident we've preserved
our behavior.</p>
<p>Let's see how this works out.</p>
<h2>Porting C to Rust, attempt 0</h2>
<p>The logs of my first attempt are fortunately lost to the howling void. I spent
an hour or 2 with Claude Code, following this policy:</p>
<ul>
<li>Port symbol X</li>
<li>Write a test for symbol X to make sure it works</li>
</ul>
<p>I wasn't comparing directly against the C version, instead relying on the unit tests
to validate the behavior was preserved. This attempt did not go well. As modules
were ported over, and we started to use symbols from previous ports, we exposed
more and more latent bugs. Eventually, you're in a state where the top-level
<code>ZopfliCompress</code> doesn't work, and you're stuck debugging the whole program to
figure out what happened.</p>
<p>I realized this wasn't going to scale. Even if I got something working, I was
looking at a lot of work and I couldn't explain how to replicate what I did with
another project.</p>
<h2>Porting C to Rust, attempt 1</h2>
<p>My <del>first</del>second attempt was <del>truly</del> slightly less crude:</p>
<ul>
<li>For each C module, ask Claude Code to write a Rust version.</li>
<li>Then write a fuzz test for that module</li>
</ul>
<p>The results of this experiment are on
<a href="https://github.com/rjpower/zopfli/tree/master/zopfli-rs/">Github</a>.</p>
<p>This time, it worked: other than writing the <a href="https://github.com/rjpower/zopfli/blob/master/port/RUST_PORTING.md">porting guidelines</a> and cajoling the model, I didn't do much work.  I didn't do any debugging by
hand or have to intercede to understand what was happening. In the case a test found a discrepancy, I would have the model write a <a href="https://github.com/rjpower/zopfli/blob/master/port/bugs/20241219_deflate_tree_encoding_discrepancy.md">bug report</a>
and then iterate until it solved the problem.</p>
<p>But again this wasn't automated and it was hardly reproducible. I had to babysit
the process quite a bit<label for="sn-babysit-06e936e9"></label><span>Mostly telling it to keep going. I probably could have written <code>while true; echo 'keep going until the test passes' | claude</code> and gotten most of the way there. </span>, and while the library seemed to work, the
results were <em>subtly different</em> than the original Zopfli C implementation. And
because of the ad-hoc approach, I didn't have a good idea of what had changed.</p>
<p>Still this was promising. So I was incentivized to try again, with a bit more rigor and automation.</p>
<h2>Porting C to Rust, "for real"</h2>
<p>Our final process would be more rigorous. We want to port a small portion of our
program at a time, and automate as much of the porting as possibly. Instead of
calling into an agent and babysitting it, we'd need to hand-roll our own
machinery to call into an LLM API and perform the necessary edits in a fully
automated fashion. So our overall process would be:</p>
<ul>
<li>
<p>Sort symbols in topological order based on the static call graph. If function F calls function G, then first port function G, then function F, etc. (Cycles need to be ported together, but we didn't have any for this library). This ensures that when we are porting a new symbol into Rust, we can call into our child symbols immediately; we don't have to implement anything new or call into C. "Symbol" here means a struct, enum or function.</p>
</li>
<li>
<p>For a given public C declaration (in a header file), create an FFI entry for it and a Rust implementation with the exact same signature. We'll do this by giving the LLM the C header &amp; source and asking for a translation.</p>
</li>
</ul>
<pre><code>// ffi.rs
pub fn AddDynamicTree(
    ll_lengths: *const ::std::os::raw::c_uint,
    d_lengths: *const ::std::os::raw::c_uint,
    bp: *mut ::std::os::raw::c_uchar,
    out: *mut *mut ::std::os::raw::c_uchar,
    outsize: *mut usize,
);

// deflate.rs
pub unsafe fn AddDynamicTree(
    ll_lengths: *const c_uint,
    d_lengths: *const c_uint,
    bp: *mut c_uchar,
    out: *mut *mut c_uchar,
    outsize: *mut usize,
) {
    ...
}
</code></pre>
<ul>
<li>Have the LLM write a fuzz test which samples over possibly inputs and compares the C and Rust implementations, asserting if there is a discrepancy <a href="https://github.com/rjpower/portkit/blob/1d5a2c21adbfc71cb0906fd16a0cf7252be5dcd5/zopfli-port/rust/fuzz/fuzz_targets/fuzz_AddDynamicTree.rs">example</a>. No attempt was made to guide the fuzz test inputs other than instructing the LLM to avoid generating invalid structures (e.g. null pointers or numbers out of an expected range).</li>
</ul>
<pre><code>#[derive(Debug, arbitrary::Arbitrary)]
struct FuzzInput {
    ll_lengths: Vec&lt;u32&gt;,
    d_lengths: Vec&lt;u32&gt;,
}

fuzz_target!(|input: FuzzInput| {
    ...
    // setup inputs
    let c_result = ffi::AddDynamicTree(...)
    let rust_result = rust::AddDynamicTree(...)

    assert_eq!(c_result, rust_result, "C and Rust diverged.");
</code></pre>
<ul>
<li>Run the fuzz test and fix until everything compiles and the fuzz test passes.</li>
</ul>
<p>We repeat this for each symbol in the program until we hit the top-level main().</p>
<p>Prior to porting I modified the C source slightly to make a few static functions
extern so that we could create an FFI to them. This would allow the LLM to port
smaller chunks at a time (otherwise there were some modules where the LLM would
have to port 1000 lines of code at once because only the main symbol was
visible). I also copied some <code>#define</code> constants by hand because my C traversal
hadn't detected them.</p>
<p>Originally I tried to break up the task of porting a symbol. We'd separately
define the FFI to C, a stub implementation of the Rust function, define a fuzz
test, then build the implementation. The idea was that this would let us verify
each of these via a separate call and we'd be more robust. Ultimately this
proved more confusing to the agents than helpful.</p>
<p>In the end, I simply prompted the LLM to do all the steps for porting a symbol
at once, reprompting it if the fuzz test didn't exist or pass.</p>
<p>And in the end... it worked? The result is a <a href="https://github.com/rjpower/portkit/tree/main/zopfli-port">Rust implementation of
Zopfli</a> that gives
<em>identical</em> results on every input I've tried to the C version. This is
different from the Syzygy results, where they ended up with a program that
compresses correctly, but not identically <label for="sn-better-46a42f45"></label><span>This isn't a claim my approach is <em>better</em> than Syzygy, just different. </span> to the C version.  Because
we locked the Rust and C versions to use the same API at each step, the
resulting program isn't very "rusty", but its a complete translation.</p>
<p>About 90% of symbols were auto-ported using gemini-2.5-pro-06-05 and a crappy
"agent" library I wrote up for this task. The remaining 10% I switched over to
running in a Claude Code, as Gemini seemed to struggle with patch formats and
imports. The only work I did manually was to adjust a few places where the LLM
was calling the FFI code from Rust and clean up some warning messages.</p>
<h2>But why does this work at all?</h2>
<p>To clarify, the surprising result is not that fuzzing would detect discrepancies
at the top-level of our library. Target specific fuzzers work great for this task:
<a href="https://github.com/csmith-project/csmith">CSmith</a> and
<a href="https://jepsen.io/">Jepsen</a> find all sorts of weird interesting bugs.
What's surprising is that with only one exception<label for="sn-claude-5296e104"></label><span>Gemini introduces a weird typo into a program, which was detected a few symbols downstream by Claude and repaired. You can see <a href="https://rjp.io/blog/claude-rust-port-conversation">the session log here</a>. </span>, the LLM translation + naive
fuzz test correctly validated the symbol behavior <em>for every symbol</em>. This meant
that we didn't run into any issues where after porting A, B ... Q, suddenly we
detect a subtle bug in R.</p>
<p>This meant we didn't have to "backtrack", or inspect the rest of the code base
as we ported symbols: each symbol ported in isolation, we tested it (implicitly
testing the dependent symbols as well), and we moved on. If this didn't work,
we'd have to inspect the whole program and debug it, over and over, as we built
it up.</p>
<p>If we think about it, this shouldn't have worked as well as it did. Fuzzing
shouldn't hold for all functions. Imagine I'm converting something like a manual
floating-point multiplier:</p>
<pre><code>mul(a_bits: &amp;[byte], b_bits: &amp;[byte], c_bits: mut &amp;[byte]):
</code></pre>
<p>If I fuzz this by providing random bytes, am I likely to detect a subtle issue
with underflow, or detect the <a href="https://en.wikipedia.org/wiki/Pentium_F00F_bug">Pentium FOOF bug</a>?  Or imagine a C parser:
fuzzing random bytes wouldn't trigger much of the parser. Without a lot of
probes, it seems hard to test these function spaces!</p>
<p>My hunch is that this works due to a combination of factors:</p>
<ul>
<li>I chose a simple library to work with. Zopfli isn't trivial, but you aren't juggling a lot of state, for example.</li>
<li>Fuzzers are good at probing the input space. If you <a href="https://llvm.org/docs/LibFuzzer.html#tracing-cmp-instructions">compile with the right arguments</a> the fuzzer will try to choose inputs which trigger different branches. You can even give good examples (a corpus) to start the fuzzer off. I didn't do this for my experiments, but its easy to imagine an LLM generating a decent starting corpus.</li>
<li>Most code doesn't express subtle logic paths. If I test if a million inputs are correctly sorted, I've probably implemented the sorter correctly.</li>
<li>Complex logic, when it exists, is broken up across functions. Our individual tests make it easier to ensure the combined calls work.</li>
<li>The LLM produces correct code most of the time! The fuzz test is just there to validate we did the right thing.</li>
</ul>
<h2>Caveats, or don't try this at home</h2>
<p>While the overall system seems to work, its far from ready to drop in to a new project.</p>
<p><em>The resulting Rust code is very "C-like"</em></p>
<p>By construction, we use the same unsafe C interface for each symbol we port.
This is different from Syzygy, which generates a safe Rust version of the
program in one shot.  This was convenient but not strictly required: I could
have tweaked the prompts and clearly indicate public/private interfaces, letting
the LLM use more idiomatic Rust for internal functions. Retaining the same
interfaces simplified writing the fuzz tests and let the LLM call into the
original C code when debugging.</p>
<p>That said, because our end result has end-to-end fuzz tests and tests for every
symbol, its now much easier to "rustify" the code with confidence. This is a
great task for future work.</p>
<p><em>The automation isn't complete</em></p>
<p>I tweaked the agent framework and prompts as I went to get better results out of Gemini. I needed to have Claude come in at the end to finish the last few symbols, and I fixed a few typos here and there. With better prompting and a better agent framework, this intervention would be reduced, but likely not go away entirely.</p>
<p><em>Zopfli is easy</em></p>
<p>Zopfli is a simple library with many functions which are input/output oriented.
It's not clear how this would work if you introduced more stateful interfaces.</p>
<h2>Conclusions and future work</h2>
<p>Whew. This ended up taking a few days and being more than the initial trivial
experiment I intended. That said, I think there's some interesting insights:</p>
<ul>
<li><em>Porting via LLMs is surprisingly cost effective and only getting cheaper</em>. Even with my multiple rounds of experimentation and tweaking agents etc, the total cost for this experiment was ~$50, or about $0.01 a line. (For comparison, Syzygy cost ~$1500 using the O3 model). I suspect this could be brought down another 10x by walking up a "complexity tree": first try porting symbols mechanically (constants, structs, enums), then trying out a cheap model before falling back to more expensive models. For instance, Gemini Flash 2.5 is capable of producing correct Rust code for some non-trivial problems.</li>
<li><em>Full automation is hard</em>. The chat based interface of LLMs/agents can lead us to believe that they are more capable than they really are. We don't often realize how much we're guiding the direction of the LLMs until we try to go completely hands off.</li>
<li><em>Full automation isn't necessary</em>. Imagine a system where you have agents port as many symbols as they possibly can, in parallel. If a symbol fails to port, you mark it "tainted" and move on. You then continue until you can't port anything else. In such a system, a human could be brought in to fix issues as they emerge, but you could still rely on the agents to handle &gt;90% of the work of porting in an asynchronous manner.</li>
</ul>
<p>If you're interested in the (truly terrible) code I used for the porting, you
can find it on <a href="https://github.com/rjpower/portkit/">Github</a>. I'm not sure if I'll
continue down this route further myself, but let me know if you're interested in
this space or trying to port a project and I'm happy to chat more!</p>


</section>

    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My iPhone 8 Refuses to Die: Now It's a Solar-Powered Vision OCR Server (118 pts)]]></title>
            <link>https://terminalbytes.com/iphone-8-solar-powered-vision-ocr-server/</link>
            <guid>44310944</guid>
            <pubDate>Wed, 18 Jun 2025 15:49:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://terminalbytes.com/iphone-8-solar-powered-vision-ocr-server/">https://terminalbytes.com/iphone-8-solar-powered-vision-ocr-server/</a>, See on <a href="https://news.ycombinator.com/item?id=44310944">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>After running for over a year, my solar-powered setup has processed <strong>83,418 OCR requests</strong> and <strong>48GB of images</strong> using nothing but Apples Vision framework and renewable energy. Most people toss their old iPhones in a drawer when they upgrade. Me? I turned mine into a server that saves me money while running completely off-grid.</p>
<p><em>Note: The OCR processing serves a separate side project unrelated to this blog.</em></p>
<p>Could I have just run this on my Mac like a normal person? Absolutely. But wheres the fun in that?</p>
<p><img loading="lazy" src="https://terminalbytes.com/iphone-8-solar-powered-vision-ocr-server/iphone-8-vision-ocr-server-solar.jpg" alt="iPhone 8 OCR Server Setup">
</p>
<h2 id="tldr---technical-summary">TL;DR - Technical Summary</h2>
<p><strong>The Setup:</strong></p>
<ul>
<li>iPhone 8 running SwiftUI app with Apple Vision OCR</li>
<li>EcoFlow River 2 Pro (768Wh) + 220W solar panel</li>
<li>Mini PC handling web services and API routing</li>
<li>Tailscale network connecting everything</li>
</ul>
<p><strong>Performance After 1+ Year:</strong></p>
<ul>
<li>83,418 total OCR requests processed</li>
<li>48GB of image data handled</li>
<li>1000+ requests on busy days</li>
<li>76% battery health after continuous operation</li>
<li>$84-120 CAD annual electricity savings</li>
</ul>
<p><strong>Key Learnings:</strong></p>
<ul>
<li>Apple Vision framework rivals cloud services for accuracy</li>
<li>Old hardware is surprisingly reliable for server workloads</li>
<li>Solar power works well with proper battery management</li>
<li>Local processing beats cloud services for privacy and cost at scale</li>
</ul>
<h2 id="why-would-you-even-do-this">Why Would You Even Do This?</h2>
<h3 id="the-logical-approach">The Logical Approach</h3>
<p>I have an image-heavy personal project that chews through hundreds of images daily, categorizing them automatically. Any reasonable person would run the OCR processing on their Mac - Apples Vision framework works great on macOS.</p>
<h3 id="the-me-approach">The Me Approach</h3>
<p>But Im not reasonable. I see a perfectly good iPhone 8 and think: <em>You know what this needs? A second career as a solar-powered image processing servant."</em> My EcoFlow River 2 Pro was sitting idle between camping trips, so switching my existing OCR server to solar felt like the natural evolution.</p>
<h3 id="the-unexpected-benefits">The Unexpected Benefits</h3>
<ul>
<li><strong>Real-time dashboard</strong> on my window sill while bird watching</li>
<li><strong>Grid independence</strong> for personal projects</li>
<li><strong>Actual cost savings</strong> that add up over time</li>
<li><strong>Amazing conversation starter</strong> when people visit</li>
</ul>
<p>The financial benefits are modest but real. Looking at my actual power consumption data: 37.4 kWh in May ($7.21) and 45.8 kWh in April ($8.82). Over a year, thats meaningful savings.</p>
<p><em>Is it practical? Debatable. Is it cool? Absolutely."</em></p>
<h2 id="what-exactly-is-this-thing">What Exactly Is This Thing?</h2>
<p>Heres my delightfully over-engineered setup:</p>
<ol>
<li><strong>Mini PC</strong> running my web server, image processing service, Plex server, and various other services</li>
<li><strong>iPhone 8</strong> perched on my window sill, running a SwiftUI app that serves as both OCR processor and real-time dashboard</li>
<li><strong>EcoFlow power station</strong> keeping both devices running off-grid</li>
<li><strong>Tailscale network</strong> connecting everything seamlessly</li>
</ol>
<p>The workflow is beautifully simple: My image processing service sends images to the phone for OCR processing using Apples Vision framework. The phone processes the text, sends it back, and updates its dashboard with processing stats. All while I watch birds outside my window and feel smug about my setup.</p>
<h2 id="the-hardware-setup-solar-meets-computing">The Hardware Setup: Solar Meets Computing</h2>
<h3 id="power-station-choice-and-reality-check">Power Station Choice and Reality Check</h3>
<p>I didnt buy the EcoFlow River 2 Pro specifically for this project. I bought it because I convinced myself I was going to become one of those outdoorsy people who camps and needs portable power. Well, turns out Im still more of an indoor cat with outdoor aspirations kind of person. But my impulse purchase isnt gathering dust!</p>
<p><img loading="lazy" src="https://terminalbytes.com/iphone-8-solar-powered-vision-ocr-server/ecoflow-river-pro-2.jpg" alt="EcoFlow River 2 Pro portable power station">
</p>
<p><em>Pro tip: When researching portable power stations, <a href="https://gearscouts.com/">GearScouts.com</a> is an excellent price comparison site that could save you some time.</em></p>
<h3 id="power-consumption-and-solar-performance">Power Consumption and Solar Performance</h3>
<table>
<thead>
<tr>
<th>Component</th>
<th>Idle Power</th>
<th>Processing Load</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>iPhone 8 OCR Server</td>
<td>0.5-1W</td>
<td>2-5W</td>
<td>Surprisingly efficient</td>
</tr>
<tr>
<td>Mini PC (multiple services)</td>
<td>15W</td>
<td>25-30W</td>
<td>Includes Plex, Archive Warriors</td>
</tr>
<tr>
<td><strong>Total Daily Consumption</strong></td>
<td><strong>~1.2kWh</strong></td>
<td><strong>Variable</strong></td>
<td>Based on actual TP-Link data</td>
</tr>
</tbody>
</table>
<p><strong>Solar Performance by Season:</strong></p>
<ul>
<li><strong>Summer</strong>: 150-220W peak input, infinite runtime with battery charging</li>
<li><strong>Fall/Spring</strong>: 20-60W average, hybrid solar/battery operation</li>
<li><strong>Winter</strong>: 5-20W if lucky, mostly battery power (15-20 hours runtime)</li>
</ul>
<p>The River 2 Pros 768Wh capacity provides excellent buffer for Canadas unpredictable weather. Its battery management system deserves credit - its not just dumping power into devices; its managing charging curves properly.</p>
<h2 id="building-the-ios-ocr-server-app">Building the iOS OCR Server App</h2>
<p>Creating a server on iOS sounds complicated, but Apples done most of the heavy lifting. The real challenge was making it run continuously without iOS deciding my app wasnt important enough to keep running.</p>
<p><img loading="lazy" src="https://terminalbytes.com/iphone-8-solar-powered-vision-ocr-server/iphone-8-ocr-server.jpeg" alt="iPhone 8 on window sill serving as OCR server">
</p>
<h3 id="apples-vision-framework-the-unsung-hero">Apples Vision Framework: The Unsung Hero</h3>
<p>Apples Vision framework is genuinely impressive and criminally underused. While everyone obsesses over ChatGPT and cloud-based OCR services, Apple quietly shipped a local OCR solution thats fast, accurate, and runs entirely on-device.</p>
<p>Heres the core processing code:</p>
<div><pre tabindex="0"><code data-lang="swift"><span>import</span> <span>Vision</span>
<span>import</span> <span>UIKit</span>

<span>func</span> <span>processImage</span>(<span>_</span> image: UIImage, completion: @escaping (String?) -&gt; Void) {
    <span>guard</span> <span>let</span> cgImage = image.cgImage <span>else</span> {
        completion(<span>nil</span>)
        <span>return</span>
    }

    <span>let</span> request = VNRecognizeTextRequest { request, error <span>in</span>
        <span>guard</span> <span>let</span> observations = request.results <span>as</span>? [VNRecognizedTextObservation] <span>else</span> {
            completion(<span>nil</span>)
            <span>return</span>
        }

        <span>let</span> recognizedText = observations.compactMap { observation <span>in</span>
            observation.topCandidates(<span>1</span>).first?.string
        }.joined(separator: <span>"</span><span>\n</span><span>"</span>)

        completion(recognizedText)
    }

    request.recognitionLevel = .accurate
    request.usesLanguageCorrection = <span>true</span>

    <span>let</span> handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
    <span>try</span>? handler.perform([request])
}
</code></pre></div><p>The accuracy rivals some cloud services Ive tested, and its processing everything locally. No API calls, no usage limits, no privacy concerns.</p>
<h3 id="swiftui-dashboard-and-analytics">SwiftUI Dashboard and Analytics</h3>
<p>The dashboard was the fun part - something that would look cool on my window sill and provide real-time stats:</p>
<div><pre tabindex="0"><code data-lang="swift"><span>struct</span> <span>DashboardView</span>: View {
    @StateObject <span>private</span> <span>var</span> server = OCRServer()
    @State <span>private</span> <span>var</span> stats = ProcessingStats()

    <span>var</span> body: some View {
        VStack(spacing: <span>20</span>) {
            Text(<span>"OCR Server Status"</span>)
                .font(.title)
                .fontWeight(.bold)

            HStack {
                StatCard(title: <span>"Requests Today"</span>, value: <span>"</span><span>\(</span>stats.requestsToday<span>)</span><span>"</span>)
                StatCard(title: <span>"Total Processed"</span>, value: <span>"</span><span>\(</span>stats.totalProcessed<span>)</span><span>"</span>)
            }

            HStack {
                StatCard(title: <span>"Avg Processing Time"</span>, value: <span>"</span><span>\(</span>stats.avgProcessingTime<span>)</span><span>ms"</span>)
                StatCard(title: <span>"Success Rate"</span>, value: <span>"</span><span>\(</span>stats.successRate<span>)</span><span>%"</span>)
            }

            BatteryView(percentage: UIDevice.current.batteryLevel)

            Text(<span>"Server running on port 8080"</span>)
                .font(.caption)
                .foregroundColor(.secondary)
        }
        .padding()
    }
}
</code></pre></div><p>I integrated Google Analytics 4 because Im a data nerd. The dashboard shows 139,917 total users with 17,643 this month, 6:28 average session duration, and 11 currently active users. Its like having a tiny data center dashboard on your window sill.</p>
<h2 id="the-solar-power-challenge">The Solar Power Challenge</h2>
<p>Running electronics on solar power sounds simple until you face Canadas weather reality. We get everything from blazing summer days (all 3 of them) to months of overcast skies that make solar panels about as useful as a chocolate teapot.</p>
<h3 id="seasonal-strategy-and-battery-management">Seasonal Strategy and Battery Management</h3>
<p>Ive developed a weather-dependent approach:</p>
<ul>
<li><strong>Summer</strong>: Solar handles everything plus charges other devices</li>
<li><strong>Fall/Spring</strong>: Hybrid solar/battery with careful monitoring</li>
<li><strong>Winter</strong>: Mostly battery power with occasional solar boosts</li>
</ul>
<p>The phones battery health held up reasonably well. After over a year of constant operation, its at 76% capacity. The power stations battery management deserves credit here.</p>
<p><img loading="lazy" src="https://terminalbytes.com/iphone-8-solar-powered-vision-ocr-server/iphone-8-battery-health-ocr-server.jpeg" alt="iPhone 8 Battery Health After OCR Server Usage">
</p>
<p>One unexpected discovery: the phone performs OCR faster when slightly warm (but not hot). Cold Canadian mornings mean slower processing times - something I never would have noticed with wall power.</p>
<h2 id="cost-analysis-solar-vs-grid-power">Cost Analysis: Solar vs Grid Power</h2>
<h3 id="investment-and-operating-costs">Investment and Operating Costs</h3>
<p><strong>Initial Investment:</strong></p>
<ul>
<li>EcoFlow River 2 Pro: $599 CAD (bought for camping anyway)</li>
<li>220W Solar Panel: $180 CAD</li>
<li>Cables, mounting hardware: ~$50 CAD</li>
<li><strong>Additional solar investment</strong>: ~$230 CAD</li>
</ul>
<p><strong>Monthly Savings:</strong>
Based on actual EcoFlow data showing 37.4-45.8 kWh monthly consumption, Im saving approximately $84-120 CAD annually. The payback period is about 2-3 years.</p>
<h3 id="comparison-with-cloud-ocr-services">Comparison with Cloud OCR Services</h3>
<p>Cloud OCR services typically charge $1.00-1.50 per 1,000 requests. With over 83,000 requests processed, cloud services would have cost me $83-125 CAD, plus privacy concerns about sending images to external servers.</p>
<p>My solar setup? Zero per-request costs and complete privacy.</p>
<h2 id="what-i-learned-after-a-year">What I Learned After a Year</h2>
<h3 id="reliability-surprises">Reliability Surprises</h3>
<p><strong>The hardware is incredibly reliable.</strong> This phone has been running continuously for over a year, and it just keeps going. Performance hasnt degraded noticeably despite the constant workload.</p>
<p><strong>iOS background processing works better than expected</strong> - once you figure out the right approach. The key is using background app refresh properly and keeping the HTTP server active with regular requests.</p>
<p><strong>Apples Vision framework improves over time.</strong> Text recognition that used to fail now works perfectly, especially with handwritten text and unusual fonts.</p>
<h3 id="common-problems-and-solutions">Common Problems and Solutions</h3>
<p><strong>Solar Power Intermittency:</strong> I configured the power station to prioritize the phone (lower power draw) and gracefully shut down the Mini PC when battery gets low. The phone can handle basic OCR requests solo for several hours if needed.</p>
<p><strong>Heat Management:</strong> Direct sunlight plus continuous processing equals thermal throttling. I added shade, improved airflow, and implemented smart processing that reduces requests when the phone reports high temperature.</p>
<p><strong>iOS Background Limitations:</strong> iOS really doesnt want apps running forever. I use background app refresh, minimal location services, and keep the HTTP server responding to requests. Its a delicate balance between staying alive and preserving battery.</p>
<h2 id="why-this-actually-matters">Why This Actually Matters</h2>
<p>Beyond the obvious coolness factor, this project demonstrates several important principles:</p>
<p><strong>Privacy First:</strong> Every image stays on my devices. No cloud uploads, no third-party access. In an era where everything gets sent to someone elses computer, truly local processing feels revolutionary.</p>
<p><strong>Energy Independence:</strong> While the savings arent life-changing, the principle matters. This proves meaningful computing workloads can run entirely on renewable energy, even in challenging climates.</p>
<p><strong>E-Waste Reduction:</strong> That phone was destined for a drawer. Now its a productive member of my tech ecosystem. How many old devices could be repurposed instead of becoming electronic waste?</p>
<p><strong>Local-First Computing:</strong> Not everything needs to be in the cloud. Sometimes the best solution is sitting right in front of you, powered by the sun, processing your data locally and privately.</p>
<p>The setup has become my go-to demonstration for visitors interested in renewable energy or local computing. Plus, I genuinely love glancing at my window sill and seeing real-time processing stats while watching birds at my feeders.</p>
<h2 id="resources-and-next-steps">Resources and Next Steps</h2>
<p>If youre interested in building something similar:</p>
<h3 id="hardware">Hardware</h3>
<ul>
<li><a href="https://us.ecoflow.com/products/river-2-pro-portable-power-station">EcoFlow River 2 Pro</a> - The power station I use</li>
<li><a href="https://www.renogy.com/100-watt-12-volt-monocrystalline-solar-panel/">Renogy 100W Solar Panel</a> - Similar to my setup</li>
<li>Any iPhone 8 or newer with iOS 13+ for Vision framework support</li>
</ul>
<h3 id="software-resources">Software Resources</h3>
<ul>
<li><a href="https://developer.apple.com/documentation/vision">Apple Vision Framework Documentation</a> - Official OCR implementation docs</li>
<li><a href="https://developer.apple.com/documentation/backgroundtasks">Background App Refresh Guide</a> - Keeping iOS apps alive</li>
<li><a href="https://github.com/Building42/Telegraph">SwiftUI HTTP Server Examples</a> - HTTP server implementation</li>
</ul>
<h3 id="power-management-tools">Power Management Tools</h3>
<ul>
<li><a href="https://www.kasasmart.com/">TP-Link Kasa Smart Plugs</a> - For monitoring actual power consumption</li>
<li>EcoFlow app - Built-in monitoring for the River 2 Pro</li>
<li><a href="https://gearscouts.com/">GearScouts.com</a> - Price comparison for power stations and outdoor gear</li>
</ul>
<p><em>This article was last updated while watching my solar-powered setup process its 83,418th OCR request, powered entirely by Canadian sunshine.</em></p>


    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Framework Laptop 12 review (167 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2025/06/framework-laptop-12-review-im-excited-to-see-what-the-2nd-generation-looks-like/</link>
            <guid>44310583</guid>
            <pubDate>Wed, 18 Jun 2025 15:09:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2025/06/framework-laptop-12-review-im-excited-to-see-what-the-2nd-generation-looks-like/">https://arstechnica.com/gadgets/2025/06/framework-laptop-12-review-im-excited-to-see-what-the-2nd-generation-looks-like/</a>, See on <a href="https://news.ycombinator.com/item?id=44310583">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="app">
    <p><a href="#main">
  Skip to content
</a></p>



<main id="main">
            <article data-id="2101033">
  
  <header>
  <div>
    <div>
      <p><span>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 40"><defs><clipPath id="section-gadgets_svg__a"><path fill="none" d="M0 0h40v40H0z"></path></clipPath><clipPath id="section-gadgets_svg__b"><path fill="none" d="M0 0h40v40H0z"></path></clipPath></defs><g clip-path="url(#section-gadgets_svg__a)"><g fill="currentColor" clip-path="url(#section-gadgets_svg__b)"><path d="M38 22c1.1 0 2-.9 2-2s-.9-2-2-2h-2v-6h2c1.1 0 2-.9 2-2s-.9-2-2-2h-2V4h-4V2c0-1.1-.9-2-2-2s-2 .9-2 2v2h-6V2c0-1.1-.9-2-2-2s-2 .9-2 2v2h-6V2c0-1.1-.9-2-2-2S8 .9 8 2v2H4v4H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v6H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v6H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v4h4v2c0 1.1.9 2 2 2s2-.9 2-2v-2h6v2c0 1.1.9 2 2 2s2-.9 2-2v-2h6v2c0 1.1.9 2 2 2s2-.9 2-2v-2h4v-4h2c1.1 0 2-.9 2-2s-.9-2-2-2h-2v-6zm-6 10H8V8h24z"></path><path d="M24.7 17.3 20 12h-7.1c-.6 0-1 .4-1 1s.4 1 1 1h6.3l4.1 4.7L20 22h8v-8z"></path><path d="m15.2 22.7 4.7 5.3H27c.6 0 1-.4 1-1s-.4-1-1-1h-6.3l-4.1-4.7 3.3-3.3h-8v8z"></path></g></g></svg>
  </span>
  <span>
    how much would you pay for personality?
  </span>
</p>
    </div>

    

    <p>
      A sturdy, thoughtful, cute design that just can't compete in its price range.
    </p>

    

    <div>
            <p><a data-pswp-width="2560" data-pswp-height="1440" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1440x810.jpeg 1440w" data-cropped="false" href="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150.jpeg" target="_blank">
              <img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150.jpeg" alt="" loading="eager" decoding="async" fetchpriority="high" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1440x810.jpeg 1440w" sizes="(max-width: 2560px) 100vw, 2560px">
            </a></p><div id="caption-2101680">
    
    <p>
      Framework's Laptop 12 has a lot of personality, but also a lot of shortcomings.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
          </div>

    <div>
    
    <p>
      Framework's Laptop 12 has a lot of personality, but also a lot of shortcomings.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
  </div>
</header>


  

  
      
    
    <div>
                      
                      
          
<p>"What's this purple laptop? It's cool."</p>
<p>Over a decade-plus of doing gadget reviews and review-adjacent things, my wife (and, lately, my 5-year-old) have mostly stopped commenting on the ever-shifting selection of laptops I have in my bag or lying around the house at any given time. Maybe she can't tell them apart, or maybe she just figures there isn't that much to say about whatever black or silver metal slab I'm carrying around. Either way, they practically never elicit any kind of response, unless there are just too many of them sitting out in too many places.</p>
<p>But she&nbsp;<em>did</em> ask about the Framework Laptop 12, the third and latest major design in Framework's slowly expanding lineup of modular, repairable, upgradeable laptops. With its five two-toned color options and sturdy plastic exterior, it's definitely more approachable and friendly-looking than the Laptop 13 or Laptop 16, both metal slabs with a somewhat less-finished and prototype-y look to them. But it retains the features that a certain kind of PC geek likes about Framework's other laptopsuser-customizable and swappable ports, an easy-to-open design, first-class Linux support, and the promise of future upgrades that improve its performance and other specs.</p>
<h2>Look and feel</h2>
<figure>
    <p><img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-1440x810.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The Laptop 12 stacked atop the Laptop 13.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>Plastic gets a bad rap, and there are indeed many subpar plastic gadgets out there. When done poorly, plastic can look and feel cheap, resulting in less durable devices that show more wear over time.</p>
<p>But well-done plastic can still feel solid and high-quality, in addition to being easier to make in different colors. Framework says the Laptop 12's chassis is a combination of ABS plastic and <a href="https://en.wikipedia.org/wiki/Thermoplastic_polyurethane">TPU plastic</a> (a more flexible, rubberized material), molded over a metal inner structure. The result is something that can probably actually take the shock of a drop or a fall better than many aluminum-and-glass laptops without feeling overly cheap or chintzy.</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>The five two-tone color optionsthe boring, businesslike black and gray, plus purple-and-gray lavender, pink-and-baby-blue bubblegum, and the green sage optionsare the most fun thing about it, and the lavender and bubblegum colors are particularly eye-catching.</p>

<figure>
    <p><img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-1440x810.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      Keyboard and trackpad. Only the lavender and gray laptops get a color-matched trackpad; the keyboard and deck are always different shades of gray.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>Matching other components to the exterior of the system can be a bit of a crapshoot, though. The screwdriver and spudger that Framework provides for upgrading and repairing all of its systems <em>does</em> match the color of the laptop, and the two-tone styluses for the touchscreens will also match the laptops when they're made available for purchase in the coming months.</p>
<p>The lavender option is the only one that can also be configured with a color-matched lavender trackpadthe only other trackpad option is gray, and the keyboard deck and the keyboard itself are all gray no matter what color laptop you pick. This is presumably meant to limit the number of different trackpad options that Framework has to manufacture and stock, but it is too bad that the laptop's keyboard and palm rest aren't as colorful as the rest of it.</p>
<p>The Laptop 12 also uses Framework's still-unique Expansion Card system for customizing the built-in ports. These are all 10 Gbps USB 3.2 Gen 2 ports rather than the Thunderbolt ports on the Intel versions of the Laptop 13, but all four support the same speeds, all four support charging, and all four support display output, so you really can put whatever port you want wherever you want it.</p>
<p>A downside of the Laptop 12 is that, as of this writing, only the USB-C Expansion Modules are available in color-matched versions. If you want USB-A, HDMI, DisplayPort, or any other kind of port on your system, you'll get the silver modules that were designed to match the finish on the Framework Laptops 13 and 16, so you'll have to put up with at least one mismatched port on your otherwise adorable system.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          

<figure>
    <p><img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-1440x810.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      Only the USB-C Expansion Cards are available in lavender, which can make for goofy-looking mismatches. But I do prefer the Framework 16-style retention switches to the Framework Laptop 13's retention buttons, which you need to hold down as you pull out the Expansion Card.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>Once you get past the adorable design, the Expansion Modules, and the sturdy construction, the system's downsides start to become more apparent. The 12.2-inch, 19201200 touchscreen gets plenty bright and has a respectable contrast ratio (440 nits and 1,775:1 in our testing, respectively). But it's surrounded by thick black bezels on all sides, particularly on the bottomit does seem that either a larger screen or a slightly smaller laptop design would be possible if so much space weren't wasted by these thick borders.</p>
<p>The display has good viewing angles but a distinctly mediocre color gamut, covering around 60 percent of the SRGB color space (compared to the high 90s for the Laptop 13 and most midrange to high-end IPS screens in other laptops). This is low enough that most colors appear slightly muted and washed outreds most noticeably, though greens aren't much better. You definitely don't need a colorimeter to see the difference here.</p>
<p>Framework's color-matched stylus isn't ready yet, but you won't need to wait for one if you want to use a pen with this touchscreen. Both the Universal Stylus Initiative (USI) 2.0 and Microsoft Pen Protocol (MPP) 2.0 specs are supported, so the Surface Pen, a bunch of Lenovo styluses, and any number of inexpensive third-party Amazon styluses will all work just fine. That said, the screen can only support one of those stylus specs at a timeMPP is on by default, and you can swap between them in the BIOS settings.</p>
<figure>
    <p><img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-1440x810.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The webcam and mic have locks to disable them so that the OS can't see or use them.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>The keyboard feels mostly fine, with good key spacing and a nice amount of travel. I noticed that I was occasionally missing letters the first couple of days I used the laptopI was pressing the keys, but they intermittently didn't register. That got better as I adjusted to the system. The trackpad is also unremarkable in a good way. Finger tracking and multi-touch gestures all worked as intended.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>But the keyboard lacks a backlight, and it doesn't have the fingerprint sensor you get with the Laptop 13. With no fingerprint sensor and no IR webcam, there are no biometric authentication options available for use with Windows Hello, so you'll either need a PIN or a password to unlock your laptop every time you want to use it. Either omission would be sort of annoying in a laptop in this price range (we complained about the lack of keyboard backlight in <a href="https://arstechnica.com/gadgets/2022/07/review-microsofts-surface-laptop-go-2-has-a-lot-of-problems-but-i-like-it-anyway/">the $700 Surface Laptop Go 2</a> a few years ago), but to be missing&nbsp;<em>both</em> is particularly frustrating in a modern system that costs this much.</p>
<h2>Repairs and upgrades</h2>



<p>We've been inside the Framework Laptop 13 enough times that we don't do deep dives into its insides anymore, but as a new (and, in some ways, more refined) design, the Laptop 12 warrants a closer look this time around.</p>
<p>Framework's pack-in Torx screwdriver is still the only tool you need to work on the Laptop 12. Undo the eight captive screws on the bottom of the laptop, and you'll be able to lift away the entire keyboard and trackpad area to expose all of the other internal components, including the RAM, SSD, battery, and the motherboard itself.</p>
<p>The motherboard is quite a bit smaller than the Framework Laptop 13 board, and the two are definitely&nbsp;<em>not</em> interchangeable. Framework has never said otherwise, but it's worth highlighting that these are two totally separate models that will have their own distinct components and upgrade pathsthat goes for parts like the speakers and battery, too.</p>

<figure>
    <p><img width="2560" height="1441" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-2048x1153.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-980x552.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-1440x811.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      Laptop 12 motherboard on top, Laptop 13 motherboard on bottom.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>As a result of that reduction in board space, the Laptop 12 can only fit a single DDR5 RAM slot, which reduces memory bandwidth and limits your RAM capacity to 48GB. It also uses shorter M.2 2230 SSDs, like the Surface lineup or the Steam Deck. Unlike a few years ago, these SSDs are now readily available at retail, and it's also easy to buy warranty-less ones on eBay or elsewhere that have been pulled from OEM systems. But they're still a bit more expensive than the more common M.2 2280 size, and you have fewer options overall.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>Framework has already published <a href="https://guides.frame.work/Guide/Framework+Laptop+12+(13th+Gen+Intel%C2%AE+Core%E2%84%A2)+DIY+Edition+Quick+Start+Guide/429?_gl=1*1dc9fwi*sg_ga4w_production_ga*NTEzNDU2MDg1LjE3NTAxODA0Mzg.*sg_ga4w_production_ga_PYG8X65YJJ*czE3NTAxODA0MzckbzEkZzEkdDE3NTAxODI3NDckajYwJGwwJGgw">a guide on setting up the DIY Edition of the laptop</a> and&nbsp;<a href="https://guides.frame.work/c/Framework_Laptop_12">a few repair guides</a> for common components. Guides for replacing bigger or more co parts, like the display or the webcam, are still listed as "coming soon."</p>
<h2>Performance and battery life</h2>
<p>I could politely describe the Laptop 12's 2.5-year-old 13th-gen Intel Core processor as "mature." This generation of Intel chips <em>has&nbsp;</em>stuck around for a lot longer than usual, to the point that Intel recently acknowledged that it has been dealing with shortages. They're appealing to PC companies because they still offer decent everyday performance for basic computing without the additional costs imposed by things like on-package memory or having some or all of the chip manufactured outside of Intel's own factories.</p>
<p>The upside of a slightly older processor is a more stable computing experience, in both Windows and Linux, since the companies and communities involved have had more time to add support and work out bugs; I had none of the sleep-and-wake issues or occasional video driver crashes I had while testing the Ryzen AI 300 version of the Framework Laptop 13.</p>


<p>The downside, of course, is that performance is pretty unexciting. These low-power U-series 12th- and 13th-gen Intel chips remain capable when it comes to day-to-day computing, but they fall far behind the likes of Intel and AMD's newer chips, Qualcomm's Snapdragon chips from the Microsoft Surface and other Copilot+ PCs, or the Apple M4 in the MacBook Air.</p>


<p>And while none of these chips are really intended for gaming laptops, the Laptop 12 isn't even a great fit for that kind of casual Steam Deck-y 3D gaming that most Framework Laptop 13 models can handle. Technically, this is the same basic Intel Iris Xe GPU that the first few generations of Framework Laptop 13 used, which is not exciting as integrated GPUs go but is at least still minimally capable. But because the Laptop 12 only has a single RAM slot instead of two, memory bandwidth is halved, which makes the GPU identify itself as "Intel UHD Graphics" to the device manager and drags down performance accordingly. (This is something these GPUs have always done, but they usually ship in systems that either have two RAM slots or soldered-down memory, so it usually doesn't come up.)</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>Framework has tuned these chips to consume the same amount of power in both the "Balanced" and "Best Performance" power modes in Windows, with a 15 W sustained power limit and a 40 W limit for shorter, bursty workloads. This keeps the laptop feeling nice and responsive for day-to-day use and helps keep a lid on power usage for battery life reasons, but it also limits its performance for extended CPU-intensive workloads like our Handbrake video encoding test.</p>

<p>The Laptop 12 takes a&nbsp;<em>lot</em> longer to accomplish these tasks than some other laptops we've tested with similar chips, either because of the lower memory bandwidth or because Best Performance mode doesn't let the chip consume a bunch of extra power. I'm not inclined to complain too much about this because it's not the kind of thing you really buy an ultraportable laptop to do, but as with light gaming, it's worth noting that the Laptop 12 doesn't hit that same "usable for these workloads in a pinch" balance that the Laptop 13 does.</p>
<figure>
    <p><img width="2048" height="1536" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008.png" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008.png 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-640x480.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-1024x768.png 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-768x576.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-1536x1152.png 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-980x735.png 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-1440x1080.png 1440w" sizes="auto, (max-width: 2048px) 100vw, 2048px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The Laptop 12's battery life is decent relative to most Laptop 13s.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>The Core i5 version of the Laptop 12 lasted around 10 hours in the PCMark Modern Office battery life test, which isn't stunning but is a step up from what the fully specced versions of the Framework Laptop 13 can offer. It will be just fine for a long flight or a full day of work or school. Our Framework reviews often complain about battery life, but I don't think it will be an issue here for most users.</p>
<h2>About that price</h2>
<p>In some ways, the Laptop 12 is trying to be a fundamentally&nbsp;<em>different</em> laptop from the Laptop 13. For all the Laptop 13's upgrades over the years, it has never had a touchscreen option, stylus support, or a convertible hinge.</p>
<p>But in most of the ways that count, the Laptop 12 is meant to be an "entry-level, lower-cost laptop," which is how Framework CEO Nirav Patel <a href="https://www.youtube.com/watch?v=Ejl-7X74tgc&amp;t=171s">has positioned it</a> in the company's announcement blog posts and videos. It features a slightly smaller, lower-resolution, less colorful screen with a lower refresh rate; a non-backlit keyboard; and considerably weaker processors. It also lacks both a fingerprint reader and a face-scanning webcam for Windows Hello.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>The issue is that these cost-cutting compromises come at a price that's a bit outside of what you'd expect of a "budget" laptop.</p>
<p>The DIY Edition of the Laptop 12 we're evaluating herea version that ships with the Windows license and all the components you need but which you assemble yourselfwill run you at least $1,176, depending on the Expansion Modules you choose for your ports. That includes 16GB of GDDR5 RAM and a 1TB M.2 2230 SSD, plus the Core i5-1334U processor option (2 P-cores, 8 E-cores). If you stepped down to a 500GB SSD instead, that's still $1,116. A pre-built editiononly available in black, but with identical specificationswould run you $1,049.</p>

<figure>
    <p><img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-1440x810.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The Laptop 13 compared to the Laptop 12. The Laptop 12 is missing quite a few quality-of-life things and has worse performance, but it isn't all that much cheaper.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>This puts the Framework Laptop 12 in the same general price range as Apple's MacBook Air, Microsoft's 13-inch Surface Laptop, and even many editions of the Framework Laptop 13. And the Laptop 12 is charming, but its day-to-day user experience falls well short of any of those devices.</p>
<p>You can make it cheaper! Say you go for the Core i3-1315U version (two P-cores, four E-cores) instead, and you buy your own 16GB stick of DDR5 RAM (roughly $50 instead of $80) and 1TB SSD ($70 or $80 for <a href="https://www.newegg.com/silicon-power-1tb/p/0D9-0021-00171?Item=9SIBDGPK454247">a decent one</a>, instead of $159). Say you have plenty of USB-C chargers at home so you don't need to pay $55 for Framework's version, and say you run Linux or ChromeOS, or you already have a Windows 11 product key, or you've brought your own Windows 11 key from one of those gray-market key selling sites (as little as $10).</p>
<p>Now we're talking about a PC that's a little under $700, which is closer to "reasonable" for a brand-new touchscreen PC. But the laptop's old CPU and poky performance also mean it's competing with a wide swath of refurbished, used, and closeout-priced older PCs from other manufacturers.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>In December, for example, I bought an SSD-less <a href="https://www.lenovo.com/us/en/p/laptops/thinkpad/thinkpadl/thinkpad-l13-yoga-gen-3-13-inch-intel/len101t0032?orgRef=https%253A%252F%252Fwww.google.com%252F&amp;srsltid=AfmBOoqjpN9S8iIG7xfG4vGA-Dv7fPWAoBj6sd6Y9oDZXN_KBVmVbRiT">Lenovo ThinkPad L13 Yoga Gen 3</a> from eBay for around $300, with around a year left on its warranty. After I'd added an SSD and reinstalled Windowsno additional cost because it had a valid Windows license alreadyI ended up with a PC with the same screen resolution and similar specs but with a better-quality display with smaller bezels that made the screen larger without making the laptop larger; a faster GPU configuration; a backlit keyboard; and a fingerprint reader.</p>
<p>I know it's not possible for everyone to just go out and buy a laptop like this. The boring black outline of a midrange ThinkPad is also the polar opposite of the Framework Laptop 12, but it's an example of what the tech-savvy buyer can find in the secondhand market if you're trying to find a cost-effective alternative to what Framework is offering here.</p>

<h2>A good laptop, but not a good value</h2>
<figure>
    <p><img width="2560" height="1441" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-2048x1153.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-980x552.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-1440x811.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The Framework Laptop 12.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>There are plenty of factors beyond Frameworks control that contribute to the Laptop 12s price, starting with on-again-off-again global trade wars and the uncertainty that comes with them. There's also Frameworks status as a niche independent PC company rather than a high-volume behemoth. When you ship the number of computers that Apple does, its almost certainly easier to make a $999 laptop that is both premium and profitable.</p>
<p>But whatever the reason, I cant escape the feeling that the Laptop 12 was meant to be cheaper than it has ended up being. The result is a computer with many of the compromises of an entry-level system, but without a matching entry-level price tag. Its hard to put a price on some of the less-tangible benefits of a Framework laptop, like ease of repairs and the promise of future upgrades, but my gut feeling is that the Framework Laptop 13 falls on the right side of that line, and the Laptop 12 doesn't.</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<p>I am charmed by the Laptop 12. It's cute and functional, and it stands out among high-end aluminum slabs. It adds some subtle refinement to elements of the original Framework Laptop 13 design, including some things I hope end up making it into some future iteration of its designsofter corners, more color options, and an easier-to-install keyboard and trackpad. And it's far from a <em>bad</em> performer for day-to-day desktop use; it's just that the old, poky processor limits its capabilities compared to other PCs that don't cost that much more than it does.</p>
<p>I probably wouldn't recommend this over the Laptop 13 for anyone interested in what Framework is doing, unless a touchscreen is a make-or-break feature, and even then, I'd encourage people to take a good, long look at Microsoft, Lenovo, Dell, or HP's convertible offerings first. But I hope that Framework does what it's done for the Laptop 13 over the last four or so years: introduce updated components, iterate on different elements of the design, and gradually bring the price down into a more reasonable range through refurbished and factory-second parts. As a $1,000-ish computer, this leaves a lot to be desired. But as the foundation for a new Framework platform, it has enough promise to be interesting.</p>
<h3>The good</h3>
<ul>
<li>Eye-catching, colorful, friendly design that stands out among metal slabs.</li>
<li>Simple to build, repair, and upgrade.</li>
<li>Dual-plastic design over a metal frame is good for durability.</li>
<li>First convertible touchscreen in the Framework laptop.</li>
<li>Customizable ports.</li>
<li>Decent performance for everyday computing.</li>
<li>Respectable battery life.</li>
</ul>
<h3>The bad</h3>
<ul>
<li>Old, slow chip isn't really suitable for light gaming or heavy productivity work that the larger Framework Laptop 13 can do.</li>
<li>Pre-built laptop only comes in boring black.</li>
<li>Mediocre colors and large bezels spoil the screen.</li>
<li>Keyboard sometimes felt like it was missing keystrokes until I had adjusted to compensate.</li>
</ul>
<h3>The ugly</h3>
<ul>
<li>It's just too expensive for what it is. It looks and feels like a lower-cost laptop, but without a dramatically lower price than the nicer, faster Framework 13.</li>
</ul>


          
                  </div>

                  
          






  <div>
  <div>
          <p><a href="https://arstechnica.com/author/andrew_cunningham/"><img src="https://cdn.arstechnica.net/wp-content/uploads/2016/05/a.cunningham-45-1.jpg" alt="Photo of Andrew Cunningham"></a></p>
  </div>

  <div>
    

    <p>
      Andrew is a Senior Technology Reporter at Ars Technica, with a focus on consumer tech including computer hardware and in-depth reviews of operating systems like Windows and macOS. Andrew lives in Philadelphia and co-hosts a weekly book podcast called <a href="https://overduepodcast.com/">Overdue</a>.
    </p>
  </div>
</div>


  <p>
    <a href="https://arstechnica.com/gadgets/2025/06/framework-laptop-12-review-im-excited-to-see-what-the-2nd-generation-looks-like/#comments" title="30 comments">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 80"><defs><clipPath id="bubble-zero_svg__a"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath><clipPath id="bubble-zero_svg__b"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath></defs><g clip-path="url(#bubble-zero_svg__a)"><g fill="currentColor" clip-path="url(#bubble-zero_svg__b)"><path d="M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40"></path><path d="M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z"></path></g></g></svg>
    30 Comments
  </a>
      </p>
              </div>
  </article>


  


  


  <div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <a href="https://arstechnica.com/tech-policy/2025/06/trump-org-launches-47-month-wireless-service-teases-odd-499-phone/">
              <img src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-2211177567-768x432.jpg" alt="Listing image for first story in Most Read: Mocked Trump Mobile yanks coverage map that ignored Trump renaming Gulf of Mexico" decoding="async" loading="lazy">
            </a>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>
  </main>





  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Homomorphically Encrypting CRDTs (168 pts)]]></title>
            <link>https://jakelazaroff.com/words/homomorphically-encrypted-crdts/</link>
            <guid>44309520</guid>
            <pubDate>Wed, 18 Jun 2025 12:59:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jakelazaroff.com/words/homomorphically-encrypted-crdts/">https://jakelazaroff.com/words/homomorphically-encrypted-crdts/</a>, See on <a href="https://news.ycombinator.com/item?id=44309520">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-content="" data-astro-cid-onuac4el=""> 



<p>Heres a problem with local-first software.</p>
<p>You want to work on a document together with a friend who lives far away from you.
That sounds like local-firsts bread and butter: store the document as a CRDT, then use some sort of sync server to merge updates and relay them between you and your friend.</p>
<p>But theres a catch: the contents of that document are secret.
So secret, in fact, that <em>you dont even want the app developer to know what they are</em>.</p>
<p>One way to solve this is end-to-end encryption.
You and your friend agree on a secret key, known only to each other.
You each use that key to encrypt your changes before sending them, decrypt them upon receipt, and no one in the middle is able to listen in.
Because the document is a CRDT, you can each still get the latest document without the sync server merging the updates.</p>
<p>That is indeed a solution, and modern browser APIs make it fairly simple to implement a basic version of it. <a href="https://plus.excalidraw.com/blog/end-to-end-encryption" data-astro-cid-bi7aps5f="">Excalidraws writeup of their implementation</a><a data-tooltip="" href="https://plus.excalidraw.com/blog/end-to-end-encryption" data-astro-cid-bi7aps5f=""> <img src="https://excalidraw.nyc3.cdn.digitaloceanspaces.com/lp-cms/media/Excalidraw%20blog%20-%20End-to-End%20Encryption%20in%20the%20Browser-1.png" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">Excalidraw Blog | End-to-End Encryption in the Browser</span> <span data-astro-cid-bi7aps5f="">Excalidraw introduces browser-based end-to-end encryption using Web Cryptography APIs for secure, private drawing storage.</span> <span data-astro-cid-bi7aps5f=""> <img src="https://plus.excalidraw.com/favicon.svg" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">plus.excalidraw.com/blog/end-to-end-encryption</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a> is only about 750 words  including code samples!<sup><a href="#user-content-fn-e2ee" id="user-content-fnref-e2ee" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">1</a></sup></p>
<p>Unfortunately, weve introduced a new problem.</p>
<p>You and your friend live far away from each other, so you tend to work while theyre sleeping and vice versa.
That was fine when the sync server could merge your changes and send you the latest document when you opened it.</p>
<p>Now, however, the server can no longer understand the changes you send.
If you want to see your friends latest changes, youll need to both be online at the same time.</p>
<p>Enter <strong>homomorphic encryption</strong>: a special form of encryption that allows a computer to <em>run programs on encrypted data without decrypting it</em>.
Using a homomorphically encrypted CRDT, a sync server could merge your friends and your changes into one document without ever knowing what the document contains.<sup><a href="#user-content-fn-otherways" id="user-content-fnref-otherways" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">2</a></sup></p>
<p>In this article, well explore how homomorphic encryption works and build a homomorphically encrypted last write wins register CRDT.
Well also learn about some fundamental limitations of homomorphic encryption, and how they affect local-first software specifically.</p>
<p>I try to assume as little knowledge as possible about both encryption and CRDTs.
If you want to brush up before continuing on, my <a href="https://jakelazaroff.com/words/an-interactive-intro-to-crdts/" data-astro-cid-bi7aps5f="">Interactive Intro to CRDTs</a><a data-tooltip="" href="https://jakelazaroff.com/words/an-interactive-intro-to-crdts/" data-astro-cid-bi7aps5f=""> <img src="https://jakelazaroff.com/og/an-interactive-intro-to-crdts.png" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">An Interactive Intro to CRDTs | jakelazaroff.com</span> <span data-astro-cid-bi7aps5f="">CRDTs don't have to be all academic papers and math jargon. Learn what CRDTs are and how they work through interactive visualizations and code samples.</span> <span data-astro-cid-bi7aps5f=""> <img src="https://jakelazaroff.com/favicon.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">jakelazaroff.com/words/an-interactive-intro-to-crdts/</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a> and Jeremy Kuns <a href="https://www.jeremykun.com/2024/05/04/fhe-overview/" data-astro-cid-bi7aps5f="">A High-Level Technical Overview of Fully Homomorphic Encryption</a><a data-tooltip="" href="https://www.jeremykun.com/2024/05/04/fhe-overview/" data-astro-cid-bi7aps5f="">  <span data-astro-cid-bi7aps5f="">A High-Level Technical Overview of Fully Homomorphic Encryption</span> <span data-astro-cid-bi7aps5f="">About two years ago, I switched teams at Google to focus on fully homomorphic encryption (abbreviated FHE, or sometimes HE). Since then Ive got to work on a lot of interesting projects, learning along the way about post-quantum cryptography, compiler design, and the ins and outs of fully homomorphic encryption.
If youve heard about FHE and youre a software person, youve probably heard two things: it lets you run programs directly on encrypted data without ever decrypting it; and its still too slow to be useful for anything.</span> <span data-astro-cid-bi7aps5f=""> <img src="https://www.jeremykun.com/favicon.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">www.jeremykun.com/2024/05/04/fhe-overview/</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a> are good places to start.</p>
<p>(Obligatory disclaimer: I am not a cryptographer!
While Im reasonably confident that my code and advice here is generally sound, cryptography is a field in which subtle bugs and exploits can look fine to the untrained eye.
Before using anything here in an environment youd describe with the word production, consult someone who works on this professionally.)</p>
<h2 id="homomorphic-hello-world">Homomorphic Hello World</h2>
<p>First, lets look at a small code sample that uses homomorphic encryption.</p>
<p>Writing the encryption code itself from scratch would take much more code than can fit in this article.
Instead, well use <a href="https://github.com/zama-ai/tfhe-rs" data-astro-cid-bi7aps5f="">THFE-rs</a><a data-tooltip="" href="https://github.com/zama-ai/tfhe-rs" data-astro-cid-bi7aps5f=""> <img src="https://opengraph.githubassets.com/0076171220c59d2546a04eccf3e34abc6618c1b595c0711a3ea1f2f2d96312ad/zama-ai/tfhe-rs" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">GitHub - zama-ai/tfhe-rs: TFHE-rs: A Pure Rust implementation of the TFHE Scheme for Boolean and Integer Arithmetics Over Encrypted Data.</span> <span data-astro-cid-bi7aps5f="">TFHE-rs: A Pure Rust implementation of the TFHE Scheme for Boolean and Integer Arithmetics Over Encrypted Data. - zama-ai/tfhe-rs</span> <span data-astro-cid-bi7aps5f=""> <img src="https://github.githubassets.com/favicons/favicon.svg" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">github.com/zama-ai/tfhe-rs</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a>, a homomorphic encryption library written in Rust.</p>
<p>The flow goes something like this:</p>
<ol>
<li>A client generates a key pair consisting of a client key and a server key.</li>
<li>The client encrypts their data using the client key and sends both the encrypted data and server key to the server.</li>
<li>The server uses the server key to perform some computation on the encrypted data and sends the result back to the client.</li>
<li>The client decrypts the result with the client key.</li>
</ol>
<p>Heres what this looks like in code.
Well take two numbers  <code>clear_a</code> and <code>clear_b</code>  and add them together.
Rather than actually sending anything over a network, well just use a function called <code>server_compute</code> to play the part of the server.</p>
<pre data-language="rust"><code is:raw=""><span>use</span> <span>tfhe<span>::</span>prelude<span>::</span></span><span>*</span><span>;</span>
<span>use</span> <span>tfhe<span>::</span></span><span>{</span>generate_keys<span>,</span> set_server_key<span>,</span> <span>ConfigBuilder</span><span>,</span> <span>FheUint32</span><span>,</span> <span>ServerKey</span><span>}</span><span>;</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>let</span> config <span>=</span> <span>ConfigBuilder</span><span>::</span><span>default</span><span>(</span><span>)</span><span>.</span><span>build</span><span>(</span><span>)</span><span>;</span>

    <span>// generate client and server keys</span>
    <span>let</span> <span>(</span>client_key<span>,</span> server_key<span>)</span> <span>=</span> <span>generate_keys</span><span>(</span>config<span>)</span><span>;</span>

    <span>// generate plaintext</span>
    <span>let</span> clear_a<span>:</span> <span>u32</span> <span>=</span> <span>27</span><span>;</span>
    <span>let</span> clear_b<span>:</span> <span>u32</span> <span>=</span> <span>128</span><span>;</span>

    <span>// encrypt plaintext and "send to server"</span>
    <span>let</span> result <span>=</span> <span>server_compute</span><span>(</span>
        server_key<span>,</span>
        <span>FheUint32</span><span>::</span><span>encrypt</span><span>(</span>clear_a<span>,</span> <span>&amp;</span>client_key<span>)</span><span>,</span>
        <span>FheUint32</span><span>::</span><span>encrypt</span><span>(</span>clear_b<span>,</span> <span>&amp;</span>client_key<span>)</span><span>,</span>
    <span>)</span><span>;</span>

    <span>// decrypt the result</span>
    <span>let</span> decrypted_result<span>:</span> <span>u32</span> <span>=</span> result<span>.</span><span>decrypt</span><span>(</span><span>&amp;</span>client_key<span>)</span><span>;</span>

    <span>// assert that the result is what we expect</span>
    <span>assert_eq!</span><span>(</span>decrypted_result<span>,</span> clear_a <span>+</span> clear_b<span>)</span><span>;</span>
<span>}</span>

<span>fn</span> <span>server_compute</span><span>(</span>key<span>:</span> <span>ServerKey</span><span>,</span> cipher_a<span>:</span> <span>FheUint32</span><span>,</span> cipher_b<span>:</span> <span>FheUint32</span><span>)</span> <span>-&gt;</span> <span>FheUint32</span> <span>{</span>
    <span>set_server_key</span><span>(</span>key<span>)</span><span>;</span>
    <span>return</span> cipher_a <span>+</span> cipher_b<span>;</span>
<span>}</span>
</code></pre>
<p>Get the keys, encrypt two numbers, add their ciphertexts together, decrypt the result.
Not too bad, right?</p>
<p>The simplicity is deceptive!
Rust supports operator overloading, so when we run <code>cipher_a + cipher_b</code> and both of the operands are <code>FheUint32</code>, whats <em>really</em> happening is that TFHE-rs runs a bunch of cryptography code.</p>
<p>Before we build our homomorphically encrypted CRDT, lets peek at what TFHE-rs is doing under the hood.</p>
<h2 id="under-the-hood">Under the Hood</h2>
<p>To start, what does it even mean to run programs on encrypted data?</p>
<p>In short, it means you can use encrypted data in certain math operations, and when you decrypt the data you get the result you would have gotten if you had performed the same operations with the plaintext data.
That requires an encryption scheme in which at least one of the following is true (Ill use the notation <code>E(a)</code> to indicate the encrypted version of the plaintext <code>a</code>):</p>
<ul>
<li><code>E(a) + E(b) = E(a + b)</code>: adding the encrypted values of the plaintext numbers <code>a</code> and <code>b</code> results in the encrypted sum of the plaintext sum <code>a + b</code>.</li>
<li><code>E(a)  E(b) = E(a  b)</code>: multiplying the encrypted values of the plaintext numbers <code>a</code> and <code>b</code> results in the encrypted product of the plaintext product <code>a  b</code>.</li>
</ul>
<p>What this means is that <strong>if you add or multiply two homomorphically encrypted values, then decrypt them, <em>you get the respective sum or product of the original plaintext values</em></strong>.</p>
<p>Heres an extremely simple example that you should absolutely never use anywhere.
First, lets pick a number as a key.
We encrypt numbers by multiplying them by the key, and decrypt numbers by dividing them.</p>
<p>Lets say our key is 7 and our plaintext numbers are 5 and 6.
We can multiply each number by our key 6 to get encrypted numbers of 35 and 42.
Even if someone has access to our encrypted numbers, they cant figure out what our original plaintext numbers were without the key.</p>
<p>What they <em>can</em> do is add the encrypted numbers together.
If they give us back the sum, 77, we can divide it by our key 7 to get 11  <em>the same result wed get by directly adding our original numbers</em>.
Try it out by changing the numbers in the playground below:</p>
<homomorphic-addition-demo></homomorphic-addition-demo>
<p>Because it satisfies the first criterion  <code>E(a) + E(b) = E(a + b)</code>  we can say that our toy encryption scheme is homomorphic over addition.
Encryption that supports only one operation is called <em>partially homomorphic encryption</em>.
All in all, there are four different levels:</p>
<ul>
<li><strong>Partially homomorphic encryption</strong> allows only one of the two operations: <em>either</em> addition <em>or</em> multiplication, but not both.</li>
<li><strong>Somewhat homomorphic encryption</strong> and <strong>leveled homomorphic encryption</strong> allow both operations, but limit the amount of times they can be used.</li>
<li><strong>Fully homomorphic encryption</strong> allows an unlimited amount of both operations.</li>
</ul>
<p>Partially homomorphic encryption is relatively easy to implement, but has limited uses.
The word relatively is doing some heavy lifting here  you or I probably couldnt come up with a partially homomorphic encryption scheme  but its simple enough that there are algorithms such as RSA that are accidentally homomorphic over one operation.</p>
<p>Supporting <em>more than one</em> operation is significantly more useful, but each calculation adds noise to the result.
Too much noise makes it impossible to decrypt.
There are two broad strategies for reducing noise: limiting the number or depth of operations (<em>somewhat</em> and <em>leveled</em> homomorphic encryption), and bootstrapping, which reduces the level of noise mid-computation (<em>fully</em> homomorphic encryption).</p>
<p>Why does it matter whether we can perform <em>both</em> addition and multiplication?</p>
<p>When we talk about doing math on encrypted data, were really talking about the underlying bits: the 1s and 0s that make it up.
To add and multiply the bits, we use the logical operations exclusive or (XOR) and binary and (AND), respectively.</p>
<p>Click on the switches in the playground below to toggle between 1 and 0.
You can see that the AND output is the product of its two inputs, and the XOR output is roughly the sum of its two inputs.<sup><a href="#user-content-fn-addition" id="user-content-fnref-addition" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">3</a></sup></p>
<logic-gates height="180"><switch-gate id="one-and-left" x="40" y="10"></switch-gate><switch-gate id="one-and-right" x="85" y="10"></switch-gate><and-gate id="one-and" x="55" y="70" left="#one-and-left" right="#one-and-right"></and-gate><output-gate x="63" y="140" center="#one-and"></output-gate><switch-gate id="one-xor-left" x="190" y="10"></switch-gate><switch-gate id="one-xor-right" x="235" y="10"></switch-gate><xor-gate id="one-xor" x="205" y="70" left="#one-xor-left" right="#one-xor-right"></xor-gate><output-gate x="213" y="140" center="#one-xor"></output-gate></logic-gates>
<p>This is called a <em>Boolean circuit</em>  essentially, a function that takes 1s and 0s as input and returns 1s and 0s as output.
In this context, the logical operations are called <em>logic gates</em>.</p>
<p>We can create new logic gates by combining ones we have.
Heres how to create inclusive or (OR) and inverter (NOT) operations using only XOR and AND.</p>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 300 310"><rect x="1" y="71" width="143" height="230" stroke="currentColor" fill="none" rx="4" ry="4"></rect><foreignObject x="0" y="0" width="150" height="300"></foreignObject><rect x="156" y="71" width="143" height="230" stroke="currentColor" fill="none" rx="4" ry="4"></rect><foreignObject x="150" y="0" width="150" height="300"></foreignObject></svg>
<p>Once weve built a gate, we can then use it to build <em>yet other</em> gates.
Heres how to make an exclusive nor (XNOR) using XOR and our newly-constructed NOT gate:</p>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 300 260"><rect x="61" y="1" width="143" height="250" stroke="currentColor" fill="none" rx="4" ry="4"></rect><foreignObject x="0" y="0" width="210" height="300"></foreignObject></svg>
<p>It turns out that combining just XOR and AND like this is enough to perform <em>any computation</em>!
All other logical operations can be created by combining only XOR and AND, which means that adding and multiplying the encrypted data is sufficient to simulate arbitrary Boolean logic.</p>
<p>Heres a circuit that implements the greater than operator on two-bit numbers (between 0 and 3).
Using only AND, XOR and the other gates weve built with them, it returns 1 if the first number is greater than the second, and 0 otherwise.</p>
<p>Type in the square boxes at the top to enter the input numbers.
The two rounded boxes below each square input box are the <em>binary representation</em> of that number.</p>
<p>Dont forget this circuit  itll come in handy later!</p>
<logic-gates height="365"><text-gate id="four-left" x="60"></text-gate><text-gate id="four-right" x="160"></text-gate><value-gate x="60" y="30" id="four-left-ms" center="#four-left" value-prop="leftValue"></value-gate><value-gate x="98" y="30" id="four-left-ls" center="#four-left" value-prop="rightValue"></value-gate><value-gate x="160" y="30" id="four-right-ms" center="#four-right" value-prop="leftValue"></value-gate><value-gate x="198" y="30" id="four-right-ls" center="#four-right" value-prop="rightValue"></value-gate><xnor-gate id="four-msbeq-1" x="25" y="110" left="#four-left-ms" right="#four-right-ms"></xnor-gate><not-gate id="four-msbgt-1" x="105" y="90" center="#four-right-ms"></not-gate><and-gate id="four-msbgt-2" x="105" y="150" left="#four-left-ms" right="#four-msbgt-1"></and-gate><not-gate id="four-lsbgt-1" x="200" y="90" center="#four-right-ls"></not-gate><and-gate id="four-lsbgt-2" x="200" y="150" left="#four-left-ls" right="#four-lsbgt-1"></and-gate><and-gate id="four-gt-1" x="140" y="220" left="#four-msbeq-1" right="#four-lsbgt-2"></and-gate><or-gate id="four-gt-2" x="112" y="280" left="#four-msbgt-2" right="#four-gt-1"></or-gate><output-gate x="120" y="330" center="#four-gt-2"></output-gate></logic-gates>
<p>In these examples, weve been looking at circuits that use plaintext 1s and 0s as their inputs and outputs.
With homomorphic encryption, the circuits operate on <em>encrypted data</em>.
Performing an AND on two encrypted bits returns another encrypted bit  and we cant find out what it is unless we have the key.</p>
<p>So thats how homomorphic encryption works in a nutshell.
You express your program as a Boolean circuit, and then simulate the circuit using the encrypted data as input.
The output of the circuit will be the encrypted result, which the client can then decrypt.</p>
<p>Crucially, <em>none of this reveals any sort of relationship between the plaintext values</em>.
For example, even if <code>E(a) + E(b)</code> were positive, <code>E(a + b)</code> might be negative.
Adding and multiplying ciphertext corresponds to the same operations on the underlying plaintext, but theres no correlation between any of the ciphertext results and the underlying plaintext results  you need to decrypt the result to figure out what happened.</p>
<h2 id="a-fully-homomorphic-crdt">A Fully Homomorphic CRDT</h2>
<p>Now that we have a high level understanding of homomorphic encryption, lets build a homomorphically-encrypted last write wins register.</p>
<p>A last write wins register holds a single value and two additional bits of metadata: a clock that gets incremented by one whenever the value is set, and an ID indicating the peer who last wrote to it.
Like all CRDTs, it also has a merge function that describes how it should be combined with another of the same type.</p>
<p>The last write wins register merge algorithm works like this:</p>
<ul>
<li>If the received clock is less than the local clock, the register doesnt change its state.</li>
<li>If the received clock is greater than the local clock, the register overwrites its local value with the received value. It also stores the received clock and peer ID.</li>
<li>Ties are broken by comparing the local peer ID to the peer ID in the received state.</li>
</ul>
<p>Heres a playground in which you can see how this algorithm works:</p>
<lwwregister-demo></lwwregister-demo>
<p>Try playing around with the latency and the network toggle.
See how updates are accepted only if the sending peers clock is higher than the receiving peers clock. If the clocks are tied, the update from the right peer will win out, since the peer ID <code>bob</code> is lexicographically greater than <code>alice</code>.</p>
<p>Okay, lets look at some code.
First, heres what an <em>unencrypted</em> last write wins register might look like in Rust:</p>
<pre data-language="rust"><code is:raw=""><span>const</span> <span>DATA_SIZE</span><span>:</span> <span>usize</span> <span>=</span> <span>16</span><span>;</span>

<span>pub</span> <span>struct</span> <span>Register</span> <span>{</span>
    <span>pub</span> peer<span>:</span> <span>u64</span><span>,</span>
    <span>pub</span> clock<span>:</span> <span>u64</span><span>,</span>
    <span>pub</span> value<span>:</span> <span>[</span><span>u8</span><span>;</span> <span>DATA_SIZE</span><span>]</span><span>,</span>
<span>}</span>

<span>impl</span> <span>Register</span> <span>{</span>
    <span>pub</span> <span>fn</span> <span>new</span><span>(</span>peer<span>:</span> <span>u64</span><span>)</span> <span>-&gt;</span> <span>Register</span> <span>{</span>
        <span>Register</span> <span>{</span>
            peer<span>,</span>
            clock<span>:</span> <span>0</span><span>,</span>
            value<span>:</span> <span>[</span><span>0</span><span>;</span> <span>DATA_SIZE</span><span>]</span><span>,</span>
        <span>}</span>
    <span>}</span>

    <span>pub</span> <span>fn</span> <span>set</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> peer<span>:</span> <span>u64</span><span>,</span> value<span>:</span> <span>[</span><span>u8</span><span>;</span> <span>DATA_SIZE</span><span>]</span><span>)</span> <span>{</span>
        <span>self</span><span>.</span>peer <span>=</span> peer<span>;</span>
        <span>self</span><span>.</span>clock <span>+=</span> <span>1</span><span>;</span>
        <span>self</span><span>.</span>value <span>=</span> value<span>;</span>
    <span>}</span>

    <span>pub</span> <span>fn</span> <span>set_string</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> peer<span>:</span> <span>u64</span><span>,</span> value<span>:</span> <span>&amp;</span><span>str</span><span>)</span> <span>{</span>
        <span>let</span> bytes <span>=</span> value<span>.</span><span>as_bytes</span><span>(</span><span>)</span><span>;</span>
        <span>let</span> len <span>=</span> bytes<span>.</span><span>len</span><span>(</span><span>)</span><span>.</span><span>min</span><span>(</span><span>DATA_SIZE</span><span>)</span><span>;</span>

        <span>let</span> <span>mut</span> data <span>=</span> <span>[</span><span>0</span><span>;</span> <span>DATA_SIZE</span><span>]</span><span>;</span>
        data<span>[</span><span>..</span>len<span>]</span><span>.</span><span>copy_from_slice</span><span>(</span><span>&amp;</span>bytes<span>[</span><span>..</span>len<span>]</span><span>)</span><span>;</span>

        <span>self</span><span>.</span><span>set</span><span>(</span>id<span>,</span> data<span>)</span><span>;</span>
    <span>}</span>

    <span>pub</span> <span>fn</span> <span>merge</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> other<span>:</span> <span>&amp;</span><span>Register</span><span>)</span> <span>{</span>
        <span>if</span> <span>self</span><span>.</span>clock <span>&gt;</span> other<span>.</span>clock <span>{</span>
            <span>return</span><span>;</span>
        <span>}</span><span>;</span>

        <span>if</span> <span>self</span><span>.</span>clock <span>==</span> other<span>.</span>clock <span>&amp;&amp;</span> <span>self</span><span>.</span>peer <span>&gt;</span> other<span>.</span>peer <span>{</span>
            <span>return</span><span>;</span>
        <span>}</span>

        <span>self</span><span>.</span>peer <span>=</span> other<span>.</span>peer<span>;</span>
        <span>self</span><span>.</span>clock <span>=</span> other<span>.</span>clock<span>;</span>
        <span>self</span><span>.</span>value <span>=</span> other<span>.</span>value<span>;</span>
    <span>}</span>
<span>}</span>
</code></pre>
<p>It has a peer ID, a clock and a value.
To merge with another register, it just takes the peer ID, clock and value from the register with the higher clock.
In case of a tie, it uses the peer ID as a tiebreaker.</p>
<p>Because Rust is a low-level language, we need separate functions to convert types such as strings into the raw bytes to store as the value.
We also store the value in an array with a statically-known size  although as well see, thats less of a Rust limitation than it is a fundamental constraint of homomorphic encryption.</p>
<p>Heres the skeleton of an <code>EncryptedRegister</code> struct:</p>
<pre data-language="rust"><code is:raw=""><span>use</span> <span>core<span>::</span></span>array<span>;</span>
<span>use</span> <span>tfhe<span>::</span>prelude<span>::</span></span><span>*</span><span>;</span>
<span>use</span> <span>tfhe<span>::</span></span><span>{</span><span>ClientKey</span><span>,</span> <span>FheUint64</span><span>,</span> <span>FheUint8</span><span>}</span><span>;</span>

<span>use</span> <span>crate</span><span>::</span><span>Register</span><span>;</span>

<span>const</span> <span>DATA_SIZE</span><span>:</span> <span>usize</span> <span>=</span> <span>16</span><span>;</span>

<span>pub</span> <span>struct</span> <span>EncryptedRegister</span> <span>{</span>
    peer<span>:</span> <span>FheUint64</span><span>,</span>
    clock<span>:</span> <span>FheUint64</span><span>,</span>
    value<span>:</span> <span>[</span><span>FheUint8</span><span>;</span> <span>DATA_SIZE</span><span>]</span><span>,</span>
<span>}</span>

<span>impl</span> <span>EncryptedRegister</span> <span>{</span>
    <span>pub</span> <span>fn</span> <span>encrypt</span><span>(</span>clear<span>:</span> <span>&amp;</span><span>Register</span><span>,</span> key<span>:</span> <span>&amp;</span><span>ClientKey</span><span>)</span> <span>-&gt;</span> <span>EncryptedRegister</span> <span>{</span>
        <span>EncryptedRegister</span> <span>{</span>
            peer<span>:</span> <span>FheUint64</span><span>::</span><span>encrypt</span><span>(</span>clear<span>.</span>peer<span>,</span> key<span>)</span><span>,</span>
            clock<span>:</span> <span>FheUint64</span><span>::</span><span>encrypt</span><span>(</span>clear<span>.</span>clock<span>,</span> key<span>)</span><span>,</span>
            value<span>:</span> <span>array<span>::</span></span><span>from_fn</span><span>(</span><span><span>|</span>i<span>|</span></span> <span>FheUint8</span><span>::</span><span>encrypt</span><span>(</span>clear<span>.</span>value<span>[</span>i<span>]</span><span>,</span> key<span>)</span><span>)</span><span>,</span>
        <span>}</span>
    <span>}</span>

    <span>pub</span> <span>fn</span> <span>decrypt</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> key<span>:</span> <span>&amp;</span><span>ClientKey</span><span>)</span> <span>-&gt;</span> <span>Register</span> <span>{</span>
        <span>Register</span> <span>{</span>
            peer<span>:</span> <span>FheUint64</span><span>::</span><span>decrypt</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>peer<span>,</span> key<span>)</span><span>,</span>
            clock<span>:</span> <span>FheUint64</span><span>::</span><span>decrypt</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>clock<span>,</span> key<span>)</span><span>,</span>
            value<span>:</span> <span>array<span>::</span></span><span>from_fn</span><span>(</span><span><span>|</span>i<span>|</span></span> <span>FheUint8</span><span>::</span><span>decrypt</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>value<span>[</span>i<span>]</span><span>,</span> key<span>)</span><span>)</span><span>,</span>
        <span>}</span>
    <span>}</span>

    <span>pub</span> <span>fn</span> <span>merge</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> other<span>:</span> <span>EncryptedRegister</span><span>)</span> <span>{</span>
        <span>// ...</span>
    <span>}</span>
<span>}</span>
</code></pre>
<p>Pretty similar to the unencrypted <code>Register</code> struct!
<code>FheUint64</code> has replaced <code>u64</code>, and <code>value</code> is now an array of <code>FheUint8</code> rather than <code>u8</code>.
These are TFHE-rs types that encrypt the corresponding Rust types.
But other than that, the struct is the same.</p>
<p>The implementation has two new methods:</p>
<ul>
<li><code>encrypt</code>, which takes a normal <code>Register</code> and a client key, encrypts all the fields and returns an <code>EncryptedRegister</code>.</li>
<li><code>decrypt</code>, which takes a client key, decrypts all the fields and returns a normal <code>Register</code>.</li>
</ul>
<p>Weve also omitted the <code>set</code> and <code>set_string</code> methods.
Since <code>EncryptedRegister</code> runs on the server, the value will never be set manually.
The only thing it needs to do is merge an incoming register with the register it has in memory.</p>
<p>Okay, so what does the <code>merge</code> method look like?</p>
<p>As we saw before, TFHE-rs overloads operators like <code>+</code> to make working with encrypted values more convenient.
For operators that dont support overloading such as <code>&lt;</code>, TFHE-rs has methods like <code>gt</code>.</p>
<p>Given that, you might think we could write the <code>merge</code> method like this:</p>
<pre data-language="rust"><code is:raw=""><span>impl</span> <span>EncryptedRegister</span> <span>{</span>
  <span>// ...</span>

  <span>pub</span> <span>fn</span> <span>merge</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> other<span>:</span> <span>EncryptedRegister</span><span>)</span> <span>{</span>
    <span>if</span> <span>self</span><span>.</span>clock<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span> <span>{</span>
      <span>return</span><span>;</span>
    <span>}</span>

    <span>if</span> <span>self</span><span>.</span>clock<span>.</span><span>eq</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span> <span>&amp;&amp;</span> <span>self</span><span>.</span>id<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>id<span>)</span> <span>{</span>
      <span>return</span><span>;</span>
    <span>}</span>

    <span>self</span><span>.</span>id <span>=</span> other<span>.</span>id<span>;</span>
    <span>self</span><span>.</span>clock <span>=</span> other<span>.</span>clock<span>;</span>
    <span>self</span><span>.</span>value <span>=</span> other<span>.</span>value<span>;</span>
  <span>}</span>
<span>}</span>
</code></pre>
<p>This will <em>definitely not work</em>!</p>
<p>Remember that we cant retrieve any information by operating on the encrypted data  <em>including information about the results of intermediate steps</em>.</p>
<p>To more clearly show the problem with this strategy, we can add some logging:</p>
<pre data-language="rust"><code is:raw=""><span>impl</span> <span>EncryptedRegister</span> <span>{</span>
  <span>// ...</span>

  <span>pub</span> <span>fn</span> <span>merge</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> other<span>:</span> <span>EncryptedRegister</span><span>)</span> <span>{</span>
    <span>if</span> <span>self</span><span>.</span>clock<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span> <span>{</span>
      <span>println!</span><span>(</span><span>"local clock is greater than other clock!"</span><span>)</span><span>;</span>
      <span>return</span><span>;</span>
    <span>}</span>

    <span>if</span> <span>self</span><span>.</span>clock<span>.</span><span>eq</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span> <span>&amp;&amp;</span> <span>self</span><span>.</span>peer<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>peer<span>)</span> <span>{</span>
      <span>println!</span><span>(</span><span>"clocks are equal but local peer is greater than other peer!"</span><span>)</span><span>;</span>
      <span>return</span><span>;</span>
    <span>}</span>

    <span>println!</span><span>(</span><span>"overwriting local data with remote data!"</span><span>)</span><span>;</span>
    <span>self</span><span>.</span>peer <span>=</span> other<span>.</span>peer<span>;</span>
    <span>self</span><span>.</span>clock <span>=</span> other<span>.</span>clock<span>;</span>
    <span>self</span><span>.</span>value <span>=</span> other<span>.</span>value<span>;</span>
  <span>}</span>
<span>}</span>
</code></pre>
<p>Although we still couldnt decrypt the encrypted data, this (fake) implementation would reveal the result of the merge!
Wed know which branches our code took, and therefore learn which decrypted clock was higher and which encrypted data was written to the register.</p>
<p>Instead, our merge function must <em>eagerly</em> evaluate all branches in our code.
It also means that all loops must run for a statically-known number of iterations.
More generally, <strong>our code must always execute as though operating on the worst case input</strong>, because altering behavior based on the input would leak information about it.</p>
<p>Heres the <em>real</em> code for our merge function:</p>
<pre data-language="rust"><code is:raw="">  <span>pub</span> <span>fn</span> <span>merge</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> other<span>:</span> <span>&amp;</span><span>EncryptedRegister</span><span>)</span> <span>{</span>
    <span>let</span> higher_clock <span>=</span> <span>self</span><span>.</span>clock<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>

    <span>let</span> equal_clock <span>=</span> <span>self</span><span>.</span>clock<span>.</span><span>eq</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>
    <span>let</span> higher_peer <span>=</span> <span>self</span><span>.</span>peer<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>peer<span>)</span><span>;</span>

    <span>let</span> keep_self <span>=</span> higher_clock <span>|</span> <span>(</span>equal_clock <span>&amp;</span> higher_peer<span>)</span><span>;</span>

    <span>self</span><span>.</span>peer <span>=</span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>peer<span>,</span> <span>&amp;</span>other<span>.</span>peer<span>)</span><span>;</span>
    <span>self</span><span>.</span>clock <span>=</span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>clock<span>,</span> <span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>
    <span>self</span><span>.</span>value <span>=</span> <span>array<span>::</span></span><span>from_fn</span><span>(</span><span><span>|</span>i<span>|</span></span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>value<span>[</span>i<span>]</span><span>,</span> <span>&amp;</span>other<span>.</span>value<span>[</span>i<span>]</span><span>)</span><span>)</span><span>;</span>
  <span>}</span>
</code></pre>
<p>Superficially, it looks fairly similar, but there are a couple of important differences.
Lets take it line by line.</p>
<p>First, we determine whether the local clock is higher than the other clock:</p>
<pre data-language="rust"><code is:raw=""><span>let</span> higher_clock <span>=</span> <span>self</span><span>.</span>clock<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>
</code></pre>
<p>If we think back to the logic gates, we can imagine whats going on under the hood here, right?
We built this exact circuit!
Ours only operated on two-bit numbers, but the idea was the same: accept two numbers and return a 0 or 1 indicating whether the first number is higher than the second.</p>
<p>(In our circuit, the result was a <em>plaintext</em> 0 or 1  but remember, homomorphic encryption operates with <em>encrypted</em> values!
The <code>gt</code> method actually returns an <code>FheBool</code>: an <em>encrypted bool</em> which indicates whether the local clock is higher than the other one.)</p>
<p>If we had the client key, we could decrypt that variable and find out its true value.
We cant do that, but we can still <em>combine it with other encrypted values</em> to write our merge algorithm.</p>
<p>Here are the conditions to break a tie between the clocks:</p>
<pre data-language="rust"><code is:raw=""><span>let</span> equal_clock <span>=</span> <span>self</span><span>.</span>clock<span>.</span><span>eq</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>
<span>let</span> higher_peer <span>=</span> <span>self</span><span>.</span>peer<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>peer<span>)</span><span>;</span>
</code></pre>
<p>Two more <code>FheBool</code>s indicating whether the clocks are equal and if the local peer ID is higher.</p>
<p>Next, we combine them:</p>
<pre data-language="rust"><code is:raw=""><span>let</span> keep_self <span>=</span> higher_clock <span>|</span> <span>(</span>equal_clock <span>&amp;</span> higher_peer<span>)</span><span>;</span>
</code></pre>
<p>This combines all those <code>FheBool</code>s to determine whether to keep the local data or overwrite it with the merged data.<sup><a href="#user-content-fn-astute" id="user-content-fnref-astute" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">4</a></sup></p>
<p>Those <code>|</code> and <code>&amp;</code> operators are bitwise AND and bitwise OR, which work exactly like the AND and OR logic gates we made earlier.
Theyre similar to the logical AND and OR were used to  <code>&amp;&amp;</code> and <code>||</code>, but with one big difference: bitwise operators are <em>eager</em>.
Whereas logical AND and OR might skip the second expression depending on the first, bitwise operators will <em>always</em> evaluate both sides.</p>
<p>Now that weve determined the register values to keep  even if we cant tell which ones  we need to write the data to the register.
Heres the secret sauce:</p>
<pre data-language="rust"><code is:raw=""><span>self</span><span>.</span>peer <span>=</span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>peer<span>,</span> <span>&amp;</span>other<span>.</span>peer<span>)</span><span>;</span>
<span>self</span><span>.</span>clock <span>=</span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>clock<span>,</span> <span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>
<span>self</span><span>.</span>value <span>=</span> <span>array<span>::</span></span><span>from_fn</span><span>(</span><span><span>|</span>i<span>|</span></span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>value<span>[</span>i<span>]</span><span>,</span> <span>&amp;</span>other<span>.</span>value<span>[</span>i<span>]</span><span>)</span><span>)</span><span>;</span>
</code></pre>
<p>Rather than <code>if</code> or <code>match</code> expressions, we use <code>FheBool</code>s <code>select</code> method.
It returns the first argument if the underlying <code>FheBool</code> value is <code>true</code>, or the second argument if the underlying value is <code>false</code>.</p>
<p>This is important: <em>the return value is different from both arguments</em>.
While decrypting the return value would reveal the same plaintext as one of the arguments, in ciphertext all three are distinct.
This means that we cant tell which values weve set on the register by the end of the merge.</p>
<p>When the merge is done, every piece of ciphertext has changed  the peer ID, the clock and the register value.
The plaintext values might have updated (or might not have!) but theres no way to tell by looking at the ciphertext.</p>
<p>Problem solved, right?
We can now have the server merge our CRDT without knowing what it contains?
Weeeellllllll</p>
<h2 id="fundamental-limitations">Fundamental Limitations</h2>
<p>Homomorphic encryption has constraints that sharply limit its effectiveness with regard to local-first software.</p>
<p>For starters: encryption keys.
In both the simple adding example and the last-write wins register, we generated a key that would be passed to the server.
That only needs to happen once, but the difference between the size of our key and the size of our data can be surprising.</p>
<p>Our register took up only 32 bytes of data  8 bytes each for the peer and clock, and 16 bytes for the value.
Meanwhile, TFHE-rs generated a <em>123 megabyte</em> server key.
We can compress the key down to about 27 megabytes, but still: thats almost 850,000 times more key than data!</p>
<p>The payload here is particularly small, but a disparity of that size isnt unheard of.
In his <a href="https://www.jeremykun.com/2024/05/04/fhe-overview/" data-astro-cid-bi7aps5f="">overview of fully homomorphic encryption</a><a data-tooltip="" href="https://www.jeremykun.com/2024/05/04/fhe-overview/" data-astro-cid-bi7aps5f="">  <span data-astro-cid-bi7aps5f="">A High-Level Technical Overview of Fully Homomorphic Encryption</span> <span data-astro-cid-bi7aps5f="">About two years ago, I switched teams at Google to focus on fully homomorphic encryption (abbreviated FHE, or sometimes HE). Since then Ive got to work on a lot of interesting projects, learning along the way about post-quantum cryptography, compiler design, and the ins and outs of fully homomorphic encryption.
If youve heard about FHE and youre a software person, youve probably heard two things: it lets you run programs directly on encrypted data without ever decrypting it; and its still too slow to be useful for anything.</span> <span data-astro-cid-bi7aps5f=""> <img src="https://www.jeremykun.com/favicon.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">www.jeremykun.com/2024/05/04/fhe-overview/</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a>, Jeremy Kun cites examples in which ciphertexts of dozens or hundreds of <em>kilobytes</em> require keys on the order of <em>gigabytes</em>.</p>
<p>Runtime performance is also  to put it lightly  lacking.
I benchmarked the unencrypted and encrypted versions of the last write wins register on an M4 MacBook Pro.
The unencrypted one averaged a merge time of 0.52 nanoseconds.</p>
<p>The encrypted one?
<em>1.06 seconds</em>.
Thats not a typo: the homomorphically encrypted merge is <em>two billion times slower</em>.<sup><a href="#user-content-fn-gpu" id="user-content-fnref-gpu" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">5</a></sup></p>
<p>Not great!</p>
<p>Thats not all.
We said before that our code must execute as though operating on the worst case input.
Even if the performance issues improve by many orders of magnitude, the worst case requirement will still impose constraints on the CRDT algorithm itself.</p>
<p>Consider a fully homomorphically encrypted last-write wins <em>map</em> CRDT.
Most maps store keys sparsely, so the map only grows in size as keys are added.</p>
<p>Heres a playground that simulates encrypting a sparse map.<sup><a href="#user-content-fn-pretend" id="user-content-fnref-pretend" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">6</a></sup>
When you modify the plaintext map on the left, the encrypted map on the right updates.
Can you see a security issue?</p>
<encrypted-map-demo></encrypted-map-demo>
<p>Imagine you only had access to the map on the right.
You could still see data being added and removed!
Furthermore, this map lazily encrypts only the data that changes, which would allow you to see exactly which key changed (if any).</p>
<p>A homomorphically encrypted map CRDT couldnt do that.
Since it must assume a worst-case input, it must store the keys <em>densely</em>: limiting the size to a fixed number of keys and reserving all the space up front.
Merging two identical maps would be exactly as computationally intensive as merging two maps in which <em>every</em> key was updated.<sup><a href="#user-content-fn-op" id="user-content-fnref-op" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">7</a></sup></p>
<p>The playground below simulates a homomorphically encrypted map.
While you can add and remove keys to the plaintext map on the left, the encrypted map on the right behaves as though every key is filled.
And no matter how you modify the plaintext map, <em>everything</em> in the encrypted map changes:</p>
<encrypted-map-demo dense="true" keys="4"></encrypted-map-demo>
<p>From the outside, theres no way to tell what changed in the map: we see the exact same number of keys, and every value has changed.
To calculate the new map, the server must go through and merge <em>every single key</em>.
After that, it needs to transfer the full map to each peer  because remember, as far as it knows, the entire map is different.</p>
<p>These are fundamental limitations of homomorphic encryption!
The requirement that homomorphically encrypted code performs as though operating on the worst-case input dramatically increases both the space and time required to update.</p>
<h2 id="parting-thoughts">Parting Thoughts</h2>
<p>I started this article thinking that local-first software and homomorphic encryption would be natural bedfellows.</p>
<p>But honestly, I came away a little less enamored.
The fundamental limitations of homomorphic encryption mean that it will always operate under a set of worst-case assumptions.
Homomorphically encrypted CRDTs arent intractable, but they are severely limited by these intrinsic constraints.</p>
<p>So the question remains: how can we secure local-first apps without severely degrading usability?</p>
<p>Luckily, Im not the only one thinking about this problem!</p>

<p>CRDTs are a relatively young technology  the paper formalizing them was published in 2011  so theres still a lot of unexplored solution space.
We may not have solved this problem yet, but Im confident that were closing in on it!</p>

<section data-footnotes="">
<ol>
<li id="user-content-fn-e2ee">
<p>More comprehensive solutions might try to implement things like <a href="https://en.wikipedia.org/wiki/Forward_secrecy" data-astro-cid-bi7aps5f="">forward secrecy</a><a data-tooltip="" href="https://en.wikipedia.org/wiki/Forward_secrecy" data-astro-cid-bi7aps5f=""> <img src="https://upload.wikimedia.org/wikipedia/commons/e/e0/KDF_chain.png" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">Forward secrecy - Wikipedia</span>  <span data-astro-cid-bi7aps5f=""> <img src="https://en.wikipedia.org/static/favicon/wikipedia.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">en.wikipedia.org/wiki/Forward_secrecy</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a>, which is absolutely not easy.
But the basic version is still better than nothing. <a href="#user-content-fnref-e2ee" data-footnote-backref="" aria-label="Back to reference 1" data-astro-cid-bi7aps5f=""></a></p>
</li>
<li id="user-content-fn-otherways">
<p>Homomorphic encryption isnt the only way to solve this problem.
You might instead just ignore it, and have the server store every version of your encrypted document  wasteful, but itd work!
You could also use a CRDT implementation that only requires <em>changes</em> to be sent to the server, rather than the full document. <a href="#user-content-fnref-otherways" data-footnote-backref="" aria-label="Back to reference 2" data-astro-cid-bi7aps5f=""></a></p>
</li>
<li id="user-content-fn-addition">
<p>You might notice that when both XOR inputs are 1, the result is 0.
You might also remember from math class that the result of 1 + 1 is, uh, not 0.
So how can XOR represent addition?</p>
<p>Remember that were operating on binary numbers  all we have is 0 and 1!
Adding to 1 in binary is like adding to 9 in decimal: since were out of digits, we instead roll that place back to 0 and <em>carry the 1 to the next place</em>, giving us 10.
The XOR gate represents the <em>sum digit</em> of that Boolean addition.
To fully represent the result, <a href="https://en.wikipedia.org/wiki/XOR_gate#Addition" data-astro-cid-bi7aps5f="">wed need to use the AND gate as well to represent the <em>carry digit</em></a><a data-tooltip="" href="https://en.wikipedia.org/wiki/XOR_gate#Addition" data-astro-cid-bi7aps5f="">  <span data-astro-cid-bi7aps5f="">XOR gate - Wikipedia</span>  <span data-astro-cid-bi7aps5f=""> <img src="https://en.wikipedia.org/static/favicon/wikipedia.ico#Addition" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">en.wikipedia.org/wiki/XOR_gate#Addition</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a>. <a href="#user-content-fnref-addition" data-footnote-backref="" aria-label="Back to reference 3" data-astro-cid-bi7aps5f=""></a></p>
</li>
<li id="user-content-fn-astute">
<p><em>Truly</em> eagle-eyed readers will notice that this is <em>also</em> the same circuit we used to determine whether one two-bit number was greater than another.
At a high level, the logic there was most significant bit is greater OR (most significant bits are equal AND least significant bit is greater).
Here, the logic is clock is greater OR (clocks are equal AND peer ID is greater). <a href="#user-content-fnref-astute" data-footnote-backref="" aria-label="Back to reference 4" data-astro-cid-bi7aps5f=""></a></p>
</li>
<li id="user-content-fn-gpu">
<p>Granted, I was only able to run it on the CPU  TFHE-rs only supports GPU acceleration on Linux  but even if I could run it on the GPU, <a href="https://docs.zama.ai/tfhe-rs/get-started/summary" data-astro-cid-bi7aps5f="">TFHE-rss benchmarks</a><a data-tooltip="" href="https://docs.zama.ai/tfhe-rs/get-started/summary" data-astro-cid-bi7aps5f="">  <span data-astro-cid-bi7aps5f="">TFHE-rs</span>  <span data-astro-cid-bi7aps5f=""> <img src="https://docs.zama.ai/~gitbook/image?url=https%3A%2F%2F572209210-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fcollections%252FprREL84Xd1lx94uAslRx%252Ficon%252F3R1LaM67E4BE3WhJZF5p%252FLogo%2520-%2520Square.png%3Falt%3Dmedia%26token%3Db39ed5d3-5537-4c62-9389-5b23f830072b&amp;width=48&amp;height=48&amp;sign=c9d8fc58&amp;sv=2" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">docs.zama.ai/tfhe-rs/get-started/summary</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a> indicate that it would only speed things up by a factor of 35.
Even at the high end of that range, the encrypted merge would <em>still</em> be 400 million times slower than the unencrypted one. <a href="#user-content-fnref-gpu" data-footnote-backref="" aria-label="Back to reference 5" data-astro-cid-bi7aps5f=""></a></p>
</li>
<li id="user-content-fn-pretend">
<p>When I say simulates encrypting, I mean displays a bunch of random hex digits.
Please humor me! <a href="#user-content-fnref-pretend" data-footnote-backref="" aria-label="Back to reference 6" data-astro-cid-bi7aps5f=""></a></p>
</li>
<li id="user-content-fn-op">
<p>Note that this all assumes a <em>state-based</em> CRDT.
An <em>operation-based</em> CRDT  where the important operation is appending to a log of events rather than merging  might have a totally different set of tradeoffs. <a href="#user-content-fnref-op" data-footnote-backref="" aria-label="Back to reference 7" data-astro-cid-bi7aps5f=""></a></p>
</li>
<li id="user-content-fn-meri">
<p>Meri Leeworthy published a great <a href="https://meri.garden/a-deep-dive-explainer-on-beekem-protocol/" data-astro-cid-bi7aps5f="">deep-dive explainer on KeyHives key encapsulation mechanism</a><a data-tooltip="" href="https://meri.garden/a-deep-dive-explainer-on-beekem-protocol/" data-astro-cid-bi7aps5f=""> <img src="https://static.meri.garden/mesh-gradient.png" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">A deep-dive explainer on Ink and Switch's BeeKEM protocol</span> <span data-astro-cid-bi7aps5f="">I'm a programmer, designer, writer and artist. I try to make tools for community autonomy, creativity, and resistance.</span> <span data-astro-cid-bi7aps5f=""> <img src="https://meri.garden/favicon.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">meri.garden/a-deep-dive-explainer-on-beekem-protocol/</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a> which is absolutely worth a read! <a href="#user-content-fnref-meri" data-footnote-backref="" aria-label="Back to reference 8" data-astro-cid-bi7aps5f=""></a></p>
</li>
</ol>
</section> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Workout.cool  Open-source fitness coaching platform (462 pts)]]></title>
            <link>https://github.com/Snouzy/workout-cool</link>
            <guid>44309320</guid>
            <pubDate>Wed, 18 Jun 2025 12:33:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Snouzy/workout-cool">https://github.com/Snouzy/workout-cool</a>, See on <a href="https://news.ycombinator.com/item?id=44309320">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#about">About</a></li>
<li><a href="#-project-origin--motivation">Project Origin &amp; Motivation</a></li>
<li><a href="#quick-start">Quick Start</a></li>
<li><a href="#exercise-database-import">Exercise Database Import</a></li>
<li><a href="#project-architecture">Project Architecture</a></li>
<li><a href="#roadmap">Roadmap</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#deployment">Deployment</a></li>
<li><a href="#resources">Resources</a></li>
<li><a href="#license">License</a></li>
<li><a href="#-sponsor-this-project">Sponsor This Project</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributors</h2><a id="user-content-contributors" aria-label="Permalink: Contributors" href="#contributors"></a></p>
<a href="https://github.com/Snouzy/workout-cool/graphs/contributors">
  <img src="https://camo.githubusercontent.com/7c851db85ae8783e22cf0dabfd6e6cce46c6c360fed141a0f287fa87d478c4f7/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d536e6f757a792f776f726b6f75742d636f6f6c" data-canonical-src="https://contrib.rocks/image?repo=Snouzy/workout-cool">
</a>
<p dir="auto"><h2 tabindex="-1" dir="auto">About</h2><a id="user-content-about" aria-label="Permalink: About" href="#about"></a></p>
<p dir="auto">A comprehensive fitness coaching platform that allows create workout plans for you, track progress, and access a vast exercise database with
detailed instructions and video demonstrations.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"> Project Origin &amp; Motivation</h2><a id="user-content--project-origin--motivation" aria-label="Permalink:  Project Origin &amp; Motivation" href="#-project-origin--motivation"></a></p>
<p dir="auto">This project was born from a personal mission to revive and improve upon a previous fitness platform. As the <strong>primary contributor</strong> to the
original <a href="https://github.com/workout-lol/workout-lol">workout.lol</a> project, I witnessed its journey and abandonment. </p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Story Behind <strong><em>workout.cool</em></strong></h3><a id="user-content-the-story-behind-workoutcool" aria-label="Permalink: The Story Behind workout.cool" href="#the-story-behind-workoutcool"></a></p>
<ul dir="auto">
<li> <strong>Original Contributor</strong>: I was the main contributor to workout.lol</li>
<li> <strong>Business Challenges</strong>: The original project faced major hurdles with exercise video partnerships (no reliable video provider) could
be established</li>
<li> <strong>Project Sale</strong>: Due to these partnership issues, the project was sold to another party</li>
<li> <strong>Abandonment</strong>: The new owner quickly realized that <strong>exercise video licensing costs were prohibitively expensive</strong>, began to be sick
and abandoned the entire project</li>
<li> <strong>Revival Attempts</strong>: For the past <strong>9 months</strong>, I've been trying to reconnect with the new stakeholder</li>
<li> <strong>Radio Silence</strong>: Despite multiple (15) attempts, there has been no response</li>
<li> <strong>New Beginning</strong>: Rather than let this valuable work disappear, I decided to create a fresh, modern implementation</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why <strong><em>workout.cool</em></strong> Exists</h3><a id="user-content-why-workoutcool-exists" aria-label="Permalink: Why workout.cool Exists" href="#why-workoutcool-exists"></a></p>
<p dir="auto"><strong>Someone had to step up.</strong></p>
<p dir="auto">The opensource fitness community deserves better than broken promises and abandoned platforms.</p>
<p dir="auto">I'm not building this for profit.</p>
<p dir="auto">This isn't just a revival : it's an evolution. <strong>workout.cool</strong> represents everything the original project could have been, with the
reliability, modern approach, and <strong>maintenance</strong> that the fitness open source community deserves.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"> From the Community, For the Community</h2><a id="user-content--from-the-community-for-the-community" aria-label="Permalink:  From the Community, For the Community" href="#-from-the-community-for-the-community"></a></p>
<p dir="auto"><strong>I'm not just a developer : I'm a user who refused to let our community down.</strong></p>
<p dir="auto">I experienced firsthand the frustration of watching a beloved tool slowly disappear. Like many of you, I had workouts saved, progress
tracked, and a routine built around the platform.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">My Mission: Rescue &amp; Revive.</h3><a id="user-content-my-mission-rescue--revive" aria-label="Permalink: My Mission: Rescue &amp; Revive." href="#my-mission-rescue--revive"></a></p>
<p dir="auto"><em>If you were part of the original workout.lol community, welcome back! If you're new here, welcome to the future of fitness platform
management.</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prerequisites</h3><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<ul dir="auto">
<li>Node.js 18+</li>
<li>Either:
<ul dir="auto">
<li>Docker</li>
<li>OR PostgreSQL external database</li>
</ul>
</li>
<li>pnpm (recommended) or npm</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Clone the repository</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/Snouzy/workout-cool.git
cd workout-cool"><pre>git clone https://github.com/Snouzy/workout-cool.git
<span>cd</span> workout-cool</pre></div>
</li>
<li>
<p dir="auto"><strong>Install dependencies</strong></p>

</li>
<li>
<p dir="auto"><strong>Set up environment variables</strong></p>

<p dir="auto">Fill in your database URL and other required environment variables:</p>
<div dir="auto" data-snippet-clipboard-copy-content="DATABASE_URL=&quot;postgresql://username:password@localhost:5432/workout_cool&quot;
BETTER_AUTH_SECRET=&quot;your-secret-key&quot;
# ... other variables"><pre><span>DATABASE_URL</span><span>=</span><span><span>"</span>postgresql://username:password@localhost:5432/workout_cool<span>"</span></span>
<span>BETTER_AUTH_SECRET</span><span>=</span><span><span>"</span>your-secret-key<span>"</span></span>
<span><span>#</span> ... other variables</span></pre></div>
</li>
<li>
<p dir="auto"><strong>Set up the database</strong></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Option 1: Using Docker</h4><a id="user-content-option-1-using-docker" aria-label="Permalink: Option 1: Using Docker" href="#option-1-using-docker"></a></p>
<p dir="auto">The project provides a convenient <code>make</code> command that handles everything:</p>

<p dir="auto">This single command will:</p>
<ul dir="auto">
<li>Start the PostgreSQL database using Docker</li>
<li>Run database migrations</li>
<li>Start the development server</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Option 2: Manual PostgreSQL Setup</h4><a id="user-content-option-2-manual-postgresql-setup" aria-label="Permalink: Option 2: Manual PostgreSQL Setup" href="#option-2-manual-postgresql-setup"></a></p>
<p dir="auto">If you prefer to use your own PostgreSQL installation:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run migrations
npx prisma migrate deploy
npx prisma generate

# Start the development server
pnpm dev"><pre><span><span>#</span> Run migrations</span>
npx prisma migrate deploy
npx prisma generate

<span><span>#</span> Start the development server</span>
pnpm dev</pre></div>
</li>
<li>
<p dir="auto"><strong>Open your browser</strong> Navigate to <a href="http://localhost:3000/" rel="nofollow">http://localhost:3000</a></p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Exercise Database Import</h2><a id="user-content-exercise-database-import" aria-label="Permalink: Exercise Database Import" href="#exercise-database-import"></a></p>
<p dir="auto">The project includes a comprehensive exercise database. To import a sample of exercises:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prerequisites for Import</h3><a id="user-content-prerequisites-for-import" aria-label="Permalink: Prerequisites for Import" href="#prerequisites-for-import"></a></p>
<ol dir="auto">
<li><strong>Prepare your CSV file</strong></li>
</ol>
<p dir="auto">Your CSV should have these columns:</p>
<div data-snippet-clipboard-copy-content="id,name,name_en,description,description_en,full_video_url,full_video_image_url,introduction,introduction_en,slug,slug_en,attribute_name,attribute_value"><pre><code>id,name,name_en,description,description_en,full_video_url,full_video_image_url,introduction,introduction_en,slug,slug_en,attribute_name,attribute_value
</code></pre></div>
<p dir="auto">You can use the provided example.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Import Commands</h3><a id="user-content-import-commands" aria-label="Permalink: Import Commands" href="#import-commands"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Import exercises from a CSV file
pnpm run import:exercises-full /path/to/your/exercises.csv

# Example with the provided sample data
pnpm run import:exercises-full ./data/sample-exercises.csv"><pre><span><span>#</span> Import exercises from a CSV file</span>
pnpm run import:exercises-full /path/to/your/exercises.csv

<span><span>#</span> Example with the provided sample data</span>
pnpm run import:exercises-full ./data/sample-exercises.csv</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">CSV Format Example</h3><a id="user-content-csv-format-example" aria-label="Permalink: CSV Format Example" href="#csv-format-example"></a></p>
<div data-snippet-clipboard-copy-content="id,name,name_en,description,description_en,full_video_url,full_video_image_url,introduction,introduction_en,slug,slug_en,attribute_name,attribute_value
157,&quot;Fentes arrires  la barre&quot;,&quot;Barbell Reverse Lunges&quot;,&quot;<p>Stand upright...</p>&quot;,&quot;<p>Stand upright...</p>&quot;,https://youtube.com/...,https://img.youtube.com/...,slug-fr,slug-en,TYPE,STRENGTH
157,&quot;Fentes arrires  la barre&quot;,&quot;Barbell Reverse Lunges&quot;,&quot;<p>Stand upright...</p>&quot;,&quot;<p>Stand upright...</p>&quot;,https://youtube.com/...,https://img.youtube.com/...,slug-fr,slug-en,PRIMARY_MUSCLE,QUADRICEPS"><pre lang="csv"><code>id,name,name_en,description,description_en,full_video_url,full_video_image_url,introduction,introduction_en,slug,slug_en,attribute_name,attribute_value
157,"Fentes arrires  la barre","Barbell Reverse Lunges","&lt;p&gt;Stand upright...&lt;/p&gt;","&lt;p&gt;Stand upright...&lt;/p&gt;",https://youtube.com/...,https://img.youtube.com/...,slug-fr,slug-en,TYPE,STRENGTH
157,"Fentes arrires  la barre","Barbell Reverse Lunges","&lt;p&gt;Stand upright...&lt;/p&gt;","&lt;p&gt;Stand upright...&lt;/p&gt;",https://youtube.com/...,https://img.youtube.com/...,slug-fr,slug-en,PRIMARY_MUSCLE,QUADRICEPS
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Available Attribute Types</h3><a id="user-content-available-attribute-types" aria-label="Permalink: Available Attribute Types" href="#available-attribute-types"></a></p>
<ul dir="auto">
<li><strong>TYPE</strong>: <code>STRENGTH</code>, <code>CARDIO</code>, <code>PLYOMETRICS</code>, <code>STRETCHING</code>, etc.</li>
<li><strong>PRIMARY_MUSCLE</strong>: <code>QUADRICEPS</code>, <code>CHEST</code>, <code>BACK</code>, <code>SHOULDERS</code>, etc.</li>
<li><strong>SECONDARY_MUSCLE</strong>: Secondary muscle groups targeted</li>
<li><strong>EQUIPMENT</strong>: <code>BARBELL</code>, <code>DUMBBELL</code>, <code>BODYWEIGHT</code>, <code>MACHINE</code>, etc.</li>
<li><strong>MECHANICS_TYPE</strong>: <code>COMPOUND</code>, <code>ISOLATION</code></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Project Architecture</h2><a id="user-content-project-architecture" aria-label="Permalink: Project Architecture" href="#project-architecture"></a></p>
<p dir="auto">This project follows <strong>Feature-Sliced Design (FSD)</strong> principles with Next.js App Router:</p>
<div data-snippet-clipboard-copy-content="src/
 app/ # Next.js pages, routes and layouts
 processes/ # Business flows (multi-feature)
 widgets/ # Composable UI with logic (Sidebar, Header)
 features/ # Business units (auth, exercise-management)
 entities/ # Domain entities (user, exercise, workout)
 shared/ # Shared code (UI, lib, config, types)
 styles/ # Global CSS, themes"><pre><code>src/
 app/ # Next.js pages, routes and layouts
 processes/ # Business flows (multi-feature)
 widgets/ # Composable UI with logic (Sidebar, Header)
 features/ # Business units (auth, exercise-management)
 entities/ # Domain entities (user, exercise, workout)
 shared/ # Shared code (UI, lib, config, types)
 styles/ # Global CSS, themes
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Architecture Principles</h3><a id="user-content-architecture-principles" aria-label="Permalink: Architecture Principles" href="#architecture-principles"></a></p>
<ul dir="auto">
<li><strong>Feature-driven</strong>: Each feature is independent and reusable</li>
<li><strong>Clear domain isolation</strong>: <code>shared</code>  <code>entities</code>  <code>features</code>  <code>widgets</code>  <code>app</code></li>
<li><strong>Consistency</strong>: Between business logic, UI, and data layers</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example Feature Structure</h3><a id="user-content-example-feature-structure" aria-label="Permalink: Example Feature Structure" href="#example-feature-structure"></a></p>
<div data-snippet-clipboard-copy-content="features/
 exercise-management/
 ui/ # UI components (ExerciseForm, ExerciseCard)
 model/ # Hooks, state management (useExercises)
 lib/ # Utilities (exercise-helpers)
 api/ # Server actions or API calls"><pre><code>features/
 exercise-management/
 ui/ # UI components (ExerciseForm, ExerciseCard)
 model/ # Hooks, state management (useExercises)
 lib/ # Utilities (exercise-helpers)
 api/ # Server actions or API calls
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Roadmap</h2><a id="user-content-roadmap" aria-label="Permalink: Roadmap" href="#roadmap"></a></p>
<p dir="auto">Here are the next steps and goals for Workout.cool:</p>
<ul>
<li>  Add new exercises and videos</li>
<li>  Mobile app (React Native)</li>
<li>  Badges and gamification system</li>
<li>  Advanced progress statistics</li>
<li>  Integration with wearables (watches, trackers)</li>
<li>  Multilingual support</li>
<li>  OAuth authentication (Google, Apple, etc.)</li>
<li>  Built-in community forum</li>
</ul>
<p dir="auto">Feel free to suggest your ideas via <a href="https://github.com/Snouzy/workout-cool/issues">issues</a>!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">We welcome contributions! Please see our <a href="https://github.com/Snouzy/workout-cool/blob/main/CONTRIBUTING.md">Contributing Guide</a> for details.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Development Workflow</h3><a id="user-content-development-workflow" aria-label="Permalink: Development Workflow" href="#development-workflow"></a></p>
<ol dir="auto">
<li>Fork the repository</li>
<li>Create a feature branch (<code>git checkout -b feature/amazing-feature</code>)</li>
<li>Make your changes</li>
<li>Commit your changes (<code>git commit -m 'feat: add amazing feature'</code>)</li>
<li>Push to the branch (<code>git push origin feature/amazing-feature</code>)</li>
<li>Open a Pull Request</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Code Style</h3><a id="user-content-code-style" aria-label="Permalink: Code Style" href="#code-style"></a></p>
<ul dir="auto">
<li>Follow TypeScript best practices</li>
<li>Use Feature-Sliced Design architecture</li>
<li>Write meaningful commit messages</li>
<li>Add tests for new features</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Deployment</h2><a id="user-content-deployment" aria-label="Permalink: Deployment" href="#deployment"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using Docker (Not ready yet : todo)</h3><a id="user-content-using-docker-not-ready-yet--todo" aria-label="Permalink: Using Docker (Not ready yet : todo)" href="#using-docker-not-ready-yet--todo"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Build the Docker image
docker build -t workout-cool .

# Run the container
docker run -p 3000:3000 workout-cool"><pre><span><span>#</span> Build the Docker image</span>
docker build -t workout-cool <span>.</span>

<span><span>#</span> Run the container</span>
docker run -p 3000:3000 workout-cool</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Manual Deployment</h3><a id="user-content-manual-deployment" aria-label="Permalink: Manual Deployment" href="#manual-deployment"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Build the application
pnpm build

# Run database migrations
export DATABASE_URL=&quot;your-production-db-url&quot;
npx prisma migrate deploy

# Start the production server
pnpm start"><pre><span><span>#</span> Build the application</span>
pnpm build

<span><span>#</span> Run database migrations</span>
<span>export</span> DATABASE_URL=<span><span>"</span>your-production-db-url<span>"</span></span>
npx prisma migrate deploy

<span><span>#</span> Start the production server</span>
pnpm start</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Resources</h2><a id="user-content-resources" aria-label="Permalink: Resources" href="#resources"></a></p>
<ul dir="auto">
<li><a href="https://feature-sliced.design/" rel="nofollow">Feature-Sliced Design</a></li>
<li><a href="https://nextjs.org/docs" rel="nofollow">Next.js Documentation</a></li>
<li><a href="https://www.prisma.io/docs/" rel="nofollow">Prisma Documentation</a></li>
<li><a href="https://github.com/better-auth/better-auth">Better Auth</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the MIT License. See the <a href="https://github.com/Snouzy/workout-cool/blob/main/LICENSE">LICENSE</a> file for details.</p>
<p dir="auto"><a href="https://github.com/Snouzy/workout-cool/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/28f4d479bf0a9b033b3a3b95ab2adc343da448a025b01aefdc0fbc7f0e169eb8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d677265656e2e737667" alt="MIT License" data-canonical-src="https://img.shields.io/badge/License-MIT-green.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"> Join the Rescue Mission</h2><a id="user-content--join-the-rescue-mission" aria-label="Permalink:  Join the Rescue Mission" href="#-join-the-rescue-mission"></a></p>
<p dir="auto"><strong>This is about rebuilding what we lost, together.</strong></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How You Can Help</h3><a id="user-content-how-you-can-help" aria-label="Permalink: How You Can Help" href="#how-you-can-help"></a></p>
<ul dir="auto">
<li> <strong>Star this repo</strong> to show the world our community is alive and thriving</li>
<li> <strong>Report issues</strong> you find. I'm listening to every single one</li>
<li> <strong>Share your feature requests</strong> finally, someone who will actually implement them !</li>
<li> <strong>Spread the word</strong> to fellow fitness enthusiasts who lost hope</li>
<li> <strong>Contribute code</strong> if you're a developer : let's build this together</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto"> Sponsor This Project</h2><a id="user-content--sponsor-this-project" aria-label="Permalink:  Sponsor This Project" href="#-sponsor-this-project"></a></p>
<p dir="auto">Appear in the README and on the website as supporter by donating:</p>
<p><a href="https://ko-fi.com/workoutcool" rel="nofollow">
    <img src="https://camo.githubusercontent.com/70e2ef5e0263b261f9a2a314bb1d6919d1d43292eed117fe8fc766a68c7d96ea/68747470733a2f2f6b6f2d66692e636f6d2f696d672f676974687562627574746f6e5f736d2e737667" alt="Sponsor on Ko-fi" data-canonical-src="https://ko-fi.com/img/githubbutton_sm.svg">
  </a>
  &nbsp;&nbsp;&nbsp;
  
  
</p>
<p dir="auto">
  <em>If you believe in open-source fitness tools and want to help this project thrive,<br>
  consider buying me a coffee  or sponsoring the continued development.</em>
</p>
<p dir="auto">
  Your support helps cover hosting costs, exercise database updates, and continuous improvement.<br>
  Thank you for keeping <strong>workout.cool</strong> alive and evolving 
</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is There a Half-Life for the Success Rates of AI Agents? (169 pts)]]></title>
            <link>https://www.tobyord.com/writing/half-life</link>
            <guid>44308711</guid>
            <pubDate>Wed, 18 Jun 2025 10:53:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tobyord.com/writing/half-life">https://www.tobyord.com/writing/half-life</a>, See on <a href="https://news.ycombinator.com/item?id=44308711">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
          <div data-collection-id="605a132f7a36283cfa0c4af9" data-edit-main-image="Banner">
            <p><h2>Writing</h2></p>
            
          </div>
            <section data-content-field="main-content" data-collection-id="605a132f7a36283cfa0c4af9" data-edit-main-image="Banner">  
  <article id="post-681b2734c79dc21300b23894" data-item-id="681b2734c79dc21300b23894">
    
    <!--POST TILE-->

    

    

    <!--MAIN CONTENT-->

    <div data-layout-label="Post Body" data-type="item" data-updated-on="1746610169593" id="item-681b2734c79dc21300b23894"><div data-block-type="2" id="block-84469d78d64003385a03">
  <p>Building on the recent empirical work of Kwa et al. (2025), I show that within their suite of research-engineering tasks the performance of AI agents on longer-duration tasks can be explained by an extremely simple mathematical model  a constant rate of failing during each minute a human would take to do the task. This implies an exponentially declining success rate with the length of the task and that each agent could be characterised by its own half-life. This empirical regularity allows us to estimate the success rate for an agent at different task lengths. And the fact that this model is a good fit for the data is suggestive of the underlying causes of failure on longer tasks  that they involve increasingly large sets of subtasks where failing any one fails the task. Whether this model applies more generally on other suites of tasks is unknown and an important subject for further work.</p><p><strong>METRs results on the length of tasks agents can reliably complete</strong></p><p>A recent paper by <a href="https://doi.org/10.48550/arXiv.2503.14499"><span>Kwa et al. (2025)</span></a> from the research organisation <a href="https://metr.org/" target="_blank">METR</a> has found an exponential trend in the duration of the tasks that frontier AI agents can solve: every 7 months, the length of task they can solve doubles.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1746614245989_8461">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png" data-image-dimensions="1926x1070" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png" width="1926" height="1070" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1746614245989_8778">

<p>These headline results are based on&nbsp; a test suite of 170 software engineering, cybersecurity, general reasoning, and ML tasks that they assembled to be indicative of the kinds of tasks that could help AI agents assist in AI research. These tasks are assembled from three different benchmarks that take different amounts of time for humans to achieve:</p>




















  
  



</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1746614245989_11913">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png" data-image-dimensions="1820x1028" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png" width="1820" height="1028" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1746614245989_12229">
  <p>In general, ability to perform a task drops off as its duration increases, so they use the AI agents performance on tasks of different lengths to estimate the task-length at which the model would have a 50% success rate. They then showed that this length has been doubling every 7 months as the capabilities of frontier agents improve. The task-lengths are measured by how long it took humans to solve the same tasks.</p><p>They used 50% success rate as their chief performance threshold because it is the easiest level to robustly estimate. They are well aware that for many tasks, the required success rate for useful work may be much higher  such as 80%, 99%, or 99.9999%. They do measure the 80% success rate and find their mean estimate to have a doubling time of 213 days, compared to 212 days of the 50% rate. These are close to identical within their margin of error (40 days for the 50% rate), so they conclude that the particular threshold doesnt seem to have much effect on their headline result about the rate of improvement.</p><p>But there is quite a gap between the 50% success rate time-horizon and the 80% success rate time horizon. For the best model (Claude 3.7 Sonnet) it could achieve a 50% success rate on tasks up to 59 minutes <em>vs</em> only 15 minutes if an 80% success rate was required. If those results generalise to the other models, then we could also see it like this: the task length for an 80% success rate is 1/4 the task length for a 50% success rate. Or in terms of improvement: what is doable with a 50% success rate now is doable with an 80% success rate in 14 months time (= 2 doubling times).</p><p>The idea of measuring improvement in AI capabilities over time via time horizons at a chosen success rate is novel and interesting. AI forecasting is often hamstrung by the lack of a good measure for the y-axis of performance over time. We can track progress within a particular benchmark, but these are often solved in a couple of years, and we lack a good measure of underlying capability that can span multiple benchmarks. METRs measure allows comparisons between very different kinds of tasks in a common currency (time it takes a human) and shows a strikingly clear trend line  suggesting it is measuring something real.</p><p>But there are grounds for criticism too. In particular, there is room to wonder how much these results generalise outside of this kind of task suite. We know that there are some tasks humans can do very quickly that AIs cant solve (e.g. some simple spatial reasoning or intuitive physics tasks) and others that would take humans an extremely long time, but AIs can do quickly (e.g. rote mathematics). So a simple measure of time it would take a human cannot explain all AI capability improvements. Kwa et al. are&nbsp; aware of this and even list several other ways that this task-suite may be non-representative of real-world performance including:</p><ul data-rte-list="default"><li><p>All tasks were automatically scorable (a domain where RL works best)</p></li><li><p>No interaction with other agents</p></li><li><p>Lax resource constraints</p></li></ul><p>For the purposes of this essay, we will take the data for what it is (performance on a particular task suite that may or may not generalise further) and explore underlying mechanisms that could explain it.</p><h3><strong>Explaining these results via a constant hazard rate</strong></h3><p>These results call out for some explanation of what is going on. For example, exactly how does the time horizon shrink as the required success probability is increased? And what does it <em>mean</em> for an agent to be able to perform an 8-hour task, but not a 16-hour task. Isnt a 16-hour task just one 8-hour task after another?</p><p><a href="https://en.wikipedia.org/wiki/Survival_analysis" target="_blank">Survival analysis</a> is the field of understanding how the probability of something failing increases as a function of time. It tracks the survival probability at a time <em>S</em>(<em>t</em>)  that is, the chance it still hasnt failed by that point. The simplest model in survival analysis is a constant hazard rate. This means that the chance of something failing in the next step (conditional on making it that far) is constant. A constant hazard rate leads to an exponentially declining survival curve. This behaviour is well-known from phenomena like radioactive decay, where there is a constant chance of decay at any moment, leading to an exponentially declining chance of the isotopes survival over time, which is often measured by a half-life.</p><p>If AI agent success-rates drop off with task length in this manner, then the 50% success rate time-horizon for each agent from Kwa et al. is precisely the <em>half-life</em> of that agent. As with the half-life of a radioisotope, this isnt just the median lifespan, it is the median remaining lifespan starting at any time  something that is only possible for an exponential survival curve. Unlike for particles, this AI agent half-life would be measured not in clock time, but in how long it takes a human to complete the task.</p><p>One rationale for this constant hazard rate model for AI agents is that tasks require getting past a series of steps each of which could end your attempt, with the longer the duration of the task, the more such steps. More precisely, if tasks could be broken down into a long sequence of equal-length subtasks with a constant (and independent) chance of failure, such that to succeed in the whole task, the agent needs to succeed in <em>all</em> subtasks, then that would create an exponential survival curve. I.e. when Pr(<em>Task</em>) = Pr(<em>Subtask</em>$_1$ &amp; <em>Substask</em>$_2$ &amp;  &amp; <em>Subtask</em>$_N$).</p><p>But we dont have to assume a perfect breakdown of the task into equal-length-equal-difficulty subtasks in order to get an exponential distribution (and corresponding constant hazard rate). We can also think of the constant hazard rate model as being agnostic as to how the task is broken down, just saying that the chance of succeeding in a subtask of duration <em>t</em> is always equal to the chance of succeeding in each of a set of smaller subtasks whose combined duration is also <em>t</em>. So on this model, it doesnt matter what level of granularity you assess the task at  whether you see it as one 60-minute task, six 10-minute tasks, or a 20-minute task plus forty 1-minute tasks  the chance of succeeding is set by the total time it would take a human to complete it.</p><p>This constant hazard rate model would predict that the time horizon for an 80% success rate is about  of the time horizon for a 50% success rate. This is because the chance of surviving three periods with an 80% success rate = (0.8)$^3$ = 0.512  50%. More precisely, the time horizon for a success probability of <em>p</em> would be ln(<em>p</em>)/ln(<em>q</em>) times as long as one with success probability <em>q</em>. So an 80% time-horizon would be ln(0.8)/ln(0.5) = 0.322 times as long as the 50% time-horizon. Kwa et al. estimate the 80% time-horizon for Claude 3.7 Sonnet to be 0.25 as long, which is close to this theoretical estimate and within the margin of error given the noisiness of the results. The following chart shows how these numbers relate to the exponential survival curve (where <em>T</em>$_{50}$ is the 50% time-horizon etc.).</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1746614245989_24919">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png" data-image-dimensions="2427x1731" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png" width="2427" height="1731" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1746614245989_25236">
  <p>Here are some useful comparisons for how the predicted time horizons over which an agent could get very high success rates compare to the measured time horizon for a 50% success rate:</p><p><em>&nbsp; T</em>$_{80}$  1/3&nbsp;<em>T</em>$_{50}$<br>&nbsp;&nbsp;<em>T</em>$_{90}$  1/7&nbsp;<em>T</em>$_{50}$<br>&nbsp;&nbsp;<em>T</em>$_{99}$  1/70 <em>T</em>$_{50}$<br>&nbsp;&nbsp;<em>T</em>$_{99.9}$  1/700 <em>T</em>$_{50}$<em><br>&nbsp; [and each additional nine of reliability beyond this divides the time horizon by 10]</em></p><p>We can also use this model to calculate how long wed expect it to take between the 50% success rate time horizon reaching a given length and a high success rate time horizon reaching that some length (on the assumption of the 7-month doubling time and the constant hazard rate model): </p><p><em>&nbsp; T</em>$_{80}$ reaches any particular length about 1 year after <em>T</em>$_{50}$ does<br>&nbsp;&nbsp;<em>T</em>$_{90}$ reaches any particular length about 2 years after <em>T</em>$_{50}$ does<br>&nbsp;&nbsp;<em>T</em>$_{99}$ reaches any particular length about 4 years after <em>T</em>$_{50}$ does<br>&nbsp;&nbsp;<em>T</em>$_{99.9}$ reaches any particular length about 6 years after <em>T</em>$_{50}$ does<br>&nbsp;&nbsp;<em>[each additional nine of reliability requires 2 more years]</em></p><p>Kwa et al. also attempt to fit a relationship between the success probability and time-horizon (see the figure below, which Ive adapted from their paper). They plot the time horizon on a log scale and note that this reveals a sigmoid-shaped decay curve of success rate (the coloured bars). They show that this is reasonably well fit by a logistic function (the black curves). The paper doesnt compare this to how well alternative functions would fit the data.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1746614245989_26796">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png" data-image-dimensions="2503x1718" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png" width="2503" height="1718" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1746614245989_27113">
  <p>The data is also well-fit by an exponential function, which also looks like a sigmoid when plotted on this logarithmic x-axis. Ive added this to the above figure (from Kwa et al.) in the form of the dotted blue curves.&nbsp; It fits the data better for some models (such as the top left) and worse on others. It fits the data roughly as well overall, while being substantially more likely <em>a priori</em>  exponential decay is <em>the </em>simplest survival curve and has only one free parameter instead of two. Moreover, while logistic functions are quite simple and natural, the black curve here is not really a logistic distribution, but the more complex log-logistic distribution which merely looks like a logistic distribution when plotted on a logarithmic x-axis.</p><p>The paper also plots the survival curve for <em>human</em> performance over increasing time periods:</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1746614245989_28357">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png" data-image-dimensions="2148x760" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png" width="2148" height="760" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1746614245989_28674">
  <p>Intriguingly, this human survival curve seems to be noticeably better than a constant hazard rate (i.e. the chance of succeeding over long timescales drops off more slowly for humans than the constant hazard rate predicts). For example, on this graph, humans had about a 50% success rate at the 1.5-hour mark, which suggests 25% at 3 hours, 12.5% at 6 hours and 6.25% at 12 hours if the hazard rate is constant. However, the humans were still above 20% success rate at that point.&nbsp;</p><p>This could indicate a different scaling behaviour of success rate with time horizon for humans compared to AI agents, which would be well worth investigating and may suggest important underlying mechanisms (e.g. that the humans were better at correcting earlier failed subtasks). If human performance scales differently with task length than AI agent performance, that would be an important result, suggesting that there is a notable inefficiency in the current AI paradigm. This warrants further research.&nbsp;</p><p>However, there are other potential explanations. For instance, it could also be an artefact of this graph being an aggregate of humans with different ability levels, since even if all individual humans have a constant hazard rate (and so each have an exponential survival curve) a mixture of different humans would be a weighted sum of exponentials with different time constants and that distribution decays slower than an exponential (see <a href="https://arxiv.org/abs/2308.09045"><span>Ord (2023)</span></a> for details).</p><p>The AI agents dont suffer from mixing different capability levels into the same statistics because there is a separate data series for each agent. However, there is a similar effect that could still be present in the measurement of AI agents performance over increasingly long tasks. It is plausible that some tasks are inherently easier than others per unit time, corresponding to different hazard rates. If so, then the survival curve over the whole task suite would be averaging different exponential decay curves together. This produces an aggregate decay curve with thicker tails than an exponential (corresponding to a declining effective hazard rate). Again, see <a href="https://arxiv.org/abs/2308.09045"><span>Ord (2023)</span></a> for details. Dealing with this effect is important as it could still be the case that the mechanism of constant hazard rate (and what it implies about the agents behaviour) holds for every task, even if it isnt visible in the aggregate of these tasks.</p><h3><strong>Upshots of the constant hazard rate model</strong></h3><p>If the constant hazard rate model is sufficient to explain the drop-off in success rates on a task suite, there are several interesting upshots:</p><ul data-rte-list="default"><li><p>It allows us to make predictions for the time-horizons at other success rates, such as 90% and 99%.</p></li></ul><ul data-rte-list="default"><li><p>It allows us to make predictions for how the success rate improves over time for a fixed task-length.</p></li></ul><ul data-rte-list="default"><li><p>It provides simple rules of thumb for predicting success probabilities (e.g. that if you double the task duration, you square the success probability).</p></li></ul><ul data-rte-list="default"><li><p>It suggests that AI agent performance (at least on this task suite) can be characterised by a half-life.</p></li></ul><ul data-rte-list="default"><li><p>It provides indirect evidence that what really is going on under the hood is that tasks are made up of many sequential subtasks and the chance of succeeding at the whole requires succeeding at every individual component. Moreover, this suggests that the current AI agents are not very good at recovering from earlier mistakes.</p></li></ul><ul data-rte-list="default"><li><p>Because the exponential distribution is the unique memoryless distribution, another way of seeing it is that the chance of failing at the next moment is independent of how far youve come  just like how the chance of a radioisotope decaying in the next minute is independent on how many minutes it has survived so far. This would be a surprising and interesting property for reasoning agents.</p></li></ul><ul data-rte-list="default"><li><p>Deviations from exponential decay for certain models may provide evidence that their hazard rate is increasing (or decreasing) with time, which might provide hints as to what they are doing wrong (or right).</p></li></ul><ul data-rte-list="default"><li><p>It can explain the same 7-month doubling time for the 50% time-horizon and 80% time-horizon: a 7-month halving-time for the underlying hazard rate would produce the 7-month doubling-time of all such time horizons.</p></li></ul><ul data-rte-list="default"><li><p>It also helps conceptually explain what the time horizons could even mean. For example, if it can complete a days work, why cant it just do that twice to produce two days of work? On the constant hazard rate model, the issue is that if has a 50% chance of succeeding on Mondays work, then it only has a 25% chance of succeeding in both Mondays and Tuesdays work, which is too low to count as reliably achieving the 2-day task (and similarly for any higher reliability threshold).</p></li></ul><p>Note that I am not claiming AI agents have a precisely constant rate of failure per minute of time it would take a human to complete the task. The claim is instead that something like this appears to be roughly true or stochastically true. All other models imply that there is some systematic change in the hazard rate over time, and my suggestion (pending more information about the precise fits of different models) is that the data doesnt warrant such assumptions.</p><p>If systematic deviations from exponential decay are found, such as the hazard rate increasing (or decreasing) with time, this might provide useful hints as to what the agents are doing wrong (or right). i.e. the constant hazard rate model can also work as a theoretical baseline from which to measure empirical deviations.</p><p>Also, note that <em>whatever</em> the survival curve is, if you lower the hazard rate by a factor of <em>k</em> at all times, the time horizons for all success rate levels also increase by a factor of <em>k</em>  (because the entire survival curve is  being stretched horizontally by a factor of <em>k</em>). Moreover, this is bidirectional  the only way to increase the time horizons at all success-rate levels by the same factor is to lower hazard rate by that factor. So to the extent that the METR evidence suggests the time horizons at every success rate are doubling at the same rate, it is also suggesting that the agent improvements are taking the form of reducing the hazard rate across all times by the same factor (i.e.. halving it every 7 months). This is intriguing and would be true whether or not there is a constant hazard rate for each model..</p><h3><strong>Further work</strong></h3><p>So far these results and analysis are merely suggestive of a constant hazard rate. Ideally one would conduct a formal statistical analysis on how well an exponential decay curve fits METRs data compared to the log-logistic they use.&nbsp;It would also be good to run robust statistical comparisons of the human decay curves versus the AI agent decay curves to see if there are systematic differences (e.g. different half-lives or different shapes that arent just an artefact of the experimental setup). And of course it is also important to know how much any of this generalises to other suites of tasks.</p><h3><strong>References</strong></h3><p>Thomas Kwa et al., <a href="https://arxiv.org/abs/2503.14499"><span>Measuring AI Ability to Complete Long Tasks</span></a>, arXiv:2503.14499 [cs.AI], 2025.</p><p>Toby Ord, <a href="https://arxiv.org/abs/2308.09045"><span>The Lindy Effect</span></a>. arXiv:2308.09045 [physics.soc-ph], 2023.</p>
</div></div>

    <!--BLOG INJECTION-->

    <!-- MathJax: Reset equation counter after every blog entry -->
$\setCounter{0}$    

    <!--CATEGORIES-->

    

  </article>
</section>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Terpstra Keyboard (188 pts)]]></title>
            <link>http://terpstrakeyboard.com/web-app/keys.htm</link>
            <guid>44308558</guid>
            <pubDate>Wed, 18 Jun 2025 10:31:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://terpstrakeyboard.com/web-app/keys.htm">http://terpstrakeyboard.com/web-app/keys.htm</a>, See on <a href="https://news.ycombinator.com/item?id=44308558">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="landing-page">
		<div>
		  
		  <p><img alt="" src="http://terpstrakeyboard.com/web-app/1x1.png">
          </p>

			<form id="settingsForm">
				<div>
					<div>
						<p><label>Tuning\Layout Quick Links</label>
                          

                          <label>Fundamental (Hz)</label>
                          
                       </p>

						<p><label>Right Facing Steps</label>
                          

                          <label>Up/Right Facing Steps</label>
                          
						</p>
                    </div>

					<div>
						<p><label>Hex Size (pixels)</label>
                          

                          <label>Rotation (degrees)</label>
                          
						</p>

						<p><label>Instrument</label>
                          

                          <label>
                              
                              <span>Enumerate Scale</span>
                          </label>

                          <label>
                              
                              <span>Use Spectrum Colors</span>
                          </label>

                          <label>
                              
                              <span>Blank Keys (No labels)</span>
                          </label>
						</p>
					</div>
				</div>

                <p><img alt="" src="http://terpstrakeyboard.com/web-app/1x1.png">
                </p>

				<div>
					<p><label>Scale (<a href="http://www.huygens-fokker.org/scala/scl_format.html" target="new">Scala format</a>)</label>
						
                  </p>

                  <div>
                    <p><label id="numberLabel">Steps To Equivalence Interval</label>
                      

                      
                      
					</p>

                    <p>
                        

                        <label id="note_colorsLabel">Color Layout</label>
                        
                    </p>
                  </div>
				</div>
				<br>
				
            </form>
        </div>

		<p>
          Designed by <a href="http://siementerpstra.com/" target="new">Siemen Terpstra</a> in the late 80s. WebApp developed by <a href="http://jamesfenn.com/" target="new">James Fenn</a> with additions and modifications by <a href="http://brandlew.com/" target="new">Brandon Lewis</a>, <a href="http://whatmusicreallyis.com/" title="What Music Really s" target="new">Bo Constantinsen</a> and <a href="https://sites.google.com/site/wangchengu/" target="new">Chengu Wang</a>. Credits to Scott Thompson and <a href="http://ozanyarman.com/" target="new">Dr Ozan Yarman</a> for contributing samples. Current version 1.5.2 (Jan. 2015  May 2019), released as Free/Libre and Open Source Software under <a href="https://www.gnu.org/licenses/gpl-3.0.en.html" target="new">GPL-3.0</a>. Download, fork, and get your name down here by fixing issues and implementing features via <a href="https://github.com/wcgbg/terpstrakeyboard/" target="new">GitHub</a>!
		</p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I counted all of the yurts in Mongolia using machine learning (197 pts)]]></title>
            <link>https://monroeclinton.com/counting-all-yurts-in-mongolia/</link>
            <guid>44307629</guid>
            <pubDate>Wed, 18 Jun 2025 07:58:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://monroeclinton.com/counting-all-yurts-in-mongolia/">https://monroeclinton.com/counting-all-yurts-in-mongolia/</a>, See on <a href="https://news.ycombinator.com/item?id=44307629">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>The <em>Fall of Civilizations</em> podcast put out a <a href="https://www.youtube.com/watch?v=YyqS9V7yHQA">6-hour episode</a>
on the history of the Mongol Empire, which I eagerly listened to. After finishing the episode I wondered
about contemporary Mongolian society, I wanted to learn what the lands that the Mongol Empire
exploded from are like in our current day. There are many ways to try to understand a society,
whether it be quantifying it or looking at the lived experiences within it. If you look at
data provided by the World Bank, youll see a country that has rapidly reduced poverty in the 21st
century, has a high economic growth rate, a healthy fertility rate, and is solidly an
upper-middle-income country. While Mongolia is a republic with a competitive party system,
<a href="https://www.worldbank.org/en/publication/worldwide-governance-indicators/interactive-data-access">Worldwide Governance Indicators</a>
from the World Bank show a government that has issues with corruption, regulatory quality, and effectiveness.</p>
<table>
<thead>
<tr>
<th>Indicator</th>
<th>Value</th>
<th>Years</th>
</tr>
</thead>
<tbody>
<tr>
<td>Population</td>
<td>3,481,145</td>
<td>2023</td>
</tr>
<tr>
<td>Fertility rate</td>
<td>2.7</td>
<td>2023</td>
</tr>
<tr>
<td>Intentional homicides (per 100,000 people)</td>
<td>6</td>
<td>2021</td>
</tr>
<tr>
<td>Individuals using the Internet (% of population)</td>
<td>1%  83%</td>
<td>2000  2023</td>
</tr>
<tr>
<td>Poverty headcount ratio at $2.15 a day (2017 PPP) (% of population)</td>
<td>11.6%  0.2%</td>
<td>2002  2022</td>
</tr>
<tr>
<td>Average GDP growth</td>
<td>6.62%</td>
<td>2003  2023</td>
</tr>
<tr>
<td>GDP per capita, PPP (current international $)</td>
<td>$4,399.4  $18,004.9</td>
<td>2003  2023</td>
</tr>
</tbody>
</table>
<blockquote>
<p>(Mongolia)</p>
</blockquote>
<p>All of these indicators are interesting to look at, but they dont really show what a society is
like. I feel you get much more understanding by going to a country, walking the streets, and
talking to people there. If youre unable to do this, the next best thing is spending hours
exploring Google Maps, which I did. I opened a satellite view of Ulaanbaatar, the capital of Mongolia.
I saw new glass buildings, Soviet-designed apartment blocks (called ugsarmal), impressive government
buildings, factories, and industrial areas. But something stood out to me. Yurts, extending for kilometers
in all directions.</p>
<p><img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/yurts-1.jpg" alt="Satellite view of Ulaanbaatar containing yurts"></p>
<blockquote>
<p>Maps Data: Google  2025 Airbus, CNES / Airbus, Maxar Technologies</p>
</blockquote>
<p>Naturally, I was impressed by the quantity of yurts I saw, and I was curious: just how many yurts (ger in Mongolian) are in
Mongolia and why? This set me on the path drawing bounding boxes on over 10,000 yurts to train a machine learning
model to count the rest of the yurts in the country. While I was training the model, I wondered what
the story behind these yurts are, I did a small investigation for later in this article. For now,
this is the story of counting them.</p>
<h2 id="counting-all-the-yurts-in-mongolia">Counting all the yurts in Mongolia</h2>
<p>I was unable to find a count of the yurts in Mongolia, this left me with
the task of doing it myself. Although I had never studied or worked with machine learning, I knew
through some osmosis that machine learning is well fit for this task. I created a simple plan in my
brain:</p>
<ol>
<li>Train a model to identify yurts</li>
<li>Reduce input space and parallelize searching of input space</li>
<li>Keep track of the yurts found</li>
</ol>
<h3 id="training-a-model-to-identify-yurts">Training a model to identify yurts</h3>
<p>The first thing I needed was training data, and lots of it. Theres many different options for satellite
imagery such as <a href="https://www.mapbox.com/imagery">Mapbox</a>, <a href="https://developers.google.com/maps/documentation/tile">Google Maps</a>,
and <a href="https://developers.arcgis.com/rest/basemap-styles/arcgis-imagery-webmap-get/">ArcGIS</a>. I
decided to use Google Maps since Im already familiar with it.</p>
<p>For digital maps, many systems break the world up into a series of 256 x 256 tiles identified by X, Y, Z values. This is
referred to as tiled web maps and allows for progressively loading maps at different zoom levels and
positions. The zoom level values tend to be 0 through 20, where 0 has the least tiles and 20 the
most. The formula for calculating the number of tiles at a given zoom (z) level is: <span> $2^z * 2^z$ </span>
.
This means increasing <code>z</code> by one will increase the tile count by four times.</p>
<p>I wrote a Python script that generated tiles from a box around Ulaanbaatar and downloaded
them to a folder to use as training data. To list the tiles inside a bounding box made up of a
southwest and northeast coordinates, I used the <a href="https://mercantile.readthedocs.io/en/latest/">mercantile package</a>.</p>
<div><pre tabindex="0"><code data-lang="python"><span>for</span> <span>tile</span> <span>in</span> <span>mercantile</span><span>.</span><span>tiles</span><span>(</span><span>sw_lng</span><span>,</span> <span>sw_lat</span><span>,</span> <span>ne_lng</span><span>,</span> <span>ne_lat</span><span>,</span> <span>zooms</span><span>=</span><span>z</span><span>):</span>
    <span>download_tile</span><span>(</span><span>*</span><span>tile</span><span>)</span>
</code></pre></div><p><img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/tile-1.jpeg" alt="Sample tile from Google Maps">
<img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/tile-2.jpeg" alt="Sample tile from Google Maps">
<img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/tile-3.jpeg" alt="Sample tile from Google Maps"></p>
<blockquote>
<p>Tiles from Google Maps, you can see yurts on the right tile. Maps Data: Google  2025 Airbus, CNES / Airbus, Maxar Technologies</p>
</blockquote>
<p>I decided to start at zoom level <code>17</code> as it is the lowest zoom level that I can still identify yurts
at. Once I downloaded several hundred tiles at this zoom level, I needed a way to label the yurts on
these tiles. Labeling is the process of drawing boxes around objects in an image. The idea is to
draw these boxes manually, creating what is called annotated data, and then training a model to do
the labeling using the annotated data.
Theres an open source tool called <a href="https://labelstud.io/">Label Studio</a> that does just
this.</p>
<p><img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/label-studio.jpg" alt="Label Studio showing yurts labeled"></p>
<blockquote>
<p>Here I drew bounding boxes on the tile around the yurts.</p>
</blockquote>
<p>A couple dozen yurts later and I wanted to try and train a model based on my tiny amount of
annotated data. I had the choice between object detection (bounding boxes) and segmentation (outline
objects). Segmentation probably would be more accurate because yurts are not rectangular,
but it seemed like it would take longer to setup. I decided to go with object detection.</p>
<p>I looked at various ways to train an object detection model, my requirements were:</p>
<ul>
<li>Open source</li>
<li>As simple as possible to setup</li>
<li>Able to quickly iterate</li>
<li>Detection speed of the model is a priority due to the potentially large amount of data</li>
<li>Has good default settings around data augmentation, warmups, loss functions, etc</li>
<li>Monitor current and previous training runs to compare accuracy</li>
</ul>
<p>After doing a brief survey of the machine learning landscape, I landed on using <a href="https://docs.ultralytics.com/">YOLO11</a> by Ultralytics.
The YOLO series is a set of models that can complete computer vision tasks, and can be trained with
custom data.
In Label Studio youre able to export to many different dataset types, YOLO being one of them. After
exporting my annotated data as a YOLO dataset, I split the dataset into training and validation data
and configured the dataset in <code>dataset.yaml</code> for YOLO to use.</p>





<div><pre tabindex="0"><code data-lang="yaml"><span>train</span><span>:</span><span> </span><span>images/train</span><span>
</span><span></span><span>val</span><span>:</span><span> </span><span>images/val</span><span>
</span><span>
</span><span></span><span>nc</span><span>:</span><span> </span><span>1</span><span>
</span><span></span><span>names</span><span>:</span><span>
</span><span>  </span>- <span>yurt</span></code></pre></div>

<p>From the ultralytics package, I used the YOLO class to use their pre-trained <code>yolo11n</code> object
detection model. Ultralytics allows easy tuning of the model with annotated data through the <code>train</code>
method of the <code>YOLO</code> class. The tuned model can be exported through <code>export</code> in various formats.</p>
<div><pre tabindex="0"><code data-lang="python"><span>from</span> <span>ultralytics</span> <span>import</span> <span>YOLO</span>

<span>model</span> <span>=</span> <span>YOLO</span><span>(</span><span>"yolo11n.pt"</span><span>)</span>
<span>model</span><span>.</span><span>train</span><span>(</span>
    <span>data</span><span>=</span><span>"dataset.yaml"</span><span>,</span>
    <span>device</span><span>=</span><span>"cpu"</span><span>,</span>
<span>)</span>

<span>path</span> <span>=</span> <span>model</span><span>.</span><span>export</span><span>(</span><span>name</span><span>=</span><span>"yurt"</span><span>)</span>
</code></pre></div><p>With some testing I found my Yurt model was less than adequate, which I expected due to the tiny
amount of annotated data. I then did a couple hours of labeling, but the model would always miss
around 10-15% of the yurts in a given tile. At this point I had two options, either increase the
zoom level or gather more training data. To base my decision I decided to calculate how many tiles I
would need to search at each zoom level.</p>
<h3 id="refining-the-search-area">Refining the search area</h3>
<p>Mongolia is 1,564,116 square kilometers, using this we can calculate how many tiles at each zoom
level there are in Mongolia. The world has <span> $2^z * 2^z$ </span>
 tiles, so
on a single axis there are <span> $2^z$ </span>
 tiles. The map projection is from a sphere
a tile will represent more or less area depending on the latitude.
To find the width of the projection at a latitude for Web Mercator, we can
use this formula where <span>$R = 6,378.137$</span>
 is the radius of the equator in kilometers and <span>$\phi = 47.923107575288114$</span>
 is the
latitude of Mongolia in degrees which is converted to radians:</p>
<p><span> $$2\pi * R * \cos(\phi * \dfrac{\pi}{180}) = 26,855.3636571$$</span></p><p>We then need to divide the number of tiles on the x-axis at this location to get the width of a
tile. For the area of a tile, just square the width and divide the area of Mongolia by the area of a
single tile to get the tile count.</p>
<table>
<thead>
<tr>
<th>Zoom Level</th>
<th>Tile Count</th>
</tr>
</thead>
<tbody>
<tr>
<td>17</td>
<td>37,258,617</td>
</tr>
<tr>
<td>18</td>
<td>149,034,469</td>
</tr>
<tr>
<td>19</td>
<td>596,137,879</td>
</tr>
<tr>
<td>20</td>
<td>2,384,551,518</td>
</tr>
</tbody>
</table>
<p>Since Mongolia is such a large country, I began to wonder if there are more ways to reduce the
amount of tiles other than just zoom level. Its a sparsely populated country, with much of the
country being uninhabited. Also, nearly all yurts are located in urban areas, with the City of
Ulaanbaatar estimating 60% of the population lives in ger (yurt) districts (City of Ulaanbaatar 17).</p>
<p>I used <a href="https://overpass-turbo.eu/">overpass turbo</a> to do a query for all the places human settlements might be in the
country and exported this data as GeoJSON. The query returned several thousand points of interest.</p>
<pre tabindex="0"><code>[out:json][timeout:25];
{{geocodeArea:Mongolia}}-&gt;.searchArea;
(
  node[place](area.searchArea);
  node[man_made](area.searchArea);
  node[historic](area.searchArea);
);
out body;
&gt;;
out skel qt;
</code></pre><p>I wanted to know how many unique tiles for searching a 2,000 meter area around each point there are,
so I wrote a script to do this using geopandas.</p>
<div><pre tabindex="0"><code data-lang="python"><span>gdf</span> <span>=</span> <span>gpd</span><span>.</span><span>read_file</span><span>(</span><span>"./mongolia.geojson"</span><span>)</span>
<span>gdf_merc</span> <span>=</span> <span>gdf</span><span>.</span><span>to_crs</span><span>(</span><span>"EPSG:3857"</span><span>)</span>
<span>gdf_merc</span><span>[</span><span>"buffer"</span><span>]</span> <span>=</span> <span>gdf_merc</span><span>.</span><span>geometry</span><span>.</span><span>buffer</span><span>(</span><span>2000</span><span>)</span>

<span>gdf_buffer</span> <span>=</span> <span>gdf_merc</span><span>.</span><span>set_geometry</span><span>(</span><span>"buffer"</span><span>)</span><span>.</span><span>to_crs</span><span>(</span><span>"EPSG:4326"</span><span>)</span>

<span>tiles</span> <span>=</span> <span>{}</span>
<span>for</span> <span>polygon</span> <span>in</span> <span>gdf_buffer</span><span>.</span><span>geometry</span><span>:</span>
    <span>minx</span><span>,</span> <span>miny</span><span>,</span> <span>maxx</span><span>,</span> <span>maxy</span> <span>=</span> <span>polygon</span><span>.</span><span>bounds</span>

    <span>for</span> <span>tile</span> <span>in</span> <span>mercantile</span><span>.</span><span>tiles</span><span>(</span><span>minx</span><span>,</span> <span>miny</span><span>,</span> <span>maxx</span><span>,</span> <span>maxy</span><span>,</span> <span>zooms</span><span>=</span><span>Z</span><span>):</span>
        <span>tiles</span><span>[</span><span>"</span><span>{}</span><span>-</span><span>{}</span><span>-</span><span>{}</span><span>"</span><span>.</span><span>format</span><span>(</span><span>str</span><span>(</span><span>tile</span><span>.</span><span>x</span><span>),</span> <span>str</span><span>(</span><span>tile</span><span>.</span><span>y</span><span>),</span> <span>str</span><span>(</span><span>tile</span><span>.</span><span>z</span><span>))]</span> <span>=</span> <span>True</span>
</code></pre></div><table>
<thead>
<tr>
<th>Zoom Level</th>
<th>Tile Count</th>
</tr>
</thead>
<tbody>
<tr>
<td>17</td>
<td>270,559</td>
</tr>
<tr>
<td>18</td>
<td>1,016,617</td>
</tr>
<tr>
<td>19</td>
<td>3,938,174</td>
</tr>
<tr>
<td>20</td>
<td>15,506,872</td>
</tr>
</tbody>
</table>
<h3 id="building-a-model-backend-for-labeling">Building a model backend for labeling</h3>
<p>To speed up the labeling of yurts I wanted Label Studio to use my model to label yurts.
Label Studio has the ability to integrate with a model backend,
essentially an API wrapper around a model, to request predictions. When labeling a tile, Label
Studio makes a request to this API for predictions. The API returns the bounding boxes for the tile.
I fix any mistakes the model made, and submit the tile. Every so often I retrain the model, creating
a feedback loop that improves the model with more and more annotated data.</p>





<div><pre tabindex="0"><code data-lang="python"><span>class</span> <span>YurtModel</span><span>:</span>
    <span># Initialize trained model to reuse across requests</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>model</span> <span>=</span> <span>YOLO</span><span>(</span><span>"best.pt"</span><span>,</span> <span>task</span><span>=</span><span>"detect"</span><span>)</span>

    <span># Task a task sent by Label Studio, and return bounding boxes of yurts</span>
    <span>def</span> <span>predict</span><span>(</span><span>self</span><span>,</span> <span>tasks</span><span>):</span>
        <span>predictions</span> <span>=</span> <span>[]</span>
        <span>for</span> <span>task</span> <span>in</span> <span>tasks</span><span>:</span>
            <span># Get the path to the file from label studio</span>
            <span>path</span> <span>=</span> <span>get_local_path</span><span>(</span>
                <span>task</span><span>[</span><span>"data"</span><span>][</span><span>"image"</span><span>],</span>
                <span>task_id</span><span>=</span><span>task</span><span>[</span><span>"id"</span><span>],</span>
            <span>)</span>

            <span>results</span> <span>=</span> <span>self</span><span>.</span><span>model</span><span>(</span><span>path</span><span>)</span>

            <span>for</span> <span>result</span> <span>in</span> <span>results</span><span>:</span>
                <span>regions</span> <span>=</span> <span>[]</span>
                <span>for</span> <span>prediction</span> <span>in</span> <span>result</span><span>.</span><span>boxes</span><span>:</span>
                    <span>xyxy</span> <span>=</span> <span>prediction</span><span>.</span><span>xyxy</span><span>[</span><span>0</span><span>]</span><span>.</span><span>tolist</span><span>()</span>
                    <span>regions</span><span>.</span><span>append</span><span>({</span>
                        <span>"model_version"</span><span>:</span> <span>"1.0"</span><span>,</span>
                        <span>"from_name"</span><span>:</span> <span>"label"</span><span>,</span>
                        <span>"to_name"</span><span>:</span> <span>"image"</span><span>,</span>
                        <span>"type"</span><span>:</span> <span>"rectanglelabels"</span><span>,</span>
                        <span>"score"</span><span>:</span> <span>prediction</span><span>.</span><span>conf</span><span>.</span><span>item</span><span>(),</span>
                        <span>"value"</span><span>:</span> <span>{</span>
                            <span>"x"</span><span>:</span> <span>xyxy</span><span>[</span><span>0</span><span>]</span> <span>/</span> <span>256</span> <span>*</span> <span>100</span><span>,</span>
                            <span>"y"</span><span>:</span> <span>xyxy</span><span>[</span><span>1</span><span>]</span> <span>/</span> <span>256</span> <span>*</span> <span>100</span><span>,</span>
                            <span>"width"</span><span>:</span> <span>(</span><span>xyxy</span><span>[</span><span>2</span><span>]</span> <span>-</span> <span>xyxy</span><span>[</span><span>0</span><span>])</span> <span>/</span> <span>256</span> <span>*</span> <span>100</span><span>,</span>
                            <span>"height"</span><span>:</span> <span>(</span><span>xyxy</span><span>[</span><span>3</span><span>]</span> <span>-</span> <span>xyxy</span><span>[</span><span>1</span><span>])</span> <span>/</span> <span>256</span> <span>*</span> <span>100</span><span>,</span>
                            <span>"rectanglelabels"</span><span>:</span> <span>[</span>
                                <span>"yurt"</span><span>,</span>
                            <span>],</span>
                        <span>},</span>
                    <span>})</span>

                <span>all_scores</span> <span>=</span> <span>[</span><span>region</span><span>[</span><span>"score"</span><span>]</span> <span>for</span> <span>region</span> <span>in</span> <span>regions</span> <span>if</span> <span>"score"</span> <span>in</span> <span>region</span><span>]</span>
                <span>avg_score</span> <span>=</span> <span>sum</span><span>(</span><span>all_scores</span><span>)</span> <span>/</span> <span>max</span><span>(</span><span>len</span><span>(</span><span>all_scores</span><span>),</span> <span>1</span><span>)</span>

                <span>predictions</span><span>.</span><span>append</span><span>({</span>
                    <span>"result"</span><span>:</span> <span>regions</span><span>,</span>
                    <span>"score"</span><span>:</span> <span>avg_score</span><span>,</span>
                    <span>"model_version"</span><span>:</span> <span>"1.0"</span><span>,</span>
                <span>})</span>

        <span>return</span> <span>{</span>
            <span>"results"</span><span>:</span> <span>predictions</span><span>,</span>
        <span>}</span>

<span>model</span> <span>=</span> <span>YurtModel</span><span>()</span></code></pre></div>

<p>We then need to fill out the API routes that Label Studio expects, which is a <code>/predict</code> route for
label studio to send tiles and receive predictions, a <code>/setup</code> route to do any initialization
required, and a <code>/health</code> route to do health checks on. I used <a href="https://fastapi.tiangolo.com/">FastAPI</a> to build the API and use
the <code>YurtModel</code> from above.</p>





<div><pre tabindex="0"><code data-lang="python"><span>@app</span><span>.</span><span>post</span><span>(</span><span>"/predict"</span><span>)</span>
<span>async</span> <span>def</span> <span>predict</span><span>(</span><span>request</span><span>:</span> <span>Request</span><span>):</span>
    <span>res</span> <span>=</span> <span>await</span> <span>request</span><span>.</span><span>json</span><span>()</span>
    <span>return</span> <span>model</span><span>.</span><span>predict</span><span>(</span><span>res</span><span>[</span><span>"tasks"</span><span>])</span>

<span>@app</span><span>.</span><span>post</span><span>(</span><span>"/setup"</span><span>)</span>
<span>async</span> <span>def</span> <span>setup</span><span>():</span>
    <span>return</span> <span>{</span>
        <span>"model_version"</span><span>:</span> <span>"1.0"</span><span>,</span>
    <span>}</span>

<span>@app</span><span>.</span><span>get</span><span>(</span><span>"/health"</span><span>)</span>
<span>async</span> <span>def</span> <span>health</span><span>():</span>
    <span>return</span> <span>{</span>
        <span>"status"</span><span>:</span> <span>"UP"</span><span>,</span>
        <span>"model_class"</span><span>:</span> <span>str</span><span>(</span><span>YurtModel</span><span>.</span><span>__class__</span><span>),</span>
    <span>}</span></code></pre></div>

<p>By relying on the model to find most of the yurts when labeling, I was able to rapidly create more
annotated data. I quickly built a dataset of over 10,000 yurts.</p>
<h3 id="monitoring-accuracy-of-each-model">Monitoring accuracy of each model</h3>
<h3 id="scaling-training-of-models">Scaling training of models</h3>
<p>As the size of the annotated data grew, training the models on my laptop became too slow.
I decided to use <a href="https://vast.ai/">vast.ai</a> to rent GPUs to do my training runs. To train
the models on vast.ai, I needed everything to run in Docker. I wrote a Dockerfile for the training
script, and I pushed it to <a href="https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry">GitHub Container Registry</a>.
In vast.ai I set up authentication with the private image registry so it could pull the image I pushed up.</p>
<p><img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/docker-auth.png" alt="vast.ai Docker authentication"></p>
<blockquote>
<p>Docker authentication in vast.ai</p>
</blockquote>
<p>Here is the Dockerfile that I used to run the training script on the dataset I created.</p>
<div><pre tabindex="0"><code data-lang="shell">FROM ghcr.io/astral-sh/uv:python3.10-bookworm-slim

WORKDIR /app

<span># Copy training script, annotated data, and requirements to image</span>
COPY scripts/train_model.py .
COPY datasets ./datasets
COPY dataset.yaml .
COPY pyproject.toml .
COPY uv.lock .

<span># Needed for ...</span>
RUN apt-get update -y <span>&amp;&amp;</span> apt-get install -y libgl1-mesa-dev libglib2.0-0

<span># Install package requirements</span>
RUN uv sync --no-dev

<span># Run the training script</span>
CMD <span>[</span><span>"uv"</span>, <span>"run"</span>, <span>"python"</span>, <span>"train_model.py"</span><span>]</span>
</code></pre></div><p>In order to build and push this image to GitHub I ran:</p>
<div><pre tabindex="0"><code data-lang="shell">docker build -t ghcr.io/monroeclinton/yurt -f Dockerfile .
docker push ghcr.io/monroeclinton/yurt:latest
</code></pre></div><p>Since the training happened in ephemeral containers, I needed a way to retrieve the finished model. I
decided to upload the model to S3 after it finished training. To monitor the accuracy of the
models, I also needed the metadata associated with the runs, so I uploaded everything in the
run folder to S3.</p>





<div><pre tabindex="0"><code data-lang="python"><span>model</span> <span>=</span> <span>YOLO</span><span>(</span><span>"yolo11n.pt"</span><span>)</span>

<span>model</span><span>.</span><span>train</span><span>(</span>
    <span>data</span><span>=</span><span>"dataset.yaml"</span><span>,</span>
    <span>epochs</span><span>=</span><span>1000</span><span>,</span>
    <span>patience</span><span>=</span><span>150</span><span>,</span>
    <span>imgsz</span><span>=</span><span>256</span><span>,</span>
    <span>device</span><span>=</span><span>"cuda"</span><span>,</span>
<span>)</span>

<span>path</span> <span>=</span> <span>model</span><span>.</span><span>export</span><span>(</span><span>name</span><span>=</span><span>"yurt"</span><span>)</span>
<span>train_dir</span> <span>=</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>dirname</span><span>(</span><span>os</span><span>.</span><span>path</span><span>.</span><span>dirname</span><span>(</span><span>path</span><span>))</span>

<span>s3</span> <span>=</span> <span>boto3</span><span>.</span><span>client</span><span>(</span>
    <span>service_name</span> <span>=</span><span>"s3"</span><span>,</span>
    <span>endpoint_url</span><span>=</span><span>os</span><span>.</span><span>environ</span><span>[</span><span>"S3_ENDPOINT"</span><span>],</span>
    <span>aws_access_key_id</span><span>=</span><span>os</span><span>.</span><span>environ</span><span>[</span><span>"S3_ACCESS_KEY_ID"</span><span>],</span>
    <span>aws_secret_access_key</span><span>=</span><span>os</span><span>.</span><span>environ</span><span>[</span><span>"S3_SECRET_ACCESS_KEY"</span><span>],</span>
    <span>region_name</span><span>=</span><span>os</span><span>.</span><span>environ</span><span>[</span><span>"S3_REGION"</span><span>],</span>
<span>)</span>

<span>timestamp</span> <span>=</span> <span>time</span><span>.</span><span>time</span><span>()</span>

<span>for</span> <span>root</span><span>,</span> <span>dirs</span><span>,</span> <span>files</span> <span>in</span> <span>os</span><span>.</span><span>walk</span><span>(</span><span>train_dir</span><span>):</span>
    <span>for</span> <span>file</span> <span>in</span> <span>files</span><span>:</span>
        <span>local_path</span> <span>=</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>join</span><span>(</span><span>root</span><span>,</span> <span>file</span><span>)</span>
        <span>s3_key</span> <span>=</span> <span>f</span><span>"models/</span><span>{</span><span>int</span><span>(</span><span>timestamp</span><span>)</span><span>}</span><span>/</span><span>{</span><span>os</span><span>.</span><span>path</span><span>.</span><span>relpath</span><span>(</span><span>local_path</span><span>,</span> <span>train_dir</span><span>)</span><span>}</span><span>"</span>
        <span>s3</span><span>.</span><span>upload_file</span><span>(</span><span>local_path</span><span>,</span> <span>os</span><span>.</span><span>environ</span><span>[</span><span>"S3_BUCKET"</span><span>],</span> <span>s3_key</span><span>)</span></code></pre></div>

<h3 id="deploying-models-and-searching-mongolia">Deploying models and searching Mongolia</h3>
<p>After dozens of training runs and greatly improving the accuracy of the model, I decided to finally
do my count of Mongolia. There were many options to run my deployment, however I made my choice based on three
criteria:</p>
<ul>
<li>Simplicity in setup and deployment</li>
<li>At least 100 instances of my model should be run</li>
<li>The bottleneck is I/O (downloading tiles), so should be deployed on many CPUs</li>
</ul>
<p>Based on these criteria, I used <a href="https://docs.docker.com/engine/swarm/">Docker Swarm</a> to orchestrate the workload.
Its already packaged in Docker, so theres no need to install anything else. Docker Swarm also is
fairly simple to set up, scale, and deploy services with. I rented eight servers, each with 16 vCPUs
(128 vCPUs total), and connected them over a private network.</p>
<p>I picked one server to be the <a href="https://docs.docker.com/engine/swarm/how-swarm-mode-works/nodes/#manager-nodes">manager node</a>.
On this server, I ran this to initialize the swarm:</p>
<div><pre tabindex="0"><code data-lang="bash">docker swarm init --advertise-addr 10.0.0.2
</code></pre></div><p>This command sets up the swarm and prints a command to run on the worker nodes to connect them to
the manager. Each worker node joined using the token and the managers address:</p>
<div><pre tabindex="0"><code data-lang="bash">docker swarm join --token SWARM_TOKEN 10.0.0.2:2377
</code></pre></div><p>I deployed the container images, which I had pushed to GHCR, and pulled with
<code>--with-registry-auth</code> to allow access from the server to GHCR.
There were two images, the <code>api</code> image and the <code>worker</code> image. The API managed a list of
search areas (the areas around the points found from overpass turbo), giving
search areas to workers, and expanding the search radius by 500 meters when yurts were found.
The workers requested search areas from the API and sent back a list of yurts found within the
search areas.</p>
<h4 id="api">API</h4>
<p>I used FastAPI to build the API, in which there were two routes.</p>
<ul>
<li>GET /search-area - Workers sent a request to this route to get a search area to search.</li>
</ul>
<p>This route first checks if there are any stale areas, where a worker had requested a search area
but never finished it. The workers should send periodic health checks to the API, if this fails then
it will return the search area to a different worker after one minute.</p>
<div><pre tabindex="0"><code data-lang="python"><span>stale_area</span> <span>=</span> <span>(</span>
    <span>db</span><span>.</span><span>query</span><span>(</span><span>SearchArea</span><span>)</span>
    <span>.</span><span>filter</span><span>(</span><span>SearchArea</span><span>.</span><span>searching</span> <span>==</span> <span>True</span><span>)</span>
    <span>.</span><span>filter</span><span>(</span><span>SearchArea</span><span>.</span><span>health_check</span> <span>&lt;</span> <span>one_minute_ago</span><span>)</span>
    <span>.</span><span>with_for_update</span><span>()</span>
    <span>.</span><span>first</span><span>()</span>
<span>)</span>
</code></pre></div><p>If there are no stale search areas, then a new point will be selected at random, and the search area
will be increased if there have been previous searches.</p>
<div><pre tabindex="0"><code data-lang="python"><span>point</span> <span>=</span> <span>(</span>
    <span>db</span><span>.</span><span>query</span><span>(</span><span>Point</span><span>)</span>
    <span>.</span><span>options</span><span>(</span><span>joinedload</span><span>(</span><span>Point</span><span>.</span><span>search_areas</span><span>))</span>
    <span>.</span><span>filter</span><span>(</span><span>Point</span><span>.</span><span>searched</span> <span>==</span> <span>False</span><span>)</span>
    <span>.</span><span>filter</span><span>(</span><span>~</span><span>Point</span><span>.</span><span>search_areas</span><span>.</span><span>any</span><span>(</span><span>SearchArea</span><span>.</span><span>searched</span> <span>==</span> <span>False</span><span>))</span>
    <span>.</span><span>order_by</span><span>(</span><span>func</span><span>.</span><span>random</span><span>())</span>
    <span>.</span><span>first</span><span>()</span>
<span>)</span>

<span>if</span> <span>not</span> <span>point</span><span>:</span>
    <span>raise</span> <span>HTTPException</span><span>(</span>
        <span>status_code</span><span>=</span><span>404</span><span>,</span> <span>detail</span><span>=</span><span>"No available point to search"</span><span>)</span>

<span>previous_areas</span> <span>=</span> <span>[</span><span>sa</span> <span>for</span> <span>sa</span> <span>in</span> <span>point</span><span>.</span><span>search_areas</span><span>]</span>
<span>if</span> <span>previous_areas</span><span>:</span>
    <span>max_meters</span> <span>=</span> <span>max</span><span>(</span><span>area</span><span>.</span><span>meters</span> <span>for</span> <span>area</span> <span>in</span> <span>previous_areas</span><span>)</span>
    <span>new_meters</span> <span>=</span> <span>max_meters</span> <span>+</span> <span>500</span>
<span>else</span><span>:</span>
    <span>new_meters</span> <span>=</span> <span>500</span>
</code></pre></div><p>A <code>SearchArea</code> has a list of tiles that are inside it. Each <code>Tile</code> has as status of <code>searched</code>.
I used geopandas, as shown earlier, to generate a bounding box over the search area and create a list
of tiles. For each of these tiles, I check the database to see if they have already been created + searched.
If they havent then they are upserted and assigned to the search area.</p>
<div><pre tabindex="0"><code data-lang="python"><span>created_tiles</span> <span>=</span> <span>(</span>
    <span>db</span><span>.</span><span>query</span><span>(</span><span>Tile</span><span>)</span>
    <span>.</span><span>filter</span><span>(</span>
        <span>tuple_</span><span>(</span><span>Tile</span><span>.</span><span>x</span><span>,</span> <span>Tile</span><span>.</span><span>y</span><span>,</span> <span>Tile</span><span>.</span><span>z</span><span>)</span><span>.</span><span>in_</span><span>(</span>
            <span>[(</span><span>tile</span><span>[</span><span>"x"</span><span>],</span> <span>tile</span><span>[</span><span>"y"</span><span>],</span> <span>tile</span><span>[</span><span>"z"</span><span>])</span>
             <span>for</span> <span>tile</span> <span>in</span> <span>tiles_to_create</span><span>]</span>
        <span>)</span>
    <span>)</span>
    <span>.</span><span>all</span><span>()</span>
<span>)</span>

<span>new_area</span><span>.</span><span>tiles</span><span>.</span><span>extend</span><span>(</span><span>created_tiles</span><span>)</span>
</code></pre></div><p>The route returns the search area, containing a list of tiles to search.</p>
<ul>
<li>POST /search-area/:id - Workers sent a request containing the yurts to this route.</li>
</ul>
<p>This route inserts the yurts into the database, and marks the <code>Point</code>, <code>SearchArea</code>, and <code>Tile</code> as
searched as needed. The <code>Point</code> gets marked as searched if no yurts are found, and the <code>SearchArea</code>
and <code>Tile</code> are marked as searched.</p>
<div><pre tabindex="0"><code data-lang="python"><span>if</span> <span>len</span><span>(</span><span>yurts_to_create</span><span>)</span> <span>&gt;</span> <span>0</span><span>:</span>
    <span>stmt</span> <span>=</span> <span>insert</span><span>(</span><span>Yurt</span><span>)</span><span>.</span><span>values</span><span>(</span><span>yurts_to_create</span><span>)</span>
    <span>stmt</span> <span>=</span> <span>stmt</span><span>.</span><span>on_conflict_do_nothing</span><span>(</span>
        <span>index_elements</span><span>=</span><span>[</span><span>"longitude"</span><span>,</span> <span>"latitude"</span><span>])</span>
    <span>db</span><span>.</span><span>execute</span><span>(</span><span>stmt</span><span>)</span>
<span>else</span><span>:</span>
    <span>db</span><span>.</span><span>execute</span><span>(</span>
        <span>update</span><span>(</span><span>Point</span><span>)</span><span>.</span><span>where</span><span>(</span>
            <span>exists</span><span>()</span><span>.</span><span>where</span><span>(</span>
                <span>(</span><span>SearchArea</span><span>.</span><span>point_id</span> <span>==</span> <span>Point</span><span>.</span><span>id</span><span>)</span> <span>&amp;</span> <span>(</span><span>Point</span><span>.</span><span>id</span> <span>==</span> <span>id</span><span>)</span>
            <span>)</span>
        <span>)</span><span>.</span><span>values</span><span>(</span>
            <span>searched</span><span>=</span><span>True</span><span>,</span>
        <span>)</span>
    <span>)</span>
</code></pre></div><h4 id="worker">Worker</h4>
<p>The worker script ran in a loop until it encountered the <code>No available point to search</code> error.
This loop consisted of requesting the <code>/search-area</code> to get a list of tiles to search, downloading
each tile, then passing the tile image to the model to detect yurts. Finally, the worker sends a
list of yurts to the API.</p>
<div><pre tabindex="0"><code data-lang="python"><span>def</span> <span>find_yurts</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>z</span><span>):</span>
    <span>filepath</span> <span>=</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>join</span><span>(</span><span>"tiles"</span><span>,</span> <span>"</span><span>{}</span><span>_</span><span>{}</span><span>_</span><span>{}</span><span>.jpeg"</span><span>.</span><span>format</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>z</span><span>))</span>

    <span>results</span> <span>=</span> <span>model</span><span>(</span><span>filepath</span><span>,</span> <span>imgsz</span><span>=</span><span>256</span><span>)</span>

    <span>yurts</span> <span>=</span> <span>[]</span>
    <span>for</span> <span>result</span> <span>in</span> <span>results</span><span>:</span>
        <span>for</span> <span>prediction</span> <span>in</span> <span>result</span><span>.</span><span>boxes</span><span>:</span>
            <span>xyxy</span> <span>=</span> <span>prediction</span><span>.</span><span>xyxy</span><span>[</span><span>0</span><span>]</span><span>.</span><span>tolist</span><span>()</span>

            <span># Find center of the bounding box</span>
            <span>pixel_x</span> <span>=</span> <span>xyxy</span><span>[</span><span>0</span><span>]</span> <span>+</span> <span>(</span><span>xyxy</span><span>[</span><span>2</span><span>]</span> <span>-</span> <span>xyxy</span><span>[</span><span>0</span><span>])</span> <span>/</span> <span>2</span>
            <span>pixel_y</span> <span>=</span> <span>xyxy</span><span>[</span><span>1</span><span>]</span> <span>+</span> <span>(</span><span>xyxy</span><span>[</span><span>3</span><span>]</span> <span>-</span> <span>xyxy</span><span>[</span><span>1</span><span>])</span> <span>/</span> <span>2</span>

            <span>lat</span><span>,</span> <span>lon</span> <span>=</span> <span>tile_xyz_to_lonlat</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>z</span><span>,</span> <span>pixel_x</span><span>,</span> <span>pixel_y</span><span>)</span>

            <span>yurts</span><span>.</span><span>append</span><span>({</span>
                <span>"latitude"</span><span>:</span> <span>lat</span><span>,</span>
                <span>"longitude"</span><span>:</span> <span>lon</span><span>,</span>
                <span>"score"</span><span>:</span> <span>prediction</span><span>.</span><span>conf</span><span>.</span><span>item</span><span>(),</span>
            <span>})</span>

    <span>return</span> <span>yurts</span>
</code></pre></div><p>I began scaling this service slowly and eventually ramped up to 120 workers running in parallel
using <code>docker service scale worker=120</code>. Each container processed its assigned tile, and if yurts
were found, posted their coordinates to the API.</p>
<h3 id="the-resulting-count">The resulting count</h3>
<p>After searching a couple million tiles I downloaded the yurt dataset, which I uploaded <a href="https://cdn.monroeclinton.com/yurts.json">here (12mb file)</a>.
In total I found 172,689 yurts with a prediction score of greater than 40%.</p>
<p>Perhaps theres some lonesome yurts far in the Gobi Desert or the Altai Mountains I missed, so we
could add a hundred or so for those. I could have also done more like
providing image context and training on more data from smaller towns, but I only have so much time.</p>
<p>For fun I did some querying using <a href="https://postgis.net/">PostGIS</a> to find areas with high concentrations
of yurts. Generally I found places that are hotels or remote areas near mines.</p>
<p><img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/many-yurts.jpeg" alt="Many yurts"></p>
<blockquote>
<p>Maps Data: Google  2025 Airbus, CNES / Airbus, Maxar Technologies</p>
</blockquote>
<h2 id="the-people-of-the-yurts">The people of the yurts</h2>
<p>Historically, yurts have been a home for the nomadic peoples of the steppe to live. As
Mongolia developed into the modern world, the usage of yurts changed with the country. For example,
I found a reference to yurts being used as makeshift schools in the early 1900s. This period was the
start of the transformation from a nomadic herder society to an urban industrial society.</p>
<blockquote>
In the rural areas, in addition to the existing 60 scribe schools, at least 49 state primary schools were established by 1917. They were largely housed in yurts and financed with state, municipal, and private funds. (Steiner-Khamsi and Stolpe 36)
</blockquote>

<p>This reflects the developmental history of Mongolia, and how people are adjusting to the modern
world. Mongolia has transitioned from a mostly nomadic herder society, to a mostly urbanized
industrial society. As people transition from one system of life to another, remnants of their old
system persist. Housing and infrastructure are expensive, so as Mongolia transformed, once nomadic
herders took their yurts to urban areas and continued living in them.</p>
<blockquote>
The 51 percent urban population reported in the 1979 census
reflected rapid migration to the cities in the 1970s. The influx of
rural people created housing problems, among them long waits for
assignment to an apartment, expansion of ger districts on the edges
of built-up areas, and pressure to invest in more housing, roads,
and other urban infrastructure. (Worden et al. 86)
</blockquote>

<p>Due to the large number of people moving to urban locations, it has been difficult for the government to
build the infrastructure needed for them. The informal settlements that grew from this difficulty
are now known as ger districts. There have been many efforts to formalize and develop these areas.
The Law on Allocation of Land to Mongolian Citizens for Ownership, passed in 2002, allowed for
existing ger district residents to formalize the land they settled, and allowed for others to
receive land from the government into the future.</p>
<p>Along with the privatization of land, the Mongolian government has been pushing for the development
of ger districts into areas with housing blocks connected to utilities. The plan for this was
published in 2014 as Ulaanbaatar 2020 Master Plan and Development Approaches for 2030. Although
progress has been slow (Choi and Enkhbat 7), they have been making progress in building housing blocks in ger
distrcts. Residents of ger districts sell or exchange their plots to developers who then build housing
blocks on them. Often this is in exchange for an apartment in the building, and often the value of the
apartment is less than the land they originally had (Choi and Enkhbat 15).</p>
<p>Based on what Ive read about the ger districts, they have been around since at least the 1970s,
and progress on developing them has been slow. When ineffective policy results in a large chunk of
the populace generationally living in yurts on the outskirts of urban areas, its clear that there
is failure.
One of the most important functions of government is inspiring the citizenry to achieve greatness.
Most governments around the world fail in this, but we should all work towards it. I think a step
the Mongolian government could take for this is to analyze which policy failures have led to such
slow progress on the ger district issue.</p>
<p>The Mongolian governments long-term vision is to provide utilities and good housing
for these areas. Although I cant contribute anything to this vision, I wish for the best
success in this plan.
Im glad to have learned about a country and people I used to know nothing about. Hopefully in the
future Ill study more about Mongolia, but for now Im off to my next project.</p>
<h3 id="further-questions">Further questions</h3>
<ul>
<li>What causes Mongolia and other countries to urbanize and industrialize?</li>
<li>Why do some Mongolians head to the cities and others stay?</li>
<li>What challenges does the Mongolian government face in developing ger districts?</li>
<li>What causes the difference in speed of development between countries?</li>
</ul>
<h2 id="references">References</h2>
<ul>
<li><p>Choi, Mack Joong, and Urandulguun Enkhbat. Distributional Effects of Ger Area Redevelopment in Ulaanbaatar, Mongolia. International Journal of Urban Sciences, vol. 24, no. 1, Jan. 2020, pp. 5068. DOI.org (Crossref), <a href="https://doi.org/10.1080/12265934.2019.1571433">https://doi.org/10.1080/12265934.2019.1571433</a>.</p>

</li>
<li><p>City of Ulaanbaatar. <em>Ulaanbaatar 2020 Master Plan and Development Approach for 2030.</em> 2014.</p>

</li>
<li>

</li>
<li><p>Steiner-Khamsi, Gita, and Ines Stolpe. <em>Educational Import: Local Encounters with Global Forces in Mongolia.</em> 1st ed, Palgrave Macmillan, 2006.</p>

</li>
<li><p>Worden, Robert L, et al. <em>Mongolia: A Country Study.</em> Washington, D.C.: Federal Research Division, Library of Congress: For sale by the Supt. of Docs., U.S. G.P.O, 1991. Pdf. Retrieved from the Library of Congress, &lt;www.loc.gov/item/90006289/&gt;.</p>

</li>
<li><p>Yang, Jeasurk, et al. <em>Poverty Mapping in Mongolia with AI-Based Ger Detection Reveals Urban Slums Persist after the COVID-19 Pandemic.</em> arXiv:2410.09522, arXiv, 12 Oct. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2410.09522">https://doi.org/10.48550/arXiv.2410.09522</a>.</p>

</li>
</ul>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MiniMax-M1 open-weight, large-scale hybrid-attention reasoning model (303 pts)]]></title>
            <link>https://github.com/MiniMax-AI/MiniMax-M1</link>
            <guid>44307290</guid>
            <pubDate>Wed, 18 Jun 2025 06:53:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/MiniMax-AI/MiniMax-M1">https://github.com/MiniMax-AI/MiniMax-M1</a>, See on <a href="https://news.ycombinator.com/item?id=44307290">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
  <themed-picture data-catalyst-inline="true"><picture>
    <source srcset="https://github.com/MiniMax-AI/MiniMax-M1/raw/main/figures/MiniMaxLogo-Dark.png" media="(prefers-color-scheme: dark)">
      <img src="https://github.com/MiniMax-AI/MiniMax-M1/raw/main/figures/MiniMaxLogo-Light.png" width="60%" alt="MiniMax">
    
  </picture></themed-picture>
</div>
<hr>
<p><a href="https://www.minimax.io/" rel="nofollow">
    <img alt="Homepage" src="https://camo.githubusercontent.com/3faa1e14d767d4a75cf9ed5610309fbb6fe899cec1f03f659f57e5961de37e9b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5f486f6d65706167652d4d696e694d61782d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530266c6f676f3d646174613a696d6167652f7376672b786d6c3b6261736536342c50484e325a79423462577875637a30696148523063446f764c336433647935334d793576636d63764d6a41774d43397a646d6369494868746247357a4f6e68736157357250534a6f644852774f693876643364334c6e637a4c6d39795a7938784f546b354c336873615735724969423261575633516d393450534977494441674e446b774c6a4532494451784d533433496a34385a47566d637a3438633352356247552b4c6d4e736379307865325a706247773649325a6d5a6a74395043397a64486c735a5434384c32526c5a6e4d2b50484268644767675932786863334d39496d4e73637930784969426b50534a4e4d6a4d7a4c6a51314c4451774c6a6778595445334c6a55314c4445334c6a55314c4441734d5377774c544d314c6a45734d46597a4d7a45754e545a684e4441754f4449734e4441754f4449734d4377774c4445744f4445754e6a4d734d4659784e4456684d5463754e5455734d5463754e5455734d4377784c4441744d7a55754d446b734d4859334f5334774e6d45304d4334344d6977304d4334344d6977774c4441734d5330344d5334324d797777566a45354e5334304d6d45784d5334324d7977784d5334324d7977774c4441734d5377794d7934794e697777646a49344c6a5932595445334c6a55314c4445334c6a55314c4441734d4377774c444d314c6a45734d4659784e4456424e4441754f4449734e4441754f4449734d4377774c4445734d5451774c4445304e56597a4d7a45754e545a684d5463754e5455734d5463754e5455734d4377774c4441734d7a55754d537777566a49784e793431614442574e4441754f4446684e4441754f4445734e4441754f4445734d4377784c4445734f4445754e6a49734d4659794f4445754e545a684d5445754e6a4d734d5445754e6a4d734d4377784c4445744d6a4d754d6a59734d4670744d6a45314c6a6b734e6a4d754e4545304d4334344e6977304d4334344e6977774c4441734d4377304d4467754e544d734d545131566a4d774d4334344e5745784e7934314e5377784e7934314e5377774c4441734d53307a4e5334774f537777646930794e6a42684e4441754f4449734e4441754f4449734d4377774c4441744f4445754e6a4d734d46597a4e7a41754f446c684d5463754e5455734d5463754e5455734d4377774c4445744d7a55754d537777566a4d7a4d4745784d5334324d7977784d5334324d7977774c4445734d4330794d7934794e697777646a51774c6a6732595451774c6a67784c4451774c6a67784c4441734d4377774c4467784c6a59794c4442574e4441754f4446684d5463754e5455734d5463754e5455734d4377774c4445734d7a55754d537777646a49324d4745304d4334344d6977304d4334344d6977774c4441734d4377344d5334324d797777566a45304e5745784e7934314e5377784e7934314e5377774c4445734d53777a4e5334784c4442574d6a67784c6a5532595445784c6a597a4c4445784c6a597a4c4441734d4377774c44497a4c6a49324c4442574d545131515451774c6a67314c4451774c6a67314c4441734d4377774c4451304f53347a4e5377784d4451754d6a46614969382b5043397a646d632b266c6f676f57696474683d3230" data-canonical-src="https://img.shields.io/badge/_Homepage-MiniMax-FF4040?style=flat-square&amp;labelColor=2C3E50&amp;logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgNDkwLjE2IDQxMS43Ij48ZGVmcz48c3R5bGU+LmNscy0xe2ZpbGw6I2ZmZjt9PC9zdHlsZT48L2RlZnM+PHBhdGggY2xhc3M9ImNscy0xIiBkPSJNMjMzLjQ1LDQwLjgxYTE3LjU1LDE3LjU1LDAsMSwwLTM1LjEsMFYzMzEuNTZhNDAuODIsNDAuODIsMCwwLDEtODEuNjMsMFYxNDVhMTcuNTUsMTcuNTUsMCwxLDAtMzUuMDksMHY3OS4wNmE0MC44Miw0MC44MiwwLDAsMS04MS42MywwVjE5NS40MmExMS42MywxMS42MywwLDAsMSwyMy4yNiwwdjI4LjY2YTE3LjU1LDE3LjU1LDAsMCwwLDM1LjEsMFYxNDVBNDAuODIsNDAuODIsMCwwLDEsMTQwLDE0NVYzMzEuNTZhMTcuNTUsMTcuNTUsMCwwLDAsMzUuMSwwVjIxNy41aDBWNDAuODFhNDAuODEsNDAuODEsMCwxLDEsODEuNjIsMFYyODEuNTZhMTEuNjMsMTEuNjMsMCwxLDEtMjMuMjYsMFptMjE1LjksNjMuNEE0MC44Niw0MC44NiwwLDAsMCw0MDguNTMsMTQ1VjMwMC44NWExNy41NSwxNy41NSwwLDAsMS0zNS4wOSwwdi0yNjBhNDAuODIsNDAuODIsMCwwLDAtODEuNjMsMFYzNzAuODlhMTcuNTUsMTcuNTUsMCwwLDEtMzUuMSwwVjMzMGExMS42MywxMS42MywwLDEsMC0yMy4yNiwwdjQwLjg2YTQwLjgxLDQwLjgxLDAsMCwwLDgxLjYyLDBWNDAuODFhMTcuNTUsMTcuNTUsMCwwLDEsMzUuMSwwdjI2MGE0MC44Miw0MC44MiwwLDAsMCw4MS42MywwVjE0NWExNy41NSwxNy41NSwwLDEsMSwzNS4xLDBWMjgxLjU2YTExLjYzLDExLjYzLDAsMCwwLDIzLjI2LDBWMTQ1QTQwLjg1LDQwLjg1LDAsMCwwLDQ0OS4zNSwxMDQuMjFaIi8+PC9zdmc+&amp;logoWidth=20">
  </a>
  <a href="https://arxiv.org/abs/2506.13585" rel="nofollow">
    <img alt="Paper" src="https://camo.githubusercontent.com/2604333fbd9843f0be50433505993398230120aca88af3c4dc94e4e8e3034c43/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f93965f50617065722d4d696e694d61782d2d4d312d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/_Paper-MiniMax--M1-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://chat.minimax.io/" rel="nofollow">
    <img alt="Chat" src="https://camo.githubusercontent.com/cf1a97c2a522fe9d780765d5cc263bf289cde77cb8a51435a994aaf202e3e89f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5f4d696e694d61785f436861742d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530266c6f676f3d646174613a696d6167652f7376672b786d6c3b6261736536342c50484e325a79423462577875637a30696148523063446f764c336433647935334d793576636d63764d6a41774d43397a646d6369494868746247357a4f6e68736157357250534a6f644852774f693876643364334c6e637a4c6d39795a7938784f546b354c336873615735724969423261575633516d393450534977494441674e446b774c6a4532494451784d533433496a34385a47566d637a3438633352356247552b4c6d4e736379307865325a706247773649325a6d5a6a74395043397a64486c735a5434384c32526c5a6e4d2b50484268644767675932786863334d39496d4e73637930784969426b50534a4e4d6a4d7a4c6a51314c4451774c6a6778595445334c6a55314c4445334c6a55314c4441734d5377774c544d314c6a45734d46597a4d7a45754e545a684e4441754f4449734e4441754f4449734d4377774c4445744f4445754e6a4d734d4659784e4456684d5463754e5455734d5463754e5455734d4377784c4441744d7a55754d446b734d4859334f5334774e6d45304d4334344d6977304d4334344d6977774c4441734d5330344d5334324d797777566a45354e5334304d6d45784d5334324d7977784d5334324d7977774c4441734d5377794d7934794e697777646a49344c6a5932595445334c6a55314c4445334c6a55314c4441734d4377774c444d314c6a45734d4659784e4456424e4441754f4449734e4441754f4449734d4377774c4445734d5451774c4445304e56597a4d7a45754e545a684d5463754e5455734d5463754e5455734d4377774c4441734d7a55754d537777566a49784e793431614442574e4441754f4446684e4441754f4445734e4441754f4445734d4377784c4445734f4445754e6a49734d4659794f4445754e545a684d5445754e6a4d734d5445754e6a4d734d4377784c4445744d6a4d754d6a59734d4670744d6a45314c6a6b734e6a4d754e4545304d4334344e6977304d4334344e6977774c4441734d4377304d4467754e544d734d545131566a4d774d4334344e5745784e7934314e5377784e7934314e5377774c4441734d53307a4e5334774f537777646930794e6a42684e4441754f4449734e4441754f4449734d4377774c4441744f4445754e6a4d734d46597a4e7a41754f446c684d5463754e5455734d5463754e5455734d4377774c4445744d7a55754d537777566a4d7a4d4745784d5334324d7977784d5334324d7977774c4445734d4330794d7934794e697777646a51774c6a6732595451774c6a67784c4451774c6a67784c4441734d4377774c4467784c6a59794c4442574e4441754f4446684d5463754e5455734d5463754e5455734d4377774c4445734d7a55754d537777646a49324d4745304d4334344d6977304d4334344d6977774c4441734d4377344d5334324d797777566a45304e5745784e7934314e5377784e7934314e5377774c4445734d53777a4e5334784c4442574d6a67784c6a5532595445784c6a597a4c4445784c6a597a4c4441734d4377774c44497a4c6a49324c4442574d545131515451774c6a67314c4451774c6a67314c4441734d4377774c4451304f53347a4e5377784d4451754d6a46614969382b5043397a646d632b266c6f676f57696474683d3230" data-canonical-src="https://img.shields.io/badge/_MiniMax_Chat-FF4040?style=flat-square&amp;labelColor=2C3E50&amp;logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgNDkwLjE2IDQxMS43Ij48ZGVmcz48c3R5bGU+LmNscy0xe2ZpbGw6I2ZmZjt9PC9zdHlsZT48L2RlZnM+PHBhdGggY2xhc3M9ImNscy0xIiBkPSJNMjMzLjQ1LDQwLjgxYTE3LjU1LDE3LjU1LDAsMSwwLTM1LjEsMFYzMzEuNTZhNDAuODIsNDAuODIsMCwwLDEtODEuNjMsMFYxNDVhMTcuNTUsMTcuNTUsMCwxLDAtMzUuMDksMHY3OS4wNmE0MC44Miw0MC44MiwwLDAsMS04MS42MywwVjE5NS40MmExMS42MywxMS42MywwLDAsMSwyMy4yNiwwdjI4LjY2YTE3LjU1LDE3LjU1LDAsMCwwLDM1LjEsMFYxNDVBNDAuODIsNDAuODIsMCwwLDEsMTQwLDE0NVYzMzEuNTZhMTcuNTUsMTcuNTUsMCwwLDAsMzUuMSwwVjIxNy41aDBWNDAuODFhNDAuODEsNDAuODEsMCwxLDEsODEuNjIsMFYyODEuNTZhMTEuNjMsMTEuNjMsMCwxLDEtMjMuMjYsMFptMjE1LjksNjMuNEE0MC44Niw0MC44NiwwLDAsMCw0MDguNTMsMTQ1VjMwMC44NWExNy41NSwxNy41NSwwLDAsMS0zNS4wOSwwdi0yNjBhNDAuODIsNDAuODIsMCwwLDAtODEuNjMsMFYzNzAuODlhMTcuNTUsMTcuNTUsMCwwLDEtMzUuMSwwVjMzMGExMS42MywxMS42MywwLDEsMC0yMy4yNiwwdjQwLjg2YTQwLjgxLDQwLjgxLDAsMCwwLDgxLjYyLDBWNDAuODFhMTcuNTUsMTcuNTUsMCwwLDEsMzUuMSwwdjI2MGE0MC44Miw0MC44MiwwLDAsMCw4MS42MywwVjE0NWExNy41NSwxNy41NSwwLDEsMSwzNS4xLDBWMjgxLjU2YTExLjYzLDExLjYzLDAsMCwwLDIzLjI2LDBWMTQ1QTQwLjg1LDQwLjg1LDAsMCwwLDQ0OS4zNSwxMDQuMjFaIi8+PC9zdmc+&amp;logoWidth=20">
  </a>
  <a href="https://www.minimax.io/platform" rel="nofollow">
    <img alt="API" src="https://camo.githubusercontent.com/033a96d7a4beb7f7872861878556c078064d874835da92f5d2ff5a0fe037c960/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29aa15f4150492d506c6174666f726d2d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/_API-Platform-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://github.com/MiniMax-AI/MiniMax-MCP">
    <img alt="MCP" src="https://camo.githubusercontent.com/a1dd6a9aad054731a18f4af8cc4f477aa43f9cf83920e1676324b97204238b5e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f9a805f4d43502d4d696e694d61785f4d43502d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/_MCP-MiniMax_MCP-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
</p>
<p><a href="https://huggingface.co/MiniMaxAI" rel="nofollow">
    <img alt="Hugging Face" src="https://camo.githubusercontent.com/94aa40386a1540394a4a71cdd73d62c70d4639e122e71df56f06b8a404754daa/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa4975f48756767696e675f466163652d4d696e694d61782d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/_Hugging_Face-MiniMax-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://github.com/MiniMax-AI/MiniMax-M1">
    <img alt="GitHub" src="https://camo.githubusercontent.com/f8147f98bcc14eb4bdbeba8461a7af5189604ad0ac027765507eaef31fb3456c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f90995f4769744875622d4d696e694d61782d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/_GitHub-MiniMax-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://www.modelscope.cn/organization/MiniMax" rel="nofollow">
    <img alt="ModelScope" src="https://camo.githubusercontent.com/0715f6acf7fc905d6a032c08bd8f41b03f492fda6b3f70bf565e853643052b05/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa496efb88f5f4d6f64656c53636f70652d4d696e694d61782d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/_ModelScope-MiniMax-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/LICENSE">
    <img alt="License" src="https://camo.githubusercontent.com/938892ddd7b2f59b079c0be16d5b388f03a3ff8868dbcf480033d0ae3690964f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29a96efb88f5f4c6963656e73652d4170616368655f322e302d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/_License-Apache_2.0-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://github.com/MiniMax-AI/MiniMax-01/blob/main/figures/wechat-qrcode.jpeg">
    <img alt="WeChat" src="https://camo.githubusercontent.com/e2a1862bfaa62514518f7eba6c5ad931f8b898d3ebf9867f69f29a83c0d4131d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f92ac5f5765436861742d4d696e694d61782d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/_WeChat-MiniMax-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">MiniMax-M1</h2><a id="user-content-minimax-m1" aria-label="Permalink: MiniMax-M1" href="#minimax-m1"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">1. Model Overview</h2><a id="user-content-1-model-overview" aria-label="Permalink: 1. Model Overview" href="#1-model-overview"></a></p>
<p dir="auto">We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model.
MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning
attention mechanism. The model is developed based on our previous <a href="https://huggingface.co/MiniMaxAI/MiniMax-Text-01" rel="nofollow">MiniMax-Text-01 model</a>,
which contains a total of 456 billion parameters with 45.9 billion parameters activated
per token. Consistent with MiniMax-Text-01, the M1 model natively supports a context length of 1
million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism
in MiniMax-M1 enables efficient scaling of test-time compute  For example, compared to DeepSeek
R1, M1 consumes 25% of the FLOPs at a generation length of 100K tokens. These properties make M1
particularly suitable for complex tasks that require processing long inputs and thinking extensively.
MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems ranging from
traditional mathematical reasoning to sandbox-based, real-world software engineering environments.
We develop an efficient RL scaling framework for M1 highlighting two perspectives: (1) We propose
CISPO, a novel algorithm that clips importance sampling weights instead of token updates, which
outperforms other competitive RL variants; (2) Our hybrid-attention design naturally enhances the
efficiency of RL, where we address unique challenges when scaling RL with the hybrid architecture. We
train two versions of MiniMax-M1 models with <a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-40k" rel="nofollow">40K</a> and
<a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-80k" rel="nofollow">80K</a> thinking budgets respectively. Experiments
on standard benchmarks show that our models outperform other strong open-weight models such as
the original DeepSeek-R1 and Qwen3-235B, particularly on complex software engineering, tool using,
and long context tasks. With efficient scaling of test-time compute, MiniMax-M1 serves as a strong
foundation for next-generation language model agents to reason and tackle real-world challenges.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/figures/TextBench.png"><img width="100%" src="https://github.com/MiniMax-AI/MiniMax-M1/raw/main/figures/TextBench.png"></a>
  <br>
  <em>Benchmark performance comparison of leading commercial and open-weight models across competition-level mathematics, coding, software engineering, agentic tool use, and long-context understanding tasks. We use the MiniMax-M1-80k model here for MiniMax-M1.</em>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">2. Evaluation</h2><a id="user-content-2-evaluation" aria-label="Permalink: 2. Evaluation" href="#2-evaluation"></a></p>
<p dir="auto"><strong>Performance of MiniMax-M1 on core benchmarks.</strong></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><strong>Category</strong></th>
<th><strong>Task</strong></th>
<th><strong>MiniMax-M1-80K</strong></th>
<th><strong>MiniMax-M1-40K</strong></th>
<th><strong>Qwen3-235B-A22B</strong></th>
<th><strong>DeepSeek-R1-0528</strong></th>
<th><strong>DeepSeek-R1</strong></th>
<th><strong>Seed-Thinking-v1.5</strong></th>
<th><strong>Claude 4 Opus</strong></th>
<th><strong>Gemini 2.5 Pro (06-05)</strong></th>
<th><strong>OpenAI-o3</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td><em>Extended Thinking</em></td>
<td><em>80K</em></td>
<td><em>40K</em></td>
<td><em>32k</em></td>
<td><em>64k</em></td>
<td><em>32k</em></td>
<td><em>32k</em></td>
<td><em>64k</em></td>
<td><em>64k</em></td>
<td><em>100k</em></td>
</tr>
<tr>
<td><em><strong>Mathematics</strong></em></td>
<td>AIME 2024</td>
<td>86.0</td>
<td>83.3</td>
<td>85.7</td>
<td>91.4</td>
<td>79.8</td>
<td>86.7</td>
<td>76.0</td>
<td>92.0</td>
<td>91.6</td>
</tr>
<tr>
<td></td>
<td>AIME 2025</td>
<td>76.9</td>
<td>74.6</td>
<td>81.5</td>
<td>87.5</td>
<td>70.0</td>
<td>74.0</td>
<td>75.5</td>
<td>88.0</td>
<td>88.9</td>
</tr>
<tr>
<td></td>
<td>MATH-500</td>
<td>96.8</td>
<td>96.0</td>
<td>96.2</td>
<td>98.0</td>
<td>97.3</td>
<td>96.7</td>
<td>98.2</td>
<td>98.8</td>
<td>98.1</td>
</tr>
<tr>
<td><em><strong>General Coding</strong></em></td>
<td>LiveCodeBench <em>(24/8~25/5)</em></td>
<td>65.0</td>
<td>62.3</td>
<td>65.9</td>
<td>73.1</td>
<td>55.9</td>
<td>67.5</td>
<td>56.6</td>
<td>77.1</td>
<td>75.8</td>
</tr>
<tr>
<td></td>
<td>FullStackBench</td>
<td>68.3</td>
<td>67.6</td>
<td>62.9</td>
<td>69.4</td>
<td>70.1</td>
<td>69.9</td>
<td>70.3</td>
<td>--</td>
<td>69.3</td>
</tr>
<tr>
<td><em><strong>Reasoning &amp; Knowledge</strong></em></td>
<td>GPQA Diamond</td>
<td>70.0</td>
<td>69.2</td>
<td>71.1</td>
<td>81.0</td>
<td>71.5</td>
<td>77.3</td>
<td>79.6</td>
<td>86.4</td>
<td>83.3</td>
</tr>
<tr>
<td></td>
<td>HLE <em>(no tools)</em></td>
<td>8.4*</td>
<td>7.2*</td>
<td>7.6*</td>
<td>17.7*</td>
<td>8.6*</td>
<td>8.2</td>
<td>10.7</td>
<td>21.6</td>
<td>20.3</td>
</tr>
<tr>
<td></td>
<td>ZebraLogic</td>
<td>86.8</td>
<td>80.1</td>
<td>80.3</td>
<td>95.1</td>
<td>78.7</td>
<td>84.4</td>
<td>95.1</td>
<td>91.6</td>
<td>95.8</td>
</tr>
<tr>
<td></td>
<td>MMLU-Pro</td>
<td>81.1</td>
<td>80.6</td>
<td>83.0</td>
<td>85.0</td>
<td>84.0</td>
<td>87.0</td>
<td>85.0</td>
<td>86.0</td>
<td>85.0</td>
</tr>
<tr>
<td><em><strong>Software Engineering</strong></em></td>
<td>SWE-bench Verified</td>
<td>56.0</td>
<td>55.6</td>
<td>34.4</td>
<td>57.6</td>
<td>49.2</td>
<td>47.0</td>
<td>72.5</td>
<td>67.2</td>
<td>69.1</td>
</tr>
<tr>
<td><em><strong>Long Context</strong></em></td>
<td>OpenAI-MRCR <em>(128k)</em></td>
<td>73.4</td>
<td>76.1</td>
<td>27.7</td>
<td>51.5</td>
<td>35.8</td>
<td>54.3</td>
<td>48.9</td>
<td>76.8</td>
<td>56.5</td>
</tr>
<tr>
<td></td>
<td>OpenAI-MRCR <em>(1M)</em></td>
<td>56.2</td>
<td>58.6</td>
<td>--</td>
<td>--</td>
<td>--</td>
<td>--</td>
<td>--</td>
<td>58.8</td>
<td>--</td>
</tr>
<tr>
<td></td>
<td>LongBench-v2</td>
<td>61.5</td>
<td>61.0</td>
<td>50.1</td>
<td>52.1</td>
<td>58.3</td>
<td>52.5</td>
<td>55.6</td>
<td>65.0</td>
<td>58.8</td>
</tr>
<tr>
<td><em><strong>Agentic Tool Use</strong></em></td>
<td>TAU-bench <em>(airline)</em></td>
<td>62.0</td>
<td>60.0</td>
<td>34.7</td>
<td>53.5</td>
<td>--</td>
<td>44.0</td>
<td>59.6</td>
<td>50.0</td>
<td>52.0</td>
</tr>
<tr>
<td></td>
<td>TAU-bench <em>(retail)</em></td>
<td>63.5</td>
<td>67.8</td>
<td>58.6</td>
<td>63.9</td>
<td>--</td>
<td>55.7</td>
<td>81.4</td>
<td>67.0</td>
<td>73.9</td>
</tr>
<tr>
<td><em><strong>Factuality</strong></em></td>
<td>SimpleQA</td>
<td>18.5</td>
<td>17.9</td>
<td>11.0</td>
<td>27.8</td>
<td>30.1</td>
<td>12.9</td>
<td>--</td>
<td>54.0</td>
<td>49.4</td>
</tr>
<tr>
<td><em><strong>General Assistant</strong></em></td>
<td>MultiChallenge</td>
<td>44.7</td>
<td>44.7</td>
<td>40.0</td>
<td>45.0</td>
<td>40.7</td>
<td>43.0</td>
<td>45.8</td>
<td>51.8</td>
<td>56.5</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">* conducted on the text-only HLE subset.</p>
<p dir="auto">Our models are evaluated with <code>temperature=1.0</code>, <code>top_p=0.95</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">SWE-bench methodology</h3><a id="user-content-swe-bench-methodology" aria-label="Permalink: SWE-bench methodology" href="#swe-bench-methodology"></a></p>
<p dir="auto">We report results derived from the Agentless scaffold. Departing from the original pipeline, our methodology employs a two-stage localization process (without any embedding-based retrieval mechanisms): initial coarse-grained file localization followed by fine-grained localization to specific files and code elements. The values for our models are calculated on the subset of n=486 verified tasks which work on our infrastructure. The excluded 14 test cases that were incompatible with our internal infrastructure are:
<code>"astropy__astropy-7606"</code>,
<code>"astropy__astropy-8707"</code>,
<code>"astropy__astropy-8872"</code>,
<code>"django__django-10097"</code>,
<code>"matplotlib__matplotlib-20488"</code>,
<code>"psf__requests-2317"</code>,
<code>"psf__requests-2931"</code>,
<code>"psf__requests-5414"</code>,
<code>"pylint-dev__pylint-6528"</code>,
<code>"pylint-dev__pylint-7277"</code>,
<code>"sphinx-doc__sphinx-10435"</code>,
<code>"sphinx-doc__sphinx-7985"</code>,
<code>"sphinx-doc__sphinx-8269"</code>,
<code>"sphinx-doc__sphinx-8475"</code></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">TAU-bench methodology</h3><a id="user-content-tau-bench-methodology" aria-label="Permalink: TAU-bench methodology" href="#tau-bench-methodology"></a></p>
<p dir="auto">We evaluate TAU-Bench with GPT-4.1 as user model and without any custom tools. The maximum number of interaction steps is 40.
Our general system prompt is:</p>
<div data-snippet-clipboard-copy-content="- In each round, you need to carefully examine the tools provided to you to determine if any can be used.
- You must adhere to all of the policies. Pay attention to the details in the terms. Solutions for most situations can be found within these policies."><pre><code>- In each round, you need to carefully examine the tools provided to you to determine if any can be used.
- You must adhere to all of the policies. Pay attention to the details in the terms. Solutions for most situations can be found within these policies.
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">3. Deployment Guide</h2><a id="user-content-3-deployment-guide" aria-label="Permalink: 3. Deployment Guide" href="#3-deployment-guide"></a></p>
<p dir="auto">Download the model from HuggingFace repository:</p>
<ul dir="auto">
<li><a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-40k" rel="nofollow">MiniMax-M1-40k</a></li>
<li><a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-80k" rel="nofollow">MiniMax-M1-80k</a></li>
</ul>
<p dir="auto">For production deployment, we recommend using <a href="https://docs.vllm.ai/en/latest/" rel="nofollow">vLLM</a> to serve MiniMax-M1. vLLM provides excellent performance for serving large language models with the following features:</p>
<ul dir="auto">
<li> Outstanding service throughout performance</li>
<li> Efficient and intelligent memory management</li>
<li> Powerful batch request processing capability</li>
<li> Deeply optimized underlying performance</li>
</ul>
<p dir="auto">For detailed vLLM deployment instructions, please refer to our <a href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/docs/vllm_deployment_guide.md">vLLM Deployment Guide</a>.
Alternatively, you can also deploy using Transformers directly. For detailed Transformers deployment instructions, you can see our <a href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/docs/transformers_deployment_guide.md">MiniMax-M1 Transformers Deployment Guide</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">4. Function Calling</h2><a id="user-content-4-function-calling" aria-label="Permalink: 4. Function Calling" href="#4-function-calling"></a></p>
<p dir="auto">The MiniMax-M1 model supports function calling capabilities, enabling the model to identify when external functions need to be called and output function call parameters in a structured format. <a href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/docs/function_call_guide.md">MiniMax-M1 Function Call Guide</a> provides detailed instructions on how to use the function calling feature of MiniMax-M1.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">5. Chatbot &amp; API</h2><a id="user-content-5-chatbot--api" aria-label="Permalink: 5. Chatbot &amp; API" href="#5-chatbot--api"></a></p>
<p dir="auto">For general use and evaluation, we provide a <a href="https://chat.minimax.io/" rel="nofollow">Chatbot</a> with online search capabilities and the <a href="https://www.minimax.io/platform/" rel="nofollow">online API</a> for developers. For general use and evaluation, we provide the <a href="https://github.com/MiniMax-AI/MiniMax-MCP">MiniMax MCP Server</a> with video generation, image generation, speech synthesis, and voice cloning for developers.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">6. Contact Us</h2><a id="user-content-6-contact-us" aria-label="Permalink: 6. Contact Us" href="#6-contact-us"></a></p>
<p dir="auto">Contact us at <a href="mailto:model@minimax.io">model@minimax.io</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI is eating our brains. MIT study: Your brain on ChatGPT (122 pts)]]></title>
            <link>https://www.media.mit.edu/projects/your-brain-on-chatgpt/overview/</link>
            <guid>44307257</guid>
            <pubDate>Wed, 18 Jun 2025 06:47:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.media.mit.edu/projects/your-brain-on-chatgpt/overview/">https://www.media.mit.edu/projects/your-brain-on-chatgpt/overview/</a>, See on <a href="https://news.ycombinator.com/item?id=44307257">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                <p>Check project's website:&nbsp;<a href="https://www.brainonllm.com/">https://www.brainonllm.com</a></p><p>With today's wide adoption of LLM products like ChatGPT from OpenAI, humans and businesses engage and use LLMs on a daily basis. Like any other tool, it carries its own set of advantages and limitations. This study focuses on finding out the <b>cognitive cost of using an LLM</b> in the educational context of writing an essay.</p><p>We assigned participants to three groups: <b>LLM group, Search Engine group, and Brain-only group, where each participant used a designated tool (or no tool in the latter) to write an essay</b>. We conducted 3 sessions with the same group assignment for each participant. In the 4th session we asked LLM group participants to use no tools (we refer to them as LLM-to-Brain), and the Brain-only group participants were asked to use LLM (Brain-to-LLM). We recruited a total of 54 participants for Sessions 1, 2, 3, and 18 participants among them completed session 4. </p><p>We used electroencephalography (EEG) to <b>record participants' brain activity </b>in order to assess their cognitive engagement and cognitive load, and to gain a deeper understanding of neural activations during the essay writing task. We performed <b>NLP analysis</b>, and we interviewed each participant after each session. We performed scoring with the help from the <b>human teachers and an AI judge</b> (a specially built AI agent).</p><p>We discovered a consistent homogeneity across the Named Entities Recognition (NERs), n-grams, ontology of topics within each group. EEG analysis presented robust evidence that LLM, Search Engine and Brain-only groups had <b>significantly different neural connectivity patterns</b>, reflecting divergent cognitive strategies. <b>Brain connectivity systematically scaled down with the amount of external support: the Brainonly group exhibited the strongest, widestranging networks, Search Engine group showed intermediate engagement, and LLM assistance elicited the weakest overall coupling.</b> In session 4, LLM-to-Brain participants showed weaker neural connectivity and under-engagement of alpha and beta networks; and the Brain-to-LLM participants demonstrated higher memory recall, and reengagement of widespread occipito-parietal and prefrontal nodes, likely supporting the visual processing, similar to the one frequently perceived in the Search Engine group. The reported<b> ownership </b>of LLM group's essays in the interviews <b>was low.</b> The Search Engine group had strong ownership, but lesser than the Brain-only group. The LLM group also <b>fell behind in their ability to quote</b> from the essays they wrote just minutes prior. </p><p>As the educational impact of LLM use only begins to settle with the general population, in this study we demonstrate the pressing matter of a likely <b>decrease in learning skills</b> based on the results of our study. The use of LLM had a measurable impact on participants, and while the benefits were initially apparent, as we demonstrated over the course of 4 months, the <b>LLM group's participants performed worse than their counterparts in the Brain-only group at all levels: neural, linguistic, scoring.</b></p><p>We hope this study serves as a preliminary guide to understanding the cognitive and practical impacts of AI on learning environments.</p><p>#cognitivedebt #brainonllm #yourbrainonchatgpt&nbsp;</p>
                            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Make little apps for you and your friends (393 pts)]]></title>
            <link>https://pontus.granstrom.me/scrappy/</link>
            <guid>44306859</guid>
            <pubDate>Wed, 18 Jun 2025 05:16:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pontus.granstrom.me/scrappy/">https://pontus.granstrom.me/scrappy/</a>, See on <a href="https://news.ycombinator.com/item?id=44306859">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Software is important to people. Most of us spend our workdays in front of computers. We use the computer in our pocket tens if not hundreds of times every day. The apps we use are almost exclusively mass-market, sold on an app-store, made for thousands if not millions of users. Or they are enterprise apps that are custom-built for hundreds of thousands of dollars.</p><p>But there isnt really any equivalent of home-made software  apps made lovingly by you for your friends and family. Apps that arent polished or flashy, but are made to <em>your</em> <em>preference</em> and help you with <em>your particular needs.</em></p><p>Were John and Pontus, and weve been exploring the potential of home-made software together.</p><p>We ended up creating a research prototype that we call <strong>Scrappy</strong>  a tool for making <strong>scrappy apps for just you and your friends.</strong> First and foremost, we aim to contribute a <em>vision</em> of what home-made software could be like. We want to make this vision as concrete as we can, by sharing a working tool and examples of apps made in it. Scrappy, in its current state, is a prototype, not a robust tool, but we hope it paints the picture we carry in our heads  of software as something that can be creative, personal, expressive. Made by anyone, for themselves and their loved ones.</p><h2 id="what-is-scrappy">What is Scrappy?</h2><p>It may not be clear what a scrappy app for you and your friends means. What kind of apps are these? Let us paint a picture with a few examples. (We call them <strong>Scrapps</strong>.)</p><div><div><p><strong>Arithmetic practice for a kid in elementary school.</strong> When outgrown, the Scrapp can be extended with harder problems.<br>(<a href="https://scrappy.jrcpl.us/s/?template=/examples/math_practice.json">try on desktop</a>)</p><video src="https://pontus.granstrom.me/scrappy/examples/math_practice.mp4" poster="https://pontus.granstrom.me/scrappy/examples/math_practice.jpg" controls=""></video></div><div><p><strong>Attendee counter for a local event.</strong> The counters state is shared, so the Scrapp can be used to let people in and out at multiple entrances.<br>(<a href="https://scrappy.jrcpl.us/s/?template=/examples/attendee_counter.json">try on desktop</a>)</p><video src="https://pontus.granstrom.me/scrappy/examples/attendee_counter.mp4" poster="https://pontus.granstrom.me/scrappy/examples/attendee_counter.jpg" controls=""></video></div></div><div><div><p><strong>Meeting cost clock,</strong> to help meetings stay on track. A Scrapp like this can be put together in 15 minutes and shared with coworkers right away.<br>(<a href="https://scrappy.jrcpl.us/s/?template=/examples/meeting_cost_clock.json">try on desktop</a>)</p><video src="https://pontus.granstrom.me/scrappy/examples/meeting_cost_clock.mp4" poster="https://pontus.granstrom.me/scrappy/examples/meeting_cost_clock.jpg" controls=""></video></div><div><p><strong>Weekly chore tracker.</strong> Let roommates flexibly swap weeks, while making sure to track whose up next, to keep things fair.<br>(<a href="https://scrappy.jrcpl.us/s/?template=/examples/chores.json">try on desktop</a>)</p><video src="https://pontus.granstrom.me/scrappy/examples/chores.mp4" poster="https://pontus.granstrom.me/scrappy/examples/chores.jpg" controls=""></video></div></div><h2 id="what-is-it-like-to-make-an-app-in-scrappy">What is it like to make an app in Scrappy?</h2><p>Scrappy is an infinite canvas of interactive objects. The workflow is similar to an app such as Figma, Miro, or Google Slides  except you can attach behaviors to the objects.</p><p>You drag objects out on the canvas  a button, a textfield, a few labels. Select an object, and you can modify its attribute in an inspector panel. Certain objects, like buttons, has attributes like when clicked that contain javascript code. When the button is clicked, that code is run  maybe it records the contents of the textfield to a label that acts as a log. You build your app step by step: tweaking and rearranging the objects, and attaching a little bit of code to them.</p><p>Theres no better way to get a feeling for an authoring environment than to see someone use it in action. In the following videos, Im making an attendee counter for an event.</p><p><strong>The basics.</strong> I start out by adding a number field to track the number of attendees, and two buttons for recording people entering and exiting the venue.</p><video controls="" playsinline="" poster="https://pontus.granstrom.me/scrappy/walkthrough/basics.jpg">
<source src="https://pontus.granstrom.me/scrappy/walkthrough/basics.mp4" type="video/mp4">Your browser cannot play this video.</video><p><strong>Reactive formulas.</strong> Next, I add a field for the venues capacity, and a warning when too many people have been let in.
I use a reactive formula to control the visibility of the warning and the border color of the field.</p><video controls="" playsinline="" poster="https://pontus.granstrom.me/scrappy/walkthrough/reactive-formulas.jpg">
<source src="https://pontus.granstrom.me/scrappy/walkthrough/reactive-formulas.mp4" type="video/mp4">Your browser cannot play this video.</video><p><strong>A shared, persistent world.</strong> Without any extra work, Scrappy apps are multiplayer.
App state is persisted and synced, like users expect from online documents like Google Sheets or Figma.</p><video controls="" playsinline="" poster="https://pontus.granstrom.me/scrappy/walkthrough/shared-persistent-world.jpg">
<source src="https://pontus.granstrom.me/scrappy/walkthrough/shared-persistent-world.mp4" type="video/mp4">Your browser cannot play this video.</video><p><strong>The app is always live.</strong> Theres no distinction between editing and running. I can edit the app while a friend is using it.</p><video controls="" playsinline="" poster="https://pontus.granstrom.me/scrappy/walkthrough/liveness.jpg">
<source src="https://pontus.granstrom.me/scrappy/walkthrough/liveness.mp4" type="video/mp4">Your browser cannot play this video.</video><p><strong>Selective sharing.</strong> I make a variant of the app thats limited to only entering and exiting people. This is done by putting a part of the app in a frame, and sharing only that frame. The limited version is still linked to the main app.</p><video controls="" playsinline="" poster="https://pontus.granstrom.me/scrappy/walkthrough/selective-sharing.jpg">
<source src="https://pontus.granstrom.me/scrappy/walkthrough/selective-sharing.mp4" type="video/mp4">Your browser cannot play this video.</video><div><div><p><strong>Visible, tangible data.</strong> Heres what the <a href="https://www.notion.so/Scrappy-Make-Little-Apps-for-You-and-Your-Friends-1ba27a2e3bb6806fb782cf2ff7e5764e?pvs=21">Meeting Cost Clock app</a> shown above looks like when zoomed out, revealing a common pattern in Scrapps.</p><p>Outside the shared frame are a bunch of fields used to compute the cost of the meeting. This lets me see the data while Im working on the Scrapp, just like in a spreadsheet, which is very helpful for debugging  and it makes future tweaking or remixing easier.</p></div><p><img src="https://pontus.granstrom.me/scrappy/walkthrough/meeting_cost_clock_internals.png" alt="Meeting cost clock internals"></p></div><h2 id="why-make-scrappy">Why make Scrappy?</h2><p>This project is driven by a desire to reimagine software creation and use. As part of a growing movement variously termed <a href="https://hackernoon.com/big-and-small-computing-73dc49901b9a">small computing</a>, <a href="https://dubroy.com/blog/casual-programming/">casual programming</a>, and <a href="https://maggieappleton.com/home-cooked-software">home-cooked software</a> we want to <a href="https://www.inkandswitch.com/end-user-programming/">emancipate end-users</a>  to empower people to express themselves without requiring them to be heavy-duty programmers, to liberate the programming of computers from the priesthood to the layperson, as Bill Atkinson worded it. We want to shift the world away from mass-market, industrially-produced software toward more <a href="https://x.com/davidhoang/status/1802140453292372272">personal, even disposable,</a> tools that are designed for and readily <a href="https://malleable.systems/">modified and adapted</a> to <a href="https://gwern.net/doc/technology/2004-03-30-shirky-situatedsoftware.html">specific social contexts</a>. Above all, we want to foster a sense of agency and to ultimately contribute to <a href="https://x.com/cwervo/status/1808578326409457834">redistributing the means of software production</a>.</p><p>We were inspired by the simplicity of tools like <a href="https://www.notion.so/">Notion</a>, <a href="https://www.tldraw.com/">tldraw</a>, and <a href="https://mmm.page/">mmm.page</a>, but wanted to empower people with richer interactivity and programming capabilities. However, knowing the strengths and limitations of the standard visual programming paradigms of blocks (e.g. <a href="https://scratch.mit.edu/">Scratch</a>, <a href="https://developers.google.com/blockly">Blockly</a>) and nodes-and-wires (e.g. <a href="https://cycling74.com/products/max">Max/MSP</a>, <a href="https://nodered.org/">Node-RED</a>, <a href="https://natto.dev/">natto</a>, <a href="https://www.holograph.so/">Holograph</a>), we deliberately wanted to go down a different path. Instead, we drew direct inspiration from media with scripting environments, both classic systems like <a href="https://en.wikipedia.org/wiki/HyperCard">HyperCard</a>, <a href="https://en.wikipedia.org/wiki/Visual_Basic_(classic)">Visual Basic</a>, and <a href="https://en.wikipedia.org/wiki/Adobe_Director">Macromedia Director</a>, as well as contemporary platforms like <a href="https://www.notion.so/Project-Concept-Definition-618cd9f0e26944f3b1ee4222c1db92c9?pvs=21">Dynamicland</a> and <a href="https://www.minecraft.net/">Minecraft</a>, where the media with scripting exist in a shared online world.</p><p>Overall, our target user experience was that of a productivity tool, specifically a canvas-based tool (e.g. <a href="https://www.figma.com/">Figma</a>, <a href="https://miro.com/">Miro</a>, and <a href="https://www.tldraw.com/">tldraw</a>)rather than programming environments (e.g. <a href="https://squeak.org/">Squeak/Smalltalk</a>, modern IDEs) and website and app builders (e.g. <a href="https://www.squarespace.com/">Squarespace</a>, <a href="https://mmm.page/">mmm.page</a>, <a href="https://bubble.io/">Bubble</a>). And we also wanted that kind of modern share link-based real-time collaboration popularized by <a href="https://docs.google.com/">Google Docs</a> and <a href="https://www.figma.com/">Figma</a>.</p><p>Finally, while we acknowledge the capabilities of AI-centric systems that leverage LLMs for code generation (e.g., <a href="https://lovable.dev/">Lovable</a>, <a href="http://bolt.new/">bolt.new</a>, and <a href="https://computer.tldraw.com/">tldraw computer</a>), we deliberately chose to focus our design on direct manipulation and user control.</p><h2 id="who-is-scrappy-for">Who is Scrappy for?</h2><p>As we were prototyping, it wasnt clear who the ideal user for Scrappy was. We left this open, to see what wed learn from building the system. Eventually, a few potential personas revealed themselves.</p><ul><li><strong>The process optimizer.</strong> In business environments, theres always some improvement that can be made using software. But the person who sees the process inefficiency likely cant make software themselves, and involving a professional programmer is expensive. So what usually happens is they make what improvements they can using tools they are familiar with, such as Excel. Here Scrappy could be a more powerful and flexible Excel, while retaining familiarity and ease of use.</li><li><strong>Teachers and students</strong>. Teaching programming requires teaching a multitude of inessential technical details: how to use the command line, how file systems work, how to set up the environment, dependency management, version control, servers and clients, and on and on. With Scrappy, you can just create a button, write a line of code and click the button to run the code.</li><li><strong>Ourselves!</strong> We are professional programmers who dont like programming. Why? Because of the all the aforementioned complexity that adds friction to what could be so much simpler. When making mass-market apps, we know we have to deal with that complexity, but when working on a fun hobby project?! Give us a break. Scrappy is that kind of break.</li><li><strong>The DIYer.</strong> People like to customize their house, grow their own vegetables, sow their own clothes, build their own furniture. Scrappy is where a DIY-inclined person makes their own little apps for themselves and their friends.</li></ul><p>As Scrappy solidified, we wanted to focus on one of these personas. Theres a pull toward business use cases, since businesses are the most willing to pay for a product, but we believe the incentives there would lead us too close to existing products like <a href="https://retool.com/">Retool</a> or <a href="https://livecode.com/">LiveCode</a>. The teaching use case is compelling, but we believe it needs a better coding experience (discoverability, better error messages, debugger) which was out of scope for us (for now). We are itching to make stuff for ourselves in Scrappy (and we are strong believers in dogfooding), but most of our projects required features that would balloon the scope.</p><p>The DIYer making home-made software is the least served by existing tools, and fits our vision of democratized computing the best. We decided this is where we could make the biggest contribution (the <a href="https://en.wikipedia.org/wiki/Blue_Ocean_Strategy">blue ocean strategy</a>), and decided to make the DIYer our target persona.</p><p>Ideally, Scrappy would let anyone with basic computer literacy make a simple app and learn from there. This is not quite the case yet  some JavaScript knowledge is required. So today, the person making Scrapps from scratch is a <strong>programmer DIYer.</strong> But when a Scrapp is shared with friends, those friends can use it and remix it without needing programming experience.</p><h2 id="what-should-i-make-in-scrappy">What should I make in Scrappy?</h2><p>Home-made, scrappy apps dont really exist today, so most people (including us!) are not used to coming up with ideas for them. When faced with a problem that would make a great Scrapp, instead our minds go to maybe theres an app for that, searching the web for one, giving up if we cannot find a good one. To start coming up with good uses for Scrappy requires a shift to a home-made mindset.</p><p>To help you build that mindset, here is an assortment of ideas for Scrapps (some of which are not feasible in the current prototype of Scrappy, but should be).</p><div><div><ul><li>Custom flashcards</li><li>Meeting agenda manager</li><li>Day clock for person with dementia</li><li>Online workshop facilitation</li><li>Consulting time tracker</li><li>Point-based voting for a board</li><li>Receipt generator</li><li>Simple word game</li><li>School grade calculator</li><li>Interactive visual recipe</li><li>Social quiz game</li></ul></div><div><ul><li>Typing tutor</li><li>Lyric writing aids (synonyms, rhymes)</li><li>Board game helper</li><li>Wedding RSVP + seating arrangement</li><li>Dynamic opening hours display</li><li>Family bulletin board</li><li>Group travel planner</li><li>Chore  allowance calculator</li><li>Chess clock productivity timer</li></ul></div></div><p>What makes a problem well-suited for Scrappy? Here are some things they have in common:</p><ul><li><strong>Shared with friends.</strong> While a Scrapp can be for just yourself, Scrappy really shines with multiples users, leveraging the shared, persistent world. Some problems that would need setting up a backend server can be built in minutes in Scrappy.</li><li><strong>Needs tweaking-as-you-go.</strong> Life changes, and so does requirements. In Scrappy, you can edit the app at any time  even while your friend is using it. No building, no deploying, no fuss.</li><li><strong>A sprinkle of computation.</strong> Scrappy shines when thought of as a shared document first, with a little bit of computation added on top. For complex systems with a lot of moving parts, we recommend reaching for traditional software engineering tools.</li><li><strong>Minimal friction.</strong> We all let out a groan inside whenever we are hit with create an account to continue. This account friction may not be much, but it multiplies when sharing with a group of people  theres always going to be someone for whom the friction is too much. Scrapps dont have this problem: just click the link.</li><li><strong>Small number of trusted users.</strong> Scrappy assumes you trust the people you share a Scrapp with, which removes a lot of friction, but if you need to control access and permissions, look elsewhere.</li><li><strong>Not mission-critical.</strong> If you need guaranteed correctness or perfect control over details, dont reach for Scrappy. Those qualities are what you pay expert engineers for.</li></ul><h2 id="scrappy-vs-mass-market-apps">Scrappy vs mass-market apps</h2><p>When faced with a scrappy problem  something small that would benefit from a computer  most people will think maybe theres an app for that, followed by searching an app store or the Internet to look for one.</p><p>If there is no app for that, or theres no good one, you could make your own in Scrappy. We hope you do! But often there <em>is</em> an app for that. If there is, it will probably be more polished than anything you can make in Scrappy. In this case, there are still reasons to consider using making your own Scrapp:</p><ul><li><strong>Does exactly what you need.</strong> And only what you need. Nothing more, nothing less.</li><li><strong>Home-made with love.</strong> Scrapps are made by you for your friends. A home-knitted sweater will always mean more to you than a store-bought one.</li><li><strong>Fun and playful.</strong> In Scrappy, its easy to play around. Tweak the colors, make a cute layout, add little inside jokes.</li><li><strong>Remixable.</strong> Easy to share with others and modify to suit your needs.</li><li><strong>Collaborative by default.</strong> All Scrappy apps are multiplayer, like a Google Doc is. You can even edit them while they are being used by someone else!</li><li><strong>No accounts and signups.</strong> If you share a Scrappy app with someone, they can start using it right away  no tedious sign-up flows stopping your friends or family from joining in.</li><li><strong>You own your data.</strong> The data is stored locally and will only be used for nefarious purposes if its creator (you) wants to!</li></ul><h2 id="scrappy-vs-ai-written-apps">Scrappy vs AI-written apps</h2><p>What about asking an LLM to make a custom, home-made app?</p><p>LLMs are getting better and better, and while they are far from able to make a full-fledged app without a lot of help from a software engineer, they can make small apps pretty reliably.</p><p>So if I can ask ChatGPT or Claude to make an app, why would I use Scrappy?</p><ul><li><strong>Scrappy is understandable.</strong> Using an LLM means going from an English prompt to pages of React code, which is too big a leap for non-programmers. They end up having to rely on the LLM to make changes, and are left helpless if the LLM doesnt do the right thing. In contrast, Scrappys objects-on-a-canvas model is easy to understand, more humane, and acts a shared substrate where user and AI can collaborate on equal footing. And because it is less overwhelming, its more likely the user will pick up some programming skills.</li><li><strong>Scrappy is collaborative.</strong> All Scrappy apps are little shared worlds, persistent and with live updating  all for free. LLMs are mainly useful for creating static front-end-only web apps. And in Scrappy, apps can be edited by multiple users in realtime, whereas AI workflows are mostly type, then wait with little room for collaboration between humans.</li><li><strong>Scrappy is more fun!</strong> While typing a few sentences of English and seeing a full app appear out of nowhere still feels like magic, it quickly grows old when youre waiting for minutes only to see the LLM misunderstood you again. In Scrappy, there is joy in tweaking things or remixing something. A spark of ooh I want it to do this and its only a few clicks and keystrokes away. A sense of creative ownership. And you can edit it together with friends!</li></ul><h2 id="scrappy-vs-hypercard-and-its-successors">Scrappy vs HyperCard (and its successors)</h2><p><a href="https://hypercard.org/">HyperCard</a> was popular among Macintosh users in the early 90s, and is often held as an exemplar of enabling home-made software and end-user programming. Decades later, there have been a number of successors to HyperCard, both commercial (<a href="https://www.mackiev.com/hyperstudio/">HyperStudio</a>, <a href="https://en.wikipedia.org/wiki/SuperCard">SuperCard</a>, <a href="https://livecode.com/">LiveCode</a>) and non-commercial (<a href="https://beyondloom.com/decker/">Decker</a> and <a href="https://hypervariety.com/WildCard/">WildCard</a>, among a number of open-source remakes, most of which are abandonware). Most of these have been quite literal replicas of HyperCard, driven by nostalgia, down to the black-and-white graphics. None have been as successful as the original.</p><p>We wanted to create something in the spirit of HyperCard, rather than recreate HyperCard. Scrappy is different from HyperCard and its direct descendants in a few key ways:</p><ul><li><strong>Designed for the Internet.</strong> Scrappy apps are easily shareable online with a simple link, whereas using HyperCard and most of its descendants is like being trapped in a virtual machine.</li><li><strong>A shared world.</strong> HyperCard stacks could be shared as a file with other users. Scrappy takes this to the next level by letting users edit and use apps at the same time.</li><li><strong>Modern UI conventions.</strong> Scrappy apps live on a high-resolution infinite canvas, with selections, copying, panning and zooming, frames for grouping, etc.</li><li><strong>Uses JavaScript for scripting.</strong> HyperCard and a number of its descendants use programming languages that arent in common use. JavaScript is the most common programming language in the world, is native to the Web and works well for a dynamic environment such as Scrappy.</li><li><strong>A larger palette of interactive objects.</strong> Many HyperCard-likes only support a few elements like buttons, text fields, and images. Scrappy supports more UI elements like sliders and timers, but also data types beyond strings: numbers, dates, and compound JSON objects.</li><li><strong>Reactive formulas, like a spreadsheet.</strong> The idea of this value changes when that value changes is familiar to many, and can be a stepping stone toward event-based programming, where the user has to think about state.</li></ul><h2 id="future-directions">Future directions</h2><p>With our prototype, we think that weve been successful at proving the ideas and design principles that we started with. But theres a lot more work to do. The number of Scrapps that can be built in a way that feel Scrappy native is still low. Much of the time, existing knowledge of JavaScript is required. To improve this, we need to continue work in both lowering the floor and raising the ceiling.</p><p>Lowering the floor means making things more friendly and approachable for people with little or no programming experience. For example:</p><ul><li><strong>Improve code discoverability.</strong> Weve made coding easier by presenting the names of objects visually on the screen, and listing their methods in the properties panel. But theres a ton more than we can do. You should be able to click on objects to discover their methods and insert them in the code. Available names should auto-complete so you dont have remember syntax and do as much typing.</li><li><strong>Improve debugging.</strong> You should be able to visualize relationships between objects, perhaps as arrows showing which objects read or modify other objects. Error messages should be better worded and show more information about what went wrong. You should be able pause and rewind execution. All of this while live collaborating on the app with a friend.</li><li><strong>Leverage AI.</strong> <a href="https://www.notion.so/Scrappy-Make-Little-Apps-for-You-and-Your-Friends-1ba27a2e3bb6806fb782cf2ff7e5764e?pvs=21">As we mentioned earlier</a>, we dont believe in having an LLM make an entire app, but we are interested in having it act as an assistant, directed by the user. Maybe youd click on the canvas and ask the AI to make start and stop buttons here, or go to a text labels when changed handler and ask the AI to write code to show an error message if using non-english characters.</li><li><strong>Make it even easier to share and remix</strong>. Its easier to learn by inspecting and tweaking other people work than it is to start from a blank canvas. We imagine a public gallery where users can publish their creations, and other users can adopt and customize them for their own needs and preferences.</li><li><strong>Make Scrapps work well on phones/tablets.</strong> A hand-sized touchscreen is too small for editing Scrapps comfortably, but using them on phones should work well  this is not currently the case. The infinite canvas paradigm means that objects have fixed positions, which is a way simpler mental model layout rules (like in CSS), but means designs arent responsive to screen size. However, drag-and-drop web page design tools like <a href="http://mmm.page/">mmm.page</a> and <a href="https://www.squarespace.com/">Squarespace</a> show a way to handle this: simply show safe areas for mobile to the user.</li></ul><p>Raising the ceiling means adding functionality and expressive power, letting users create more things with less effort. For example:</p><ul><li><strong>Add support for collections</strong>. Currently, you can edit and store strings, numbers, dates, and JSON data, but you cannot store lists of them or make an editable tables, like in a spreadsheet. We also dont have any kind of layout containers, like lists, grids, or stacks. Adding this would let authors express more things visually, and they wouldnt have to resort to JavaScript knowledge and hidden state.</li><li><strong>Instanced frames.</strong> Frames let you <a href="https://www.notion.so/Submission-writeup-1a227a2e3bb680c5be5ddc988af65ce2?pvs=21">selectively share</a> parts of your app, but that frame is fully synced in real-time across users. This is desirable in some cases and undesirable in others. For example, when sharing a form, each user should only see and edit their own copy. You should be able to share instances of the form, that still collects all the data in one place. Another example is a board game helper where theres some hidden information and users should see some shared UI and some UI only visible to them.</li><li><strong>Tools for data processing.</strong> Weve found ourselves wanting to use Scrappy to process tabular data. Things like: do the same operations to all rows, filter the table based on some criteria, etc. This can be done in JavaScript, but there should be a Scrappy-native way of doing this, where the data is shown.</li><li><strong>Better support for reuse.</strong> Currently, if you want to repeat an object or set of objects in your Scrapp, you have to manually edit them one by one, or write code to manage them. Instead, you should be able to define a reusable component and make instances that stay linked to the main component. Figma has this, PowerPoint has slide masters, HyperCard has card backgrounds, all to this effect. Further, these components could be shared across projects, or even with other users.</li><li><strong>Allow extending Scrappy.</strong> Some capabilities will be out of reach using Scrappys primitives. Currently, we are the only ones able to add new objects, but wed want to open this up to more people. We expect this would require programming and web expertise, so it wouldnt be something for a traditional engineer, not the typical Scrappy user.</li><li><strong>Clean up the conceptual model.</strong> Currently, some of the objects store data values, and some support event handlers like when clicked and when changed handlers. The current implementation is a bit arbitrary about this, which is not only confusing but also limiting. Common behaviors like editing, clicking, and storing data should be made more consistent and freely mixable  like the <a href="https://en.wikipedia.org/wiki/Entity_component_system">entity component systems</a> of game engines like <a href="https://unity.com/">Unity</a>.</li></ul><h2 id="conclusion">Conclusion</h2><p>We believe computers should work for people, and dream of a future where computing, like cooking or word processing, is available to everyone. Where you can solve your own small, unique problems with small, unique apps. Where you dont just rely on mass-market apps made by expert programmers. Where you share home-made little apps with family and friends.</p><p>Scrappy is our contribution to this dream. Each Scrapp is a live, persistent world, easily shared and remixed, closer to familiar productivity apps than alien developer tools. Like any vision, ours is incomplete, but weve grounded our explorations in a working prototype with examples of apps.</p><a href="https://scrappy.jrcpl.us/">Try Scrappy! (desktop only)</a><p>We hope Scrappy will inspire you to further chase this particular windmill. If it does, please let us know!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Locally hosting an internet-connected server (132 pts)]]></title>
            <link>https://mjg59.dreamwidth.org/72095.html</link>
            <guid>44306792</guid>
            <pubDate>Wed, 18 Jun 2025 04:58:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mjg59.dreamwidth.org/72095.html">https://mjg59.dreamwidth.org/72095.html</a>, See on <a href="https://news.ycombinator.com/item?id=44306792">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I'm lucky enough to have a <a href="https://www.monkeybrains.net/">weird niche ISP</a> available to me, so I'm paying $35 a month for around 600MBit symmetric data. Unfortunately they don't offer static IP addresses to residential customers, and nor do they allow multiple IP addresses per connection, and I'm the sort of person who'd like to run a bunch of stuff myself, so I've been looking for ways to manage this.</p><p>What I've ended up doing is renting a cheap VPS from a vendor that lets me add multiple IP addresses for minimal extra cost. The precise nature of the VPS isn't relevant - you just want a machine (it doesn't need much CPU, RAM, or storage) that has multiple world routeable IPv4 addresses associated with it and has no port blocks on incoming traffic. Ideally it's geographically local and peers with your ISP in order to reduce additional latency, but that's a nice to have rather than a requirement.</p><p>By setting that up you now have multiple real-world IP addresses that people can get to. How do we get them to the machine in your house you want to be accessible? First we need a connection between that machine and your VPS, and the easiest approach here is <a href="https://wireguard.com/">Wireguard</a>. We only need a point-to-point link, nothing routable, and none of the IP addresses involved need to have anything to do with any of the rest of your network. So, on your local machine you want something like:</p><tt><p>[Interface]<br>PrivateKey = privkeyhere<br>ListenPort = 51820<br>Address = localaddr/32</p><p>[Peer]<br>Endpoint = VPS:51820 <br>PublicKey = pubkeyhere <br>AllowedIPs = VPS/0</p></tt><p>And on your VPS, something like:</p><tt><p>[Interface]<br>Address = vpswgaddr/32<br>SaveConfig = true<br>ListenPort = 51820<br>PrivateKey = privkeyhere</p><p>[Peer]<br>PublicKey = pubkeyhere<br>AllowedIPs = localaddr/32</p></tt><p>The addresses here are (other than the VPS address) arbitrary - but they do need to be consistent, otherwise Wireguard is going to be unhappy and your packets will not have a fun time. Bring that interface up with <a href="https://www.wireguard.com/quickstart/">wg-quick</a> and make sure the devices can ping each other. Hurrah! That's the easy bit.</p><p>Now you want packets from the outside world to get to your internal machine. Let's say the external IP address you're going to use for that machine is </p><tt>321.985.520.309</tt><p> and the wireguard address of your local system is </p><tt>867.420.696.005</tt><p>. On the VPS, you're going to want to do:</p><tt>iptables -t nat -A PREROUTING -p tcp -d 321.985.520.309 -j DNAT --to-destination 867.420.696.005</tt><p>Now, all incoming packets for </p><tt>321.985.520.309</tt><p> will be rewritten to head towards </p><tt>867.420.696.005</tt><p> instead (make sure you've set </p><tt>net.ipv4.ip_forward</tt><p> to 1 via </p><tt>sysctl</tt><p>!). Victory! Or is it? Well, no.</p><p>What we're doing here is rewriting the destination address of the packets so instead of heading to an address associated with the VPS, they're now going to head to your internal system over the Wireguard link. Which is then going to ignore them, because the </p><tt>AllowedIPs</tt><p> statement in the config only allows packets coming from your VPS, and these packets still have their original source IP. We could rewrite the source IP to match the VPS IP, but then you'd have no idea where any of these packets were coming from, and that sucks. Let's do something better. On the local machine, in the peer, let's update </p><tt>AllowedIps</tt><p> to </p><tt>0.0.0.0/0</tt><p> to permit packets form any source to appear over our Wireguard link. But if we bring the interface up now, it'll try to route <em>all</em> traffic over the Wireguard link, which isn't what we want. So we'll add </p><tt>table = off</tt><p> to the </p><tt>interface</tt><p> stanza of the config to disable that, and now we can bring the interface up without breaking everything but still allowing packets to reach us. However, we do still need to tell the kernel how to reach the remote VPN endpoint, which we can do with </p><tt>ip route add vpswgaddr dev wg0</tt><p>. Add this to the </p><tt>interface</tt><p> stanza as:</p><tt><p>PostUp = ip route add vpswgaddr dev wg0<br>PreDown = ip route del vpswgaddr dev wg0</p></tt><p>That's half the battle. The problem is that they're going to show up there with the source address still set to the original source IP, and your internal system is (because Linux) going to notice it has the ability to just send replies to the outside world via your ISP rather than via Wireguard and nothing is going to work. Thanks, Linux. Thinux.</p><p>But there's a way to solve this - policy routing. Linux allows you to have multiple separate routing tables, and define policy that controls which routing table will be used for a given packet. First, let's define a new table reference. On the local machine, edit </p><tt>/etc/iproute2/rt_tables</tt><p> and add a new entry that's something like:</p><tt><p>1 wireguard</p></tt><p>where "1" is just a standin for a number not otherwise used there. Now edit your wireguard config and replace </p><tt>table=off</tt><p> with </p><tt>table=wireguard</tt><p> - Wireguard will now update the </p><tt>wireguard</tt><p> routing table rather than the global one. Now all we need to do is to tell the kernel to push packets into the appropriate routing table - we can do that with </p><tt>ip rule add from localaddr lookup wireguard</tt><p>, which tells the kernel to take any packet coming from our Wireguard address and push it via the Wireguard routing table. Add that to your Wireguard interface config as:</p><tt><p>PostUp = ip rule add from localaddr lookup wireguard<br>PreDown = ip rule del from localaddr lookup wireguard</p></tt><p>and now your local system is effectively on the internet.</p><p>You can do this for multiple systems - just configure additional Wireguard interfaces on the VPS and make sure they're all listening on different ports. If your local IP changes then your local machines will end up reconnecting to the VPS, but to the outside world their accessible IP address will remain the same. It's like having a real IP without the pain of convincing your ISP to give it to you.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Lstr  A modern, interactive tree command written in Rust (201 pts)]]></title>
            <link>https://github.com/bgreenwell/lstr</link>
            <guid>44306041</guid>
            <pubDate>Wed, 18 Jun 2025 02:07:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/bgreenwell/lstr">https://github.com/bgreenwell/lstr</a>, See on <a href="https://news.ycombinator.com/item?id=44306041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">lstr</h2><a id="user-content-lstr" aria-label="Permalink: lstr" href="#lstr"></a></p>
<p dir="auto"><a href="https://github.com/bgreenwell/lstr/actions"><img src="https://github.com/bgreenwell/lstr/actions/workflows/ci.yml/badge.svg" alt="Build Status"></a>
<a href="https://crates.io/crates/lstr" rel="nofollow"><img src="https://camo.githubusercontent.com/b6682dc3f177211760ed9636185d7aafc140643f821ab6e95f05d8052f37919d/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f6c7374722e737667" alt="Latest Version" data-canonical-src="https://img.shields.io/crates/v/lstr.svg"></a>
<a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/6cd0120cc4c5ac11d28b2c60f76033b52db98dac641de3b2644bb054b449d60c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"></a></p>
<p dir="auto">A blazingly fast, minimalist directory tree viewer, written in Rust. Inspired by the command line program <a href="https://github.com/Old-Man-Programmer/tree">tree</a>, with a powerful interactive mode.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/bgreenwell/lstr/blob/main/assets/lstr-demo.gif"><img src="https://github.com/bgreenwell/lstr/raw/main/assets/lstr-demo.gif" alt="" data-animated-image=""></a>
<em>An interactive overview of <code>lstr</code>'s project structure... using <code>lstr</code>.</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Philosophy</h2><a id="user-content-philosophy" aria-label="Permalink: Philosophy" href="#philosophy"></a></p>
<ul dir="auto">
<li><strong>Fast:</strong> Runs directory scans in parallel by default to maximize speed on modern hardware.</li>
<li><strong>Minimalist:</strong> Provides essential features without the bloat. The core experience is clean and uncluttered.</li>
<li><strong>Interactive:</strong> An optional TUI mode for fluid, keyboard-driven exploration.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><strong>High-performance:</strong> Scans directories in parallel to be as fast as possible.</li>
<li><strong>Classic and interactive modes:</strong> Use <code>lstr</code> for a classic <code>tree</code>-like view, or launch <code>lstr interactive</code> for a fully interactive TUI.</li>
<li><strong>Rich information display (optional):</strong>
<ul dir="auto">
<li>Display file-specific icons with <code>--icons</code> (requires a Nerd Font).</li>
<li>Show file permissions with <code>-p</code>.</li>
<li>Show file sizes with <code>-s</code>.</li>
<li><strong>Git Integration:</strong> Show file statuses (<code>Modified</code>, <code>New</code>, <code>Untracked</code>, etc.) directly in the tree with the <code>-G</code> flag.</li>
</ul>
</li>
<li><strong>Smart filtering:</strong>
<ul dir="auto">
<li>Respects your <code>.gitignore</code> files with the <code>-g</code> flag.</li>
<li>Control recursion depth (<code>-L</code>) or show only directories (<code>-d</code>).</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">You need the Rust toolchain installed on your system to build <code>lstr</code>.</p>
<ol dir="auto">
<li><strong>Clone the repository:</strong>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/bgreenwell/lstr.git
cd lstr"><pre>git clone https://github.com/bgreenwell/lstr.git
<span>cd</span> lstr</pre></div>
</li>
<li><strong>Build and install using Cargo:</strong>
<div dir="auto" data-snippet-clipboard-copy-content="# This compiles in release mode and copies the binary to ~/.cargo/bin
cargo install --path ."><pre><span><span>#</span> This compiles in release mode and copies the binary to ~/.cargo/bin</span>
cargo install --path <span>.</span></pre></div>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="lstr [OPTIONS] [PATH]
lstr interactive [OPTIONS] [PATH]"><pre>lstr [OPTIONS] [PATH]
lstr interactive [OPTIONS] [PATH]</pre></div>
<p dir="auto">Note that <code>PATH</code> defaults to the current directory (<code>.</code>) if not specified.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Option</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-a</code>, <code>--all</code></td>
<td>List all files and directories, including hidden ones.</td>
</tr>
<tr>
<td><code>--color &lt;WHEN&gt;</code></td>
<td>Specify when to use color output (<code>always</code>, <code>auto</code>, <code>never</code>).</td>
</tr>
<tr>
<td><code>-d</code>, <code>--dirs-only</code></td>
<td>List directories only, ignoring all files.</td>
</tr>
<tr>
<td><code>-g</code>, <code>--gitignore</code></td>
<td>Respect <code>.gitignore</code> and other standard ignore files.</td>
</tr>
<tr>
<td><code>-G</code>, <code>--git-status</code></td>
<td>Show git status for files and directories.</td>
</tr>
<tr>
<td><code>--icons</code></td>
<td>Display file-specific icons; requires a <a href="https://www.nerdfonts.com/" rel="nofollow">Nerd Font</a>.</td>
</tr>
<tr>
<td><code>-L</code>, <code>--level &lt;LEVEL&gt;</code></td>
<td>Maximum depth to descend.</td>
</tr>
<tr>
<td><code>-p</code>, <code>--permissions</code></td>
<td>Display file permissions (Unix-like systems only).</td>
</tr>
<tr>
<td><code>-s</code>, <code>--size</code></td>
<td>Display the size of files.</td>
</tr>
<tr>
<td><code>--expand-level &lt;LEVEL&gt;</code></td>
<td><strong>Interactive mode only:</strong> Initial depth to expand the interactive tree.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Interactive mode</h2><a id="user-content-interactive-mode" aria-label="Permalink: Interactive mode" href="#interactive-mode"></a></p>
<p dir="auto">Launch the TUI with <code>lstr interactive [OPTIONS] [PATH]</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Keyboard controls</h3><a id="user-content-keyboard-controls" aria-label="Permalink: Keyboard controls" href="#keyboard-controls"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Key(s)</th>
<th>Action</th>
</tr>
</thead>
<tbody>
<tr>
<td><code></code> / <code>k</code></td>
<td>Move selection up.</td>
</tr>
<tr>
<td><code></code> / <code>j</code></td>
<td>Move selection down.</td>
</tr>
<tr>
<td><code>Enter</code></td>
<td><strong>Context-aware action:</strong><br>- If on a file: Open it in the default editor (<code>$EDITOR</code>).<br>- If on a directory: Toggle expand/collapse.</td>
</tr>
<tr>
<td><code>q</code> / <code>Esc</code></td>
<td>Quit the application normally.</td>
</tr>
<tr>
<td><code>Ctrl</code>+<code>s</code></td>
<td><strong>Shell integration:</strong> Quits and prints the selected path to stdout.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto"><strong>1. List the contents of the current directory</strong></p>

<p dir="auto"><strong>2. Explore a project interactively, ignoring gitignored files</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="lstr interactive -g --icons"><pre>lstr interactive -g --icons</pre></div>
<p dir="auto"><strong>3. Display a directory with file sizes and permissions (classic view)</strong></p>

<p dir="auto"><strong>4. See the git status of all files in a project</strong></p>

<p dir="auto"><strong>5. Start an interactive session with all data displayed</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="lstr interactive -gG --icons -s -p"><pre>lstr interactive -gG --icons -s -p</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Piping and shell interaction</h2><a id="user-content-piping-and-shell-interaction" aria-label="Permalink: Piping and shell interaction" href="#piping-and-shell-interaction"></a></p>
<p dir="auto">The classic <code>view</code> mode is designed to work well with other command-line tools via pipes (<code>|</code>).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Interactive fuzzy finding with <code>fzf</code></h3><a id="user-content-interactive-fuzzy-finding-with-fzf" aria-label="Permalink: Interactive fuzzy finding with fzf" href="#interactive-fuzzy-finding-with-fzf"></a></p>
<p dir="auto">This is a powerful way to instantly find any file in a large project.</p>

<p dir="auto"><code>fzf</code> will take the tree from <code>lstr</code> and provide an interactive search prompt to filter it.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Paging large trees with <code>less</code> or <code>bat</code></h3><a id="user-content-paging-large-trees-with-less-or-bat" aria-label="Permalink: Paging large trees with less or bat" href="#paging-large-trees-with-less-or-bat"></a></p>
<p dir="auto">If a directory is too large to fit on one screen, pipe the output to a <em>pager</em>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Using less (the -R flag preserves color)
lstr -L 10 | less -R

# Using bat (a modern pager that understands colors)
lstr --icons | bat"><pre><span><span>#</span> Using less (the -R flag preserves color)</span>
lstr -L 10 <span>|</span> less -R

<span><span>#</span> Using bat (a modern pager that understands colors)</span>
lstr --icons <span>|</span> bat</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Changing directories with <code>lstr</code></h3><a id="user-content-changing-directories-with-lstr" aria-label="Permalink: Changing directories with lstr" href="#changing-directories-with-lstr"></a></p>
<p dir="auto">You can use <code>lstr</code> as a visual <code>cd</code> command. Add the following function to your shell's startup file (e.g., <code>~/.bashrc</code>, <code>~/.zshrc</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="# A function to visually change directories with lstr
lcd() {
    # Run lstr and capture the selected path into a variable.
    # The TUI will draw on stderr, and the final path will be on stdout.
    local selected_dir
    selected_dir=&quot;$(lstr interactive -g --icons)&quot;

    # If the user selected a path (and didn't just quit), `cd` into it.
    # Check if the selection is a directory.
    if [[ -n &quot;$selected_dir&quot; &amp;&amp; -d &quot;$selected_dir&quot; ]]; then
        cd &quot;$selected_dir&quot;
    fi
}"><pre><span><span>#</span> A function to visually change directories with lstr</span>
<span>lcd</span>() {
    <span><span>#</span> Run lstr and capture the selected path into a variable.</span>
    <span><span>#</span> The TUI will draw on stderr, and the final path will be on stdout.</span>
    <span>local</span> selected_dir
    selected_dir=<span><span>"</span><span><span>$(</span>lstr interactive -g --icons<span>)</span></span><span>"</span></span>

    <span><span>#</span> If the user selected a path (and didn't just quit), `cd` into it.</span>
    <span><span>#</span> Check if the selection is a directory.</span>
    <span>if</span> [[ <span>-n</span> <span><span>"</span><span>$selected_dir</span><span>"</span></span> <span>&amp;&amp;</span> <span>-d</span> <span><span>"</span><span>$selected_dir</span><span>"</span></span> ]]<span>;</span> <span>then</span>
        <span>cd</span> <span><span>"</span><span>$selected_dir</span><span>"</span></span>
    <span>fi</span>
}</pre></div>
<p dir="auto">After adding this and starting a new shell session (or running <code>source ~/.bashrc</code>), you can simply run:</p>

<p dir="auto">This will launch the <code>lstr</code> interactive UI. Navigate to the directory you want, press <code>Ctrl+s</code>, and your shell's current directory will instantly change.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance and concurrency</h2><a id="user-content-performance-and-concurrency" aria-label="Permalink: Performance and concurrency" href="#performance-and-concurrency"></a></p>
<p dir="auto">By default, <code>lstr</code> uses a parallel directory walker to maximize speed on multi-core systems. This parallelism is managed by the excellent <a href="https://crates.io/crates/rayon" rel="nofollow">rayon</a> thread pool, which is used internally by <code>lstr</code>'s directory traversal engine.</p>
<p dir="auto">For advanced use cases, such as benchmarking or limiting CPU usage, you can control the number of threads by setting the <code>RAYON_NUM_THREADS</code> environment variable before running the command.</p>
<p dir="auto"><strong>To force single-threaded (serial) execution:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="RAYON_NUM_THREADS=1 lstr ."><pre>RAYON_NUM_THREADS=1 lstr <span>.</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Inspiration</h2><a id="user-content-inspiration" aria-label="Permalink: Inspiration" href="#inspiration"></a></p>
<p dir="auto">The philosophy and functionality of <code>lstr</code> are heavily inspired by the excellent C-based <a href="https://github.com/Old-Man-Programmer/tree">tree</a> command line program. This project is an attempt to recreate that classic utility in modern, safe Rust.</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>