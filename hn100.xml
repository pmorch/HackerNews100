<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 09 Jan 2025 04:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[You don't have to pay the Microsoft 365 price increase (221 pts)]]></title>
            <link>https://www.consumer.org.nz/articles/you-don-t-have-to-pay-the-microsoft-365-price-increase</link>
            <guid>42640180</guid>
            <pubDate>Thu, 09 Jan 2025 00:25:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.consumer.org.nz/articles/you-don-t-have-to-pay-the-microsoft-365-price-increase">https://www.consumer.org.nz/articles/you-don-t-have-to-pay-the-microsoft-365-price-increase</a>, See on <a href="https://news.ycombinator.com/item?id=42640180">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p>Microsoft 365 is an all-in-one cloud service. It includes office software like Word and Excel, the storage service OneDrive and a few other miscellaneous bits and pieces. </p>

<p>Until November 2024, Microsoft 365 Personal cost $129 a year, or $12 a month. </p>

<p>Now, the price has risen to $179 annually, or $17 a month – an increase of about 40%.</p>

<p>The 365 Family plan, which includes up to six user accounts, has also risen in cost by $50 a year. </p>

<figure><img alt="Website promo image 1290 x 860  27 " srcset="https://d3c7odttnp7a2d.cloudfront.net/assets/13123/Website_Promo_Image_1290_x_860__27__medium.jpg 416w, https://d3c7odttnp7a2d.cloudfront.net/assets/13123/Website_Promo_Image_1290_x_860__27__medium%402x.jpg 832w, https://d3c7odttnp7a2d.cloudfront.net/assets/13123/Website_Promo_Image_1290_x_860__27__full_width.jpg 1010w, https://d3c7odttnp7a2d.cloudfront.net/assets/13123/Website_Promo_Image_1290_x_860__27__full_width%402x.jpg 2020w" sizes="(min-width: 38em) 830px, 100vw" src="https://d3c7odttnp7a2d.cloudfront.net/assets/13123/Website_Promo_Image_1290_x_860__27__full_width%402x.jpg"></figure>

<p>Customers have been told the price jump comes with integration of Microsoft’s <a href="https://www.consumer.org.nz/articles/generative-ai-how-it-works-and-what-it-s-good-for">generative AI</a> chatbot, Copilot, into office apps. (Think AI-generated images for PowerPoint slides or Word docs condensed into shorter summaries.) </p>

<p>But crucially, 365 plans don’t provide unlimited access to Copilot – that requires an extra $37/month subscription called Copilot Pro. Instead, the plans provide a bank of 60 credits per month, where each credit pays for one action that makes use of Copilot.  </p>

<p>Credits don’t carry over if you don’t use them – the first day of each month wipes the slate clean and resets to 60 again. On a Family plan, the credits can only be used by the primary account, not the others in the household. </p>

<p>On face value, a price hike of around 30–40% for a half-hearted implementation of an AI tool seems like a bad deal – at least for some of the tens of thousands of 365 subscribers in Aotearoa.</p>
<h2 id="article-opting-out-of-the-increase" data-sidebar-menu-item="true">Opting out of the increase</h2>
<p>There’s one way to stay on your existing plan, but it’s extremely well hidden and requires calling Microsoft’s bluff. </p>

<p>You have to pretend to cancel your plan! </p>

<p>Log into your Microsoft account at account.microsoft.com. Find your 365 subscription and select “Manage”. Then select “Cancel subscription”. </p>

<p>That’s when a new option will miraculously appear – Microsoft 365 Classic, which has no price increase or Copilot AI. In other words, it’s your old plan. </p>

<figure><img alt="Customers are presented with three subscription options – their current plan, the cheaper “Classic” plan and a “Basic” OneDrive-only plan." srcset="https://d3c7odttnp7a2d.cloudfront.net/assets/13113/Website_Promo_Image_1290_x_860__1__medium.png 416w, https://d3c7odttnp7a2d.cloudfront.net/assets/13113/Website_Promo_Image_1290_x_860__1__medium%402x.png 832w, https://d3c7odttnp7a2d.cloudfront.net/assets/13113/Website_Promo_Image_1290_x_860__1__full_width.png 1010w, https://d3c7odttnp7a2d.cloudfront.net/assets/13113/Website_Promo_Image_1290_x_860__1__full_width%402x.png 2020w" sizes="(min-width: 38em) 830px, 100vw" src="https://d3c7odttnp7a2d.cloudfront.net/assets/13113/Website_Promo_Image_1290_x_860__1__full_width%402x.png"><figcaption>Customers are presented with three subscription options – their current plan, the cheaper “Classic” plan and a “Basic” OneDrive-only plan.</figcaption></figure>

<p>Customers aren’t told about this option anywhere else. It isn’t mentioned in the official post announcing Copilot integration, the email notifying customers of the price rise or on Microsoft’s marketing website. In all three situations, only the new AI-studded versions of 365 are listed. </p>

<p>We strongly encourage all Microsoft 365 customers to consider whether they want to pay a premium for Copilot integration or would prefer to keep the service they know. As it stands, customers aren’t being given that choice. </p>

<p>We’d like to thank Consumer NZ member Rob, who tipped us off about this change and how to get around it.</p>
<h2 id="article-you-can-also-get-the-cheaper-plan-as-a-new-customer" data-sidebar-menu-item="true">You can also get the cheaper plan as a new customer</h2>
<p>To test whether 365 Classic was only available to existing customers at the time of the announcement, we signed up to one month’s worth of 365 Personal, then immediately went to cancel it.  </p>

<p>To our surprise, we were also able to access the classic plan at the old price – we just had to pay for one month at the new, higher price first. </p>

<p>So, anyone who isn’t currently on a 365 plan but might want to access Office apps or OneDrive storage in the future also needs to know about this “secret” tier to be able to make an informed decision about the best option for them.</p>
<h2 id="article-what-this-says-about-microsoft" data-sidebar-menu-item="true">What this says about Microsoft</h2>
<p>Frankly, we feel this is a flagrant breach of goodwill and trust for Microsoft’s customers. </p>

<p>It’s unethical for a company to automatically upgrade all its customers to a more expensive product they didn’t ask for – especially when customers aren’t even informed they have the ability to opt out. It’s also potentially misleading and a breach of the Fair Trading Act.  </p>

<p>It reflects the powerful market position Microsoft enjoys in this space. As long as Word remains the default word processor that everyone uses, Microsoft has consumers over a barrel and can charge what it likes for office apps. We don’t think that’s fair and would encourage consumers to consider free alternatives, like LibreOffice and WPS Office. </p>

<p>This issue also highlights how tech companies are having to smuggle recent innovations into consumers’ lives because many consumers aren’t willing to pay for new features upfront. Microsoft, in particular, has invested many billions of dollars toward its Copilot product and needs to recoup that cost somehow. </p>

<p>Rather than sneaking an extra $5 a month, perhaps Microsoft should focus its efforts on making Copilot valuable enough that its customers will actually be willing to pay for it.</p>
<h2 id="article-the-wider-issue" data-sidebar-menu-item="true">The wider issue</h2>
<p>Hiding the ability to opt out of a change to a pricier plan is a textbook example of a “dark pattern”. </p>

<p>A dark pattern is when a user interface is designed to trick users into making choices they wouldn’t otherwise make.  </p>

<p>Other instances include preselected tick boxes for extras like extended warranties when shopping online or services such as gyms that can be joined online but only cancelled in person. This particular practice is <a href="https://www.consumer.org.nz/articles/you-don-t-have-to-go-to-the-gym-to-cancel-your-membership">likely in breach of our country’s consumer law</a>, but most dark patterns are legal. </p>

<p>The Australian federal government is currently consulting on <a href="https://www.consumer.org.nz/articles/australia-is-banning-dynamic-pricing-should-aotearoa-do-the-same">a ban of unfair trading practices</a>, which will prohibit many dark patterns, including subscription traps, dynamic pricing and hidden fees. There’s no immediate plan for a similar piece of legislation in Aotearoa, but we’d like to see the same protections introduced here. </p>

<p>Dark patterns have become a particularly pressing consumer issue in the wake of subscription models taking over many industries, including the “software-as-a-service” trend that Microsoft 365 is a part of. </p>

<p>We don’t think a business should have to use deceptive designs to sell its products. Using dark patterns, like Microsoft is doing in this instance, suggests a company that doesn’t believe its products are good enough to stand on their own in the market. </p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[28h Days: year 1 update (102 pts)]]></title>
            <link>https://sidhion.com/blog/28h_days_update_1/</link>
            <guid>42639779</guid>
            <pubDate>Wed, 08 Jan 2025 23:37:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sidhion.com/blog/28h_days_update_1/">https://sidhion.com/blog/28h_days_update_1/</a>, See on <a href="https://news.ycombinator.com/item?id=42639779">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-bvzihdzo=""> <!-- Note: `max-w` determined empirically. The `0` in the font used for body text is too wide, so I had to figure out a number that fit 2.6 alphabets in a single line. --> <article>     <p>It’s been a little over a year since I started living <a href="https://sidhion.com/blog/28h_days" target="_self" data-astro-cid-zbkkag4h=""> 28h days  </a> .
I figured it might be a good idea to write an yearly update, and doing it at the beginning of the year feels appropriate, so here we are.</p>
<p>The one-sentence summary is that moving to a 28h-day schedule is the second best thing I’ve done for my health, with the first one being to regularly exercise.
Keep reading for details, what I had to change, some important things I discovered, and some funny realisations.</p>
<p>It took me around 2 months to adapt to this new schedule.
Those 2 months were hard, trying to get my body used to the new cycle while I got super sleepy after 10 or 11 hours into the day, but also with the SO complaining that I wasn’t available to reply to messages sometimes (because I was sleeping when she was awake), which was no encouragement to keep going.</p>
<p>Things improved after I made an adjustment: if I got too sleepy before the 10- or 11-hour mark, I’d take a nap shorter than 1 hour.
I realised that on certain days, especially when I exercised during my morning, it was almost impossible to stay up the full 18h, so the nap helped me keep going while sticking to the weekly sleep schedule.
I still deploy the <em>strategic nap</em> when needed.</p>
<p>When I travel and visit friends, or friends visit me, or I make plans with people to do things over many days, it's useful to be on a 24h schedule.
This happened a few times so far, and after each one I got better at moving between both schedules.
Since they “match” during weekends, that's when I move from one schedule to another.
I discovered that it's important to stay on the same schedule for the entire week until the next opportunity to move back to the other schedule (i.e. weekends).
Napping is the key to smoothly shift my sleep schedule, and by now I'm able to effortlessly go to 24h when needed, which isn't more than 4 or 5 times distributed over an year.</p>
<p>A consequence of the shifting schedule is that words like “daily”, “day”, “night”, “morning”, “breakfast”, “lunch”, “dinner” get a different meaning and sometimes end up confusing other people.
For example, on Wednesdays I eat my breakfast around 22:00, and my lunch for that “day” falls on Thursdays around 04:00.
People have a hard time following these things, and I haven’t found an easier way to convey this kind of information to them, so I avoid talking about this with others.</p>
<p>In fact, I avoid talking about the schedule at all with others.
I’ve experienced wildly different reactions to my 28h schedule.
Some people find it amusing or a curiosity, others seem interested and end up asking a few questions, but there are those who get offended or think I’m crazy or being unhealthy and try to lecture me or get me to stop.
I’m lucky when the latter kind decide to stop talking to me or change subjects instead.
Because of these reactions, I only comment about this schedule when I need to make plans involving other people <em>and</em> they need to know why I’m not available on certain days.</p>
<p>The internal benefits more than make up for the slightly trickier external interactions.
My sleep has been the most consistent throughout the year when compared to any other year (I don’t keep track of this in any scientific way, only mental notes that I make along the way).
I feel like I have a lot of time every day, and at the end of the day I usually feel like I got a lot of things done.
My workouts got more regular, and I increased the amount of workouts I get every week.</p>
<p>Since my schedule shifts compared to the 24h one, there’s always at least one day in the week when I’m online at the same time as friends that live all over the world.
This means I can chat with people in Australia when they’re going through their day at least once a week, for example.
This is useful for meetings and/or video calls.</p>
<p>I also get to go to the gym between 05:30 and 06:00, which means way fewer people working out at the same time as me, so I never have to wait to use any equipment and the environment isn’t as noisy.
As a result, I get my workouts done faster and feel less drained after them, which is why I increased the amount of workouts per week: they became a more pleasant experience.
I still have to force myself to exercise, but there are hardly any negative experiences since I don’t need to interact with other people anymore.</p>
<p>Even small things became more pleasant.
For example, now I can open my window to let fresh air in when everyone else’s sleeping and there’s almost no noise on the street (I’m in Canada, and it’s essentially impossible to keep windows open for too long for a good part of the year since it’s too cold outside, so I can't leave them unattended).
I’ve had a lot of opportunities to watch the sun rise without forcing myself to wake up earlier.
Since my weekdays only partially overlap normal business hours, there’s less noise on average on my day.
It seems silly, but noise makes a noticeable effect on my mood, so I’m more often on better mood now.</p>
<p>Early on, one of the most important realisations I made was that I had to change my meal plans.
Since I had 6 days on my week, if I kept following the meal plan I had made for a 7-day week it would mean I'd essentially skip an entire day worth of nutrition.
As a result, I increased my daily calorie intake to compensate for the longer days.</p>
<p>Since I now live 6-day weeks, I also do certain things less often, like showering or brushing my teeth.
Brazilians <a href="https://worldpopulationreview.com/country-rankings/bathing-habits-by-country" target="_blank" data-astro-cid-zbkkag4h=""> take the most showers <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" fill="none" data-astro-cid-zbkkag4h=""> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 6H7a1 1 0 0 0-1 1v10a1 1 0 0 0 1 1h10a1 1 0 0 0 1-1v-5m-6 0 7.5-7.5M15 3h6v6" data-astro-cid-zbkkag4h=""></path> </svg> </a>  out of any other people in the world, so by showering less often I'm also moving closer to the global average.</p>
<p>I still have enough opportunities throughout the week to do things that need to happen at “normal people schedule”, like getting groceries, getting the occasional meal with someone, going to the dentist, and so on.</p>
<p>In summary, 28h days have been a successful experiment for me, and I can’t see myself willingly going back to a 24h schedule again.
It’s only been a year, but I feel like I already found 95% of the “short-term changes” between both schedules.
There might be some longer-term changes, mostly associated with my health, but if these exist, they’ll likely start showing up in the next few years.
We’ll see what the future holds.</p>
<p>Until then, I’ll enjoy as much as possible this freedom that I discovered.</p>   </article> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Steve Langasek, one of Ubuntu Linux's leading lights, has died (129 pts)]]></title>
            <link>https://thenewstack.io/steve-langasek-one-of-ubuntu-linuxs-leading-lights-has-died/</link>
            <guid>42639563</guid>
            <pubDate>Wed, 08 Jan 2025 23:13:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thenewstack.io/steve-langasek-one-of-ubuntu-linuxs-leading-lights-has-died/">https://thenewstack.io/steve-langasek-one-of-ubuntu-linuxs-leading-lights-has-died/</a>, See on <a href="https://news.ycombinator.com/item?id=42639563">Hacker News</a></p>
Couldn't get https://thenewstack.io/steve-langasek-one-of-ubuntu-linuxs-leading-lights-has-died/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Salesforce Will Hire No More Software Engineers in 2025, Says Marc Benioff (247 pts)]]></title>
            <link>https://www.salesforceben.com/salesforce-will-hire-no-more-software-engineers-in-2025-says-marc-benioff/</link>
            <guid>42639417</guid>
            <pubDate>Wed, 08 Jan 2025 22:56:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.salesforceben.com/salesforce-will-hire-no-more-software-engineers-in-2025-says-marc-benioff/">https://www.salesforceben.com/salesforce-will-hire-no-more-software-engineers-in-2025-says-marc-benioff/</a>, See on <a href="https://news.ycombinator.com/item?id=42639417">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Salesforce will not be hiring any more software engineers in 2025 amid significant productivity boosts from AI, Marc Benioff has revealed.</p><p>The CEO and founder of Salesforce told the <a href="https://www.youtube.com/watch?v=Xgsxi7IGMEU" target="_blank" rel="noreferrer noopener">20VC with Harry Stebbings</a> podcast that the cloud giant was in the midst of doing its business plan for next year, and Agentforce – the company’s flagship artificial intelligence product – was the “only thing that really matters today”.</p> <h2 id="h-30-productivity-boost-from-ai">“30% Productivity Boost” from AI</h2><p>In a long-ranging conversation with the venture capitalist, Marc outlined the reasons why his company decided to implement the hiring freeze.</p><p>When asked if Salesforce would have more or fewer employees in five years’ time, he said he thinks the company will “probably be larger”.</p><p>But he went on to say: <strong>“We’re not adding any more software engineers next year because we have increased the productivity this year with Agentforce and with other AI technology that we’re using for engineering teams by more than 30% – to the point where our engineering velocity is incredible. I can’t believe what we’re achieving in engineering.”</strong></p><p><strong>“And then, we will have less support engineers next year because we have an agentic layer. We will have more salespeople next year because we really need to explain to people exactly the value that we can achieve with AI. So, we will probably add another 1,000 to 2,000 salespeople in the short term.”</strong></p><p>He also confirmed a focus shift in the company which many in the ecosystem have been commenting on for some time:</p><blockquote><p>“Everything needs to become about Agentforce at Salesforce – this is the only thing that really matters today.” <cite></cite></p></blockquote><p>Marc and Salesforce rarely miss an opportunity to talk up Agentforce, with the AI platform clearly becoming the company’s primary focus in recent months.</p><p>We at Salesforce Ben tested it out ourselves in early <a href="https://www.salesforceben.com/agentforce-is-now-available-on-salesforce-help-but-is-it-actually-helpful/" target="_blank" rel="noreferrer noopener">December</a> when the AI became available on the cloud giant’s help portal. Our reviewer reported mixed results, finding that experienced Salesforce professionals who provided clear and detailed instructions could find use in it, while more entry-level people might make poor choices and accumulate technical debt from following an agent’s instructions.</p><h2 id="h-winter-is-coming">Winter Is Coming…</h2><p>While December is a time of celebration for many, it may be slightly less so for people working at Salesforce – which has been known to let staff go at the start of the year.</p><p>In January 2024, <a href="https://www.salesforceben.com/salesforce-announces-layoffs/" target="_blank" rel="noreferrer noopener">we reported</a> how the ‘mothership’ was laying off 700 workers – roughly 1% of its 70,000-strong global workforce. This was no doubt heartbreaking news for many of those affected, but it was a drop in the bucket compared to the previous year’s lay-offs. In January 2023, <a href="https://www.salesforceben.com/salesforce-to-lay-off-10-of-workforce-what-does-this-mean/" target="_blank" rel="noreferrer noopener">Salesforce Ben reported</a> how the cloud giant was letting go of 7,000 people – roughly 10% of the entire company.</p><p>At the time, CEO Marc Benioff sent a letter to employees saying that they had “hired too many people” during the coronavirus pandemic. Tech giants like Salesforce saw huge demand for their services throughout the COVID-19 outbreak, with cloud services providing much-needed remote working options for businesses amid stay-at-home orders.</p><p>As Marc himself wrote in his letter to employees: <strong>“As our revenue accelerated through the pandemic, we hired too many people leading into this economic downturn we’re now facing, and I take responsibility for that.”</strong></p><h2 id="h-final-thoughts">Final Thoughts</h2><p>It remains to be seen whether Salesforce’s campaign of staff cutting following the COVID pandemic has well and truly come to an end, though nervous Salesforce staff might take solace in Marc Benioff’s words that the company will “probably be larger” in five years.</p><p>Still, with January approaching, no doubt some ‘mothership’ team members will be anxious that 2025 will see a repeat of the layoffs of the previous two years. And if the Agentforce productivity bump means no further software engineers, what else might it mean?</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LA wildfires force thousands to evacuate, NASA JPL closed (220 pts)]]></title>
            <link>https://www.theregister.com/2025/01/08/los_angeles_fires_jpl/</link>
            <guid>42638735</guid>
            <pubDate>Wed, 08 Jan 2025 21:40:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/01/08/los_angeles_fires_jpl/">https://www.theregister.com/2025/01/08/los_angeles_fires_jpl/</a>, See on <a href="https://news.ycombinator.com/item?id=42638735">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>With several major wildfires raging out of control in Los Angeles County, fire crews are risking their lives to protect people, homes, and a key NASA facility.</p>
<p>At the time of the writing, about 70,000 people were told to evacuate their homes as the fires erupted after a dry winter, and hurricane-force winds howling across Southern California spread the flames <a target="_blank" rel="nofollow" href="https://www.nbclosangeles.com/news/california-wildfires/map-la-county-wildfires-palisades-eaton-hurst/3597500/">over thousands of acres</a>. Some 400,000 residents are without power, and water pressure needed to fight the blazes is reportedly failing, with hydrants said to be running dry.</p>
<p>Two people so far are reported dead, more than 1,000 structures have been destroyed, and California Governor Gavin Newsom has declared a state of emergency. The National Guard - along with thousands of firefighters (many of them prisoners <a target="_blank" rel="nofollow" href="https://www.davispoliticalreview.com/article/the-use-of-inmate-firefighters-and-its-injustices">working</a> for $10 a day or less) - have been deployed to help tackle the conflagration.</p>

    

<p>"This is a highly dangerous windstorm that’s creating extreme fire risk – and we’re not out of the woods. We’re already seeing the destructive impacts with this fire in Pacific Palisades that grew rapidly in a matter of minutes," Newsom <a target="_blank" rel="nofollow" href="https://www.gov.ca.gov/2025/01/07/governor-newsom-proclaims-state-of-emergency-meets-with-first-responders-in-pacific-palisades-amid-dangerous-fire-weather/">said</a>.</p>

        


        

<p>"Our deepest thanks go to our expert firefighters and first responders who jumped quickly into fighting this dangerous fire."</p>
<p>A crew has been deployed to protect NASA's Jet Propulsion Laboratory, which houses top science talent - not to mention the primary control stations for the agency's Mars and outer space hardware. The site is on emergency lockdown and several staff report losing their homes to wildfire.</p>

        

<p>"JPL is closed except for emergency personnel," <a target="_blank" rel="nofollow" href="https://x.com/LaurieofMars/status/1877033688539468061">tweeted</a> the site's director Laurie Leshin. "No fire damage so far (some wind damage) but it is very close to the lab. Hundreds of JPLers have been evacuated from their homes &amp; many have lost homes. Special thx to our emergency crews. Pls keep us in your thoughts &amp; stay safe."</p>
<ul>

<li><a href="https://www.theregister.com/2023/12/16/fossil_fuels_wildfires/">Shame about those wildfires. We'll just let the fossil fuel giants off the hook, then?</a></li>

<li><a href="https://www.theregister.com/2023/08/18/ai_maui_fire_book/">'AI-written history' of Maui wildfire becomes Amazon bestseller, fuels conspiracies</a></li>

<li><a href="https://www.theregister.com/2022/10/20/california_wildfires_co2e/">California wildfires hit CTRL+Z on 18 years of CO2e removal</a></li>

<li><a href="https://www.theregister.com/2024/11/28/voyager_engineer_feature/">'Best job at JPL': What it's like to be an engineer on the Voyager project</a></li>
</ul>
<p>"My heart goes out to those affected by the Palisades Fire in Southern California," <a target="_blank" rel="nofollow" href="https://x.com/SenBillNelson/status/1877023652501151950">said</a> NASA Administrator Bill Nelson. "Many in our NASA family, including our teams at Armstrong and JPL, are affected. Grace and I are praying for their safety and the safety of the first responders battling the blaze."</p>
<p>Should the worst happen to JPL, NASA has backup plans in place. Control of the Mars rovers and spacecraft will be shifted to the Goldstone Deep Space Network just north of Barstow, California. But generations of scientific data and hardware could be lost.</p>
<p>With the fires <a target="_blank" rel="nofollow" href="https://www.cnn.com/weather/live-news/los-angeles-pacific-palisades-eaton-wildfires-01-08-25/index.html">still largely unconfined</a>, and no sign of letup from the winds spreading the blazes, the loss of JPL is looking increasingly likely. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I had to take down my course-swapping site or be expelled (500 pts)]]></title>
            <link>https://www.linkedin.com/posts/jdkaim_github-jdkaimhuskyswap-huskyswap-project-activity-7282609173316415488-1jdb</link>
            <guid>42638626</guid>
            <pubDate>Wed, 08 Jan 2025 21:27:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.linkedin.com/posts/jdkaim_github-jdkaimhuskyswap-huskyswap-project-activity-7282609173316415488-1jdb">https://www.linkedin.com/posts/jdkaim_github-jdkaimhuskyswap-huskyswap-project-activity-7282609173316415488-1jdb</a>, See on <a href="https://news.ycombinator.com/item?id=42638626">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content" role="main">
      <section>
              <h2>                  JD Kaim’s Post</h2>

                
    

    
      

    <article data-activity-urn="urn:li:activity:7282609173316415488" data-featured-activity-urn="urn:li:activity:7282609173316415488" data-attributed-urn="urn:li:share:7282609172460793856">
<!---->
      
              
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    
    
    
    

    
    

<!---->  
        

      <div data-test-id="main-feed-activity-card__entity-lockup">
          <a href="https://www.linkedin.com/in/jdkaim?trk=public_post_feed-actor-image" data-tracking-control-name="public_post_feed-actor-image" data-tracking-will-navigate="">
            
      
  
<!---->          </a>
      <div>
        

            <p>
<!---->                CS @ UW | Prev @ GitHub
            </p>

            <p><span>
                <time>
                  

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    

      19h
  
<!---->                </time>
<!---->            </span>
                </p></div>

<!---->    </div>

      
            
            
  <p dir="ltr" data-test-id="main-feed-activity-card__commentary">Welp, looks I might be getting expelled from the University of Washington.
&nbsp;
As many of you know, last quarter I built an initial version of HuskySwap for a class project. It was a simple app designed to help students find partners to trade spots in critical classes after they filled up. I’m sure other people have pursued this kind of thing before, but there’s no go-to solution at UW and I wanted to contribute something cool for everyone.
&nbsp;
At first it was a fun project to catch up to the latest versions of .NET and Angular with some practice implementing features around database abstraction, real-time chat, and the full stack challenges associated with things like role-based permissions. I was probably going to build it anyway, so it was great that I could actually get course credit for CSE 403.
&nbsp;
I was really proud of the MVP and excited that the people I demoed it for actually wanted to use it. So I set aside some time in my winter quarter schedule to start hardening it so I could launch ahead of spring quarter registration.
&nbsp;
Then I came across the swagger docs for integrating with the registration system. The site says:
&nbsp;
“The Student Web Service gives your application access to information in the Student database such as course data, registration data, section data, person data, and term data (general academic data).”
&nbsp;
I got excited because it sounded like they were supportive of scenarios like mine. After all, UW aspires to be top ranked in both CS and business. Of course they would F/foster :P an entrepreneurial spirit!
&nbsp;
I reached out to request an access token. I wasn’t too ambitious up front; all I wanted was read-only access to automate the course catalog import instead of having to manually enter every class. But if things went well it would be awesome to automate the process further or even find other gaps I could fill to improve the community experience.
&nbsp;
Several hours later I received an unambiguous response: a “Notice of Violation of Registration Tampering Abuse Policy”. I was instructed to take down my demo site (and its handful of fake demo classes) or else they would begin the process that would culminate with my expulsion. I had originally reached out hoping to make the school experience better for everyone and they immediately went nuclear.
&nbsp;
I’m not too worried about the threat since taking down the demo site is no big deal. But I’m a little heartbroken to see how the school views me. I thought they’d be excited about what I was pursuing—or at least that I was pursuing anything—but instead they’d rather have me simply leave.
&nbsp;
I’m scheduled to graduate in a few months and am eager to move on to projects that don’t need to be cleared with the UW Registrar. If you know of anyone looking for a full-time software engineer with a knack for getting the attention of senior leadership, please send them my way! I can start full-time in June.
&nbsp;
Link to repo: <a href="https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FgXKEBtQi&amp;urlhash=Q6nu&amp;trk=public_post-text" target="_self" rel="nofollow" data-tracking-control-name="public_post-text" data-tracking-will-navigate="">https://lnkd.in/gXKEBtQi</a></p>



        

        
            
      
<!---->      
    
    
    

    
      <a href="https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fgithub%2Ecom%2FJDKaim%2FHuskySwap&amp;urlhash=IUnT&amp;trk=public_post_feed-article-content" target="_self" data-tracking-control-name="public_post_feed-article-content" data-tracking-will-navigate="" data-test-id="article-content">
        

      
<!---->      <section>
        <header>
            

            <h3 data-test-id="article-content__subtitle">
                github.com
            </h3>
        </header>
<!---->      </section>
    
      </a>
  
  
  
      
  
                  

      
          
        

      
          
        

      
            
      
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    

      
  
  
        

      
                
    
    
    

    
  
                      

      
              
            

    
    
    
    
    
    

<!---->
    

    
    

      
  
  
            

    
    
    
    
    
    

<!---->
    

    
    

      
  
  
            

    
    
    
    
    
    

<!---->
    

    
    

      
  
  
            

    
    
    
    
    
    

<!---->
    

    
    

      
  
  
            

    
    
    
    
    
    

<!---->
    

    
    

      
  
  
            

    
    
    
    
    
    

<!---->
    

    
    

      
  
  
            

    
    
    
    
    
    

<!---->
    

    
    

      
  
  
            

    
    
    
    
    
    

<!---->
    

    
    

      
  
  
            

    
    
    
    
    
    

<!---->
    

    
    

      
  
  
              <a href="https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fjdkaim_github-jdkaimhuskyswap-huskyswap-project-activity-7282609173316415488-1jdb&amp;trk=public_post_see-more-comments" data-test-id="main-feed-activity-card-with-comments__see-more-comments" data-tracking-control-name="public_post_see-more-comments" data-tracking-will-navigate="">
                See more comments
              </a>
      
        

      
              

    
    
    
    
    
    

    
  
        
    </article>
  
      
  
  
            </section>
      <section>

            <h2>
              Explore topics
            </h2>

<!---->
        
      </section>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Some programming language ideas (111 pts)]]></title>
            <link>https://jerf.org/iri/post/2025/programming_language_ideas/</link>
            <guid>42637304</guid>
            <pubDate>Wed, 08 Jan 2025 18:58:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jerf.org/iri/post/2025/programming_language_ideas/">https://jerf.org/iri/post/2025/programming_language_ideas/</a>, See on <a href="https://news.ycombinator.com/item?id=42637304">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Programming languages seem to have somewhat stagnated to me. A lot of
shuffling around ideas that already exist, not a lot of new ones.</p>
<p>This is not necessarily bad. Shuffling around ideas that already exist
is a natural part of refining them. Shuffling around ideas that
already exist is safer than a radical rewrite of existing
convention. Even taking a language that already exists and giving it a
new standard library can be worthwhile.</p>
<p>However, I occasionally have some ideas that may then trigger other
ideas in people, and in the interests of getting them out of my head,
I’m posting them here.</p>
<h2 id="disclaimers">Disclaimers</h2>
<p>None of these ideas are fleshed out to a specification level. Some of
these are little more than goals with no idea how to manifest
them in a real language.</p>
<p>While I mostly don’t know of languages that do these things, that
doesn’t mean I’m claiming they don’t exist, just that I don’t know of
them, because <em>of course</em> I am not intimately familiar with every
language that has every been written.</p>
<p>Some of these ideas are probably bad, even outright crazy. At least
one of them is something that I would put in the <em>known</em> bad category,
and the idea is mostly just “but what if someone could figure out a
way to fix it?”</p>
<p>Some of these are mutually contradictory and can’t live in one language.</p>
<p>In general, while I can’t control how people react to this list,
should this end up on, say, Hacker News, I’m looking more for replies
of the form “that’s interesting and it makes me think of this
other interesting idea” and less “that’s stupid and could never
work because X, Y, and Z so <a href="https://radio-weblogs.com/0107584/stories/2002/05/05/stopEnergyByDaveWiner.html">everyone stop talking about new
ideas</a>” or “why
hasn’t jerf heard of this other obscure language that tried that 30
years ago”. (Because, again, <em>of course</em> I don’t know everything that
has been tried.)</p>
<h2 id="loosen-up-the-functions">Loosen Up The Functions</h2>
<p>This idea comes from Erlang, though it doesn’t quite follow through on
it to the extent I’m talking about here.</p>
<p>A function call is a very strong primitive. There is no possibility a
function call can fail. This is so deeply ingrained into our
understanding of function calls that we can’t even see it. Note this
is not a matter of the “function returning an error” or
“throwing an exception”, this is
“the code reached for <code>strlen</code> and it <em>wasn’t there</em>”. Or in the case
of dynamic languages, not that it couldn’t “find” a particular
function but the code to <em>locate</em> it suddenly is gone.</p>
<p>The closest we get is “I ran out of memory trying to make this call”
and most of us, most of the time, just ignore that possibility.</p>
<p><a href="https://news.ycombinator.com/item?id=30831309">I wrote a comment on Hacker News a while back about how network RPC
failed in the 90s when it tried to pretend to <em>be</em> a native local
function</a>. It isn’t,
and it can’t pretend to be a native local function. RPC functions have
errors that can happen that a normal function can’t have. And it’s not
just a matter of returning an error value; sometimes the error is
“this call should have taken 250 nanoseconds but it froze for a minute
before timing out”. While it’s no problem for a computer to wait for a
minute, trying to build a program on operations that may take
“somewhere in this range of 9 orders of magnitude to execute” is not a
very useful primitive.</p>
<p>Well, what if you loosen up the concept of a function until all
function calls are “looser” and <em>can</em> express all the failure cases
that also arise with RPC? This would include things like:</p>
<ul>
<li>Pervasively include a concept of timing out for all functions.</li>
<li>Pervasively make functions easy to wait on or skip past; in “async”
terms, solve the problem by making <em>all</em> function calls async.</li>
</ul>
<p>And so on.</p>
<p>Then, once you’ve done that, you will find that it is annoying to call
a single function and do all the error handling you need to do, so you
make that easier, with maybe scoped error handling declarations or
some more defaults or something.</p>
<p>Then, RPC really <em>can</em> be as easy as calling a function in your
program, because you lowered what a function promises.</p>
<p>Erlang implements this to some degree in its various gen_* services,
but still has a conventional concept of function call overall.</p>
<p>The downside of this is that it is questionable whether a function
call’s simplicity can be recovered to the point that programmers are
willing to use this. Ultimately, <code>strlen</code> isn’t going to fail to be
available and I probably want to just wait for it regardless, and
somehow the syntax is going to need to make that reasonably easy, even
if it’s as simple as prefixing such calls with a symbol that means
“just pretend this is local” or perhaps embedding
“yeah, this is actually a local call” into the type system.</p>
<p>Or possibly it can all be left alone and the compiler can simply
optimize local function calls when possible, which is a lot of the time.</p>
<h2 id="capabilities">Capabilities</h2>
<p>This is not a new idea, so I won’t go deeply into what it is, and I
have been told some languages are playing with it, but it is something
I’d like to see more of.</p>
<p>There’s a language called E that tried to bring this about, but my
impression is that it was built on top of Java, and that’s probably
too great a change to try to build on Java. It really needs it own
language from top to bottom.</p>
<p>The time was probably not ripe. Consider this not a claim to novelty,
but the observation that <em>maybe</em> the time is right for this now.</p>
<p>A possible hazard here is trying to make capabilities <em>too</em> much,
e.g., also trying to write Rust-style mutation controls into them. Or
perhaps that would work like gangbusters, I dunno.</p>
<p>We keep trying to half-ass bodge capabilities on the side of existing
programs, and it honestly just keeps not working that well. Maybe
instead of writing conventional code and then trying to work out
exactly what capabilities that capability-oblivious code and
programmers ended up calling for, it’s time to put this into languages
themselves.</p>
<h2 id="production-level-releases">Production-Level Releases</h2>
<p>We’ve learned a lot about production-quality releases in the past
several years. Little of this has made it back into the
languages. This has probably been a good thing, because we’ve needed
the freedom to experiment, but I think it’s time for languages to
start embedding solution to these problems into themselves so we can
harvest the benefits of solutions.</p>
<p>We really ought to be able to:</p>
<ul>
<li>Have a fully standardized logging interface now, such that external
libraries can just provide configurable logging output.</li>
<li>Built-in metrics from day one, so third-party libraries can just
provide metrics gathering and processing.</li>
<li>Build in some sort of “request context” usable for request tracing
and stuff.</li>
</ul>
<p>This is an example of something that isn’t even a
“language feature”. It’s not like we need custom syntax constructs for
metrics. (Although if you’re going for a very “pure” language, maybe
some way of having some ability to ping metrics reliably without that
“counting” as impure would be helpful.) Just getting good-enough
versions of this stuff into the standard library would be enough.</p>
<p>There’s a number of these capabilities that are getting fairly mature
and could be lifted up into a new language directly, e.g. I think
“structured logging” has probably matured to this point.</p>
<p>The disadvantage of lifting into the language is that anything so
lifted becomes difficult to change once 1.0 hits. The advantage is
that the rest of the standard library and third party libraries can
integrate with it. It’s great that language X has 7 viable logging
libraries but it becomes more difficult for other libraries to build
on an assumption of what “logging” looks like.</p>
<p>This is an example of where a new language that is otherwise “just”
shuffling around older ideas could still get a leg up on its
competition. It takes a <em>lot</em> of maturity to write these interfaces,
though. For each of the things I mentioned, you really need to get a
lot of experienced devs together and make sure you’re pulling in the
best tested versions of the capability. This is, for better or worse,
not a place for some 19-year-old who has never worked with any of
these things to just splat out some off-the-cuff interface
specification that gets cast in stone. We already have that.</p>
<h2 id="semi-dynamic-language">Semi-Dynamic Language</h2>
<p>Many programmers like the convenience of dynamic languages. I spent
the first ~15 years of my career about 100% dynamic, and I’m not sure
I include myself in that set. Still, they’re pretty popular.</p>
<p>The problem is that they seem to be fundamentally slow. Some people
still discuss their performance as if it’s the year 2000 and maybe
someday we’ll get “sufficiently smart compilers”, but the reality is
that <em>immense</em> effort has been poured into speeding these things up
and it is no longer appropriate to “hope” for what may happen
someday. And the result has been… some success. Mixed success. You
can get dynamic languages to go faster, but it costs you a <em>lot</em> of
RAM and you still tend to cap out at 10x slower than C. It’s a lot of
work for a fairly marginal reward, worth it only because they’re so
popular that a 4x speedup multiplied across
“all the dynamic code in the world” is still well worth fighting for.</p>
<p>Alternatively, you can go the LuaJIT route and just hack out bits of
the language that don’t JIT well. But this seems to be only minimally
popular. Other than that it’s a good idea, though.</p>
<p>But the thing about “dynamicness” is that if you look, the vast, vast
majority of it takes place at startup, or at very defined times such
as “I’m loading in a new user plugin”. Almost no code is constantly
sitting there and dynamically modifying this and that as it runs. Yet
you <em>pay</em> for this dynamicness all the time. Every attribute lookup
needs to run a whole bunch of code to correctly look up an attribute,
in case someone has modified the lookup procedure since the last time
the value was looked up, or, in the case of a JIT, the JIT’s
procedures still need to be correct <em>as if</em> this can happen all the
time, which is inevitably slower than code that can’t do that.</p>
<p>What about a language where for any given bit of code, the dynamicness
is only a <em>phase of compilation</em>? The code can do whatever during
initialization, load database tables to dynamically construct classes
or whatever, but once it’s done, there’s a point where it locks down,
becomes nearly statically-typed (not necessarily fully, you could look
at this as an incremental typing situation), and being dynamic is no
longer possible?</p>
<p>I’m not sure what all this would look like. It’s a sketch of an
idea. Partially because I’m pretty satisfied in my own programming
world with static languages.</p>
<p>But if there was a phase where everything locked down, then a JIT
would have vastly more power to optimize the code safely. JITs have to
do so much work to deal with “well <em>what if</em> someone passes in
something really pathological to this function later?” and it seems
like they could go a lot faster if they could be rigidly guaranteed by
the types in the final compiled code that couldn’t happen.</p>
<p>You may also be able to create a sort of hybrid compile phase, where
the code is not “compiled”, but you can still run something like a
“check” that verifies the locked-down program is coherent according to
whatever rules the runtime or the user want to implement.</p>
<p>While I’m not aware of anything that works exactly like what I have in
mind, it is clearly a position on a well-explored continuum of
“exactly when does compilation happen?” and not some brand-new idea. I
reiterate that I’m not making a claim that any of this is brand
new. The Common Language Runtime’s ILR and subsequent compilation on a
target system is reasonably close to this, but focused on something
different. Some Lisps may be able to do all this, although I don’t
know if they quite do what I’m talking about here; I’m talking about
there being a very distinct point where the programmer says “OK, I’m
done being dynamic” for any given piece of code. Shader compilation
for video games may have a component of this, especially including the
ability to cache compilation outputs.</p>
<p>Another view on this idea is, “Isn’t it about time someone wrote a
dynamic scripting language that was designed from <em>day one</em> to be easy
to JIT?” What we have out in the world right now is either dynamic
scripting languages where the JITs came along literally a decade or
two after the language was created, and the JIT basically had to be
instantly 100% compatible with a language that was never designed for
JIT’ing right out of the gate to be even remotely useful, or we have
LuaJIT where an existing language got bits and pieces sliced out of
it, but we don’t have anything that I know of where the language was
designed from the start to be dynamic, but still easy to JIT.</p>
<p>While you’re at it, you’ll naturally also create a dynamic scripting
language that handles threading properly from the beginning, rather
than trying to retrofit it on to a decades-old code base. A dynamic
scripting language with perhaps a 2-3x slowdown over C (or, to put it
another way, basically the same speed as Go) that is also
natively capable of near-static-language threading speeds could raise
a lot of eyebrows.</p>
<h2 id="value-database">Value Database</h2>
<p>Smalltalk and another esoteric programming environment I used for a
while called Frontier had an idea of a persistent data store
environment. Basically, you could set <code>global.x = 1</code>, shut your
program down, and start it up again, and it would still be there.
And by that I mean, storing a value persistently was literally that
easy; no opening a file and dumping JSON and loading it later, no
fussing with SQLite and having to interact with a foreign SQL
interface (which, no matter how nice that may be, is not your
language’s native paradigm), none of that. Just
“set this value and keep it forever”.</p>
<p>This… is a superficially appealing but bad idea, unfortunately.
It’s on the list of Things I See People Angrily Claim Programming
Needs To Do To Level Up, right there along with Everything Should Be
Visual Programming and the recent “Low Code” burst that seems to have
died down again. It’s an entrant that doesn’t show up often, but I’ve
seen it enough that it’s on my list.</p>
<p>But it carries some significant disadvantages,
most notably that entropy tends to attack this shared store pretty
badly. The developer sets a value in their store, then sends the code
out to production, but whoops, it turns out the code absolutely
depends on that value being set and it fails everywhere else. It’s
takes a lot of work to set up a scenario in which twiddling a <em>run-time</em>
variable for debugging in your staging environment can propagate
straight into a bug on production because of accidental dependencies
on that value, but persistent stores are up for the challenge!</p>
<p>So, my own experience certainly attests to the fact that this is far
from a magic solution to all our problems.</p>
<p>However, I still wonder if this can’t be fetched from the dustbin of
history somehow, with some sort of better controls on what goes into
these stores. Base it on event streaming? Access controls? Some sort
of structural typing system that ensures that a “table” is of some
shape directly, before trying to use values of it and failing? Just
plain typing these things?</p>
<p>Because, my gosh, what a mess you could make on the one hand… but on
the other, I can’t tell you how nice it is to just say <code>myval.x = 5</code>
and it’s just <em>there</em> as <code>myval.x</code> tomorrow, with no queries, no
mappings, no ORMs, no files, no failures… just <em>boom</em>, there.</p>
<h2 id="a-truly-relational-language">A Truly Relational Language</h2>
<p>Although, on the note of “no fussing with SQLite”, how about a
language whose fundamental data type is a relational DB table?</p>
<p>You wouldn’t actually want SQL; you’d want to go back to relational
principles and build something that works as a programming language
rather than banging SQL together. The many, <em>many</em> technologies in the
world like LINQ in the .Net world or SQLAlchemy show at least a
possibility of what that would look like, although being able to sit
down at the language grammar level to integrate this even more deeply
opens up even more interesting possibilities than LINQ.</p>
<p>(Being able to emit SQL from the language for when you really do want
to talk to an SQL database is probably a good idea. This is harder
than it looks at first glance. You definitely want to study LINQ, and consider how you
allow the user to use things like <a href="https://dev.mysql.com/doc/refman/8.4/en/select.html"><code>SQL_NO_CACHE</code> or
<code>SQL_CALC_FOUND_ROWS</code></a>,
because you need to be <em>able</em> to do those sorts of things to SQL even
if you don’t need them in every query.)</p>
<p>Relational databases are clearly a tech that is here to stay, yet most
modern languages still treat them as an exotic thing to be dipped into
every once in a while at great cost. Maybe there’s a special “table”
data type as the data programmers have with their “data frames”, or
you get something nice like LINQ, but the language is still ultimately
either product or sum type data structures as its native
representation and there’s always this foreign conversion step to go
from the relational data to the “real” data.</p>
<p>You know you’d have something like this when you could query across
three different data types in your code, and get the results in the
form of some ad-hoc data type specifically for that query, which could
then be natively passed around and perhaps even have methods added to
it directly.</p>
<p>In the type theory world this is heavily related to <a href="https://www.reddit.com/r/haskell/comments/4f7fyn/what_are_row_types_exactly/">row
types</a>. I
am not aware of a language that uses them natively. (Although as is
often the case, if you squint hard enough at a dynamically-typed
language it can “look like” row types, but that’s again because by
punting on types entirely it can “look like” a lot of things, but in
the end if you violate the types you just get an exception thrown.)
Although there is more work to be done to make this idea work, row
types is just where I’d start. You’d still want to examine things like
“can I put methods on some sort of row type in such a way that the
method doesn’t care <em>how</em> the row type is constructed?”</p>
<p>That is, suppose you had a user ID and a username in one table, that
linked to an identity that contained their human name. By querying
across these two things you could end up with a User ID/Username/Human
Name tuple, even though that doesn’t literally exist as a data type in
your system… could you work out a way to put a method on this anyhow
that might do something like a debug dump of those three things? A
method on a datatype that never concretely exists as a declared data
type? There’s some interesting possibilities here.</p>
<p>(This also may harmonize with the JIT idea above. This would tend to
create a proliferation of possible types that some code somewhere
could use; conceivably even an infinite number of them depending on
how you implement it. Conventional generics-based precompilation may
not really be possible. But in practice there would be a finite and
generally relatively small set of those types actually used and a JIT
that could determine what those are and JIT them to native-ish speeds
could potentially recover a lot of performance out of this by not
compiling all of the myriad possible types that <em>could</em> have a
UserID/Username/HumanName in them with their static struct offsets
until the type is actually used.)</p>
<h2 id="a-language-to-encourage-modular-monoliths">A Language To Encourage Modular Monoliths</h2>
<p>The <a href="https://www.geeksforgeeks.org/what-is-a-modular-monolith/">modular
monolith</a>
has been a structure that has been flying under the radar lately, but
I feel like it’s coming up more and more often. Personally, everything
I write large enough to need architecture is now a modular monolith,
and I find it a fantastic way to program at at least medium scales. I
have to admit I have not yet tried it on a truly large project. My
guess it is that it should continue to scale, albeit possibly
requiring more discipline to maintain, but it doesn’t seem to be
gassing out in my own uses yet.</p>
<p>To have a modular monolith, you need to use dependency injection and
interfaces. You write as much code as possible in terms of “Hey, this
is what I need; I need a way to turn DNS addresses into IP address,
and I need a way to turn email addresses into user accounts, and I
need this and that and the other thing”, and then you construct each
component of the modular monolith by providing each of them components
with all the services it needs.</p>
<p>I think “modular monolith” is arguably what should be the “default”
architecture for any non-trivial project.</p>
<p>However, modern languages tend to fight you on that.</p>
<p>Static languages require extensive declaration of interfaces of some
sort to do this, and as such, it requires much more discipline than
hard-wiring together everything with concrete types. As such, in real
code, lots of things end up hardwired together just due to the sheer
hassle of interfaces, even in the languages where they are the easiest.</p>
<p>Dynamic languages are nominally easier, once again because they pretty
much punt on everything, but the trade off is no compile-time guarantee
that all the services you are getting passed actually do the things
you want them to do. In practice this becomes scarier and scarier to
do as you scale up, because now every time you call a new method on
some passed-in parameter you are changing the interface for all things
passed in to that method, and there is effectively no way to notify
the callers of that method. After all this may even be a library and
you may have no human connection whatsoever to the caller.</p>
<p>I’d be interested in something that strikes a middle ground; a static
language with compile time guarantees, but one where <em>all</em> function
parameters are <em>automatically</em> interfaces, even if they are given an
“exemplar” type in their type signature. If I declare something as a
“string”, and what I do with that string is concatenate it with
another string and iterate on Unicode codepoints, what if the compiler
just automatically was able to take anything that could “concatenate
itself to a string” and “iterate on Unicode codepoints” and accept it
by treating it as if there was an interface declaration right there already?</p>
<p>(It would be interesting to see if you can get type inference to the
point that “exemplar types” are no longer necessary but working out if
that is the case is well beyond the level of work I’m doing here.)</p>
<p>Every parameter coming in to a function could have an interface
automatically extracted out of it just by what the user does to that
value. Integration through a language server could do something like
extract that interface out automatically. I don’t know if it should be
implicit and checked at compile time, or if you might want to do
something like “on save, automatically reify all interfaces into
actual declarations the human can see”. If it is left implicit we
definitely want the language server to have a command that returns
“this is the actual interface for this parameter”.</p>
<p>(On that note, not worth its own section, but I think that there’s a
lot of interesting “write a static language that assumes you’re
writing it with the language server and provides very rich querying
capabilities” like that that could be done. You see a lot of good
ideas in the best IDEs for that sort of thing but the ideas always end
up detached from the languages and eventually stranded when the IDE
line comes to an end. Collecting those capabilities up into the
language project itself and integrating the language serve right into
the design of the language at all phases should probably have some
interesting effects.)</p>
<p>I think I’d want to see something like the Python module system, where
technically, libraries themselves are objects, which means that entire
libraries could be swapped out by providing a different one.</p>
<p>You could in principle merge this with another interesting idea, which
is more extensive use of <a href="https://www.geeksforgeeks.org/static-and-dynamic-scoping/">dynamic
scopes</a> to
do something like provide a built-in service registry of things that
look like a fancy dependency-injection library, so some code can do
something like “fork my current registry, change the UserProvider to
this other object, and run this test code”. Or change the definition
of a “transaction”. Or whatever.</p>
<p>In theory, if you successfully made sure there aren’t any back doors
to this system (like “primitive types like ints are just ints and they
can’t be shimmed”), this would almost automatically make <em>any</em> system
written in it a modular monolith. Of course, it might be a super
<em>messy</em> modular monolith, but in principle any function in the system,
even though the whole thing is statically typed, could be executed in
such a way that everything it depends on, regardless of whether it was
written for being swappable, is swappable, with enough work. You could
do things like have literally <em>any</em> code that reads &amp; writes to a
filesystem be executed in a context that provides a fake file system
for testing, without the code itself having to do any explicit
declaration of that fact.</p>
<p>You’d want to block truly global variables entirely, although if you
combine them with the dynamic scope idea, you can put things in the
dynamic scope for similar uses.</p>
<p>There’s probably also some interesting synergies with structured
concurrency, and having these dynamic scopes attached to execution
contexts. Make sure your dynamic scopes also have the capabilities
that the Go contexts do and you’d end up with some interesting
possibilities.</p>
<h2 id="modular-linting">Modular Linting</h2>
<p>This is another place where I absolutely make no claims about what may
be happening in the many dozens of language communities in the
world. I’m just making an observation from one of the ones I’m deeply
into and suggesting it’s a good pattern, not that it’s the only place
it happens. Plus I’m going to say it isn’t happening enough anyhow
even where it is happening.</p>
<p>With that throat clearing, the Go world happened to end up with a lot
of various linters over the years. Eventually they were combined into
a project called
<a href="https://golangci-lint.run/usage/linters/">golangci-lint</a>, which has
become the <em>de facto</em> linter for the community. I linked to the list
of linters built in so you can see what’s in there.</p>
<p>What’s interesting about golangci-lint, though, is that the various
linters are largely independent from each other, each independent
projects written by various developers to scratch a particular
itch. They were later merged together technically, but are in
principle still just a big pile of community linters with a nice
modular interface on top.</p>
<p>This isn’t about the language at all, but it would be interesting for
a language <em>project</em> to reify that. golangci-lint eventually shared an
AST view among a lot of its linters; a project could copy that idea
and write it in early. Let linters be fully modular, perhaps even by
mentioning them by github project through a fully standardized
interface that doesn’t require them to even be “integrated” into a
single executable from a third-party project.</p>
<p>What’s neat about this approach is that if it was made a part of the
language design process, a lot of things that aren’t necessarily
<em>important</em> could be kicked to optional linting. For instance, Go was
especially a bit notorious when it first came out for mandating that
all imported packages were used, and all declared variables were
used. Complaints about that over the years have quieted down, but
that’s a good example of something that could have been taken out of
the compiler and shuffled off into a linter provided by the main
project.</p>
<p>There’s definitely some downsides too… you end up with “dialects” of
the language, but, the truth is, you end up with that
anyhow. Generally developers learn pretty quickly not to fire their
own bespoke linting configuration at other people’s libraries.</p>
<p>But it would be interesting to see how much could be kicked out to
linters, like, do you want to insist that all values of an enumeration
(whether a classic int or the later trend towards using that term for
what I think of as “sum types”) are checked in switch statements or
not? Do you want to validate your printf parameters are correct?
Do you want a linter flagging every time you use an external program
to pass your arguments through some check routine? Perhaps things like
HTML template libraries could even ship with their own linters to flag
suspicious constructs for injection, formally as part of the library.</p>
<p>This pairs in an interesting way with the parenthetical about leaning
on a Language Server more; taking the Language Server as a core part
of the project makes the language task bigger, but allowing for
community linters and kicking non-essential aspects of the language
out to the linters shrinks what the core of the language project has
to worry about.</p>
<p>This is also one of the only ideas in this list that doesn’t need to
be in the language from the very, very beginning, and could be added
either by a young language design team or even just a motivated
external developer to an existing project. Things like Python or C#
have too much inertia for a dedicated dev, but you might be able to
get the momentum in something still young like Nim or Zig.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Facebook is removing stories about pornographic ads (245 pts)]]></title>
            <link>https://www.404media.co/facebook-is-censoring-404-media-stories-about-facebooks-censorship/</link>
            <guid>42637267</guid>
            <pubDate>Wed, 08 Jan 2025 18:55:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/facebook-is-censoring-404-media-stories-about-facebooks-censorship/">https://www.404media.co/facebook-is-censoring-404-media-stories-about-facebooks-censorship/</a>, See on <a href="https://news.ycombinator.com/item?id=42637267">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
<!--kg-card-begin: html-->

<!--kg-card-end: html-->
<p>In early December I got the kind of tip we’ve been getting a lot over the past year. A reader had noticed a post from someone on Reddit complaining about a very graphic sexual ad appearing in their Instagram Reels. I’ve seen a lot of ads for scams or shady dating sites recently, and some of them were pretty suggestive, to put it mildly, but the ad the person on Reddit complained about was straight up a close up image of a vagina.&nbsp;</p><p>The reader who tipped 404 Media did exactly what I would have done, which is look up the advertiser in Facebook’s Ad Library, and found that the <a href="https://www.facebook.com/ads/library/?active_status=all&amp;ad_type=all&amp;country=ALL&amp;is_targeted_country=false&amp;media_type=all&amp;search_type=page&amp;view_all_page_id=450572341476695"><u>same advertiser</u></a> was running around 800 ads across all of Meta’s platforms in November, the vast majority of which are just different close-up images of vaginas. When clicked, the ad takes users to a variety of sites for "confidential dating” or “hot dates” in your area. Facebook started to remove some of these ads on December 13, but at the time of writing, most of them were still undetected by its moderators according to the Ad Library.</p><p>Like I said, we get a lot of tips like this these days. We get so many, in fact, that we don’t write stories about them unless there’s something novel or that our readers need to know about them. Facebook taking money to put explicit porn in its ads despite it being a clear violation of its own policies is not new, but definitely a new low for the company and a clear indicator of <a href="https://www.404media.co/has-facebook-stopped-trying/"><u>Facebook’s “fuck it” approach</u></a> to content moderation, and moderation of its ads specifically.&nbsp;&nbsp;&nbsp;</p>
</div><div>
  <div>
    <h2>This post is for paid members only</h2>
    <p>Become a paid member for unlimited ad-free access to articles, bonus podcast content, and more.</p>
    <p><a href="https://www.404media.co/membership/">Subscribe</a>
  </p></div>
  <div>
    <h2>Sign up for free access to this post</h2>
    <p>Free members get access to posts like this one along with an email round-up of our week's stories.</p>
    <p><a href="https://www.404media.co/signup/">Subscribe</a>
  </p></div>
  <p>Already have an account? <a href="https://www.404media.co/signin/" data-portal="signin">Sign in</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NeuralSVG: An Implicit Representation for Text-to-Vector Generation (421 pts)]]></title>
            <link>https://sagipolaczek.github.io/NeuralSVG/</link>
            <guid>42636873</guid>
            <pubDate>Wed, 08 Jan 2025 18:16:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sagipolaczek.github.io/NeuralSVG/">https://sagipolaczek.github.io/NeuralSVG/</a>, See on <a href="https://news.ycombinator.com/item?id=42636873">Hacker News</a></p>
<div id="readability-page-1" class="page">




<div>
                    

                    
                                        
                    <p><span>1 - Tel Aviv University</span>
                    </p>
                    <p><span>2 - MIT CSAIL</span>
                    </p>
                                                    
                    

                </div>

<section>
    <!-- <div class="hero-body"> -->
    <div>
                            <p><img src="https://sagipolaczek.github.io/NeuralSVG/static/figures_nsvg/teaser-1.png" alt="NeTI"></p><h2>
                                NeuralSVG generates vector graphics from text prompts with ordered and editable shapes. 
                                Our method supports dynamic conditioning, such as background color, which facilitating the generation 
                                of multiple color palettes for a single learned representation.
                            </h2>
                        </div>

    <div>
                    <!-- div class="item">
                      <p style="margin-bottom: 30px">
                    <video poster="" id="tree" autoplay controls muted loop height="100%">
                      <source src="static/figures/video.mp4"
                      type="video/mp4">
                    </video>
                    </p>
                    </div -->
                    <h2>Abstract</h2>
                    <p>
                            Vector graphics are essential in design, providing artists with a versatile medium for creating resolution-independent
                             and highly editable visual content. Recent advancements in vision-language and diffusion models have fueled interest 
                             in text-to-vector graphics generation. However, existing approaches often suffer from over-parameterized outputs or 
                             treat the layered structure --- a core feature of vector graphics --- as a secondary goal, diminishing their practical 
                             use. Recognizing the importance of layered SVG representations, we propose NeuralSVG, an implicit neural representation
                              for generating vector graphics from text prompts. Inspired by Neural Radiance Fields (NeRFs), NeuralSVG encodes the 
                              entire scene into the weights of a small MLP network, optimized using Score Distillation Sampling (SDS). To encourage 
                              a layered structure in the generated SVG, we introduce a dropout-based regularization technique that strengthens the 
                              standalone meaning of each shape. We additionally demonstrate that utilizing a neural representation provides an added 
                              benefit of inference-time control, enabling users to dynamically adapt the generated SVG based on user-provided inputs, 
                              all with a single learned representation. Through extensive qualitative and quantitative evaluations, we demonstrate that 
                              NeuralSVG outperforms existing methods in generating structured and flexible SVG.
                        </p>
                </div>

    <!-- <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title has-text-centered">Video</h2>
          <center>
            <iframe width="630" height="354" src="" title="YouTube video player" frameborder="0" 
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
            allowfullscreen></iframe>
          </center>
        </div>
      </div>
    </section> -->

    <div>
                <h2>Examples of Text-to-Vector Generation <br> with NeuralSVG
                </h2>
                
            </div>

    <div>
                    <h2>How Does it Work?</h2>
                    
                    <p><img src="https://sagipolaczek.github.io/NeuralSVG/static/figures_nsvg/method.png" alt="neuralsvg">
                    </p>
                    <ul>
                      <li> We learn an implicit neural representation for generating vector graphics from text prompts. We encode the SVG into the weights of a small MLP network, optimized using an Score Distillation Sampling (SDS).
                      </li>
                      <br>
                      <li> To promote an ordered representation, we use a dropout-based technique to encourages each learned shape to have a meaningful and ordered role in the overall scene.
                      </li>
                      <br>
                      <li> Our neural representation enables inference-time control over the generated asset such as dynamically adjusting the color palette or aspect ratio of the generated SVG, all with a single learned representation.
                      </li>
                      <br>
                    </ul>
                </div>

    <div>
            <div>
                    <h2>What Can it Do?</h2>
                    
                    <h3>Encouraging Ordered Representation with Nested Dropout</h3>
                    <p>
                        We show results generated by our method when keeping a varying number of learned shapes in the final rendering. Even with a small number of shapes, our approach effectively captures the coarse structure of the scene.
                    </p>
                </div>
            

            <div>
                  
                  <h3>Color Palette Control</h3>
                  <p> 
                    Given a learned representation, we render the result using different background colors specified by the user, resulting in varying color palettes in the resulting SVGs. The upper row shows colors observed during training while the bottom row shows unobserved (generalized) colors.
                  </p>
              </div>
          

          <div>
                
                <h3>Aspect Ratio Control</h3>
                <p>
                    We present results from optimizing NeuralSVG with aspect ratios of 1:1 and 4:1. In each pair, the left image displays the generated SVG with a 1:1 aspect ratio (square format), while the right image shows the model's output with a 4:1 aspect ratio.
                </p>
            </div>
        

          <div>
                
                <h3>Sketch Generation</h3>
                <p>
                    NeuralSVG generates sketches with varying numbers of strokes using a single network, without any modifications to the framework.
                </p>
            </div>
        

        </div>


    



</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bringing SerenityOS to real hardware, one driver at a time (204 pts)]]></title>
            <link>https://sdomi.pl/weblog/23-serenityos-realhw/</link>
            <guid>42636086</guid>
            <pubDate>Wed, 08 Jan 2025 16:54:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sdomi.pl/weblog/23-serenityos-realhw/">https://sdomi.pl/weblog/23-serenityos-realhw/</a>, See on <a href="https://news.ycombinator.com/item?id=42636086">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h3>Bringing SerenityOS to real hardware, one driver at a time</h3>
<p><img src="https://sdomi.pl/weblog/23-serenityos-realhw/img/cover.png" alt="cover image. the serenityos logo is in the middle, with more colorful squiggly lines around it. the whole image is purposefully pixelated, and the palette used has maybe 8 colors tops.">
<small>img by <a href="https://mastodon.catgirl.cloud/@multisn8">Multisn8</a>, derivative of <a href="https://github.com/SerenityOS/artwork/blob/master/images/ladyball_vector/shaded.svg">the ladyball logo</a> by frhun. Licensed under CC BY-SA 4.0</small></p><hr>

<p>Many moons ago, around the time when Andreas formally resigned from being Serenity's BDFL, I decided that I want to get involved in the project more seriously. Looking at it from a perspective of "what do I <i>not</i> like about this (codebase)", the first thing that came to mind was that it runs HERE <b>points at QEMU</b> and not THERE <b>points at real hardware</b>. Obvious oversight, let's fix it.</p>

<hr>

<p>First thing I looked at was UEFI support, but there's an ongoing effort by spholz which I didn't want to hijack. For my purposes, kernel from the master branch successfully boots from GRUB running on a TianoCore UEFI runtime, so this is <i>fine</i>.</p>

<p>Debugging the OS on the same machine as your main isn't fun, and I wanted to focus my efforts on porting it to something I'd actually want to daily drive - which limited me to recent-ish hardware. I went onto Allegro (polish online auction site) and searched for the cheapest Chromebook I could find. After discarding the literal <i>cheapest</i> option (due to it being a fire hazard), I ended up with a Dell 3100 (baseboard Octopus, model Fleex) for a whopping 95PLN (~25EUR). The specs are as follows:</p>

<ul>
	<li>Intel Celeron N4020 (2 cores, no HT)</li>
	<li>4GB of DDR4</li>
	<li>32GB of eMMC on-board storage</li>
	<li>1366x768 TN screen driven by the UHD600 IGP</li>
	<li>2x USB-A, 2x USB-C, 3.5mm jack</li>
	<li>a keyboard that's somehow better than Dell's top-end business laptops</li>
</ul>
<p>(<i>note: starting here, i'll refer to this laptop simply as <span>octopus</span>, lowercase. it's a cute hostname</i>)</p>

<p>I had a few reasons for picking a Chromebook. The most notable one is that Cr50, the security chip / embedded controller, gives literal superpowers with regards to closed-case debugging <small>(foreshadowing...)</small>. Chromebooks are also plentiful and cheap, so not only is it easy to replace mine if I break it, it's also an easily obtainable target for others.</p>

<h3>cros_ec woes</h3>

<div><p><img src="https://sdomi.pl/img/haruhi.jpg"></p><h3>Haruhi says...</h3><p><span>"This post goes off-the-rails quite quickly. Next section isn't about Serenity directly, rather about prep work and cursed hardware hacks that ensued.<p>Feel free to <a href="#serenity">skip</a> ahead if this doesn't interest you...<br>But we all know this is exactly what you came here for. Strap on."</p></span></p></div>
<p>For the uninitiated, almost all chromebooks from ~2018 onwards have one USB-C port that, with a SuzyQ cable, can be used for debugging. Cr50 exposes three ttyUSB devices:</p>

<ul>
	<li>internal Cr50 console</li>
	<li>AP console (aka: chromebook's serial port)</li>
	<li>cros_ec console (aka: the embedded controller)</li>
</ul>
<p>The general idea was to use this to get easy, portable access to the serial console without needing to keep the laptop open with wires hanging off it. cros_ec can also do some cool stuff such as emulating keypresses and controlling the power state, so a "KVM" solution wasn't out of the question.</p>

<p>Unfortunately, reality had different plans:</p>

<p><img src="https://sdomi.pl/weblog/23-serenityos-realhw/img/1.png" alt="IM screenshot. I'm talking with Alicja, who tells me that fleex, my laptop, is missing some resistors for SuzyQ. I'm responding with a mix of bargaining, sadness and anger.">
	<small>Picture 1 - my disappointment was immense and my day was ruined</small>
</p>

<p>I made a new SuzyQ cable to check if my old one broke. I triple-checked it with another chromebook to verify that it's really soldered correctly. Then I went to ask others, and I learned that octopus is one of the few laptops where Cr50 <i>doesn't</i> work for CCD; Dell cheaped out and didn't include a few resistors on the board. Some have reported limited success, but only under specific constraints ("debug works only in the left port, upside down, and the right port has to have a charger connected"), but I wasn't able to get this working <i>at all</i>. I shelved the project for the rest of the day out of pure disappointment.</p>

<h3>Alternatives</h3>

<p>Well, I already have the device, and I kinda started to like it.<br>
Can I do <b>anything</b> to make it useful?</p>

<p><img src="https://sdomi.pl/weblog/23-serenityos-realhw/img/2.jpg">
<small>Picture 2 - lots of test pads. sadly, none of them relevant</small></p><p>I spent some time poking around the Nuvoton and H1B2C chips (first one implements the EC, second one the Cr50 debug bridge). As far as I understood the scraps of knowledge around the web, bridging <i>something</i> here would make it talk over USB, but I didn't figure anything out. At a few points octopus stopped turning on, but thankfully there was no permanent damage - this laptop is a heavy, stable cookie. Of note: my current sources tell me that the missing resistors are only supposed to affect SPI flashing, not the USB bridge itself - therefore it is unknown why I couldn't (and still can't) get Cr50 debug to work at all.</p>

<hr>

<p>Out of options and desperate for answers, <strike>I booked a flight to Japan</strike> I checked whether a generic Pi Pico board could fit inside the empty space inside the laptop. And, yes, quite easily!</p>

<p><img src="https://sdomi.pl/weblog/23-serenityos-realhw/img/3.jpg" alt="Pi Pico board laid down on top of another PCB in the case"><img src="https://sdomi.pl/weblog/23-serenityos-realhw/img/4.jpg" alt="Same thing, but the lower PCB is chomped off. Pi Pico now lays flush.">
<small>Pictures 3, 4 - this would be a perfect place if not for that daughterboard... <b>CHOMP</b></small></p><p>I found some schematics for similar laptops online, but none for <i>exactly</i> octopus. My board had two big debug ports - one of them turned out to be some Intel crap, containing an underdocumented JTAG and a bunch of test points irrelevant for my purposes. The other one was Google Servo, which is equally hard to search for due to name reuse - Google churned out a bunch of different debug probes under the name "Servo", gave them really confusing names and published little to no docs about them, beyond <a href="https://chromium.googlesource.com/chromiumos/third_party/hdctools/+/HEAD/docs/servo.md">one docs page</a>.</p>

<p><img src="https://sdomi.pl/weblog/23-serenityos-realhw/img/5.jpg" alt="Screenshot of a technical drawing of the Google Servo connector">
<small>Picture 5 - presumed pinout, found in some leaked schematics</small></p><p>In the end, I took my <a href="https://glasgow-embedded.org/">Glasgow</a>, launched the UART applet with frequency detection, and started probing. The procedure was literally to slowly tap a male end of a dupont-style cable to the suspected UART TX, while Linux was sending a bunch of <span>meow</span>s in a loop to <span>/dev/ttyS1</span>.</p>

<p><img src="https://sdomi.pl/weblog/23-serenityos-realhw/img/6.jpg" alt="board from octopus is laid out belly-up, still connected to power and a wifi antenna">
<small>Picture 6 - the 4W TDP means that not only is this Celeron passively cooled - it barely gets warm without a heatsink. In this picture, octopus is turned on, and it's still just 37°C CPU die</small></p><p>Finding the proper TX pad took me maybe a few minutes of probing. Doing the same for RX was a bit tougher: you need to be actively transmitting, and if you hit the wrong line, you're at risk of resetting the board. Exactly that has happened two times before I found the pin - luckily, no permanent harm was done.</p>

<p><img src="https://sdomi.pl/weblog/23-serenityos-realhw/img/7.png" alt="IM screenshot. I pasted a terminal screenshot of my contraption communicating through serial. I labelled the picture 'Bonjour'">
	<small>Picture 7 - Top terminal is on a Pi Pico, bottom terminal is octopus' serial port over SSH</small>
</p>

<p>Hooray, we have both pins! I looked for two more RX/TX pins for the EC; Surprisingly, through cross-referencing pinouts I found with what I already soldered onto, it took me maybe ten minutes to find those, too. Now, onto soldering:</p>

<p><img src="https://sdomi.pl/weblog/23-serenityos-realhw/img/8.jpg" alt="some shoddy soldering to the servo port. 5 cables in total, 4 red, one blue.">
<small>Picture 8 - after seeing this, WeirdTreeThing was flabbergasted that I was able to solder to a pitch this small</small></p><p>Thankfully, I didn't lift any pads! I used some epoxy to secure all the wires so they wouldn't lift; deciding against kapton tape or hot glue was instrumental for longevity of those connections, and I'm happy to report that in the past 6 months I haven't had a single problem with those hacky connections.</p>

<p><img src="https://sdomi.pl/weblog/23-serenityos-realhw/img/9.jpg" alt="more shoddy soldering, this time to a flash chip on the opposite side of the board. all the cables are nicely attached to the board with epoxy">
<small>Picture 9 - UV-cure epoxy changed my life. Also visible: Bringus-inspired<br>method of asserting dominance in the Write-Protect realm: Just Bridge It(tm)</small></p><p>RP2040 can also use SPI peripherals, and comparing to the Servo connector, soldering 6 cables to the flash chip was a walk in the park. I did a small hack with cutting the trace going to the write-protect pin and bridging it to ground. This gave me full write access without needing to ask the Cr50 security chip for permission.</p>

<p><img src="https://sdomi.pl/weblog/23-serenityos-realhw/img/10.jpg" alt="rat's nest of cables. oh god what happened here">
<small>Picture 10 - it's not the prettiest, but who's checking</small>

<img src="https://sdomi.pl/weblog/23-serenityos-realhw/img/11.jpg" alt="nice end result, showcasing an additional cable now sticking out of the laptop">
<small>Picture 11 - end result; I cut an additional hole in the side of the case</small></p><p>On the software side of things, I chose CircuitPython (because it exposes USB mass storage for script/data upload). Bridging an UART to the cdc_acm usb device was trivial, but since I hooked up the SPI flash too, it would be neat to be able to flash it, too - and that's more complicated.</p>

<p>In the OSS realm, EEPROM flashing (especially SPI) is usually done through <a href="https://flashrom.org/">flashrom</a>. It supports quite a bunch of different flashing methods; The one most relevant to me was <a href="https://www.flashrom.org/supported_hw/supported_prog/serprog/overview.html">serprog</a>, which is a light protocol to proxy SPI over UART. There already was an <a href="https://github.com/stacksmashing/pico-serprog">impl for the Pico</a> written by stacksmashing in C, but using that one would require me to reflash the pico every time I wanted to flash the BIOS...</p>

<p>So instead I wrote an implementation in CircuitPython. I based it heavily on <a href="https://github.com/GlasgowEmbedded/glasgow/blob/main/software/glasgow/applet/interface/spi_flashrom/__init__.py">the serprog Glasgow applet</a>, because even though it's Amaranth, it was the clearest, easiest to understand implementation I found - and parts of it I could copy 1:1.</p>

<p><img src="https://sdomi.pl/weblog/23-serenityos-realhw/img/12.png" alt="terminal screenshot. i'm running flashrom -p serprog:dev=/dev/ttyACM1; the output says that Programmer name is PicoCCD, meow~!"></p><hr>
<p><img src="https://sdomi.pl/weblog/23-serenityos-realhw/img/13.png" alt="terminal screenshot. first successful read from serprog">
<small>Pictures 12, 13 - first signs of life, and first successful read</small>Sometime after the 1st flash, I randomly noticed how the datasheet said (in very big letters) that my chip was 1.8V. I... somehow never noticed that, and used 3.3V for it. Regardless, it was already soldered, glued, closed in the case... The mistake lives to this day (and hasn't caused any harm... so far).</p>

<p><img src="https://sdomi.pl/weblog/23-serenityos-realhw/img/14.png" alt="IM screenshot. i pasted a screenshot from the datasheet, which clearly says that my SPI flash chip is 1.8V. I comment 'hey did you know this also works with 3.3?'">
	<small>Picture 14 - Oops?</small>
</p>

<p>All of my code is consolidated into PicoCCD, a quick-and-dirty closed-case-debug solution. The <a href="https://git.sakamoto.pl/domi/PicoCCD">repo is on my Forgejo</a>.</p>

<p><img src="https://sdomi.pl/weblog/23-serenityos-realhw/img/lell.jpg" alt="a 'lell' laptop, booting modified coreboot with a blobcat in the middle">
<small>Picture 15 - <a href="https://donotsta.re/notice/Aj479A11wZbCQnp452">"i don't need anything else here, it's already perfect"</a></small></p><h3 id="serenity">Finally, time for SerenityOS</h3>

<p>For debugging purposes, I set up Alpine Linux with a bunch of basic utilities for fetching an externally-built SerenityOS kernel. Over time, this grew into a bigger contraption, which notably includes a GRUB entry that automatically downloads artifacts from my build machine, decompresses and overwrites the Kernel, and unpacks a .tar with the userspace. At the time of writing, iteration time (between making a change and testing a change) is around 20 seconds, which is quite respectable for hacking on baremetal.</p>

<p>Anyhoo, first test run: I prepared a GRUB boot entry (that essentially boiled down to <span>multiboot /Kernel serial_debug</span>), eagerly waited for it to show something, and... nothing. Neither on the screen, nor on the serial port. What gives?</p>

<p>Lack of any output on the screen lead me to some StackExchange post which suggested adding <span>insmod all_video</span> to the boot entry. It didn't fix anything by itself (some kernel fixes were still necessary), but it was the right call anyways. Moving onto serial, the lack of output was more worrying - especially since I was getting logs from coreboot just a few seconds prior.</p>

<h3>A primer on 16550 UART</h3>

<p><img src="https://sdomi.pl/weblog/23-serenityos-realhw/img/15.jpg" alt="screencap from a Cathode Ray Dude video. Gravis is sitting on the right of the frame, the left is occupied by text immitating a dictionary definition. Serial Port - a hole into which you can throw bytes, and they come out somewhere else. thats it - abraham lincoln">
<small>Picture 16 - "sometimes they don't come out and it makes me sad" ~ irth<br>(screencap from a <a href="https://youtube.com/watch?v=7mqkTFq7Ekg">CRD video on dialup VoIP</a>, licensed under CC-BY. greetz to Gravis)</small></p><p>What we call "serial", in reality can be anything from classic RS-232, through TTL UART (RS-232, but lower voltages), or in rare cases, some wholly unrelated low pin-count bus (USB is serial, technically). It has to transmit data in series (one bit at a time), as opposed to in parallel (<i>at least</i> 2 bits at a time, in most cases 8 or more) - but even that requirement is blurry. Originally, RS-232 also transmitted some metadata (hardware flow control, etc), but today it's <i>really</i> rare to see UART with more than two wires and ground.</p>

<p>From the perspective of a user, the lowest common denominator is that UART transmits data, and is usually quite slow (although even this isn't always true, see HSUART). On all but the lowest layer, the bit-by-bit nature is abstracted away, and one just sees <b>bytes</b> passing through - not bits. Thus, even the "serial" part becomes irrelevant.</p>

<p>Similarly, from the programmer's perspective, serial is just a place to read and write bytes from. Even on the lowest abstraction layer, in virtually all cases you <b>will send bytes</b>. The only defining quirk of the standard is stripped away as soon as possible.</p>

<p>But there's a missing link: we need something to convert those bytes into a series of bits, and then into actual electrical signals. Ever since the first IBM PC, this was the job of a serial interface chip - historically, either an <a href="https://en.wikipedia.org/wiki/8250_UART">8250</a> or the newer drop-in replacement, <a href="https://en.wikipedia.org/wiki/16550_UART">16550</a>. Nowadays, the serial controller is tightly integrated into the chipset, but virtually all of them still use the same register map as a 40+ year old chip would. Thanks to this, as vague as it may be, serial is one of the very few standard interfaces which are present on <i>almost</i> all PCs in some shape or form.</p>

<h3>16550: The interface</h3>

<p>Historically, all external devices on the IBM PC were port-mapped to the x86 CPU. Thus, to interface with them, one would use a special instruction like <span>outb</span> for writes or <span>inb</span> for reads. This Was Widely Regarded As A Bad Move, because it made things <i>slow as molasses</i>. Then, MMIO (Memory-Mapped IO) came, and with it, hacks like <a href="https://en.wikipedia.org/wiki/Direct_memory_access">DMA</a> became possible - and really fast in comparison. A lot of devices moved onto MMIO for the faster speeds and lower CPU overhead.</p>

<p>But serial generally didn't. By the time DMA became viable on consumer-grade machines, serial didn't need to compete to be high-speed; It got superseded by faster parallel ports, USB, and briefly - SCSI. This put it in a great place for being a generic debug port, because as long as one can physically access the RX/TX pins, executing <span>outb 0x3f8, 0x41</span> will get yield an <span>A</span> on the other side. The initialization sequence is around 5 instructions, and it's trivial to implement even in small projects.</p>

<p>... except, as I wrote: "generally". A lot of sources online insist that 16550-compatibles were always port-mapped, but my laptop here begs to differ:</p>



<p><code>domi@octopus:~$ doas dmesg | grep 16550
[    0.501954] Serial: 8250/16550 driver, 4 ports, IRQ sharing enabled
[    4.278345] dw-apb-uart.5: ttyS0 at MMIO 0x9112f000 (irq = 4, base_baud = 115200) is a 16550A
[    4.299158] dw-apb-uart.6: ttyS1 at MMIO 0x180000000 (irq = 6, base_baud = 115200) is a 16550A
</code>
<small>Figure 1 - Linux kernel logs, showing MMIO and some addresses</small></p><p>Digging further, I found out that the whole thing is provided by a PCI device:</p>



<p><code>domi@octopus:~$ lspci | grep UART
00:18.0 Signal processing controller: Intel Corporation Celeron/Pentium Silver Processor Serial IO UART Host Controller (rev 06)
00:18.2 Signal processing controller: Intel Corporation Celeron/Pentium Silver Processor Serial IO UART Host Controller (rev 06)
</code>
<small>Figure 2 - lspci output</small></p><p>This is <i>bad</i>, because PCI is infamously hard to setup. Serenity has a PCI bus implementation, but we're so early in the boot process that it's a no-go. Even if it wasn't, our generic PCISerialDevice so far hasn't been used in a MMIO context; Hacking up a driver for this without any debug output would be less than ideal.</p>

<h3>Horrors beyond our comprehension feat. port80</h3>

<p>One of the more peculiar features of the embedded controller on chromeos devices is that it logs all writes to <a href="https://en.wikipedia.org/wiki/Power-on_self-test#Progress_and_error_reporting">IO port 0x80</a>. Traditionally, this port is used for POST status reporting. If your motherboard has a 7-segment boot code display, <b>it's decoding port 80</b>! And well, nothing stops you from writing to port 80 <i>after</i> your BIOS finishes the POST.</p>



<p><code>#!/bin/bash
# silly hack: write to a port from userspace on Linux
if [[ ! $2 ]]; then
	echo "usage: $0 &lt;port&gt; &lt;byte&gt;"
	exit 1
fi
doas dd seek=$((0x$1)) bs=1 count=1 of=/dev/port &lt; &lt;(xxd -p -r &lt;&lt;&lt; "$2")
</code>


<small>Figure 3 - save this as <span>outb</span>, chmod +x and run <span>./outb 80 ff</span>, then look at your POST code display</small></p><p>I tested my hypothesis with the above script under linux, and sure enough, I was able to read the bytes back on the cros_ec console!</p>

<p>Armed with.. all of this, I got to work. Serenity starts execution at <a href="https://github.com/SerenityOS/serenity/blob/b5c6e656f84ec2b3f5a8ae343feeee914a42dd89/Kernel/Arch/init.cpp#L165">Kernel/Arch/init.cpp</a>, so I spread a bunch of <span>IO::out8(0x80, 1);</span> around to track how far it got. I narrowed it down to crashing on <span>Memory::MemoryManager::initialize(0);</span>, and then decided that my manual debugging approach isn't gonna cut it long-term.</p>

<h3>What is port 0x80, if not a "serial" port?</h3>

<p>So, one could wonder, what would happen if in <a href="https://github.com/SerenityOS/serenity/blob/b5c6e656f84ec2b3f5a8ae343feeee914a42dd89/Kernel/Arch/x86_64/DebugOutput.cpp#L52">the serial write routine</a> the address grew legs and walked back from 0x3f8 to 0x80?</p>

<p><img src="https://sdomi.pl/weblog/23-serenityos-realhw/img/16.png" alt="terminal output from cros_ec. we're dumping bytes - care to decode what they do? :3">
	<small>Picture 17 - view from the cros_ec console after enabling <span>port80 intscroll</span></small>
</p>

<p>Answer is - briefly, a lot of bytes. Then something overflows, and cros_ec stops being able to reliably relay data. Even later, something else breaks even further and corrupts the whole cros_ec log output (not visible above). Oops?</p>

<p>We can work around that by <a href="https://github.com/SerenityOS/serenity/blob/4723c905e6e29bfe12e3dd3283b12683938f2350/Kernel/Arch/x86_64/DebugOutput.cpp#L46-L59">inserting a bunch of dummy waitstates</a> between writes - it sucks, it's slow, but it will have to do. A real serial IO chip would have a big buffer to make sure this doesn't happen, but we don't get that luxury here.</p>



<pre><code>void debug_output_cool(char ch)
{
	static bool was_cr = false;
	if (ch == 'n' &amp;&amp; !was_cr) {
		IO::out8(serial_com1_io_port, 'r');
		for (int i=0; i&lt;27768000; i++)
			asm volatile("nop");
	}

	IO::out8(serial_com1_io_port, ch);

	for (int i=0; i&lt;27768000; i++)
		asm volatile("nop");

	was_cr = ch == 'r';
}
</code></pre>
<p><small>Figure 4 - artisanally crafted for loops, for a balance of "perf" and "reliability"</small></p><p>We also need some abstraction to automatically parse lines from cros_ec and decode the bytes as ASCII. I'm doing this with a horrifying bash oneliner:</p>


<p><code># first, launch picocom
picocom -b 115200 --logfile /tmp/log /dev/ttyACM1

# then, in a second shell..
watch -n 1 "cat /tmp/log | tr '\r' '\n' | grep -a -i port | sed 's/.*: //;s/]//;s/0x07//;s/0x//' | cut -c 1-2 | xxd -p -r | tail -n 50"
</code>
<small>Figure 5 - absolutely incomprehensible bash oneliner</small></p><p>Printing all boot messages through this makes the system go from "boots in a few seconds" to "boots in a few minutes", but this is a price I was willing to pay.</p>

<hr>

<video controls="" src="https://f.sakamoto.pl/weblog-content/23/serial.webm"></video>
<p><small><b>WARNING: This video is a bit flickery</b></small></p><h3>serenity, again</h3>

<p>Armed with boot logs, I asked for support from the community.. or that's what I should have done. Instead, I spent a few days reading through the codebase, frantically trying to make sense of everything at once, only to finally give up and ask for help anyways. If you're in this situation, just ask. Open-source folks usually don't bite (as long as you search beforehand, and <a href="https://dontasktoask.com/">formulate your questions correctly</a>), and that very much applies to all frequent Serenity developers; they're a bunch of really nice folks :3</p>

<p>After some back-and-forth, spholz linked me <a href="https://github.com/SerenityOS/serenity/pull/24435">this MR</a>, which at the time was still open. Building from this branch made the generic framebuffer work!</p>

<p><img src="https://sdomi.pl/weblog/23-serenityos-realhw/img/17.png">
	<small>Picture 19 - task failed successfully</small>
</p>

<h3>The eMMC saga</h3>

<p>Previously mentioned StorageManagement crash looked something like this:</p>

<p><code>136.086 [init_stage2(1:1)]: PCI: Failed to initialize SD Host Controller (PCI [0000:00:1c:00] - PCI::HardwareID [8086:31cc]): Error(errno=74)
[init_stage2(1:1)]: ASSERTION FAILED: !m_controllers.is_empty()
[init_stage2(1:1)]: ./Kernel/Devices/Storage/StorageManagement.cpp:168 in void Kernel::StorageManagement::enumerate_storage_devices()
[init_stage2(1:1)]: KERNEL PANIC! :^(
</code>
<small>Figure 6 - SerenityOS crash debug output</small></p><p>As I wrote before, octopus has a 32GB eMMC chip. Serenity already had <i>some</i> SD drivers, so one could assume that making MMC work with those won't be a lot of work. Unfortunately: reality struck.</p>

<h3>What is MMC, anyways?</h3>

<p>If you got this far, I suspect you know that a long time ago MMC was a thing. Then SD became a thing too, and MMC died within just a few years. The details on <i>how</i> and <i>why</i> all of this happened are a story in itself, a part of which I researched and wrote down in a <a href="https://sdomi.pl/weblog/21-nobody-knows-what-happened-at-mmca/">previous blogpost</a>.</p>

<p>For the purposes of this post, all one needs to know is that MMC is <i>similar</i> to SD (but with a different init sequence), and that it never truly died: it haunts eMMC chips.</p>

<h3>Write the damn driver already!!</h3>

<p>In somewhat oversimplified terms, to get an SD / MMC card to work, we need:</p>

<ol>
	<li>a Host Controller; they used to come in a large variety of shapes and sizes, but nowadays it's usually an SDHCI (SD Host Controller), as specified by <a href="https://www.sdcard.org/downloads/pls/">the SD Association</a></li>
	<li>a bus to connect that host controller to; in our case, PCI</li>
	<li>the proper protocol implemented to talk between the host and the card</li>
</ol>

<p>As showcased by the crash log, we already had the first two. The protocol is publicly specified, but only for SD cards, not MMC - ever since that became a JEDEC standard in 2007, you need to <a href="https://archive.org/download/SD-specs/JESD84-B51.pdf">pay up to get it in a legit way</a>. Having to marry two diverging standards lead me into a rabbithole of comparing several different versions of both to figure out what's what, how it evolved, and what exactly I need to implement.</p>

<p>As an aside: JEDEC's standard is considered "open", but SD Association's is "proprietary", just because of the SD security extensions, even though the base version is available for free. I love vendor associations.</p>

<h3>SD vs MMC vs eMMC: the differences</h3>

<p>Depending on the source, you'll get vastly different stories on what differs between what. At the time, I cared most about the card initialization sequence; <a href="https://github.com/SerenityOS/serenity/blob/94a5e88aa879869de57060d5504134ff81157100/Kernel/Devices/Storage/SD/SDHostController.cpp#L160-L162">In serenity</a>, we start with sending CMD0 and waiting for a response. This should pass both on SD as well as MMC. Then, we send CMD8 to begin the voltage setup - MMC doesn't support this, so we should get an error. At that point, some sources suggest trying to reset the card (or the whole controller) and assuming that the card is MMC from here on out. Other sources, such as <a href="https://forum.microchip.com/s/topic/a5C3l000000M1X0EAK/t235946?comment=P-1847318">this wonderful forum thread</a> suggest a much more... comprehensive approach:</p>




<p><span>According to the SD spec (SD Specifications, Part 1, Physical Layer, Simplified Specification, Version 2.00, July 27, 2006) figure 7-2: SPI Mode Initialization Flow, there are 5 possible outcomes:<p>
1) CMD8 fails and CMD58 fails: must be MMC, thus initialize using CMD1<br>
2) CMD8 fails and CMD58 passes: must be Ver1.x Standard Capacity SD Memory Card<br>
3) CMD8 passes and CMD58 passes (CCS = 0): must be Ver2.00 or later Standard Capacity SD Memory Card<br>
4) CMD8 passes and CMD58 passes (CCS = 1): must be Ver2.00 or later High Capacity SD Memory Card<br>
5) CMD8 passes and CMD58 passes but indicates non compatible voltage range: unusable card</p><p>
I'm not sure whether knowing the Version of the SD Card makes any difference to the reading/writing/erasing logic.</p><p>
<b>~ Spruce, 28th September 2006</b></p></span></p><p>Anyhoo, I chose to only implement basic checks - and unless we run into some weird compat issues, we probably won't bother with the more advanced ones. I don't exactly yearn to code up this vendor-flavored spaghetti.</p>

<hr>

<p>The rest of the initialization flow also differs from SD, but it boils down to:</p>

<ol>
	<li>After the reset, set the clock to 400KHz</li>
	<li>"wait for 1ms, then wait for 74 more clock cycles" (sic!)</li>
	<li>Send CMD0, wait for response</li>
	<li>Send CMD1 in a loop (!) until 31st bit from the response is 1 (JEDEC standard is slightly misleading on this point)</li>
	<li>Once the loop finishes, save the value. That's our Operating Conditions register</li>
	<li>Continue with the SD initialization algo, sans for querying SD-specific registers</li>
	<li>Optionally, probe for High-Speed compatibility and turn on one of the <i>various</i> HS modes (there are more than 4, along with two different ways of compatibility probing!)</li>
</ol>

<p>My code was getting to around point 4, but no matter what I did further down the line, my card wouldn't respond to <i>any</i> requests. I spent a <b>days</b> trying to debug what was going wrong, and I couldn't find anything. One evening during a routine session of pulling my hair out, I randomly removed all the parts responsible for the controller reset - and while that didn't fix it entirely, the eMMC started responding! This suggested that the <a href="https://github.com/SerenityOS/serenity/blob/94a5e88aa879869de57060d5504134ff81157100/Kernel/Devices/Storage/SD/SDHostController.cpp#L520-L532">reset function</a> was to blame:</p>

<p><code>ErrorOr<void> SDHostController::reset_host_controller()
{
	m_registers-&gt;host_configuration_0 = 0;
	m_registers-&gt;host_configuration_1 = m_registers-&gt;host_configuration_1 | software_reset_for_all;
	if (!retry_with_timeout(
		[&amp;] {
			return (m_registers-&gt;host_configuration_1 &amp; software_reset_for_all) == 0;
		})) {
		return EIO;
	}

	return {};
}
</void></code>
<small>Figure 7 - fragment of SDHostController.cpp</small></p><p>First weird thing about this function: no standard I found defined a <span>host_configuration</span> register. There was a Host Control register which was roughly in the same spot as our weird 32bit blob, but nothing seemed to fully match. AS IT TURNS OUT: the previous entity touching this code grouped a bunch of registers into two arbitrary <span>host_configuration</span> groups, then never fully finished initializing them - see how we just set the 1st register to 0.</p>

<p>Why is this important? The first group contains a smaller <i>Power Control</i> register, controlling the power regulator for the card. On some hardware implementations (including all that use eMMC) this register is <b>required to power on the card itself</b>, while other designs (that connect the power rail directly to the slot) may ignore it entirely - hence the misconfiguration.</p>

<p>As a temporary hack, I just stole the value initially contained in <span>host_configuration_0</span> and rolled with that.</p>

<p><b>tl;dr: talking to the card is significantly easier if we power it on.</b></p><hr>

<p>After figuring this <i>tiny</i> detail out, I needed a few more hours to debug and disable specific commands in the sequence that were only valid for SD cards. The rest went quite unremarkably - finally getting sensible debug output from the controller helped my nerves a lot, too.</p>

<div><p>Soon, we (really slowly) spawned a (partially corrupted) graphical session, which (unfortunately) quickly locked up. Debugging all of those issues will be described in a future part of this series (if attention span allows).</p><p>I don't have a good picture of that semi-broken state, so please enjoy one from when I managed to unbreak the framebuffer - something I will (hopefully) describe in detail in the next post.</p></div>

<p><img src="https://sdomi.pl/weblog/23-serenityos-realhw/img/alive.jpg" alt="picture of a laptop screen. there are two terminal windows and a taskbar running">
<small>Picture 20 - (out of the lack of a better picture) finally! it's alive!</small></p><p>This post itself took me on a learning journey that spanned around 6 months, many of them busy with other things that got in the way (wink wink, <a href="https://sdomi.pl/weblog/?id=22">Project SERVFAIL</a>). I plan to finish up cleaning up and upstreaming my patches sometime this year.</p>

<hr>



<hr><p><a href="https://ko-fi.com/sdomi/"><img alt="Support me on ko-fi!" src="https://sdomi.pl/img/ko-fi.png"></a></p><hr><h3>Comments:</h3></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: The Atlas of Space (413 pts)]]></title>
            <link>https://atlasof.space/</link>
            <guid>42634787</guid>
            <pubDate>Wed, 08 Jan 2025 14:47:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://atlasof.space/">https://atlasof.space/</a>, See on <a href="https://news.ycombinator.com/item?id=42634787">Hacker News</a></p>
Couldn't get https://atlasof.space/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Fidget (379 pts)]]></title>
            <link>https://www.mattkeeter.com/projects/fidget/</link>
            <guid>42634624</guid>
            <pubDate>Wed, 08 Jan 2025 14:30:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mattkeeter.com/projects/fidget/">https://www.mattkeeter.com/projects/fidget/</a>, See on <a href="https://news.ycombinator.com/item?id=42634624">Hacker News</a></p>
Couldn't get https://www.mattkeeter.com/projects/fidget/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Robotics 101 at UMich: Applied numerical linear algebra as intro linear algebra (195 pts)]]></title>
            <link>https://robotics.umich.edu/academics/courses/course-offerings/rob101-fall-2020/</link>
            <guid>42633805</guid>
            <pubDate>Wed, 08 Jan 2025 13:11:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://robotics.umich.edu/academics/courses/course-offerings/rob101-fall-2020/">https://robotics.umich.edu/academics/courses/course-offerings/rob101-fall-2020/</a>, See on <a href="https://news.ycombinator.com/item?id=42633805">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
		<main id="main">

			
<article id="post-6411" class="page">

	<div>
				
			
<h2>Fall 2020 (or <a href="https://robotics.umich.edu/academic-program/course-offerings/rob101-fall-2022/">see Fall 2022</a>)</h2>



<figure><img fetchpriority="high" decoding="async" width="1024" height="512" src="https://robotics.umich.edu/wp-content/uploads/2020/07/ROB101-landscape-clena-copy@4x-100-1024x512.jpg" alt="" srcset="https://robotics.umich.edu/wp-content/uploads/2020/07/ROB101-landscape-clena-copy@4x-100-1024x512.jpg 1024w, https://robotics.umich.edu/wp-content/uploads/2020/07/ROB101-landscape-clena-copy@4x-100-300x150.jpg 300w, https://robotics.umich.edu/wp-content/uploads/2020/07/ROB101-landscape-clena-copy@4x-100-768x384.jpg 768w, https://robotics.umich.edu/wp-content/uploads/2020/07/ROB101-landscape-clena-copy@4x-100-1536x768.jpg 1536w, https://robotics.umich.edu/wp-content/uploads/2020/07/ROB101-landscape-clena-copy@4x-100-2048x1024.jpg 2048w, https://robotics.umich.edu/wp-content/uploads/2020/07/ROB101-landscape-clena-copy@4x-100-1200x600.jpg 1200w, https://robotics.umich.edu/wp-content/uploads/2020/07/ROB101-landscape-clena-copy@4x-100-1400x700.jpg 1400w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="//robotics.umich.edu/wp-content/plugins/a3-lazy-load/assets/images/lazy_placeholder.gif" data-src="https://robotics.umich.edu/wp-content/uploads/2020/07/ROB101-landscape-clena-copy@4x-100-1024x512.jpg" data-srcset="https://robotics.umich.edu/wp-content/uploads/2020/07/ROB101-landscape-clena-copy@4x-100-1024x512.jpg 1024w, https://robotics.umich.edu/wp-content/uploads/2020/07/ROB101-landscape-clena-copy@4x-100-300x150.jpg 300w, https://robotics.umich.edu/wp-content/uploads/2020/07/ROB101-landscape-clena-copy@4x-100-768x384.jpg 768w, https://robotics.umich.edu/wp-content/uploads/2020/07/ROB101-landscape-clena-copy@4x-100-1536x768.jpg 1536w, https://robotics.umich.edu/wp-content/uploads/2020/07/ROB101-landscape-clena-copy@4x-100-2048x1024.jpg 2048w, https://robotics.umich.edu/wp-content/uploads/2020/07/ROB101-landscape-clena-copy@4x-100-1200x600.jpg 1200w, https://robotics.umich.edu/wp-content/uploads/2020/07/ROB101-landscape-clena-copy@4x-100-1400x700.jpg 1400w"></figure>



<div>
<p>Computational Linear Algebra is a pilot first-semester, first-year undergraduate course that will show how mathematics and computation are unified for reasoning about data and making discoveries about the world.</p>



<p>Linear algebra and coding are rapidly becoming an essential foundation for the modern engineer in a computational world.&nbsp; Students in this course will gain insights into the mathematical theory of linear algebra and its realization in practical computational tools.&nbsp; </p>



<p>Math is the language of engineering, but coding is believing and realizing it. The mathematical content of ROB 101 will be built around systems of linear equations, their representation as matrices, and numerical methods for their analysis. These methods will be given life through the lens of robotics and contemporary intelligent systems and their compelling applications.</p>




</div>



<figure><div>
<p><iframe title="Flipping the script on engineering education" width="905" height="509" src="https://www.youtube.com/embed/4oXJTSBCe_U?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p>
</div></figure>



<div><details open=""><summary>What are the topics covered in this course?</summary><div>
<ul><li><a href="https://julialang.org/">Julia</a> as an open-source “dynamic” programming language. Quoting from Wikipedia: “Julia is a high-level, high-performance, dynamic programming language. While it is a general-purpose language and can be used to write any application, many of its features are well-suited for high-performance numerical analysis and computational science.” All of the mathematical topics treated in the course will be made operational through computational exercises in Julia. </li><li>Small sets of linear equations, their solvability, and how to find solutions: underdetermined, overdetermined, and just right. Scaling up to large sets of equations (thousands of variables) through vectors, matrices, determinants, and matrix inverses. </li><li>Regression as a means of building models and making predictions. </li><li>A camera pin-hole model and camera calibration via regression. </li><li>Concept of factoring a matrix. Inner products and orthogonal vectors. Symmetric matrices and orthogonal matrices. </li><li>Finding solutions to nonlinear equations via gradient descent. </li><li>Optimization as a tool for solving robot perception problems. </li><li>Linear transformations with applications to spatial coordinates. </li><li>LiDAR point clouds and blobs. </li><li>Accelerometers, single-axis gyroscopes, encoders, and what is an ODE.</li></ul>
</div></details></div>



<div><details><summary>Why should I take ROB 101. What is so special about it?</summary><div>
<p>Engineering math education is stuck in the Sputnik era: we force you to do four semesters of calculus before you can do anything interesting in engineering. ROB 101 seeks to break through with new ideas. You will see how engineers are using mathematics and computing to solve large and important problems. You will still do drill problems to firm up concepts with teeny tiny problems with two or three variables, but you will also solve problems in the Julia programming language with hundreds of variables.</p>



<p>Instead of seeing the “rules of calculus” as your introduction to college mathematics, you will experience the “raison d’etre of mathematics in engineering”, in other words, why we employ it so heavily in engineering in the first place. And instead of starting mathematics by learning the innumerable ways to get a closed-form expression by a change of variable in an integral, you will learn how to formulate and perform relevant computations with algorithms. In short, you will have a palpable understanding that computation and mathematics are your friends instead of hoops to be jumped through on the way to a degree.&nbsp;</p>
</div></details></div>



<div><details><summary>What do I need to know to be successful in this course?</summary><p>We assume no prior knowledge of Linear Algebra or programming. We assume that you have completed a high school Algebra course where you learned how to solve quadratic equations. That is it.</p></details></div>



<div><details><summary>What personal characteristics are needed for this course?</summary><div>
<p>You have been admitted to U-M Engineering — that tells us that you are up for a challenge! You’re smart,&nbsp;and you want to become a good engineer. We also need you to be a person who respects others and demonstrates humility. The field of Robotics is so broad that no one person can master even ⅓ of the subject. </p>



<p>The first thing we teach our graduate students is to say the words: “Can you help me with this?” In Robotics, asking for help is not a sign of weakness. It is a sign that you respect and appreciate a fellow human being. If you cannot bring yourself to ask for help, then you’re probably not the right person for ROB 101.&nbsp;&nbsp;</p>
</div></details></div>



<div><details><summary>What technology do I need to succeed in this course?</summary><p>You need to be able to access the web. The course will use the Julia programming language delivered to you in the form of Jupyter notebooks that can be run in a web browser. If you can access the U-M course learning site, Canvas, then you can access all of the course materials and turn in all of the homework assignments and projects.</p></details></div>



<div><details><summary>Will I need to be on campus to take exams or to attend lectures?</summary><div>
<p>No. The class is being offered in hybrid mode: your instructors will lecture in a classroom to anyone who wishes to attend. The lecture will be simultaneously broadcast on Zoom and recorded. The recorded lectures will be posted on the ROB 101 Canvas site.&nbsp; A student instructor will help the faculty member to monitor the chatbox on Zoom. You will be encouraged to participate. We’ll have anonymous means to ask your opinions about stuff.&nbsp;</p>



<p>All exams and quizzes will be conducted in electronic form. There will be a window of time during which they can be completed. ROB 101 will be much different than a typical college course: we are not planning on two midterms and a final exam. We are planning on course projects to be where you demonstrate your ability to synthesize information.</p>
</div></details></div>



<div><details><summary>What requirement will ROB 101 meet?</summary><div>
<p>ROB 101 is intended to fulfill the linear algebra prerequisite of many courses in Engineering, with specific attention to more streamlined preparation for Robotics and AI courses.</p>



<p>Right now, it does not replace any required course, which is normal in the pilot semester of a new course. We are working with individual departments to see if they might accept it in place of linear algebra requisites (Math 214 and Math 217, for example) for other Engineering courses (computer vision, for example). You may not have a definite answer before you begin the course.</p>
</div></details></div>



<div><details><summary>How many students are you planning to admit?</summary><p>In this pilot phase, only 30 students maximum will be admitted.</p></details></div>



<div><details><summary>What is Linear Algebra?</summary><p>Roughly speaking, it’s a branch of mathematics that deals with linear equations in several variables. It also includes a number of tools, such as vectors, matrices, spaces of vectors, and linear transformations.</p></details></div>



<div><details><summary>Why is Linear Algebra so important?</summary><p>It has become the language of computer vision, machine learning, robotics, and autonomy. It provides very compact and insightful ways to manipulate information. Linear Algebra and programming go together like hand and glove.</p></details></div>



<div><details><summary>What about Matlab? Why are you using the Julia Programming Language?</summary><p>Robotics and many other fields are supporting open-source tools of all kinds. When researchers submit papers nowadays, they are expected to share software that supports the mathematical and experimental concepts in the paper. In the past, many researchers indeed used MATLAB. While it’s a great programming environment, it’s also very expensive. Also, you must have a local copy on your computer in order to use it. Julia, on the other hand, is open-source software and we can deliver it to you via a browser. This provides near-universal accessibility.&nbsp;</p></details></div>



<div><details><summary>If I need MATLAB in another course, will I be at a disadvantage?</summary><p>The programming structures in Julia and MATLAB are very similar. It will probably take you two weeks to get up to speed in MATLAB once you learn Julia. In addition, learning Julia will help your acquisition of C++.</p></details></div>



<div><details><summary>What are the downsides to Julia?</summary><p>It’s a new language. The help pages are not as beautifully organized as the help pages for MATLAB. You’ll learn to google a lot as you acquire skills in Julia.</p></details></div>



<div><details><summary>Is Linear Algebra hard to learn?</summary><p>Your instructors believe that linear algebra is easier to learn than calculus and they 100% know that it is much easier to begin doing meaningful things in engineering with Linear Algebra than it is with Calculus. We’ll do three projects in the course. In the first one, we’ll have you building a map that a robot could use to navigate on the North Campus Grove.</p></details></div>



<div><details><summary>Why is it a new thing to be teaching Linear Algebra to first-semester, first-year undergraduate students?</summary><p>That is a very good question! A number of us have been advocating for it for a long time. The Robotics faculty at Michigan, led by Professor <a href="https://robotics.umich.edu/profile/chad-jenkins/">Chad Jenkins</a>, finally decided to do something about it. Normally, mathematics courses are developed through the Department of Mathematics. We want you to see how engineers use math and think about math. Other schools are starting to do this as well. Professor Steven Boyd has introduced a Linear Algebra course at Stanford that is required for first-year engineering students. Will ROB 101 be a required course at Michigan someday? Hard to say! We have to survive the pilot offering first.&nbsp;</p></details></div>



<div><details><summary>Do I actually need to be available MWF, 10-11am, since lectures are recorded?</summary><p>It is fine to follow the course asynchronously, which means you watch the recorded lectures at a time of your choosing. You are not required to attend in person.</p></details></div>



<div><details><summary>What section should I choose, and what is the difference between hybrid (001) or distance (881)?</summary><p>No one is obliged or even encouraged to attend in person. Attendance in person is a personal choice. We suggest you select the hybrid section if you are planning to be in Ann Arbor for Fall 2020. If you are not planning to be in Ann Arbor for the fall term, due to concerns of COVID-19, for example, then we suggest the distance learning section.</p></details></div>



<div><details><summary>Where will the in-person class be held?</summary><p>We plan for the course to meet in person in the new <a href="https://robotics.umich.edu/about/ford-motor-company-robotics-building/" title="Ford Robotics Building">Robotics Building</a> on North Campus. But again, attendance in person is a personal choice. No one is obliged to attend in person. </p></details></div>



<div><details><summary>How will grades be assigned?</summary><div>
<p>Fairly! Here is what we are currently thinking. We are open to suggestions.&nbsp;&nbsp;</p>



<ul><li>15% Ten (10) roughly weekly homework sets. This is where mistakes have almost no cost.&nbsp;</li><li>60% Projects (Three&nbsp;projects at 20% each)</li><li>15% Quizzes (Four to six very short quizzes offered online, so obviously, you can consult the web when doing them)&nbsp;</li><li>5% Final presentation (Two minute video on some aspect of&nbsp; “Computational Mathematics for Engineering”; you will have a lot of freedom in the topic. )</li><li>5% Class engagement with online tools such as piazza (online chat for math), constructive comments on posted course notes, HW sets, recorded lectures, other videos, etc.</li></ul>




</div></details></div>

			</div><!-- .post-content-->

</article><!-- #post-## -->

		</main><!-- #main -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We Cracked a 512-Bit DKIM Key for Less Than $8 in the Cloud (613 pts)]]></title>
            <link>https://dmarcchecker.app/articles/crack-512-bit-dkim-rsa-key</link>
            <guid>42633501</guid>
            <pubDate>Wed, 08 Jan 2025 12:32:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dmarcchecker.app/articles/crack-512-bit-dkim-rsa-key">https://dmarcchecker.app/articles/crack-512-bit-dkim-rsa-key</a>, See on <a href="https://news.ycombinator.com/item?id=42633501">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In our <a href="https://dmarcchecker.app/articles/spf-dkim-dmarc-adoption-2024">study on the SPF, DKIM, and DMARC records of the top 1M websites</a>, we were surprised to uncover more than 1,700 public DKIM keys that were shorter than 1,024 bits in length. This finding was unexpected, as RSA keys shorter than 1,024 bits are considered insecure, and their use in DKIM has been deprecated since the introduction of <a href="https://datatracker.ietf.org/doc/html/rfc8301" target="_blank">RFC 8301</a> in 2018.</p><p>Driven by curiosity, we decided to explore whether we could crack one of these keys. Our goal was to extract the private key from a public RSA key, enabling us to sign emails as if we were the original sender. We were also curious to see whether emails signed with this compromised key would pass the DKIM verification checks of major email providers like Gmail, Outlook.com, and Yahoo Mail, or if they would outright refuse to verify a signature generated with such a short key.</p><p>For our experiment, we chose redfin.com after discovering a 512-bit RSA public key at <i>key1._domainkey.redfin.com</i> (now no longer available):</p><pre>$ dig +short TXT key1._domainkey.redfin.com
"k=rsa; p=MFwwDQYJKoZIhvcNAQEBBQADSwAwSAJBAMx7VnoRmk/wFPeFWxrVUde6AJQI51/uPFL2CbiHGMnRSnLjPs72AgxAVHIe5QrNQ2riR5+7u47Sgh5R5va/d0cCAwEAAQ=="</pre><h2>Decoding the RSA Public Key</h2><p>The public key, located in the DKIM record’s <code>p</code> tag, is encoded in <a href="https://en.wikipedia.org/wiki/X.690#DER_encoding" target="_blank">ASN.1 DER format</a>, and further encoded as Base64. To decode the key to obtain the modulus (<i>n</i>) and the public exponent (<i>e</i>), we used a couple of lines of Python code:</p><pre>$ python3
&gt;&gt;&gt; from Crypto.PublicKey import RSA
&gt;&gt;&gt; RSA.import_key('-----BEGIN PUBLIC KEY-----\n' + 'MFwwDQYJKoZIhvcNAQEBBQADSwAwSAJBAMx7VnoRmk/wFPeFWxrVUde6AJQI51/uPFL2CbiHGMnRSnLjPs72AgxAVHIe5QrNQ2riR5+7u47Sgh5R5va/d0cCAwEAAQ==' + '\n-----END PUBLIC KEY-----')
RsaKey(n=10709580243955269690347257968368575486652256021267387585731784527165077094358215924099792804326677548390607229176966588251215467367272433485332943072098119, e=65537)</pre><h2>Factorizing the RSA Modulus</h2><p>With the modulus <i>n</i> in hand, our next step was to identify the two prime numbers, <i>p</i> and <i>q</i>, whose product equals <i>n</i>. This process, known as factoring, can be quite challenging to perform efficiently. Fortunately, we found a powerful open-source tool called <a href="https://cado-nfs.gitlabpages.inria.fr/" target="_blank">CADO-NFS</a>, which offers an easy-to-use implementation of the Number Field Sieve (NFS) algorithm — the most efficient method available for factoring large integers.</p><p>Since factoring takes a lot of computing power and we didn’t want to tie up our computers for days, we went with renting a cloud server. We chose a server with 8 dedicated vCPUs (AMD EPYC 7003 series) and 32 GB of RAM from Hetzner, installing Ubuntu as the OS. Setting up CADO-NFS was straightforward:</p><pre>git clone https://gitlab.inria.fr/cado-nfs/cado-nfs.git
cd cado-nfs
make</pre><p>To ensure the server had enough memory for the task, we added 32 GB of swap space:</p><pre>sudo fallocate -l 32G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile</pre><p>We started the factorization by calling the <code>cado-nfs.py</code> script with <i>n</i> as the input:</p><pre>./cado-nfs.py 10709580243955269690347257968368575486652256021267387585731784527165077094358215924099792804326677548390607229176966588251215467367272433485332943072098119</pre><p>The process took approximately 86 hours on our 8-vCPU server, successfully factorizing <i>n</i> into <i>p</i> and <i>q</i>:</p><pre>Info:Complete Factorization / Discrete logarithm: Total cpu/elapsed time for entire Complete Factorization 2.20529e+06/309865 [3d 14:04:25]
97850895333751392558280999318309697780438485965134147739065017624372104720767 109447953515671602102748820944693252789237215829169932130613751100276125683257</pre><p>Although opting for a more powerful server or distributing the workload across several systems (a process simplified by CADO-NFS) could have expedited the task, we weren’t pressed for time and didn’t mind the wait.</p><h2>Constructing the RSA Private Key</h2><p>After determining <i>p</i> and <i>q</i>, we had all the necessary components to construct the RSA private key. The process involved using Python and the PyCryptodome library:</p><pre>$ python3
&gt;&gt;&gt; from Crypto.PublicKey import RSA
&gt;&gt;&gt; from Crypto.Util.number import inverse
&gt;&gt;&gt; p = 97850895333751392558280999318309697780438485965134147739065017624372104720767
&gt;&gt;&gt; q = 109447953515671602102748820944693252789237215829169932130613751100276125683257
&gt;&gt;&gt; e = 65537
&gt;&gt;&gt; n = p * q
&gt;&gt;&gt; phi = (p-1) * (q-1)
&gt;&gt;&gt; d = inverse(e, phi)
&gt;&gt;&gt; key = RSA.construct((n, e, d, p, q))
&gt;&gt;&gt; private_key = key.export_key()
&gt;&gt;&gt; print(private_key.decode())</pre><p>These Python lines output the private key in PEM format, ready for use:</p><pre>-----BEGIN RSA PRIVATE KEY-----
MIIBOgIBAAJBAMx7VnoRmk/wFPeFWxrVUde6AJQI51/uPFL2CbiHGMnRSnLjPs72
AgxAVHIe5QrNQ2riR5+7u47Sgh5R5va/d0cCAwEAAQJAPliEv2dKk4DyA54nbwEH
mSzfLEOiuD8dKXZW9GpMhou72DYYcc5YD0PeQW0uGGsusnTZXRU3Kd3cmVfeR+np
4QIhANhVpOQ440Gqlda3nqCOAag12jq8ET+qr1G7VL8x9PF/AiEA8flYr5rUO6Io
/5HRoHq6p7dA75PRK+7v79o0/ijfTjkCIEdWPpCPfckKomxykllpWnyIfZT+rUVs
WHHAL1r480erAiAz3xD87ALtGbESQE8gyM50n5sjAJwJf/odf7h2d4qPOQIhAKwr
Nv6s5cQiwbYgm1KND83nrkxe6uFQlu9ilkdwAIY4
-----END RSA PRIVATE KEY-----</pre><h2>Sending DKIM-Signed Test Mails From @redfin.com</h2><p>With the RSA private key integrated into our OpenDKIM setup, we proceeded to the testing phase. We crafted a simple email with a FROM address of security@redfin.com, then sent it to a variety of email hosting services. Although most providers correctly identified the 512-bit key as insecure and rejected our DKIM signature, three major providers — Yahoo Mail, Mailfence, and Tuta — reported a <code>dkim=pass</code> result.</p><p><img src="https://dmarcchecker.app/assets/images/articles/dkim-pass-redfin-yahoo.png" alt="Yahoo Mail: Authentication Results"></p><p>Here’s how each provider responded:</p><ul><li>Gmail: <span>FAIL</span></li><li>Outlook: <span>FAIL</span></li><li>Yahoo Mail: <span><strong>PASS</strong></span></li><li>Zoho: <span>FAIL</span></li><li>Fastmail: <span>FAIL</span></li><li>Proton Mail: <span>FAIL</span></li><li>Mailfence: <span><strong>PASS</strong></span></li><li>Tuta: <span><strong>PASS</strong></span></li><li>GMX: <span>FAIL</span></li><li>OnMail: <span>FAIL</span></li></ul><p>Given that redfin.com also has a valid DMARC record (<code>v=DMARC1;p=reject;pct=100;rua=mailto:a+99923342@fdmarc.net;ruf=mailto:f+99923342@fdmarc.net;ri=3600;fo=1;</code>), passing the DKIM check for <code>redfin.com</code> also means that our email passed DMARC verification and met the requirements for <a href="https://bimigroup.org/" target="_blank">BIMI</a>.</p><h2>Final Thoughts</h2><p>Three decades ago, breaking a 512-bit RSA public key was a feat achievable only with a supercomputer. Today, it’s possible to do so in just a few hours for less than US$8 on a cloud server. And if you have a powerful computer at home with 16 or more cores, you could accomplish this even more swiftly and cost-effectively.</p><p>There’s no good reason to use 512 or 768-bit keys these days. Email providers should automatically reject any DKIM signature generated with an RSA key shorter than 1,024 bits. We’ve alerted Yahoo, Mailfence, and Tuta about our findings and shared this advice with them.</p><p>Domain owners must also take action by reviewing their DNS settings for any outdated DKIM records that don’t comply with the 1,024-bit minimum standard. A simple way to check a DKIM record’s <code>p</code> tag is to count its Base64 characters: a 1,024-bit RSA public key will have at least 216 characters.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bye-bye Windows gaming? SteamOS officially expands past the Steam Deck (212 pts)]]></title>
            <link>https://arstechnica.com/gaming/2025/01/bye-bye-windows-gaming-steamos-officially-expands-past-the-steam-deck/</link>
            <guid>42633269</guid>
            <pubDate>Wed, 08 Jan 2025 11:56:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gaming/2025/01/bye-bye-windows-gaming-steamos-officially-expands-past-the-steam-deck/">https://arstechnica.com/gaming/2025/01/bye-bye-windows-gaming-steamos-officially-expands-past-the-steam-deck/</a>, See on <a href="https://news.ycombinator.com/item?id=42633269">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                      
                      
          <p>Almost exactly a year ago, we were <a href="https://arstechnica.com/gaming/2024/01/why-more-pc-gaming-handhelds-should-ditch-windows-for-steamos/">publicly yearning</a> for the day when more portable gaming PC makers could ditch Windows in favor of SteamOS (without having to resort to <a href="https://arstechnica.com/gadgets/2023/06/the-linux-coders-turning-the-rog-ally-and-other-handhelds-into-steam-deck-clones/">touchy unofficial workarounds</a>). Now, that day has finally come, with Lenovo <a href="https://news.lenovo.com/pressroom/press-releases/lenovo-legion-unleashes-next-gen-gaming-power-at-ces-2025/">announcing</a> the upcoming Legion Go S as the first non-Valve handheld to come with an officially licensed copy of SteamOS preinstalled. And Valve promises that it will soon ship a beta version of SteamOS for users to "download and test themselves."</p>
<p>As Lenovo's slightly downsized followup to <a href="https://arstechnica.com/gaming/2023/09/lenovo-legion-go-suggests-gaming-portables-arent-big-enough-need-a-mouse/">2023's massive Legion Go</a>, the Legion Go S won't feature the detachable controllers of its predecessor. But the new PC gaming handheld will come in two distinct versions, one with the now-standard Windows 11 installation and another edition that's the first to sport the (<a href="https://arstechnica.com/gaming/2024/12/the-return-of-steam-machines-valve-rolls-out-new-powered-by-steamos-branding/">recently leaked</a>) "Powered by SteamOS" branding.</p>
<p>The lack of a Windows license seems to contribute to a lower starting cost for the "Powered by SteamOS" edition of the Legion Go S, which will start at $500 when it's made available in May. Lenovo says the Windows edition of the device—available starting this month—will start at $730, with "additional configurations" available in May starting as low as $600.</p>
<figure>
    <p><img width="1024" height="493" src="https://cdn.arstechnica.net/wp-content/uploads/2025/01/legiongoswindows.png" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/01/legiongoswindows.png 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/legiongoswindows-640x308.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/legiongoswindows-768x370.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/legiongoswindows-980x472.png 980w" sizes="auto, (max-width: 1024px) 100vw, 1024px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The Windows version of the Legion Go S will come with a different color and a higher price.

              <span>
          Credit:

                      <a href="https://news.lenovo.com/pressroom/press-releases/lenovo-legion-unleashes-next-gen-gaming-power-at-ces-2025/" target="_blank">
          
          Lenovo

                      </a>
                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>Both the Windows and SteamOS versions of the Legion Go S will weigh in at 1.61 lbs with an 8-inch 1200p 120 Hz LCD screen, up to 32GB of RAM, and either AMD's <a href="https://arstechnica.com/gadgets/2025/01/amds-new-ryzen-z2-cpus-boost-gaming-handhelds-if-you-buy-the-best-one/">new Ryzen Z2 Go chipset</a>&nbsp;or an older Z1 core.</p>
<h2>Watch out, Windows?</h2>
<p>Valve <a href="https://store.steampowered.com/news/app/593110/view/529834914570306831">said in a blog post on Tuesday</a> that the Legion Go S will sport the same version of SteamOS currently found on the Steam Deck. The company's work getting SteamOS onto the Legion Go S will also "improve compatibility with other handhelds," Valve said, and the company "is working on SteamOS support for more devices in the future."</p>

          
                      
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Comet is a handheld Linux computer that brings extensibility (134 pts)]]></title>
            <link>https://mecha.so/comet</link>
            <guid>42632865</guid>
            <pubDate>Wed, 08 Jan 2025 10:44:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mecha.so/comet">https://mecha.so/comet</a>, See on <a href="https://news.ycombinator.com/item?id=42632865">Hacker News</a></p>
Couldn't get https://mecha.so/comet: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[The Aging Programmer (212 pts)]]></title>
            <link>https://www.youtube.com/watch?v=mVWQQeSOD0M</link>
            <guid>42632772</guid>
            <pubDate>Wed, 08 Jan 2025 10:22:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=mVWQQeSOD0M">https://www.youtube.com/watch?v=mVWQQeSOD0M</a>, See on <a href="https://news.ycombinator.com/item?id=42632772">Hacker News</a></p>
Couldn't get https://www.youtube.com/watch?v=mVWQQeSOD0M: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Operating System in 1,000 Lines – Intro (799 pts)]]></title>
            <link>https://operating-system-in-1000-lines.vercel.app/en</link>
            <guid>42631873</guid>
            <pubDate>Wed, 08 Jan 2025 07:18:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://operating-system-in-1000-lines.vercel.app/en">https://operating-system-in-1000-lines.vercel.app/en</a>, See on <a href="https://news.ycombinator.com/item?id=42631873">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Hey there! In this book, we're going to build a small operating system from scratch, step by step.</p>
<p>You might get intimidated when you hear OS or kernel development, the basic functions of an OS (especially the kernel) are surprisingly simple. Even Linux, which is often cited as a huge open-source software, was only 8,413 lines in version 0.01. Today's Linux kernel is overwhelmingly large, but it started with a tiny codebase, just like your hobby project.</p>
<p>We'll implement basic context switching, paging, user mode, a command-line shell, a disk device driver, and file read/write operations in C. Sounds like a lot, however, it's only 1,000 lines of code!</p>
<p>One thing you should remember is, it's not easy as it sounds. The tricky part of creating your own OS is debugging. You can't do <code>printf</code> debugging until you implement it. You'll need to learn different debugging techniques and skills you've never needed in application development. Especially when starting "from scratch", you'll encounter challenging parts like boot process and paging. But don't worry! We'll also learn "how to debug an OS" too!</p>
<p>The tougher the debugging, the more satisfying it is when it works. Let's dive into exciting world of OS development!</p>
<ul>
<li>You can download the implementation examples from <a href="https://github.com/nuta/operating-system-in-1000-lines">GitHub</a>.</li>
<li>This book is available under the <a href="https://creativecommons.jp/faq">CC BY 4.0 license</a>. The implementation examples and source code in the text are under the <a href="https://opensource.org/licenses/MIT">MIT license</a>.</li>
<li>The prerequisites are familiarity with C language and with UNIX-like environment. If you've <code>gcc hello.c &amp;&amp; ./a.out</code>, you're good to go!</li>
<li>This book was originally written as an appendix of my book <em><a href="https://www.shuwasystem.co.jp/book/9784798068718.html">Design and Implementation of Microkernels</a></em> (written in Japanese).</li>
</ul>
<p>Happy OS hacking!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gate-level simulation of ASIC in browser (184 pts)]]></title>
            <link>https://znah.net/tt09/</link>
            <guid>42631629</guid>
            <pubDate>Wed, 08 Jan 2025 06:38:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://znah.net/tt09/">https://znah.net/tt09/</a>, See on <a href="https://news.ycombinator.com/item?id=42631629">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <h2>TT09 VGA CA</h2>
    
    <p>
        This is a gate-level simulation of my <a href="https://tinytapeout.com/">TinyTapeout</a> '09 submission running directly in your browser. The circuit occupies a single 160x100 μm tile on a multiproject chip and generates a 60fps 640x480 VGA signal that forms a scrolling animation of several <a href="https://mathworld.wolfram.com/ElementaryCellularAutomaton.html">elementary 1D cellular automata</a> rules. Most of the tile area is occupied by two 160-bit shift registers: one (left) stores the current image line, while another (right) stores the first line of the next frame (4th line of the current).
    </p>
    <p>
        This demo combines a tiny custom WebAssembly gate-level <a href="https://github.com/znah/tt09/blob/main/gates.c">simulator</a> with a <a href="https://google.github.io/swissgl/">SwissGL</a>-based interactive visualization of gate activations. The overlay displays the content of the simulated VGA screen and the current ray position.
    </p>
    <p>Links:</p>
    <ul>
        <li><a href="https://github.com/znah/tt09-vga-ca">ASIC design repository</a> (<a href="https://github.com/znah/tt09-vga-ca/blob/main/src/project.v">project.v</a>)</li>
        <li><a href="https://github.com/znah/tt09">Demo page repository</a></li>
        <li><a href="https://tinytapeout.com/runs/tt09/tt_um_znah_vga_ca">TinyTapeout'09 tile</a></li>
        <li><a href="https://znah.net/tt09-vga-ca/">Circuit 3D view</a></li>
    </ul>

    <iframe id="video" width="560" height="315" src="https://www.youtube-nocookie.com/embed/XtRncOmQN7c?si=HIwZzaOeo9nyTHsK" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>


</div></div>]]></description>
        </item>
    </channel>
</rss>