<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 05 Feb 2024 05:00:08 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Caesars abruptly cancels contract with DEF CON (156 pts)]]></title>
            <link>https://forum.defcon.org/node/248360</link>
            <guid>39256930</guid>
            <pubDate>Mon, 05 Feb 2024 03:22:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://forum.defcon.org/node/248360">https://forum.defcon.org/node/248360</a>, See on <a href="https://news.ycombinator.com/item?id=39256930">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
							
								
								<div> <p><a href="https://filedata/fetch?id=248362&amp;d=1707096806" target="_blank" rel="nofollow"></a><a href="https://forum.defcon.org/filedata/fetch?id=248362&amp;d=1707096806"><img alt="Click image for larger version  Name:	IMG_0685.jpg Views:	0 Size:	91.2 KB ID:	248362" title="IMG_0685.jpg" data-attachmentid="248362" width="600" height="464" data-align="center" src="https://forum.defcon.org/filedata/fetch?id=248362&amp;d=1707096806&amp;type=medium" data-fullsize-url="filedata/fetch?id=248362&amp;d=1707096806" data-thumb-url="filedata/fetch?id=248362&amp;d=1707096806&amp;type=thumb" data-title="Click on the image to see the original version" data-caption="IMG_0685.jpg"></a></p><br>
 </div>    <div>
<p><span><b>DEF CON was canceled. We un-canceled it.</b></span></p></div> <p>


After a great 25 year relationship Caesars abruptly terminated their contract with DEF CON, leaving us with no venue for DC 32, and just about seven months to Con!</p><p>

We don‚Äôt know why Caesars canceled us, they won‚Äôt say beyond it being a strategy change and it is not related to anything that DEF CON or our community has done. This kind of no-notice cancellation of a contract is unheard of in the conference business. The parting is confusing, but amicable.</p><p>

So now we have a challenge. Without a venue, will we be able to UN-Cancel DEF CON 32 before time runs out?</p><p>

Hackers are flexible. We find solutions. We need a space that can handle an event our size, and configurable enough to accommodate our content. We need a location close to our announced dates, and with super short notice... No small feat.</p><p>

We immediately scrambled a venue strike team to Las Vegas. Floors were walked. Meetings were held. Hands were shook and options weighed. When the smoke cleared, the field narrowed to one obvious choice and we began forging the requisite agreements.</p><p>

W00T! DEF CON Is UN-CANCELED!</p><p>

DEF CON 32 will still be August 8-11 2024, but now held at the Las Vegas Convention Center (LVCC) with workshops and training at the Sahara.</p><p>

DEF CON 32 will be an adventure where we can try things not possible in our old Casino Hotel spaces. What specifically you ask? Well we are still learning all the specifics but we will have more space, a proper food court, and the largest indoor venue LCD wall in the country.<br>
There are still many questions to be answered, and we have started a live FAQ section on the Forums for DEF CON 32 where we will be updating questions and answers. The initial FAQ is located here: <a href="https://forum.defcon.org/node/248358" target="_blank">https://forum.defcon.org/node/248358</a></p><p>

I look forward to seeing everyone this summer, the start of a new DEF CON era!</p><p>

The Dark Tangent</p><p>

P.S. We made shirts and stickers:<br>
<a href="https://shop.defcon.org/products/def-con-un-canceled-mens-t-shirt" target="_blank">https://shop.defcon.org/products/def...d-mens-t-shirt</a><br>
<a href="https://shop.defcon.org/products/def-con-un-canceled-sticker-set" target="_blank">https://shop.defcon.org/products/def...ed-sticker-set</a></p></div><div><p>
		
		
			Last edited by <a href="https://forum.defcon.org/member/6-dark-tangent" data-vbnamecard="6">The Dark Tangent</a>; <span>1 hour ago</span>.
		
		

	</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Netflix: Piracy is difficult to compete against and growing rapidly (170 pts)]]></title>
            <link>https://torrentfreak.com/netflix-piracy-is-difficult-to-compete-against-and-growing-rapidly-240204/</link>
            <guid>39254807</guid>
            <pubDate>Sun, 04 Feb 2024 21:44:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://torrentfreak.com/netflix-piracy-is-difficult-to-compete-against-and-growing-rapidly-240204/">https://torrentfreak.com/netflix-piracy-is-difficult-to-compete-against-and-growing-rapidly-240204/</a>, See on <a href="https://news.ycombinator.com/item?id=39254807">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>

<span property="itemListElement" typeof="ListItem"><a property="item" typeof="WebPage" title="Go to TorrentFreak." href="https://torrentfreak.com/"><span property="name">Home</span></a><meta property="position" content="1"></span> &gt; <span property="itemListElement" typeof="ListItem"><a property="item" typeof="WebPage" title="Go to the Piracy category archives." href="https://torrentfreak.com/category/piracy/"><span property="name">Piracy</span></a><meta property="position" content="2"></span> &gt; <span></span>
</p>
<p>
<span> </span>
As a member of ACE and the MPA, Netflix is at the frontline of the global battle against online piracy. The company doesn't often address the subject directly but in a recent SEC filing, Netflix writes that it's difficult to compete against the free entertainment piracy offers. Not only that, it's growing rapidly too.
</p>
</div><div>
<p><img decoding="async" src="https://torrentfreak.com/images/netflix-logo-1.jpg" alt="netflix logo" width="300" height="180" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20180'%3E%3C/svg%3E" data-lazy-src="https://torrentfreak.com/images/netflix-logo-1.jpg">From the launch of its online streaming service fifteen years ago, Netflix positioned itself as a piracy competitor.</p>
<p>The idea was to take market share away from piracy sites, by offering a legal and more convenient streaming platform.</p>
<p>Initially, this seemed to work. Netflix amassed hundreds of millions of subscribers, some of whom left their piracy habits behind. However, as the ‚Äòstreaming wars‚Äô turned legal and convenient streaming platforms into isolated and pricey content silos, momentum started to shift. </p>
<p>In recent years piracy <a href="https://torrentfreak.com/canada-is-a-video-piracy-hotspot-while-brazil-shows-positive-signs-240121/">started to grow again</a>, including in well-served markets such as the United States. In theory, this <a href="https://torrentfreak.com/could-piracy-help-netflix-win-the-streaming-wars-240108/">may help Netflix</a> in its battle with other legal platforms, but that‚Äôs a consolation prize if the war against piracy is lost. </p>
<p>There are no concrete signs that Netflix is crumbling, but piracy is a concern. This <a href="https://torrentfreak.com/netflix-sees-popcorn-time-as-a-serious-competitor-150121/">isn‚Äôt breaking news</a>; piracy has been repeatedly highlighted as tough competition in the company‚Äôs <a href="https://www.investopedia.com/terms/1/10-k.asp">10-K filings</a> at the SEC.</p>
<h2>Piracy is a Tough Competitor</h2>
<p>Earlier this week, Netflix submitted its latest 10-K filing. The mandatory document provides information that helps investors to gather key information about publicly traded companies. In the ‚Äúcompetition‚Äù section of the annual overview, piracy is again mentioned several times.</p>
<center><img decoding="async" src="https://torrentfreak.com/images/sec-netflix.jpg" alt="sec netflix" width="600" height="366" srcset="https://torrentfreak.com/images/sec-netflix.jpg 1166w, https://torrentfreak.com/images/sec-netflix-300x183.jpg 300w" sizes="(max-width: 600px) 100vw, 600px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20600%20366'%3E%3C/svg%3E" data-lazy-srcset="https://torrentfreak.com/images/sec-netflix.jpg 1166w, https://torrentfreak.com/images/sec-netflix-300x183.jpg 300w" data-lazy-src="https://torrentfreak.com/images/sec-netflix.jpg"></center>
<p>Netflix explains that the online video landscape is a competitive business. New services and distribution models could impact the business of the leading video streaming platform. This includes legal competitors as well as piracy. </p>
<p>‚ÄúThe various economic models underlying these channels include subscription, transactional, ad-supported and piracy-based models. All of these have the potential to capture meaningful segments of the entertainment video market,‚Äù Netflix writes. </p>
<p>These are in part standard disclosures, as every company faces competition. However, Netflix believes that online piracy is particularly compelling because it‚Äôs free for consumers. That makes it very hard to compete against. </p>
<p>‚ÄúPiracy also threatens to damage our business, as its fundamental proposition to consumers is so compelling and difficult to compete against: virtually all content for free,‚Äù Netflix writes.</p>
<h2>Growing and Hard to Stop</h2>
<p>When Netflix launched, its on-demand streaming experience was more convenient than most pirate sites. At the time, torrent sites were dominant but still required users to have some technical knowledge and the patience to wait for content to download.</p>
<p>Today, most pirate sites use on-demand streaming, taking away a major edge for Netflix. And because piracy is so compelling for consumers, it is growing rapidly worldwide, threatening legal services. </p>
<p>‚ÄúIn light of the compelling consumer proposition, piracy services are subject to rapid global growth, and our efforts to prevent that growth may be insufficient,‚Äù Netflix notes. </p>
<p>‚ÄúIf we are unable to successfully or profitably compete with current and new competitors, our business will be adversely affected, and we may not be able to increase or maintain market share, revenues or profitability.‚Äù</p>
<h2>(Un)authorized Copying?</h2>
<p>The concerns voiced by Netflix are real, but the company isn‚Äôt near its demise. These 10-K filings are supposed to detail risks and Netflix is not the only company mentioning piracy as a potential threat. </p>
<center><strong>A Netflix Competitor</strong></center><br><center><img decoding="async" src="https://torrentfreak.com/images/netflix-compet.jpg" alt="netflix competitor" width="600" height="464" srcset="https://torrentfreak.com/images/netflix-compet.jpg 1342w, https://torrentfreak.com/images/netflix-compet-300x232.jpg 300w" sizes="(max-width: 600px) 100vw, 600px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20600%20464'%3E%3C/svg%3E" data-lazy-srcset="https://torrentfreak.com/images/netflix-compet.jpg 1342w, https://torrentfreak.com/images/netflix-compet-300x232.jpg 300w" data-lazy-src="https://torrentfreak.com/images/netflix-compet.jpg"></center>
<p>When we started looking for similar mentions by other businesses, we stumbled upon similar concerns and, strangely enough, some identical ones. Apparently, there‚Äôs quite a bit of copying going on, as SEC filings from several companies include identical passages.</p>
<p><em><a href="https://d18rn0p25nwr6d.cloudfront.net/CIK-0001065280/c5e64982-659f-4726-97c9-c57767c3bec3.pdf">Netflix</a>: ‚ÄúIn light of the compelling consumer proposition, piracy services are subject to rapid global growth‚Äù</em></p><p><em>
<p><a href="https://www.sec.gov/Archives/edgar/data/1936037/000119312524007757/d356530ds1a.htm">Triller Corp</a>: ‚ÄúIn light of the compelling consumer proposition, piracy services are subject to rapid global growth‚Äù</p>
<p><a href="https://www.sec.gov/Archives/edgar/data/1484769/000162828023005135/fubo-20221231.htm">FuboTV</a>: ‚ÄúIn light of the compelling consumer proposition, piracy services are subject to rapid global growth‚Äù</p>
<p><a href="https://ir.cssentertainment.com/node/11811/html">Redbox Entertainment</a>: ‚ÄúIn light of the compelling consumer proposition, piracy services are subject to rapid global growth‚Äù</p>
<p><a href="https://finance.yahoo.com/sec-filing/IMAQ/0001654954-23-010817_1846235?guccounter=1">IMAQ</a>: ‚ÄúIn light of the compelling consumer proposition, piracy services are subject to rapid global growth‚Äù</p>
</em></p><p><em><a href="https://www.sec.gov/Archives/edgar/data/1776909/000121390020035136/f424b31120_curiositystream.htm">CuriosityStream</a>: ‚ÄúIn light of the compelling consumer proposition, piracy services are subject to rapid global growth‚Äù</em></p>
<p>We don‚Äôt know where these references originate. Netflix has mentioned it for a while, that‚Äôs for sure, and apparently, the use of this language is widespread and subject to rapid global growth.</p>
<p>It‚Äôs clear, however, that piracy is a concern for Netflix. While Reed Hastings <a href="https://torrentfreak.com/netflix-uses-pirate-sites-to-determine-what-shows-to-buy-130914/">wasn‚Äôt worried about piracy</a> a decade ago, the company now spends millions of dollars tackling the problem. </p>
<p>The streaming giant <a href="https://torrentfreak.com/netflix-becomes-a-member-of-the-mpaa/">joined the MPA</a> a few years ago and is also a <a href="https://torrentfreak.com/mpaa-dramatically-expanding-ace-global-anti-piracy-coalition-190507/">member of anti-piracy coalition ACE</a>. In addition, Netflix also has an <a href="https://torrentfreak.com/netflix-continues-to-expand-its-global-anti-piracy-team-220307">in-house anti-piracy department</a> that keeps an eye on piracy threats.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stract: Open-souce, non-profit search engine (197 pts)]]></title>
            <link>https://stract.com/</link>
            <guid>39254172</guid>
            <pubDate>Sun, 04 Feb 2024 20:36:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stract.com/">https://stract.com/</a>, See on <a href="https://news.ycombinator.com/item?id=39254172">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Customise your search with an  <a href="https://stract.com/settings/optics" data-svelte-h="svelte-1prlvom">optic</a>:</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sapling: Experimental vi-inspired editor where you edit code, not text (101 pts)]]></title>
            <link>https://github.com/kneasle/sapling</link>
            <guid>39253798</guid>
            <pubDate>Sun, 04 Feb 2024 20:01:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/kneasle/sapling">https://github.com/kneasle/sapling</a>, See on <a href="https://news.ycombinator.com/item?id=39253798">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:kneasle/sapling" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="Fsrp2c6q423zz6YqE3COJ13hI1gxgZJF7u1vW0MznVOU3CfAiahsOiJGTwC0rg9BW4yt5wuwFh4xtYWYtITvjA" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="kneasle/sapling" data-current-org="" data-current-owner="kneasle" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=eIDiJxiRn9X4Ldpmp%2FYINDNuwHLz1I8s2NIaxvfFgi%2F%2FniPwb2VIgOMzeCXS8YpMtM%2BxzMsBWlAHPXcxZhZcOq0eC3q%2Bymk%2FPbNphvk84Nb1EFTy87QL2MXuqfzMkLJPVYWM9CutROcLSZwBR0TS5Ln%2FWqy%2FlLgOmlijy0J1KTv5IiRKsEyv0gTx15ANtPf1jc%2FxuHsAspCdzeiVadX2VN%2Fg6Y0JlYjmz6gvW47YkYpMUGdB5nbDlLxd82dOUw9g0ZxlA7%2B3Pb7wW%2FbM6NjbRt4RXlbIdmNJwsOgAi0g9YGOLnvK6AOBWn1Qh2bXvifCt05JuYAKAVAouse1KJ2%2BWO5BvYmlvHSUqgIslLmQ4itFpomkHftLyHUBlJlU0Q3Ar9wjt0nlI9dtSFlvkZpNaRgn9nhiQkj5%2FISgU0uuNrSZL3BEdYfTGQyMcnX7wj2wRVaE1XE6xJpzh5xH%2Fn9%2FLjr9glF0JeFy%2FRCO30YLvQCJ6pBOXQm3raXBKBl28UYg2PyxtFfPvfFgZA%3D%3D--6hPzDRoQuNTrC3ih--Q53Zv4sllbX0E3fLdnEPQw%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=kneasle%2Fsapling" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/kneasle/sapling&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="5b16bce859c90f213c6c1a71abc030b2573635ddcd37619222be5d27b402d63f" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why G√∂del, Escher, Bach is the most influential book in my life (280 pts)]]></title>
            <link>https://philosophygeek.medium.com/why-g%C3%B6del-escher-bach-is-the-most-influential-book-in-my-life-49d785a4e428</link>
            <guid>39253099</guid>
            <pubDate>Sun, 04 Feb 2024 18:48:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://philosophygeek.medium.com/why-g%C3%B6del-escher-bach-is-the-most-influential-book-in-my-life-49d785a4e428">https://philosophygeek.medium.com/why-g%C3%B6del-escher-bach-is-the-most-influential-book-in-my-life-49d785a4e428</a>, See on <a href="https://news.ycombinator.com/item?id=39253099">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a rel="noopener follow" href="https://philosophygeek.medium.com/?source=post_page-----49d785a4e428--------------------------------"><div aria-hidden="false"><p><img alt="Mark Johnson" src="https://miro.medium.com/v2/resize:fill:88:88/1*ivAx6b9Z9gnoFsJceGMkFQ.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div><p id="2061"><a href="https://www.amazon.com/G%C3%B6del-Escher-Bach-Eternal-Golden/dp/0465026567" rel="noopener ugc nofollow" target="_blank"><em>G√∂del, Escher, Bach: An Eternal Golden Braid</em></a><em> </em>(henceforth: GEB), the Pulitzer Prize winning book written in 1978 by Douglas Hofstadter, is described in its cryptic tagline as ‚Äúa metaphorical fugue on minds and machines in the spirit of Lewis Carroll.‚Äù</p><p id="9ab3">I recently reread GEB and got fired up by how brilliantly Hofstadter fuses computation, epistemology, and consciousness. After failed attempts at explaining the book to three of my smartest friends, I decided to write something up.</p><p id="7f72">The problem is that a simple reduction like ‚ÄúGEB is about how complex systems arise from simpler systems‚Äù is akin to describing <em>Ulysses</em> as ‚Äúa day in the life of Leopold Bloom.‚Äù More detailed descriptions run the risk of diving into the depth that‚Äôs only understandable after having read the book.</p><p id="ca55">This post is a more modest attempt to explain to myself why GEB is important, and focuses on three mental models that have profoundly affected my life: <strong>epistemic limits</strong>,<strong> self-reference</strong>, and <strong>isomorphism</strong>.</p><p id="b371">If it causes you to read or reread it, then all the better.</p><p id="00df">Here we go!</p><figure><figcaption>Kurt and Albert, hanging out at Princeton.</figcaption></figure><p id="d440">The main character of the book is <a href="https://en.wikipedia.org/wiki/Kurt_G%C3%B6del" rel="noopener ugc nofollow" target="_blank">Kurt G√∂del</a>, the most important person in the 20th century you‚Äôve never heard of. G√∂del is the kind of guy that shows up to <a href="https://en.wikipedia.org/wiki/Albert_Einstein" rel="noopener ugc nofollow" target="_blank">his buddy‚Äôs</a> 70th birthday with <a href="https://en.wikipedia.org/wiki/G%C3%B6del_metric" rel="noopener ugc nofollow" target="_blank">an exact solution to the Einstein field equations</a> as a present. Despite being the greatest mathematician of his generation, he wasn‚Äôt stuffy in the least: his favorite movie was Snow White and the Seven Dwarves.</p><p id="b348">G√∂del is most famous for his Incompleteness Theorems, which established limits on mathematics. For the first chunk of the 20th century, mathematicians were obsessed with formalizing mathematics and then proving meta-theorems <em>about</em> those formal systems. In particular, there was a strongly-held belief that for any well-formed formula (a ‚Äúgrammatically correct‚Äù statement in math, e.g., A=B is well-formed whereas AA==+B is not), you could use mathematics to <em>decide</em> whether it was true or false.</p><p id="909f">If you think about it for a second, this makes perfect sense: it <em>seems</em> like you should be able to determine whether any statement is true or false.</p><p id="3010">Nope! G√∂del proved in 1931 that mathematics is not decidable, an earth-shattering result. He proved that there are statements in mathematics, which are <em>true but not provable</em> within the system. Worse yet, it turns out that you can‚Äôt build a more powerful mathematical system. Once a system becomes sufficiently complex, there will always be statements which are undecidable. You‚Äôre left with a choice: either have weak system of mathematics or accept that there will always be theorems out of reach. A rough analogy to incompleteness Heisenberg‚Äôs Uncertainty Principle, which shows that physics makes it impossible to determine <em>both</em> the position and velocity of a particle with exact precision.</p><p id="fa72">Wouldn‚Äôt it be nice if every question had an answer? That‚Äôs a lovely fantasy, but G√∂del shows that there are <strong>fundamental epistemic limits to the universe</strong>, things that no genius will help us to know, no alien race could teach us, no machine could be built to solve, and no new kinds of mathematics will uncover. How frustrating.</p><p id="167d">A key feature of powerful mathematical systems (or perhaps, any system that generates complexity‚Ä¶) is that they involve <strong>self-reference</strong>, that is, they contain ways of talking about themselves. ‚ÄúThis sentence is true‚Äù is an example. Because self-referential systems can manipulate and talk about themselves, they systems are very powerful and immediately run into fun paradoxes. Is the statement ‚ÄúThis sentence is false.‚Äù true or false? Either way, it doesn‚Äôt end well.</p><p id="c684">A third major theme of the book is <strong>isomorphism</strong>, which is unique to Hofstadter‚Äôs vernacular. In formal mathematics, ‚Äúisomorphism‚Äù takes on a version of ‚Äúequivalence.‚Äù For example, it turns out that many different formalizations of mathematics are provably isomorphic, like Turing Machines, arithmetic, set theory, and formal logic. Hofstadter deliberately uses the term more loosely to describe two systems that are structurally similar. I find this quite useful because it forces one to define the structures of the system, why they are similar, and why other parts of the system are less important. We might describe the way that planets fly around stars as <em>isomorphic</em> to the way that electrons fly around nuclei.</p><figure><figcaption>Escher‚Äôs famous Drawing Hands.</figcaption></figure><p id="8a61">The two minor characters, <a href="https://mcescher.com/" rel="noopener ugc nofollow" target="_blank">M.C.Escher</a> and <a href="https://en.wikipedia.org/wiki/Johann_Sebastian_Bach" rel="noopener ugc nofollow" target="_blank">Johann Sebastian Bach</a>, are reflections of G√∂del in art and both liberally use self reference. Escher draws pictures of hands drawing hands (!) and water ‚Äúfalling‚Äù in an infinite loop. His images don‚Äôt just play tricks on the eye, they force paradoxical conclusions, regardless of your angle of interpretation. On the musical side, Papa Bach was most famous for his complex fugues, which are basically the same melody played on top of each other. Common versions of this you might have sung as a child are ‚ÄúRow, row, row your boat‚Äù and ‚ÄúFr√®re Jacques.‚Äù Both Escher and Bach are woven into the story (like a fugue?), providing tangible examples to the more abstruse mathematical concepts.</p><figure><figcaption>A playlist full of Bach, fugues, and other tracks referenced in GEB.</figcaption></figure><p id="b898">Perhaps the most astonishing part of the book is the quality of the writing itself. Each chapter begins with a clever dialog between Achilles and the Tortoise (inspired by Lewis Carroll), and a few of their anthropomorphic friends. They deal with a whole range of bizarre situations, like record player so powerful that it can play any record (including a record that can destroy the record player) and asking a Djinn for a meta-wish (‚ÄúI wish for 5 more wishes‚Äù). Hofstadter‚Äôs greatest achievement is his palindromic Crab Canon in Chapter VII, which is a dialog that can be read backwards and forwards. Of course, these aren‚Äôt just cute dialogs: each is isomorphic to the themes in the following chapter. Oftentimes a dialog is a more understandable exposition of the chapter‚Äôs theme than the chapter itself.</p><p id="440f">And, naturally in a book about self-reference, GEB itself is highly self-referential. Themes are often resolved hundreds of pages later and require going back to appreciate fully the depth of Hofstadter‚Äôs argument. Mercifully, he‚Äôs a gifted and lucid writer so, even though there are chapters that are dense, it‚Äôs always tractable to read.</p><p id="5afb">After 742 pages and even after having written the paragraphs above, I still struggle with a simple answer to the question: ‚ÄúWhat is this book about?‚Äù The best I can come up with is that GEB equips you with mental models to contemplate philosophy.</p><p id="04c7">So to end, a few personal examples about how GEB has influenced my own thinking.</p><p id="1696"><a rel="noopener" href="https://philosophygeek.medium.com/moving-from-com-to-org-34150bea9ade">I recently joined Stand Together</a>, who shares my strong belief in <strong>bottom up solutions</strong>. Perhaps the idea that bottom up solutions are better isn‚Äôt just an empirical statement of sociology, but fundamental to the nature of complex systems. Indeed, Hofstadter goes through many examples of how complexity emerges from simpler systems, often which look nothing like the higher-level systems. Consciousness itself doesn‚Äôt exist in neurons, and yet neurons as a system create consciousness in humans (this is critical to Hoftstadter‚Äôs argument that machines can think). There‚Äôs also a fantastical example in a dialog with the Anteater, who has conversations with Aunt Hilary, an ant colony. She is perfectly capable of having a robust conversation with the Anteater, powered by the ants in the colony. Of course, the ants themselves are individuals with their own cares and concerns and has no knowledge of the emergent intelligence, much like Aunt Hilary has no knowledge of her inner workings.</p><p id="0ddf">How DNA expresses as proteins, how the brain functions at multiple levels, how we understand and use words, how programs don‚Äôt have access to the underlying transistors, how Aunt Hilary doesn‚Äôt know what the ants are doing‚Ä¶ all of these are a set of isomorphisms that suggest that bottom up is better than top down. An additional tenet of Stand Together is ‚Äúbelieve in people,‚Äù which means that the smallest units act intelligently. Like ants or neurons, we make local decisions every day that bubble up into the structure of society, without anyone telling us what to do.</p><p id="fbbe">The idea that epistemic limits exist in something as universal as mathematics has humbled me about the limits of knowledge for complex human systems. Utopian thought experiments often generate useful frames of exploration, but ought not be confused with reality. Utopians often try to pull out the ‚Äúbugs‚Äù from human systems, often which are endemic to the system, or ‚Äúfeatures,‚Äù as we might say in the trade. Bugs might not be desirable, but sometimes, <em>the bugs can‚Äôt just be ripped out of the system without destroying the system itself</em>. Our time would be better spent figuring out‚Äîwithin the system‚Äîoptimize for minimizing the downsides of the ‚Äúbugs‚Äù while maximizing the value of the features. Think about this with respect to capitalism, socialism, and communism‚Ä¶</p><p id="c7f8">A final area where GEB has influenced me is in designing software products. Hugh Dubberly has been my collaborator for years, starting off with our deep dive into cybernetics, the study of feedback loops. We believe that iteration is key to quality; perfection is impossible out of the gate. Further, the system used to generate quality software is a series of feedback loops between customers and the company, product and engineering, and so on. Though specific product frameworks have changed over the years, that obsession with iteration and feedback permeates everything I‚Äôve implemented.</p><p id="ea19">My modest goal in writing this post was to have something I could send to a friend, rather than to spend an hour fumbling a feeble explanation of <em>G√∂del, Escher, Bach</em>. I had a secondary goal in the back of my head‚Ä¶ if you have a copy of GEB on your shelf collecting dust and you‚Äôve never read more than a chapter or two, dust it off and see how it goes this time.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ProofWiki: Online compendium of mathematical proofs (106 pts)]]></title>
            <link>https://proofwiki.org/wiki/Main_Page</link>
            <guid>39252531</guid>
            <pubDate>Sun, 04 Feb 2024 17:51:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://proofwiki.org/wiki/Main_Page">https://proofwiki.org/wiki/Main_Page</a>, See on <a href="https://news.ycombinator.com/item?id=39252531">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mw-content-text" lang="en" dir="ltr"><h2><span id="Welcome_to_.7F.27.22.60UNIQ-MathJax-1-QINU.60.22.27.7F"></span><span id="Welcome_to_'&quot;`UNIQ-MathJax-1-QINU`&quot;'">
Welcome to $\mathsf{Pr} \infty \mathsf{fWiki}$
</span></h2>
<div><p><a href="https://proofwiki.org/wiki/File:Logo.png"><img alt="Logo.png" src="https://proofwiki.org/w/images/c/c9/Logo.png" decoding="async" width="135" height="135"></a></p>
<p><b>$\mathsf{Pr} \infty \mathsf{fWiki}$</b> is an online compendium of mathematical proofs! Our goal is the collection, collaboration and classification of mathematical proofs. If you are interested in helping create an online resource for math proofs feel free to <b><a href="https://proofwiki.org/wiki/Special:RequestAccount" title="Special:RequestAccount">register for an account</a></b>. Thanks and enjoy!
</p><p>If you have any questions, comments, or suggestions please post on the <b><a href="https://proofwiki.org/wiki/Talk:Main_Page" title="Talk:Main Page">discussion</a></b> page, or contact one of the <span><a rel="nofollow" href="https://www.proofwiki.org/w/index.php?title=Special%3AListUsers&amp;username=&amp;group=sysop&amp;limit=50">administrators</a></span>. Also, feel free to take a look at the <a href="https://proofwiki.org/wiki/Help:FAQ" title="Help:FAQ">frequently asked questions</a> because you may not be the first with your idea.
</p><p>To see what's currently happening in the community, visit the <b><a href="https://proofwiki.org/wiki/ProofWiki:Community_Portal" title="ProofWiki:Community Portal"> community portal</a></b>.
</p>
</div>
<center><big><a href="https://proofwiki.org/wiki/Category:Proofs" title="Category:Proofs">26,727 Proofs</a> <b>‚Äî</b> <a href="https://proofwiki.org/wiki/Category:Definitions" title="Category:Definitions">25,226 Definitions</a> <b>‚Äî</b> <a href="https://proofwiki.org/wiki/Help:Contents" title="Help:Contents">Help</a></big></center>
<p><a href="https://twitter.com/ProofWiki" data-show-count="false">Follow @ProofWiki</a>
</p>
<h2><span id="Featured_Proof">
Featured Proof
</span></h2>


<h2><span id="Theorem">Theorem</span></h2>
<dl><dd>$\ds \sum_{n \mathop = 0}^\infty \frac {F_n} {2^n} = 2$</dd></dl>
<p>where $F_n$ is the $n$th <a href="https://proofwiki.org/wiki/Definition:Fibonacci_Number" title="Definition:Fibonacci Number">Fibonacci number</a>.
</p>
<h2><span id="Proof">Proof</span></h2>
<p>Let us define a <a href="https://proofwiki.org/wiki/Definition:Sample_Space" title="Definition:Sample Space">sample space</a> which satisfies the <a href="https://proofwiki.org/wiki/Definition:Probability_Measure/Definition_3" title="Definition:Probability Measure/Definition 3">Kolmogorov Axioms</a> such that it is the set of all combinations of <a href="https://proofwiki.org/wiki/Definition:Coin/Coin-Tossing" title="Definition:Coin/Coin-Tossing">flipping</a> a fair <a href="https://proofwiki.org/wiki/Definition:Coin" title="Definition:Coin">coin</a> until you receive two <a href="https://proofwiki.org/wiki/Definition:Coin/Head" title="Definition:Coin/Head">heads</a> in a row.
</p><p>Let $X_n$ be the event of some outcome from <a href="https://proofwiki.org/wiki/Definition:Coin/Coin-Tossing" title="Definition:Coin/Coin-Tossing">flipping</a> $n$ fair <a href="https://proofwiki.org/wiki/Definition:Coin" title="Definition:Coin">coins</a> in a row, then $\Pr(X_n) = \dfrac 1 {2^n}$.
</p><p>In the <a href="https://proofwiki.org/wiki/Definition:Sample_Space" title="Definition:Sample Space">sample space</a> defined above, we now demonstrate that for a given number of flips $n$, there are exactly $F_{n - 1}$ outcomes contained in the <a href="https://proofwiki.org/wiki/Definition:Sample_Space" title="Definition:Sample Space">sample space</a>.
</p>
<h3><span id="Illustration">Illustration</span></h3>
<dl><dd>$\begin{array}{c|c|cc}
n &amp; \map f n &amp; \text {Sample Space}: \Omega \\
\hline
1 &amp; 0 &amp; \text {impossible} \\
2 &amp; 1 &amp; HH \\
3 &amp; 1 &amp; THH \\
4 &amp; 2 &amp; (HTHH), (TTHH) \\
5 &amp; 3 &amp; (THTHH), (HTTHH), (TTTHH) \\
6 &amp; 5 &amp; (HTHTHH), (TTHTHH), (THTTHH), (HTTTHH), (TTTTHH) \\
\hline
\cdots &amp; \cdots &amp; \cdots \\
\hline
n &amp; F_{n - 1} &amp; \cdots \\
\hline
\end{array}$</dd></dl>
<p><br>
Reviewing the illustration above, for any given value of $n$:
</p><p>For <b>ALL</b> combinations displayed in <a href="https://proofwiki.org/wiki/Definition:Matrix/Row" title="Definition:Matrix/Row">row</a> $n$ (that is $\map f n$) , we can place a $T$ in front and that new combination would exist in the <a href="https://proofwiki.org/wiki/Definition:Sample_Space" title="Definition:Sample Space">sample space</a> for $\paren {n + 1}$.
</p><p>For example:
</p>
<dl><dd>$\paren {HTHH}, \paren {TTHH} \to \paren {THTHH}, \paren {TTTHH}$</dd></dl>
<p><br>
However, we also see that for only those combinations starting with a $T$ (that is $\map f {n - 1}$), can we place an $H$ in front and that new combination will also exist in the <a href="https://proofwiki.org/wiki/Definition:Sample_Space" title="Definition:Sample Space">sample space</a> for $\paren {n + 1}$.
</p><p>For example:
</p>
<dl><dd>$\paren {TTHH} \to \paren {HTTHH}$</dd></dl>
<p><br>
Therefore, we have:
</p>
<table>
<tbody><tr>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \map f n\)
</td>
<td>\(=\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds F_{n - 1}\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td></tr>
<tr>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \map f {n + 1}\)
</td>
<td>\(=\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \map f n + \map f {n - 1}\)
</td>
<td>
</td>
<td>
</td>
<td>$\map f n$ is adding a $T$ in front and $\map f {n - 1}$ is adding an $H$ in front
</td>
<td>
</td></tr>
<tr>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \)
</td>
<td>\(=\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds F_{n - 1} + F_{n - 2}\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td></tr>
<tr>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \)
</td>
<td>\(=\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds F_n\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td></tr></tbody></table>
<p>The sum of the probabilities of outcomes in a sample space is one by the second <a href="https://proofwiki.org/wiki/Definition:Probability_Measure/Definition_3" title="Definition:Probability Measure/Definition 3">Kolmogorov Axiom</a>.
</p>
<table>
<tbody><tr>
<td>\((\text {II})\) &nbsp;
</td>
<td>$:$ &nbsp;
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>&nbsp;&nbsp; \(\ds \map \Pr \Omega \)
</td>
<td>&nbsp; \(\ds = \) &nbsp;
</td>
<td>\(\ds 1 \) &nbsp;&nbsp;
</td>
<td>&nbsp;&nbsp;
</td></tr></tbody></table>
<p>Hence:
</p>
<table>
<tbody><tr>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \sum_{n \mathop = 1}^\infty \frac {F_{n - 1} } {2^n}\)
</td>
<td>\(=\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds 1\)
</td>
<td>
</td>
<td>
</td>
<td><a href="https://proofwiki.org/wiki/Definition:Probability_Measure/Definition_3" title="Definition:Probability Measure/Definition 3">$2$nd Kolmogorov Axiom</a>
</td>
<td>
</td></tr>
<tr>
<td>
</td>
<td>
</td>
<td>\(\ds \leadsto \ \ \)
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \sum_{n \mathop = 0}^\infty \frac {F_n} {2^{n + 1} }\)
</td>
<td>\(=\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds 1\)
</td>
<td>
</td>
<td>
</td>
<td>reindexing the sum
</td>
<td>
</td></tr>
<tr>
<td>
</td>
<td>
</td>
<td>\(\ds \leadsto \ \ \)
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \sum_{n \mathop = 0}^\infty \frac {F_n} {2^n}\)
</td>
<td>\(=\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds 2\)
</td>
<td>
</td>
<td>
</td>
<td>multiplying both sides by $2$
</td>
<td>
</td></tr></tbody></table>
<p>$\blacksquare$
</p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple fixes zero-day bug in Apple Vision Pro that 'may have been exploited' (105 pts)]]></title>
            <link>https://techcrunch.com/2024/01/31/apple-vision-pro-zero-day-security-bug-exploited/</link>
            <guid>39252321</guid>
            <pubDate>Sun, 04 Feb 2024 17:32:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/01/31/apple-vision-pro-zero-day-security-bug-exploited/">https://techcrunch.com/2024/01/31/apple-vision-pro-zero-day-security-bug-exploited/</a>, See on <a href="https://news.ycombinator.com/item?id=39252321">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary">A day after reporters published their first hands-on review of Apple‚Äôs Vision Pro, the technology giant released its first security patch for the mixed reality headset to fix a vulnerability that ‚Äúmay have been exploited‚Äù by hackers in the wild.</p>
<p>On Wednesday, Apple released visionOS 1.0.2, the software that runs on the Vision Pro, with a fix for a vulnerability in WebKit, the browser engine that runs Safari and other web apps. Apple said the bug, if exploited, allowed malicious code to run on an affected device.</p>
<p>It‚Äôs the same vulnerability that Apple patched last week when <a href="https://techcrunch.com/2024/01/23/iphone-users-should-turn-on-apples-stolen-device-protection-feature/" target="_blank" rel="noopener">it rolled out iOS 17.3</a>, which included fixes for iPhones, iPads, Macs and Apple TV ‚Äî all of which rely on WebKit. No patches for this bug, <a href="https://support.apple.com/en-us/HT214070" target="_blank" rel="noopener">officially tracked as CVE-2024-23222</a>, were released for Apple Watch.</p>
<p>It‚Äôs not immediately clear if malicious hackers used the vulnerability to specifically exploit Apple‚Äôs Vision Pro, and Apple spokesperson Scott Radcliffe would not say when asked by TechCrunch.</p>
<p>It also isn‚Äôt yet known who was exploiting the vulnerability, or for what reason.</p>
<p>It is not uncommon for malicious actors, such as spyware makers, to target weaknesses in WebKit as a way to break into the device‚Äôs underlying operating system and the user‚Äôs personal data. WebKit bugs can sometimes be exploited when a victim visits a malicious domain in their browser, or the in-app browser.</p>
<p>Apple rolled out several patches for WebKit bugs last year.</p>
<p>Vision Pro is expected to be available starting Friday.</p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How do transformers work? (114 pts)]]></title>
            <link>https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi</link>
            <guid>39252235</guid>
            <pubDate>Sun, 04 Feb 2024 17:24:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi">https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi</a>, See on <a href="https://news.ycombinator.com/item?id=39252235">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><em><span>üëã Hi, this is</span><a href="https://twitter.com/gergelyorosz" rel="nofollow ugc noopener"> </a><span>Venkat and here with a free, full issue of the The ZenMode Engineer Newsletter. In every issue, I cover one topic explained in a simpler terms in areas related to computer technologies and beyond.</span></em></p><p>Transformers have become synonymous with cutting-edge AI, particularly in the realm of natural language processing (NLP).</p><p>But what exactly makes them tick? How do these models navigate the intricacies of language with such remarkable efficiency and accuracy? </p><p>Buckle up, because we're about to  learn the heart of the transformer architecture.</p><p>But.. Before we deep dive into it lets understand where its been used.. if you have used google translate/ ChatGPT both rely on these.</p><blockquote><p><em><strong>Google Translate:</strong><span> This widely used platform relies heavily on transformers to achieve fast and accurate translations across over 100 languages. It considers the entire sentence context, not just individual words, leading to more natural-sounding translations.</span></em></p><p><em><strong>Netflix Recommendation System:</strong><span> Ever wondered how Netflix suggests shows and movies you might enjoy? Transformers analyze your viewing history and other users' data to identify patterns and connections, ultimately recommending content tailored to your preferences.</span></em></p></blockquote><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png" width="727" height="1024" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:727,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><strong>The Big Picture: Encoder and Decoder Dance</strong></p><p>Imagine a factory, but instead of assembling physical objects, it processes language. This factory has two main departments:</p><ol><li><p><strong>The Encoder:</strong><span> This is the information extractor, meticulously dissecting the input text, understanding its individual elements, and uncovering the hidden connections between them.</span></p></li><li><p><strong>The Decoder:</strong><span> Armed with the encoder's insights, the decoder crafts the desired output, be it a translated sentence, a concise summary, or even a brand new poem.</span></p></li></ol><p><strong>Encoder: Decoding the Input Labyrinth</strong></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png" width="255" height="338.1111111111111" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:537,&quot;width&quot;:405,&quot;resizeWidth&quot;:255,&quot;bytes&quot;:167254,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The encoder's journey begins with </span><strong>Input Embedding</strong><span>, where each word is transformed from its textual form into a numerical representation (vector). Think of it as assigning each word a unique identifier.  </span></p><p>Consider this example.</p><ol><li><p><strong>Input Text:</strong><span> The process begins with the raw text sentence, such as "The cat sat on the mat."</span></p></li><li><p><strong>Input Embedding Layer:</strong></p><ul><li><p>This layer acts as a translator, converting each word into a numerical vector.</p></li><li><p>Imagine a large dictionary where each word has a corresponding vector address.</p></li><li><p>These vectors capture various aspects of word meaning:</p><ul><li><p>Semantic relationships (e.g., "cat" is closer to "pet" than "chair").</p></li><li><p>Syntactic roles (e.g., "cat" is often a noun, while "sat" is a verb).</p></li><li><p>Context within the sentence (e.g., "mat" here likely refers to a floor mat).</p></li></ul></li></ul></li><li><p><strong>Vector Representation:</strong></p><ul><li><p>The output of this layer is a sequence of numerical vectors, each representing a word:</p><ul><li><p>"The" -&gt; [0.2, 0.5, -0.1, ...]</p></li><li><p>"cat" -&gt; [0.8, -0.3, 0.4, ...]</p></li><li><p>"sat" -&gt; [-0.1, 0.7, 0.2, ...]</p></li><li><p>...</p></li></ul></li></ul></li></ol><p>But the encoder doesn't stop there. It employs the following key mechanisms to delve deeper:</p><ul><li><p><strong>Self-Attention Layer:</strong><span> This is the game-changer. Imagine shining a spotlight on each word, but instead of illuminating it in isolation, you also highlight how it connects to all other words in the sentence. This allows the encoder to grasp the context, nuances, and relationships within the text, not just the individual words.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif" width="1456" height="876" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:876,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>ref from Raimi Karim blog (used only to refernce)</figcaption></figure></div><p><span>Consider  this example sentence again "</span><em><strong>The quick brown fox jumps over the lazy dog.</strong></em><span>" </span></p><ol><li><p><strong>Word Embeddings:</strong><span> First, each word is transformed into a numerical representation called a "word embedding." Think of it as assigning each word a unique identifier in a giant vocabulary map.</span></p></li><li><p><strong>Query, Key, Value:</strong><span> Next, the Self-Attention mechanism creates three special vectors for each word:</span></p><ul><li><p><strong>Query (Q):</strong><span> This vector asks "What information do I need from other words?"</span></p></li><li><p><strong>Key (K):</strong><span> This vector acts like a label, saying "This is the information I have to offer."</span></p></li><li><p><strong>Value (V):</strong><span> This vector holds the actual information, like the word's meaning and context.</span></p></li></ul></li><li><p><strong>Attention Scores:</strong><span> Now comes the interesting part. The Self-Attention layer compares the Query vector of each word with the Key vectors of all other words in the sentence. </span></p><p><span>This helps it understand how relevant each word is to the current word. Based on this comparison, it calculates an </span><strong>attention score</strong><span> for each pair of words.</span></p><p>Imagine shining a spotlight on each word. The brighter the spotlight on another word, the higher the attention score, meaning the more relevant that word is to the current word.</p></li><li><p><strong>Weighted Values:</strong><span> Finally, the Self-Attention layer uses the attention scores to weigh the Value vectors of all other words. Words with higher attention scores get more weight, contributing more to the final representation of the current word.</span></p><p>Think of it like taking a weighted average of the information from other words, where the weights are determined by how relevant they are.</p></li><li><p><strong>New Word Representation:</strong><span> By considering the context provided by other words, the Self-Attention layer creates a new, enriched representation of each word. This new representation captures not just the word's own meaning, but also how it relates to and is influenced by other words in the sentence.</span></p></li></ol></li><li><p><strong>Multi-Head Attention:</strong><span> This is like having multiple teams of analysts, each focusing on different aspects of the connections between words. It allows the encoder to capture various facets of the relationships, enriching its understanding.</span></p><p><strong>Sentence:</strong><span> "The quick brown fox jumps over the lazy dog."</span></p><ol><li><p><strong>Individual Heads:</strong><span> Instead of one Self-Attention mechanism, Multi-Head Attention uses several independent "heads" (often 4-8). Each head has its own set of Query, Key, and Value vectors for each word.</span></p></li><li><p><strong>Diverse Attention:</strong><span> Each head computes attention scores differently, focusing on various aspects of word relationships:</span></p><ul><li><p>One head might attend to grammatical roles (e.g., "fox" and "jumps").</p></li><li><p>Another might focus on word order (e.g., "the" and "quick").</p></li><li><p>Another might capture synonyms or related concepts (e.g., "quick" and "fast").</p></li></ul></li><li><p><strong>Combining Perspectives:</strong><span> After each head generates its own weighted values, their outputs are concatenated. This combines the diverse insights from different attention mechanisms.</span></p></li><li><p><strong>Final Representation:</strong><span> This combined representation holds a richer understanding of the sentence, incorporating various relationships between words, not just a single focus.</span></p></li></ol></li><li><p><strong>Positional Encoding:</strong><span> Since transformers don't process word order directly, this layer injects information about each word's position in the sentence. It's like giving the analysts a map so they know the order in which to consider the words.</span></p><p>Sure, let's delve into positional encoding using an example sentence:</p><p><strong>Sentence:</strong><span> "The quick brown fox jumps over the lazy dog."</span></p><p><strong>Here's how positional encoding works step-by-step:</strong></p><ol><li><p><strong>Word Embeddings:</strong></p><ul><li><p>Each word ("The", "quick", etc.) is converted into a numerical representation called a word embedding, like a unique identifier in a vast vocabulary map.</p></li><li><p>Imagine these embeddings as vectors:</p><ul><li><p>"The": [0.2, 0.5, -0.1, ...]</p></li><li><p>"quick": [0.8, -0.3, 0.4, ...]</p></li><li><p>"brown": [..., ...]</p></li><li><p>...</p></li></ul></li></ul></li><li><p><strong>Positional Information:</strong></p><ul><li><p>Each word's embedding is combined with additional values based on its position in the sentence.</p></li><li><p>These values are calculated using sine and cosine functions at different frequencies:</p><ul><li><p>Lower frequencies capture long-range dependencies (e.g., "quick" and "fox" are related).</p></li><li><p>Higher frequencies encode short-range relationships (e.g., "jumps" and "over" are close).</p></li></ul></li><li><p>Think of these additional values as "position vectors":</p><ul><li><p>"The": [position 1 vector]</p></li><li><p>"quick": [position 2 vector]</p></li><li><p>"brown": [position 3 vector]</p></li><li><p>...</p></li></ul></li></ul></li><li><p><strong>Combining Embeddings and Positions:</strong></p><ul><li><p>The original word embedding and the position vector are added together, creating a new, enriched representation for each word:</p><ul><li><p>"The": [0.2, 0.5, -0.1, ...] + [position 1 vector] = new enriched embedding</p></li><li><p>"quick": [0.8, -0.3, 0.4, ...] + [position 2 vector] = new enriched embedding</p></li><li><p>"brown": [..., ...] + [position 3 vector] = new enriched embedding</p></li><li><p>...</p></li></ul></li></ul></li><li><p><strong>Understanding Order:</strong></p><ul><li><p>Even if the sentence order changes (e.g., "Dog lazy jumps..."), the position vectors ensure relative positions are maintained.</p></li><li><p>The model can still learn that "jumps" is more related to "over" than, say, "The".</p></li></ul></li></ol></li><li><p><strong>Feed Forward Network(FFN):</strong><span> This adds a layer of non-linearity, enabling the model to learn more complex relationships that might not be easily captured by attention mechanisms alone.</span></p><p>You've already delved into the sentence through previous layers. You understand individual words, their relationships, and their positions. Now, the FFN arrives like a detective magnifying glass, ready to uncover intricate details not immediately visible.</p><p><strong>The FFN does this through three key steps:</strong></p><ol><li><p><strong>Non-linear Transformation:</strong><span> Instead of straightforward calculations, the FFN uses non-linear functions like ReLU to add complexity. Think of it as applying a special filter to the existing information, revealing hidden patterns and connections that simple arithmetic might miss. This allows the FFN to capture more nuanced relationships between words.</span></p></li><li><p><strong>Multi-layered Analysis:</strong><span> The FFN isn't just one step; it's typically a chain of two or more fully connected layers. Each layer builds upon the previous one, transforming the information step-by-step. Imagine you're examining the sentence under increasing magnification, uncovering finer details with each layer.</span></p></li><li><p><strong>Dimensionality Shift:</strong><span> The FFN expands the information's size (e.g., from 512 dimensions to 2048) in the first layer. This allows it to analyze a wider range of features and capture more complex patterns. Think of it as spreading out the information on a larger canvas for deeper examination. Then, it contracts it back to the original size (e.g., 512 again) in the final layer to ensure compatibility with subsequent layers.</span></p></li></ol><p><strong>Applying this to our sentence:</strong></p><ul><li><p>Imagine the FFN helps identify that "quick" and "brown" not only describe the "fox" but also subtly connect to its perceived speed through their combined meaning.</p></li><li><p>Or, it might delve deeper into the relationship between "jumps" and "over," understanding the action and spatial context beyond just their individual definitions.</p></li></ul></li><li><p><strong>Repeat, Refine, Repeat:</strong><span> These layers (self-attention, multi-head attention, etc.) are stacked and repeated multiple times. With each iteration, the encoder refines its understanding, building a comprehensive representation of the input text.</span></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif" width="480" height="322" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:322,&quot;width&quot;:480,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>image source: pillow lab blog</figcaption></figure></div><p><strong>Decoder: Weaving the Output Tapestry</strong></p><p>Now, the decoder takes the baton. But unlike the encoder, it has an additional challenge: generating the output word by word without peeking at the future. To achieve this, it utilizes:</p><ul><li><p><strong>Masked Self-Attention:</strong><span> Similar to the encoder's self-attention, but with a twist. The decoder only attends to previously generated words, ensuring it doesn't cheat and use future information. It's like writing a story one sentence at a time, without knowing how it ends.</span></p></li><li><p><strong>Encoder-Decoder Attention:</strong><span> This mechanism allows the decoder to consult the encoded input, like referring back to a reference document while writing. It ensures the generated output stays coherent and aligned with the original text.</span></p></li><li><p><strong>Multi-Head Attention and Feed Forward Network:</strong><span> Just like the encoder, these layers help the decoder refine its understanding of the context and relationships within the text.</span></p></li><li><p><strong>Output Layer:</strong><span> Finally, the decoder translates its internal representation into the actual output word, one by one. It's like the final assembly line, putting the pieces together to form the desired outcome.</span></p></li></ul><p><strong>Beyond the Basics:</strong></p><p>Remember, this is just a glimpse into the fascinating world of transformers. The specific architecture can vary depending on the task and dataset, with different numbers of layers and configurations. </p><p>Additionally, each layer involves complex mathematical operations that go beyond the scope of this explanation. </p><p>But hopefully, this has equipped you with a fundamental understanding of how transformers work and why they have revolutionized the field of NLP. </p><p>So, the next time you encounter a seamless machine translation or marvel at the creativity of an AI-powered text generator, remember the intricate dance of the encoder and decoder within the transformer, weaving magic with the power of attention and parallel processing.</p><p><em>Paper: https://arxiv.org/abs/1706.03762 </em></p><div data-attrs="{&quot;url&quot;:&quot;https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}" data-component-name="CaptionedButtonToDOM"><p>Thank you for reading The ZenMode. This post is public so feel free to share it.</p><p data-attrs="{&quot;url&quot;:&quot;https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}" data-component-name="ButtonCreateButton"><a href="https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel="nofollow ugc noopener"><span>Share</span></a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beyond self-attention: How a small language model predicts the next token (261 pts)]]></title>
            <link>https://shyam.blog/posts/beyond-self-attention/</link>
            <guid>39251909</guid>
            <pubDate>Sun, 04 Feb 2024 16:54:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://shyam.blog/posts/beyond-self-attention/">https://shyam.blog/posts/beyond-self-attention/</a>, See on <a href="https://news.ycombinator.com/item?id=39251909">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="single"><p><time datetime="2024-01-29 17:25:21 -0700 -0700">Jan 29, 2024</time>
<span>¬∑</span>
<span>17754 words</span>
<span>¬∑</span>
<span>84 minute read</span></p><div><p>I trained a small (~10 million parameter) <a href="https://en.wikipedia.org/wiki/Transformer_%28machine_learning_model%29" target="_blank" rel="noopener">transformer</a> following <a href="https://karpathy.ai/" target="_blank" rel="noopener">Andrej Karpathy</a>‚Äôs excellent tutorial, <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" target="_blank" rel="noopener">Let‚Äôs build GPT: from scratch, in code, spelled out</a>. After getting it working, I wanted to understand, as deeply as possible, what it was doing internally and how it produced its results.</p><p>The <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">original paper</a>, as well every transformer tutorial I found, focuses primarily on <a href="https://machinelearningmastery.com/the-transformer-attention-mechanism/" target="_blank" rel="noopener">multi-head self-attention</a>, the mechanism by which transformers learn multiple relationships between tokens without relying on recurrences or convolution. But none of the papers or tutorials I encountered give a satisfying explanation of what happens <em>after attention</em>: <strong>how exactly do the results of the attention computation turn into accurate predictions for the next token?</strong></p><p>I thought I could run a few example prompts through the small but working transformer I‚Äôd trained, examine the internal states, and figure this out. What I thought would be a quick investigation turned out to be a 6-month deep dive, but yielded some results I think are worth sharing. Specifically, I have a working theory that explains how the transformer produces its predictions and some empirical evidence that suggests this explanation is at least plausible.</p><p>For those readers familiar with transformers and eager for the punchline, here it is: Each transformer block (containing a multi-head self-attention layer and feed-forward network) learns weights that associate a given prompt with a class of strings found in the training corpus. <strong>The distribution of tokens that follow those strings in the training corpus is, approximately, what the block outputs as its predictions for the next token.</strong> Each block may associate the same prompt with a different class of training corpus strings, resulting in a different distribution of next tokens and thus different predictions. The final transformer output is a linear combination of each block‚Äôs predictions.</p><p>I implemented imperative code that does what I‚Äôm proposing the transformer is doing. It produces outputs very similar to the transformer, which I‚Äôll review in detail in a <a href="#evaluating-the-approximation">later section</a>.</p><p>In this post, I‚Äôm going to briefly introduce the model and training data, demo some evidence for my proposed explanation, give a detailed walkthrough of the imperative code implementation of it, and present the supporting evidence I have for my theory. I‚Äôve tried to keep the main narrative succinct, with links to relevant technical details and justifications in the <a href="#appendices">appendices</a> or other notebooks in the <a href="https://shyam.blog/posts/beyond-self-attention/%28https://github.com/spather/transformer-experiments%29">repo</a>.</p><blockquote><p>This project is my first foray into this type of open-ended ML research. I‚Äôm sure I have made errors or omissions that would be obvious to more experienced researchers. I welcome any feedback on this work at <code>shyam.pather at gmail dot com</code>.</p></blockquote><h2 id="the-model-and-setup">The Model and Setup <a href="#the-model-and-setup">üîó</a></h2><blockquote><h3 id="disclaimer">Disclaimer <a href="#disclaimer">üîó</a></h3><p>I want to start by saying upfront: the code for the model I trained isn‚Äôt mine. It came from <a href="https://karpathy.ai/" target="_blank" rel="noopener">Andrej Karpathy</a>‚Äôs video, <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" target="_blank" rel="noopener">Let‚Äôs build GPT: from scratch, in code, spelled out</a> (highly recommend).</p><p>I typed in the code by copying what I saw on the screen as I watched the video. For things that weren‚Äôt clear onscreen, I referenced the <a href="https://github.com/karpathy/ng-video-lecture" target="_blank" rel="noopener">GitHub repo for the video</a> and the <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener">nanoGPT repo</a>. After getting it working, I made only minor changes to make it work with the rest of the code in/structure of <a href="https://github.com/spather/transformer-experiments" target="_blank" rel="noopener">my repository</a>, resulting in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/models/transformer.ipynb" target="_blank" rel="noopener">this implementation</a>. In summary: the core language model is Andrej Karpathy‚Äôs work, not mine. The analysis and all the supporting code behind it are my original contributions. I‚Äôll acknowledge and cite influential papers, posts, tutorials, and other resources in the relevant places.</p></blockquote><h3 id="model-overview">Model Overview <a href="#model-overview">üîó</a></h3><p>The model is a 6-block, decoder-only <a href="https://en.wikipedia.org/wiki/Transformer_%28machine_learning_model%29" target="_blank" rel="noopener">transformer</a>:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/a9f2adc6c1c25ebb263caf42df37f4429c4ed44eda0a0a228cba52b7a00aeb9d.png" alt=""></p><p>It‚Äôs trained on the <a href="https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt" target="_blank" rel="noopener">TinyShakespeare data set</a> which contains 40,000 lines of Shakespeare‚Äôs plays. After about an hour of training on an RTX 4000 GPU, it is able to produce reasonable-looking faux Shakespeare.</p><p>Given a prompt, the model predicts tokens that it thinks should follow. Let‚Äôs look at an example: starting with the prompt, <code>ROMEO:</code>, and sampling 500 tokens from the model‚Äôs predictions, we get:</p><pre tabindex="0"><code>ROMEO:
If thou wilt triumphant be virtue, and since from any
bold virtue that is made a bawd of earth, then the
duke desires of patience and perish:
take up the other husband, dislike his tent
back.

First Citizen:
Ourself goes, go back: you have no consul, but the disguised gods.

Second Citizen:
We choose him in the world, he did runk itself.

First Citizen:
Sir, I am I a man changed him and thriving, I have heard the
king.

CORIOLANUS:
Consider him!

AUFIDIUS:
Most gracious irice, and you must danc
</code></pre><p>It‚Äôs not Shakespeare but structurally, it‚Äôs plausible Shakespeare. It looks like the script for a play, the language sounds archaic, the character names/titles come from real Shakespeare plays. Most of the words are English words. Punctuation and capitalization are mostly sensible. Clearly, none of the text actually makes sense, but still, it‚Äôs not bad for an hour of training.</p><p>The <strong>tokens in the model are characters</strong>, not words. Given a prompt, the model predicts a probability distribution for the next character. For example, given the prompt <code>'my most gr</code>, the model predicts these probabilities for the next token:</p><pre tabindex="0"><code>'a' 0.819
'e' 0.081
'i' 0.059
'o' 0.036
'u' 0.004
'y' 0.001
'w' 0.000
'r' 0.000
'g' 0.000
's' 0.000
</code></pre><p><a href="#i-model-details">Appendix I</a> provides a few more details about the model. Beyond that, if you want to know more, <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/models/transformer.ipynb" target="_blank" rel="noopener">the code</a> and <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" target="_blank" rel="noopener">Andrej‚Äôs video</a> are the best resources.</p><h3 id="transformer-block-structure">Transformer Block Structure <a href="#transformer-block-structure">üîó</a></h3><p>Each of the 6 blocks in the architecture diagram above contains two significant sub-components: a multi-head self-attention layer and a feed-forward network, wired together via a mix of direct and residual connections as follows:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/72a30adc39ebf5f278c0a257fb46f26e6d666d113736e36ce394db587110260c.png" alt=""></p><p>The <code>Block</code> module implements this wiring in PyTorch:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>Block</span>(nn<span>.</span>Module):
</span></span><span><span>    <span>"""One transformer block"""</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> __init__(self, n_embed, n_head):
</span></span><span><span>        super()<span>.</span>__init__()
</span></span><span><span>        head_size <span>=</span> n_embed <span>//</span> n_head
</span></span><span><span>        self<span>.</span>sa <span>=</span> MultiHeadAttention(n_head, head_size)
</span></span><span><span>        self<span>.</span>ffwd <span>=</span> FeedForward(n_embed)
</span></span><span><span>        self<span>.</span>ln1  <span>=</span> nn<span>.</span>LayerNorm(n_embed)
</span></span><span><span>        self<span>.</span>ln2 <span>=</span> nn<span>.</span>LayerNorm(n_embed)
</span></span><span><span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>forward</span>(self, x):
</span></span><span><span>        x <span>=</span> x <span>+</span> self<span>.</span>sa(self<span>.</span>ln1(x)) <span># The `x +` part is a skip connection</span>
</span></span><span><span>        x <span>=</span> x <span>+</span> self<span>.</span>ffwd(self<span>.</span>ln2(x)) <span># The `x +` part is a skip connection</span>
</span></span><span><span>
</span></span><span><span>        <span>return</span> x
</span></span></code></pre></div><p>While many words have been written and spoken about multi-head attention, comparatively little has been said about the feed-forward network because, it seems, comparatively little is known:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/b8214cdd1f6c9466bb984529984c757d780148fb4fe44bfed7714216e12bff73.png" alt=""></p><p>Screenshot from <a href="https://stats.stackexchange.com/q/485910">https://stats.stackexchange.com/q/485910</a></p><p>I started this investigation wondering what comes after attention. Literally, the feed-forward network does. In the transformer I studied, across all 6 blocks, the feed-forward networks comprise over 65% of the total trainable parameters, so they must play some important role.</p><p>As I‚Äôll show <a href="#transformation-via-vector-addition">later</a>, it turns out that the output of the feed-forward network is the primary factor that determines how a block transforms its input into its output.</p><h2 id="demo-my-proposal-in-action">Demo: My Proposal In Action <a href="#demo-my-proposal-in-action">üîó</a></h2><p>In this section, I‚Äôm going to show an example that illustrates what I‚Äôm proposing the transformer is doing. In the next section, I‚Äôll go into detail about how this is implemented.</p><p>Imagine we did the following:</p><ul><li>Ran the prompt, <code>'And only l'</code>, through the model and extracted the output value of the feed-forward network in the first transformer block.</li><li>Went back to the training corpus, found all substrings of the same length as our prompt (10-characters), ran all of them through the model, and filtered out just the ones whose feed-forward network outputs in the first block have a cosine similarity of 0.95 or greater when compared to that of the prompt, <code>'And only l'</code>.</li></ul><p>We‚Äôd come up with this set of strings:</p><pre tabindex="0"><code>'hat only l'    's sickly l'    ' as\nthey l'   'r kingly l'    're; they l'
'eby they l'    'ar, they l'    'im, only l'    'ling any l'    'life may l'
'nobility l'    'e\nBy any l'   ' as they l'    ', if any l'    ' hastily l'
'tly they l'    ' ghastly l'    '\nMy only l'   'For many l'    'r in any l'
' till my l'    'all they l'    'hen they l'    'at Henry l'    'oolishly l'
'er:\nThey l'   'may they l'    'or stony l'    'ur Henry l'    'l gladly l'
'yet they l'    'y;\nDelay l'   'e, on my l'    'or Henry l'    'I dearly l'
' if they l'    ' she may l'    't\nfairly l'   'ould say l'    'd all my l'
'her they l'    ' Stanley l'    ' and may l'    'uld they l'    'u all my l'
'friendly l'    'h gently l'    'e deadly l'    'f all my l'    'n all my l'
'Ere they l'    'steel my l'    ' tell my l'    'e kingly l'    'learn my l'
'd he say l'    't basely l'    'Thursday l'    'iciously l'    " 'if any l"
' as many l'    'hy glory l'    'not very l'    'a goodly l'    'e surely l'
'quiously l'    ', fairly l'    'lord! my l'    'entle my l'    ', he may l'
'our holy l'    ' worldly l'    ' my only l'    ' all, my l'
'ul, they l'    'o lately l'    's in any l'    ' no lady l'
'ter many l'    'Our holy l'    't vainly l'    'e\nA lady l'
' you may l'    'y greedy l'    'untimely l'    'directly l'
'er on my l'    'e wistly l'    'ng Henry l'    'And only l'
's kindly l'    'KE:\nThey l'   ' of many l'    'o, on my l'
</code></pre><p>There‚Äôs a clear pattern across these: they all end in <code>y l</code> and several of them end in <code>ly l</code>. Similarity in the space of feed-forward network outputs seems to correspond to human-interpretable patterns.</p><p>Next, imagine we went back to the training corpus, found each of these strings and built a distribution of all the characters that came after them. We‚Äôd find, for example:</p><ul><li><code>'hat only l'</code> is followed by <code>i</code> (‚ÄúT<code>hat only l</code><strong>i</strong>ke a gulf it did remain‚Äù)</li><li><code>'l gladly l'</code> is followed by <code>e</code> (‚ÄúI‚Äôl<code>l gladly l</code><strong>e</strong>arn.‚Äù)</li><li><code>'n all my l'</code> is followed by both <code>a</code> and <code>i</code> (‚ÄúI<code>n all my l</code><strong>a</strong>nds and leases whatsoever‚Äù and ‚Äúnever saw you before i<code>n all my l</code><strong>i</strong>fe‚Äù)</li></ul><p>Doing this for the complete set of 94 strings, we‚Äôd end up with this distribution:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/170aed320bd4ab2e2647d8d1ef50b499b215ce1905cff1b5db6fe78dd83c3df3.png" alt=""></p><p>The various tokens in our model‚Äôs vocabulary appear on the x-axis and the normalized frequency of occurrence on the y-axis. This plot shows that <code>i</code> was the most frequent, then <code>o</code>, then <code>a</code>, and finally, <code>e</code>.</p><p>Now let‚Äôs look at the final output of the transformer as a whole when given <code>And only l</code> as a prompt:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/08cb64e75759e2a329a0a296931433b06c02c2fb92b6dd0bb440af807cde1d86.png" alt=""></p><p>This is a probability distribution representing the model‚Äôs predictions for the next token. Notice that it‚Äôs strikingly similar to the normalized frequency distribution shown in the previous plot!</p><p>We can quantify how similar they are. <a href="https://en.wikipedia.org/wiki/Hellinger_distance" target="_blank" rel="noopener">Hellinger distance</a> is a measure of overlap between probability distributions. Given distributions \(P\) and \(Q\), the Hellinger distance between them is:</p><p>$$
H(P, Q) = \frac{1}{\sqrt{2}} \sqrt{\sum_{i=1}^n (\sqrt{p_i} - \sqrt{q_i})^2}
$$</p><p>Or, in code:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>hellinger_distance</span>(
</span></span><span><span>    p: torch<span>.</span>Tensor,
</span></span><span><span>    q: torch<span>.</span>Tensor,
</span></span><span><span>):
</span></span><span><span>    <span>return</span> ((p<span>.</span>sqrt() <span>-</span> q<span>.</span>sqrt())<span>**</span><span>2</span>)<span>.</span>sum(dim<span>=-</span><span>1</span>)<span>.</span>sqrt() <span>/</span> math<span>.</span>sqrt(<span>2</span>)
</span></span></code></pre></div><p>Hellinger distance of 0 means the two distributions are identical and 1 means they have no overlap.</p><p>The Hellinger distance between the two distributions above - the distribution formed from the tokens that follow the strings with similar feed-forward network outputs and the distribution the model predicts - is 0.07: very nearly identical.</p><p>For the sake of keeping the demo brief, I chose an example where the first block‚Äôs similar strings alone are enough to produce a distribution that closely matches the final output of the transformer. Typically, we‚Äôd need to need to do the same exercise - finding the strings in the training corpus that produce similar feed-forward network outputs to the prompt and building a distribution from the tokens that succeed them - for all 6 transformer blocks, and then calculate a weighted sum of the resulting distributions in order to get a good match. We‚Äôll do that in the next section and see that <strong>across a sample of 20,000 prompts, the average Hellinger distance between distributions computed this way and the corresponding transformer output was just 0.17</strong>.</p><p>This small average Hellinger distances suggests the results produced by this approach are a good approximation for the transformer‚Äôs outputs. In addition, as I‚Äôll explain in the <a href="#interpretation-why-does-the-approximation-work">interpretation</a> section, I think the approach itself is a reasonable approximation of what the transformer is actually doing.</p><h2 id="implementation-approximating-the-transformer-output-with-feed-forward-network-outputs">Implementation: Approximating the Transformer Output with Feed-forward Network Outputs <a href="#implementation-approximating-the-transformer-output-with-feed-forward-network-outputs">üîó</a></h2><p>In this section, I‚Äôm going to walk through in some detail and with code, the exact procedure I used to approximate the transformer‚Äôs output using strings that produced similar feed-forward network outputs. If you‚Äôre not interested in the implementation, skip this section and proceed to the <a href="#evaluating-the-approximation">evaluation</a> section.</p><p>To recap, this is the procedure to compute the approximation:</p><ol><li>Run a prompt through the model and save the feed-forward network outputs for each block.</li><li>For each block:<ul><li>Find the strings in the training corpus that produce the most similar feed-forward network outputs to the prompt for that block.</li><li>For each string found, build a frequency distribution of the tokens that come after it in the training corpus.</li><li>Sum the frequency distributions for all strings found for the current block.</li></ul></li><li>Compute a weighted sum of the frequency distributions for each block computed in the previous step.</li><li>Normalize the weighted sum to get a probability distribution.</li></ol><h3 id="procedure-setup">Procedure Setup <a href="#procedure-setup">üîó</a></h3><p>The first step of the procedure - running a prompt through the model and saving the feed-forward network outputs for each block - is straightforward to accomplish with some basic PyTorch hooks. But the first part of step two - finding the strings in the training corpus that produce similar feed-forward network outputs - requires some additional machinery to do efficiently.</p><p>I did all the analysis with length 10 strings for compute and storage efficiency (but I also observed that the results hold for both shorter and longer strings). The 1,115,394-character long training corpus contains 858,923 unique, length 10 substrings. Each feed-forward network output is a 384-dimensional vector of <code>float32</code> values and the model produces 6 of them (one for each block). Comparing the 6 384-dimensional feed-forward outputs for any prompt to 6 * 858,923 = 5,153,538 feed-forward outputs from all the other strings takes a long time. To able to work with this data, I had to pre-compute things. I built the following pipeline:</p><ol><li>I chose 20,000 length 10 strings from the training corpus at random to use as prompts in this experiment.</li><li>Overnight, I ran a process to compute the cosine similarity between the feed-forward network outputs the model produced for the 20,000 prompts and those it produced for the 858,923 unique length 10 substrings of the training corpus. I did this in batches and saved the results to disk.</li><li>Even after pre-computing the cosine similarity results, searching through all of them to find the closest matches took a long time. Experiments showed matches of interest never had a cosine similarity below 0.7, so I ran another step to pre-filter the results of step 2 to just those entries with cosine similarity &gt;= 0.7. This greatly reduced the number of entries to search through.</li></ol><p>The code for this pre-computation and pre-filtering is too much to include in this post, but the implementation is available in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/experiments/cosine-sims.ipynb" target="_blank" rel="noopener">the <code>cosine-sims</code> experiment notebook</a>.</p><h3 id="procedure-walkthrough">Procedure Walkthrough <a href="#procedure-walkthrough">üîó</a></h3><p>In this section, we‚Äôll build up the code step by step and run it on one prompt at a time and for just one block. Over the following sections, we‚Äôll extend it to additional blocks, run it across a large number of prompts, and examine the results.</p><p>First, we need to grab 20,000 length 10 strings from the training corpus to use as prompts:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Get all the unique substrings in the text</span>
</span></span><span><span>strings10 <span>=</span> all_unique_substrings(text<span>=</span>ts<span>.</span>text, substring_length<span>=</span><span>10</span>)
</span></span><span><span>
</span></span><span><span>n_prompts <span>=</span> <span>20000</span>
</span></span><span><span>
</span></span><span><span>torch<span>.</span>manual_seed(<span>1337</span>)
</span></span><span><span>indices <span>=</span> torch<span>.</span>randperm(len(strings10))[:n_prompts]
</span></span><span><span>prompts <span>=</span> [strings10[i<span>.</span>item()] <span>for</span> i <span>in</span> indices]
</span></span></code></pre></div><p>As described in the <a href="#procedure-setup">Procedure Setup</a> section, I previously ran all these strings through the model, grabbed the feed-forward network outputs for each block, and pre-computed the cosine similarities to all the unique length 10 substrings in the training corpus. And then I pre-filtered the results to just those with cosine similarity &gt;= 0.7.</p><p>The <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/experiments/cosine-sims.ipynb" target="_blank" rel="noopener">the <code>cosine-sims</code> experiment notebook</a> that implements all this also exports a helper function, <code>filter_on_prefiltered_results()</code>, that we can use to find the most similar strings to a given prompt by searching over the pre-filtered results.</p><blockquote><p>If you‚Äôre curious about how this works, check out the notebook. It‚Äôs pretty straightforward and the unit test provides a simple example that illustrates the shape of the inputs and outputs.</p></blockquote><p>To use <code>filter_on_prefiltered_results()</code>, we just need to tell it how to find the prefiltered files:</p><div><pre tabindex="0"><code data-lang="python"><span><span>prefiltered_threshold<span>=</span><span>0.7</span>
</span></span><span><span>prefiltered_results_folder <span>=</span> environment<span>.</span>data_root <span>/</span> <span>'cosine_sim_results/large_files/slen10'</span> <span>/</span> <span>f</span><span>'prefiltered_</span><span>{</span>prefiltered_threshold<span>}</span><span>'</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>prefiltered_filename</span>(block_idx: int, q_idx: int) <span>-&gt;</span> Path:
</span></span><span><span>    <span>return</span> prefiltered_results_folder <span>/</span> <span>f</span><span>'cosine_sim_ffwd_out_</span><span>{</span>q_idx<span>:</span><span>05d</span><span>}</span><span>_</span><span>{</span>block_idx<span>:</span><span>02d</span><span>}</span><span>.pt'</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>load_prefiltered_data</span>(block_idx: int, q_idx: int):
</span></span><span><span>    <span>return</span> torch<span>.</span>load(prefiltered_filename(block_idx, q_idx))
</span></span></code></pre></div><blockquote><p>Note on the use of <code>q_idx</code> here and in the rest of the code: <code>q_idx</code> refers to ‚Äúquery index‚Äù. The job that pre-computes all the cosine similarities takes a set of ‚Äúqueries‚Äù or values to compare to. These queries are the feed-forward network outputs the model produces for the prompts. There is a 1:1 correspondence between queries and prompts and so I‚Äôve used the terms interchangeably in the code.</p></blockquote><p>To start, we‚Äôll use the same prompt - <code>'And only l'</code> - we used in the earlier demo. It happens to be the prompt at index 57:</p><pre tabindex="0"><code>'And only l'
</code></pre><p>We‚Äôll find the strings whose feed-forward network outputs in block 0 had a cosine similarity of 0.95 or greater when compared to the block 0 feed forward network output of the prompt.</p><div><pre tabindex="0"><code data-lang="python"><span><span>block_idx <span>=</span> <span>0</span>
</span></span><span><span>similarity_threshold<span>=</span><span>0.95</span>
</span></span><span><span>q_idx <span>=</span> <span>57</span>
</span></span><span><span>similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>    load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>    q_idx_start<span>=</span>q_idx,
</span></span><span><span>    q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>    filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_threshold
</span></span><span><span>)
</span></span><span><span>similar_strings <span>=</span> [
</span></span><span><span>    [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>    <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>]
</span></span><span><span>len(similar_strings[<span>0</span>])
</span></span></code></pre></div><pre tabindex="0"><code>94
</code></pre><p>This produced the 94 similar strings we saw in the demo. We can print them again to be sure:</p><div><pre tabindex="0"><code data-lang="python"><span><span>print(<span>f</span><span>"Original string: </span><span>{</span>repr(prompts[q_idx])<span>}</span><span>"</span>)
</span></span><span><span>print(<span>"Similar strings: </span><span>\n</span><span>"</span>)
</span></span><span><span>
</span></span><span><span>data_columns<span>=</span>[
</span></span><span><span>    [repr(s) <span>for</span> s <span>in</span> similar_strings[<span>0</span>][i : i <span>+</span> <span>20</span>]] <span>for</span> i <span>in</span> range(<span>0</span>, len(similar_strings[<span>0</span>]), <span>20</span>)
</span></span><span><span>]
</span></span><span><span>
</span></span><span><span>print(text_table(
</span></span><span><span>    headers<span>=</span>[],
</span></span><span><span>    data_columns<span>=</span>data_columns,
</span></span><span><span>    col_widths<span>=</span>[<span>18</span> <span>for</span> _ <span>in</span> data_columns]
</span></span><span><span>))
</span></span></code></pre></div><pre tabindex="0"><code>Original string: 'And only l'
Similar strings:

'hat only l'      's sickly l'      ' as\nthey l'     'r kingly l'      're; they l'
'eby they l'      'ar, they l'      'im, only l'      'ling any l'      'life may l'
'nobility l'      'e\nBy any l'     ' as they l'      ', if any l'      ' hastily l'
'tly they l'      ' ghastly l'      '\nMy only l'     'For many l'      'r in any l'
' till my l'      'all they l'      'hen they l'      'at Henry l'      'oolishly l'
'er:\nThey l'     'may they l'      'or stony l'      'ur Henry l'      'l gladly l'
'yet they l'      'y;\nDelay l'     'e, on my l'      'or Henry l'      'I dearly l'
' if they l'      ' she may l'      't\nfairly l'     'ould say l'      'd all my l'
'her they l'      ' Stanley l'      ' and may l'      'uld they l'      'u all my l'
'friendly l'      'h gently l'      'e deadly l'      'f all my l'      'n all my l'
'Ere they l'      'steel my l'      ' tell my l'      'e kingly l'      'learn my l'
'd he say l'      't basely l'      'Thursday l'      'iciously l'      " 'if any l"
' as many l'      'hy glory l'      'not very l'      'a goodly l'      'e surely l'
'quiously l'      ', fairly l'      'lord! my l'      'entle my l'      ', he may l'
'our holy l'      ' worldly l'      ' my only l'      ' all, my l'
'ul, they l'      'o lately l'      's in any l'      ' no lady l'
'ter many l'      'Our holy l'      't vainly l'      'e\nA lady l'
' you may l'      'y greedy l'      'untimely l'      'directly l'
'er on my l'      'e wistly l'      'ng Henry l'      'And only l'
's kindly l'      'KE:\nThey l'     ' of many l'      'o, on my l'
</code></pre><p>Next, we‚Äôll need to build a frequency distribution for the tokens that came after these strings in the text. To make this easy and efficient (we‚Äôll eventually be doing many times), we can pre-compute the next token frequency distributions for all the unique length 10 substrings in the training corpus. The helper function <code>build_next_token_map()</code>, implemented in the <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/common/text-analysis.ipynb" target="_blank" rel="noopener">text-analysis module</a>, does this.</p><div><pre tabindex="0"><code data-lang="python"><span><span>next_token_map10 <span>=</span> build_next_token_map(
</span></span><span><span>    text<span>=</span>ts<span>.</span>text,
</span></span><span><span>    prefix_len<span>=</span><span>10</span>,
</span></span><span><span>    vocab_size<span>=</span>tokenizer<span>.</span>vocab_size,
</span></span><span><span>    stoi<span>=</span>tokenizer<span>.</span>stoi
</span></span><span><span>)
</span></span></code></pre></div><p>The return value stored in <code>next_token_map10</code> is a dictionary that maps each unique length 10 substring in the training corpus to a frequency distribution of the tokens that come after it. Conceptually, it looks something like this:</p><div><pre tabindex="0"><code data-lang="python"><span><span>{
</span></span><span><span>    <span>'the common'</span>: {
</span></span><span><span>        <span>' '</span>: <span>12</span>, <span>"'"</span>: <span>1</span>, <span>','</span>: <span>1</span>, <span>'?'</span>: <span>1</span>, <span>'a'</span>: <span>1</span>, <span>'s'</span>: <span>5</span>, <span>'w'</span>: <span>3</span>
</span></span><span><span>    },
</span></span><span><span>    <span>' the gods '</span>: {
</span></span><span><span>        <span>'b'</span>: <span>1</span>, <span>'c'</span>: <span>1</span>, <span>'d'</span>: <span>2</span>, <span>'f'</span>: <span>1</span>, <span>'g'</span>: <span>1</span>, <span>'h'</span>: <span>2</span>, <span>'k'</span>: <span>2</span>, <span>'s'</span>: <span>2</span>, <span>'t'</span>: <span>1</span>, <span>'w'</span>: <span>2</span>
</span></span><span><span>    },
</span></span><span><span>    <span>' authority'</span>: {
</span></span><span><span>        <span>'</span><span>\n</span><span>'</span>: <span>1</span>, <span>' '</span>: <span>5</span>, <span>','</span>: <span>5</span>, <span>':'</span>: <span>2</span>, <span>';'</span>: <span>1</span>
</span></span><span><span>    },
</span></span><span><span>    <span>...</span>
</span></span><span><span>}
</span></span></code></pre></div><p>In reality, the values are actually tensors of shape <code>(vocab_size,)</code> where <code>vocab_size</code> is the number of unique tokens the vocabulary (65, in our case). The item at index <code>i</code> in the tensor is the count of occurrences of the <code>i</code>th token after the string in that entry‚Äôs key. So it looks more like:</p><div><pre tabindex="0"><code data-lang="python"><span><span>{
</span></span><span><span>      <span>'the common'</span>: torch<span>.</span>tensor([
</span></span><span><span>            <span>0</span>, <span>12</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>5</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>3</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>
</span></span><span><span>      ]),
</span></span><span><span>      <span>' the gods '</span>: torch<span>.</span>tensor([
</span></span><span><span>            <span>0</span>, <span>12</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>5</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>3</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>
</span></span><span><span>      ]),
</span></span><span><span>      <span>' authority'</span>: torch<span>.</span>tensor([
</span></span><span><span>          <span>0</span>, <span>12</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>          <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>          <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>          <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>5</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>3</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>
</span></span><span><span>      ]),
</span></span><span><span>    <span>...</span>
</span></span><span><span>}
</span></span></code></pre></div><p>Next, we need to sum the frequency distributions for all the strings we found to have similar feed-forward network outputs to our prompt. Because <code>next_token_map10</code> stores the individual frequency distributions as tensors, this is easy to accomplish:</p><div><pre tabindex="0"><code data-lang="python"><span><span>total_freq_distribution <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    next_token_map10[string] <span>for</span> string <span>in</span> similar_strings[<span>0</span>]
</span></span><span><span>])<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span></code></pre></div><p>We stack up the distributions for each similar string into a single tensor and then sum across all of them. We can now turn this into a probability distribution by dividing each entry by the sum of all the entries:</p><div><pre tabindex="0"><code data-lang="python"><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum()
</span></span></code></pre></div><p>Finally, we can visualize this distribution:</p><div><pre tabindex="0"><code data-lang="python"><span><span>plot_prob_distribution_for_tokens(prob_distribution, title<span>=</span><span>'Probability distribution using only block 0 similar strings'</span>)
</span></span></code></pre></div><p><img src="https://shyam.blog/posts/beyond-self-attention/images/c04c3fbe83a543ea834691f8ef6c5ecdee18522f3e3e456cd9ea81209eb60b00.png" alt=""></p><p>It‚Äôs the same distribution we saw in the demo.</p><p>Now let‚Äôs code the comparison to the model output:</p><div><pre tabindex="0"><code data-lang="python"><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>logits<span>.</span>plot_probs(title<span>=</span><span>'Probability distribution from model'</span>)
</span></span></code></pre></div><p><img src="https://shyam.blog/posts/beyond-self-attention/images/08cb64e75759e2a329a0a296931433b06c02c2fb92b6dd0bb440af807cde1d86.png" alt=""></p><p>Again, the two distributions look very similar, and in this example, the approximation uses only values from the first block. To better compare them, we can look at the distributions in text form:</p><div><pre tabindex="0"><code data-lang="python"><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution, tokenizer<span>.</span>itos)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
i: 0.437            i: 0.389
o: 0.204            o: 0.250
a: 0.195            a: 0.222
e: 0.160            e: 0.139
</code></pre><p>Finally, we can also compare the Hellinger distance between these distributions:</p><div><pre tabindex="0"><code data-lang="python"><span><span>hellinger_distance(prob_distribution, logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor(0.0711)
</code></pre><p>By combining the next token frequency distributions of the similar strings from just the first layer of the model, we are able to pretty closely approximate the output of the transformer. Of course, I chose an example that works particularly well.</p><p>Here‚Äôs an example where the frequency distribution from just the first layer doesn‚Äôt work well:</p><pre tabindex="0"><code>'hing tremb'
</code></pre><p>Using the same method, we can identify 57 strings from the training corpus that produce similar feed-forward network outputs to the prompt:</p><div><pre tabindex="0"><code data-lang="python"><span><span>block_idx <span>=</span> <span>0</span>
</span></span><span><span>similarity_threshold<span>=</span><span>0.95</span>
</span></span><span><span>similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>    load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>    q_idx_start<span>=</span>q_idx,
</span></span><span><span>    q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>    filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_threshold
</span></span><span><span>)
</span></span><span><span>similar_strings <span>=</span> [
</span></span><span><span>    [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>    <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>]
</span></span><span><span>len(similar_strings[<span>0</span>])
</span></span></code></pre></div><pre tabindex="0"><code>57
</code></pre><p>We can look up, sum, and normalize the frequency distributions of tokens that follow these strings in the training corpus, and compare the result to the model outputs, as we did before:</p><div><pre tabindex="0"><code data-lang="python"><span><span>total_freq_distribution <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    next_token_map10[string] <span>for</span> string <span>in</span> similar_strings[<span>0</span>]
</span></span><span><span>])<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum()
</span></span><span><span>
</span></span><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution, tokenizer<span>.</span>itos)
</span></span><span><span>
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
l: 0.999            e: 0.543
e: 0.000            l: 0.343
r: 0.000            r: 0.114
</code></pre><p>Unlike the previous example, these distributions are quite different. The top 3 tokens are the same in each, but they‚Äôre in the wrong order and their probabilities are far apart. These differences contribute to a large Hellinger distance:</p><div><pre tabindex="0"><code data-lang="python"><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>hellinger_distance(prob_distribution, logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor(0.6305)
</code></pre><p>For the prompt, <code>'hing tremb'</code>, just using the values from the first block results in a poor approximation of the transformer‚Äôs output. We‚Äôll soon add the contributions from other blocks and when we do, we‚Äôll get the Hellinger distance between the approximation and the real transformer output for this prompt down from 0.63 to just 0.02.</p><h3 id="similarity-thresholds">Similarity Thresholds <a href="#similarity-thresholds">üîó</a></h3><p>In the preceding examples, I used a similarity threshold of 0.95: I searched for strings whose feed-forward network outputs in block 0 produced values with a cosine similarity of 0.95 or greater when compared to the feed-forward network output of the prompt.</p><p>A different threshold would have yielded different results. For example, doing the same exercise for prompt id 57 (<code>'And only l'</code>) with a threshold of 0.90 finds 612 similar strings, vs the 94 we had before:</p><div><pre tabindex="0"><code data-lang="python"><span><span>block_idx <span>=</span> <span>0</span>
</span></span><span><span>similarity_threshold<span>=</span><span>0.90</span>
</span></span><span><span>q_idx <span>=</span> <span>57</span>
</span></span><span><span>similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>    load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>    q_idx_start<span>=</span>q_idx,
</span></span><span><span>    q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>    filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_threshold
</span></span><span><span>)
</span></span><span><span>similar_strings <span>=</span> [
</span></span><span><span>    [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>    <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>]
</span></span><span><span>len(similar_strings[<span>0</span>])
</span></span></code></pre></div><pre tabindex="0"><code>612
</code></pre><p>If we do the rest of the approximation procedure, we see different (and worse) results:</p><div><pre tabindex="0"><code data-lang="python"><span><span>total_freq_distribution <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    next_token_map10[string] <span>for</span> string <span>in</span> similar_strings[<span>0</span>]
</span></span><span><span>])<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum()
</span></span><span><span>
</span></span><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution, tokenizer<span>.</span>itos)
</span></span><span><span>
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
i: 0.437            o: 0.584
o: 0.204            i: 0.251
a: 0.195            a: 0.095
e: 0.160            e: 0.066
u: 0.004            u: 0.002
l: 0.000            y: 0.001
</code></pre><p>The top 5 tokens are the same, but when ranked by probability, the approximation has a different ordering than the model. The Hellinger distance is also higher:</p><div><pre tabindex="0"><code data-lang="python"><span><span>hellinger_distance(prob_distribution, logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor(0.2856)
</code></pre><p>Loosening the similarity threshold introduced strings into the calculation that resulted in a worse approximation. Tightening beyond 0.95 also produces worse results than we got with 0.95, presumably because we‚Äôre excluding strings that were needed to produce a good approximation:</p><div><pre tabindex="0"><code data-lang="python"><span><span>block_idx <span>=</span> <span>0</span>
</span></span><span><span>similarity_threshold<span>=</span><span>0.97</span>
</span></span><span><span>q_idx <span>=</span> <span>57</span>
</span></span><span><span>similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>    load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>    q_idx_start<span>=</span>q_idx,
</span></span><span><span>    q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>    filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_threshold
</span></span><span><span>)
</span></span><span><span>similar_strings <span>=</span> [
</span></span><span><span>    [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>    <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>]
</span></span><span><span>len(similar_strings[<span>0</span>])
</span></span></code></pre></div><pre tabindex="0"><code>33
</code></pre><div><pre tabindex="0"><code data-lang="python"><span><span>total_freq_distribution <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    next_token_map10[string] <span>for</span> string <span>in</span> similar_strings[<span>0</span>]
</span></span><span><span>])<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum()
</span></span><span><span>
</span></span><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution, tokenizer<span>.</span>itos)
</span></span><span><span>
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
i: 0.437            o: 0.278
o: 0.204            i: 0.250
a: 0.195            a: 0.250
e: 0.160            e: 0.222
</code></pre><div><pre tabindex="0"><code data-lang="python"><span><span>hellinger_distance(prob_distribution, logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor(0.1498)
</code></pre><p>For the first block, 0.95 appears to be a sweet spot. I came up with this threshold through manual tuning: trying different values and binary searching towards one that produced the best results. The full history of this tuning exercise is in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/40_widening_similar_space.ipynb" target="_blank" rel="noopener">the similar space analysis notebook</a>.</p><p>In the end, I found the following thresholds produce the best results for each block:</p><table><thead><tr><th>Block</th><th>Similarity Threshold</th></tr></thead><tbody><tr><td>0</td><td>0.95</td></tr><tr><td>1</td><td>0.94</td></tr><tr><td>2</td><td>0.85</td></tr><tr><td>3</td><td>0.76</td></tr><tr><td>4</td><td>0.81</td></tr><tr><td>5</td><td>0.89</td></tr></tbody></table><blockquote><p>When I first started exploring this space, I assumed the approximation would get better the more similarity I could find. I tried a number of techniques, including experimenting with Euclidean distance vs cosine similarity, searching across strings of different lengths, etc. Every time I succeeded in finding strings with more similar feed-forward network outputs to use in the approximation, the results got worse. I realized that, at least for some blocks, including <em>less</em> similar values in the mix produced better approximations, probably because those blocks had learned to map prompts to broader classes of strings in the training corpus.</p></blockquote><h3 id="going-beyond-the-first-block">Going Beyond the First Block <a href="#going-beyond-the-first-block">üîó</a></h3><p>Thus far, we‚Äôve only considered feed-forward network outputs from the first block. Now we‚Äôll incorporate the contributions from the other blocks.</p><p>First, let‚Äôs find the strings that produce similar feed-forward network outputs in each block, using the similarity thresholds listed above. For now, we‚Äôll do this for just one query (index 57, <code>'And only l'</code>):</p><div><pre tabindex="0"><code data-lang="python"><span><span>similarity_thresholds<span>=</span>[<span>0.95</span>, <span>0.94</span>, <span>0.85</span>, <span>0.76</span>, <span>0.81</span>, <span>0.89</span>]
</span></span><span><span>q_idx <span>=</span> <span>57</span>
</span></span><span><span>
</span></span><span><span>similar_strings_per_block <span>=</span> []
</span></span><span><span>
</span></span><span><span><span>for</span> block_idx <span>in</span> range(n_layer):
</span></span><span><span>    similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>        load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>        q_idx_start<span>=</span>q_idx,
</span></span><span><span>        q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>        filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_thresholds[block_idx]
</span></span><span><span>    )
</span></span><span><span>    similar_strings <span>=</span> [
</span></span><span><span>        [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>        <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>    ]
</span></span><span><span>    similar_strings_per_block<span>.</span>append(similar_strings)
</span></span></code></pre></div><p>Let‚Äôs summarize how many strings we found for each block based on these thresholds:</p><div><pre tabindex="0"><code data-lang="python"><span><span>print(text_table(
</span></span><span><span>    headers<span>=</span>[<span>"Block Index"</span>, <span>"Similarity Threshold"</span>, <span>"# of Similar Strings"</span>],
</span></span><span><span>    data_columns<span>=</span>[
</span></span><span><span>        [<span>f</span><span>"</span><span>{</span>block_idx<span>:</span><span>&gt;10</span><span>}</span><span>"</span> <span>for</span> block_idx <span>in</span> range(n_layer)],
</span></span><span><span>        [<span>f</span><span>"</span><span>{</span>threshold<span>:</span><span>&gt;19</span><span>}</span><span>"</span> <span>for</span> threshold <span>in</span> similarity_thresholds],
</span></span><span><span>        [<span>f</span><span>"</span><span>{</span>len(similar_strings[<span>0</span>])<span>:</span><span>&gt;19</span><span>}</span><span>"</span> <span>for</span> similar_strings <span>in</span> similar_strings_per_block],
</span></span><span><span>    ],
</span></span><span><span>    col_widths<span>=</span>[<span>14</span>, <span>23</span>, <span>23</span>]
</span></span><span><span>))
</span></span></code></pre></div><pre tabindex="0"><code>Block Index   Similarity Threshold   # of Similar Strings
-----------   --------------------   --------------------
         0                   0.95                     94
         1                   0.94                     47
         2                   0.85                     70
         3                   0.76                    108
         4                   0.81                    175
         5                   0.89                   2237
</code></pre><p>Now that we‚Äôve identified the right strings for each block, we can do the next step of the approximation procedure: build the frequency distributions for the tokens that follow those strings, and sum them up. We‚Äôre going to be doing this several times over, so let‚Äôs define a function for it:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>frequency_distribution_from_similar_strings</span>(
</span></span><span><span>    similar_strings_per_block: Sequence[Sequence[Sequence[str]]],
</span></span><span><span>    next_token_map: Dict[str, torch<span>.</span>Tensor],
</span></span><span><span>) <span>-&gt;</span> torch<span>.</span>Tensor:
</span></span><span><span>    <span># freqs_per_block_per_query is a list of lists of tensors. The outer list has</span>
</span></span><span><span>    <span># one item per block. The inner list has one item per query. Each</span>
</span></span><span><span>    <span># tensor is the next token frequency distribution for a particular</span>
</span></span><span><span>    <span># block and query.</span>
</span></span><span><span>    freqs_per_block_per_query: List[List[torch<span>.</span>Tensor]] <span>=</span> [[] <span>for</span> _ <span>in</span> range(n_layer)]
</span></span><span><span>
</span></span><span><span>    <span>for</span> block_idx <span>in</span> range(n_layer):
</span></span><span><span>        <span>for</span> similar_strings <span>in</span> similar_strings_per_block[block_idx]:
</span></span><span><span>            freqs_per_block_per_query[block_idx]<span>.</span>append(
</span></span><span><span>                torch<span>.</span>stack([next_token_map[string] <span>for</span> string <span>in</span> similar_strings])<span>.</span>sum(
</span></span><span><span>                    dim<span>=</span><span>0</span>
</span></span><span><span>                )
</span></span><span><span>            )
</span></span><span><span>
</span></span><span><span>    <span># Stack all frequency tensors into a single tensor of shape</span>
</span></span><span><span>    <span># (n_layer, n_queries, vocab_size)</span>
</span></span><span><span>    freqs <span>=</span> torch<span>.</span>stack(
</span></span><span><span>        [
</span></span><span><span>            torch<span>.</span>stack(freqs_per_block_per_query[block_idx])
</span></span><span><span>            <span>for</span> block_idx <span>in</span> range(n_layer)
</span></span><span><span>        ]
</span></span><span><span>    )
</span></span><span><span>
</span></span><span><span>    <span>return</span> freqs
</span></span></code></pre></div><p>This function, <code>frequency_distribution_from_similar_strings()</code>, does the equivalent of this code we looked at earlier:</p><div><pre tabindex="0"><code data-lang="python"><span><span>total_freq_distribution <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    next_token_map10[string] <span>for</span> string <span>in</span> similar_strings[<span>0</span>]
</span></span><span><span>])<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span></code></pre></div><p>But with two key differences:</p><ul><li>It does this calculation for all the blocks, using the similar strings we found for each block above.</li><li>It allows for more than one query. In the code we‚Äôve looked at so far, we only evaluated the approximation for a single prompt. In the next section, we‚Äôll be running it for lots of prompts so I‚Äôve written the code in a more general form to a allow for this. Specifically, the code allows for <code>similar_strings_per_block</code> to contain not just a single list of strings per block but multiple: one for each query.</li></ul><p>Let‚Äôs run this on the <code>similar_strings_per_block</code> we constructed earlier:</p><div><pre tabindex="0"><code data-lang="python"><span><span>freq_distribution <span>=</span> frequency_distribution_from_similar_strings(
</span></span><span><span>    similar_strings_per_block,
</span></span><span><span>    next_token_map10,
</span></span><span><span>)
</span></span><span><span>freq_distribution<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([6, 1, 65])
</code></pre><p>It produces a tensor of shape <code>(6, 1, 65)</code>: 6 blocks, 1 query, 65 tokens in the vocabulary. If we‚Äôd been working with more queries, the middle dimension would be larger.</p><p>So now we have a frequency distribution for each block, based on the strings found for each block using the similarity thresholds. We now need to turn this into a probability distribution.</p><p>Earlier, when we just had a single frequency distribution for a single block, we just normalized it. But now we have multiple frequency distributions - one for each block - and need to combine them. In my experiments, I found that a weighted sum of these distributions produced the best results.</p><p>As with the similarity thresholds, I was able to find a set of good weights by trial and error. I also tried a deep-learning approach to find weights, but did not get better results than with the hand-tuned approach. The procedure for both hand-tuning and learning weights is implemented in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/40_widening_similar_space.ipynb" target="_blank" rel="noopener">the similar space notebook</a>, the same one used for tuning thresholds.</p><p>For now, let‚Äôs use the optimal weights I found:</p><div><pre tabindex="0"><code data-lang="python"><span><span>weights <span>=</span> torch<span>.</span>tensor([<span>0.01</span>, <span>0.01</span>, <span>0.1</span>, <span>1.5</span>, <span>6</span>, <span>0.01</span>])<span>.</span>unsqueeze(dim<span>=</span><span>1</span>)<span>.</span>unsqueeze(dim<span>=</span><span>2</span>) <span># (n_layer, 1, 1)</span>
</span></span><span><span>total_freq_distribution <span>=</span> (freq_distribution <span>*</span> weights)<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum(dim<span>=-</span><span>1</span>, keepdim<span>=</span><span>True</span>)
</span></span></code></pre></div><p>We multiply the frequency distributions by the weights, sum across all blocks, and then normalize into a probability distribution. We can now look at how the approximation‚Äôs distribution compares to the model‚Äôs.</p><blockquote><p>Note: in the code below, we have to index into the <code>prob_distribution</code> tensor with <code>[0]</code> because its first dimension is the number of queries. We‚Äôre only working with a single query, so we can just take the first element.</p></blockquote><div><pre tabindex="0"><code data-lang="python"><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution[<span>0</span>], tokenizer<span>.</span>itos)
</span></span><span><span>
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
i: 0.437            i: 0.363
o: 0.204            o: 0.265
a: 0.195            a: 0.213
e: 0.160            e: 0.147
u: 0.004            u: 0.011
l: 0.000            y: 0.000
</code></pre><div><pre tabindex="0"><code data-lang="python"><span><span>hellinger_distance(prob_distribution[<span>0</span>], logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor(0.0731)
</code></pre><p>In this particular case, adding the other layers didn‚Äôt change the approximation much (if anything, it‚Äôs very slightly worse based on Hellinger distance). But let‚Äôs look at the example that didn‚Äôt work well when we considered just the first layer: prompt id 40 (<code>'hing tremb'</code>).</p><div><pre tabindex="0"><code data-lang="python"><span><span>similarity_thresholds<span>=</span>[<span>0.95</span>, <span>0.94</span>, <span>0.85</span>, <span>0.76</span>, <span>0.81</span>, <span>0.89</span>]
</span></span><span><span>q_idx <span>=</span> <span>40</span>
</span></span><span><span>
</span></span><span><span>similar_strings_per_block <span>=</span> []
</span></span><span><span>
</span></span><span><span><span>for</span> block_idx <span>in</span> range(n_layer):
</span></span><span><span>    similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>        load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>        q_idx_start<span>=</span>q_idx,
</span></span><span><span>        q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>        filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_thresholds[block_idx]
</span></span><span><span>    )
</span></span><span><span>    similar_strings <span>=</span> [
</span></span><span><span>        [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>        <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>    ]
</span></span><span><span>    similar_strings_per_block<span>.</span>append(similar_strings)
</span></span><span><span>
</span></span><span><span>freq_distribution <span>=</span> frequency_distribution_from_similar_strings(
</span></span><span><span>    similar_strings_per_block,
</span></span><span><span>    next_token_map10,
</span></span><span><span>)
</span></span><span><span>weights <span>=</span> torch<span>.</span>tensor([<span>0.01</span>, <span>0.01</span>, <span>0.1</span>, <span>1.5</span>, <span>6</span>, <span>0.01</span>])<span>.</span>unsqueeze(dim<span>=</span><span>1</span>)<span>.</span>unsqueeze(dim<span>=</span><span>2</span>) <span># (n_layer, 1, 1)</span>
</span></span><span><span>total_freq_distribution <span>=</span> (freq_distribution <span>*</span> weights)<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum(dim<span>=-</span><span>1</span>, keepdim<span>=</span><span>True</span>)
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>
</span></span><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution[<span>0</span>], tokenizer<span>.</span>itos)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
l: 0.999            l: 0.997
e: 0.000            e: 0.002
r: 0.000            r: 0.000
</code></pre><div><pre tabindex="0"><code data-lang="python"><span><span>hellinger_distance(prob_distribution, logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor([0.0233])
</code></pre><p>Remember that for this example, when we used just the first layer‚Äôs similar strings, the approximation was quite different from the model‚Äôs prediction and had a Hellinger distance of &gt;0.63. Now it‚Äôs nearly identical and has a Hellinger distance of 0.02. So using the rest of the layers really helped this example.</p><p>In the next section, we‚Äôll extend the code to evaluate the approximation over the whole set of 20,000 prompts. The section after that will look at how well the approximation does across all the prompts.</p><h3 id="extending-to-all-20000-prompts">Extending to All 20,000 Prompts <a href="#extending-to-all-20000-prompts">üîó</a></h3><p>We now have all the pieces we need to run the approximation procedure for all 20,000 prompts. First, let‚Äôs find the strings with similar feed-forward network outputs for all the prompts, for all blocks:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Takes about 7 minutes to run</span>
</span></span><span><span>
</span></span><span><span>similarity_thresholds<span>=</span>[<span>0.95</span>, <span>0.94</span>, <span>0.85</span>, <span>0.76</span>, <span>0.81</span>, <span>0.89</span>]
</span></span><span><span>
</span></span><span><span>similar_strings_per_block <span>=</span> []
</span></span><span><span>
</span></span><span><span><span>for</span> block_idx <span>in</span> range(n_layer):
</span></span><span><span>    similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>        load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>        q_idx_start<span>=</span><span>0</span>,
</span></span><span><span>        q_idx_end<span>=</span>n_prompts,
</span></span><span><span>        filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_thresholds[block_idx]
</span></span><span><span>    )
</span></span><span><span>    similar_strings <span>=</span> [
</span></span><span><span>        [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>        <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>    ]
</span></span><span><span>    similar_strings_per_block<span>.</span>append(similar_strings)
</span></span></code></pre></div><p>Next, we compute the frequency distributions for each query based on the strings we found, perform the weighted sum, and normalize to produce a probability distribution.</p><div><pre tabindex="0"><code data-lang="python"><span><span>freq_distribution <span>=</span> frequency_distribution_from_similar_strings(
</span></span><span><span>    similar_strings_per_block,
</span></span><span><span>    next_token_map10,
</span></span><span><span>)
</span></span><span><span>weights <span>=</span> torch<span>.</span>tensor([<span>0.01</span>, <span>0.01</span>, <span>0.1</span>, <span>1.5</span>, <span>6</span>, <span>0.01</span>])<span>.</span>unsqueeze(dim<span>=</span><span>1</span>)<span>.</span>unsqueeze(dim<span>=</span><span>2</span>) <span># (n_layer, 1, 1)</span>
</span></span><span><span>total_freq_distribution <span>=</span> (freq_distribution <span>*</span> weights)<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum(dim<span>=-</span><span>1</span>, keepdim<span>=</span><span>True</span>)
</span></span><span><span>prob_distribution<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([20000, 65])
</code></pre><p>The output is a tensor of shape (20000, 65): one 65-entry distribution for each of 20,000 prompts.</p><p>In order to compare, we need to run all the prompts through the model and get the output probability distributions the model predicts:</p><div><pre tabindex="0"><code data-lang="python"><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_strings(prompts)
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>model_probs <span>=</span> logits<span>.</span>probs()
</span></span><span><span>model_probs <span>=</span> model_probs[:, <span>-</span><span>1</span>, :] <span># We're only interested in the last token</span>
</span></span></code></pre></div><p>Now we have outputs from the approximation and from the model for all prompts. In the next section, we‚Äôll measure the Hellinger distance between them and evaluate the results.</p><h2 id="evaluating-the-approximation">Evaluating the Approximation <a href="#evaluating-the-approximation">üîó</a></h2><p>In earlier sections, we compared output from the approximation to output from the model for individual prompts. Now that we have both outputs for all prompts, we can compare them and look at aggregate results.</p><p>First, we can compute the Hellinger distance between the approximation and the model‚Äôs prediction for each prompt:</p><div><pre tabindex="0"><code data-lang="python"><span><span>h <span>=</span> hellinger_distance(prob_distribution, model_probs)
</span></span><span><span>h<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([20000])
</code></pre><p>This produced 20,000 Hellinger distance scores, one for each prompt. We can start by looking at some basic stats:</p><div><pre tabindex="0"><code data-lang="python"><span><span>h<span>.</span>mean(), h<span>.</span>std(), h<span>.</span>min(), h<span>.</span>max()
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(0.1677), tensor(0.1215), tensor(0.0013), tensor(0.9994))
</code></pre><p>The average Hellinger distance is just below 0.17, with a standard deviation of around 0.12, suggesting a distribution that skews low (a good thing). We‚Äôve also got at least one really excellent sample (a min of 0.0013) and at least one really terrible one (max of 0.9994).</p><p>Let‚Äôs look at the distribution:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/e9c543696d0748c74bccfac0780e9e6a5cd7610dafc6e650fb5dab2192fc8399.png" alt=""></p><p>Indeed, the distribution is skewed left, indicating most queries have Hellinger distance scores on the lower end.</p><p>The numbers and the distribution graph look promising, but is the approximation really a good one? It‚Äôs hard to say without something to compare against and it‚Äôs not obvious what a good comparison might be.</p><p>A thought experiment: let‚Äôs imagine that for some prompt, the model produced a distribution that looked like this:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/d01e3755f6278cc6f19ae5656ab3ba6fd7b4ecb59c69303db814b6cc43fb0435.png" alt=""></p><p>The tokens <code>b</code> and <code>d</code> have nearly the same predicted probability (0.49 vs 0.51). The model predicts an approximately equal chance of these tokens coming next. Now imagine our approximation, or another model, predicted this distribution:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/02bddd27aedd082ab84a2b5dd45dacae4e745dc49ed363df5725916e4b370844.png" alt=""></p><p>Nearly the same, but the probabilities are reversed: <code>b</code> has probability 0.51 and <code>d</code> has 0.49. Would we care about this difference? Clearly both distributions are saying that <code>b</code> and <code>d</code> are about equally likely. If used for inference, either distribution would probably produce acceptable results. For most use cases I could imagine, the difference would just be noise.</p><p>The Hellinger distance between the two imagined distributions above is 0.0141. Not zero, but we‚Äôre saying it doesn‚Äôt matter for practical purposes. If 0.0141 is a Hellinger distance that doesn‚Äôt matter much, what about 0.02? Or 0.025? We can imagine there is some threshold Hellinger distance below which we wouldn‚Äôt care and above which we would consider distributions to be meaningfully different. What is that threshold value?</p><p>If we knew it, then we could look at how close the average Hellinger distance between our approximation‚Äôs predictions and model‚Äôs come to this threshold. That would be a measure of the goodness of the approximation.</p><p>I did an experiment to estimate what the threshold is. I trained the same transformer architecture three more times, starting with a different random seed each time and stopping at approximately the same training and validation loss as I did for the original model. This gave me three alternative transformers with roughly the same performance, but with different weights due to the different random initial starting points:</p><table><thead><tr><th>Model</th><th>Seed</th><th>Est. Training Loss</th><th>Est. Validation Loss</th></tr></thead><tbody><tr><td>Original Model</td><td>1337</td><td>0.9334</td><td>1.5063</td></tr><tr><td>Alternate 1</td><td>1442</td><td>0.9293</td><td>1.5038</td></tr><tr><td>Alternate 2</td><td>88</td><td>0.9294</td><td>1.4991</td></tr><tr><td>Alternate 3</td><td>99999</td><td>0.9339</td><td>1.4941</td></tr></tbody></table><blockquote><p>I used the same training/validation sets, hyperparameters, optimizer, etc. for the three alternate models as for the original model. The training code and output for the alternate models is in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/experiments/alternate-models.ipynb" target="_blank" rel="noopener">the <code>alternate-models</code> experiment notebook</a>. Training code for the original model is at the end of <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/models/transformer.ipynb" target="_blank" rel="noopener">the main transformer notebook</a>.</p></blockquote><p>I then ran the same 20,000 prompts through the alternative models and calculated the Hellinger distance between their outputs and that of the original model. <a href="#ii-evaluation-of-main-model-vs-3-alternate-models">Appendix II</a> shows the code used to do this. The table below shows the aggregate results.</p><table><thead><tr><th>Comparison</th><th>Mean Hellinger Distance</th></tr></thead><tbody><tr><td>Original vs Alternate 1</td><td>0.1064 ¬± 0.0823</td></tr><tr><td>Original vs Alternate 2</td><td>0.1057 ¬± 0.0817</td></tr><tr><td>Original vs Alternate 3</td><td>0.1053 ¬± 0.0828</td></tr></tbody></table><p>The original model and the three alternate models are ‚Äúequivalent‚Äù in the sense that they perform about equally well in terms of training and validation loss. I could have used any of them as the basis for this post. In other words, the differences between them likely aren‚Äôt meaningful - just noise.</p><p>Across all three alternate models, the average Hellinger distance was ~0.11 ¬± 0.08. We only have 3 data points, so it‚Äôs not a perfect measure, but ~0.11 is probably a reasonable lower bound for the threshold Hellinger distance we are looking for.</p><p>For comparison, the average Hellinger distance between the model and the approximation was ~0.17. A little higher than 0.11, but within a standard deviation.</p><p>Plotting the distributions of the various Hellinger distances shows this nicely:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/d82fe84ec51461c861f5fbc1c2c273935d4d8ac8ceceaeeb292d79cd5fb9ee19.png" alt=""></p><p>There is clearly less deviation between the alternates and the original model than between the approximation and the original model, but it‚Äôs not wildly different. I think this result suggests the approximation is quite good. Most of the difference is within the ‚Äúacceptable noise‚Äù threshold.</p><h2 id="interpretation-why-does-the-approximation-work">Interpretation: Why Does the Approximation Work? <a href="#interpretation-why-does-the-approximation-work">üîó</a></h2><p>The analysis in the previous section shows that the outputs of the approximation are quite similar to the transformer‚Äôs outputs. But that doesn‚Äôt necessarily mean that the approximation procedure is similar to what the transformer is actually doing. The approximation and the transformer might just represent two different ways of computing the same result.</p><p>My intuition is that this is not the case: <strong>I think the approximation is at least something like what the transformer is doing</strong>. In this section, I‚Äôll break down <em>how</em> I think the transformer computes something similar to the approximation and then present some supporting evidence.</p><p>The key ideas are:</p><ul><li>The transformer, as its name suggests*, performs a series of transformations on its embedded input. The transformer blocks transform embeddings within embedding space and the final linear layer at the end transforms from embedding space to logit space.</li><li>Within each transformer block, the transformation from input to output embedding is done via vector addition: the block‚Äôs output embedding is its input embedding plus the output of the self-attention layer, plus the output of the feed-forward network. Of the two added components, the feed-forward network output value is dominant in determining the final output.</li><li>Within embedding space, subspaces exist that correspond to specific tokens. An embedding within the subspace for a particular token produces an output distribution in which all the probability is concentrated on that token (that token has probability near 1 and all other tokens have probability near 0). Embeddings that lie between the subspaces for multiple tokens result in outputs that distribute all the probability across those tokens.</li><li>The feed-forward network output at each block is an ‚Äúadjustment vector‚Äù that orients the block output towards the subspaces for the tokens that the approximation procedure would predict: those that follow the strings in the training corpus that produce similar feed-forward network outputs at that block.</li></ul><p>In the subsections below, I‚Äôll go into each of these ideas in more detail.</p><blockquote><p>*It‚Äôs unclear whether the name ‚Äútransformer‚Äù alludes to transforming an input sequence to an output sequence (the use case in the original paper was machine translation) or the transformations within the layers of the model.</p></blockquote><h3 id="the-model-is-a-series-of-transformations">The Model is a Series of Transformations <a href="#the-model-is-a-series-of-transformations">üîó</a></h3><p>Once the input to the model has been embedded, we can view the model as a series of transformations:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/c0b6c591002c7e1f931a0bcc794b88454aab02158d1125bef2f8422dbc0e8264.png" alt=""></p><p>The sequence of 6 transformer blocks takes a tensor in embedding space (\(\mathbb{R}^{384}\), since <code>n_embed=384</code>) as input and outputs another tensor in embedding space. In this sense, represents a transformation <em>within</em> embedding space. In fact, each transformer block is itself a transformation within embedding space and the stack of all 6 blocks composes these individual transformations. It isn‚Äôt literally implemented this way in code, but its equivalent to:</p><div><pre tabindex="0"><code data-lang="python"><span><span>output_embedding <span>=</span> block6(block5(block4(block3(block2(block1(input_embedding))))))
</span></span></code></pre></div><p>At the end of the sequence of blocks, the model sends the output embedding through a LayerNorm operation and then a linear layer that transforms from embedding space into logit space (\(\mathbb{R}^{65}\), since <code>vocab_size=65</code>). Finally, the softmax layer at the end turns the logits into probabilities for the next token.</p><h3 id="transformation-via-vector-addition">Transformation via Vector Addition <a href="#transformation-via-vector-addition">üîó</a></h3><p>We looked at the internal logic within a transformer block in the earlier <a href="#transformer-block-structure">Transformer Block Structure</a> section. To recap, the <code>forward()</code> method of the <code>Block</code> module looks like this:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>Block</span>(nn<span>.</span>Module):
</span></span><span><span>    <span>"""One transformer block"""</span>
</span></span><span><span>
</span></span><span><span>    <span>...</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>forward</span>(self, x):
</span></span><span><span>        x <span>=</span> x <span>+</span> self<span>.</span>sa(self<span>.</span>ln1(x)) <span># The `x +` part is a skip connection</span>
</span></span><span><span>        x <span>=</span> x <span>+</span> self<span>.</span>ffwd(self<span>.</span>ln2(x)) <span># The `x +` part is a skip connection</span>
</span></span><span><span>
</span></span><span><span>        <span>return</span> x
</span></span></code></pre></div><p><a id="block-logic-with-intermediates"></a>
This is equivalent to the following code, which, by using some intermediate local variables, clarifies what‚Äôs really going on:</p><div><pre tabindex="0"><code data-lang="python"><span><span>    <span>def</span> <span>forward</span>(self, x):
</span></span><span><span>        sa_out <span>=</span> self<span>.</span>sa(self<span>.</span>ln1(x))
</span></span><span><span>        ffwd_out <span>=</span> self<span>.</span>ffwd(self<span>.</span>ln2(x <span>+</span> sa_out))
</span></span><span><span>
</span></span><span><span>        <span>return</span> x <span>+</span> sa_out <span>+</span> ffwd_out
</span></span></code></pre></div><p><strong>The output of the block is equal to the input (<code>x</code>), plus the self-attention output (<code>sa_out</code>), plus the feed forward network output (<code>ffwd_out</code>).</strong> We can think of the block as taking the input embedding, and then making two adjustments to it.</p><p>These values being added together are vectors in \(\mathbb{R}^{384}\). If we imagine the embedding space reduced to just two dimensions, it might look something like this:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/12e04e2fdd477aaa55660b4020a4ae6b9a72c55038f98009049330cfecfc4291.png" alt=""></p><p>The red vector represents the input embedding. The green vector represents the self-attention output (<code>sa_out</code> in code), and the blue vector represents the feed-forward network output (<code>ffwd_out</code> in code). The gray arrow represent the final sum, or the output of the first block: where you end up when you arrange the individual vectors tip to tail.</p><p>The plot above shows the additions that happen within just one block. Subsequent blocks add their self-attention outputs and feed-forward network outputs, starting from the output of this block. If we add the vectors from those other blocks to the diagram, it looks like this:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/24b90fb31e5e330043b12b6a3b0bf9e8bed15bbd4cd57e98f0cbdc4b393fc71f.png" alt=""></p><p>Again, the red arrow represents the input vector, each green arrow represents one block‚Äôs self-attention output, each blue arrow represents one block‚Äôs feed-forward network output. Arranged tip to tail, their endpoint represents the final output from the stack of 6 blocks, depicted by the gray arrow.</p><p>Though it‚Äôs only in two dimensions, the diagram above is based on real data and is drawn ‚Äúto scale‚Äù, in a way: the length of each 2D vector is the same as the \(\mathbb{R}^{384}\) vector it represents for a real query (index 57). In addition, the cosine similarity between each 2D blue / green arrow and the sum of the arrows that precede it is the same as the cosine similarity between the corresponding self-attention/feed-forward network output and the block input in the real data.</p><blockquote><p>Code to generate the 2D representation from real data is in the <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/70_embedding_adjustments.ipynb" target="_blank" rel="noopener">embedding adjustments analysis notebook</a>.</p></blockquote><p>We can observe two interesting patterns:</p><ul><li>The feed-forward network outputs are generally longer than the self-attention outputs (the vectors have larger norms)</li><li>Within a given block, the feed-forward network output and the self-attention output point in roughly the same direction.</li></ul><p>Look at what happens when we eliminate the self-attention outputs from the vector sum, leaving just the feed-forward network outputs:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/7fff50753ede8a54541e69eaf00215ea285f523817e8361d1fe08ac5e0c6cd8a.png" alt=""></p><p>The inner blue curve in the above plot represents the sum of the input vector and only the feed-forward network outputs from each block. The tip to tail arrangement of these vectors ends at a point far from where the previous arrangement (including the self-attention outputs) ended. But notice that the feed-forward-only endpoint (shorter gray arrow) is quite closely aligned in <em>direction</em> with the original endpoint (longer gray arrow).</p><p>This plot shows values for only one query and we lose a lot of information dropping from 384 dimensions to 2. But the pattern does seem to hold in general and in the full, high-dimensional embedding space. The <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/70_embedding_adjustments.ipynb" target="_blank" rel="noopener">embedding adjustments analysis notebook</a> provides a deep-dive into this phenomenon across all 20,000 queries.</p><p>The takeaway is that <strong>simplifying the transformation performed by the blocks to just the contributions of the feed-forward networks results in an output vector that is shorter (has a smaller norm) than the original output but points in roughly the same direction</strong>. And the difference in norms would have no impact on the transformer‚Äôs final output, because of the LayerNorm operation after the stack of blocks. That LayerNorm step will adjust the norm of any input vector to similar value regardless of its initial magnitude; the final linear layer that follows it will always see inputs of approximately the same norm (see <a href="#iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm">Appendix III</a> for a walk-through of this).</p><p>An important clarification: I‚Äôm not suggesting that we could remove the self attention computation from the transformer. The feed-forward networks take the self-attention output as part of their input (<code>ffwd_out = self.ffwd(self.ln2(x + </code><strong><code>sa_out</code></strong><code>))</code>); they would compute very different values were the self-attention outputs removed. What I am saying is that, after all block processing has been completed as normal including the self-attention computations, we get roughly the same result if we consider only the feed-forward network contributions, as our approximation does. This is probably because the feed-forward network outputs pass on some of the information they receive as input from the self-attention output.</p><p>For some additional evidence that an approximation based only on feed-forward network outputs can produce similar outputs to the transformer, see <a href="#iv-summary-of-experiment-on-relative-impact-of-self-attention-and-feed-forward-network-outputs">Appendix IV</a>.</p><h3 id="token-subspaces">Token Subspaces <a href="#token-subspaces">üîó</a></h3><p>In the examples we‚Äôve seen so far, the model outputs have been distributions that include significant non-zero probabilities for several tokens. For example:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/a90c56ba1e72733d29a614671ad61789b5196562087c1464273da469843cb54d.png" alt=""></p><p>Though we haven‚Äôt seen one yet, we might wonder whether <strong>specific inputs</strong> exist that compel the model to predict a <strong>single token</strong> with <strong>near certainty</strong>. In other words, do some inputs cause the model to output a probability distribution in which just one token has probability very near 1 and all other tokens very near zero? Such a distribution might look like this:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/ecc30428196c016ef2970ee406b6ea46a79a2a24c1521d8843c2dc90f83ffc83.png" alt=""></p><p>In fact, we can ask this question about any stage of the model. ‚ÄúInput‚Äù doesn‚Äôt have to refer to the initial input to the model, but could be the input to any layer within the model. For example, consider only the layers that transform the final block‚Äôs output embedding to logit space (the final LayerNorm and linear layers):</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/4c4339d5f6612480cf52b0d34f2c1732c99485033c1196076543f86ec0925af5.png" alt=""></p><p>Is there some embedding block 6 might emit that would yield an output probability distribution in which some token, say the letter <code>a</code>, has probability very near 1?</p><h4 id="learning-token-subspaces">Learning Token Subspaces <a href="#learning-token-subspaces">üîó</a></h4><p>With the right math, it may be possible to find this embedding analytically. But it‚Äôs also possible to ‚Äúlearn‚Äù (in the sense of deep learning) such an embedding. Here‚Äôs the basic idea:</p><ul><li>Pick a point in the transformer where the input to subsequent layers is an embedding. This could be the input to any of the transformer blocks, or the point right after the final block (as shown in the diagram above).</li><li>Pick a token to learn an embedding for.</li><li>Create an embedding tensor and initialize it with random values. This tensor is the parameter the learning algorithm will optimize; the weights of the transformer are fixed.</li><li>Execute a forward pass by evaluating the transformer from the selected point, using the embedding as input. This will produce some set of logits.</li><li>Compute <a href="https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/#nll" target="_blank" rel="noopener">negative log likelihood loss</a> relative to the token we‚Äôre learning an embedding for.</li><li>Do a backward pass, updating the embedding tensor according to the gradients.</li></ul><p>My implementation of this is in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/experiments/learn-embeddings.ipynb" target="_blank" rel="noopener">the learned embeddings notebook</a>. I used it to learn embeddings for all tokens at various stages of the model and saved them. We can load one - a learned embedding that produces a distribution giving token <code>a</code> probability almost 1 - and check that it does what we expect when given to the part of the model shown in the diagram above:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/0dce2cb6b27518fe0a1a26685995c9050cf791b48cc4e407a080182039905dca.png" alt=""></p><p>As expected, all the probability mass is concentrated on <code>a</code>. Inference using this distribution would generate <code>a</code> with near certainty.</p><p>The same procedure can learn embeddings for use at other parts of the model. If we wanted to find an embedding for <code>a</code> that could be input to block 6, we could run the same learning algorithm but use this part of the transformer in the forward pass:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/37991d03eeb45809255009149e32de771715221bb9d315efb6e2aa78b26a897c.png" alt=""></p><p>It‚Äôs more computationally expensive to learn embeddings at earlier stages of the model because the optimizer has to contend with a larger computation graph involving operations from all included blocks. Thankfully, as I‚Äôll explain <a href="#use-only-final-subspaces">shortly</a>, we need only the embeddings learned for the part of the transformer after all the blocks (embeddings that go straight into the final LayerNorm layer) to show how the transformer operates like the approximation.</p><h4 id="from-embeddings-to-subspaces">From Embeddings to Subspaces <a href="#from-embeddings-to-subspaces">üîó</a></h4><p>For any token, the procedure described in the previous section can learn an embedding that makes the model predict that token with probability near 1. It turns out <strong>there isn‚Äôt just one such embedding for each token.</strong> We can learn many different embeddings that all produce probability distributions that assign a given token nearly all the probability mass. It was easy to learn thousands of unique embeddings for every token in the vocabulary.</p><p>I think <strong>the model has learned a complex, non-linear embedding subspace corresponding to each token</strong>. Any embedding within that subspace results in an output distribution that assigns the token near certain probability. Each embedding I was able to learn is probably a point in the embedding subspace for the corresponding token.</p><p>If we imagine the full embedding space (\(\mathbb{R}^{384}\)) reduced to \(\mathbb{R}^3\) (and the complex subspaces reduced to 2D planes), it might look something like this:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/04a243b63386cc0e853d350cb0177eeb33f2c15d2aacdaf59ddb4dc38f48b444.png" alt=""></p><p>I don‚Äôt know how to determine the exact subspaces for each token mathematically. But I do know how to get a workable approximation of them <strong>if we‚Äôre willing to pretend that they are linear</strong>. They are almost certainly not linear, even at the end of the model, because of the non-linear LayerNorm operation. But they are likely <em>closer</em> to linear near the end of the model because the LayerNorm is the only non-linearity. Earlier in the model, each feed-forward network introduces an additional non-linearity via its ReLU operation.</p><blockquote><p><a href="https://www.lesswrong.com/posts/jfG6vdJZCwTQmG7kb/re-examining-layernorm" target="_blank" rel="noopener">This post on LessWrong</a> illustrates of the non-linearity of LayerNorm clearly.</p></blockquote><p>Pretending the subspaces are linear actually works quite well for the part of the model after the transformer blocks. And that is the only part of the model we need to consider for this analysis (as I‚Äôll explain <a href="#use-only-final-subspaces">soon</a>).</p><h4 id="linear-approximations-for-subspaces">Linear Approximations for Subspaces <a href="#linear-approximations-for-subspaces">üîó</a></h4><p>The idea is quite simple: for a given token, we can learn a whole lot of different embeddings, treating each one as a data point. Then we can determine the best fitting line, plane, or other low-dimensional linear subspace that fits the data.</p><p>Again, if we imagine our embedding space reduced to just 3 dimensions, it might look something like the following diagram. The blue dots each represent a learned embedding and the red arrow is the line that minimizes projected distance from each point.</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/a32cb4b311513c8c1bbb0af5cae1d68b1e96efeddfbde076f8e9fca02772d605.png" alt=""></p><p>We can use Singular Value Decomposition (SVD) to find the best fitting linear subspace for the learned embeddings.</p><blockquote><p>To learn more about singular value decomposition in this context, I recommend reading Jeremy Kun‚Äôs excellent two-part post. <a href="https://jeremykun.com/2016/04/18/singular-value-decomposition-part-1-perspectives-on-linear-algebra/" target="_blank" rel="noopener">Part 1</a> <a href="https://jeremykun.com/2016/05/16/singular-value-decomposition-part-2-theorem-proof-algorithm/" target="_blank" rel="noopener">Part 2</a>.</p></blockquote><p><a href="#v-performing-svd-to-get-a-linear-approximation-of-a-token-subspace">Appendix V</a> walks through the code that uses SVD to find a linear approximation for the subspace corresponding to one token. I did this for all tokens, using the embeddings I learned for the final stages of the transformer. In every case, I was able to find a single vector (1-D subspace) that approximates the token subspace quite well.</p><blockquote><p>For completeness, I also tried this at earlier stages of the transformer and found, as expected, that the linear approximations, even at higher dimensions, didn‚Äôt fit the data as well. The relevant experiments are in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/50_approximation_details.ipynb" target="_blank" rel="noopener">the approximation details notebook</a></p></blockquote><h4 id="mixing-subspace-approximations">Mixing Subspace Approximations <a href="#mixing-subspace-approximations">üîó</a></h4><p>By learning a large number of embeddings for each token and then using SVD on them, we can find one vector for each token that approximates its subspace. Given one of these vectors, any embedding that falls on its span will produce an output distribution that concentrates all the probability mass on the corresponding token. But many of the real transformer outputs we‚Äôve seen distribute the probability mass across several tokens. How do we get from subspaces for individual tokens to embeddings that produce these more diverse distributions?</p><p>We can create embeddings that produce probability distributions where several tokens have substantial probability via linear combinations of the subspace approximation vectors for those tokens. This is the distribution we get when we create an embedding by simply adding the approximation vectors for the subspaces for <code>a</code> and <code>b</code>:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/211fa3aed3f5cbbe1a5adf41e014e27068c39ffaf21aff36c833a851b500e211.png" alt=""></p><p>The sum of subspace approximations vectors for two tokens is an embedding somewhere in between the two subspaces, which results in a final distribution that is the combination of the two tokens.</p><p>Sadly, adding the approximation vectors for <code>a</code> and <code>b</code>, without weighting either one, results in not quite a 50-50 distribution across the two tokens (as shown above). I think there are three reasons for this:</p><ol><li>The approximation vectors are just approximations and not perfect representations of their subspaces.</li><li>The subspace approximation vectors are not perfectly orthogonal. To the extent that <code>a</code>‚Äôs vector has a small component that points in the direction of <code>b</code>, the sum results in an overweighting of <code>b</code>.</li><li>The final linear layer of the model produces logits of different magnitudes for different tokens. For example, given the approximation for <code>a</code>, the logit for <code>a</code> is ~18.2. The logit for <code>b</code> from its approximation is ~19.5.</li></ol><p>Together, these errors accumulate and the softmax function at the very end exaggerates even small differences. For more analysis on the reasoning behind the differences and how they might be compensated for, see <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/60_combining_token_subspaces.ipynb" target="_blank" rel="noopener">the combining token subspaces notebook</a>.</p><p>These imperfections aside, I think we can conclude that <strong>it‚Äôs possible to derive an embedding that produces a distribution for multiple tokens via a linear combination of the approximation vectors for those tokens‚Äô subspaces</strong>.</p><h3 id="putting-it-all-together">Putting it All Together <a href="#putting-it-all-together">üîó</a></h3><p>To summarize where we are, the preceding sections have shown:</p><ul><li>The transformer blocks perform a series of transformations in embedding space.</li><li>Those transformations can be thought of as moving from one point in embedding space to another by adding the feed-forward network output vector to the input embedding.</li><li>Embedding space contains subspaces corresponding to predicting particular tokens and embeddings between subspaces for multiple tokens result in predictions including all those tokens.</li></ul><p>This section adds the final piece, which is the correspondence between what the transformer is doing and what the approximation is doing:</p><ul><li>Within a block, adding the feed-forward network output vector to the input produces an output embedding that better aligns with the embedding subspaces of specific tokens. And <strong>those tokens are the same ones predicted in the approximation</strong>: they‚Äôre the tokens that follow the strings in the training corpus that yield similar feed-forward network outputs to the current prompt.</li></ul><p>Let‚Äôs look at an example that shows this. The following is the output distribution predicted by the approximation for the prompt, <code>med me Aut</code> (query index 33), using only the feed-forward network outputs from the final block:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/0a46316fd1ec97000bed4b44242e7aad0809b16acd65a98507ff1b987e313291.png" alt=""></p><p>Based on the strings in the training corpus with similar feed-forward network outputs at the final block, the approximation predicts <code>o</code> is the most likely next token and <code>h</code> is next.</p><p>Next, we need to look at the feed-forward network output for the prompt in this block and determine which token subspaces it‚Äôs most oriented towards. I‚Äôm going to show a little code here, because I think it‚Äôs the best way to explain what‚Äôs going on. Readers who aren‚Äôt interested in the implementation can focus only on the output.</p><p>First we need to actually grab the feed-forward outputs (we haven‚Äôt needed them so far because we‚Äôve been working with precomputed/prefiltered similarity data). We‚Äôll use some <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/models/transformer-helpers.ipynb" target="_blank" rel="noopener">helper functions</a> that provide easy access to the transformer‚Äôs intermediate representations:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Tokenize the strings</span>
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_strings(prompts)
</span></span><span><span>
</span></span><span><span><span># Embed the tokens</span>
</span></span><span><span>embeddings <span>=</span> accessors<span>.</span>embed_tokens(tokens)
</span></span><span><span>
</span></span><span><span><span># Instantiate TransformerAccessors</span>
</span></span><span><span>accessors <span>=</span> TransformerAccessors(m, device)
</span></span><span><span>
</span></span><span><span><span># Run them through the model with hooks attached that let us look at</span>
</span></span><span><span><span># intermediate values</span>
</span></span><span><span>_, io_accessors <span>=</span> accessors<span>.</span>run_model(embeddings)
</span></span><span><span>
</span></span><span><span><span># Grab the outputs of the ffwd networks at each layer</span>
</span></span><span><span>ffwd_outs <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    io_accessors[block_idx]<span>.</span>output(<span>'ffwd'</span>)[:, <span>-</span><span>1</span>, :]<span>.</span>clone()
</span></span><span><span>    <span>for</span> block_idx <span>in</span> range(n_layer)
</span></span><span><span>])
</span></span><span><span>
</span></span><span><span><span># Free up some memory</span>
</span></span><span><span><span>del</span> io_accessors
</span></span><span><span>_ <span>=</span> gc<span>.</span>collect()
</span></span><span><span>
</span></span><span><span>ffwd_outs<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([6, 20000, 384])
</code></pre><p>To determine which token subspaces the feed-forward network output aligns with, we‚Äôll project it onto the subspace approximation for each token, then determine which projections are most similar to the original vector. To do this, we‚Äôll need to get the projection matrix for the rank 1 approximation to each token subspace:</p><blockquote><p>The code below uses the <code>projection_matrix_for_rank_k_approximation()</code> helper function, defined in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/common/svd-helpers.ipynb" target="_blank" rel="noopener">the SVD helpers notebook</a>.</p></blockquote><blockquote><p>In the case of a rank 1 approximation, the projection isn‚Äôt really necessary. We could just take the cosine similarity with the approximation vector, but I wanted to keep this code general because I tried out higher-dimensional approximations in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/50_approximation_details.ipynb" target="_blank" rel="noopener">other places</a>.</p></blockquote><div><pre tabindex="0"><code data-lang="python"><span><span>filename_for_token <span>=</span> FilenameForToken(tokenizer)
</span></span><span><span>subspace_dims <span>=</span> <span>1</span>
</span></span><span><span>projection_matrices <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    projection_matrix_for_rank_k_approximation(
</span></span><span><span>        original_matrix<span>=</span>torch<span>.</span>load(
</span></span><span><span>            learned_embeddings_dir <span>/</span>  <span>'no_blocks'</span> <span>/</span> <span>f</span><span>"</span><span>{</span>filename_for_token(token)<span>}</span><span>.pt"</span>,
</span></span><span><span>            map_location<span>=</span>device,
</span></span><span><span>        )[:, <span>0</span>, :],
</span></span><span><span>        k<span>=</span>subspace_dims,
</span></span><span><span>    )
</span></span><span><span>    <span>for</span> token <span>in</span> tokenizer<span>.</span>chars
</span></span><span><span>])
</span></span></code></pre></div><p>Now we‚Äôll perform the projections and find the top 5 most similar ones to the original feed-forward output vector:</p><div><pre tabindex="0"><code data-lang="python"><span><span>projections <span>=</span> projection_matrices <span>@</span> ffwd_outs[block_idx, q_idx, :]
</span></span><span><span>values, indices <span>=</span> torch<span>.</span>topk(
</span></span><span><span>    F<span>.</span>cosine_similarity(projections, ffwd_outs[block_idx][q_idx], dim<span>=-</span><span>1</span>),
</span></span><span><span>    k<span>=</span><span>5</span>,
</span></span><span><span>    dim<span>=</span><span>0</span>,
</span></span><span><span>)
</span></span><span><span>tokens <span>=</span> [tokenizer<span>.</span>chars[i<span>.</span>item()] <span>for</span> i <span>in</span> indices]
</span></span><span><span>list(zip(tokens, values<span>.</span>tolist()))
</span></span></code></pre></div><pre tabindex="0"><code>[('o', 0.5074884295463562),
 ('h', 0.40787822008132935),
 ('i', 0.26926180720329285),
 ('u', 0.22823508083820343),
 ('y', 0.20325089991092682)]
</code></pre><p>It turns out that <code>o</code> and <code>h</code> are the most similar, indicating that the feed-forward network output is most oriented towards the subspaces for these tokens. And these are the same tokens that the approximation predicted from the strings with similar feed-forward network outputs (see the distribution above).</p><p>Another example, this time looking at query index 36 (<code>if and thy</code>), but staying in the final block:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/fb2e608eb48c085de00f28831641ff4d49f7f3fd63b198fc3fc08d02a3cc7c45.png" alt=""></p><div><pre tabindex="0"><code data-lang="python"><span><span>projections <span>=</span> projection_matrices <span>@</span> ffwd_outs[block_idx, q_idx, :]
</span></span><span><span>values, indices <span>=</span> torch<span>.</span>topk(
</span></span><span><span>    F<span>.</span>cosine_similarity(projections, ffwd_outs[block_idx][q_idx], dim<span>=-</span><span>1</span>),
</span></span><span><span>    k<span>=</span><span>5</span>,
</span></span><span><span>    dim<span>=</span><span>0</span>,
</span></span><span><span>)
</span></span><span><span>tokens <span>=</span> [tokenizer<span>.</span>chars[i<span>.</span>item()] <span>for</span> i <span>in</span> indices]
</span></span><span><span>list(zip(tokens, values<span>.</span>tolist()))
</span></span></code></pre></div><pre tabindex="0"><code>[(' ', 0.5869003534317017),
 ('s', 0.47689366340637207),
 ('\n', 0.38412901759147644),
 ('$', 0.23048195242881775),
 ('a', 0.21783535182476044)]
</code></pre><p>Here <code></code>(space), <code>s</code>, and <code>\n</code> (newline) were the tokens predicted from what follows the strings with similar feed-forward outputs, and indeed these are the token subspaces most aligned with the prompt‚Äôs feed-forward output.</p><h4 id="aggregate-performance">Aggregate Performance <a href="#aggregate-performance">üîó</a></h4><p>In the previous section, I purposely picked examples that exhibit strong correlation between the approximation‚Äôs predictions and the most aligned subspaces, to illustrate the point most clearly. Of course, there are other examples for which the correlation is less strong. Rather than looking at specific cases, let‚Äôs try to get a sense of how well the correlation holds up across all 20,000 prompts.</p><p>This immediately leads to a question: what is the right measure of aggregate performance? Unfortunately, even if the hypothesis - that the prompt‚Äôs feed-forward output aligns with the subspaces for tokens predicted from the strings with similar feed forward outputs - is true, a few practical issues make it difficult to demonstrate objectively:</p><ul><li>We don‚Äôt have exact definitions of the token subspaces, just imperfect, linear approximations.</li><li>Magnitudes don‚Äôt line up: the tokens with the most probability mass in the approximation‚Äôs predictions don‚Äôt always correspond to the subspaces with the greatest cosine similarity (because of the imperfect approximations, because the adjustment required may be bigger or smaller for some tokens vs others based on the input embedding‚Äôs current alignment, because, as explained in the <a href="#mixing-subspace-approximations">Mixing Subspace Approximations</a> section, the model is more ‚Äúsensitive‚Äù to some tokens than others).</li></ul><p>Given these impediments, we can‚Äôt just do something simple like normalizing the cosine similarities and computing Hellinger distance with the predicted probability distribution.</p><p>Instead, we need to devise a criterion on which to judge whether the data from a particular prompt supports the hypothesis or not. Then we can evaluate aggregate performance by how many of the 20,000 prompts satisfy the criterion. I experimented with several different approaches and in the end came up with this candidate criterion:</p><p>High-level description: <em>Do the subspaces for the tokens containing 90% of the probability mass in the approximation‚Äôs predictions appear in the top half of all token subspaces when ranked by cosine similarity with the prompt‚Äôs feed-forward output vector?</em></p><p>Exact definition:</p><ul><li>Define <code>top_n</code> as the number of tokens required to cover at least 90% of the probability mass in the approximation‚Äôs predictions for this prompt.</li><li>Define <code>n_subspaces</code> as <code>tokenizer.vocab_size // 2</code> (32, based on our 65-token vocabulary).</li><li>Determine: Are the subspaces for the first <code>top_n</code> tokens predicted by the approximation in the first <code>n_subspaces</code> subspaces ranked by cosine similarity with the prompt‚Äôs feed-forward output vector?</li></ul><p>Admittedly, this is an arbitrary definition and reasonable people could debate any of the specifics. But I do think it gives as an indication of whether the data from a particular example prompt supports the hypothesis, while allowing for some of the measurement challenges noted above.</p><p>I evaluated this criteria at three places: the outputs of blocks 6, 5, and 4, using projection matrices derived from learned embeddings at each of these places.</p><blockquote><p>I didn‚Äôt evaluate at earlier blocks because the GPU time required to learn embeddings at those blocks became prohibitive. The further back in the model, the bigger the computation graph that the learning algorithm needs to optimize over.</p></blockquote><p>The table below shows the results:</p><blockquote><p>The code that produced these results appears at the end of <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/50_approximation_details.ipynb" target="_blank" rel="noopener">the approximation details notebook</a></p></blockquote><table><thead><tr><th>Block</th><th># of Prompts Satisfying Criterion</th></tr></thead><tbody><tr><td>6</td><td>16357 (81.78%)</td></tr><tr><td>5</td><td>10142 (50.71%)</td></tr><tr><td>4</td><td>7760 (38.80%)</td></tr></tbody></table><p>These numbers aren‚Äôt exactly a ringing endorsement. As expected, they get worse the further back we go, probably due to the increased non-linearity.</p><p><a id="use-only-final-subspaces"></a>
What if we always used the subspace approximations from the very end of the transformer (which are likely to be the most linear), even when comparing against feed-forward network outputs from earlier blocks? The results get better:</p><table><thead><tr><th>Block</th><th># of Prompts Satisfying Criterion</th></tr></thead><tbody><tr><td>6</td><td>16357 (81.78%)</td></tr><tr><td>5</td><td>13652 (68.26%)</td></tr><tr><td>4</td><td>11630 (58.15%)</td></tr><tr><td>3</td><td>11469 (57.34%)</td></tr><tr><td>2</td><td>10404 (52.02%)</td></tr><tr><td>1</td><td>9942 (49.71%)</td></tr></tbody></table><blockquote><p>Like many good findings, this one resulted from a bug. I accidentally ran the analysis using the projection matrices for the final part of the transformer with the feed-forward network outputs from earlier blocks and was surprised when the numbers turned out to be so good.</p></blockquote><p>It‚Äôs valid to use the subspace approximations (and corresponding projection matrices) from the end of the transformer at earlier stages. All blocks operate in the same embedding space and each one seems to make a small refinement on the output of its predecessors, rather than wild changes in direction. So if any block‚Äôs feed-forward network output adjusts an embedding towards the subspaces for a set of tokens as defined at the end of the transformer, it is likely also adjusting it towards whatever the subspaces would be for those same tokens at the block where it operates.</p><blockquote><p>The <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens" target="_blank" rel="noopener">logit lens post</a>, in the section ‚Äúwhy? / is this surprising?‚Äù provides an explanation that supports this idea. In summary, the residual connections encourage the transformer to learn weights that operate within the same basis across blocks and the use of weight decay in training results in a computation that‚Äôs spread out over as many layers as possible, with each layer making only a small, incremental change.</p></blockquote><p>To put these numbers in perspective, I investigated how likely it would be for the criterion I‚Äôve defined here to be satisfied by chance. In other words, if we assume the hypothesis is false and that the cosine similarities between the feed-forward network output and the token subspace approximation vectors are random, rather than expressing meaningful relationships, how likely would it be for the criterion to still be satisfied?</p><p>I ran a simulation of this, taking care to ensure that the distribution of randomly generated cosine similarities matches the real data, among other details. The implementation is at the end of <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/50_approximation_details.ipynb" target="_blank" rel="noopener">the approximation details notebook</a>. The final results are:</p><table><thead><tr><th>Block</th><th>Likely % of Prompts Satisfying Criteria By Chance</th></tr></thead><tbody><tr><td>6</td><td>20.76% ¬± 0.25%</td></tr><tr><td>5</td><td>20.55% ¬± 0.26%</td></tr><tr><td>4</td><td>18.37% ¬± 0.24%</td></tr><tr><td>3</td><td>18.20% ¬± 0.24%</td></tr><tr><td>2</td><td>17.04% ¬± 0.23%</td></tr><tr><td>1</td><td>16.31% ¬± 0.23%</td></tr></tbody></table><p>So the best performance numbers we have are clearly much better than chance. But in fairness, they‚Äôre still not a slam dunk.</p><p>Even when we use approximations for the most linear subspaces we have, I think there is still a lot of noise in the measurement, for all the reasons outlined earlier in this section. Personally, I take the numbers to be overall supportive of the hypothesis, at least directionally, though I wish the evidence was more conclusive.</p><h3 id="final-summary-of-correspondence-between-transformer-and-approximation">Final Summary of Correspondence Between Transformer and Approximation <a href="#final-summary-of-correspondence-between-transformer-and-approximation">üîó</a></h3><p>The analyses in this post point towards two ideas. First, that the approximation and the transformer produce similar outputs. Second, that there is a correspondence between the approximation procedure and what the transformer is doing. I think the evidence for the first idea is quite strong. The evidence for the second is less clear cut, but still suggests it‚Äôs probably at least partially right.</p><p>To close, I want to provide a high-level summary of what I think that correspondence is, even if I can‚Äôt yet demonstrate it more definitively:</p><table><thead><tr><th>Concept</th><th>Transformer</th><th>Approximation</th></tr></thead><tbody><tr><td>Prompts map to classes of strings in the training corpus.</td><td>The transformer learns an embedding scheme, along with weights in its self-attention and feed-forward networks, that cause strings in the training corpus with similar characteristics to produce similar output values. Prompts that share those characteristics also produce similar output values.</td><td>The approximation performs the same mapping as the transformer by examining the feed-forward network outputs from all substrings in the training corpus and identifying the ones similar to the outputs from a given prompt.</td></tr><tr><td>Predictions for the tokens likely to follow a prompt derive from the frequency distribution of tokens that follow strings in the training corpus that produce feed-forward network output values similar to those of the prompt.</td><td>A feed-forward network output is a compressed, latent representation, in the embedding space, of the frequency distribution of the tokens that follow strings in the training corpus that produce similar outputs. The weights in the final linear layer map the latent representation into logit space such that it become the correct probability distribution after applying the softmax operation.</td><td>The approximation reconstructs the same frequency distribution manually, by looking up the strings identified as having similar outputs in the training corpus and counting the tokens that follow them. Normalizing the frequency distribution turns it into a probability distribution.</td></tr><tr><td>Final output is a weighted sum of predictions from each block.</td><td>As shown <a href="#transformation-via-vector-addition">earlier</a>, the transformer output is roughly the vector sum of all feed-forward network outputs and the input embedding. The learned weights in the layers within a block determine the magnitude and direction of the output and thus how much it influences the overall direction of the final sum.</td><td>The approximation performs a weighted sum of the distributions determined for each block. The weights control the degree of influence of any given block and are manually selected to produce results as close to the transformer‚Äôs as possible.</td></tr></tbody></table><h3 id="what-about-attention">What About Attention? <a href="#what-about-attention">üîó</a></h3><p>I began this post by observing that most explanations of how transformers work focus on attention but don‚Äôt say how attention results turn into the final predictions. I may be guilty of the opposite: I‚Äôve written at length about how the transformers produce their output probabilities and said very little about attention.</p><p>To wrap up the analysis, I‚Äôd like to rectify this with a few words about attention. In the mechanism I‚Äôve laid out, whether executed in the form of the approximation or the transformer, a key operation is mapping the prompt to a class of strings from the training corpus at each block. Predictions for the next token follow directly from the distribution of tokens that follow those strings in the training corpus. <strong>Making good predictions depends on mapping the prompt to the right class of training corpus strings. And that is the job of self-attention.</strong></p><p>The self-attention layers learn to identify patterns across the tokens that make up a prompt. Those patterns might be simple, such as a common sequence appearing at the beginning or end of the prompt (for example, as we saw <a href="#demo-my-proposal-in-action">earlier</a>, strings that end in ‚Äòy l‚Äô). They can also be more general: instead of matching specific tokens, they might match <em>kinds</em> of tokens, such as vowels or capitals, in specific places. The learned weights in the attention heads determine which patterns they respond to, and thus which strings in the training corpus produce similar values. The output of the self-attention heads, when passed through the feed-forward network, yield representations in embedding space that encode information about the distribution of tokens in the training corpus that follow those strings.</p><p>Because the transformer has multiple blocks and each block has multiple attention heads (6 blocks and 6 heads per block in the one we looked at), it‚Äôs possible to evaluate each prompt against a large number of different potential patterns. The richness and diversity of the patterns that the attention heads can identify gives the transformer its predictive power.</p><h2 id="closing-thoughts">Closing Thoughts <a href="#closing-thoughts">üîó</a></h2><p>I started this project because I wanted to understand the transformer architecture. It‚Äôs given me a satisfying explanation of what at least one transformer is doing, but has been even more fruitful as an exercise in learning how to learn. This was my first foray into an open-ended ML research project on my own. It taught me how to interrogate the internals of models, how to set up experiments to answer questions, and, perhaps most importantly, how to keep moving the project forward when I felt stuck.</p><p>Language models have always seemed magical to me, from the first time I used ChatGPT. I wondered if finding a reductive explanation for what happens internally would rob them of their magic. In fact, I think the opposite has happened. I‚Äôve come to appreciate the beauty in an elegantly simple mechanism that produces such rich complexity in its outputs.</p><p>I don‚Äôt know whether the results I found here have any generality beyond the small transformer I trained or if any of it will be of use to anyone else. Regardless, it‚Äôs been a joy to do this work and I‚Äôm grateful to have had the things I needed along the way: time, resources, and endless support from my family and mentors.</p><h2 id="appendices">Appendices <a href="#appendices">üîó</a></h2><h3 id="i-model-details">I: Model Details <a href="#i-model-details">üîó</a></h3><p>Some notable specs:</p><ul><li>Vocabulary size: 65 (the unique characters in the TinyShakespeare dataset)</li><li>Embedding size (<code>n_embed</code>): 384</li><li>Number of transformer blocks (<code>n_layer</code>): 6</li><li>Number of attention heads (<code>n_head</code>): 6</li><li>Context window size (<code>block_size</code>): 256</li></ul><p>The feed-forward networks comprise over 65% of the total trainable parameters:</p><div><pre tabindex="0"><code data-lang="python"><span><span>all_trainable_params <span>=</span> [p <span>for</span> p <span>in</span> m<span>.</span>parameters() <span>if</span> p<span>.</span>requires_grad]
</span></span><span><span>n_all_trainable_params <span>=</span> sum([np<span>.</span>prod(p<span>.</span>size()) <span>for</span> p <span>in</span> all_trainable_params])
</span></span><span><span>
</span></span><span><span>ffwd_trainable_params <span>=</span> [
</span></span><span><span>    p
</span></span><span><span>    <span>for</span> block_idx <span>in</span> range(n_layer)
</span></span><span><span>    <span>for</span> p <span>in</span> m<span>.</span>blocks[block_idx]<span>.</span>ffwd<span>.</span>parameters()
</span></span><span><span>    <span>if</span> p<span>.</span>requires_grad
</span></span><span><span>]
</span></span><span><span>n_ffwd_trainable_params <span>=</span> sum([np<span>.</span>prod(p<span>.</span>size()) <span>for</span> p <span>in</span> ffwd_trainable_params])
</span></span><span><span>
</span></span><span><span>print(
</span></span><span><span>    <span>f</span><span>"</span><span>{</span>n_ffwd_trainable_params<span>:</span><span>,</span><span>}</span><span> ffwd params out of </span><span>{</span>n_all_trainable_params<span>:</span><span>,</span><span>}</span><span> total params (</span><span>{</span>n_ffwd_trainable_params <span>/</span> n_all_trainable_params<span>:</span><span>.2%</span><span>}</span><span>)"</span>
</span></span><span><span>)
</span></span></code></pre></div><pre tabindex="0"><code>7,089,408 ffwd params out of 10,788,929 total params (65.71%)
</code></pre><h3 id="ii-evaluation-of-main-model-vs-3-alternate-models">II: Evaluation of Main Model vs 3 Alternate Models <a href="#ii-evaluation-of-main-model-vs-3-alternate-models">üîó</a></h3><p>As described in the <a href="#evaluating-the-approximation">Evaluation section</a>, I trained the same transformer architecture used for the main model in this post three additional times, starting from a different random seed each time. This appendix shows the code used to measure the average Hellinger distance between the output of the main models and each of the three alternates, across the 20,000 sample prompts. The results provide a plausible lower bound for the threshold Hellinger distance that indicates a meaningful change in an output probability distribution.</p><p>First, we instantiate the three alternate models from their saved weights:</p><div><pre tabindex="0"><code data-lang="python"><span><span>alt_models_dir <span>=</span> environment<span>.</span>data_root <span>/</span> <span>'alternate-models/model-training/20240112-training/outputs/'</span>
</span></span><span><span><span>assert</span> alt_models_dir<span>.</span>exists(), <span>"Alternate models directory does not exist. Run the training code in ../experiments/alternate-models.ipynb."</span>
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="python"><span><span><span># Instantiate the three alternative trained models</span>
</span></span><span><span>m_alt1, _ <span>=</span> create_model_and_tokenizer(
</span></span><span><span>    saved_model_filename<span>=</span>alt_models_dir <span>/</span> <span>'shakespeare-20240112-1.pt'</span>,
</span></span><span><span>    dataset<span>=</span>ts,
</span></span><span><span>    device<span>=</span>device,
</span></span><span><span>)
</span></span><span><span>m_alt2, _ <span>=</span> create_model_and_tokenizer(
</span></span><span><span>    saved_model_filename<span>=</span>alt_models_dir <span>/</span> <span>'shakespeare-20240112-2.pt'</span>,
</span></span><span><span>    dataset<span>=</span>ts,
</span></span><span><span>    device<span>=</span>device,
</span></span><span><span>)
</span></span><span><span>m_alt3, _ <span>=</span> create_model_and_tokenizer(
</span></span><span><span>    saved_model_filename<span>=</span>alt_models_dir <span>/</span> <span>'shakespeare-20240112-3.pt'</span>,
</span></span><span><span>    dataset<span>=</span>ts,
</span></span><span><span>    device<span>=</span>device,
</span></span><span><span>)
</span></span></code></pre></div><p>Next, we feed the input tokens into the original model and the three alternate models and get their output probability distributions:</p><div><pre tabindex="0"><code data-lang="python"><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_strings(prompts)
</span></span><span><span>
</span></span><span><span>model_probs <span>=</span> get_model_probs(m, tokens)
</span></span><span><span>alt_model_probs1 <span>=</span> get_model_probs(m_alt1, tokens)
</span></span><span><span>alt_model_probs2 <span>=</span> get_model_probs(m_alt2, tokens)
</span></span><span><span>alt_model_probs3 <span>=</span> get_model_probs(m_alt3, tokens)
</span></span></code></pre></div><p>Now we can compute the Hellinger distance between the outputs from the three alternative models and the outputs from the original model. Remember that each of the model probabilities tensors (<code>model_probs</code> and <code>alt_model_probs*</code>) is a 20,000x65 tensor i.e. 20,000 probability distributions of 65 elements each.</p><p>We‚Äôre computing the Hellinger distance between those probability distributions. So for each alternative model, we end up with 20,000 Hellinger distance values. These values tell us, for each prompt, how the probability distribution for the next token predicted by one of the alternate models differed from the probability distribution predicted by the original model.</p><div><pre tabindex="0"><code data-lang="python"><span><span>h_alt1 <span>=</span> hellinger_distance(model_probs, alt_model_probs1)
</span></span><span><span>h_alt2 <span>=</span> hellinger_distance(model_probs, alt_model_probs2)
</span></span><span><span>h_alt3 <span>=</span> hellinger_distance(model_probs, alt_model_probs3)
</span></span></code></pre></div><p>With the Hellinger distances computed, we can look at aggregate stats:</p><div><pre tabindex="0"><code data-lang="python"><span><span>h_alts <span>=</span> torch<span>.</span>stack([h_alt1, h_alt2, h_alt3], dim<span>=</span><span>1</span>)
</span></span><span><span>h_alts<span>.</span>mean(dim<span>=</span><span>0</span>), h_alts<span>.</span>std(dim<span>=</span><span>0</span>), h_alts<span>.</span>min(dim<span>=</span><span>0</span>)<span>.</span>values, h_alts<span>.</span>max(dim<span>=</span><span>0</span>)<span>.</span>values
</span></span></code></pre></div><pre tabindex="0"><code>(tensor([0.1064, 0.1057, 0.1053]),
 tensor([0.0823, 0.0817, 0.0828]),
 tensor([0.0005, 0.0008, 0.0008]),
 tensor([0.8351, 0.7881, 0.8743]))
</code></pre><p>For all three alternate models, the average Hellinger distance was ~0.11 ¬± 0.08. All had very small minimums (&lt;= 0.0008) and maximums around ~0.80.</p><h3 id="iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm">III: The Output Embedding‚Äôs Norm Doesn‚Äôt Matter Because of the Final LayerNorm <a href="#iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm">üîó</a></h3><p>This appendix demonstrates the assertion from the <a href="#transformation-via-vector-addition">Transformation via Vector Addition</a> that the norm of the output embeddings from the final transformer block does not matter, because of the LayerNorm before the final linear layer.</p><p>To begin, let‚Äôs grab the final block outputs for the first 1000 prompts:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Get the block outputs for the first 1000 prompts</span>
</span></span><span><span>
</span></span><span><span>tokens_sample <span>=</span> encoding_helpers<span>.</span>tokenize_strings(prompts[:<span>1000</span>])
</span></span><span><span>_, io_accessors_sample <span>=</span> accessors<span>.</span>run_model(accessors<span>.</span>embed_tokens(tokens_sample))
</span></span><span><span>
</span></span><span><span>final_block_outputs <span>=</span> io_accessors_sample[<span>-</span><span>1</span>]<span>.</span>output(<span>'.'</span>)[:, <span>-</span><span>1</span>, :]<span>.</span>clone()
</span></span><span><span>
</span></span><span><span><span>del</span> io_accessors_sample
</span></span><span><span>_ <span>=</span> gc<span>.</span>collect()
</span></span><span><span>
</span></span><span><span>final_block_outputs<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([1000, 384])
</code></pre><p>Next, let‚Äôs create a copy of those outputs scaled by a factor of 10:</p><div><pre tabindex="0"><code data-lang="python"><span><span>scaled_final_block_outputs <span>=</span> final_block_outputs <span>*</span> <span>10</span>
</span></span><span><span>scaled_final_block_outputs<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([1000, 384])
</code></pre><p>Comparing average norms, we see that those of the scaled outputs indeed are 10 times bigger:</p><div><pre tabindex="0"><code data-lang="python"><span><span>final_block_outputs<span>.</span>norm(dim<span>=-</span><span>1</span>)<span>.</span>mean(), scaled_final_block_outputs<span>.</span>norm(dim<span>=-</span><span>1</span>)<span>.</span>mean()
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(22.8909), tensor(228.9091))
</code></pre><p>Now, let‚Äôs put both the original and scaled outputs through the final LayerNorm of the model and calculate the average norm of the results:</p><div><pre tabindex="0"><code data-lang="python"><span><span>layer_normed_original <span>=</span> m<span>.</span>ln_f(final_block_outputs)<span>.</span>detach()
</span></span><span><span>layer_normed_scaled <span>=</span> m<span>.</span>ln_f(scaled_final_block_outputs)<span>.</span>detach()
</span></span><span><span>
</span></span><span><span>layer_normed_original<span>.</span>norm(dim<span>=-</span><span>1</span>)<span>.</span>mean(), layer_normed_scaled<span>.</span>norm(dim<span>=-</span><span>1</span>)<span>.</span>mean()
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(23.1262), tensor(23.1263))
</code></pre><p>They‚Äôre virtually identical.</p><p>In the preceding example, the output norms were so close to identical because the two inputs differed only in scale: they had the same direction, or cosine similarity of 1. Vectors that have different norms and different directions will emerge from the LayerNorm with norms that are still quite similar but a little further apart.</p><p>To see an example, we can add a little noise to one of the vectors and then scale it:</p><div><pre tabindex="0"><code data-lang="python"><span><span>original_vector <span>=</span> final_block_outputs[<span>0</span>]
</span></span><span><span>
</span></span><span><span><span># Add some random noise to the original vector</span>
</span></span><span><span>torch<span>.</span>manual_seed(<span>42</span>) <span># keep the noise consistent</span>
</span></span><span><span>comparison_vector <span>=</span> original_vector <span>+</span> torch<span>.</span>randn_like(original_vector) <span>*</span> <span>0.1</span>
</span></span><span><span>
</span></span><span><span><span># And scale it</span>
</span></span><span><span>comparison_vector <span>=</span> comparison_vector <span>/</span> comparison_vector<span>.</span>norm()
</span></span><span><span>comparison_vector <span>*=</span> <span>10</span> <span>*</span> original_vector<span>.</span>norm()
</span></span><span><span>
</span></span><span><span>original_vector<span>.</span>norm(), comparison_vector<span>.</span>norm(), F<span>.</span>cosine_similarity(original_vector, comparison_vector, dim<span>=-</span><span>1</span>)
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(23.7909), tensor(237.9092), tensor(0.9967))
</code></pre><p>The <code>comparison_vector</code>‚Äôs norm is exactly 10x that of <code>original_vector</code>, but they‚Äôre not perfectly aligned in direction, though still quite close.</p><div><pre tabindex="0"><code data-lang="python"><span><span>m<span>.</span>ln_f(original_vector)<span>.</span>detach()<span>.</span>norm(), m<span>.</span>ln_f(comparison_vector)<span>.</span>detach()<span>.</span>norm()
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(23.1496), tensor(23.1671))
</code></pre><p>Their norms after layer norm are close but further apart than in the previous example.</p><p>If we add a lot more noise, we‚Äôll end up with two vectors with quite different directions:</p><div><pre tabindex="0"><code data-lang="python"><span><span>original_vector <span>=</span> final_block_outputs[<span>0</span>]
</span></span><span><span>
</span></span><span><span><span># Add some random noise to the original vector</span>
</span></span><span><span>torch<span>.</span>manual_seed(<span>4211</span>) <span># keep the noise consistent</span>
</span></span><span><span>comparison_vector <span>=</span> original_vector <span>+</span> torch<span>.</span>randn_like(original_vector) <span>*</span> <span>2</span>
</span></span><span><span>
</span></span><span><span><span># And scale it</span>
</span></span><span><span>comparison_vector <span>=</span> comparison_vector <span>/</span> comparison_vector<span>.</span>norm()
</span></span><span><span>comparison_vector <span>*=</span> <span>10</span> <span>*</span> original_vector<span>.</span>norm()
</span></span><span><span>
</span></span><span><span>original_vector<span>.</span>norm(), comparison_vector<span>.</span>norm(), F<span>.</span>cosine_similarity(original_vector, comparison_vector, dim<span>=-</span><span>1</span>)
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(23.7909), tensor(237.9093), tensor(0.5178))
</code></pre><p>But their norms after layer norm are only a little more divergent:</p><div><pre tabindex="0"><code data-lang="python"><span><span>m<span>.</span>ln_f(original_vector)<span>.</span>detach()<span>.</span>norm(), m<span>.</span>ln_f(comparison_vector)<span>.</span>detach()<span>.</span>norm()
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(23.1496), tensor(23.0546))
</code></pre><p>So in summary:</p><ul><li>The LayerNorm will remove substantial differences in input norms.</li><li>Norms of the outputs from the LayerNorm will vary a little depending on how closely aligned the input vectors were.</li></ul><h3 id="iv-summary-of-experiment-on-relative-impact-of-self-attention-and-feed-forward-network-outputs">IV: Summary of Experiment on Relative Impact of Self-Attention and Feed Forward Network Outputs <a href="#iv-summary-of-experiment-on-relative-impact-of-self-attention-and-feed-forward-network-outputs">üîó</a></h3><p>The <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/70_embedding_adjustments.ipynb" target="_blank" rel="noopener">embedding adjustments analysis notebook</a> contains the implementation of an experiment to understand the relative impact of the self-attention outputs and the feed-forward network outputs on the final output of the transformer. This appendix summarizes the experiment and the results.</p><p>Experiment procedure:</p><ul><li>I ran all 20,000 prompts through the model and captured the final output probability distributions as well as the intermediate self-attention outputs, feed-forward network outputs, and final block outputs for each block.</li><li>For each block, I then ran two tests. First, instead of sending the block output as normally implemented (<code>x + sa_out + ffwd_out</code>, as shown <a href="#block-logic-with-intermediates">earlier</a>) to the next stage of the model, I sent a version that omits the self-attention output i.e. just <code>x + ffwd_out</code>, and saved the final output probability distribution that resulted. Then, I did the same thing but removed the feed-forward network output instead, sending on just <code>x + sa_out</code>.</li><li>I then calculated the Hellinger distance between the probability distribution produced with the regular block output and that produced by each of the two modifications.</li></ul><p>The table below shows the results, averaged across all 20,000 prompts:</p><blockquote><p>For the implementation of this analysis, see <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/70_embedding_adjustments.ipynb" target="_blank" rel="noopener">embedding adjustments analysis notebook</a></p></blockquote><table><thead><tr><th>Block</th><th>H(output(<code>x+sa_out+ffwd_out</code>), output(<code>x+ffwd_out</code>))</th><th>H(output(<code>x+sa_out+ffwd_out</code>), output(<code>x+sa_out</code>))</th></tr></thead><tbody><tr><td>1</td><td>0.11 ¬± 0.07</td><td>0.70 ¬± 0.17</td></tr><tr><td>2</td><td>0.07 ¬± 0.04</td><td>0.19 ¬± 0.11</td></tr><tr><td>3</td><td>0.09 ¬± 0.07</td><td>0.15 ¬± 0.10</td></tr><tr><td>4</td><td>0.06 ¬± 0.05</td><td>0.13 ¬± 0.10</td></tr><tr><td>5</td><td>0.04 ¬± 0.03</td><td>0.14 ¬± 0.10</td></tr><tr><td>6</td><td>0.03 ¬± 0.03</td><td>0.17 ¬± 0.10</td></tr></tbody></table><blockquote><p>Remember that Hellinger distance ranges between 0 and 1, with 0 meaning identical and 1 meaning no overlap. A larger Hellinger distance in this table means a larger divergence between the experiment output and the normal transformer output.</p></blockquote><p>The effect is most pronounced in the first block: omitting the feed-forward network output results in an almost completely different probability distribution (H = 0.70) but omitting the self-attention output results in a very similar distribution (H = 0.11). Across the rest of the layers, the difference is less dramatic, but the feed-forward network output always has the larger impact.</p><blockquote><p>Though I think these results support the notion that an approximation based only on feed-forward network outputs can produce similar results to the transformer, it would be interesting to see if the approximation would improve if we include the self-attention outputs, particularly for some of the intermediate layers. But I‚Äôm leaving that as an area for future investigation.</p></blockquote><h3 id="v-performing-svd-to-get-a-linear-approximation-of-a-token-subspace">V: Performing SVD to Get a Linear Approximation of a Token Subspace <a href="#v-performing-svd-to-get-a-linear-approximation-of-a-token-subspace">üîó</a></h3><p>This appendix walks through an example of how we can find a linear approximation for a token subspace using SVD. First, let‚Äôs load all the embeddings learned for the token <code>a</code> at the output of the last block of the transformer (input to the final layer norm and linear layer):</p><div><pre tabindex="0"><code data-lang="python"><span><span>learned_embeddings_dir <span>=</span> environment<span>.</span>data_root <span>/</span> <span>'learned_embeddings'</span>
</span></span><span><span>multi_emb_a <span>=</span> torch<span>.</span>load(learned_embeddings_dir <span>/</span> <span>'no_blocks'</span> <span>/</span> <span>'lower_a.pt'</span>, map_location<span>=</span>device)
</span></span><span><span>multi_emb_a<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([100, 1, 384])
</code></pre><p>We‚Äôve got 100 different 384-dimensional embedding vectors. Each one, when given as input to the final blocks in the transformer, produces an output distribution that assigns nearly all the probability mass to the token, <code>a</code>. Each one can be thought of as a point in the subspace for token <code>a</code>.</p><p>We can stack these embeddings form a 100x384 matrix:</p><p>$$
\begin{bmatrix}
e_{1,1} &amp; e_{1,2} &amp; \dots &amp; e_{1,384} \\
e_{2,1} &amp; e_{2,2} &amp; \dots &amp; e_{2,384} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
e_{100,1} &amp; e_{100,2} &amp; \dots &amp; e_{100,384}
\end{bmatrix}
$$</p><p>Next, we can run SVD on this matrix:</p><div><pre tabindex="0"><code data-lang="python"><span><span>_, S, V <span>=</span> torch<span>.</span>linalg<span>.</span>svd(multi_emb_a[:, <span>-</span><span>1</span>, :])
</span></span></code></pre></div><p>For this analysis, we‚Äôre only interested in the singular values (<code>S</code>) and the right singular vectors (<code>V</code>). We can plot the singular values:</p><div><pre tabindex="0"><code data-lang="python"><span><span>_ <span>=</span> plt<span>.</span>plot(S<span>.</span>numpy(), <span>'-o'</span>)
</span></span></code></pre></div><p><img src="https://shyam.blog/posts/beyond-self-attention/images/ec5755d7859f77e55c3bf34a432c33226741616e9b493b5eec96c716ac1e7fe5.png" alt=""></p><p>The first singular value is much larger (over 6x) than the next, which suggests the first right singular vector alone might be a good approximation for the subspace that predicts token <code>a</code>. We can test what gets predicted when we use this first right singular vector as an embedding:</p><div><pre tabindex="0"><code data-lang="python"><span><span>v0a <span>=</span> adjust_singular_vector_sign(V[<span>0</span>], multi_emb_a[:, <span>-</span><span>1</span>, :])
</span></span><span><span>logits <span>=</span> LogitsWrapper(accessors<span>.</span>logits_from_embedding(unsqueeze_emb(v0a)), tokenizer)
</span></span><span><span>logits<span>.</span>plot_probs(title<span>=</span><span>'Next Token Probability Distribution from First Right Singular Vector of embeddings for Token "a"'</span>)
</span></span></code></pre></div><p><img src="https://shyam.blog/posts/beyond-self-attention/images/968e8e424522937d5366586abd902715e1cdf771fddbb2d4a36144cbe07746e2.png" alt=""></p><p>In this distribution, <code>a</code> has probability near 1 and every other token has probability near 0. So the first right singular vector is effectively another embedding that produces an output predicting <code>a</code> with near certainty.</p><p>But it‚Äôs different from the other 100 learned embeddings in an important way: it‚Äôs the vector that is best aligned with <em>all</em> of them. More formally, it‚Äôs the vector that minimizes the squared distance to all 100 other embedding vectors. In this way, the first right singular vector is like a good summary of the embedding vectors we started from.</p><p>The first right singular vector is a unit vector (as are all the singular vectors):</p><pre tabindex="0"><code>tensor(1.0000)
</code></pre><p>Any vector along its span will produce an output distribution predicting <code>a</code>, similar to the one above (see <a href="#iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm">Appendix III</a> for an explanation of why the transformer output is invariant to the scale of the final embedding). So the span of this vector is a good, linear approximation to the subspace for token <code>a</code>.</p><p>The same results we saw here for token <code>a</code> hold for the other tokens too. For each, if we stack all the learned embeddings and perform SVD, we find that the first right singular vector forms a good linear approximation of the token subspace.</p></div></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Browser extensions are underrated: the promise of hackable software (473 pts)]]></title>
            <link>https://www.geoffreylitt.com/2019/07/29/browser-extensions</link>
            <guid>39251095</guid>
            <pubDate>Sun, 04 Feb 2024 15:43:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.geoffreylitt.com/2019/07/29/browser-extensions">https://www.geoffreylitt.com/2019/07/29/browser-extensions</a>, See on <a href="https://news.ycombinator.com/item?id=39251095">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure>
  <img src="https://www.geoffreylitt.com/images/article_images/legos.jpg?1705878965" alt="Lego bricks">
  <figcaption>Photo by <a href="https://unsplash.com/photos/2FaCKyEEtis">Rick Mason on Unsplash</a></figcaption>
</figure>

<p>Recent conversations about web browser extensions have focused on controversy: <a href="https://arstechnica.com/information-technology/2019/07/dataspii-inside-the-debacle-that-dished-private-data-from-apple-tesla-blue-origin-and-4m-people/">malicious browser extensions capturing web history</a>, and <a href="https://www.wired.com/story/google-chrome-ad-blockers-extensions-api/?verso=true">Google limiting the capabilities used by ad blockers</a>. These are important discussions, but we shouldn‚Äôt lose sight of the big picture: browser extensions are a special ecosystem worth celebrating.</p>

<p>Among major software platforms today, <strong>browser extensions are the rare exception that allow and encourage users to modify the apps that we use</strong>, in creative ways not intended by their original developers. On smartphone and desktop platforms, this sort of behavior ranges from unusual to impossible, but in the browser it‚Äôs an everyday activity.</p>

<p>Browser extensions remind us what it‚Äôs like to have deep control over how we use our computers.</p>

<h2 id="assembling-our-own-software">Assembling our own software</h2>

<p>Once a software platform reaches a certain level of openness, it can fundamentally change the way that normal users relate to their software. By installing four different Gmail extensions that modify everything from the visual design to the core functionality, in some sense, I‚Äôve put together my own email client. <strong>Instead of being a passive user of pre-built applications, I can start assembling my own personalized way of using my computer.</strong></p>

<p>The popularity of browser extensions proves that many people are interested in customizing their software, and it‚Äôs not just  a hobby for power users. There are over 180,000 extensions on the Chrome store, and nearly half of all Chrome users have browser extensions installed.<sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup> When people have an easy way to extend their software with useful functionality, they apparently take advantage.</p>

<h2 id="hackable-platforms-not-custom-apis">Hackable platforms, not custom APIs</h2>

<p>Browser extensions have remarkably broad use cases. I personally use Chrome extensions that fill in my passwords, help me read Japanese kanji, simplify the visual design of Gmail, let me highlight and annotate articles, save articles for later reading, play videos at 2x speed, and of course, block ads.</p>

<p><strong>The key to this breadth is that most extensions modify applications in ways that the original developers didn‚Äôt specifically plan for.</strong> When Japanese newspapers publish articles, they‚Äôre not thinking about compatibility with the kanji reading extension. Extension authors gain creative freedom because they don‚Äôt need to use application-specific APIs that reflect the original developers‚Äô view of how people might want to extend their application.</p>

<p>The web platform has a few qualities that enable this sort of unplanned extensibility. The foundational one is that the classic web deployment style is to ship all the client code to the browser in human-readable form. (Source maps are a key to preserving this advantage as we ship more code that‚Äôs minified or compiled from other languages.) The web‚Äôs layout model also promotes extensibility by encouraging standardized semantic markup‚Äîmy password manager extension works because web pages reliably use form tags for password submissions instead of building their own version.</p>

<p>Even with these advantages, it can still require clever tricks to modify a site in ways that it wasn‚Äôt built for. But it‚Äôs often a reasonable amount of work, not a years-long reverse engineering effort. The sheer variety of extensions available shows that extension authors are willing to jump through a few hoops to create useful software.</p>

<p>Occasionally there are tensions between website developers and extension authors, but it seems far more common that developers are fine with their sites being extended in creative ways, as long as they don‚Äôt have to do any extra work. Extensions can even make life easier for application developers: if there‚Äôs a niche request that a small minority of users want, a motivated community member can just build an extension to support it. By building on a hackable platform, developers allow their users to get even more value out of their applications.</p>



<p>Many browser extensions are generic tools designed to enhance my use of all websites. I can use my annotation extension on every website everywhere, instead of needing a different highlighting tool for each article I read. Just like using a physical highlighter with paper articles, I can master the tool once, and get a lot of leverage by applying it in different contexts.</p>

<p>In many software platforms, we think of the operating system as providing the cross-cutting tools, and third parties as providing standalone ‚Äúapps‚Äù that are used in isolation. <strong>With browser extensions, third parties are also adding tools;</strong> a single piece of software has the leverage to change my experience across all the apps I use.</p>

<p>When software is built in small units, it also changes the economics. Most extensions I use are free, and are perhaps too small in their feature set to support a full-blown business. And yet, people still choose to make them, and I benefit immensely from these little bits of software. Browsing the extension store feels more like going to a local flea market than going to a supermarket. Massive software built by huge companies isn‚Äôt the only way.</p>

<h2 id="the-origins-of-openness">The origins of openness</h2>

<p>It‚Äôs not an accident that this openness emerged on the web platform.</p>

<p>Since the beginning of personal computing, there‚Äôs been a philosophical tradition that encourages using computers as an interactive medium where people contribute their own ideas and build their own tools‚Äîauthorship over consumption. This idea is reflected in systems like Smalltalk, Hypercard, and more recently, <a href="https://dynamicland.org/">Dynamicland</a>.</p>

<p>When Tim Berners-Lee created the World Wide Web, he imagined it fitting into this tradition. ‚ÄúMy vision was a system in which sharing what you knew or thought should be as easy as learning what someone else knew.‚Äù<sup id="fnref2"><a href="#fn2" rel="footnote">2</a></sup> There were some hiccups along the way<sup id="fnref3"><a href="#fn3" rel="footnote">3</a></sup>, but eventually that vision largely won out, and the Web became a place where anyone can publish their opinions or photos through social media platforms.</p>

<p>Still, there‚Äôs a catch. When you‚Äôre using Facebook, you‚Äôre operating within a confined experience. You‚Äôre forced to publish in a certain format, and to use their app in a certain way (that includes, of course, seeing all the ads). There‚Äôs more room for authorship than just browsing a news website, but only within the strict lines the app has painted for you.</p>

<p><strong>Browser extensions offer a deeper type of control.</strong> Instead of merely typing into the provided text box, we can color outside the lines and deeply modify the way we use any application on the web. Browser extensions offer a kind of decentralization: large companies building major websites don‚Äôt get to dictate all the details of our experience.</p>

<h2 id="improving-on-extensions">Improving on extensions</h2>

<p>We clearly need to work on protecting people from malicious extensions that invade their privacy. But beyond that, here are some bigger picture opportunities I see for improving on extensions:</p>

<p><strong>Accessibility:</strong> Today, it requires a big jump to go from using browser extensions to creating them: you need to learn a fair amount of web development to get started, and you can‚Äôt easily develop extensions in the browser itself. What if there were a quick way to get started developing and sharing extensions in the browser? You could imagine smoothly transitioning from editing a website in the developer tools to publishing a small extension.</p>

<p><em>Update</em>: I‚Äôve started working on a system called <a href="https://sdg.csail.mit.edu/projects/wildcard">Wildcard</a> to work towards this vision.</p>

<p><strong>Compatibility:</strong> Because extensions hook into websites in unsupported ways, updates to websites often result in extensions temporarily breaking, and extension authors scrambling to fix them. Can we make it easier for website developers and extension authors to form stable connections between their software, without necessarily resorting to using explicit extension APIs?</p>

<p>There are existing practices that fit into this category already‚Äîfor example, using clean semantic markup, human-readable CSS, and source maps makes it easier to develop an extension.</p>

<p>A simple change that would allow for more stable extensions would be to give users more control over when they upgrade to new versions of cloud software. If I have a 3 month window to continue using an old version after the new one is released, that would give extension authors more time to upgrade their software for the new version.</p>

<p><strong>Power:</strong> Web extensions are limited in their power by the typical architecture of web applications: they have broad rights to modify the browser client, but the server is off limits. For example, if my social media app‚Äôs server only provides an endpoint for querying my posts in chronological order, no browser extension can ever search through all my posts by keyword. How could we rethink the client-server boundary to enable extensions to make even deeper modifications?</p>

<p>This raises tough questions around security and privacy. The modern browser extension API has done a good job balancing extensibility with security, and yet we‚Äôre still grappling with the consequences of browser extensions invading people‚Äôs privacy. Giving extensions more power would raise the stakes further. Still, we shouldn‚Äôt give up in the name of security‚Äîwe should fight for extensibility as a value and find ways to balance these interests.</p>

<h2 id="the-next-platform">The next platform</h2>

<p>I‚Äôm intrigued by a couple projects that are rethinking the web in ways that might make it more extensible:</p>

<p>The <a href="https://beakerbrowser.com/about/">Beaker Browser</a> and the decentralized web community are exploring how the web works without centralized servers. It seems like their proposed architecture would give users fuller control over modifying the ‚Äúserver‚Äù side of web applications.</p>

<p>Tim Berners-Lee is working on a new project called <a href="https://inrupt.com/blog/one-small-step-for-the-web">SOLID</a>. I don‚Äôt yet understand precisely what they‚Äôre up to, but given Tim‚Äôs involvement I figure it‚Äôs worth paying attention. A key principle is giving users more ownership over their data, which would enable people to use extensions and other software to manipulate their data in flexible ways beyond what application server APIs allow.</p>

<p>Computing is still young, and platforms are changing quickly. Modern browser extensions and smartphone platforms have only been around for about a decade. These platforms will evolve, and there will be new platforms after them, and we will get to collectively decide how open they will be.</p>

<p>Browser extensions give us one example to strive for: a place where we routinely hack the software we use and make it our own. <span>‚ñ™</span></p>

<p><a href="https://news.ycombinator.com/item?id=20556382"><em>Discuss on Hacker News</em>
</a></p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rolling Airframe Missile (166 pts)]]></title>
            <link>https://www.navalgazing.net/RAM</link>
            <guid>39250896</guid>
            <pubDate>Sun, 04 Feb 2024 15:24:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.navalgazing.net/RAM">https://www.navalgazing.net/RAM</a>, See on <a href="https://news.ycombinator.com/item?id=39250896">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>I've previously discussed <a href="https://www.navalgazing.net/Standard-Part-2">Standard</a>, <a href="https://www.navalgazing.net/Sea-Sparrow">Sea Sparrow</a>, <a href="https://www.navalgazing.net/ESSM">ESSM</a> and <a href="https://www.navalgazing.net/Phalanx">Phalanx</a>, but there is one last air-defense weapon that deserves discussion.  This is the <a href="https://en.wikipedia.org/wiki/RIM-116_Rolling_Airframe_Missile" rel="nofollow">RIM-116 Rolling Airframe Missile</a>, better known as RAM.
</p>
<p><span> <img width="600" src="https://www.navalgazing.net/attach/RAMLaunchGreenBay.jpg?v=1706810557.jpg" alt=""><br><span>A RAM is launched from USS <em>Green Bay</em>  </span></span></p>
<p>Much like the other point-defense systems, RAM's origins trace back to <em><a href="https://www.navalgazing.net/Eilat">Eilat</a></em>, and the panic that it provoked within the USN.  It was conceived to work in pretty much the same niche as Phalanx, providing a last-ditch defense against incoming anti-ship missiles.  Effective as it was, Phalanx had a serious limitation, even while it was still in development.  The use of a gun limited effective range to no more than 1500 yards, which was a serious problem in the face of supersonic missiles.  The available window to engage such a weapon was short, and even if the Phalanx did shoot it down, the debris was likely to strike the defended ship.  The obvious solution was to use a missile, which could engage at significantly longer range.
<a name="break" id="break"></a>
</p>
<p>The initial program that led to RAM was based on the <a href="https://en.wikipedia.org/wiki/FIM-43_Redeye" rel="nofollow">Redeye missile</a>, the first American man-portable SAM system, although it would be fitted with a combined radar/IR seeker to allow it to engage closing targets (which were difficult to engage purely with IR seekers at the time) at reasonable range.  The only real concern was the small size of the missile, 2.75" in diameter and about 18 lb, and Congress directed the Navy to study something the size of Sidewinder, 5" and about 160 lb, instead.  The initial contract was signed with General Dynamics in 1976, with West Germany coming onboard as a development partner.  Development didn't go particularly smoothly, with delays from testing and cost adding up to around 5 years, and both the US and Germany came close to withdrawing from the program at various points.  But things were eventually worked out, and RAM entered in the early 90s.
</p>
<p><span> <img width="600" src="https://www.navalgazing.net/attach/RAMMissileFlightGHWB.jpg?v=1706810558.jpg" alt=""><br><span>A RAM in flight</span></span></p>
<p>RAM is a rather unusual missile.  It gets its name because it is fired from a rifled tube and rolls in flight thanks to the tube and four fins.  The roll allows it to use only two control fins in flight, instead of the usual four.  The basic airframe, motor, fuze and warhead initially came from the <a href="https://en.wikipedia.org/wiki/AIM-9_Sidewinder" rel="nofollow">Sidewinder</a>, while the IR seeker was derived from the <a href="https://en.wikipedia.org/wiki/FIM-92_Stinger" rel="nofollow">Stinger</a>.  Because RAM was expected to be fired at incoming targets, where the hot engine would not be visible, the seeker would need to rely on glint (reflected IR radiation from the sun), which sharply limited range. As a result, initial guidance would be provided by a passive RF system, which could home in on the radar seeker of a typical cruise missile.<a id="fnr1_1" href="#fn1_1"><sup>1</sup></a>  The rolling missile could also get away with a 2-sensor radar inferometer, instead of requiring four sensors, like a more conventional missile.  The accuracy of the RF seeker was limited, hence the inclusion of the IR seeker, but it was in theory possible for the missile to use it all the way to the target if it's dark or cloudy and the IR seeker doesn't work.  The wide cone of the RF sensor also allows the missile to be fired "around the corner", reducing the size of sectors blocked by the ship's structure by 10-15¬∞.
</p>
<p><span> <img width="600" src="https://www.navalgazing.net/attach/SailorLoadingRAMTruman.jpg?v=1706810695.jpg" alt=""><br><span>Sailors load a RAM launcher aboard the <em>Truman</em></span></span></p>
<p>But there were serious concerns about the effectiveness of RAM against missiles that used IR or semi-active homing, so even before the Block 0 missile entered service, work began on Block 1, with an imaging IR seeker that is capable of searching for targets on its own.  It still has the RF seeker, and is capable of using the original dual-sensor mode, homing entirely on IR or switching to IR search if it loses RF track.  Block 1 entered service in 1999, and achieved a 95% success rate in intercepting incoming missiles across 180 trials.  A further upgrade took place in 2002, to give better performance against helicopters, slow aircraft and surface targets.  This upgrade, known as HAS, was implemented entirely as a software upgrade.  In the mid-2000s, a second upgrade, Block 2, was started.  It was a considerably bigger overhaul than Block 1, with a new 6.25" motor<a id="fnr1_2" href="#fn1_2"><sup>2</sup></a> (which increased range significantly, from 6 nm to 10 nm), a 4-fin steering system, and an improved RF seeker.  Block 2 was cleared for service in 2015, and is currently in production.
</p>
<p><span> <img width="600" src="https://www.navalgazing.net/attach/JMSDF_DDH-183_SeaRamIzumo.jpg?v=1706810829.jpg" alt=""><br><span>SeaRAM onboard Japanese helicopter destroyer <em>Izumo</em></span></span></p>
<p>The fire-and-forget nature of RAM meant that it imposed relatively minimal burdens on the firing ship's combat system, particularly given the ability of the missile to search for targets after launch.  The combat system still mattered in terms of firing at the correct time and in the right direction, but it opened up new possibilities for smaller ships that couldn't support Sea Sparrow or the like.  The most extreme version of this was SeaRAM, which was essentially a <a href="https://www.navalgazing.net/Phalanx">Phalanx</a> system with the gun removed and replaced by an 11-round RAM launcher.  Like Phalanx, it is capable of operating independently of the ship that carries it, automatically detecting and engaging incoming missiles.  SeaRAM is primarily carried by the <a href="https://www.navalgazing.net/LCS-Part-1"><em>Independence</em> class LCS</a>, although a few <a href="https://www.navalgazing.net/The-Arleigh-Burke-Class"><em>Burke</em>s</a> are also fitted with the system to provide some defense against cruise missiles when the main radar is in ballistic missile defense mode.
</p>
<p><span> <img height="440" src="https://www.navalgazing.net/attach/RIM-116_Rolling_Airframe_Missile_Launcher_Ozelot.jpg?v=1706810828.jpg" alt=""><br><span>A RAM launcher on the German missile boat <em>Ozelot</em></span></span></p>
<p>But the more common launcher is the 21-round Mk 49, a trainable launcher integrated with the ship's combat system and fitted to a number of ships, including all American aircraft carriers and amphibious ships, as well as the <em>Freedom</em> class LCS.  Germany has equipped all of its warships with RAM, while Egypt, Greece, Japan, Mexico, Qatar, South Korea, Saudi Arabia, Turkey and the UAE have bought the system for their ships as well.  If anything, RAM seems destined for wider service in the years to come.  It is the most minimal system capable of providing protection against anti-ship missiles, a threat that has been graphically demonstrated in the Red Sea in recent months, and which is only likely to continue to proliferate.
</p>
<hr>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Plastic bans work. Billions of plastic bags were avoided in the US alone (237 pts)]]></title>
            <link>https://www.zmescience.com/science/news-science/plastic-bans-work-billions-of-plastic-bags-were-avoided-in-the-us-alone/</link>
            <guid>39250434</guid>
            <pubDate>Sun, 04 Feb 2024 14:28:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.zmescience.com/science/news-science/plastic-bans-work-billions-of-plastic-bags-were-avoided-in-the-us-alone/">https://www.zmescience.com/science/news-science/plastic-bans-work-billions-of-plastic-bags-were-avoided-in-the-us-alone/</a>, See on <a href="https://news.ycombinator.com/item?id=39250434">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
								
<p>‚ÄúThe bottom line is that <a href="https://www.zmescience.com/ecology/environmental-issues/congo-bans-plastic-bags-321313/">plastic bag bans</a> work,‚Äù said Faye Park, president of the U.S. PIRG Education Fund, in a statement. ‚ÄúPeople realize quickly it‚Äôs easy to live without plastic bags and get used to bringing a bag from home or skipping a bag when they can.‚Äù</p>



<figure><a href="https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-scaled.jpg"><picture><source srcset="https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1024x683.webp 1024w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1536x1024.jpg 1536w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-2048x1365.jpg 2048w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-750x500.jpg 750w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1140x760.jpg 1140w" sizes="(max-width: 1024px) 100vw, 1024px" type="image/webp"><img src="https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1024x683.jpg" height="683" width="1024" srcset="https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1024x683.jpg 1024w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1536x1024.jpg 1536w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-2048x1365.jpg 2048w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-750x500.jpg 750w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1140x760.jpg 1140w" sizes="(max-width: 1024px) 100vw, 1024px" alt="plastic bag waste" fetchpriority="high" decoding="async"> </picture></a><figcaption>Image via Unsplash.</figcaption></figure>



<h2 id="plastic-ain-t-all-that-fantastic">Plastic ain‚Äôt all that fantastic</h2>



<p>Plastic bags are a victim of their own success. When they <a href="https://www.unep.org/news-and-stories/story/birth-ban-history-plastic-shopping-bag">were first patented in Europe in 1965</a>, society was shocked to see how cheap and durable they could be. Within a decade or two they became mainstream on the continent and in North America, and it wasn‚Äôt long before they started being widely used on the entire planet.</p>



<p>But plastics were just a little too durable. They didn‚Äôt go away. They started accumulating in landfills and in the oceans. The environmental impact of plastic bags gained attention with the discovery of the Great <a href="https://www.zmescience.com/ecology/pollution-ecology/great-pacific-garbage-patch-06102016/">Pacific Garbage Patch</a> in 1997. Plastic bags (and plastic in general) had left its mark on the planet in an unprecedented form of pollution.</p><!-- Tag ID: zmescience_300x250_InContent -->





<p>Fast forward a couple more decades, and countries started fighting their urge to use <a href="https://www.zmescience.com/research/materials/cheap-fabrics-from-plastic-15032021/">cheap plastics</a> and implement bans or other measures against plastic bags ‚Äî and finally, there‚Äôs some good news.</p>



<p>San Francisco pioneered the movement in the U.S. by passing the <a href="https://www.zmescience.com/ecology/environmental-issues/bottled-water-ban-national-park-43423/">nation‚Äôs first plastic bag ban</a> in 2007. Several other U.S. cities and <a href="https://www.zmescience.com/ecology/hawaii-bans-plastic-bags-04062012/">states implemented plastic bag bans</a> or restrictions. By 2023, ten states had statewide bans, with similar laws proposed in others‚Äã‚Äã. To get a state of how much this of a difference this made, five studied bans resulted in an average elimination of <a href="https://www.zmescience.com/ecology/plastic-bag-tax-uk-22112016/">almost 300 plastic bags</a> per person per year‚Äã‚Äã. Overall, in the US alone, <a href="https://www.zmescience.com/research/tea-plastic-particles-ocean-234523521/">billions of plastic bags</a> were avoided with anti-plastic bag measures.</p>




<h2 id="h-the-case-against-plastic">The case against plastic</h2>



<p>The case against plastic bags is straightforward.  Plastic pollution kills at least <a href="https://wwf.org.au/blogs/plastic-in-our-oceans-is-killing-marine-mammals/">100,000 marine mammals</a> and <a href="https://sustainabledevelopment.un.org/content/documents/Ocean_Factsheet_Pollution.pdf">1 million seabirds</a> every year and entanglement in plastic and other types of litter kills roughly 1,000 turtles per year. Plastic bags aren‚Äôt responsible for all of that, but they make up an important part of the problem.</p>







<blockquote>

</blockquote>



<p>The results, which were published in a report, also highlight that imperfect measures leave loopholes or encourage buyers to opt for other single use bags.</p>



<blockquote>
<p>Well-designed <a href="https://www.zmescience.com/science/news-science/europe-single-use-plastic-24102018/">single-use plastic bag bans</a> across the country have successfully reduced single-use plastic bag consumption, cut down on plastic bag litter and driven consumers to make more sustainable bag choices. Policymakers should pursue these policies at the state and local levels,‚Äù the report says.</p>
</blockquote>



<p>The idea isn‚Äôt to shift from one type of single-use bag to another type of single-use bag. Paper bags are easier to recycle than plastic, but they take 3-4 times more energy to produce and usually generate more solid waste.</p><!-- Tag ID: zmescience_300x250_InContent_3 -->




<p>Ultimately, the report concludes that regulation is the best current way to address plastic waste and plastic pollution.</p>



<blockquote>
<p>‚ÄúGrocery stores, restaurants and retail shops should not be permitted to distribute plastic film bags of any thickness at checkout. Stores should be required to charge a fee of at least 10 cents for single-use paper bags. A 10-cent paper bag fee will limit the expected increase in paper bag use after a bag ban is imposed and may even reduce paper bag consumption altogether.‚Äù</p>



<p>‚ÄúLocal and state governments should conduct regular enforcement to ensure compliance.‚Äù</p>
</blockquote>



<p>You can read the <a href="https://environmentamerica.org/center/resources/plastic-bag-bans-work/">report in its entirety here</a>.</p>

<!-- AI CONTENT END 1 -->
								
								
																	
																	
							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Write code for the web - Apple doesn't care about you, Mr. Developer (419 pts)]]></title>
            <link>https://mrmr.io/apple/</link>
            <guid>39250406</guid>
            <pubDate>Sun, 04 Feb 2024 14:24:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mrmr.io/apple/">https://mrmr.io/apple/</a>, See on <a href="https://news.ycombinator.com/item?id=39250406">Hacker News</a></p>
<div id="readability-page-1" class="page"><div tabindex="-1" id="___gatsby"><main><div><h3>Write code for the web</h3><p>This is a yarn of three threads, and it got a bit long. The tldr is</p>
<ol>
<li>
<p>Apple doesn‚Äôt care for me as a developer</p>
</li>
<li>
<p>I should write code for the web</p>
</li>
<li>
<p>Nothing is set in stone, really</p>
</li>
</ol>
<p>The story is personal, but I hope readers find something they can relate to in
their own life stream (I guess that's the point of blogs)</p>
<h3>Apple doesn't care for you, Mr. Developer</h3>
<p>Apple cares for me as a customer, but it doesn‚Äôt care for me as a developer.
This dynamic had been subconsciously griefing me for years, until I was able to
formulate it consciously recently.</p>
<p>The dependency goes <em>Developer -&gt; Apple</em> and <em>Apple -&gt; Consumer</em>, there is no
reverse arrow from Apple to the developers.</p>
<p>If all developers stopped building for Apple's platforms tomorrow, Apple will
still survive, almost intact, since it doesn‚Äôt hurt their core value prop (I'll
expand on this below). Apple has no dependency on individual developers. They
care for and need to collaborate with corporate dev ‚Äúpartners‚Äù, but that's
different.</p>
<p>Since companies, especially ginormous multinationals, behave in purely game
theoretic cost/benefit terms, and since Apple has no reason to care about
developers: indeed it doesn‚Äôt.</p>
<blockquote>
<p>Just to restate the obvious - such a stance isn't necessarily the case. There
are other multinationals who've figured that developers form a core part of
their strategy.</p>
</blockquote>
<p>This realisation has made me happier since I now know my place. I can like their
products without wanting to develop for them.</p>
<hr>
<p>Google has a bug. I have dynamic light/dark mode, and if I search on Google at
night, the first page shows up in light mode. With the rest of my machine in a
subdued state, the glaring white of the background hurts my eyes.</p>
<p>In the morning, my laptop automatically switches back to light mode. Now when I
search on Google, the results first show up in an unreadable black background.</p>
<p>One would think that of all the leetcode certified staff, there must be someone
there who would know an O(n) algorithm for fixing this bug. But no, this bug has
persisted for years (I've counted), and it is unlikely to be fixed in the future
unless it gets accidentally fixed as part of some overhaul.</p>
<p>Apologies for the snide, I know some great people who work there. My point is
that Google isn't fixing this bug not because it doesn‚Äôt know how to, but
because it doesn‚Äôt care. This bug has zero impact on its bottom line.</p>
<p>The people like me who use alternate search engines like DDG have already moved
on years ago. The rest of the (overwhelming) majority is stuck with Google. No
matter how bad their search is - UX or results - they have a captive audience.</p>
<blockquote>
<p>I think DDG etc also have a marketing blind spot - they keep pitching
themselves as a more privacy friendly alternative, they never go after the
main course. I don‚Äôt use DDG because of its privacy benefits, I use it because
it reminds me of early Google - simple low clutter UX, good quality verbatim
search results, and unobtrusive ads.</p>
</blockquote>
<p>Okay Manav, but I thought you were ranting about Apple, why bring in Google?</p>
<p>Google is an example where I never grieved much because I understood the
dynamics. I know that I, the customer, am not their game theoretic target. So
when I have to invariably use their products and face another user hostile
interaction, I try to shrug it off and move on. I know that Google has entered a
rent seeking phase, and while it is sad that the world is giving all its video
content up to it for even more of a hostage situation in the future, but that‚Äôs
for governments to deal with.</p>
<p>With Apple I didn‚Äôt understand the dynamics.</p>
<hr>
<p>2009-ish. I struggled to find a computer for my mom. Windows (at that time, I
don't know about now) was just too insecure. Linux required constant tech
support. Eventually I prepared for her an OpenBSD machine running Firefox and a
basic game (Bubbles I think).</p>
<p>Obviously, this was not ideal. It fulfilled some goals - it worked without
requiring any tech support when I wasn't around, and I was ensuring her data
safety and privacy - and she was surprisingly happy with too, but I was not
happy about how this was such a shrivelled parody of what things could be, and
how it limited the ways she could use computers to enrich her life.</p>
<p>Around this time, I joined a new job as a developer for a company that was
making iOS apps. After a week or so of using the mac at work it hit me - <em>this
is the computer I wanted for my mom!</em></p>
<p>That is Apple‚Äôs value prop.</p>
<p>As soon as I had saved enough I bought her a MacBook. And heartfeltly thanked
Steve Jobs for engendering it.</p>
<p>The earth has done many a revolutions since then, and Jobs has left earth, and
the form factor my mother uses has changed from a laptop to an iPad. But it
still satisfies that core value prop.</p>
<p>Let's consider a different context. Even if there were no apps in my phone, I
would still buy an iPhone. For myself likely, but most certainly for her.</p>
<p>The people running Apple know all this. In the deep dungeons of Menlo Park when
there are meetings of the core council, after all the sacrificial lambs have
been slain and the blood and gore washed away, out comes the elder spreadsheet
that encodes Apple‚Äôs business model, but nowhere in them is any cell, input or
output, which involves developers. Sure, Apple doesn't mind if developers are
also happy. But it knows it doesn‚Äôt need to care if they are.</p>
<p>This lack of caring is never expressed out loud. It isn‚Äôt some conspiracy, it
just is one of those things - you wouldn‚Äôt walk up to someone who you don‚Äôt care
about and tell them you don‚Äôt care.</p>
<p>Unfortunately all this results in sometimes schizophrenic behaviour on Apple‚Äôs
part, as there are many individuals working at Apple who <em>do care</em> about
developers and are trying to make things better.</p>
<hr>
<p>2016-ish. As part of my annual Apple simping I was watching WWDC when they
announced Apple Music APIs.</p>
<p>I was ecstatic. My coworkers were puzzled at my ecstasy, ‚ÄúAll this, can‚Äôt we
already do with Spotify‚Äôs API?‚Äù. I didn‚Äôt know how to answer that, so I just
repeated how <em>this changes everything</em>.</p>
<p>Of course, and as is usual, I was wrong. It didn‚Äôt change everything. In fact,
it didn‚Äôt change <em>anything</em>.</p>
<p>Apple‚Äôs own music player was, and still is, unusably bad. Even talking about it
makes me angry. The people making it can‚Äôt be so incompetent, and it seems to be
working for the rest of the world, so I've never known what to make of this
situation.</p>
<p>With the APIs out, I'd thought maybe things will change. But nothing happened.
Apple‚Äôs own player continues to make my blood pressure high anytime I have to
use it. And whatever alternative players I have tried just all seem to go for
the same generic ‚ÄúSpotify‚Äù approach to music.</p>
<blockquote>
<p>I think it is because the people who're making these apps were never around in
the Justin Frankel era of Winamp, and haven't seen how a music player can
provide a fast, seamless, endless <em>yet</em> still personal approach to music.</p>
<p>That era is not coming back because it relied on piracy, but luckily we don't
need to anymore - that's the great thing about Apple's music catalog! We can
recreate that experience without needing to sail the high seas.</p>
</blockquote>
<p>So that's the backstory. Now recently I had a bunch of free time, and thought
that I‚Äôll write a music player. Mostly for myself, but I also wanted to write a
tutorial. This is what I wrote in the README in the first commit:</p>
<blockquote>
<p>Here I'll be writing down my notes as I build Flowers. The world needs not one
music player, or two, but many: each of us has our own way of connecting with
music, and so maybe these notes will help others build the flower they want,
nay need.</p>
</blockquote>
<p>Cute, dumb, and in vain.</p>
<p>As I went about it, I realized that 8 years down the line, not only is the API
still buggy, it is also still not public!</p>
<p>Firstly you need to pay Apple. No, not for bulk usage etc, but <em>just to try out
the API</em>. So there there goes my dream of writing a walkthrough ‚Äì nobody‚Äôs going
to pay Apple 100 bucks just to try things out. And it also partially explains
why there has been no innovation.</p>
<p>But that's not even it - Even if you pay them, you get a restricted API.</p>
<p>The point where I disgustedly gave up was when I found out that while I was
jumping all these kafkaesque hoops, instead you could just go to Apple's own web
music player, type <code>MusicKit.getInstance().developerToken</code> in your browser
console, and you‚Äôll get an unrestricted root token for free! </p>
<hr>
<p>All these anecdotes Manav, what does all this mean?</p>

<p>Write code that runs on the web. We‚Äôre lucky to have a shared platform that no
single entity owns. Even benevolence can be ruined by incompetence.</p>
<p>The web platform is in a precarious place ‚Äì overreaching governments, browser
duopolies, a complex developer ecosystem ‚Äì so it is not a given it‚Äôll remain
thriving. But so far it has survived. And every year longer it survives, the
more the chances that it‚Äôll continue to thrive in the future.</p>
<p>Ironically Google is the good guy here, they‚Äôre doing great work for the web. On
the other hand, literally every single workaround I‚Äôve had to write in the
recent past in web related code has been due to Safari's princessness.</p>
<h2>Nothing is set in stone</h2>
<p>Which brings me to the third thread of this story, how all this made me
reevalute my relationship with companies.</p>
<p>Someone I know, someone who has a better grip on living than me, told me once
that it's not useful to put people into the buckets of good and bad. People are
a mix. Bad folks can do good actions sometimes, and vice versa.</p>
<p>I don't know to what extent I've been able to internalize their message, but
that's for another day. What I realized is that the same applies to companies.</p>
<p>Companies are like people. I don't know if they're sentient, but otherwise they
share many attributes with us: they're intelligent (they were the AI before AI),
have personalities, they are born, thrive, live and die, they're even legal
persons.</p>
<p>Just like we can't live without other people, we can't live without companies.
They have many inhumane characteristics, but like them or not, such fractal
conceptions of human organizations will always be around.</p>
<p>By not permanently bucketing companies into good or bad, I can have a more fluid
interaction with them - I can reduce my dependence on them when they try to put
me in a zero sum game, or reengage more with them if they're willing to be more
symbiotic. Nothing is set in stone, really.</p>
<hr>
<blockquote><p>I think most large companies and medium-size companies, and even small
companies, are starting to look at the web as the ultimate direct-to-customer
distribution chain, bypassing all middlemen, going directly from the supplier to
the consumer.</p><p>‚Äì Steve Jobs, Make Something Wonderful</p></blockquote></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A reasonable configuration language (119 pts)]]></title>
            <link>https://ruudvanasseldonk.com/2024/a-reasonable-configuration-language</link>
            <guid>39250320</guid>
            <pubDate>Sun, 04 Feb 2024 14:12:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ruudvanasseldonk.com/2024/a-reasonable-configuration-language">https://ruudvanasseldonk.com/2024/a-reasonable-configuration-language</a>, See on <a href="https://news.ycombinator.com/item?id=39250320">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="content" itemscope=""><header><p>written by <br>published <time datetime="2024-02-04" itemprop="datePublished">4 February, 2024</time></p></header><p><span>About six months ago</span>, I was fed up with it. The particular <em>it</em> was <abbr>HCL</abbr> ‚Äî Hashicorp Configuration Language ‚Äî but that was just the trigger, it was hardly the only offender. The issue I was struggling with that day was to define six cloud storage buckets in Terraform. They were similar, but not quite identical. The kind of thing you‚Äôd do with <a href="https://github.com/ruuda/rcl/blob/bedbd3eea1129ba6053427d67b77a955240ceca8/examples/buckets.rcl#L9-L10">a two-line nested loop</a> in any general-purpose language, but where all the ways of achieving that in <abbr>HCL</abbr> were so much hassle, that is was far simpler to just copy-paste the config six times.</p><p>Although this <abbr>HCL</abbr> episode was the droplet, my bucket of frustration had been filling up for a long time:</p><ul><li><strong>GitHub Actions workflows</strong> that differ in only a few commands ‚Äî there is no native way to abstract those. The same applies to jobs within a workflow.</li><li><strong>Kubernetes manifests</strong> that are 80% the same for most applications, and an entire industry that fails to adopt a proper solution for this, and instead resorts to templating yaml, which to me is <a href="https://ruudvanasseldonk.com/2023/01/11/the-yaml-document-from-hell#templating-yaml-is-a-terrible-terrible-idea">very obviously very wrong on so many levels</a>.</li><li><strong>The prevalence of yaml in general</strong>, a format that does solve some problems (adding comments and a lighter syntax to json), but in the process introduces <a href="https://ruudvanasseldonk.com/2023/01/11/the-yaml-document-from-hell">so many new problems</a> that the cure is almost as bad as the disease.</li><li><strong>Ansible playbooks</strong> that are too similar to justify duplicating, but different enough that parametrizing over data is insufficient. Related to this, the parameter data is difficult to share between Ansible and other tools.</li></ul><p>So that day, when I was in a particularly defiant mood, I decided to write my own configuration language. With list comprehensions. And types.</p><p><img alt="I‚Äôll build my own configuration language. With list comprehensions. And types." src="https://ruudvanasseldonk.com/images/ill-build-my-own-configuration-language.png" width="1024" height="768"></p><p>I never expected or intended for it to go anywhere ‚Äî it was just a way to vent. But six months later, <a href="https://github.com/ruuda/rcl"><em>Ruud‚Äôs Configuration Language</em></a> is no longer completely vaporware. I find it increasingly useful, and I think it might benefit others too. So let‚Äôs dive in!</p><h2 id="a-functional-foundation"><a href="#a-functional-foundation"></a>A functional foundation</h2><p>To be clear, I‚Äôm not criticizing the designers of Ansible or <abbr>HCL</abbr>. The limits of these tools are a natural consequence of their organic growth: you start out with a tool that needs simple configuration, adoption grows and people start doing more complex things with it, and suddenly you find yourself without a good way to do abstraction. So as a quick stopgap, you <a href="https://developer.hashicorp.com/terraform/language/v1.7.x/meta-arguments/for_each">bolt on</a> control flow <a href="https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_loops.html#standard-loops">encoded inside</a> the <a href="https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#jobsjob_idstrategymatrix">data format</a>, because that‚Äôs easy to do within the limits of the existing syntax.</p><p>When it comes to adding more principled abstraction features, the authors have a background in infrastructure administration, not in language design or type theory. So they accidentally implement <a href="https://developer.hashicorp.com/terraform/language/v1.7.x/functions/flatten">some functions</a> in an ad-hoc way that seemed helpful, but causes surprises down the line. (A <code>flatten</code> that <em>sometimes</em> flattens recursively can‚Äôt be typed properly, which breaks generic code.) Many of JavaScript and <abbr>PHP</abbr>‚Äôs idiosyncrasies can be explained in the same way.</p><p>The <a href="https://nixos.org/manual/nix/stable/language/index.html">Nix language</a> had a more solid foundation in functional programming from the start, which enables abstraction in a natural way. Even though it predates Terraform by more than a decade, the language has stood the test of time far better than <abbr>HCL</abbr>. With very few changes, it scaled to massive configuration repositories like <a href="https://github.com/NixOS/nixpkgs">Nixpkgs</a>, and although Nix has issues, abstracting repetition away is not one of them. I‚Äôve used Nix to generate repetitive GitHub Actions workflows, and of course it is at the heart of NixOS, where it generates configuration files such as systemd units from a consistent declarative specification. This is the power of having few simple features that compose well.</p><p>Though Nix is great, I don‚Äôt think it is the answer to all configuration problems. Nix-the-language is intimately tied to Nix-the-package-manager and the Nix store, and the <abbr>ML</abbr>-style syntax can look foreign to people who are used to more mainstream languages. Still, Nix has many good ideas that have been proven to work, and my own configuration language is heavily inspired by it.</p><p>The other language that I take a lot of inspiration from is Python. Python is not primarily a functional language, but you can certainly use it in that way (avoid mutation, write pure functions, prefer list comprehensions over loops, etc.), and this is very natural. I find the syntax pleasant and readable: the meaning of idiomatic Python code is clear even to people who are not intimately familiar with the language. As a configuration language, Python is not bad! In fact, I‚Äôve <em>also</em> used Python to generate repetitive GitHub Actions configurations. List and dict literals are very similar to json, and with functions, list comprehensions, and format strings, there is ample room to abstract repetitive configuration. Types can help to document and enforce structure.</p><p>But like Nix, I don‚Äôt think that Python is the answer to all configuration problems. A Python file is still primarily code, not data. You can have an entry point <code>json.dump</code> data to files or stdout, but it‚Äôs not always easy to import or evaluate intermediate pieces in isolation. Python‚Äôs module system is great for larger codebases, but less suitable for sharing pieces of data between many small scripts.</p><p>For my own language, I took the parts that I like about Nix: functional, more data than code, but with enough room to code when needed, and simple features that compose well. I took what I like about Python: the clean and familiar syntax, list comprehensions, format strings, and types. And consciously or unconsciously, I‚Äôm influenced by many more languages that I‚Äôve been exposed to. Those ideas I combined into a language that <em>I</em> like working with.</p><h2 id="oh-no-yet-another-configuration-language"><a href="#oh-no-yet-another-configuration-language"></a>Oh no, yet another configuration language!</h2><p>I am not the first person to be frustrated by the lack of abstraction features in various tools, nor am I the first person to think that a configuration language would solve that. There exist more configuration languages than I can count on one hand already (see <a href="#appendix-a-non-exhaustive-list-of-configuration-languages">the appendix</a>), and probably many more that I‚Äôm not aware of. So why add one more to the mix? Why is <em>this one</em> going to <em>really</em> solve all our problems, when five more mature ones haven‚Äôt seen widespread adoption (yet)?</p><p>First of all, I did not start out writing my own language thinking it would be a viable alternative to existing configuration languages. I started it to vent, because I find it fun to work on, because it‚Äôs a good learning exercise, and because I can do things in exactly the way that <em>I</em> want to. Dhall has been around longer, has wider support, and a bigger community. But I don‚Äôt really like the syntax and the way it names some things. That‚Äôs a superficial complaint, and if I was looking for a tool to solve my configuration problems with the least amount of effort, then I can set my taste aside ‚Äî I‚Äôll get used to it. But for a personal project that I spend my free time on, I enjoy exploring ideas and building exactly the tool that <em>I</em> want to have.</p><p>So that‚Äôs how it started, as a toy project. I put a big vaporware warning on it, expecting that I would lose interest in it before it got to a point where it was useful. It‚Äôs certainly <a href="https://ruudvanasseldonk.com/2017/04/27/a-language-for-designing-slides">not the first time</a> that I‚Äôm writing a toy language that stalled, and maybe this one will meet the same fate. (I do still occasionally use Pris, and occasionally I get excited about adding features, but it‚Äôs mostly abandoned, like many of my side projects.) But then my tool started being useful. First in unexpected places (as a <code>jq</code> replacement, more on that below), and as I added features, in more places, to the point where now ‚Äî despite its shortcomings ‚Äî I would prefer it over some of the tools that I use at my day job.</p><p>So now what, is it a Serious Software Project now? No, it‚Äôs still a hobby project without stability promise. I don‚Äôt recommend using it for anything serious. But it‚Äôs also <em>useful</em> to the point where I expect I‚Äôll keep it in my toolbelt for the forseeable future ‚Äî if only as a <code>jq</code> replacement. And if it‚Äôs useful to me, maybe it‚Äôs useful to others, so that‚Äôs why I‚Äôm writing about it today.</p><h2 id="ruuds-configuration-language"><a href="#ruuds-configuration-language"></a>Ruud‚Äôs Configuration Language</h2><p>So what is this language? I call it <abbr>RCL</abbr>, named after myself in Bender meme style, but it turns out that <code>rcl</code> is a pretty good file extension and name for a command-line tool. If you prefer, it might stand for Reasonable Configuration Language, or, in classic <abbr>GNU</abbr> style, for <abbr>RCL</abbr> configuration language.</p><p>The language is a superset of json. This makes it easy to export data from many tools and incrementally upgrade it to <abbr>RCL</abbr>, including from yaml: just serialize it to json, and you‚Äôre good to go. This is a valid <abbr>RCL</abbr> document:</p><div id="cb1"><pre><code><span id="cb1-1"><span>{</span></span>
<span id="cb1-2">  <span>"buckets"</span><span>:</span> <span>[</span></span>
<span id="cb1-3">    <span>{</span></span>
<span id="cb1-4">      <span>"name"</span><span>:</span> <span>"bucket-0"</span><span>,</span></span>
<span id="cb1-5">      <span>"location"</span><span>:</span> <span>"eu-west1"</span><span>,</span></span>
<span id="cb1-6">      <span>"delete-after-seconds"</span><span>:</span> <span>86400</span></span>
<span id="cb1-7">    <span>}</span><span>,</span></span>
<span id="cb1-8">    <span>{</span></span>
<span id="cb1-9">      <span>"name"</span><span>:</span> <span>"bucket-1"</span><span>,</span></span>
<span id="cb1-10">      <span>"location"</span><span>:</span> <span>"eu-west1"</span><span>,</span></span>
<span id="cb1-11">      <span>"delete-after-seconds"</span><span>:</span> <span>86400</span></span>
<span id="cb1-12">    <span>}</span></span>
<span id="cb1-13">  <span>]</span></span>
<span id="cb1-14"><span>}</span></span></code></pre></div><p>It‚Äôs 2024, so <abbr>RCL</abbr> has some features that you might expect from a ‚Äúmodern‚Äù language: trailing commas and numeric underscores. Furthermore, dicts can be written with <code>ident = value</code> syntax to omit the quotes and reduce some line noise:</p><pre><code>{
  <span>buckets</span> = [
    {
      <span>name</span> = <span>"bucket-0"</span>,
      <span>location</span> = <span>"eu-west1"</span>,
      <span>delete-after-seconds</span> = <span>86_400</span>,
    },
    {
      <span>name</span> = <span>"bucket-1"</span>,
      <span>location</span> = <span>"eu-west1"</span>,
      <span>delete-after-seconds</span> = <span>86_400</span>,
    },
  ],
}</code></pre><p>There are arithmetic expressions as you would expect, list comprehensions, format strings, and functions:</p><pre><code>{
  <span>buckets</span> = [
    <span>for</span> <span>i</span> <span>in</span> <span>std</span>.<span>range</span>(<span>0</span>, <span>2</span>):
    {
      <span>name</span> = <span>f"bucket-</span><span>{</span><span>i</span><span>}</span><span>"</span>,
      <span>location</span> = <span>"eu-west1"</span>,
      <span>delete-after-seconds</span> = <span>24</span> <span>*</span> <span>3600</span>,
    },
  ],
}</code></pre><p>For validation, the <a href="https://docs.ruuda.nl/rcl/type_list/#key_by"><code>key_by</code></a> method is useful. In the above example, if we‚Äôd name the buckets by hand and there are many of them, how do we ensure that we don‚Äôt accidentally create two buckets with the same name? We can do that by building a mapping from name to bucket:</p><pre><code><span>let</span> <span>buckets</span> = [
  <span>// Omitted here for brevity, defined as before.</span>
];

<span>// Build a mapping of bucket name to bucket. If a key (bucket name)</span>
<span>// occurs multiple times, this will fail with an error that reports</span>
<span>// the offending key and the associated values. The type annotation</span>
<span>// is for clarification, it is not mandatory.</span>
<span>let</span> <span>buckets_by_name</span>: <span>Dict</span>[<span>String</span>, <span>Dynamic</span>] = <span>buckets</span>.<span>key_by</span>(<span>b</span> <span>=&gt;</span> <span>b</span>.<span>name</span>);

<span>// Constructing the mapping is enough for validation, the document still</span>
<span>// evaluates to the same dict as before. Note, the left "buckets" is the</span>
<span>// name of the field, the right "buckets" is a variable reference.</span>
{ <span>buckets</span> = <span>buckets</span> }
</code></pre><p>This is just a quick overview of some features. For a more thorough introduction, check out <a href="https://docs.ruuda.nl/rcl/tutorial/">the tutorial</a> and <a href="https://docs.ruuda.nl/rcl/syntax/">the syntax guide</a>.</p><p>An <abbr>RCL</abbr> document is always an expression, and you can evaluate it to a json document with the <code>rcl</code> command-line tool:</p><pre><code>rcl evaluate --output=json buckets.rcl</code></pre><p>The tool can also output in <abbr>RCL</abbr> syntax, which is a bit less noisy when inspecting data, and it‚Äôs a way to upgrade json documents to <abbr>RCL</abbr>. Aside from the standalone command-line tool, I also recently added a Python module that enables importing <abbr>RCL</abbr> documents in much the same way as <code>json.loads</code>.</p><p>Abstraction in a single document is nice, but the real power comes from <em>imports</em>. These allow you to break down configuration into small reusable pieces. Let‚Äôs say that all your cloud resources are in the same location. Then we might have a file <code>cloud_config.rcl</code>:</p><pre><code>{
  default_location = <span>"eu-west1"</span>,
}</code></pre><p>Then in <code>buckets.rcl</code>, we can use that like so:</p><pre><code><span>let</span> cloud_config = <span>import</span> <span>"cloud_config.rcl"</span>;
{
  <span>buckets</span> = [
    <span>for</span> <span>i</span> <span>in</span> <span>std</span>.<span>range</span>(<span>0</span>, <span>2</span>):
    {
      <span>name</span> = <span>f"bucket-</span><span>{</span><span>i</span><span>}</span><span>"</span>,
      <span>location</span> = cloud_config.default_location,
      <span>delete-after-seconds</span> = <span>24</span> <span>*</span> <span>3600</span>,
    },
  ],
}</code></pre><p>Because every document is an expression, you can always evaluate it and inspect it, even if it‚Äôs only an intermediate stage in a larger configuration. For more fine-grained inspection there is <a href="https://docs.ruuda.nl/rcl/syntax/#debug-tracing"><code>trace</code></a>, and with <a href="https://docs.ruuda.nl/rcl/rcl_query/"><code>rcl query</code></a> you can evaluate an expression against a document to drill down into it. For example, to look only at the first bucket:</p><pre><code>rcl query buckets.rcl 'input.buckets[0]'</code></pre><p>This feature is what made <abbr>RCL</abbr> useful for a use case that I did not anticipate: querying json documents.</p><h2 id="an-unexpected-jq-replacement"><a href="#an-unexpected-jq-replacement"></a>An unexpected jq replacement</h2><p>I use <a href="https://jqlang.github.io/jq/">jq</a> a lot. Most of the time, only to pretty-print a json document returned from some <abbr>API</abbr>. Because <abbr>RCL</abbr> is a superset of json, <code>rcl</code> can do that too now:</p><pre><code>curl --silent https://api.example.com | rcl evaluate</code></pre><p>By itself that is nothing special, the true power comes when querying. Jq features its own stream processing <abbr>DSL</abbr>, and for simple expressions I can usually remember the syntax ‚Äî unpack the list, extract a few fields. But when it gets more complex, I‚Äôm at a loss. A while while ago, I was dealing with a json document that had roughly this structure:</p><div id="cb5"><pre><code><span id="cb5-1"><span>[</span></span>
<span id="cb5-2">  <span>{</span> <span>"name"</span><span>:</span> <span>"server-1"</span><span>,</span> <span>"tags"</span><span>:</span> <span>[</span><span>"amd"</span><span>,</span> <span>"fast"</span><span>]</span> <span>}</span><span>,</span></span>
<span id="cb5-3">  <span>{</span> <span>"name"</span><span>:</span> <span>"server-2"</span><span>,</span> <span>"tags"</span><span>:</span> <span>[</span><span>"intel"</span><span>,</span> <span>"slow"</span><span>]</span> <span>}</span><span>,</span></span>
<span id="cb5-4">  <span>{</span> <span>"name"</span><span>:</span> <span>"server-3"</span> <span>}</span><span>,</span></span>
<span id="cb5-5">  <span>{</span> <span>"name"</span><span>:</span> <span>"server-4"</span><span>,</span> <span>"tags"</span><span>:</span> <span>[</span><span>"amd"</span><span>,</span> <span>"vm"</span><span>,</span> <span>"slow"</span><span>]</span> <span>}</span></span>
<span id="cb5-6"><span>]</span></span></code></pre></div><p>I wanted to know the names of all the machines that had a particular tag applied. That the <code>tags</code> field is missing from some machines complicates that, and the real input consisted of hundreds of machines, so fixing that by hand was not feasible. I spent about 10 minutes struggling with <code>jq</code> and scrolling through unhelpful Stack Overflow answers. I did not think to try ChatGPT at the time, but in hindsight it <em>almost</em> gets the query right to a point where I could then get it working myself. But fundamentally, these kind of queries come up so infrequently that the things I learn about <code>jq</code> never really stick. ChatGPT is no excuse to tolerate bad tools: if the one-liner is easy to write, that‚Äôs still faster than leaving your terminal. At that point I remembered: I have a language in which this query is straightforward to express, and it can import json!</p><pre><code>$ rcl query --output=raw machines.json '[
  for m in input:
  if m.get("tags", []).contains("amd"):
  m.name
]'
server-1
server-4</code></pre><p>That‚Äôs how <abbr>RCL</abbr>, even though it is intended as a configuration language, became one of my most frequently used query languages.</p><h2 id="the-future-of-rcl"><a href="#the-future-of-rcl"></a>The future of <abbr>RCL</abbr></h2><p>That day when I was fed up with <abbr>HCL</abbr> and I ran <code>git init</code>, I didn‚Äôt expect to produce anything useful aside from entertaining myself for a few evenings. Now six months later, <abbr>RCL</abbr> is no longer vaporware, and it regularly solves real problems for me!</p><p>Some parts of <abbr>RCL</abbr> are already quite polished. It has mostly good error reporting, <a href="https://docs.ruuda.nl/rcl/">there is reference documentation</a>, it has an autoformatter, and it is very well tested with a suite of golden tests and fuzzers. Although I‚Äôm not sure at what point it starts being worth the complication of an additional tool, <abbr>RCL</abbr> can define cloud storage buckets today with <a href="https://developer.hashicorp.com/terraform/language/syntax/json">Terraform‚Äôs json syntax</a>. But <abbr>RCL</abbr> is also far from ready for prime time: there is no syntax highlighting for any editor aside from Vim, the type system is a work in progress, it doesn‚Äôt support floats yet, the Python module doesn‚Äôt expose errors nicely, the autoformatter has quirks, and I‚Äôm still ambivalent about whether there should be a <code>:</code> after <code>else</code>.</p><p>But most of all, I‚Äôm not sure whether I <em>want</em> <abbr>RCL</abbr> to experience prime time. Of course it is very gratifying to see your project be adopted and solve real-world problems for other people. I‚Äôm proud of what I built so far and I <em>want</em> people to see it and try it ‚Äî that‚Äôs why I publish everything as free and open source software, and that‚Äôs why I‚Äôm writing this post. It always cheers me up when somebody who found one of my projects useful or interesting sends me an e-mail. But I also already experience a bit of maintainer fatigue from some of my successful Rust crates, and I don‚Äôt always spend the time on them that they deserve. When a project takes off, inevitably users start making requests, having opinions, and submitting well-intentioned but low-quality contributions. Keeping up with that takes time and mental energy. I like working on <abbr>RCL</abbr> right now, because I get to build it in exactly the way I want, and it solves exactly the problems that I have. Building a tool for the open source community would require making different trade-offs. For now, I‚Äôm treating it as a source-available project. It solves a need for me, and if others find it useful that‚Äôs great, but it is provided as-is. Maybe Haskell‚Äôs <em>avoid success at all cost</em> isn‚Äôt such a bad idea.</p><h2 id="appendix-a-non-exhaustive-list-of-configuration-languages"><a href="#appendix-a-non-exhaustive-list-of-configuration-languages"></a>Appendix: A non-exhaustive list of configuration languages</h2><p>Aside from Nix, Python, and <abbr>HCL</abbr>, which I‚Äôve already discussed extensively, I am aware of the following configuation languages. For the ones that I‚Äôve used or at least evaluated briefly, I added my personal impressions, but beware that these are very superficial.</p><p><a href="https://github.com/Azure/bicep"><strong>Bicep</strong></a> ‚Äî Microsoft‚Äôs <abbr>DSL</abbr> for configuring Azure resources declaratively. I haven‚Äôt looked into it in much detail because I don‚Äôt work with Azure, but it looks potentially interesting.</p><p><a href="https://cuelang.org/"><strong>Cue</strong></a> ‚Äî Out of all the configuration languages that I evaluated during a company hackathon, I found Cue to be the most promising one. Its type system is interesting: it helps to constrain and validate configuration (as you would expect from a type system), but it also plays a role in eliminating boilerplate. Like Nix, Cue is based on few simple constructs that compose well, and grounded in solid theory. It took me some time before it clicked, but when it did, Cue became really powerful. A few things I don‚Äôt like about it are the package/module system that has its roots in the Go ecosystem, and its string interpolation syntax which is hideous. The command-line tooling works but could be more polished, and I found it to become slow quickly, even for fairly small configurations. It has <a href="https://cuelang.org/docs/usecases/configuration/#comparisons">a page</a> comparing itself against a few other configuation languages.</p><p><a href="https://dhall-lang.org/"><strong>Dhall</strong></a> ‚Äî This is the first configuration language that I learned about many years ago. From what I can tell, it is one of the most mature and widely supported configuration languages. I use <a href="https://github.com/purescript/spago">Spago</a>, the PureScript package manager, in some of my projects, and it uses Dhall as its configuration format. Unfortunately it looks like it is being <a href="https://github.com/purescript/spago/tree/bbe37b6cd497aa544bd0761fa7a56a5f5d002a87#migrate-from-spagodhall-to-spagoyaml">deprecated</a> in favor of yaml. I tried to use Dhall once to solve an Advent of Code challenge, but got stuck immediately because it‚Äôs not possible to split strings in Dhall. Of course, this is an unfair test to evaluate a configuration language on, but it does give an impression of the expressivity of a language. I‚Äôve used Nix to <a href="https://github.com/ruuda/adventofcode/blob/c452562c72cdd203df4dd0fd631596e6c0e2aa13/2022/03/main.nix">solve</a> a few Advent of Code challenges in the past, and this year I <a href="https://github.com/ruuda/adventofcode/blob/c452562c72cdd203df4dd0fd631596e6c0e2aa13/2023/11/main.rcl">solved</a> a few in <abbr>RCL</abbr>, which went pretty well for small inputs, but the lack of unbounded loops and tail calls make it unsuitable as a general-purpose language. Although I used to work as a Haskell developer, the formatting and names of built-in functions in Dhall look awkward to me.</p><p><a href="https://json-e.js.org/"><strong><abbr>JSON</abbr>-e</strong></a> ‚Äî A json parametrization language. I discovered this one in Rimu‚Äôs list of related projects. I think I‚Äôve seen it mentioned a few times before, but I haven‚Äôt evaluated it at all.</p><p><a href="https://jsonnet.org/"><strong>Jsonnet</strong></a> ‚Äî I never properly evaluated Jsonnet, but probably I should. Superficially it looks like one of the more mature formats, and in many ways it looks similar to <abbr>RCL</abbr>. Its has <a href="https://jsonnet.org/articles/comparisons.html">a page</a> comparing itself against other configuration languages.</p><p><a href="https://kcl-lang.io/"><strong><abbr>KCL</abbr></strong></a> ‚Äî This is an odd one. From the website and repository it looks like a lot of resources went into this project, but somehow I‚Äôve never seen it come up or be used anywhere. I only learned about it when I started searching for configuration languages. From the way it describes itself, it sounds like the tool I want, but I am generally wary of tools that use lots of buzzwords, especially when it involves the words ‚Äúmodern‚Äù and ‚Äúcloud native‚Äù. I should evaluate it properly at some point. It has <a href="https://kcl-lang.io/docs/0.6.0/user_docs/getting-started/intro/#how-to-choose">a page</a> comparing itself against other configuation languages.</p><p><a href="https://nickel-lang.org/"><strong>Nickel</strong></a> ‚Äî <a href="https://www.tweag.io/blog/2020-10-22-nickel-open-sourcing/">An attempt to create a language similar to Nix</a>, but without being tied to the package manager and Nix store. It looked very promising to me, but after evaluating it during a company hackathon, I found it difficult or impossible to express sanity checks that I can easily express in Cue and <abbr>RCL</abbr>. Its has <a href="https://github.com/tweag/nickel/blob/6cf2902d3db768618e1d990c549671e308dd3ff4/RATIONALE.md#comparison-with-alternatives">a page</a> comparing itself against other configuation languages.</p><p><a href="https://pkl-lang.org/"><strong>Pkl</strong></a> ‚Äî A configuration language by Apple. The timing is eerie: I wrote this post on a Saturday with the intention of proofreading and publishing it the next day, and right that Sunday morning, <a href="https://pkl-lang.org/blog/introducing-pkl.html">the Pkl announcement post</a> was on the Hacker News frontpage. From the comments, it <a href="https://news.ycombinator.com/item?id=39248081">has been in use</a> at Apple internally for a few years already. I haven‚Äôt had the opportunity to evaluate it yet. Its has <a href="https://pkl-lang.org/main/current/introduction/comparison.html">a page</a> comparing itself against other configuation languages, but only superficially.</p><p><a href="https://www.pulumi.com/"><strong>Pulumi</strong></a> ‚Äî Not a configuration language, but an infrastructure automation tool like Terraform. It can be configured using existing general-purpose programming languages. I haven‚Äôt had the opportunity to try it, but I suppose I don‚Äôt get to complain about <abbr>HCL</abbr> without at least acknowledging Pulumi‚Äôs existence.</p><p><a href="https://rimu.dev/"><strong>Rimu</strong></a> ‚Äî I stumbled upon this one recently while browsing <a href="https://github.com/topics/configuration-language">the configuration-language tag</a> on GitHub. It might be an eerie case of <a href="https://en.wikipedia.org/wiki/Multiple_discovery">parallel discovery</a>: like <abbr>RCL</abbr>, it looks like a configuration language developed as a side project, written in Rust, started in August 2023, and not ready for serious use yet. Unlike <abbr>RCL</abbr>, its syntax is based on yaml.</p><p><a href="https://bazel.build/rules/language"><strong>Starlark</strong></a> ‚Äî A Python dialect used by the Bazel build tool. I used it intensively when I was working with Blaze/Bazel, and it works well for defining build targets. Starlark has multiple implementations, including <a href="https://github.com/facebookexperimental/starlark-rust">one in Rust</a> that can be used as a standalone command-line tool, but all the implementations clearly focus on being embedded. From my limited attempts to use them in an infrastructure-as-code repository, they are not suitable for incremental adoption there.</p><p><a href="https://www.typescriptlang.org/"><strong>TypeScript</strong></a> ‚Äî Not a configuration language, but it deserves a mention here, because <abbr>RCL</abbr> intends to be json with abstraction and types, and since TypeScript is a superset of JavaScript, which is a superset of json, it falls in the same category of tools that can type and abstract json. I haven‚Äôt used TypeScript enough to have a strong opinion on its type system. Possibly <abbr>RCL</abbr>‚Äôs type system will end up being similar.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Inuit Parents Teach Kids To Control Their Anger (2019) (121 pts)]]></title>
            <link>https://www.npr.org/sections/goatsandsoda/2019/03/13/685533353/a-playful-way-to-teach-kids-to-control-their-anger</link>
            <guid>39250304</guid>
            <pubDate>Sun, 04 Feb 2024 14:10:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.npr.org/sections/goatsandsoda/2019/03/13/685533353/a-playful-way-to-teach-kids-to-control-their-anger">https://www.npr.org/sections/goatsandsoda/2019/03/13/685533353/a-playful-way-to-teach-kids-to-control-their-anger</a>, See on <a href="https://news.ycombinator.com/item?id=39250304">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storytext">
      <div id="res702589837">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
</div>
<div>
    <div>
        <p>
                For more than 30 years, the Inuit welcomed anthropologist Jean Briggs into their lives so she could study how they raise their children. Briggs is pictured during a 1974 visit to Baffin Island.
                <b aria-label="Image credit">
                    
                    Jean Briggs Collection / American Philosophical Society
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Jean Briggs Collection / American Philosophical Society
        
    </span>
</p></div>
   </div>
   <p>Back in the 1960s, a Harvard graduate student made a landmark discovery about the nature of human anger.</p>   <p>At age 34, Jean Briggs traveled above the Arctic Circle and lived out on the tundra for 17 months. There were no roads, no heating systems, no grocery stores. Winter temperatures could easily dip below minus 40 degrees Fahrenheit.</p>   <p>Briggs persuaded an Inuit family to "adopt" her and "try to keep her alive," as the anthropologist <a href="https://link.springer.com/article/10.1007%2FBF02805482">wrote </a>in 1970.</p>   <div id="res701118871">
                  <p>This story is part of a series from NPR's Science desk called <strong><a href="https://www.npr.org/series/688838187/the-other-side-of-anger">The Other Side of Anger.</a></strong> There's no question we are in angry times. It's in our politics, our schools and homes. Anger can be a destructive emotion, but it can also be a positive force.</p>         <p>Join NPR in our exploration of anger and what we can learn from this powerful emotion. <strong><a href="https://www.npr.org/series/688838187/the-other-side-of-anger">Read and listen to stories in the series here. </a></strong></p>
      </div>
   
<!-- END ID="RES701118871" CLASS="BUCKETWRAP LISTTEXT" -->
   <p>At the time, many Inuit families lived similar to the way their ancestors had for thousands of years. They built igloos in the winter and tents in the summer. "And we ate only what the animals provided, such as fish, seal and caribou," says <a href="https://www.eagle-eye.com/Myna-Ishulutak">Myna Ishulutak</a>, a film producer and language teacher who lived a similar lifestyle as a young girl.</p>   <p>Briggs quickly realized something remarkable was going on in these families: The adults had an extraordinary ability to control their anger.</p>   
   <p>"They never acted in anger toward me, although they were angry with me an awful lot," Briggs told the Canadian Broadcasting Corp. in an interview.</p>   <div id="res702588179">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
</div>
<div>
    <div>
        <p>
                Myna Ishulutak (upper right, in blue jacket) lived a seminomadic life as a child. Above: photos of the girl and her family in the hunting camp of Qipisa during the summer of 1974.
                <b aria-label="Image credit">
                    
                    Jean Briggs Collection / American Philosophical Society
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Jean Briggs Collection / American Philosophical Society
        
    </span>
</p></div>
   </div>
   <p>Even just showing a smidgen of frustration or irritation was considered weak and childlike, Briggs observed.</p>   <p>For instance, one time someone knocked a boiling pot of tea across the igloo, damaging the ice floor. No one changed their expression. "Too bad," the offender said calmly and went to refill the teapot.</p>   <p>In another instance, a fishing line ‚Äî which had taken days to braid ‚Äî immediately broke on the first use. No one flinched in anger. "Sew it together," someone said quietly.</p>   <p>By contrast, Briggs seemed like a wild child, even though she was trying very hard to control her anger. "My ways were so much cruder, less considerate and more impulsive," she told the CBC. "[I was] often impulsive in an antisocial sort of way. I would sulk or I would snap or I would do something that they never did."</p>   <p>Briggs, who died in 2016, wrote up her observations in <a href="http://www.hup.harvard.edu/catalog.php?isbn=9780674608283">her first book</a>, <em>Never in Anger</em>. But she was left with a lingering question: How do Inuit parents instill this ability in their children? How do Inuit take tantrum-prone toddlers and turn them into cool-headed adults?</p>   
   <p>Then in 1971, Briggs found a clue.</p>   
   
<!-- END ID="RES702773512" CLASS="BUCKETWRAP INTERNALLINK MEDIAPROMO PRIMARY" -->
   
   
<!-- END ID="RES702766744" CLASS="BUCKETWRAP INTERNALLINK MEDIAPROMO PRIMARY" -->
   <p>She was walking on a stony beach in the Arctic when she saw a young mother playing with her toddler ‚Äî a little boy about 2 years old. The mom picked up a pebble and said, "'Hit me! Go on. Hit me harder,'" Briggs remembered.</p>   <p>The boy threw the rock at his mother, and she exclaimed, "Ooooww. That hurts!"</p>   <p>Briggs was completely befuddled. The mom seemed to be teaching the child the opposite of what parents want. And her actions seemed to contradict everything Briggs knew about Inuit culture.</p>   <p>"I thought, 'What is going on here?' " Briggs said in the radio interview.</p>   <p>Turns out, the mom was executing a powerful parenting tool to teach her child how to control his anger ‚Äî and one of the most intriguing parenting strategies I've come across.</p>   <div id="res702586950">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                Iqaluit, pictured in winter, is the capital of the Canadian territory of Nunavut.
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1200.webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1200.jpg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1200.jpg" alt="" src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1200.jpg">
        </picture>
    </div>
<div>
        <p>Iqaluit, pictured in winter, is the capital of the Canadian territory of Nunavut.</p>
        <p><span aria-label="Image credit">
            
            Johan Hallberg-Campbell for NPR
            
        </span>
    </p></div>
   </div>
   <h3><strong>No scolding, no timeouts</strong></h3>   <p>It's early December in the Arctic town of Iqaluit, Canada. And at 2 p.m., the sun is already calling it a day. Outside, the temperature is a balmy minus 10 degrees Fahrenheit. A light snow is swirling.</p>   <p>I've come to this seaside town, after reading Briggs' book, in search of parenting wisdom, especially when it comes to teaching children to control their emotions. Right off the plane, I start collecting data.</p>   <p>I sit with elders in their 80s and 90s while they lunch on "country food" ‚Äîstewed seal, frozen beluga whale and raw caribou. I talk with moms selling hand-sewn sealskin jackets at a high school craft fair. And I attend a parenting class, where day care instructors learn how their ancestors raised small children hundreds ‚Äî perhaps even thousands ‚Äî of years ago.</p>   <div id="res702586420">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                The elders of Iqaluit have lunch at the local senior center. On Thursdays, what they call "country food" is on the menu, things like caribou, seal and ptarmigan.
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1200.webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1200.jpg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1200.jpg" alt="" src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1200.jpg">
        </picture>
    </div>
<div>
        <p>The elders of Iqaluit have lunch at the local senior center. On Thursdays, what they call "country food" is on the menu, things like caribou, seal and ptarmigan.</p>
        <p><span aria-label="Image credit">
            
            Johan Hallberg-Campbell for NPR
            
        </span>
    </p></div>
   </div>
   <p>Across the board, all the moms mention one golden rule: Don't shout or yell at small children.</p>   <p>Traditional Inuit parenting is incredibly nurturing and tender. If you took all the parenting styles around the world and ranked them by their gentleness, the Inuit approach would likely rank near the top.<strong> </strong>(They even have a special kiss for babies, where you put your nose against the cheek and sniff the skin.)</p>   <p>The culture views scolding ‚Äî or even speaking to children in an angry voice ‚Äî as inappropriate, says Lisa Ipeelie, a radio producer and mom who grew up with 12 siblings. "When they're little, it doesn't help to raise your voice," she says. "It will just make your own heart rate go up."</p>   
   <p>Even if the child hits you or bites you, there's no raising your voice?</p>   <p>"No," Ipeelie says with a giggle that seems to emphasize how silly my question is. "With little kids, you often think they're pushing your buttons, but that's not what's going on. They're upset about something, and you have to figure out what it is."</p>   <div id="res702585934">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
</div>
<div>
    <div>
        <p>
                Traditionally, the women and children in the community eat with an ulu knife.
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
   </div>
   <p>Traditionally, the Inuit saw yelling at a small child as demeaning. It's as if the adult is having a tantrum; it's basically stooping to the level of the child, Briggs documented.</p>   <p>Elders I spoke with say intense colonization over the past century is damaging these traditions. And, so, the community is working hard to keep the parenting approach intact.</p>   <p>Goota Jaw is at the front line of this effort. She teaches the parenting class at the <a href="https://www.arcticcollege.ca/programs">Arctic College</a>. Her own parenting style is so gentle that she doesn't even believe in giving a child a timeout for misbehaving.</p>   <p>"Shouting, 'Think about what you just did. Go to your room!' " Jaw says. "I disagree with that. That's not how we teach our children. Instead you are just teaching children to run away."</p>   <p>And you are teaching them to be angry, says clinical psychologist and author Laura Markham. "When we yell at a child ‚Äî or even threaten with something like 'I'm starting to get angry,' we're training the child to yell," says <a href="https://www.psychologytoday.com/us/experts/laura-markham-phd">Markham</a>. "We're training them to yell when they get upset and that yelling solves problems."</p>   <p>In contrast, parents who control their own anger are helping their children learn to do the same, Markham says. "Kids learn emotional regulation from us."</p>   <p>I asked Markham if the Inuit's no-yelling policy might be their first secret of raising cool-headed kids. "Absolutely," she says.</p>   <h3><strong>Playing soccer with your head</strong></h3>   <p>Now at some level, all moms and dads know they shouldn't yell at kids. But if you don't scold or talk in an angry tone, how do you discipline? How do you keep your 3-year-old from running into the road? Or punching her big brother?</p>   
   <p>For thousands of years, the Inuit have relied on an ancient tool with an ingenious twist: "We use storytelling to discipline," Jaw says.</p>   
   
<!-- END ID="RES703045640" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>Jaw isn't talking about fairy tales, where a child needs to decipher the moral. These are oral stories passed down from one generation of Inuit to the next, designed to sculpt kids' behaviors in the moment.<strong> </strong>Sometimes even save their lives.</p>   <p>For example, how do you teach kids to stay away from the ocean, where they could easily drown? Instead of yelling, "Don't go near the water!" Jaw says Inuit parents take a pre-emptive approach and tell kids a special story about what's inside the water. "It's the sea monster," Jaw says, with a giant pouch on its back just for little kids.</p>   <p>"If a child walks too close to the water, the monster will put you in his pouch, drag you down to the ocean and adopt you out to another family," Jaw says.</p>   <p>"Then we don't need to yell at a child," Jaw says, "because she is already getting the message."</p>   <p>Inuit parents have an array of stories to help children learn respectful behavior, too. For example, to get kids to listen to their parents, there is a story about ear wax, says film producer Myna Ishulutak.</p>   <p>"My parents would check inside our ears, and if there was too much wax in there, it meant we were not listening," she says.</p>   <p>And parents tell their kids: If you don't ask before taking food, long fingers could reach out and grab you, Ishulutak says.</p>   <div id="res702594556">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                Inuit parents tell their children to beware of the northern lights. If you don't wear your hat in the winter, they'll say, the lights will come, take your head and use it as a soccer ball!
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1200.webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1200.jpg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1200.jpg" alt="" src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1200.jpg">
        </picture>
    </div>
<div>
        <p>Inuit parents tell their children to beware of the northern lights. If you don't wear your hat in the winter, they'll say, the lights will come, take your head and use it as a soccer ball!</p>
        <p><span aria-label="Image credit">
            
            Johan Hallberg-Campbell for NPR
            
        </span>
    </p></div>
   </div>
   <p>Then there's the story of northern lights, which helps kids learn to keep their hats on in the winter.</p>   <p>"Our parents told us that if we went out without a hat, the northern lights are going to take your head off and use it as a soccer ball," Ishulutak says. "We used to be so scared!" she exclaims and then erupts in laughter.</p>   <p>At first, these stories seemed to me a bit too scary for little children. And my knee-jerk reaction was to dismiss them. But my opinion flipped 180 degrees after I watched my own daughter's response to similar tales ‚Äî and after I learned more about humanity's intricate relationship with storytelling.</p>   
   <p>Oral storytelling is what's known as a human universal. For tens of thousands of years, it has been a key way that parents teach children about values and how to behave.</p>   <p>Modern hunter-gatherer groups use stories to teach sharing, respect for both genders and conflict avoidance, a recent study <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5717173">reported</a>, after analyzing 89 stories from nine different tribes in Southeast Asia and Africa. With the Agta, a hunter-gatherer population of the Philippines, good storytelling skills are prized more than hunting skills or medicinal knowledge, the study found.</p>   <p>Today many American parents outsource their oral storytelling to screens. And in doing so, I wonder if we're missing out on an easy ‚Äî and effective ‚Äî way of disciplining and changing behavior. Could small children be somehow "wired" to learn through stories?</p>   <div id="res702585548">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
</div>
<div>
    <div>
        <p>
                Inuit parenting is gentle and tender. They even have a special kiss for kids called <em>kunik</em>. (Above) Maata Jaw gives her daughter the nose-to-cheek Inuit sniff.
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
   </div>
   <p>"Well, I'd say kids learn well through narrative and explanations," says psychologist <a href="http://starlabkids.org/deena_weisberg/">Deena Weisberg</a> at Villanova University, who studies how small children interpret fiction. "We learn best through things that are interesting to us. And stories, by their nature, can have lots of things in them that are much more interesting in a way that bare<strong> </strong>statements don't."</p>   <p>Stories with a dash of danger pull in kids like magnets, Weisberg says. And they turn a tension-ridden activity like disciplining into a playful interaction that's ‚Äî dare, I say it ‚Äî fun.</p>   <p>"Don't discount the playfulness of storytelling," Weisberg says. "With stories, kids get to see stuff happen that doesn't really happen in real life. Kids think that's fun. Adults think it's fun, too."</p>   <h3><strong>Why don't you hit me?</strong></h3>   <div id="res702583935">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
</div>
<div>
    <div>
        <p>
                Inuit filmmaker and language teacher Myna Ishulutak as a little girl. Anthropologist Jean Briggs spent six months with the family in the 1970s documenting the child's upbringing.
                <b aria-label="Image credit">
                    
                    Jean Briggs Collection / American Philosophical Society
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Jean Briggs Collection / American Philosophical Society
        
    </span>
</p></div>
   </div>
   <p>Back up in Iqaluit, Myna Ishulutak is reminiscing about her childhood out on the land. She and her family lived in a hunting camp with about 60 other people. When she was a teenager, her family settled in a town.</p>   <p>"I miss living on the land so much," she says as we eat a dinner of baked Arctic char. "We lived in a sod house. And when we woke up in the morning, everything would be frozen until we lit the oil lamp."</p>   
   <p>I ask her if she's familiar with the work of Jean Briggs. Her answer leaves me speechless.</p>   <p>Ishulutak reaches into her purse and brings out Briggs' second book, <em>Inuit Morality Play, </em>which details the life of a 3-year-old girl dubbed Chubby Maata.</p>   <p>"This book is about me and my family," Ishulutak says. "I am Chubby Maata."</p>   <p>In the early 1970s, when Ishulutak was about 3 years old, her family welcomed Briggs into their home for six months and allowed her to study the intimate details of their child's day-to-day life.</p>   <div id="res702582321">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
</div>
<div>
    <div>
        <p>
                Myna Ishulutak today in Iqaluit, Canada. As the mother of two grown boys, she says, "When you're shouting at them all the time they tend to kind of block you. So there's a saying: 'Never shout at them.' "
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
   </div>
   <p>What Briggs documented is a central component to raising cool-headed kids.</p>   <p>When a child in the camp acted in anger ‚Äî hit someone or had a tantrum ‚Äî there was no punishment. Instead, the parents waited for the child to calm down and then, in a peaceful moment, did something that Shakespeare would understand all too well: They put on a drama. (As the Bard once wrote, "the play's the thing wherein I'll catch the conscience of the king.")</p>   <p>"The idea is to give the child experiences that will lead the child to develop rational thinking," Briggs told the CBC in 2011.</p>   <p>In a nutshell, the parent would act out what happened when the child misbehaved, including the real-life consequences of that behavior.</p>   <p>The parent always had a playful, fun tone. And typically the performance starts with a question, tempting the child to misbehave.</p>   <p>For example, if the child is hitting others, the mom may start a drama by asking: "Why don't you hit me?"</p>   <p>Then the child has to think: "What should I do?" If the child takes the bait and hits the mom, she doesn't scold or yell but instead acts out the consequences. "Ow, that hurts!" she might exclaim.</p>   <p>The mom continues to emphasize the consequences by asking a follow-up question. For example: "Don't you like me?" or "Are you a baby?" She is getting across the idea that hitting hurts people's feelings, and "big girls" wouldn't hit. But, again, all questions are asked with a hint of playfulness.</p>   
   <p>The parent repeats the drama from time to time until the child stops hitting the mom during the dramas and the misbehavior ends.</p>   <p>Ishulutak says these dramas teach children not to be provoked easily. "They teach you to be strong emotionally," she says, "to not take everything so seriously or to be scared of teasing."</p>   <p>Psychologist <a href="http://www.psychology.illinois.edu/people/pjm">Peggy Miller</a>, at the University of Illinois, agrees: "When you're little, you learn that people will provoke you, and these dramas teach you to think and maintain some equilibrium."</p>   <p>In other words, the dramas offer kids a chance to <em>practice </em>controlling their anger, Miller says, during times when they're not actually angry.</p>   <p>This practice is likely critical for children learning to control their anger. Because here's the thing about anger: Once someone is already angry, it is not easy for that person to squelch it ‚Äî even for adults.</p>   <p>"When you try to control or change your emotions in the moment, that's a really hard thing to do," says <a href="https://lisafeldmanbarrett.com/">Lisa Feldman Barrett</a>, a psychologist at Northeastern University who studies how emotions work.</p>   <p>But if you <em>practice </em>having a different response or a different emotion at times when you're not angry, you'll have a better chance of managing your anger in those hot-button moments, Feldman Barrett says.</p>   <p>"That practice is essentially helping to rewire your brain to be able to make a different emotion [besides anger] much more easily," she says.</p>   <p>This emotional practice may be even more important for children, says psychologist Markham, because kids' brains are still developing the circuitry needed for self-control.</p>   <p>"Children have all kinds of big emotions," she says. "They don't have much prefrontal cortex yet. So what we do in responding to our child's emotions shapes their brain."</p>   <div id="res702582120">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                A lot has changed in the Arctic since the Canadian government forced Inuit families to settle in towns. But the community is trying to preserve traditional parenting practices.
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1200.webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1200.jpg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1200.jpg" alt="" src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1200.jpg">
        </picture>
    </div>
<div>
        <p>A lot has changed in the Arctic since the Canadian government forced Inuit families to settle in towns. But the community is trying to preserve traditional parenting practices.</p>
        <p><span aria-label="Image credit">
            
            Johan Hallberg-Campbell for NPR
            
        </span>
    </p></div>
   </div>
   <p>Markham recommends an approach close to that used by Inuit parents. When the kid misbehaves, she suggests, wait until everyone is calm. Then in a peaceful moment, go over what happened with the child. You can simply tell them the story about what occurred or use two stuffed animals to act it out.</p>   <p>"Those approaches develop self-control," Markham says.</p>   <p>Just be sure you do two things when you replay the misbehavior, she says. First, keep the child involved by asking many questions. For example, if the child has a hitting problem, you might stop midway through the puppet show and ask,"Bobby, wants to hit right now. Should he?"</p>   
   <p>Second, be sure to keep it fun. Many parents overlook play as a tool for discipline, Markham says. But fantasy play offers oodles of opportunities to teach children proper behavior.</p>   <p>"Play is their work," Markham says. "That's how they learn about the world and about their experiences."</p>   <p>Which seems to be something the Inuit have known for hundreds, perhaps even, thousands of years.</p>   <div id="res702581804">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                Inuit parents value the playful side of kids even when disciplining them. Above: Maata Jaw and daughter.
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1200.webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1200.jpg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1200.jpg" alt="" src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1200.jpg">
        </picture>
    </div>
<div>
        <p>Inuit parents value the playful side of kids even when disciplining them. Above: Maata Jaw and daughter.</p>
        <p><span aria-label="Image credit">
            
            Johan Hallberg-Campbell for NPR
            
        </span>
    </p></div>
   </div>
   <h3><strong>Share Your Tips</strong></h3>   <p><em>How do you get your kids to do things without yelling or shouting? Or, how did your parents get you to do things without yelling or scolding? Share your advice, tips and stories, and we may include them in a story for NPR.</em></p>   <p><strong>This submission form is now closed. </strong></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New Linux glibc flaw lets attackers get root on major distros (146 pts)]]></title>
            <link>https://www.bleepingcomputer.com/news/security/new-linux-glibc-flaw-lets-attackers-get-root-on-major-distros/</link>
            <guid>39250076</guid>
            <pubDate>Sun, 04 Feb 2024 13:35:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bleepingcomputer.com/news/security/new-linux-glibc-flaw-lets-attackers-get-root-on-major-distros/">https://www.bleepingcomputer.com/news/security/new-linux-glibc-flaw-lets-attackers-get-root-on-major-distros/</a>, See on <a href="https://news.ycombinator.com/item?id=39250076">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img alt="Linux" height="900" src="https://www.bleepstatic.com/content/hl-images/2023/06/22/Linux.jpg" width="1600"></p>
<p>‚ÄãUnprivileged attackers can get root access on multiple major Linux distributions in default configurations by exploiting a newly disclosed local privilege escalation (LPE) vulnerability in the GNU C Library (glibc).</p>
<p>Tracked as <a href="https://www.qualys.com/2024/01/30/cve-2023-6246/syslog.txt" target="_blank" rel="nofollow noopener">CVE-2023-6246</a>, this security flaw was found in glibc's __vsyslog_internal() function, called by the widely-used syslog and vsyslog functions for writing messages to the system message logger.</p>
<p>The bug is due to a <a href="https://cwe.mitre.org/data/definitions/122.html" target="_blank" rel="nofollow noopener">heap-based buffer overflow weakness</a> accidentally introduced in glibc 2.37 in August 2022 and later backported to glibc 2.36 when addressing a less severe vulnerability tracked as CVE-2022-39046.</p>
<p>"The buffer overflow issue poses a significant threat as it could allow local privilege escalation, enabling an unprivileged user to gain full root access through crafted inputs to applications that employ these logging functions," Qualys security researchers said.</p>
<p>"Although the vulnerability requires specific conditions to be exploited (such as an unusually long argv[0] or openlog() ident argument), its impact is significant due to the widespread use of the affected library."</p>
<h2>Impacts Debian, Ubuntu, and Fedora systems</h2>
<p>While testing their findings, Qualys confirmed that Debian 12 and 13, Ubuntu 23.04 and 23.10, and Fedora 37 to 39 were all vulnerable to CVE-2023-6246 exploits, allowing any unprivileged user to escalate privileges to full root access on default installations.</p>
<p>Although their tests were limited to a handful of distros, the researchers added that "other distributions are probably also exploitable."</p>
<p>While analyzing glibc for other potential security issues, the researchers also found three other vulnerabilities, two of them‚Äîharder to exploit‚Äîin the __vsyslog_internal() function (CVE-2023-6779 and CVE-2023-6780) and a third one (a <a href="https://www.qualys.com/2024/01/30/qsort.txt" target="_blank" rel="nofollow noopener">memory corruption issue</a> still waiting for a CVEID) in glibc's qsort () function.</p>
<p>"The recent discovery of these vulnerabilities is not just a technical concern but a matter of widespread security implications,"&nbsp;<a href="https://blog.qualys.com/vulnerabilities-threat-research/2024/01/30/qualys-tru-discovers-important-vulnerabilities-in-gnu-c-librarys-syslog" target="_blank" rel="nofollow noopener">said</a> Saeed Abbasi, Product Manager at Qualys' Threat Research Unit.</p>
<p>"These flaws highlight the critical need for strict security measures in software development, especially for core libraries widely used across many systems and applications."</p>
<h2>Other Linux root escalation flaws found by Qualys</h2>
<p>Over the past few years, researchers at Qualys have found several other Linux security vulnerabilities that can let attackers gain complete control over unpatched Linux systems, even in default configurations.</p>
<p>Vulnerabilities they discovered include a flaw in glibc's ld.so dynamic loader (<a href="https://www.bleepingcomputer.com/news/security/new-looney-tunables-linux-bug-gives-root-on-major-distros/" target="_blank">Looney Tunables</a>), one in Polkit's pkexec component (<a href="https://www.bleepingcomputer.com/news/security/linux-system-service-bug-gives-root-on-all-major-distros-exploit-released/" target="_blank">dubbed PwnKit</a>), another in the Kernel's filesystem layer (<a href="https://www.bleepingcomputer.com/news/security/new-linux-kernel-bug-lets-you-get-root-on-most-modern-distros/" target="_blank">dubbed Sequoia</a>), and in the Sudo Unix program (aka <a href="https://www.bleepingcomputer.com/news/security/new-linux-sudo-flaw-lets-local-users-gain-root-privileges/" target="_blank">Baron Samedit</a>).</p>
<p>Days after the Looney Tunables flaw (<a href="https://access.redhat.com/security/cve/cve-2023-4911" target="_blank" rel="nofollow noopener">CVE-2023-4911</a>) was disclosed, proof-of-concept (PoC) exploits were <a href="https://www.bleepingcomputer.com/news/security/exploits-released-for-linux-flaw-giving-root-on-major-distros/" target="_blank">published online</a>, and threat actors <a href="https://www.bleepingcomputer.com/news/security/hackers-exploit-looney-tunables-linux-bug-steal-cloud-creds/" target="_blank">started exploiting it</a> one month later to steal cloud service provider (CSP) credentials in Kinsing malware attacks.</p>
<p>The Kinsing gang is known for deploying cryptocurrency mining malware on compromised cloud-based systems, including Kubernetes, Docker APIs, Redis, and Jenkins servers.</p>
<p>CISA later <a href="https://www.bleepingcomputer.com/news/security/cisa-orders-federal-agencies-to-patch-looney-tunables-linux-bug/" target="_blank">ordered U.S. federal agencies</a> to secure their Linux systems against CVE-2023-4911 attacks after adding it to its catalog of actively exploited bugs and tagging it as posing "significant risks to the federal enterprise."</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rye: A Vision Continued (155 pts)]]></title>
            <link>https://lucumr.pocoo.org/2024/2/4/rye-a-vision/</link>
            <guid>39249005</guid>
            <pubDate>Sun, 04 Feb 2024 10:15:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lucumr.pocoo.org/2024/2/4/rye-a-vision/">https://lucumr.pocoo.org/2024/2/4/rye-a-vision/</a>, See on <a href="https://news.ycombinator.com/item?id=39249005">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
  

  
  <p>written on Sunday, February 4, 2024
  

  </p><p>In April of last year I released <a href="https://rye-up.com/">Rye</a> to the public.
Rye, both then and now, represents my very personal vision of what an improved
Python packaging and project management solution can look like.
Essentially, it's a comprehensive user experience, designed so that the
only tool a Python programmer would need to interface with is Rye itself
and it gets you from zero to one in a minute.  It is capable of
bootstrapping Python by automatically downloading different Python
versions, it creates virtualenvs, it manages dependencies, and lints and
formats.  Initially developed for my own use, I decided to release it to
the public, and the feedback has been overwhelmingly positive.</p>
<p>When I introduced it, I initiated a discussion thread titled <a href="https://github.com/mitsuhiko/rye/discussions/6">‚ÄúShould Rye
Exist‚Äù</a> referencing the
well known <a href="https://xkcd.com/927/">XKCD #929</a> which humorously comments
on the proliferation of competing standards.  I did not feel well throwing
yet another Python packaging tool into the ring.</p>
<p>Yet it exists now and has user.  This standard issue however I think is
helped a bit by the fact that Rye doesn't actually do any of these things
itself.  It wraps established tools:</p>
<ul>
<li><strong>Downloading Python</strong>: it provides an automated way to get access to
the amazing <a href="https://github.com/indygreg/python-build-standalone/">Indygreg Python Builds</a>
as well as the PyPy binary distributions.</li>
<li><strong>Linting and Formatting</strong>: it bundles <a href="https://github.com/astral-sh/ruff">ruff</a>
and makes it available with <cite>rye lint</cite> and <cite>rye fmt</cite>.</li>
<li><strong>Managing Virtualenvs</strong>: it uses the well established <a href="https://virtualenv.pypa.io/en/latest/">virtualenv</a> library under the hood.</li>
<li><strong>Building Wheels</strong>: it delegates that work largely to <a href="https://pypi.org/project/build/">build</a>.</li>
<li><strong>Publishing</strong>: its publish command uses <a href="https://pypi.org/project/twine/">twine</a> to accomplish this task.</li>
<li><strong>Locking and Dependency Installation:</strong> is today implemented by
using <a href="https://pypi.org/project/unearth/">unearth</a> and
<a href="https://github.com/jazzband/pip-tools/">pip-tools</a>.</li>
</ul>
<p>As you can see, Rye is not revolutionary and it's not intended to be.  Rye
itself doesn't do all that much as it delegates all the core functionality
to other tools in the ecosystem.  Rye packages these tools together in a
user-friendly manner, significantly reducing the cognitive load for
developers.  This convenience eliminates the need to learn about various
tools, read extensive documentation, and integrate these components
independently.  Rye lets you get from no Python on a computer to a fully
functioning Python project in under a minute with linting, formatting and
everything in place.  It is sufficiently opinionated that many important
decisions are made for you.  For instance it starts you out with using
<cite>pyproject.toml</cite> and picks a wheel build system for you.  It also picks
the linter and formatter, and the preferred Python distribution and
decides on a build tool.</p>
<div id="defaults-matter">
<h2>Defaults Matter</h2>
<p>Rye is designed to select the best tools for the job ‚Äî it picks winners.
Why does it do that?  This approach is inspired by my admiration for the
developer experience in the Rust ecosystem, particularly the seamless
integration of <cite>rustup</cite> and <cite>cargo</cite>.  Their functionality made me long for
a similar experience within the Python community.  Crucially the way this
works in the Rust world does not mean that <cite>cargo</cite> does everything.  When
you run <cite>cargo build</cite> it invokes <cite>rustc</cite>, when you run <cite>cargo doc</cite> it runs
<cite>rustdoc</cite>.  When you invoke <cite>cargo clippy</cite> it runs <cite>clippy</cite> for you and so
worth.  Cargo is a manager that delegates the important work to bespoke
tools that are improved by sometimes entirely different teams.  This also
means that tools can be swapped out if they are found to be not the right
choice any more.  The experience in the Rust world also showed me that
excellent Windows support is just a must have.  That's why Rye is not just
a great experience on macOS and Linux, it's also excellent on Windows.</p>
<p>I am convinced that the Python community is deserving of an excellent
developer experience, and Rye, as it stands today, offers a promising
beginning.  My belief is supported by evidence gathered from conducting
in-person user interviews and demos, where Rye was well received.  In
fact, every individual who I was able to give a guided tour of Rye was
impressed by how swiftly one could start working with Python.  Because it
was demonstrably designed to avoid interference with any pre-existing
Python configurations, Rye allows for a smooth and gradual integration and
the emotional barrier of picking it up even for people who use other tools
was shown to be low.</p>
<p>That said, Rye is a one person project and it does not address the
fundamental challenges of some of the issues we have in the Python
ecosystem.  It does not solve multi version dependencies, it does not
offer better performance for the installation of dependencies.  It does
not help with distributing executables for end user applications or
anything like this.  However I am getting multiple signals that the time
is right for a tool like Rye to not just exist, but also to rally a larger
number of the Python community embrace some of these standardization
ideas.</p>
</div>
<div id="what-s-next">
<h2>What's Next?</h2>
<p><a href="https://github.com/Kwpolska">Chris Warrick</a> recently <a href="https://chriswarrick.com/blog/2024/01/15/python-packaging-one-year-later/">wrote a blog post</a>
where he looked back at the last year of Python packaging that made the
rounds on Twitter.  It laments a bit that we did not make much of a
progress in packaging and it also talks a bit about Rye and correctly
points out that Rye does not have enough contributors (basically just me).
That's not a healthy setup.</p>
<p>I still don't really know if Rye <em>should</em> exist.  It has not yet become
established and there are plenty of rough edges.  I personally really
enjoy using it but at the same time every time I use it, I get reminded
that it would stop existing if I did not invest time into it which in some
sense is what keeps me going on it.</p>
<p>However I would love to see the community converge to a Rye like solution,
no matter where it comes from.</p>
</div>
<div id="learn-more">
<h2>Learn More</h2>
<p>Did I spark your interest?  I would really appreciate it if you give it a
try and give feedback:</p>
<p><em>a 16 minute introduction to Rye</em>
    <iframe width="782" height="441" src="https://www.youtube.com/embed/q99TYA7LnuA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p><ul>
<li><a href="https://rye-up.com/">Project Website</a></li>
<li><a href="https://rye-up.com/guide/">User Guide and Documentation</a></li>
<li><a href="https://github.com/mitsuhiko/rye">GitHub Project</a></li>
<li><a href="https://github.com/mitsuhiko/rye/discussions">Discussion Forums</a></li>
<li><a href="https://discord.gg/drbkcdtSbg">Discord</a></li>
</ul>
</div>


  
  <p>This entry was tagged
    
      <a href="https://lucumr.pocoo.org/tags/announcement/">announcement</a> and 
      <a href="https://lucumr.pocoo.org/tags/python/">python</a>
  

      </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You are what you love (113 pts)]]></title>
            <link>https://gspanos.tech/posts/facts-1/</link>
            <guid>39248931</guid>
            <pubDate>Sun, 04 Feb 2024 09:57:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gspanos.tech/posts/facts-1/">https://gspanos.tech/posts/facts-1/</a>, See on <a href="https://news.ycombinator.com/item?id=39248931">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>  <blockquote>
<p>Facts is a series of articles where I express my foldable opinions. I know, right?</p>
</blockquote>
<h2 id="working-with-emotion">Working with Emotion</h2>
<p>I find it really challenging to talk with people who have completely separated their work from their emotional being. For me, work is part of life and it should be a meaningful one.</p>
<p>I recently had a talk with a close colleague that had to do with working with emotion. Their thesis was that there is no place for emotional thinking and analysis when working. They insisted on saying that the analytical brain should be almost exclusively responsible for how we behave in a work environment.</p>
<p>Not only do I find this contradicting everything that my work ethic consists of, but I also find this mantra unsustainable.</p>
<p>Firstly, let‚Äôs make clear that it‚Äôs unlikely that you‚Äôre good at something you don‚Äôt love. If you managed to do that, you‚Äôve probably dedicated the hours to something that does not fulfill you. The goal is not happiness. It‚Äôs about fulfillment. After all, you have to do what you love. You <em>are</em> that, how can you be doing anything else?</p>
<p>This belief steers me towards believing that there has to be a clear emotional foundation on how we work. To be great, to achieve things, and to provide, we have to convert emotions into outcomes. Rather than the analytical brain being the fuel, it seems it‚Äôs more of a catalyst.</p>
<pre tabindex="0"><code><span><span>function</span><span> ValueOfOutcome</span><span>(</span><span>emotionalDrive</span><span>) {</span></span>
<span><span>	// real life situations modifier</span></span>
<span><span>	const</span><span> situationsModifier;</span></span>
<span><span>	// Value of Outcome is the result of the Analytical Process of an emotionalDrive times the situationModifier</span></span>
<span><span>	return</span><span> AnalyticalProcess</span><span>(emotionalDrive) </span><span>*</span><span> situationsModifier;</span></span>
<span><span>}</span></span></code></pre>
<p>Emotion cannot be separate from work. It has to be a part of it. When working, you‚Äôre expressing yourself. You express beliefs, opinions, and strategies, world views. You cannot detach yourself completely from work. I doubt that you ever should.</p>
<h2 id="peer-to-peer-communications">Peer-to-peer communications</h2>
<p>People around you are fully aware when you‚Äôre doing something for the sake of doing it. No, you‚Äôre not hiding it well enough. No, you don‚Äôt convince people that you‚Äôre having a great time, when you‚Äôre not. No one who pays careful attention to what they experience really believes that you‚Äôre doing great, while not loving what you do. Again, you might do ok. And ok can be fine. You have to decide if <em>‚Äúok‚Äù</em> is enough for you.</p>
<p>In any context, people do get it when you‚Äôre acting. Most of the time.</p>
<p>People who pay attention get it almost every single time.</p>
<h2 id="you-should-be-doing-great">You should be doing great</h2>
<p>Everyone wants to excel, right? Not on everything of course, but on what they love, sure. After all, the positive feedback loop of ‚Äúlove‚Äù is a mandatory part of its survival over time.</p>
<p>If you want to enjoy yourself when working, try doing something you love. It‚Äôs about extroversion. It‚Äôs about getting it out and sharing it. People do notice when you love what you do. The reimbursement is not always fair. Frankly, it usually isn‚Äôt. But at least you‚Äôre doing what you love and, for most people after a certain living standard, this is reimbursement enough.</p>
<p>If you don‚Äôt know what you love, try playing with a couple of things. Try gathering some new experiences. It‚Äôll be fun!</p>
<p>If you know what you love and are afraid to pursue to make a living out of this, stop doing that immediately. Everything else is just a compromise. And while compromise can be mandatory, it cannot constitute a permanent state of being. Compromise is a trap for more compromise. Go out and do what you love. This is who you are.</p>
<p>If you know what you love and already do that for a living, <em>thank you</em>. Your contributions are deeply appreciated.</p>
<p>George Spanos</p>
<p><a href="https://moby-it.com/" rel="nofollow, noopener, noreferrer" target="_blank">Moby IT</a></p>    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Finance worker pays out $25M after VC with deepfake CFO (355 pts)]]></title>
            <link>https://edition.cnn.com/2024/02/04/asia/deepfake-cfo-scam-hong-kong-intl-hnk/index.html</link>
            <guid>39248649</guid>
            <pubDate>Sun, 04 Feb 2024 08:43:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://edition.cnn.com/2024/02/04/asia/deepfake-cfo-scam-hong-kong-intl-hnk/index.html">https://edition.cnn.com/2024/02/04/asia/deepfake-cfo-scam-hong-kong-intl-hnk/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=39248649">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-editable="main" data-track-zone="main" data-reorderable="main">  <article data-uri="cms.cnn.com/_components/article/instances/cls6vbf7p0025a9nramq6c335@published" role="main" data-unselectable="true">
      
  <section data-tabcontent="Content">
    <main>
                <div data-image-variation="image" data-breakpoints="{&quot;image--eq-extra-small&quot;: 115, &quot;image--eq-small&quot;: 300}" data-uri="cms.cnn.com/_components/image/instances/cls6vg79l00063b6hf65f80kr@published" data-name="GettyImages-1437811938.jpg" data-component-name="image" data-observe-resizes="" data-original-ratio="0.6666666666666666" data-original-height="2000" data-original-width="3000" data-url="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-1437811938.jpg?c=original" data-editable="lede" data-freewheel-lede="true">
       <picture><source height="383" width="680" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-1437811938.jpg?c=16x9&amp;q=h_383,w_680,c_fill/f_webp" type="image/webp"><source height="653" width="1160" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-1437811938.jpg?c=16x9&amp;q=h_653,w_1160,c_fill/f_webp" type="image/webp"><source height="605" width="1075" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-1437811938.jpg?c=16x9&amp;q=h_605,w_1075,c_fill/f_webp" type="image/webp"><source height="833" width="1480" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-1437811938.jpg?c=16x9&amp;q=h_833,w_1480,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-1437811938.jpg?c=16x9&amp;q=h_833,w_1480,c_fill" alt="Authorities are increasingly concerned at the damaging potential posed by artificial intelligence technology." onload="this.classList.remove('image__dam-img--loading')" onerror="imageLoadError(this)" height="2000" width="3000"></picture>
    </div>
        
        
            <div data-editable="content" itemprop="articleBody" data-reorderable="content">
                    <p><cite>
      <span data-editable="location"></span>
      <span data-editable="source">CNN</span>
        &nbsp;‚Äî&nbsp;
    </cite>
</p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls6vbf7p0024a9nrhngzfc7l@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            A finance worker at a multinational firm was tricked into paying out $25 million to fraudsters using deepfake technology to pose as the company‚Äôs chief financial officer in a video conference call, according to Hong Kong police.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls732ubh00063d5vtonp5jy8@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The elaborate scam saw the worker duped into attending a video call with what he thought were several other members of staff, but all of whom were in fact deepfake recreations, Hong Kong police said at a briefing on Friday.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls72x77f00043d5v9kft5o5o@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            ‚Äú(In the) multi-person video conference, it turns out that everyone [he saw] was fake,‚Äù  senior superintendent Baron Chan Shun-ching told the city‚Äôs public broadcaster RTHK.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls73gcs300083d5vqrgwlj90@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Chan said the worker had grown suspicious after he received a message that was purportedly from the company‚Äôs UK-based chief financial officer. Initially, the worker suspected it was a phishing email, as it talked of the need for a secret transaction to be carried out.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls73gdgl000a3d5v2mmdlyua@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            However, the worker put aside his early doubts after the video call because other people in attendance had looked and sounded just like colleagues he recognized, Chan said.
    </p>

<div data-image-variation="image" data-breakpoints="{&quot;image--eq-extra-small&quot;: 115, &quot;image--eq-small&quot;: 300}" data-uri="cms.cnn.com/_components/image/instances/cls6zz14y000w3b6h93sv6453@published" data-name="" data-component-name="image" data-observe-resizes="" data-original-ratio="0.666015625" data-original-height="682" data-original-width="1024" data-url="https://media.cnn.com/api/v1/images/stellar/prod/c90d6199-9933-4c4e-a054-38b43dc829d9.jpg?q=h_682,w_1024,x_0,y_0" data-editable="settings">
       <picture><source height="682" width="1024" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c90d6199-9933-4c4e-a054-38b43dc829d9.jpg?q=w_1110,c_fill/f_webp" type="image/webp"><source height="682" width="1024" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c90d6199-9933-4c4e-a054-38b43dc829d9.jpg?q=w_1015,c_fill/f_webp" type="image/webp"><source height="682" width="1024" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c90d6199-9933-4c4e-a054-38b43dc829d9.jpg?q=w_1160,c_fill/f_webp" type="image/webp"><source height="682" width="1024" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c90d6199-9933-4c4e-a054-38b43dc829d9.jpg?q=w_680,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/c90d6199-9933-4c4e-a054-38b43dc829d9.jpg?q=w_1110,c_fill" alt="This aerial photo taken on December 19, 2018 shows a general view of the skyline of Hong Kong. " onload="this.classList.remove('image__dam-img--loading')" onerror="imageLoadError(this)" height="682" width="1024" loading="lazy"></picture>
    </div>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls73ms2y000h3d5vq6h91nw6@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Believing everyone else on the call was real, the worker agreed to remit a total of $200 million Hong Kong dollars ‚Äì about $25.6 million, the police officer added.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls73p5qm000k3d5v8dltfh94@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The case is one of several recent episodes in which fraudsters are believed to have used deepfake technology to modify publicly available video and other footage to cheat people out of money.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls74r4vs001h3d5vp6iv426g@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            At the press briefing Friday, Hong Kong police said they had made six arrests in connection with such scams.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls7406x6000o3d5v7tamfvbm@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Chan said that eight stolen Hong Kong identity cards ‚Äì all of which had been reported as lost by their owners ‚Äì were used to make 90 loan applications and 54 bank account registrations between July and September last year.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls73p7jl000m3d5v9dn91735@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            On at least 20 occasions, AI deepfakes had been used to trick facial recognition programs by imitating the people pictured on the identity cards, according to police.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls7466h2000w3d5vo93ao08o@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The scam involving the fake CFO was only discovered when the employee later checked with the corporation‚Äôs head office.
    </p>

  


    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls745fva000u3d5vtnlcd70u@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Hong Kong police did not reveal the name or details of the company or the worker.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls718rv600163b6hcoz7rplm@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Authorities across the world are growing increasingly concerned at the sophistication of deepfake technology and the nefarious uses it can be put to.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls6yumv1000f3b6hofxm8j8t@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            At the end of January, pornographic, AI-generated images of the American pop star <a href="https://www.cnn.com/2024/01/25/tech/taylor-swift-ai-generated-images/index.html">Taylor Swift</a> spread across social media, underscoring the damaging potential posed by artificial intelligence technology.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls74e05b00153d5vox49fjhu@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The photos - which show the singer in sexually suggestive and explicit positions - were viewed tens of millions of times before being removed from social platforms.
    </p>

                </div>
    </main>
  </section>
</article>

</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to Replace Your CPAP in Only 666 Days (331 pts)]]></title>
            <link>https://aphyr.com/posts/368-how-to-replace-your-cpap-in-only-666-days</link>
            <guid>39248631</guid>
            <pubDate>Sun, 04 Feb 2024 08:36:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aphyr.com/posts/368-how-to-replace-your-cpap-in-only-666-days">https://aphyr.com/posts/368-how-to-replace-your-cpap-in-only-666-days</a>, See on <a href="https://news.ycombinator.com/item?id=39248631">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p><em>This story is not practical advice. For me, it‚Äôs closing the book on an almost two-year saga. For you, I hope it‚Äôs an enjoyable bit of bureaucratic schadenfreude. For Anthem, I hope it‚Äôs the subject of a series of painful but transformative meetings. This is not an isolated event. I‚Äôve had dozens of struggles with Anthem customer support, and they all go like this.</em></p>
<p><em>If you‚Äôre looking for practical advice: it‚Äôs this. Be polite. Document everything. Keep a log. Follow the claims process. Check the laws regarding insurance claims in your state. If you pass the legally-mandated deadline for your claim, call customer service. Do not allow them to waste a year of your life, or force you to resubmit your claim from scratch. Initiate a complaint with your state regulators, and escalate directly to <a href="mailto:gail.boudreaux@elevancehealth.com">Gail Boudreaux‚Äôs team</a>‚Äìor whoever Anthem‚Äôs current CEO is.</em></p>
<p>To start, experience an equipment failure.</p>
<p>Use your CPAP daily for six years. Wake up on day zero with it making a terrible sound. Discover that the pump assembly is failing. Inquire with Anthem Ohio, your health insurer, about how to have it repaired. Allow them to refer you to a list of local durable medical equipment providers. Start calling down the list. Discover half the list are companies like hair salons. Eventually reach a company in your metro which services CPAPs. Discover they will not repair broken equipment unless a doctor tells them to.</p>
<p>Leave a message with your primary care physician. Call the original sleep center that provided your CPAP. Discover they can‚Äôt help, since you‚Äôre no longer in the same state. Return to your primary, who can‚Äôt help either, because he had nothing to do with your prescription. Put the sleep center and your primary in touch, and ask them to talk.</p>
<p>On day six, call your primary to check in. He‚Äôs received a copy of your sleep records, and has forwarded them to a local sleep center you haven‚Äôt heard of. They, in turn, will talk to Anthem for you.</p>
<p>On day 34, receive an approval letter labeled ‚Äúconfirmation of medical necessity‚Äù from Anthem, directed towards the durable medical equipment company. Call that company and confirm you‚Äôre waitlisted for a new CPAP. They are not repairable. Begin using your partner‚Äôs old CPAP, which is not the right class of device, but at least it helps.</p>
<p>Over the next 233 days, call that medical equipment company regularly. Every time, inquire whether there‚Äôs been any progress, and hear ‚Äúwe‚Äôre still out of stock‚Äù. Ask them you what the manufacturer backlog might be, how many people are ahead of you in line, how many CPAPs they <em>do</em> receive per month, or whether anyone has ever received an actual device from them. They won‚Äôt answer any questions. Realize they are never going to help you.</p>
<p>On day 267, realize there is no manufacturer delay. The exact machine you need is in stock on CPAP.com. Check to make sure there‚Äôs a claims process for getting reimbursed by Anthem. Pay over three thousand dollars for it. When it arrives, enjoy being able to breathe again.</p>
<p>On day 282, follow CPAP.com‚Äôs documentation to file a claim with Anthem online. Include your prescription, receipt, shipping information, and the confirmation of medical necessity Anthem sent you.</p>
<p>On day 309, open the mail to discover a mysterious letter from Anthem. They‚Äôve received your appeal. You do not recall appealing anything. There is no information about what might have been appealed, but something will happen within 30-60 days. There is nothing about your claim.</p>
<p>On day 418, emerge from a haze of lead, asbestos, leaks, and a host of other home-related nightmares; remember Anthem still hasn‚Äôt said anything about your claim. Discover your claim no longer appears on Anthem‚Äôs web site. Call Anthem customer service. They have no record of your claim either. Ask about the appeal letter you received. Listen, gobsmacked, as they explain that they decided your claim was in fact an appeal, and transferred it immediately to the appeals department. The appeals department examined the appeal and looked for the claim it was appealing. Finding none, they decided the appeal was moot, and rejected it. At no point did anyone inform you of this. Explain to Anthem‚Äôs agent that you filed a claim online, not an appeal. At their instruction, resign yourself to filing the entire claim again, this time using a form via physical mail. Include a detailed letter explaining the above.</p>
<p>On day 499, retreat from the battle against home entropy to call Anthem again. Experience a sense of growing dread as the customer service agent is completely unable to locate either of your claims. After a prolonged conversation, she finds it using a different tool. There is no record of the claim from day 418. There was a claim submitted on day 282. Because the claim does not appear in her system, there is no claim. There is a claim. There is no claim. Experience the cognitive equivalent of the Poltergeist hallway shot as the agent tells you ‚ÄúOur members are not eligible for charges for claim submission‚Äù.</p>
<p>Hear the sentence ‚ÄúThere is a claim‚Äù. Hear the sentence ‚ÄúThere is no claim‚Äù. Write these down in the detailed log you‚Äôve been keeping of this unfurling Kafkaesque debacle. Ask again if there is anyone else who can help. There is no manager you can speak to. There is no tier II support. ‚ÄúI‚Äôm the only one you can talk to,‚Äù she says. Write that down.</p>
<p>Call CPAP.com, which has a help line staffed by caring humans. Explain that contrary to their documentation, Anthem now says members cannot file claims for equipment directly. Ask if they are the provider. Discover the provider for the claim is probably your primary care physician, who has no idea this is happening. Leave a message with him anyway. Leave a plaintive message with your original sleep center for good measure.</p>
<p>On day 502, call your sleep center again. They don‚Äôt submit claims to insurance, but they confirm that some people <em>do</em> successfully submit claims to Anthem using the process you‚Äôve been trying. They confirm that Anthem is, in fact, hot garbage. Call your primary, send them everything you have, and ask if they can file a claim for you.</p>
<p>On day 541, receive a letter from Anthem, responding to your inquiry. You weren‚Äôt aware you filed one.</p>
<blockquote>
<p>Please be informed that we have received your concern. Upon review we have noticed that there is no claim billed for the date of service mentioned in the submitted documents, Please provide us with a valid claim. If not submitted,provide us with a valid claim iamge to process your claim further.</p>
</blockquote>
<p>Stare at the letter, typos and all. Contemplate your insignificance in the face of the vast and uncaring universe that is Anthem.</p>
<p>On day 559, steel your resolve and call Anthem again. Wait as this representative, too, digs for evidence of a claim. Listen with delight as she finds your documents from day 282. Confirm that yes, a claim definitely exists. Have her repeat that so you can write it down. Confirm that the previous agent was lying: members can submit claims. At her instruction, fill out the claim form a third time. Write a detailed letter, this time with a Document Control Number (DCN). Submit the entire package via registered mail. Wait for USPS to confirm delivery eight days later.</p>
<p>On day 588, having received no response, call Anthem again. Explain yourself. You‚Äôre getting good at this. Let the agent find a reference number for an appeal, but not the claim. Incant the magic DCN, which unlocks your original claim.  ‚ÄúI was able to confirm that this was a claim submitted form for a member,‚Äù he says. He sees your claim form, your receipts, your confirmation of medical necessity. However: ‚ÄúWe still don‚Äôt have the claim‚Äù.</p>
<p>Wait for him to try system after system. Eventually he confirms what you heard on day 418: the claims department transferred your claims to appeals. ‚ÄúActually this is not an appeal, but it was denied as an appeal.‚Äù Agree as he decides to submit your claim manually again, with the help of his supervisor. Write down the call ref number: he promises you‚Äôll receive an email confirmation, and an Explanation of Benefits in 30-40 business days.</p>
<p>‚ÄúI can assure you this is the last time you are going to call us regarding this.‚Äù</p>
<p>While waiting for this process, recall insurance is a regulated industry. Check the Ohio Revised Code. Realize that section 3901.381 establishes deadlines for health insurers to respond to claims. They should have paid or denied each of your claims within 30 days‚Äì45 if supporting documentation was required. Leave a message with the Ohio Department of Insurance‚Äôs Market Conduct Division. File an insurance complaint with ODI as well.</p>
<p>Grimly wait as no confirmation email arrives.</p>
<p>On day 602, open an email from Anthem. They are ‚Äúable to put the claim in the system and currenty on processed [sic] to be applied‚Äù. They‚Äôre asking for more time. Realize that Anthem is well past the 30-day deadline under the Ohio Revised Code for all three iterations of your claim.</p>
<p>On day 607, call Anthem again. She explains that the claim will be received and processed as of your benefits. She asks you to allow 30-45 days from today. Quote section 3901.381 to her. She promises to expedite the request; it should be addressed within 72 business hours. Like previous agents, she promises to call you back. Nod, knowing she won‚Äôt.</p>
<p>On day 610, email the Ohio Department of Insurance to explain that Anthem has found entirely new ways to avoid paying their claims on time. It‚Äôs been 72 hours without a callback; call Anthem again. She says ‚ÄúYou submitted a claim and it was received‚Äù on day 282. She says the claim was expedited. Ask about the status of that expedited resolution. ‚ÄúBecause on your plan we still haven‚Äôt received any claims,‚Äù she explains. Wonder if you‚Äôre having a stroke.</p>
<p>Explain that it has been 328 days since you submitted your claim, and ask what is going on. She says that since the first page of your mailed claim was a letter, that might have caused it to be processed as an appeal. Remind yourself Anthem told you to enclose that letter. Wait as she attempts to refer you to the subrogation department, until eventually she gives up: the subrogation department doesn‚Äôt want to help.</p>
<p>Call the subrogation department yourself. Allow Anthem‚Äôs representative to induce in you a period of brief aphasia. She wants to call a billing provider. Try to explain there is none: you purchased the machine yourself. She wants to refer you to collections. Wonder why on earth Anthem would want money from <em>you</em>. Write down ‚ÄúI literally can‚Äôt understand what she thinks is going on‚Äù in your log. Someone named Adrian will call you by tomorrow.</p>
<p>Contemplate alternative maneuvers. Go on a deep Google dive, searching for increasingly obscure phrases gleaned from Anthem‚Äôs bureaucracy. Trawl through internal training PDFs for Anthem‚Äôs ethics and compliance procedures. Call their compliance hotline: maybe someone cares about the law. It‚Äôs a third-party call center for Elevance Health. Fail to realize this is another name for Anthem. Begin drawing a map of Anthem‚Äôs corporate structure.</p>
<p>From a combination of publicly-available internal slide decks, LinkedIn, and obscure HR databases, discover the name, email, and phone number of Anthem‚Äôs Chief Compliance Officer. Call her, but get derailed by an internal directory that requires a 10-digit extension. Try the usual tricks with automated phone systems. No dice.</p>
<p>Receive a call from an Anthem agent. Ask her what happened to ‚Äú72 hours‚Äù. She says there‚Äôs been no response from the adjustments team. She doesn‚Äôt know when a response will come. There‚Äôs no one available to talk to. Agree to speak to another representative tomorrow. It doesn‚Äôt matter: they‚Äôll never call you.</p>
<p>Do more digging. Guess the CEO‚Äôs email from what you can glean of Anthem‚Äôs account naming scheme. Write her an email with a short executive summary and a detailed account of the endlessly-unfolding Boschian hellscape in which her company has entrapped you. A few hours later, receive an acknowledgement from an executive concierge at Elevance (Anthem). It‚Äôs polite, formal, and syntactically coherent. She and promises to look into things. Smile. Maybe this will work.</p>
<p>On day 617, receive a call from the executive concierge. 355 days after submission, she‚Äôs identified a problem with your claim. CPAP.com provided you with an invoice with a single line item (the CPAP) and two associated billing codes (a CPAP and humidifier). Explain that they are integrated components of a single machine. She understands, but insists you need a receipt with multiple line items for them anyway. Anthem has called CPAP.com, but they can‚Äôt discuss an invoice unless you call them. Explain you‚Äôll call them right now.</p>
<p>Call CPAP.com. Their customer support continues to be excellent. Confirm that it is literally impossible to separate the CPAP and humidifier, or to produce an invoice with two line items for a single item. Nod as they ask what the hell Anthem is doing. Recall that this is the exact same machine Anthem covered for you eight years ago. Start a joint call with the CPAP.com representative and Anthem‚Äôs concierge. Explain the situation to her voicemail.</p>
<p>On day 623, receive a letter from ODI. Anthem has told ODI this was a problem with the billing codes, and ODI does not intervene in billing code issues. They have, however, initiated a secretive second investigation. There is no way to contact the second investigator.</p>
<p>Write a detailed email to the concierge and ODI explaining that it took over three hundred days for Anthem to inform you of this purported billing code issue. Explain again that it is a single device. Emphasize that Anthem has been handling claims for this device for roughly a decade.</p>
<p>Wait. On day 636, receive a letter from Anthem‚Äôs appeals department. They‚Äôve received your request for an appeal. You never filed one. They want your doctor or facility to provide additional information to Carelon Medical Benefits Management. You have never heard of Carelon. There is no explanation of how to reach Carelon, or what information they might require. The letter concludes: ‚ÄúThere is currently no authorization on file for the services rendered.‚Äù You need to seek authorization from a department called ‚ÄúUtilization Management‚Äù.</p>
<p>Call the executive concierge again. Leave a voicemail asking what on earth is going on.</p>
<p>On day 637, receive an email: she‚Äôs looking into it.</p>
<p>On day 644, Anthem calls you. It‚Äôs a new agent who is immensely polite. Someone you‚Äôve never heard of was asked to work on another project, so she‚Äôs taking over your case. She has no updates yet, but promises to keep in touch.</p>
<p>She does so. On day 653, she informs you Anthem will pay your claim in full. On day 659, she provides a check number. On day 666, the check arrives.</p>
<p>Deposit the check. Write a thank you email to the ODI and Anthem‚Äôs concierge. Write this, too, down in your log.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why is the mouse cursor slightly tilted and not straight? (426 pts)]]></title>
            <link>https://ux.stackexchange.com/questions/52336/why-is-the-mouse-cursor-slightly-tilted-and-not-straight</link>
            <guid>39248225</guid>
            <pubDate>Sun, 04 Feb 2024 06:57:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ux.stackexchange.com/questions/52336/why-is-the-mouse-cursor-slightly-tilted-and-not-straight">https://ux.stackexchange.com/questions/52336/why-is-the-mouse-cursor-slightly-tilted-and-not-straight</a>, See on <a href="https://news.ycombinator.com/item?id=39248225">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mainbar" role="main" aria-label="question and answers">
                
<div data-questionid="52336" data-position-on-page="0" data-score="604" id="question">
        

        

<div>
    
    <div itemprop="text">
                
<p>Is this a legacy thing or does a tilted cursor serves a purpose? I can tell that, the angle provides a totally vertical left edge which helps when highlighting text but what else apart from that?</p>

<p>EDIT: When cursor is swapped by the little hand cursor when hovered over buttons, the angle seems to be smaller. Why the difference?</p>
    </div>

        

    <div>
    <div>
        <p>
            asked <span title="2014-02-17 09:41:59Z">Feb 17, 2014 at 9:41</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/7393/thanos"><p><img src="https://www.gravatar.com/avatar/aac452da1312fa333c36015a5e591f01?s=64&amp;d=identicon&amp;r=PG" alt="Thanos's user avatar" width="32" height="32"></p></a>
    </div>
    
</div>
    
</div>




            <p><span itemprop="commentCount">8</span></p>
    </div>



                
                
                <div id="answers">
                    


                                    
<div id="answer-52338" data-answerid="52338" data-parentid="52336" data-score="729" data-position-on-page="1" data-highest-scored="1" data-question-has-accepted-highest-score="1" itemprop="acceptedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
                
<p>This is the historical reason:</p>

<p><img src="https://i.stack.imgur.com/e1zH5.png" alt="Concept drawing of the standard mouse cursor at an angle"></p>

<p>(Concept drawing taken from document: <a href="http://bitsavers.trailing-edge.com/pdf/xerox/parc/techReports/VLSI-81-1_The_Optical_Mouse.pdf">VLSI-81-1_The_Optical_Mouse.pdf</a>)</p>

<p>The mouse, and therefore the mouse cursor, was <a href="http://arstechnica.com/features/2005/05/gui/2/">invented by Douglas Engelbart</a>, and was initially <a href="http://origin.arstechnica.com/images/gui/4-NLSgui.jpg">an arrow pointing up</a>. </p>

<p>When the <a href="http://arstechnica.com/features/2005/05/gui/3/">XEROX PARC</a> machine was built, the cursor changed into a tilted arrow. It was found that, given the low resolution of the screens in those days, drawing a straight line (left edge of arrow) and a line at a 45 degree angle (right edge of arrow) was easier to do and more recognizable than the straight cursor.</p>
    </div>
    <div>
            
            <div>
        <a href="https://ux.stackexchange.com/users/40110/code-maverick"><p><img src="https://www.gravatar.com/avatar/c056c352518943b11095c83a4ef2b31f?s=64&amp;d=identicon&amp;r=PG" alt="Code Maverick's user avatar" width="32" height="32"></p></a>
    </div>


            <div>
    <div>
        <p>
            answered <span title="2014-02-17 09:47:52Z">Feb 17, 2014 at 9:47</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/5657/bart-gijssens"><p><img src="https://www.gravatar.com/avatar/54b1215ffb534c90dd7ea7f480d28c51?s=64&amp;d=identicon&amp;r=PG" alt="Bart Gijssens's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/5657/bart-gijssens">Bart Gijssens</a><span itemprop="name">Bart Gijssens</span></p><p><span title="reputation score 17,317" dir="ltr">17.3k</span><span>4 gold badges</span><span>49 silver badges</span><span>62 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    
</div>




            <p><span itemprop="commentCount">9</span></p>
    </div>


                                    
<div id="answer-52370" data-answerid="52370" data-parentid="52336" data-score="393" data-position-on-page="2" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
                    

<p>Take your right hand and point to your question.</p>

<p>There, you see. </p>

<p><img src="https://i.stack.imgur.com/xqGFX.jpg" alt="finger pointing at screen"></p>
    </div>
    <div>
            
            <div>
        <a href="https://ux.stackexchange.com/users/127876/silas-reel"><p><img src="https://lh5.googleusercontent.com/-yjvPGG9oHpw/AAAAAAAAAAI/AAAAAAAAAAA/AAN31DV4PX0I1kJiPjyoOIMz70ejP2SvbA/mo/photo.jpg?sz=64" alt="Silas Reel's user avatar" width="32" height="32"></p></a>
    </div>


            <div>
    <div>
        <p>
            answered <span title="2014-02-17 18:13:23Z">Feb 17, 2014 at 18:13</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/43246/jturolla"><p><img src="https://www.gravatar.com/avatar/d24c555de0ab090f0b822155f31affe4?s=64&amp;d=identicon&amp;r=PG" alt="jturolla's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/43246/jturolla">jturolla</a><span itemprop="name">jturolla</span></p><p><span title="reputation score " dir="ltr">3,511</span><span>1 gold badge</span><span>10 silver badges</span><span>7 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    
</div>




            <p><span itemprop="commentCount">27</span></p>
    </div>

                                    
<div id="answer-52349" data-answerid="52349" data-parentid="52336" data-score="189" data-position-on-page="3" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p>In addition to <a href="https://ux.stackexchange.com/a/52338/43668">Bart's answer</a>, I'd like to add one more reason. </p>

<p>The reason the arrow was tilted to the left was so that the click position was easier to calculate, because the origin of the cursor's bitmap was in the upper left.  This saved the mouse tracking subroutine a calculation on every click (its not much but it helped on older machines).  </p>

<p><a href="http://www.reddit.com/r/explainlikeimfive/comments/1qhzym/" rel="noreferrer">Source</a></p>
    </div>
    <div>
            
            <div>
        <a href="https://ux.stackexchange.com/users/-1/community"><p><img src="https://www.gravatar.com/avatar/a007be5a61f6aa8f3e85ae2fc18dd66e?s=64&amp;d=identicon&amp;r=PG" alt="Community's user avatar" width="32" height="32"></p></a>
    </div>


            <div>
    <div>
        <p>
            answered <span title="2014-02-17 14:40:50Z">Feb 17, 2014 at 14:40</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/21591/jameo"><p><img src="https://www.gravatar.com/avatar/939c911747739eeb05c16b6b8a922ed9?s=64&amp;d=identicon&amp;r=PG" alt="Jameo's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/21591/jameo">Jameo</a><span itemprop="name">Jameo</span></p><p><span title="reputation score " dir="ltr">1,853</span><span>1 gold badge</span><span>11 silver badges</span><span>8 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    
</div>




            <p><span itemprop="commentCount">14</span></p>
    </div>


                                    
<div id="answer-52558" data-answerid="52558" data-parentid="52336" data-score="124" data-position-on-page="4" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<h2>Low level visual cognition</h2>
<p>In addition to the various answers given, there is also sense in a tilted mouse pointer if one considers the visual processes in our brain.</p>
<p>Visual information arriving from our eyes is first processed in the primary visual cortex by the V1 area, then by the V2 area. These two areas recognise low-level visual features (hue, lightness, size, orientation, etc.).</p>
<h2>The popout effect</h2>
<p>As visual information is processed by these areas, some visual irregularities truly pop out (ie, they are highly distinguishable), which greatly helps visual search (trying to find an item in a visually busy field). The popular name for this phenomenon is <strong>the popout effect</strong>.</p>
<p>A famous research from 1988 - <a href="http://www2.psychology.uiowa.edu/faculty/hollingworth/prosem/Treisman_Gormican_88_PR_FeatureAnalysisIn.pdf" rel="noreferrer">A. Treisman, and S. Gormican: Feature analysis in early vision: Evidence from search asymmetries</a> summarises many of these popout effects, and the irregularities they involve.</p>
<h2>Orientation</h2>
<p>One such irregularity is <strong>orientation</strong>, and it is neatly explained by the following illustration:</p>
<p><img src="https://i.stack.imgur.com/4xWaH.png" alt="3 images showing many vertical lines and how a tilted line pops out"></p>
<p>You should find it next to impossible to find the search target in 1 (a straight line in a group of straight lines). But rather easy in 2 - finding a tilted line in a group of straight lines. In 3 it should be equally next to impossible to find the tilted line in a group of tilted lines (of the same angle).</p>
<p>Since vertical and horizontal orientations are the most common ones on screens (and in life in general) a tilted mouse pointer will be more easily found.</p>
<p>More information can be found in Chapter 2 (What we can easily see) of <a href="http://www.amazon.co.uk/Visual-Thinking-Kaufmann-Interactive-Technologies/dp/0123708966/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1384303964&amp;sr=1-1&amp;keywords=visual+thinking+for+design" rel="noreferrer">Visual Thinking for Design</a>, Ware 2008.</p>
    </div>
    <div>
            
            <div>
        <a href="https://ux.stackexchange.com/users/-1/community"><p><img src="https://www.gravatar.com/avatar/a007be5a61f6aa8f3e85ae2fc18dd66e?s=64&amp;d=identicon&amp;r=PG" alt="Community's user avatar" width="32" height="32"></p></a>
    </div>


            <div>
    <div>
        <p>
            answered <span title="2014-02-19 23:38:31Z">Feb 19, 2014 at 23:38</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/16924/izhaki"><p><img src="https://www.gravatar.com/avatar/35c050eac0eab06a8c3b6fec8c2bb5c0?s=64&amp;d=identicon&amp;r=PG" alt="Izhaki's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/16924/izhaki">Izhaki</a><span itemprop="name">Izhaki</span></p><p><span title="reputation score 32,465" dir="ltr">32.5k</span><span>5 gold badges</span><span>66 silver badges</span><span>99 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    
</div>




            <p><span itemprop="commentCount">13</span></p>
    </div>

                                    
<div id="answer-52355" data-answerid="52355" data-parentid="52336" data-score="80" data-position-on-page="5" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
                    

<p>I've always thought that the arrow cursor is shaped similarly to your hand if you were point (naturally) at the screen with your (as typically dominant) right hand.</p>

<p>I have no support of this other than my own subjective experience but it strikes me as a natural shape when trying to relate real world interaction into a low resolution computer screen where rendering something resembling a hand would be impossible.</p>

<p>[Edit: Someone stole the only thunder I've ever had on StackAnything. Thanks!]</p>

<p><img src="https://i.stack.imgur.com/qGzNQ.jpg" alt="Hand pointing at screen"></p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2014-02-17 15:23:08Z">Feb 17, 2014 at 15:23</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/43174/user43174"><p><img src="https://www.gravatar.com/avatar/a69a703c86958e2d50e517ffd86c5e01?s=64&amp;d=identicon&amp;r=PG&amp;f=y&amp;so-version=2" alt="user43174's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/43174/user43174">user43174</a><span itemprop="name">user43174</span></p><p><span title="reputation score " dir="ltr">853</span><span>5 silver badges</span><span>4 bronze badges</span>
        </p>
    </div>
</div>
    
</div>




            <p><span itemprop="commentCount">6</span></p>
    </div>

                                    
<div id="answer-52461" data-answerid="52461" data-parentid="52336" data-score="49" data-position-on-page="6" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p>In case anyone wonders : some less known interfaces did use a straight arrow as pointed in <a href="http://www.reddit.com/r/explainlikeimfive/comments/1qhzym/">Reddit</a></p>

<p><img src="https://i.stack.imgur.com/rJmmW.gif" alt="enter image description here"></p>

<p><img src="https://i.stack.imgur.com/ukk6x.gif" alt="enter image description here"></p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2014-02-19 00:48:06Z">Feb 19, 2014 at 0:48</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/25125/gildas-fr%c3%a9mont"><p><img src="https://i.stack.imgur.com/RHbGy.jpg?s=64&amp;g=1" alt="Gildas Fr√©mont's user avatar" width="32" height="32"></p></a>
    </div>
    
</div>
    
</div>




            <p><span itemprop="commentCount">1</span></p>
    </div>

                                    
<div id="answer-52360" data-answerid="52360" data-parentid="52336" data-score="22" data-position-on-page="7" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p>Also, there is another answer to this question. As a rule, the <strong>arrow</strong> mouse cursor must have one sharp tip (vertex) - because it is an arrow :) </p>

<p>On the other hand, it is better for a mouse cursor to look good and slick. </p>

<p>But drawing sharp tip on a rectangular pixel based display is very hard, especially without anti-aliasing. </p>

<p>The 0 degrees (horizontal or vertical) and 45 degrees lines are the only possible lines that look smooth without anti-aliasing. </p>

<p>That is why almost all arrow mouse cursors are based on one straight and one 45 degrees lines. As a result, the bisector line has angle of 45/2 = 22.5 degrees.</p>

<p>The tail of the arrow is much harder to be drawn well, but it is not so important as well. </p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2014-02-17 16:30:01Z">Feb 17, 2014 at 16:30</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/25879/johnfound"><p><img src="https://i.stack.imgur.com/R81XM.png?s=64&amp;g=1" alt="johnfound's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/25879/johnfound">johnfound</a><span itemprop="name">johnfound</span></p><p><span title="reputation score " dir="ltr">1,116</span><span>8 silver badges</span><span>16 bronze badges</span>
        </p>
    </div>
</div>
    
</div>




            <p><span itemprop="commentCount">4</span></p>
    </div>

                                    
<div id="answer-68326" data-answerid="68326" data-parentid="52336" data-score="7" data-position-on-page="8" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p><strong>It is a right-handed world.</strong> </p>

<p>It used to be that if you switched our right/left click buttons the arrow would point towards the right (opposite of the images cited). </p>

<p>This supports that the arrow mimics a hand pointing while providing angular contrast. Without a reference, it is an extension of the <em>desktop</em> metaphor.</p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2014-12-03 19:19:22Z">Dec 3, 2014 at 19:19</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/10767/ken"><p><img src="https://i.stack.imgur.com/htBYy.png?s=64&amp;g=1" alt="Ken's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/10767/ken">Ken</a><span itemprop="name">Ken</span></p><p><span title="reputation score " dir="ltr">1,232</span><span>7 silver badges</span><span>10 bronze badges</span>
        </p>
    </div>
</div>
    
</div>




            <p><span itemprop="commentCount">1</span></p>
    </div>

                                    
<div id="answer-97398" data-answerid="97398" data-parentid="52336" data-score="5" data-position-on-page="9" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p>The fact that the mouse cursor is slightly tilted to the left makes a lot of sense. 
A very interesting fact:</p>

<p>If it were straight, it would take a nanosecond more to place the cursor on the desired object. Human mind is generally used to perceiving elements from left to the right, that is why the cursor is designed into the opposite direction, anticipating the intent of interaction with the element you are about to click on.</p>

<p>A nanosecond of time optimization is the closest thing to the absolute idea of irrelevance. With that I agree. However, on a perception level, it makes a huge difference. </p>

<p>The tilted cursor becomes similar to an athlete who's always on the start position, ready to take off towards anything you want to click on at any time.</p>

<p>It's a sensation that gives you so much comfort without you realizing why.</p>

<p>Semiotics, Cognitive Science and Psychology are all embedded into the simple and subtle decision of keeping the tilted cursor, just to simplify by a bit your experience.</p>

<p>Why was it tilted in the first place? Well, in its history, it seems like it was only an accident determined by some technical limitations:</p>

<p><a href="http://www.fastcodesign.com/3026625/why-the-mouse-cursor-is-tilted-instead-of-vertical" rel="nofollow">Why Your Mouse Cursor Looks The Way It Does</a></p>
    </div>
    <div>
            
            <div>
    
    <div>
        <a href="https://ux.stackexchange.com/users/54669/devin"><p><img src="https://i.stack.imgur.com/egmb3.jpg?s=64&amp;g=1" alt="Devin's user avatar" width="32" height="32"></p></a>
    </div>
    <div>
        <p><a href="https://ux.stackexchange.com/users/54669/devin">Devin</a></p><p><span title="reputation score 37,762" dir="ltr">37.8k</span><span>15 gold badges</span><span>79 silver badges</span><span>140 bronze badges</span>
        </p>
    </div>
</div>


            <div>
    <div>
        <p>
            answered <span title="2016-07-29 04:24:58Z">Jul 29, 2016 at 4:24</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/86656/mircea"><p><img src="https://graph.facebook.com/10208868140194564/picture?type=large" alt="Mircea's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/86656/mircea">Mircea</a><span itemprop="name">Mircea</span></p><p><span title="reputation score " dir="ltr">522</span><span>3 silver badges</span><span>4 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    
</div>




            <p><span itemprop="commentCount">1</span></p>
    </div>

                                    
<div id="answer-68302" data-answerid="68302" data-parentid="52336" data-score="3" data-position-on-page="10" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
    
    <div itemprop="text">
<p>The angle, the cursor is inclined at gives a better feeling of pointing something. A cursor straight at 90 degree would not provide a good effect.It provides  improved appearance on low resolution screens.</p>

<p>Also the position calculation would become a lot easier when done from the top left corner of the pixel.</p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2014-12-03 13:04:10Z">Dec 3, 2014 at 13:04</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/41491/ashu"><p><img src="https://i.stack.imgur.com/3KnoW.jpg?s=64&amp;g=1" alt="ashu's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/41491/ashu">ashu</a><span itemprop="name">ashu</span></p><p><span title="reputation score " dir="ltr">249</span><span>1 silver badge</span><span>9 bronze badges</span>
        </p>
    </div>
</div>
    
</div>

                                    
<div id="answer-101006" data-answerid="101006" data-parentid="52336" data-score="2" data-position-on-page="11" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
    
    <p>A straight cursor would also obscure more of the object underneath raising the same issues when designing for touch interfaces</p>
    <div>
    <div>
        <p>
            answered <span title="2016-11-01 22:07:43Z">Nov 1, 2016 at 22:07</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/47160/mark-c"><p><img src="https://graph.facebook.com/710302166/picture?type=large" alt="Mark C's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/47160/mark-c">Mark C</a><span itemprop="name">Mark C</span></p><p><span title="reputation score " dir="ltr">151</span><span>1 silver badge</span><span>4 bronze badges</span>
        </p>
    </div>
</div>
    
</div>

                                    
<div id="answer-135748" data-answerid="135748" data-parentid="52336" data-score="2" data-position-on-page="12" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
    
    <div itemprop="text">
<p>Well, the cursor is a pointer, and mimics pointer angles from real life (~30-45¬∞ to the vertical).</p>
<p><a href="https://i.stack.imgur.com/J8xzk.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/J8xzk.png" alt="Pointers in the real-world"></a></p>
<p>Importantly, that angle serves to <strong>guide the eye down the length of the pointer</strong>, in the direction going "into" the screen, <strong>towards a single point</strong>, in the same way as perspective drawings do:</p>
<p><a href="https://i.stack.imgur.com/mKdC5.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/mKdC5.png" alt="Perspective drawing"></a></p>
<p>On the contrary, a straight arrow seems to point in the general up-direction, targeting no one point in particular. Have you ever used, or seen someone use, a pointer stick vertically upwards? That is indeed awkward, and reserved for moments where the object being pointed to is high up and well beyond the height of the person and the length of the stick combined, and can be vague in conveying what is actually being pointed at.</p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2020-11-26 20:32:45Z">Nov 26, 2020 at 20:32</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/28743/snag"><p><img src="https://www.gravatar.com/avatar/4a6245cc648b214ad6a440bfe18d0152?s=64&amp;d=identicon&amp;r=PG&amp;f=y&amp;so-version=2" alt="SNag's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/28743/snag">SNag</a><span itemprop="name">SNag</span></p><p><span title="reputation score " dir="ltr">9,597</span><span>3 gold badges</span><span>22 silver badges</span><span>26 bronze badges</span>
        </p>
    </div>
</div>
    
</div>


                                    



                            <h2 data-loc="1">
                                
                            </h2>
                </div>
            </div></div>]]></description>
        </item>
    </channel>
</rss>