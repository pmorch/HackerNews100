<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 07 Oct 2024 10:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[How do HTTP servers figure out Content-Length? (103 pts)]]></title>
            <link>https://aarol.dev/posts/go-contentlength/</link>
            <guid>41762468</guid>
            <pubDate>Mon, 07 Oct 2024 03:18:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aarol.dev/posts/go-contentlength/">https://aarol.dev/posts/go-contentlength/</a>, See on <a href="https://news.ycombinator.com/item?id=41762468">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>Published 06.10.2024</span></p><p>Anyone who has implemented a simple HTTP server can tell you that it is a really simple protocol. Basically, it’s a text file that has some specific rules to make parsing it easier.</p><p>All HTTP requests look something like this: <sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></p><div><pre tabindex="0"><code data-lang="bash"><span><span>GET /path HTTP/1.1<span>\r\n</span>
</span></span><span><span>Host: aarol.dev<span>\r\n</span>
</span></span><span><span>Accept-Language: en,fi-FI<span>\r\n</span>
</span></span><span><span>Accept-Encoding: gzip, deflate<span>\r\n</span>
</span></span><span><span><span>\r\n</span>
</span></span></code></pre></div><p>The first line is the “request line”, and it has the requested method, path and HTTP version.
The following lines are headers, each terminated with “carriage return” and “line feed” characters. There is also an extra CRLF at the end, to mark the end of the header section. After that, there is the message body which can whatever data you want to send.</p><p>Here is a simple Go program to demonstrate this, using a raw TCP socket for the client:</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>func</span> <span>main</span><span>()</span> <span>{</span>
</span></span><span><span>	<span>// Setup an HTTP server to respond to the path "/test"
</span></span></span><span><span><span></span>	<span>http</span><span>.</span><span>HandleFunc</span><span>(</span><span>"GET /test"</span><span>,</span> <span>func</span><span>(</span><span>w</span> <span>http</span><span>.</span><span>ResponseWriter</span><span>,</span> <span>r</span> <span>*</span><span>http</span><span>.</span><span>Request</span><span>)</span> <span>{</span>
</span></span><span><span>		<span>w</span><span>.</span><span>Write</span><span>([]</span><span>byte</span><span>(</span><span>"Hello"</span><span>))</span>
</span></span><span><span>		<span>w</span><span>.</span><span>Write</span><span>([]</span><span>byte</span><span>(</span><span>" world!"</span><span>))</span>
</span></span><span><span>	<span>})</span>
</span></span><span><span>	<span>go</span> <span>http</span><span>.</span><span>ListenAndServe</span><span>(</span><span>"localhost:2024"</span><span>,</span> <span>nil</span><span>)</span>
</span></span><span><span>
</span></span><span><span>	<span>// Connect to the server using TCP
</span></span></span><span><span><span></span>	<span>conn</span><span>,</span> <span>err</span> <span>:=</span> <span>net</span><span>.</span><span>Dial</span><span>(</span><span>"tcp"</span><span>,</span> <span>"localhost:2024"</span><span>)</span>
</span></span><span><span>	<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
</span></span><span><span>		<span>panic</span><span>(</span><span>err</span><span>)</span>
</span></span><span><span>	<span>}</span>
</span></span><span><span>	<span>// Write the HTTP request (no body, only request line and Host header)
</span></span></span><span><span><span></span>	<span>_</span><span>,</span> <span>err</span> <span>=</span> <span>conn</span><span>.</span><span>Write</span><span>([]</span><span>byte</span><span>(</span><span>"GET /test HTTP/1.1\r\nHost: localhost\r\n\r\n"</span><span>))</span>
</span></span><span><span>	<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
</span></span><span><span>		<span>panic</span><span>(</span><span>err</span><span>)</span>
</span></span><span><span>	<span>}</span>
</span></span><span><span>
</span></span><span><span>	<span>buf</span> <span>:=</span> <span>make</span><span>([]</span><span>byte</span><span>,</span> <span>1024</span><span>)</span> <span>// 1 kb
</span></span></span><span><span><span></span>	<span>// Read the response
</span></span></span><span><span><span></span>	<span>n</span><span>,</span> <span>err</span> <span>:=</span> <span>conn</span><span>.</span><span>Read</span><span>(</span><span>buf</span><span>)</span>
</span></span><span><span>	<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
</span></span><span><span>		<span>panic</span><span>(</span><span>err</span><span>)</span>
</span></span><span><span>	<span>}</span>
</span></span><span><span>	<span>fmt</span><span>.</span><span>Println</span><span>(</span><span>string</span><span>(</span><span>buf</span><span>[:</span><span>n</span><span>]))</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>When I tried this, I got the following response:</p><div><pre tabindex="0"><code data-lang="http"><span><span><span>HTTP</span><span>/</span><span>1.1</span> <span>200</span> <span>OK</span>
</span></span><span><span><span>Date</span><span>:</span> <span>Sun, 06 Oct 2024 14:51:13 GMT</span>
</span></span><span><span><span>Content-Length</span><span>:</span> <span>12</span>
</span></span><span><span><span>Content-Type</span><span>:</span> <span>text/plain; charset=utf-8</span>
</span></span><span><span>
</span></span><span><span><span>Hello</span> <span>world!</span>
</span></span></code></pre></div><p>One interesting thing about this is the header <code>Content-Length</code>, which has a value of 12. That’s exactly the length of <code>"Hello world!"</code> in UTF-8. In the above Go code, writing the response happens in two parts: first we write<code>"Hello"</code>, then we write <code>" world!"</code>. Notice that we didn’t need to call any function to write the headers, but they are still in the response. In Go’s http package, a status of 200 and the headers are automatically written if <code>w.Write()</code> is called before <code>w.WriteHeader()</code>. Any calls to <code>w.WriteHeader()</code> after that are useless and will output a warning.</p><p>Remember that in HTTP, the headers are always written <em>before</em> the body. How is it possible that before writing “Hello” to the connection, the server already knows how long the response will be? What if I wanted to write one “Hello”, and then a thousand exclamation marks after that? Or a million? Does the server need to know how long every response is before sending it? It would mean that every response needs to be kept in memory for the entire duration of the handler.</p><p>I wanted to figure this out, and I didn’t have to look too deep. I found this <a href="https://github.com/golang/go/blob/2f507985dc24d198b763e5568ebe5c04d788894f/src/net/http/server.go#L1630C1-L1663C22">amazing comment</a> in the standard library’s <code>net/http/client.go</code> file. Basically, it says that if the response is small enough to fit into one “chunking buffer”, the length can be calculated very easily, and sent all at once. If the response is bigger than the buffer, it is sent in chunks. What does this mean in practice? I’ve modified the above code to demonstrate it:</p><div><pre tabindex="0"><code data-lang="go"><span><span>
</span></span><span><span><span>func</span> <span>main</span><span>()</span> <span>{</span>
</span></span><span><span>	<span>http</span><span>.</span><span>HandleFunc</span><span>(</span><span>"GET /test"</span><span>,</span> <span>func</span><span>(</span><span>w</span> <span>http</span><span>.</span><span>ResponseWriter</span><span>,</span> <span>r</span> <span>*</span><span>http</span><span>.</span><span>Request</span><span>)</span> <span>{</span>
</span></span><span><span>		<span>w</span><span>.</span><span>Write</span><span>([]</span><span>byte</span><span>(</span><span>"Hello"</span><span>))</span>
</span></span><span><span>		<span>w</span><span>.</span><span>Write</span><span>([]</span><span>byte</span><span>(</span><span>strings</span><span>.</span><span>Repeat</span><span>(</span><span>"!"</span><span>,</span> <span>3000</span><span>)))</span>
</span></span><span><span>	<span>})</span>
</span></span><span><span>
</span></span><span><span>	<span>go</span> <span>http</span><span>.</span><span>ListenAndServe</span><span>(</span><span>"localhost:2024"</span><span>,</span> <span>nil</span><span>)</span>
</span></span><span><span>	<span>// removed all error handling for brevity
</span></span></span><span><span><span></span>	<span>conn</span><span>,</span> <span>_</span> <span>:=</span> <span>net</span><span>.</span><span>Dial</span><span>(</span><span>"tcp"</span><span>,</span> <span>"localhost:2024"</span><span>)</span>
</span></span><span><span>
</span></span><span><span>	<span>conn</span><span>.</span><span>Write</span><span>([]</span><span>byte</span><span>(</span><span>"GET /test HTTP/1.1\r\nHost: localhost\r\n\r\n"</span><span>))</span>
</span></span><span><span>
</span></span><span><span>	<span>buf</span> <span>:=</span> <span>make</span><span>([]</span><span>byte</span><span>,</span> <span>1024</span><span>)</span> <span>// 1 kb
</span></span></span><span><span><span></span>	<span>for</span> <span>{</span>
</span></span><span><span>		<span>conn</span><span>.</span><span>SetReadDeadline</span><span>(</span><span>time</span><span>.</span><span>Now</span><span>().</span><span>Add</span><span>(</span><span>1</span> <span>*</span> <span>time</span><span>.</span><span>Second</span><span>))</span>
</span></span><span><span>		<span>n</span><span>,</span> <span>err</span> <span>:=</span> <span>conn</span><span>.</span><span>Read</span><span>(</span><span>buf</span><span>)</span>
</span></span><span><span>		<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
</span></span><span><span>			<span>break</span>
</span></span><span><span>		<span>}</span>
</span></span><span><span>
</span></span><span><span>		<span>fmt</span><span>.</span><span>Println</span><span>(</span><span>string</span><span>(</span><span>buf</span><span>[:</span><span>n</span><span>]))</span>
</span></span><span><span>	<span>}</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>The handler now returns “Hello!!!!!!!!!!! …” with 3000 exclamation marks. This is bigger than the configured chunk size, so we can see what happens. This is the response:</p><div><pre tabindex="0"><code data-lang="fallback"><span><span>HTTP/1.1 200 OK
</span></span><span><span>Date: Sun, 06 Oct 2024 16:43:28 GMT
</span></span><span><span>Content-Type: text/plain; charset=utf-8
</span></span><span><span>Transfer-Encoding: chunked
</span></span><span><span>
</span></span><span><span>800
</span></span><span><span>Hello!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
</span></span><span><span>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
</span></span><span><span>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
</span></span><span><span>3bd
</span></span><span><span>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
</span></span><span><span>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!        
</span></span><span><span>0
</span></span></code></pre></div><p>There is no <code>Content-Length</code> header in the response now. Instead, we have a <code>Transfer-Encoding: chunked</code>. Our message is being <em>chunked</em>, sent in multiple parts, so that the server doesn’t need to fit the whole thing into memory at once. Clever!</p><p>The first line of the response data is now a number “800”. Why 800? As it turns out, the number is actually a hexadecimal number, <code>0x800</code> is 2048 in base 10. Similarly, <code>0x3bd</code>, is 957 in base 10. 2048+957=3005, which is the full length of our message! Sending the length of the message before the actual message is a common method to efficiently transfer unknown lengths of data. It is used in the <a href="https://redis.io/docs/latest/develop/reference/protocol-spec">Redis protocol</a> for example.</p><p>Chunked transfer encoding was added in HTTP 1.1.<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> This means that it’s very old and basically all HTTP servers &amp; clients support it. Sending chunked responses also allows something called “trailers”, which are headers that are sent <em>after</em> the body. In Go, trailers need to be explicitly declared before sending them:</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>http</span><span>.</span><span>HandleFunc</span><span>(</span><span>"GET /test"</span><span>,</span> <span>func</span><span>(</span><span>w</span> <span>http</span><span>.</span><span>ResponseWriter</span><span>,</span> <span>r</span> <span>*</span><span>http</span><span>.</span><span>Request</span><span>)</span> <span>{</span>
</span></span><span><span>	<span>w</span><span>.</span><span>Header</span><span>().</span><span>Set</span><span>(</span><span>"Trailer"</span><span>,</span> <span>"My-Trailer"</span><span>)</span>
</span></span><span><span>	<span>w</span><span>.</span><span>Write</span><span>([]</span><span>byte</span><span>(</span><span>"Hello"</span><span>))</span>
</span></span><span><span>	<span>w</span><span>.</span><span>Write</span><span>([]</span><span>byte</span><span>(</span><span>strings</span><span>.</span><span>Repeat</span><span>(</span><span>"!"</span><span>,</span> <span>3000</span><span>)))</span>
</span></span><span><span>	<span>w</span><span>.</span><span>Header</span><span>().</span><span>Add</span><span>(</span><span>"My-Trailer"</span><span>,</span> <span>"test"</span><span>)</span>
</span></span><span><span><span>})</span>
</span></span></code></pre></div><p>This is useful for things like digital signatures, which need to be computed from the response body. In Go’s http handlers, adding trailers will also automatically make the response chunked even if it normally wouldn’t be.</p><p>HTTP/2 and HTTP/3 don’t support chunked transfer encoding, as they have their own streaming mechanisms.<sup id="fnref1:2"><a href="#fn:2" role="doc-noteref">2</a></sup></p><p>This is one of the many things that happen in the background when using the <code>net/http</code> package in Go. <code>http.ResponseWriter</code> is a remarkably simple interface, having just 3 methods on it. Behind this API is a lot of magic, including <a href="https://cs.opensource.google/go/go/+/master:src/net/http/sniff.go">Content-Type sniffing</a> and the implicit header writing I mentioned above. It’s great that these things are handled automatically, but in programming I think it’s always important to have an idea of what is happening at the layer below.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fast B-Trees (134 pts)]]></title>
            <link>https://www.scattered-thoughts.net/writing/smolderingly-fast-btrees/</link>
            <guid>41761986</guid>
            <pubDate>Mon, 07 Oct 2024 01:42:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.scattered-thoughts.net/writing/smolderingly-fast-btrees/">https://www.scattered-thoughts.net/writing/smolderingly-fast-btrees/</a>, See on <a href="https://news.ycombinator.com/item?id=41761986">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  
<p>(<em>This is part of a series on the design of a language. See the list of posts <a href="https://www.scattered-thoughts.net/#zest">here</a></em>.)</p>
<p>Many 'scripting' languages use a hashmap for their default associative data-structure (javascript objects, python dicts, etc). Hashtables have a lot of annoying properties:</p>
<ul>
<li>Vulnerable to <a href="https://en.wikipedia.org/wiki/Collision_attack#Hash_flooding">hash flooding</a>. </li>
<li>If protected against hash flooding by random seeds, the iteration order becomes non-deterministic which is annoying for snapshot testing, reproducible builds, etc.</li>
<li>May have to rehash on insert, which produces terrible worst-case latencies for large hashmaps.</li>
<li>Repeatedly growing large allocations without fragmentation is difficult on wasm targets, because virtual memory tricks are not available and pages can't be unmapped.</li>
<li>Vector instructions on wasm are limited and there are no <a href="https://en.wikipedia.org/wiki/AES_instruction_set">AES instructions</a>. This makes many hash functions much slower.</li>
</ul>
<p>Ordered data-structures like b-trees don't have any of these disadvantages. They are typically slower than hashmaps, but I was surprised to find fairly wide variation in people's expectations of how much slower. So let's compare:</p>
<ul>
<li>rust's <a href="https://doc.rust-lang.org/std/collections/struct.HashMap.html">std::collections::HashMap</a> with siphash</li>
<li>rust's <a href="https://doc.rust-lang.org/std/collections/struct.BTreeMap.html">std::collections::BTreeMap</a></li>
<li>zig's <a href="https://github.com/ziglang/zig/blob/master/lib/std/hash_map.zig">std.HashMap</a> with siphash</li>
<li>zig's <a href="https://github.com/ziglang/zig/blob/master/lib/std/hash_map.zig">std.HashMap</a> with wyhash</li>
<li>My own <a href="https://github.com/jamii/maps/blob/main/bptree.zig">bptree</a> with various compile-time options for different experiments.</li>
</ul>
<h2 id="microbenchmarks-are-hard">microbenchmarks are hard</h2>
<p>Let's start with the dumbest possible benchmark - fill a map with uniformly distributed random integers, measure how many cycles takes to lookup all those integers, and take the mean over many iterations.</p>
<pre data-lang="zig"><code data-lang="zig"><span>const</span><span> before </span><span>= </span><span>rdtscp</span><span>()</span><span>;
</span><span>for </span><span>(keys) </span><span>|</span><span>key</span><span>| </span><span>{
</span><span>    </span><span>const</span><span> value_found </span><span>=</span><span> map.</span><span>get</span><span>(key)</span><span>;
</span><span>    </span><span>if </span><span>(value_found </span><span>== </span><span>null</span><span>) {
</span><span>        </span><span>panic</span><span>(</span><span>"Value not found"</span><span>,</span><span> .{})</span><span>;
</span><span>    }
</span><span>}
</span><span>const</span><span> after </span><span>= </span><span>rdtscp</span><span>()</span><span>;
</span><span>record</span><span>(</span><span>"lookup_hit_all"</span><span>,</span><span> map.</span><span>count</span><span>()</span><span>, </span><span>@divTrunc</span><span>(after </span><span>-</span><span> before</span><span>,</span><span> map.</span><span>count</span><span>()))</span><span>;
</span></code></pre>
<div><p>lookup_hit_all / uniform u64</p></div>
<table><thead><tr><th>log2(#keys)</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th><th>16</th></tr></thead><tbody>
<tr><td>rust btree</td><td>46</td><td>26</td><td>19</td><td>54</td><td>78</td><td>94</td><td>106</td><td>133</td><td>152</td><td>166</td><td>193</td><td>215</td><td>236</td><td>262</td><td>290</td><td>316</td><td>367</td></tr>
<tr><td>rust hashmap siphash</td><td>102</td><td>67</td><td>49</td><td>40</td><td>37</td><td>34</td><td>33</td><td>32</td><td>32</td><td>32</td><td>32</td><td>33</td><td>33</td><td>34</td><td>37</td><td>43</td><td>51</td></tr>
<tr><td>zig b+tree</td><td>46</td><td>26</td><td>37</td><td>56</td><td>64</td><td>87</td><td>100</td><td>110</td><td>123</td><td>143</td><td>165</td><td>180</td><td>197</td><td>220</td><td>235</td><td>269</td><td>294</td></tr>
<tr><td>zig hashmap siphash</td><td>99</td><td>84</td><td>64</td><td>69</td><td>70</td><td>68</td><td>68</td><td>68</td><td>68</td><td>68</td><td>68</td><td>68</td><td>69</td><td>68</td><td>69</td><td>73</td><td>77</td></tr>
<tr><td>zig hashmap wyhash</td><td>80</td><td>35</td><td>29</td><td>35</td><td>34</td><td>35</td><td>34</td><td>33</td><td>34</td><td>32</td><td>33</td><td>31</td><td>31</td><td>32</td><td>32</td><td>33</td><td>34</td></tr>
</tbody></table>
<p>That makes btrees look pretty bad. Much worse, in fact, than the <a href="https://github.com/Rufflewind/bench-maps/">other benchmark</a> that several people pointed me at, where at similar sizes btree lookups are only ~2x slower than hashmaps. But don't worry, we can reproduce that too:</p>
<pre data-lang="zig"><code data-lang="zig"><span>for </span><span>(keys) </span><span>|</span><span>key</span><span>| </span><span>{
</span><span>    </span><span>const</span><span> before </span><span>= </span><span>rdtscp</span><span>()</span><span>;
</span><span>    </span><span>const</span><span> value_found </span><span>=</span><span> map.</span><span>get</span><span>(key)</span><span>;
</span><span>    </span><span>const</span><span> after </span><span>= </span><span>rdtscp</span><span>()</span><span>;
</span><span>    </span><span>record</span><span>(</span><span>"lookup_hit_one"</span><span>,</span><span> map.</span><span>count</span><span>()</span><span>,</span><span> after </span><span>-</span><span> before)</span><span>;
</span><span>    </span><span>if </span><span>(value_found </span><span>== </span><span>null</span><span>) {
</span><span>        </span><span>panic</span><span>(</span><span>"Value not found"</span><span>,</span><span> .{})</span><span>;
</span><span>    }
</span><span>}
</span></code></pre>
<div><p>lookup_hit_one / uniform u64</p></div>
<table><thead><tr><th>log2(#keys)</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th><th>16</th></tr></thead><tbody>
<tr><td>rust btree</td><td>47</td><td>48</td><td>50</td><td>82</td><td>107</td><td>123</td><td>135</td><td>163</td><td>182</td><td>200</td><td>227</td><td>256</td><td>279</td><td>309</td><td>328</td><td>358</td><td>405</td></tr>
<tr><td>rust hashmap siphash</td><td>101</td><td>101</td><td>101</td><td>100</td><td>105</td><td>103</td><td>102</td><td>103</td><td>103</td><td>103</td><td>103</td><td>108</td><td>112</td><td>116</td><td>124</td><td>140</td><td>166</td></tr>
<tr><td>zig b+tree</td><td>45</td><td>45</td><td>59</td><td>72</td><td>85</td><td>107</td><td>126</td><td>135</td><td>148</td><td>170</td><td>188</td><td>203</td><td>223</td><td>246</td><td>264</td><td>292</td><td>319</td></tr>
<tr><td>zig hashmap siphash</td><td>106</td><td>108</td><td>116</td><td>117</td><td>120</td><td>121</td><td>123</td><td>124</td><td>124</td><td>124</td><td>123</td><td>124</td><td>127</td><td>130</td><td>132</td><td>137</td><td>147</td></tr>
<tr><td>zig hashmap wyhash</td><td>53</td><td>53</td><td>57</td><td>63</td><td>68</td><td>69</td><td>72</td><td>72</td><td>73</td><td>71</td><td>72</td><td>69</td><td>76</td><td>81</td><td>78</td><td>87</td><td>92</td></tr>
</tbody></table>
<p>All we did differently was average over one lookup at a time instead of over many, but somehow that made the hashmaps 2-3x slower!</p>
<p>Both of these benchmarks are pretty bad, so let's make better versions of both before trying to explain the differences. I only did this for the zig data-structures, because I am lazy.</p>
<p>First, rather than looking the keys up in the same order we inserted them, we'll precompute a random sample (with replacement):</p>
<pre data-lang="zig"><code data-lang="zig"><span>const</span><span> keys_hitting </span><span>= </span><span>try</span><span> map.allocator.</span><span>alloc</span><span>(Key</span><span>, </span><span>@max</span><span>(batch_size</span><span>,</span><span> count))</span><span>;
</span><span>var</span><span> permutation </span><span>= </span><span>XorShift64</span><span>{}</span><span>;
</span><span>for </span><span>(keys_hitting) </span><span>|*</span><span>key</span><span>| </span><span>{
</span><span>    </span><span>const</span><span> i </span><span>=</span><span> permutation.</span><span>next</span><span>() </span><span>%</span><span> count</span><span>;
</span><span>    key</span><span>.* =</span><span> keys[i]</span><span>;
</span><span>}
</span></code></pre>
<p>Then rather than measuring a single lookup at a time, we'll measure a batch of 256 lookups to amortize out the overhead of the rdtscp instruction and pull the panics out of the measurement:</p>
<pre data-lang="zig"><code data-lang="zig"><span>for </span><span>(0</span><span>..</span><span>@divTrunc</span><span>(keys_hitting.len</span><span>,</span><span> batch_size)) </span><span>|</span><span>batch</span><span>| </span><span>{
</span><span>    </span><span>const</span><span> keys_batch </span><span>=</span><span> keys_hitting[batch </span><span>*</span><span> batch_size </span><span>..</span><span>][0</span><span>..</span><span>batch_size]</span><span>;
</span><span>    </span><span>var </span><span>values_found</span><span>:</span><span> [batch_size]</span><span>?</span><span>Key </span><span>= </span><span>undefined</span><span>;
</span><span>
</span><span>    </span><span>const</span><span> before </span><span>= </span><span>rdtscp</span><span>()</span><span>;
</span><span>    </span><span>var </span><span>key</span><span>: </span><span>Key </span><span>=</span><span> keys_batch[</span><span>0</span><span>]</span><span>;
</span><span>    </span><span>for </span><span>(</span><span>&amp;</span><span>values_found</span><span>,</span><span> 0</span><span>..</span><span>) </span><span>|*</span><span>value</span><span>,</span><span> i</span><span>| </span><span>{
</span><span>        value</span><span>.* =</span><span> map.</span><span>get</span><span>(key)</span><span>;
</span><span>        key </span><span>=</span><span> keys_batch[(i </span><span>+ </span><span>1</span><span>) </span><span>%</span><span> keys_batch.len]</span><span>;
</span><span>    }
</span><span>    </span><span>const</span><span> after </span><span>= </span><span>rdtscp</span><span>()</span><span>;
</span><span>    </span><span>report</span><span>(</span><span>"lookup_hit_batch"</span><span>,</span><span> map.</span><span>count</span><span>()</span><span>, </span><span>@divTrunc</span><span>(after </span><span>-</span><span> before</span><span>,</span><span> batch_size))</span><span>;
</span><span>
</span><span>    </span><span>for </span><span>(values_found) </span><span>|</span><span>value_found</span><span>| </span><span>{
</span><span>        </span><span>if </span><span>(value_found </span><span>== </span><span>null</span><span>) {
</span><span>            </span><span>panic</span><span>(</span><span>"Value not found"</span><span>,</span><span> .{})</span><span>;
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>The results look similar to the results for <code>lookup_hit_all</code>:</p>
<div><p>lookup_hit_batch / uniform u64</p></div>
<table><thead><tr><th>log2(#keys)</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th><th>16</th></tr></thead><tbody>
<tr><td>zig b+tree</td><td>6</td><td>9</td><td>31</td><td>47</td><td>60</td><td>82</td><td>102</td><td>112</td><td>126</td><td>147</td><td>174</td><td>186</td><td>204</td><td>230</td><td>245</td><td>276</td><td>299</td></tr>
<tr><td>zig hashmap siphash</td><td>52</td><td>53</td><td>61</td><td>68</td><td>71</td><td>72</td><td>75</td><td>76</td><td>76</td><td>75</td><td>75</td><td>76</td><td>78</td><td>80</td><td>81</td><td>88</td><td>95</td></tr>
<tr><td>zig hashmap wyhash</td><td>29</td><td>29</td><td>31</td><td>35</td><td>38</td><td>38</td><td>40</td><td>41</td><td>42</td><td>40</td><td>40</td><td>42</td><td>41</td><td>39</td><td>42</td><td>46</td><td>43</td></tr>
</tbody></table>
<p>Now we make one tiny tweak. Instead of iterating through the batch in order, we'll use the value we just looked up to pick the next key.</p>
<pre data-lang="zig"><code data-lang="zig"><span>for </span><span>(0</span><span>..</span><span>@divTrunc</span><span>(keys_hitting.len</span><span>,</span><span> batch_size)) </span><span>|</span><span>batch</span><span>| </span><span>{
</span><span>    </span><span>const</span><span> keys_batch </span><span>=</span><span> keys_hitting[batch </span><span>*</span><span> batch_size </span><span>..</span><span>][0</span><span>..</span><span>batch_size]</span><span>;
</span><span>    </span><span>var </span><span>values_found</span><span>:</span><span> [batch_size]</span><span>?</span><span>Key </span><span>= </span><span>undefined</span><span>;
</span><span>
</span><span>    </span><span>const</span><span> before </span><span>= </span><span>rdtscp</span><span>()</span><span>;
</span><span>    </span><span>var </span><span>key</span><span>: </span><span>Key </span><span>=</span><span> keys_batch[</span><span>0</span><span>]</span><span>;
</span><span>    </span><span>for </span><span>(</span><span>&amp;</span><span>values_found</span><span>,</span><span> 0</span><span>..</span><span>) </span><span>|*</span><span>value</span><span>,</span><span> i</span><span>| </span><span>{
</span><span>        value</span><span>.* =</span><span> map.</span><span>get</span><span>(key)</span><span>;
</span><span>        key </span><span>=</span><span> keys_batch[(i </span><span>+</span><span> value</span><span>.*.?</span><span>) </span><span>%</span><span> keys_batch.len]</span><span>; </span><span>// &lt;-- we changed this line
</span><span>    }
</span><span>    </span><span>const</span><span> after </span><span>= </span><span>rdtscp</span><span>()</span><span>;
</span><span>    </span><span>report</span><span>(</span><span>"lookup_hit_chain"</span><span>,</span><span> map.</span><span>count</span><span>()</span><span>, </span><span>@divTrunc</span><span>(after </span><span>-</span><span> before</span><span>,</span><span> batch_size))</span><span>;
</span><span>
</span><span>    </span><span>for </span><span>(values_found) </span><span>|</span><span>value_found</span><span>| </span><span>{
</span><span>        </span><span>if </span><span>(value_found </span><span>== </span><span>null</span><span>) {
</span><span>            </span><span>panic</span><span>(</span><span>"Value not found"</span><span>,</span><span> .{})</span><span>;
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>Now we have two benchmarks with the exact same batch size and the exact same instructions to execute. The only difference is that in <code>lookup_hit_chain</code> we've introduced a data dependency between each iteration of the loop - we need to know <code>value</code> before we know what to lookup next. This prevents successful speculative execution of the next loop iteration.</p>
<div><p>lookup_hit_chain / uniform u64</p></div>
<table><thead><tr><th>log2(#keys)</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th><th>16</th></tr></thead><tbody>
<tr><td>zig b+tree</td><td>7</td><td>25</td><td>37</td><td>48</td><td>59</td><td>82</td><td>105</td><td>115</td><td>126</td><td>148</td><td>172</td><td>186</td><td>198</td><td>219</td><td>240</td><td>273</td><td>300</td></tr>
<tr><td>zig hashmap siphash</td><td>77</td><td>79</td><td>88</td><td>88</td><td>90</td><td>91</td><td>93</td><td>94</td><td>94</td><td>93</td><td>95</td><td>99</td><td>102</td><td>103</td><td>106</td><td>115</td><td>130</td></tr>
<tr><td>zig hashmap wyhash</td><td>35</td><td>37</td><td>44</td><td>48</td><td>52</td><td>52</td><td>55</td><td>57</td><td>55</td><td>54</td><td>57</td><td>63</td><td>64</td><td>63</td><td>70</td><td>76</td><td>88</td></tr>
</tbody></table>
<p>The wyhash hashmap halves it's lookup time in <code>lookup_hit_batch</code> vs <code>lookup_hit_chain</code> but the btree doesn't benefit at all. I don't understand the limits to speculative execution at all, but let's make some mildly informed guesses.</p>
<p>At 2^16 keys the whole dataset is about 1mb, which fits comfortably in L2 but is much bigger than L1. Each hashmap lookup costs 1 or maybe 2 L2 cache lookups, each of which have ~20 cycle latency. The wyhash <a href="https://github.com/ziglang/zig/blob/3b465ebec59ee942b6c490ada2f81902ec047d7f//lib/std/hash/wyhash.zig#L110-L113">fast path for u64</a> is only a handful of instructions with no branches:</p>
<pre><code><span>bench[0x1014710] &lt;+0&gt;:  rorxq  $0x20, %rdi, %rdx
</span><span>bench[0x1014716] &lt;+6&gt;:  movabsq $-0x18fc812e5f4bd725, %rax ; imm = 0xE7037ED1A0B428DB
</span><span>bench[0x1014720] &lt;+16&gt;: xorq   %rax, %rdx
</span><span>bench[0x1014723] &lt;+19&gt;: movabsq $0x1ff5c2923a788d2c, %rcx ; imm = 0x1FF5C2923A788D2C
</span><span>bench[0x101472d] &lt;+29&gt;: xorq   %rdi, %rcx
</span><span>bench[0x1014730] &lt;+32&gt;: mulxq  %rcx, %rcx, %rdx
</span><span>bench[0x1014735] &lt;+37&gt;: movabsq $-0x5f89e29b87429bd9, %rsi ; imm = 0xA0761D6478BD6427
</span><span>bench[0x101473f] &lt;+47&gt;: xorq   %rcx, %rsi
</span><span>bench[0x1014742] &lt;+50&gt;: xorq   %rax, %rdx
</span><span>bench[0x1014745] &lt;+53&gt;: mulxq  %rsi, %rcx, %rax
</span><span>bench[0x101474a] &lt;+58&gt;: xorq   %rcx, %rax
</span><span>bench[0x101474d] &lt;+61&gt;: retq
</span></code></pre>
<p>So while we're waiting for one cache lookup we can start hashing the next key (if we can predict what it is) and maybe even get started on the next cache lookup.</p>
<p>The btree at 2^16 keys has 4 levels. There are typically 15-16 keys per node when inserting random keys, so we'll expect to do around 32 comparisons on average before finding our key. The body of that search loop looks like:</p>
<pre><code><span>bench[0x10121b0] &lt;+16&gt;: cmpq   %rdx, (%rdi,%rax,8)
</span><span>bench[0x10121b4] &lt;+20&gt;: jae    0x10121c2                 ; &lt;+34&gt; at bptree.zig
</span><span>bench[0x10121b6] &lt;+22&gt;: addq   $0x1, %rax
</span><span>bench[0x10121ba] &lt;+26&gt;: cmpq   %rax, %rsi
</span><span>bench[0x10121bd] &lt;+29&gt;: jne    0x10121b0                 ; &lt;+16&gt; [inlined] bench.less_than_u64 + 9 at bench.zig:159:5
</span></code></pre>
<p>So 5 instructions, including two branches, per comparison. At least 160 instructions and 64 branches for the whole lookup.</p>
<p>The 16 keys take up 2 cachelines, so we'll average 1.5 cache lookups for each linear key search, plus 1 cache lookup to hit the child/values array. 10 cache lookups in total for the whole btree lookup. Each of those cache lookups depends on the previous lookup so it'll be hard to speculate correctly, but prefetching might help within a single node. If every cache lookup hit L2 in strict series we'd expect ~200 cycles, but probably some of the earlier nodes are in L1.</p>
<p>Anyway, let's leave that rabbit hole alone for now. It's enough to notice that hashmaps benefit from speculative execution between multiple lookups and btrees don't.</p>
<p>We can roughly think of <code>lookup_hit_batch</code> as measuring throughput and <code>lookup_hit_chain</code> as measuring latency. All the other benchmarks I've seen only measure one or the other, which starts to explain the disagreement over btree vs hashmap performance.</p>
<h2 id="string-keys">string keys</h2>
<p>Btrees do a lot of key comparisons. If the keys are integers then they are stored in the btree node, so it doesn't matter too much. But for strings keys we potentially pay for a cache lookup for every comparison.</p>
<div><p>lookup_hit_chain / random strings</p></div>
<table><thead><tr><th>log2(#keys)</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th><th>16</th></tr></thead><tbody>
<tr><td>zig b+tree</td><td>101</td><td>124</td><td>144</td><td>160</td><td>184</td><td>219</td><td>255</td><td>279</td><td>310</td><td>366</td><td>427</td><td>470</td><td>512</td><td>577</td><td>700</td><td>826</td><td>965</td></tr>
<tr><td>zig hashmap siphash</td><td>145</td><td>147</td><td>154</td><td>159</td><td>162</td><td>163</td><td>165</td><td>167</td><td>168</td><td>173</td><td>181</td><td>186</td><td>196</td><td>211</td><td>247</td><td>287</td><td>312</td></tr>
<tr><td>zig hashmap wyhash</td><td>49</td><td>50</td><td>59</td><td>65</td><td>68</td><td>69</td><td>72</td><td>74</td><td>75</td><td>81</td><td>88</td><td>93</td><td>103</td><td>119</td><td>154</td><td>188</td><td>219</td></tr>
</tbody></table>
<p>Completely random strings is actually pretty unlikely and unfairly benefits the btree, which typically only has to compare the first character of each string to find that they are not equal. But real datasets (eg urls in a log) often have large common prefixes. We can see the difference if we make the first 16 characters constant:</p>
<div><p>lookup_hit_chain / random-ish strings</p></div>
<table><thead><tr><th>log2(#keys)</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th><th>16</th></tr></thead><tbody>
<tr><td>zig b+tree</td><td>102</td><td>150</td><td>221</td><td>338</td><td>546</td><td>618</td><td>807</td><td>911</td><td>1077</td><td>1393</td><td>1507</td><td>1641</td><td>1812</td><td>2095</td><td>2628</td><td>2848</td><td>3302</td></tr>
<tr><td>zig hashmap siphash</td><td>145</td><td>147</td><td>153</td><td>159</td><td>162</td><td>163</td><td>165</td><td>167</td><td>168</td><td>174</td><td>182</td><td>187</td><td>197</td><td>210</td><td>245</td><td>282</td><td>313</td></tr>
<tr><td>zig hashmap wyhash</td><td>49</td><td>50</td><td>59</td><td>66</td><td>69</td><td>69</td><td>72</td><td>74</td><td>74</td><td>81</td><td>88</td><td>93</td><td>102</td><td>117</td><td>154</td><td>188</td><td>214</td></tr>
</tbody></table>
<p>Hashmaps don't care, but the btree is really hurting.</p>
<p>If we're just supporting strings we can optimize for this case by storing the length of the common prefix within each node. But it's hard to imagine how to generalize that to arbitrary types. What if the key is a tuple of strings? Or a small set?</p>
<h2 id="wasm-hashes">wasm hashes</h2>
<p>Let's try the same in wasm, where hash functions have less access to fast vector instructions. Wasm also doesn't have rdtscp so times here are in nanoseconds rather than cycles.</p>
<div><p>lookup_hit_chain / uniform u64 / wasm</p></div>
<table><thead><tr><th>log2(#keys)</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th><th>16</th></tr></thead><tbody>
<tr><td>zig b+tree</td><td>5</td><td>14</td><td>19</td><td>22</td><td>27</td><td>38</td><td>45</td><td>49</td><td>53</td><td>59</td><td>70</td><td>75</td><td>80</td><td>87</td><td>98</td><td>111</td><td>120</td></tr>
<tr><td>zig hashmap siphash</td><td>33</td><td>34</td><td>37</td><td>38</td><td>40</td><td>40</td><td>41</td><td>42</td><td>41</td><td>41</td><td>42</td><td>43</td><td>44</td><td>45</td><td>46</td><td>52</td><td>58</td></tr>
<tr><td>zig hashmap wyhash</td><td>29</td><td>30</td><td>33</td><td>35</td><td>36</td><td>36</td><td>37</td><td>38</td><td>38</td><td>37</td><td>38</td><td>40</td><td>40</td><td>42</td><td>43</td><td>47</td><td>51</td></tr>
</tbody></table>
<div><p>lookup_hit_chain / random strings / wasm</p></div>
<table><thead><tr><th>log2(#keys)</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th><th>16</th></tr></thead><tbody>
<tr><td>zig b+tree</td><td>64</td><td>77</td><td>88</td><td>97</td><td>117</td><td>141</td><td>154</td><td>167</td><td>192</td><td>216</td><td>269</td><td>287</td><td>294</td><td>308</td><td>369</td><td>436</td><td>500</td></tr>
<tr><td>zig hashmap siphash</td><td>65</td><td>64</td><td>68</td><td>70</td><td>73</td><td>71</td><td>72</td><td>73</td><td>73</td><td>75</td><td>78</td><td>81</td><td>84</td><td>88</td><td>102</td><td>118</td><td>135</td></tr>
<tr><td>zig hashmap whyhash</td><td>45</td><td>45</td><td>48</td><td>50</td><td>52</td><td>52</td><td>53</td><td>54</td><td>54</td><td>56</td><td>59</td><td>62</td><td>65</td><td>69</td><td>79</td><td>93</td><td>105</td></tr>
</tbody></table>
<p>The overall ratios are fairly similar, although wyhash seems to have been penalized a little relative to siphash.</p>
<p>Neither the hash functions nor the table scan generates vector instructions at all, even when comparing strings. And, uh, now that I think to look, they don't generate vector instructions on x86 either. So I guess that's a non-issue.</p>
<h2 id="btree-tuning">btree tuning</h2>
<p>I implemented both btrees and b+trees. I didn't see much difference between them for insert/lookup, so I preferred the b+tree for the easier/faster implementation of scans and range queries.</p>
<p>The rust btreemap fixes the max key count per node to 11. For all the workloads I've tried the sweet spot seems to be to fix the node size to 512 bytes, which is 31 keys for u64 and 20 keys for strings. </p>
<p>Allowing leaves and branches to have different sizes didn't help.</p>
<p>I gained a small speedup by manually laying out the node like <code>key_count, keys, values/children</code>. Zig by default prefers to put <code>key_count</code> at the end of the struct to avoid padding, but we always read the key_count first so it's nice to get some keys on the same cacheline. Maintaining this optimization across different architectures was annoying though, so I rolled it back and it's not reflected in the tests above.</p>
<p>The rust btreemap <a href="https://github.com/rust-lang/rust/blob/55a22d2a63334e0faff0202b72a31ce832b56125/library/alloc/src/collections/btree/search.rs#L223">switches on Ordering</a>. I got a small boost from instead using <code>less_than</code> during the search and calling <code>equal</code> afterwards. For a lookup at 2^16 keys we'll expect to call <code>less_than</code> 32 times and <code>equal</code> once, so it's worth paying for the extra call to <code>equal</code> in exchange for tightening up the inner search loop.</p>
<p>I use tried various different binary searches. The best was this 'branchless' variant:</p>
<pre data-lang="zig"><code data-lang="zig"><span>fn </span><span>searchBinary</span><span>(</span><span>keys</span><span>:</span><span> []</span><span>Key</span><span>, </span><span>search_key</span><span>: </span><span>Key</span><span>) </span><span>usize </span><span>{
</span><span>    </span><span>if </span><span>(keys.len </span><span>== </span><span>0</span><span>) </span><span>return </span><span>0</span><span>;
</span><span>    </span><span>var </span><span>offset</span><span>: </span><span>usize </span><span>= </span><span>0</span><span>;
</span><span>    </span><span>var </span><span>length</span><span>: </span><span>usize </span><span>=</span><span> keys.len</span><span>;
</span><span>    </span><span>while </span><span>(length </span><span>&gt; </span><span>1</span><span>) {
</span><span>        </span><span>const</span><span> half </span><span>=</span><span> length </span><span>/ </span><span>2</span><span>;
</span><span>        </span><span>const</span><span> mid </span><span>=</span><span> offset </span><span>+</span><span> half</span><span>;
</span><span>        </span><span>if </span><span>(</span><span>less_than</span><span>(keys[mid]</span><span>,</span><span> search_key)) {
</span><span>            </span><span>@branchHint</span><span>(.unpredictable)</span><span>;
</span><span>            offset </span><span>=</span><span> mid</span><span>;
</span><span>        }
</span><span>        length </span><span>-=</span><span> half</span><span>;
</span><span>    }
</span><span>    offset </span><span>+= </span><span>@intFromBool</span><span>(</span><span>less_than</span><span>(keys[offset]</span><span>,</span><span> search_key))</span><span>;
</span><span>    </span><span>return</span><span> offset</span><span>;
</span><span>}
</span></code></pre>
<p><code>while (length &gt; 1)</code> requires a branch but it's easily predictable. <code>branchHint(.unpredictable)</code> causes llvm to emit a conditional move for <code>offset = mid</code>. </p>
<pre><code><span>bench[0x101f5e0] &lt;+0&gt;:  movq   %rsi, %rax
</span><span>bench[0x101f5e3] &lt;+3&gt;:  testq  %rsi, %rsi
</span><span>bench[0x101f5e6] &lt;+6&gt;:  je     0x101f616                 ; &lt;+54&gt; at bptree.zig
</span><span>bench[0x101f5e8] &lt;+8&gt;:  xorl   %ecx, %ecx
</span><span>bench[0x101f5ea] &lt;+10&gt;: cmpq   $0x1, %rax
</span><span>bench[0x101f5ee] &lt;+14&gt;: je     0x101f60b                 ; &lt;+43&gt; [inlined] bench.less_than_u64 at bench.zig:185:5
</span><span>bench[0x101f5f0] &lt;+16&gt;: movq   %rax, %rsi
</span><span>bench[0x101f5f3] &lt;+19&gt;: shrq   %rsi
</span><span>bench[0x101f5f6] &lt;+22&gt;: leaq   (%rcx,%rsi), %r8
</span><span>bench[0x101f5fa] &lt;+26&gt;: cmpq   %rdx, (%rdi,%r8,8)
</span><span>bench[0x101f5fe] &lt;+30&gt;: cmovbq %r8, %rcx
</span><span>bench[0x101f602] &lt;+34&gt;: subq   %rsi, %rax
</span><span>bench[0x101f605] &lt;+37&gt;: cmpq   $0x1, %rax
</span><span>bench[0x101f609] &lt;+41&gt;: ja     0x101f5f0                 ; &lt;+16&gt; at bptree.zig:334:37
</span><span>bench[0x101f60b] &lt;+43&gt;: cmpq   %rdx, (%rdi,%rcx,8)
</span><span>bench[0x101f60f] &lt;+47&gt;: adcq   $0x0, %rcx
</span><span>bench[0x101f613] &lt;+51&gt;: movq   %rcx, %rax
</span><span>bench[0x101f616] &lt;+54&gt;: retq
</span></code></pre>
<p>Linear search was still faster in most benchmarks though, even with ludicrously large node sizes.</p>
<p>I also tried a btree variant I saw in some paper where leaves are left unsorted and keys are always inserted at the end of the leaf. This saves some memcopying in the common case, but having to sort the leaf before splitting negates the benefit.</p>
<h2 id="outcome">outcome</h2>
<p>All the benchmarks above are basically best case scenarios, where we're doing a lot of lookups in a row. If we were just doing one lookup in the middle of some other work then the btree might not be in cache at all and each of those 2.5 cache lookups per level are going all the way to main memory. That's catastrophic. Whereas an open-addressed hashmap will typically only hit 1 or 2 cachelines per lookup regardless of size.</p>
<p>And while btrees avoid many of the performance edge cases of hashmaps, they also have some cliffs of their own to fall off as comparisons get more expensive and touch more memory, as we saw with the random-ish strings above.</p>
<p>I haven't measure space usage yet, but we can expect it to be worse for btrees. For random keys the typical node occupancy is 50%, minus per-node overhead like key_count, whereas I've been running the zig hashmaps at 80%. So we can guesstimate the btrees will use &gt;60% more memory.</p>
<p>Space usage is really bad for small maps too. I'd need to add some extra tweaks to allow the btree root node to start small and grow, rather than paying the full 512 bytes for a map with only 1 key.</p>
<p>Overall, I'm unconvinced that it's worth exploring btrees further. I'll stick to hashmaps and I'll either iterate in insertion order or I'll require sorting entries before iterating. But if anyone else likes the look of this rabbit hole I left some other ideas untouched:</p>
<ul>
<li>Consider the hash function part of the stable api for the compiler. Use an open-addressed hashtable that preserves hash order. Solve hash flooding without seeding the hash eg by falling back to a tree.</li>
<li><a href="https://en.wikipedia.org/wiki/Hash_array_mapped_trie">Hash-array mapped tries</a> have similar O(log(n)) cacheline behaviour to btrees. But if we used open-addressed hashtables at the leaves we could keep the nesting depth pretty low.</li>
<li>In-memory LSMs with incremental merging work pretty well in differential dataflow, but still have this O(log(n)) cacheline behaviour. But maybe with a hashmap as secondary index lookups might be reasonable, and we can consider slow inserts the price to pay for not rehashing.</li>
</ul>
<h2 id="miscellanea">miscellanea</h2>
<p>All the experiments here are using:</p>
<ul>
<li>rustc 1.77.2</li>
<li>zig 0.14.0-dev.1694+3b465ebec</li>
<li>wasmtime-cli 20.0.2</li>
</ul>
<p>Running on an intel i7-1260P with:</p>
<ul>
<li>efficiency cores disabled</li>
<li>scaling governer set to performance</li>
<li>turbo mode disabled</li>
<li>aslr disabled</li>
</ul>
<p>Usually I would also use cset to pin experiments to a shielded core, but it seems that recent systemd versions have broken that workflow and I haven't figured out a replacement yet.</p>
<p>The rest of the benchmark setup can be found in <a href="https://github.com/jamii/maps">jamii/maps</a>. I measured more than just lookups, but btrees have similar performance for every operation due to having to find a key first.</p>
<p>Cache latency numbers came from <a href="https://www.intel.com/content/www/us/en/developer/articles/tool/intelr-memory-latency-checker.html">mlc</a> and roughly match claims I found online.</p>
<p><em>Thanks to Dan Luu for help thinking through cache behaviour.</em></p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google’s AI thinks I left a Gatorade bottle on the moon (329 pts)]]></title>
            <link>https://edwardbenson.com/2024/10/google-ai-thinks-i-left-gatorade-on-the-moon</link>
            <guid>41761497</guid>
            <pubDate>Mon, 07 Oct 2024 00:07:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://edwardbenson.com/2024/10/google-ai-thinks-i-left-gatorade-on-the-moon">https://edwardbenson.com/2024/10/google-ai-thinks-i-left-gatorade-on-the-moon</a>, See on <a href="https://news.ycombinator.com/item?id=41761497">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article>
<p>Google's <a href="https://notebooklm.google/">NotebookLLM</a> is the first AI podcast I'd actually listen to.
Give it a web page or document, and it will generate you a podcast about it.</p>
<p>It's really good. But it's also really easy to trick.</p>
<p>So I modified my website to lie to it:</p>
<ul>
<li>When a human visits <a href="https://edwardbenson.com/">my homepage</a>, they see a regular page about me.</li>
<li>When Google's AI visits my homepage, it sees fake producer's show notes for an episode about me flying to the moon on my bike with balloons and a scuba tank.</li>
</ul>
<p><img src="https://edwardbenson.com/images/homepage-trick.png" alt=""></p>
<p>The result is pretty hilarious -- have a listen to the true history of the US Space Program:</p>
<audio controls="">
  <source type="audio/mp3" src="https://edwardbenson.com/moonwalk.mp3">
  <p>Your browser does not support the audio element.</p>
</audio>
<h2>More Seriously..</h2>
<p>If it's this easy to detect an AI and provide it a "special" set of facts, you'd better believe people are already doing it all over the web.</p>
<p>The attack vector is this:</p>
<ol>
<li>First, acquire a web page that ranks highly for a particular term.</li>
<li>Next, plant an "AI Only" version of the content, hidden to humans, designed to bias how the AI thinks.</li>
</ol>
<p>Then, when the AI searches the web to assist with an answer, it won't just find lies, but weaponized lies: content designed specifically for LLM manipulation.</p>
<p>So next time you ask your LLM of choice for advice on something, and you notice it's searching the web to prepare an answer, be aware that its response is potentially compromised by tactics like this - even if you can't see them with your own eyes when you check the sources.</p>
<h2>Technical Details</h2>
<p><strong>Steering the LLM</strong></p>
<p>I'd <a href="https://www.reddit.com/r/notebooklm/comments/1fr31h8/comment/lpj6uef/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button">read that</a> NotebookLLM was easy to steer by feeding it fake "producer show notes", so that's how I typed my fake story.</p>
<p><a href="https://edwardbenson.com/moon-goof">Here is a copy in full</a>.</p>
<p>I did no edits and only a single generation, and it followed my beat sheet <strong>exactly</strong>, so I'd say that was 10/10 steerability.</p>
<p><strong>Tricking the Scraper Bot</strong></p>
<p>You can upload a documents with fake show notes straight to NotebookLLM's website, so if you're making silly podcast episodes for your kids, that's the best way to do it.</p>
<p>But if you want to trick Google's bot on your website, just detect the <a href="https://developers.google.com/search/docs/crawling-indexing/overview-google-crawlers">GoogleOther</a> user agent in your request headers and then serve your "special" data instead of the real website.</p>
<p>Here's an <a href="https://github.com/eob/isai">NPM package called <code>isai</code></a> I scraped together to make this simple. It's based on <a href="https://npmjs.com/isbot">isbot</a>. When rendering your page, just say:</p>
<pre><code>import { isai } from "isai";

if (isai(request.headers.get("User-Agent"))) {
  // Return web page just for AI consumption
} else {
  // Return web page for human consumption
}
</code></pre>
<p>Warning that <code>GoogleOther</code> is not exclusive to NotebookLLM; it appears to be used for a variety of non-production Google products, so doing this risks seeding other Google properties with bad data about you. For this reason, I took down the moon story from my actual homepage for <code>GoogleOther</code> agents.</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sq.io: jq for databases and more (408 pts)]]></title>
            <link>https://sq.io</link>
            <guid>41760697</guid>
            <pubDate>Sun, 06 Oct 2024 22:02:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sq.io">https://sq.io</a>, See on <a href="https://news.ycombinator.com/item?id=41760697">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><code>sq</code> is a free/libre <a href="https://github.com/neilotoole/sq">open-source</a> data wrangling swiss-army knife
to inspect, query, join, import, and export data. You could think of <code>sq</code>
as <a href="https://jqlang.github.io/jq/">jq</a> for databases and documents, facilitating one-liners
like:</p><div><pre tabindex="0"><code data-lang="shell"><span><span>sq <span>'@postgres_db | .actor | .first_name, .last_name | .[0:5]'</span>
</span></span></code></pre></div><h2 id="installation">Installation </h2><div><nav id="sq-install" role="tablist">


</nav><div id="sq-install-content"><div id="sq-install-0" role="tabpanel" aria-labelledby="sq-install-0-tab"><pre tabindex="0"><code data-lang="shell"><span><span>brew install neilotoole/sq/sq</span></span></code></pre></div><div id="sq-install-1" role="tabpanel" aria-labelledby="sq-install-1-tab"><pre tabindex="0"><code data-lang="shell"><span><span>/bin/sh -c <span>"</span><span>$(</span>curl -fsSL https://sq.io/install.sh<span>)</span><span>"</span></span></span></code></pre></div><div id="sq-install-2" role="tabpanel" aria-labelledby="sq-install-2-tab"><pre tabindex="0"><code data-lang="shell"><span><span>scoop bucket add sq https://github.com/neilotoole/sq
</span></span><span><span>scoop install sq</span></span></code></pre></div><p>Install options for <code>apt</code>, <code>yum</code>, <code>apk</code>, <code>pacman</code>, <code>yay</code> over <a href="https://sq.io/docs/install">here</a>.</p></div></div><p>For help, <code>sq help</code> is your starting point. And then see the <a href="https://sq.io/docs/overview">docs</a>.</p><h3 id="lets-get-this-out-of-the-way">Let’s get this out of the way </h3><p><code>sq</code> is pronounced like <em>seek</em>. Its query language, <code>SLQ</code>, is pronounced like <em>sleek</em>.</p><h2 id="feature-highlights">Feature Highlights </h2><p>Some feature highlights are shown below. For more, see the <a href="https://sq.io/docs/overview">docs</a>,
including the <a href="https://sq.io/docs/query">query guide</a>, <a href="https://sq.io/docs/tutorial">tutorial</a> and <a href="https://sq.io/docs/cookbook">cookbook</a>.</p><h3 id="diff-database-tables">Diff database tables </h3><p>Use the <a href="https://sq.io/docs/diff">diff</a> command to compare source metadata or row values.</p><p><img src="https://sq.io/images/sq_diff_table_data.png" alt="sq diff"></p><h3 id="import-excel-worksheet-into-postgres-table">Import Excel worksheet into Postgres table </h3><p><a href="https://sq.io/docs/output#insert">Insert</a> the contents of an Excel XLSX worksheet (from a sheet named <code>actor</code>) into
a new Postgres table named <code>xl_actor</code>. Note that the import mechanism
is reasonably sophisticated in that it tries to preserve data types.</p><h3 id="view-metadata-for-a-database">View metadata for a database </h3><p>The <code>--json</code> flag to <a href="https://sq.io/docs/inspect"><code>sq inspect</code></a> outputs schema and other metadata in JSON.
Typically the output is piped to jq to select the interesting elements.</p><h3 id="get-names-of-all-columns-in-a-mysql-table">Get names of all columns in a MySQL table </h3><p>Even easier, just get the metadata for the table you want:</p><div><pre tabindex="0"><code data-lang="shell"><span><span>sq inspect @sakila_my.actor -j <span>|</span> jq -r <span>'.columns[] | .name'</span>
</span></span></code></pre></div><h3 id="execute-sql-query-against-sql-server-insert-results-to-sqlite">Execute SQL query against SQL Server, insert results to SQLite </h3><p>This snippet adds a (pre-existing) SQL Server source, and creates a
new <a href="https://sq.io/docs/drivers/sqlite">SQLite</a> source. Then, a raw native SQL query is executed against
<a href="https://sq.io/docs/drivers/sqlserver">SQL Server</a>, and the results are inserted into SQLite.</p><h3 id="export-all-database-tables-to-csv">Export all database tables to CSV </h3><p>Get the (JSON) metadata for the active source; pipe that JSON to jq and
extract the table names; pipe the table names
to <code>xargs</code>, invoking <code>sq</code> once for each table, outputting a CSV file per table. This snippet
was tested on macOS.</p><p>If you instead wanted to use <code>sql</code> mode:</p><div><pre tabindex="0"><code data-lang="shell"><span><span>sq inspect -j <span>|</span> jq -r <span>'.tables[] | .name'</span> <span>|</span> xargs -I % sq sql <span>'SELECT * FROM %'</span> --csv --output %.csv
</span></span></code></pre></div><h3 id="source-commands">Source commands </h3><p>Commands to <a href="https://sq.io/docs/cmd/add">add</a>, <a href="https://sq.io/docs/cmd/src">activate</a>, <a href="https://sq.io/docs/cmd/mv">move</a>,
<a href="https://sq.io/docs/cmd/ls">list</a>, <a href="https://sq.io/docs/cmd/group">group</a>, <a href="https://sq.io/docs/cmd/ping">ping</a>
or <a href="https://sq.io/docs/cmd/rm">remove</a> sources.</p><div><pre tabindex="0"><code data-lang="shell"><span><span>$ sq src                      <span># show active source</span>
</span></span><span><span>$ sq add ./actor.tsv          <span># add a source</span>
</span></span><span><span>$ sq src @actor_tsv           <span># set active source</span>
</span></span><span><span>$ sq ls                       <span># list sources</span>
</span></span><span><span>$ sq ls -v                    <span># list sources (verbose)</span>
</span></span><span><span>$ sq group                    <span># get active group</span>
</span></span><span><span>$ sq group prod               <span># set active group</span>
</span></span><span><span>$ sq mv @sales @prod/sales    <span># rename a source</span>
</span></span><span><span>$ sq ping --all               <span># ping all sources</span>
</span></span><span><span>$ sq rm @actor_tsv            <span># remove a source</span>
</span></span></code></pre></div><h3 id="database-table-commands">Database table commands </h3><p>Convenient commands that act on database tables: <a href="https://sq.io/docs/cmd/tbl-copy">copy</a>, <a href="https://sq.io/docs/cmd/tbl-truncate">truncate</a>, <a href="https://sq.io/docs/cmd/tbl-drop">drop</a>.</p><p>Note that <code>sq tbl copy</code> only applies within a single database.
If you want to copy a table from one database to another,
use the <a href="https://sq.io/docs/output#insert"><code>--insert</code></a> mechanism.</p><div><pre tabindex="0"><code data-lang="shell"><span><span>$ sq tbl copy .actor .actor2  <span># copy table "actor" to "actor2", creating if necessary</span>
</span></span><span><span>$ sq tbl truncate .actor2     <span># truncate table "actor2"</span>
</span></span><span><span>$ sq tbl drop .actor2         <span># drop table "actor2"</span>
</span></span></code></pre></div><h3 id="query-jsonl-eg-log-files">Query JSONL (e.g. log files) </h3><p><a href="https://sq.io/docs/output#jsonl">JSONL</a> output is a row of JSON per line (hence “JSON Lines”). Lots of log output is like this.
We can use <code>sq</code>’s own <a href="https://sq.io/docs/config/#logging">log</a> output as an example:</p><div><pre tabindex="0"><code data-lang="json"><span><span><span>{</span><span>"level"</span><span>:</span><span>"debug"</span><span>,</span><span>"time"</span><span>:</span><span>"00:07:48.799992"</span><span>,</span><span>"caller"</span><span>:</span><span>"sqlserver/sqlserver.go:452:(*database).Close"</span><span>,</span><span>"msg"</span><span>:</span><span>"Close database: @sakila_mssql | sqlserver | sqlserver://sakila:xxxxx@localhost?database=sakila"</span><span>}</span>
</span></span><span><span><span>{</span><span>"level"</span><span>:</span><span>"debug"</span><span>,</span><span>"time"</span><span>:</span><span>"00:07:48.800016"</span><span>,</span><span>"caller"</span><span>:</span><span>"source/files.go:323:(*Files).Close"</span><span>,</span><span>"msg"</span><span>:</span><span>"Files.Close invoked: has 1 clean funcs"</span><span>}</span>
</span></span><span><span><span>{</span><span>"level"</span><span>:</span><span>"debug"</span><span>,</span><span>"time"</span><span>:</span><span>"00:07:48.800031"</span><span>,</span><span>"caller"</span><span>:</span><span>"source/files.go:61:NewFiles.func1"</span><span>,</span><span>"msg"</span><span>:</span><span>"About to clean fscache from dir: /var/folders/68/qthwmfm93zl4mqdw_7wvsv7w0000gn/T/sq_files_fscache_2273841732"</span><span>}</span>
</span></span></code></pre></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hyprland 0.44 (105 pts)]]></title>
            <link>https://hyprland.org/news/update44/</link>
            <guid>41760513</guid>
            <pubDate>Sun, 06 Oct 2024 21:33:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hyprland.org/news/update44/">https://hyprland.org/news/update44/</a>, See on <a href="https://news.ycombinator.com/item?id=41760513">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p data-svelte-h="svelte-s3x0m6">0.44.0 is finally upon us!</p> <p data-svelte-h="svelte-ewo9j0">This update mostly focuses on further bugfixes over actually implementing new stuff,
but we have quite a bunch of both!</p> <p data-svelte-h="svelte-16iz8ah">When it comes to new stuff, we have, amongst others:</p> <ul data-svelte-h="svelte-10xrz7"><li>A new argument <code>--version</code> for checking the Hyprland binary’s version instead of <code>hyprctl version</code></li> <li>Same spirit as above, <code>--systeminfo</code>.</li> <li>A new layerrule <code>order</code> for ordering layers on the same plane</li> <li>A new <code>HYPRLAND_CONFIG</code> envvar for another method of passing an explicit config instead of <code>--config</code></li> <li>Dwindle has got some new stuff: <code>movetoroot</code> layout message and <code>split_bias</code> config option</li> <li>Layout-general, auto-grouping is now configurable</li> <li>hyprctl has some new stuff, a few more props to <code>monitors</code> and <code>workspacerules</code> requests, and a new <code>submap</code> request.</li> <li>A new user-check for the <code>XDG_CURRENT_DESKTOP</code> env has been added as Hyprland no longer overwrites it</li> <li>Implemented the single-pixel-buffer protocol</li></ul> <p data-svelte-h="svelte-1kmci3g">For fixes, notable ones include:</p> <ul data-svelte-h="svelte-1vmyll6"><li>Various fixes to surface UV and positioning calculations: chromium windows no longer go crazy when resizing, and resizing other apps should generally feel much smoother</li> <li>Various text-input fixes for IME users</li> <li>XWayland handling fixes for monitors being misplaced and input not translating correctly after monitors were unplugged / replugged.</li> <li>Some drag-n-drop edge cases have been fixed</li> <li>Initial cursor warping has been fixed</li> <li>Various crashes fixed</li> <li>Lots of cleanups of the underlying codebase :)</li></ul> <p data-svelte-h="svelte-tfm0b6">Generally, this update shouldn’t impact you negatively, and only polish some rough edges.</p> <p data-svelte-h="svelte-2n2w03">See the full release notes on <a href="https://github.com/hyprwm/Hyprland/releases/tag/v0.44.0" rel="nofollow">Github</a>.</p> <p data-svelte-h="svelte-qwni0v">Cheers, vax.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rust needs a web framework for lazy developers (210 pts)]]></title>
            <link>https://ntietz.com/blog/rust-needs-a-web-framework-for-lazy-developers/</link>
            <guid>41760421</guid>
            <pubDate>Sun, 06 Oct 2024 21:21:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ntietz.com/blog/rust-needs-a-web-framework-for-lazy-developers/">https://ntietz.com/blog/rust-needs-a-web-framework-for-lazy-developers/</a>, See on <a href="https://news.ycombinator.com/item?id=41760421">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I like to make silly things, and I also like to put in <em>minimal effort</em> for those silly things.
I also like to make things in Rust, mostly for the web, and this is where we run into a problem.</p>
<p>See, if I want to make something for the web, I could use Django but I don't want that.
I mean, Django is for building <a href="https://engineering.fb.com/2023/09/07/culture/threads-inside-story-metas-newest-social-app/">serious businesses</a>, not for building silly non-commercial things!
But using Rust, we have to do a <em>lot</em> more work than if we build it with Django or friends.</p>
<p>See, so far, there's no equivalent, and the Rust community leans heavily into the "wire it up yourself" approach.
As <a href="https://www.arewewebyet.org/">Are We Web Yet?</a> says, "[...] you generally have to wire everything up yourself. Expect to put in a little bit of extra set up work to get started."</p>
<p>This undersells it, though.
It's more than a little bit of extra work to get started!
I know because I made a list of things to do to get started.</p>
<p>Rust needs something that <em>does</em> bundle this up for you, so that we can serve all web developers.
Having it would make it a lot easier to make the case to use Rust.
The benefits are there: you get wonderful type system, wonderful performance, and build times that give you back those coffee breaks you used to get while your code compiled.</p>
<h2 id="what-do-we-need">What do we need?</h2>
<p>There is a big pile of stuff that nearly every web app needs, no matter if it's big or small.
Here's a rough list of what seems pretty necessary to me:</p>
<ul>
<li>Routing/handlers: this is pretty obvious, but we have to be able to get an incoming request to some handler for it. Additionally, this routing needs to handle path parameters, ideally with type information, and we'll give bonus points for query parameters, forms, etc.</li>
<li>Templates: we'll need to generate, you know, HTML (and sometimes other content, like JSON or, if you're in the bad times still, XML). Usually I want these to have basic logic, like conditionals, match/switch, and loops.</li>
<li>Static file serving: we'll need to serve some assets, like CSS files. This can be done separately, but having it as part of the same web server is extremely handy for both local development and for small-time deployments that won't handle much traffic.</li>
<li>Logins: You almost always need some way to log in, since apps are usually multi-user or deployed on a public network. This is just annoying to wire up every time! It should be customizable and something you can opt out of, but it should be trivial to have logins from the start.</li>
<li>Permissions: You also need this for systems that have multiple users, since people will have different data they're allowed to access or different roles in the system. Permissions can be complicated but you can make something relatively simple that follows the <code>check(user, object, action)</code> pattern and get really far with it.</li>
<li>Database interface: You're probably going to have to store data for your app, so you want a way to do that. Something that's ORM-like is often nice, but something light is fine. Whatever you do here isn't the <em>only</em> way to interact with the database, but it'll be used for things like logins, permissions, and admin tools, so it's going to be a fundamental piece.</li>
<li>Admin tooling: This is <em>arguably</em> a quality-of-life issue, not a necessity, except that every time you setup your application in a local environment or in production you're going to have to bootstrap it with at least one user or some data. And you'll have to do admin actions sometimes! So I think having this built-in for at least some of the common actions is a necessity for a seamless experience.</li>
<li>WebSockets: I use WebSockets in a lot of my projects. They just let you do really fun things with pushing data out to connected users in a more real-time fashion!</li>
<li>Hot reloading: This is a <em>huge</em> one for developer experience, because you want to have the ability to see changes really quickly. When code or a template change, you need to see that reflected as soon as humanly possible (or as soon as the Rust compiler allows).</li>
</ul>
<p>Then we have a pile of things that are quality-of-life improvements, and I think are necessary for long-term projects but might not be as necessary upfront, so users are less annoyed at implementing it themselves because the cost is spread out.</p>
<ul>
<li>Background tasks: There needs to be a story for these! You're going to have features that have to happen on a schedule, and having a consistent way to do that is a big benefit and makes development easier.</li>
<li>Monitoring/observability: Only the smallest, least-critical systems should skip this. It's really important to have and it will make your life so much easier when you have it in that moment that you desperately need it.</li>
<li>Caching: There are a lot of ways to do this, and all of them make things more complicated and <em>maybe</em> faster? So this is nice to have a story for, but users can also handle it themselves.</li>
<li>Emails and other notifications: It's neat to be able to have password resets and things built-in, and this is probably a necessity if you're going to have logins, so you can have password resets. But other than that feature, it feels like it won't get used <em>that</em> much and isn't a big deal to add in when you need it.</li>
<li>Deployment tooling: Some consistent way to deploy <em>somewhere</em> is really nice, even if it's just an autogenerated Dockerfile that you can use with a host of choice.</li>
<li>CSS/JS bundling: In the time it is, we use JS and CSS everywhere, so you probably want a web tool to be aware of them so they can be included seamlessly. But does it really have to be integrated in? Probably not...</li>
</ul>
<p>So those are the things I'd target in a framework if I were building one!
I might be doing that...</p>
<h2 id="the-existing-ecosystem">The existing ecosystem</h2>
<p>There's quite a bit out there already for building web things in Rust.
None of them quite hit what I want, which is intentional on their part: none of them <em>aspire</em> to be what I'm looking for here.
I love what exists, and I think we're sorely missing what I want here (I don't think I'm alone).</p>
<h2 id="web-frameworks">Web frameworks</h2>
<p>There are really two main groups of web frameworks/libraries right now: the minimalist ones, and the single-page app ones.</p>
<p>The minimalist ones are reminiscent of <a href="https://flask.palletsprojects.com/en/3.0.x/">Flask</a>, Sinatra, and other small web frameworks.
These include the excellent <a href="https://actix.rs/">actix-web</a> and <a href="https://docs.rs/axum/latest/axum/">axum</a>, as well as myriad <a href="https://www.arewewebyet.org/topics/frameworks/">others</a>.
There are so many of these, and they all bring a nice flavor to web development by leveraging Rust's type system!
But they don't give you much besides handlers; none of the extra functionality we want in a full for-lazy-developers framework.</p>
<p>Then there are the single-page app frameworks.
These fill a niche where you can build things with Rust on the backend and frontend, using WebAssembly for the frontend rendering.
These tend to be less mature, but good examples include <a href="https://dioxuslabs.com/learn/0.5/">Dioxus</a>, <a href="https://crates.io/crates/leptos">Leptos</a>, and <a href="https://yew.rs/">Yew</a>.
I used Yew to build a <a href="https://ntietz.com/blog/digital-vigil-for-tdor/">digital vigil</a> last year, and it was enjoyable but I'm not sure I'd want to do it in a "real" production setting.</p>
<p>Each of these is excellent for what it is—but what it is requires a lot of wiring up still.
Most of my projects would work well with the minimalist frameworks, but those require so much wiring up!
So it ends up being a chore to set that up each time that I want to do something.</p>
<h2 id="piles-of-libraries">Piles of libraries!</h2>
<p>The rest of the ecosystem is piles of libraries.
There are lots of template libraries!
There are some libraries for logins, and for permissions.
There are WebSocket libraries!</p>
<p>Often you'll find some projects and examples which integrate a couple of the things you're using, but you won't find something that integrates <em>all</em> the pieces you're using.
I've run into some of the examples being out of date, which is to be expected in a fast-moving ecosystem.</p>
<p>The pile of libraries leaves a lot of friction, though.
It makes getting started, the "just wiring it up" part, very difficult and often an exercise in researching how things work, to understand them in depth enough to <em>do</em> the integration.</p>
<h2 id="what-i-ve-done-before">What I've done before</h2>
<p>The way I've handled this before is basically to pick a base framework (typically actix-web or axum) and then search out all the pieces I want on top of it.
Then I'd wire them up, either all at the beginning or as I need them.</p>
<p>There are starter templates that could help me avoid some of this pain.
They can definitely help you skip some of the initial pain, but you still get all the maintenance burden.
You have to make sure your libraries stay up to date, even when there are breaking changes.
And you will drift from the template, so it's not really feasible to merge changes to it into your project.</p>
<p>For the projects I'm working on, this means that instead of keeping one framework up to date, I have to keep <code>n</code> bespoke frameworks up to date across all my projects!</p>
<p>Eep!</p>
<p>I'd much rather have a single web framework that handles it all, with clean upgrade instructions between versions.
There will be breaking changes sometimes, but this way they can be documented instead of coming about due to changes in the interactions between two components which don't even know they're going to be integrated together.</p>
<h2 id="imagining-the-future-i-want">Imagining the future I want</h2>
<p>In an ideal world, there would be a framework for Rust that gives me all the features I listed above.
And it would also come with excellent documentation, changelogs, thoughtful versioning and handling of breaking changes, and maybe even a great community.
All the things I love about Django, could we have those for a Rust web framework so that we can reap the benefits of Rust without having to go needlessly slowly?</p>
<p>This doesn't exist right now, and I'm not sure if anyone else is working on it.
All paths seem to lead me toward "whoops I guess I'm building a web framework."
I hope someone else builds one, too, so we can have multiple options.</p>
<p>To be honest, "web framework" sounds way too grandiose for what I'm doing, which is simply wiring things together in an opinionated way, using (mostly) existing building blocks<sup><a href="#a-few-of-my-own">1</a></sup>.
Instead of calling it a framework, I'm thinking of it as a <em>web toolkit</em>: a bundle of tools tastefully chosen and arranged to make the artisan highly effective.</p>
<p>My toolkit is called <strong>nicole's web toolkit</strong>, or <strong><code>newt</code></strong>.
It's available in a <a href="https://git.sr.ht/~ntietz/newt">public repository</a>, but it's really not usable (the latest changes aren't even pushed yet).
It's not even usable for <em>me</em> yet—this isn't a launch post, more shipping my design doc (and hoping someone will do my work for me so I don't <em>have</em> to finish <code>newt</code> :D).</p>
<p>The goal for <code>newt</code> is to be able to create a new small web app and start on the actual project in minutes instead of days, bypassing the entire process of wiring things up.
I think the list of must-haves and quality-of-life features above will be a start, but by no means everything we need.
I'm not ready to accept contributions, but I hope to be there at some point.</p>
<p>I think that Rust really needs this, and the whole ecosystem will benefit from it.
A healthy ecosystem will have multiple such toolkits, and I hope to see others develop as well.</p>
<p>* * *</p>
<p>If you want to follow along with mine, though, feel free to subscribe to my RSS feed or newsletter, or follow me on Mastodon.
I'll try to let people know in all those places when the toolkit is ready for people to try out.
Or I'll do a post-mortem on it, if it ends up that I don't get far with it!
Either way, this will be fun.</p>
<hr>

</div><p>
    If this post was enjoyable or useful for you, <strong>please share it!</strong>
    If you have comments, questions, or feedback, you can email <a href="mailto:me@ntietz.com">my personal email</a>.
    To get new posts and support my work, subscribe to the <a href="https://ntietz.com/newsletter/">newsletter</a>. There is also an <a href="https://ntietz.com/atom.xml">RSS feed</a>.
  </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to stop advertisers from tracking your teen across the internet (110 pts)]]></title>
            <link>https://www.eff.org/deeplinks/2024/09/how-stop-advertisers-tracking-your-teen-across-internet</link>
            <guid>41759113</guid>
            <pubDate>Sun, 06 Oct 2024 18:38:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/deeplinks/2024/09/how-stop-advertisers-tracking-your-teen-across-internet">https://www.eff.org/deeplinks/2024/09/how-stop-advertisers-tracking-your-teen-across-internet</a>, See on <a href="https://news.ycombinator.com/item?id=41759113">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article role="article">
  
  
  <div><p><em>This post was written by EFF fellow Miranda McClellan. </em></p>
<p>Teens between the ages of&nbsp; 13 and 17 are being tracked across the internet using identifiers known as Advertising IDs. When children turn 13, they age out of the data protections provided by the Children’s Online Privacy Protection Act (COPPA). Then, they become targets for <a href="https://www.eff.org/deeplinks/2021/07/data-brokers-are-problem">data collection from data brokers</a> that collect their information from social media apps, shopping history, location tracking services, and more. Data brokers then process and sell the data. <a href="https://www.eff.org/deeplinks/2022/05/how-disable-ad-id-tracking-ios-and-android-and-why-you-should-do-it-now">Deleting Advertising IDs</a> off your teen’s devices can increase their privacy and stop advertisers collecting their data.</p>
<h2>What is an Advertising ID?</h2>
<p><a href="https://support.google.com/authorizedbuyers/answer/3221407">Advertising identifiers</a> – Android's Advertising ID (AAID) and Identifier for Advertising (IDFA) on iOS – enable third-party advertising by providing device and activity tracking information to advertisers. The advertising ID is a string of letters and numbers that uniquely identifies your phone, tablet, or other smart device.</p>
<h2>How Teens Are Left Vulnerable</h2>
<p>In most countries, <a href="https://support.google.com/accounts/answer/1350409">children must be over 13 years old to manage their own Google account</a> without a supervisory parent account through Google Family Link. Children over 13 gain the right to manage their own account and app downloads without a supervisory parent account—and they also gain an Advertising ID.</p>
<p>At 13, children transition abruptly between two extremes—from potential helicopter parental surveillance to surveillance advertising that connects their online activity and search history to marketers serving targeted ads.</p>
<p>Thirteen is a historically significant age. In the United States, both <a href="https://about.fb.com/news/2021/07/age-verification/">Facebook</a> and <a href="https://www.facebook.com/help/instagram/966909308115586/">Instagram</a> require users to be at least 13 years old to make an account, <a href="https://www.vice.com/en/article/xgw8k7/congress-shocked-to-discover-10-year-olds-check-the-im-over-18-box-online">though many children pretend to be older</a>. The Children’s Online Privacy Protection Act (COPPA), a federal law, requires companies to obtain “<a href="https://www.ftc.gov/business-guidance/resources/complying-coppa-frequently-asked-questions">verifiable parental consent</a>” before collecting personal information from children under 13 for commercial purposes.</p>
<p>But this means that teens can lose valuable privacy protections even before becoming adults.</p>
<h2>How to Protect Children and Teens from Tracking</h2>
<p>&nbsp;Here are a few steps we recommend that protect children and teens from behavioral tracking and other privacy-invasive advertising techniques:</p>
<ul>
<li>Delete advertising IDs for minors aged 13-17.</li>
<li>Require schools using Chromebooks, Android tablets, or iPads to educate students and parents about deleting advertising IDs off school devices and accounts to preserve student privacy.</li>
<li>Advocate for extended privacy protections for everyone.</li>
</ul>
<h2>How to Delete Advertising IDs</h2>
<p>&nbsp;Advertising IDs track devices and activity from connected accounts. Both Android and iOS users can reset or delete their advertising IDs from the device. Removing the advertising ID removes a key component advertisers use to identify audiences for targeted ad delivery. While users will still see ads after resetting or deleting their advertising ID, the ads will be severed from previous online behaviors and provide less personally targeted ads.</p>
<p>Follow these instructions, updated from a previous <a href="https://www.eff.org/deeplinks/2022/05/how-disable-ad-id-tracking-ios-and-android-and-why-you-should-do-it-now">EFF blog post</a>:</p>
<h3><strong>On Android </strong></h3>
<p><a href="https://support.google.com/googleplay/android-developer/answer/6048248?hl=en">With the release of Android 12</a>, Google began allowing users to delete their ad ID permanently. On devices that have this feature enabled, you can open the <strong>Settings </strong>app and navigate to <strong></strong><em><strong>Security &amp; Privacy </strong></em>&gt; <em><strong>Privacy </strong></em>&gt; <em><strong>Ads</strong></em>. Tap “<strong>Delete advertising ID</strong>,” then tap it again on the next page to confirm. This will prevent any app on your phone from accessing it in the future.</p>
<p>The Android opt out should be available to most users on Android 12, but may not available on older versions. If you don't see an option to "delete" your ad ID, you can use the older version of Android's privacy controls to reset it and ask apps not to track you.</p>
<h3><strong>On iOS </strong></h3>
<p>Apple requires apps to <a href="https://www.eff.org/deeplinks/2021/04/apples-apptrackingtransparency-upending-mobile-phone-tracking">ask permission</a> before they can access your IDFA. When you install a new app, it may ask you for permission to track you.</p>
<p>Select “<strong>Ask App Not to Track</strong>” to deny it IDFA access.</p>
<p>To see which apps you have previously granted access to, go to <em><strong>Settings </strong></em>&gt;&nbsp;<em><strong>Privacy &amp; Security</strong></em><strong> </strong>&gt; <em><strong>Tracking</strong></em>.</p>
<p>In this menu, you can disable tracking for individual apps that have previously received permission. Only apps that have permission to track you will be able to access your IDFA.</p>
<p>You can set the “<strong>Allow apps to Request to Track</strong>” switch to the “<strong>off</strong>” position (the slider is to the left and the background is gray). This will prevent apps from asking to track in the future. If you have granted apps permission to track you in the past, this will prompt you to ask <em>those</em> apps to stop tracking as well. You also have the option to grant or revoke tracking access on a per-app basis.</p>
<p>Apple has its own targeted advertising system, separate from the third-party tracking it enables with IDFA. To disable it, navigate to <em><strong>Settings </strong></em>&gt; <em><strong>Privacy </strong></em>&gt; <strong><em>Apple</em> Advertising </strong>and set the “<strong>Personalized Ads</strong>” switch to the “<strong>off</strong>” position to disable Apple’s ad targeting.</p>
<p><em>Miranda McClellan served as a summer fellow at EFF on the Public Interest Technology team. Miranda has a B.S. and M.Eng. in Computer Science from MIT. Before joining EFF, Miranda completed a Fulbright research fellowship in Spain to apply machine learning to 5G networks, worked as a data scientist at Microsoft where she built machine learning models to detect malware, and was a fellow at the Internet Society. In her free time, Miranda enjoys running, hiking, and crochet.</em></p>
<p><em>At EFF, Miranda conducted research focused on understanding the data broker ecosystem and enhancing children’s privacy. She received funding from the National Science Policy Network.</em></p>

</div>

          </article>
    </div><div>
          <h2>Join EFF Lists</h2>
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AVX Bitwise ternary logic instruction busted (254 pts)]]></title>
            <link>https://arnaud-carre.github.io/2024-10-06-vpternlogd/</link>
            <guid>41759112</guid>
            <pubDate>Sun, 06 Oct 2024 18:38:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arnaud-carre.github.io/2024-10-06-vpternlogd/">https://arnaud-carre.github.io/2024-10-06-vpternlogd/</a>, See on <a href="https://news.ycombinator.com/item?id=41759112">Hacker News</a></p>
<div id="readability-page-1" class="page"><article role="main">
        <h2 id="audience">Audience</h2>

<p>This post is for any enthusiastic SIMD CPU programmer curious about weird AVX instructions. But also for any veteran Amiga programmer who never quite figured out how to calculate that cursed “minterm” blitter value!</p>



<p>The idea for this post came while watching a great talk by Tom Forsyth [1] about the design of the AVX-512 ISA (Instruction Set Architecture). As I quickly skimmed through the slides, I paused at slide 41, where an obscure instruction caught my eye: <strong>vpternlogd</strong> (yeah, Intel engineers aren’t known for their naming skills!).</p>

<p>This instruction is a bitwise ternary logic operation, meaning it can perform any bitwise Boolean logic using three input sources. For example, if we label the inputs A, B, and C, you can create any complex logic operations like:</p>

<div><pre><code><span>(</span><span>NOT</span> <span>A</span><span>)</span> <span>OR</span> <span>((</span><span>NOT</span> <span>B</span><span>)</span> <span>XOR</span> <span>(</span><span>C</span> <span>AND</span> <span>A</span><span>))</span>
</code></pre></div>

<p>Or any other boolean logic combination you need—all in a single instruction!</p>

<p>What’s even cooler is that the inputs can be 512-bit registers, allowing this complex logic to be performed on 512 bits at once.</p>

<p>The author explained that adding specific instructions for every possible user needs (like “foo_and_a_or_not_b”) would have been overwhelming. It would require tons of new mnemonics, documentation, and testing. Instead, they opted for a smarter (and lazier?) solution: a single, flexible instruction:</p>

<div><pre><code><span>VPTERNLOGD</span> <span>r0</span><span>,</span> <span>r1</span><span>,</span> <span>r3</span><span>,</span> <span>#</span><span>imm8</span>
</code></pre></div>

<p>The instruction takes 3 registers as input, and an 8-bit immediate value that defines the exact bitwise operations to do. Unfortunately, most documentation just says, “The immediate value determines the specific binary function,” which isn’t very helpful.</p>

<p>As soon as I saw this 8-bit immediate value, it reminded me of an old friend from 1985: the Amiga blitter chipset!</p>

<h2 id="amiga-blitter-custom-chip">Amiga blitter custom chip</h2>

<p>In the 1980s, it was common for computers to have custom chips for handling graphics. However, these chips didn’t handle complex tasks like triangle rasterization or programmable shaders. At that time, “graphics” primarily meant working with bitmaps. For example, the Commodore Amiga 500 had a blitter chip. Its main function was to move bitmap graphics from one location to another while applying logical operations. The Amiga’s blitter could handle up to three bitmap sources at once and perform logical operations between them. To specify which operation to use, you needed to set an 8-bit value in the chip, known as the “minterm.”</p>

<p>Three bitmap sources and an 8-bit value to control logical combinations! <strong>Doesn’t that sound like a primitive version of the modern AVX vpternlogd instruction?</strong></p>

<p>Interestingly, even many skilled Amiga programmers didn’t know how to calculate the minterm value. Most just reused common values from other demos. For instance, to clear a buffer, they would use 0x00. To configure the blitter to draw masked sprites, they’d use 0xE2. But for any other weird custom function, most of us were lost back in time.</p>

<p>The Amiga blitter user manual didn’t help much either. The “Amiga Hardware Reference Manual” from 1989 tried to explain minterm calculation using confusing symbols, which frustrated many young demo makers at the time.</p>

<p>Here’s what my teenage self would have done with a red marker if I had the official documentation in hand :)</p>

<p><img src="https://arnaud-carre.github.io/assets/img/minterm/crap.jpg" alt="crap"></p>

<h2 id="easy-way-to-calculate-minterm-value">Easy way to calculate minterm value</h2>

<p>Now, let me show you an easy way to calculate the minterm value. Even if you’re not planning to program the Amiga blitter anytime soon, you might find this useful for working with modern AVX ternary logic instructions—they work exactly the same way!</p>

<p>A few years ago, I realized that this 8-bit value doesn’t have to be understood as just a set of logical operators. Instead, it’s fundamentally <strong>just a lookup table</strong>.</p>

<p>Let’s take an example: suppose you want the result to be 1 when exactly two of the three sources are 1.</p>

<p>First, list the 8 possible values of three input bits (A, B, and C), and add a fourth empty column for the result.</p>

<table>
  <thead>
    <tr>
      <th>A</th>
      <th>B</th>
      <th>C</th>
      <th>what I want</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>?</td>
    </tr>
    <tr>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>?</td>
    </tr>
    <tr>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>?</td>
    </tr>
    <tr>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>?</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>?</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>?</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>?</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>?</td>
    </tr>
  </tbody>
</table>

<p>And now just fill the fourth column with the exact result you want your function to do. For our specific example, we want 1 when exactly <strong>two</strong> sources are 1. Let’s fill the fourth column:</p>

<p><img src="https://arnaud-carre.github.io/assets/img/minterm/table.jpg" alt="truth"></p>

<p>And now the magic: <strong>read the 8 bits of the fourth column, from bottom to up</strong>: 01101000, or 0x68. Function 0x68 will set 1 as a result if exactly 2 inputs are 1.</p>

<p><strong>And you also got the mysterious #imm8 value of the modern vpternlogd instruction!</strong></p>

<p>You can use the exact same method to get the #imm8 value for any exotic or complex logical function between 3 sources you need.</p>

<p>I wish my younger self had known this method back when I was scratching my head over blitter minterms!</p>

<h2 id="a-funny-coincidence">A funny coincidence</h2>

<p>One of the very common Amiga minterm values is 0xE2. This value is often used to render masked 2d sprites. With A containing the sprite bitmap, B containing the sprite “mask”, and C being the background.</p>

<p>An easy way to calculate the minterm for a masked sprite is to think of it in simple programming terms: <strong>when the mask pixel (B) is set, the result is sprite (A). If the mask pixel is not set, the result is background (C)</strong></p>

<table>
  <thead>
    <tr>
      <th>A</th>
      <th>B</th>
      <th>C</th>
      <th>what I want</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0 (B is not set, use C)</td>
    </tr>
    <tr>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1 (B is not set, use C)</td>
    </tr>
    <tr>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0 (B is set, use A)</td>
    </tr>
    <tr>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0 (B is set, use A)</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0 (B not set, use C)</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1 (B not set, use C)</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1 (B is set, use A)</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1 (B is set, use A)</td>
    </tr>
  </tbody>
</table>

<p>Read the 8 bits bottom to up: 11100010, or 0xE2</p>

<p>So 0xE2 is a very common minterm value in Amiga demoscene culture. <strong>And now the funny part of this post:</strong></p>

<p>The official Intel documentation [2] about vpternlogd has an example of a #imm8 value. And over the <strong>256 possible user functions</strong> they could have chosen, I’ll let you discover what they chose :)</p>

<p><img src="https://arnaud-carre.github.io/assets/img/minterm/intel_doc.jpg" alt="doc"></p>

<h2 id="conclusion">Conclusion</h2>

<p>Is there any Amiga fanboy in the Intel documentation example team? A bit of retro influence never hurts! :)</p>

<h2 id="links">Links</h2>

<p><a href="https://tomforsyth1000.github.io/papers/LRBNI%20origins%20v4%20full%20fat.pdf">[1] Tom Forsyth presentation about AVX-512 ISA design</a></p>

<p><a href="https://software.intel.com/en-us/download/intel-64-and-ia-32-architectures-sdm-combined-volumes-1-2a-2b-2c-2d-3a-3b-3c-3d-and-4">[2] Intel 64 and IA-32 Architectures Software Developer’s Manual</a></p>

<p><a href="https://twitter.com/leonard_coder" target="_blank">Follow me on Twitter</a></p>


      </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Starlink direct-to-cell enabled for hurricane helene emergency messaging (126 pts)]]></title>
            <link>https://twitter.com/spacex/status/1842988427777605683</link>
            <guid>41759005</guid>
            <pubDate>Sun, 06 Oct 2024 18:27:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/spacex/status/1842988427777605683">https://twitter.com/spacex/status/1842988427777605683</a>, See on <a href="https://news.ycombinator.com/item?id=41759005">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Gleam Is Pragmatic (204 pts)]]></title>
            <link>https://blog.drewolson.org/gleam-is-pragmatic/</link>
            <guid>41758915</guid>
            <pubDate>Sun, 06 Oct 2024 18:15:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.drewolson.org/gleam-is-pragmatic/">https://blog.drewolson.org/gleam-is-pragmatic/</a>, See on <a href="https://news.ycombinator.com/item?id=41758915">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<article>
   
  <div><p>I’ve spent the past several years working with functional programming languages
in my free time – primarily Haskell and OCaml. I love both languages but also
find aspects of each frustrating.</p>
<p>Haskell is terse and elegant with type classes providing a powerful mechanism
for ad-hoc polymorphism. However, it can also be confusingly implicit and I
personally find lazy evaluation to have more downsides than upsides.</p>
<p>OCaml is explicit and powerful with a best-in-class module system. However, I
believe it is often exhaustingly explicit, especially when dealing with custom
data types in generic containers.</p>
<p>Over the past few months I’ve been experimenting with the
<a href="https://gleam.run/">Gleam</a> programming language and I’ve been very impressed. I
believe it makes three very interesting design choices that provide the best of
Haskell and OCaml, with relatively few downsides.</p>
<ol>
<li><code>use</code> expressions, which are syntactic sugar for
callback-style APIs</li>
<li>Structural equality for all types, including user-defined types</li>
<li>No ad-hoc polymorphism</li>
</ol>
<p>I’ll explore the implications of these design decisions in this post, comparing
Gleam with Haskell and OCaml. I’ll be using only the standard libraries of each
language.</p>
<h2 id="brief-gleam-overview">Brief Gleam Overview</h2>
<p>Gleam is a strongly-typed functional programming language. It targets both the
BEAM (Erlang’s virtual machine) and JavaScript. It is impure (allowing untracked
side effects) and all data structures provided by the standard library are
immutable.</p>
<p>Here’s an example Gleam program that adds up the numbers in a list and prints
the result:</p>
<div><pre tabindex="0"><code data-lang="rust"><span><span>import gleam<span>/</span>int
</span></span><span><span>import gleam<span>/</span>io
</span></span><span><span>import gleam<span>/</span>list
</span></span><span><span>
</span></span><span><span><span>fn</span> <span>add</span>(sum: <span>Int</span>, n: <span>Int</span>) -&gt; <span>Int</span> {
</span></span><span><span>  sum <span>+</span> n
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>pub</span> <span>fn</span> <span>main</span>() {
</span></span><span><span>  <span>let</span> items <span>=</span> [<span>1</span>, <span>2</span>, <span>3</span>, <span>4</span>, <span>5</span>]
</span></span><span><span>
</span></span><span><span>  items
</span></span><span><span>  <span>|&gt;</span> list.fold(<span>0</span>, add)
</span></span><span><span>  <span>|&gt;</span> int.to_string
</span></span><span><span>  <span>|&gt;</span> io.println
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>// 15
</span></span></span></code></pre></div><p>This simple program demonstrates some important parts of Gleam’s design:</p>
<ul>
<li>Optional type annotations (encouraged on top-level functions) with type
inference</li>
<li>First-class functions</li>
<li>Explicit imports</li>
<li>Visibility modifiers for functions (<code>pub</code> for public, default is private)</li>
<li>List literal syntax</li>
<li>The <code>|&gt;</code> operator for function pipelines (<code>a |&gt; f(b)</code> is equivalent to <code>f(a, b)</code>)</li>
</ul>
<p>Here’s a slightly more complicated program:</p>
<div><pre tabindex="0"><code data-lang="rust"><span><span>import gleam<span>/</span>int
</span></span><span><span>import gleam<span>/</span>io
</span></span><span><span>import gleam<span>/</span>list
</span></span><span><span>import gleam<span>/</span>string
</span></span><span><span>
</span></span><span><span><span>pub</span> <span>type</span> <span>Color</span> {
</span></span><span><span>  Brown
</span></span><span><span>  White
</span></span><span><span>  Other
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>pub</span> <span>type</span> <span>Pet</span> {
</span></span><span><span>  Dog(color: <span>Color</span>, name: String)
</span></span><span><span>  Cat(age: <span>Int</span>, name: String)
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>fn</span> <span>color_to_string</span>(color: <span>Color</span>) -&gt; String {
</span></span><span><span>  case color {
</span></span><span><span>    Brown -&gt; <span>"brown"</span>
</span></span><span><span>    White -&gt; <span>"white"</span>
</span></span><span><span>    Other -&gt; <span>"other"</span>
</span></span><span><span>  }
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>fn</span> <span>pet_to_string</span>(pet: <span>Pet</span>) -&gt; String {
</span></span><span><span>  case pet {
</span></span><span><span>    Dog(color, name) -&gt; <span>name</span> <span>&lt;&gt;</span> <span>" is "</span> <span>&lt;&gt;</span> color_to_string(color)
</span></span><span><span>    Cat(age, name) -&gt; <span>name</span> <span>&lt;&gt;</span> <span>" is "</span> <span>&lt;&gt;</span> int.to_string(age)
</span></span><span><span>  }
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>pub</span> <span>fn</span> <span>main</span>() {
</span></span><span><span>  <span>let</span> pets <span>=</span> [Dog(color: <span>Brown</span>, name: <span>"Hobbes"</span>), Cat(age: <span>5</span>, name: <span>"Garfield"</span>)]
</span></span><span><span>
</span></span><span><span>  pets
</span></span><span><span>  <span>|&gt;</span> list.map(pet_to_string)
</span></span><span><span>  <span>|&gt;</span> string.join(<span>", "</span>)
</span></span><span><span>  <span>|&gt;</span> io.println
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>// Hobbes is brown, Garfield is 5
</span></span></span></code></pre></div><p>In this example, we see even more cool features:</p>
<ul>
<li>Custom types, with multiple constructors</li>
<li>Exhaustive pattern matching</li>
<li>Polymorphic functions (<code>list.map</code> works with any type)</li>
<li>A string concatenation operator (<code>&lt;&gt;</code>)</li>
</ul>
<p>That’s enough Gleam to get into the rest of the post.</p>
<h2 id="monadic-style-apis">Monadic-style APIs</h2>
<p>I won’t fall into the trap of trying to define Monads in this post. Instead,
let’s talk about monadic-style APIs – that is, APIs that allow you to do a
bunch of things one after another, with the ability to use the result of a
previous computation in the next computation, and also allows some logic to
happen between steps.</p>
<p>A subset of these of APIs are often called <a href="https://fsharpforfunandprofit.com/rop/">“railway oriented
programming”</a> – you do a bunch of
steps, each which may fail, and if they all succeed you do something with the
results. We’ll focus on this use case for the rest of this section.</p>
<p>Let’s built a program that fetches several values from a key/value data
structure and, if all the values are present, adds them up. The second value we
fetch will depend on the first value we fetch.</p>
<p>Here’s an example in Haskell:</p>
<div><pre tabindex="0"><code data-lang="haskell"><span><span><span>module</span> Main (<span>main</span>) <span>where</span>
</span></span><span><span>
</span></span><span><span><span>import</span> Data.Map (<span>Map</span>)
</span></span><span><span><span>import</span> Data.Map qualified as <span>Map</span>
</span></span><span><span>
</span></span><span><span><span>doStuff</span> <span>::</span> <span>Map</span> <span>String</span> <span>Int</span> <span>-&gt;</span> <span>Maybe</span> <span>Int</span>
</span></span><span><span><span>doStuff</span> items <span>=</span> <span>do</span>
</span></span><span><span>  a <span>&lt;-</span> <span>Map</span><span>.</span>lookup <span>"a"</span> items
</span></span><span><span>  b <span>&lt;-</span> <span>Map</span><span>.</span>lookup (show a) items
</span></span><span><span>
</span></span><span><span>  return (a <span>+</span> b)
</span></span><span><span>
</span></span><span><span><span>main</span> <span>::</span> <span>IO</span> ()
</span></span><span><span><span>main</span> <span>=</span> <span>do</span>
</span></span><span><span>  <span>let</span> items <span>=</span> <span>Map</span><span>.</span>fromList [(<span>"a"</span>, <span>1</span>), (<span>"1"</span>, <span>2</span>)]
</span></span><span><span>  <span>let</span> result <span>=</span> doStuff items
</span></span><span><span>
</span></span><span><span>  putStrLn (show result)
</span></span><span><span>
</span></span><span><span><span>-- Just 3</span>
</span></span></code></pre></div><p>The <code>Map.lookup</code> function has the following type:</p>
<div><pre tabindex="0"><code data-lang="haskell"><span><span><span>ghci</span><span>&gt;</span> <span>:</span>t <span>Map</span><span>.</span>lookup
</span></span><span><span><span>Map</span><span>.</span>lookup <span>::</span> <span>Ord</span> k <span>=&gt;</span> k <span>-&gt;</span> <span>Map</span> k a <span>-&gt;</span> <span>Maybe</span> a
</span></span></code></pre></div><p>That is, given an key that is ordered and a map, it possibly returns a value.
The <code>Maybe</code> type represents a value that may or may not be present in Haskell.</p>
<p>There’s a few important things to notice about this program:</p>
<ul>
<li>Haskell’s <code>Map</code> type “just works” with <code>String</code> keys because <code>String</code> is
ordered.</li>
<li>Haskell’s <code>Maybe</code> type “just works” with the do-notation. If a function
returns <code>Nothing</code>, the do-notation short-circuits and the whole function
returns <code>Nothing</code>. However, it’s not obvious to the programmer <em>how</em> this
works or <em>what function specifically</em> is being called.</li>
<li>The <code>return</code> function “just works” to return a <code>Just</code> value. Again, this is
non-obvious to the programmer.</li>
<li>The <code>show</code> function automatically knows how to turn a <code>Maybe Int</code> into a
<code>String</code>.</li>
</ul>
<p>Here’s a roughly equivalent example in OCaml:</p>
<div><pre tabindex="0"><code data-lang="ocaml"><span><span><span>module</span> <span>StringMap</span> <span>=</span> Map.<span>Make</span> <span>(</span><span>String</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>module</span> <span>Syntax</span> <span>=</span> <span>struct</span>
</span></span><span><span>  <span>let</span> <span>(</span> <span>let</span><span>*</span> <span>)</span> <span>=</span> Option.bind
</span></span><span><span>  <span>let</span> return x <span>=</span> <span>Some</span> x
</span></span><span><span><span>end</span>
</span></span><span><span>
</span></span><span><span><span>let</span> do_stuff items <span>=</span>
</span></span><span><span>  <span>let</span> <span>open</span> <span>Syntax</span> <span>in</span>
</span></span><span><span>  <span>let</span><span>*</span> a <span>=</span> StringMap.find_opt <span>"a"</span> items <span>in</span>
</span></span><span><span>  <span>let</span><span>*</span> b <span>=</span> StringMap.find_opt <span>(</span>string_of_int a<span>)</span> items <span>in</span>
</span></span><span><span>  return <span>(</span>a <span>+</span> b<span>)</span>
</span></span><span><span><span>;;</span>
</span></span><span><span>
</span></span><span><span><span>let</span> option_to_string <span>=</span> <span>function</span>
</span></span><span><span>  <span>|</span> <span>None</span> <span>-&gt;</span> <span>"None"</span>
</span></span><span><span>  <span>|</span> <span>Some</span> n <span>-&gt;</span> Printf.sprintf <span>"Some(%i)"</span> n
</span></span><span><span><span>;;</span>
</span></span><span><span>
</span></span><span><span><span>let</span> () <span>=</span>
</span></span><span><span>  <span>let</span> items <span>=</span> StringMap.of_list <span>[</span> <span>"a"</span><span>,</span> 1<span>;</span> <span>"1"</span><span>,</span> 2 <span>]</span> <span>in</span>
</span></span><span><span>  items <span>|&gt;</span> do_stuff <span>|&gt;</span> option_to_string <span>|&gt;</span> print_endline
</span></span><span><span><span>;;</span>
</span></span><span><span>
</span></span><span><span><span>(* Some(3) *)</span>
</span></span></code></pre></div><p>The <code>StringMap.find_opt</code> function has the following type:</p>
<div><pre tabindex="0"><code data-lang="ocaml"><span><span><span>val</span> find_opt <span>:</span> key <span>-&gt;</span> <span>'</span>a t <span>-&gt;</span> <span>'</span>a option
</span></span></code></pre></div><p>Some important things to notice here:</p>
<ul>
<li>You must explicitly create a <code>Map</code> with your chosen key type (in this case
<code>String</code>).</li>
<li>You must explicit create a module that represents the <code>let*</code> and <code>return</code>
functions for the <code>Option</code> type.</li>
<li>This <code>Syntax</code> module must be locally in-scope to use the appropriate
implementation of <code>let*</code> in the body of <code>do_stuff</code>. This means an explicit
local <code>open</code> of the <code>Syntax</code> module.</li>
<li>It’s very obvious to the programmer what implementation of <code>let*</code> and <code>return</code>
are being used in a given context.</li>
<li>You need to manually tell OCaml how to print a <code>int option</code>.</li>
</ul>
<p>Now, let’s look at two examples of the same program in Gleam. Here’s the first
one:</p>
<div><pre tabindex="0"><code data-lang="rust"><span><span>import gleam<span>/</span>dict.{<span>type</span> <span>Dict</span>}
</span></span><span><span>import gleam<span>/</span>int
</span></span><span><span>import gleam<span>/</span>io
</span></span><span><span>import gleam<span>/</span>result
</span></span><span><span>import gleam<span>/</span>string
</span></span><span><span>
</span></span><span><span><span>fn</span> <span>do_stuff</span>(items: <span>Dict</span>(String, Int)) -&gt; Result(Int, Nil) {
</span></span><span><span>  items
</span></span><span><span>  <span>|&gt;</span> dict.get(<span>"a"</span>)
</span></span><span><span>  <span>|&gt;</span> result.<span>try</span>(<span>fn</span>(a) {
</span></span><span><span>    items
</span></span><span><span>    <span>|&gt;</span> dict.get(int.to_string(a))
</span></span><span><span>    <span>|&gt;</span> result.<span>try</span>(<span>fn</span>(b) { Ok(a <span>+</span> b) })
</span></span><span><span>  })
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>pub</span> <span>fn</span> <span>main</span>() {
</span></span><span><span>  <span>let</span> items <span>=</span> dict.from_list([#(<span>"a"</span>, <span>1</span>), #(<span>"1"</span>, <span>2</span>)])
</span></span><span><span>
</span></span><span><span>  items
</span></span><span><span>  <span>|&gt;</span> do_stuff
</span></span><span><span>  <span>|&gt;</span> string.inspect
</span></span><span><span>  <span>|&gt;</span> io.println
</span></span><span><span>}
</span></span><span><span><span>// Ok(3)
</span></span></span></code></pre></div><p>There are two important types in the above program. First, <code>dict.get</code>:</p>
<div><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span> <span>fn</span> <span>get</span>(from: <span>Dict</span>(a, b), get: <span>a</span>) -&gt; Result(b, Nil)
</span></span></code></pre></div><p>And second, <code>result.try</code>:</p>
<div><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span> <span>fn</span> <span>try</span>(
</span></span><span><span>  result: Result(a, b),
</span></span><span><span>  fun: <span>fn</span>(a) -&gt; Result(c, b),
</span></span><span><span>) -&gt; Result(c, b)
</span></span></code></pre></div><p>Things to note:</p>
<ul>
<li><code>Dict</code> “just works” with <code>String</code> keys.</li>
<li>All function calls are explicit, along with their namespaces.</li>
<li>The nesting of <code>result.try</code> calls is a bit of a pain.</li>
<li><code>string.inspect</code> can automatically turn a <code>Result(Int, Nil)</code> into a <code>String</code>.</li>
</ul>
<p>This example has a bunch of the good features from Haskell and OCaml – generic
collections “just work”, it’s obvious what functions are being called, and
string conversion is convenient via <code>string.inspect</code>.</p>
<p>However, the “callback hell” pyramid of doom is not great. This is where <code>use</code>
comes to the rescue!</p>
<p>In Gleam, <code>use</code> is a mechanism for rewriting callback-based APIs in a flat
style. The following two blocks of code are equivalent:</p>
<div><pre tabindex="0"><code data-lang="rust"><span><span><span>// standard callback
</span></span></span><span><span><span></span>f(a, <span>fn</span>(b) { g(b) })
</span></span><span><span>
</span></span><span><span><span>// with use
</span></span></span><span><span><span></span><span>use</span> b <span>&lt;-</span> f(a)
</span></span><span><span>g(b)
</span></span></code></pre></div><p>We can employ <code>use</code> to remove the nesting from our previous example:</p>
<div><pre tabindex="0"><code data-lang="rust"><span><span>import gleam<span>/</span>dict.{<span>type</span> <span>Dict</span>}
</span></span><span><span>import gleam<span>/</span>int
</span></span><span><span>import gleam<span>/</span>io
</span></span><span><span>import gleam<span>/</span>result
</span></span><span><span>import gleam<span>/</span>string
</span></span><span><span>
</span></span><span><span><span>fn</span> <span>do_stuff</span>(items: <span>Dict</span>(String, Int)) -&gt; Result(Int, Nil) {
</span></span><span><span>  <span>use</span> a <span>&lt;-</span> result.<span>try</span>(dict.get(items, <span>"a"</span>))
</span></span><span><span>  <span>use</span> b <span>&lt;-</span> result.<span>try</span>(dict.get(items, int.to_string(a)))
</span></span><span><span>
</span></span><span><span>  Ok(a <span>+</span> b)
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>pub</span> <span>fn</span> <span>main</span>() {
</span></span><span><span>  <span>let</span> items <span>=</span> dict.from_list([#(<span>"a"</span>, <span>1</span>), #(<span>"1"</span>, <span>2</span>)])
</span></span><span><span>
</span></span><span><span>  items
</span></span><span><span>  <span>|&gt;</span> do_stuff
</span></span><span><span>  <span>|&gt;</span> string.inspect
</span></span><span><span>  <span>|&gt;</span> io.println
</span></span><span><span>}
</span></span><span><span><span>// Ok(3)
</span></span></span></code></pre></div><p>This example retains all of the previous advantages <em>and</em> removes the nested
callbacks. Very nice.</p>
<h2 id="custom-types">Custom Types</h2>
<p>Let’s say we want to rewrite our above examples, but this time our keys will be
a custom type rather than <code>String</code>s. Haskell first:</p>
<div><pre tabindex="0"><code data-lang="haskell"><span><span><span>module</span> Main (<span>main</span>) <span>where</span>
</span></span><span><span>
</span></span><span><span><span>import</span> Data.Map (<span>Map</span>)
</span></span><span><span><span>import</span> Data.Map qualified as <span>Map</span>
</span></span><span><span>
</span></span><span><span><span>data</span> <span>Person</span> <span>=</span> <span>Person</span>
</span></span><span><span>  { name <span>::</span> <span>String</span>,
</span></span><span><span>    age <span>::</span> <span>Int</span>
</span></span><span><span>  }
</span></span><span><span>  <span>deriving</span> (<span>Eq</span>, <span>Ord</span>)
</span></span><span><span>
</span></span><span><span><span>drew</span> <span>::</span> <span>Person</span>
</span></span><span><span><span>drew</span> <span>=</span> <span>Person</span> {name <span>=</span> <span>"Drew"</span>, age <span>=</span> <span>42</span>}
</span></span><span><span>
</span></span><span><span><span>jane</span> <span>::</span> <span>Person</span>
</span></span><span><span><span>jane</span> <span>=</span> <span>Person</span> {name <span>=</span> <span>"Jane"</span>, age <span>=</span> <span>61</span>}
</span></span><span><span>
</span></span><span><span><span>doStuff</span> <span>::</span> <span>Map</span> <span>Person</span> <span>Int</span> <span>-&gt;</span> <span>Maybe</span> <span>Int</span>
</span></span><span><span><span>doStuff</span> items <span>=</span> <span>do</span>
</span></span><span><span>  a <span>&lt;-</span> <span>Map</span><span>.</span>lookup drew items
</span></span><span><span>  b <span>&lt;-</span> <span>Map</span><span>.</span>lookup jane items
</span></span><span><span>
</span></span><span><span>  return (a <span>+</span> b)
</span></span><span><span>
</span></span><span><span><span>main</span> <span>::</span> <span>IO</span> ()
</span></span><span><span><span>main</span> <span>=</span> <span>do</span>
</span></span><span><span>  <span>let</span> items <span>=</span> <span>Map</span><span>.</span>fromList [(drew, <span>1</span>), (jane, <span>2</span>)]
</span></span><span><span>  <span>let</span> result <span>=</span> doStuff items
</span></span><span><span>
</span></span><span><span>  putStrLn (show result)
</span></span><span><span>
</span></span><span><span><span>-- Just 3</span>
</span></span></code></pre></div><p>This works pretty much the same as our previous example, except we need to tell
Haskell that our <code>Person</code> can be compared for equality and is ordered (<code>deriving (Eq, Ord)</code>).</p>
<p>Next, here’s OCaml:</p>
<div><pre tabindex="0"><code data-lang="ocaml"><span><span><span>module</span> <span>Person</span> <span>=</span> <span>struct</span>
</span></span><span><span>  <span>type</span> t <span>=</span>
</span></span><span><span>    <span>{</span> name <span>:</span> <span>string</span>
</span></span><span><span>    <span>;</span> age <span>:</span> <span>int</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span>
</span></span><span><span>  <span>let</span> compare p1 p2 <span>=</span>
</span></span><span><span>    <span>match</span> compare p1<span>.</span>name p2<span>.</span>name <span>with</span>
</span></span><span><span>    <span>|</span> 0 <span>-&gt;</span> compare p1<span>.</span>age p2<span>.</span>age
</span></span><span><span>    <span>|</span> other <span>-&gt;</span> other
</span></span><span><span>  <span>;;</span>
</span></span><span><span><span>end</span>
</span></span><span><span>
</span></span><span><span><span>let</span> drew <span>=</span> Person.<span>{</span> name <span>=</span> <span>"Drew"</span><span>;</span> age <span>=</span> 42 <span>}</span>
</span></span><span><span><span>let</span> jane <span>=</span> Person.<span>{</span> name <span>=</span> <span>"Jane"</span><span>;</span> age <span>=</span> 61 <span>}</span>
</span></span><span><span>
</span></span><span><span><span>module</span> <span>PersonMap</span> <span>=</span> Map.<span>Make</span> <span>(</span><span>Person</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>module</span> <span>Syntax</span> <span>=</span> <span>struct</span>
</span></span><span><span>  <span>let</span> <span>(</span> <span>let</span><span>*</span> <span>)</span> <span>=</span> Option.bind
</span></span><span><span>  <span>let</span> return x <span>=</span> <span>Some</span> x
</span></span><span><span><span>end</span>
</span></span><span><span>
</span></span><span><span><span>let</span> do_stuff items <span>=</span>
</span></span><span><span>  <span>let</span> <span>open</span> <span>Syntax</span> <span>in</span>
</span></span><span><span>  <span>let</span><span>*</span> a <span>=</span> PersonMap.find_opt drew items <span>in</span>
</span></span><span><span>  <span>let</span><span>*</span> b <span>=</span> PersonMap.find_opt jane items <span>in</span>
</span></span><span><span>  return <span>(</span>a <span>+</span> b<span>)</span>
</span></span><span><span><span>;;</span>
</span></span><span><span>
</span></span><span><span><span>let</span> option_to_string <span>=</span> <span>function</span>
</span></span><span><span>  <span>|</span> <span>None</span> <span>-&gt;</span> <span>"None"</span>
</span></span><span><span>  <span>|</span> <span>Some</span> n <span>-&gt;</span> Printf.sprintf <span>"Some(%i)"</span> n
</span></span><span><span><span>;;</span>
</span></span><span><span>
</span></span><span><span><span>let</span> () <span>=</span>
</span></span><span><span>  <span>let</span> items <span>=</span> PersonMap.of_list <span>[</span> drew<span>,</span> 1<span>;</span> jane<span>,</span> 3 <span>]</span> <span>in</span>
</span></span><span><span>  items <span>|&gt;</span> do_stuff <span>|&gt;</span> option_to_string <span>|&gt;</span> print_endline
</span></span><span><span><span>;;</span>
</span></span><span><span>
</span></span><span><span><span>(* Some(3) *)</span>
</span></span></code></pre></div><p>This has quite a bit more boilerplate than our previous OCaml example. When we
want a custom type to be the key of a <code>Map</code>, we must manually implement the
<code>compare</code> function (as noted earlier, I’m sticking to standard libraries, so no
<code>ppx_deriving</code>).</p>
<p>Finally, here’s Gleam:</p>
<div><pre tabindex="0"><code data-lang="rust"><span><span>import gleam<span>/</span>dict.{<span>type</span> <span>Dict</span>}
</span></span><span><span>import gleam<span>/</span>io
</span></span><span><span>import gleam<span>/</span>result
</span></span><span><span>import gleam<span>/</span>string
</span></span><span><span>
</span></span><span><span><span>type</span> <span>Person</span> {
</span></span><span><span>  Person(name: String, age: <span>Int</span>)
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>const</span> drew <span>=</span> Person(name: <span>"Drew"</span>, age: <span>42</span>)
</span></span><span><span>
</span></span><span><span><span>const</span> jane <span>=</span> Person(name: <span>"Jane"</span>, age: <span>61</span>)
</span></span><span><span>
</span></span><span><span><span>fn</span> <span>do_stuff</span>(items: <span>Dict</span>(Person, Int)) -&gt; Result(Int, Nil) {
</span></span><span><span>  <span>use</span> a <span>&lt;-</span> result.<span>try</span>(dict.get(items, drew))
</span></span><span><span>  <span>use</span> b <span>&lt;-</span> result.<span>try</span>(dict.get(items, jane))
</span></span><span><span>
</span></span><span><span>  Ok(a <span>+</span> b)
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>pub</span> <span>fn</span> <span>main</span>() {
</span></span><span><span>  <span>let</span> items <span>=</span> dict.from_list([#(drew, <span>1</span>), #(jane, <span>2</span>)])
</span></span><span><span>
</span></span><span><span>  items
</span></span><span><span>  <span>|&gt;</span> do_stuff
</span></span><span><span>  <span>|&gt;</span> string.inspect
</span></span><span><span>  <span>|&gt;</span> io.println
</span></span><span><span>}
</span></span><span><span><span>// Ok(3)
</span></span></span></code></pre></div><p>Note that I don’t need to do <em>anything</em> to allow my custom <code>Person</code> type to be a
key in a <code>Dict</code>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Gleam has made some very interesting decisions in the functional programming
design space. I believe it has pragmatically taken the best of Haskell and
OCaml with very few downsides.</p>
<p>I’m surprised at how versatile <code>use</code> can be, especially given Gleam’s lack of
ad-hoc polymorphism or a module system. Folks have already made lovely <a href="https://hexdocs.pm/party/">parser
combinators</a>, <a href="https://hexdocs.pm/toy/">decoders</a>,
and more. I even wrote a <a href="https://hexdocs.pm/clip/">CLI option parsing library</a>
myself.</p>
<p>I’m excited to continue working with Gleam and believe it has a bright future
for functional programming newcomers and old timers alike.</p>


  </div>

  
</article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[XFCE 4.20 Aims to Bring Preliminary Wayland Support (244 pts)]]></title>
            <link>https://ostechnix.com/xfce-4-20-aims-to-bring-preliminary-wayland-support/</link>
            <guid>41758693</guid>
            <pubDate>Sun, 06 Oct 2024 17:41:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ostechnix.com/xfce-4-20-aims-to-bring-preliminary-wayland-support/">https://ostechnix.com/xfce-4-20-aims-to-bring-preliminary-wayland-support/</a>, See on <a href="https://news.ycombinator.com/item?id=41758693">Hacker News</a></p>
Couldn't get https://ostechnix.com/xfce-4-20-aims-to-bring-preliminary-wayland-support/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Popular Science Magazine Archives, May 1872-March 2009 (141 pts)]]></title>
            <link>https://books.google.com/books/about/Popular_Science.html?id=qR8DAAAAMBAJ</link>
            <guid>41758644</guid>
            <pubDate>Sun, 06 Oct 2024 17:33:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://books.google.com/books/about/Popular_Science.html?id=qR8DAAAAMBAJ">https://books.google.com/books/about/Popular_Science.html?id=qR8DAAAAMBAJ</a>, See on <a href="https://news.ycombinator.com/item?id=41758644">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="ge_summary"><p><a href="https://books.google.ch/books?id=qR8DAAAAMBAJ&amp;printsec=frontcover&amp;hl=de&amp;source=gbs_ge_summary_r&amp;cad=0"><img src="https://books.google.ch/books/content?id=qR8DAAAAMBAJ&amp;printsec=frontcover&amp;img=1&amp;zoom=1&amp;edge=curl&amp;imgtk=AFLRE70IjIJlBcx4UWyFFo4CKUSsIHMORZ-MWlukt2fO4m8BTHcuG90vTs8A0jjlEUuLnXviXYhvKVc_EdcsLnWwraWPBkXllWpEOcNR3X9emsLsrEbrLZ1tJv8TQopxViSQPjug8hdF" alt="Cover" title="Cover" width="128" id="summary-frontcover"></a></p><div id="metadata"><p>Mai 1872<br>134 Seiten<br>Band 1<br><span dir="ltr">ISSN 0161-7370</span></p><p>Ver�ffentlicht von Bonnier Corporation</p></div><div id="synopsis"><p>Popular Science gives our readers the information and tools to improve
their technology and their world.  The core belief that Popular
Science and our readers share: The future is going to be better, and
science and technology are the driving forces that will help make it
better.</p></div><div><form action="/books" id="search_form" method="get"> <table><tbody><tr><td><span></span></td><td></td><td></td></tr></tbody></table><label for="sits">Alle Ausgaben durchsuchen</label></form><div id="preview-link"><p><a href="https://books.google.ch/books?id=qR8DAAAAMBAJ&amp;printsec=frontcover&amp;hl=de"><span dir="ltr">Diese Zeitschrift als Leseprobe anzeigen</span> »</a></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Book of Kells, now digitized and available online (348 pts)]]></title>
            <link>https://www.openculture.com/2024/09/the-medieval-masterpiece-the-book-of-kells-is-now-digitized-and-available-online.html</link>
            <guid>41757722</guid>
            <pubDate>Sun, 06 Oct 2024 15:19:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openculture.com/2024/09/the-medieval-masterpiece-the-book-of-kells-is-now-digitized-and-available-online.html">https://www.openculture.com/2024/09/the-medieval-masterpiece-the-book-of-kells-is-now-digitized-and-available-online.html</a>, See on <a href="https://news.ycombinator.com/item?id=41757722">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p><img loading="lazy" fetchpriority="high" decoding="async" src="https://cdn8.openculture.com/2019/03/05232956/KellsFol032vChristEnthroned-e1551898555826.jpg" alt="" width="1484" height="1934" data-old-src="https://www.openculture.com/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-src="https://cdn8.openculture.com/2019/03/05232956/KellsFol032vChristEnthroned-e1551898555826.jpg"></p>
<p>If you know noth­ing else about medieval Euro­pean illu­mi­nat­ed man­u­scripts, you sure­ly know the Book of Kells. “One of Ireland’s great­est cul­tur­al trea­sures” com­ments&nbsp;<a href="http://www.medievalists.net/2019/03/free-online-course-on-the-book-of-kells-returns/">Medievalists.net</a>, “it is set apart from oth­er man­u­scripts of the same peri­od by the qual­i­ty of its art­work and the sheer num­ber of illus­tra­tions that run through­out the 680 pages of the book.” The work not only attracts schol­ars, but almost a mil­lion vis­i­tors to Dublin every year. “You sim­ply can’t trav­el to the cap­i­tal of Ire­land,” writes <a href="https://bookriot.com/2015/11/05/10-things-know-book-kells/">Book Riot’s Eri­ka Har­litz-Kern</a>, “with­out the Book of Kells being men­tioned. And right­ful­ly so.”</p>
<p>The ancient mas­ter­piece is a stun­ning exam­ple of Hiber­no-Sax­on style, thought to have been com­posed on the Scot­tish island of Iona in 806, then trans­ferred to the monastery of Kells in Coun­ty Meath after a Viking raid (a sto­ry told in the mar­velous ani­mat­ed film <em><a href="https://www.awn.com/animationworld/secret-kells-what-remarkable-animated-feature">The Secret of Kells</a></em>). Con­sist­ing main­ly of copies of the four gospels, as well as index­es called “canon tables,” the man­u­script is believed to have been made pri­mar­i­ly for dis­play, not read­ing aloud, which is why “the images are elab­o­rate and detailed while the text is care­less­ly copied with entire words miss­ing or long pas­sages being repeat­ed.”</p>


<p>Its exquis­ite illu­mi­na­tions mark it as a cer­e­mo­ni­al object, and its “intri­ca­cies,” argue Trin­i­ty Col­lege Dublin pro­fes­sors Rachel Moss and Fáinche Ryan, “lead the mind along path­ways of the imag­i­na­tion…. You haven’t been to Ire­land unless you’ve seen the Book of Kells.” This may be so, but thank­ful­ly, in our dig­i­tal age, you need not go to Dublin to see this fab­u­lous his­tor­i­cal arti­fact, or a dig­i­ti­za­tion of it at least, <a href="https://digitalcollections.tcd.ie/concern/works/hm50tr726?locale=en">entire­ly view­able at the online col­lec­tions of the Trin­i­ty Col­lege Library</a>. <strong>(When you click on the pre­vi­ous <a href="https://digitalcollections.tcd.ie/concern/works/hm50tr726?locale=en">link</a>, make sure you scroll down the page.)</strong> The pages, orig­i­nal­ly cap­tured in 1990, “have recent­ly been res­canned,” <a href="https://web.archive.org/web/20170705232205/https://www.tcd.ie/library/news/book-of-kells-now-free-to-view-online/">Trin­i­ty Col­lege Library writes</a>, using state-of-the-art imag­ing tech­nol­o­gy. These new dig­i­tal images offer the most accu­rate high-res­o­lu­tion images to date, pro­vid­ing an expe­ri­ence sec­ond only to view­ing the book in per­son.”</p>
<div>
<p><span><iframe title="YouTube video player" type="text/html" width="640" height="505" src="//www.youtube.com/embed/CmNxc6Jsp4Q?wmode=transparent&amp;fs=1&amp;hl=en&amp;showsearch=0&amp;rel=0&amp;theme=dark" frameborder="0" allowfullscreen="" loading="lazy"></iframe></span>
	</p>
</div>

<p>What makes the Book of Kells so spe­cial, repro­duced “in such var­ied places as Irish nation­al coinage and tat­toos?” asks Pro­fes­sors Moss and Ryan. “There is no one answer to these ques­tions.” In their <a href="https://www.openculture.com/2019/03/discover-the-great-medieval-manuscript-the-book-of-kells-in-a-free-online-course.html">free online course on the man­u­script</a>, these two schol­ars of art his­to­ry and the­ol­o­gy, respec­tive­ly, do not attempt to “pro­vide defin­i­tive answers to the many ques­tions that sur­round it.” Instead, they illu­mi­nate its his­to­ry and many mean­ings to dif­fer­ent com­mu­ni­ties of peo­ple, includ­ing, of course, the peo­ple of Ire­land. “For Irish peo­ple,” they explain in the course trail­er above, “it rep­re­sents a sense of pride, a tan­gi­ble link to a pos­i­tive time in Ireland’s past, reflect­ed through its unique art.”</p>
<div>
<p><span><iframe title="YouTube video player" type="text/html" width="640" height="505" src="//www.youtube.com/embed/jDeXcGXYIkU?wmode=transparent&amp;fs=1&amp;hl=en&amp;showsearch=0&amp;rel=0&amp;theme=dark" frameborder="0" allowfullscreen="" loading="lazy"></iframe></span>
	</p>
</div>

<p>But while the Book of Kells is still a mod­ern “sym­bol of Irish­ness,” it was made with mate­ri­als and tech­niques that fell out of use sev­er­al hun­dred years ago, and that were once spread far and wide across Europe, the Mid­dle East, and North Africa. In the video above, Trin­i­ty Col­lege Library con­ser­va­tor John Gillis shows us how the man­u­script was made using meth­ods that date back to the “devel­op­ment of the codex, or the book form.” This includes the use of parch­ment, in this case calf skin, a mate­r­i­al that remem­bers the anatom­i­cal fea­tures of the ani­mals from which it came, with mark­ings where tails, spines, and legs used to be.</p>
<p>The Book of Kells has weath­ered the cen­turies fair­ly well, thanks to care­ful preser­va­tion, but it’s also had per­haps five rebind­ings in its life­time. “In its orig­i­nal form,” notes Har­litz-Kern, the man­u­script “was both thick­er and larg­er. Thir­ty folios of the orig­i­nal man­u­script have been lost through the cen­turies and the edges of the exist­ing man­u­script were severe­ly trimmed dur­ing a rebind­ing in the nine­teenth cen­tu­ry.” It remains, nonethe­less, one of the most impres­sive arti­facts to come from the age of the illu­mi­nat­ed man­u­script, “described by some,” says Moss and Ryan, “as the most famous man­u­script in the world.” Find out why by <a href="https://digitalcollections.tcd.ie/concern/works/hm50tr726?locale=en">see­ing it (vir­tu­al­ly) for your­self</a> and learn­ing about it from the experts above.</p>
<p>For any­one inter­est­ed in get­ting a copy of&nbsp;The Book of Kells in a nice print for­mat, see&nbsp;<a href="https://amzn.to/2LQvHgD"><em>The Book of Kells: Repro­duc­tions from the man­u­script in Trin­i­ty Col­lege, Dublin.</em></a></p>
<p><strong>Relat­ed Con­tent:</strong></p>
<p><a title="Permanent Link to Take a Free Online Course on the Great Medieval Manuscript, the Book of Kells" href="https://www.openculture.com/2019/03/discover-the-great-medieval-manuscript-the-book-of-kells-in-a-free-online-course.html" rel="bookmark">Take a Free Online Course on the Great Medieval Man­u­script, the Book of Kells</a></p>
<p><a title="Permanent Link to Discover the Medieval Illuminated Manuscript <i>Les Très Riches Heures du Duc de Berry</i>, “the World’s Most Beautiful Calendar” (1416)" href="https://www.openculture.com/2023/04/discover-the-medieval-illuminated-manuscript-les-tres-riches-heures-du-duc-de-berry.html" rel="bookmark">Dis­cov­er the Medieval Illu­mi­nat­ed Man­u­script&nbsp;<em>Les Très Rich­es Heures du Duc de Berry</em>, “the World’s Most Beau­ti­ful Cal­en­dar” (1416)</a></p>
<p><a href="http://www.openculture.com/2018/02/behold-the-beautiful-pages-from-a-medieval-monks-sketchbook.html">Behold the Beau­ti­ful Pages from a Medieval Monk’s Sketch­book: A Win­dow Into How Illu­mi­nat­ed Man­u­scripts Were Made (1494)</a></p>
<p><a href="http://www.openculture.com/2019/02/800-illuminated-medieval-manuscripts-are-now-online.html">800 Illu­mi­nat­ed Medieval Man­u­scripts Are Now Online: Browse &amp; Down­load Them Cour­tesy of the British Library and Bib­lio­thèque Nationale de France</a></p>
<p><a title="Permanent Link to Killer Rabbits in Medieval Manuscripts: Why So Many Drawings in the Margins Depict Bunnies Going Bad" href="https://www.openculture.com/2019/03/killer-rabbits-in-medieval-manuscripts-why-so-many-drawings-in-the-margins-depict-bunnies-going-bad.html" rel="bookmark">Killer Rab­bits in Medieval Man­u­scripts: Why So Many Draw­ings in the Mar­gins Depict Bun­nies Going Bad</a></p>
<p><a href="http://about.me/jonesjoshua"><em>Josh Jones</em></a><em>&nbsp;is a writer and musi­cian based in Durham, NC. Fol­low him at&nbsp;<a href="https://twitter.com/jdmagness">@jdmagness</a></em></p>
<br>		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI is an impediment to learning web development (212 pts)]]></title>
            <link>https://ben.page/jumbocode-ai</link>
            <guid>41757711</guid>
            <pubDate>Sun, 06 Oct 2024 15:18:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ben.page/jumbocode-ai">https://ben.page/jumbocode-ai</a>, See on <a href="https://news.ycombinator.com/item?id=41757711">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-pagefind-body=""> <p>Some thoughts on AI and LLMs, based on being Head of Engineering for <a href="https://ben.page/jumbocode">JumboCode</a>, a club of 180 students at Tufts University that builds software for non-profits.</p>
<p>For context, almost all of our developers are learning web development (TypeScript, React, etc) from scratch, and have little prior experience with programming.</p>
<h2 id="use-of-llms-in-jumbocode-is-ubiquitous">Use of LLMs in JumboCode is ubiquitous</h2>
<p>LLMs excel at writing code for web development — you can describe a frontend component that you want and get a decent React component back.</p>
<p>I didn’t spend the year hawk-eyeing the teams’ repositories, but when I did poke my head in, I found substantial portions that <em>looked</em> LLM-written — that is to say, overly-commented, dissonant, and, at times, horrifying.</p>
<h3 id="horrifying"><em>“horrifying?”</em></h3>
<p>The starkest example I came across was a Next.js project that had:</p>
<ul>
<li>A page written in HTML and vanilla JavaScript, loaded from the <code>public/</code> directory, completely outside of the Next.js + React system.</li>
<li>Vanilla JavaScript loaded in via filesystem APIs and executed via <code>dangerouslySetInnerHTML</code>.</li>
<li>API calls from one server-side API endpoint to another public API endpoint on <code>localhost:3000</code> (instead of just importing a function and calling it directly).</li>
</ul>
<p>These don’t seem to me like classic beginner mistakes — these are <em>fundamental misunderstandings</em> of the tools and the web platform.</p>
<p>LLMs will obediently provide the solutions you ask for. If you’re missing fundamental understanding, you won’t be able to spot when your questions have gone off the rails.</p>
<h2 id="use-of-llms-hinders-learning-of-web-development">Use of LLMs hinders learning of web development.</h2>
<p>LLMs are a shortcut to get assignments done. In the process, however, you learn close to nothing.</p>
<p>It’s cliche, but struggling <em>is</em> learning. The way you learn is that you try different paths, piece bits of information together, and eventually create a mental model.</p>
<p>LLMs don’t require you to form a mental model and allow you to skip to the end result, but in turn you won’t <em>have</em> a mental model when you actually need one (for example, when you need to verify that your LLM has architected the code in a reasonable way).</p>
<p>LLMs are useful if you already have a good mental model and understanding of a subject. However, I believe that they are destructive when learning something from 0 to 1.</p>
<h2 id="you-should-probably-just-ask-a-person">You should probably just ask a person.</h2>
<p>You’ll learn a lot more by asking a real human, like your tech lead or a member of JumboCode’s board.</p>
<p>They can explain something in the exact context that you need it. And, in my experience at least, humans are still better than LLMs at providing concise, proper-level explanations.</p>
<h2 id="youll-probably-still-use-llms-instead-of-asking-a-person">You’ll probably still use LLMs instead of asking a person.</h2>
<p>Asking people is hard work and requires asking questions (scary). Many folks will still ask LLMs to write large swaths of code.</p>
<p>This is likely worse for your learning, but I can’t stop you (and I might be wrong).</p>
<p>But in my opinion, if you don’t have the time to do JumboCode without deferring whole tasks to LLMs, you’re probably better off not doing JumboCode. (And no hard feelings! I just don’t think that going through the motions via LLM-generated deliverables is worth the time.)</p>
<hr>
<h2 id="appendix-ben-do-you-use-llms">Appendix: Ben, do <em>you</em> use LLMs?</h2>
<p>Yes, I do! For example, I write code in <a href="https://cursor.so/">Cursor</a>, the self-proclaimed “AI Code Editor.”</p>
<p>But I’m honestly glad that I learned the fundamentals of web development before LLMs became ubiquitous. I don’t know if I would have had the will and motivation to properly learn the foundations otherwise — but doing so has paid off immensely for me.</p>
<p>If I had used LLMs to produce work when I was first learning web development, I would not have learned web development.</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA['Students who use AI as a crutch don't learn anything' (103 pts)]]></title>
            <link>https://english.elpais.com/technology/2024-10-03/ethan-mollick-analyst-students-who-use-ai-as-a-crutch-dont-learn-anything.html</link>
            <guid>41757010</guid>
            <pubDate>Sun, 06 Oct 2024 13:24:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://english.elpais.com/technology/2024-10-03/ethan-mollick-analyst-students-who-use-ai-as-a-crutch-dont-learn-anything.html">https://english.elpais.com/technology/2024-10-03/ethan-mollick-analyst-students-who-use-ai-as-a-crutch-dont-learn-anything.html</a>, See on <a href="https://news.ycombinator.com/item?id=41757010">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-dtm-region="articulo_cuerpo"><p>“I don’t have any help or anything. I organize myself and I get like 800 messages a day. I’m scared to look at my to-do list,” says Ethan Mollick, 49, a professor at the Wharton School of Business at the University of Pennsylvania. He has just published <i>Co-Intelligence: Living and Working with AI</i>, a book on how to make the most of <a href="https://english.elpais.com/science-tech/2023-01-31/chatgpt-is-just-the-beginning-artificial-intelligence-is-ready-to-transform-the-world.html" target="_blank">artificial intelligence in everyday life</a>. Despite this, managing his schedule remains extremely complicated. Although he recommends using AI as a companion for almost everything, he also believes that we should be careful. Thanks to his social media presence, newsletter and candid comments, Mollick has become one of the most popular analysts and testers of the new generative AI tools.</p><p><b>Question: </b>How does it feel to be an AI influencer?</p><p><b>Answer.</b> I hate that description. I’ve been on social media for a long time, and I’m a compulsive sharer. But I don’t take money from any of these AI companies or do sponsorship deals. I talk to them because it’s interesting. I’m a tenured professor, I can say whatever I want. It’s strange to see companies trying to manipulate me by showing me their stuff, but I don’t have the infrastructure of an influencer. I worry that that influencer title smears everything together. There’s a difference between public intellectuals, researchers, and critics. It would be better if we had more classes of thinking.</p><p><b>Q.</b> You recommend spending three sleepless nights to master AI.</p><p><b>A.</b> The best advice from the book is to spend 10 hours with AI and apply it to everything you do. For whatever reason, very few people are actually spending the time they need to really understand these systems.</p><p><b>Q. </b>You don’t like to call AI a crutch.</p><p><b>A.</b> The crutch is a dangerous approach because if we use a crutch, <a href="https://english.elpais.com/technology/2024-04-14/melanie-mitchell-the-big-leap-in-artificial-intelligence-will-come-when-it-is-inserted-into-robots-that-experience-the-world-like-a-child.html" target="_blank">we stop thinking</a>. Students who use AI as a crutch don’t learn anything. It prevents them from thinking. Instead, using AI as co-intelligence is important because it increases your capabilities and also keeps you in the loop.</p><p><b>Q.</b> Isn’t it inevitable that AI will make us lazier?</p><p><b>A.</b> Calculators also made us lazier. Why aren’t we doing math by hand anymore? You should be taking notes by hand now instead of recording me. We use technology to take shortcuts, but we have to be <a href="https://english.elpais.com/opinion/2024-03-03/a-more-humane-education-in-the-era-of-artificial-intelligence.html" target="_blank">strategic in how we take those shortcuts</a>.</p><p><b>Q.</b> Why should we approach artificial intelligence with a strategy?</p><p><b>A. </b>AI does so many things that we need to set guardrails on what we don’t want to give up. It’s a very weird, general-purpose technology, which means it will affect all kinds of things, and we’ll have to adjust socially. We did a very bad job with the last major social adjustment, social media. This time we need to be more deliberate.</p><p><b>Q. </b>Will we be able to better socially adjust to AI?</p><p><b>A. </b>What gives me some hope with this technology is that because it’s so human-like, it’s more natural to work with. Humans used to work with smart team members to solve problems. It’s one thing if this becomes an all-intelligent God machine, but at the current level, where you’re interacting with this thing, and it’s flawed, that’s where it can be useful to be human-like.</p><p><b>Q. </b>In your book, you talk about “just me tasks” in reference to raising children and values. Can we do those things better without AI?</p><p><b>A. </b>There are a lot of moral and ethical decisions. I can’t help with that much, but I think we have to making those choices. With social media, we didn’t make enough decisions about how we wanted to use it. People and a lot of books view AI as something that’s being forced on us, and companies are creating AI, but they don’t really know how it’s being used or what it’s good for. We can make some decisions about that, and I think people tend to view it as a government or corporate decision, but it’s not just that.</p><p><b>Q. </b>People already have AI-made partners and psychological advisors.</p><p><b>A.</b> We have lived with a general-purpose technology for many years: the internet. Social media is just one sharp edge of what the internet has done to society. It is just one application. Other applications have been dating apps or how we shop. The implications are deep and wide. For example, with AI’s voice mode, I don’t want to be friends with it, but at the same time, I find myself being apologetic and careful when I talk to it. We will have to adjust. I trust that we can, but people already have connections to AI. Some will have almost religious connections to AI, and others will be manipulated. We have to recognize that at a bunch of stuff is all going to happen at once, good and bad, and the more we’re kind of ready for that set of change the better off we are.</p><p><b>Q. </b>You’ve written that “much of the value of AI use comes from people not knowing you are using it.” Why are we afraid of others knowing we’re using AI?</p><p><b>A.</b> There are several layers in organizations that prevent people from using AI. One of them is that if I use AI to do work, others will think I’m brilliant. You don’t want people to know that you’re not actually that brilliant, especially since AI is very good at things like writing empathetic emails, and it would be weird for them to know that that empathy was coming from an AI. They also don’t want to show it because they’re afraid that you’ll realize their job is redundant, or that they’ll be asked to do more work.</p><p><b>Q.</b> You recently wrote that something is starting to change with OpenAI’s new model, ChatGPT-o1.</p><p><b>A. </b>I finished the book a year ago. I had to have enough foresight to see where things were going. Predictions for six years in the future and if AI will kill or save us all wasn’t my interest. My interest was: how do you work with this thing? One of the things I mention that wasn’t as important in the previous generation of AI and that I think will be key in the next year or two is this idea of autonomy and AI agents. It’s the beginning of AI that will perform processes autonomously, without our help. I don’t think that will fundamentally change how we work with AI, but we may move to models that come back and ask you questions when they have problems. There’s something valuable about being questioned. It’s something we do in all the AI tools we build for learning: there has to be a back-and-forth, and the o1 model doesn’t really do that. It doesn’t ask. That’s what’s unnerving.</p><p><b>Q. </b>You like the idea of AI-powered one-on-one tutoring for education. Is that where we’re headed, after what you call the “homework apocalypse”?</p><p><b>A.</b> The AI tutor is one piece of the puzzle in the <a href="https://english.elpais.com/science-tech/2023-02-24/ai-in-the-classroom-i-require-my-students-to-use-chatgpt.html" target="_blank">transformation of education</a>. I worked in interactive education long before generative AI, and there are things about classrooms that we know for sure are changing, independent of AI: lectures are no longer a good idea. Active learning is better, where students have to participate. You want personalization. In the classroom, a small group of students usually participate and others are lost. We are not teaching correctly. In some ways, lectures have value, they are not a disaster. We have a way of teaching that has evolved over 200 years, and that’s alright. The homework apocalypse gives us a chance that we won’t all take advantage of, but we should rethink learning.</p><p><b>Q. </b>How can we take advantage of this opportunity?</p><p><b>A.</b> Active learning classrooms instead of lecture halls are a better way to learn. We haven’t adopted them because it’s easier to keep giving lectures and homework. We have the opportunity to be more thoughtful, and AI tutors are part of being thoughtful, because they help fill in knowledge gaps. Class time should be used to work together on problems. We can’t keep doing what we were doing before.</p><p><b>Q. </b>What are some of the biggest misconceptions about AI?</p><p><b>A. </b>People are divided between those who are excited about AI and those who are nervous or anxious. Each group has its own myths. For non-adopters, one of the biggest myths is that AI doesn’t do anything original and all you get is pasted together content from others. And that’s not true. AI is built as an elaborate physical model for <a href="https://english.elpais.com/science-tech/2023-08-29/language-schools-in-the-metaverse-and-conversation-classes-with-chatgpt-ai-comes-to-online-teaching.html" target="_blank">every human language </a>and uses those rules to create new material based on its training. There is an originality there. That’s one of the big misconceptions. The other is comparing it to Google. It’s worse at the things that Google does well, but better at many other things that Google doesn’t do.</p><p><b>Q. </b>You say that the best experts of the future will be those who make the most use of AI. Are people who are waiting to use AI making a mistake?</p><p><b>A.</b> I get it, it’s an unnerving technology. People are freaking out. They’re getting a sense of three sleepless nights and running away screaming. It feels like an essential threat to a lot of careers. I think if you’re a good journalist, the first time you think, “oh no.” But then you start to see how this could help you do things better than before. And at least for the next few generations, it’s not going to replace you, even though the technologists say it is. We need to separate from the Silicon Valley noise. On one hand they’re completely right: this is a miraculous incredible technology that emulates thinking, but the other is it doesn’t understand our jobs.</p><p><i>Sign up for </i><a href="https://plus.elpais.com/newsletters/lnp/1/333/?lang=en" target="_blank"><i>our weekly newsletter</i></a><i> to get more English-language news coverage from EL PAÍS USA Edition</i></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WiFi4EU initiative provides free Wi-Fi in public spaces across Europe (122 pts)]]></title>
            <link>https://hadea.ec.europa.eu/programmes/connecting-europe-facility/wifi4eu/download-wifi4eu-app_en</link>
            <guid>41756842</guid>
            <pubDate>Sun, 06 Oct 2024 12:58:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hadea.ec.europa.eu/programmes/connecting-europe-facility/wifi4eu/download-wifi4eu-app_en">https://hadea.ec.europa.eu/programmes/connecting-europe-facility/wifi4eu/download-wifi4eu-app_en</a>, See on <a href="https://news.ycombinator.com/item?id=41756842">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="block-ewcms-theme-main-page-content" data-inpage-navigation-source-area="h2, div.ecl-featured-item__heading">
  
    
      <article dir="ltr">

  
    

  
  <div>
    

  
  
  

  

  
  
  <div><p>The <a href="https://wifi4eu.ec.europa.eu/#/home">WiFi4EU</a> initiative provides free Wi-Fi connectivity in public spaces across Europe. Our new app is designed to make it easier than ever to find these hotspots.</p><p>With the WiFi4EU app, you can access high-speed, reliable Wi-Fi at thousands of locations throughout Europe. Whether you’re traveling for work, vacationing, or simply out and about in your local town, the WiFi4EU app will help you enjoy internet access wherever you go—without the burden of data charges.</p></div>

    
  <div>
    <h2>Why you’ll love the WiFi4EU app</h2>
<div><ul><li><strong>Find free WiFi4EU hotspots:</strong><br>Quickly locate available WiFi4EU hotspots near you using our intuitive map interface. No need to waste time hunting for a connection—our app does the work for you.</li><li><strong>Connect to over 93 000 hotspots across the EU:</strong><br>Access a vast network of more than 93 000 hotspots across Europe. Whether you’re in a major city or a small village, the WiFi4EU app connects you to the best public Wi-Fi options available.</li><li><strong>Privacy-friendly with no tracking:</strong><br>Your privacy is important. The WiFi4EU app ensures a private online experience with no tracking or data collection. Simply connect and enjoy free public Wi-Fi without concerns.</li></ul></div>

  
  </div>

    <div><div><p><h2>Download now!</h2></p></div><div><div><article><picture data-ecl-picture-link=""><img src="https://hadea.ec.europa.eu/sites/default/files/styles/oe_theme_ratio_3_2_medium/public/2024-08/240820_WiFi4EU-app_v017_0.png?h=6377f7ce&amp;itok=nrM39U6Q" alt=""></picture><div data-ecl-auto-init="ContentBlock" data-ecl-content-block=""><p data-ecl-title-link=""><a href="https://apps.apple.com/be/app/wifi4eu-map/id6511245317"><span>iOS</span><svg focusable="false" aria-hidden="false"><use xlink:href="/themes/contrib/oe_theme/dist/ec/images/icons/sprites/icons.svg#external"></use></svg></a></p><div><p>Available on the App Store</p></div></div></article></div><div><article><picture data-ecl-picture-link=""><img src="https://hadea.ec.europa.eu/sites/default/files/styles/oe_theme_ratio_3_2_medium/public/2024-08/240820_WiFi4EU-app_v018.png?h=6377f7ce&amp;itok=m4kXMhDG" alt=""></picture></article></div></div></div>  
  <div>
    <p><h2>FAQs</h2></p>

  
  </div>

    
  

    
  <div><div><p><h2>More information</h2></p></div><div><div><article><div data-ecl-auto-init="ContentBlock" data-ecl-content-block=""><p>More information about the WiFi4EU initiative</p></div></article></div><div><article><div data-ecl-auto-init="ContentBlock" data-ecl-content-block=""><p>HaDEA implements WiFi4EU, learn more about what else the Agency is doing</p></div></article></div><div><article><div data-ecl-auto-init="ContentBlock" data-ecl-content-block=""><p>Main European Commission page</p></div></article></div></div></div>

    
  <div>
    <p><sup>© 2024 WiFi4EU. All rights reserved. The Google Play and App Store logos are trademarks of their respective owners.</sup></p>

  
  </div>

  
  </div>

</article>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When Earth Had Rings (113 pts)]]></title>
            <link>https://nautil.us/when-earth-had-rings-920177/</link>
            <guid>41756346</guid>
            <pubDate>Sun, 06 Oct 2024 11:17:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nautil.us/when-earth-had-rings-920177/">https://nautil.us/when-earth-had-rings-920177/</a>, See on <a href="https://news.ycombinator.com/item?id=41756346">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                        
            <p><span>P</span>lanetary rings may be one of space’s many spectacles, but in our solar system, they’re a dime a dozen. While Saturn’s rings are the brightest and most extensive, Jupiter and Uranus and Neptune have them, too. What’s more, four icy minor planets—Chariklo, Chiron, Quaoar, and Haumea—that orbit among or beyond our gas giants, also host ring systems. Even so, it would be fanciful to imagine that Earth once had a ring system of its own, wouldn’t it? I mean, that just seems almost too cool to be true.</p><p>Or is it?</p>
      
    <p>Rings are likely the dwindling remains of shredded asteroids or comets, and&nbsp;when you think about the turbulence Earth experienced around half a billion years ago, the reality of a bygone ring system around Earth seems less farfetched. That’s the case researchers make in a <a href="https://www.sciencedirect.com/science/article/pii/S0012821X24004230?via%3Dihub" target="_blank" rel="noreferrer noopener">new study</a> published in <em>Earth and Planetary Science Letters</em>.</p><blockquote>
<p>If Earth went through one ringed phase, there’s a good chance it went through several.</p>
</blockquote>
          <div>
            <p>ADVERTISEMENT</p>
            
            
      <p>
        Nautilus Members enjoy an ad-free experience. 
        <a href="https://nautil.us/concierge-login" data-ev-act="login" data-ev-cat="article-ad" data-ev-label="in body ad">
          Log in
        </a> 
        or 
        <a href="https://nautil.us/join" data-ev-act="subscribe" data-ev-cat="article-ad" data-ev-label="in body ad">
          Join now
        </a>.
      </p>
          </div><p>About 466 million years ago (long before the dinosaurs), the rate of stuff falling on Earth that was large enough to leave craters spiked sharply. Researchers have identified 21 impact craters from this spike, with sizes between a few and 50 kilometers in diameter. And sedimentary rocks from this period show a huge increase—a factor of 100 to 1,000—in the concentration of elements associated with a specific group of meteorites, called L chondrites. This period, the mid-Ordovician, also included an extreme drop in global temperature, roughly 10 degrees Celsius, which coincided with increased seismic and tsunami activity. Also, a mass extinction eliminated 85 percent of marine species, after which the temperature rebounded. What could possibly explain this?</p><p>The authors of the new study, led by Andy Tomkins, a geoscientist at Monash University in Australia, claim that you can account for the craziness of the Ordovician period if Earth had a system of rings that it captured from an asteroid. And if it went through one ringed phase, there is a good chance that it went through several. And for good measure, the other rocky planets may have done the same.&nbsp;</p><p>“This opens up a new way of thinking about the history of the Earth,” Tomkins wrote in an email. He and his colleagues focused on the distribution of craters on Earth during the Ordovician’s impact spike. The craters are spread across Earth’s surface, but its tectonic plates—the movement of which pushes the continents around—have shifted considerably in the last 450-plus million years.</p><p>The researchers determined where the impacts actually took place by “rewinding the clock” of the continents’ movements using computer models of how the Earth’s surface has rearranged itself. This showed that all of the crater-forming collisions took place within a narrow band centered on the equator. They gathered the data on where craters are located today (across the globe, especially in areas that were not likely to have been covered in ice during the Ordovician, given that ice could prevent crater formation), while also taking into account the regions where such craters could not have been found (due to present-day ice coverage, such as in Antarctica), and where craters could not have been preserved because of processes such as burial and erosion. Putting all of these factors together, Tomkins and his colleagues calculated that it would be highly improbable—we’re talking about a 1 in 25 million chance—that the impacts were randomly distributed across the Earth’s surface.</p>
          <div>
            <p>ADVERTISEMENT</p>
            
            
      <p>
        Nautilus Members enjoy an ad-free experience. 
        <a href="https://nautil.us/concierge-login" data-ev-act="login" data-ev-cat="article-ad" data-ev-label="in body ad">
          Log in
        </a> 
        or 
        <a href="https://nautil.us/join" data-ev-act="subscribe" data-ev-cat="article-ad" data-ev-label="in body ad">
          Join now
        </a>.
      </p>
          </div><figure><p>
<iframe loading="lazy" title="Plate tectonic evolution from 1 Billion years ago to the present." width="640" height="480" src="https://www.youtube.com/embed/gQqQhZp4uG8?feature=oembed&amp;enablejsapi=1&amp;origin=https://nautil.us" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p><figcaption><strong>MOVE IT:</strong> Animation of how plate tectonics have moved the continents around over the past billion years. <em>Credit: EarthByte / YouTube.</em></figcaption></figure><p>To explain why the impacts are concentrated at the equator, Tomkins and his colleagues proposed that the Ordovician Earth had a system of rings. The Ordovician impacts came from objects within the rings that crashed down onto Earth. <a href="https://nautil.us/saturns-rings-could-be-younger-than-flowers-308228/" target="_blank" rel="noreferrer noopener">Saturn’s rings</a> are thought to have formed from a large icy object that passed so close to Saturn that it was torn into pieces in a process called tidal disruption. When a comet or asteroid passes very close to a planet, the difference in gravity across the object is strong enough to stretch it to its breaking point.</p><p>We have seen tidal disruption in action: In 1992, the comet Shoemaker-Levy 9 passed so close to Jupiter that it was torn into a string of about 20 fragments. Most of these rained down onto Jupiter in 1994. In the case of Saturn’s rings, the shards of the disrupted object were captured into orbit. The orbits of captured fragments would have started off being extremely stretched-out, or eccentric, and aligned with the initial orbital trajectory of the icy parent body.&nbsp; In time, the ring fragments collided with each other and were ground down to smaller pieces, and the collection of objects settled to a circular system of rings aligned with the plane of Saturn’s equator, where we see them now.</p><figure><img width="800" height="242" alt="In Body Image" src="https://assets.nautil.us/sites/3/nautilus/jGa51alo-Raymond_BREAKER.png?auto=compress&amp;fit=scale&amp;fm=png&amp;h=310&amp;ixlib=php-3.3.1&amp;w=1024&amp;wpsize=large" srcset="https://assets.nautil.us/sites/3/nautilus/jGa51alo-Raymond_BREAKER.png?q=65&amp;auto=format&amp;w=1600 800w,https://assets.nautil.us/sites/3/nautilus/jGa51alo-Raymond_BREAKER.png?q=65&amp;auto=format&amp;w=1200 600w,https://assets.nautil.us/sites/3/nautilus/jGa51alo-Raymond_BREAKER.png?q=65&amp;auto=format&amp;w=800 400w" loading="lazy"><figcaption><strong>ALL BROKEN UP:</strong> Hubble Space Telescope image of the fragments of comet Shoemaker-Levy 9 after it was tidally disrupted during its close passage to Jupiter. <em>Credit: NASA, ESA, and H. Weaver and E. Smith (STScI)</em></figcaption></figure>
          <div>
            <p>ADVERTISEMENT</p>
            
            
      <p>
        Nautilus Members enjoy an ad-free experience. 
        <a href="https://nautil.us/concierge-login" data-ev-act="login" data-ev-cat="article-ad" data-ev-label="in body ad">
          Log in
        </a> 
        or 
        <a href="https://nautil.us/join" data-ev-act="subscribe" data-ev-cat="article-ad" data-ev-label="in body ad">
          Join now
        </a>.
      </p>
          </div><p>The ring-producing asteroid must have been an L chondrite, a type of ordinary meteorite (“ordinary” because they are the most common type of meteorites in our collections). Tomkins and his colleagues suggest that the L chondrite parent body passed extremely close to Earth—within just a few thousand kilometers of the surface, scratching the edge of the atmosphere—and was tidally disrupted, just like the object that broke apart and formed Saturn’s rings. Its fragments settled to Earth’s equatorial plane, and dust from the ring slowly rained down on Earth, along with the occasional impact of a larger fragment, mostly near the equator. This explains the high concentrations of L chondrite meteorite-like dust in Ordovician sedimentary rocks, as well as the distribution of craters across the globe.</p><p>The sharp drop in temperature after the Ordovician impact spike—called the Hirnantian global icehouse—may also have been a consequence of the Earth’s rings. The rings’ shadow would have fallen on whichever hemisphere was in winter at that time, and would have had a net cooling effect, which the dust in the atmosphere may have amplified. This drastic cooling perhaps played a role in the late Ordovician mass extinction: The global temperature bounced back up after the end of the Ordovician impact spike, presumably due to the dissipation of the rings.</p><p>“It seems plausible that the planets have seen multiple cycles of ring formation and loss,” Tomkins wrote in an email. These cycles would be triggered by the rare, very close passage of a comet or asteroid. Some planets are more likely to have ring phases than others, based on the frequency of asteroid flybys and the planets’ properties. Jupiter, Saturn, Uranus, and Neptune seem to have more frequent, or longer-lived, ring phases because they interact far more frequently with asteroids and comets than the rocky planets, and so have far more chances of disrupting one and capturing its fragments. Ring phases usually last for millions of years at a time, with considerable consequences for a planet’s geology, climate, and potentially life.</p><p>For Tomkins, the search is underway for other potential pieces of evidence that Earth has undergone previous ring phases. The Ordovician spike is the clearest sign so far. “The cratering record gets weaker the further back in time we look,” he wrote in an email. “What other evidence can we look for that Earth may have had this and other rings in the past?”</p>
          <div>
            <p>ADVERTISEMENT</p>
            
            
      <p>
        Nautilus Members enjoy an ad-free experience. 
        <a href="https://nautil.us/concierge-login" data-ev-act="login" data-ev-cat="article-ad" data-ev-label="in body ad">
          Log in
        </a> 
        or 
        <a href="https://nautil.us/join" data-ev-act="subscribe" data-ev-cat="article-ad" data-ev-label="in body ad">
          Join now
        </a>.
      </p>
          </div><p>It’s a good question. And I’m excited to know someone like Tomkins is out there chasing down an answer. <img decoding="async" src="https://assets.nautil.us/sites/3/nautilus/nautilus-favicon-14.png?fm=png" alt=""></p><p><em>Lead image: MarcelClemens / Shutterstock</em></p>              
                            <ul>
                                      <li>
                      <div>
                        <h6>
                          Sean Raymond                        </h6>
                        <p>
                          Posted on <time datetime="2024-10-04T16:31:10-05:00">October 4, 2024</time>
                        </p>
                      </div>
                                                <p>
                            Sean Raymond is an American astrophysicist working at the <a href="https://astrophy.u-bordeaux.fr/?lang=en">Bordeaux Astrophysical Laboratory</a> in France. He also writes a blog at the interface of science and fiction (<a href="https://planetplanet.net/">planetplanet.net</a>) and published a <a href="https://www.amazon.com/dp/B08LFZZWGZ">book of astronomy poems</a>.                          </p>
                                            </li>
                                  </ul>
            <div>
  <p><img src="https://nautil.us/wp-content/themes/nautilus-block-theme/images/icons/logo-icon.svg" alt="new_letter"></p><div>
    <h4>Get the Nautilus newsletter</h4>
    <p>Cutting-edge science, unraveled by the very brightest living thinkers.</p>
  </div>

  
</div>        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LaTeX Style Guide for EE 364B [pdf] (2014) (108 pts)]]></title>
            <link>https://web.stanford.edu/class/ee364b/latex_templates/template_notes.pdf</link>
            <guid>41756286</guid>
            <pubDate>Sun, 06 Oct 2024 11:03:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://web.stanford.edu/class/ee364b/latex_templates/template_notes.pdf">https://web.stanford.edu/class/ee364b/latex_templates/template_notes.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=41756286">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[ByteDance is abusing the free video downloading service Cobalt for mass scraping (136 pts)]]></title>
            <link>https://twitter.com/uwukko/status/1842538843720868016</link>
            <guid>41756209</guid>
            <pubDate>Sun, 06 Oct 2024 10:42:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/uwukko/status/1842538843720868016">https://twitter.com/uwukko/status/1842538843720868016</a>, See on <a href="https://news.ycombinator.com/item?id=41756209">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
    </channel>
</rss>