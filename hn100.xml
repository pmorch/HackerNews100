<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 18 Jun 2025 18:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Framework Laptop 12 review (102 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2025/06/framework-laptop-12-review-im-excited-to-see-what-the-2nd-generation-looks-like/</link>
            <guid>44310583</guid>
            <pubDate>Wed, 18 Jun 2025 15:09:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2025/06/framework-laptop-12-review-im-excited-to-see-what-the-2nd-generation-looks-like/">https://arstechnica.com/gadgets/2025/06/framework-laptop-12-review-im-excited-to-see-what-the-2nd-generation-looks-like/</a>, See on <a href="https://news.ycombinator.com/item?id=44310583">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="app">
    <p><a href="#main">
  Skip to content
</a></p>



<main id="main">
            <article data-id="2101033">
  
  <header>
  <div>
    <div>
      <p><span>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 40"><defs><clipPath id="section-gadgets_svg__a"><path fill="none" d="M0 0h40v40H0z"></path></clipPath><clipPath id="section-gadgets_svg__b"><path fill="none" d="M0 0h40v40H0z"></path></clipPath></defs><g clip-path="url(#section-gadgets_svg__a)"><g fill="currentColor" clip-path="url(#section-gadgets_svg__b)"><path d="M38 22c1.1 0 2-.9 2-2s-.9-2-2-2h-2v-6h2c1.1 0 2-.9 2-2s-.9-2-2-2h-2V4h-4V2c0-1.1-.9-2-2-2s-2 .9-2 2v2h-6V2c0-1.1-.9-2-2-2s-2 .9-2 2v2h-6V2c0-1.1-.9-2-2-2S8 .9 8 2v2H4v4H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v6H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v6H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v4h4v2c0 1.1.9 2 2 2s2-.9 2-2v-2h6v2c0 1.1.9 2 2 2s2-.9 2-2v-2h6v2c0 1.1.9 2 2 2s2-.9 2-2v-2h4v-4h2c1.1 0 2-.9 2-2s-.9-2-2-2h-2v-6zm-6 10H8V8h24z"></path><path d="M24.7 17.3 20 12h-7.1c-.6 0-1 .4-1 1s.4 1 1 1h6.3l4.1 4.7L20 22h8v-8z"></path><path d="m15.2 22.7 4.7 5.3H27c.6 0 1-.4 1-1s-.4-1-1-1h-6.3l-4.1-4.7 3.3-3.3h-8v8z"></path></g></g></svg>
  </span>
  <span>
    how much would you pay for personality?
  </span>
</p>
    </div>

    

    <p>
      A sturdy, thoughtful, cute design that just can't compete in its price range.
    </p>

    

    <div>
            <p><a data-pswp-width="2560" data-pswp-height="1440" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1440x810.jpeg 1440w" data-cropped="false" href="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150.jpeg" target="_blank">
              <img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150.jpeg" alt="" loading="eager" decoding="async" fetchpriority="high" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1440x810.jpeg 1440w" sizes="(max-width: 2560px) 100vw, 2560px">
            </a></p><div id="caption-2101680">
    
    <p>
      Framework's Laptop 12 has a lot of personality, but also a lot of shortcomings.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
          </div>

    <div>
    
    <p>
      Framework's Laptop 12 has a lot of personality, but also a lot of shortcomings.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
  </div>
</header>


  

  
      
    
    <div>
                      
                      
          
<p>"What's this purple laptop? It's cool."</p>
<p>Over a decade-plus of doing gadget reviews and review-adjacent things, my wife (and, lately, my 5-year-old) have mostly stopped commenting on the ever-shifting selection of laptops I have in my bag or lying around the house at any given time. Maybe she can't tell them apart, or maybe she just figures there isn't that much to say about whatever black or silver metal slab I'm carrying around. Either way, they practically never elicit any kind of response, unless there are just too many of them sitting out in too many places.</p>
<p>But she&nbsp;<em>did</em> ask about the Framework Laptop 12, the third and latest major design in Framework's slowly expanding lineup of modular, repairable, upgradeable laptops. With its five two-toned color options and sturdy plastic exterior, it's definitely more approachable and friendly-looking than the Laptop 13 or Laptop 16, both metal slabs with a somewhat less-finished and prototype-y look to them. But it retains the features that a certain kind of PC geek likes about Framework's other laptops—user-customizable and swappable ports, an easy-to-open design, first-class Linux support, and the promise of future upgrades that improve its performance and other specs.</p>
<h2>Look and feel</h2>
<figure>
    <p><img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-1440x810.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The Laptop 12 stacked atop the Laptop 13.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>Plastic gets a bad rap, and there are indeed many subpar plastic gadgets out there. When done poorly, plastic can look and feel cheap, resulting in less durable devices that show more wear over time.</p>
<p>But well-done plastic can still feel solid and high-quality, in addition to being easier to make in different colors. Framework says the Laptop 12's chassis is a combination of ABS plastic and <a href="https://en.wikipedia.org/wiki/Thermoplastic_polyurethane">TPU plastic</a> (a more flexible, rubberized material), molded over a metal inner structure. The result is something that can probably actually take the shock of a drop or a fall better than many aluminum-and-glass laptops without feeling overly cheap or chintzy.</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>The five two-tone color options—the boring, businesslike black and gray, plus purple-and-gray lavender, pink-and-baby-blue bubblegum, and the green sage options—are the most fun thing about it, and the lavender and bubblegum colors are particularly eye-catching.</p>

<figure>
    <p><img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-1440x810.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      Keyboard and trackpad. Only the lavender and gray laptops get a color-matched trackpad; the keyboard and deck are always different shades of gray.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>Matching other components to the exterior of the system can be a bit of a crapshoot, though. The screwdriver and spudger that Framework provides for upgrading and repairing all of its systems <em>does</em> match the color of the laptop, and the two-tone styluses for the touchscreens will also match the laptops when they're made available for purchase in the coming months.</p>
<p>The lavender option is the only one that can also be configured with a color-matched lavender trackpad—the only other trackpad option is gray, and the keyboard deck and the keyboard itself are all gray no matter what color laptop you pick. This is presumably meant to limit the number of different trackpad options that Framework has to manufacture and stock, but it is too bad that the laptop's keyboard and palm rest aren't as colorful as the rest of it.</p>
<p>The Laptop 12 also uses Framework's still-unique Expansion Card system for customizing the built-in ports. These are all 10 Gbps USB 3.2 Gen 2 ports rather than the Thunderbolt ports on the Intel versions of the Laptop 13, but all four support the same speeds, all four support charging, and all four support display output, so you really can put whatever port you want wherever you want it.</p>
<p>A downside of the Laptop 12 is that, as of this writing, only the USB-C Expansion Modules are available in color-matched versions. If you want USB-A, HDMI, DisplayPort, or any other kind of port on your system, you'll get the silver modules that were designed to match the finish on the Framework Laptops 13 and 16, so you'll have to put up with at least one mismatched port on your otherwise adorable system.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          

<figure>
    <p><img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-1440x810.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      Only the USB-C Expansion Cards are available in lavender, which can make for goofy-looking mismatches. But I do prefer the Framework 16-style retention switches to the Framework Laptop 13's retention buttons, which you need to hold down as you pull out the Expansion Card.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>Once you get past the adorable design, the Expansion Modules, and the sturdy construction, the system's downsides start to become more apparent. The 12.2-inch, 1920×1200 touchscreen gets plenty bright and has a respectable contrast ratio (440 nits and 1,775:1 in our testing, respectively). But it's surrounded by thick black bezels on all sides, particularly on the bottom—it does seem that either a larger screen or a slightly smaller laptop design would be possible if so much space weren't wasted by these thick borders.</p>
<p>The display has good viewing angles but a distinctly mediocre color gamut, covering around 60 percent of the SRGB color space (compared to the high 90s for the Laptop 13 and most midrange to high-end IPS screens in other laptops). This is low enough that most colors appear slightly muted and washed out—reds most noticeably, though greens aren't much better. You definitely don't need a colorimeter to see the difference here.</p>
<p>Framework's color-matched stylus isn't ready yet, but you won't need to wait for one if you want to use a pen with this touchscreen. Both the Universal Stylus Initiative (USI) 2.0 and Microsoft Pen Protocol (MPP) 2.0 specs are supported, so the Surface Pen, a bunch of Lenovo styluses, and any number of inexpensive third-party Amazon styluses will all work just fine. That said, the screen can only support one of those stylus specs at a time—MPP is on by default, and you can swap between them in the BIOS settings.</p>
<figure>
    <p><img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-1440x810.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The webcam and mic have locks to disable them so that the OS can't see or use them.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>The keyboard feels mostly fine, with good key spacing and a nice amount of travel. I noticed that I was occasionally missing letters the first couple of days I used the laptop—I was pressing the keys, but they intermittently didn't register. That got better as I adjusted to the system. The trackpad is also unremarkable in a good way. Finger tracking and multi-touch gestures all worked as intended.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>But the keyboard lacks a backlight, and it doesn't have the fingerprint sensor you get with the Laptop 13. With no fingerprint sensor and no IR webcam, there are no biometric authentication options available for use with Windows Hello, so you'll either need a PIN or a password to unlock your laptop every time you want to use it. Either omission would be sort of annoying in a laptop in this price range (we complained about the lack of keyboard backlight in <a href="https://arstechnica.com/gadgets/2022/07/review-microsofts-surface-laptop-go-2-has-a-lot-of-problems-but-i-like-it-anyway/">the $700 Surface Laptop Go 2</a> a few years ago), but to be missing&nbsp;<em>both</em> is particularly frustrating in a modern system that costs this much.</p>
<h2>Repairs and upgrades</h2>



<p>We've been inside the Framework Laptop 13 enough times that we don't do deep dives into its insides anymore, but as a new (and, in some ways, more refined) design, the Laptop 12 warrants a closer look this time around.</p>
<p>Framework's pack-in Torx screwdriver is still the only tool you need to work on the Laptop 12. Undo the eight captive screws on the bottom of the laptop, and you'll be able to lift away the entire keyboard and trackpad area to expose all of the other internal components, including the RAM, SSD, battery, and the motherboard itself.</p>
<p>The motherboard is quite a bit smaller than the Framework Laptop 13 board, and the two are definitely&nbsp;<em>not</em> interchangeable. Framework has never said otherwise, but it's worth highlighting that these are two totally separate models that will have their own distinct components and upgrade paths—that goes for parts like the speakers and battery, too.</p>

<figure>
    <p><img width="2560" height="1441" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-2048x1153.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-980x552.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-1440x811.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      Laptop 12 motherboard on top, Laptop 13 motherboard on bottom.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>As a result of that reduction in board space, the Laptop 12 can only fit a single DDR5 RAM slot, which reduces memory bandwidth and limits your RAM capacity to 48GB. It also uses shorter M.2 2230 SSDs, like the Surface lineup or the Steam Deck. Unlike a few years ago, these SSDs are now readily available at retail, and it's also easy to buy warranty-less ones on eBay or elsewhere that have been pulled from OEM systems. But they're still a bit more expensive than the more common M.2 2280 size, and you have fewer options overall.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>Framework has already published <a href="https://guides.frame.work/Guide/Framework+Laptop+12+(13th+Gen+Intel%C2%AE+Core%E2%84%A2)+DIY+Edition+Quick+Start+Guide/429?_gl=1*1dc9fwi*sg_ga4w_production_ga*NTEzNDU2MDg1LjE3NTAxODA0Mzg.*sg_ga4w_production_ga_PYG8X65YJJ*czE3NTAxODA0MzckbzEkZzEkdDE3NTAxODI3NDckajYwJGwwJGgw">a guide on setting up the DIY Edition of the laptop</a> and&nbsp;<a href="https://guides.frame.work/c/Framework_Laptop_12">a few repair guides</a> for common components. Guides for replacing bigger or more co parts, like the display or the webcam, are still listed as "coming soon."</p>
<h2>Performance and battery life</h2>
<p>I could politely describe the Laptop 12's 2.5-year-old 13th-gen Intel Core processor as "mature." This generation of Intel chips <em>has&nbsp;</em>stuck around for a lot longer than usual, to the point that Intel recently acknowledged that it has been dealing with shortages. They're appealing to PC companies because they still offer decent everyday performance for basic computing without the additional costs imposed by things like on-package memory or having some or all of the chip manufactured outside of Intel's own factories.</p>
<p>The upside of a slightly older processor is a more stable computing experience, in both Windows and Linux, since the companies and communities involved have had more time to add support and work out bugs; I had none of the sleep-and-wake issues or occasional video driver crashes I had while testing the Ryzen AI 300 version of the Framework Laptop 13.</p>


<p>The downside, of course, is that performance is pretty unexciting. These low-power U-series 12th- and 13th-gen Intel chips remain capable when it comes to day-to-day computing, but they fall far behind the likes of Intel and AMD's newer chips, Qualcomm's Snapdragon chips from the Microsoft Surface and other Copilot+ PCs, or the Apple M4 in the MacBook Air.</p>


<p>And while none of these chips are really intended for gaming laptops, the Laptop 12 isn't even a great fit for that kind of casual Steam Deck-y 3D gaming that most Framework Laptop 13 models can handle. Technically, this is the same basic Intel Iris Xe GPU that the first few generations of Framework Laptop 13 used, which is not exciting as integrated GPUs go but is at least still minimally capable. But because the Laptop 12 only has a single RAM slot instead of two, memory bandwidth is halved, which makes the GPU identify itself as "Intel UHD Graphics" to the device manager and drags down performance accordingly. (This is something these GPUs have always done, but they usually ship in systems that either have two RAM slots or soldered-down memory, so it usually doesn't come up.)</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>Framework has tuned these chips to consume the same amount of power in both the "Balanced" and "Best Performance" power modes in Windows, with a 15 W sustained power limit and a 40 W limit for shorter, bursty workloads. This keeps the laptop feeling nice and responsive for day-to-day use and helps keep a lid on power usage for battery life reasons, but it also limits its performance for extended CPU-intensive workloads like our Handbrake video encoding test.</p>

<p>The Laptop 12 takes a&nbsp;<em>lot</em> longer to accomplish these tasks than some other laptops we've tested with similar chips, either because of the lower memory bandwidth or because Best Performance mode doesn't let the chip consume a bunch of extra power. I'm not inclined to complain too much about this because it's not the kind of thing you really buy an ultraportable laptop to do, but as with light gaming, it's worth noting that the Laptop 12 doesn't hit that same "usable for these workloads in a pinch" balance that the Laptop 13 does.</p>
<figure>
    <p><img width="2048" height="1536" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008.png" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008.png 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-640x480.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-1024x768.png 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-768x576.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-1536x1152.png 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-980x735.png 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-1440x1080.png 1440w" sizes="auto, (max-width: 2048px) 100vw, 2048px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The Laptop 12's battery life is decent relative to most Laptop 13s.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>The Core i5 version of the Laptop 12 lasted around 10 hours in the PCMark Modern Office battery life test, which isn't stunning but is a step up from what the fully specced versions of the Framework Laptop 13 can offer. It will be just fine for a long flight or a full day of work or school. Our Framework reviews often complain about battery life, but I don't think it will be an issue here for most users.</p>
<h2>About that price</h2>
<p>In some ways, the Laptop 12 is trying to be a fundamentally&nbsp;<em>different</em> laptop from the Laptop 13. For all the Laptop 13's upgrades over the years, it has never had a touchscreen option, stylus support, or a convertible hinge.</p>
<p>But in most of the ways that count, the Laptop 12 is meant to be an "entry-level, lower-cost laptop," which is how Framework CEO Nirav Patel <a href="https://www.youtube.com/watch?v=Ejl-7X74tgc&amp;t=171s">has positioned it</a> in the company's announcement blog posts and videos. It features a slightly smaller, lower-resolution, less colorful screen with a lower refresh rate; a non-backlit keyboard; and considerably weaker processors. It also lacks both a fingerprint reader and a face-scanning webcam for Windows Hello.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>The issue is that these cost-cutting compromises come at a price that's a bit outside of what you'd expect of a "budget" laptop.</p>
<p>The DIY Edition of the Laptop 12 we're evaluating here—a version that ships with the Windows license and all the components you need but which you assemble yourself—will run you at least $1,176, depending on the Expansion Modules you choose for your ports. That includes 16GB of GDDR5 RAM and a 1TB M.2 2230 SSD, plus the Core i5-1334U processor option (2 P-cores, 8 E-cores). If you stepped down to a 500GB SSD instead, that's still $1,116. A pre-built edition—only available in black, but with identical specifications—would run you $1,049.</p>

<figure>
    <p><img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-1440x810.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The Laptop 13 compared to the Laptop 12. The Laptop 12 is missing quite a few quality-of-life things and has worse performance, but it isn't all that much cheaper.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>This puts the Framework Laptop 12 in the same general price range as Apple's MacBook Air, Microsoft's 13-inch Surface Laptop, and even many editions of the Framework Laptop 13. And the Laptop 12 is charming, but its day-to-day user experience falls well short of any of those devices.</p>
<p>You can make it cheaper! Say you go for the Core i3-1315U version (two P-cores, four E-cores) instead, and you buy your own 16GB stick of DDR5 RAM (roughly $50 instead of $80) and 1TB SSD ($70 or $80 for <a href="https://www.newegg.com/silicon-power-1tb/p/0D9-0021-00171?Item=9SIBDGPK454247">a decent one</a>, instead of $159). Say you have plenty of USB-C chargers at home so you don't need to pay $55 for Framework's version, and say you run Linux or ChromeOS, or you already have a Windows 11 product key, or you've brought your own Windows 11 key from one of those gray-market key selling sites (as little as $10).</p>
<p>Now we're talking about a PC that's a little under $700, which is closer to "reasonable" for a brand-new touchscreen PC. But the laptop's old CPU and poky performance also mean it's competing with a wide swath of refurbished, used, and closeout-priced older PCs from other manufacturers.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>In December, for example, I bought an SSD-less <a href="https://www.lenovo.com/us/en/p/laptops/thinkpad/thinkpadl/thinkpad-l13-yoga-gen-3-13-inch-intel/len101t0032?orgRef=https%253A%252F%252Fwww.google.com%252F&amp;srsltid=AfmBOoqjpN9S8iIG7xfG4vGA-Dv7fPWAoBj6sd6Y9oDZXN_KBVmVbRiT">Lenovo ThinkPad L13 Yoga Gen 3</a> from eBay for around $300, with around a year left on its warranty. After I'd added an SSD and reinstalled Windows—no additional cost because it had a valid Windows license already—I ended up with a PC with the same screen resolution and similar specs but with a better-quality display with smaller bezels that made the screen larger without making the laptop larger; a faster GPU configuration; a backlit keyboard; and a fingerprint reader.</p>
<p>I know it's not possible for everyone to just go out and buy a laptop like this. The boring black outline of a midrange ThinkPad is also the polar opposite of the Framework Laptop 12, but it's an example of what the tech-savvy buyer can find in the secondhand market if you're trying to find a cost-effective alternative to what Framework is offering here.</p>

<h2>A good laptop, but not a good value</h2>
<figure>
    <p><img width="2560" height="1441" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-2048x1153.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-980x552.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-1440x811.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The Framework Laptop 12.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>There are plenty of factors beyond Framework’s control that contribute to the Laptop 12’s price, starting with on-again-off-again global trade wars and the uncertainty that comes with them. There's also Framework’s status as a niche independent PC company rather than a high-volume behemoth. When you ship the number of computers that Apple does, it’s almost certainly easier to make a $999 laptop that is both premium and profitable.</p>
<p>But whatever the reason, I can’t escape the feeling that the Laptop 12 was meant to be cheaper than it has ended up being. The result is a computer with many of the compromises of an entry-level system, but without a matching entry-level price tag. It’s hard to put a price on some of the less-tangible benefits of a Framework laptop, like ease of repairs and the promise of future upgrades, but my gut feeling is that the Framework Laptop 13 falls on the “right” side of that line, and the Laptop 12 doesn't.</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<p>I am charmed by the Laptop 12. It's cute and functional, and it stands out among high-end aluminum slabs. It adds some subtle refinement to elements of the original Framework Laptop 13 design, including some things I hope end up making it into some future iteration of its design—softer corners, more color options, and an easier-to-install keyboard and trackpad. And it's far from a <em>bad</em> performer for day-to-day desktop use; it's just that the old, poky processor limits its capabilities compared to other PCs that don't cost that much more than it does.</p>
<p>I probably wouldn't recommend this over the Laptop 13 for anyone interested in what Framework is doing, unless a touchscreen is a make-or-break feature, and even then, I'd encourage people to take a good, long look at Microsoft, Lenovo, Dell, or HP's convertible offerings first. But I hope that Framework does what it's done for the Laptop 13 over the last four or so years: introduce updated components, iterate on different elements of the design, and gradually bring the price down into a more reasonable range through refurbished and factory-second parts. As a $1,000-ish computer, this leaves a lot to be desired. But as the foundation for a new Framework platform, it has enough promise to be interesting.</p>
<h3>The good</h3>
<ul>
<li>Eye-catching, colorful, friendly design that stands out among metal slabs.</li>
<li>Simple to build, repair, and upgrade.</li>
<li>Dual-plastic design over a metal frame is good for durability.</li>
<li>First convertible touchscreen in the Framework laptop.</li>
<li>Customizable ports.</li>
<li>Decent performance for everyday computing.</li>
<li>Respectable battery life.</li>
</ul>
<h3>The bad</h3>
<ul>
<li>Old, slow chip isn't really suitable for light gaming or heavy productivity work that the larger Framework Laptop 13 can do.</li>
<li>Pre-built laptop only comes in boring black.</li>
<li>Mediocre colors and large bezels spoil the screen.</li>
<li>Keyboard sometimes felt like it was missing keystrokes until I had adjusted to compensate.</li>
</ul>
<h3>The ugly</h3>
<ul>
<li>It's just too expensive for what it is. It looks and feels like a lower-cost laptop, but without a dramatically lower price than the nicer, faster Framework 13.</li>
</ul>


          
                  </div>

                  
          






  <div>
  <div>
          <p><a href="https://arstechnica.com/author/andrew_cunningham/"><img src="https://cdn.arstechnica.net/wp-content/uploads/2016/05/a.cunningham-45-1.jpg" alt="Photo of Andrew Cunningham"></a></p>
  </div>

  <div>
    

    <p>
      Andrew is a Senior Technology Reporter at Ars Technica, with a focus on consumer tech including computer hardware and in-depth reviews of operating systems like Windows and macOS. Andrew lives in Philadelphia and co-hosts a weekly book podcast called <a href="https://overduepodcast.com/">Overdue</a>.
    </p>
  </div>
</div>


  <p>
    <a href="https://arstechnica.com/gadgets/2025/06/framework-laptop-12-review-im-excited-to-see-what-the-2nd-generation-looks-like/#comments" title="30 comments">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 80"><defs><clipPath id="bubble-zero_svg__a"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath><clipPath id="bubble-zero_svg__b"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath></defs><g clip-path="url(#bubble-zero_svg__a)"><g fill="currentColor" clip-path="url(#bubble-zero_svg__b)"><path d="M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40"></path><path d="M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z"></path></g></g></svg>
    30 Comments
  </a>
      </p>
              </div>
  </article>


  


  


  <div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <a href="https://arstechnica.com/tech-policy/2025/06/trump-org-launches-47-month-wireless-service-teases-odd-499-phone/">
              <img src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-2211177567-768x432.jpg" alt="Listing image for first story in Most Read: Mocked Trump Mobile yanks coverage map that ignored Trump renaming Gulf of Mexico" decoding="async" loading="lazy">
            </a>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>
  </main>





  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Homomorphically Encrypting CRDTs (137 pts)]]></title>
            <link>https://jakelazaroff.com/words/homomorphically-encrypted-crdts/</link>
            <guid>44309520</guid>
            <pubDate>Wed, 18 Jun 2025 12:59:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jakelazaroff.com/words/homomorphically-encrypted-crdts/">https://jakelazaroff.com/words/homomorphically-encrypted-crdts/</a>, See on <a href="https://news.ycombinator.com/item?id=44309520">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-content="" data-astro-cid-onuac4el=""> 



<p>Here’s a problem with local-first software.</p>
<p>You want to work on a document together with a friend who lives far away from you.
That sounds like local-first’s bread and butter: store the document as a CRDT, then use some sort of sync server to merge updates and relay them between you and your friend.</p>
<p>But there’s a catch: the contents of that document are secret.
So secret, in fact, that <em>you don’t even want the app developer to know what they are</em>.</p>
<p>One way to solve this is end-to-end encryption.
You and your friend agree on a secret key, known only to each other.
You each use that key to encrypt your changes before sending them, decrypt them upon receipt, and no one in the middle is able to listen in.
Because the document is a CRDT, you can each still get the latest document without the sync server merging the updates.</p>
<p>That is indeed a solution, and modern browser APIs make it fairly simple to implement a basic version of it. <a href="https://plus.excalidraw.com/blog/end-to-end-encryption" data-astro-cid-bi7aps5f="">Excalidraw’s writeup of their implementation</a><a data-tooltip="" href="https://plus.excalidraw.com/blog/end-to-end-encryption" data-astro-cid-bi7aps5f=""> <img src="https://excalidraw.nyc3.cdn.digitaloceanspaces.com/lp-cms/media/Excalidraw%20blog%20-%20End-to-End%20Encryption%20in%20the%20Browser-1.png" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">Excalidraw Blog | End-to-End Encryption in the Browser</span> <span data-astro-cid-bi7aps5f="">Excalidraw introduces browser-based end-to-end encryption using Web Cryptography APIs for secure, private drawing storage.</span> <span data-astro-cid-bi7aps5f=""> <img src="https://plus.excalidraw.com/favicon.svg" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">plus.excalidraw.com/blog/end-to-end-encryption</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a> is only about 750 words — including code samples!<sup><a href="#user-content-fn-e2ee" id="user-content-fnref-e2ee" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">1</a></sup></p>
<p>Unfortunately, we’ve introduced a new problem.</p>
<p>You and your friend live far away from each other, so you tend to work while they’re sleeping and vice versa.
That was fine when the sync server could merge your changes and send you the latest document when you opened it.</p>
<p>Now, however, the server can no longer understand the changes you send.
If you want to see your friend’s latest changes, you’ll need to both be online at the same time.</p>
<p>Enter <strong>homomorphic encryption</strong>: a special form of encryption that allows a computer to <em>run programs on encrypted data without decrypting it</em>.
Using a homomorphically encrypted CRDT, a sync server could merge your friend’s and your changes into one document without ever knowing what the document contains.<sup><a href="#user-content-fn-otherways" id="user-content-fnref-otherways" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">2</a></sup></p>
<p>In this article, we’ll explore how homomorphic encryption works and build a homomorphically encrypted last write wins register CRDT.
We’ll also learn about some fundamental limitations of homomorphic encryption, and how they affect local-first software specifically.</p>
<p>I try to assume as little knowledge as possible about both encryption and CRDTs.
If you want to brush up before continuing on, my <a href="https://jakelazaroff.com/words/an-interactive-intro-to-crdts/" data-astro-cid-bi7aps5f="">Interactive Intro to CRDTs</a><a data-tooltip="" href="https://jakelazaroff.com/words/an-interactive-intro-to-crdts/" data-astro-cid-bi7aps5f=""> <img src="https://jakelazaroff.com/og/an-interactive-intro-to-crdts.png" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">An Interactive Intro to CRDTs | jakelazaroff.com</span> <span data-astro-cid-bi7aps5f="">CRDTs don't have to be all academic papers and math jargon. Learn what CRDTs are and how they work through interactive visualizations and code samples.</span> <span data-astro-cid-bi7aps5f=""> <img src="https://jakelazaroff.com/favicon.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">jakelazaroff.com/words/an-interactive-intro-to-crdts/</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a> and Jeremy Kun’s <a href="https://www.jeremykun.com/2024/05/04/fhe-overview/" data-astro-cid-bi7aps5f="">A High-Level Technical Overview of Fully Homomorphic Encryption</a><a data-tooltip="" href="https://www.jeremykun.com/2024/05/04/fhe-overview/" data-astro-cid-bi7aps5f="">  <span data-astro-cid-bi7aps5f="">A High-Level Technical Overview of Fully Homomorphic Encryption</span> <span data-astro-cid-bi7aps5f="">About two years ago, I switched teams at Google to focus on fully homomorphic encryption (abbreviated FHE, or sometimes HE). Since then I’ve got to work on a lot of interesting projects, learning along the way about post-quantum cryptography, compiler design, and the ins and outs of fully homomorphic encryption.
If you’ve heard about FHE and you’re a software person, you’ve probably heard two things: it lets you run programs directly on encrypted data without ever decrypting it; and it’s still too slow to be useful for anything.</span> <span data-astro-cid-bi7aps5f=""> <img src="https://www.jeremykun.com/favicon.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">www.jeremykun.com/2024/05/04/fhe-overview/</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a> are good places to start.</p>
<p>(Obligatory disclaimer: I am not a cryptographer!
While I’m reasonably confident that my code and advice here is generally sound, cryptography is a field in which subtle bugs and exploits can look fine to the untrained eye.
Before using anything here in an environment you’d describe with the word “production”, consult someone who works on this professionally.)</p>
<h2 id="homomorphic-hello-world">Homomorphic Hello World</h2>
<p>First, let’s look at a small code sample that uses homomorphic encryption.</p>
<p>Writing the encryption code itself from scratch would take much more code than can fit in this article.
Instead, we’ll use <a href="https://github.com/zama-ai/tfhe-rs" data-astro-cid-bi7aps5f="">THFE-rs</a><a data-tooltip="" href="https://github.com/zama-ai/tfhe-rs" data-astro-cid-bi7aps5f=""> <img src="https://opengraph.githubassets.com/0076171220c59d2546a04eccf3e34abc6618c1b595c0711a3ea1f2f2d96312ad/zama-ai/tfhe-rs" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">GitHub - zama-ai/tfhe-rs: TFHE-rs: A Pure Rust implementation of the TFHE Scheme for Boolean and Integer Arithmetics Over Encrypted Data.</span> <span data-astro-cid-bi7aps5f="">TFHE-rs: A Pure Rust implementation of the TFHE Scheme for Boolean and Integer Arithmetics Over Encrypted Data. - zama-ai/tfhe-rs</span> <span data-astro-cid-bi7aps5f=""> <img src="https://github.githubassets.com/favicons/favicon.svg" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">github.com/zama-ai/tfhe-rs</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a>, a homomorphic encryption library written in Rust.</p>
<p>The flow goes something like this:</p>
<ol>
<li>A client generates a key pair consisting of a client key and a server key.</li>
<li>The client encrypts their data using the client key and sends both the encrypted data and server key to the server.</li>
<li>The server uses the server key to perform some computation on the encrypted data and sends the result back to the client.</li>
<li>The client decrypts the result with the client key.</li>
</ol>
<p>Here’s what this looks like in code.
We’ll take two numbers — <code>clear_a</code> and <code>clear_b</code> — and add them together.
Rather than actually sending anything over a network, we’ll just use a function called <code>server_compute</code> to play the part of the server.</p>
<pre data-language="rust"><code is:raw=""><span>use</span> <span>tfhe<span>::</span>prelude<span>::</span></span><span>*</span><span>;</span>
<span>use</span> <span>tfhe<span>::</span></span><span>{</span>generate_keys<span>,</span> set_server_key<span>,</span> <span>ConfigBuilder</span><span>,</span> <span>FheUint32</span><span>,</span> <span>ServerKey</span><span>}</span><span>;</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>let</span> config <span>=</span> <span>ConfigBuilder</span><span>::</span><span>default</span><span>(</span><span>)</span><span>.</span><span>build</span><span>(</span><span>)</span><span>;</span>

    <span>// generate client and server keys</span>
    <span>let</span> <span>(</span>client_key<span>,</span> server_key<span>)</span> <span>=</span> <span>generate_keys</span><span>(</span>config<span>)</span><span>;</span>

    <span>// generate plaintext</span>
    <span>let</span> clear_a<span>:</span> <span>u32</span> <span>=</span> <span>27</span><span>;</span>
    <span>let</span> clear_b<span>:</span> <span>u32</span> <span>=</span> <span>128</span><span>;</span>

    <span>// encrypt plaintext and "send to server"</span>
    <span>let</span> result <span>=</span> <span>server_compute</span><span>(</span>
        server_key<span>,</span>
        <span>FheUint32</span><span>::</span><span>encrypt</span><span>(</span>clear_a<span>,</span> <span>&amp;</span>client_key<span>)</span><span>,</span>
        <span>FheUint32</span><span>::</span><span>encrypt</span><span>(</span>clear_b<span>,</span> <span>&amp;</span>client_key<span>)</span><span>,</span>
    <span>)</span><span>;</span>

    <span>// decrypt the result</span>
    <span>let</span> decrypted_result<span>:</span> <span>u32</span> <span>=</span> result<span>.</span><span>decrypt</span><span>(</span><span>&amp;</span>client_key<span>)</span><span>;</span>

    <span>// assert that the result is what we expect</span>
    <span>assert_eq!</span><span>(</span>decrypted_result<span>,</span> clear_a <span>+</span> clear_b<span>)</span><span>;</span>
<span>}</span>

<span>fn</span> <span>server_compute</span><span>(</span>key<span>:</span> <span>ServerKey</span><span>,</span> cipher_a<span>:</span> <span>FheUint32</span><span>,</span> cipher_b<span>:</span> <span>FheUint32</span><span>)</span> <span>-&gt;</span> <span>FheUint32</span> <span>{</span>
    <span>set_server_key</span><span>(</span>key<span>)</span><span>;</span>
    <span>return</span> cipher_a <span>+</span> cipher_b<span>;</span>
<span>}</span>
</code></pre>
<p>Get the keys, encrypt two numbers, add their ciphertexts together, decrypt the result.
Not too bad, right?</p>
<p>The simplicity is deceptive!
Rust supports operator overloading, so when we run <code>cipher_a + cipher_b</code> and both of the operands are <code>FheUint32</code>, what’s <em>really</em> happening is that TFHE-rs runs a bunch of cryptography code.</p>
<p>Before we build our homomorphically encrypted CRDT, let’s peek at what TFHE-rs is doing under the hood.</p>
<h2 id="under-the-hood">Under the Hood</h2>
<p>To start, what does it even mean to “run programs on encrypted data”?</p>
<p>In short, it means you can use encrypted data in certain math operations, and when you decrypt the data you get the result you would have gotten if you had performed the same operations with the plaintext data.
That requires an encryption scheme in which at least one of the following is true (I’ll use the notation <code>E(a)</code> to indicate the encrypted version of the plaintext <code>a</code>):</p>
<ul>
<li><code>E(a) + E(b) = E(a + b)</code>: adding the encrypted values of the plaintext numbers <code>a</code> and <code>b</code> results in the encrypted sum of the plaintext sum <code>a + b</code>.</li>
<li><code>E(a) × E(b) = E(a × b)</code>: multiplying the encrypted values of the plaintext numbers <code>a</code> and <code>b</code> results in the encrypted product of the plaintext product <code>a × b</code>.</li>
</ul>
<p>What this means is that <strong>if you add or multiply two homomorphically encrypted values, then decrypt them, <em>you get the respective sum or product of the original plaintext values</em></strong>.</p>
<p>Here’s an extremely simple example that you should absolutely never use anywhere.
First, let’s pick a number as a key.
We “encrypt” numbers by multiplying them by the key, and “decrypt” numbers by dividing them.</p>
<p>Let’s say our key is 7 and our “plaintext” numbers are 5 and 6.
We can multiply each number by our key 6 to get “encrypted” numbers of 35 and 42.
Even if someone has access to our encrypted numbers, they can’t figure out what our original plaintext numbers were without the key.</p>
<p>What they <em>can</em> do is add the encrypted numbers together.
If they give us back the sum, 77, we can divide it by our key 7 to get 11 — <em>the same result we’d get by directly adding our original numbers</em>.
Try it out by changing the numbers in the playground below:</p>
<homomorphic-addition-demo></homomorphic-addition-demo>
<p>Because it satisfies the first criterion — <code>E(a) + E(b) = E(a + b)</code> — we can say that our toy encryption scheme is homomorphic over addition.
Encryption that supports only one operation is called <em>partially homomorphic encryption</em>.
All in all, there are four different levels:</p>
<ul>
<li><strong>Partially homomorphic encryption</strong> allows only one of the two operations: <em>either</em> addition <em>or</em> multiplication, but not both.</li>
<li><strong>Somewhat homomorphic encryption</strong> and <strong>leveled homomorphic encryption</strong> allow both operations, but limit the amount of times they can be used.</li>
<li><strong>Fully homomorphic encryption</strong> allows an unlimited amount of both operations.</li>
</ul>
<p>Partially homomorphic encryption is relatively easy to implement, but has limited uses.
The word “relatively” is doing some heavy lifting here — you or I probably couldn’t come up with a partially homomorphic encryption scheme — but it’s simple enough that there are algorithms such as RSA that are accidentally homomorphic over one operation.</p>
<p>Supporting <em>more than one</em> operation is significantly more useful, but each calculation adds “noise” to the result.
Too much noise makes it impossible to decrypt.
There are two broad strategies for reducing noise: limiting the number or “depth” of operations (<em>somewhat</em> and <em>leveled</em> homomorphic encryption), and “bootstrapping”, which reduces the level of noise mid-computation (<em>fully</em> homomorphic encryption).</p>
<p>Why does it matter whether we can perform <em>both</em> addition and multiplication?</p>
<p>When we talk about doing math on encrypted data, we’re really talking about the underlying bits: the 1s and 0s that make it up.
To add and multiply the bits, we use the logical operations “exclusive or” (XOR) and “binary and” (AND), respectively.</p>
<p>Click on the switches in the playground below to toggle between 1 and 0.
You can see that the AND output is the product of its two inputs, and the XOR output is roughly the sum of its two inputs.<sup><a href="#user-content-fn-addition" id="user-content-fnref-addition" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">3</a></sup></p>
<logic-gates height="180"><switch-gate id="one-and-left" x="40" y="10"></switch-gate><switch-gate id="one-and-right" x="85" y="10"></switch-gate><and-gate id="one-and" x="55" y="70" left="#one-and-left" right="#one-and-right"></and-gate><output-gate x="63" y="140" center="#one-and"></output-gate><switch-gate id="one-xor-left" x="190" y="10"></switch-gate><switch-gate id="one-xor-right" x="235" y="10"></switch-gate><xor-gate id="one-xor" x="205" y="70" left="#one-xor-left" right="#one-xor-right"></xor-gate><output-gate x="213" y="140" center="#one-xor"></output-gate></logic-gates>
<p>This is called a <em>Boolean circuit</em> — essentially, a function that takes 1s and 0s as input and returns 1s and 0s as output.
In this context, the logical operations are called <em>logic gates</em>.</p>
<p>We can create new logic gates by combining ones we have.
Here’s how to create “inclusive or” (OR) and inverter (NOT) operations using only XOR and AND.</p>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 300 310"><rect x="1" y="71" width="143" height="230" stroke="currentColor" fill="none" rx="4" ry="4"></rect><foreignObject x="0" y="0" width="150" height="300"></foreignObject><rect x="156" y="71" width="143" height="230" stroke="currentColor" fill="none" rx="4" ry="4"></rect><foreignObject x="150" y="0" width="150" height="300"></foreignObject></svg>
<p>Once we’ve built a gate, we can then use it to build <em>yet other</em> gates.
Here’s how to make an “exclusive nor” (XNOR) using XOR and our newly-constructed NOT gate:</p>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 300 260"><rect x="61" y="1" width="143" height="250" stroke="currentColor" fill="none" rx="4" ry="4"></rect><foreignObject x="0" y="0" width="210" height="300"></foreignObject></svg>
<p>It turns out that combining just XOR and AND like this is enough to perform <em>any computation</em>!
All other logical operations can be created by combining only XOR and AND, which means that adding and multiplying the encrypted data is sufficient to simulate arbitrary Boolean logic.</p>
<p>Here’s a circuit that implements the “greater than” operator on two-bit numbers (between 0 and 3).
Using only AND, XOR and the other gates we’ve built with them, it returns 1 if the first number is greater than the second, and 0 otherwise.</p>
<p>Type in the square boxes at the top to enter the input numbers.
The two rounded boxes below each square input box are the <em>binary representation</em> of that number.</p>
<p>Don’t forget this circuit — it’ll come in handy later!</p>
<logic-gates height="365"><text-gate id="four-left" x="60"></text-gate><text-gate id="four-right" x="160"></text-gate><value-gate x="60" y="30" id="four-left-ms" center="#four-left" value-prop="leftValue"></value-gate><value-gate x="98" y="30" id="four-left-ls" center="#four-left" value-prop="rightValue"></value-gate><value-gate x="160" y="30" id="four-right-ms" center="#four-right" value-prop="leftValue"></value-gate><value-gate x="198" y="30" id="four-right-ls" center="#four-right" value-prop="rightValue"></value-gate><xnor-gate id="four-msbeq-1" x="25" y="110" left="#four-left-ms" right="#four-right-ms"></xnor-gate><not-gate id="four-msbgt-1" x="105" y="90" center="#four-right-ms"></not-gate><and-gate id="four-msbgt-2" x="105" y="150" left="#four-left-ms" right="#four-msbgt-1"></and-gate><not-gate id="four-lsbgt-1" x="200" y="90" center="#four-right-ls"></not-gate><and-gate id="four-lsbgt-2" x="200" y="150" left="#four-left-ls" right="#four-lsbgt-1"></and-gate><and-gate id="four-gt-1" x="140" y="220" left="#four-msbeq-1" right="#four-lsbgt-2"></and-gate><or-gate id="four-gt-2" x="112" y="280" left="#four-msbgt-2" right="#four-gt-1"></or-gate><output-gate x="120" y="330" center="#four-gt-2"></output-gate></logic-gates>
<p>In these examples, we’ve been looking at circuits that use plaintext 1s and 0s as their inputs and outputs.
With homomorphic encryption, the circuits operate on <em>encrypted data</em>.
Performing an AND on two encrypted bits returns another encrypted bit — and we can’t find out what it is unless we have the key.</p>
<p>So that’s how homomorphic encryption works in a nutshell.
You express your program as a Boolean circuit, and then simulate the circuit using the encrypted data as input.
The output of the circuit will be the encrypted result, which the client can then decrypt.</p>
<p>Crucially, <em>none of this reveals any sort of relationship between the plaintext values</em>.
For example, even if <code>E(a) + E(b)</code> were positive, <code>E(a + b)</code> might be negative.
Adding and multiplying ciphertext corresponds to the same operations on the underlying plaintext, but there’s no correlation between any of the ciphertext results and the underlying plaintext results — you need to decrypt the result to figure out what happened.</p>
<h2 id="a-fully-homomorphic-crdt">A Fully Homomorphic CRDT</h2>
<p>Now that we have a high level understanding of homomorphic encryption, let’s build a homomorphically-encrypted last write wins register.</p>
<p>A last write wins register holds a single value and two additional bits of metadata: a “clock” that gets incremented by one whenever the value is set, and an ID indicating the peer who last wrote to it.
Like all CRDTs, it also has a merge function that describes how it should be combined with another of the same type.</p>
<p>The last write wins register merge algorithm works like this:</p>
<ul>
<li>If the received clock is less than the local clock, the register doesn’t change its state.</li>
<li>If the received clock is greater than the local clock, the register overwrites its local value with the received value. It also stores the received clock and peer ID.</li>
<li>Ties are broken by comparing the local peer ID to the peer ID in the received state.</li>
</ul>
<p>Here’s a playground in which you can see how this algorithm works:</p>
<lwwregister-demo></lwwregister-demo>
<p>Try playing around with the latency and the network toggle.
See how updates are accepted only if the sending peer’s clock is higher than the receiving peer’s clock. If the clocks are tied, the update from the right peer will win out, since the peer ID <code>bob</code> is lexicographically greater than <code>alice</code>.</p>
<p>Okay, let’s look at some code.
First, here’s what an <em>unencrypted</em> last write wins register might look like in Rust:</p>
<pre data-language="rust"><code is:raw=""><span>const</span> <span>DATA_SIZE</span><span>:</span> <span>usize</span> <span>=</span> <span>16</span><span>;</span>

<span>pub</span> <span>struct</span> <span>Register</span> <span>{</span>
    <span>pub</span> peer<span>:</span> <span>u64</span><span>,</span>
    <span>pub</span> clock<span>:</span> <span>u64</span><span>,</span>
    <span>pub</span> value<span>:</span> <span>[</span><span>u8</span><span>;</span> <span>DATA_SIZE</span><span>]</span><span>,</span>
<span>}</span>

<span>impl</span> <span>Register</span> <span>{</span>
    <span>pub</span> <span>fn</span> <span>new</span><span>(</span>peer<span>:</span> <span>u64</span><span>)</span> <span>-&gt;</span> <span>Register</span> <span>{</span>
        <span>Register</span> <span>{</span>
            peer<span>,</span>
            clock<span>:</span> <span>0</span><span>,</span>
            value<span>:</span> <span>[</span><span>0</span><span>;</span> <span>DATA_SIZE</span><span>]</span><span>,</span>
        <span>}</span>
    <span>}</span>

    <span>pub</span> <span>fn</span> <span>set</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> peer<span>:</span> <span>u64</span><span>,</span> value<span>:</span> <span>[</span><span>u8</span><span>;</span> <span>DATA_SIZE</span><span>]</span><span>)</span> <span>{</span>
        <span>self</span><span>.</span>peer <span>=</span> peer<span>;</span>
        <span>self</span><span>.</span>clock <span>+=</span> <span>1</span><span>;</span>
        <span>self</span><span>.</span>value <span>=</span> value<span>;</span>
    <span>}</span>

    <span>pub</span> <span>fn</span> <span>set_string</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> peer<span>:</span> <span>u64</span><span>,</span> value<span>:</span> <span>&amp;</span><span>str</span><span>)</span> <span>{</span>
        <span>let</span> bytes <span>=</span> value<span>.</span><span>as_bytes</span><span>(</span><span>)</span><span>;</span>
        <span>let</span> len <span>=</span> bytes<span>.</span><span>len</span><span>(</span><span>)</span><span>.</span><span>min</span><span>(</span><span>DATA_SIZE</span><span>)</span><span>;</span>

        <span>let</span> <span>mut</span> data <span>=</span> <span>[</span><span>0</span><span>;</span> <span>DATA_SIZE</span><span>]</span><span>;</span>
        data<span>[</span><span>..</span>len<span>]</span><span>.</span><span>copy_from_slice</span><span>(</span><span>&amp;</span>bytes<span>[</span><span>..</span>len<span>]</span><span>)</span><span>;</span>

        <span>self</span><span>.</span><span>set</span><span>(</span>id<span>,</span> data<span>)</span><span>;</span>
    <span>}</span>

    <span>pub</span> <span>fn</span> <span>merge</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> other<span>:</span> <span>&amp;</span><span>Register</span><span>)</span> <span>{</span>
        <span>if</span> <span>self</span><span>.</span>clock <span>&gt;</span> other<span>.</span>clock <span>{</span>
            <span>return</span><span>;</span>
        <span>}</span><span>;</span>

        <span>if</span> <span>self</span><span>.</span>clock <span>==</span> other<span>.</span>clock <span>&amp;&amp;</span> <span>self</span><span>.</span>peer <span>&gt;</span> other<span>.</span>peer <span>{</span>
            <span>return</span><span>;</span>
        <span>}</span>

        <span>self</span><span>.</span>peer <span>=</span> other<span>.</span>peer<span>;</span>
        <span>self</span><span>.</span>clock <span>=</span> other<span>.</span>clock<span>;</span>
        <span>self</span><span>.</span>value <span>=</span> other<span>.</span>value<span>;</span>
    <span>}</span>
<span>}</span>
</code></pre>
<p>It has a peer ID, a clock and a value.
To merge with another register, it just takes the peer ID, clock and value from the register with the higher clock.
In case of a tie, it uses the peer ID as a tiebreaker.</p>
<p>Because Rust is a low-level language, we need separate functions to convert types such as strings into the raw bytes to store as the value.
We also store the value in an array with a statically-known size — although as we’ll see, that’s less of a Rust limitation than it is a fundamental constraint of homomorphic encryption.</p>
<p>Here’s the skeleton of an <code>EncryptedRegister</code> struct:</p>
<pre data-language="rust"><code is:raw=""><span>use</span> <span>core<span>::</span></span>array<span>;</span>
<span>use</span> <span>tfhe<span>::</span>prelude<span>::</span></span><span>*</span><span>;</span>
<span>use</span> <span>tfhe<span>::</span></span><span>{</span><span>ClientKey</span><span>,</span> <span>FheUint64</span><span>,</span> <span>FheUint8</span><span>}</span><span>;</span>

<span>use</span> <span>crate</span><span>::</span><span>Register</span><span>;</span>

<span>const</span> <span>DATA_SIZE</span><span>:</span> <span>usize</span> <span>=</span> <span>16</span><span>;</span>

<span>pub</span> <span>struct</span> <span>EncryptedRegister</span> <span>{</span>
    peer<span>:</span> <span>FheUint64</span><span>,</span>
    clock<span>:</span> <span>FheUint64</span><span>,</span>
    value<span>:</span> <span>[</span><span>FheUint8</span><span>;</span> <span>DATA_SIZE</span><span>]</span><span>,</span>
<span>}</span>

<span>impl</span> <span>EncryptedRegister</span> <span>{</span>
    <span>pub</span> <span>fn</span> <span>encrypt</span><span>(</span>clear<span>:</span> <span>&amp;</span><span>Register</span><span>,</span> key<span>:</span> <span>&amp;</span><span>ClientKey</span><span>)</span> <span>-&gt;</span> <span>EncryptedRegister</span> <span>{</span>
        <span>EncryptedRegister</span> <span>{</span>
            peer<span>:</span> <span>FheUint64</span><span>::</span><span>encrypt</span><span>(</span>clear<span>.</span>peer<span>,</span> key<span>)</span><span>,</span>
            clock<span>:</span> <span>FheUint64</span><span>::</span><span>encrypt</span><span>(</span>clear<span>.</span>clock<span>,</span> key<span>)</span><span>,</span>
            value<span>:</span> <span>array<span>::</span></span><span>from_fn</span><span>(</span><span><span>|</span>i<span>|</span></span> <span>FheUint8</span><span>::</span><span>encrypt</span><span>(</span>clear<span>.</span>value<span>[</span>i<span>]</span><span>,</span> key<span>)</span><span>)</span><span>,</span>
        <span>}</span>
    <span>}</span>

    <span>pub</span> <span>fn</span> <span>decrypt</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> key<span>:</span> <span>&amp;</span><span>ClientKey</span><span>)</span> <span>-&gt;</span> <span>Register</span> <span>{</span>
        <span>Register</span> <span>{</span>
            peer<span>:</span> <span>FheUint64</span><span>::</span><span>decrypt</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>peer<span>,</span> key<span>)</span><span>,</span>
            clock<span>:</span> <span>FheUint64</span><span>::</span><span>decrypt</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>clock<span>,</span> key<span>)</span><span>,</span>
            value<span>:</span> <span>array<span>::</span></span><span>from_fn</span><span>(</span><span><span>|</span>i<span>|</span></span> <span>FheUint8</span><span>::</span><span>decrypt</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>value<span>[</span>i<span>]</span><span>,</span> key<span>)</span><span>)</span><span>,</span>
        <span>}</span>
    <span>}</span>

    <span>pub</span> <span>fn</span> <span>merge</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> other<span>:</span> <span>EncryptedRegister</span><span>)</span> <span>{</span>
        <span>// ...</span>
    <span>}</span>
<span>}</span>
</code></pre>
<p>Pretty similar to the unencrypted <code>Register</code> struct!
<code>FheUint64</code> has replaced <code>u64</code>, and <code>value</code> is now an array of <code>FheUint8</code> rather than <code>u8</code>.
These are TFHE-rs types that encrypt the corresponding Rust types.
But other than that, the struct is the same.</p>
<p>The implementation has two new methods:</p>
<ul>
<li><code>encrypt</code>, which takes a normal <code>Register</code> and a client key, encrypts all the fields and returns an <code>EncryptedRegister</code>.</li>
<li><code>decrypt</code>, which takes a client key, decrypts all the fields and returns a normal <code>Register</code>.</li>
</ul>
<p>We’ve also omitted the <code>set</code> and <code>set_string</code> methods.
Since <code>EncryptedRegister</code> runs on the server, the value will never be set manually.
The only thing it needs to do is merge an incoming register with the register it has in memory.</p>
<p>Okay, so what does the <code>merge</code> method look like?</p>
<p>As we saw before, TFHE-rs overloads operators like <code>+</code> to make working with encrypted values more convenient.
For operators that don’t support overloading such as <code>&lt;</code>, TFHE-rs has methods like <code>gt</code>.</p>
<p>Given that, you might think we could write the <code>merge</code> method like this:</p>
<pre data-language="rust"><code is:raw=""><span>impl</span> <span>EncryptedRegister</span> <span>{</span>
  <span>// ...</span>

  <span>pub</span> <span>fn</span> <span>merge</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> other<span>:</span> <span>EncryptedRegister</span><span>)</span> <span>{</span>
    <span>if</span> <span>self</span><span>.</span>clock<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span> <span>{</span>
      <span>return</span><span>;</span>
    <span>}</span>

    <span>if</span> <span>self</span><span>.</span>clock<span>.</span><span>eq</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span> <span>&amp;&amp;</span> <span>self</span><span>.</span>id<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>id<span>)</span> <span>{</span>
      <span>return</span><span>;</span>
    <span>}</span>

    <span>self</span><span>.</span>id <span>=</span> other<span>.</span>id<span>;</span>
    <span>self</span><span>.</span>clock <span>=</span> other<span>.</span>clock<span>;</span>
    <span>self</span><span>.</span>value <span>=</span> other<span>.</span>value<span>;</span>
  <span>}</span>
<span>}</span>
</code></pre>
<p>This will <em>definitely not work</em>!</p>
<p>Remember that we can’t retrieve any information by operating on the encrypted data — <em>including information about the results of intermediate steps</em>.</p>
<p>To more clearly show the problem with this strategy, we can add some logging:</p>
<pre data-language="rust"><code is:raw=""><span>impl</span> <span>EncryptedRegister</span> <span>{</span>
  <span>// ...</span>

  <span>pub</span> <span>fn</span> <span>merge</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> other<span>:</span> <span>EncryptedRegister</span><span>)</span> <span>{</span>
    <span>if</span> <span>self</span><span>.</span>clock<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span> <span>{</span>
      <span>println!</span><span>(</span><span>"local clock is greater than other clock!"</span><span>)</span><span>;</span>
      <span>return</span><span>;</span>
    <span>}</span>

    <span>if</span> <span>self</span><span>.</span>clock<span>.</span><span>eq</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span> <span>&amp;&amp;</span> <span>self</span><span>.</span>peer<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>peer<span>)</span> <span>{</span>
      <span>println!</span><span>(</span><span>"clocks are equal but local peer is greater than other peer!"</span><span>)</span><span>;</span>
      <span>return</span><span>;</span>
    <span>}</span>

    <span>println!</span><span>(</span><span>"overwriting local data with remote data!"</span><span>)</span><span>;</span>
    <span>self</span><span>.</span>peer <span>=</span> other<span>.</span>peer<span>;</span>
    <span>self</span><span>.</span>clock <span>=</span> other<span>.</span>clock<span>;</span>
    <span>self</span><span>.</span>value <span>=</span> other<span>.</span>value<span>;</span>
  <span>}</span>
<span>}</span>
</code></pre>
<p>Although we still couldn’t decrypt the encrypted data, this (fake) implementation would reveal the result of the merge!
We’d know which branches our code took, and therefore learn which decrypted clock was higher and which encrypted data was written to the register.</p>
<p>Instead, our merge function must <em>eagerly</em> evaluate all branches in our code.
It also means that all loops must run for a statically-known number of iterations.
More generally, <strong>our code must always execute as though operating on the worst case input</strong>, because altering behavior based on the input would leak information about it.</p>
<p>Here’s the <em>real</em> code for our merge function:</p>
<pre data-language="rust"><code is:raw="">  <span>pub</span> <span>fn</span> <span>merge</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> other<span>:</span> <span>&amp;</span><span>EncryptedRegister</span><span>)</span> <span>{</span>
    <span>let</span> higher_clock <span>=</span> <span>self</span><span>.</span>clock<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>

    <span>let</span> equal_clock <span>=</span> <span>self</span><span>.</span>clock<span>.</span><span>eq</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>
    <span>let</span> higher_peer <span>=</span> <span>self</span><span>.</span>peer<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>peer<span>)</span><span>;</span>

    <span>let</span> keep_self <span>=</span> higher_clock <span>|</span> <span>(</span>equal_clock <span>&amp;</span> higher_peer<span>)</span><span>;</span>

    <span>self</span><span>.</span>peer <span>=</span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>peer<span>,</span> <span>&amp;</span>other<span>.</span>peer<span>)</span><span>;</span>
    <span>self</span><span>.</span>clock <span>=</span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>clock<span>,</span> <span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>
    <span>self</span><span>.</span>value <span>=</span> <span>array<span>::</span></span><span>from_fn</span><span>(</span><span><span>|</span>i<span>|</span></span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>value<span>[</span>i<span>]</span><span>,</span> <span>&amp;</span>other<span>.</span>value<span>[</span>i<span>]</span><span>)</span><span>)</span><span>;</span>
  <span>}</span>
</code></pre>
<p>Superficially, it looks fairly similar, but there are a couple of important differences.
Let’s take it line by line.</p>
<p>First, we determine whether the local clock is higher than the other clock:</p>
<pre data-language="rust"><code is:raw=""><span>let</span> higher_clock <span>=</span> <span>self</span><span>.</span>clock<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>
</code></pre>
<p>If we think back to the logic gates, we can imagine what’s going on under the hood here, right?
We built this exact circuit!
Ours only operated on two-bit numbers, but the idea was the same: accept two numbers and return a 0 or 1 indicating whether the first number is higher than the second.</p>
<p>(In our circuit, the result was a <em>plaintext</em> 0 or 1 — but remember, homomorphic encryption operates with <em>encrypted</em> values!
The <code>gt</code> method actually returns an <code>FheBool</code>: an <em>encrypted bool</em> which indicates whether the local clock is higher than the other one.)</p>
<p>If we had the client key, we could decrypt that variable and find out its true value.
We can’t do that, but we can still <em>combine it with other encrypted values</em> to write our merge algorithm.</p>
<p>Here are the conditions to break a tie between the clocks:</p>
<pre data-language="rust"><code is:raw=""><span>let</span> equal_clock <span>=</span> <span>self</span><span>.</span>clock<span>.</span><span>eq</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>
<span>let</span> higher_peer <span>=</span> <span>self</span><span>.</span>peer<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>peer<span>)</span><span>;</span>
</code></pre>
<p>Two more <code>FheBool</code>s indicating whether the clocks are equal and if the local peer ID is higher.</p>
<p>Next, we combine them:</p>
<pre data-language="rust"><code is:raw=""><span>let</span> keep_self <span>=</span> higher_clock <span>|</span> <span>(</span>equal_clock <span>&amp;</span> higher_peer<span>)</span><span>;</span>
</code></pre>
<p>This combines all those <code>FheBool</code>s to determine whether to keep the local data or overwrite it with the merged data.<sup><a href="#user-content-fn-astute" id="user-content-fnref-astute" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">4</a></sup></p>
<p>Those <code>|</code> and <code>&amp;</code> operators are bitwise AND and bitwise OR, which work exactly like the AND and OR logic gates we made earlier.
They’re similar to the logical AND and OR we’re used to — <code>&amp;&amp;</code> and <code>||</code>, but with one big difference: bitwise operators are <em>eager</em>.
Whereas logical AND and OR might skip the second expression depending on the first, bitwise operators will <em>always</em> evaluate both sides.</p>
<p>Now that we’ve determined the register values to keep — even if we can’t tell which ones — we need to write the data to the register.
Here’s the secret sauce:</p>
<pre data-language="rust"><code is:raw=""><span>self</span><span>.</span>peer <span>=</span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>peer<span>,</span> <span>&amp;</span>other<span>.</span>peer<span>)</span><span>;</span>
<span>self</span><span>.</span>clock <span>=</span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>clock<span>,</span> <span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>
<span>self</span><span>.</span>value <span>=</span> <span>array<span>::</span></span><span>from_fn</span><span>(</span><span><span>|</span>i<span>|</span></span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>value<span>[</span>i<span>]</span><span>,</span> <span>&amp;</span>other<span>.</span>value<span>[</span>i<span>]</span><span>)</span><span>)</span><span>;</span>
</code></pre>
<p>Rather than <code>if</code> or <code>match</code> expressions, we use <code>FheBool</code>’s <code>select</code> method.
It returns the first argument if the underlying <code>FheBool</code> value is <code>true</code>, or the second argument if the underlying value is <code>false</code>.</p>
<p>This is important: <em>the return value is different from both arguments</em>.
While decrypting the return value would reveal the same plaintext as one of the arguments, in ciphertext all three are distinct.
This means that we can’t tell which values we’ve set on the register by the end of the merge.</p>
<p>When the merge is done, every piece of ciphertext has changed — the peer ID, the clock and the register value.
The plaintext values might have updated (or might not have!) but there’s no way to tell by looking at the ciphertext.</p>
<p>Problem solved, right?
We can now have the server merge our CRDT without knowing what it contains?
Weeeellllllll…</p>
<h2 id="fundamental-limitations">Fundamental Limitations</h2>
<p>Homomorphic encryption has constraints that sharply limit its effectiveness with regard to local-first software.</p>
<p>For starters: encryption keys.
In both the simple adding example and the last-write wins register, we generated a key that would be passed to the server.
That only needs to happen once, but the difference between the size of our key and the size of our data can be surprising.</p>
<p>Our register took up only 32 bytes of data — 8 bytes each for the peer and clock, and 16 bytes for the value.
Meanwhile, TFHE-rs generated a <em>123 megabyte</em> server key.
We can compress the key down to about 27 megabytes, but still: that’s almost 850,000 times more key than data!</p>
<p>The payload here is particularly small, but a disparity of that size isn’t unheard of.
In his <a href="https://www.jeremykun.com/2024/05/04/fhe-overview/" data-astro-cid-bi7aps5f="">overview of fully homomorphic encryption</a><a data-tooltip="" href="https://www.jeremykun.com/2024/05/04/fhe-overview/" data-astro-cid-bi7aps5f="">  <span data-astro-cid-bi7aps5f="">A High-Level Technical Overview of Fully Homomorphic Encryption</span> <span data-astro-cid-bi7aps5f="">About two years ago, I switched teams at Google to focus on fully homomorphic encryption (abbreviated FHE, or sometimes HE). Since then I’ve got to work on a lot of interesting projects, learning along the way about post-quantum cryptography, compiler design, and the ins and outs of fully homomorphic encryption.
If you’ve heard about FHE and you’re a software person, you’ve probably heard two things: it lets you run programs directly on encrypted data without ever decrypting it; and it’s still too slow to be useful for anything.</span> <span data-astro-cid-bi7aps5f=""> <img src="https://www.jeremykun.com/favicon.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">www.jeremykun.com/2024/05/04/fhe-overview/</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a>, Jeremy Kun cites examples in which ciphertexts of dozens or hundreds of <em>kilobytes</em> require keys on the order of <em>gigabytes</em>.</p>
<p>Runtime performance is also — to put it lightly — lacking.
I benchmarked the unencrypted and encrypted versions of the last write wins register on an M4 MacBook Pro.
The unencrypted one averaged a merge time of 0.52 nanoseconds.</p>
<p>The encrypted one?
<em>1.06 seconds</em>.
That’s not a typo: the homomorphically encrypted merge is <em>two billion times slower</em>.<sup><a href="#user-content-fn-gpu" id="user-content-fnref-gpu" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">5</a></sup></p>
<p>Not great!</p>
<p>That’s not all.
We said before that our code must execute as though operating on the worst case input.
Even if the performance issues improve by many orders of magnitude, the “worst case” requirement will still impose constraints on the CRDT algorithm itself.</p>
<p>Consider a fully homomorphically encrypted last-write wins <em>map</em> CRDT.
Most maps store keys sparsely, so the map only grows in size as keys are added.</p>
<p>Here’s a playground that simulates encrypting a sparse map.<sup><a href="#user-content-fn-pretend" id="user-content-fnref-pretend" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">6</a></sup>
When you modify the plaintext map on the left, the encrypted map on the right updates.
Can you see a security issue?</p>
<encrypted-map-demo></encrypted-map-demo>
<p>Imagine you only had access to the map on the right.
You could still see data being added and removed!
Furthermore, this map lazily encrypts only the data that changes, which would allow you to see exactly which key changed (if any).</p>
<p>A homomorphically encrypted map CRDT couldn’t do that.
Since it must assume a worst-case input, it must store the keys <em>densely</em>: limiting the size to a fixed number of keys and reserving all the space up front.
Merging two identical maps would be exactly as computationally intensive as merging two maps in which <em>every</em> key was updated.<sup><a href="#user-content-fn-op" id="user-content-fnref-op" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">7</a></sup></p>
<p>The playground below simulates a homomorphically encrypted map.
While you can add and remove keys to the plaintext map on the left, the encrypted map on the right behaves as though every key is filled.
And no matter how you modify the plaintext map, <em>everything</em> in the encrypted map changes:</p>
<encrypted-map-demo dense="true" keys="4"></encrypted-map-demo>
<p>From the outside, there’s no way to tell what changed in the map: we see the exact same number of keys, and every value has changed.
To calculate the new map, the server must go through and merge <em>every single key</em>.
After that, it needs to transfer the full map to each peer — because remember, as far as it knows, the entire map is different.</p>
<p>These are fundamental limitations of homomorphic encryption!
The requirement that homomorphically encrypted code performs as though operating on the worst-case input dramatically increases both the space and time required to update.</p>
<h2 id="parting-thoughts">Parting Thoughts</h2>
<p>I started this article thinking that local-first software and homomorphic encryption would be natural bedfellows.</p>
<p>But honestly, I came away… a little less enamored.
The fundamental limitations of homomorphic encryption mean that it will always operate under a set of worst-case assumptions.
Homomorphically encrypted CRDTs aren’t intractable, but they are severely limited by these intrinsic constraints.</p>
<p>So the question remains: how can we secure local-first apps without severely degrading usability?</p>
<p>Luckily, I’m not the only one thinking about this problem!</p>

<p>CRDTs are a relatively young technology — the paper formalizing them was published in 2011 — so there’s still a lot of unexplored solution space.
We may not have solved this problem yet, but I’m confident that we’re closing in on it!</p>

<section data-footnotes="">
<ol>
<li id="user-content-fn-e2ee">
<p>More comprehensive solutions might try to implement things like <a href="https://en.wikipedia.org/wiki/Forward_secrecy" data-astro-cid-bi7aps5f="">forward secrecy</a><a data-tooltip="" href="https://en.wikipedia.org/wiki/Forward_secrecy" data-astro-cid-bi7aps5f=""> <img src="https://upload.wikimedia.org/wikipedia/commons/e/e0/KDF_chain.png" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">Forward secrecy - Wikipedia</span>  <span data-astro-cid-bi7aps5f=""> <img src="https://en.wikipedia.org/static/favicon/wikipedia.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">en.wikipedia.org/wiki/Forward_secrecy</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a>, which is absolutely not easy.
But the basic version is still better than nothing. <a href="#user-content-fnref-e2ee" data-footnote-backref="" aria-label="Back to reference 1" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-otherways">
<p>Homomorphic encryption isn’t the only way to solve this problem.
You might instead just ignore it, and have the server store every version of your encrypted document — wasteful, but it’d work!
You could also use a CRDT implementation that only requires <em>changes</em> to be sent to the server, rather than the full document. <a href="#user-content-fnref-otherways" data-footnote-backref="" aria-label="Back to reference 2" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-addition">
<p>You might notice that when both XOR inputs are 1, the result is 0.
You might also remember from math class that the result of 1 + 1 is, uh, not 0.
So how can XOR represent addition?</p>
<p>Remember that we’re operating on binary numbers — all we have is 0 and 1!
Adding to 1 in binary is like adding to 9 in decimal: since we’re out of digits, we instead roll that place back to 0 and <em>carry the 1 to the next place</em>, giving us 10.
The XOR gate represents the <em>sum digit</em> of that Boolean addition.
To fully represent the result, <a href="https://en.wikipedia.org/wiki/XOR_gate#Addition" data-astro-cid-bi7aps5f="">we’d need to use the AND gate as well to represent the <em>carry digit</em></a><a data-tooltip="" href="https://en.wikipedia.org/wiki/XOR_gate#Addition" data-astro-cid-bi7aps5f="">  <span data-astro-cid-bi7aps5f="">XOR gate - Wikipedia</span>  <span data-astro-cid-bi7aps5f=""> <img src="https://en.wikipedia.org/static/favicon/wikipedia.ico#Addition" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">en.wikipedia.org/wiki/XOR_gate#Addition</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a>. <a href="#user-content-fnref-addition" data-footnote-backref="" aria-label="Back to reference 3" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-astute">
<p><em>Truly</em> eagle-eyed readers will notice that this is <em>also</em> the same circuit we used to determine whether one two-bit number was greater than another.
At a high level, the logic there was “most significant bit is greater” OR (“most significant bits are equal” AND “least significant bit is greater”).
Here, the logic is “clock is greater” OR (“clocks are equal” AND “peer ID is greater”). <a href="#user-content-fnref-astute" data-footnote-backref="" aria-label="Back to reference 4" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-gpu">
<p>Granted, I was only able to run it on the CPU — TFHE-rs only supports GPU acceleration on Linux — but even if I could run it on the GPU, <a href="https://docs.zama.ai/tfhe-rs/get-started/summary" data-astro-cid-bi7aps5f="">TFHE-rs’s benchmarks</a><a data-tooltip="" href="https://docs.zama.ai/tfhe-rs/get-started/summary" data-astro-cid-bi7aps5f="">  <span data-astro-cid-bi7aps5f="">TFHE-rs</span>  <span data-astro-cid-bi7aps5f=""> <img src="https://docs.zama.ai/~gitbook/image?url=https%3A%2F%2F572209210-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fcollections%252FprREL84Xd1lx94uAslRx%252Ficon%252F3R1LaM67E4BE3WhJZF5p%252FLogo%2520-%2520Square.png%3Falt%3Dmedia%26token%3Db39ed5d3-5537-4c62-9389-5b23f830072b&amp;width=48&amp;height=48&amp;sign=c9d8fc58&amp;sv=2" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">docs.zama.ai/tfhe-rs/get-started/summary</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a> indicate that it would only speed things up by a factor of 3–5.
Even at the high end of that range, the encrypted merge would <em>still</em> be 400 million times slower than the unencrypted one. <a href="#user-content-fnref-gpu" data-footnote-backref="" aria-label="Back to reference 5" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-pretend">
<p>When I say “simulates encrypting”, I mean “displays a bunch of random hex digits”.
Please humor me! <a href="#user-content-fnref-pretend" data-footnote-backref="" aria-label="Back to reference 6" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-op">
<p>Note that this all assumes a <em>state-based</em> CRDT.
An <em>operation-based</em> CRDT — where the important operation is appending to a log of events rather than merging — might have a totally different set of tradeoffs. <a href="#user-content-fnref-op" data-footnote-backref="" aria-label="Back to reference 7" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-meri">
<p>Meri Leeworthy published a great <a href="https://meri.garden/a-deep-dive-explainer-on-beekem-protocol/" data-astro-cid-bi7aps5f="">deep-dive explainer on KeyHive’s key encapsulation mechanism</a><a data-tooltip="" href="https://meri.garden/a-deep-dive-explainer-on-beekem-protocol/" data-astro-cid-bi7aps5f=""> <img src="https://static.meri.garden/mesh-gradient.png" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">A deep-dive explainer on Ink and Switch's BeeKEM protocol</span> <span data-astro-cid-bi7aps5f="">I'm a programmer, designer, writer and artist. I try to make tools for community autonomy, creativity, and resistance.</span> <span data-astro-cid-bi7aps5f=""> <img src="https://meri.garden/favicon.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">meri.garden/a-deep-dive-explainer-on-beekem-protocol/</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a> which is absolutely worth a read! <a href="#user-content-fnref-meri" data-footnote-backref="" aria-label="Back to reference 8" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
</ol>
</section> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Workout.cool – Open-source fitness coaching platform (351 pts)]]></title>
            <link>https://github.com/Snouzy/workout-cool</link>
            <guid>44309320</guid>
            <pubDate>Wed, 18 Jun 2025 12:33:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Snouzy/workout-cool">https://github.com/Snouzy/workout-cool</a>, See on <a href="https://news.ycombinator.com/item?id=44309320">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#about">About</a></li>
<li><a href="#-project-origin--motivation">Project Origin &amp; Motivation</a></li>
<li><a href="#quick-start">Quick Start</a></li>
<li><a href="#exercise-database-import">Exercise Database Import</a></li>
<li><a href="#project-architecture">Project Architecture</a></li>
<li><a href="#roadmap">Roadmap</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#deployment">Deployment</a></li>
<li><a href="#resources">Resources</a></li>
<li><a href="#license">License</a></li>
<li><a href="#-sponsor-this-project">Sponsor This Project</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributors</h2><a id="user-content-contributors" aria-label="Permalink: Contributors" href="#contributors"></a></p>
<a href="https://github.com/Snouzy/workout-cool/graphs/contributors">
  <img src="https://camo.githubusercontent.com/7c851db85ae8783e22cf0dabfd6e6cce46c6c360fed141a0f287fa87d478c4f7/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d536e6f757a792f776f726b6f75742d636f6f6c" data-canonical-src="https://contrib.rocks/image?repo=Snouzy/workout-cool">
</a>
<p dir="auto"><h2 tabindex="-1" dir="auto">About</h2><a id="user-content-about" aria-label="Permalink: About" href="#about"></a></p>
<p dir="auto">A comprehensive fitness coaching platform that allows create workout plans for you, track progress, and access a vast exercise database with
detailed instructions and video demonstrations.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🎯 Project Origin &amp; Motivation</h2><a id="user-content--project-origin--motivation" aria-label="Permalink: 🎯 Project Origin &amp; Motivation" href="#-project-origin--motivation"></a></p>
<p dir="auto">This project was born from a personal mission to revive and improve upon a previous fitness platform. As the <strong>primary contributor</strong> to the
original <a href="https://github.com/workout-lol/workout-lol">workout.lol</a> project, I witnessed its journey and abandonment. 🥹</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Story Behind <strong><em>workout.cool</em></strong></h3><a id="user-content-the-story-behind-workoutcool" aria-label="Permalink: The Story Behind workout.cool" href="#the-story-behind-workoutcool"></a></p>
<ul dir="auto">
<li>🏗️ <strong>Original Contributor</strong>: I was the main contributor to workout.lol</li>
<li>💼 <strong>Business Challenges</strong>: The original project faced major hurdles with exercise video partnerships (no reliable video provider) could
be established</li>
<li>💰 <strong>Project Sale</strong>: Due to these partnership issues, the project was sold to another party</li>
<li>📉 <strong>Abandonment</strong>: The new owner quickly realized that <strong>exercise video licensing costs were prohibitively expensive</strong>, began to be sick
and abandoned the entire project</li>
<li>🔄 <strong>Revival Attempts</strong>: For the past <strong>9 months</strong>, I've been trying to reconnect with the new stakeholder</li>
<li>📧 <strong>Radio Silence</strong>: Despite multiple (15) attempts, there has been no response</li>
<li>🚀 <strong>New Beginning</strong>: Rather than let this valuable work disappear, I decided to create a fresh, modern implementation</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why <strong><em>workout.cool</em></strong> Exists</h3><a id="user-content-why-workoutcool-exists" aria-label="Permalink: Why workout.cool Exists" href="#why-workoutcool-exists"></a></p>
<p dir="auto"><strong>Someone had to step up.</strong></p>
<p dir="auto">The opensource fitness community deserves better than broken promises and abandoned platforms.</p>
<p dir="auto">I'm not building this for profit.</p>
<p dir="auto">This isn't just a revival : it's an evolution. <strong>workout.cool</strong> represents everything the original project could have been, with the
reliability, modern approach, and <strong>maintenance</strong> that the fitness open source community deserves.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">👥 From the Community, For the Community</h2><a id="user-content--from-the-community-for-the-community" aria-label="Permalink: 👥 From the Community, For the Community" href="#-from-the-community-for-the-community"></a></p>
<p dir="auto"><strong>I'm not just a developer : I'm a user who refused to let our community down.</strong></p>
<p dir="auto">I experienced firsthand the frustration of watching a beloved tool slowly disappear. Like many of you, I had workouts saved, progress
tracked, and a routine built around the platform.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">My Mission: Rescue &amp; Revive.</h3><a id="user-content-my-mission-rescue--revive" aria-label="Permalink: My Mission: Rescue &amp; Revive." href="#my-mission-rescue--revive"></a></p>
<p dir="auto"><em>If you were part of the original workout.lol community, welcome back! If you're new here, welcome to the future of fitness platform
management.</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prerequisites</h3><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<ul dir="auto">
<li>Node.js 18+</li>
<li>Either:
<ul dir="auto">
<li>Docker</li>
<li>OR PostgreSQL external database</li>
</ul>
</li>
<li>pnpm (recommended) or npm</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Clone the repository</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/Snouzy/workout-cool.git
cd workout-cool"><pre>git clone https://github.com/Snouzy/workout-cool.git
<span>cd</span> workout-cool</pre></div>
</li>
<li>
<p dir="auto"><strong>Install dependencies</strong></p>

</li>
<li>
<p dir="auto"><strong>Set up environment variables</strong></p>

<p dir="auto">Fill in your database URL and other required environment variables:</p>
<div dir="auto" data-snippet-clipboard-copy-content="DATABASE_URL=&quot;postgresql://username:password@localhost:5432/workout_cool&quot;
BETTER_AUTH_SECRET=&quot;your-secret-key&quot;
# ... other variables"><pre><span>DATABASE_URL</span><span>=</span><span><span>"</span>postgresql://username:password@localhost:5432/workout_cool<span>"</span></span>
<span>BETTER_AUTH_SECRET</span><span>=</span><span><span>"</span>your-secret-key<span>"</span></span>
<span><span>#</span> ... other variables</span></pre></div>
</li>
<li>
<p dir="auto"><strong>Set up the database</strong></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Option 1: Using Docker</h4><a id="user-content-option-1-using-docker" aria-label="Permalink: Option 1: Using Docker" href="#option-1-using-docker"></a></p>
<p dir="auto">The project provides a convenient <code>make</code> command that handles everything:</p>

<p dir="auto">This single command will:</p>
<ul dir="auto">
<li>Start the PostgreSQL database using Docker</li>
<li>Run database migrations</li>
<li>Start the development server</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Option 2: Manual PostgreSQL Setup</h4><a id="user-content-option-2-manual-postgresql-setup" aria-label="Permalink: Option 2: Manual PostgreSQL Setup" href="#option-2-manual-postgresql-setup"></a></p>
<p dir="auto">If you prefer to use your own PostgreSQL installation:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run migrations
npx prisma migrate deploy
npx prisma generate

# Start the development server
pnpm dev"><pre><span><span>#</span> Run migrations</span>
npx prisma migrate deploy
npx prisma generate

<span><span>#</span> Start the development server</span>
pnpm dev</pre></div>
</li>
<li>
<p dir="auto"><strong>Open your browser</strong> Navigate to <a href="http://localhost:3000/" rel="nofollow">http://localhost:3000</a></p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Exercise Database Import</h2><a id="user-content-exercise-database-import" aria-label="Permalink: Exercise Database Import" href="#exercise-database-import"></a></p>
<p dir="auto">The project includes a comprehensive exercise database. To import a sample of exercises:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prerequisites for Import</h3><a id="user-content-prerequisites-for-import" aria-label="Permalink: Prerequisites for Import" href="#prerequisites-for-import"></a></p>
<ol dir="auto">
<li><strong>Prepare your CSV file</strong></li>
</ol>
<p dir="auto">Your CSV should have these columns:</p>
<div data-snippet-clipboard-copy-content="id,name,name_en,description,description_en,full_video_url,full_video_image_url,introduction,introduction_en,slug,slug_en,attribute_name,attribute_value"><pre><code>id,name,name_en,description,description_en,full_video_url,full_video_image_url,introduction,introduction_en,slug,slug_en,attribute_name,attribute_value
</code></pre></div>
<p dir="auto">You can use the provided example.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Import Commands</h3><a id="user-content-import-commands" aria-label="Permalink: Import Commands" href="#import-commands"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Import exercises from a CSV file
pnpm run import:exercises-full /path/to/your/exercises.csv

# Example with the provided sample data
pnpm run import:exercises-full ./data/sample-exercises.csv"><pre><span><span>#</span> Import exercises from a CSV file</span>
pnpm run import:exercises-full /path/to/your/exercises.csv

<span><span>#</span> Example with the provided sample data</span>
pnpm run import:exercises-full ./data/sample-exercises.csv</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">CSV Format Example</h3><a id="user-content-csv-format-example" aria-label="Permalink: CSV Format Example" href="#csv-format-example"></a></p>
<div data-snippet-clipboard-copy-content="id,name,name_en,description,description_en,full_video_url,full_video_image_url,introduction,introduction_en,slug,slug_en,attribute_name,attribute_value
157,&quot;Fentes arrières à la barre&quot;,&quot;Barbell Reverse Lunges&quot;,&quot;<p>Stand upright...</p>&quot;,&quot;<p>Stand upright...</p>&quot;,https://youtube.com/...,https://img.youtube.com/...,slug-fr,slug-en,TYPE,STRENGTH
157,&quot;Fentes arrières à la barre&quot;,&quot;Barbell Reverse Lunges&quot;,&quot;<p>Stand upright...</p>&quot;,&quot;<p>Stand upright...</p>&quot;,https://youtube.com/...,https://img.youtube.com/...,slug-fr,slug-en,PRIMARY_MUSCLE,QUADRICEPS"><pre lang="csv"><code>id,name,name_en,description,description_en,full_video_url,full_video_image_url,introduction,introduction_en,slug,slug_en,attribute_name,attribute_value
157,"Fentes arrières à la barre","Barbell Reverse Lunges","&lt;p&gt;Stand upright...&lt;/p&gt;","&lt;p&gt;Stand upright...&lt;/p&gt;",https://youtube.com/...,https://img.youtube.com/...,slug-fr,slug-en,TYPE,STRENGTH
157,"Fentes arrières à la barre","Barbell Reverse Lunges","&lt;p&gt;Stand upright...&lt;/p&gt;","&lt;p&gt;Stand upright...&lt;/p&gt;",https://youtube.com/...,https://img.youtube.com/...,slug-fr,slug-en,PRIMARY_MUSCLE,QUADRICEPS
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Available Attribute Types</h3><a id="user-content-available-attribute-types" aria-label="Permalink: Available Attribute Types" href="#available-attribute-types"></a></p>
<ul dir="auto">
<li><strong>TYPE</strong>: <code>STRENGTH</code>, <code>CARDIO</code>, <code>PLYOMETRICS</code>, <code>STRETCHING</code>, etc.</li>
<li><strong>PRIMARY_MUSCLE</strong>: <code>QUADRICEPS</code>, <code>CHEST</code>, <code>BACK</code>, <code>SHOULDERS</code>, etc.</li>
<li><strong>SECONDARY_MUSCLE</strong>: Secondary muscle groups targeted</li>
<li><strong>EQUIPMENT</strong>: <code>BARBELL</code>, <code>DUMBBELL</code>, <code>BODYWEIGHT</code>, <code>MACHINE</code>, etc.</li>
<li><strong>MECHANICS_TYPE</strong>: <code>COMPOUND</code>, <code>ISOLATION</code></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Project Architecture</h2><a id="user-content-project-architecture" aria-label="Permalink: Project Architecture" href="#project-architecture"></a></p>
<p dir="auto">This project follows <strong>Feature-Sliced Design (FSD)</strong> principles with Next.js App Router:</p>
<div data-snippet-clipboard-copy-content="src/
├── app/ # Next.js pages, routes and layouts
├── processes/ # Business flows (multi-feature)
├── widgets/ # Composable UI with logic (Sidebar, Header)
├── features/ # Business units (auth, exercise-management)
├── entities/ # Domain entities (user, exercise, workout)
├── shared/ # Shared code (UI, lib, config, types)
└── styles/ # Global CSS, themes"><pre><code>src/
├── app/ # Next.js pages, routes and layouts
├── processes/ # Business flows (multi-feature)
├── widgets/ # Composable UI with logic (Sidebar, Header)
├── features/ # Business units (auth, exercise-management)
├── entities/ # Domain entities (user, exercise, workout)
├── shared/ # Shared code (UI, lib, config, types)
└── styles/ # Global CSS, themes
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Architecture Principles</h3><a id="user-content-architecture-principles" aria-label="Permalink: Architecture Principles" href="#architecture-principles"></a></p>
<ul dir="auto">
<li><strong>Feature-driven</strong>: Each feature is independent and reusable</li>
<li><strong>Clear domain isolation</strong>: <code>shared</code> → <code>entities</code> → <code>features</code> → <code>widgets</code> → <code>app</code></li>
<li><strong>Consistency</strong>: Between business logic, UI, and data layers</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example Feature Structure</h3><a id="user-content-example-feature-structure" aria-label="Permalink: Example Feature Structure" href="#example-feature-structure"></a></p>
<div data-snippet-clipboard-copy-content="features/
└── exercise-management/
├── ui/ # UI components (ExerciseForm, ExerciseCard)
├── model/ # Hooks, state management (useExercises)
├── lib/ # Utilities (exercise-helpers)
└── api/ # Server actions or API calls"><pre><code>features/
└── exercise-management/
├── ui/ # UI components (ExerciseForm, ExerciseCard)
├── model/ # Hooks, state management (useExercises)
├── lib/ # Utilities (exercise-helpers)
└── api/ # Server actions or API calls
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Roadmap</h2><a id="user-content-roadmap" aria-label="Permalink: Roadmap" href="#roadmap"></a></p>
<p dir="auto">Here are the next steps and goals for Workout.cool:</p>
<ul>
<li> 🏋️‍♂️ Add new exercises and videos</li>
<li> 📱 Mobile app (React Native)</li>
<li> 🏆 Badges and gamification system</li>
<li> 📊 Advanced progress statistics</li>
<li> 🤝 Integration with wearables (watches, trackers)</li>
<li> 🌍 Multilingual support</li>
<li> 🔒 OAuth authentication (Google, Apple, etc.)</li>
<li> 💬 Built-in community forum</li>
</ul>
<p dir="auto">Feel free to suggest your ideas via <a href="https://github.com/Snouzy/workout-cool/issues">issues</a>!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">We welcome contributions! Please see our <a href="https://github.com/Snouzy/workout-cool/blob/main/CONTRIBUTING.md">Contributing Guide</a> for details.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Development Workflow</h3><a id="user-content-development-workflow" aria-label="Permalink: Development Workflow" href="#development-workflow"></a></p>
<ol dir="auto">
<li>Fork the repository</li>
<li>Create a feature branch (<code>git checkout -b feature/amazing-feature</code>)</li>
<li>Make your changes</li>
<li>Commit your changes (<code>git commit -m 'feat: add amazing feature'</code>)</li>
<li>Push to the branch (<code>git push origin feature/amazing-feature</code>)</li>
<li>Open a Pull Request</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Code Style</h3><a id="user-content-code-style" aria-label="Permalink: Code Style" href="#code-style"></a></p>
<ul dir="auto">
<li>Follow TypeScript best practices</li>
<li>Use Feature-Sliced Design architecture</li>
<li>Write meaningful commit messages</li>
<li>Add tests for new features</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Deployment</h2><a id="user-content-deployment" aria-label="Permalink: Deployment" href="#deployment"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using Docker (Not ready yet : todo)</h3><a id="user-content-using-docker-not-ready-yet--todo" aria-label="Permalink: Using Docker (Not ready yet : todo)" href="#using-docker-not-ready-yet--todo"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Build the Docker image
docker build -t workout-cool .

# Run the container
docker run -p 3000:3000 workout-cool"><pre><span><span>#</span> Build the Docker image</span>
docker build -t workout-cool <span>.</span>

<span><span>#</span> Run the container</span>
docker run -p 3000:3000 workout-cool</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Manual Deployment</h3><a id="user-content-manual-deployment" aria-label="Permalink: Manual Deployment" href="#manual-deployment"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Build the application
pnpm build

# Run database migrations
export DATABASE_URL=&quot;your-production-db-url&quot;
npx prisma migrate deploy

# Start the production server
pnpm start"><pre><span><span>#</span> Build the application</span>
pnpm build

<span><span>#</span> Run database migrations</span>
<span>export</span> DATABASE_URL=<span><span>"</span>your-production-db-url<span>"</span></span>
npx prisma migrate deploy

<span><span>#</span> Start the production server</span>
pnpm start</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Resources</h2><a id="user-content-resources" aria-label="Permalink: Resources" href="#resources"></a></p>
<ul dir="auto">
<li><a href="https://feature-sliced.design/" rel="nofollow">Feature-Sliced Design</a></li>
<li><a href="https://nextjs.org/docs" rel="nofollow">Next.js Documentation</a></li>
<li><a href="https://www.prisma.io/docs/" rel="nofollow">Prisma Documentation</a></li>
<li><a href="https://github.com/better-auth/better-auth">Better Auth</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the MIT License. See the <a href="https://github.com/Snouzy/workout-cool/blob/main/LICENSE">LICENSE</a> file for details.</p>
<p dir="auto"><a href="https://github.com/Snouzy/workout-cool/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/28f4d479bf0a9b033b3a3b95ab2adc343da448a025b01aefdc0fbc7f0e169eb8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d677265656e2e737667" alt="MIT License" data-canonical-src="https://img.shields.io/badge/License-MIT-green.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🤝 Join the Rescue Mission</h2><a id="user-content--join-the-rescue-mission" aria-label="Permalink: 🤝 Join the Rescue Mission" href="#-join-the-rescue-mission"></a></p>
<p dir="auto"><strong>This is about rebuilding what we lost, together.</strong></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How You Can Help</h3><a id="user-content-how-you-can-help" aria-label="Permalink: How You Can Help" href="#how-you-can-help"></a></p>
<ul dir="auto">
<li>🌟 <strong>Star this repo</strong> to show the world our community is alive and thriving</li>
<li>🐛 <strong>Report issues</strong> you find. I'm listening to every single one</li>
<li>💡 <strong>Share your feature requests</strong> finally, someone who will actually implement them !</li>
<li>🔄 <strong>Spread the word</strong> to fellow fitness enthusiasts who lost hope</li>
<li>🤝 <strong>Contribute code</strong> if you're a developer : let's build this together</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">💖 Sponsor This Project</h2><a id="user-content--sponsor-this-project" aria-label="Permalink: 💖 Sponsor This Project" href="#-sponsor-this-project"></a></p>
<p dir="auto">Appear in the README and on the website as supporter by donating:</p>
<p><a href="https://ko-fi.com/workoutcool" rel="nofollow">
    <img src="https://camo.githubusercontent.com/70e2ef5e0263b261f9a2a314bb1d6919d1d43292eed117fe8fc766a68c7d96ea/68747470733a2f2f6b6f2d66692e636f6d2f696d672f676974687562627574746f6e5f736d2e737667" alt="Sponsor on Ko-fi" data-canonical-src="https://ko-fi.com/img/githubbutton_sm.svg">
  </a>
  &nbsp;&nbsp;&nbsp;
  
  
</p>
<p dir="auto">
  <em>If you believe in open-source fitness tools and want to help this project thrive,<br>
  consider buying me a coffee ☕ or sponsoring the continued development.</em>
</p>
<p dir="auto">
  Your support helps cover hosting costs, exercise database updates, and continuous improvement.<br>
  Thank you for keeping <strong>workout.cool</strong> alive and evolving 💪
</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is There a Half-Life for the Success Rates of AI Agents? (136 pts)]]></title>
            <link>https://www.tobyord.com/writing/half-life</link>
            <guid>44308711</guid>
            <pubDate>Wed, 18 Jun 2025 10:53:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tobyord.com/writing/half-life">https://www.tobyord.com/writing/half-life</a>, See on <a href="https://news.ycombinator.com/item?id=44308711">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
          <div data-collection-id="605a132f7a36283cfa0c4af9" data-edit-main-image="Banner">
            <p><h2>Writing</h2></p>
            
          </div>
            <section data-content-field="main-content" data-collection-id="605a132f7a36283cfa0c4af9" data-edit-main-image="Banner">  
  <article id="post-681b2734c79dc21300b23894" data-item-id="681b2734c79dc21300b23894">
    
    <!--POST TILE-->

    

    

    <!--MAIN CONTENT-->

    <div data-layout-label="Post Body" data-type="item" data-updated-on="1746610169593" id="item-681b2734c79dc21300b23894"><div data-block-type="2" id="block-84469d78d64003385a03">
  <p>Building on the recent empirical work of Kwa et al. (2025), I show that within their suite of research-engineering tasks the performance of AI agents on longer-duration tasks can be explained by an extremely simple mathematical model — a constant rate of failing during each minute a human would take to do the task. This implies an exponentially declining success rate with the length of the task and that each agent could be characterised by its own half-life. This empirical regularity allows us to estimate the success rate for an agent at different task lengths. And the fact that this model is a good fit for the data is suggestive of the underlying causes of failure on longer tasks — that they involve increasingly large sets of subtasks where failing any one fails the task. Whether this model applies more generally on other suites of tasks is unknown and an important subject for further work.</p><p><strong>METR’s results on the length of tasks agents can reliably complete</strong></p><p>A recent paper by <a href="https://doi.org/10.48550/arXiv.2503.14499"><span>Kwa et al. (2025)</span></a> from the research organisation <a href="https://metr.org/" target="_blank">METR</a> has found an exponential trend in the duration of the tasks that frontier AI agents can solve: every 7 months, the length of task they can solve doubles.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1746614245989_8461">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png" data-image-dimensions="1926x1070" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png" width="1926" height="1070" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1746614245989_8778">

<p>These headline results are based on&nbsp; a test suite of 170 software engineering, cybersecurity, general reasoning, and ML tasks that they assembled to be indicative of the kinds of tasks that could help AI agents assist in AI research. These tasks are assembled from three different benchmarks that take different amounts of time for humans to achieve:</p>




















  
  



</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1746614245989_11913">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png" data-image-dimensions="1820x1028" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png" width="1820" height="1028" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1746614245989_12229">
  <p>In general, ability to perform a task drops off as its duration increases, so they use the AI agent’s performance on tasks of different lengths to estimate the task-length at which the model would have a 50% success rate. They then showed that this length has been doubling every 7 months as the capabilities of frontier agents improve. The task-lengths are measured by how long it took humans to solve the same tasks.</p><p>They used 50% success rate as their chief performance threshold because it is the easiest level to robustly estimate. They are well aware that for many tasks, the required success rate for useful work may be much higher — such as 80%, 99%, or 99.9999%. They do measure the 80% success rate and find their mean estimate to have a doubling time of 213 days, compared to 212 days of the 50% rate. These are close to identical within their margin of error (±40 days for the 50% rate), so they conclude that the particular threshold doesn’t seem to have much effect on their headline result about the rate of improvement.</p><p>But there is quite a gap between the 50% success rate time-horizon and the 80% success rate time horizon. For the best model (Claude 3.7 Sonnet) it could achieve a 50% success rate on tasks up to 59 minutes <em>vs</em> only 15 minutes if an 80% success rate was required. If those results generalise to the other models, then we could also see it like this: the task length for an 80% success rate is 1/4 the task length for a 50% success rate. Or in terms of improvement: what is doable with a 50% success rate now is doable with an 80% success rate in 14 months’ time (= 2 doubling times).</p><p>The idea of measuring improvement in AI capabilities over time via time horizons at a chosen success rate is novel and interesting. AI forecasting is often hamstrung by the lack of a good measure for the y-axis of performance over time. We can track progress within a particular benchmark, but these are often solved in a couple of years, and we lack a good measure of underlying capability that can span multiple benchmarks. METR’s measure allows comparisons between very different kinds of tasks in a common currency (time it takes a human) and shows a strikingly clear trend line — suggesting it is measuring something real.</p><p>But there are grounds for criticism too. In particular, there is room to wonder how much these results generalise outside of this kind of task suite. We know that there are some tasks humans can do very quickly that AIs can’t solve (e.g. some simple spatial reasoning or intuitive physics tasks) and others that would take humans an extremely long time, but AIs can do quickly (e.g. rote mathematics). So a simple measure of ‘time it would take a human’ cannot explain all AI capability improvements. Kwa et al. are&nbsp; aware of this and even list several other ways that this task-suite may be non-representative of real-world performance including:</p><ul data-rte-list="default"><li><p>All tasks were automatically scorable (a domain where RL works best)</p></li><li><p>No interaction with other agents</p></li><li><p>Lax resource constraints</p></li></ul><p>For the purposes of this essay, we will take the data for what it is (performance on a particular task suite that may or may not generalise further) and explore underlying mechanisms that could explain it.</p><h3><strong>Explaining these results via a constant hazard rate</strong></h3><p>These results call out for some explanation of what is going on. For example, exactly how does the time horizon shrink as the required success probability is increased? And what does it <em>mean</em> for an agent to be able to perform an 8-hour task, but not a 16-hour task. Isn’t a 16-hour task just one 8-hour task after another?</p><p><a href="https://en.wikipedia.org/wiki/Survival_analysis" target="_blank">Survival analysis</a> is the field of understanding how the probability of something failing increases as a function of time. It tracks the survival probability at a time <em>S</em>(<em>t</em>) — that is, the chance it still hasn’t failed by that point. The simplest model in survival analysis is a constant hazard rate. This means that the chance of something failing in the next step (conditional on making it that far) is constant. A constant hazard rate leads to an exponentially declining survival curve. This behaviour is well-known from phenomena like radioactive decay, where there is a constant chance of decay at any moment, leading to an exponentially declining chance of the isotope’s survival over time, which is often measured by a half-life.</p><p>If AI agent success-rates drop off with task length in this manner, then the 50% success rate time-horizon for each agent from Kwa et al. is precisely the <em>half-life</em> of that agent. As with the half-life of a radioisotope, this isn’t just the median lifespan, it is the median remaining lifespan starting at any time — something that is only possible for an exponential survival curve. Unlike for particles, this AI agent half-life would be measured not in clock time, but in how long it takes a human to complete the task.</p><p>One rationale for this constant hazard rate model for AI agents is that tasks require getting past a series of steps each of which could end your attempt, with the longer the duration of the task, the more such steps. More precisely, if tasks could be broken down into a long sequence of equal-length subtasks with a constant (and independent) chance of failure, such that to succeed in the whole task, the agent needs to succeed in <em>all</em> subtasks, then that would create an exponential survival curve. I.e. when Pr(<em>Task</em>) = Pr(<em>Subtask</em>$_1$ &amp; <em>Substask</em>$_2$ &amp; … &amp; <em>Subtask</em>$_N$).</p><p>But we don’t have to assume a perfect breakdown of the task into equal-length-equal-difficulty subtasks in order to get an exponential distribution (and corresponding constant hazard rate). We can also think of the constant hazard rate model as being agnostic as to how the task is broken down, just saying that the chance of succeeding in a subtask of duration <em>t</em> is always equal to the chance of succeeding in each of a set of smaller subtasks whose combined duration is also <em>t</em>. So on this model, it doesn’t matter what level of granularity you assess the task at — whether you see it as one 60-minute task, six 10-minute tasks, or a 20-minute task plus forty 1-minute tasks — the chance of succeeding is set by the total time it would take a human to complete it.</p><p>This constant hazard rate model would predict that the time horizon for an 80% success rate is about ⅓ of the time horizon for a 50% success rate. This is because the chance of surviving three periods with an 80% success rate = (0.8)$^3$ = 0.512 ≈ 50%. More precisely, the time horizon for a success probability of <em>p</em> would be ln(<em>p</em>)/ln(<em>q</em>) times as long as one with success probability <em>q</em>. So an 80% time-horizon would be ln(0.8)/ln(0.5) = 0.322 times as long as the 50% time-horizon. Kwa et al. estimate the 80% time-horizon for Claude 3.7 Sonnet to be 0.25 as long, which is close to this theoretical estimate and within the margin of error given the noisiness of the results. The following chart shows how these numbers relate to the exponential survival curve (where <em>T</em>$_{50}$ is the 50% time-horizon etc.).</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1746614245989_24919">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png" data-image-dimensions="2427x1731" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png" width="2427" height="1731" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1746614245989_25236">
  <p>Here are some useful comparisons for how the predicted time horizons over which an agent could get very high success rates compare to the measured time horizon for a 50% success rate:</p><p><em>&nbsp; T</em>$_{80}$ ≈ 1/3&nbsp;<em>T</em>$_{50}$<br>&nbsp;&nbsp;<em>T</em>$_{90}$ ≈ 1/7&nbsp;<em>T</em>$_{50}$<br>&nbsp;&nbsp;<em>T</em>$_{99}$ ≈ 1/70 <em>T</em>$_{50}$<br>&nbsp;&nbsp;<em>T</em>$_{99.9}$ ≈ 1/700 <em>T</em>$_{50}$<em><br>&nbsp; [and each additional ‘nine’ of reliability beyond this divides the time horizon by 10]</em></p><p>We can also use this model to calculate how long we’d expect it to take between the 50% success rate time horizon reaching a given length and a high success rate time horizon reaching that some length (on the assumption of the 7-month doubling time and the constant hazard rate model): </p><p><em>&nbsp; T</em>$_{80}$ reaches any particular length about 1 year after <em>T</em>$_{50}$ does<br>&nbsp;&nbsp;<em>T</em>$_{90}$ reaches any particular length about 2 years after <em>T</em>$_{50}$ does<br>&nbsp;&nbsp;<em>T</em>$_{99}$ reaches any particular length about 4 years after <em>T</em>$_{50}$ does<br>&nbsp;&nbsp;<em>T</em>$_{99.9}$ reaches any particular length about 6 years after <em>T</em>$_{50}$ does<br>&nbsp;&nbsp;<em>[each additional ‘nine’ of reliability requires 2 more years…]</em></p><p>Kwa et al. also attempt to fit a relationship between the success probability and time-horizon (see the figure below, which I’ve adapted from their paper). They plot the time horizon on a log scale and note that this reveals a sigmoid-shaped decay curve of success rate (the coloured bars). They show that this is reasonably well fit by a logistic function (the black curves). The paper doesn’t compare this to how well alternative functions would fit the data.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1746614245989_26796">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png" data-image-dimensions="2503x1718" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png" width="2503" height="1718" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1746614245989_27113">
  <p>The data is also well-fit by an exponential function, which also looks like a sigmoid when plotted on this logarithmic x-axis. I’ve added this to the above figure (from Kwa et al.) in the form of the dotted blue curves.&nbsp; It fits the data better for some models (such as the top left) and worse on others. It fits the data roughly as well overall, while being substantially more likely <em>a priori</em> — exponential decay is <em>the </em>simplest survival curve and has only one free parameter instead of two. Moreover, while logistic functions are quite simple and natural, the black curve here is not really a logistic distribution, but the more complex log-logistic distribution which merely looks like a logistic distribution when plotted on a logarithmic x-axis.</p><p>The paper also plots the survival curve for <em>human</em> performance over increasing time periods:</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1746614245989_28357">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png" data-image-dimensions="2148x760" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png" width="2148" height="760" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1746614245989_28674">
  <p>Intriguingly, this human survival curve seems to be noticeably better than a constant hazard rate (i.e. the chance of succeeding over long timescales drops off more slowly for humans than the constant hazard rate predicts). For example, on this graph, humans had about a 50% success rate at the 1.5-hour mark, which suggests 25% at 3 hours, 12.5% at 6 hours and 6.25% at 12 hours if the hazard rate is constant. However, the humans were still above 20% success rate at that point.&nbsp;</p><p>This could indicate a different scaling behaviour of success rate with time horizon for humans compared to AI agents, which would be well worth investigating and may suggest important underlying mechanisms (e.g. that the humans were better at correcting earlier failed subtasks). If human performance scales differently with task length than AI agent performance, that would be an important result, suggesting that there is a notable inefficiency in the current AI paradigm. This warrants further research.&nbsp;</p><p>However, there are other potential explanations. For instance, it could also be an artefact of this graph being an aggregate of humans with different ability levels, since even if all individual humans have a constant hazard rate (and so each have an exponential survival curve) a mixture of different humans would be a weighted sum of exponentials with different time constants and that distribution decays slower than an exponential (see <a href="https://arxiv.org/abs/2308.09045"><span>Ord (2023)</span></a> for details).</p><p>The AI agents don’t suffer from mixing different capability levels into the same statistics because there is a separate data series for each agent. However, there is a similar effect that could still be present in the measurement of AI agents’ performance over increasingly long tasks. It is plausible that some tasks are inherently easier than others per unit time, corresponding to different hazard rates. If so, then the survival curve over the whole task suite would be averaging different exponential decay curves together. This produces an aggregate decay curve with thicker tails than an exponential (corresponding to a declining effective hazard rate). Again, see <a href="https://arxiv.org/abs/2308.09045"><span>Ord (2023)</span></a> for details. Dealing with this effect is important as it could still be the case that the mechanism of constant hazard rate (and what it implies about the agents’ behaviour) holds for every task, even if it isn’t visible in the aggregate of these tasks.</p><h3><strong>Upshots of the constant hazard rate model</strong></h3><p>If the constant hazard rate model is sufficient to explain the drop-off in success rates on a task suite, there are several interesting upshots:</p><ul data-rte-list="default"><li><p>It allows us to make predictions for the time-horizons at other success rates, such as 90% and 99%.</p></li></ul><ul data-rte-list="default"><li><p>It allows us to make predictions for how the success rate improves over time for a fixed task-length.</p></li></ul><ul data-rte-list="default"><li><p>It provides simple rules of thumb for predicting success probabilities (e.g. that if you double the task duration, you square the success probability).</p></li></ul><ul data-rte-list="default"><li><p>It suggests that AI agent performance (at least on this task suite) can be characterised by a half-life.</p></li></ul><ul data-rte-list="default"><li><p>It provides indirect evidence that what really is going on under the hood is that tasks are made up of many sequential subtasks and the chance of succeeding at the whole requires succeeding at every individual component. Moreover, this suggests that the current AI agents are not very good at recovering from earlier mistakes.</p></li></ul><ul data-rte-list="default"><li><p>Because the exponential distribution is the unique memoryless distribution, another way of seeing it is that the chance of failing at the next moment is independent of how far you’ve come — just like how the chance of a radioisotope decaying in the next minute is independent on how many minutes it has survived so far. This would be a surprising and interesting property for reasoning agents.</p></li></ul><ul data-rte-list="default"><li><p>Deviations from exponential decay for certain models may provide evidence that their hazard rate is increasing (or decreasing) with time, which might provide hints as to what they are doing wrong (or right).</p></li></ul><ul data-rte-list="default"><li><p>It can explain the same 7-month doubling time for the 50% time-horizon and 80% time-horizon: a 7-month halving-time for the underlying hazard rate would produce the 7-month doubling-time of all such time horizons.</p></li></ul><ul data-rte-list="default"><li><p>It also helps conceptually explain what the time horizons could even mean. For example, if it can complete a day’s work, why can’t it just do that twice to produce two days of work? On the constant hazard rate model, the issue is that if has a 50% chance of succeeding on Monday’s work, then it only has a 25% chance of succeeding in both Monday’s and Tuesday’s work, which is too low to count as reliably achieving the 2-day task (and similarly for any higher reliability threshold).</p></li></ul><p>Note that I am not claiming AI agents have a precisely constant rate of failure per minute of time it would take a human to complete the task. The claim is instead that something like this appears to be roughly true or stochastically true. All other models imply that there is some systematic change in the hazard rate over time, and my suggestion (pending more information about the precise fits of different models) is that the data doesn’t warrant such assumptions.</p><p>If systematic deviations from exponential decay are found, such as the hazard rate increasing (or decreasing) with time, this might provide useful hints as to what the agents are doing wrong (or right). i.e. the constant hazard rate model can also work as a theoretical baseline from which to measure empirical deviations.</p><p>Also, note that <em>whatever</em> the survival curve is, if you lower the hazard rate by a factor of <em>k</em> at all times, the time horizons for all success rate levels also increase by a factor of <em>k</em>  (because the entire survival curve is  being stretched horizontally by a factor of <em>k</em>). Moreover, this is bidirectional — the only way to increase the time horizons at all success-rate levels by the same factor is to lower hazard rate by that factor. So to the extent that the METR evidence suggests the time horizons at every success rate are doubling at the same rate, it is also suggesting that the agent improvements are taking the form of reducing the hazard rate across all times by the same factor (i.e.. halving it every 7 months). This is intriguing and would be true whether or not there is a constant hazard rate for each model..</p><h3><strong>Further work</strong></h3><p>So far these results and analysis are merely suggestive of a constant hazard rate. Ideally one would conduct a formal statistical analysis on how well an exponential decay curve fits METRs data compared to the log-logistic they use.&nbsp;It would also be good to run robust statistical comparisons of the human decay curves versus the AI agent decay curves to see if there are systematic differences (e.g. different half-lives or different shapes that aren’t just an artefact of the experimental setup). And of course it is also important to know how much any of this generalises to other suites of tasks.</p><h3><strong>References</strong></h3><p>Thomas Kwa et al., <a href="https://arxiv.org/abs/2503.14499"><span>Measuring AI Ability to Complete Long Tasks</span></a>, arXiv:2503.14499 [cs.AI], 2025.</p><p>Toby Ord, <a href="https://arxiv.org/abs/2308.09045"><span>The Lindy Effect</span></a>. arXiv:2308.09045 [physics.soc-ph], 2023.</p>
</div></div>

    <!--BLOG INJECTION-->

    <!-- MathJax: Reset equation counter after every blog entry -->
$\setCounter{0}$    

    <!--CATEGORIES-->

    

  </article>
</section>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Terpstra Keyboard (166 pts)]]></title>
            <link>http://terpstrakeyboard.com/web-app/keys.htm</link>
            <guid>44308558</guid>
            <pubDate>Wed, 18 Jun 2025 10:31:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://terpstrakeyboard.com/web-app/keys.htm">http://terpstrakeyboard.com/web-app/keys.htm</a>, See on <a href="https://news.ycombinator.com/item?id=44308558">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="landing-page">
		<div>
		  
		  <p><img alt="" src="http://terpstrakeyboard.com/web-app/1x1.png">
          </p>

			<form id="settingsForm">
				<div>
					<div>
						<p><label>Tuning\Layout Quick Links</label>
                          

                          <label>Fundamental (Hz)</label>
                          
                       </p>

						<p><label>Right Facing Steps</label>
                          

                          <label>Up/Right Facing Steps</label>
                          
						</p>
                    </div>

					<div>
						<p><label>Hex Size (pixels)</label>
                          

                          <label>Rotation (degrees)</label>
                          
						</p>

						<p><label>Instrument</label>
                          

                          <label>
                              
                              <span>Enumerate Scale</span>
                          </label>

                          <label>
                              
                              <span>Use Spectrum Colors</span>
                          </label>

                          <label>
                              
                              <span>Blank Keys (No labels)</span>
                          </label>
						</p>
					</div>
				</div>

                <p><img alt="" src="http://terpstrakeyboard.com/web-app/1x1.png">
                </p>

				<div>
					<p><label>Scale (<a href="http://www.huygens-fokker.org/scala/scl_format.html" target="new">Scala format</a>)</label>
						
                  </p>

                  <div>
                    <p><label id="numberLabel">Steps To Equivalence Interval</label>
                      

                      
                      
					</p>

                    <p>
                        

                        <label id="note_colorsLabel">Color Layout</label>
                        
                    </p>
                  </div>
				</div>
				<br>
				
            </form>
        </div>

		<p>
          Designed by <a href="http://siementerpstra.com/" target="new">Siemen Terpstra</a> in the late ’80’s. WebApp developed by <a href="http://jamesfenn.com/" target="new">James Fenn</a> with additions and modifications by <a href="http://brandlew.com/" target="new">Brandon Lewis</a>, <a href="http://whatmusicreallyis.com/" title="What Music Really İs" target="new">Bo Constantinsen</a> and <a href="https://sites.google.com/site/wangchengu/" target="new">Chengu Wang</a>. Credits to Scott Thompson and <a href="http://ozanyarman.com/" target="new">Dr Ozan Yarman</a> for contributing samples. Current version 1.5.2 (Jan. 2015 — May 2019), released as Free/Libre and Open Source Software under <a href="https://www.gnu.org/licenses/gpl-3.0.en.html" target="new">GPL-3.0</a>. Download, fork, and get your name down here by fixing issues and implementing features via <a href="https://github.com/wcgbg/terpstrakeyboard/" target="new">GitHub</a>!
		</p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I counted all of the yurts in Mongolia using machine learning (175 pts)]]></title>
            <link>https://monroeclinton.com/counting-all-yurts-in-mongolia/</link>
            <guid>44307629</guid>
            <pubDate>Wed, 18 Jun 2025 07:58:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://monroeclinton.com/counting-all-yurts-in-mongolia/">https://monroeclinton.com/counting-all-yurts-in-mongolia/</a>, See on <a href="https://news.ycombinator.com/item?id=44307629">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>The <em>Fall of Civilizations</em> podcast put out a <a href="https://www.youtube.com/watch?v=YyqS9V7yHQA">6¾-hour episode</a>
on the history of the Mongol Empire, which I eagerly listened to. After finishing the episode I wondered
about contemporary Mongolian society, I wanted to learn what the lands that the Mongol Empire
exploded from are like in our current day. There are many ways to try to understand a society,
whether it be quantifying it or looking at the lived experiences within it. If you look at
data provided by the World Bank, you’ll see a country that has rapidly reduced poverty in the 21st
century, has a high economic growth rate, a healthy fertility rate, and is solidly an
upper-middle-income country. While Mongolia is a republic with a competitive party system,
<a href="https://www.worldbank.org/en/publication/worldwide-governance-indicators/interactive-data-access">Worldwide Governance Indicators</a>
from the World Bank show a government that has issues with corruption, regulatory quality, and effectiveness.</p>
<table>
<thead>
<tr>
<th>Indicator</th>
<th>Value</th>
<th>Years</th>
</tr>
</thead>
<tbody>
<tr>
<td>Population</td>
<td>3,481,145</td>
<td>2023</td>
</tr>
<tr>
<td>Fertility rate</td>
<td>2.7</td>
<td>2023</td>
</tr>
<tr>
<td>Intentional homicides (per 100,000 people)</td>
<td>6</td>
<td>2021</td>
</tr>
<tr>
<td>Individuals using the Internet (% of population)</td>
<td>1% → 83%</td>
<td>2000 → 2023</td>
</tr>
<tr>
<td>Poverty headcount ratio at $2.15 a day (2017 PPP) (% of population)</td>
<td>11.6% → 0.2%</td>
<td>2002 → 2022</td>
</tr>
<tr>
<td>Average GDP growth</td>
<td>6.62%</td>
<td>2003 → 2023</td>
</tr>
<tr>
<td>GDP per capita, PPP (current international $)</td>
<td>$4,399.4 → $18,004.9</td>
<td>2003 → 2023</td>
</tr>
</tbody>
</table>
<blockquote>
<p>(“Mongolia”)</p>
</blockquote>
<p>All of these indicators are interesting to look at, but they don’t really show what a society is
like. I feel you get much more understanding by going to a country, walking the streets, and
talking to people there. If you’re unable to do this, the next best thing is spending hours
exploring Google Maps, which I did. I opened a satellite view of Ulaanbaatar, the capital of Mongolia.
I saw new glass buildings, Soviet-designed apartment blocks (called ugsarmal), impressive government
buildings, factories, and industrial areas. But something stood out to me. Yurts, extending for kilometers
in all directions.</p>
<p><img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/yurts-1.jpg" alt="Satellite view of Ulaanbaatar containing yurts"></p>
<blockquote>
<p>Maps Data: Google © 2025 Airbus, CNES / Airbus, Maxar Technologies</p>
</blockquote>
<p>Naturally, I was impressed by the quantity of yurts I saw, and I was curious: just how many yurts (ger in Mongolian) are in
Mongolia and why? This set me on the path drawing bounding boxes on over 10,000 yurts to train a machine learning
model to count the rest of the yurts in the country. While I was training the model, I wondered what
the story behind these yurts are, I did a small investigation for later in this article. For now,
this is the story of counting them.</p>
<h2 id="counting-all-the-yurts-in-mongolia">Counting all the yurts in Mongolia</h2>
<p>I was unable to find a count of the yurts in Mongolia, this left me with
the task of doing it myself. Although I had never studied or worked with machine learning, I knew
through some osmosis that machine learning is well fit for this task. I created a simple plan in my
brain:</p>
<ol>
<li>Train a model to identify yurts</li>
<li>Reduce input space and parallelize searching of input space</li>
<li>Keep track of the yurts found</li>
</ol>
<h3 id="training-a-model-to-identify-yurts">Training a model to identify yurts</h3>
<p>The first thing I needed was training data, and lots of it. There’s many different options for satellite
imagery such as <a href="https://www.mapbox.com/imagery">Mapbox</a>, <a href="https://developers.google.com/maps/documentation/tile">Google Maps</a>,
and <a href="https://developers.arcgis.com/rest/basemap-styles/arcgis-imagery-webmap-get/">ArcGIS</a>. I
decided to use Google Maps since I’m already familiar with it.</p>
<p>For digital maps, many systems break the world up into a series of 256 x 256 tiles identified by X, Y, Z values. This is
referred to as tiled web maps and allows for progressively loading maps at different zoom levels and
positions. The zoom level values tend to be 0 through 20, where 0 has the least tiles and 20 the
most. The formula for calculating the number of tiles at a given zoom (z) level is: <span> $2^z * 2^z$ </span>
.
This means increasing <code>z</code> by one will increase the tile count by four times.</p>
<p>I wrote a Python script that generated tiles from a box around Ulaanbaatar and downloaded
them to a folder to use as training data. To list the tiles inside a bounding box made up of a
southwest and northeast coordinates, I used the <a href="https://mercantile.readthedocs.io/en/latest/">mercantile package</a>.</p>
<div><pre tabindex="0"><code data-lang="python"><span>for</span> <span>tile</span> <span>in</span> <span>mercantile</span><span>.</span><span>tiles</span><span>(</span><span>sw_lng</span><span>,</span> <span>sw_lat</span><span>,</span> <span>ne_lng</span><span>,</span> <span>ne_lat</span><span>,</span> <span>zooms</span><span>=</span><span>z</span><span>):</span>
    <span>download_tile</span><span>(</span><span>*</span><span>tile</span><span>)</span>
</code></pre></div><p><img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/tile-1.jpeg" alt="Sample tile from Google Maps">
<img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/tile-2.jpeg" alt="Sample tile from Google Maps">
<img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/tile-3.jpeg" alt="Sample tile from Google Maps"></p>
<blockquote>
<p>Tiles from Google Maps, you can see yurts on the right tile. Maps Data: Google © 2025 Airbus, CNES / Airbus, Maxar Technologies</p>
</blockquote>
<p>I decided to start at zoom level <code>17</code> as it is the lowest zoom level that I can still identify yurts
at. Once I downloaded several hundred tiles at this zoom level, I needed a way to label the yurts on
these tiles. Labeling is the process of drawing boxes around objects in an image. The idea is to
draw these boxes manually, creating what is called annotated data, and then training a model to do
the labeling using the annotated data.
There’s an open source tool called <a href="https://labelstud.io/">Label Studio</a> that does just
this.</p>
<p><img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/label-studio.jpg" alt="Label Studio showing yurts labeled"></p>
<blockquote>
<p>Here I drew bounding boxes on the tile around the yurts.</p>
</blockquote>
<p>A couple dozen yurts later and I wanted to try and train a model based on my tiny amount of
annotated data. I had the choice between object detection (bounding boxes) and segmentation (outline
objects). Segmentation probably would be more accurate because yurts are not rectangular,
but it seemed like it would take longer to setup. I decided to go with object detection.</p>
<p>I looked at various ways to train an object detection model, my requirements were:</p>
<ul>
<li>Open source</li>
<li>As simple as possible to setup</li>
<li>Able to quickly iterate</li>
<li>Detection speed of the model is a priority due to the potentially large amount of data</li>
<li>Has good default settings around data augmentation, warmups, loss functions, etc</li>
<li>Monitor current and previous training runs to compare accuracy</li>
</ul>
<p>After doing a brief survey of the machine learning landscape, I landed on using <a href="https://docs.ultralytics.com/">YOLO11</a> by Ultralytics.
The YOLO series is a set of models that can complete computer vision tasks, and can be trained with
custom data.
In Label Studio you’re able to export to many different dataset types, YOLO being one of them. After
exporting my annotated data as a YOLO dataset, I split the dataset into training and validation data
and configured the dataset in <code>dataset.yaml</code> for YOLO to use.</p>





<div><pre tabindex="0"><code data-lang="yaml"><span>train</span><span>:</span><span> </span><span>images/train</span><span>
</span><span></span><span>val</span><span>:</span><span> </span><span>images/val</span><span>
</span><span>
</span><span></span><span>nc</span><span>:</span><span> </span><span>1</span><span>
</span><span></span><span>names</span><span>:</span><span>
</span><span>  </span>- <span>yurt</span></code></pre></div>

<p>From the ultralytics package, I used the YOLO class to use their pre-trained <code>yolo11n</code> object
detection model. Ultralytics allows easy tuning of the model with annotated data through the <code>train</code>
method of the <code>YOLO</code> class. The tuned model can be exported through <code>export</code> in various formats.</p>
<div><pre tabindex="0"><code data-lang="python"><span>from</span> <span>ultralytics</span> <span>import</span> <span>YOLO</span>

<span>model</span> <span>=</span> <span>YOLO</span><span>(</span><span>"yolo11n.pt"</span><span>)</span>
<span>model</span><span>.</span><span>train</span><span>(</span>
    <span>data</span><span>=</span><span>"dataset.yaml"</span><span>,</span>
    <span>device</span><span>=</span><span>"cpu"</span><span>,</span>
<span>)</span>

<span>path</span> <span>=</span> <span>model</span><span>.</span><span>export</span><span>(</span><span>name</span><span>=</span><span>"yurt"</span><span>)</span>
</code></pre></div><p>With some testing I found my Yurt model was less than adequate, which I expected due to the tiny
amount of annotated data. I then did a couple hours of labeling, but the model would always miss
around 10-15% of the yurts in a given tile. At this point I had two options, either increase the
zoom level or gather more training data. To base my decision I decided to calculate how many tiles I
would need to search at each zoom level.</p>
<h3 id="refining-the-search-area">Refining the search area</h3>
<p>Mongolia is 1,564,116 square kilometers, using this we can calculate how many tiles at each zoom
level there are in Mongolia. The world has <span> $2^z * 2^z$ </span>
 tiles, so
on a single axis there are <span> $2^z$ </span>
 tiles. The map projection is from a sphere
a tile will represent more or less area depending on the latitude.
To find the width of the projection at a latitude for Web Mercator, we can
use this formula where <span>$R = 6,378.137$</span>
 is the radius of the equator in kilometers and <span>$\phi = 47.923107575288114$</span>
 is the
latitude of Mongolia in degrees which is converted to radians:</p>
<p><span> $$2\pi * R * \cos(\phi * \dfrac{\pi}{180}) = 26,855.3636571$$</span></p><p>We then need to divide the number of tiles on the x-axis at this location to get the width of a
tile. For the area of a tile, just square the width and divide the area of Mongolia by the area of a
single tile to get the tile count.</p>
<table>
<thead>
<tr>
<th>Zoom Level</th>
<th>Tile Count</th>
</tr>
</thead>
<tbody>
<tr>
<td>17</td>
<td>37,258,617</td>
</tr>
<tr>
<td>18</td>
<td>149,034,469</td>
</tr>
<tr>
<td>19</td>
<td>596,137,879</td>
</tr>
<tr>
<td>20</td>
<td>2,384,551,518</td>
</tr>
</tbody>
</table>
<p>Since Mongolia is such a large country, I began to wonder if there are more ways to reduce the
amount of tiles other than just zoom level. It’s a sparsely populated country, with much of the
country being uninhabited. Also, nearly all yurts are located in urban areas, with the City of
Ulaanbaatar estimating 60% of the population lives in ger (yurt) districts (City of Ulaanbaatar 17).</p>
<p>I used <a href="https://overpass-turbo.eu/">overpass turbo</a> to do a query for all the places human settlements might be in the
country and exported this data as GeoJSON. The query returned several thousand points of interest.</p>
<pre tabindex="0"><code>[out:json][timeout:25];
{{geocodeArea:Mongolia}}-&gt;.searchArea;
(
  node[place](area.searchArea);
  node[man_made](area.searchArea);
  node[historic](area.searchArea);
);
out body;
&gt;;
out skel qt;
</code></pre><p>I wanted to know how many unique tiles for searching a 2,000 meter area around each point there are,
so I wrote a script to do this using geopandas.</p>
<div><pre tabindex="0"><code data-lang="python"><span>gdf</span> <span>=</span> <span>gpd</span><span>.</span><span>read_file</span><span>(</span><span>"./mongolia.geojson"</span><span>)</span>
<span>gdf_merc</span> <span>=</span> <span>gdf</span><span>.</span><span>to_crs</span><span>(</span><span>"EPSG:3857"</span><span>)</span>
<span>gdf_merc</span><span>[</span><span>"buffer"</span><span>]</span> <span>=</span> <span>gdf_merc</span><span>.</span><span>geometry</span><span>.</span><span>buffer</span><span>(</span><span>2000</span><span>)</span>

<span>gdf_buffer</span> <span>=</span> <span>gdf_merc</span><span>.</span><span>set_geometry</span><span>(</span><span>"buffer"</span><span>)</span><span>.</span><span>to_crs</span><span>(</span><span>"EPSG:4326"</span><span>)</span>

<span>tiles</span> <span>=</span> <span>{}</span>
<span>for</span> <span>polygon</span> <span>in</span> <span>gdf_buffer</span><span>.</span><span>geometry</span><span>:</span>
    <span>minx</span><span>,</span> <span>miny</span><span>,</span> <span>maxx</span><span>,</span> <span>maxy</span> <span>=</span> <span>polygon</span><span>.</span><span>bounds</span>

    <span>for</span> <span>tile</span> <span>in</span> <span>mercantile</span><span>.</span><span>tiles</span><span>(</span><span>minx</span><span>,</span> <span>miny</span><span>,</span> <span>maxx</span><span>,</span> <span>maxy</span><span>,</span> <span>zooms</span><span>=</span><span>Z</span><span>):</span>
        <span>tiles</span><span>[</span><span>"</span><span>{}</span><span>-</span><span>{}</span><span>-</span><span>{}</span><span>"</span><span>.</span><span>format</span><span>(</span><span>str</span><span>(</span><span>tile</span><span>.</span><span>x</span><span>),</span> <span>str</span><span>(</span><span>tile</span><span>.</span><span>y</span><span>),</span> <span>str</span><span>(</span><span>tile</span><span>.</span><span>z</span><span>))]</span> <span>=</span> <span>True</span>
</code></pre></div><table>
<thead>
<tr>
<th>Zoom Level</th>
<th>Tile Count</th>
</tr>
</thead>
<tbody>
<tr>
<td>17</td>
<td>270,559</td>
</tr>
<tr>
<td>18</td>
<td>1,016,617</td>
</tr>
<tr>
<td>19</td>
<td>3,938,174</td>
</tr>
<tr>
<td>20</td>
<td>15,506,872</td>
</tr>
</tbody>
</table>
<h3 id="building-a-model-backend-for-labeling">Building a model backend for labeling</h3>
<p>To speed up the labeling of yurts I wanted Label Studio to use my model to label yurts.
Label Studio has the ability to integrate with a model backend,
essentially an API wrapper around a model, to request predictions. When labeling a tile, Label
Studio makes a request to this API for predictions. The API returns the bounding boxes for the tile.
I fix any mistakes the model made, and submit the tile. Every so often I retrain the model, creating
a feedback loop that improves the model with more and more annotated data.</p>





<div><pre tabindex="0"><code data-lang="python"><span>class</span> <span>YurtModel</span><span>:</span>
    <span># Initialize trained model to reuse across requests</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>model</span> <span>=</span> <span>YOLO</span><span>(</span><span>"best.pt"</span><span>,</span> <span>task</span><span>=</span><span>"detect"</span><span>)</span>

    <span># Task a task sent by Label Studio, and return bounding boxes of yurts</span>
    <span>def</span> <span>predict</span><span>(</span><span>self</span><span>,</span> <span>tasks</span><span>):</span>
        <span>predictions</span> <span>=</span> <span>[]</span>
        <span>for</span> <span>task</span> <span>in</span> <span>tasks</span><span>:</span>
            <span># Get the path to the file from label studio</span>
            <span>path</span> <span>=</span> <span>get_local_path</span><span>(</span>
                <span>task</span><span>[</span><span>"data"</span><span>][</span><span>"image"</span><span>],</span>
                <span>task_id</span><span>=</span><span>task</span><span>[</span><span>"id"</span><span>],</span>
            <span>)</span>

            <span>results</span> <span>=</span> <span>self</span><span>.</span><span>model</span><span>(</span><span>path</span><span>)</span>

            <span>for</span> <span>result</span> <span>in</span> <span>results</span><span>:</span>
                <span>regions</span> <span>=</span> <span>[]</span>
                <span>for</span> <span>prediction</span> <span>in</span> <span>result</span><span>.</span><span>boxes</span><span>:</span>
                    <span>xyxy</span> <span>=</span> <span>prediction</span><span>.</span><span>xyxy</span><span>[</span><span>0</span><span>]</span><span>.</span><span>tolist</span><span>()</span>
                    <span>regions</span><span>.</span><span>append</span><span>({</span>
                        <span>"model_version"</span><span>:</span> <span>"1.0"</span><span>,</span>
                        <span>"from_name"</span><span>:</span> <span>"label"</span><span>,</span>
                        <span>"to_name"</span><span>:</span> <span>"image"</span><span>,</span>
                        <span>"type"</span><span>:</span> <span>"rectanglelabels"</span><span>,</span>
                        <span>"score"</span><span>:</span> <span>prediction</span><span>.</span><span>conf</span><span>.</span><span>item</span><span>(),</span>
                        <span>"value"</span><span>:</span> <span>{</span>
                            <span>"x"</span><span>:</span> <span>xyxy</span><span>[</span><span>0</span><span>]</span> <span>/</span> <span>256</span> <span>*</span> <span>100</span><span>,</span>
                            <span>"y"</span><span>:</span> <span>xyxy</span><span>[</span><span>1</span><span>]</span> <span>/</span> <span>256</span> <span>*</span> <span>100</span><span>,</span>
                            <span>"width"</span><span>:</span> <span>(</span><span>xyxy</span><span>[</span><span>2</span><span>]</span> <span>-</span> <span>xyxy</span><span>[</span><span>0</span><span>])</span> <span>/</span> <span>256</span> <span>*</span> <span>100</span><span>,</span>
                            <span>"height"</span><span>:</span> <span>(</span><span>xyxy</span><span>[</span><span>3</span><span>]</span> <span>-</span> <span>xyxy</span><span>[</span><span>1</span><span>])</span> <span>/</span> <span>256</span> <span>*</span> <span>100</span><span>,</span>
                            <span>"rectanglelabels"</span><span>:</span> <span>[</span>
                                <span>"yurt"</span><span>,</span>
                            <span>],</span>
                        <span>},</span>
                    <span>})</span>

                <span>all_scores</span> <span>=</span> <span>[</span><span>region</span><span>[</span><span>"score"</span><span>]</span> <span>for</span> <span>region</span> <span>in</span> <span>regions</span> <span>if</span> <span>"score"</span> <span>in</span> <span>region</span><span>]</span>
                <span>avg_score</span> <span>=</span> <span>sum</span><span>(</span><span>all_scores</span><span>)</span> <span>/</span> <span>max</span><span>(</span><span>len</span><span>(</span><span>all_scores</span><span>),</span> <span>1</span><span>)</span>

                <span>predictions</span><span>.</span><span>append</span><span>({</span>
                    <span>"result"</span><span>:</span> <span>regions</span><span>,</span>
                    <span>"score"</span><span>:</span> <span>avg_score</span><span>,</span>
                    <span>"model_version"</span><span>:</span> <span>"1.0"</span><span>,</span>
                <span>})</span>

        <span>return</span> <span>{</span>
            <span>"results"</span><span>:</span> <span>predictions</span><span>,</span>
        <span>}</span>

<span>model</span> <span>=</span> <span>YurtModel</span><span>()</span></code></pre></div>

<p>We then need to fill out the API routes that Label Studio expects, which is a <code>/predict</code> route for
label studio to send tiles and receive predictions, a <code>/setup</code> route to do any initialization
required, and a <code>/health</code> route to do health checks on. I used <a href="https://fastapi.tiangolo.com/">FastAPI</a> to build the API and use
the <code>YurtModel</code> from above.</p>





<div><pre tabindex="0"><code data-lang="python"><span>@app</span><span>.</span><span>post</span><span>(</span><span>"/predict"</span><span>)</span>
<span>async</span> <span>def</span> <span>predict</span><span>(</span><span>request</span><span>:</span> <span>Request</span><span>):</span>
    <span>res</span> <span>=</span> <span>await</span> <span>request</span><span>.</span><span>json</span><span>()</span>
    <span>return</span> <span>model</span><span>.</span><span>predict</span><span>(</span><span>res</span><span>[</span><span>"tasks"</span><span>])</span>

<span>@app</span><span>.</span><span>post</span><span>(</span><span>"/setup"</span><span>)</span>
<span>async</span> <span>def</span> <span>setup</span><span>():</span>
    <span>return</span> <span>{</span>
        <span>"model_version"</span><span>:</span> <span>"1.0"</span><span>,</span>
    <span>}</span>

<span>@app</span><span>.</span><span>get</span><span>(</span><span>"/health"</span><span>)</span>
<span>async</span> <span>def</span> <span>health</span><span>():</span>
    <span>return</span> <span>{</span>
        <span>"status"</span><span>:</span> <span>"UP"</span><span>,</span>
        <span>"model_class"</span><span>:</span> <span>str</span><span>(</span><span>YurtModel</span><span>.</span><span>__class__</span><span>),</span>
    <span>}</span></code></pre></div>

<p>By relying on the model to find most of the yurts when labeling, I was able to rapidly create more
annotated data. I quickly built a dataset of over 10,000 yurts.</p>
<h3 id="monitoring-accuracy-of-each-model">Monitoring accuracy of each model</h3>
<h3 id="scaling-training-of-models">Scaling training of models</h3>
<p>As the size of the annotated data grew, training the models on my laptop became too slow.
I decided to use <a href="https://vast.ai/">vast.ai</a> to rent GPUs to do my training runs. To train
the models on vast.ai, I needed everything to run in Docker. I wrote a Dockerfile for the training
script, and I pushed it to <a href="https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry">GitHub Container Registry</a>.
In vast.ai I set up authentication with the private image registry so it could pull the image I pushed up.</p>
<p><img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/docker-auth.png" alt="vast.ai Docker authentication"></p>
<blockquote>
<p>Docker authentication in vast.ai</p>
</blockquote>
<p>Here is the Dockerfile that I used to run the training script on the dataset I created.</p>
<div><pre tabindex="0"><code data-lang="shell">FROM ghcr.io/astral-sh/uv:python3.10-bookworm-slim

WORKDIR /app

<span># Copy training script, annotated data, and requirements to image</span>
COPY scripts/train_model.py .
COPY datasets ./datasets
COPY dataset.yaml .
COPY pyproject.toml .
COPY uv.lock .

<span># Needed for ...</span>
RUN apt-get update -y <span>&amp;&amp;</span> apt-get install -y libgl1-mesa-dev libglib2.0-0

<span># Install package requirements</span>
RUN uv sync --no-dev

<span># Run the training script</span>
CMD <span>[</span><span>"uv"</span>, <span>"run"</span>, <span>"python"</span>, <span>"train_model.py"</span><span>]</span>
</code></pre></div><p>In order to build and push this image to GitHub I ran:</p>
<div><pre tabindex="0"><code data-lang="shell">docker build -t ghcr.io/monroeclinton/yurt -f Dockerfile .
docker push ghcr.io/monroeclinton/yurt:latest
</code></pre></div><p>Since the training happened in ephemeral containers, I needed a way to retrieve the finished model. I
decided to upload the model to S3 after it finished training. To monitor the accuracy of the
models, I also needed the metadata associated with the runs, so I uploaded everything in the
run folder to S3.</p>





<div><pre tabindex="0"><code data-lang="python"><span>model</span> <span>=</span> <span>YOLO</span><span>(</span><span>"yolo11n.pt"</span><span>)</span>

<span>model</span><span>.</span><span>train</span><span>(</span>
    <span>data</span><span>=</span><span>"dataset.yaml"</span><span>,</span>
    <span>epochs</span><span>=</span><span>1000</span><span>,</span>
    <span>patience</span><span>=</span><span>150</span><span>,</span>
    <span>imgsz</span><span>=</span><span>256</span><span>,</span>
    <span>device</span><span>=</span><span>"cuda"</span><span>,</span>
<span>)</span>

<span>path</span> <span>=</span> <span>model</span><span>.</span><span>export</span><span>(</span><span>name</span><span>=</span><span>"yurt"</span><span>)</span>
<span>train_dir</span> <span>=</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>dirname</span><span>(</span><span>os</span><span>.</span><span>path</span><span>.</span><span>dirname</span><span>(</span><span>path</span><span>))</span>

<span>s3</span> <span>=</span> <span>boto3</span><span>.</span><span>client</span><span>(</span>
    <span>service_name</span> <span>=</span><span>"s3"</span><span>,</span>
    <span>endpoint_url</span><span>=</span><span>os</span><span>.</span><span>environ</span><span>[</span><span>"S3_ENDPOINT"</span><span>],</span>
    <span>aws_access_key_id</span><span>=</span><span>os</span><span>.</span><span>environ</span><span>[</span><span>"S3_ACCESS_KEY_ID"</span><span>],</span>
    <span>aws_secret_access_key</span><span>=</span><span>os</span><span>.</span><span>environ</span><span>[</span><span>"S3_SECRET_ACCESS_KEY"</span><span>],</span>
    <span>region_name</span><span>=</span><span>os</span><span>.</span><span>environ</span><span>[</span><span>"S3_REGION"</span><span>],</span>
<span>)</span>

<span>timestamp</span> <span>=</span> <span>time</span><span>.</span><span>time</span><span>()</span>

<span>for</span> <span>root</span><span>,</span> <span>dirs</span><span>,</span> <span>files</span> <span>in</span> <span>os</span><span>.</span><span>walk</span><span>(</span><span>train_dir</span><span>):</span>
    <span>for</span> <span>file</span> <span>in</span> <span>files</span><span>:</span>
        <span>local_path</span> <span>=</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>join</span><span>(</span><span>root</span><span>,</span> <span>file</span><span>)</span>
        <span>s3_key</span> <span>=</span> <span>f</span><span>"models/</span><span>{</span><span>int</span><span>(</span><span>timestamp</span><span>)</span><span>}</span><span>/</span><span>{</span><span>os</span><span>.</span><span>path</span><span>.</span><span>relpath</span><span>(</span><span>local_path</span><span>,</span> <span>train_dir</span><span>)</span><span>}</span><span>"</span>
        <span>s3</span><span>.</span><span>upload_file</span><span>(</span><span>local_path</span><span>,</span> <span>os</span><span>.</span><span>environ</span><span>[</span><span>"S3_BUCKET"</span><span>],</span> <span>s3_key</span><span>)</span></code></pre></div>

<h3 id="deploying-models-and-searching-mongolia">Deploying models and searching Mongolia</h3>
<p>After dozens of training runs and greatly improving the accuracy of the model, I decided to finally
do my count of Mongolia. There were many options to run my deployment, however I made my choice based on three
criteria:</p>
<ul>
<li>Simplicity in setup and deployment</li>
<li>At least 100 instances of my model should be run</li>
<li>The bottleneck is I/O (downloading tiles), so should be deployed on many CPUs</li>
</ul>
<p>Based on these criteria, I used <a href="https://docs.docker.com/engine/swarm/">Docker Swarm</a> to orchestrate the workload.
It’s already packaged in Docker, so there’s no need to install anything else. Docker Swarm also is
fairly simple to set up, scale, and deploy services with. I rented eight servers, each with 16 vCPUs
(128 vCPUs total), and connected them over a private network.</p>
<p>I picked one server to be the <a href="https://docs.docker.com/engine/swarm/how-swarm-mode-works/nodes/#manager-nodes">manager node</a>.
On this server, I ran this to initialize the swarm:</p>
<div><pre tabindex="0"><code data-lang="bash">docker swarm init --advertise-addr 10.0.0.2
</code></pre></div><p>This command sets up the swarm and prints a command to run on the worker nodes to connect them to
the manager. Each worker node joined using the token and the manager’s address:</p>
<div><pre tabindex="0"><code data-lang="bash">docker swarm join --token SWARM_TOKEN 10.0.0.2:2377
</code></pre></div><p>I deployed the container images, which I had pushed to GHCR, and pulled with
<code>--with-registry-auth</code> to allow access from the server to GHCR.
There were two images, the <code>api</code> image and the <code>worker</code> image. The API managed a list of
search areas (the areas around the points found from overpass turbo), giving
search areas to workers, and expanding the search radius by 500 meters when yurts were found.
The workers requested search areas from the API and sent back a list of yurts found within the
search areas.</p>
<h4 id="api">API</h4>
<p>I used FastAPI to build the API, in which there were two routes.</p>
<ul>
<li>GET /search-area - Workers sent a request to this route to get a search area to search.</li>
</ul>
<p>This route first checks if there are any stale areas, where a worker had requested a search area
but never finished it. The workers should send periodic health checks to the API, if this fails then
it will return the search area to a different worker after one minute.</p>
<div><pre tabindex="0"><code data-lang="python"><span>stale_area</span> <span>=</span> <span>(</span>
    <span>db</span><span>.</span><span>query</span><span>(</span><span>SearchArea</span><span>)</span>
    <span>.</span><span>filter</span><span>(</span><span>SearchArea</span><span>.</span><span>searching</span> <span>==</span> <span>True</span><span>)</span>
    <span>.</span><span>filter</span><span>(</span><span>SearchArea</span><span>.</span><span>health_check</span> <span>&lt;</span> <span>one_minute_ago</span><span>)</span>
    <span>.</span><span>with_for_update</span><span>()</span>
    <span>.</span><span>first</span><span>()</span>
<span>)</span>
</code></pre></div><p>If there are no stale search areas, then a new point will be selected at random, and the search area
will be increased if there have been previous searches.</p>
<div><pre tabindex="0"><code data-lang="python"><span>point</span> <span>=</span> <span>(</span>
    <span>db</span><span>.</span><span>query</span><span>(</span><span>Point</span><span>)</span>
    <span>.</span><span>options</span><span>(</span><span>joinedload</span><span>(</span><span>Point</span><span>.</span><span>search_areas</span><span>))</span>
    <span>.</span><span>filter</span><span>(</span><span>Point</span><span>.</span><span>searched</span> <span>==</span> <span>False</span><span>)</span>
    <span>.</span><span>filter</span><span>(</span><span>~</span><span>Point</span><span>.</span><span>search_areas</span><span>.</span><span>any</span><span>(</span><span>SearchArea</span><span>.</span><span>searched</span> <span>==</span> <span>False</span><span>))</span>
    <span>.</span><span>order_by</span><span>(</span><span>func</span><span>.</span><span>random</span><span>())</span>
    <span>.</span><span>first</span><span>()</span>
<span>)</span>

<span>if</span> <span>not</span> <span>point</span><span>:</span>
    <span>raise</span> <span>HTTPException</span><span>(</span>
        <span>status_code</span><span>=</span><span>404</span><span>,</span> <span>detail</span><span>=</span><span>"No available point to search"</span><span>)</span>

<span>previous_areas</span> <span>=</span> <span>[</span><span>sa</span> <span>for</span> <span>sa</span> <span>in</span> <span>point</span><span>.</span><span>search_areas</span><span>]</span>
<span>if</span> <span>previous_areas</span><span>:</span>
    <span>max_meters</span> <span>=</span> <span>max</span><span>(</span><span>area</span><span>.</span><span>meters</span> <span>for</span> <span>area</span> <span>in</span> <span>previous_areas</span><span>)</span>
    <span>new_meters</span> <span>=</span> <span>max_meters</span> <span>+</span> <span>500</span>
<span>else</span><span>:</span>
    <span>new_meters</span> <span>=</span> <span>500</span>
</code></pre></div><p>A <code>SearchArea</code> has a list of tiles that are inside it. Each <code>Tile</code> has as status of <code>searched</code>.
I used geopandas, as shown earlier, to generate a bounding box over the search area and create a list
of tiles. For each of these tiles, I check the database to see if they have already been created + searched.
If they haven’t then they are upserted and assigned to the search area.</p>
<div><pre tabindex="0"><code data-lang="python"><span>created_tiles</span> <span>=</span> <span>(</span>
    <span>db</span><span>.</span><span>query</span><span>(</span><span>Tile</span><span>)</span>
    <span>.</span><span>filter</span><span>(</span>
        <span>tuple_</span><span>(</span><span>Tile</span><span>.</span><span>x</span><span>,</span> <span>Tile</span><span>.</span><span>y</span><span>,</span> <span>Tile</span><span>.</span><span>z</span><span>)</span><span>.</span><span>in_</span><span>(</span>
            <span>[(</span><span>tile</span><span>[</span><span>"x"</span><span>],</span> <span>tile</span><span>[</span><span>"y"</span><span>],</span> <span>tile</span><span>[</span><span>"z"</span><span>])</span>
             <span>for</span> <span>tile</span> <span>in</span> <span>tiles_to_create</span><span>]</span>
        <span>)</span>
    <span>)</span>
    <span>.</span><span>all</span><span>()</span>
<span>)</span>

<span>new_area</span><span>.</span><span>tiles</span><span>.</span><span>extend</span><span>(</span><span>created_tiles</span><span>)</span>
</code></pre></div><p>The route returns the search area, containing a list of tiles to search.</p>
<ul>
<li>POST /search-area/:id - Workers sent a request containing the yurts to this route.</li>
</ul>
<p>This route inserts the yurts into the database, and marks the <code>Point</code>, <code>SearchArea</code>, and <code>Tile</code> as
searched as needed. The <code>Point</code> gets marked as searched if no yurts are found, and the <code>SearchArea</code>
and <code>Tile</code> are marked as searched.</p>
<div><pre tabindex="0"><code data-lang="python"><span>if</span> <span>len</span><span>(</span><span>yurts_to_create</span><span>)</span> <span>&gt;</span> <span>0</span><span>:</span>
    <span>stmt</span> <span>=</span> <span>insert</span><span>(</span><span>Yurt</span><span>)</span><span>.</span><span>values</span><span>(</span><span>yurts_to_create</span><span>)</span>
    <span>stmt</span> <span>=</span> <span>stmt</span><span>.</span><span>on_conflict_do_nothing</span><span>(</span>
        <span>index_elements</span><span>=</span><span>[</span><span>"longitude"</span><span>,</span> <span>"latitude"</span><span>])</span>
    <span>db</span><span>.</span><span>execute</span><span>(</span><span>stmt</span><span>)</span>
<span>else</span><span>:</span>
    <span>db</span><span>.</span><span>execute</span><span>(</span>
        <span>update</span><span>(</span><span>Point</span><span>)</span><span>.</span><span>where</span><span>(</span>
            <span>exists</span><span>()</span><span>.</span><span>where</span><span>(</span>
                <span>(</span><span>SearchArea</span><span>.</span><span>point_id</span> <span>==</span> <span>Point</span><span>.</span><span>id</span><span>)</span> <span>&amp;</span> <span>(</span><span>Point</span><span>.</span><span>id</span> <span>==</span> <span>id</span><span>)</span>
            <span>)</span>
        <span>)</span><span>.</span><span>values</span><span>(</span>
            <span>searched</span><span>=</span><span>True</span><span>,</span>
        <span>)</span>
    <span>)</span>
</code></pre></div><h4 id="worker">Worker</h4>
<p>The worker script ran in a loop until it encountered the <code>No available point to search</code> error.
This loop consisted of requesting the <code>/search-area</code> to get a list of tiles to search, downloading
each tile, then passing the tile image to the model to detect yurts. Finally, the worker sends a
list of yurts to the API.</p>
<div><pre tabindex="0"><code data-lang="python"><span>def</span> <span>find_yurts</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>z</span><span>):</span>
    <span>filepath</span> <span>=</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>join</span><span>(</span><span>"tiles"</span><span>,</span> <span>"</span><span>{}</span><span>_</span><span>{}</span><span>_</span><span>{}</span><span>.jpeg"</span><span>.</span><span>format</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>z</span><span>))</span>

    <span>results</span> <span>=</span> <span>model</span><span>(</span><span>filepath</span><span>,</span> <span>imgsz</span><span>=</span><span>256</span><span>)</span>

    <span>yurts</span> <span>=</span> <span>[]</span>
    <span>for</span> <span>result</span> <span>in</span> <span>results</span><span>:</span>
        <span>for</span> <span>prediction</span> <span>in</span> <span>result</span><span>.</span><span>boxes</span><span>:</span>
            <span>xyxy</span> <span>=</span> <span>prediction</span><span>.</span><span>xyxy</span><span>[</span><span>0</span><span>]</span><span>.</span><span>tolist</span><span>()</span>

            <span># Find center of the bounding box</span>
            <span>pixel_x</span> <span>=</span> <span>xyxy</span><span>[</span><span>0</span><span>]</span> <span>+</span> <span>(</span><span>xyxy</span><span>[</span><span>2</span><span>]</span> <span>-</span> <span>xyxy</span><span>[</span><span>0</span><span>])</span> <span>/</span> <span>2</span>
            <span>pixel_y</span> <span>=</span> <span>xyxy</span><span>[</span><span>1</span><span>]</span> <span>+</span> <span>(</span><span>xyxy</span><span>[</span><span>3</span><span>]</span> <span>-</span> <span>xyxy</span><span>[</span><span>1</span><span>])</span> <span>/</span> <span>2</span>

            <span>lat</span><span>,</span> <span>lon</span> <span>=</span> <span>tile_xyz_to_lonlat</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>z</span><span>,</span> <span>pixel_x</span><span>,</span> <span>pixel_y</span><span>)</span>

            <span>yurts</span><span>.</span><span>append</span><span>({</span>
                <span>"latitude"</span><span>:</span> <span>lat</span><span>,</span>
                <span>"longitude"</span><span>:</span> <span>lon</span><span>,</span>
                <span>"score"</span><span>:</span> <span>prediction</span><span>.</span><span>conf</span><span>.</span><span>item</span><span>(),</span>
            <span>})</span>

    <span>return</span> <span>yurts</span>
</code></pre></div><p>I began scaling this service slowly and eventually ramped up to 120 workers running in parallel
using <code>docker service scale worker=120</code>. Each container processed its assigned tile, and if yurts
were found, posted their coordinates to the API.</p>
<h3 id="the-resulting-count">The resulting count</h3>
<p>After searching a couple million tiles I downloaded the yurt dataset, which I uploaded <a href="https://cdn.monroeclinton.com/yurts.json">here (12mb file)</a>.
In total I found 172,689 yurts with a prediction score of greater than 40%.</p>
<p>Perhaps there’s some lonesome yurts far in the Gobi Desert or the Altai Mountains I missed, so we
could add a hundred or so for those. I could have also done more like
providing image context and training on more data from smaller towns, but I only have so much time.</p>
<p>For fun I did some querying using <a href="https://postgis.net/">PostGIS</a> to find areas with high concentrations
of yurts. Generally I found places that are hotels or remote areas near mines.</p>
<p><img src="https://monroeclinton.com/counting-all-yurts-in-mongolia/many-yurts.jpeg" alt="Many yurts"></p>
<blockquote>
<p>Maps Data: Google © 2025 Airbus, CNES / Airbus, Maxar Technologies</p>
</blockquote>
<h2 id="the-people-of-the-yurts">The people of the yurts</h2>
<p>Historically, yurts have been a home for the nomadic peoples of the steppe to live. As
Mongolia developed into the modern world, the usage of yurts changed with the country. For example,
I found a reference to yurts being used as makeshift schools in the early 1900s. This period was the
start of the transformation from a nomadic herder society to an urban industrial society.</p>
<blockquote>
In the rural areas, in addition to the existing 60 scribe schools, at least 49 state primary schools were established by 1917. They were largely housed in yurts and financed with state, municipal, and private funds. (Steiner-Khamsi and Stolpe 36)
</blockquote>

<p>This reflects the developmental history of Mongolia, and how people are adjusting to the modern
world. Mongolia has transitioned from a mostly nomadic herder society, to a mostly urbanized
industrial society. As people transition from one system of life to another, remnants of their old
system persist. Housing and infrastructure are expensive, so as Mongolia transformed, once nomadic
herders took their yurts to urban areas and continued living in them.</p>
<blockquote>
The 51 percent urban population reported in the 1979 census
reflected rapid migration to the cities in the 1970s. The influx of
rural people created housing problems, among them long waits for
assignment to an apartment, expansion of ger districts on the edges
of built-up areas, and pressure to invest in more housing, roads,
and other urban infrastructure. (Worden et al. 86)
</blockquote>

<p>Due to the large number of people moving to urban locations, it has been difficult for the government to
build the infrastructure needed for them. The informal settlements that grew from this difficulty
are now known as ger districts. There have been many efforts to formalize and develop these areas.
The Law on Allocation of Land to Mongolian Citizens for Ownership, passed in 2002, allowed for
existing ger district residents to formalize the land they settled, and allowed for others to
receive land from the government into the future.</p>
<p>Along with the privatization of land, the Mongolian government has been pushing for the development
of ger districts into areas with housing blocks connected to utilities. The plan for this was
published in 2014 as Ulaanbaatar 2020 Master Plan and Development Approaches for 2030. Although
progress has been slow (Choi and Enkhbat 7), they have been making progress in building housing blocks in ger
distrcts. Residents of ger districts sell or exchange their plots to developers who then build housing
blocks on them. Often this is in exchange for an apartment in the building, and often the value of the
apartment is less than the land they originally had (Choi and Enkhbat 15).</p>
<p>Based on what I’ve read about the ger districts, they have been around since at least the 1970s,
and progress on developing them has been slow. When ineffective policy results in a large chunk of
the populace generationally living in yurts on the outskirts of urban areas, it’s clear that there
is failure.
One of the most important functions of government is inspiring the citizenry to achieve greatness.
Most governments around the world fail in this, but we should all work towards it. I think a step
the Mongolian government could take for this is to analyze which policy failures have led to such
slow progress on the ger district issue.</p>
<p>The Mongolian government’s long-term vision is to provide utilities and good housing
for these areas. Although I can’t contribute anything to this vision, I wish for the best
success in this plan.
I’m glad to have learned about a country and people I used to know nothing about. Hopefully in the
future I’ll study more about Mongolia, but for now I’m off to my next project.</p>
<h3 id="further-questions">Further questions</h3>
<ul>
<li>What causes Mongolia and other countries to urbanize and industrialize?</li>
<li>Why do some Mongolians head to the cities and others stay?</li>
<li>What challenges does the Mongolian government face in developing ger districts?</li>
<li>What causes the difference in speed of development between countries?</li>
</ul>
<h2 id="references">References</h2>
<ul>
<li><p>Choi, Mack Joong, and Urandulguun Enkhbat. “Distributional Effects of Ger Area Redevelopment in Ulaanbaatar, Mongolia.” International Journal of Urban Sciences, vol. 24, no. 1, Jan. 2020, pp. 50–68. DOI.org (Crossref), <a href="https://doi.org/10.1080/12265934.2019.1571433">https://doi.org/10.1080/12265934.2019.1571433</a>.</p>

</li>
<li><p>City of Ulaanbaatar. <em>Ulaanbaatar 2020 Master Plan and Development Approach for 2030.</em> 2014.</p>

</li>
<li>

</li>
<li><p>Steiner-Khamsi, Gita, and Ines Stolpe. <em>Educational Import: Local Encounters with Global Forces in Mongolia.</em> 1st ed, Palgrave Macmillan, 2006.</p>

</li>
<li><p>Worden, Robert L, et al. <em>Mongolia: A Country Study.</em> Washington, D.C.: Federal Research Division, Library of Congress: For sale by the Supt. of Docs., U.S. G.P.O, 1991. Pdf. Retrieved from the Library of Congress, &lt;www.loc.gov/item/90006289/&gt;.</p>

</li>
<li><p>Yang, Jeasurk, et al. <em>Poverty Mapping in Mongolia with AI-Based Ger Detection Reveals Urban Slums Persist after the COVID-19 Pandemic.</em> arXiv:2410.09522, arXiv, 12 Oct. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2410.09522">https://doi.org/10.48550/arXiv.2410.09522</a>.</p>

</li>
</ul>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MiniMax-M1 open-weight, large-scale hybrid-attention reasoning model (270 pts)]]></title>
            <link>https://github.com/MiniMax-AI/MiniMax-M1</link>
            <guid>44307290</guid>
            <pubDate>Wed, 18 Jun 2025 06:53:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/MiniMax-AI/MiniMax-M1">https://github.com/MiniMax-AI/MiniMax-M1</a>, See on <a href="https://news.ycombinator.com/item?id=44307290">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
  <themed-picture data-catalyst-inline="true"><picture>
    <source srcset="https://github.com/MiniMax-AI/MiniMax-M1/raw/main/figures/MiniMaxLogo-Dark.png" media="(prefers-color-scheme: dark)">
      <img src="https://github.com/MiniMax-AI/MiniMax-M1/raw/main/figures/MiniMaxLogo-Light.png" width="60%" alt="MiniMax">
    
  </picture></themed-picture>
</div>
<hr>
<p><a href="https://www.minimax.io/" rel="nofollow">
    <img alt="Homepage" src="https://camo.githubusercontent.com/3faa1e14d767d4a75cf9ed5610309fbb6fe899cec1f03f659f57e5961de37e9b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5f486f6d65706167652d4d696e694d61782d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530266c6f676f3d646174613a696d6167652f7376672b786d6c3b6261736536342c50484e325a79423462577875637a30696148523063446f764c336433647935334d793576636d63764d6a41774d43397a646d6369494868746247357a4f6e68736157357250534a6f644852774f693876643364334c6e637a4c6d39795a7938784f546b354c336873615735724969423261575633516d393450534977494441674e446b774c6a4532494451784d533433496a34385a47566d637a3438633352356247552b4c6d4e736379307865325a706247773649325a6d5a6a74395043397a64486c735a5434384c32526c5a6e4d2b50484268644767675932786863334d39496d4e73637930784969426b50534a4e4d6a4d7a4c6a51314c4451774c6a6778595445334c6a55314c4445334c6a55314c4441734d5377774c544d314c6a45734d46597a4d7a45754e545a684e4441754f4449734e4441754f4449734d4377774c4445744f4445754e6a4d734d4659784e4456684d5463754e5455734d5463754e5455734d4377784c4441744d7a55754d446b734d4859334f5334774e6d45304d4334344d6977304d4334344d6977774c4441734d5330344d5334324d797777566a45354e5334304d6d45784d5334324d7977784d5334324d7977774c4441734d5377794d7934794e697777646a49344c6a5932595445334c6a55314c4445334c6a55314c4441734d4377774c444d314c6a45734d4659784e4456424e4441754f4449734e4441754f4449734d4377774c4445734d5451774c4445304e56597a4d7a45754e545a684d5463754e5455734d5463754e5455734d4377774c4441734d7a55754d537777566a49784e793431614442574e4441754f4446684e4441754f4445734e4441754f4445734d4377784c4445734f4445754e6a49734d4659794f4445754e545a684d5445754e6a4d734d5445754e6a4d734d4377784c4445744d6a4d754d6a59734d4670744d6a45314c6a6b734e6a4d754e4545304d4334344e6977304d4334344e6977774c4441734d4377304d4467754e544d734d545131566a4d774d4334344e5745784e7934314e5377784e7934314e5377774c4441734d53307a4e5334774f537777646930794e6a42684e4441754f4449734e4441754f4449734d4377774c4441744f4445754e6a4d734d46597a4e7a41754f446c684d5463754e5455734d5463754e5455734d4377774c4445744d7a55754d537777566a4d7a4d4745784d5334324d7977784d5334324d7977774c4445734d4330794d7934794e697777646a51774c6a6732595451774c6a67784c4451774c6a67784c4441734d4377774c4467784c6a59794c4442574e4441754f4446684d5463754e5455734d5463754e5455734d4377774c4445734d7a55754d537777646a49324d4745304d4334344d6977304d4334344d6977774c4441734d4377344d5334324d797777566a45304e5745784e7934314e5377784e7934314e5377774c4445734d53777a4e5334784c4442574d6a67784c6a5532595445784c6a597a4c4445784c6a597a4c4441734d4377774c44497a4c6a49324c4442574d545131515451774c6a67314c4451774c6a67314c4441734d4377774c4451304f53347a4e5377784d4451754d6a46614969382b5043397a646d632b266c6f676f57696474683d3230" data-canonical-src="https://img.shields.io/badge/_Homepage-MiniMax-FF4040?style=flat-square&amp;labelColor=2C3E50&amp;logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgNDkwLjE2IDQxMS43Ij48ZGVmcz48c3R5bGU+LmNscy0xe2ZpbGw6I2ZmZjt9PC9zdHlsZT48L2RlZnM+PHBhdGggY2xhc3M9ImNscy0xIiBkPSJNMjMzLjQ1LDQwLjgxYTE3LjU1LDE3LjU1LDAsMSwwLTM1LjEsMFYzMzEuNTZhNDAuODIsNDAuODIsMCwwLDEtODEuNjMsMFYxNDVhMTcuNTUsMTcuNTUsMCwxLDAtMzUuMDksMHY3OS4wNmE0MC44Miw0MC44MiwwLDAsMS04MS42MywwVjE5NS40MmExMS42MywxMS42MywwLDAsMSwyMy4yNiwwdjI4LjY2YTE3LjU1LDE3LjU1LDAsMCwwLDM1LjEsMFYxNDVBNDAuODIsNDAuODIsMCwwLDEsMTQwLDE0NVYzMzEuNTZhMTcuNTUsMTcuNTUsMCwwLDAsMzUuMSwwVjIxNy41aDBWNDAuODFhNDAuODEsNDAuODEsMCwxLDEsODEuNjIsMFYyODEuNTZhMTEuNjMsMTEuNjMsMCwxLDEtMjMuMjYsMFptMjE1LjksNjMuNEE0MC44Niw0MC44NiwwLDAsMCw0MDguNTMsMTQ1VjMwMC44NWExNy41NSwxNy41NSwwLDAsMS0zNS4wOSwwdi0yNjBhNDAuODIsNDAuODIsMCwwLDAtODEuNjMsMFYzNzAuODlhMTcuNTUsMTcuNTUsMCwwLDEtMzUuMSwwVjMzMGExMS42MywxMS42MywwLDEsMC0yMy4yNiwwdjQwLjg2YTQwLjgxLDQwLjgxLDAsMCwwLDgxLjYyLDBWNDAuODFhMTcuNTUsMTcuNTUsMCwwLDEsMzUuMSwwdjI2MGE0MC44Miw0MC44MiwwLDAsMCw4MS42MywwVjE0NWExNy41NSwxNy41NSwwLDEsMSwzNS4xLDBWMjgxLjU2YTExLjYzLDExLjYzLDAsMCwwLDIzLjI2LDBWMTQ1QTQwLjg1LDQwLjg1LDAsMCwwLDQ0OS4zNSwxMDQuMjFaIi8+PC9zdmc+&amp;logoWidth=20">
  </a>
  <a href="https://arxiv.org/abs/2506.13585" rel="nofollow">
    <img alt="Paper" src="https://camo.githubusercontent.com/2604333fbd9843f0be50433505993398230120aca88af3c4dc94e4e8e3034c43/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f93965f50617065722d4d696e694d61782d2d4d312d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/📖_Paper-MiniMax--M1-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://chat.minimax.io/" rel="nofollow">
    <img alt="Chat" src="https://camo.githubusercontent.com/cf1a97c2a522fe9d780765d5cc263bf289cde77cb8a51435a994aaf202e3e89f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5f4d696e694d61785f436861742d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530266c6f676f3d646174613a696d6167652f7376672b786d6c3b6261736536342c50484e325a79423462577875637a30696148523063446f764c336433647935334d793576636d63764d6a41774d43397a646d6369494868746247357a4f6e68736157357250534a6f644852774f693876643364334c6e637a4c6d39795a7938784f546b354c336873615735724969423261575633516d393450534977494441674e446b774c6a4532494451784d533433496a34385a47566d637a3438633352356247552b4c6d4e736379307865325a706247773649325a6d5a6a74395043397a64486c735a5434384c32526c5a6e4d2b50484268644767675932786863334d39496d4e73637930784969426b50534a4e4d6a4d7a4c6a51314c4451774c6a6778595445334c6a55314c4445334c6a55314c4441734d5377774c544d314c6a45734d46597a4d7a45754e545a684e4441754f4449734e4441754f4449734d4377774c4445744f4445754e6a4d734d4659784e4456684d5463754e5455734d5463754e5455734d4377784c4441744d7a55754d446b734d4859334f5334774e6d45304d4334344d6977304d4334344d6977774c4441734d5330344d5334324d797777566a45354e5334304d6d45784d5334324d7977784d5334324d7977774c4441734d5377794d7934794e697777646a49344c6a5932595445334c6a55314c4445334c6a55314c4441734d4377774c444d314c6a45734d4659784e4456424e4441754f4449734e4441754f4449734d4377774c4445734d5451774c4445304e56597a4d7a45754e545a684d5463754e5455734d5463754e5455734d4377774c4441734d7a55754d537777566a49784e793431614442574e4441754f4446684e4441754f4445734e4441754f4445734d4377784c4445734f4445754e6a49734d4659794f4445754e545a684d5445754e6a4d734d5445754e6a4d734d4377784c4445744d6a4d754d6a59734d4670744d6a45314c6a6b734e6a4d754e4545304d4334344e6977304d4334344e6977774c4441734d4377304d4467754e544d734d545131566a4d774d4334344e5745784e7934314e5377784e7934314e5377774c4441734d53307a4e5334774f537777646930794e6a42684e4441754f4449734e4441754f4449734d4377774c4441744f4445754e6a4d734d46597a4e7a41754f446c684d5463754e5455734d5463754e5455734d4377774c4445744d7a55754d537777566a4d7a4d4745784d5334324d7977784d5334324d7977774c4445734d4330794d7934794e697777646a51774c6a6732595451774c6a67784c4451774c6a67784c4441734d4377774c4467784c6a59794c4442574e4441754f4446684d5463754e5455734d5463754e5455734d4377774c4445734d7a55754d537777646a49324d4745304d4334344d6977304d4334344d6977774c4441734d4377344d5334324d797777566a45304e5745784e7934314e5377784e7934314e5377774c4445734d53777a4e5334784c4442574d6a67784c6a5532595445784c6a597a4c4445784c6a597a4c4441734d4377774c44497a4c6a49324c4442574d545131515451774c6a67314c4451774c6a67314c4441734d4377774c4451304f53347a4e5377784d4451754d6a46614969382b5043397a646d632b266c6f676f57696474683d3230" data-canonical-src="https://img.shields.io/badge/_MiniMax_Chat-FF4040?style=flat-square&amp;labelColor=2C3E50&amp;logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgNDkwLjE2IDQxMS43Ij48ZGVmcz48c3R5bGU+LmNscy0xe2ZpbGw6I2ZmZjt9PC9zdHlsZT48L2RlZnM+PHBhdGggY2xhc3M9ImNscy0xIiBkPSJNMjMzLjQ1LDQwLjgxYTE3LjU1LDE3LjU1LDAsMSwwLTM1LjEsMFYzMzEuNTZhNDAuODIsNDAuODIsMCwwLDEtODEuNjMsMFYxNDVhMTcuNTUsMTcuNTUsMCwxLDAtMzUuMDksMHY3OS4wNmE0MC44Miw0MC44MiwwLDAsMS04MS42MywwVjE5NS40MmExMS42MywxMS42MywwLDAsMSwyMy4yNiwwdjI4LjY2YTE3LjU1LDE3LjU1LDAsMCwwLDM1LjEsMFYxNDVBNDAuODIsNDAuODIsMCwwLDEsMTQwLDE0NVYzMzEuNTZhMTcuNTUsMTcuNTUsMCwwLDAsMzUuMSwwVjIxNy41aDBWNDAuODFhNDAuODEsNDAuODEsMCwxLDEsODEuNjIsMFYyODEuNTZhMTEuNjMsMTEuNjMsMCwxLDEtMjMuMjYsMFptMjE1LjksNjMuNEE0MC44Niw0MC44NiwwLDAsMCw0MDguNTMsMTQ1VjMwMC44NWExNy41NSwxNy41NSwwLDAsMS0zNS4wOSwwdi0yNjBhNDAuODIsNDAuODIsMCwwLDAtODEuNjMsMFYzNzAuODlhMTcuNTUsMTcuNTUsMCwwLDEtMzUuMSwwVjMzMGExMS42MywxMS42MywwLDEsMC0yMy4yNiwwdjQwLjg2YTQwLjgxLDQwLjgxLDAsMCwwLDgxLjYyLDBWNDAuODFhMTcuNTUsMTcuNTUsMCwwLDEsMzUuMSwwdjI2MGE0MC44Miw0MC44MiwwLDAsMCw4MS42MywwVjE0NWExNy41NSwxNy41NSwwLDEsMSwzNS4xLDBWMjgxLjU2YTExLjYzLDExLjYzLDAsMCwwLDIzLjI2LDBWMTQ1QTQwLjg1LDQwLjg1LDAsMCwwLDQ0OS4zNSwxMDQuMjFaIi8+PC9zdmc+&amp;logoWidth=20">
  </a>
  <a href="https://www.minimax.io/platform" rel="nofollow">
    <img alt="API" src="https://camo.githubusercontent.com/033a96d7a4beb7f7872861878556c078064d874835da92f5d2ff5a0fe037c960/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29aa15f4150492d506c6174666f726d2d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/⚡_API-Platform-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://github.com/MiniMax-AI/MiniMax-MCP">
    <img alt="MCP" src="https://camo.githubusercontent.com/a1dd6a9aad054731a18f4af8cc4f477aa43f9cf83920e1676324b97204238b5e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f9a805f4d43502d4d696e694d61785f4d43502d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/🚀_MCP-MiniMax_MCP-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
</p>
<p><a href="https://huggingface.co/MiniMaxAI" rel="nofollow">
    <img alt="Hugging Face" src="https://camo.githubusercontent.com/94aa40386a1540394a4a71cdd73d62c70d4639e122e71df56f06b8a404754daa/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa4975f48756767696e675f466163652d4d696e694d61782d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/🤗_Hugging_Face-MiniMax-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://github.com/MiniMax-AI/MiniMax-M1">
    <img alt="GitHub" src="https://camo.githubusercontent.com/f8147f98bcc14eb4bdbeba8461a7af5189604ad0ac027765507eaef31fb3456c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f90995f4769744875622d4d696e694d61782d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/🐙_GitHub-MiniMax-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://www.modelscope.cn/organization/MiniMax" rel="nofollow">
    <img alt="ModelScope" src="https://camo.githubusercontent.com/0715f6acf7fc905d6a032c08bd8f41b03f492fda6b3f70bf565e853643052b05/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa496efb88f5f4d6f64656c53636f70652d4d696e694d61782d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/🤖️_ModelScope-MiniMax-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/LICENSE">
    <img alt="License" src="https://camo.githubusercontent.com/938892ddd7b2f59b079c0be16d5b388f03a3ff8868dbcf480033d0ae3690964f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29a96efb88f5f4c6963656e73652d4170616368655f322e302d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/⚖️_License-Apache_2.0-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
  <a href="https://github.com/MiniMax-AI/MiniMax-01/blob/main/figures/wechat-qrcode.jpeg">
    <img alt="WeChat" src="https://camo.githubusercontent.com/e2a1862bfaa62514518f7eba6c5ad931f8b898d3ebf9867f69f29a83c0d4131d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f92ac5f5765436861742d4d696e694d61782d4646343034303f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d324333453530" data-canonical-src="https://img.shields.io/badge/💬_WeChat-MiniMax-FF4040?style=flat-square&amp;labelColor=2C3E50">
  </a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">MiniMax-M1</h2><a id="user-content-minimax-m1" aria-label="Permalink: MiniMax-M1" href="#minimax-m1"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">1. Model Overview</h2><a id="user-content-1-model-overview" aria-label="Permalink: 1. Model Overview" href="#1-model-overview"></a></p>
<p dir="auto">We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model.
MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning
attention mechanism. The model is developed based on our previous <a href="https://huggingface.co/MiniMaxAI/MiniMax-Text-01" rel="nofollow">MiniMax-Text-01 model</a>,
which contains a total of 456 billion parameters with 45.9 billion parameters activated
per token. Consistent with MiniMax-Text-01, the M1 model natively supports a context length of 1
million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism
in MiniMax-M1 enables efficient scaling of test-time compute – For example, compared to DeepSeek
R1, M1 consumes 25% of the FLOPs at a generation length of 100K tokens. These properties make M1
particularly suitable for complex tasks that require processing long inputs and thinking extensively.
MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems ranging from
traditional mathematical reasoning to sandbox-based, real-world software engineering environments.
We develop an efficient RL scaling framework for M1 highlighting two perspectives: (1) We propose
CISPO, a novel algorithm that clips importance sampling weights instead of token updates, which
outperforms other competitive RL variants; (2) Our hybrid-attention design naturally enhances the
efficiency of RL, where we address unique challenges when scaling RL with the hybrid architecture. We
train two versions of MiniMax-M1 models with <a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-40k" rel="nofollow">40K</a> and
<a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-80k" rel="nofollow">80K</a> thinking budgets respectively. Experiments
on standard benchmarks show that our models outperform other strong open-weight models such as
the original DeepSeek-R1 and Qwen3-235B, particularly on complex software engineering, tool using,
and long context tasks. With efficient scaling of test-time compute, MiniMax-M1 serves as a strong
foundation for next-generation language model agents to reason and tackle real-world challenges.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/figures/TextBench.png"><img width="100%" src="https://github.com/MiniMax-AI/MiniMax-M1/raw/main/figures/TextBench.png"></a>
  <br>
  <em>Benchmark performance comparison of leading commercial and open-weight models across competition-level mathematics, coding, software engineering, agentic tool use, and long-context understanding tasks. We use the MiniMax-M1-80k model here for MiniMax-M1.</em>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">2. Evaluation</h2><a id="user-content-2-evaluation" aria-label="Permalink: 2. Evaluation" href="#2-evaluation"></a></p>
<p dir="auto"><strong>Performance of MiniMax-M1 on core benchmarks.</strong></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><strong>Category</strong></th>
<th><strong>Task</strong></th>
<th><strong>MiniMax-M1-80K</strong></th>
<th><strong>MiniMax-M1-40K</strong></th>
<th><strong>Qwen3-235B-A22B</strong></th>
<th><strong>DeepSeek-R1-0528</strong></th>
<th><strong>DeepSeek-R1</strong></th>
<th><strong>Seed-Thinking-v1.5</strong></th>
<th><strong>Claude 4 Opus</strong></th>
<th><strong>Gemini 2.5 Pro (06-05)</strong></th>
<th><strong>OpenAI-o3</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td><em>Extended Thinking</em></td>
<td><em>80K</em></td>
<td><em>40K</em></td>
<td><em>32k</em></td>
<td><em>64k</em></td>
<td><em>32k</em></td>
<td><em>32k</em></td>
<td><em>64k</em></td>
<td><em>64k</em></td>
<td><em>100k</em></td>
</tr>
<tr>
<td><em><strong>Mathematics</strong></em></td>
<td>AIME 2024</td>
<td>86.0</td>
<td>83.3</td>
<td>85.7</td>
<td>91.4</td>
<td>79.8</td>
<td>86.7</td>
<td>76.0</td>
<td>92.0</td>
<td>91.6</td>
</tr>
<tr>
<td></td>
<td>AIME 2025</td>
<td>76.9</td>
<td>74.6</td>
<td>81.5</td>
<td>87.5</td>
<td>70.0</td>
<td>74.0</td>
<td>75.5</td>
<td>88.0</td>
<td>88.9</td>
</tr>
<tr>
<td></td>
<td>MATH-500</td>
<td>96.8</td>
<td>96.0</td>
<td>96.2</td>
<td>98.0</td>
<td>97.3</td>
<td>96.7</td>
<td>98.2</td>
<td>98.8</td>
<td>98.1</td>
</tr>
<tr>
<td><em><strong>General Coding</strong></em></td>
<td>LiveCodeBench <em>(24/8~25/5)</em></td>
<td>65.0</td>
<td>62.3</td>
<td>65.9</td>
<td>73.1</td>
<td>55.9</td>
<td>67.5</td>
<td>56.6</td>
<td>77.1</td>
<td>75.8</td>
</tr>
<tr>
<td></td>
<td>FullStackBench</td>
<td>68.3</td>
<td>67.6</td>
<td>62.9</td>
<td>69.4</td>
<td>70.1</td>
<td>69.9</td>
<td>70.3</td>
<td>--</td>
<td>69.3</td>
</tr>
<tr>
<td><em><strong>Reasoning &amp; Knowledge</strong></em></td>
<td>GPQA Diamond</td>
<td>70.0</td>
<td>69.2</td>
<td>71.1</td>
<td>81.0</td>
<td>71.5</td>
<td>77.3</td>
<td>79.6</td>
<td>86.4</td>
<td>83.3</td>
</tr>
<tr>
<td></td>
<td>HLE <em>(no tools)</em></td>
<td>8.4*</td>
<td>7.2*</td>
<td>7.6*</td>
<td>17.7*</td>
<td>8.6*</td>
<td>8.2</td>
<td>10.7</td>
<td>21.6</td>
<td>20.3</td>
</tr>
<tr>
<td></td>
<td>ZebraLogic</td>
<td>86.8</td>
<td>80.1</td>
<td>80.3</td>
<td>95.1</td>
<td>78.7</td>
<td>84.4</td>
<td>95.1</td>
<td>91.6</td>
<td>95.8</td>
</tr>
<tr>
<td></td>
<td>MMLU-Pro</td>
<td>81.1</td>
<td>80.6</td>
<td>83.0</td>
<td>85.0</td>
<td>84.0</td>
<td>87.0</td>
<td>85.0</td>
<td>86.0</td>
<td>85.0</td>
</tr>
<tr>
<td><em><strong>Software Engineering</strong></em></td>
<td>SWE-bench Verified</td>
<td>56.0</td>
<td>55.6</td>
<td>34.4</td>
<td>57.6</td>
<td>49.2</td>
<td>47.0</td>
<td>72.5</td>
<td>67.2</td>
<td>69.1</td>
</tr>
<tr>
<td><em><strong>Long Context</strong></em></td>
<td>OpenAI-MRCR <em>(128k)</em></td>
<td>73.4</td>
<td>76.1</td>
<td>27.7</td>
<td>51.5</td>
<td>35.8</td>
<td>54.3</td>
<td>48.9</td>
<td>76.8</td>
<td>56.5</td>
</tr>
<tr>
<td></td>
<td>OpenAI-MRCR <em>(1M)</em></td>
<td>56.2</td>
<td>58.6</td>
<td>--</td>
<td>--</td>
<td>--</td>
<td>--</td>
<td>--</td>
<td>58.8</td>
<td>--</td>
</tr>
<tr>
<td></td>
<td>LongBench-v2</td>
<td>61.5</td>
<td>61.0</td>
<td>50.1</td>
<td>52.1</td>
<td>58.3</td>
<td>52.5</td>
<td>55.6</td>
<td>65.0</td>
<td>58.8</td>
</tr>
<tr>
<td><em><strong>Agentic Tool Use</strong></em></td>
<td>TAU-bench <em>(airline)</em></td>
<td>62.0</td>
<td>60.0</td>
<td>34.7</td>
<td>53.5</td>
<td>--</td>
<td>44.0</td>
<td>59.6</td>
<td>50.0</td>
<td>52.0</td>
</tr>
<tr>
<td></td>
<td>TAU-bench <em>(retail)</em></td>
<td>63.5</td>
<td>67.8</td>
<td>58.6</td>
<td>63.9</td>
<td>--</td>
<td>55.7</td>
<td>81.4</td>
<td>67.0</td>
<td>73.9</td>
</tr>
<tr>
<td><em><strong>Factuality</strong></em></td>
<td>SimpleQA</td>
<td>18.5</td>
<td>17.9</td>
<td>11.0</td>
<td>27.8</td>
<td>30.1</td>
<td>12.9</td>
<td>--</td>
<td>54.0</td>
<td>49.4</td>
</tr>
<tr>
<td><em><strong>General Assistant</strong></em></td>
<td>MultiChallenge</td>
<td>44.7</td>
<td>44.7</td>
<td>40.0</td>
<td>45.0</td>
<td>40.7</td>
<td>43.0</td>
<td>45.8</td>
<td>51.8</td>
<td>56.5</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">* conducted on the text-only HLE subset.</p>
<p dir="auto">Our models are evaluated with <code>temperature=1.0</code>, <code>top_p=0.95</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">SWE-bench methodology</h3><a id="user-content-swe-bench-methodology" aria-label="Permalink: SWE-bench methodology" href="#swe-bench-methodology"></a></p>
<p dir="auto">We report results derived from the Agentless scaffold. Departing from the original pipeline, our methodology employs a two-stage localization process (without any embedding-based retrieval mechanisms): initial coarse-grained file localization followed by fine-grained localization to specific files and code elements. The values for our models are calculated on the subset of n=486 verified tasks which work on our infrastructure. The excluded 14 test cases that were incompatible with our internal infrastructure are:
<code>"astropy__astropy-7606"</code>,
<code>"astropy__astropy-8707"</code>,
<code>"astropy__astropy-8872"</code>,
<code>"django__django-10097"</code>,
<code>"matplotlib__matplotlib-20488"</code>,
<code>"psf__requests-2317"</code>,
<code>"psf__requests-2931"</code>,
<code>"psf__requests-5414"</code>,
<code>"pylint-dev__pylint-6528"</code>,
<code>"pylint-dev__pylint-7277"</code>,
<code>"sphinx-doc__sphinx-10435"</code>,
<code>"sphinx-doc__sphinx-7985"</code>,
<code>"sphinx-doc__sphinx-8269"</code>,
<code>"sphinx-doc__sphinx-8475"</code></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">TAU-bench methodology</h3><a id="user-content-tau-bench-methodology" aria-label="Permalink: TAU-bench methodology" href="#tau-bench-methodology"></a></p>
<p dir="auto">We evaluate TAU-Bench with GPT-4.1 as user model and without any custom tools. The maximum number of interaction steps is 40.
Our general system prompt is:</p>
<div data-snippet-clipboard-copy-content="- In each round, you need to carefully examine the tools provided to you to determine if any can be used.
- You must adhere to all of the policies. Pay attention to the details in the terms. Solutions for most situations can be found within these policies."><pre><code>- In each round, you need to carefully examine the tools provided to you to determine if any can be used.
- You must adhere to all of the policies. Pay attention to the details in the terms. Solutions for most situations can be found within these policies.
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">3. Deployment Guide</h2><a id="user-content-3-deployment-guide" aria-label="Permalink: 3. Deployment Guide" href="#3-deployment-guide"></a></p>
<p dir="auto">Download the model from HuggingFace repository:</p>
<ul dir="auto">
<li><a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-40k" rel="nofollow">MiniMax-M1-40k</a></li>
<li><a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-80k" rel="nofollow">MiniMax-M1-80k</a></li>
</ul>
<p dir="auto">For production deployment, we recommend using <a href="https://docs.vllm.ai/en/latest/" rel="nofollow">vLLM</a> to serve MiniMax-M1. vLLM provides excellent performance for serving large language models with the following features:</p>
<ul dir="auto">
<li>🔥 Outstanding service throughout performance</li>
<li>⚡ Efficient and intelligent memory management</li>
<li>📦 Powerful batch request processing capability</li>
<li>⚙️ Deeply optimized underlying performance</li>
</ul>
<p dir="auto">For detailed vLLM deployment instructions, please refer to our <a href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/docs/vllm_deployment_guide.md">vLLM Deployment Guide</a>.
Alternatively, you can also deploy using Transformers directly. For detailed Transformers deployment instructions, you can see our <a href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/docs/transformers_deployment_guide.md">MiniMax-M1 Transformers Deployment Guide</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">4. Function Calling</h2><a id="user-content-4-function-calling" aria-label="Permalink: 4. Function Calling" href="#4-function-calling"></a></p>
<p dir="auto">The MiniMax-M1 model supports function calling capabilities, enabling the model to identify when external functions need to be called and output function call parameters in a structured format. <a href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/docs/function_call_guide.md">MiniMax-M1 Function Call Guide</a> provides detailed instructions on how to use the function calling feature of MiniMax-M1.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">5. Chatbot &amp; API</h2><a id="user-content-5-chatbot--api" aria-label="Permalink: 5. Chatbot &amp; API" href="#5-chatbot--api"></a></p>
<p dir="auto">For general use and evaluation, we provide a <a href="https://chat.minimax.io/" rel="nofollow">Chatbot</a> with online search capabilities and the <a href="https://www.minimax.io/platform/" rel="nofollow">online API</a> for developers. For general use and evaluation, we provide the <a href="https://github.com/MiniMax-AI/MiniMax-MCP">MiniMax MCP Server</a> with video generation, image generation, speech synthesis, and voice cloning for developers.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">6. Contact Us</h2><a id="user-content-6-contact-us" aria-label="Permalink: 6. Contact Us" href="#6-contact-us"></a></p>
<p dir="auto">Contact us at <a href="mailto:model@minimax.io">model@minimax.io</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI is eating our brains. MIT study: Your brain on ChatGPT (120 pts)]]></title>
            <link>https://www.media.mit.edu/projects/your-brain-on-chatgpt/overview/</link>
            <guid>44307257</guid>
            <pubDate>Wed, 18 Jun 2025 06:47:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.media.mit.edu/projects/your-brain-on-chatgpt/overview/">https://www.media.mit.edu/projects/your-brain-on-chatgpt/overview/</a>, See on <a href="https://news.ycombinator.com/item?id=44307257">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                <p>Check project's website:&nbsp;<a href="https://www.brainonllm.com/">https://www.brainonllm.com</a></p><p>With today's wide adoption of LLM products like ChatGPT from OpenAI, humans and businesses engage and use LLMs on a daily basis. Like any other tool, it carries its own set of advantages and limitations. This study focuses on finding out the <b>cognitive cost of using an LLM</b> in the educational context of writing an essay.</p><p>We assigned participants to three groups: <b>LLM group, Search Engine group, and Brain-only group, where each participant used a designated tool (or no tool in the latter) to write an essay</b>. We conducted 3 sessions with the same group assignment for each participant. In the 4th session we asked LLM group participants to use no tools (we refer to them as LLM-to-Brain), and the Brain-only group participants were asked to use LLM (Brain-to-LLM). We recruited a total of 54 participants for Sessions 1, 2, 3, and 18 participants among them completed session 4. </p><p>We used electroencephalography (EEG) to <b>record participants' brain activity </b>in order to assess their cognitive engagement and cognitive load, and to gain a deeper understanding of neural activations during the essay writing task. We performed <b>NLP analysis</b>, and we interviewed each participant after each session. We performed scoring with the help from the <b>human teachers and an AI judge</b> (a specially built AI agent).</p><p>We discovered a consistent homogeneity across the Named Entities Recognition (NERs), n-grams, ontology of topics within each group. EEG analysis presented robust evidence that LLM, Search Engine and Brain-only groups had <b>significantly different neural connectivity patterns</b>, reflecting divergent cognitive strategies. <b>Brain connectivity systematically scaled down with the amount of external support: the Brain‑only group exhibited the strongest, widest‑ranging networks, Search Engine group showed intermediate engagement, and LLM assistance elicited the weakest overall coupling.</b> In session 4, LLM-to-Brain participants showed weaker neural connectivity and under-engagement of alpha and beta networks; and the Brain-to-LLM participants demonstrated higher memory recall, and re‑engagement of widespread occipito-parietal and prefrontal nodes, likely supporting the visual processing, similar to the one frequently perceived in the Search Engine group. The reported<b> ownership </b>of LLM group's essays in the interviews <b>was low.</b> The Search Engine group had strong ownership, but lesser than the Brain-only group. The LLM group also <b>fell behind in their ability to quote</b> from the essays they wrote just minutes prior. </p><p>As the educational impact of LLM use only begins to settle with the general population, in this study we demonstrate the pressing matter of a likely <b>decrease in learning skills</b> based on the results of our study. The use of LLM had a measurable impact on participants, and while the benefits were initially apparent, as we demonstrated over the course of 4 months, the <b>LLM group's participants performed worse than their counterparts in the Brain-only group at all levels: neural, linguistic, scoring.</b></p><p>We hope this study serves as a preliminary guide to understanding the cognitive and practical impacts of AI on learning environments.</p><p>#cognitivedebt #brainonllm #yourbrainonchatgpt&nbsp;</p>
                            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Make little apps for you and your friends (365 pts)]]></title>
            <link>https://pontus.granstrom.me/scrappy/</link>
            <guid>44306859</guid>
            <pubDate>Wed, 18 Jun 2025 05:16:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pontus.granstrom.me/scrappy/">https://pontus.granstrom.me/scrappy/</a>, See on <a href="https://news.ycombinator.com/item?id=44306859">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Software is important to people. Most of us spend our workdays in front of computers. We use the computer in our pocket tens if not hundreds of times every day. The apps we use are almost exclusively mass-market, sold on an app-store, made for thousands if not millions of users. Or they are enterprise apps that are custom-built for hundreds of thousands of dollars.</p><p>But there isn’t really any equivalent of home-made software — apps made lovingly by you for your friends and family. Apps that aren’t polished or flashy, but are made to <em>your</em> <em>preference</em> and help you with <em>your particular needs.</em></p><p>We’re John and Pontus, and we’ve been exploring the potential of home-made software together.</p><p>We ended up creating a research prototype that we call <strong>Scrappy</strong> — a tool for making <strong>scrappy apps for just you and your friends.</strong> First and foremost, we aim to contribute a <em>vision</em> of what home-made software could be like. We want to make this vision as concrete as we can, by sharing a working tool and examples of apps made in it. Scrappy, in its current state, is a prototype, not a robust tool, but we hope it paints the picture we carry in our heads — of software as something that can be creative, personal, expressive. Made by anyone, for themselves and their loved ones.</p><h2 id="what-is-scrappy">What is Scrappy?</h2><p>It may not be clear what “a scrappy app for you and your friends” means. What kind of apps are these? Let us paint a picture with a few examples. (We call them “<strong>Scrapps</strong>”.)</p><div><div><p><strong>Arithmetic practice for a kid in elementary school.</strong> When outgrown, the Scrapp can be extended with harder problems.<br>(<a href="https://scrappy.jrcpl.us/s/?template=/examples/math_practice.json">try on desktop</a>)</p><video src="https://pontus.granstrom.me/scrappy/examples/math_practice.mp4" poster="https://pontus.granstrom.me/scrappy/examples/math_practice.jpg" controls=""></video></div><div><p><strong>Attendee counter for a local event.</strong> The counter’s state is shared, so the Scrapp can be used to let people in and out at multiple entrances.<br>(<a href="https://scrappy.jrcpl.us/s/?template=/examples/attendee_counter.json">try on desktop</a>)</p><video src="https://pontus.granstrom.me/scrappy/examples/attendee_counter.mp4" poster="https://pontus.granstrom.me/scrappy/examples/attendee_counter.jpg" controls=""></video></div></div><div><div><p><strong>Meeting cost clock,</strong> to help meetings stay on track. A Scrapp like this can be put together in 15 minutes and shared with coworkers right away.<br>(<a href="https://scrappy.jrcpl.us/s/?template=/examples/meeting_cost_clock.json">try on desktop</a>)</p><video src="https://pontus.granstrom.me/scrappy/examples/meeting_cost_clock.mp4" poster="https://pontus.granstrom.me/scrappy/examples/meeting_cost_clock.jpg" controls=""></video></div><div><p><strong>Weekly chore tracker.</strong> Let roommates flexibly swap weeks, while making sure to track whose up next, to keep things fair.<br>(<a href="https://scrappy.jrcpl.us/s/?template=/examples/chores.json">try on desktop</a>)</p><video src="https://pontus.granstrom.me/scrappy/examples/chores.mp4" poster="https://pontus.granstrom.me/scrappy/examples/chores.jpg" controls=""></video></div></div><h2 id="what-is-it-like-to-make-an-app-in-scrappy">What is it like to make an app in Scrappy?</h2><p>Scrappy is an infinite canvas of interactive objects. The workflow is similar to an app such as Figma, Miro, or Google Slides — except you can attach behaviors to the objects.</p><p>You drag objects out on the canvas — a button, a textfield, a few labels. Select an object, and you can modify its attribute in an inspector panel. Certain objects, like buttons, has attributes like “when clicked” that contain javascript code. When the button is clicked, that code is run — maybe it records the contents of the textfield to a label that acts as a log. You build your app step by step: tweaking and rearranging the objects, and attaching a little bit of code to them.</p><p>There’s no better way to get a feeling for an authoring environment than to see someone use it in action. In the following videos, I’m making an attendee counter for an event.</p><p><strong>The basics.</strong> I start out by adding a number field to track the number of attendees, and two buttons for recording people entering and exiting the venue.</p><video controls="" playsinline="" poster="https://pontus.granstrom.me/scrappy/walkthrough/basics.jpg">
<source src="https://pontus.granstrom.me/scrappy/walkthrough/basics.mp4" type="video/mp4">Your browser cannot play this video.</video><p><strong>Reactive formulas.</strong> Next, I add a field for the venue’s capacity, and a warning when too many people have been let in.
I use a reactive formula to control the visibility of the warning and the border color of the field.</p><video controls="" playsinline="" poster="https://pontus.granstrom.me/scrappy/walkthrough/reactive-formulas.jpg">
<source src="https://pontus.granstrom.me/scrappy/walkthrough/reactive-formulas.mp4" type="video/mp4">Your browser cannot play this video.</video><p><strong>A shared, persistent world.</strong> Without any extra work, Scrappy apps are multiplayer.
App state is persisted and synced, like users expect from online documents like Google Sheets or Figma.</p><video controls="" playsinline="" poster="https://pontus.granstrom.me/scrappy/walkthrough/shared-persistent-world.jpg">
<source src="https://pontus.granstrom.me/scrappy/walkthrough/shared-persistent-world.mp4" type="video/mp4">Your browser cannot play this video.</video><p><strong>The app is always live.</strong> There’s no distinction between editing and running. I can edit the app while a friend is using it.</p><video controls="" playsinline="" poster="https://pontus.granstrom.me/scrappy/walkthrough/liveness.jpg">
<source src="https://pontus.granstrom.me/scrappy/walkthrough/liveness.mp4" type="video/mp4">Your browser cannot play this video.</video><p><strong>Selective sharing.</strong> I make a variant of the app that’s limited to only entering and exiting people. This is done by putting a part of the app in a frame, and sharing only that frame. The limited version is still linked to the main app.</p><video controls="" playsinline="" poster="https://pontus.granstrom.me/scrappy/walkthrough/selective-sharing.jpg">
<source src="https://pontus.granstrom.me/scrappy/walkthrough/selective-sharing.mp4" type="video/mp4">Your browser cannot play this video.</video><div><div><p><strong>Visible, tangible data.</strong> Here’s what the <a href="https://www.notion.so/Scrappy-Make-Little-Apps-for-You-and-Your-Friends-1ba27a2e3bb6806fb782cf2ff7e5764e?pvs=21">Meeting Cost Clock app</a> shown above looks like when zoomed out, revealing a common pattern in Scrapps.</p><p>Outside the shared frame are a bunch of fields used to compute the cost of the meeting. This lets me see the data while I’m working on the Scrapp, just like in a spreadsheet, which is very helpful for debugging — and it makes future tweaking or remixing easier.</p></div><p><img src="https://pontus.granstrom.me/scrappy/walkthrough/meeting_cost_clock_internals.png" alt="Meeting cost clock internals"></p></div><h2 id="why-make-scrappy">Why make Scrappy?</h2><p>This project is driven by a desire to reimagine software creation and use. As part of a growing movement variously termed “<a href="https://hackernoon.com/big-and-small-computing-73dc49901b9a">small computing</a>,” “<a href="https://dubroy.com/blog/casual-programming/">casual programming</a>”, and “<a href="https://maggieappleton.com/home-cooked-software">home-cooked software</a>” we want to <a href="https://www.inkandswitch.com/end-user-programming/">emancipate end-users</a> — to “empower people to express themselves without requiring them to be heavy-duty programmers,” to “liberate the programming of computers from the priesthood to the layperson”, as Bill Atkinson worded it. We want to shift the world away from mass-market, industrially-produced software toward more <a href="https://x.com/davidhoang/status/1802140453292372272">personal, even disposable,</a> tools that are designed for and readily <a href="https://malleable.systems/">modified and adapted</a> to <a href="https://gwern.net/doc/technology/2004-03-30-shirky-situatedsoftware.html">specific social contexts</a>. Above all, we want to foster a sense of agency and to ultimately contribute to “<a href="https://x.com/cwervo/status/1808578326409457834">redistributing the means of software production</a>”.</p><p>We were inspired by the simplicity of tools like <a href="https://www.notion.so/">Notion</a>, <a href="https://www.tldraw.com/">tldraw</a>, and <a href="https://mmm.page/">mmm.page</a>, but wanted to empower people with richer interactivity and programming capabilities. However, knowing the strengths and limitations of the standard visual programming paradigms of blocks (e.g. <a href="https://scratch.mit.edu/">Scratch</a>, <a href="https://developers.google.com/blockly">Blockly</a>) and nodes-and-wires (e.g. <a href="https://cycling74.com/products/max">Max/MSP</a>, <a href="https://nodered.org/">Node-RED</a>, <a href="https://natto.dev/">natto</a>, <a href="https://www.holograph.so/">Holograph</a>), we deliberately wanted to go down a different path. Instead, we drew direct inspiration from “media with scripting” environments, both classic systems like <a href="https://en.wikipedia.org/wiki/HyperCard">HyperCard</a>, <a href="https://en.wikipedia.org/wiki/Visual_Basic_(classic)">Visual Basic</a>, and <a href="https://en.wikipedia.org/wiki/Adobe_Director">Macromedia Director</a>, as well as contemporary platforms like <a href="https://www.notion.so/Project-Concept-Definition-618cd9f0e26944f3b1ee4222c1db92c9?pvs=21">Dynamicland</a> and <a href="https://www.minecraft.net/">Minecraft</a>, where the “media with scripting” exist in a shared online world.</p><p>Overall, our target user experience was that of a productivity tool, specifically a canvas-based tool (e.g. <a href="https://www.figma.com/">Figma</a>, <a href="https://miro.com/">Miro</a>, and <a href="https://www.tldraw.com/">tldraw</a>)—rather than programming environments (e.g. <a href="https://squeak.org/">Squeak/Smalltalk</a>, modern IDEs) and website and app builders (e.g. <a href="https://www.squarespace.com/">Squarespace</a>, <a href="https://mmm.page/">mmm.page</a>, <a href="https://bubble.io/">Bubble</a>). And we also wanted that kind of modern “share link”-based real-time collaboration popularized by <a href="https://docs.google.com/">Google Docs</a> and <a href="https://www.figma.com/">Figma</a>.</p><p>Finally, while we acknowledge the capabilities of AI-centric systems that leverage LLMs for code generation (e.g., <a href="https://lovable.dev/">Lovable</a>, <a href="http://bolt.new/">bolt.new</a>, and <a href="https://computer.tldraw.com/">tldraw computer</a>), we deliberately chose to focus our design on direct manipulation and user control.</p><h2 id="who-is-scrappy-for">Who is Scrappy for?</h2><p>As we were prototyping, it wasn’t clear who the ideal user for Scrappy was. We left this open, to see what we’d learn from building the system. Eventually, a few potential personas revealed themselves.</p><ul><li><strong>The process optimizer.</strong> In business environments, there’s always some improvement that can be made using software. But the person who sees the process inefficiency likely can’t make software themselves, and involving a professional programmer is expensive. So what usually happens is they make what improvements they can using tools they are familiar with, such as Excel. Here Scrappy could be a more powerful and flexible Excel, while retaining familiarity and ease of use.</li><li><strong>Teachers and students</strong>. Teaching programming requires teaching a multitude of inessential technical details: how to use the command line, how file systems work, how to set up the environment, dependency management, version control, servers and clients, and on and on. With Scrappy, you can just create a button, write a line of code and click the button to run the code.</li><li><strong>Ourselves!</strong> We are professional programmers who don’t like programming. Why? Because of the all the aforementioned complexity that adds friction to what could be so much simpler. When making mass-market apps, we know we have to deal with that complexity, but when working on a fun hobby project?! Give us a break. Scrappy is that kind of break.</li><li><strong>The DIYer.</strong> People like to customize their house, grow their own vegetables, sow their own clothes, build their own furniture. Scrappy is where a DIY-inclined person makes their own little apps for themselves and their friends.</li></ul><p>As Scrappy solidified, we wanted to focus on one of these personas. There’s a pull toward business use cases, since businesses are the most willing to pay for a product, but we believe the incentives there would lead us too close to existing products like <a href="https://retool.com/">Retool</a> or <a href="https://livecode.com/">LiveCode</a>. The teaching use case is compelling, but we believe it needs a better coding experience (discoverability, better error messages, debugger) which was out of scope for us (for now). We are itching to make stuff for ourselves in Scrappy (and we are strong believers in dogfooding), but most of our projects required features that would balloon the scope.</p><p>The DIYer making home-made software is the least served by existing tools, and fits our vision of democratized computing the best. We decided this is where we could make the biggest contribution (the <a href="https://en.wikipedia.org/wiki/Blue_Ocean_Strategy">blue ocean strategy</a>), and decided to make the DIYer our target persona.</p><p>Ideally, Scrappy would let anyone with basic computer literacy make a simple app and learn from there. This is not quite the case yet — some JavaScript knowledge is required. So today, the person making Scrapps from scratch is a <strong>programmer DIYer.</strong> But when a Scrapp is shared with friends, those friends can use it and remix it without needing programming experience.</p><h2 id="what-should-i-make-in-scrappy">What should I make in Scrappy?</h2><p>Home-made, scrappy apps don’t really exist today, so most people (including us!) are not used to coming up with ideas for them. When faced with a problem that would make a great Scrapp, instead our minds go to “maybe there’s an app for that”, searching the web for one, giving up if we cannot find a good one. To start coming up with good uses for Scrappy requires a shift to a home-made mindset.</p><p>To help you build that mindset, here is an assortment of ideas for Scrapps (some of which are not feasible in the current prototype of Scrappy, but should be).</p><div><div><ul><li>Custom flashcards</li><li>Meeting agenda manager</li><li>Day clock for person with dementia</li><li>Online workshop facilitation</li><li>Consulting time tracker</li><li>Point-based voting for a board</li><li>Receipt generator</li><li>Simple word game</li><li>School grade calculator</li><li>Interactive visual recipe</li><li>Social quiz game</li></ul></div><div><ul><li>Typing tutor</li><li>Lyric writing aids (synonyms, rhymes)</li><li>Board game helper</li><li>Wedding RSVP + seating arrangement</li><li>Dynamic opening hours display</li><li>Family bulletin board</li><li>Group travel planner</li><li>Chore → allowance calculator</li><li>Chess clock productivity timer</li></ul></div></div><p>What makes a problem well-suited for Scrappy? Here are some things they have in common:</p><ul><li><strong>Shared with friends.</strong> While a Scrapp can be for just yourself, Scrappy really shines with multiples users, leveraging the shared, persistent world. Some problems that would need setting up a backend server can be built in minutes in Scrappy.</li><li><strong>Needs tweaking-as-you-go.</strong> Life changes, and so does requirements. In Scrappy, you can edit the app at any time — even while your friend is using it. No building, no deploying, no fuss.</li><li><strong>A sprinkle of computation.</strong> Scrappy shines when thought of as a shared document first, with a little bit of computation added on top. For complex systems with a lot of moving parts, we recommend reaching for traditional software engineering tools.</li><li><strong>Minimal friction.</strong> We all let out a groan inside whenever we are hit with “create an account to continue”. This “account friction” may not be much, but it multiplies when sharing with a group of people — there’s always going to be someone for whom the friction is too much. Scrapps don’t have this problem: just click the link.</li><li><strong>Small number of trusted users.</strong> Scrappy assumes you trust the people you share a Scrapp with, which removes a lot of friction, but if you need to control access and permissions, look elsewhere.</li><li><strong>Not mission-critical.</strong> If you need guaranteed correctness or perfect control over details, don’t reach for Scrappy. Those qualities are what you pay expert engineers for.</li></ul><h2 id="scrappy-vs-mass-market-apps">Scrappy vs mass-market apps</h2><p>When faced with a “scrappy” problem — something small that would benefit from a computer — most people will think “maybe there’s an app for that”, followed by searching an app store or the Internet to look for one.</p><p>If there is no app for that, or there’s no good one, you could make your own in Scrappy. We hope you do! But often there <em>is</em> an app for that. If there is, it will probably be more polished than anything you can make in Scrappy. In this case, there are still reasons to consider using making your own Scrapp:</p><ul><li><strong>Does exactly what you need.</strong> And only what you need. Nothing more, nothing less.</li><li><strong>Home-made with love.</strong> Scrapps are made by you for your friends. A home-knitted sweater will always mean more to you than a store-bought one.</li><li><strong>Fun and playful.</strong> In Scrappy, it’s easy to play around. Tweak the colors, make a cute layout, add little inside jokes.</li><li><strong>Remixable.</strong> Easy to share with others and modify to suit your needs.</li><li><strong>Collaborative by default.</strong> All Scrappy apps are multiplayer, like a Google Doc is. You can even edit them while they are being used by someone else!</li><li><strong>No accounts and signups.</strong> If you share a Scrappy app with someone, they can start using it right away — no tedious sign-up flows stopping your friends or family from joining in.</li><li><strong>You own your data.</strong> The data is stored locally and will only be used for nefarious purposes if its creator (you) wants to!</li></ul><h2 id="scrappy-vs-ai-written-apps">Scrappy vs AI-written apps</h2><p>What about asking an LLM to make a custom, home-made app?</p><p>LLMs are getting better and better, and while they are far from able to make a full-fledged app without a lot of help from a software engineer, they can make small apps pretty reliably.</p><p>So if I can ask ChatGPT or Claude to make an app, why would I use Scrappy?</p><ul><li><strong>Scrappy is understandable.</strong> Using an LLM means going from an English prompt to pages of React code, which is too big a leap for non-programmers. They end up having to rely on the LLM to make changes, and are left helpless if the LLM doesn’t do the right thing. In contrast, Scrappy’s objects-on-a-canvas model is easy to understand, more humane, and acts a shared substrate where user and AI can collaborate on equal footing. And because it is less overwhelming, it’s more likely the user will pick up some programming skills.</li><li><strong>Scrappy is collaborative.</strong> All Scrappy apps are little shared worlds, persistent and with live updating — all for free. LLMs are mainly useful for creating static front-end-only web apps. And in Scrappy, apps can be edited by multiple users in realtime, whereas AI workflows are mostly “type, then wait” with little room for collaboration between humans.</li><li><strong>Scrappy is more fun!</strong> While typing a few sentences of English and seeing a full app appear out of nowhere still feels like magic, it quickly grows old when you’re waiting for minutes only to see the LLM misunderstood you again. In Scrappy, there is joy in tweaking things or remixing something. A spark of “ooh I want it to do this” and it’s only a few clicks and keystrokes away. A sense of creative ownership. And you can edit it together with friends!</li></ul><h2 id="scrappy-vs-hypercard-and-its-successors">Scrappy vs HyperCard (and its successors)</h2><p><a href="https://hypercard.org/">HyperCard</a> was popular among Macintosh users in the early 90s, and is often held as an exemplar of enabling home-made software and end-user programming. Decades later, there have been a number of successors to HyperCard, both commercial (<a href="https://www.mackiev.com/hyperstudio/">HyperStudio</a>, <a href="https://en.wikipedia.org/wiki/SuperCard">SuperCard</a>, <a href="https://livecode.com/">LiveCode</a>) and non-commercial (<a href="https://beyondloom.com/decker/">Decker</a> and <a href="https://hypervariety.com/WildCard/">WildCard</a>, among a number of open-source remakes, most of which are abandonware). Most of these have been quite literal replicas of HyperCard, driven by nostalgia, down to the black-and-white graphics. None have been as successful as the original.</p><p>We wanted to create something in the spirit of HyperCard, rather than recreate HyperCard. Scrappy is different from HyperCard and its direct descendants in a few key ways:</p><ul><li><strong>Designed for the Internet.</strong> Scrappy apps are easily shareable online with a simple link, whereas using HyperCard and most of its descendants is like being trapped in a virtual machine.</li><li><strong>A shared world.</strong> HyperCard stacks could be shared as a file with other users. Scrappy takes this to the next level by letting users edit and use apps at the same time.</li><li><strong>Modern UI conventions.</strong> Scrappy apps live on a high-resolution infinite canvas, with selections, copying, panning and zooming, frames for grouping, etc.</li><li><strong>Uses JavaScript for scripting.</strong> HyperCard and a number of its descendants use programming languages that aren’t in common use. JavaScript is the most common programming language in the world, is native to the Web and works well for a dynamic environment such as Scrappy.</li><li><strong>A larger palette of interactive objects.</strong> Many HyperCard-likes only support a few elements like buttons, text fields, and images. Scrappy supports more UI elements like sliders and timers, but also data types beyond strings: numbers, dates, and compound JSON objects.</li><li><strong>Reactive formulas, like a spreadsheet.</strong> The idea of “this value changes when that value changes” is familiar to many, and can be a stepping stone toward event-based programming, where the user has to think about state.</li></ul><h2 id="future-directions">Future directions</h2><p>With our prototype, we think that we’ve been successful at proving the ideas and design principles that we started with. But there’s a lot more work to do. The number of Scrapps that can be built in a way that feel “Scrappy native” is still low. Much of the time, existing knowledge of JavaScript is required. To improve this, we need to continue work in both “lowering the floor” and “raising the ceiling”.</p><p>Lowering the floor means making things more friendly and approachable for people with little or no programming experience. For example:</p><ul><li><strong>Improve code discoverability.</strong> We’ve made coding easier by presenting the names of objects visually on the screen, and listing their methods in the properties panel. But there’s a ton more than we can do. You should be able to click on objects to discover their methods and insert them in the code. Available names should auto-complete so you don’t have remember syntax and do as much typing.</li><li><strong>Improve debugging.</strong> You should be able to visualize relationships between objects, perhaps as arrows showing which objects read or modify other objects. Error messages should be better worded and show more information about what went wrong. You should be able pause and rewind execution. All of this while live collaborating on the app with a friend.</li><li><strong>Leverage AI.</strong> <a href="https://www.notion.so/Scrappy-Make-Little-Apps-for-You-and-Your-Friends-1ba27a2e3bb6806fb782cf2ff7e5764e?pvs=21">As we mentioned earlier</a>, we don’t believe in having an LLM make an entire app, but we are interested in having it act as an assistant, directed by the user. Maybe you’d click on the canvas and ask the AI to “make start and stop buttons here”, or go to a text label’s “when changed” handler and ask the AI to write code to “show an error message if using non-english characters”.</li><li><strong>Make it even easier to share and remix</strong>. It’s easier to learn by inspecting and tweaking other people’ work than it is to start from a blank canvas. We imagine a public gallery where users can publish their creations, and other users can adopt and customize them for their own needs and preferences.</li><li><strong>Make Scrapps work well on phones/tablets.</strong> A hand-sized touchscreen is too small for editing Scrapps comfortably, but using them on phones should work well — this is not currently the case. The infinite canvas paradigm means that objects have fixed positions, which is a way simpler mental model layout rules (like in CSS), but means designs aren’t responsive to screen size. However, drag-and-drop web page design tools like <a href="http://mmm.page/">mmm.page</a> and <a href="https://www.squarespace.com/">Squarespace</a> show a way to handle this: simply show safe areas for mobile to the user.</li></ul><p>Raising the ceiling means adding functionality and expressive power, letting users create more things with less effort. For example:</p><ul><li><strong>Add support for collections</strong>. Currently, you can edit and store strings, numbers, dates, and JSON data, but you cannot store lists of them or make an editable tables, like in a spreadsheet. We also don’t have any kind of layout containers, like lists, grids, or stacks. Adding this would let authors express more things visually, and they wouldn’t have to resort to JavaScript knowledge and hidden state.</li><li><strong>Instanced frames.</strong> Frames let you <a href="https://www.notion.so/Submission-writeup-1a227a2e3bb680c5be5ddc988af65ce2?pvs=21">selectively share</a> parts of your app, but that frame is fully synced in real-time across users. This is desirable in some cases and undesirable in others. For example, when sharing a form, each user should only see and edit their own copy. You should be able to share instances of the form, that still collects all the data in one place. Another example is a board game helper where there’s some hidden information and users should see some shared UI and some UI only visible to them.</li><li><strong>Tools for data processing.</strong> We’ve found ourselves wanting to use Scrappy to process tabular data. Things like: do the same operations to all rows, filter the table based on some criteria, etc. This can be done in JavaScript, but there should be a Scrappy-native way of doing this, where the data is shown.</li><li><strong>Better support for reuse.</strong> Currently, if you want to repeat an object or set of objects in your Scrapp, you have to manually edit them one by one, or write code to manage them. Instead, you should be able to define a reusable component and make instances that stay linked to the main component. Figma has this, PowerPoint has slide masters, HyperCard has card backgrounds, all to this effect. Further, these components could be shared across projects, or even with other users.</li><li><strong>Allow extending Scrappy.</strong> Some capabilities will be out of reach using Scrappy’s primitives. Currently, we are the only ones able to add new objects, but we’d want to open this up to more people. We expect this would require programming and web expertise, so it wouldn’t be something for a traditional engineer, not the typical Scrappy user.</li><li><strong>Clean up the conceptual model.</strong> Currently, some of the objects store data values, and some support event handlers like “when clicked” and “when changed” handlers. The current implementation is a bit arbitrary about this, which is not only confusing but also limiting. Common behaviors like editing, clicking, and storing data should be made more consistent and freely “mixable” — like the <a href="https://en.wikipedia.org/wiki/Entity_component_system">entity component systems</a> of game engines like <a href="https://unity.com/">Unity</a>.</li></ul><h2 id="conclusion">Conclusion</h2><p>We believe computers should work for people, and dream of a future where computing, like cooking or word processing, is available to everyone. Where you can solve your own small, unique problems with small, unique apps. Where you don’t just rely on mass-market apps made by expert programmers. Where you share home-made little apps with family and friends.</p><p>Scrappy is our contribution to this dream. Each Scrapp is a live, persistent world, easily shared and remixed, closer to familiar productivity apps than alien developer tools. Like any vision, ours is incomplete, but we’ve grounded our explorations in a working prototype with examples of apps.</p><a href="https://scrappy.jrcpl.us/">Try Scrappy! (desktop only)</a><p>We hope Scrappy will inspire you to further chase this particular windmill. If it does, please let us know!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Locally hosting an internet-connected server (101 pts)]]></title>
            <link>https://mjg59.dreamwidth.org/72095.html</link>
            <guid>44306792</guid>
            <pubDate>Wed, 18 Jun 2025 04:58:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mjg59.dreamwidth.org/72095.html">https://mjg59.dreamwidth.org/72095.html</a>, See on <a href="https://news.ycombinator.com/item?id=44306792">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I'm lucky enough to have a <a href="https://www.monkeybrains.net/">weird niche ISP</a> available to me, so I'm paying $35 a month for around 600MBit symmetric data. Unfortunately they don't offer static IP addresses to residential customers, and nor do they allow multiple IP addresses per connection, and I'm the sort of person who'd like to run a bunch of stuff myself, so I've been looking for ways to manage this.</p><p>What I've ended up doing is renting a cheap VPS from a vendor that lets me add multiple IP addresses for minimal extra cost. The precise nature of the VPS isn't relevant - you just want a machine (it doesn't need much CPU, RAM, or storage) that has multiple world routeable IPv4 addresses associated with it and has no port blocks on incoming traffic. Ideally it's geographically local and peers with your ISP in order to reduce additional latency, but that's a nice to have rather than a requirement.</p><p>By setting that up you now have multiple real-world IP addresses that people can get to. How do we get them to the machine in your house you want to be accessible? First we need a connection between that machine and your VPS, and the easiest approach here is <a href="https://wireguard.com/">Wireguard</a>. We only need a point-to-point link, nothing routable, and none of the IP addresses involved need to have anything to do with any of the rest of your network. So, on your local machine you want something like:</p><tt><p>[Interface]<br>PrivateKey = privkeyhere<br>ListenPort = 51820<br>Address = localaddr/32</p><p>[Peer]<br>Endpoint = VPS:51820 <br>PublicKey = pubkeyhere <br>AllowedIPs = VPS/0</p></tt><p>And on your VPS, something like:</p><tt><p>[Interface]<br>Address = vpswgaddr/32<br>SaveConfig = true<br>ListenPort = 51820<br>PrivateKey = privkeyhere</p><p>[Peer]<br>PublicKey = pubkeyhere<br>AllowedIPs = localaddr/32</p></tt><p>The addresses here are (other than the VPS address) arbitrary - but they do need to be consistent, otherwise Wireguard is going to be unhappy and your packets will not have a fun time. Bring that interface up with <a href="https://www.wireguard.com/quickstart/">wg-quick</a> and make sure the devices can ping each other. Hurrah! That's the easy bit.</p><p>Now you want packets from the outside world to get to your internal machine. Let's say the external IP address you're going to use for that machine is </p><tt>321.985.520.309</tt><p> and the wireguard address of your local system is </p><tt>867.420.696.005</tt><p>. On the VPS, you're going to want to do:</p><tt>iptables -t nat -A PREROUTING -p tcp -d 321.985.520.309 -j DNAT --to-destination 867.420.696.005</tt><p>Now, all incoming packets for </p><tt>321.985.520.309</tt><p> will be rewritten to head towards </p><tt>867.420.696.005</tt><p> instead (make sure you've set </p><tt>net.ipv4.ip_forward</tt><p> to 1 via </p><tt>sysctl</tt><p>!). Victory! Or is it? Well, no.</p><p>What we're doing here is rewriting the destination address of the packets so instead of heading to an address associated with the VPS, they're now going to head to your internal system over the Wireguard link. Which is then going to ignore them, because the </p><tt>AllowedIPs</tt><p> statement in the config only allows packets coming from your VPS, and these packets still have their original source IP. We could rewrite the source IP to match the VPS IP, but then you'd have no idea where any of these packets were coming from, and that sucks. Let's do something better. On the local machine, in the peer, let's update </p><tt>AllowedIps</tt><p> to </p><tt>0.0.0.0/0</tt><p> to permit packets form any source to appear over our Wireguard link. But if we bring the interface up now, it'll try to route <em>all</em> traffic over the Wireguard link, which isn't what we want. So we'll add </p><tt>table = off</tt><p> to the </p><tt>interface</tt><p> stanza of the config to disable that, and now we can bring the interface up without breaking everything but still allowing packets to reach us. However, we do still need to tell the kernel how to reach the remote VPN endpoint, which we can do with </p><tt>ip route add vpswgaddr dev wg0</tt><p>. Add this to the </p><tt>interface</tt><p> stanza as:</p><tt><p>PostUp = ip route add vpswgaddr dev wg0<br>PreDown = ip route del vpswgaddr dev wg0</p></tt><p>That's half the battle. The problem is that they're going to show up there with the source address still set to the original source IP, and your internal system is (because Linux) going to notice it has the ability to just send replies to the outside world via your ISP rather than via Wireguard and nothing is going to work. Thanks, Linux. Thinux.</p><p>But there's a way to solve this - policy routing. Linux allows you to have multiple separate routing tables, and define policy that controls which routing table will be used for a given packet. First, let's define a new table reference. On the local machine, edit </p><tt>/etc/iproute2/rt_tables</tt><p> and add a new entry that's something like:</p><tt><p>1 wireguard</p></tt><p>where "1" is just a standin for a number not otherwise used there. Now edit your wireguard config and replace </p><tt>table=off</tt><p> with </p><tt>table=wireguard</tt><p> - Wireguard will now update the </p><tt>wireguard</tt><p> routing table rather than the global one. Now all we need to do is to tell the kernel to push packets into the appropriate routing table - we can do that with </p><tt>ip rule add from localaddr lookup wireguard</tt><p>, which tells the kernel to take any packet coming from our Wireguard address and push it via the Wireguard routing table. Add that to your Wireguard interface config as:</p><tt><p>PostUp = ip rule add from localaddr lookup wireguard<br>PreDown = ip rule del from localaddr lookup wireguard</p></tt><p>and now your local system is effectively on the internet.</p><p>You can do this for multiple systems - just configure additional Wireguard interfaces on the VPS and make sure they're all listening on different ports. If your local IP changes then your local machines will end up reconnecting to the VPS, but to the outside world their accessible IP address will remain the same. It's like having a real IP without the pain of convincing your ISP to give it to you.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Lstr – A modern, interactive tree command written in Rust (194 pts)]]></title>
            <link>https://github.com/bgreenwell/lstr</link>
            <guid>44306041</guid>
            <pubDate>Wed, 18 Jun 2025 02:07:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/bgreenwell/lstr">https://github.com/bgreenwell/lstr</a>, See on <a href="https://news.ycombinator.com/item?id=44306041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">lstr</h2><a id="user-content-lstr" aria-label="Permalink: lstr" href="#lstr"></a></p>
<p dir="auto"><a href="https://github.com/bgreenwell/lstr/actions"><img src="https://github.com/bgreenwell/lstr/actions/workflows/ci.yml/badge.svg" alt="Build Status"></a>
<a href="https://crates.io/crates/lstr" rel="nofollow"><img src="https://camo.githubusercontent.com/b6682dc3f177211760ed9636185d7aafc140643f821ab6e95f05d8052f37919d/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f6c7374722e737667" alt="Latest Version" data-canonical-src="https://img.shields.io/crates/v/lstr.svg"></a>
<a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/6cd0120cc4c5ac11d28b2c60f76033b52db98dac641de3b2644bb054b449d60c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"></a></p>
<p dir="auto">A blazingly fast, minimalist directory tree viewer, written in Rust. Inspired by the command line program <a href="https://github.com/Old-Man-Programmer/tree">tree</a>, with a powerful interactive mode.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/bgreenwell/lstr/blob/main/assets/lstr-demo.gif"><img src="https://github.com/bgreenwell/lstr/raw/main/assets/lstr-demo.gif" alt="" data-animated-image=""></a>
<em>An interactive overview of <code>lstr</code>'s project structure... using <code>lstr</code>.</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Philosophy</h2><a id="user-content-philosophy" aria-label="Permalink: Philosophy" href="#philosophy"></a></p>
<ul dir="auto">
<li><strong>Fast:</strong> Runs directory scans in parallel by default to maximize speed on modern hardware.</li>
<li><strong>Minimalist:</strong> Provides essential features without the bloat. The core experience is clean and uncluttered.</li>
<li><strong>Interactive:</strong> An optional TUI mode for fluid, keyboard-driven exploration.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><strong>High-performance:</strong> Scans directories in parallel to be as fast as possible.</li>
<li><strong>Classic and interactive modes:</strong> Use <code>lstr</code> for a classic <code>tree</code>-like view, or launch <code>lstr interactive</code> for a fully interactive TUI.</li>
<li><strong>Rich information display (optional):</strong>
<ul dir="auto">
<li>Display file-specific icons with <code>--icons</code> (requires a Nerd Font).</li>
<li>Show file permissions with <code>-p</code>.</li>
<li>Show file sizes with <code>-s</code>.</li>
<li><strong>Git Integration:</strong> Show file statuses (<code>Modified</code>, <code>New</code>, <code>Untracked</code>, etc.) directly in the tree with the <code>-G</code> flag.</li>
</ul>
</li>
<li><strong>Smart filtering:</strong>
<ul dir="auto">
<li>Respects your <code>.gitignore</code> files with the <code>-g</code> flag.</li>
<li>Control recursion depth (<code>-L</code>) or show only directories (<code>-d</code>).</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">You need the Rust toolchain installed on your system to build <code>lstr</code>.</p>
<ol dir="auto">
<li><strong>Clone the repository:</strong>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/bgreenwell/lstr.git
cd lstr"><pre>git clone https://github.com/bgreenwell/lstr.git
<span>cd</span> lstr</pre></div>
</li>
<li><strong>Build and install using Cargo:</strong>
<div dir="auto" data-snippet-clipboard-copy-content="# This compiles in release mode and copies the binary to ~/.cargo/bin
cargo install --path ."><pre><span><span>#</span> This compiles in release mode and copies the binary to ~/.cargo/bin</span>
cargo install --path <span>.</span></pre></div>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="lstr [OPTIONS] [PATH]
lstr interactive [OPTIONS] [PATH]"><pre>lstr [OPTIONS] [PATH]
lstr interactive [OPTIONS] [PATH]</pre></div>
<p dir="auto">Note that <code>PATH</code> defaults to the current directory (<code>.</code>) if not specified.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Option</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-a</code>, <code>--all</code></td>
<td>List all files and directories, including hidden ones.</td>
</tr>
<tr>
<td><code>--color &lt;WHEN&gt;</code></td>
<td>Specify when to use color output (<code>always</code>, <code>auto</code>, <code>never</code>).</td>
</tr>
<tr>
<td><code>-d</code>, <code>--dirs-only</code></td>
<td>List directories only, ignoring all files.</td>
</tr>
<tr>
<td><code>-g</code>, <code>--gitignore</code></td>
<td>Respect <code>.gitignore</code> and other standard ignore files.</td>
</tr>
<tr>
<td><code>-G</code>, <code>--git-status</code></td>
<td>Show git status for files and directories.</td>
</tr>
<tr>
<td><code>--icons</code></td>
<td>Display file-specific icons; requires a <a href="https://www.nerdfonts.com/" rel="nofollow">Nerd Font</a>.</td>
</tr>
<tr>
<td><code>-L</code>, <code>--level &lt;LEVEL&gt;</code></td>
<td>Maximum depth to descend.</td>
</tr>
<tr>
<td><code>-p</code>, <code>--permissions</code></td>
<td>Display file permissions (Unix-like systems only).</td>
</tr>
<tr>
<td><code>-s</code>, <code>--size</code></td>
<td>Display the size of files.</td>
</tr>
<tr>
<td><code>--expand-level &lt;LEVEL&gt;</code></td>
<td><strong>Interactive mode only:</strong> Initial depth to expand the interactive tree.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Interactive mode</h2><a id="user-content-interactive-mode" aria-label="Permalink: Interactive mode" href="#interactive-mode"></a></p>
<p dir="auto">Launch the TUI with <code>lstr interactive [OPTIONS] [PATH]</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Keyboard controls</h3><a id="user-content-keyboard-controls" aria-label="Permalink: Keyboard controls" href="#keyboard-controls"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Key(s)</th>
<th>Action</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>↑</code> / <code>k</code></td>
<td>Move selection up.</td>
</tr>
<tr>
<td><code>↓</code> / <code>j</code></td>
<td>Move selection down.</td>
</tr>
<tr>
<td><code>Enter</code></td>
<td><strong>Context-aware action:</strong><br>- If on a file: Open it in the default editor (<code>$EDITOR</code>).<br>- If on a directory: Toggle expand/collapse.</td>
</tr>
<tr>
<td><code>q</code> / <code>Esc</code></td>
<td>Quit the application normally.</td>
</tr>
<tr>
<td><code>Ctrl</code>+<code>s</code></td>
<td><strong>Shell integration:</strong> Quits and prints the selected path to stdout.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto"><strong>1. List the contents of the current directory</strong></p>

<p dir="auto"><strong>2. Explore a project interactively, ignoring gitignored files</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="lstr interactive -g --icons"><pre>lstr interactive -g --icons</pre></div>
<p dir="auto"><strong>3. Display a directory with file sizes and permissions (classic view)</strong></p>

<p dir="auto"><strong>4. See the git status of all files in a project</strong></p>

<p dir="auto"><strong>5. Start an interactive session with all data displayed</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="lstr interactive -gG --icons -s -p"><pre>lstr interactive -gG --icons -s -p</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Piping and shell interaction</h2><a id="user-content-piping-and-shell-interaction" aria-label="Permalink: Piping and shell interaction" href="#piping-and-shell-interaction"></a></p>
<p dir="auto">The classic <code>view</code> mode is designed to work well with other command-line tools via pipes (<code>|</code>).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Interactive fuzzy finding with <code>fzf</code></h3><a id="user-content-interactive-fuzzy-finding-with-fzf" aria-label="Permalink: Interactive fuzzy finding with fzf" href="#interactive-fuzzy-finding-with-fzf"></a></p>
<p dir="auto">This is a powerful way to instantly find any file in a large project.</p>

<p dir="auto"><code>fzf</code> will take the tree from <code>lstr</code> and provide an interactive search prompt to filter it.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Paging large trees with <code>less</code> or <code>bat</code></h3><a id="user-content-paging-large-trees-with-less-or-bat" aria-label="Permalink: Paging large trees with less or bat" href="#paging-large-trees-with-less-or-bat"></a></p>
<p dir="auto">If a directory is too large to fit on one screen, pipe the output to a <em>pager</em>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Using less (the -R flag preserves color)
lstr -L 10 | less -R

# Using bat (a modern pager that understands colors)
lstr --icons | bat"><pre><span><span>#</span> Using less (the -R flag preserves color)</span>
lstr -L 10 <span>|</span> less -R

<span><span>#</span> Using bat (a modern pager that understands colors)</span>
lstr --icons <span>|</span> bat</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Changing directories with <code>lstr</code></h3><a id="user-content-changing-directories-with-lstr" aria-label="Permalink: Changing directories with lstr" href="#changing-directories-with-lstr"></a></p>
<p dir="auto">You can use <code>lstr</code> as a visual <code>cd</code> command. Add the following function to your shell's startup file (e.g., <code>~/.bashrc</code>, <code>~/.zshrc</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="# A function to visually change directories with lstr
lcd() {
    # Run lstr and capture the selected path into a variable.
    # The TUI will draw on stderr, and the final path will be on stdout.
    local selected_dir
    selected_dir=&quot;$(lstr interactive -g --icons)&quot;

    # If the user selected a path (and didn't just quit), `cd` into it.
    # Check if the selection is a directory.
    if [[ -n &quot;$selected_dir&quot; &amp;&amp; -d &quot;$selected_dir&quot; ]]; then
        cd &quot;$selected_dir&quot;
    fi
}"><pre><span><span>#</span> A function to visually change directories with lstr</span>
<span>lcd</span>() {
    <span><span>#</span> Run lstr and capture the selected path into a variable.</span>
    <span><span>#</span> The TUI will draw on stderr, and the final path will be on stdout.</span>
    <span>local</span> selected_dir
    selected_dir=<span><span>"</span><span><span>$(</span>lstr interactive -g --icons<span>)</span></span><span>"</span></span>

    <span><span>#</span> If the user selected a path (and didn't just quit), `cd` into it.</span>
    <span><span>#</span> Check if the selection is a directory.</span>
    <span>if</span> [[ <span>-n</span> <span><span>"</span><span>$selected_dir</span><span>"</span></span> <span>&amp;&amp;</span> <span>-d</span> <span><span>"</span><span>$selected_dir</span><span>"</span></span> ]]<span>;</span> <span>then</span>
        <span>cd</span> <span><span>"</span><span>$selected_dir</span><span>"</span></span>
    <span>fi</span>
}</pre></div>
<p dir="auto">After adding this and starting a new shell session (or running <code>source ~/.bashrc</code>), you can simply run:</p>

<p dir="auto">This will launch the <code>lstr</code> interactive UI. Navigate to the directory you want, press <code>Ctrl+s</code>, and your shell's current directory will instantly change.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance and concurrency</h2><a id="user-content-performance-and-concurrency" aria-label="Permalink: Performance and concurrency" href="#performance-and-concurrency"></a></p>
<p dir="auto">By default, <code>lstr</code> uses a parallel directory walker to maximize speed on multi-core systems. This parallelism is managed by the excellent <a href="https://crates.io/crates/rayon" rel="nofollow">rayon</a> thread pool, which is used internally by <code>lstr</code>'s directory traversal engine.</p>
<p dir="auto">For advanced use cases, such as benchmarking or limiting CPU usage, you can control the number of threads by setting the <code>RAYON_NUM_THREADS</code> environment variable before running the command.</p>
<p dir="auto"><strong>To force single-threaded (serial) execution:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="RAYON_NUM_THREADS=1 lstr ."><pre>RAYON_NUM_THREADS=1 lstr <span>.</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Inspiration</h2><a id="user-content-inspiration" aria-label="Permalink: Inspiration" href="#inspiration"></a></p>
<p dir="auto">The philosophy and functionality of <code>lstr</code> are heavily inspired by the excellent C-based <a href="https://github.com/Old-Man-Programmer/tree">tree</a> command line program. This project is an attempt to recreate that classic utility in modern, safe Rust.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Grug Brained Developer (2022) (958 pts)]]></title>
            <link>https://grugbrain.dev/</link>
            <guid>44303542</guid>
            <pubDate>Tue, 17 Jun 2025 20:24:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://grugbrain.dev/">https://grugbrain.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=44303542">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <div>
  <p><a href="https://www.redbubble.com/i/sticker/Programmer-Grug-by-colossalbreaker/42915272.EJUG5">
    <img alt="grug" src="https://grugbrain.dev/grug.png">
  </a></p><h2>
     The Grug Brained Developer<br>
     <small>A layman's guide to thinking like the self-aware smol brained</small>
  </h2>
</div>

<h2>Introduction</h2>
<p>this collection of thoughts on software development gathered by grug brain developer</p>
<p>grug brain developer not so smart, but grug brain developer program many long year and learn some things
although mostly still confused</p>
<p>grug brain developer try collect learns into small, easily digestible and funny page, not only for you, the young grug, but also for him
because as grug brain developer get older he forget important things, like what had for breakfast or if put pants on</p>
<p>big brained developers are many, and some not expected to like this, make sour face</p>
<p><em>THINK</em> they are big brained developers many, many more, and more even definitely probably maybe not like this, many
sour face (such is internet)</p>
<p>(note: grug once think big brained but learn hard way)</p>
<p>is fine!</p>
<p>is free country sort of and end of day not really matter too much, but grug hope you fun reading and maybe learn from
many, many mistake grug make over long program life</p>
<h2><a name="grug-on-complexity"></a><a href="#grug-on-complexity">The Eternal Enemy: Complexity</a></h2>
<p>apex predator of grug is complexity</p>
<p>complexity bad</p>
<p>say again:</p>
<p>complexity <em>very</em> bad</p>
<p><em>you</em> say now:</p>
<p>complexity <em>very</em>, <em>very</em> bad</p>
<p>given choice between complexity or one on one against t-rex, grug take t-rex: at least grug see t-rex</p>
<p>complexity is spirit demon that enter codebase through well-meaning but ultimately very clubbable non grug-brain
developers and project managers who not fear complexity spirit demon or even know about sometime</p>
<p>one day code base understandable and grug can get work done, everything good!</p>
<p>next day impossible: complexity demon spirit has entered code and very dangerous situation!</p>
<p>grug no able see complexity demon, but grug sense presence in code base</p>
<p>demon complexity spirit mocking him make change here break unrelated thing there what!?! mock mock mock ha ha so funny
grug love programming and not becoming shiney rock speculator like grug senior advise</p>
<p>club not work on demon spirit complexity and bad idea actually hit developer who let spirit in with club: sometimes grug
himself!</p>
<p>sadly, often grug himself</p>
<p>so grug say again and say often: complexity <em>very</em>, <em>very</em> bad</p>
<h2><a name="grug-on-saying-no"></a><a href="#grug-on-saying-no">Saying No</a></h2>
<p>best weapon against complexity spirit demon is magic word: "no"</p>
<p>"no, grug not build that feature"</p>
<p>"no, grug not build that abstraction"</p>
<p>"no, grug not put water on body every day or drink less black think juice you stop repeat ask now"</p>
<p>note, this good engineering advice but bad career advice: "yes" is magic word for more shiney rock and put in
charge of large tribe of developer</p>
<p>sad but true: learn "yes" then learn blame other grugs when fail, ideal career advice</p>
<p>but grug must to grug be true, and "no" is magic grug word.  Hard say at first, especially if you nice grug and don't like
disappoint people (many such grugs!) but  easier over time even though shiney rock pile not as high as might otherwise be</p>
<p>is ok: how many shiney rock grug really need anyway?</p>
<h2><a name="grug-on-saying-ok"></a><a href="#grug-on-saying-ok">Saying ok</a></h2>
<p>sometimes compromise necessary or no shiney rock, mean no dinosaur meat, not good, wife firmly remind grug
about young grugs at home need roof, food, and so forth, no interest in complexity demon spirit rant by grug for
fiftieth time</p>
<p>in this situation, grug recommend "ok"</p>
<p>"ok, grug build that feature"</p>
<p>then grug spend time think of <a href="https://en.wikipedia.org/wiki/Pareto_principle">80/20 solution</a> to problem and build that instead.<br>
80/20 solution say "80 want with 20 code"  solution maybe not have all bell-whistle that project manager want, maybe a
little ugly, but work and deliver most value, and keep demon complexity spirit at bay for most part to extent</p>
<p>sometimes probably best just not tell project manager and do it 80/20 way.  easier forgive than permission, project managers
mind like butterfly at times overworked and dealing with many grugs.  often forget what even feature supposed to do or move on or
quit or get fired grug see many such cases</p>
<p>anyway is in project managers best interest anyway so grug not to feel too bad for this approach usually</p>
<h2><a name="grug-on-factring-your-code"></a><a href="#grug-on-factring-your-code">Factoring Your Code</a></h2>
<p>next strategy very harder: break code base up properly (fancy word: "factor your code properly")  here is hard give general
advice because each system so different.  however, one thing grug come to believe: not factor your application too early!</p>
<p>early on in project everything very abstract and like water: very little solid holds for grug's struggling brain to hang
on to.  take time to develop "shape" of system and learn what even doing.  grug try not to factor in early part of project
and then, at some point, good cut-points emerge from code base</p>
<p>good cut point has narrow interface with rest of system: small number of functions or abstractions that hide complexity
demon internally, like trapped in crystal</p>
<p>grug quite satisfied when complexity demon trapped properly in crystal, is best feeling to trap mortal enemy!</p>
<p>grug try watch patiently as cut points emerge from code and slowly refactor, with code base taking shape over time along
with experience.  no hard/ fast rule for this: grug know cut point when grug see cut point, just take time to build
skill in seeing, patience</p>
<p>sometimes grug go too early and get abstractions wrong, so grug bias towards waiting</p>
<p>big brain developers often not like this at all and invent many abstractions start of project</p>
<p>grug tempted to reach for club and yell "big brain no maintain code!  big brain move on next architecture committee
leave code for grug deal with!"</p>
<p>but grug learn control passions, major difference between grug and animal</p>
<p>instead grug try to limit damage of big brain developer early in project by giving them thing like
UML diagram (not hurt code, probably throw away anyway) or by demanding working demo tomorrow</p>
<p>working demo especially good trick: force big brain make something to actually work to talk about and code to look at that do
thing, will help big brain see reality on ground more quickly</p>
<p>remember!  big brain have big brain!  need only be harness for good and not in service of spirit complexity demon on
accident, many times seen</p>
<p>(best grug brain able to herd multiple big brain in right direction and produce many complexity demon trap crystals, large
shiney rock pile awaits such grug!)</p>
<p>also sometimes call demo approach "prototype", sound fancier to project manager</p>
<p>grug say prototype early in software making, <em>especially</em> if many big brains</p>
<h2><a name="grug-on-testing"></a><a href="#grug-on-testing">Testing</a></h2>
<p>grug have love/hate relationship with test: test save grug many, many uncountable time and grug love and respect test</p>
<p>unfortunately also many test shamans exist.  some test shaman make test idol, demand things like "first test" before grug
even write code or have any idea what grug doing domain!</p>
<p>how grug test what grug not even understand domain yet!?</p>
<p>"Oh, don't worry: the tests will show you what you need to do."</p>
<p>grug once again catch grug slowly reaching for club, but grug stay calm</p>
<p>grug instead prefer write most tests after prototype phase, when code has begun firm up</p>
<p>but, note well: grug must here be very disciplined!</p>
<p>easy grug to move on and not write tests because "work on grugs machine"!</p>
<p>this very, very bad: no guarantee work on other machine and no guarantee work on grug machine in future, many times</p>
<p>test shaman have good point on importance of test, even if test shaman often sometimes not complete useful
feature in life and talk only about test all time, deserve of club but heart in right place</p>
<p>also, test shaman often talk unit test very much, but grug not find so useful.  grug experience that ideal tests are not
unit test or either end-to-end test, but in-between test</p>
<p><a href="https://en.wikipedia.org/wiki/Unit_testing">unit tests</a> fine, ok, but break as implementation change (much compared api!)
and make refactor hard and, frankly, many bugs anyway often due interactions other code.  often throw away when code change.</p>
<p>grug write unit test mostly at start of project, help get things going but not get too attached or expect value long time</p>
<p><a href="https://smartbear.com/solutions/end-to-end-testing/">end to end</a> tests good, show whole system work, but! hard to
understand when break and drive grug crazy very often, sometimes grugs just end up ignoring because "oh, that break all
time"  very bad!</p>
<p>in-between tests, grug hear shaman call <a href="https://en.wikipedia.org/wiki/Integration_testing">"integration tests"</a> sometime
often with sour look on face. but grug say integration test sweet spot according to grug: high level enough test correctness
of system, low level enough, with good debugger, easy to see what break</p>
<p>grug prefer some unit tests especially at start but not 100% all code test and definitely not "first test".  "test along
the way" work pretty well for grug, especially as grug figure things out</p>
<p>grug focus much ferocious integration test effort as cut point emerge and system stabilize!  cut point api hopefully stable
compared implementation and integration test remain valuable many long time, and easy debug</p>
<p>also small, well curated end-to-end test suite is created to be kept working religiously on pain of clubbing. focus of important
end-to-end test on most common UI features and few most important edge cases, but not too many or become impossible maintain
and then ignored</p>
<p>this ideal set of test to grug</p>
<p>you may not like, but this peak grug testing</p>
<p>also, grug dislike <a href="https://en.wikipedia.org/wiki/Mock_object">mocking</a> in test, prefer only when absolute necessary
to (rare/never) and coarse grain mocking (cut points/systems) only at that</p>
<p>one exception "first test" dislike by grug: when bug found.  grug always try first reproduce bug with regression test
<em>then</em> fix bug, this case only for some reason work better</p>
<h2><a name="grug-on-agile"></a><a href="#grug-on-agile">Agile</a></h2>
<p>grug think agile not terrible, not good</p>
<p>end of day, not worst way to organize development, maybe better than others grug supposes is fine</p>
<p>danger, however, is agile shaman!  many, many shiney rock lost to agile shaman!</p>
<p>whenever agile project fail, agile shaman say "you didn't do agile right!"  grug note this awfully convenient for agile
shaman, ask more shiney rock better agile train young grugs on agile, danger!</p>
<p>grug tempted reach for club when too much agile talk happen but always stay calm</p>
<p>prototyping, tools and hiring good grugs better key to success software: agile process ok and help some but sometimes hurt taken
too seriously</p>
<p>grug say <a href="https://en.wikipedia.org/wiki/No_Silver_Bullet">no silver club</a> fix all software problems no matter what agile
shaman say (danger!)</p>
<h2><a name="grug-on-refactoring"></a><a href="#grug-on-refactoring">Refactoring</a></h2>
<p>refactoring fine activity and often good idea, especially later in project when code firmed up</p>
<p>however, grug note that many times in career "refactors" go horribly off rails and end up causing more harm than good</p>
<p>grug not sure exactly why some refactors work well, some fail, but grug notice that larger refactor, more
likely failure appear to be</p>
<p>so grug try to keep refactors relatively small and not be "too far out from shore" during refactor.  ideally system work
entire time and each step of finish before other begin.</p>
<p>end-to-end tests are life saver here, but often very hard understand why broke... such is refactor life.</p>
<p>also grug notice that introducing too much abstraction often lead to refactor failure and system failure.  good example
was <a href="https://www.webopedia.com/definitions/j2ee/">J2EE</a> introduce, many big brain sit around thinking too much abstraction, nothing good came of it many project hurt</p>
<p>another good example when company grug work for introduce <a href="https://www.techtarget.com/searchnetworking/definition/OSGi">OSGi</a> to help
manage/trap spriit complexity demon in code base.  not only OSGi not help, but make complexity demon much more powerful!
took multiple man year of best developers to rework as well to boot!  more complex spirit and now features impossible
implement! very bad!</p>
<h2><a name="grug-on-chestertons-fence"></a><a href="#grug-on-chestertons-fence">Chesterton's Fence</a></h2>
<p>wise grug shaman <a href="https://en.wikipedia.org/wiki/G._K._Chesterton">chesterton</a> once say</p>
<blockquote>
<p>here exists in such a case a certain institution or law; let us say, for the sake of simplicity, a fence or gate erected across a road. The more modern type of reformer goes gaily up to it and says, “I don’t see the use of this; let us clear it away.” To which the more intelligent type of reformer will do well to answer: “If you don’t see the use of it, I certainly won’t let you clear it away. Go away and think. Then, when you can come back and tell me that you do see the use of it, I may allow you to destroy it.”</p>
</blockquote>
<p>many older grug learn this lesson well not start tearing code out willy nilly, no matter how ugly look</p>
<p>grug understand all programmer platonists at some level wish music of spheres perfection in code.  but danger is here,
world is ugly and gronky many times and so also must code be</p>
<p>humility not often come big brained or think big brained
easily or grug even, but grug often find "oh, grug no like look of this, grug fix" lead many hours pain grug and no better or system
worse even</p>
<p>grug early on in career often charge into code base waving club wildly and smash up everything, learn not good</p>
<p>grug not say no improve system ever, quite foolish, but recommend take time understand system first especially bigger system is and
is respect code working today even if not perfect</p>
<p>here tests often good hint for why fence not to be smashed!</p>
<h2><a name="grug-on-microservices"></a><a href="#grug-on-microservices">Microservices</a></h2>
<p>grug wonder why big brain take hardest problem, factoring system correctly, and introduce network call too</p>
<p>seem very confusing to grug</p>
<h2><a name="grug-on-tools"></a><a href="#grug-on-tools">Tools</a></h2>
<p>grug love tool.  tool and control passion what separate grug from dinosaurs!  tool allow grug brain to create code that
not possible otherwise by doing thinking for grug, always good relief! grug always spend time in new place learning
tools around him to maximize productivity: learn tools for two weeks make development often twice faster and often
have dig around ask other developers help, no docs</p>
<p>code completion in IDE allow grug not have remembered all API, very important!</p>
<p>java programming nearly impossible without it for grug!</p>
<p>really make grug think some time</p>
<p>good debugger worth weight in shiney rocks, in fact also more: when faced with bug grug would often trade all shiney rock and
perhaps few children for good debugger and anyway debugger no weigh anything far as grug can tell</p>
<p>grug always recommend new programmer learn available debugger very deeply, features like conditional break points, expression
evaluation, stack navigation, etc teach new grug more about computer than university class often!</p>
<p>grug say never be not improving tooling</p>
<h2><a name="grug-on-type-systems"></a><a href="#grug-on-type-systems">Type Systems</a></h2>
<p>grug very like type systems make programming easier.  for grug, type systems most value when grug hit dot on keyboard and
list of things grug can do pop up magic.  this 90% of value of type system or more to grug</p>
<p>big brain type system shaman often say type correctness main point type system, but grug note some big brain type system
shaman not often ship code.  grug suppose code never shipped is correct, in some sense, but not really what grug mean
when say correct</p>
<p>grug say tool magic pop up of what can do and complete of code major most benefit of type system, correctness also good but not
so nearly so much</p>
<p>also, often sometimes caution beware big brains here!</p>
<p>some type big brain think in type systems and talk in lemmas, potential danger!</p>
<p>danger abstraction too high, big brain type system code become astral projection of platonic generic turing model of
computation into code base.  grug confused and agree some level very elegant but also very hard do anything like
record number of club inventory for Grug Inc. task at hand</p>
<p>generics especially dangerous here, grug try limit generics to container classes for most part where most value add</p>
<p>temptation generics very large is trick!  spirit demon complex love this one trick! beware!</p>
<p>always most value type system come: hit dot see what grug can do, never forget!</p>
<h2><a name="grug-on-expression-complexity"></a><a href="#grug-on-expression-complexity">Expression Complexity</a></h2>
<p>grug once like to minimize lines of code much as possible.  write code like this:</p>
<pre><code>  if(contact &amp;&amp; !contact.isActive() &amp;&amp; (contact.inGroup(FAMILY) || contact.inGroup(FRIENDS))) {
    // ...
  }
</code></pre>
<p>over time grug learn this hard debug, learn prefer write like so:</p>
<pre><code>  if(contact) {
    var contactIsInactive = !contact.isActive();
    var contactIsFamilyOrFriends = contact.inGroup(FAMILY) || contact.inGroup(FRIENDS);
    if(contactIsInactive &amp;&amp; contactIsFamilyOrFriends) {
        // ...
    }
  }
</code></pre>
<p>grug hear screams from young grugs at horror of many line of code and pointless variable and grug prepare defend self with club</p>
<p>club fight start with other developers attack and grug yell: "easier debug!  see result of each expression more clearly and good name!  easier
understand conditional expression!  EASIER DEBUG!"</p>
<p>definitely easier debug and once club fight end calm down and young grug think a bit, they realize grug right</p>
<p>grug still catch grug writing code like first example and often regret, so grug not judge young grug</p>
<h2><a name="grug-on-dry"></a><a href="#grug-on-dry">DRY</a></h2>
<p><a href="https://en.wikipedia.org/wiki/Don%27t_repeat_yourself">DRY</a> mean Don't Repeat Self, powerful maxim over mind of most
developers</p>
<p>grug respect DRY and good advice, however grug recommend balance in all things, as gruggest big brain aristotle recommend</p>
<p>grug note humourous graph by Lea Verou correspond with grug passion not repeat:</p>
<img alt="code concerns over time" src="https://grugbrain.dev/over-time.png">
<p>over time past ten years program grug not as concerned repeat code.  so long as repeat code simple enough and obvious
enough, and grug begin feel repeat/copy paste code with small variation is better than many callback/closures passed arguments
or elaborate object model: too hard complex for too little benefit at times</p>
<p>hard balance here, repeat code always still make grug stare and say "mmm" often, but experience show repeat code
sometimes often better than complex DRY solution</p>
<p>note well!  grug encourage over literal developer not take does work line too serious, is joke</p>
<h2><a name="grug-on-soc"></a><a href="#grug-on-soc">Separation of Concerns (SoC)</a></h2>
<p><a href="https://en.wikipedia.org/wiki/Separation_of_concerns">Separation of Concern (SoC)</a> another powerful idea over many developer
mind, idea to separate different aspects of system into distinct sections code</p>
<p>canonical example from web development: separation of style (css file), markup (html file) and logic (javascript file)</p>
<p>here grug much more sour faced than DRY and in fact write big brained essay on alternative design principle
<a href="https://htmx.org/essays/locality-of-behaviour/">locality of behavior (LoB)</a> against SoC</p>
<p>grug much prefer put code on the thing that do the thing.  now when grug look at the thing grug know the thing what the
thing do, alwasy good relief!</p>
<p>when separate of concern grug must often all over tarnation many file look understand what how button do, much confuse
and time waste: bad!</p>
<h2><a name="grug-on-closures"></a><a href="#grug-on-closures">Closures</a></h2>
<p>grug like closures for right job and that job usually abstracting operation over collection of objects</p>
<p>grug warn closures like salt, type systems and generics: small amount go long way, but easy spoil things too much use
give heart attack</p>
<p>javascript developers call very special complexity demon spirit in javascript "callback hell" because too much closure
used by javascript libraries very sad but also javascript developer get what deserved let grug be frank</p>
<h2><a name="grug-on-logging"></a><a href="#grug-on-logging">Logging</a></h2>
<p>grug huge fan of logging and encourage lots of it, especially in cloud deployed.  some non-grugs say logging expensive
and not important.  grug used think this way no more</p>
<p>funny story: grug learn idol <a href="https://en.wikipedia.org/wiki/Rob_Pike">rob pike</a> working on logging at google and decide:
"if rob pike working on logging, what grug do there?!?" so not pursue.  turn out logging <em>very</em> important to google so
of course best programmer work on it, grug!</p>
<p>don't be such grug brain, grug, much less shiney rock now!</p>
<p>oh well, grug end up at good company anyway and rob pike dress habit
<a href="https://www.youtube.com/watch?v=KINIAgRpkDA">increasingly erratic</a>, so all work out in end, but
point stand: logging very important!</p>
<p>grug tips on logging are:</p>
<ul>
<li>log all major logical branches within code (if/for)</li>
<li>if "request" span multiple machine in cloud infrastructure, include request ID in all so logs can be grouped</li>
<li>if possible make log level dynamically controlled, so grug can turn on/off when need debug issue (many!)</li>
<li>if possible make log level per user, so can debug specific user issue</li>
</ul>
<p>last two points are especially handy club when fighting bugs in production systems very often</p>
<p>unfortunately log libraries often very complex (java, <a href="https://stackify.com/logging-java/">why you do?</a>) but worth investing
time in getting logging infrastructure "just right" pay off big later in grug experience</p>
<p>logging need taught more in schools, grug think</p>
<h2><a name="grug-on-concurrency"></a><a href="#grug-on-concurrency">Concurrency</a></h2>
<p>grug, like all sane developer, fear concurrency</p>
<p>as much as possible, grug try to rely on simple concurrency models like stateless web request handlers and simple
remote job worker queues where jobs no interdepend and simple api</p>
<p><a href="https://en.wikipedia.org/wiki/Optimistic_concurrency_control">optimistic concurrency</a> seem work well for web stuff</p>
<p>occasionally grug reach for <a href="https://en.wikipedia.org/wiki/Thread-local_storage">thread local variable</a>, usually when
writing framework code</p>
<p>some language have good concurrent data structure, like java <a href="https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ConcurrentHashMap.html">ConcurrentHashMap</a>
but still need careful grug work to get right</p>
<p>grug has never used <a href="https://en.wikipedia.org/wiki/Erlang_(programming_language)">erlang</a>, hear good things, but language
look wierd to grug sorry</p>
<h2><a name="grug-on-optimizing"></a><a href="#grug-on-optimizing">Optimizing</a></h2>
<p>ultra biggest of brain developer once say:</p>
<blockquote>
<p>premature optimization is the root of all evil</p>
</blockquote>
<p>this everyone mostly know and grug in humble violent agreement with ultra biggest of big brain</p>
<p>grug recommend always to have concrete, real world perf profile showing specific perf issue before begin optimizing.</p>
<p>never know what actual issue might be, grug often surprise!  very often!</p>
<p>beware only cpu focus: easy to see cpu and much big o notation thinking having been done in school,
but often not root of all slowness, surprise to many including grug</p>
<p>hitting network equivalent of many, many millions cpu cycle and always to be minimized if possible, note well big brain
microservice developer!</p>
<p>inexperienced big brain developer see nested loop and often say "O(n^2)?  Not on my watch!"</p>
<p>complexity demon spirit smile</p>
<h2><a name="grug-on-apis"></a><a href="#grug-on-apis">APIs</a></h2>
<p>grug love good apis.  good apis not make grug think too much</p>
<p>unfortunately, many apis very bad, make grug think quite a bit.  this happen many reasons, here two:</p>
<ul>
<li>API creators think in terms of implementation or domain of API, rather than in terms of use of API</li>
<li>API creators think too abstract and big brained</li>
</ul>
<p>usually grug not care too deeply about detail of api: want write file or sort list or whatever, just want to call
<code>write()</code> or <code>sort()</code> or whatever</p>
<p>but big brain api developers say:</p>
<p>"not so fast, grug!  is that file <em>open for write</em>? did you define a <em>Comparator</em> for that sort?"</p>
<p>grug find self restraining hand reaching for club again</p>
<p>not care about that stuff right now, just want sort and write file mr big brain!</p>
<p>grug recognize that big brain api designer have point and that <em>sometime</em> these things matter, but often do not.
big brain api developers better if design for simple cases with simple api, make complex cases possible
with more complex api</p>
<p>grug call this "layering" apis: two or three different apis at different level complexity for various grug needs</p>
<p>also, if object oriented, put api on thing instead of elsewhere. java worst at this!</p>
<p>grug want filter list in java</p>
<p>"Did you convert it to a stream?"</p>
<p>fine, grug convert to stream</p>
<p>"OK, now you can filter."</p>
<p>OK, but now need return list!  have stream!</p>
<p>"Well, did you collect your stream into a list?"</p>
<p>what?</p>
<p>"Define a Collector&lt;? super T, A, R&gt; to collect your stream into a list"</p>
<p>grug now swear on ancestor grave he club every single person in room, but count two instead and remain calm</p>
<p>put common thing like <code>filter()</code> on list and make return list, listen well big brain java api developer!</p>
<p>nobody care about "stream" or even hear of "stream" before, is not networking api, all java grugs use list mr big brain!</p>
<h2><a name="grug-on-parsing"></a><a href="#grug-on-parsing">Parsing</a></h2>
<p>grug love make programming language at drop of hat and
say <a href="https://en.wikipedia.org/wiki/Recursive_descent_parser">recursive descent</a>
most fun and beautiful way create parser</p>
<p>unfortunately many big brain school teach only parser generator tool.  here grug usual love of tool is not: parser
generator tool generate code of awful snakes nest: impossible understand, bottom up, what?  hide recursive nature of
grammar from grug and debug impossible, very bad according grug!</p>
<p>grug think this because while complexity demon bad for code base and understand, complexity demon very good for generation
of much academic papers, sad but true</p>
<p>production parser almost always recursive descent, despite ignore by schools!  grug furious when learn how simple parse
is! parsing not big brain only magic: so can you!</p>
<p>grug very elated find big brain developer Bob Nystrom redeem the big brain tribe and write excellent book on recursive
descent: <a href="https://craftinginterpreters.com/">Crafting Interpreters</a></p>
<p>book available online free, but grug highly recommend all interested grugs purchase book on general principle, provide
much big brain advice and grug love book <em>very</em> much except visitor pattern (trap!)</p>
<h2><a name="grug-on-visitor-pattern"></a><a href="#grug-on-visitor-pattern">The Visitor Pattern</a></h2>
<p><a href="https://en.wikipedia.org/wiki/Visitor_pattern">bad</a></p>
<h2><a name="grug-on-front-end-development"></a><a href="#grug-on-front-end-development">Front End Development</a></h2>
<p>some non-grugs, when faced with web development say:</p>
<p>"I know, I'll split my front end and back end codebase up and use a hot new SPA library talking to a GraphQL JSON API back end
over HTTP (which is funny because I'm not transferring hypertext)"</p>
<p>now you have two complexity demon spirit lairs</p>
<p>and, what is worse, front end complexity demon spirit even more powerful and have deep spiritual hold on entire front end
industry as far as grug can tell</p>
<p>back end developers try keep things simple and can work ok, but front end developers make very complex very quickly and
introduce lots of code, demon complex spirit</p>
<p>even when website just need put form into database or simple brochure site!</p>
<p>everyone do this now!</p>
<p>grug not sure why except maybe facebook and google say so, but that not seem very good reason to grug</p>
<p>grug not like big complex front end libraries everyone use</p>
<p>grug make <a href="https://htmx.org/">htmx</a> and <a href="https://hyperscript.org/">hyperscript</a> to avoid</p>
<p>keep complexity low, simple HTML, avoid lots javascript, the natural ether of spirit complexity demon</p>
<p>maybe they work for you, but no job post, sorry</p>
<p>react better for job and also some type application, but also you become alcolyte of complexity demon whether you like
or no, sorry such is front end life</p>
<h2><a name="grug-on-fads"></a><a href="#grug-on-fads">Fads</a></h2>
<p>grug note lots of fads in development, especially front end development today</p>
<p>back end better more boring because all bad ideas have tried at this point maybe (still retry some!)</p>
<p>still trying all bad ideas in front end development so still much change and hard to know</p>
<p>grug recommend taking all revolutionary new approach with grain salt: big brains have working for long
time on computers now, most ideas have tried at least once</p>
<p>grug not saying can't learn new tricks or no good new ideas, but also much of time wasted on recycled bad ideas, lots of
spirit complexity demon power come from putting new idea willy nilly into code base</p>
<h2><a name="grug-on-fold"></a><a href="#grug-on-fold">Fear Of Looking Dumb</a></h2>
<p>note!  very good if senior grug willing to say publicly: "hmmm, this too complex for grug"!</p>
<p>many developers Fear Of Looking Dumb (FOLD), grug also at one time FOLD, but grug learn get over: very important senior
grug say "this too complicated and confuse to me"</p>
<p>this make it ok for junior grugs to admit too complex and not understand as well, often such case!  FOLD major source of
complexity demon power over developer, especially young grugs!</p>
<p>take FOLD power away, very good of senior grug!</p>
<p>note: important to make thinking face and look big brained when saying though.  be prepare for big brain or, worse and
much more common, <em>thinks</em> is big brain to make snide remark of grug</p>
<p>be strong! no FOLD!</p>
<p>club sometimes useful here, but more often sense of humor and especially last failed project by big brain very useful,
so collect and be calm</p>
<h2><a name="grug-on-imposter-syndrom"></a><a href="#grug-on-imposter-syndrom">Impostor Syndrome</a></h2>
<p>grug note many such impostor feels in development</p>
<p>always grug one of two states: grug is ruler of all survey, wield code club like thor OR grug have no idea what doing</p>
<p>grug is mostly latter state most times, hide it pretty well though</p>
<p>now, grug make softwares of much work and <a href="https://star-history.com/#bigskysoftware/htmx&amp;bigskysoftware/_hyperscript&amp;Date">moderate open source success</a>
, and yet grug himself often feel not any idea what doing!  very often!  grug still fear make mistake break everyone code and
disappoint other grugs, imposter!</p>
<p>is maybe nature of programming for most grug to feel impostor and be ok with is best: nobody imposter if everybody imposter</p>
<p>any young grug read this far probably do fine in program career even if frustrations and worry is always to be there, sorry</p>
<h2><a name="grug-reads"></a><a href="#grug-reads">Reads</a></h2>
<p>grug like these:</p>
<ul>
<li><a href="https://www.dreamsongs.com/WorseIsBetter.html">Worse is Better</a></li>
<li><a href="https://www.dreamsongs.com/Files/worse-is-worse.pdf">Worse is Better is Worse</a></li>
<li><a href="https://www.dreamsongs.com/Files/IsWorseReallyBetter.pdf">Is Worse Really Better?</a></li>
<li><a href="https://www.goodreads.com/en/book/show/39996759-a-philosophy-of-software-design">A Philosophy of Software Design</a></li>
</ul>
<h2><a name="lol-lmao"></a><a href="#lol-lmao">Conclusion</a></h2>
<p><em>you</em> say: complexity <em>very</em>, <em>very</em> bad</p>

  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bzip2 crate switches from C to 100% Rust (317 pts)]]></title>
            <link>https://trifectatech.org/blog/bzip2-crate-switches-from-c-to-rust/</link>
            <guid>44303361</guid>
            <pubDate>Tue, 17 Jun 2025 20:06:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://trifectatech.org/blog/bzip2-crate-switches-from-c-to-rust/">https://trifectatech.org/blog/bzip2-crate-switches-from-c-to-rust/</a>, See on <a href="https://news.ycombinator.com/item?id=44303361">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <p>Today we published <code>bzip2</code> version <code>0.6.0</code>, which uses our rust implementation of the bzip2 algorithm, <code>libbz2-rs-sys</code>, by default. The <code>bzip2</code> crate is now faster and easier to cross-compile.</p>
<p>The <code>libbz2-rs-sys</code> crate can also be built as a C dynamic library, if you have a C project that would benefit from these improvements.</p>
<h2 id="why-though">Why though?</h2>
<p>Why bother working on this algorithm from the 90s that sees very little use today? The thing is that many protocols and libraries still need to support bzip2 to be compliant with their specification, so many project still, deep down in their dependency tree, depend on bzip2. We've used our experience from zlib-rs to modernize the <code>bzip2</code>  implementation.</p>
<p>We've previously written about the implementation details of <code>libbz2-rs-sys</code> in <a href="https://trifectatech.org/blog/translating-bzip2-with-c2rust/">"Translating bzip2 with c2rust"</a>, now let's look at the benefits of this work.</p>
<h3 id="improved-performance">Improved performance</h3>
<p>Our rust implementation generally outperforms the C implementation, though there are a couple of cases where we only match C performance. We are not aware of any cases where we are substantially slower.</p>
<p>For compression, we are a fair amount faster. For bzip2, the <code>level</code> indicates how much working memory is used. It doesn't influence performance by much, and for <code>sample3.ref</code> level 1 already allocates more memory than the file is large, so higher levels are irrelevant.</p>
<table><thead><tr><th>name</th><th>c (cpu cycles)</th><th>rust (cpu cycles)</th><th>Δ</th></tr></thead><tbody>
<tr><td>sample3.ref (level 1)</td><td><code>38.51M ±  77.03K</code></td><td><code>33.53M ±  90.52K</code></td><td><code>-14.87%</code></td></tr>
<tr><td>silesia-small.tar (level 1)</td><td><code> 3.43G ±   2.06M</code></td><td><code> 3.00G ±   6.31M</code></td><td><code>-14.30%</code></td></tr>
<tr><td>silesia-small.tar (level 9)</td><td><code> 3.47G ±   4.86M</code></td><td><code> 3.17G ±   4.43M</code></td><td><code>- 9.66%</code></td></tr>
</tbody></table>
<p>For decompression there is a bit more of a spread, but we again see significant speedups across the board.</p>
<table><thead><tr><th>name</th><th>c (cpu cycles)</th><th>rust (cpu cycles)</th><th>Δ</th></tr></thead><tbody>
<tr><td>sample3.bz2</td><td><code> 2.53M ±  30.08K</code></td><td><code> 2.42M ±   8.95K</code></td><td><code>- 4.48%</code></td></tr>
<tr><td>sample1.bz2</td><td><code> 9.63M ±  40.44K</code></td><td><code> 8.86M ±  10.64K</code></td><td><code>- 8.63%</code></td></tr>
<tr><td>sample2.bz2</td><td><code>20.47M ±  55.28K</code></td><td><code>19.02M ±  36.13K</code></td><td><code>- 7.67%</code></td></tr>
<tr><td>dancing-color.ps.bz2</td><td><code>87.46M ± 481.02K</code></td><td><code>83.16M ± 548.86K</code></td><td><code>- 5.17%</code></td></tr>
<tr><td>re2-exhaustive.txt.bz2</td><td><code> 1.89G ±  12.29M</code></td><td><code> 1.76G ±  12.64M</code></td><td><code>- 7.65%</code></td></tr>
<tr><td>zip64support.tar.bz2</td><td><code> 2.32G ±  12.09M</code></td><td><code> 2.11G ±  15.42M</code></td><td><code>-10.00%</code></td></tr>
</tbody></table>
<p>One caveat is that on our macOS benchmark machine we occasionally see some lower numbers for decompression. We are not sure what causes the variance, and measuring performance on macOS in a detailed way has turned out to be difficult (e.g there is no tool like <code>perf</code> to automate performance tracking that we could get to work).</p>
<h3 id="enabling-cross-compilation">Enabling cross-compilation</h3>
<p>Cross-compilation of a rust project with C dependencies often works out of the box (because the <code>cc</code> crate tries to handle it), but when it doesn't the errors can be hard to debug. Similarly linking to system libraries can cause confusing and hard-to-reproduce issues.</p>
<p>For bzip2, compilation to webassembly has long been an issue. By removing the C dependency and using rust code instead, the complications of compiling C just disappear: cross-compilation just works. Also building for windows or android just works. Besides providing a better experience for users, this change is also a major maintenance win.</p>
<h3 id="symbols-are-not-exported-by-default">Symbols are not exported (by default)</h3>
<p>Using a C dependency means that its symbols are exported (so that a rust <code>extern</code> block can find them). The exported names can conflict when another dependency declares the same symbols.</p>
<p>By default, <code>libbz2-rs-sys</code> does not export its symbols, which means that it will never conflict with other dependencies. If your rust project does need to emit the symbols, there is a feature flag to enable exporting symbols.</p>
<h3 id="run-tests-with-miri">Run tests with miri</h3>
<p>Writing a performant bzip2 implementation requires some unsafe code, and replicating the C interface in rust requires a lot more. Luckily we are able to run that code under MIRI.</p>
<p>More importantly, higher-level libraries or applications that use <code>bzip2</code> can now run with MIRI as well.</p>
<h2 id="audit">Audit</h2>
<p>The audit found one logic bug (an off-by-one error), and fixed some limitations in our fuzzer.
Beyond that, there were no significant findings (yay!).  We do want to thank the reviewers from <a href="https://www.radicallyopensecurity.com/">Radically Open Security</a>, specifically Christian Reitter, for sharing their fuzzing experience. The full audit report can be found <a href="https://github.com/trifectatechfoundation/libbzip2-rs/blob/main/docs/audits/NGICore%20bzip2%20in%20rust%20code%20audit%20report%202025%201.0.pdf">here</a>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The <code>bzip2</code> crate is faster now. You can go back to never having to think about it.</p>
<h3 id="thanks">Thanks</h3>
<ul>
<li><a href="https://github.com/alexcrichton">Alex Crichton</a> for sharing maintainership of the <code>bzip2</code> crate</li>
<li><a href="https://www.radicallyopensecurity.com/">Radically Open Security</a> for the audit and sharing their expertise</li>
<li><a href="https://nlnet.nl/">NLnet Foundation</a> for funding this work</li>
</ul>

                <br>
                
                <hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Google Translate can tell us about vibecoding (263 pts)]]></title>
            <link>https://ingrids.space/posts/what-google-translate-can-tell-us-about-vibecoding/</link>
            <guid>44302870</guid>
            <pubDate>Tue, 17 Jun 2025 19:23:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ingrids.space/posts/what-google-translate-can-tell-us-about-vibecoding/">https://ingrids.space/posts/what-google-translate-can-tell-us-about-vibecoding/</a>, See on <a href="https://news.ycombinator.com/item?id=44302870">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

    <title>What Google Translate Can Tell Us About Vibecoding | Ingrid's Space</title>

    

    
      

    

    


    

    <p>There has been rather a lot of doomsaying (and perhaps astroturfing) lately about LLMs as the end of computer programming. Much of the discussion has been lacking nuance, so I’d like to add mine. I see claims from one side that “I used <code>$LLM_SERVICE_PROVIDER</code> to make a small throwaway tool, so all programmers will be unemployed in <code>$ARBITRARY_TIME_WINDOW</code>”, and from the other side flat-out rejections of the idea that this type of tool can have any utility.<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> I think it best sheds light on these claims to examine them in the context of another field that’s been ahead of the curve on this: translation.</p>
<p>Google translate has been around for a while, and has gone through some technological iterations; I’m most interested in discussing its recent incarnations since the switch to <a href="https://arxiv.org/abs/1609.08144">neural machine translation</a> in 2016. Over the years I’ve heard much made about how this is the end of translation and interpretation as professions. I suspect the people who say such things have never actually worked with translator or interpreter. The emblematic example I’ve encountered is “I went on holiday to Japan and we used Google Translate everywhere, there’s no need to hire an interpreter or learn Japanese anymore”. While this undoubtedly speaks for the usefulness of current machine translation technology, the second half of the sentence calls for some scrutiny, particularly “anymore”. I feel confident in asserting that people who say this would not have hired a translator or learned Japanese in a world without Google Translate; they’d have either not gone to Japan at all, or gone anyway and been clueless foreigners as tourists are wont to do.</p>
<p>Indeed it turns out <a href="https://www.npr.org/sections/planet-money/2024/06/18/g-s1-4461/if-ai-is-so-good-why-are-there-still-so-many-jobs-for-translators">the number of available job opportunities for translators and interpreters has actually been increasing</a>. This is not to say that the technology isn’t good, I think it’s pretty close to as good as it can be at what it does. It’s also not to say that machine translation hasn’t changed the profession of translation: in the article linked above, Bridget Hylak, a representative from the American Translators Association, is quoted as saying “Since the advent of neural machine translation (NMT) around 2016, which marked a significant improvement over traditional machine translation like Google Translate, we [translators and interpreters] have been integrating AI into our workflows.”</p>
<p>To explain this apparent contradiction, we need to understand what it is translators actually do because, like us programmers, they suffer from having the nature of their work consistently misunderstood by non-translators. The laity’s image of a translator is a walking dictionary and grammar reference, who substitutes words and and grammatical structures from one language to another with ease, the reality is that a translators’ and interpreters’ work is mostly about ensuring context, navigating ambiguity, and handling cultural sensitivity. This is what Google Translate cannot currently do.</p>
<p>To give a simple example, Norwegian is an extremely closely related language to English and should be an easy translation candidate. The languages share a tonne of cognates, very similar grammar, and similar cultural context; even the idioms tend to translate verbatim. Yet there remain important cultural differences, and a particularly friction-prone one is Norwegian’s lack of polite language. It’s technically possible to say please in Norwegian (vær så snill, or vennligst), but Norwegians tend to prefer blunt communication, and these are not used much in practice. At the dinner table a Norwegian is likely to say something like “Jeg vil ha potetene” (literally “I will have the potatoes”, which sounds presumptuous and haughty in English) where a brit might say “Could I please have some potatoes?”. A good interpreter would have the necessary context for this (or ask for clarification if they’re not sure) and provide a sensitive translation, Google Translate just gives the blunt direct translation. You can probably work past such misunderstandings at dinner with your foreign in-laws (and people do), but it should be apparent why it’s inadvisable to <a href="https://web.archive.org/web/20170811181816/http://www.businessinsider.com/teesside-magistrates-court-forced-to-rely-on-google-translate-because-it-had-no-interpreter-2017-8">subsititute Google Translate for an interpreter at a court hearing</a>. And Norwegian is an easy case. Returning to our tourists, Japanese has wildly different grammar to English, including things like omitting subjects from sentences where it’s apparent from context. In many of these cases you can’t construct a grammatical English sentence without a subject, so Google translate will make one up. Would you be comfortable with a computer inserting a made up subject into your sentence?</p>
<p>All this is not to say Google Translate is doing a bad job. Were I given “Jeg vil ha potetene” with no context or ability to clarify and asked to translate it to English, I’d give the same answer. Maybe the person does want to be rude, how should I know? As a bilingual, I actually do make heavy use of Google Translate, but my use case isn’t “Here’s a block of text, translate it for me”. Instead I have more specific and subtle workflows like “I already know what I want to say, how to say it, and can navigate cultural nuance, but I’m not happy with my wording, I’d like to see the most statistically likely way someone else might phrase this” (A task language models really excel in, as it turns out). I suspect this is what Bridget Hylak meant when she said she has been integrating AI into her workflows (though I also suspect her tools and workflows are more sophisticated than mine).<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></p>
<p>It’s a similar story for programming. I think it’s even fair to characterise us as translators, just from squishy humans that speak in ambiguity and cultural nuance, to computers that deal only in absolutes.<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> There’s the added complication that we create new abstractions a lot more aggressively in programming languages, and that’s probably why it took machine translation to programming languagues a little while to catch up to machine translation between natural languages, but Big Tech™ chucked all of open source into a wood chipper, and we’re there now.</p>
<p>For what it’s worth, I don’t think it’s inconceivable that some future form of AI could handle context and ambiguity as well as humans do, but I do think we’re at least one more <a href="">AI winter</a> away from that, especially considering that today’s AI moguls seem to have no capacity for nuance, and care more about their tools appearing slick and frictionless than providing responsible output.</p>



</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLMs pose an interesting problem for DSL designers (203 pts)]]></title>
            <link>https://kirancodes.me/posts/log-lang-design-llms.html</link>
            <guid>44302797</guid>
            <pubDate>Tue, 17 Jun 2025 19:17:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kirancodes.me/posts/log-lang-design-llms.html">https://kirancodes.me/posts/log-lang-design-llms.html</a>, See on <a href="https://news.ycombinator.com/item?id=44302797">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<main id="content">
<div id="outline-container-Programming%20Language%20Design%20in%20the%20Era%20of%20LLMs%3A%20A%20Return%20to%20Mediocrity%3F">
<h2 id="Programming%20Language%20Design%20in%20the%20Era%20of%20LLMs%3A%20A%20Return%20to%20Mediocrity%3F">Programming Language Design in the Era of LLMs: A Return to Mediocrity?<br><span><time datetime="2025-06-17">17 Jun, 2025</time></span>&nbsp;&nbsp;&nbsp;<span><span>programming_languages</span></span>&nbsp;<span><span>code</span></span>&nbsp;<span><span>llm</span></span>&nbsp;<span><span>perspectives</span></span></h2>
<div id="text-Programming%20Language%20Design%20in%20the%20Era%20of%20LLMs%3A%20A%20Return%20to%20Mediocrity%3F">
<p>
The most exciting part of Programming Languages (PL) research for me
has always been in Programming Language Design.
</p>

<p>
By carefully crafting a language with a syntax and semantics tailored
for a specific domain, PL designers can provide an interface for end
users that seamlessly aligns with the sensibilities and intuitions of
practitioners, allowing users to focus on the "interesting" parts of a
problem and tackle larger and more complex problems.
</p>

<p>
Instead of writing a verbose sequence of API calls to display a dialog
to a user in a video game:
</p>
<div>
<pre><code><span># </span><span>example code for a VN</span>
character.draw<span>(</span><span>"alice"</span>, character.LEFT, 0.1<span>)</span>
character.draw<span>(</span><span>"bob"</span>, character.RIGHT, 0.1<span>)</span>
character.say<span>(</span><span>"alice"</span>, <span>"hello there!"</span><span>)</span>
character.say<span>(</span><span>"bob"</span>, <span>"hi!"</span><span>)</span>
character.state<span>(</span><span>"alice"</span>, <span>"sad"</span><span>)</span>
character.say<span>(</span><span>"alice"</span>, <span>"did you hear the news?"</span><span>)</span>
</code></pre>
</div>
<p>
A DSL instead allows designers to focus on the <i>high-level</i> of what the conversation should be:
</p>
<div>
<pre><code><span># </span><span>example DSL for dialog</span>
  <span>[</span> alice @ left <span>in</span> 0.1, bob @right <span>in</span> 0.1  <span>]</span>
alice: hello there!
bob: hi!
alice<span>[</span>sad<span>]</span>: did you hear the news?...
</code></pre>
</div>
<p>
By encoding the "common sense rules" of a domain into the language
itself, we can make writing incorrect programs impossible, and
eliminate cognitive load and minimise the surface area for bugs and exploits. 
</p>

<blockquote>
<p>
A DSL for every domain.  When you have eliminated all that is
incorrect, then whatever remain, however complex, esoteric or
convoluted, simply must be correct.
</p>
</blockquote>

<p>
This has been a fun, exciting and impactful thread of research for the
past several decades, but it would be remiss of me at this point to
not mention the "e-<b>LLM</b>-ephant" in the room. It has only been a few
years, and LLMs and LLM-generated code has already permeated widely
across the software ecosystem and continuously forces developers to
reevaluate their preconceptions of what is and isn't possible for a
machine to generate. Namely, this also includes problems that we might
previously have sought to tackle with language design (eliminating
boilerplate, capturing conventions or common sense etc.).
</p>

<p>
This emerging landscape holds a lot of potential, and there are many
interesting questions in asking how LLMs can contribute to software
development, but as I watch, I am also noticing a worrying trend of
LLM developments supplanting advances and interest in the design of
DSLs: why craft a DSL that eliminates all boilerplate when an LLM can
generate whatever code you need?
</p>

<p>
Is there a future for Language Design in this new era of LLMs? The
point of this blog post is to present some thoughts I've been thinking
about in this emerging space, prompt for discussion, and outline some
potential ways forward language design can co-exist and collaborate
with the advances in LLMs.
</p>
</div>
<div id="outline-container-The%20LLM%20Problem%2C%20or%2C%20rather%2C%20Everything%20is%20Easier%20in%20Python">
<h3 id="The%20LLM%20Problem%2C%20or%2C%20rather%2C%20Everything%20is%20Easier%20in%20Python">The LLM Problem, or, rather, Everything is Easier in Python<br></h3>
<div id="text-The%20LLM%20Problem%2C%20or%2C%20rather%2C%20Everything%20is%20Easier%20in%20Python">
<p>
Let's start with what I see as the biggest problem that the
introduction of LLMs is presenting to language design: <b>everything is
easier in Python</b>.
</p>

<p>
That's a little hyperbolic, but what I'm really getting at here is
that LLMs have been consistently found to have substantially higher
efficiacies when operating in programming languages that are well
represented within their training set – think languages like Python,
Javascript, Typescript etc.
</p>


<figure id="org0a469f1">
<img src="https://kirancodes.me/images/llm-comparison-graph.png" alt="llm-comparison-graph.png">

</figure>

<p>
For example, the above graph taken from the paper <a href="https://dl.acm.org/doi/abs/10.1145/3689735">"Knowledge Transfer
from High-Resource to Low-Resource Programming Languages for Code
LLMs"</a> (2024) shows the performance of one of the LLM models
(StarCoderBase-15B) on solving programming tasks in several
languages against the proportion of the training data represented by
files from that language.
</p>

<p>
The paper presents a technique for improving the performance of these
"low-reseource" languages by synthetically generating data, and so the
upward lines in this graph represent improvements after fine-tuning
using this data, but such transformations have only been applied to
smaller models, and certainly nothing at the scale of the production
models (ChatGPT, CoPilot, Gemini, Claude etc.) that are the most
actively used nowadays.
</p>

<p>
Looking at the original points in this data, the graph plots a bleak
picture: as languages become more niche and specific, the performance
of these models drops off a cliff and becomes abysmal — and bear in
mind, even these "low-resource" langauges themselves are in their own
right, production, industrial systems with millions of users; just not
producing enough code for the LLM to do well on them.
</p>

<blockquote>
<p>
If the performance of LLMs drop so sharply even for these general
purpose languages, then what can anyone expect from running an LLM on
a domain specific language?
</p>
</blockquote>

<p>
Suddenly <b>the opportunity cost for a DSL has just doubled</b>: in the land
of LLMs, a DSL requires not only the investment of build and design
the language and tooling itself, but the end users will have to
sacrifice the use of LLMs to generate any code for your DSL.
</p>

<p>
This brings me to my biggest fear moving forward: will DSLs stagnate?
Will anyone bother writing DSLs if using a niche language forces them
to elide any use of LLMs? or has the barrier to entry to DSL design
simply just jumped up, where now developers will have to work extra
hard to build DSLs that justify losing the ability to use LLMs with
your DSL?
</p>
</div>
</div>
<div id="outline-container-Emerging%20Directions%20of%20Language%20Design%20in%20the%20Land%20of%20LLMs">
<h3 id="Emerging%20Directions%20of%20Language%20Design%20in%20the%20Land%20of%20LLMs">Emerging Directions of Language Design in the Land of LLMs<br></h3>
<div id="text-Emerging%20Directions%20of%20Language%20Design%20in%20the%20Land%20of%20LLMs">
<p>
Okay, so with the doomer-posting out of the way, in this section I
want to take a little bit more of a more optimistic perspective and
think about ways in which language design might evolve and adjust to
work in cooperation with LLMs.
</p>

<p>
So far, there are three interesting directions that I immediately see
for the future, but if you have more I'd love to hear about them!
</p>
</div>
<div id="outline-container-Language%20Design%20Direction%201%3A%20Teaching%20LLMs%20about%20DSLs%20%28through%20Python%3F%29">
<h4 id="Language%20Design%20Direction%201%3A%20Teaching%20LLMs%20about%20DSLs%20%28through%20Python%3F%29">Language Design Direction 1: Teaching LLMs about DSLs (through Python?)<br></h4>
<div id="text-Language%20Design%20Direction%201%3A%20Teaching%20LLMs%20about%20DSLs%20%28through%20Python%3F%29">
<p>
Okay, so the problem with using LLMs on DSLs is that, by their very
nature, the syntax and semantics of a DSL will differ substantially
from general programming languages. This means that without further
context, it can be challenging for an LLM to understand what
constructs in a DSL mean and how they should be used together to
achieve different programming tasks…
</p>

<p>
So… how about we give them that context?
</p>

<p>
I've seen a trend in recent papers such as <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/48bb60a0c0aebb4142bf314bd1a5c6a0-Paper-Conference.pdf">Verified Code Transpilation
with LLMs</a> (2024) where researchers have had success in generating
expressions in niche languages (in this case, logical invariants) by
instead asking the LLM to generate expressions in a well-known
language (in this case, Python), and manually translating to the
obscure language of interest.
</p>

<p>
In the mentioned paper, the authors want to use LLMs to automatically
transpile tensor-processing code into different DSLs. In order to
ensure that the code is correct, they also ask the LLM to generate
invariants that can be used to prove equivalence of both programs:
</p>
<div>
<pre><code><span># </span><span>example invariant from the paper Verified Code Transpilation with LLMs</span>
<span>def</span> <span>invariant_outer</span><span>(</span>row, col, b, a, out<span>)</span>:
   <span>return</span> row <span>&gt;=</span> 0 <span>and</span> row <span>&lt;=</span> <span>len</span><span>(</span>b<span>)</span> <span>and</span>
      out <span>==</span> matrix_scalar_sub<span>(</span>255, matrix_add<span>(</span>b<span>[</span>:i<span>]</span>, a<span>[</span>:i<span>]</span><span>)</span>
</code></pre>
</div>
<p>
The key trick in this paper is the use of Python as an intermediate
language, where the authors ask LLMs to generate code in restricted
subsets of python first, and then write programs to "lift" these
python expressions into their DSLs of choice. In this way, the authors
can effectively use LLMs to generate code for bespoke DSLs without
having to do expensive fine-tuning steps or retraining the model.
</p>

<p>
Generalising this idea and broadening to the problem of language
design in the big, the question I'd like to pose is, can we do this
kind of translation automatically? For example, can we create DSL
design frameworks that also come with LLM-friendly python descriptions
of their semantics? Maybe we can produce frameworks that will test
that these python encodings have the same behaviour as the code they
model, can we automatically generate the python descriptions from the
implementation of the DSL itself?
</p>
</div>
</div>
<div id="outline-container-Language%20Design%20Direction%202%3A%20Bridging%20Formal%20and%20Informal%20with%20LLMs%20in%20DSLs">
<h4 id="Language%20Design%20Direction%202%3A%20Bridging%20Formal%20and%20Informal%20with%20LLMs%20in%20DSLs">Language Design Direction 2: Bridging Formal and Informal with LLMs in DSLs<br></h4>
<div id="text-Language%20Design%20Direction%202%3A%20Bridging%20Formal%20and%20Informal%20with%20LLMs%20in%20DSLs">
<p>
Taking another stab at the problem, another interesting direction that
I see at the intersection of DSLs and LLMs is in investigating new
ways of designing DSLs that work with LLM-based coding workflows.
</p>

<p>
Let me start with a brief interlude on how I have been using LLMs in
my own work, and how they have changed how I write certain kinds of
code – namely, scripts.
</p>

<p>
A lot of my programming work on a day-to-day basis involves working on
the internals of various verification systems. For these kinds of
systems, all the code is pretty complex and intricate, and I really
need to write every line myself. In these situations, LLMs really
aren't that helpful and any LLM-generated code usually fails to
maintain important invariants of the system and use the appropriate
APIs.
</p>

<p>
In contrast, scripts, are things that I write fairly infrequently, are
usually one of, and something where using LLMs has substantially
changed the way in which I write these programs.
</p>

<p>
Here's an example of a prompt that I recently wrote to generate some
Python code to do some basic data analysis:
</p>
<blockquote>
<ul>
<li>write a massively parallel script that iterates through all thy
files in <code>afp-versions</code>, runs the function <code>split_file</code> which you
pass in the text contents of the file which returns a pandas df with
the columns <code>name</code>, <code>start_line</code>, <code>end_line</code>, <code>first_word</code></li>

<li>for each file, record: version (name of immediate subdir under
<code>afp-versions</code> that we are in), and project, (the thy files will be
under a subdir thys after the verison, so the project is the
immediate subdir under that. (extend all rows in the df returned by
<code>split_file</code> with these params)</li>

<li>iteratively merge all of these files into a single dataframe
incrementally and with restarting, and show progress using tqdm, and
use threadpool for parallelism</li>
</ul>
</blockquote>
<p>
Now the description above mostly explains what this code was meant to
do, and the LLM generated code did exactly what I needed it to do.
</p>

<p>
Now, the interesting thing about this snippet from a language design
perspective is that it generates an "incomplete" program – in
particular, I don't ask the LLM to generate the function <code>split_file</code>,
and instead just give a specification for what its inputs and outputs
will be and as relevant to the rest of the task.
</p>

<p>
In a broad sense, when I'm interacting with the LLM for these kinds of
scrappy one-off scripts, I'm outlining the high level plan, and asking
the LLM to generate the glue code, and then manually implementing the
"interesting" part of the problem myself.
</p>

<p>
From a language design perspective, the question that I'd like to pose
from these experiences is, how can we incorporate these kinds of
workflows into a DSL? Namely, how can we bridge the gap between the
formal and informal? My manually written code is in the realm of
"formal", and my textual prompt is in the realm of "informal", and in
this snippet, I do that by encoding a specification of the formal in
the informal as a text component. Can we do this automatically? Can we
build DSLs that integrate seamlessly with informal text? Maybe
automatically generating the natural language specifications based on
the types/analysis that the DSL itself does?
</p>
</div>
</div>
<div id="outline-container-Language%20Design%20Direction%203%3A%20Language%20Design%20for%20Verified%20LLM%20Synthesis">
<h4 id="Language%20Design%20Direction%203%3A%20Language%20Design%20for%20Verified%20LLM%20Synthesis">Language Design Direction 3: Language Design for Verified LLM Synthesis<br></h4>
<div id="text-Language%20Design%20Direction%203%3A%20Language%20Design%20for%20Verified%20LLM%20Synthesis">
<p>
This is probably the most actively being researched of the directions
I've covered in this post so far, but another interesting area for
Language Design to move towards following the advent of LLMs is
towards the design of specification langauges.
</p>

<p>
So at a high level, since LLMs have been picking up steam, there has
been a cottage industry of researchers jumping into the fray of
investigating whether we can use verification langauges, such as Dafny
or Boogie and so on, to be able to verify the output of LLM-generated
code. The first paper I saw in this direction was <a href="https://dl.acm.org/doi/abs/10.1145/3643763">"Towards AI-Assisted
Synthesis of Verified Dafny Methods"</a> (2024), though I'm sure there are
now many many more that have been published, and several more in the
works.
</p>

<div>
<pre><code><span>// </span><span>Example from Towards AI-Assisted Synthesis of Verified Dafny Methods</span>
<span>method</span> <span>FindSmallest</span><span>(</span><span>s</span>: <span>array</span>&lt;<span>int</span>&gt;<span>)</span> <span>returns</span> <span>(</span><span>min</span>: <span>int</span><span>)</span>
  <span>requires</span> s.Length &gt; 0
  <span>ensures</span> <span>forall</span> i :: 0 &lt;= i &lt; s.Length ==&gt; min &lt;= s<span>[</span>i<span>]</span>
  <span>ensures</span> <span>exists</span> i :: 0 &lt;= i &lt; s.Length &amp;&amp; min == s<span>[</span>i<span>]</span> <span>{</span>
 ...
<span>}</span>
</code></pre>
</div>
<p>
Instead of just asking the model to generate some code, which may be
complex, intricate and contain bugs, the authors suggest instead
asking the model to generate programs in verified languages such as
Dafny with specifications. This way, users can just look at the
specifications to understand what the program does and do not need to
understand the LLM code unless the verification fails.<sup><a id="fnr.1" href="#fn.1" role="doc-backlink">1</a></sup>
</p>

<p>
From a language design perspective, the interesting questions in this
domain consist of asking how we can a) integrate these specifications
into DSLs? and b) how we can better design our verification DSLs to
capture properties of interest in bespoke domains – of course, the
properties you care about for a dialog DSL are going to be quite
different from one for maybe a packet routing DSL. Can we
automatically build specification langauges from the implementation of
our language DSL?
</p>
</div>
</div>
</div>
<div id="outline-container-Conclusion%3A%20Language%20Design%20for%20LLMs">
<h3 id="Conclusion%3A%20Language%20Design%20for%20LLMs">Conclusion: Language Design for LLMs<br></h3>
<div id="text-Conclusion%3A%20Language%20Design%20for%20LLMs">
<p>
To conclude this article, I think LLMs pose an interesting problem for
the DSL designers – the opportunity cost for using niche languages is
now substantially increasing, and so we, as language designers, will
be held to a higher standard to justify the use of our DSLs. At the
same time, they certainly have radically changed the space of what is
possible, and also opened up several interesting problems to explore
moving forwards.
</p>

<p>
Main takeway, language design and DSLs will have to adjust for this
crazy new world we're living in, and if we're not careful, there's a
very real chance the space of language design will stagnate, and we'll
lose the diversity of fun and interesting DSLs and everyone will just
end up writing Python…
</p>
</div>
</div>
</div>
</main>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Iran asks its people to delete WhatsApp from their devices (333 pts)]]></title>
            <link>https://apnews.com/article/iran-whatsapp-meta-israel-d9e6fe43280123c9963802e6f10ac8d1</link>
            <guid>44302752</guid>
            <pubDate>Tue, 17 Jun 2025 19:12:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/iran-whatsapp-meta-israel-d9e6fe43280123c9963802e6f10ac8d1">https://apnews.com/article/iran-whatsapp-meta-israel-d9e6fe43280123c9963802e6f10ac8d1</a>, See on <a href="https://news.ycombinator.com/item?id=44302752">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        <p>Iranian state television on Tuesday afternoon urged the country’s public to remove the messaging platform WhatsApp from their smartphones, alleging the app — without offering specific evidence — gathered user information to send to Israel.</p><p>In a statement, WhatsApp said it was “concerned these false reports will be an excuse for our services to be blocked at a time when people need them the most.” WhatsApp uses end-to-end encryption, meaning a service provider in the middle can’t read a message.</p><p>“We do not track your precise location, we don’t keep logs of who everyone is messaging and we do not track the personal messages people are sending one another,” it added. “We do not provide bulk information to any government.”</p><p><span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/privacy-encryption-signal-whatsapp-9faf31ed3411bc5b7cab0647b4ab224d">End-to-end encryption</a></span> means that messages are scrambled so that only the sender and recipient can see them. If anyone else intercepts the message, all they will see is a garble that can’t be unscrambled without the key.</p>
    
<p>WhatsApp is owned by Meta Platforms, the parent company of Facebook and Instagram. </p><p>Iran has blocked access to various social media platforms over the years but many people in the country use proxies and virtual private networks, or VPNs, to access them. It banned WhatsApp and Google Play in 2022 during mass protests against the government over the death of a woman held by the country’s morality police. That ban was <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/iran-social-media-whatsapp-google-d886b47c427f33f96fb85e7c78d0b831">lifted late last year</a></span>.</p><p>WhatsApp had been one of Iran’s most popular messaging apps besides Instagram and Telegram. </p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[From SDR to 'Fake HDR': Mario Kart World on Switch 2 (108 pts)]]></title>
            <link>https://www.alexandermejia.com/from-sdr-to-fake-hdr-mario-kart-world-on-switch-2-undermines-modern-display-potential/</link>
            <guid>44302704</guid>
            <pubDate>Tue, 17 Jun 2025 19:07:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.alexandermejia.com/from-sdr-to-fake-hdr-mario-kart-world-on-switch-2-undermines-modern-display-potential/">https://www.alexandermejia.com/from-sdr-to-fake-hdr-mario-kart-world-on-switch-2-undermines-modern-display-potential/</a>, See on <a href="https://news.ycombinator.com/item?id=44302704">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">

		
					<main id="main">
				

<article id="post-385" itemtype="https://schema.org/CreativeWork" itemscope="itemscope">

	
	
<div ast-blocks-layout="true" itemprop="text">

		
		
<p>Nintendo’s Switch 2 launched on <strong>June 5th 2025</strong> with <em>Mario Kart World</em> headlining the platform and <em>on paper</em> showcasing its new <strong>4K60 + HDR</strong> output pipeline. That promise lands in a market where HDR is the consumer standard: analysts value the global HDR TV segment at <em>≈ <a href="https://www.linkedin.com/pulse/global-hdr-tv-market-comprehensive-analysis-drivers-zpkke/" target="_blank" rel="noopener" title="">US $150B in 2024</a></em> and project <em>US $250 B by 2033</em>. &nbsp;Consumer TVs with HDR functionality started shipping in 2015, and seeing that <a href="https://www.prweb.com/releases/npd-us-tv-purchasers-more-motivated-by-screen-size-and-picture-quality-than-ever-before-810878558.html" target="_blank" rel="noopener" title="most consumers replace their TV every 6.4">most consumers replace their TV every 6.4</a> years this means a high percentage console owners in 2025 now game on HDR capable screens.</p>



<p>Yet, as I’ll show in this article, <em>Mario Kart World</em> surfaces an industry-wide problem: <strong>SDR-first authoring with a last-minute tonemap hack ruins the experience.&nbsp; </strong>I’d figure that SDR games that masquerade as HDR, or “<em>fake HDR</em>” coined by some more incendiary YouTubers, would have been a trend left back in 2020, but here we are in 2025 with a new generation of consoles with a headliner game that still reduces color gamut to SDR, and has no more dynamic range than the SDR presentation.&nbsp; In this article I’ll show my evidence of why I think this game is a “<em>fake HDR</em>” title, and what developers can do to avoid this in the future.</p>



<p>I approach this critique with some experience. <a href="https://www.alexandermejia.com/dolby-vision-on-xbox-series-x/" target="_blank" rel="noopener" title="">I led Dolby Vision for Games program</a> on Xbox Series X|S, helping developers ship Dolby Vision masters on <em>Godfall</em>, <em>Halo Infinite</em>, and <em>COD Warzone</em>, and I’m consulting on more titles still under NDA. Those projects taught me that <strong>HDR excellence starts at the very first art review—not in the final weeks of polish</strong>.</p>



<h2><strong>Test Methodology</strong></h2>



<p>Here is my capture chain that I’m gathering this information with.&nbsp; If you think I’m doing something wrong I’d love to know.</p>



<p><strong>Hardware &amp; capture path</strong></p>



<ul>
<li>Launch Model Nintendo Switch 2 over HDMI on the official dock to –&gt;</li>



<li>Blackmagic DeckLink 4K mini.</li>



<li>Captured in BlackMagic Media Express (3.8.1) in ProRes 4444 on Mac OS (15.1) Mac Studio M1 Ultra</li>



<li>Viewed on Asus ProArt PA27UCX mastering monitor (2,000 nits, Rec.2020 PQ, hardware calibrated)</li>



<li>Davinci Resolve to analyze captures</li>
</ul>



<h3>How can you view this best at home?</h3>



<p>Images posted on this site are in HDR in the AVIF format with the full rec.2020 PQ image data.&nbsp; It’s lossy, but should give a good representation of the HDR experience as long as you have a  display that goes up to 1,000 nits to view the full dynamic range of the scene.&nbsp; You should have HDR enabled on your OS.</p>



<p>I recommend viewing this on a Macbook Pro laptop with the built-in Pro Display XDR, or a high end PC HDR monitor (Similar to the ProArt I’ve mentioned earlier, but there are many more displays of similar quality or better now), or an LG C4/G4 or greater TV in the Cinema mode picture preset.</p>



<p>Google Chrome and other Chromium based browsers seem to have the best viewing experience.&nbsp; Safari will be adding HDR image support later, but as of this writing, it’s not out of beta.</p>



<h3>Capture Procedure</h3>



<ol start="1">
<li>Set Nintendo Switch 2 Console settings to 4k 60, HDR, limited range</li>



<li>Closed the game if it was running already (Some titles on consoles set their HDR tonemap only once at startup, this bypasses any issues when swapping the OS mapping during testing)</li>



<li>Opened the game from a clean start and Ran a single lap in Mario Bros. Circuit at 50CC</li>



<li>Recorded using BlackMagic Media Express into ProRes 4444 Codec (This is a 16-bit 4:4:4 capture medium, however the source content is only 10-bpc)</li>



<li>Recorded a lap pass utilizing steps 1-4.&nbsp; Each time changing the Switch 2 “Maximum Brightness” HDR calibration: <strong>205 nits → 1,000 nits → 2,000 nits → 10,000 nits</strong> (I verified console UI allows the full PQ range utilizing Davinci Resolve).</li>



<li>Captured gameplay, and analyzed in Davinci Resolve Waveform Monitor</li>
</ol>



<h2>Findings</h2>



<h3>Static tone mapping and a clamped 1,000 nit ceiling</h3>



<p>The game’s peak brightness at console 205 nits, is about 205 nits.&nbsp; This is behaving like it should</p>



<div><figure><img decoding="async" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/205Nits.avif ,https://www.alexandermejia.com/wp-content/uploads/2025/06/205Nits.avif 780w, https://www.alexandermejia.com/wp-content/uploads/2025/06/205Nits.avif 360w" sizes="(max-width: 480px) 150px" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/205Nits.avif" alt="Mario Kart World screenshot at max 205 nits" width="1920" height="1080" title="Mario Kart World at 205 nits" role="img"></figure></div>



<figure><img decoding="async" width="1024" height="542" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/205nit_histogram-1024x542.png" alt="" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/205nit_histogram-1024x542.png 1024w, https://www.alexandermejia.com/wp-content/uploads/2025/06/205nit_histogram-300x159.png 300w, https://www.alexandermejia.com/wp-content/uploads/2025/06/205nit_histogram-768x407.png 768w, https://www.alexandermejia.com/wp-content/uploads/2025/06/205nit_histogram.png 1280w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>The game’s peak brightness at console 2,000 nits is about 900 nits.&nbsp; This is likely the intended behavior but makes the art look washed out as clouds do not have defined detail.</p>



<div><figure><img loading="lazy" decoding="async" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/2000Nits.avif ,https://www.alexandermejia.com/wp-content/uploads/2025/06/2000Nits.avif 780w, https://www.alexandermejia.com/wp-content/uploads/2025/06/2000Nits.avif 360w" sizes="auto, (max-width: 480px) 150px" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/2000Nits.avif" alt="Mario Kart world at Max 2000 nits." width="1920" height="1080" title="2000Nits" role="img"></figure></div>



<div><figure><img decoding="async" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/2000nithistogram-1024x542.jpg ,https://www.alexandermejia.com/wp-content/uploads/2025/06/2000nithistogram.jpg 780w, https://www.alexandermejia.com/wp-content/uploads/2025/06/2000nithistogram.jpg 360w" sizes="auto, (max-width: 480px) 150px" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/2000nithistogram-1024x542.jpg" alt="" width="1280" height="678" title="2000nithistogram" loading="lazy" role="img"></figure></div>



<p>Even when the user cranks the console brightness to 10,000 nits, <strong>captured peaks in game never exceed ~950 nits</strong></p>



<div><figure><img loading="lazy" decoding="async" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/10kNits.avif ,https://www.alexandermejia.com/wp-content/uploads/2025/06/10kNits.avif 780w, https://www.alexandermejia.com/wp-content/uploads/2025/06/10kNits.avif 360w" sizes="auto, (max-width: 480px) 150px" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/10kNits.avif" alt="Mario Kart World screenshot taken with console at 10,000 nits peak brightness" width="1920" height="1080" title="10kNits Mario Kart World" role="img"></figure></div>



<figure><img loading="lazy" decoding="async" width="1024" height="542" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/10knit_histogram-1024x542.jpg" alt="" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/10knit_histogram-1024x542.jpg 1024w, https://www.alexandermejia.com/wp-content/uploads/2025/06/10knit_histogram-300x159.jpg 300w, https://www.alexandermejia.com/wp-content/uploads/2025/06/10knit_histogram-768x407.jpg 768w, https://www.alexandermejia.com/wp-content/uploads/2025/06/10knit_histogram.jpg 1280w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>Nintendo’s own test image peaks at only ~500 nits even if you set 10,000 nits peak brightness.&nbsp; Not a good sign that they took HDR seriously.</p>



<div><figure><img loading="lazy" decoding="async" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/testimage_nx2.avif ,https://www.alexandermejia.com/wp-content/uploads/2025/06/testimage_nx2.avif 780w, https://www.alexandermejia.com/wp-content/uploads/2025/06/testimage_nx2.avif 360w" sizes="auto, (max-width: 480px) 150px" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/testimage_nx2.avif" alt="The test Image Nintendo Switch 2 shows when at 10,000 nits peak brightness selected." width="1920" height="1080" title="Nintendo Switch 2 HDR Test Image" role="img"></figure></div>



<figure><img loading="lazy" decoding="async" width="1024" height="542" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/testhitogram-1024x542.jpg" alt="" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/testhitogram-1024x542.jpg 1024w, https://www.alexandermejia.com/wp-content/uploads/2025/06/testhitogram-300x159.jpg 300w, https://www.alexandermejia.com/wp-content/uploads/2025/06/testhitogram-768x407.jpg 768w, https://www.alexandermejia.com/wp-content/uploads/2025/06/testhitogram.jpg 1280w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>I’m unable to peek into the math in how they are tonemapping from FP16 linear space to the Rec.2100 Display space, but from my testing I can safely assume they are using a single linear tonemap which scales some fixed function math to go from FP16 -&gt; Rec.2100 with a scale factor from the console brightness setting.&nbsp;</p>



<p>It’s likely that the game’s creative intent is designed to only hit ~1,000 nits of brightness.&nbsp; This would be OK if they were limited in resources and could only get their hands on 1,000 nit peak brightness reference displays and needed to be conservative in their mastering. But why does the game not achieve 1,000 nits peak brightness when the Console max brightness is set to 2,000 or 10,000? This suggests an oversight in how their HDR tonemapper works, or they utilized scaling that is not dynamic. &nbsp;</p>



<h3>Single Slope Tone Mapping</h3>



<p>If the game’s true creative intent is to peak at 1,000 nits, then any console HDR brightness setting over 1,000 nits should have no effect on the games output.&nbsp; This suggests to me that the console peak brightness is just part of the equation they use to scale, and they aren’t considering their own creative intent for HDR.</p>



<p>It’s also possible that they didn’t have enough time to fine tune HDR and their creative intent was not achieved.&nbsp; Typically, with dynamically tonemapped games, extending the console max brightness results in seeing more detail and headroom for objects that are bright, like clouds and skies.&nbsp; In Mario Kart world, this scaling does nothing to reveal any more detail regardless of how the peak brightness setting for HDR is set.</p>



<p>This is likely due to the game being art directed only for SDR.&nbsp; In SDR you typically have to make tradeoffs in how contrasty a scene can feel, between details in the darkest or brightest objects.&nbsp; SDR is limited in dynamic range, you<em> have to give up something</em>. The easiest way to solve this issue is to take your SDR frame buffer and extend it to HDR utilizing your own tone mapping function.&nbsp; Many developers under a time constraint build these in a tool called Davinci resolve, and export a LUT to save time on this transform.&nbsp; It’s not going to give you a true HDR representation, but it’s something a single rendering engineer can cook up and implement in a week that is just a brighter, stretched version of their approved and art directed SDR version of the game.</p>



<p>To test my theory, I can use Davinci Resolve’s Rec.709 to Rec.2020 PQ color space transform and dial in settings that give me nearly identical looks.&nbsp; The clouds seem to be dynamic so some sections may show shadows that aren’t there, but when I stretch the SDR into a 1,000 nit Rec.2100 image, I get an incredibly similar result.</p>



<figure><p><img loading="lazy" decoding="async" id="400" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/a_10knit.avif" alt="" width="1920" height="1080"><img loading="lazy" decoding="async" id="401" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/b_sdrgrade.avif" alt="" width="1920" height="1080"></p><figcaption>Left = HDR capture    Right = Personal SDR inverse tonemapped to HDR</figcaption></figure>



<div>
<div>
<figure><img loading="lazy" decoding="async" width="218" height="300" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/CST_settings-218x300.png" alt="Davinci Resolve settings used to inverse tonemap from SDR to HDR" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/CST_settings-218x300.png 218w, https://www.alexandermejia.com/wp-content/uploads/2025/06/CST_settings-743x1024.png 743w, https://www.alexandermejia.com/wp-content/uploads/2025/06/CST_settings-768x1059.png 768w, https://www.alexandermejia.com/wp-content/uploads/2025/06/CST_settings.png 830w" sizes="auto, (max-width: 218px) 100vw, 218px"></figure>
</div>



<p>Because I can get such a close match utilizing this method, confirms to me that there is one SDR master of the game, and they are extending it by utilizing tone mapper functions from the FP16 linear image into the Rec.2020 PQ space.&nbsp; Simply put, you’re getting an SDR image, stretched into HDR10 in Mario Kart World.</p>
</div>







<p><h3>Wait, Why Do I See Banding In The Sky?</h3></p>



<div><figure><img loading="lazy" decoding="async" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/Skybanding.avif ,https://www.alexandermejia.com/wp-content/uploads/2025/06/Skybanding.avif 780w, https://www.alexandermejia.com/wp-content/uploads/2025/06/Skybanding.avif 360w" sizes="auto, (max-width: 480px) 150px" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/Skybanding.avif" alt="Banding in the Sky of Mario Kart World in HDR" width="1920" height="1080" title="Skybanding" role="img"></figure></div>



<p>This is fully visible at 1:1 in the ProRes 4444 capture.  I’ve zoomed in to showcase the banding on a smaller image and so that SDR users can see this as well.</p>



<p>Banding over the sky dome remains no matter if viewed in SDR, or HDR10.&nbsp; While this is minor, these issues are more visible in HDR where the image can appear brighter.&nbsp; This can happen due to lower bit-depth precision when blending this texture, or utilizing <a href="https://microsoft.github.io/DirectX-Specs/d3d/D3D12R9G9B9E5Format.html" target="_blank" rel="noopener" title="R9G9B9E5">R9G9B9E5</a> as a back buffer to reduce memory bandwidth which requires extra care like <em><a href="https://en.wikipedia.org/wiki/Dither" target="_blank" rel="noopener" title="dithering">dithering</a></em> needed to cover up these transitions.&nbsp; While I can’t do external testing to determine the exact cause, It’s something an art team would likely catch and fix if they had done art reviews in HDR10, rather than sticking to SDR, where problems like this can be very hard to see, especially on smaller displays.</p>



<h3>Stuck in SDR Color space</h3>



<p>The game’s art style is very colorful and bright, however the game makes no use of the extended color gamut afforded by Rec.2020.&nbsp; Instead all colors are clamped in Rec.709, the same color space you get when in SDR. This is easily seen by just analzing the color gamut from the HDR capture and seeing that no pixel values fall in the extended color gamut triangle.</p>



<div><figure><img decoding="async" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/HighestColorVolume-1024x673.jpg ,https://www.alexandermejia.com/wp-content/uploads/2025/06/HighestColorVolume.jpg 780w, https://www.alexandermejia.com/wp-content/uploads/2025/06/HighestColorVolume.jpg 360w" sizes="auto, (max-width: 480px) 150px" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/HighestColorVolume-1024x673.jpg" alt="" width="1372" height="902" title="The Highest Color Volume" loading="lazy" role="img"><figcaption>See that Yellow Triangle Labeled Rec.2020? That’s all the color that could be used in game</figcaption></figure></div>



<p>You might look at the HDR representations of this game and think “Wait, the game appears more colorful” and this is because of the <a href="https://en.wikipedia.org/wiki/Hunt_effect_(color)" target="_blank" rel="noopener" title="">Hunt Effect</a>. The Hunt Effect describes how we think a brighter color is more saturated, but in reality, it’s just an optical illusion. &nbsp;&nbsp;This means Nintendo has left another key piece of HDR on the table likely due to their focus on creating the game for SDR first.&nbsp; &nbsp;Their artists are missing out on bright saturated colors, like UI elements, Sparks that fly off the tires, and explosions that could be directed to pop more if they chose to.</p>



<div><figure><img decoding="async" srcset="https://www.alexandermejia.com/wp-content/uploads/2025/06/hunteffect-300x201.png ,https://www.alexandermejia.com/wp-content/uploads/2025/06/hunteffect.png 780w, https://www.alexandermejia.com/wp-content/uploads/2025/06/hunteffect.png 360w" sizes="auto, (max-width: 480px) 150px" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/hunteffect-300x201.png" alt="" width="300" height="201" title="The Hunt Effect" loading="lazy" role="img"><figcaption>Each color in this image is the same saturation on the CIE chart, but in different brightness<br>By Tom Axford 1 – Own work, building on this spectrum, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=124441017</figcaption></figure></div>



<h3>Subjective Competitive Analysis</h3>



<p>Just looking at other gameplay content that I’ve mastered before in HDR, wide color gamut can be used to help enhance the Red Sands in this Unreal demo called Rural Australia.&nbsp; I used this several times to showcase how extended color gamut made a large difference in the kinds of visuals you could achieve, even in a desert where you might think extended color gamut wasn’t really necessary.&nbsp;</p>



<div><figure><img decoding="async" srcset="https://www.alexandermejia.com/wp-content/uploads/2024/08/Rural_Austrailia.avif ,https://www.alexandermejia.com/wp-content/uploads/2024/08/Rural_Austrailia.avif 780w, https://www.alexandermejia.com/wp-content/uploads/2024/08/Rural_Austrailia.avif 360w" sizes="auto, (max-width: 480px) 150px" src="https://www.alexandermejia.com/wp-content/uploads/2024/08/Rural_Austrailia.avif" alt="" width="1280" height="720" title="Rural_Austrailia" loading="lazy" role="img"></figure></div>



<p>Putting these two gameplay clips side by side showcases what you’re missing when you don’t utilize wide color gamut, and high peak brightness.  You can more easily call out specific details, or emphasize colorful elements.  It’s more stimulating, and in general your consumers will think your game looks better.  If this developer could do this level of work in 2 months, imagine what your game would look like if you built for HDR in the first place.</p>



<figure><video controls="" loop="" preload="auto" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/GodfallUltimate_Vs_MKW-20.5.mp4"></video></figure>



<p><a href="https://www.youtube.com/watch?v=Ym3MKdm0igs" target="_blank" rel="noopener" title="">YouTube Link</a> if high quality video above doesn’t work</p>



<h2>Why Developers Do This: The SDR First Pipeline Trap</h2>



<p>Most teams still:</p>



<ol start="1">
<li><strong>Author lighting in sRGB / Rec. 709</strong> Often not viewing HDR until it’s too late.</li>



<li>Hold daily art reviews in SDR. Locking in their 0-1 range in Rec.709 making moving to wider color gamut requiring lots of rework.</li>



<li>During Production lots of art time is spent mapping the final FP16 image into SDR.</li>



<li>Then it’s thrown over the fence to rendering engineers and technical artists to make the SDR image look good in HDR.&nbsp; I hope 2-4 weeks is enough to polish the whole HDR game!</li>
</ol>



<p>I get it.&nbsp; Developers have been living the last 30+ years of their life in SDR. Moving to HDR requires different practices, and some re-learning.&nbsp; But when <a href="https://www.playstationlifestyle.net/2019/05/10/esa-essential-facts-2019-report-video-games/#:~:text=price%2C%20the%20largest%20factor%20when,with%20past%20games%20in%20the" target="_blank" rel="noopener" title="">Gamers in ESA surveys</a> report that the quality of the graphics being the #2 factor in deciding when to purchase a game, you better maximize your HDR rendering, as you can increase the visual quality of your game without having to re-author new assets if done correctly.</p>



<p>In my experience once artists view their game in HDR, major art issues surface.&nbsp; It’s not that HDR creates these issues, but it reveals things that would normally be hidden by lower dynamic range and lower color volume like the banding issue above. &nbsp;Technical Artists have to compromise, usually in a rush by making a single static tonemapping curve for HDR, and lock the color gamut to Rec.709 intentionally bringing their game look closer to SDR.</p>



<p>On paper this ticks the HDR box with minimal risk, but why even bother spending the time on HDR if you’re going to make it look like a stretched version of SDR?&nbsp; TVs already do this by extending maximum brightness, and they do it in a way the user typically prefers.&nbsp; TV manufacturers have gotten really good at making SDR images stretched into pseudo HDR as they’ve been doing this for decades now.</p>



<p>PC gamers have been complaining about these HDR issues for years now. &nbsp;Check any Reddit forum or YouTuber complaining about <em>Fake HDR</em>.&nbsp; Even <a href="https://www.digitaltrends.com/computing/pc-gaming-hdr-problem-a-way-out/" target="_blank" rel="noopener" title="Digital Trends called static HDR “sad, misleading, and embarrassing”"><em>Digital Trends</em> called static HDR “sad, misleading, and embarrassing”</a>.&nbsp; We can do better…</p>



<h2>Why It Matters in 2025</h2>



<ul>
<li><strong>Consumer hardware is good, really good.</strong> – Even mid-range TVs now exceed 1,000 nits and ship with per-frame dynamic tone mapping, Dolby Vision, and HDR10+ capabilities.</li>



<li><strong>Competitive titles are raising the bar</strong> – Games like Call of Duty Warzone, and Godfall: Ultimate Edition are exceeding 1,000 nits peak brightness and have wider color gamuts.&nbsp; I know, I helped them achieve these goals.</li>



<li><strong>HDR is mainstream</strong> – From just a quick browsing of BestBuy, nearly all TVs over 42” are 4K and support HDR. 9<sup>th</sup> gen consoles are shipping with HDR on by default. <em>The majority of your audience is HDR-equipped</em>.</li>



<li><strong>Visuals suffer</strong> – Working SDR first, limits your maximum brightness, limits your color gamut, and limits the impact your wow moments of a sunset, or a truly scary dimly lit dungeon.</li>
</ul>



<h2>What Can Developers Do?</h2>



<p>Developers, specifically art directors and rendering programmers need to take HDR seriously from the beginning.&nbsp;</p>



<ol start="1">
<li><strong>Commit to upgrading your pipelines to Wide Color Gamut at the start</strong>
<ul>
<li>Not all your textures need to be WCG, but you can have a checkbox for textures that exceed rec.709 so they can be processed by your crunching tools.</li>



<li>Too deep into production to make major pipeline changes? &nbsp;Many Developers allow for WCG in their VFX only by using numbers over 1.0 while assuming all their textures are sRGB/Rec.709. This works very well as a first step that doesn’t require you ripping up your texture pipeline but delivering WCG in areas where it has the highest impact like VFX.</li>
</ul>
</li>



<li><strong>Allow your game to have dynamic Range</strong>
<ul>
<li>As a rule of thumb I like to set my high noon daylight scenes to be at least 5x brighter than nighttime at a minimum.&nbsp; Try it out on a reference monitor and bring in all your environment artists and ask which one looks better.</li>



<li>Typically you can leave your SDR scenes for nighttime untouched. These will hit about 50-100 nits brightness.&nbsp;</li>



<li>That means cranking up the sun intensity of your outdoor scenes by about 5x gets you into a ballpark that most embedded tone mappers in TVs can play along with.</li>



<li>VFX needs to be adjusted, make standards for how bright of values your VFX artists should use for emissives like fire, sparks, gunshots, pickups etc.&nbsp; <em>Don’t let obvious things like the blast from a gun be brighter than the sun.</em></li>
</ul>
</li>



<li><strong>Build a Dynamic Tone Mapping service, or utilize ones by Dolby and HDR10+</strong>
<ul>
<li>Because not every display will be capable of hitting 2000+ nits, you should add your own dynamic tone mapping for the best results.</li>



<li>Remove “Auto Iris” or “Automatic Exposure Adjustments” from your game.&nbsp; Let the dynamic tone mapper adjust these based off the user’s display.</li>



<li>This will prevent pulsing on higher end displays, but still ensure that folks on HDR displays that may only peak at 200-300 nits still get a good experience.</li>



<li>Dolby Handles tone mapping down to SDR and has trim controls to adjust it.&nbsp; I know I’m biased towards that solution because I worked on it. Ask developers who have used it how it helped them if you want more data points.</li>
</ul>
</li>



<li><strong>Make HDR review the standard</strong>
<ul>
<li>Supply calibrated HDR monitors to art/lighting.&nbsp; Get more in your studio if budget allows for it.&nbsp;</li>



<li>Do your art reviews on calibrated HDR displays.</li>



<li>HDR offers fantastic dynamic range, color, and creative freedom for your images to look like whatever you want when done well.&nbsp; If SDR looks disappointing compared to HDR then you’re doing it right.&nbsp; (Seriously, do not let your art director reduce the quality of your HDR image in some attempt to make SDR look better, point them to this article or get on a call with them.)</li>
</ul>
</li>
</ol>



<p>If you’re Unreal based, I shipped plugins add Dolby Vision to Unreal Engine projects, even ones designed just for SDR.&nbsp; It solves many of these problems if you’re a smaller team without rendering engineering support.&nbsp; However, you’ll need to contact Dolby to get access to these plugins at the moment.&nbsp;</p>



<h2>What Mario Kart World Could Have Looked like…</h2>



<p>While the tone mapping for the game is baked in pretty tough, I did some color grading in Davinci Resolve by eliminating the 1000 nit ceiling and letting it go to 2000 nits.&nbsp; Clouds do look much better and more realistic; however detail is still lacking in those areas.&nbsp; I can’t magically paint on new detail in an afternoon.</p>



<figure><p><img loading="lazy" decoding="async" id="408" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/base_still.avif" alt="" width="1920" height="1080"><img loading="lazy" decoding="async" id="409" src="https://www.alexandermejia.com/wp-content/uploads/2025/06/graded_v2.avif" alt="" width="1920" height="1080"></p><figcaption>Left == Peak 950 Nit capture     Right == Graded to simulate 2,000 nit capture</figcaption></figure>



<p>I do like the roads and the characters being around 90-110 nits on a high noon sunny day.&nbsp; This feels right to me, but the VFX isn’t bright enough.&nbsp; Because of this they don’t pop and give you the feedback that you’re hitting a blue spark on your drift.&nbsp; Some of the UI elements that should stick out, like a 4,000 nit bright VFX pop when you pick up something. It’s all of these elements that make your game look more impressive even though you’re utilizing the same assets. &nbsp;Experiment with higher brightness and wider color gamut,&nbsp; You can achieve stunning results you just couldn’t achieve in SDR.</p>



<p>You’re already making your game in HDR and utilizing FP16 linear.&nbsp; <em>Don’t just throw the dynamic range all away at the end because you chose to be SDR first.</em></p>



<h2>Conclusion</h2>



<p><em>Mario Kart World</em> reveals that even the highest caliber of developers aren’t taking HDR seriously. The root cause is <strong>SDR first workflows</strong> that tack on a static tone mapper. Players lose, artist intent is capped, and a flagship hardware features are squandered.</p>



<p>If you’re shipping this year and want to avoid a similar post-mortem, let’s talk. I consult studios of any size on <strong>HDR first rendering pipelines, Dolby Vision integration, and dynamic tone-mapping strategies</strong>. Reach out: <a href="mailto:alexander.mejia@human-interact.com" title=""><strong>alexander.mejia@human-interact.com</strong>.</a></p>



<p>The era of checkbox HDR is over—let’s build images that finally exploit the full HDR canvas.</p>

		
		
			</div>

	
</article><!-- #post-## -->

		<!-- #comments -->

			</main><!-- #main -->
			
		
	</div></div>]]></description>
        </item>
    </channel>
</rss>