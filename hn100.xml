<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 18 Sep 2025 18:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Yes, Jimmy Kimmel's suspension was government censorship (106 pts)]]></title>
            <link>https://www.theverge.com/policy/781148/jimmy-kimmel-charlie-kirk-monologue-brendan-carr-censorship-first-amendment</link>
            <guid>45292130</guid>
            <pubDate>Thu, 18 Sep 2025 17:03:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/policy/781148/jimmy-kimmel-charlie-kirk-monologue-brendan-carr-censorship-first-amendment">https://www.theverge.com/policy/781148/jimmy-kimmel-charlie-kirk-monologue-brendan-carr-censorship-first-amendment</a>, See on <a href="https://news.ycombinator.com/item?id=45292130">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="zephr-anchor"><p>Yesterday, Disney-owned ABC <a href="https://www.theverge.com/news/780471/disney-abc-jimmy-kimmel-live-charlie-kirk">suspended <em>Jimmy Kimmel Live</em></a> “indefinitely” for a comment Kimmel made about the response to Charlie Kirk’s death. The response from Republican commentators has been <a href="https://www.hollywoodreporter.com/news/politics-news/maga-reaction-jimmy-kimmel-suspension-1236374491/">predictably gleeful</a>. They’ve positioned the suspension as a reversal of “cancel culture,” a “deplatforming” that’s simply fair retaliation for antagonizing them. “Jimmy Kimmel does have Free Speech, he is free to speak just not on ABC,” posted comedian <a href="https://x.com/w_terrence/status/1968521190693019710">Terrence Kentrell Williams on X</a>.</p><p>This framing is transparently false. ABC’s suspension of Kimmel was the result of an explicit threat from President Donald Trump’s Federal Communications Commission Chair Brendan Carr, aimed at Disney and companies that worked with it. The move was effective because of the FCC’s authority to regulate broadcast TV and, perhaps more importantly, to approve communications mergers in a hyper-consolidated landscape. It was repeating a playbook Carr recently used on Disney’s fellow media giant Paramount. And it’s an unabashed attempt at the government dictating the speech of private TV networks and entertainers, as objectionable and un-American as a McCarthyist blacklist.</p><p>The starting point of all this is a pretty tame late-night show monologue. On Monday, Kimmel said the following:</p><p>“We hit some new lows over the weekend, with the MAGA gang desperately trying to characterize this kid who murdered Charlie Kirk as anything other than one of them and doing everything they can to score political points from it.”</p><p>The line led into a clip from last week in which Trump <a href="https://www.nytimes.com/2025/09/16/arts/television/late-night-trump-white-house-ballroom.html">responded to Kirk’s death</a> by bragging about the new White House ballroom — “he’s at the fourth stage of grief: construction,” Kimmel quipped.</p><p>Carr and other Republicans loudly interpreted the remark as claiming Tyler Robinson, who’s charged with killing Kirk, was part of MAGA. (An <a href="https://www.nytimes.com/interactive/2025/09/16/us/tyler-robinson-charges.html">indictment released</a> Tuesday says Robinson believed Kirk was spreading “hate”; at the time of Kimmel’s monologue, the evidence was <a href="https://www.theverge.com/politics/777313/charlie-kirks-alleged-killer-scratched-bullets-with-a-helldivers-combo-and-a-furry-sex-meme">mostly confusing</a> and speculation was rife.)</p><p>On Wednesday, Carr <a href="https://x.com/bennyjohnson/status/1968359685045838041">appeared in high dudgeon</a> for an interview with conservative commentator Benny Johnson. “It appears to be some of the sickest conduct possible,” Carr intoned over a clip of Kimmel’s statement. It was also, he said, legally actionable. Broadcasters “have a license granted by us at the FCC, and that comes with an obligation to operate in the public interest,” he said. “We can do this the easy way or the hard way. These companies can find ways to change conduct and take action, frankly, on Kimmel, or there’s going to be additional work for the FCC ahead.”</p><p>Carr and Johnson walked out the threat a little further. They noted that Disney and ABC aren’t issued a central license to broadcast; licenses are granted to individual TV stations, owned in small numbers by Disney but mainly by separate companies that broadcast ABC and other networks’ programming. “FCC regulatory action focuses on these individual stations,” Carr noted. “The public interest means you can’t be running a narrow partisan circus and still meeting your public interest obligations.”</p><p>Citing rules against “news distortion” and “broadcast hoaxes,” Carr repeated that “there’s actions we can take on licensed broadcasters. And frankly, I think it’s really past time that a lot of these licensed broadcasters themselves push back on Comcast and Disney and say, ‘Listen, we are going to preempt, we are not going to run Kimmel anymore until you straighten this out, because we, a licensed broadcaster, are running the possibility of fines or license revocation from the FCC if we continue to run content that ends up being a pattern of news distortion.’”</p><p>This is <a href="https://www.theverge.com/24283652/fcc-license-donald-trump-elon-musk-first-amendment-fairness-doctrine-please-vote-decoder">not how any pre-Carr FCC in recent memory</a> (or arguably before that) has defined the “public interest” requirement. If the issue is a “narrow partisan circus,” the highly partisan Fox — whose news division <a href="https://www.theverge.com/2023/4/18/23688582/fox-dominion-election-defamation-lawsuit-settled-trial-averted">settled a massive defamation suit</a> for lying about voting machine companies in 2023 — has been a stolid fixture on TV stations across the country for decades. Carr invoked rules against “news distortion” and “broadcast hoaxes,” neither of which makes logical sense as a charge against Kimmel. The bar for stripping a license is typically high — a rare example is <a href="https://www.nytimes.com/1979/12/07/archives/black-group-is-awarded-license-for-television-station-in.html">the 1989 revocation</a> of aggressively pro-segregationist station WLBT, which among other things, blacked out broadcasts involving civil rights.</p><p>Some commentators on the left <em>have </em>pushed for the FCC to do what Carr threatened, just with Fox instead of ABC, particularly <a href="https://prospect.org/blogs-and-newsletters/tap/2023-08-28-will-fox-lose-its-broadcast-license/">after the defamation settlement</a>. But the FCC has never pulled a station’s license as a result of this urging, and <a href="https://deadline.com/2025/09/brendan-carr-jimmy-kimmel-public-interest-1236547721/">in 2019 Carr decried</a> even Democratic commissioner Jessica Rosenworcel’s suggestion they crack down on TV and radio e-cigarette ads, saying the FCC “does not have a roving mandate to police speech in the name of the ‘public interest.’”</p><p>Carr’s statement on Wednesday was unambiguous: ABC-affiliated TV stations needed to stop airing Kimmel’s show ASAP, or they could be fined or lose their license. And the stations’ parent companies were listening. Nexstar, which owns around 200 stations and reaches roughly 39 percent of US households, <a href="https://www.nexstar.tv/nexstar-abc-affiliates-to-preempt-jimmy-kimmel-live-indefinitely-beginning-tonight/">said quickly</a> that it would no longer air <em>Jimmy Kimmel Live</em>. So did <a href="https://sbgi.net/sinclair-says-kimmel-suspension-is-not-enough-calls-on-fcc-and-abc-to-take-additional-action/">fellow giant Sinclair Broadcast Group</a>.</p><p>Those companies, particularly Nexstar, have reasons beyond station licensing to keep Carr happy. Nexstar is pushing for a $6.2 billion merger with broadcaster Tegna that would require the FCC to loosen the rules on TV station consolidation, something CEO Perry Sook <a href="https://www.newscaststudio.com/2025/08/22/nexstar-ceo-perry-sook-defends-6-2b-tegna-acquisition-as-regulatory-shift-creates-opening/">has expressed hope</a> that Trump’s “deregulatory moment” will enable.</p><p>Carr has shown himself willing to slow-walk deals with companies that earn his ire. A merger between Paramount and Skydance was not approved until Paramount subsidiary CBS <a href="https://www.theverge.com/news/696422/paramount-settlement-trump-cbs-lawsuit">agreed to pay $16 million</a> to resolve a blatantly frivolous lawsuit filed by Trump. It proceeded on the grounds that Skydance promote a “diversity of viewpoints from across the political and ideological spectrum” and employ an ombudsman who would “receive and evaluate any complaints of bias or other concerns involving CBS.” CBS also ended the show of Kimmel’s fellow late-night host Stephen Colbert, a decision <a href="https://www.theverge.com/news/709544/stephen-colberts-version-of-the-late-show-will-end-next-may">CBS called financial</a> that was nonetheless widely seen as a concession to Trump — and that was celebrated by Carr.</p><p>The pressure from Carr, Sinclair, and Nexstar quickly reached ABC, which made a terse announcement it had pulled Kimmel’s show off the air. Outlets with inside sources have indicated this wasn’t because of public outrage or because Kimmel’s bosses found the remarks inappropriate — <a href="https://www.rollingstone.com/tv-movies/tv-movie-news/jimmy-kimmel-out-abc-charlie-kirk-comments-1235430078/"><em>Rolling Stone</em> reports</a> that “multiple execs felt that Kimmel had not actually said anything over the line.” Instead, they were “pissing themselves” over the threat of Trump administration retaliation, one source said.</p><p>Disney, too, has pressure points beyond broadcast license fines. The FCC chair has previously threatened to investigate it for having (non-“ideological,” of course) diversity programs. And as <a href="https://www.status.news/p/jimmy-kimmel-pulled-fcc-decision">Oliver Darcy of <em>Status</em> notes</a>, the company is “working to complete a high-stakes deal with the NFL, one that is crucial to the future of ESPN” and <a href="https://www.reuters.com/legal/litigation/espn-nfl-deal-faces-regulatory-hurdles-2025-08-07/">requires regulatory approval</a> from the Department of Justice. Trump <a href="https://www.nbcnews.com/politics/justice-department/attorney-general-pam-bondi-doj-hate-speech-rcna231633">incidentally told</a> an ABC journalist this week that the DOJ might “come after ABC” for “hate” offenses, responding to questions about a “hate speech” crackdown declared by Attorney General Pam Bondi after Kirk’s death.</p><p>Under the First Amendment, a government official like Carr is allowed to call Kimmel talentless or unfunny. He’s allowed to say Kimmel shouldn’t be on the air. He’s <em>not </em>allowed to accompany this with a clear threat backed by government authority. “The easy way or the hard way” isn’t healthy debate, it’s <a href="https://www.theverge.com/2025/1/20/24346317/trump-gangster-tech-regulation-corruption-grift">gangster talk</a>. Or more precisely, government jawboning.</p><p>Even the substantially Trump-picked, overwhelmingly conservative Supreme Court has condemned something very similar to Carr’s conduct. <a href="https://www.techdirt.com/2025/09/17/cowardly-disney-caves-to-brendan-carrs-bogus-censorial-threats-pulling-jimmy-kimmel/">Mike Masnick at <em>Techdirt</em> points out</a> that a ruling last year, in the case <em>NRA v. Vullo</em>, declared flatly that “the First Amendment prohibits government officials from relying on the ‘threat of invoking legal sanctions and other means of coercion . . . to achieve the suppression’ of disfavored speech.” That’s true even if they do so through intermediaries like Nexstar rather than attacking the speaker directly.</p><p>Disney, of course, didn’t have to bow to a clearly unconstitutional threat. CEO Bob Iger and Disney Entertainment chief Dana Walden, according to <em>Status</em>, were among the latest powerful figures who made a cowardly decision to appease Carr and Trump rather than stand up for themselves and their employees in public and, if necessary, in court.</p><p>But even if there’s plenty of blame to go around, Carr’s threat is impossible to ignore — and the implicit comparisons to conservative provocateurs being banned on social media, or commentators being fired after outcry for a hateful statement, or any other example of alleged “cancel culture,” off-base. Whatever the underlying offense, this isn’t a private company independently making a business judgment about its public image and financial interests. It’s a government official inserting himself into the process of making entertainment, decreeing what a comedian is allowed to say.</p><p>Carr and Johnson aren’t denying the pressure campaign. Carr responded to a request for comment from <em>Status</em> with a grinning emoji and <a href="https://x.com/BrendanCarrFCC/status/1968449919221416427">thanked Nexstar on X</a> for “doing the right thing.” Johnson <a href="https://x.com/bennyjohnson/status/1968464339515417001">crowed on X</a> that he had “ended Jimmy Kimmel’s career” by bringing Carr on to “announce investigations into ABC and Disney.”<br>It’s unclear whether Kimmel will come back on the air at some point — but either way, the Trump administration’s <a href="https://www.theverge.com/policy/779799/republican-charlie-kirk-first-amendment-crackdown-continues">war on free speech</a> is still going strong.</p><p><em><strong>Correction:</strong> The proposed merger between Nexstar and Tegna is worth $6.2 billion, not million.</em></p><p><span><a href="https://www.theverge.com/policy/781148/jimmy-kimmel-charlie-kirk-monologue-brendan-carr-censorship-first-amendment#comments"><span>0<!-- --> <!-- -->Comments</span></a></span></p><div><p><span><strong>Follow topics and authors</strong> from this story to see more like this in your personalized homepage feed and to receive email updates.</span></p><ul><li id="follow-author-article_footer-dmcyOmF1dGhvclByb2ZpbGU6MTcy"><span aria-expanded="false" aria-haspopup="true" role="button" tabindex="0"><span><span><svg width="9" height="9" viewBox="0 0 9 9" fill="none" xmlns="http://www.w3.org/2000/svg" aria-label="Follow"><path d="M5 0H4V4H0V5H4V9H5V5H9V4H5V0Z"></path></svg></span><span>Adi Robertson</span></span></span></li><li></li><li></li><li></li><li></li><li></li></ul></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[American Prairie unlocks another 70k acres in Montana (167 pts)]]></title>
            <link>https://earthhope.substack.com/p/victory-for-public-access-american</link>
            <guid>45291132</guid>
            <pubDate>Thu, 18 Sep 2025 15:47:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://earthhope.substack.com/p/victory-for-public-access-american">https://earthhope.substack.com/p/victory-for-public-access-american</a>, See on <a href="https://news.ycombinator.com/item?id=45291132">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>Public lands and public access are now constantly under threat in the U.S., but there’s still good news to share. </p><p><span>Ambitious conservation nonprofit </span><a href="https://americanprairie.org/" rel="">American Prairie</a><span> has secured its second-largest land purchase and leasing arrangement to date, buying up the 70,000-acre Anchor Ranch in Montana, which had been listed for sale for $35 million. The group bought the land from </span><a href="https://dailymontanan.com/2025/09/06/american-prairie-announces-new-acquisitions-access-in-breaks/" rel="">two billionaire Texas brothers </a><span>who’d kept the public locked out of one of the only western access roads into adjacent public land, the Upper Missouri River Breaks National Monument, which totals almost 400,000 acres. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!dM3L!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80005b74-8e2f-4967-858d-b63faecdd9a1_6214x4143.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!dM3L!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80005b74-8e2f-4967-858d-b63faecdd9a1_6214x4143.jpeg 424w, https://substackcdn.com/image/fetch/$s_!dM3L!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80005b74-8e2f-4967-858d-b63faecdd9a1_6214x4143.jpeg 848w, https://substackcdn.com/image/fetch/$s_!dM3L!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80005b74-8e2f-4967-858d-b63faecdd9a1_6214x4143.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!dM3L!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80005b74-8e2f-4967-858d-b63faecdd9a1_6214x4143.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!dM3L!,w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80005b74-8e2f-4967-858d-b63faecdd9a1_6214x4143.jpeg" width="1200" height="800.2747252747253" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/80005b74-8e2f-4967-858d-b63faecdd9a1_6214x4143.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;large&quot;,&quot;height&quot;:971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:1200,&quot;bytes&quot;:17891594,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://earthhope.substack.com/i/173040155?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80005b74-8e2f-4967-858d-b63faecdd9a1_6214x4143.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:&quot;center&quot;,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!dM3L!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80005b74-8e2f-4967-858d-b63faecdd9a1_6214x4143.jpeg 424w, https://substackcdn.com/image/fetch/$s_!dM3L!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80005b74-8e2f-4967-858d-b63faecdd9a1_6214x4143.jpeg 848w, https://substackcdn.com/image/fetch/$s_!dM3L!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80005b74-8e2f-4967-858d-b63faecdd9a1_6214x4143.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!dM3L!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80005b74-8e2f-4967-858d-b63faecdd9a1_6214x4143.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Photo courtesy of American Prairie.</figcaption></figure></div><p>“This is a once-in-a-generation opportunity to secure an important piece of ecologically and culturally significant land,” said American Prairie CEO Alison Fox in a press release. “But this acquisition is equally important as a way to return public access to the people so they can explore, hunt, and recreate on land that’s been off-limits for many years.”</p><p><span>The group </span><a href="https://www.facebook.com/reel/1059049422744737" rel="">shared videos</a><span> of staff unlocking a gate and tearing down “no trespassing” signs along a 3.8-mile section of Bullwhacker Road, which was the subject of lawsuits for many years. The move was hailed by hunting and </span><a href="https://www.backpacker.com/news-and-events/new-land-acquisition-american-prairie/" rel="">recreation</a><span> groups that had fought to keep the road open but lost their bid in 2011 when a judge ruled the road was private. The move essentially blocked public access to 50,000 acres of the monument.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!J7xO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ea423fc-25e7-49de-a6d9-3d224d3c8a4d_822x1259.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!J7xO!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ea423fc-25e7-49de-a6d9-3d224d3c8a4d_822x1259.jpeg 424w, https://substackcdn.com/image/fetch/$s_!J7xO!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ea423fc-25e7-49de-a6d9-3d224d3c8a4d_822x1259.jpeg 848w, https://substackcdn.com/image/fetch/$s_!J7xO!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ea423fc-25e7-49de-a6d9-3d224d3c8a4d_822x1259.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!J7xO!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ea423fc-25e7-49de-a6d9-3d224d3c8a4d_822x1259.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!J7xO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ea423fc-25e7-49de-a6d9-3d224d3c8a4d_822x1259.jpeg" width="302" height="462.5523114355231" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3ea423fc-25e7-49de-a6d9-3d224d3c8a4d_822x1259.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1259,&quot;width&quot;:822,&quot;resizeWidth&quot;:302,&quot;bytes&quot;:149002,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://earthhope.substack.com/i/173040155?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91723c4c-eb88-4a04-8ccc-65523c3aa196_3939x1294.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!J7xO!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ea423fc-25e7-49de-a6d9-3d224d3c8a4d_822x1259.jpeg 424w, https://substackcdn.com/image/fetch/$s_!J7xO!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ea423fc-25e7-49de-a6d9-3d224d3c8a4d_822x1259.jpeg 848w, https://substackcdn.com/image/fetch/$s_!J7xO!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ea423fc-25e7-49de-a6d9-3d224d3c8a4d_822x1259.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!J7xO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ea423fc-25e7-49de-a6d9-3d224d3c8a4d_822x1259.jpeg 1456w" sizes="100vw"></picture></div></a></figure></div><p><span>Building ties with new allies is critical for American Prairie, which has faced </span><a href="https://dailymontanan.com/2021/12/05/gianforte-knudsen-try-to-stop-american-prairies-bison-through-political-pressure/" rel="">decades of distrust </a><span>in a conservative state run on beef cattle ranching. The state refuses to classify bison as wildlife instead of livestock, meaning they must be fenced. </span></p><p>Undeterred, American Prairie keeps buying up land, tearing down or widening its fences, and growing its 900-head bison herd. Its holdings now total over 600,000 acres.</p><p>The group is trying to preserve the last untouched swath of shortgrass prairie on the planet, 3.2 million acres that are home to bison, badgers, black-tailed prairie dog, black-footed ferret, pronghorn, sage grouse, and swift fox, most of which are endangered species.</p><p>— American Prairie</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!13BE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09c7cb7b-ec23-4cc7-bfdf-27918c630159_1568x1015.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!13BE!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09c7cb7b-ec23-4cc7-bfdf-27918c630159_1568x1015.jpeg 424w, https://substackcdn.com/image/fetch/$s_!13BE!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09c7cb7b-ec23-4cc7-bfdf-27918c630159_1568x1015.jpeg 848w, https://substackcdn.com/image/fetch/$s_!13BE!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09c7cb7b-ec23-4cc7-bfdf-27918c630159_1568x1015.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!13BE!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09c7cb7b-ec23-4cc7-bfdf-27918c630159_1568x1015.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!13BE!,w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09c7cb7b-ec23-4cc7-bfdf-27918c630159_1568x1015.jpeg" width="1200" height="777.1978021978022" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/09c7cb7b-ec23-4cc7-bfdf-27918c630159_1568x1015.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;large&quot;,&quot;height&quot;:943,&quot;width&quot;:1456,&quot;resizeWidth&quot;:1200,&quot;bytes&quot;:418572,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://earthhope.substack.com/i/173040155?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09c7cb7b-ec23-4cc7-bfdf-27918c630159_1568x1015.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:&quot;center&quot;,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!13BE!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09c7cb7b-ec23-4cc7-bfdf-27918c630159_1568x1015.jpeg 424w, https://substackcdn.com/image/fetch/$s_!13BE!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09c7cb7b-ec23-4cc7-bfdf-27918c630159_1568x1015.jpeg 848w, https://substackcdn.com/image/fetch/$s_!13BE!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09c7cb7b-ec23-4cc7-bfdf-27918c630159_1568x1015.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!13BE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09c7cb7b-ec23-4cc7-bfdf-27918c630159_1568x1015.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>American Prairie landholdings map as of September 2025. Anchor Ranch is marked in blue in the top left.</figcaption></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!kahJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b60595-04cc-4716-843f-072d5f907ceb_3072x2048.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!kahJ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b60595-04cc-4716-843f-072d5f907ceb_3072x2048.jpeg 424w, https://substackcdn.com/image/fetch/$s_!kahJ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b60595-04cc-4716-843f-072d5f907ceb_3072x2048.jpeg 848w, https://substackcdn.com/image/fetch/$s_!kahJ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b60595-04cc-4716-843f-072d5f907ceb_3072x2048.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!kahJ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b60595-04cc-4716-843f-072d5f907ceb_3072x2048.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!kahJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b60595-04cc-4716-843f-072d5f907ceb_3072x2048.jpeg" width="1456" height="971" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/20b60595-04cc-4716-843f-072d5f907ceb_3072x2048.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2084152,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://earthhope.substack.com/i/173040155?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b60595-04cc-4716-843f-072d5f907ceb_3072x2048.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!kahJ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b60595-04cc-4716-843f-072d5f907ceb_3072x2048.jpeg 424w, https://substackcdn.com/image/fetch/$s_!kahJ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b60595-04cc-4716-843f-072d5f907ceb_3072x2048.jpeg 848w, https://substackcdn.com/image/fetch/$s_!kahJ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b60595-04cc-4716-843f-072d5f907ceb_3072x2048.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!kahJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b60595-04cc-4716-843f-072d5f907ceb_3072x2048.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Anchor Ranch lies in a key migration corridor for pronghorn, elk, and deer. Photo of pronghorn doe and calves by Diane Hargreaves, courtesy of American Prairie.</figcaption></figure></div><p><span>Read more from </span><em>Earth Hope</em><span> about American Prairie:</span></p><div data-component-name="DigestPostEmbed"><a href="https://earthhope.substack.com/p/bison-country-just-grew-bigger" rel="noopener" target="_blank"><h2>Bison country just grew bigger</h2></a><div><div><a href="https://earthhope.substack.com/p/bison-country-just-grew-bigger" rel="noopener" target="_blank"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!bPcv!,w_280,h_280,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3a937f5-23ee-482c-ad7e-11bc48e0dedf_1383x2048.jpeg"><img src="https://substackcdn.com/image/fetch/$s_!bPcv!,w_280,h_280,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3a937f5-23ee-482c-ad7e-11bc48e0dedf_1383x2048.jpeg" sizes="100vw" alt="Bison country just grew bigger" width="280" height="280"></picture></a></div><div><p>Under Montana’s big sky, where buffalo by the tens of thousands once thundered across temperate grasslands teeming with wildlife like prairie dogs and ferrets, elk and grizzly bears, a nonprofit is pushing forward with its ambitious plan to save one of the l…</p></div></div></div><p><em>Grist </em><span>(April 2025)</span><em>:</em><strong><span> </span><a href="https://grist.org/indigenous/wildlife-not-livestock-why-the-eastern-shoshone-in-wyoming-are-reclassifying-buffaloes/" rel="">Wildlife, not livestock: Why the Eastern Shoshone in Wyoming are reclassifying buffalo</a></strong></p><p><span>Wind River Tribal Buffalo Initiative (August 2025): </span><strong><a href="https://windriverbuffalo.org/northern-arapaho-tribe-officially-classifies-buffalo-as-wildlife/" rel="">Northern Arapaho Tribe officially classifies buffalo as wildlife on Wind River Indian Reservation</a></strong></p><p><span>National Caucus of Environmental Legislators (May, 2025): </span><strong><a href="https://ncel.net/articles/colorado-grants-wild-bison-legal-protection/" rel="">Colorado grants wild bison legal protection</a></strong></p><p><span>Earthjustice: </span><strong><a href="https://earthjustice.org/article/for-tribes-that-have-bison-youve-got-something-back-that-was-taken-from-you" rel="">In Montana, wild bison</a></strong><a href="https://earthjustice.org/article/for-tribes-that-have-bison-youve-got-something-back-that-was-taken-from-you" rel=""> </a><strong><a href="https://earthjustice.org/article/for-tribes-that-have-bison-youve-got-something-back-that-was-taken-from-you" rel="">are back, and an entire ecosystem is healing</a></strong><span> (In 2013, bison returned to the Fort Belknap Reservation, which is adjacent to the lands American Prairie is protecting.)</span></p><p><span>High Country News: </span><strong><a href="https://www.hcn.org/issues/53-2/indigenous-affairs-tribes-reclaiming-the-national-bison-range/" rel="">Reclaiming the National Bison Range</a><span> (</span></strong><span>For 113 years, an 18,000-acre federal wildlife refuge for bison lay like a donut hole inside the </span><a href="https://www.bisonrange.org/history/" rel="">Flathead Indian Reservation</a><span> in Montana. In 2021, Congress passed legislation to return it to the Confederated Salish and Kootenai Tribes.) </span></p><p><a href="https://earthhope.substack.com/subscribe?coupon=8bb41ac1" rel="">HELP SUPPORT SOLUTIONS-BASED JOURNALISM FOR JUST $20</a><span>. </span><em>Earth Hope</em><span> is not affiliated with American Prairie, but all subscription revenues this week (Sept 9-16) will go to American Prairie.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://earthhope.substack.com/p/victory-for-public-access-american?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://earthhope.substack.com/p/victory-for-public-access-american?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p data-attrs="{&quot;url&quot;:&quot;https://earthhope.substack.com/p/victory-for-public-access-american/comments&quot;,&quot;text&quot;:&quot;Leave a comment&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://earthhope.substack.com/p/victory-for-public-access-american/comments" rel=""><span>Leave a comment</span></a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!xkzA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bccb03b-a709-4771-beec-c355bfc3c40c_1895x2412.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!xkzA!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bccb03b-a709-4771-beec-c355bfc3c40c_1895x2412.jpeg 424w, https://substackcdn.com/image/fetch/$s_!xkzA!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bccb03b-a709-4771-beec-c355bfc3c40c_1895x2412.jpeg 848w, https://substackcdn.com/image/fetch/$s_!xkzA!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bccb03b-a709-4771-beec-c355bfc3c40c_1895x2412.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!xkzA!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bccb03b-a709-4771-beec-c355bfc3c40c_1895x2412.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!xkzA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bccb03b-a709-4771-beec-c355bfc3c40c_1895x2412.jpeg" width="413" height="525.6105769230769" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2bccb03b-a709-4771-beec-c355bfc3c40c_1895x2412.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1853,&quot;width&quot;:1456,&quot;resizeWidth&quot;:413,&quot;bytes&quot;:4709662,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://earthhope.substack.com/i/173040155?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bccb03b-a709-4771-beec-c355bfc3c40c_1895x2412.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!xkzA!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bccb03b-a709-4771-beec-c355bfc3c40c_1895x2412.jpeg 424w, https://substackcdn.com/image/fetch/$s_!xkzA!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bccb03b-a709-4771-beec-c355bfc3c40c_1895x2412.jpeg 848w, https://substackcdn.com/image/fetch/$s_!xkzA!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bccb03b-a709-4771-beec-c355bfc3c40c_1895x2412.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!xkzA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bccb03b-a709-4771-beec-c355bfc3c40c_1895x2412.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Prairie dog on American Prairie lands. Photo by Dennis Lingohr, courtesy of American Prairie.</figcaption></figure></div><p><em>Earth Hope</em><span> is a solutions-based journalism project that highlights environmental success stories to inspire action. I’m </span><a href="https://open.substack.com/users/183550901-amanda-royal?utm_source=mentions" rel="">Amanda Royal</a><span>, a former newspaper reporter and current eco-news junkie. </span><a href="https://earthhope.substack.com/about" rel="">Read more</a><span> about this project and what inspired it. Visit </span><a href="https://earthhope.substack.com/" rel="">earthhope.substack.com</a><span> for more stories.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TernFS – An exabyte scale, multi-region distributed filesystem (125 pts)]]></title>
            <link>https://www.xtxmarkets.com/tech/2025-ternfs/</link>
            <guid>45290245</guid>
            <pubDate>Thu, 18 Sep 2025 14:36:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.xtxmarkets.com/tech/2025-ternfs/">https://www.xtxmarkets.com/tech/2025-ternfs/</a>, See on <a href="https://news.ycombinator.com/item?id=45290245">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent">
    
<p><strong>September 2025</strong></p>
<p>XTX is an algorithmic trading firm: it builds statistical models that produce price forecasts for over 50,000 financial instruments worldwide. We use those forecasts to make trades. As XTX's research efforts to build better models ramped up, the demand for resources kept increasing.</p>
<p>The firm started out with a couple of desktops and an NFS server, and 10 years later ended up with tens of thousands of high-end GPUs, hundreds of thousands of CPUs, and hundreds of petabytes of storage.</p>
<p>As compute grew, storage struggled to keep up. We rapidly outgrew NFS first and existing open-source and commercial filesystems later. After evaluating a variety of third-party solutions, we made the decision to implement our own filesystem, which we called TernFS<sup><a href="#f1">[1]</a></sup>.</p>

<p>We have decided to open source our efforts: TernFS is <a href="https://github.com/XTXMarkets/ternfs">available as free software on our public GitHub.</a> This post <a href="#another-filesystem">motivates TernFS</a>, explains its <a href="#high-level">high-level architecture</a>, and then explores some <a href="#important-details">key implementation details</a>. If you just want to spin up a local TernFS cluster, head to the <a href="https://github.com/XTXMarkets/ternfs?tab=readme-ov-file#playing-with-a-local-ternfs-instance">README</a>.</p>

<h2>Another filesystem?</h2>
<p>There's a reason why every major tech company has developed its own distributed filesystem — they're crucial to running large-scale compute efforts, and liable to cause intense disruption if they malfunction. <sup><a href="#f2">[2]</a></sup></p>

<p>XTX was in the same position, so we designed TernFS to be a one-stop solution for most of our storage needs, going from relatively 'cold' storage of raw market data to short-lived random-access data used to communicate between GPU jobs running on our cluster.</p>
<p>TernFS:</p>
<ul>
<li>Is designed to scale up to tens of exabytes, trillions of files, millions of concurrent clients.</li>
<li>Stores file contents redundantly to protect against drive failures.</li>
<li>Has no single point of failure in its metadata services.</li>
<li>Supports file snapshot to protect against accidental file deletion.</li>
<li>Can span across multiple regions.</li>
<li>Is hardware agnostic and uses TCP/IP to communicate.</li>
<li>Utilizes different types of storage (such as flash vs. hard disks) cost effectively.</li>
<li>Exposes read/write access through its own API over TCP and UDP, and a Linux kernel filesystem module.</li>
<li>Requires no external service and has a minimal set of build dependencies. <sup><a href="#f3">[3]</a></sup></li>
</ul>

<p>Naturally, there are some limitations, the main ones being:</p>
<ul>
<li>Files are immutable — once they're written they can't be modified.</li>
<li>TernFS should not be used for tiny files — our median file size is 2MB.</li>
<li>The throughput of directory creation and removal is significantly constrained compared to other operations.</li>
<li>TernFS is permissionless, deferring that responsibility to other services.</li>
</ul>
<p>We started designing TernFS in early 2022 and began putting it into production in summer 2023. By mid-2024 all of our machine learning efforts were driven out of TernFS, and we're migrating the rest of the firm's storage needs onto it as well.</p>
<p>As of September 2025, our TernFS deployment stores more than 500PB across 30,000 disks, 10,000 flash drives, and three data centres. At peak we serve multiple terabytes per second. To this day, we haven't lost a single byte.</p>

<h2>High-level overview</h2>
<p>Now that the stage is set, we're ready to explain the various components that make up TernFS. TernFS' core API is implemented by four services:</p>
<ul>
<li><em>Metadata shards</em> store the directory structure and file metadata.</li>
<li>The <em>cross-directory coordinator</em> (or CDC) executes cross-shard transactions.</li>
<li><em>Block services</em> store file contents.</li>
<li>The <em>registry</em> stores information about all the other services and monitors them.</li>
</ul>
<pre><code>
 A ──► B means "A sends requests to B" 
                                       
                                       
 ┌────────────────┐                    
 │ Metadata Shard ◄─────────┐          
 └─┬────▲─────────┘         │          
   │    │                   │          
   │    │                   │          
   │ ┌──┴──┐                │          
   │ │ CDC ◄──────────┐     │          
   │ └──┬──┘          │     │          
   │    │             │ ┌───┴────┐     
   │    │             └─┤        │     
 ┌─▼────▼────┐          │ Client │     
 │ Registry  ◄──────────┤        │     
 └──────▲────┘          └─┬──────┘     
        │                 │            
        │                 │            
 ┌──────┴────────┐        │            
 │ Block Service ◄────────┘            
 └───────────────┘

</code></pre>

<p>In the next few sections, we'll describe the high-level design of each service and then give more background on <a href="#important-details">other relevant implementation details</a>.<sup><a href="#f4">[4]</a></sup></p>

<h3>Metadata</h3>
<p>To talk about metadata, we first need to explain what metadata <em>is</em> in TernFS. The short answer is: 'everything that is not file contents.' The slightly longer answer is:</p>
<ul>
<li>Directory entries, including all files and directory names.</li>
<li>File metadata including creation/modification/access time, logical file size, and so on.</li>
<li>The mapping between files and the <a href="#block-services">blocks containing their contents</a>.</li>
<li>Other ancillary data structures to facilitate maintenance operations.</li>
</ul>
<p>TernFS' metadata is split into 256 logical <em>shards</em>. Shards never communicate with each other. This is a general principle in TernFS: each service is disaggregated from the others, deferring to the clients to communicate with each service directly.<sup><a href="#f5">[5]</a></sup></p>

<p>A logical shard is further split into five physical instances, one leader and four followers, in a typical distributed consensus setup. The distributed consensus engine is provided by a purpose-built Raft-like implementation, which we call LogsDB, while RocksDB is used to implement read/write capabilities within a shard instance.</p>
<p>Currently all reads and writes go through the leader, but it would be trivial to allow clients to read from followers, and with a bit more effort to switch to a write-write setup.</p>
<pre><code>    ┌─────────┐ ┌─────────┐       ┌───────────┐ 
    │ Shard 0 │ │ Shard 1 │  ...  │ Shard 255 │ 
    └─────────┘ │         │       └───────────┘ 
            ┌───┘         └───────────────────┐ 
            │                                 │ 
            │                  ┌────────────┐ │ 
            │ ┌───────────┐    │ Replica 0  │ │ 
            │ │           ◄────► (follower) │ │ 
 ┌────────┐ │ │ Replica 3 ◄──┐ └────────────┘ │ 
 │ Client ├─┼─► (leader)  ◄─┐│ ┌────────────┐ │ 
 └────────┘ │ │           ◄┐│└─► Replica 1  │ │ 
            │ └───────────┘││  │ (follower) │ │ 
            │              ││  └────────────┘ │ 
            │              ││  ┌────────────┐ │ 
            │              │└──► Replica 2  │ │ 
            │              │   │ (follower) │ │ 
            │              │   └────────────┘ │ 
            │              │   ┌────────────┐ │ 
            │              └───► Replica 4  │ │ 
            │                  │ (follower) │ │ 
            │                  └────────────┘ │ 
            └─────────────────────────────────┘ 
</code></pre>

<p>Splitting the metadata into 256 shards from the get-go simplifies the design, given that horizontal scaling of metadata requires no rebalancing, just the addition of more metadata servers.</p>
<p>For instance, our current deployment can serve hundreds of petabytes and more than 100,000 compute nodes with just 10 metadata servers per data centre, with each server housing roughly 25 shard leaders and 100 shard followers.</p>
<p>Given that the metadata servers are totally decoupled from one another, this means that we can scale metadata performance by 25× trivially, and by 100× if we were to start offloading metadata requests to followers.</p>
<p>TernFS shards metadata by assigning each directory to a single shard. This is done in a simple round-robin fashion by the <a href="#cdc">cross-directory coordinator</a>. Once a directory is created, all its directory entries and the files in it are housed in the same shard.</p>
<p>This design decision has downsides: TernFS assumes that the load will be spread across the 256 logical shards naturally. This is not a problem in large deployments, given that they will contain many directories, but it is something to keep in mind.<sup><a href="#f6">[6]</a></sup></p>


<h3>Cross-directory transactions</h3>
<p>Most of the metadata activity is contained within a single shard:</p>
<ul>
<li>File creation, same-directory renames, and deletion.</li>
<li>Listing directory contents.</li>
<li>Getting attributes of files or directories.</li>
</ul>
<p>However, some operations do require coordination between shards, namely directory creation, directory removal, and moving directory entries across different directories.</p>
<p>The <em>cross-directory coordinator</em> (CDC) performs these distributed transactions using a privileged metadata shard API. The CDC transactions are stateful, and therefore the CDC uses RocksDB and LogsDB much like the metadata shards themselves to persist its state safely.</p>
<pre><code> ┌────────┐    ┌──────────┐ ┌───────────┐ 
 │ Client ├─┐  │ Shard 32 │ │ Shard 103 │ 
 └────────┘ │  └────────▲─┘ └─▲─────────┘ 
 ┌─────┬────┼───────────┼─────┼─┐         
 │ CDC │  ┌─▼──────┐    │     │ │         
 ├─────┘  │ Leader ├────┴─────┘ │         
 │        └─────▲──┘            │         
 │              │               │         
 │       ┌──────┴───────┐       │         
 │       │              │       │         
 │ ┌─────▼────┐    ┌────▼─────┐ │         
 │ │ Follower │ .. │ Follower │ │         
 │ └──────────┘    └──────────┘ │         
 └──────────────────────────────┘   
</code></pre>

<p>The CDC executes transactions in parallel, which increases throughput considerably, but it is still a bottleneck when it comes to creating, removing, or moving directories. This means that TernFS has a relatively low throughput when it comes to CDC operations.<sup><a href="#f7">[7]</a></sup> <a name="block-services"></a></p>

<h3>Block services, or file contents</h3>
<p>In TernFS, files are split into chunks of data called <em>blocks</em>. Blocks are read and written to by <em>block services</em>. A block service is typically a single drive (be it a hard disk or a flash drive) storing blocks. At XTX a typical storage server will contain around 100 hard disks or 25 flash drives — or in TernFS parlance 100 or 25 block services.<sup><a href="#f8">[8]</a></sup></p>

<p>Read/write access to the block service is provided using a simple TCP API currently implemented by a Go process. This process is hardware agnostic and uses the Go standard library to read and write blocks to a conventional local file system. We originally planned to rewrite the Go process in C++, and possibly write to block devices directly, but the idiomatic Go implementation has proven performant enough for our needs so far. <a name="registry"></a></p>
<h3>The registry</h3>
<p>The final piece of the TernFS puzzle is the <em>registry</em>. The registry stores the location of each instance of service (be it a metadata shard, the CDC, or a block storage node). A client only needs to know the address of the registry to mount TernFS — it'll then gather the locations of the other services from it.</p>
<p>In TernFS all locations are IPv4 addresses. Working with IPv4 directly simplifies the kernel module considerably, since DNS lookups are quite awkward in the Linux kernel. The exception to this rule is addressing the registry itself, for which DNS is used.</p>
<p>The registry also stores additional information, such as the capacity and available size of each drive, who is a follower or a leader in LogsDB clusters, and so on.</p>
<p>Predictably, the registry itself is a RocksDB and LogsDB C++ process, given its statefulness. <a name="going-global"></a></p>
<h3>Going global</h3>
<p>TernFS tries very hard not to lose data, by storing both metadata and file contents on many different drives and servers. However, we also want to be resilient to the temporary or even permanent loss of one entire data centre. Therefore, TernFS can transparently scale across multiple <em>locations</em>.</p>
<p>The intended use for TernFS locations is for each location to converge to the same dataset. This means that each location will have to be provisioned with roughly equal resources.<sup><a href="#f9">[9]</a></sup> Both metadata and file contents replication are asynchronous. In general, we judge the event of losing an entire data centre rare enough to tolerate a time window where data is not fully replicated across locations.</p>

<p>Metadata replication is set up so that one location is the metadata primary. Write operations in non-primary locations pay a latency price since they are acknowledged only after they are written to the primary location, replicated, and applied in the originating location. In practice this hasn't been an issue since metadata write latencies are generally overshadowed by writing file contents.</p>
<p>There is no automated procedure to migrate off a metadata primary location — again, we deem it a rare enough occurrence to tolerate manual intervention. In the future we plan to move from the current protocol to a multi-master protocol where each location can commit writes independently, which would reduce write latencies on secondary locations and remove the privileged status of the primary location.</p>
<p>File contents, unlike metadata, are written locally to the location the client is writing from. Replication to other locations happens in two ways: proactively and on-demand. Proactive replication is performed by tailing the metadata log and replicating new file contents. On-demand replication happens when a client requests file content which has not been replicated yet. <a name="important-details"></a> <a name="speaking-ternfs"></a></p>
<h2>Important Details</h2>
<p>Now that we've laid down the high-level design of TernFS, we can talk about several key implementation details that make TernFS safer, more performant, and more flexible.</p>
<h3>Talking to TernFS</h3>
<h4>Speaking TernFS' language</h4>
<p>The most direct way to talk to TernFS is by using its own API. All TernFS messages are defined using a custom serialization format we call <em>bincode</em>. We chose to develop a custom serialization format since we needed it to work within the confines of the <a href="#posix-shaped">Linux kernel</a> and to be easily chopped into UDP packets.</p>
<p>We intentionally kept the TernFS API stateless, in the sense that each request executes without regard to previous requests made by the same client. This is in contrast to protocols like NFS, whereby each connection is very stateful, holding resources such as open files, locks, and so on.</p>
<p>A stateless API dramatically simplifies the state machines that make up the TernFS core services, therefore simplifying their testing. It also forces each request to be idempotent, or in any case have clear retry semantics, since they might have to be replayed, which facilitates testing further.</p>
<p>It also allows the metadata shards and CDC API to be based on UDP rather than TCP, which makes the server and clients (especially the kernel module) simpler, due to doing away with the need for keeping TCP connections. The block service API is TCP based, since it is used to stream large amounts of contiguous data, and any UDP implementation would have to re-implement a reliable stream protocol. The registry API is also TCP-based, given that it is rarely used by clients, and occasionally needs to return large amounts of data.</p>
<p>While the TernFS API is simple out-of-the-box, we provide a permissively licensed Go library implementing common tasks that clients might want to perform, such as caching directory policies and retrying requests. This library is used to implement many TernFS processes that are not part of the core TernFS services, such as <a href="#scrubbing">scrubbing</a>, <a href="#snapshots">garbage collection</a>, <a href="#migrations">migrations</a>, and the <a href="#web-ui">web UI</a>.</p>

<h4>Making TernFS POSIX-shaped</h4>
<p>While the Go library is used for most ancillary tasks, some with high performance requirements, the main way to access TernFS at XTX is through its Linux kernel module.</p>
<p>This is because, when migrating our machine learning workflows to TernFS, we needed to support a vast codebase working with files directly. This not only meant that we needed to expose TernFS as a normal filesystem, but also that said normal filesystem API needed to be robust and performant enough for our machine learning needs.<sup><a href="#f10">[10]</a></sup></p>

<p>For this reason, we opted to work with Linux directly, rather than using FUSE. Working directly with the Linux kernel not only gave us the confidence that we could achieve our performance requirements but also allowed us to bend the POSIX API to our needs, something that would have been more difficult if we had used FUSE.<sup><a href="#f11">[11]</a></sup></p>

<p>The main obstacle when exposing TernFS as a 'normal' filesystem is that TernFS files are immutable. More specifically, TernFS files are fully written before being 'linked' into the filesystem as a directory entry. This is intentional: it lets us cleanly separate the API for 'under construction' files and 'completed files', and it means that half-written files are not visible.</p>
<p>However this design is essentially incompatible with POSIX, which endows the user with near-absolute freedom when it comes to manipulating a file. Therefore, the TernFS kernel module is <em>not</em> POSIX-compliant, but rather exposes enough POSIX to allow many programs to work without modifications, but not all.</p>
<p>In practice this means that programs which write files left-to-right and never modify the files' contents will work out-of-the-box. While this might seem very restrictive, we found that a surprising number of programs worked just fine.<sup><a href="#f12">[12]</a></sup> Programs that did not follow this pattern were modified to first write to a temporary file and then copy the finished file to TernFS.</p>

<p>While we feel that writing our own kernel module was the right approach, it proved to be the trickiest part of TernFS, and we would not have been able to implement it without <a href="#block-proofs">some important safety checks</a> in the TernFS core services.<sup><a href="#f13">[13]</a></sup></p>

<h4>S3 gateway</h4>
<p>Almost all the storage-related activity at XTX is due to our machine-learning efforts, and for those purposes the TernFS' kernel module has served us well. However, as TernFS proved itself there, we started to look into offering TernFS to the broader firm.</p>
<p>Doing so through the kernel module presented multiple challenges. For starters installing a custom kernel module on every machine that needed to reach TernFS is operationally cumbersome. Moreover, while all machine-learning happens in clusters housed in the same data centre as TernFS itself, we wanted to expose TernFS in a way that's more amenable to less local networks, for instance by removing the need for UDP. Finally, TernFS does not have any built-in support for permissions or authentication, which is a requirement in multi-tenant scenarios.</p>
<p>To solve all these problems, we implemented a gateway for TernFS, which exposes a TernFS subtree using the S3 API. The gateway is a simple Go process turning S3 calls into TernFS API calls. The S3 gateway is not currently open sourced since it is coupled to authentication services internal to XTX, but we have open sourced a minimal S3 gateway to serve as a starting point for third-party contributors to build their own.</p>
<p>We've also planned an NFS gateway to TernFS, but we haven't had a pressing enough need yet to complete it.</p>

<h4>The web UI and the JSON interface</h4>
<p>Finally, a view of TernFS is provided by its web UI. The web UI is a stateless Go program which exposes most of the state of TernFS in an easy-to-use interface. This state includes the full filesystem contents (both metadata and file contents), the status of each service including information about decommissioned block services, and so on.</p>
<p>Moreover, the web UI also exposes the <a href="#speaking-ternfs">direct TernFS API</a> in JSON form, which is very useful for small scripts and curl-style automation that does not warrant a full-blown Go program.</p>

<h3>Directory Policies</h3>
<p>To implement some of the functionality we'll describe below, TernFS adopts a system of per-directory policies.</p>
<p>Policies are used for all sorts of decisions, including:</p>
<ul>
<li><a href="#reed-solomon">How to redundantly store files.</a></li>
<li><a href="#drive-type-picking">On which type of drive to store files.</a></li>
<li><a href="#snapshots">How long to keep files around after deletion.</a></li>
</ul>
<p>Each of the topics above (and a few more we haven't mentioned) correspond to a certain policy <em>tag</em>. The body of the policies are stored in the metadata together with the other directory attributes.</p>
<p>Policies are inherited: if a directory does not contain a certain policy tag, it transitively inherits from the parent directory. TernFS clients store a cache of policies to allow for traversal-free policy lookup for most directories.</p>
<h3>Keeping blocks in check</h3>
<p>A filesystem is no good if it loses, leaks, corrupts, or otherwise messes up its data. TernFS deploys a host of measures to minimize the chance of anything going wrong. So far, these have worked: we've never lost data in our production deployment of TernFS. This section focuses on the measures in place to specifically safeguard files' blocks.</p>
<h4>Against bitrot, or CRC32-C</h4>
<p>The first and possibly most obvious measure consists of aggressively checksumming all TernFS' data. The metadata is automatically checksummed by RocksDB, and every block is stored in a format interleaving 4KiB pages with 4byte CRC32-C checksums.</p>
<p>CRC32-C was picked since it is a high-quality checksum and implemented on most modern silicon.<sup><a href="#f14">[14]</a></sup> It also exhibits some desirable properties when used together with <a href="#block-proofs">Reed-Solomon coding</a>.</p>

<p>4KiB was picked since it is the read boundary used by Linux filesystems and is fine-grained while still being large enough to render the storage overhead of the 4byte checksums negligible.</p>
<p>Interleaving the CRCs with the block contents does not add any safety, but it does improve operations in two important ways. First, it allows for safe partial reads: clients can demand only a few pages from a block which is many megabytes in size and still check the reads against its checksum. Second, it allows <a href="#scrubbing">scrubbing</a> files locally on the server which hosts the blocks, without communicating with other services at all.</p>

<h4>Storing files redundantly, or Reed-Solomon codes</h4>
<p>We've been talking about files being split into blocks, but we haven't really explained <em>how</em> files become blocks.</p>
<p>The first thing we do to a file is split it into <em>spans</em>. Spans are at most 100MiB and are present just to divide files into sections of a manageable size.</p>
<p>Then each span is divided into D <em>data blocks</em>, and P <em>parity blocks</em>. D and P are determined by the corresponding <a href="#directory-policies">directory policy</a> in which the file is created. When D is 1, the entire contents of the span become a single block, and that block is stored D+P times. This scheme is equivalent to a simple mirroring scheme and allows it to lose up to P blocks before losing file data.</p>
<p>While wasteful, mirroring the entire contents of the file can be useful for very hot files, since TernFS clients will pick a block at random to read from, thereby sharing the read load across many block services. And naturally files which we do not care much for can be stored with D = 1 and P = 0, without any redundancy.</p>
<p>That said, most files will not be stored using mirroring but rather using Reed-Solomon coding. Other resources can be consulted to understand the <a href="https://mazzo.li/posts/reed-solomon.html">high-level idea</a> and the <a href="https://www.corsix.org/content/reed-solomon-for-software-raid">low-level details</a> of Reed-Solomon coding, but the gist is it allows us to split a span into D equally sized blocks (some padding might be necessary), and then generate P blocks of equal size such that up to any P blocks can be lost while retaining the ability to reconstruct all the other blocks.</p>
<p>As mentioned, D and P are fully configurable, but at XTX we tend to use D = 10 and P = 4, which allows us to lose up to any four drives for any file.</p>

<h4>Drive type picking</h4>
<p>We now know how to split files into a bunch of blocks. The next question is: which drives to pick to store the blocks on. The first decision is which kind of drive to use. At XTX we separate drives into two broad categories for this purpose — flash and spinning disks.</p>
<p>When picking between these two, we want to balance two needs: minimizing the cost of hardware by utilizing hard disks if we can <sup><a href="#f15">[15]</a></sup>, and maximizing hard disk productivity by having them reading data most of the time, rather than seeking.</p>

<p>To achieve that, directory policies offer a way to tune how large each block will be, and to tune which drives will be picked based on block size. This allows us to configure TernFS so that larger files that can be read sequentially are stored on hard disks, while random-access or small files are stored on flash. <sup><a href="#f16">[16]</a></sup></p>

<p>Currently this system is not adaptive, but we found that in practice it's easy to carve out sections of the filesystem which are not read sequentially. We have a default configuration which assumes sequential reads and then uses hard disks down to roughly 2.5MB blocks, below which hard disks stop being productive enough and blocks start needing to be written to flash. <a name="block-service-picking"></a></p>
<h4>Block service picking</h4>
<p>OK, we now know what type of drive to select for our files, but we still have tens of thousands of individual drives to pick from. Picking the 'right' individual drive requires some sophistication.</p>
<p>The first thing to note is that drive failures or unavailability are often correlated. For instance, at XTX a single server handles 102 spinning disks. If the server is down, faulty, or needs to be decommissioned, it'll render its 102 disks temporarily or permanently unavailable.</p>
<p>It's therefore wise to spread a file's blocks across many servers. To achieve this, each TernFS block service (which generally corresponds to a single drive) has a <em>failure domain</em>. When picking block services in which to store the blocks for a given file, TernFS will make sure that each block is in a separate failure domain. In our TernFS deployment a failure domain corresponds to a server, but other users might wish to tie it to some other factor as appropriate.</p>
<p>TernFS also tries hard to avoid write bottlenecks by spreading the current write load across many disks. Moreover, since new drives can be added at any time, it tries to converge to a situation where each drive is roughly equally filled by assigning writing more to drives with more available space.</p>
<p>Mechanically this is achieved by having each shard periodically request a set of block services to use for writing from the registry. When handing out block services to shards, the registry selects block services according to several constraints:</p>
<ul>
<li>It never gives block services from the same failure domain to the same shard</li>
<li>It minimizes the variance in how many shards each block service is currently assigned to</li>
<li>It prioritizes block services which have more available space.</li>
</ul>
<p>Then when a client wants to write a new span, requiring D+P blocks, the shard simply selects D+P block services randomly amongst the ones it last received from the registry.</p>
<p>One concept currently absent from TernFS is what is often known as 'copyset replication'. When assigning disks to files at random (even with the caveat of failure domains) the probability of rendering at least one file unreadable quickly becomes a certainty as more and more drives fail:</p>
<p><img src="https://www.xtxmarkets.com/assets/tech/2025-ternfs-faileddisks.png" alt="Probability of data loss vs Failed disks" title="Probability of data loss vs Failed disks"></p>

<p>Copysets reduce the likelihood of data loss occurring by choosing blocks out of a limited number of sets of drives, as opposed to picking the drives randomly. This dramatically reduces the probability of data loss<sup><a href="#f17">[17]</a></sup>.  They are generally a good idea, but we haven't found them to be worthwhile, for a few reasons.</p>

<p>First, evacuating a 20TB drive takes just a few minutes, and in the presence of multiple failed drives the migrator process evacuates first the files which are present in multiple failed drives to get ahead of possible data loss. This means that for TernFS to lose data within a single data centre tens of drives would have to fail within a matter of seconds.</p>
<p>More importantly, our TernFS deployment is replicated across three data centres. This replication eliminates the chance of losing data due to 'independent' drive failures — thousands of drives would need to fail at once. Obviously, data centre wide events <em>can</em> cause a large proportion of the drives within it to fail, but having such an event in three data centres at once is exceedingly unlikely.</p>
<p>Finally, copysets are not without drawbacks or complications. Assigning drives at random is an optimal strategy when it comes to evacuating drives quickly, since the files with blocks in the drives to be evacuated will be evenly spread over the rest of the filesystem, and since we only ever need to replace the failed blocks given that we're not constrained by fitting the new set of blocks in predetermined copysets. This means that the evacuation procedure will not be bottlenecked by drive throughput, which is what enables evacuation to finish in a matter of minutes. Moreover, the algorithm to distribute drives to shards is significantly simpler and more flexible than if it needed to care about copysets.</p>
<p>However, users that wish to deploy TernFS within a single data centre might wish to implement some form of copyset replication. Such a change would be entirely contained to the registry and would not change any other component.</p>

<h4>Block Proofs</h4>
<p>We now have a solid scheme to store files redundantly (thanks to Reed-Solomon codes) and protect against bitrot (thanks to the checksums). However, said schemes are only as good as their implementation.</p>
<p>As previously mentioned, TernFS clients communicate their intention to write a file to metadata servers, the metadata servers select block services that the blocks should be written to, and the clients then write the blocks to block services independently of the metadata services. The same happens when a client wants to erase blocks: the client first communicates its intentions to delete the blocks to the right metadata shard and then performs the erasing itself.</p>
<p>This poses a challenge. While verifying the correctness of the core TernFS services is feasible, verifying all clients is not, but we'd still like to prevent buggy clients from breaking key invariants of the filesystem.</p>
<p>Buggy clients can wreak havoc in several ways:</p>
<ul>
<li>They can <em>leak data</em> by writing blocks to block services that are not referenced anywhere in the metadata.</li>
<li>They can <em>lose data</em> by erasing blocks which are still referenced in metadata.</li>
<li>They can <em>corrupt data</em> by telling the metadata services they'll write something and then writing something else.</li>
</ul>
<p>We address all these points by using what we call <em>block proofs</em>. To illustrate how block proofs work, it's helpful to go through the steps required to write new data to a file.</p>
<ol>
<li>When a client is creating a file, it'll do so by adding its <a href="#reed-solomon">file spans</a> one-by-one. For each span the client wants to add it sends an 'initiate span creation' request to the right metadata shard. This request contains both the overall checksum of the span, and the checksum of each block in it (including parity blocks).</li>
<li>The metadata shard checks the consistency of the checksum of the span and of its blocks, something it can do thanks to <a href="https://mazzo.li/posts/rs-crc.html">some desirable mathematical properties</a> of CRCs.</li>
<li>The shard picks block services for the blocks to be written in and returns this information to the client together with a signature for each 'block write' instruction.</li>
<li>The client forwards this signature to the block services, which will refuse to write the block without it. Crucially, the cryptographic signature ranges over a unique identity for the block (ensuring we only write the block we mean to write), together with its checksum, ensuring we don't write the wrong data.<sup><a href="#f18">[18]</a></sup></li>
<li>After committing the block to disk, the block service returns a 'block written' signature to the client.</li>
<li>Finally, the client forwards the block written signature back to the shard, which certifies that the span has been written only when it has received the signatures for all the blocks that make up the span. <sup><a href="#f19">[19]</a></sup></li>
</ol>

<p>Similarly, when a client wants to delete a span, it first asks the metadata shard to start doing so. The metadata shard marks the span as 'in deletion' and returns a bunch of 'block erase' signatures to the client. The client then forwards the signatures to the block services that hold the blocks, which delete the blocks, and return a 'block erased' signature. The clients forward these signatures back to the metadata shards, which can then forget about the span entirely.</p>

<p>We use AES to generate the signatures for simplicity but note that the goal here is not protecting ourselves from malicious clients — just buggy ones. The keys used for the signature are not kept secret, and CRC32-C is not a secure checksum. That said, we've found this scheme enormously valuable in the presence of <a href="#posix-shaped">complex clients</a>. We spent considerable efforts making the core services very simple so we could then take more implementation risks in the clients, with the knowledge that we would have a very low chance of corrupting the filesystem itself.</p>

<h4>Scrubbing</h4>
<p>Finally, if things go wrong, we need to notice. The most common failure mode for a drive is for it to fail entirely, in which case our internal hardware monitoring system will pick it up and migrate from it automatically. The more insidious (and still very common) case is a single sector failing in a drive, which will only be noticed when we try to read the block involving that sector.</p>
<p>This is acceptable for files which are read frequently, but some files might be very 'cold' but still very important.</p>
<p>Consider the case of raw market data taps which are immediately converted to some processed, lossy format. While we generally will use the file containing the processed data, it's paramount to store the raw market data forever so that if we ever want to include more information from the original market data, we can. So important cold files might go months or even years without anyone reading them, and in the meantime, we might find that enough blocks have been corrupted to render them unreadable.<sup><a href="#f20">[20]</a></sup></p>

<p>To make sure this does not happen, a process called the <em>scrubber</em> continuously reads every block that TernFS stores, and replaces blocks with bad sectors before they can cause too much damage.</p>

<h3>Snapshots and garbage collection</h3>
<p>We've talked at length about what TernFS does to try to prevent data loss due to hardware failure or bugs in clients. However, the most common type of data loss is due to human error — the <code>rm —rf / home/alice/notes.txt</code> scenario.</p>
<p>To protect against these scenarios, TernFS implements a lightweight snapshotting system. When files or directories are deleted, their contents aren't actually deleted. Instead, a weak reference to them is created. We call such weak references <em>snapshot</em> directory entries.</p>
<p>Snapshot entries are not be visible through the kernel module or the S3 gateway, but are visible through <a href="#speaking-ternfs">the direct API</a>, and at XTX we have developed internal tooling to easily recover deleted files through it.<sup><a href="#f21">[21]</a></sup> Deleted files are also visible through the TernFS web UI.</p>

<p>Given that 'normal' file operations do not delete files, but rather make them a snapshot, the task of freeing up space is delegated to an external Go process, the <em>garbage collector</em>. The garbage collector traverses the filesystem and removes expired snapshots, which involves deleting their blocks permanently. Snapshot expiry is predictably regulated by <a href="#directory-policies">directory policies</a>.</p>
<h3>Keeping TernFS healthy</h3>
<p>This last section covers how we (humans of XTX) notice problems in TernFS, and how TernFS self-heals when things go wrong — both key topics if we want to ensure no data loss and notice performance problems early.</p>
<h4>Performance metrics</h4>
<p>TernFS exposes a plethora of performance metrics through the HTTP <a href="https://docs.influxdata.com/influxdb/v2/reference/syntax/line-protocol/">InfluxDB line protocol</a>. While connecting TernFS to a service which ingests these metrics is optional, it is <em>highly</em> recommended for any production service.</p>
<p>Moreover, the kernel module exposes many performance metrics itself through DebugFS.</p>
<p>Both types of metrics, especially when used in tandem, have proved invaluable to resolve performance problems quickly.</p>
<h4>Logging and alerts</h4>
<p>TernFS services log their output to files in a simple line-based format. The internal logging API is extremely simple and includes support for syslog levels out-of-the-box. At XTX we run TernFS as normal systemd services and use journalctl to view logs.</p>
<p>As with metrics, the kernel module includes various logging facilities as well. The first type of logging is just through dmesg, but the kernel module also includes numerous tracepoints for low-overhead opt-in logging of many operations.</p>
<p>TernFS is also integrated with XTX's internal alerting system, called <em>XMon</em>, to page on call developers when things go wrong. XMon is not open source, but all the alerts are also rendered as error lines in logs. <sup><a href="#f22">[22]</a></sup> We plan to eventually move to having alerts feed off performance metrics, which would make them independent from XMon, although we don't have plans to do so in the short-term. <a name="migrations"></a></p>

<h4>Migrations</h4>
<p>Finally, there's the question of what to do when drives die — and they will die, frequently, when you have 50,000 of them. While drives dying is not surprising, we've been surprised at the variety of different drive failures. <sup><a href="#f23">[23]</a></sup> A malfunctioning drive might:</p>

<ul>
<li>Produce IO errors when reading specific files. This is probably due to a single bad sector.</li>
<li>Produce IO errors when reading or writing anything. This might happen because enough bad sectors have gone bad and the drive cannot remap them, or for a variety of other reasons.</li>
<li>Return wrong data. This is usually caught by the built-in error correction codes in the hard drives, but not always.</li>
<li>Lie about data being successfully persisted. This can manifest in a variety of ways: file size being wrong on open, file contents being partially zero'd out, and so on.</li>
<li>Disappear from the mount list, only to reappear when the machine is rebooted, but missing some data.</li>
</ul>
<p>When clients fail to read from a drive, they'll automatically fall back on other drives to reconstruct the missing data, which is extremely effective in hiding failures from the end-user. That said, something needs to be done about the bad drives, and <a href="#block-service-picking">done quickly to avoid permanent data loss</a>.</p>
<p>The TernFS registry allows marking drives as faulty. Faulty drives are then picked up by the <em>migrator</em>, a Go process which waits for bad drives and then stores all its blocks onto freshly picked block services.</p>
<p>TernFS also tries to mark drives as bad automatically using a simple heuristic based on the rate of IO errors the drive is experiencing. The number of drives automatically marked as faulty is throttled to avoid having this check go awry and mark the whole cluster as faulty, which would not be catastrophic but would still be messy to deal with.</p>
<p>Moreover, drives that are faulty in subtle ways might not be picked up by the heuristics, which means that occasionally a sysadmin will need to mark a drive as faulty manually, after which the migrator will evacuate them.</p>
<h2>Closing thoughts</h2>
<p>At XTX we feel strongly about utilizing our resources efficiently. When it comes to software, this means having software that gets close to some theoretical optimum when it comes to total cost of ownership. This culture was borne out by competing hard for technological excellence when doing on-exchange trading at first, and by our ever-growing hardware costs as our business has grown later.</p>
<p>Such idealized tools might not exist or be available yet, in which case we're happy to be the tool makers. TernFS is a perfect example of this and we're excited to open source this component of our business for the community.</p>
<p>Crucially, the cost of implementation of a new solution is often overblown compared to the cost of tying yourself to an ill-fitting, expensive third-party solution. Designing and implementing a solution serving exactly your needs allows for much greater simplicity. If the requirements do change, as often happens, changes can be implemented very quickly, again only catering to your needs.</p>
<p>That said, we believe that TernFS' set of trade-offs are widely shared across many organizations dealing with large-scale storage workloads, and we hope we'll contribute to <a href="https://xkcd.com/927/">at least slowing down the seemingly constant stream of new filesystems</a>.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fuck, you're still sad? (252 pts)]]></title>
            <link>https://bessstillman.substack.com/p/oh-fuck-youre-still-sad</link>
            <guid>45290021</guid>
            <pubDate>Thu, 18 Sep 2025 14:17:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bessstillman.substack.com/p/oh-fuck-youre-still-sad">https://bessstillman.substack.com/p/oh-fuck-youre-still-sad</a>, See on <a href="https://news.ycombinator.com/item?id=45290021">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><audio src="https://bessstillman.substack.com/api/v1/audio/upload/fd01a944-08be-4ddd-ae11-37c2087b24e6/src" preload="auto">Audio playback is not supported on your browser. Please upgrade.</audio></p><p><strong>My husband Jake has been dead for a year and I still don’t believe it. </strong><span>Not really. Not when I look for his marginalia in new books, or read an article about self-driving cars and text him a link, or when I see an interesting new Malaysian restaurant and have the urge to make us a reservation. Certainly not when I look at our daughter, Athena, who wears Jake’s face as her own and who, especially when she’s examining an object closely, looks out at the world through the same perceptive brown eyes.</span></p><p>I can still see the future I’d imagined for us as clearly as I recall the past. In doing so, time collapses into a single point: Now. It feels like Jake is here right now.</p><p>Apparently, that’s a disease.</p><p><span>The American Psychiatric Association describes “disordered grief,” also known as “prolonged grief,” as a loss that occurred at least one measly year ago for adults (for children there’s an even faster grief clock). The diagnosis is made when people experience three of the following symptoms</span><s>,</s><span> every day</span><s>,</s><span> in the month before the diagnosis is made:</span></p><p>Avoidance of reminders the person has died; intense emotional pain; or, alternately, emotional numbness; difficulty with reintegration; feeling that life is meaningless; intense loneliness; feeling as though part of oneself has died; a marked sense of disbelief.</p><p>Just three?</p><p>I imagine a makeshift consensus group clad in tweed in some back room at a Psychiatric conference deciding on the shelf life of grief over coffee and Costco muffins, like it’s yogurt that’s starting to curdle. Is there a sniff test for pain? How long, exactly, is too long?</p><p>It took almost six months just to stop expecting Jake to text asking me to pick him up from Sky Harbor Airport – Terminal 3 – apologizing for his flight’s long delay. And although I no longer wonder if he’s about to walk through the door, my brain hasn’t given up the fleeting but frequent thought that he might still pick up the phone if I call.</p><p><span>There’s a persistent, searching feeling, as if he’s just around a corner. Last week, I met a friend at Cartel Coffee after work and was confused when she, not Jake, sat down on the couch beside me. Why had Jake sent her when he and I usually meet here after I work a night shift? Then yesterday, while loading groceries into the trunk of my car, I glanced back at the store entrance and thought- Jesus, is he </span><em>still </em><span>in the produce section picking out the perfect zucchini?</span></p><p><strong>In neuroscience, a prediction error refers to the discrepancy</strong><span> between what an organism expects to happen and what actually occurs. The ability to make accurate predictions comes from repetition. When Jake laughed at my stupid jokes every time I told them; when he got irritated because I pushed too hard with the nib of his fine-tipped pen, but still lent it to me anytime I asked; when he reached out to squeeze my hand whenever I felt anxious, that wasn’t just love—that was how I built the mental model of my life. When Jake died, even though he could no longer laugh at my jokes, or lend me a pen, or hold my hans, my brain still expected him to.</span></p><p><span>Dismantling that mental model</span><s>—</s><span>resolving the prediction error–also requires repetition. For a year, I’ve been freshly reminded that the clacking sound from the other room isn’t Jake at his keyboard, but the refrigerator making ice; the bed is always empty when I sneak in after a late shift; the text alert on my phone isn’t Jake sending me a photo of the fancy heirloom bean soup he made for dinner, but a spam message. Sometimes, I’ll go ahead and dial Jake’s phone number in case the laws of entropy have changed, and he picks up (hey, you never know), but his phone only ever rings and then drops to voicemail. He never even recorded a message.</span></p><p>Repetition is the only way to create accurate predictions. Repetition is the only way to dismantle them. And in doing so, dismantle myself. Grief, then, is a terrible kind of learning.</p><p><span>But it seems I’m a slow learner, and, as Jake would confirm, a resistant one: Last night I logged into Jake’s gmail and forwarded myself one of the weekly letters he used to send me, chasing the way my heart reflexively jumps when I see his name in my inbox–</span><s> </s><span>even if it was me who put it there.</span></p><p>So what if I cling to disbelief. In those moments of brief delusion, I feel like myself again.</p><p><span>Which only sharpens the truth: Part of me died with Jake. That’s not a symptom. It’s anatomy. </span><a href="https://bessstillman.substack.com/p/the-year-i-didnt-survive" rel="">My brain isn’t the same, and neither is my body</a><em>. </em><span>A person missing an arm isn’t told it’s a sickness to believe they’re structurally altered. A phantom limb is still gone, even if its ghost causes pain. Death, too, is an amputation.</span></p><p>In as little as six weeks, Axolotls can regrow not just their limbs, but parts of their brain. Starfish create an entire body from a single arm. Zebrafish can regenerate their heart.</p><p>What human has ever regrown their heart? And in just one year?</p><p><strong>For a diagnosis of disordered grief to be made, symptoms</strong><span> not only have to be present a year after the death, but “significantly impact daily life and functioning.” I can’t imagine anything that “significantly impacts”</span><s> </s><span>life </span><em>more</em><span> than death, and not only for the dead guy. And yet since the night Jake died, I’ve been able to shower, drive, and do laundry. I’ve birthed a baby, nursed her, and kept her alive. I’ve returned to work in the hospital and—as far as I know—I haven’t killed anyone. I pay my bills. I brush my teeth.</span></p><p><span>I function. I appear to function very well. Maybe that means I </span><em>am </em><span>well.</span></p><p><span>That’s not to say that I haven’t suspected otherwise. In the first months after Jake died, when I wasn’t paralyzed by grief, I thought that meant something was wrong with me. Then, seven months after Jake’s death, when I suddenly </span><em>was</em><span> paralyzed by grief, I thought that meant something was wrong with me. Countless self-help books reassure me that there’s “no right way to grieve,” but it definitely feels like there’s a wrong way, and we’re quick to diagnose it.</span></p><p><strong>We medicalize grief because we fear it</strong><span>. A diagnosis–</span><s> </s><span>naming what ails us</span><s> </s><span>–</span><s> </s><span>means we can fix it. Every shift I work, I have patients who are disappointed when I don’t have a clear diagnosis for them—even if it means I’ve ruled out a life-threatening one. Ambiguity means sitting with uncertainty and waiting to see how pain evolves. In a world where we swipe midway through 30 second video reels like rats hitting a cocaine lever, who has the patience for that? If what ails us has a name, that means we understand it. If we understand it, we can cure it, if we cure it, we won’t suffer.</span></p><p>Grief resists naming. It shifts and adapts. It’s not the same for any two people, or for any two losses.</p><p>Before the psychiatrists come for me, I understand that the spirit behind the diagnosis isn’t to pathologize a normal human experience, but to pathologize too much of that experience. As if there could be too much being human: Too much sadness. Too much struggle. Too much love.</p><p>I’m too much. And people, I think, are afraid of me. I walk into a room not as Bess, but as a reminder that awful things can happen randomly to any of us. When people first found out about Jake’s tongue cancer, they often asked what his risk factors were: did he smoke heavily? Was it HPV-positive? Did he chew tobacco? They needed to reassure themselves that their own lack of similar risks made them safe. And yet, Jake had no risk factors, which made askers visibly uncomfortable. I remember the way their faces strained to find a plausible explanation that, at the very least, excluded them from the horrible randomness of an impersonal universe.</p><p>I watch people perform the same futile calculations when they find out that I was widowed two months before the birth of my daughter. But what could the risk factors have possibly been for such a fate? What could I have done or not done that made me more susceptible to marrying a man who was dead by his 40th birthday?</p><p>Maybe the problem isn’t that my grief needs to resolve faster, but that other people need it to; then they can still believe that, when their grief comes, it will pass swiftly.</p><p><strong>There’s no modern cultural framework for dealing with death</strong><span>. We hide it, sanitize it, convince ourselves we have the technology to outsmart it, as if the singularity already occurred and we aren’t all still headed for the same six-</span><s> </s><span>foot hole. Memento mori have been replaced by positivity culture. And death, once part of public life, is tucked behind hospital walls for ER docs like me to witness. </span></p><p><span>The Victorians had mourning dress that made their grief visible. Ancient Greek funerals proceeded through the streets with professional wailers in their wake</span><strong>. </strong><span>Grief, once collective, is now treated as if it’s contagious. It’s like glitter: grief gets everywhere, attaches to everything; just walking past it means you’ll find it stuck to your own body in odd places for months.</span></p><p>Is it any wonder, then, that I’ve walked for miles with my baby daughter in her stroller, away from the gaze of family and friends, to keep my grief off display? So I can weep until my throat is raw. So I can sweat and scream until I’m filthy with rage. I duck into the bathroom at work whenever I feel tears coming, splash cold water on my face, and, ten seconds later, walk out with a smile and a wave to whoever is in the hallway.</p><p><span>And while time has taught me to manage the public messiness of grief, if anything, that’s given it space to grow in private. It feels a little shameful, the way it surges like desire behind closed doors, the way I wonder if it’s leaking out around my edges. Secretly–or maybe not so secretly now-I’ve thought: It’s been a </span><em>year</em><span>. Shouldn’t I be better by now?</span></p><p><span>But of course there’s still pain. Of course there’s still anger, bitterness and sorrow. Of course, there’s still loneliness-</span><s> </s><span>Jake’s remains are in a box on my bookshelf beside his copy of </span><em>Lord of the Rings</em><span> while his side of the bed remains empty. Our daughter Athena’s small, sticky hands rest on my cheek, her body dense and warm in my arms while I feed her from my own breast, and yet I’m still starving to be touched. Jake made me promise that I’d eat, and I do, but I’m never full. There’s a constant, baseline, gnawing ache.</span></p><p><strong><span>Maybe I’m repelled by the concept of “disordered grief</span><s>,</s><span>”</span></strong><span> because I can’t conceive of ordered grief. Grief resists linearity. I began grieving Jake while he was still alive, as the cancer relentlessly ate away at both his body and our future together. I still love him although he’s been dead for a year. My pain is recursive: I relentlessly cycle from moments of contentment or joy to shock and sadness and yearning, until the feelings become familiar, but no less breathtaking.</span></p><p><span>Every day grief comes in a different order. Some days I wake up at 4am, seized by the need to hold Jake’s hand</span><s>,</s><span> and feel anger that all I have is a plaster model of it. Other days, grief waits till I’m performing a physical exam on a patient and their wet cough reminds me of the way I would awaken in the middle of the night to hear Jake choking on his own saliva. Time seems to fold in on itself: Sometimes, I close my eyes while I breastfeed my daughter and the cocktail of oxytocin and prolactin saturates my brain in a way that resurrects Jake with hallucinatory vividness. Suddenly, we’re 27 and running out of the cold Seattle rain into Belle’s Buns for coffee, and then Athena unlatches from my nipple and I’ve lost him again.</span></p><p><strong>Time diverged when Jake died. </strong><span>For the rest of the world, a year has passed  since his death, and yet, somehow, it seems like it’s only just happened for me. At first, I was hurt when the flurry of concern and well-wishing that permeated the first weeks after his death naturally receded. Promised visits  failed to materialize. Calls were skipped. People move on with their own lives that continue at normal speed, while a large part of me is still kneeling beside Jake’s corpse, my fingers pressed against his absent pulse.</span></p><p>I don’t know how to resolve that discrepancy. How is it possible to reintegrate into a world that doesn’t understand that mine stopped? It’s hard enough trying to speak to people who haven’t experienced a similar loss, and even more difficult trying to be understood by people living a year in the future. I hadn’t realized before that grief isn’t an illness so much as a physics problem. Catching up may be impossible. I can only move so quickly.</p><p><strong>Every cure is about timing.</strong><span> When a patient comes to the emergency room, I’m only as useful as my ability to react quickly. It only helps if I give epinephrine to the anaphylactic before their throat closes and they develop a hypoxic brain injury. I have 90 minutes to get a patient with a massive heart attack diagnosed, stabilized and into the cardiac cath lab for stenting before their heart is irreversibly damaged. In a patient with a massive pulmonary embolism, I may only have minutes to administer tPA. My job, really, isn’t just to figure out that death is coming, but how fast. Sometimes, that’s impossible, and I lose the race. Other times–and these are the most exciting saves–a patient is clinically dead and I snatch them back. Sometimes it takes a few seconds; in extremely rare instances, hours.</span></p><p>But at no time in over a decade of training did I learn how long it takes to bring a person back to life when they’re not the one who’s died.</p><p><span>A year is nothing.</span><strong> </strong><span>Jake will be dead forever. Then I will be too. In the meantime, I’m not going to wait to be cured of grief so I can return to life. This</span><em> is</em><span> life.</span><em> </em><span>If you’re a mortal who loves other mortals the APA’s list isn’t a warning of what you might feel if you don’t grieve right; it’s a list of what you will feel, again and again, during a lifetime of discovering what’s still worth living for.</span></p><p>Is that sickness? I don’t feel sick. I just still feel love.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Gl2F!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff21a7141-8401-4529-80f5-ccae1e64a8bb_1086x724.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Gl2F!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff21a7141-8401-4529-80f5-ccae1e64a8bb_1086x724.jpeg 424w, https://substackcdn.com/image/fetch/$s_!Gl2F!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff21a7141-8401-4529-80f5-ccae1e64a8bb_1086x724.jpeg 848w, https://substackcdn.com/image/fetch/$s_!Gl2F!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff21a7141-8401-4529-80f5-ccae1e64a8bb_1086x724.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!Gl2F!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff21a7141-8401-4529-80f5-ccae1e64a8bb_1086x724.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Gl2F!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff21a7141-8401-4529-80f5-ccae1e64a8bb_1086x724.jpeg" width="1086" height="724" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f21a7141-8401-4529-80f5-ccae1e64a8bb_1086x724.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:724,&quot;width&quot;:1086,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:163207,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://bessstillman.substack.com/i/173911439?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff21a7141-8401-4529-80f5-ccae1e64a8bb_1086x724.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Gl2F!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff21a7141-8401-4529-80f5-ccae1e64a8bb_1086x724.jpeg 424w, https://substackcdn.com/image/fetch/$s_!Gl2F!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff21a7141-8401-4529-80f5-ccae1e64a8bb_1086x724.jpeg 848w, https://substackcdn.com/image/fetch/$s_!Gl2F!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff21a7141-8401-4529-80f5-ccae1e64a8bb_1086x724.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!Gl2F!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff21a7141-8401-4529-80f5-ccae1e64a8bb_1086x724.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Geizhals Preisvergleich Donates USD 10k to the Perl and Raku Foundation (155 pts)]]></title>
            <link>https://www.perl.com/article/geizhals-donates-to-tprf/</link>
            <guid>45289834</guid>
            <pubDate>Thu, 18 Sep 2025 14:01:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.perl.com/article/geizhals-donates-to-tprf/">https://www.perl.com/article/geizhals-donates-to-tprf/</a>, See on <a href="https://news.ycombinator.com/item?id=45289834">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
              
              <p>Sep 18, 2025 by
              
              
                
                
                <a href="#author-bio-olaf-alders">Olaf Alders</a>
              
              </p>
               <img alt="" src="https://www.perl.com/images/geizhals-donates-to-tprf/geizhals_logo_official.svg">
                <p>Today The Perl and Raku Foundation is thrilled to announce a donation of USD
10,000 from <a href="https://geizhals.at/">Geizhals Preisvergleich</a>. This gift helps to
secure the future of The Perl 5 Core Maintenance Fund.</p>
<blockquote>
<p>Perl has been an integral part of our product price comparison platform
from the start of the company 25 years ago. Supporting the Perl 5 Core
Maintenance Fund means supporting both present and future of a
substantial pillar of Modern Open Source Computing, for us and other
current or prospective users.</p></blockquote>
<p>– Michael Kröll of Geizhals Preisvergleich</p>
<blockquote>
<p>“Geizhals is not only providing core funding for the Perl ecosystem, but also
supporting developers, actively contributing to European conferences, and
employing Perl coders. Their interest in the strategic maintenance and
development of Perl and CPAN is of great value to us all, and their
investment is very much appreciated.”</p></blockquote>
<p>– Stuart J Mackintosh, President of The Perl and Raku Foundation</p>
<p>But who exactly is Geizhals, and why does their support matter so much to the
Perl community?</p>
<p>Geizhals Preisvergleich began in July of 1997 as a hobby project—and yes,
“Geizhals” literally translates to “skinflint” in English (they even operate
<a href="https://skinflint.co.uk/">skinflint.co.uk</a> for UK users!). From those humble
beginnings, they’ve leveraged the power of Perl to scale up to serving <a href="https://unternehmen.geizhals.at/">4.3
million monthly users</a>. With Perl being a key
part of their infrastructure, they have generously decided to support the Perl
5 Core Maintenance Fund.</p>
<p>While many of us know about the Core Maintenance Fund, the specific problems it
addresses often remain invisible to users. I reached out to the maintainers
whose work is supported by this fund. This is what core maintainer Tony Cook
had to say:</p>
<blockquote>
<p>My work tends to be little things, I review other people’s work which I think
improves quality and velocity, and fix more minor issues, some examples would
be:</p>
<ul>
<li>
<p>a fix to signal handling where perl could crash where an external library
created threads (<a href="https://github.com/perl/perl5/issues/22487">#22487</a>)</p>
</li>
<li>
<p>fix a segmentation fault in smartmatch against a sub if the sub exited via a
loop exit op (such as last)
(<a href="https://github.com/perl/perl5/issues/16608">#16608</a>)</p>
</li>
<li>
<p>fixed a bug where a regexp warning could leak memory.</p>
</li>
<li>
<p>prevent a confusing undefined warning message when accessing a sub
parameter that was placeholder for a hash element indexed by an
undef key (<a href="https://github.com/perl/perl5/issues/22423">#22423</a>)</p>
</li>
</ul></blockquote>
<p>What Tony has highlighted are the kinds of bug fixes which collectively help to
ensure that Perl remains stable, secure and reliable for the many organisations
and individuals who depend on it.</p>
<p>With organizations like Geizhals Preisvergleich funding the work which Tony and
others put into maintaining the Perl 5 core, we can work together to ensure that
the Perl core continues to receive the maintenance which it deserves, for many
years to come. Whether you’re a startup using Perl for rapid prototyping or an
enterprise running mission-critical systems, your support helps ensure Perl
remains reliable for everyone. Please join us on this journey.</p>
<p>For more information on how to become a sponsor, please contact:
<a href="mailto:olaf@perlfoundation.org">olaf@perlfoundation.org</a></p>

              </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Flipper Zero Geiger Counter (140 pts)]]></title>
            <link>https://kasiin.top/blog/2025-08-04-flipper_zero_geiger_counter_module/</link>
            <guid>45289453</guid>
            <pubDate>Thu, 18 Sep 2025 13:28:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kasiin.top/blog/2025-08-04-flipper_zero_geiger_counter_module/">https://kasiin.top/blog/2025-08-04-flipper_zero_geiger_counter_module/</a>, See on <a href="https://news.ycombinator.com/item?id=45289453">Hacker News</a></p>
Couldn't get https://kasiin.top/blog/2025-08-04-flipper_zero_geiger_counter_module/: Error: read ECONNRESET]]></description>
        </item>
        <item>
            <title><![CDATA[The quality of AI-assisted software depends on unit of work management (104 pts)]]></title>
            <link>https://blog.nilenso.com/blog/2025/09/15/ai-unit-of-work/</link>
            <guid>45289168</guid>
            <pubDate>Thu, 18 Sep 2025 13:06:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.nilenso.com/blog/2025/09/15/ai-unit-of-work/">https://blog.nilenso.com/blog/2025/09/15/ai-unit-of-work/</a>, See on <a href="https://news.ycombinator.com/item?id=45289168">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The craft of AI-assisted software creation is substantially about correctly managing units of work.</p>

<p>When I was new to this emerging craft of AI-assisted coding, I was getting lousy results, despite the models being rather intelligent. Turns out the major bottleneck is not intelligence, but rather providing the correct context.</p>

<p>Andrej Karpathy, <a href="https://youtube.com/clip/Ugkx7m0MVzHTnKXdoDjlqei60zlK4DWCXWr2?si=kIwnm0xQXdSKMQCC">while referencing</a> my <a href="https://blog.nilenso.com/blog/2025/05/29/ai-assisted-coding/">earlier article on this topic</a>, described the work of AI-assisted engineering as “putting AI on a tight leash”. What does a tight leash look like for a process where AI agents are operating on your code more independently than ever? He dropped a hint: work on small chunks of a single concrete thing.</p>

<h2 id="the-right-sized-unit-of-work-respects-the-context">The right sized unit of work respects the context</h2>

<p>I like the term <a href="https://simonwillison.net/2023/Jan/23/riley-goodside/">context engineering</a>, because it has opened up the vocabulary to better describe why managing units of work is perhaps the most important technique to get better results out of AI tools. It centers our discussion around the “canvas” against which our AI is generating code.</p>

<p>I like <a href="https://blog.nilenso.com/blog/2025/09/15/ai-unit-of-work/docs.anthropic.com/en/docs/build-with-claude/context-windows">Anthropic’s visualisation</a> from their docs:</p>

<p><img src="https://blog.nilenso.com/images/blog/context-window-thinking-tools.jpg" alt="Anthropic's visualisation of a context window filling up for each turn until it exceeds the window limit"></p>

<p>The generated output of the LLM is a sample of the next token probability. Every time we generate a token, what has already been generated in the previous iteration is appended to the context window. What this context window looks like has a huge influence on the quality of your generated output.</p>

<p><a href="https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html">Drew Breunig wrote an excellent article</a> about all kinds of things that can go wrong with your context and proposed various techniques to fix them.</p>

<p>The best AI-assisted craftsmen are often thinking about the design and arrangement of their context to get the AI to one-shot a solution. This is tricky and effortful, contrary to what the AI coding hype suggests.</p>

<p>If you don’t provide the necessary information in the context to do a good job, your AI will hallucinate or generate code that is not congruent with the practices of your codebase. It is especially brittle at integration points of your software system.</p>

<p>On the other hand, if you fill up the context with too much information, and <a href="https://research.trychroma.com/context-rot">the quality of your output degrades</a>, because of a lack of focused attention.</p>

<p>Breaking down your task into “right-sized” units of work, which describe just the right amount of detail is perhaps the most powerful lever to improve your context window, and thus the correctness and quality of the generated code.</p>

<h2 id="the-right-sized-unit-of-work-controls-the-propagation-of-errors">The right sized unit of work controls the propagation of errors</h2>

<p>Time for some napkin maths.</p>

<p>Let’s say your AI agent has a 5% chance of making a mistake. I’m not just referring to hallucinations—it could be a subtle mistake because it forgot to look up some documentation or you missed a detail in your specification.</p>

<p>In an agentic multi-turn workflow, which is what all coding workflows are converging to, this error compounds. If your task takes 10 turns to implement, you will have a (1 – 0.95)<sup>10</sup>&nbsp;=&nbsp;59.9% chance of success. Not very high.</p>

<p><a href="https://utkarshkanwat.com/writing/betting-against-agents">Utkarsh Kanwat in his blog post</a> has made the same argument. His conclusion was that any AI agent would need some kind of pause-and-verify gating mechanism at each step for a long-horizon task.</p>

<table>
  <thead>
    <tr>
      <th rowspan="2">Per-action<br>error rate</th>
      <th colspan="4">Overall Success Rate</th>
    </tr>
    <tr>
      <th>5 turns</th>
      <th>10 turns</th>
      <th>20 turns</th>
      <th>50 turns</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>0.1%</td><td>99.5%</td><td>99.0%</td><td>98.0%</td><td>95.1%</td></tr>
    <tr><td>1%</td><td>95.1%</td><td>90.4%</td><td>81.8%</td><td>60.5%</td></tr>
    <tr><td>5%</td><td>77.4%</td><td>59.9%</td><td>35.8%</td><td>7.7%</td></tr>
    <tr><td>10%</td><td>59.0%</td><td>34.9%</td><td>12.2%</td><td>0.5%</td></tr>
    <tr><td>20%</td><td>32.8%</td><td>10.7%</td><td>1.2%</td><td>0.0%</td></tr>
  </tbody>
</table>

<p>What does the state of the art for multi-turn error rates look like? METR recently published a popular chart <a href="https://metr.org/blog/2025-07-14-how-does-time-horizon-vary-across-domains/">describing how AI models are getting better at long-horizon tasks</a>. Currently GPT-5 is at the top of the leaderboard, where it can perform ~2-hour long tasks at around a 70% success rate. Working backwards (let’s say a 2 hour task is 50+ turns) this would amount to a sub-1% error rate per action.</p>

<p><img src="https://blog.nilenso.com/images/blog/metr.png" alt="Chart showing results of METR's chart showing task horizons increasing over time"></p>

<p>Doesn’t a &lt;1% error rate per action seem suspicious to you? As a regular user of agentic coding tools (my current one is Codex CLI), I’ll eat my shoe if GPT-5 starts nailing my tasks 99.9% of the time.</p>

<p>My intuition derived from experience tells me that even the best AI right now isn’t even 95% likely to be correct. So where is the difference coming from? It needs a closer look at the actual paper:</p>

<blockquote>
  <p>Our tasks typically use environments that do not significantly change unless directly acted upon by the agent. In contrast, real tasks often occur in the context of a changing environment.</p>

  <p>[…]</p>

  <p>Similarly, very few of our tasks are punishing of single mistakes. This is in part to reduce the expected cost of collecting human baselines.</p>
</blockquote>

<p>This is not at all like the tasks I am doing.</p>

<p>METR acknowledges the messiness of the real world. They have come up with a “messiness rating” for their tasks, and the “mean messiness” of their tasks is 3.2/16.</p>

<p>By METR’s definitions, the kind of software engineering work that I’m mostly exposed to would score at least around 7-8, given that software engineering projects are path-dependent, dynamic and without clear counterfactuals. I have worked on problems that get to around 13/16 levels of messiness.</p>

<blockquote>
  <p>An increase in task messiness by 1 point reduces mean success rates by roughly 8.1%</p>
</blockquote>

<p>Extrapolating from METR’s measured effect of messiness, GPT-5 would go from 70% to around 40% success rate for 2-hour tasks. This maps to my experienced reality.</p>

<p>I am not certain that pure intelligence can solve for messiness. Robustness to environmental chaos and the fuzzy nature of reality is fundamentally about managing context well. Until we find the magic sauce that solves this, it is clear that we need a workflow that can break down our problem into units of work, with verifiable checkpoints to manage the compounding of errors.</p>

<p>These verifiable checkpoints need to be <em>legible to humans</em>.</p>

<p><img src="https://blog.nilenso.com/images/blog/unit-of-work-management.jpg" alt="A diagram of boxes that represent units of work, with circles that represent checkpoints where users can verify outcomes and make corrections"></p>

<h2 id="so-what-is-the-right-sized-unit-of-work">So, what is the “right sized” unit of work?</h2>

<p>The right sized unit of work needs to be small and describe the desired outcome concisely.</p>

<p>The desired outcome on completion of a unit of work needs to be human-legible. I argue that it needs to provide legible <em>business value</em>. Ultimately, the users of software are going to be humans (or systems that model human constructs). Therefore, an elegant way to break down a project is to model it as small units of work that provide legible business value at each checkpoint. This will serve the purpose of respecting the context window of the LLM and help manage the propagation of errors.</p>

<p>Software engineers have already defined a unit of work that provides business value and serve as the placeholder for all the context and negotiation of scope—User Stories. I think they are a good starting point to help us break down a large problem into smaller problems that an LLM can one-shot, while providing a concrete result. They center <em>user outcomes</em>, which unlike “tasks”, are robust to the messy dynamic environment of software development.</p>

<p>Deliverable business value is also what all stakeholders can understand and work with. Software is not built in a vacuum by developers—it needs the coordination of teams, product owners, business people and users. The fact that AI agents work in their own context environment separate from the other stakeholders hurts effectiveness and transfer of its benefits. I think this is an important gap that needs to be bridged.</p>

<table>
  <thead>
    <tr>
      <th>&nbsp;</th>
      <th>unit size</th>
      <th>outcome of completion</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>TODO item</td>
      <td>small</td>
      <td>incremental technical value</td>
    </tr>
    <tr>
      <td>“Plan Mode”</td>
      <td>large</td>
      <td>technical value</td>
    </tr>
    <tr>
      <td>Amazon Kiro Spec</td>
      <td>small</td>
      <td>technical value</td>
    </tr>
    <tr>
      <td>User Story</td>
      <td>small</td>
      <td>business value</td>
    </tr>
  </tbody>
</table>

<p>Most AI agents today have well-functioning “planning” modes. These are good at keeping the agent on rails, but they mostly provide technical value, and not necessarily a legible business outcome. I believe planning is complementary to our idea of breaking down a project into small units of business value. My proposed unit of work can be planned with existing planning tools. And I believe this is superior to planning over a large unit of work due to the context rot issues described earlier.</p>

<p>Of course, plain old User Stories as described in the Agile canon is not sufficient. It needs to be accompanied by “something more” that can nudge the agents to gather the right context that serves the business value outcome of the stories. What that “something more” could look like is something we hope to answer in the coming months.</p>

<h2 id="the-storymachine-experiment">The StoryMachine experiment</h2>

<p>To test whether user stories with “something more” can indeed serve as optimal units of work that that have the properties I described above, we are running an experiment called <a href="https://github.com/nilenso/storymachine">StoryMachine</a>. Currently StoryMachine does not do much—it reads your PRD and Tech Specs and produces story cards. It is still early days. But we will set up an evaluation system that will help us iterate to a unit of work description that helps us build useful software effortlessly. I hope to share updates on what we find in the coming months.</p>

<p>I want the craft of AI-assisted development to be less effortful and less like a slot-machine. And our best lever to get there is managing the unit of work.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[KDE is now my favorite desktop (511 pts)]]></title>
            <link>https://kokada.dev/blog/kde-is-now-my-favorite-desktop/</link>
            <guid>45288690</guid>
            <pubDate>Thu, 18 Sep 2025 12:17:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kokada.dev/blog/kde-is-now-my-favorite-desktop/">https://kokada.dev/blog/kde-is-now-my-favorite-desktop/</a>, See on <a href="https://news.ycombinator.com/item?id=45288690">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
            <p>From <a href="https://kokada.dev/blog/from-gaming-rig-to-personal-computer-my-journey-with-nixos-and-jovian">my last blog
post</a>,
I am now using KDE as the desktop environment for my gaming rig. The reason is
because I want a reasonably easy to use Linux desktop for when my wife needs to
use the PC for something other than gaming, and this was the reason why my
"traditional" <a href="https://swaywm.org/">Sway</a> setup was a no-go.</p>
<p>But, after using KDE for a while I am starting to really appreciate how good it
is. And no, this is not compared to other Linux desktops, but also with both
Windows and macOS (that I need to use often, especially the later since my job
gave me a MacBook Pro).</p>
<p>To start, KDE is surprisingly feature-complete. For example, the network applet
gives lots of information that in other operational systems are either not
available or difficult to access. It is easy to see in the screenshot below:</p>
<p><a href="https://github.com/thiagokokada/blog/raw/main/posts/2025-09-17/Screenshot_20250917_191837.png"><img src="https://github.com/thiagokokada/blog/raw/main/posts/2025-09-17/Screenshot_20250917_191837.png" alt="Wi-Fi information available in the network applet from
KDE"></a></p>
<p>You can see things like channel, signal strength, frequency, MAC address, BSSID
address (so the MAC address of the router). It even includes a handy button to
share the Wi-Fi information via QR code, so you can easily setup a new mobile
device like Android.</p>
<p>By the way, the crop and blur from that screenshot above? I made everything
using the integrated screenshot tool. I didn't need to open an external
application even once. It is also really smart, I need to redo this screenshot
a few times and it kept the cropping to the exact area I was taking the
screenshot before.</p>
<p>Another example, I wanted <a href="https://steamcommunity.com/">Steam</a> to start
automatically with the system, but it has the bad habit of putting its main
window at the top. Really annoying since it sometimes ended up stealing up the
focus. However KDE has this "Window Rules" feature inside "Window Management"
settings where you can pretty much control whatever you want about application
windows. Really useful tool.</p>
<p>KDE also has lots of really well integrated tools. For example, I am using some
Flatpak applications and I can easily configure the permissions via System
Settings. Or if I want hardware information like
<a href="https://en.wikipedia.org/wiki/Self-Monitoring,_Analysis_and_Reporting_Technology">SMART</a>
status, I can just open Info Center. I can prevent the screen and computer to
sleep at the click of a button (something that in both Windows and macOS I need
to install a separate program). The list goes on, I keep getting surprised how
many things that I used to need a third-party program that KDE just has
available by default.</p>
<p><a href="https://github.com/thiagokokada/blog/raw/main/posts/2025-09-17/Screenshot_20250917_192302.png"><img src="https://github.com/thiagokokada/blog/raw/main/posts/2025-09-17/Screenshot_20250917_192302.png" alt="Flatpak permission
management"></a></p>
<p>But not only KDE is fully featured, it is also fast. Now to be clear, this is
a completely subjective analysis but I find KDE faster than Windows 11 in the
same hardware, especially for things integrated in the system itself. For
example, while opening Windows settings it can take a few seconds after a cold
boot, the KDE's System Settings is pretty much instantaneous. Even compared
with macOS in my MacBook Pro M2 Pro (that is of course comparing Apples and
Bananas), KDE just feels snappier. I actually can't find much difference
between KDE and my Sway setup to be honest, except maybe for the heavy use of
animations (that can be disabled, but I ended up liking it after a while).</p>
<p>I will not say KDE is perfect though. At the first launch I got one issue where
it started without the task bar because I connected this PC to both my monitor
and TV, but the TV is used exclusively for gaming. However, KDE considered my
TV the primary desktop and put the task bar only in that monitor, and even
disabling the TV didn't add the task bar to my monitor. Easily fixed by
manually adding a task bar, but an annoying problem (especially when you're not
used to the desktop). There were also a few other minor issues that I don't
remember right now.</p>
<p>After using KDE for about a week I can say that this is the first time that I
really enjoy a desktop environment on Linux, after all those years. Props for
the KDE developers for making the experience so good.</p>
<p><a href="https://github.com/thiagokokada/blog/raw/main/posts/2025-09-17/Screenshot_20250917_195215.png"><img src="https://github.com/thiagokokada/blog/raw/main/posts/2025-09-17/Screenshot_20250917_195215.png" alt="About this System"></a></p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You Had No Taste Before AI (186 pts)]]></title>
            <link>https://matthewsanabria.dev/posts/you-had-no-taste-before-ai/</link>
            <guid>45288551</guid>
            <pubDate>Thu, 18 Sep 2025 12:00:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matthewsanabria.dev/posts/you-had-no-taste-before-ai/">https://matthewsanabria.dev/posts/you-had-no-taste-before-ai/</a>, See on <a href="https://news.ycombinator.com/item?id=45288551">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>There’s been an influx of people telling others to develop taste to use AI.
Designers. Marketers. Developers. All of them touting the same message. It’s
ironic, though. These are the same people who never questioned why their designs
all look identical, never iterated beyond the first draft, and never asked if
their work actually solved the problem at hand.</p>
<p>They’re not alone. The loudest voices preaching about taste and AI are often the
ones who never demonstrated taste before AI.</p>
<h2 id="what-is-taste">What is Taste? <span><a href="#what-is-taste" aria-label="Anchor">#</a></span></h2><p>The technology industry has a tendency to use words that mean multiple things
without describing which definition they are referring to. When I read about
taste and AI I usually see people referring to the following definition.</p>
<blockquote>
<p>Critical judgment, discernment, or appreciation of aesthetic quality.</p></blockquote>
<p>In the context of AI, this definition manifests itself in several ways.</p>
<p><strong>Contextual Appropriateness</strong>: Knowing when AI-generated content fits the
situation and when it doesn’t. Put another way, knowing when a human touch is
needed (e.g., a message to a loved one).</p>
<p><strong>Quality Recognition</strong>: Being able to distinguish between useful AI-generated
content and slop. This requires domain knowledge to truly discern aesthetic
quality rather than just functional quality.</p>
<p><strong>Iterative Refinement</strong>: Understanding that AI is a starting point that
requires further iteration. This point is most similar to how culinary taste is
applied to refine a dish by iterating on the recipe and presentation.</p>
<p><strong>Ethical Boundaries</strong>: Recognizing when AI crosses the lines of authenticity,
legality, and respect. Basically, don’t use AI to do bad things.</p>
<p>None of these skills are new. These are the same skills we should have been
applying to our work all along. Why are we asking about taste and AI now when
we should have been applying taste the whole time? Perhaps people advocating for
taste are telling on themselves.</p>
<h2 id="being-tasteless">Being Tasteless <span><a href="#being-tasteless" aria-label="Anchor">#</a></span></h2><p>Some people have no taste. In the best case that may be due to lack of
experience but in the worst case it may be due to ignorance. I’m noticing that
many people worried about tasteless AI-generated content are often guilty of
producing tasteless content themselves, usually manifesting as the following.</p>
<ul>
<li>
<p>Copying and pasting code without understanding it.</p>
</li>
<li>
<p>Sending resumes and emails that aren’t proofread and edited.</p>
</li>
<li>
<p>Asking others to review code without giving it a self review.</p>
</li>
<li>
<p>Noticing a quality issue and failing to document or fix it.</p>
</li>
<li>
<p>Designing websites that look exactly like every other company’s website.</p>
</li>
<li>
<p>Regurgitating content from the trending influencer of the week.</p>
</li>
</ul>
<p>Where’s the taste here? Where’s the critical judgment, discernment, or
appreciation of aesthetic quality that separates mediocrity from excellence?</p>
<p>It’s not there because most people haven’t developed their taste yet. AI didn’t
create this tasteless problem. People did. Now that everyone can generate
content at the speed of thought we’re noticing that not all content is actually
good. To play on a popular quote from Ratatouille, anyone can cook, but not
everyone is a chef. Don’t complain about mediocre work when you’re producing
mediocre work yourself.</p>
<h2 id="spectrum-of-taste">Spectrum of Taste <span><a href="#spectrum-of-taste" aria-label="Anchor">#</a></span></h2><p>What about the nature of taste itself? Should people focus on developing depth
of taste in specific domains or breadth of taste across many domains? My short
answer is a bit of both, if possible.</p>
<p>Depth of taste means becoming an expert within a particular domain. We’ve all
met such experts and even asked them for help on tricky, bespoke topics within
their domain. A person with depth of taste can recognize when AI-generated
content is refined and of high quality versus merely functional. This kind of
taste comes from years of experience in a specific role coupled with deep domain
knowledge.</p>
<p>Breadth of taste means becoming knowledgeable across multiple domains and
understanding how those domains interface with one another. A person with
breadth of taste can recognize when AI-generated content is contextually
appropriate, authentic, and of enough quality to use for their needs. This
kind of taste comes from years of experience across multiple roles coupled with
moderate domain knowledge.</p>
<p>Breadth of taste is more valuable with AI. When using AI, you’re constantly
switching between domains: a software engineer writing documentation, a marketer
creating designs. Breadth lets you maintain quality across these contexts while
recognizing when you need domain expertise. You iterate faster because you have
opinions about what “good enough” looks like across multiple domains.</p>
<p>The people I see being most effective with AI developed a breadth of taste that
they use to determine what good AI-generated content looks like, regardless
of domain. They can recognize when something feels off, even if they can’t
articulate exactly why. They understand their own limitations and know when to
seek expertise in a specific domain. That’s not to say those with depth of taste
can’t be successful with AI, but I see those people reluctant to use AI because
they are more knowledgeable than AI in a particular domain.</p>
<h2 id="it-tastes-bitter">It Tastes Bitter <span><a href="#it-tastes-bitter" aria-label="Anchor">#</a></span></h2><p>If you’re reading this thinking you have to spend time developing your taste,
good! Perhaps I’ve left a bitter taste in your mouth. The good news is you’re
not alone. There are many people that need to hear this to better their
taste, myself included. The challenge here is recognizing that it’s not about
developing taste for AI but rather about developing taste, period. If you’ve had
poor taste before AI you’ll have poor taste with AI. If you’ve had good taste
before AI, you’ll be able to apply that taste with AI.</p>
<p>Instead of treating AI taste as some mystical new skill, focus on the
fundamentals that were always important. Here are some actionable ways to
develop your taste.</p>
<p><strong>Tomorrow</strong>: Pick one piece of work you’re proud of and one you’re not.
Write down specifically what makes them different. That’s taste in action.</p>
<p><strong>This week</strong>: Find three examples of excellence in a domain you work in. Study
them. What patterns emerge? What choices did the creators make?</p>
<p><strong>This month</strong>: Take something you’ve created with or without AI and iterate
on it a few times. Each iteration should have a specific improvement based on a
specific critique.</p>
<p><strong>Always</strong>: When someone preaches about AI taste, ask them to show you their
work from before AI. If they can’t demonstrate taste in their pre-AI work,
they’re not qualified to lecture you about it now.</p>
<p>The people succeeding with AI aren’t the ones who suddenly discovered taste.
They’re the ones who already had it and simply adapted their standards to a
new tool. Develop your taste with or without AI. The medium doesn’t matter, the
fundamentals do.</p>
<p>Stop waiting for AI to force you to develop taste. Start now.</p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia buys $5B in Intel stock in seismic deal (538 pts)]]></title>
            <link>https://www.tomshardware.com/pc-components/cpus/nvidia-and-intel-announce-jointly-developed-intel-x86-rtx-socs-for-pcs-with-nvidia-graphics-also-custom-nvidia-data-center-x86-processors-nvidia-buys-usd5-billion-in-intel-stock-in-seismic-deal</link>
            <guid>45288161</guid>
            <pubDate>Thu, 18 Sep 2025 11:04:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/pc-components/cpus/nvidia-and-intel-announce-jointly-developed-intel-x86-rtx-socs-for-pcs-with-nvidia-graphics-also-custom-nvidia-data-center-x86-processors-nvidia-buys-usd5-billion-in-intel-stock-in-seismic-deal">https://www.tomshardware.com/pc-components/cpus/nvidia-and-intel-announce-jointly-developed-intel-x86-rtx-socs-for-pcs-with-nvidia-graphics-also-custom-nvidia-data-center-x86-processors-nvidia-buys-usd5-billion-in-intel-stock-in-seismic-deal</a>, See on <a href="https://news.ycombinator.com/item?id=45288161">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/gBeVGpDwSKA49BwFrqRooH-1856-80.png.webp 1920w, https://cdn.mos.cms.futurecdn.net/gBeVGpDwSKA49BwFrqRooH-1200-80.png.webp 1200w, https://cdn.mos.cms.futurecdn.net/gBeVGpDwSKA49BwFrqRooH-1024-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/gBeVGpDwSKA49BwFrqRooH-970-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/gBeVGpDwSKA49BwFrqRooH-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/gBeVGpDwSKA49BwFrqRooH-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/gBeVGpDwSKA49BwFrqRooH-320-80.png.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/gBeVGpDwSKA49BwFrqRooH.png" alt="asdf" srcset="https://cdn.mos.cms.futurecdn.net/gBeVGpDwSKA49BwFrqRooH-1856-80.png 1920w, https://cdn.mos.cms.futurecdn.net/gBeVGpDwSKA49BwFrqRooH-1200-80.png 1200w, https://cdn.mos.cms.futurecdn.net/gBeVGpDwSKA49BwFrqRooH-1024-80.png 1024w, https://cdn.mos.cms.futurecdn.net/gBeVGpDwSKA49BwFrqRooH-970-80.png 970w, https://cdn.mos.cms.futurecdn.net/gBeVGpDwSKA49BwFrqRooH-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/gBeVGpDwSKA49BwFrqRooH-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/gBeVGpDwSKA49BwFrqRooH-320-80.png 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/gBeVGpDwSKA49BwFrqRooH.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/gBeVGpDwSKA49BwFrqRooH.png" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<figcaption>
<span>(Image credit: Nvidia)</span>
</figcaption>
</div>

<div id="article-body">
<p id="dd0fa8e5-b304-4f38-98c1-c6e171c2a53a">In a surprise announcement that finds two long-time rivals working together, Nvidia and Intel announced today that the companies will jointly develop multiple new generations of x86 products together — a seismic shift with profound implications for the entire world of technology. Before the news broke, Tom's Hardware spoke with Nvidia representatives to learn more details about the company’s plans.</p><p>The products include x86 Intel CPUs tightly fused with an Nvidia RTX graphics chiplet for the consumer gaming PC market, named the ‘Intel x86 RTX SOCs.’ Nvidia will also have Intel build custom x86 data center CPUs for its AI products for hyperscale and enterprise customers. Additionally, Nvidia will buy $5 billion in Intel common stock at $23.28 per share, representing a roughly 5% ownership stake in Intel. (Intel stock is now up 33% in premarket trading.)</p><p>Nvidia emphasized that the companies are committed to multi-generation roadmaps for the co-developed products, which represents a strong investment in the x86 ecosystem. But representatives tells us it also remains fully committed to other announced product roadmaps and architectures, including the company's Arm-based <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidias-project-digits-desktop-ai-supercomputer-fits-in-the-palm-of-your-hand-usd3-000-to-bring-1-pflops-of-performance-home" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/gpus/nvidias-project-digits-desktop-ai-supercomputer-fits-in-the-palm-of-your-hand-usd3-000-to-bring-1-pflops-of-performance-home">GB10 Grace Blackwell processors for workstations</a> and the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/nvidia-unveils-144-core-grace-cpu-superchip-claims-arm-chip-15x-faster-than-amds-epyc-rome" data-before-rewrite-localise="https://www.tomshardware.com/news/nvidia-unveils-144-core-grace-cpu-superchip-claims-arm-chip-15x-faster-than-amds-epyc-rome">Nvidia Grace</a> <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/nvidia-details-grace-hopper-cpu-superchip-design-144-cores-on-4n-tsmc-process" data-before-rewrite-localise="https://www.tomshardware.com/news/nvidia-details-grace-hopper-cpu-superchip-design-144-cores-on-4n-tsmc-process">CPUs for data centers</a>, as well as the next-gen <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/nvidias-rubin-gpu-and-vera-cpu-data-center-ai-platforms-begin-tape-out-both-chips-in-fab-and-on-track-for-2026" data-before-rewrite-localise="https://www.tomshardware.com/tech-industry/artificial-intelligence/nvidias-rubin-gpu-and-vera-cpu-data-center-ai-platforms-begin-tape-out-both-chips-in-fab-and-on-track-for-2026">Vera CPUs</a>. Nvidia says it also remains committed to products on its internal roadmaps that haven’t been publicly disclosed yet, indicating that the new roadmap with Intel will merely be additive to existing initiatives.</p><p>The chip giant hasn’t disclosed whether it will use Intel Foundry to produce any of these products yet. However, while Intel has used TSMC to manufacture some recent products, its goal is to bring production of most high-performance products back into its own foundries.</p><p>Some products never left. For instance, Intel’s existing <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/intel-launches-granite-rapids-xeon-6900p-series-with-120-cores-matches-amd-epycs-core-counts-for-the-first-time-since-2017" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/cpus/intel-launches-granite-rapids-xeon-6900p-series-with-120-cores-matches-amd-epycs-core-counts-for-the-first-time-since-2017">Granite Rapids</a> data center processors use the ‘Intel 3’ node, and the upcoming <a data-analytics-id="inline-link" href="https://www.tomshardware.com/desktops/servers/intel-reveals-288-core-xeon" data-before-rewrite-localise="https://www.tomshardware.com/desktops/servers/intel-reveals-288-core-xeon">Clearwater Forest Xeons</a> will use Intel’s own 18A process node for compute. This suggests that at least some of the Nvidia-custom x86 silicon, particularly for the data center, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/nvidia-ceo-intel-test-chip-results-for-next-gen-process-look-good" data-before-rewrite-localise="https://www.tomshardware.com/news/nvidia-ceo-intel-test-chip-results-for-next-gen-process-look-good">could be fabbed on Intel nodes</a>. Intel also uses TSMC to fabricate many of its client x86 processors, however, so we won’t know for sure until official announcements are made — particularly for the RTX GPU chiplet.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-6S7ZPUsULrjZhoioYnhg6Z"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div><p>While the two companies have engaged in heated competition in some market segments, Intel and Nvidia have partnered for decades, ensuring interoperability between their hardware and software for products spanning both the client and data center markets. And the PCIe interface has long been used to connect Intel CPUs and Nvidia GPUs. The new partnership will find tighter integration using the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/nvidia-announces-nvlink-fusion-to-allow-custom-cpus-and-ai-accelerators-to-work-with-its-products" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/cpus/nvidia-announces-nvlink-fusion-to-allow-custom-cpus-and-ai-accelerators-to-work-with-its-products">NVLink interface for CPU-to-GPU communication</a>, which affords up to 14 times more bandwidth along with lower latency than PCIe, thus granting the new x86 products access to the highest performance possible when paired with GPUs. Let’s dive into the details we’ve learned so far.</p><h2 id="intel-x86-rtx-socs-for-the-pc-gaming-market-3">Intel x86 RTX SOCs for the PC gaming market</h2><p id="ca731ff8-3e95-42f5-affb-2bcbd162c7fc">For the PC market, the Intel x86 RTX SoC chips will come with an x86 CPU chiplet tightly connected with an Nvidia RTX GPU chiplet via the NVLink interface. This type of processor will have both CPU and GPU units merged into one compact chip package that externally looks much like a standard CPU, rivaling AMD’s competing APU products.</p><p>This type of tight integration packs all the gaming prowess into one package without an external discrete GPU, providing power and footprint advantages. As such, these chips will be heavily focused on thin-and-light gaming laptops and small form-factor PCs, much like today’s APUs from AMD. However, it’s possible the new Nvidia/Intel chips could come in multiple flavors and permeate further into the Intel stack over time.</p><p>Intel has worked on a similar type of chip before with AMD; there is at least one significant technical difference between these initiatives, however. Intel launched its <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/intel-hades-canyon-nuc-vr,5536.html" data-before-rewrite-localise="https://www.tomshardware.com/reviews/intel-hades-canyon-nuc-vr,5536.html">Kaby Lake-G chip in 2017</a> with an Intel processor fused into the same package as an AMD Radeon GPU chiplet, much the same as the description of the new Nvidia/Intel chips. You can see an image of the Intel/AMD chip below.</p><div aria-hidden="false" data-swipeable="true" data-hydrate="true" id="slice-container-imageGallery-6S7ZPUsULrjZhoioYnhg6Z-r0GQqSmxKDyfaegDCSFcLIEvP0Ymx0AV"><figure data-bordeaux-image-check="false"><div><picture data-hydrate="true"><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-normal="https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm.jpg" data-original-mos="https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm.jpg" data-pin-nopin="true" data-slice-image="true"><source type="image/jpeg" srcset="https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-normal="https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm.jpg" data-original-mos="https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm.jpg" data-pin-nopin="true" data-slice-image="true"><img src="https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm.jpg" alt="sdf" srcset="https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-normal="https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm.jpg" data-original-mos="https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm.jpg" data-pin-nopin="true" data-slice-image="true"></picture></div><figcaption><span>An RTX GPU chiplet connected to an Intel CPU chiplet via the fast and efficient NVLink interface. </span></figcaption></figure></div><p id="06b33e49-7ca0-467a-933e-b68f8f0ddbe8">This SoC had a CPU at one end connected via a PCIe connection to the separate AMD GPU chiplet, which is flanked by a small, dedicated memory package. This separate memory package was only usable by the GPU. The Nvidia/Intel products will have an RTX GPU chiplet connected to the CPU chiplet via the faster and more efficient NVLink interface, and we’re told it will have uniform memory access (UMA), meaning both the CPU and GPU will be able to access the same pool of memory.</p><p>Intel notoriously <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/intel-discontinue-kaby-lake-g-amd-graphics,40577.html" data-before-rewrite-localise="https://www.tomshardware.com/news/intel-discontinue-kaby-lake-g-amd-graphics,40577.html">axed the Kaby Lake-G products in 2019</a>, and the existing systems were <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/intel-graphics-driver-update-hades-canyon-amd-12-month-delay" data-before-rewrite-localise="https://www.tomshardware.com/news/intel-graphics-driver-update-hades-canyon-amd-12-month-delay">left without proper driver support</a> for <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/windows-11-kaby-lake-g-drivers" data-before-rewrite-localise="https://www.tomshardware.com/news/windows-11-kaby-lake-g-drivers">quite some time</a>, in part because Intel was responsible for validating the drivers, and then finger-pointing ensued. We’re told that both Intel and Nvidia will be responsible for their respective drivers for the new models, with Nvidia naturally providing its own GPU drivers. However, Intel will build and sell the consumer processors.</p><p>We haven’t spoken with Intel yet, but the limited scope of this project means that Intel’s proprietary Xe graphics architecture will most assuredly live on as the primary integrated GPU (iGPU) for its mass-market products.</p><h2 id="nvidia-s-first-x86-data-center-cpus-3">Nvidia's first x86 data center CPUs</h2><p id="7fcd26a5-8509-41c5-a5da-0629cef0d258">Intel will fabricate custom x86 data center CPUs for Nvidia, which Nvidia will then sell as its own products to enterprise and data center customers. However, the entirety and extent of the modification are currently unknown. We do know that Nvidia will employ its NVLink interface, which tells us the chips could leverage Nvidia’s new <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/nvidia-announces-nvlink-fusion-to-allow-custom-cpus-and-ai-accelerators-to-work-with-its-products" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/cpus/nvidia-announces-nvlink-fusion-to-allow-custom-cpus-and-ai-accelerators-to-work-with-its-products">NVLink Fusion</a> tech that enables custom CPUs and accelerators to enable faster, more efficient communication with Nvidia’s GPUs than found with the PCIe interface.</p><figure data-bordeaux-image-check="" id="7ade6161-9180-406c-b223-19e2635b8553"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/MftMZVxs3dkte2VoNsxtMi-1200-80.png.webp 1200w, https://cdn.mos.cms.futurecdn.net/MftMZVxs3dkte2VoNsxtMi-1024-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/MftMZVxs3dkte2VoNsxtMi-970-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/MftMZVxs3dkte2VoNsxtMi-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/MftMZVxs3dkte2VoNsxtMi-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/MftMZVxs3dkte2VoNsxtMi-320-80.png.webp 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/MftMZVxs3dkte2VoNsxtMi.png" alt="NVLink Fusion" srcset="https://cdn.mos.cms.futurecdn.net/MftMZVxs3dkte2VoNsxtMi-1200-80.png 1200w, https://cdn.mos.cms.futurecdn.net/MftMZVxs3dkte2VoNsxtMi-1024-80.png 1024w, https://cdn.mos.cms.futurecdn.net/MftMZVxs3dkte2VoNsxtMi-970-80.png 970w, https://cdn.mos.cms.futurecdn.net/MftMZVxs3dkte2VoNsxtMi-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/MftMZVxs3dkte2VoNsxtMi-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/MftMZVxs3dkte2VoNsxtMi-320-80.png 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/MftMZVxs3dkte2VoNsxtMi.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/MftMZVxs3dkte2VoNsxtMi.png">
</picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: Nvidia)</span></figcaption></figure><p id="730616fb-ebb0-4d0b-a3de-38195e8eb02e">Intel has long offered custom Xeons to its customers, primarily hyperscalers, often with relatively minor tweaks to clock rates, cache capacities, and other specifications. In fact, these mostly slightly-modified custom Xeon models once comprised more than 50% of Intel’s Xeon shipments. Intel has endured several years of market share erosion due to AMD’s advances, most acutely in the hyperscale market. Therefore, it is unclear if the 50% number still holds true, as hyperscalers were the primary customers for custom models.</p><p>Intel has <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/intel-announces-idm-20-foundry" data-before-rewrite-localise="https://www.tomshardware.com/news/intel-announces-idm-20-foundry">long said that it will design completely custom x86 chips for customers</a> as part of its IDM 2.0 strategy. However, aside from a recent announcement of <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/intel-outlines-a-plan-to-get-back-in-the-game-pause-fab-projects-in-europe-make-the-foundry-unit-an-independent-subsidiary-and-streamline-the-x86-portfolio" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/cpus/intel-outlines-a-plan-to-get-back-in-the-game-pause-fab-projects-in-europe-make-the-foundry-unit-an-independent-subsidiary-and-streamline-the-x86-portfolio">custom AWS chips</a> that sound like the slightly modified Xeons mentioned above, we haven’t heard of any large-scale uptake for significantly modified custom x86 processors. Intel <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/intel-ousts-ceo-of-products-as-part-of-the-latest-executive-shake-up-ending-30-year-career-company-also-establishes-new-custom-chip-design-unit" data-before-rewrite-localise="https://www.tomshardware.com/tech-industry/intel-ousts-ceo-of-products-as-part-of-the-latest-executive-shake-up-ending-30-year-career-company-also-establishes-new-custom-chip-design-unit">announced a new custom chip design unit just two weeks ago</a>, so it will be interesting to learn the extent of the customization for Nvidia’s x86 data center CPUs.</p><p>Nvidia already uses Intel’s Xeons in several of its systems, like the Nvidia DGX B300, but these systems still use the PCIe interface to communicate with the CPU. Intel’s new collaboration with Nvidia will obviously open up new opportunities, given the tighter integration with NVLink and all the advantages it brings with it. The likelihood of AMD adopting NVLink Fusion is somewhere around zero, as the company is heavily invested in its own <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/amd-infinity-fabric-cpu-to-gpu" data-before-rewrite-localise="https://www.tomshardware.com/news/amd-infinity-fabric-cpu-to-gpu">Infinity Fabric (XGMI)</a> and <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/ualink-has-nvidias-nvlink-in-the-crosshairs-final-specs-support-up-to-1-024-gpus-with-200-gt-s-bandwidth" data-before-rewrite-localise="https://www.tomshardware.com/tech-industry/ualink-has-nvidias-nvlink-in-the-crosshairs-final-specs-support-up-to-1-024-gpus-with-200-gt-s-bandwidth">Ultra Accelerator Link (UALink)</a> initiatives, which aim to provide an open-standard interconnect to rival NVLink and democratize rack-scale interconnect technologies. Intel is also a member of UALink, which uses AMD’s Infinity Fabric protocol as the foundation.</p><h2 id="dollar-and-cents-geopolitics-3">Dollar and Cents, Geopolitics</h2><p id="e09444e0-bebc-4edc-83cf-79a58ac357b5">Nvidia’s $5 billion purchase of Intel common stock will come at $23.28 a share, roughly 6% below the current market value, but several aspects of this investment remain unclear. Nvidia hasn’t stated whether it will have a seat on the board (which is unlikely) or how it will vote on matters requiring shareholder approval. It is also unclear if Intel will issue new stock (primary issuance) for Nvidia to purchase, as it did when the U.S. government recently became an Intel shareholder (that is likely). Naturally, the investment is subject to approval from regulators.</p><p>Nvidia’s buy-in comes on the heels of the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/big-tech/trump-says-u-s-govt-will-take-a-10-percent-ownership-stake-in-intel-lip-bu-tan-reportedly-agreed-to-unprecedented-arrangement-for-a-domestic-chipmaker" data-before-rewrite-localise="https://www.tomshardware.com/tech-industry/big-tech/trump-says-u-s-govt-will-take-a-10-percent-ownership-stake-in-intel-lip-bu-tan-reportedly-agreed-to-unprecedented-arrangement-for-a-domestic-chipmaker">U.S government buying $10 billion of newly-created Intel stock</a>, granting the country a 9.9% ownership stake at $20.47 per share. The U.S. government won’t have a seat on the board and agreed to vote with Intel’s board on matters requiring shareholder approval “with limited exceptions.” <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/semiconductors/softbank-to-buy-usd2-billion-in-intel-shares-at-usd23-each-firm-still-owns-majority-share-of-arm" data-before-rewrite-localise="https://www.tomshardware.com/tech-industry/semiconductors/softbank-to-buy-usd2-billion-in-intel-shares-at-usd23-each-firm-still-owns-majority-share-of-arm">Softbank has also recently purchased $2 billion worth of primary issuance Intel stock</a> at $23 per share.</p><div id="slice-container-table-6S7ZPUsULrjZhoioYnhg6Z-JiGkjEwwBtiv7WLzNY6ijtNd9T35JBCY"><div><p>Swipe to scroll horizontally</p><svg viewBox="0 0 23 30" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M21.554 15.726a2.878 2.878 0 0 0-1.705-.374 2.881 2.881 0 0 0-1.388-3.068 2.877 2.877 0 0 0-1.992-.333 2.884 2.884 0 0 0-.1-.766 2.865 2.865 0 0 0-1.346-1.75c-.47-.27-.996-.4-1.527-.385l2.742-4.73a2.87 2.87 0 0 0 .323-.83h2.612V2.084h-2.661A2.861 2.861 0 0 0 15.18.385a2.903 2.903 0 0 0-3.952 1.055l-.373.644H2.983l1.003-1L2.99.09 1.28 1.793l-.999.995L2.99 5.484l.998-.994-1.003-.999h7.054L6.505 9.586c-.34.066-.905.186-1.523.366-1.405.41-2.321.895-2.8 1.483-.742.911-1.159 2.513-1.277 4.898l-.001.01c-.067 1.816.946 6.943.99 7.16a.688.688 0 0 0 1.35-.266c-.01-.051-1.023-5.177-.963-6.84.127-2.556.598-3.64.97-4.098.133-.163.602-.587 2.104-1.027l.206-.058-1.425 2.458a.685.685 0 0 0 .252.937c.33.19.75.077.94-.251L12.42 2.126a1.52 1.52 0 0 1 2.07-.552c.35.2.6.527.705.916.105.39.051.797-.15 1.145l-4.767 8.222a.685.685 0 0 0 .252.937c.33.19.75.077.94-.25l.794-1.368c.201-.348.529-.597.92-.702a1.508 1.508 0 0 1 1.854 1.066c.105.39.052.796-.15 1.144l-.377.652-.002.002-.898 1.55a.685.685 0 0 0 .252.938c.329.189.75.077.94-.251l.9-1.551c.201-.348.528-.597.92-.702a1.512 1.512 0 0 1 1.703 2.21l-1.223 2.11a.685.685 0 0 0 .252.938c.33.189.75.076.941-.252l.5-.862c.202-.348.529-.597.92-.702.392-.104.8-.051 1.15.15.723.416.972 1.34.554 2.06l-3.525 6.08c-.517.892-1.57 1.795-3.044 2.611-1.156.64-2.163.998-2.173 1.002a.685.685 0 0 0 .23 1.333.688.688 0 0 0 .229-.04c.18-.062 4.419-1.575 5.952-4.22l3.524-6.08a2.878 2.878 0 0 0-1.059-3.934Z" fill="#333"></path></svg></div><div><table tabindex="0"><caption>Purchases of Intel Stock</caption><tbody><tr><td colspan="1"><span>Row 0 - Cell 0 </span></td><td colspan="1"><p>Total</p></td><td colspan="1"><p>Share Price</p></td><td colspan="1"><p>Stake in Intel</p></td></tr><tr><td colspan="1"><p>Nvidia</p></td><td colspan="1"><p>$5 Billion</p></td><td colspan="1"><p>$23.28</p></td><td colspan="1"><p>~5%</p></td></tr><tr><td colspan="1"><p>U.S. Government</p></td><td colspan="1"><p>$9 Billion</p></td><td colspan="1"><p>$20.47</p></td><td colspan="1"><p>~9.9%</p></td></tr><tr><td colspan="1"><p>Softbank</p></td><td colspan="1"><p>$2 Billion</p></td><td colspan="1"><p>$23</p></td><td colspan="1"><span>Row 3 - Cell 3 </span></td></tr></tbody></table></div></div><p id="c44d2679-944b-4a74-827f-1a98316271f9">The U.S. government says it invested in Intel with the goal of bolstering US technology, manufacturing, and national security, and the investments from the private sector also help solidify the struggling Intel. Altogether, these investments represent a significant cash influx for Intel as it attempts to maintain the heavy cap-ex investments required to compete with TSMC, all while struggling with a negative amount of free cash flow.</p><p>“AI is powering a new industrial revolution and reinventing every layer of the computing stack — from silicon to systems to software. At the heart of this reinvention is Nvidia’s CUDA architecture,” said Nvidia CEO Jensen Huang. “This historic collaboration tightly couples NVIDIA’s AI and accelerated computing stack with Intel’s CPUs and the vast x86 ecosystem—a fusion of two world-class platforms. Together, we will expand our ecosystems and lay the foundation for the next era of computing.”</p><p>“Intel’s x86 architecture has been foundational to modern computing for decades – and we are innovating across our portfolio to enable the workloads of the future,” said Intel CEO Lip-Bu Tan. “Intel’s leading data center and client computing platforms, combined with our process technology, manufacturing and advanced packaging capabilities, will complement Nvidia's AI and accelerated computing leadership to enable new breakthroughs for the industry. We appreciate the confidence Jensen and the Nvidia team have placed in us with their investment and look forward to the work ahead as we innovate for customers and grow our business.”</p><p>We’ll learn more details of the new partnership later today when Nvidia CEO Jensen Huang and Intel CEO Lip-Bu Tan hold a <a data-analytics-id="inline-link" href="https://events.q4inc.com/attendee/108505485" data-url="https://events.q4inc.com/attendee/108505485" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">webcast press conference at 10 am PT</a>.</p><p><em><strong>This is breaking news…more to come.</strong></em></p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank" data-url="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank" data-url="https://google.com/preferences/source?q=" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
</div>



<!-- Drop in a standard article here maybe? -->




<div id="slice-container-authorBio-6S7ZPUsULrjZhoioYnhg6Z"><p>Paul Alcorn is the Editor-in-Chief for Tom's Hardware US. He also writes news and reviews on CPUs, storage, and enterprise hardware.</p></div>
</section>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[This Website Has No Class (175 pts)]]></title>
            <link>https://aaadaaam.com/notes/no-class/</link>
            <guid>45287155</guid>
            <pubDate>Thu, 18 Sep 2025 08:41:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aaadaaam.com/notes/no-class/">https://aaadaaam.com/notes/no-class/</a>, See on <a href="https://news.ycombinator.com/item?id=45287155">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
  <article animate-children="">
    <time><span>Sep 14, 2025</span></time>
    
    <p>In my recent post, <a href="https://aaadaaam.com/notes/useful-defaults/">“There’s no such thing as a CSS reset”</a>, I wrote this:</p>
<blockquote>
<p>Think of elements like components, but ones that come packed in the browser. Custom elements, without the “custom” part. You can just like, <em>use them</em>.</p>
</blockquote>
<p>The line continued to rattle around in my head, and a few weeks later when I was digging into some cleanup work I came to an uncomfortable realization; <em>I wasn’t really taking my own advice</em>. Sure, I was setting some default element styles, but I was leaving <em>a lot</em> on the table. I felt attacked. Called out even. Present me, <em>positively roasted</em> by past me. There was only one possible solution; <strong>refactor my website.</strong></p>
<p>I like to apply severe constraints in designing and building this site – I think constraints lead to interesting, creative solutions – and it was no different this time around. Instead of relying on built in elements <em>a bit more</em>, I decided to <em>banish classes from my website completely</em>. I haven’t used a class-free approach since the CSS Zen Garden days, and wanted to se how it felt with modern HTML and CSS.</p>
<h2 id="doubling-down-on-styled-defaults">Doubling down on styled defaults</h2>
<p>CSS for the site was structured around 3 cascade layers; <code>base</code>, <code>components</code>, and <code>utilities</code>. Everything in <code>base</code> was already tag selectors, so the task at hand was to change my approach for components, and eliminate utilities completely.</p>
<p>Step 1? <em>Mitigation.</em> There was plenty of code that could have been styled defaults but wasn’t, so I gave all my markup a thorough review, increasing use of semantic elements, extracting common patterns in the form of new element defaults, and making more use of contextual element styling. By contextual styling, I mean going from something like this:</p>
<pre><code><span>.header-primary</span> <span>{</span>
  <span>margin-block</span><span>:</span> <span>clamp</span><span>(</span><span>var</span><span>(</span>--size-sm<span>)</span><span>,</span> 4vw<span>,</span> <span>var</span><span>(</span>--size-lg<span>)</span><span>)</span> <span>var</span><span>(</span>--size-flex<span>)</span><span>;</span>
<span>}</span></code></pre>
<p>To something like this:</p>
<pre><code><span>body</span> <span>{</span>
  <span>background-color</span><span>:</span> <span>var</span><span>(</span>--color-sheet<span>)</span><span>;</span>

  <span>&amp; &gt; header</span> <span>{</span>
    <span>margin-block</span><span>:</span> <span>clamp</span><span>(</span><span>var</span><span>(</span>--size-sm<span>)</span><span>,</span> 4vw<span>,</span> <span>var</span><span>(</span>--size-lg<span>)</span><span>)</span> <span>var</span><span>(</span>--size-flex<span>)</span><span>;</span>
  <span>}</span>
<span>}</span></code></pre>
<p>It was a good start, and modern features like nesting, <code>:where()</code>, and <code>:has()</code> made this feel better that it did 20 years ago, but I took things way too far with contextual styles. Taken to the extreme, you end up with overloaded selector definitions and progressively more esoteric selector patterns. I knew I was down the rabbit hole when I did something like this:</p>
<pre><code><span>li</span> <span>{</span>
  <span>&amp;:has( &gt; a + p)</span> <span>{</span>
    <span>padding-block</span><span>:</span> <span>var</span><span>(</span>--size-lg<span>)</span><span>;</span>
    <span>border-block-end</span><span>:</span> <span>var</span><span>(</span>--border-default<span>)</span><span>;</span>
    <span>text-wrap</span><span>:</span> balance<span>;</span>

    <span>&amp; &gt; a</span> <span>{</span>
      <span>font-size</span><span>:</span> <span>var</span><span>(</span>--font-xxl<span>)</span><span>;</span>
    <span>}</span>

    <span>&amp; &gt; p</span> <span>{</span>
      <span>margin-block</span><span>:</span> <span>var</span><span>(</span>--size-sm<span>)</span><span>;</span>
    <span>}</span>
  <span>}</span>
<span>}</span></code></pre>
<p>I still needed a “real” solution for components, and a way to manage variants.</p>

<p>I had an inkling of a solution, which is to leverage patterns from custom elements and web components, sans js. By virtue of their progressively enhanced nature, custom tag names and custom attributes are 100% valid HTML, javascript or no. That inkling turned into fervent belief after reading Keith Cirkel’s excellent post <a href="https://www.keithcirkel.co.uk/css-classes-considered-harmful/">“CSS classes considered harmful”</a>.</p>
<p>Revisiting the example above, now we’ve got a pattern like this:</p>
<pre><code><span>note-pad</span> <span>{</span>
  <span>padding-block</span><span>:</span> <span>var</span><span>(</span>--size-lg<span>)</span><span>;</span>
  <span>border-block-end</span><span>:</span> <span>var</span><span>(</span>--border-default<span>)</span><span>;</span>
  <span>text-wrap</span><span>:</span> balance<span>;</span>

  <span>&amp; a</span> <span>{</span>
    <span>font-size</span><span>:</span> <span>var</span><span>(</span>--font-xxl<span>)</span><span>;</span>
  <span>}</span>

  <span>&amp; p</span> <span>{</span>
    <span>margin-block</span><span>:</span> <span>var</span><span>(</span>--size-sm<span>)</span><span>;</span>
  <span>}</span>
<span>}</span></code></pre>
<p>Custom attributes become a go-to for handling former BEM modifiers, but instead of relying on stylistic writing convention to fake a key-value pair, you get an <em>actual</em> key-value pair.</p>
<pre><code><span>random-pattern</span> <span>{</span>
  <span>&amp; [shape-type="1"]</span> <span>{</span>
    <span>border</span><span>:</span> 0.1rem solid <span>var</span><span>(</span>--color-sheet<span>)</span><span>;</span>
    <span>background-color</span><span>:</span> <span>var</span><span>(</span>--color-sheet<span>)</span><span>;</span>
    <span>filter</span><span>:</span> <span><span>url</span><span>(</span><span>"#noise1"</span><span>)</span></span><span>;</span>
  <span>}</span>

  <span>&amp; [shape-type="2"]</span> <span>{</span>
    <span>background</span><span>:</span> <span>var</span><span>(</span>--pattern-lines-horizontal<span>)</span><span>;</span>
    <span>background-size</span><span>:</span> <span>var</span><span>(</span>--pattern-scale<span>)</span><span>;</span>
  <span>}</span>
<span>}</span></code></pre>
<p>Now, you can use <code>data-whatever</code> for attributes, but really, any two dash-separated words are safe. Personally, I think dropping the <code>data</code> prefix feels better and allows for richer semantics.</p>
<p>You can argue that both of these techniques are re-inventing classes in various ways. Kind of! You can use custom element names in lieu of semantic tags, just like you can slap a class on a div. But these techniques, particularly with how you can seamlessly enhance to true custom elements or web components, feels like a coherent end-to-end system in a way that class-based approaches don’t. <em>It’s tags and attributes, all the way down.</em></p>
<h2 id="would-i-do-this-again">Would I do this again?</h2>
<p>On the plus side, the user outcomes are decidedly positive; I removed a non-trivial amount of CSS (now about ~5KB of CSS over the wire for the entire site), and accessibility is without question better due to having to paid much closer attention to markup. Also, <em>just look</em> at that markup. So clean. So shiny.</p>
<p>On the flipside, this feels like an approach that <em>simply asks more of authors</em>. It requires more careful planning compared to pure component approaches; you can’t think of things in purely isolated terms. All to say, I’m very happy to ship this on my personal website, I’d be less likely to advocate for this approach on a large project with varied levels of frontend knowledge.</p>
<p>There’s a variation here that’s more encapsulated (use custom tag names with abandon), but that pulls on what feels like an unresolved thread; replacing a semantic element with a custom tag name that has no semantic value <em>feels bad</em>, and adding extra wrappers around everything <em>also feels bad</em>.</p>
<p>All to say, I’m not quite ready to say that this is The One True Way I’ll build all sites from now on, but I also can’t help but feel like I’ve crossed some kind of threshold. I used to think classes were fine. Now I’m not so sure. I don’t know exactly where it’ll lead yet, but this feels like one of those exercises that’ll have a lasting influence on my work.</p>
<hr>
<p><em>A mea culpa; I only got 99% of the way there. I use <a href="https://www.11ty.dev/docs/plugins/syntaxhighlight/">11ty’s syntax highlighting plugin</a>, which uses classes for styling. I gave <a href="https://andreruffert.github.io/syntax-highlight-element/">syntax-highlight</a> a hard look, but I don’t love the idea of introducing client-side js where none need exist, and the authoring experience would be a step back, so I begrudgingly left it alone for now.</em></p>

    
  </article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pnpm has a new setting to stave off supply chain attacks (168 pts)]]></title>
            <link>https://pnpm.io/blog/releases/10.16</link>
            <guid>45286526</guid>
            <pubDate>Thu, 18 Sep 2025 07:12:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pnpm.io/blog/releases/10.16">https://pnpm.io/blog/releases/10.16</a>, See on <a href="https://news.ycombinator.com/item?id=45286526">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__blog-post-container"><h2 id="minor-changes">Minor Changes<a href="#minor-changes" aria-label="Direct link to Minor Changes" title="Direct link to Minor Changes">​</a></h2>
<h3 id="new-setting-for-delayed-dependency-updates">New setting for delayed dependency updates<a href="#new-setting-for-delayed-dependency-updates" aria-label="Direct link to New setting for delayed dependency updates" title="Direct link to New setting for delayed dependency updates">​</a></h3>
<p>There have been several incidents recently where popular packages were successfully attacked. To reduce the risk of installing a compromised version, we are introducing a new setting that delays the installation of newly released dependencies. In most cases, such attacks are discovered quickly and the malicious versions are removed from the registry within an hour.</p>
<p>The new setting is called <a href="https://pnpm.io/settings#minimumreleaseage"><code>minimumReleaseAge</code></a>. It specifies the number of minutes that must pass after a version is published before pnpm will install it. For example, setting <code>minimumReleaseAge: 1440</code> ensures that only packages released at least one day ago can be installed.</p>
<p>If you set <code>minimumReleaseAge</code> but need to disable this restriction for certain dependencies, you can list them under the <a href="https://pnpm.io/settings#minimumreleaseageexclude"><code>minimumReleaseAgeExclude</code></a> setting. For instance, with the following configuration pnpm will always install the latest version of webpack, regardless of its release time:</p>
<div><pre tabindex="0"><code><span><span>minimumReleaseAgeExclude</span><span>:</span><span></span><br></span><span><span></span><span>-</span><span> webpack</span><br></span></code></pre></div>
<p>Related issue: <a href="https://github.com/pnpm/pnpm/issues/9921" target="_blank" rel="noopener noreferrer">#9921</a>.</p>
<h3 id="advanced-dependency-filtering-with-finder-functions">Advanced dependency filtering with finder functions<a href="#advanced-dependency-filtering-with-finder-functions" aria-label="Direct link to Advanced dependency filtering with finder functions" title="Direct link to Advanced dependency filtering with finder functions">​</a></h3>
<p>Added support for <a href="https://pnpm.io/finders"><code>finders</code></a>.</p>
<p>In the past, <code>pnpm list</code> and <code>pnpm why</code> could only search for dependencies by <strong>name</strong> (and optionally version). For example:</p>

<p>prints the chain of dependencies to any installed instance of <code>minimist</code>:</p>
<div><pre tabindex="0"><code><span><span>verdaccio 5.20.1</span><br></span><span><span>├─┬ handlebars 4.7.7</span><br></span><span><span>│ └── minimist 1.2.8</span><br></span><span><span>└─┬ mv 2.1.1</span><br></span><span><span>└─┬ mkdirp 0.5.6</span><br></span><span><span>  └── minimist 1.2.8</span><br></span></code></pre></div>
<p>What if we want to search by <strong>other properties</strong> of a dependency, not just its name? For instance, find all packages that have <code>react@17</code> in their peer dependencies?</p>
<p>This is now possible with "finder functions". Finder functions can be declared in <code>.pnpmfile.cjs</code> and invoked with the <code>--find-by=&lt;function name&gt;</code> flag when running <code>pnpm list</code> or <code>pnpm why</code>.</p>
<p>Let's say we want to find any dependencies that have React 17 in peer dependencies. We can add this finder to our <code>.pnpmfile.cjs</code>:</p>
<div><pre tabindex="0"><code><span><span>module</span><span>.</span><span>exports</span><span> </span><span>=</span><span> </span><span>{</span><span></span><br></span><span><span></span><span>finders</span><span>:</span><span> </span><span>{</span><span></span><br></span><span><span>  </span><span>react17</span><span>:</span><span> </span><span>(</span><span>ctx</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>{</span><span></span><br></span><span><span>    </span><span>return</span><span> ctx</span><span>.</span><span>readManifest</span><span>(</span><span>)</span><span>.</span><span>peerDependencies</span><span>?.</span><span>react </span><span>===</span><span> </span><span>"^17.0.0"</span><span>;</span><span></span><br></span><span><span>  </span><span>}</span><span>,</span><span></span><br></span><span><span></span><span>}</span><span>,</span><span></span><br></span><span><span></span><span>}</span><span>;</span><br></span></code></pre></div>
<p>Now we can use this finder function by running:</p>
<div><pre tabindex="0"><code><span><span>pnpm why --find-by=react17</span><br></span></code></pre></div>
<p>pnpm will find all dependencies that have this React in peer dependencies and print their exact locations in the dependency graph.</p>
<div><pre tabindex="0"><code><span><span>@apollo/client 4.0.4</span><br></span><span><span>├── @graphql-typed-document-node/core 3.2.0</span><br></span><span><span>└── graphql-tag 2.12.6</span><br></span></code></pre></div>
<p>It is also possible to print out some additional information in the output by returning a string from the finder. For example, with the following finder:</p>
<div><pre tabindex="0"><code><span><span>module</span><span>.</span><span>exports</span><span> </span><span>=</span><span> </span><span>{</span><span></span><br></span><span><span></span><span>finders</span><span>:</span><span> </span><span>{</span><span></span><br></span><span><span>  </span><span>react17</span><span>:</span><span> </span><span>(</span><span>ctx</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>{</span><span></span><br></span><span><span>    </span><span>const</span><span> manifest </span><span>=</span><span> ctx</span><span>.</span><span>readManifest</span><span>(</span><span>)</span><span>;</span><span></span><br></span><span><span>    </span><span>if</span><span> </span><span>(</span><span>manifest</span><span>.</span><span>peerDependencies</span><span>?.</span><span>react </span><span>===</span><span> </span><span>"^17.0.0"</span><span>)</span><span> </span><span>{</span><span></span><br></span><span><span>      </span><span>return</span><span> </span><span>`</span><span>license: </span><span>${</span><span>manifest</span><span>.</span><span>license</span><span>}</span><span>`</span><span>;</span><span></span><br></span><span><span>    </span><span>}</span><span></span><br></span><span><span>    </span><span>return</span><span> </span><span>false</span><span>;</span><span></span><br></span><span><span>  </span><span>}</span><span>,</span><span></span><br></span><span><span></span><span>}</span><span>,</span><span></span><br></span><span><span></span><span>}</span><span>;</span><br></span></code></pre></div>
<p>Every matched package will also print out the license from its <code>package.json</code>:</p>
<div><pre tabindex="0"><code><span><span>@apollo/client 4.0.4</span><br></span><span><span>├── @graphql-typed-document-node/core 3.2.0</span><br></span><span><span>│   license: MIT</span><br></span><span><span>└── graphql-tag 2.12.6</span><br></span><span><span>  license: MIT</span><br></span></code></pre></div>
<p>Related PR: <a href="https://github.com/pnpm/pnpm/pull/9946" target="_blank" rel="noopener noreferrer">#9946</a>.</p>
<h2 id="patch-changes">Patch Changes<a href="#patch-changes" aria-label="Direct link to Patch Changes" title="Direct link to Patch Changes">​</a></h2>
<ul>
<li>Fix deprecation warning printed when executing pnpm with Node.js 24 <a href="https://github.com/pnpm/pnpm/issues/9529" target="_blank" rel="noopener noreferrer">#9529</a>.</li>
<li>Throw an error if <code>nodeVersion</code> is not set to an exact semver version <a href="https://github.com/pnpm/pnpm/issues/9934" target="_blank" rel="noopener noreferrer">#9934</a>.</li>
<li><code>pnpm publish</code> should be able to publish a <code>.tar.gz</code> file <a href="https://github.com/pnpm/pnpm/pull/9927" target="_blank" rel="noopener noreferrer">#9927</a>.</li>
<li>Canceling a running process with Ctrl-C should make <code>pnpm run</code> return a non-zero exit code <a href="https://github.com/pnpm/pnpm/issues/9626" target="_blank" rel="noopener noreferrer">#9626</a>.</li>
</ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CERN Animal Shelter for Computer Mice (280 pts)]]></title>
            <link>https://computer-animal-shelter.web.cern.ch/index.shtml</link>
            <guid>45286369</guid>
            <pubDate>Thu, 18 Sep 2025 06:53:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://computer-animal-shelter.web.cern.ch/index.shtml">https://computer-animal-shelter.web.cern.ch/index.shtml</a>, See on <a href="https://news.ycombinator.com/item?id=45286369">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-wrap">
		
		<p><b>We are back!!!</b> After the <u><a href="https://computer-animal-shelter.web.cern.ch/disaster.shtml">disaster</a></u> early 2012, we have been able to secure new funds and are happy to annouce the reopening of the CERN Animal Shelter for Computer Mice <b>on the lawn in front of the <u><a href="http://maps.cern.ch/mapsearch/mapcernlite.htm?no=513">CERN Computer Centre</a></u></b>. Our shelter is open all week-days from 8:30 to 17:30.</p>
		
		<hr>
		<p><b>
		<table> 
		<tbody><tr><td>In the hay...</td><td>Eating...</td><td>Drinking...</td></tr>
		</tbody></table>
		<img width="32%" src="https://computer-animal-shelter.web.cern.ch/images/hay.JPG" alt="In the hay...">
		<img width="32%" src="https://computer-animal-shelter.web.cern.ch/images/eating.JPG" alt="Eating...">
		<img width="32%" src="https://computer-animal-shelter.web.cern.ch/images/drinking.JPG" alt="Drinking...">
		<table> 
		<tbody><tr><td>Cuddling...</td><td>Playing...</td><td>Panicking...</td></tr>
		</tbody></table>
		<img width="32%" src="https://computer-animal-shelter.web.cern.ch/images/cuddling.JPG" alt="Cuddling...">
		<img width="32%" src="https://computer-animal-shelter.web.cern.ch/images/playing.JPG" alt="Playing...">
		<img width="32%" src="https://computer-animal-shelter.web.cern.ch/images/losing.JPG" alt="Panicking...">
		</b></p><hr><p>
		A message from our sponsor --- A message from our sponsor --- A message from our sponsor
		</p><div id="add">
		<table><tbody><tr>
		<td><img height="200px" src="https://computer-animal-shelter.web.cern.ch/images/Sheep_Phishing-mouse-pointer_modified.jpg" alt=""></td>
		<td>
		<h2>"Stop — Think — Click"...</h2>

		<p>...is the basic recommendation for securely browsing the Internet and for securely reading emails. <b>Users who have followed this recommendation in the past were <i>less likely</i> to have their computer infected or their computing account compromised</b>. However, <i>still too many users</i> click on malicious web-links, and put their computer and account at risk.</p>

		<p>Therefore, in order to avoid clicking at all, <span color="red"><b>all CERN users are asked to disconnect their computer mice from CERN computers</b></span>, and bring them to the CERN Animal Shelter for Computer Mice.</p>
		</td>
		</tr></tbody></table>
		<p>Let us help you:<br> visit <b><a href="https://cern.ch/Computer.Security">https://cern.ch/Computer.Security</a></b> or contact <b><a href="mailto:Computer.Security@cern.ch">Computer.Security@cern.ch</a></b></p>

		</div><p>
		The Computer Animal Shelter declines responsibility for the content of our sponsor's message.
		</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[European ant is the first known animal to clone members of another species (117 pts)]]></title>
            <link>https://www.livescience.com/animals/ants/almost-like-science-fiction-european-ant-is-the-first-known-animal-to-clone-members-of-another-species</link>
            <guid>45285780</guid>
            <pubDate>Thu, 18 Sep 2025 05:27:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.livescience.com/animals/ants/almost-like-science-fiction-european-ant-is-the-first-known-animal-to-clone-members-of-another-species">https://www.livescience.com/animals/ants/almost-like-science-fiction-european-ant-is-the-first-known-animal-to-clone-members-of-another-species</a>, See on <a href="https://news.ycombinator.com/item?id=45285780">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/HmpjKYcmyZFkFqw522MN7o-1920-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/HmpjKYcmyZFkFqw522MN7o-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/HmpjKYcmyZFkFqw522MN7o-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/HmpjKYcmyZFkFqw522MN7o-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/HmpjKYcmyZFkFqw522MN7o-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/HmpjKYcmyZFkFqw522MN7o-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/HmpjKYcmyZFkFqw522MN7o-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/HmpjKYcmyZFkFqw522MN7o.jpg" alt="Two winged male ants on a black background. The ant on the left is covered in hairs and the ant on the left is hairless." srcset="https://cdn.mos.cms.futurecdn.net/HmpjKYcmyZFkFqw522MN7o-1920-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/HmpjKYcmyZFkFqw522MN7o-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/HmpjKYcmyZFkFqw522MN7o-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/HmpjKYcmyZFkFqw522MN7o-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/HmpjKYcmyZFkFqw522MN7o-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/HmpjKYcmyZFkFqw522MN7o-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/HmpjKYcmyZFkFqw522MN7o-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/HmpjKYcmyZFkFqw522MN7o.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/HmpjKYcmyZFkFqw522MN7o.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<figcaption>
<span>The same Iberian harvester ant (<em>Messor ibericus</em>) queen produced the hairy male <em>Messor ibericus</em> (on the left) and the hairless male <em>Messor structor</em> (on the right), despite them being members of distantly related species.</span>
<span>(Image credit: Jonathan Romiguier, Yannick Juvé and Laurent Soldati)</span>
</figcaption>
</div>

<div id="article-body">
<p id="b751b378-6387-41e6-954c-a55161598d4d">Queen ants in southern Europe produce male clones of an entirely different species — tearing up the playbook of reproductive biology and suggesting we need to rethink our understanding of species barriers.</p><p>The workers in Iberian harvester ant (<em>Messor ibericus</em>) colonies are all hybrids, with queens needing to mate with males from a distantly related species, <em>Messor structor</em>, to keep the colony functioning. But researchers found that some Iberian harvester ant populations have no <em>M. structor</em> colonies nearby.</p><p id="3cfb1adf-71c2-4ade-9de4-b095c6a9e0da">"We had to face the facts and try to see if there is something special within <em>Messor ibericus</em> colonies," Romiguier said.</p><p>In setting out to resolve this paradox, Romiguier and his team found that queen Iberian harvester ants also lay eggs containing male <em>M. structor</em> ants, with these males ultimately fathering the workers. This discovery, published Sept. 3 in the journal <a data-analytics-id="inline-link" href="https://go.redirectingat.com/?id=92X1590019&amp;xcust=livescience_us_7386292083305503411&amp;xs=1&amp;url=https%3A%2F%2Fwww.nature.com%2Farticles%2Fs41586-025-09425-w&amp;sref=https%3A%2F%2Fwww.livescience.com%2Fanimals%2Fants%2Falmost-like-science-fiction-european-ant-is-the-first-known-animal-to-clone-members-of-another-species" target="_blank" data-url="https://www.nature.com/articles/s41586-025-09425-w" referrerpolicy="no-referrer-when-downgrade" rel="sponsored noopener" data-hl-processed="skimlinks" data-placeholder-url="https://go.redirectingat.com/?id=92X1590019&amp;xcust=hawk-custom-tracking&amp;xs=1&amp;url=https%3A%2F%2Fwww.nature.com%2Farticles%2Fs41586-025-09425-w&amp;sref=https%3A%2F%2Fwww.livescience.com%2Fanimals%2Fants%2Falmost-like-science-fiction-european-ant-is-the-first-known-animal-to-clone-members-of-another-species" data-google-interstitial="false" data-merchant-name="nature.com" data-merchant-network="SkimLinks"><u>Nature</u></a>, is the first time any animal has been recorded producing offspring from another species as part of their normal life cycle.</p><p>"In the early stages, it was kind of a joke in the team," Romiguier said. "But the more we got results, the more it became a hypothesis and not a joke anymore."</p><p><strong>Related:</strong> <a data-analytics-id="inline-link" href="https://www.livescience.com/bee-creates-perfect-clone-army.html" data-before-rewrite-localise="https://www.livescience.com/bee-creates-perfect-clone-army.html"><u><strong>Single bee is making an immortal clone army thanks to a genetic fluke</strong></u></a></p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-6KAGhFhf7SDUHe8ELuy8sK"><section><p>Get the world’s most fascinating discoveries delivered straight to your inbox.</p></section></div><p>Ants are <a data-analytics-id="inline-link" href="https://go.redirectingat.com/?id=92X1590019&amp;xcust=livescience_us_8116884233061264853&amp;xs=1&amp;url=https%3A%2F%2Fwww.nature.com%2Farticles%2Fnature09205&amp;sref=https%3A%2F%2Fwww.livescience.com%2Fanimals%2Fants%2Falmost-like-science-fiction-european-ant-is-the-first-known-animal-to-clone-members-of-another-species" target="_blank" data-url="https://www.nature.com/articles/nature09205" referrerpolicy="no-referrer-when-downgrade" rel="sponsored noopener" data-hl-processed="skimlinks" data-placeholder-url="https://go.redirectingat.com/?id=92X1590019&amp;xcust=hawk-custom-tracking&amp;xs=1&amp;url=https%3A%2F%2Fwww.nature.com%2Farticles%2Fnature09205&amp;sref=https%3A%2F%2Fwww.livescience.com%2Fanimals%2Fants%2Falmost-like-science-fiction-european-ant-is-the-first-known-animal-to-clone-members-of-another-species" data-google-interstitial="false" data-merchant-name="nature.com" data-merchant-network="SkimLinks"><u>eusocial</u></a> insects, meaning their colonies form cooperative super-organisms predominantly made up of infertile females, called workers, and a small number of reproductive females, called queens. Males solely exist to fertilize queens during their <a data-analytics-id="inline-link" href="https://www.nhm.ac.uk/discover/when-why-winged-ants-swarm-nuptial-flight.html" target="_blank" data-url="https://www.nhm.ac.uk/discover/when-why-winged-ants-swarm-nuptial-flight.html" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>mating flight</u></a> and die soon after.</p><p>Queens only mate once in their lives and store the sperm from this meeting in a special organ. She then draws from this sperm stash to lay new eggs containing one of three types of offspring: queens, workers or males.</p><p>However, Iberian harvester ants mating with males of their own species can only produce new queens. This is thought to be a result of <a data-analytics-id="inline-link" href="https://doi.org/10.1002/evl3.253" target="_blank" data-url="https://doi.org/10.1002/evl3.253" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>selfish queen genes</u></a>, where the DNA from male <em>M. ibericus</em> guarantees its survival across generations by biasing larvae to produce fertile queens rather than infertile workers — known as "royal cheaters."</p><figure data-bordeaux-image-check="" id="de51f6e2-901e-4cd9-be04-2dd42bef4c90"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/aXsHbWNpdFYYiTh2xA34i9-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/aXsHbWNpdFYYiTh2xA34i9-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/aXsHbWNpdFYYiTh2xA34i9-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/aXsHbWNpdFYYiTh2xA34i9-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/aXsHbWNpdFYYiTh2xA34i9-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/aXsHbWNpdFYYiTh2xA34i9-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/aXsHbWNpdFYYiTh2xA34i9.jpg" alt="Two male ants with wings on a black background." srcset="https://cdn.mos.cms.futurecdn.net/aXsHbWNpdFYYiTh2xA34i9-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/aXsHbWNpdFYYiTh2xA34i9-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/aXsHbWNpdFYYiTh2xA34i9-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/aXsHbWNpdFYYiTh2xA34i9-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/aXsHbWNpdFYYiTh2xA34i9-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/aXsHbWNpdFYYiTh2xA34i9-320-80.jpg 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/aXsHbWNpdFYYiTh2xA34i9.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/aXsHbWNpdFYYiTh2xA34i9.jpg">
</picture></p></div><figcaption itemprop="caption description"><span>These two ants share the same mitochondrial DNA but different nuclear DNA. </span><span itemprop="copyrightHolder">(Image credit: Jonathan Romiguier, Yannick Juvé and Laurent Soldati)</span></figcaption></figure><p id="9e70406f-6b33-4f19-9a46-55842b7d79c2">To avoid this, queens must use sperm from male <em>M. structor </em>ants to produce their workers.</p><p>This was why the presence of thriving isolated <em>M. ibericus</em> colonies was such a conundrum.</p><p>To find answers, the researchers first sampled 132 males from 26 Iberian harvester ant colonies to figure out whether there were <em>M. structor</em> males present. They found that 58 were covered in hair and 74 were hairless. A closer inspection of the nuclear genomes of a subset of these ants revealed that all hairy ones were <em>M. ibericus</em> and all bald ones were <em>M. structor</em>.</p><p>But this was not proof that the queens were laying male eggs of two different species — there could have been some hidden <em>M. structor </em>queens producing the odd male. So the team sequenced the <a data-analytics-id="inline-link" href="https://www.genome.gov/genetics-glossary/Mitochondrial-DNA" target="_blank" data-url="https://www.genome.gov/genetics-glossary/Mitochondrial-DNA" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>mitochondrial DNA</u></a>, which is passed down by the mother, of 24 of the <em>M. structor</em> males, and found it came from the same mother as the <em>M. ibericus </em>male nestmates.</p><p>"This was the detail that made me realize that 'maybe we are on to something very, very, very big,'" Romiguier said.</p><p id="6b230632-a9cb-470d-944f-c19033d9acd2">The team then separated 16 queens from laboratory colonies and looked at the genetic sequences of their freshly laid eggs. They found that 9% of their eggs contained <em>M. structor</em> ants. They then directly observed a single queen producing males of both species by monitoring its broods weekly over an 18-month period.</p><p>Together, all these findings show that Iberian harvester ant queens are cloning <em>M. structor</em> males and not passing on any of their own nuclear DNA. Researchers now need to pinpoint the exact mechanism underlying this cloning, Romiguier said, and find out at what point the maternal DNA is removed.</p><p id="0c79d4e8-8fd7-427a-8925-ed2ba6ad8d4f"><a data-analytics-id="inline-link" href="https://ebe.ulb.be/ebe/Fournier.html" target="_blank" data-url="https://ebe.ulb.be/ebe/Fournier.html" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>Denis Fournier</u></a>, an evolutionary biologist and ecologist at the Free University of Brussels, Belgium, who was not involved in the research, said that it was "almost like science fiction" when he first learned of this discovery. "It's jaw-dropping! Most of us learn that species boundaries are firm, yet here is a system where ants regularly cross them as part of normal life," he told Live Science in an email.</p><p>The team have called this new reproductive system "xenoparity," meaning the birth of a different species. Romiguier said the team aren't exactly sure when this system first emerged in the Iberian harvester ants, but it's somewhere between when <em>M. ibericus </em>and <em>M. structor</em> split along different evolutionary paths 5 million years ago and a few thousand years ago.</p><p>"This discovery is a great reminder to stay open to the unexpected," Fournier said, noting that the finding opens up new questions about cooperation, conflict and dependency in nature. "Now that we know such a system is possible, it’s exciting to think that old, puzzling data might suddenly make sense in light of this discovery," he added.</p>
</div>

<div id="slice-container-authorBio-6KAGhFhf7SDUHe8ELuy8sK"><p>Sophie is a U.K.-based staff writer at Live Science. She covers a wide range of topics, having previously reported on research spanning from bonobo communication to the first water in the universe. Her work has also appeared in outlets including New Scientist, The Observer and BBC Wildlife, and she was shortlisted for the Association of British Science Writers' 2025 "Newcomer of the Year" award for her freelance work at New Scientist. Before becoming a science journalist, she completed a doctorate in evolutionary anthropology from the University of Oxford, where she spent four years looking at why some chimps are better at using tools than others.</p></div>

</section>





</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Towards a Physics Foundation Model (106 pts)]]></title>
            <link>https://arxiv.org/abs/2509.13805</link>
            <guid>45284766</guid>
            <pubDate>Thu, 18 Sep 2025 03:06:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2509.13805">https://arxiv.org/abs/2509.13805</a>, See on <a href="https://news.ycombinator.com/item?id=45284766">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2509.13805">View PDF</a>
    <a href="https://arxiv.org/html/2509.13805v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Foundation models have revolutionized natural language processing through a ``train once, deploy anywhere'' paradigm, where a single pre-trained model adapts to countless downstream tasks without retraining. Access to a Physics Foundation Model (PFM) would be transformative -- democratizing access to high-fidelity simulations, accelerating scientific discovery, and eliminating the need for specialized solver development. Yet current physics-aware machine learning approaches remain fundamentally limited to single, narrow domains and require retraining for each new system. We present the General Physics Transformer (GPhyT), trained on 1.8 TB of diverse simulation data, that demonstrates foundation model capabilities are achievable for physics. Our key insight is that transformers can learn to infer governing dynamics from context, enabling a single model to simulate fluid-solid interactions, shock waves, thermal convection, and multi-phase dynamics without being told the underlying equations. GPhyT achieves three critical breakthroughs: (1) superior performance across multiple physics domains, outperforming specialized architectures by up to 29x, (2) zero-shot generalization to entirely unseen physical systems through in-context learning, and (3) stable long-term predictions through 50-timestep rollouts. By establishing that a single model can learn generalizable physical principles from data alone, this work opens the path toward a universal PFM that could transform computational science and engineering.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Florian Wiesner [<a href="https://arxiv.org/show-email/a5962c15/2509.13805" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Wed, 17 Sep 2025 08:19:57 UTC (5,623 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[UC Berkeley gives personal information for 150 students and staff to government (150 pts)]]></title>
            <link>https://www.dailycal.org/news/campus/uc-berkeley-turns-over-personal-information-of-more-than-150-students-and-staff-to-federal/article_a4aad3e1-bbba-42cc-92d7-a7964d9641c5.html</link>
            <guid>45284477</guid>
            <pubDate>Thu, 18 Sep 2025 02:33:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dailycal.org/news/campus/uc-berkeley-turns-over-personal-information-of-more-than-150-students-and-staff-to-federal/article_a4aad3e1-bbba-42cc-92d7-a7964d9641c5.html">https://www.dailycal.org/news/campus/uc-berkeley-turns-over-personal-information-of-more-than-150-students-and-staff-to-federal/article_a4aad3e1-bbba-42cc-92d7-a7964d9641c5.html</a>, See on <a href="https://news.ycombinator.com/item?id=45284477">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-body" itemprop="articleBody" false="">
                                <meta itemprop="isAccessibleForFree" content="true">
                                
                                
                                <p>UC Berkeley has provided the personal information of roughly 160 students, staff and faculty to the federal government in a directive from the UC Office of the President.&nbsp;</p><p dir="ltr"><span>In an attempt to comply with an</span> <a href="https://www.dailycal.org/news/campus/department-of-education-announces-antisemitism-inquiries-into-uc-berkeley-and-4-other-universities/article_ec4465a6-e389-11ef-b66c-03a9cd0727a9.html"><span>investigation</span></a> <span>into alleged campus antisemitism by the Department of Education, Office of Civil Rights, or OCR, UC Berkeley released the names of individuals and their “potential connection” to reports of alleged antisemitism, according to an email from campus spokesperson Janet Gilmore.&nbsp;</span></p><p dir="ltr"><span>Affected individuals received an email Sept. 4 from the campus Office of Legal Affairs notifying them that their names and information had been released. The message said the information had been disclosed over two weeks earlier.&nbsp;</span></p><p dir="ltr"><span>“As part of its investigation, OCR required production of comprehensive documents, including files and reports related to alleged antisemitic incidents,” the Office of Legal Affairs email read. “This notice is to inform you that, as required by law and as per directions provided by the UC systemwide Office of General Counsel (OGC), your name was included in reports as part of the documents provided by OGC to OCR for its investigation on August 18, 2025.”</span></p><p dir="ltr"><span>One campus graduate student, who received the message and was provided anonymity due to fears of retaliation, claimed the release targeted Muslim and Arab individuals who had previously expressed support for Palestine.&nbsp;&nbsp;</span></p><p dir="ltr"><span>“I think (the message was sent) to anybody who has ever been accused of antisemitism, which of course, includes a lot of Palestinians,” the student said. “Whenever we teach about Palestine, it usually leads to an investigation. I think they flagged and sent all of that information to the federal government.”</span></p><p dir="ltr"><span>The student claimed they had been the subject of a false report of antisemitism to the campus Title IX and XI Office for the Prevention of Harassment and Discrimination, or OPHD. They said other students who received the notification had OPHD cases that were determined to be unsubstantiated or stand open.&nbsp;</span></p><p dir="ltr"><span>While OPHD is the primary office for any harassment or discrimination reports, Gilmore said documents were sent from, “multiple campus offices to address (OCR’s) questions regarding campus handling of antisemitism on campus.”&nbsp;</span></p><p dir="ltr"><span>Campus officials did not say which offices provided information or what criteria were used to determine which individuals were associated with “antisemitism.”&nbsp;</span></p><p dir="ltr"><span>In February, the Department of Education</span> <a href="https://www.dailycal.org/news/campus/department-of-education-announces-antisemitism-inquiries-into-uc-berkeley-and-4-other-universities/article_ec4465a6-e389-11ef-b66c-03a9cd0727a9.html"><span>initiated</span></a> <span>an investigation into UC Berkeley’s handling of campus antisemitism. This, alongside an</span> <a href="https://www.dailycal.org/news/campus/department-of-justice-to-investigate-antisemitism-claims-against-the-university-of-california/article_7bc53322-fb31-11ef-85b8-8b9125661cf5.html"><span>investigation</span></a> <span>from the DOJ and Chancellor Rich Lyons’s</span> <a href="https://www.dailycal.org/news/national/uc-berkeley-chancellor-weathers-political-storm-in-house-antisemitism-hearing/article_589e8a3d-2a61-4a12-8e9b-ecdc8d47fa66.html"><span>testimony</span></a> <span>to Congress&nbsp; this summer represent a year-long crackdown on universities following the 2024 pro-Palestine encampments.&nbsp;</span></p><p dir="ltr"><span>Alongside UC Berkeley, UCSF, UCLA, UC Davis and UC San Diego have been targeted in Department of Education antisemitism inquiries. According to the campus grad student, some of these campuses also released student information to OCR at the direction of UCOP.&nbsp;</span></p><p dir="ltr"><span>However, these campuses did not send notifications to affected individuals, the student said. Alongside campus community members, the student said they notified pro-Palestinian groups across the UC that their members' personal information may have been shared with the federal government.&nbsp;</span></p><p dir="ltr"><span>UCOP did not comment on the compliance of other campuses or the specific directives given to UC Berkeley.</span></p><p dir="ltr"><span>In a Sunday Instagram post, UC Berkeley Students for Justice in Palestine, or SJP, decried the university for its “betrayal” to students, claiming campus administrators had previously provided them assurances that “identities would remain protected.”&nbsp;</span></p><p dir="ltr"><span>“Chancellor Rich Lyons should not have given assurances that he wouldn't be giving our information to the federal government,” the student said. “Beyond that, he should never have bowed down so easily. I would think that a university that prides itself on being this liberal haven would at least stand up to a fascist like Donald Trump.”&nbsp;</span></p><p dir="ltr"><span>In the final line of the email notification, the Office of Legal Affairs said the OCR investigation is still ongoing and further disclosures could be required.&nbsp;</span></p><p dir="ltr"><span>Moreover, the student and other individuals affected by the disclosure said this action has made them fearful, as they worry about how the information is going to be used by the Trump administration.&nbsp;</span></p><p dir="ltr"><span>“We’re concerned about how are they going to use that information to further repress us not only on campus, but also in our everyday lives,” the student said.” One of the things that I'm getting ready to do is my research year; and now I have to consult lawyers about even if it's safe to do my research, which is what I came here to do.”</span></p>
                                
                                
                            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: The text disappears when you screenshot it (477 pts)]]></title>
            <link>https://unscreenshottable.vercel.app/?text=Hello</link>
            <guid>45284311</guid>
            <pubDate>Thu, 18 Sep 2025 02:18:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://unscreenshottable.vercel.app/?text=Hello">https://unscreenshottable.vercel.app/?text=Hello</a>, See on <a href="https://news.ycombinator.com/item?id=45284311">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Slack is extorting us with a $195k/yr bill increase (2536 pts)]]></title>
            <link>https://skyfall.dev/posts/slack</link>
            <guid>45283887</guid>
            <pubDate>Thu, 18 Sep 2025 01:37:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://skyfall.dev/posts/slack">https://skyfall.dev/posts/slack</a>, See on <a href="https://news.ycombinator.com/item?id=45283887">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <div>  <p>An open letter, or something</p> </div> <p><time datetime="2025-09-18T18:00:00.000Z"> September 18th 2025 </time> </p>  </div><div>  <p>For nearly 11 years, Hack Club - a nonprofit that provides coding education and community to teenagers worldwide - has used Slack as the tool for communication. We weren’t freeloaders. A few years ago, when Slack transitioned us from their free nonprofit plan to a $5,000/year arrangement, we happily paid. It was reasonable, and we valued the service they provided to our community.</p>
<p>However, two days ago, Slack reached out to us and said that if we don’t agree to pay an extra $50k <strong>this week</strong> and $200k a year, they’ll deactivate our Slack workspace and delete all of our message history.</p>
<p>One could argue that Slack is free to stop providing us the nonprofit offer at any time, but in my opinion, a six month grace period is the <em>bare minimum</em> for a massive hike like this, if not more. Essentially, Salesforce (a <strong>$230 billion</strong> company) is strong-arming a small nonprofit for teens, by providing less than a week to pony up a pretty massive sum of money, or risk cutting off all our communications. That’s absurd.</p>
<h2 id="the-impact">The impact</h2>
<p>The small amount of notice has also been catastrophic for the programs that we run. Dozens of our staff and volunteers are now scrambling to update systems, rebuild integrations and migrate <em>years</em> of institutional knowledge. The opportunity cost of this forced migration is simply staggering.</p>
<p><img width="752" height="55" alt="image" src="https://github.com/user-attachments/assets/48097101-1521-4f50-b970-9557a0b7eefd">
<img width="1146" height="103" alt="image" src="https://github.com/user-attachments/assets/f09902a1-42cb-4cd7-9a32-21cdbfb3fd05">
<img width="1146" height="134" alt="image" src="https://github.com/user-attachments/assets/dbfc784a-d06b-44d8-a050-ec8c16c5a98b">
<img width="611" height="274" alt="image" src="https://github.com/user-attachments/assets/8a41302f-2e5f-41c1-933f-d856094c587a"></p><p>Anyway, we’re moving to Mattermost. This experience has taught us that owning your data is incredibly important, and if you’re a small business especially, then I’d advise you move away too.</p>
<hr>
<p><em>This post was rushed out because, well, this has been a shock! If you’d like any additional details then feel free to send me an email.</em></p>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hypervisor 101 in Rust (156 pts)]]></title>
            <link>https://tandasat.github.io/Hypervisor-101-in-Rust/</link>
            <guid>45283731</guid>
            <pubDate>Thu, 18 Sep 2025 01:18:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tandasat.github.io/Hypervisor-101-in-Rust/">https://tandasat.github.io/Hypervisor-101-in-Rust/</a>, See on <a href="https://news.ycombinator.com/item?id=45283731">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-wrapper">

            <div id="content" class="page">
                    <main>
                        <h2 id="welcome-to-hypervisor-101-in-rust"><a href="#welcome-to-hypervisor-101-in-rust">Welcome to Hypervisor 101 in Rust</a></h2>
<p>This is a day long course to quickly learn the inner working of hypervisors and techniques to write them for high-performance fuzzing.</p>
<p>This course covers foundation of hardware-assisted virtualization technologies, such as VMCS/VMCB, guest-host world switches, EPT/NPT, as well as useful features and techniques such as exception interception for virtual machine introspection for fuzzing.</p>
<p>The class is made up of lectures using the materials within this directory and hands-on exercises with source code under the <code>Hypervisor-101-in-Rust/hypervisor</code> directory.</p>
<p>This lecture materials are written for the <code>gcc2023</code> branch, which notionally have incomplete code for step-by-step exercises. Check out the starting point of the branch as below to go over hands-on exercises before you start.</p>
<pre><code>git checkout b17a59dd634a7b0c2b9a6d493fc9b0ff22dcfce5
</code></pre>

                    </main>

                    <nav aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->

                            <a rel="next prefetch" href="https://tandasat.github.io/Hypervisor-101-in-Rust/introduction/prerequisites.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i></i>
                            </a>

                        
                    </nav>
                </div>

            <nav aria-label="Page navigation">

                    <a rel="next prefetch" href="https://tandasat.github.io/Hypervisor-101-in-Rust/introduction/prerequisites.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i></i>
                    </a>
            </nav>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta Ray-Ban Display (577 pts)]]></title>
            <link>https://www.meta.com/blog/meta-ray-ban-display-ai-glasses-connect-2025/</link>
            <guid>45283306</guid>
            <pubDate>Thu, 18 Sep 2025 00:30:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.meta.com/blog/meta-ray-ban-display-ai-glasses-connect-2025/">https://www.meta.com/blog/meta-ray-ban-display-ai-glasses-connect-2025/</a>, See on <a href="https://news.ycombinator.com/item?id=45283306">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Stepping Down as Libxml2 Maintainer (157 pts)]]></title>
            <link>https://discourse.gnome.org/t/stepping-down-as-libxml2-maintainer/31398</link>
            <guid>45283196</guid>
            <pubDate>Thu, 18 Sep 2025 00:17:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://discourse.gnome.org/t/stepping-down-as-libxml2-maintainer/31398">https://discourse.gnome.org/t/stepping-down-as-libxml2-maintainer/31398</a>, See on <a href="https://news.ycombinator.com/item?id=45283196">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/DiscussionForumPosting" id="main-outlet" role="main">
      <meta itemprop="headline" content="Stepping down as libxml2 maintainer">
      
      <meta itemprop="datePublished" content="2025-09-15T12:06:01Z">
        <meta itemprop="articleSection" content="Platform">
      <meta itemprop="keywords" content="announcement, libxml2">
      


          <div id="post_1">
            <div>
              


              <p><span>
                  <time datetime="2025-09-15T12:06:01Z">
                    September 15, 2025, 12:06pm
                  </time>
                  <meta itemprop="dateModified" content="2025-09-15T12:06:01Z">
              <span itemprop="position">1</span>
              </span>
            </p></div>
            <div itemprop="text">
              <p>I’m stepping down as maintainer of libxml2 which means that this project is more or less unmaintained for now.</p>
<p>I will fix regressions in the 2.15 release until the end of 2025.</p>
            </div>

            

          </div>
          <div id="post_2" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.gnome.org/u/tragivictoria"><span itemprop="name">tragivictoria</span></a>
                (Victoria 🏳️‍⚧️🏳️‍🌈)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-09-15T14:04:28Z">
                    September 15, 2025,  2:04pm
                  </time>
                  <meta itemprop="dateModified" content="2025-09-15T14:04:28Z">
              <span itemprop="position">2</span>
              </span>
            </p>
            <p>Thank you for your hard work!</p>

            

          </div>
          <div id="post_3" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.gnome.org/u/mcatanzaro"><span itemprop="name">mcatanzaro</span></a>
                (Michael Catanzaro)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-09-15T16:18:11Z">
                    September 15, 2025,  4:18pm
                  </time>
                  <meta itemprop="dateModified" content="2025-09-15T16:18:11Z">
              <span itemprop="position">3</span>
              </span>
            </p>
            <p>Yes, thank you for maintaining libxml2 for such a long time!</p>

            

          </div>
          <div id="post_4" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.gnome.org/u/imcsk8"><span itemprop="name">imcsk8</span></a>
                (Iván Chavero)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-09-15T18:56:45Z">
                    September 15, 2025,  6:56pm
                  </time>
                  <meta itemprop="dateModified" content="2025-09-15T18:56:45Z">
              <span itemprop="position">4</span>
              </span>
            </p>
            <div itemprop="text">
              <p>Hello, since I’ve stepped in as libxslt maintainer I’ve been studying both libxslt and libxml2 codebases. I have the time to maintain the library I just need to get familiar with the latest changes you introduced like:</p>


<p>I haven’t find how to manage both output and input buffers. I found functions like: <em>xmlOutputBufferCreateIO</em> but by the places in which I’ve found them is not clear on how to use them.</p>
<p>Should I send you an email with my questions or do you prefer other means of communication?</p>
            </div>

            

          </div>
          <div id="post_5" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.gnome.org/u/sri"><span itemprop="name">sri</span></a>
                (sri)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-09-15T19:32:59Z">
                    September 15, 2025,  7:32pm
                  </time>
                  <meta itemprop="dateModified" content="2025-09-15T19:32:59Z">
              <span itemprop="position">5</span>
              </span>
            </p>
            <p>Thank you Nick for maintaining the key libraries of the internet and used in millions of products globaly. Best of luck to you.</p>

            

          </div>
          <div id="post_6" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>I am one of those millions of people that use this library on the behalf of us Thank you very much!!</p>

            

          </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[One Token to rule them all – Obtaining Global Admin in every Entra ID tenant (291 pts)]]></title>
            <link>https://dirkjanm.io/obtaining-global-admin-in-every-entra-id-tenant-with-actor-tokens/</link>
            <guid>45282497</guid>
            <pubDate>Wed, 17 Sep 2025 23:03:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dirkjanm.io/obtaining-global-admin-in-every-entra-id-tenant-with-actor-tokens/">https://dirkjanm.io/obtaining-global-admin-in-every-entra-id-tenant-with-actor-tokens/</a>, See on <a href="https://news.ycombinator.com/item?id=45282497">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
        <header>
          
          
            <p> 




  17 minute read
</p>
          
        </header>
      

      <section itemprop="text">
        
        <p>While preparing for my Black Hat and DEF CON talks in July of this year, I found the most impactful Entra ID vulnerability that I will probably ever find. This vulnerability could have allowed me to compromise every Entra ID tenant in the world (except probably those in national cloud deployments<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup>). If you are an Entra ID admin reading this, yes that means complete access to your tenant. The vulnerability consisted of two components: undocumented impersonation tokens, called “Actor tokens”, that Microsoft uses in their backend for service-to-service (S2S) communication. Additionally, there was a critical flaw in the (legacy) Azure AD Graph API that failed to properly validate the originating tenant, allowing these tokens to be used for cross-tenant access.</p>

<p>Effectively this means that with a token I requested in my lab tenant I could authenticate as <em>any user</em>, including Global Admins, in <em>any other tenant</em>. Because of the nature of these Actor tokens, they are not subject to security policies like Conditional Access, which means there was no setting that could have mitigated this for specific hardened tenants. Since the Azure AD Graph API is an older API for managing the core Azure AD / Entra ID service, access to this API could have been used to make any modification in the tenant that Global Admins can do, including taking over or creating new identities and granting them any permission in the tenant. With these compromised identities the access could also be extended to Microsoft 365 and Azure.</p>

<p>I reported this vulnerability the same day to the Microsoft Security Response Center (MSRC). Microsoft fixed this vulnerability on their side within days of the report being submitted and has rolled out further mitigations that block applications from requesting these Actor tokens for the Azure AD Graph API. Microsoft also issued <a href="https://msrc.microsoft.com/update-guide/vulnerability/CVE-2025-55241">CVE-2025-55241</a> for this vulnerability.</p>

<h2 id="impact">Impact</h2>
<p>These tokens allowed full access to the Azure AD Graph API in any tenant. Requesting Actor tokens does not generate logs. Even if it did they would be generated in my tenant instead of in the victim tenant, which means there is no record of the existence of these tokens.</p>

<p>Furthermore, the Azure AD Graph API does not have API level logging. Its successor, the Microsoft Graph, does have this logging, but for the Azure AD Graph this telemetry source is still in a very limited preview and I’m not aware of any tenant that currently has this available. Since there is no API level logging, it means the following Entra ID data could be accessed without any traces:</p>

<ul>
  <li>User information including all their personal details stored in Entra ID.</li>
  <li>Group and role information.</li>
  <li>Tenant settings and (Conditional Access) policies.</li>
  <li>Applications, Service Principals, and any application permission assignment.</li>
  <li>Device information and BitLocker keys synced to Entra ID.</li>
</ul>

<p>This information could be accessed by impersonating a regular user in the victim tenant. If you want to know the full impact, my tool <a href="https://github.com/dirkjanm/ROADtools">roadrecon</a> uses the same API, if you run it then everything you find in the GUI of the tool could have been accessed and modified by an attacker abusing this flaw.</p>

<p>If a Global Admin was impersonated, it would also be possible to <strong>modify</strong> any of the above objects and settings. This would result in full tenant compromise with access to any service that uses Entra ID for authentication, such as SharePoint Online and Exchange Online. It would also provide full access to any resource hosted in Azure, since these resources are controlled from the tenant level and Global Admins can grant themselves rights on Azure subscriptions. Modifying objects in the tenant does (usually) result in audit logs being generated. That means that while theoretically all data in Microsoft 365 could have been compromised, doing anything other than reading the directory information would leave audit logs that could alert defenders, though without knowledge of the specific artifacts that modifications with these Actor tokens generate, it would appear as if a legitimate Global Admin performed the actions.</p>

<p>Based on Microsoft’s internal telemetry, they did not detect any abuse of this vulnerability. If you want to search for possible abuse artifacts in your own environment, a KQL detection is included at the end of this post.</p>

<h2 id="technical-details">Technical details</h2>
<h2 id="actor-tokens">Actor tokens</h2>
<p>Actor tokens are tokens that are issued by the “Access Control Service”. I don’t know the exact origins of this service, but it appears to be a legacy service that is used for authentication with SharePoint applications and also seems to be used by Microsoft internally. I came across this service while investigating hybrid Exchange setups. These hybrid setups used to provision a certificate credential on the Exchange Online Service Principal (SP) in the tenant, with which it can perform authentication. These hybrid attacks were the topic of some talks I did this summer, the slides are on the <a href="https://dirkjanm.io/talks/">talks</a> page. In this case the hybrid part is not relevant, as in my lab I could also have added a credential on the Exchange Online SP without the complete hybrid setup. Exchange is not the only app which can do this, but since I found this in Exchange we will keep talking about these tokens in the context of Exchange.</p>

<p>Exchange will request Actor tokens when it wants to communicate with other services on behalf of a user. The Actor token allows it to “act” as another user in the tenant when talking to Exchange Online, SharePoint and as it turns out the Azure AD Graph. The Actor token (a JSON Web Token / JWT) looks as follows when decoded:</p>

<div><pre><code>{
    "alg": "RS256",
    "kid": "_jNwjeSnvTTK8XEdr5QUPkBRLLo",
    "typ": "JWT",
    "x5t": "_jNwjeSnvTTK8XEdr5QUPkBRLLo"
}
{
    "aud": "00000002-0000-0000-c000-000000000000/graph.windows.net@6287f28f-4f7f-4322-9651-a8697d8fe1bc",
    "exp": 1752593816,
    "iat": 1752507116,
    "identityprovider": "00000001-0000-0000-c000-000000000000@6287f28f-4f7f-4322-9651-a8697d8fe1bc",
    "iss": "00000001-0000-0000-c000-000000000000@6287f28f-4f7f-4322-9651-a8697d8fe1bc",
    "nameid": "00000002-0000-0ff1-ce00-000000000000@6287f28f-4f7f-4322-9651-a8697d8fe1bc",
    "nbf": 1752507116,
    "oid": "a761cbb2-fbb6-4c80-aa50-504962316eb2",
    "rh": "1.AXQAj_KHYn9PIkOWUahpfY_hvAIAAAAAAAAAwAAAAAAAAACtAQB0AA.",
    "sub": "a761cbb2-fbb6-4c80-aa50-504962316eb2",
    "trustedfordelegation": "true",
    "xms_spcu": "true"
}.[signature from Entra ID]
</code></pre></div>

<p>There are a few fields here that differ from regular Entra ID access tokens:</p>

<ul>
  <li>The <code>aud</code> field contains the GUID of the Azure AD Graph API, as well as the URL <code>graph.windows.net</code> and the tenant it was issued to <code>6287f28f-4f7f-4322-9651-a8697d8fe1bc</code>.</li>
  <li>The expiry is exactly 24 hours after the token was issued.</li>
  <li>The <code>iss</code> contains the GUID of the Entra ID token service itself, called “Azure ESTS Service”, and again the tenant GUID where it was issued.</li>
  <li>The token contains the claim <code>trustedfordelegation</code>, which is <code>True</code> in this case, meaning we can use this token to impersonate other identities. Many Microsoft apps could request such tokens. Non-Microsoft apps requesting an Actor token would receive a token with this field set to <code>False</code> instead.</li>
</ul>

<p>When using this Actor token, Exchange would embed this in an <strong>unsigned</strong> JWT that is then sent to the resource provider, in this case the Azure AD graph. In the rest of the blog I call these <strong>impersonation tokens</strong> since they are used to impersonate users.</p>

<div><pre><code>{
    "alg": "none",
    "typ": "JWT"
}
{
    "actortoken": "eyJ0eXAiOiJKV1Qi&lt;snip&gt;TxeLkNB8v2rWWMLGpaAaFJlhA",
    "aud": "00000002-0000-0000-c000-000000000000/graph.windows.net@6287f28f-4f7f-4322-9651-a8697d8fe1bc",
    "exp": 1756926566,
    "iat": 1756926266,
    "iss": "00000002-0000-0ff1-ce00-000000000000@6287f28f-4f7f-4322-9651-a8697d8fe1bc",
    "nameid": "10032001E2CBE43B",
    "nbf": 1756926266,
    "nii": "urn:federation:MicrosoftOnline",
    "sip": "doesnt@matter.com",
    "smtp": "doesnt@matter.com",
    "upn": "doesnt@matter.com"
}.[no signature]
</code></pre></div>

<p>The <code>sip</code>, <code>smtp</code>, <code>upn</code> fields are used when accessing resources in Exchange online or SharePoint, but are ignored when talking to the Azure AD Graph, which only cares about the <code>nameid</code>. This <code>nameid</code> originates from an attribute of the user that is called the <code>netId</code> on the Azure AD Graph. You will also see it reflected in tokens issued to users, in the <code>puid</code> claim, which stands for Passport UID. I believe these identifiers are an artifact from the original codebase which Microsoft used for its Microsoft Accounts (consumer accounts or MSA). They are still used in Entra ID, for example to map guest users to the original identity in their home tenant.</p>

<p>As I mentioned before, these impersonation tokens are not signed. That means that once Exchange has an Actor token, it can use the one Actor token to impersonate anyone against the target service it was requested for, for 24 hours. In my personal opinion, this whole Actor token design is something that never should have existed. It lacks almost every security control that you would want:</p>

<ul>
  <li>There are no logs when Actor tokens are issued.</li>
  <li>Since these services can craft the unsigned impersonation tokens without talking to Entra ID, there are also no logs when they are created or used.</li>
  <li>They cannot be revoked within their 24 hours validity.</li>
  <li>They completely bypass any restrictions configured in Conditional Access.</li>
  <li>We have to rely on logging from the resource provider to even know these tokens were used in the tenant.</li>
</ul>

<p>Microsoft uses these tokens to talk to other services in their backend, something that Microsoft calls service-to-service (S2S) communication. If one of these tokens leaks, it can be used to access all the data in an entire tenant without any useful telemetry or mitigation. In July of this year, Microsoft did publish <a href="https://www.microsoft.com/en-us/security/blog/2025/07/08/enhancing-microsoft-365-security-by-eliminating-high-privilege-access/">a blog</a> about removing these insecure legacy practices from their environment, but they do not provide any transparency about how many services still use these tokens.</p>

<h2 id="the-fatal-flaw-leading-to-cross-tenant-compromise">The fatal flaw leading to cross-tenant compromise</h2>
<p>As I was refining my slide deck and polished up my proof-of-concept code for requesting and generating these tokens, I tested more variants of using these tokens, changing various fields to see if the tokens still worked with the modified information. As one of the tests I changed the tenant ID of the impersonation token to a different tenant in which none of my test accounts existed. The Actor tokens tenant ID was my <code>iminyour.cloud</code> tenant, with tenant ID <code>6287f28f-4f7f-4322-9651-a8697d8fe1bc</code> and the unsigned JWT generated had the tenant ID <code>b9fb93c1-c0c8-4580-99f3-d1b540cada32</code>.</p>

<p><img src="https://dirkjanm.io/assets/img/actortokens/tenantchange.png" alt="Changed tenant ID"></p>

<p>I sent this token to <code>graph.windows.net</code> using my CLI tool <code>roadtx</code>, expecting a generic access denied since I had a tenant ID mismatch. However, I was instead greeted by a curious error message:</p>

<p><img src="https://dirkjanm.io/assets/img/actortokens/usernotfound.png" alt="Error message indicating the user does not exist"></p>

<p><em>Note that these are the actual screenshots I made during my research, which is why the formatting may not work as well in this blog</em></p>

<p>The error message suggested that while my token was valid, the identity could not be found in the tenant. Somehow the API seemed to accept my token even with the mismatching tenant. I quickly looked up the <code>netId</code> of a user that did exist in the target tenant, crafted a token and the Azure AD Graph happily returned the data I requested. I tested this in a few more test tenants I had access to, to make sure I was not crazy, but I could indeed access data in other tenants, as long as I knew their tenant ID (which is public information) and the <code>netId</code> of a user in that tenant.</p>

<p>To demonstrate the vulnerability, here I am using a Guest user in the target tenant to query the <code>netId</code> of a Global Admin. Then I impersonate the Global Admin using the same Actor token, and can perform any action in the tenant as that Global Admin over the Azure AD Graph.</p>

<p>First I craft an impersonation token for a Guest user in my victim tenant:</p>

<p><img src="https://dirkjanm.io/assets/img/actortokens/guesttoken.png" alt="Craft impersonation token for Guest user"></p>

<p>I use this token to query the <code>netId</code> of a Global Admin:</p>

<p><img src="https://dirkjanm.io/assets/img/actortokens/findga.png" alt="Query Global Admin"></p>

<p>Then I create an impersonation token for this Global Admin (the UPN is kept the same since it is not validated by the API):</p>

<p><img src="https://dirkjanm.io/assets/img/actortokens/gaimpersonate.png" alt="Craft impersonation token for Global Admin"></p>

<p>And finally this token is used to access the tenant as the Global Admin, listing the users, something the guest user was not able to do:</p>

<p><img src="https://dirkjanm.io/assets/img/actortokens/queryusers.png" alt="Query data in the tenant"></p>

<p>I can even run roadrecon with this impersonation token, which queries all Azure AD Graph API endpoints to enumerate the available information in the tenant.</p>

<p><img src="https://dirkjanm.io/assets/img/actortokens/runroadrecon.png" alt="Running roadrecon in a tenant with an impersonation token"></p>

<p>None of these actions would generate any logs in the victim tenant.</p>

<h2 id="practical-abuse">Practical abuse</h2>
<p>With this vulnerability it would be possible to compromise any Entra ID tenant. Starting with an Actor token from an attacker controlled tenant, the following steps would lead to full control over the victim tenant:</p>

<ol>
  <li>Find the tenant ID for the victim tenant, this can be done using public APIs based on the domain name.</li>
  <li>Find a valid <code>netId</code> of a regular user in the tenant. Methods for this will be discussed below.</li>
  <li>Craft an impersonation token with the Actor token from the attacker tenant, using the tenant ID and <code>netId</code> of the user in the victim tenant.</li>
  <li>List all Global Admins in the tenant and their <code>netId</code>.</li>
  <li>Craft an impersonation token for the Global Admin account.</li>
  <li>Perform any read or write action over the Azure AD Graph API.</li>
</ol>

<p>If an attacker makes any modifications in the tenant in step 6, that would be the only event in this chain that generates any telemetry in the victim tenant. An attacker could for example create new user accounts, grant these Global Admin privileges and then sign in interactively to any Entra ID, Microsoft 365 or third party application that integrates with the victim tenant. Alternatively they could add credentials on existing applications, grant these apps API permissions and use that to exfiltrate emails or files from Microsoft 365, a technique that is popular among threat actors. An attacker could also add credentials to <a href="https://dirkjanm.io/azure-ad-privilege-escalation-application-admin/">Microsoft Service Principals</a> in the victim tenant, several of which can request Actor tokens that allow impersonation against SharePoint or Exchange. For my DEF CON and Black Hat talks I made a demo video about using these Actor tokens to obtain Global Admin access. The video uses Actor tokens within a tenant, but the same technique could have been applied to any other tenant by abusing this vulnerability.</p>

<video width="100%" controls="">
  <source src="https://dirkjanm.io/assets/raw/demo_graph.mp4" type="video/mp4">
</video>

<h2 id="finding-netids">Finding netIds</h2>
<p>Since tenant IDs can be resolved when the domain name of a tenant is known, the only identifier that is not immediately available to the attacker is a valid <code>netId</code> for a user in that specific tenant. As I mentioned above, these IDs are added to Entra ID access tokens as the <code>puid</code> claim. Any token found online, in screenshots, examples or logs, even those that are long expired or with an obfuscated signature, would provide an attacker with enough information to breach the tenant. Threat actors that still have old tokens for any tenant from previous breaches can immediately access those tenants again as long as the victim account still exists.</p>

<p>The above is probably not a very common occurrence. What is a more realistic attack is simply brute-forcing the <code>netId</code>. Unlike object IDs, which are randomly generated, netIds are actually incremental. Looking at the differences in netIds between my tenant and those of some tenants I analyzed, I found the difference between a newly created user in my tenant and their newest user to be in the range of 100.000 to 100 million. Simply brute forcing the <code>netId</code> could be accomplished in minutes to hours for any target tenant, and the more user exist in a tenant the easier it is to find a match. Since this does not generate any logs it isn’t a noisy attack either. Because of the possibility to brute force these netIds I would say this vulnerability could have been used to take over any tenant without any prerequisites. There is however a third technique which is even more effective (and more fun from a technical level).</p>

<h2 id="compromising-tenants-by-hopping-over-b2b-trusts">Compromising tenants by hopping over B2B trusts</h2>
<p>I previously mentioned that a users <code>netId</code> is used to establish links between a user account in multiple tenants. This is something that I researched a few years ago when I gave a talk at <a href="https://dirkjanm.io/assets/raw/US-22-Mollema-Backdooring-and-hijacking-Azure-AD-accounts_final.pdf">Black Hat USA 22</a> about external identities. The below screenshot is taken from one of my slides, which illustrates this:</p>

<p><img src="https://dirkjanm.io/assets/img/actortokens/guestlink.png" alt="Guest user link based on netid"></p>

<p>The way this works is as follows. Suppose we have tenant A and tenant B. A user in tenant B is invited into tenant A. In the new guest account that is created in tenant A, their <code>netId</code> is stored on the <code>alternativeSecurityIds</code> attribute. That means that an attacker wanting to abuse this bug can simply read that attribute in tenant A, put it in an impersonation token for tenant B and then impersonate the victim in their home tenant. It should be noted that this works <strong>against the direction of invite</strong>. Any user in any tenant where you accept an invite will be able to read your <code>netId</code>, and with this bug could have impersonated you in your home tenant. In your home tenant you have a full user account, which can enumerate other users. This is not a bug or risk with B2B trusts, but is simply an unintended consequence of the B2B design mechanism. A guest account in someone else’s tenant would also be sufficient with the default Entra ID guest settings because the default settings allow users to query the <code>netId</code> of a user as long as the UPN is known.</p>

<p>To abuse this, a threat actor could perform the following steps, given that they have access to at least one tenant with a guest user:</p>

<ol>
  <li>Query the guest users and their <code>alternativeSecurityIds</code> attribute which gives the <code>netId</code>.</li>
  <li>Query the tenant ID of the guest users home tenant based on the domain name in their UPN.</li>
  <li>Create an impersonation token, impersonating the victim in their home tenant.</li>
  <li>Optionally list Global Admins and impersonate those to compromise the entire tenant.</li>
  <li>Repeat step 1 for each tenant that was compromised.</li>
</ol>

<p>The steps above can be done in 2 API calls per tenant, which do not generate any logs. Most tenants will have guest users from multiple distinct other tenants. This means the number of tenants you compromise with this scales exponentially and the information needed to compromise the majority of all tenants worldwide could have been gathered within minutes using a single Actor token. After at least 1 user is known per victim tenant, the attacker can selectively perform post-compromise actions in these tenants by impersonating Global Admins.</p>

<p>Looking at the list of guest users in the tenants of some of my clients, this technique would be extremely powerful. I also observed that one of the first tenants you will likely compromise is Microsoft’s own tenant, since Microsoft consultants often get invited to customer tenants. Many MSPs and Microsoft Partners will have a guest account in the Microsoft tenant, so from the Microsoft tenant a compromise of most major service provider tenants is one step away.</p>

<p>Needless to say, as much as I would have liked to test this technique in practice to see how fast this would spread out, I only tested the individual steps in my own tenants and did not access any data I’m not authorized to.</p>

<h2 id="detection">Detection</h2>
<p>While querying data over the Azure AD Graph does not leave any logs, modifying data does (usually) generate audit logs. If modifications are done with Actor tokens, these logs look a bit curious.</p>

<p><img src="https://dirkjanm.io/assets/img/actortokens/initiatedby.png" alt="Initiated by exchange and a global admin" width="60%"></p>

<p>Since Actor tokens involve both the app and the user being impersonated, it seems Entra ID gets confused about who actually made the change, and it will log the UPN of the impersonated Global Admin, but the display name of Exchange. Luckily for defenders this creates a nice giveaway when Actor tokens are used in the tenant. After some testing and filtering with some fellow researchers that work on the blue side (thanks to Fabian Bader and Olaf Hartong) we came up with the following detection query:</p>

<pre><code>AuditLogs
| where not(OperationName has "group")
| where not(OperationName == "Set directory feature on tenant")
| where InitiatedBy has "user"
| where InitiatedBy.user.displayName has_any ( "Office 365 Exchange Online", "Skype for Business Online", "Dataverse", "Office 365 SharePoint Online", "Microsoft Dynamics ERP")
</code></pre>

<p>The exclusion for group operations is there because some of these products do actually use Actor tokens to perform operations on your behalf. For example creating specific groups via the Exchange Online PowerShell module will make Exchange use an Actor token on your behalf and create the group in Entra ID.</p>

<h2 id="conclusion">Conclusion</h2>
<p>This blog discussed a critical token validation failure in the Azure AD Graph API. While the vulnerability itself was a bad oversight in the token handling, the whole concept of Actor tokens is a protocol that was designed to behave with all the properties mentioned in the paragraphs above. If it weren’t for the complete lack of security measures in these tokens, I don’t think such a big impact with such limited telemetry would have been possible.</p>

<p>Thanks to the people at MSRC who immediately picked up the vulnerability report, searched for potential variants in other resources, and to the engineers who followed up with fixes for the Azure AD Graph and blocked Actor tokens for the Azure AD Graph API requested with credentials stored on Service Principals, essentially restricting the usage of these Actor tokens to only Microsoft internal services.</p>

<h2 id="disclosure-timeline">Disclosure timeline</h2>

<ul>
  <li>July 14, 2025 - reported issue to MSRC.</li>
  <li>July 14, 2025 - MSRC case opened.</li>
  <li>July 15, 2025 - reported further details on the impact.</li>
  <li>July 15, 2025 - MSRC requested to halt further testing of this vulnerability.</li>
  <li>July 17, 2025 - Microsoft pushed a fix for the issue globally into production.</li>
  <li>July 23, 2025 - Issue confirmed as resolved by MSRC.</li>
  <li>August 6, 2025 - Further mitigations pushed out preventing Actor tokens being issued for the Azure AD Graph with SP credentials.</li>
  <li>September 4, 2025 - <a href="https://msrc.microsoft.com/update-guide/vulnerability/CVE-2025-55241">CVE-2025-55241</a> issued.</li>
  <li>September 17, 2025 - Release of this blogpost.</li>
</ul>



        
      </section>

      

      


      
  

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ABC Pulls Jimmy Kimmel Live from the Air 'Indefinitely' (148 pts)]]></title>
            <link>https://www.vulture.com/article/abc-pulls-jimmy-kimmel-live-from-the-air-indefinitely.html</link>
            <guid>45282485</guid>
            <pubDate>Wed, 17 Sep 2025 23:00:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vulture.com/article/abc-pulls-jimmy-kimmel-live-from-the-air-indefinitely.html">https://www.vulture.com/article/abc-pulls-jimmy-kimmel-live-from-the-air-indefinitely.html</a>, See on <a href="https://news.ycombinator.com/item?id=45282485">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-editable="main" data-track-zone="main">  <article role="main" data-track-type="article-detail" data-uri="www.vulture.com/_components/article/instances/cmfojo9bx000j0jik2r8w9h6u@published" data-content-channel="TV" data-crosspost="" data-type="Breaking-News-Original Reporting" data-syndication="original" data-headline="ABC Pulls Jimmy Kimmel Live! From the Air ‘Indefinitely’" data-authors="Josef Adalian" data-publish-date="2025-09-17" data-tags="tv, comedy, late night, jimmy kimmel live!, jimmy kimmel, politics, charlie kirk, vulture homepage lede" data-issue-date="" data-components-count="4" data-canonical-url="http://www.vulture.com/article/abc-pulls-jimmy-kimmel-live-from-the-air-indefinitely.html">


  
  
  
  <header>
    <div>
          

            <p><span data-editable="bylines">
            <p><span>By</span> <span>
        ,
          <span>who has covered the television industry since 1992</span><span>&nbsp;</span>
          <span>and writes Buffering, a newsletter about streaming</span>
      </span></p>

              </span>
          </p>
        </div>
    
  </header>
  <section>
    
    <div id="vulture-zephr-anchor" data-editable="content">
      <div>
          <div>
            <picture> <source media="(min-resolution: 192dpi) and (min-width: 1180px), (-webkit-min-device-pixel-ratio: 2) and (min-width: 1180px)" srcset="https://pyxis.nymag.com/v1/imgs/8c0/d93/983356eab25f0dade8b73751d607a07a7f-jimmy-kimmel.2x.rhorizontal.w700.jpg 2x" width="700" height="467"> <source media="(min-width: 1180px) " srcset="https://pyxis.nymag.com/v1/imgs/8c0/d93/983356eab25f0dade8b73751d607a07a7f-jimmy-kimmel.rhorizontal.w700.jpg" width="700" height="467"> <source media="(min-resolution: 192dpi) and (min-width: 768px), (-webkit-min-device-pixel-ratio: 2) and (min-width: 768px)" srcset="https://pyxis.nymag.com/v1/imgs/8c0/d93/983356eab25f0dade8b73751d607a07a7f-jimmy-kimmel.2x.rhorizontal.w700.jpg 2x" width="700" height="467"> <source media="(min-width: 768px)" srcset="https://pyxis.nymag.com/v1/imgs/8c0/d93/983356eab25f0dade8b73751d607a07a7f-jimmy-kimmel.rhorizontal.w700.jpg" width="700" height="467"> <source media="(min-resolution: 192dpi), (-webkit-min-device-pixel-ratio: 2)" srcset="https://pyxis.nymag.com/v1/imgs/8c0/d93/983356eab25f0dade8b73751d607a07a7f-jimmy-kimmel.2x.rhorizontal.w700.jpg" width="700" height="467"> <img src="https://pyxis.nymag.com/v1/imgs/8c0/d93/983356eab25f0dade8b73751d607a07a7f-jimmy-kimmel.rhorizontal.w700.jpg" data-content-img="" alt="JIMMY KIMMEL" width="700" height="467" fetchpriority="high"> </picture>
          </div>
            <div>
              <p><span>Photo: Randy Holmes/Disney via Getty Images</span>
              </p>
            </div>
              </div>
        <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/cmfojo9bx000i0jik5fd1vt55@published" data-word-count="65">Conservative cancel culture has come for Jimmy Kimmel: Walt Disney–owned ABC has announced it’s pulling new episodes of <em>Jimmy Kimmel Live! </em>“indefinitely” following right-wing outrage over comments he made on his September 15 show about the reaction to the <a href="https://nymag.com/intelligencer/article/charlie-kirk-shooting-at-utah-university-q-and-a-live-updates.html">killing of right-wing podcaster and provocateur Charlie Kirk</a>. Disney’s&nbsp;decision follows a move by one of its major affiliate groups, Nexstar, to preempt the show in response.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/cmfokcub900253b74tnikyb3l@published" data-word-count="133">While Nexstar didn’t say exactly what Kimmel had said that it objected to, and ABC offered no further explanation of its move, FCC chairman Brendan Carr earlier on Wednesday denounced this part of the host’s Monday monologue, <a href="https://deadline.com/2025/09/fcc-jimmy-kimmel-charlie-kirk-suspect-1236547238/">per Deadline</a>: “We had some new lows over the weekend with the MAGA gang desperately trying to characterize this kid who murdered Charlie Kirk as anything other than one of them and with everything they can to score political points from it.” Around 6 p.m. ET Wednesday, Nexstar issued this statement: “Nexstar strongly objects to recent comments made by Mr. Kimmel concerning the killing of Charlie Kirk and will replace the show with other programming in its ABC-affiliated markets.” When Vulture asked ABC for comment, a network rep replied, “<em>Jimmy Kimmel Live!</em> will be preempted indefinitely.”</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/cmfokcub900263b74w1qr3a7v@published" data-word-count="28">Vulture has reached out to Kimmel’s reps for comment and asked Nexstar and ABC for additional clarification of today’s actions. We’ll update this story when we know more.</p>

  


    </div>

      


          



      <span>ABC Pulls <em>Jimmy Kimmel Live!</em> From the Air ‘Indefinitely’</span>



    <dialog>
      <span>
        <svg width="6" height="14" viewBox="0 0 6 14" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M4.84191 13.478C4.84191 13.826 4.64391 14 4.24791 14H1.54791C1.22391 14 1.06191 13.85 1.06191 13.55V10.85C1.06191 10.586 1.17591 10.454 1.40391 10.454H4.51791C4.73391 10.454 4.84191 10.574 4.84191 10.814V13.478ZM4.13991 8.708C4.12791 8.888 4.07391 9.02 3.97791 9.104C3.89391 9.176 3.74991 9.212 3.54591 9.212H2.30391C2.12391 9.212 2.00391 9.176 1.94391 9.104C1.89591 9.032 1.85991 8.918 1.83591 8.762L0.935906 1.058C0.923906 0.926 0.947906 0.823999 1.00791 0.751999C1.07991 0.679999 1.16991 0.643999 1.27791 0.643999H4.67991C4.91991 0.643999 5.02791 0.769999 5.00391 1.022L4.13991 8.708Z" fill="#DB2800"></path>
</svg>

      </span>
      <span></span>
      <span>
        <svg width="14" height="13" viewBox="0 0 14 13" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path fill-rule="evenodd" clip-rule="evenodd" d="M12.9823 1.22855C13.1775 1.03329 13.1775 0.716709 12.9823 0.521447C12.787 0.326184 12.4704 0.326184 12.2751 0.521447L7.00185 5.79474L1.72855 0.521447C1.53329 0.326184 1.21671 0.326184 1.02145 0.521447C0.826184 0.716709 0.826184 1.03329 1.02145 1.22855L6.29474 6.50185L1.02145 11.7751C0.826184 11.9704 0.826184 12.287 1.02145 12.4823C1.21671 12.6775 1.53329 12.6775 1.72855 12.4823L7.00185 7.20896L12.2751 12.4823C12.4704 12.6775 12.787 12.6775 12.9823 12.4823C13.1775 12.287 13.1775 11.9704 12.9823 11.7751L7.70896 6.50185L12.9823 1.22855Z" fill="#DA4022"></path>
</svg>

      </span>
    </dialog>

  </section>
  

</article>

  

</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ABC yanks Jimmy Kimmel's show 'indefinitely' after remarks about Charlie Kirk (454 pts)]]></title>
            <link>https://www.cnn.com/2025/09/17/media/jimmy-kimmel-charlie-kirk-trump-fcc-brendan-carr</link>
            <guid>45282482</guid>
            <pubDate>Wed, 17 Sep 2025 23:00:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnn.com/2025/09/17/media/jimmy-kimmel-charlie-kirk-trump-fcc-brendan-carr">https://www.cnn.com/2025/09/17/media/jimmy-kimmel-charlie-kirk-trump-fcc-brendan-carr</a>, See on <a href="https://news.ycombinator.com/item?id=45282482">Hacker News</a></p>
Couldn't get https://www.cnn.com/2025/09/17/media/jimmy-kimmel-charlie-kirk-trump-fcc-brendan-carr: Error: Request failed with status code 451]]></description>
        </item>
        <item>
            <title><![CDATA[A postmortem of three recent issues (353 pts)]]></title>
            <link>https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues</link>
            <guid>45281139</guid>
            <pubDate>Wed, 17 Sep 2025 20:41:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues">https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues</a>, See on <a href="https://news.ycombinator.com/item?id=45281139">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p>Between August and early September, three infrastructure bugs intermittently degraded Claude's response quality. We've now resolved these issues and want to explain what happened.</p><p>In early August, a number of users began reporting degraded responses from Claude. These initial reports were difficult to distinguish from normal variation in user feedback. By late August, the increasing frequency and persistence of these reports prompted us to open an investigation that led us to uncover three separate infrastructure bugs.</p><p>To state it plainly: We never reduce model quality due to demand, time of day, or server load. The problems our users reported were due to infrastructure bugs alone.</p><p>We recognize users expect consistent quality from Claude, and we maintain an extremely high bar for ensuring infrastructure changes don't affect model outputs. In these recent incidents, we didn't meet that bar. The following postmortem explains what went wrong, why detection and resolution took longer than we would have wanted, and what we're changing to prevent similar future incidents.</p><p>We don't typically share this level of technical detail about our infrastructure, but the scope and complexity of these issues justified a more comprehensive explanation.</p><h2 id="how-we-serve-claude-at-scale">How we serve Claude at scale</h2><p>We serve Claude to millions of users via our first-party API, Amazon Bedrock, and Google Cloud's Vertex AI. We deploy Claude across multiple hardware platforms, namely AWS Trainium, NVIDIA GPUs, and Google TPUs. This approach provides the capacity and geographic distribution necessary to serve users worldwide.</p><p>Each hardware platform has different characteristics and requires specific optimizations. Despite these variations, we have strict equivalence standards for model implementations. Our aim is that users should get the same quality responses regardless of which platform serves their request. This complexity means that any infrastructure change requires careful validation across all platforms and configurations.</p><h2 id="timeline-of-events">Timeline of events</h2><div><figure><img alt="Illustrative timeline of events on the Claude API. Yellow: issue detected, Red: degradation worsened, Green: fix deployed." loading="lazy" width="3840" height="1800" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd707dfc2effceba608d04007bc776132a3e57838-3840x1800.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd707dfc2effceba608d04007bc776132a3e57838-3840x1800.png&amp;w=3840&amp;q=75"><figcaption>Illustrative timeline of events on the <strong>Claude API</strong>. Yellow: issue detected, Red: degradation worsened, Green: fix deployed.</figcaption></figure></div><p>The overlapping nature of these bugs made diagnosis particularly challenging. The first bug was introduced on August 5, affecting approximately 0.8% of requests made to Sonnet 4. Two more bugs arose from deployments on August 25 and 26.</p><p>Although initial impacts were limited, a load balancing change on August 29 started to increase affected traffic. This caused many more users to experience issues while others continued to see normal performance, creating confusing and contradictory reports.</p><h2 id="three-overlapping-issues">Three overlapping issues</h2><p>Below we describe the three bugs that caused the degradation, when they occurred, and how we resolved them:</p><h3 id="1-context-window-routing-error">1. Context window routing error</h3><p>On August 5, some Sonnet 4 requests were misrouted to servers configured for the upcoming <a href="https://docs.claude.com/en/docs/build-with-claude/context-windows#1m-token-context-window">1M token</a> <a href="https://docs.claude.com/en/docs/build-with-claude/context-windows">context window</a>. This bug initially affected 0.8% of requests. On August 29, a routine load balancing change unintentionally increased the number of short-context requests routed to the 1M context servers. At the worst impacted hour on August 31, 16% of Sonnet 4 requests were affected.</p><p>Approximately 30% of Claude Code users who made requests during this period had at least one message routed to the wrong server type, resulting in degraded responses. On Amazon Bedrock, misrouted traffic peaked at 0.18% of all Sonnet 4 requests from August 12. Incorrect routing affected less than 0.0004% of requests on Google Cloud's Vertex AI between August 27 and September 16.</p><p>However, some users were affected more severely, as our routing is "sticky". This meant that once a request was served by the incorrect server, subsequent follow-ups were likely to be served by the same incorrect server.</p><p><strong>Resolution:</strong> We fixed the routing logic to ensure short- and long-context requests were directed to the correct server pools. We deployed the fix on September 4. A rollout to our first-party platforms and Google Cloud’s Vertex was completed by September 16. The fix is in the process of being rolled out on Bedrock.</p><h3 id="2-output-corruption">2. Output corruption</h3><p>On August 25, we deployed a misconfiguration to the Claude API TPU servers that caused an error during token generation. An issue caused by a runtime performance optimization occasionally assigned a high probability to tokens that should rarely be produced given the context, for example producing Thai or Chinese characters in response to English prompts, or producing obvious syntax errors in code. A small subset of users that asked a question in English might have seen "สวัสดี" in the middle of the response, for example.</p><p>This corruption affected requests made to Opus 4.1 and Opus 4 on August 25-28, and requests to Sonnet 4 August 25–September 2. Third-party platforms were not affected by this issue.</p><p><strong>Resolution:</strong> We identified the issue and rolled back the change on September 2. We've added detection tests for unexpected character outputs to our deployment process.</p><h3 id="3-approximate-top-k-xlatpu-miscompilation">3. Approximate top-k XLA:TPU miscompilation</h3><p>On August 25, we deployed code to improve how Claude selects tokens during text generation. This change inadvertently triggered a latent bug in the XLA:TPU<sup>[1] </sup>compiler, which has been confirmed to affect requests to Claude Haiku 3.5.</p><p>We also believe this could have impacted a subset of Sonnet 4 and Opus 3 on the Claude API. Third-party platforms were not affected by this issue.</p><p><strong>Resolution:</strong> We first observed the bug affecting Haiku 3.5 and rolled it back on September 4. We later noticed user reports of problems with Opus 3 that were compatible with this bug, and rolled it back on September 12. After extensive investigation we were unable to reproduce this bug on Sonnet 4 but decided to also roll it back out of an abundance of caution.</p><p>Simultaneously, we have (a) been working with the XLA:TPU team on a fix for the compiler bug and (b) rolled out a fix to use exact top-k with enhanced precision. For details, see the deep dive below.</p><h2 id="a-closer-look-at-the-xla-compiler-bug">A closer look at the XLA compiler bug</h2><p>To illustrate the complexity of these issues, here's how the XLA compiler bug manifested and why it proved particularly challenging to diagnose.</p><p>When Claude generates text, it calculates probabilities for each possible next word, then randomly chooses a sample from this probability distribution. We use "top-p sampling" to avoid nonsensical outputs—only considering words whose cumulative probability reaches a threshold (typically 0.99 or 0.999). On TPUs, our models run across multiple chips, with probability calculations happening in different locations. To sort these probabilities, we need to coordinate data between chips, which is complex.<sup>[2]</sup></p><p>In December 2024, we discovered our TPU implementation would occasionally drop the most probable token when <a href="https://docs.claude.com/en/docs/about-claude/glossary#temperature">temperature</a> was zero. We deployed a workaround to fix this case.</p><div><figure><img alt="Code snippet of a December 2024 patch to work around the unexpected dropped token bug when temperature = 0." loading="lazy" width="2000" height="500" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fefee0d3d25f6b03cbfc57e70e0e364dcd8b82fe0-2000x500.png&amp;w=2048&amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fefee0d3d25f6b03cbfc57e70e0e364dcd8b82fe0-2000x500.png&amp;w=3840&amp;q=75 2x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fefee0d3d25f6b03cbfc57e70e0e364dcd8b82fe0-2000x500.png&amp;w=3840&amp;q=75"><figcaption>Code snippet of a December 2024 patch to work around the unexpected dropped token bug when temperature = 0.</figcaption></figure></div><p>The root cause involved mixed precision arithmetic. Our models compute next-token probabilities in <a href="https://github.com/tensorflow/tensorflow/blob/f41959ccb2d9d4c722fe8fc3351401d53bcf4900/tensorflow/core/framework/bfloat16.h">bf16</a> (16-bit floating point). However, the vector processor is <a href="https://dl.acm.org/doi/pdf/10.1145/3360307">fp32-native</a>, so the TPU compiler (XLA) can optimize runtime by converting some operations to fp32 (32-bit). This optimization pass is guarded by the <code>xla_allow_excess_precision</code> flag which defaults to true.</p><p>This caused a mismatch: operations that should have agreed on the highest probability token were running at different precision levels. The precision mismatch meant they didn't agree on which token had the highest probability. This caused the highest probability token to sometimes disappear from consideration entirely.</p><p>On August 26, we deployed a rewrite of our sampling code to fix the precision issues and improve how we handled probabilities at the limit that reach the top-p threshold. But in fixing these problems, we exposed a trickier one.</p><div><figure><img alt="Code snippet showing minimized reproducer merged as part of the August 11 change that root-caused the “bug” being worked around in December 2024; in reality, it’s expected behavior of the xla_allow_excess_precision flag." loading="lazy" width="2000" height="2560" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F6d10e58c0bd5fd7cb03dc0adc716cb1e4f039343-2000x2560.png&amp;w=2048&amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F6d10e58c0bd5fd7cb03dc0adc716cb1e4f039343-2000x2560.png&amp;w=3840&amp;q=75 2x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F6d10e58c0bd5fd7cb03dc0adc716cb1e4f039343-2000x2560.png&amp;w=3840&amp;q=75"><figcaption>Code snippet showing a minimized reproducer merged as part of the August 11 change that root-caused the "bug" being worked around in December 2024. In reality, it’s expected behavior of the <code>xla_allow_excess_precision</code> flag.</figcaption></figure></div><p>Our fix removed the December workaround because we believed we'd solved the root cause. This led to a deeper bug in the <a href="https://docs.jax.dev/en/latest/_autosummary/jax.lax.approx_max_k.html">approximate top-k</a> operation—a performance optimization that quickly finds the highest probability tokens.<sup>[3]</sup> This approximation sometimes returned completely wrong results, but only for certain batch sizes and model configurations. The December workaround had been inadvertently masking this problem.</p><div><figure><img alt="Slack message showing reproducer of the underlying approximate top-k bug shared with the XLA:TPU engineers who developed the algorithm. The code returns correct results when run on CPUs." loading="lazy" width="2400" height="1404" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7e42db934d0e84ea40fc56b416ddb09b2097a5ff-2400x1404.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7e42db934d0e84ea40fc56b416ddb09b2097a5ff-2400x1404.png&amp;w=3840&amp;q=75"><figcaption>Reproducer of the underlying approximate top-k bug shared with the XLA:TPU engineers who <a href="https://arxiv.org/pdf/2206.14286">developed the algorithm</a>. The code returns correct results when run on CPUs.</figcaption></figure></div><p>The bug's behavior was frustratingly inconsistent. It changed depending on unrelated factors such as what operations ran before or after it, and whether debugging tools were enabled. The same prompt might work perfectly on one request and fail on the next.</p><p>While investigating, we also discovered that the exact top-k operation no longer had the prohibitive performance penalty it once did. We switched from approximate to exact top-k and standardized some additional operations on fp32 precision.<sup>[4]</sup> Model quality is non-negotiable, so we accepted the minor efficiency impact.</p><h2 id="why-detection-was-difficult">Why detection was difficult</h2><p>Our validation process ordinarily relies on benchmarks alongside safety evaluations and performance metrics. Engineering teams perform spot checks and deploy to small "canary" groups first.</p><p>These issues exposed critical gaps that we should have identified earlier. The evaluations we ran simply didn't capture the degradation users were reporting, in part because Claude often recovers well from isolated mistakes. Our own privacy practices also created challenges in investigating reports. Our internal privacy and security controls limit how and when engineers can access user interactions with Claude, in particular when those interactions are not reported to us as feedback. This protects user privacy but prevents engineers from examining the problematic interactions needed to identify or reproduce bugs.</p><p>Each bug produced different symptoms on different platforms at different rates. This created a confusing mix of reports that didn't point to any single cause. It looked like random, inconsistent degradation.</p><p>More fundamentally, we relied too heavily on noisy evaluations. Although we were aware of an increase in reports online, we lacked a clear way to connect these to each of our recent changes. When negative reports spiked on August 29, we didn't immediately make the connection to an otherwise standard load balancing change.</p><h2 id="what-were-changing">What we're changing</h2><p>As we continue to improve our infrastructure, we're also improving the way we evaluate and prevent bugs like those discussed above across all platforms where we serve Claude. Here's what we're changing:</p><ul><li><strong>More sensitive evaluations:</strong> To help discover the root cause of any given issue, we’ve developed evaluations that can more reliably differentiate between working and broken implementations. We’ll keep improving these evaluations to keep a closer eye on model quality.</li><li><strong>Quality evaluations in more places:</strong> Although we run regular evaluations on our systems, we will run them continuously on true production systems to catch issues such as the context window load balancing error.</li><li><strong>Faster debugging tooling:</strong> We'll develop infrastructure and tooling to better debug community-sourced feedback without sacrificing user privacy. Additionally, some bespoke tools developed here will be used to reduce the remediation time in future similar incidents, if those should occur.</li></ul><p>Evals and monitoring are important. But these incidents have shown that we also need continuous signal from users when responses from Claude aren't up to the usual standard. Reports of specific changes observed, examples of unexpected behavior encountered, and patterns across different use cases all helped us isolate the issues.</p><p>It remains particularly helpful for users to continue to send us their feedback directly. You can use the <code>/bug</code> command in Claude Code or you can use the "thumbs down" button in the Claude apps to do so. Developers and researchers often create new and interesting ways to evaluate model quality that complement our internal testing. If you'd like to share yours, reach out to <a href="mailto:feedback@anthropic.com">feedback@anthropic.com</a>.</p><p>We remain grateful to our community for these contributions.</p><p><sup>[1]</sup> XLA:TPU is the optimizing compiler that translates <a href="https://openxla.org/xla/architecture">XLA</a> High Level Optimizing language—often written using <a href="https://docs.jax.dev/en/latest">JAX</a>—to TPU machine instructions.</p><p><sup>[2]</sup> Our models are too large for single chips and are partitioned across tens of chips or more, making our sorting operation a distributed sort. TPUs (just like GPUs and Trainium) also have different performance characteristics than CPUs, requiring different implementation techniques using vectorized operations instead of serial algorithms.</p><p><sup>[3]</sup> We had been using this approximate operation because it yielded substantial performance improvements. The approximation works by accepting potential inaccuracies in the lowest probability tokens, which shouldn't affect quality—except when the bug caused it to drop the highest probability token instead.</p><p><sup>[4]</sup> Note that the now-correct top-k implementation may result in slight differences in the inclusion of tokens near the top-p threshold, and in rare cases users may benefit from re-tuning their choice of top-p.</p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Famous cognitive psychology experiments that failed to replicate (161 pts)]]></title>
            <link>https://buttondown.com/aethermug/archive/aether-mug-famous-cognitive-psychology/</link>
            <guid>45279898</guid>
            <pubDate>Wed, 17 Sep 2025 18:55:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://buttondown.com/aethermug/archive/aether-mug-famous-cognitive-psychology/">https://buttondown.com/aethermug/archive/aether-mug-famous-cognitive-psychology/</a>, See on <a href="https://news.ycombinator.com/item?id=45279898">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main components="[object Object]"><p><em>TL;DR is the part in bold below.</em></p>
<p>The field of psychology had a big crisis in the 2010s, when many widely accepted results turned out to be much less solid than previously thought. It's called the <a href="https://en.wikipedia.org/w/index.php?title=Replication_crisis" rel="nofollow noopener noreferrer" target="_blank">replication crisis</a>, because labs around the world tried and failed to replicate, in new experiments, previous results published by their original "discoverers". In other words, many reported psychological effects were either non-existent—artifacts of the experimenter's flawed setup—or so much weaker than originally claimed that they lost most of their intellectual sparkle.</p>
<p>(The crisis spanned other fields as well, but I mostly care about psychology here, especially the cognitive kind.)</p>
<p>This is very old news, and I've been vaguely aware of several of the biggest disgraced results for years, but I keep on forgetting which are (still probably) real and which aren't. This is not good. <em>Most</em> results in the field do actually replicate and are robust<sup>[citation needed]</sup>, so it would be a pity to lose confidence in the whole field just because of a few bad apples.</p>
<p><strong>This post is a compact reference list of the most (in)famous cognitive science results that failed to replicate and should, for the time being, be considered false.</strong> The only goal is to offset the trust-undermining effects of my poor memory—and perhaps yours, too?—with a bookmarkable page.</p>
<p>This can't be a comprehensive list: if a study is <em>not</em> on this page, it's not guaranteed to be fully replicated. Still, this should cover most of the high-profile debunked theories that laypeople like me may have heard of.</p>
<p><em>Credit: I enlisted the help of Kimi K2, o3, and Sonnet 4 to gather and fact-check this list. I also checked, pruned, and de-hallucinated all the results.</em></p>

<h3>Ego Depletion Effect</h3>
<ul>
<li><strong>Claimed result:</strong> We have a "willpower battery" that gradually depletes during the day as we exercise self-control. (I remember reading Baumeister's pop-science book and being awed by the implications of their findings; I might have known it sounded too good to be true.)</li>
<li><strong>Representative paper:</strong> <a href="https://psycnet.apa.org/doiLanding?doi=10.1037%2F0022-3514.74.5.1252" rel="nofollow noopener noreferrer" target="_blank">Baumeister et al. 1998</a></li>
<li><strong>Replication status:</strong> <em>did not replicate</em></li>
<li><strong>Source:</strong> <a href="https://journals.sagepub.com/doi/10.1177/1745691616652873" rel="nofollow noopener noreferrer" target="_blank">Hagger et (63!) al. 2016</a></li>
</ul>
<h3>Power Posing Effect</h3>
<ul>
<li><strong>Claimed result:</strong> Adopting expansive body postures for 2 minutes (like standing with hands on hips or arms raised) increases testosterone, decreases cortisol, and makes people feel more powerful and take more risks.</li>
<li><strong>Representative paper:</strong> <a href="https://doi.org/10.1177/0956797610383437" rel="nofollow noopener noreferrer" target="_blank">Carney, Cuddy, &amp; Yap (2010)</a></li>
<li><strong>Replication status:</strong> <em>did not replicate</em></li>
<li><strong>Source:</strong> <a href="https://doi.org/10.1177/0956797614553946" rel="nofollow noopener noreferrer" target="_blank">Ranehill et al. (2015)</a></li>
</ul>
<h3>Social Priming: Elderly Words Effect</h3>
<ul>
<li><strong>Claimed result:</strong> People walk more slowly after being exposed to words related to elderly stereotypes.</li>
<li><strong>Representative paper:</strong> <a href="https://doi.org/10.1037/0022-3514.71.2.230" rel="nofollow noopener noreferrer" target="_blank">Bargh, Chen, &amp; Burrows (1996)</a></li>
<li><strong>Replication status:</strong> <em>did not replicate</em></li>
<li><strong>Source:</strong> <a href="https://doi.org/10.1371/journal.pone.0029081" rel="nofollow noopener noreferrer" target="_blank">Doyen et al. (2012)</a> (I like how they prove that the psychological effect was actually in the experimenters, rather than the subjects!)</li>
</ul>
<h3>Money Priming Effect</h3>
<ul>
<li><strong>Claimed result:</strong> Simply thinking about money makes you more selfish and more likely to endorse free market values.</li>
<li><strong>Representative paper:</strong> <a href="https://doi.org/10.1126/science.1132491" rel="nofollow noopener noreferrer" target="_blank">Vohs, Mead, &amp; Goode (2006)</a></li>
<li><strong>Replication status:</strong> <em>did not replicate</em></li>
<li><strong>Source:</strong> <a href="https://pubmed.ncbi.nlm.nih.gov/26214168/" rel="nofollow noopener noreferrer" target="_blank">Rohrer, Pashler, &amp; Harris (2015)</a></li>
</ul>
<h3>ESP Precognition Effect</h3>
<ul>
<li><strong>Claimed result:</strong> In some cases, people can predict future events "that could not otherwise be anticipated through any known inferential process".</li>
<li><strong>Representative paper:</strong> <a href="https://doi.org/10.1037/a0021524" rel="nofollow noopener noreferrer" target="_blank">Bem (2011)</a></li>
<li><strong>Replication status:</strong> <em>did not replicate</em></li>
<li><strong>Source:</strong> <a href="https://doi.org/10.1037/a0029709" rel="nofollow noopener noreferrer" target="_blank">Galak et al. (2012)</a>, <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0033423" rel="nofollow noopener noreferrer" target="_blank">Ritchie, Wiseman, &amp; French (2012)</a></li>
</ul>
<h3>Cleanliness and Morality Effect</h3>
<ul>
<li><strong>Claimed result:</strong> Being clean or thinking about cleanliness makes people more morally lax.</li>
<li><strong>Representative paper:</strong> <a href="https://doi.org/10.1111/j.1467-9280.2008.02227.x" rel="nofollow noopener noreferrer" target="_blank">Schnall, Benton, &amp; Harvey (2008)</a></li>
<li><strong>Replication status:</strong> <em>did not replicate</em></li>
<li><strong>Source:</strong> <a href="https://doi.org/10.1027/1864-9335/a000186" rel="nofollow noopener noreferrer" target="_blank">Johnson, Cheung, &amp; Donnellan (2014)</a></li>
</ul>
<h3>Glucose and Ego Depletion Effect</h3>
<ul>
<li><strong>Claimed result:</strong> Connected to the debunked ego-depletion effect, this one claims that adding glucose to your blood "recharges" the willpower battery. (For a while, I may have drunk more orange juice than usual after reading Baumeister's book. At least it's healthy-ish.)</li>
<li><strong>Representative paper:</strong> <a href="https://doi.org/10.1037/0022-3514.92.2.325" rel="nofollow noopener noreferrer" target="_blank">Gailliot &amp; Baumeister (2007)</a></li>
<li><strong>Replication status:</strong> <em>did not replicate</em></li>
<li><strong>Source:</strong> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0195666313005072" rel="nofollow noopener noreferrer" target="_blank">Lange &amp; Eggert (2014)</a></li>
</ul>
<h3>Hunger and Risk-Taking Effect</h3>
<ul>
<li><strong>Claimed result:</strong> People exposed to the scent of freshly baked cookies become less sensitive to risk and take more risks to obtain food.</li>
<li><strong>Representative paper:</strong> <a href="https://onlinelibrary.wiley.com/doi/10.1002/bdm.520" rel="nofollow noopener noreferrer" target="_blank">Ditto et al. 2006</a></li>
<li><strong>Replication status:</strong> <em>did not replicate</em></li>
<li><strong>Source:</strong> <a href="https://doi.org/10.1016/j.foodqual.2018.02.014" rel="nofollow noopener noreferrer" target="_blank">Festjens, Bruyneel, &amp; Dewitte (2018)</a></li>
</ul>
<h3>Psychological Distance &amp; Construal Level Theory</h3>
<ul>
<li><strong>Claimed result</strong>: "Psychologically distant" events are processed more abstractly, while "psychologically near" events are processed more concretely. E.g., you worry about the difficulty of a task if you have to do it tomorrow, but you see the same task's attractive side if it is planned far in the future.</li>
<li><strong>Representative paper</strong>: <a href="https://pubmed.ncbi.nlm.nih.gov/20438233/" rel="nofollow noopener noreferrer" target="_blank">Trope &amp; Liberman (2010)</a>, building on <a href="https://nyuscholars.nyu.edu/en/publications/the-role-of-feasibility-and-desirability-considerations-in-near-a" rel="nofollow noopener noreferrer" target="_blank">Liberman &amp; Trope (1998)</a></li>
<li><strong>Replication status</strong>: <em>serious credibility problems</em></li>
<li><strong>Source</strong>: A <a href="https://climr.org/about/" rel="nofollow noopener noreferrer" target="_blank">collaboration</a> between 73 labs around the world is vetting this theory right now because of many doubts about its validity.</li>
</ul>
<h3>Ovulation &amp; Mate Preferences Effect</h3>
<ul>
<li><strong>Claimed result:</strong> Women are more attracted to hot guys during high-fertility days of their cycles.</li>
<li><strong>Representative paper:</strong> <a href="https://doi.org/10.1037/a0035438" rel="nofollow noopener noreferrer" target="_blank">Gildersleeve, Haselton, &amp; Fales (2014)</a></li>
<li><strong>Replication status:</strong> <em>did not replicate</em></li>
<li><strong>Source:</strong> <a href="https://publications.goettingen-research-online.de/bitstream/2/77327/1/10.1177_0956797619882022.pdf" rel="nofollow noopener noreferrer" target="_blank">Stern, Gerlach, &amp; Penke (2020)</a></li>
</ul>
<h3>Marshmallow Test &amp; Long-Term Success Effect</h3>
<ul>
<li><strong>Claimed result:</strong> Children's ability to resist eating a marshmallow when left alone in a room at age 4-5 strongly predicts adolescent achievement, with those who waited longer showing better life outcomes.</li>
<li><strong>Representative paper:</strong> <a href="https://doi.org/10.1037/0012-1649.26.6.978" rel="nofollow noopener noreferrer" target="_blank">Shoda, Mischel, &amp; Peake (1990)</a></li>
<li><strong>Replication status:</strong> <em>did not replicate significantly</em></li>
<li><strong>Source:</strong> <a href="https://doi.org/10.1177/0956797618761661" rel="nofollow noopener noreferrer" target="_blank">Watts, Duncan, &amp; Quan (2018)</a></li>
</ul>
<h3>Stereotype Threat (Women's Math Performance) Effect</h3>
<ul>
<li><strong>Claimed result:</strong> Women risk being judged by the negative stereotype that women have weaker math ability, and this apprehension disrupts their math performance on difficult tests.</li>
<li><strong>Representative paper:</strong> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0022103198913737" rel="nofollow noopener noreferrer" target="_blank">Spencer, Steele, &amp; Quinn (1999)</a></li>
<li><strong>Replication status:</strong> <em>did not replicate</em></li>
<li><strong>Source:</strong> <a href="https://doi.org/10.1016/j.jsp.2014.10.002" rel="nofollow noopener noreferrer" target="_blank">Flore &amp; Wicherts (2015)</a></li>
</ul>
<h3>Smile to Feel Better Effect</h3>
<ul>
<li><strong>Claimed result</strong>: Holding a pen in your teeth (forcing a smile-like expression) makes you rate cartoons as funnier compared to holding a pen with your lips (preventing smiling). More broadly, facial expressions can influence emotional experiences: "fake it till you make it."</li>
<li><strong>Representative paper</strong>: <a href="https://psycnet.apa.org/record/1988-25514-001" rel="nofollow noopener noreferrer" target="_blank">Strack, Martin, &amp; Stepper (1988)</a></li>
<li><strong>Replication status</strong>: <em>did not replicate</em></li>
<li><strong>Source</strong>: <a href="https://journals.sagepub.com/doi/full/10.1177/1745691616674458" rel="nofollow noopener noreferrer" target="_blank">Wagenmakers et (54!) al. (2016)</a></li>
</ul>
<h3>Objective Measurement of Biases</h3>
<ul>
<li><strong>Claimed result</strong>: You can predict if someone is racist by how quickly they answer certain trick questions.</li>
<li><strong>Representative paper</strong>: <a href="https://psycnet.apa.org/doiLanding?doi=10.1037%2F0022-3514.74.6.1464" rel="nofollow noopener noreferrer" target="_blank">Greenwald, McGhee, &amp; Schwartz (1998)</a></li>
<li><strong>Replication status</strong>: <em>mixed evidence with small effects</em></li>
<li><strong>Source</strong>: <a href="https://pubmed.ncbi.nlm.nih.gov/23773046/" rel="nofollow noopener noreferrer" target="_blank">Oswald et al. (2013)</a> shows that the prediction power is small at best.</li>
</ul>
<h3>Mozart Effect</h3>
<ul>
<li><strong>Claimed result</strong>: Listening to Mozart temporarily makes you smarter.</li>
<li><strong>Representative paper</strong>: <a href="https://www.nature.com/articles/365611a0" rel="nofollow noopener noreferrer" target="_blank">Rauscher, Shaw, &amp; Ky (1993)</a></li>
<li><strong>Replication status</strong>: <em>did not replicate</em></li>
<li><strong>Source</strong>: <a href="https://www.sciencedirect.com/science/article/abs/pii/S0160289610000267" rel="nofollow noopener noreferrer" target="_blank">Pietschnig, Voracek, &amp; Formann (2010)</a> (What a title!)</li>
</ul>
<h3>Growth Mindset Interventions</h3>
<ul>
<li><strong>Claimed result:</strong> Teaching students that intelligence is malleable (not fixed) dramatically improves academic performance.</li>
<li><strong>Representative paper:</strong> <a href="https://psycnet.apa.org/doiLanding?doi=10.1037%2F0033-295X.95.2.256" rel="nofollow noopener noreferrer" target="_blank">Dweck, &amp; Leggett (1988)</a></li>
<li><strong>Replication status:</strong> <em>mixed results</em> - many failed replications but also some successful replications</li>
<li><strong>Failed replication source:</strong> <a href="https://pubmed.ncbi.nlm.nih.gov/31464486/" rel="nofollow noopener noreferrer" target="_blank">Li &amp; Bates 2019</a></li>
<li><strong>Notable successful replication:</strong> <a href="https://www.nature.com/articles/s41586-019-1466-y" rel="nofollow noopener noreferrer" target="_blank">Yeager et al. 2019 in Nature</a></li>
</ul>
<h3>Bilinguals Are Smarter</h3>
<ul>
<li><strong>Claimed result:</strong> Being bilingual provides substantial cognitive advantages in attention, task-switching, and executive control.</li>
<li><strong>Representative paper:</strong> <a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(12)00056-3" rel="nofollow noopener noreferrer" target="_blank">Bialystok, Craik, &amp; Luk (2012)</a></li>
<li><strong>Replication status:</strong> <em>did not replicate</em></li>
<li><strong>Source:</strong> <a href="https://pubmed.ncbi.nlm.nih.gov/29494195/" rel="nofollow noopener noreferrer" target="_blank">Lehtonen et al. 2018</a></li>
</ul>
<p>Did I miss any famous debunked studies? Let me know by replying to this newsletter, and I'll add it to the list. ●</p>
<div><p>Cover image:</p><p><em>Photo by Rebecca Freeman, Unsplash</em></p></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Optimizing ClickHouse for Intel's 280 core processors (211 pts)]]></title>
            <link>https://clickhouse.com/blog/optimizing-clickhouse-intel-high-core-count-cpu</link>
            <guid>45279792</guid>
            <pubDate>Wed, 17 Sep 2025 18:46:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://clickhouse.com/blog/optimizing-clickhouse-intel-high-core-count-cpu">https://clickhouse.com/blog/optimizing-clickhouse-intel-high-core-count-cpu</a>, See on <a href="https://news.ycombinator.com/item?id=45279792">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><blockquote>
<p>This is a guest post from Jiebin Sun, Zhiguo Zhou, Wangyang Guo and Tianyou Li, performance optimization engineers at Intel Shanghai.</p>
</blockquote>
<p>Intel's latest processor generations are pushing the number of cores in a server to unprecedented levels - from 128 P-cores per socket in Granite Rapids to 288 E-cores per socket in Sierra Forest, with future roadmaps targeting 200+ cores per socket. These numbers multiply on multi-socket systems, such servers may consist of 400 and more cores. The paradigm of "more, not faster cores" is driven by physical limitations. Since the end of Dennard scaling in the mid-2000s, power density concerns made it increasingly difficult to push single-thread performance further.</p>
<p>For analytical databases like ClickHouse, ultra-high core counts represent a huge opportunity and a complex challenge at the same time. While more cores theoretically mean more power to process tasks in parallel, most databases struggle to utilize the available hardware fully. Bottlenecks for parallel processing  like lock contention, cache coherence, non-uniform memory access (NUMA), memory bandwidth, and coordination overhead become significantly worse as the core count increases.</p>

<p>Over the past three years, I dedicated a part of my professional life to understand and optimize ClickHouse's scalability on Intel Xeon ultra-high core count processors. My work focused on using various profiling and analysis tools - including perf, emon, and Intel VTune - to analyze all 43 ClickBench queries on ultra-high core count servers systematically, identifying bottlenecks, and optimizing the ClickHouse accordingly.</p>
<p>The results have been exciting: individual optimizations routinely deliver speedups of multiple times for individual queries, in some cases up to 10x. The geometric mean of all 43 ClickBench queries consistently improved between 2% and 10% per optimization. The results demonstrate that ClickHouse can be made scale very well on ultra-high core count systems.</p>

<p>Beyond single-thread performance, several key challenges must be addressed to optimize performance in ultra-high core count systems.</p>
<ol>
<li><strong>Cache coherence overhead</strong>: Bouncing cache lines costs CPU cycles.</li>
<li><strong>Lock contention</strong>: Amdahl's Law becomes brutal for serialized code sections as little as 1% of the overall code.</li>
<li><strong>Memory bandwidth</strong>: Utilizing the memory bandwidth effectively is a persistent challenge for data-intensive systems. Proper memory reuse, management and caching becomes critical.</li>
<li><strong>Thread coordination</strong>: The cost of synchronizing threads grows super-linearly with the number of threads.</li>
<li><strong>NUMA effects</strong>: The memory latency and bandwidth on multi-socket systems differs for local or remote memory.</li>
</ol>
<p>This blog post summarizes our optimizations for ClickHouse on ultra-high core count servers. All of them were merged into the main codeline and they now help to speed up queries in ClickHouse deployments around the globe.</p>
<p><strong>Hardware setup</strong>: Our work was conducted on Intel's latest generation platforms, including 2 x 80 vCPUs Ice Lake (ICX), 2 x 128 vCPUs Sapphire Rapids (SPR), 1 x 288 vCPUs Sierra Forest (SRF), and 2 x 240 vCPUs Granite Rapids (GNR). SMT (Hyper-threading) was enabled, except on SRF which doesn't support SMT, and high-memory-bandwidth configurations.</p>
<p><strong>Software setup</strong>: We used perf, Intel VTune, pipeline visualization, and other custom profiling infrastructure.</p>

<p>Through a systematic analysis of ClickHouse's performance on ultra-high core count systems, I identified five areas with a high potential for optimization. Each area addresses a different aspect of scalability, and together they form a comprehensive approach to unlocking the full potential of ultra-high core count systems.</p>
<p>My journey began with the most fundamental challenge: lock contention.</p>
<h2 id="bottleneck-1-lock-contention"><strong>Bottleneck 1: Lock contention</strong> </h2>
<p>According to queue theory, if N threads compete for the same lock, the cycles grows quadratically (N^2). For example, if we go from 8 to 80 cores, lock wait times increase by (80/8)² = 100x. Furthermore, cache coherence traffic for the mutex itself grows linearly with the core count, and the overhead for context switching compounds the problem. In such settings, every mutex becomes a potential scalability obstacle, and seemingly innocent synchronization patterns can bring entire systems to their knee.</p>
<p>The key insight is that lock contention isn't just about removing locks - it's about rethinking more fundamentally how threads coordinate and share state. This requires a multi-pronged approach: reducing the duration of critical sections, replacing exclusive locks (mutexes) with more granular synchronization primitives, and in some cases, eliminating shared state entirely.</p>

<p>After resolving jemalloc page faults (an optimization detailed below), a new hotspot appeared in <code>native_queued_spin_lock_slowpath</code> which consumed 76% of the CPU time. This function was called from <code>QueryConditionCache::write</code> on 2×240 vCPU systems.</p>
<p><strong>What is the query condition cache?</strong></p>
<p><a href="https://clickhouse.com/docs/operations/query-condition-cache">ClickHouse’s query condition cache</a> stores the results of WHERE filters, enabling the database to skip irrelevant data. In each SELECT query, multiple threads check if cache entries must be updated based on different criteria:</p>
<ul>
<li>the hash of the filter condition (as cache key)</li>
<li>the read mark ranges</li>
<li>whether the currently read part has a final mark</li>
</ul>
<p>The query condition cache is read-heavy, i.e. there are far more reads than writes, but the original implementation used exclusive locking for all operations.</p>
<p><strong>Reducing critical paths in read-heavy workloads</strong></p>
<p>This optimization demonstrates the importance of reducing the time spent holding locks, especially write locks in read-heavy code.</p>
<p>With 240 threads within a single query, the original code created a perfect storm:</p>
<ol>
<li><strong>Unnecessary write locks</strong>: All threads acquired exclusive locks, even when they only read cache entries.</li>
<li><strong>Long critical sections</strong>: Expensive updates of cache entries were performed inside exclusive locks.</li>
<li><strong>Redundant work</strong>: Multiple threads updated the same cache entries potentially multiple times.</li>
</ol>
<p>Our optimization uses <a href="https://en.wikipedia.org/wiki/Double-checked_locking">double-checked locking</a> with atomic operations to resolve these bottlenecks:</p>
<ol>
<li>The code now first checks with atomic reads (no locking), respectively under a shared lock if an update is needed at all (fast path).</li>
<li>Next, the code checks immediately after acquiring an exclusive lock (slow path) if an update is actually required - another thread may have performed the same update in the meantime.</li>
</ol>
<p><strong>Implementation</strong></p>
<p>Based on <a href="https://github.com/ClickHouse/ClickHouse/pull/80247/files">PR #80247</a>, the optimization introduces a fast path which checks if an update is needed before acquiring the expensive write lock.</p>
<pre><code><span>/// Original code</span>
<span>void</span> <span>updateCache</span><span>(mark_ranges, has_final_mark)</span>
{
    acquire_exclusive_lock(cache_mutex);  <span>/// 240 threads wait here!</span>

    <span>/// Always update marks, even if already in desired state</span>
    <span>for</span> (<span>const</span> <span>auto</span> &amp; range : mark_ranges)
        set_marks_to_false(range.begin, range.end);

    <span>if</span> (has_final_mark):
        set_final_mark_to_false();

    release_lock(cache_mutex);
}
</code></pre>
<pre><code>
<span>/// Optimized code</span>
<span>void</span> <span>updateCache</span><span>(mark_ranges, has_final_mark)</span>
{
    <span>/// Fast path: Check if update is needed with a cheap shared lock</span>
    acquire_shared_lock(cache_mutex);  <span>/// Multiple threads can read simultaneously</span>

    need_update = <span>false</span>;
    <span>for</span> (<span>const</span> <span>auto</span> &amp; range : mark_ranges)
    {
        <span>if</span> (any_marks_are_true(range.begin, range.end))
        {
            need_update = <span>true</span>;
            <span>break</span>;
        }
    }

    <span>if</span> (has_final_mark &amp;&amp; final_mark_is_true())
        need_update = <span>true</span>;

    release_shared_lock(cache_mutex);

    <span>if</span> (!need_update)
        <span>return</span>;  <span>/// Early out - no expensive lock needed!</span>

    <span>/// Slow path: Actually need to update, acquire exclusive lock</span>
    acquire_exclusive_lock(cache_mutex);

    <span>/// Double-check: verify update is still needed after acquiring lock</span>
    need_update = <span>false</span>;
    <span>for</span> (<span>const</span> <span>auto</span> &amp; range : mark_ranges)
    {
        <span>if</span> (any_marks_are_true(range.begin, range.end))
        {
            need_update = <span>true</span>;
            <span>break</span>;
        }
    }

    <span>if</span> (has_final_mark &amp;&amp; final_mark_is_true())
        need_update = <span>true</span>;

    <span>if</span> (need_update)
    {
        <span>// Perform the actual updates only if still needed</span>
        <span>for</span> (<span>const</span> <span>auto</span> &amp; range : mark_ranges)
            set_marks_to_false(range.begin, range.end);

        <span>if</span> (has_final_mark)
            set_final_mark_to_false();
    }

    release_lock(cache_mutex);
}
</code></pre>
<p><strong>Performance impact</strong></p>
<p>The optimized code delivered impressive performance improvements:</p>
<ul>
<li>CPU cycles spend for <code>native_queued_spin_lock_slowpath</code> reduced from 76% to 1%</li>
<li>The QPS of ClickBench queries Q10 and Q11 improved by 85% and 89%</li>
<li>The geometric mean of all ClickBench queries improved by 8.1%</li>
</ul>

<p>ClickHouse's query profiler was frequently creating and deleting a global timer_id variable, causing lock contention during query profiling.</p>
<p><strong>Query profiler timer usage</strong></p>
<p>ClickHouse's query profiler uses POSIX timers to sample thread stacks in periodic intervals for performance analysis. The original implementation:</p>
<ul>
<li>created and deleted timer_id frequently during profiling, and</li>
<li>required global synchronization for all operations that read or write the timer.</li>
</ul>
<p>Usage of shared data structures that needed protection with locks caused significant overhead.</p>
<p><strong>Eliminating global state with thread-local storage</strong></p>
<p>Here, we eliminated lock contention by thread-local storage, removing the need for shared state. Now, each thread has its own timer_id. This avoids shared state and the overhead of thread synchronization. To update a timer, it is no longer required to acquire locks.</p>
<p><strong>Technical solution</strong></p>
<pre><code><span>/// Original code</span>
<span><span>class</span> <span>QueryProfiler</span>
{</span>
    <span>static</span> global_mutex timer_management_lock

    <span>void</span> <span>startProfiling</span><span>()</span>
    {
        timer_id = create_new_timer();  <span>/// Expensive system call</span>

        acquire_exclusive_lock(timer_management_lock);  <span>/// Global lock!</span>
        update_shared_timer_state(timer_id);  <span>/// Modify shared state</span>
        release_lock(timer_management_lock);
    }

    <span>void</span> <span>stopProfiling</span><span>()</span>
    {
        acquire_exclusive_lock(timer_management_lock);
        cleanup_shared_timer_state(timer_id);
        release_lock(timer_management_lock);

        delete_timer(timer_id);
    }
}
</code></pre>
<pre><code><span>/// Optimized code</span>
<span><span>class</span> <span>QueryProfiler</span>
{</span>
    <span>static</span> <span>thread_local</span> timer_id per_thread_timer;
    <span>static</span> <span>thread_local</span> boolean timer_initialized;

    <span>void</span> <span>startProfiling</span><span>()</span>
    {
        <span>if</span> (!timer_initialized)
        {
            per_thread_timer = create_new_timer();  <span>/// Once per thread</span>
            timer_initialized = <span>true</span>;
        }

        <span>/// Reuse existing timer - no locks, no system calls!</span>
        enable_timer(per_thread_timer);
    }

    <span>void</span> <span>stopProfiling</span><span>()</span>
    {
        <span>/// Just disable timer - no deletion, no locks!</span>
        disable_timer(per_thread_timer);
    }
}
</code></pre>
<p><strong>Performance impact</strong></p>
<p>The new implementation has the following advantages:</p>
<ul>
<li>It eliminated timer-related lock contention hotspots from profiling traces</li>
<li>It reduced timer create/delete system calls through reuse</li>
<li>It makes profiling on ultra-high core count servers more scalable.</li>
</ul>
<p>Thread-local storage can eliminate lock contention by removing the need for shared state. Global synchronization becomes unnecessary if threads maintain their own state.</p>

<p>Memory optimization on ultra-high core count systems differs a lot from single-threaded memory management. Memory allocators themselves become contention points, memory bandwidth is divided across more cores, and allocation patterns that work fine on small systems can create cascading performance problems at scale. It is crucial to be mindful of how much memory is allocated and how memory is used.</p>
<p>This class of optimizations involves the allocator’s behavior, reducing pressure on memory bandwidth, and sometimes completely rethinking algorithms to eliminate memory-intensive operations altogether.</p>

<p>This optimization is motivated by high page fault rates and excessive resident memory usage which we observed for certain aggregation queries on ultra-high core count systems.</p>
<p><strong>Understanding two-level hash tables in ClickHouse</strong></p>
<p>Aggregation in ClickHouse uses different hash tables, depending on the data type, data distribution and data size. Large aggregation states are maintained in ephemeral hash tables.</p>
<ul>
<li>The <strong>1st level</strong> consists of 256 static buckets, each pointing to a 2nd level hash table.</li>
<li><strong>2nd level</strong> hash tables grow independently of each other.</li>
</ul>
<p><strong>Memory reuse for two-level hash tables</strong></p>
<p>At the end of an aggregation query, all hash tables used by the query are deallocated. In particular, the 256 sub-hash tables are deallocated and their memory is merged into larger free memory blocks.</p>
<p>jemalloc (as ClickHouse’s memory allocator) unfortunately prevented the reuse of merged memory blocks for future smaller allocations. This is because by default, only memory from blocks up to 64x larger than the requested size can be reused. This issue in jemalloc is very subtle but critical on ultra-high core count systems.</p>
<p>Based on <a href="https://github.com/jemalloc/jemalloc/pull/2842">jemalloc issue #2842</a>, we noticed a fundamental problem with jemalloc’s memory reuse for the irregularly-sized allocations typical in two-level hash tables:</p>
<ol>
<li><strong>Extent management issue</strong>: When large allocations are freed, jemalloc fails to efficiently track and reuse these memory extents.</li>
<li><strong>Size class fragmentation</strong>: Memory gets trapped in size classes that don't match future allocation patterns.</li>
<li><strong>Metadata overhead</strong>: Excessive metadata structures prevent efficient memory coalescing.</li>
<li><strong>Page fault amplification</strong>: New allocations trigger page faults instead of reusing existing committed pages.</li>
</ol>
<p>We identified jemalloc's <code>lg_extent_max_active_fit</code> parameter as the root cause - it was too restrictive for ClickHouse's allocation patterns.</p>
<p>We contributed the fix to <a href="https://github.com/jemalloc/jemalloc/pull/2842">jemalloc PR #2842</a>, but jemalloc didn’t have new stable releases for an extended period. Fortunately, we could resolve this issue through jemalloc's configuration parameters at compilation time.</p>
<p>Based on ClickHouse <a href="https://github.com/ClickHouse/ClickHouse/pull/80245">PR #80245</a>, the fix involved tuning jemalloc's configuration parameters:</p>
<pre><code><span>/// Original jemalloc configuration</span>
JEMALLOC_CONFIG_MALLOC_CONF = <span>"oversize_threshold:0,muzzy_decay_ms:0,dirty_decay_ms:5000"</span>
<span>/// lg_extent_max_active_fit defaults to 6, meaning memory can be reused from extents up to 64x larger than the requested allocation size</span>
</code></pre>
<pre><code><span>/// Optimized jemalloc configuration</span>
JEMALLOC_CONFIG_MALLOC_CONF = <span>"oversize_threshold:0,muzzy_decay_ms:0,dirty_decay_ms:5000,lg_extent_max_active_fit:8"</span>
<span>/// lg_extent_max_active_fit is set to 8.</span>
<span>/// This allows memory reuse from extents up to 256x larger</span>
<span>/// than the requested allocation size (2^8 = 256x vs default 2^6 = 64x).</span>
<span>/// The 256x limit matches ClickHouse's two-level hash table structure (256 buckets).</span>
<span>/// This enables efficient reuse of merged hash table memory blocks.</span>
</code></pre>
<p><strong>Performance impact</strong></p>
<p>The optimization improved</p>
<ul>
<li>the performance of ClickBench query Q35 by 96.1%,</li>
<li>memory usage (VmRSS, resident memory) and page faults reduced for the same query went down by 45.4% and 71%, respectively.</li>
</ul>
<p>The behavior of the memory allocator can have a dramatic impact on ultra-high core count systems.</p>

<p>ClickBench query Q29 was memory-bound and bottlenecked in excessive memory accesses caused by redundant computations of the form <code>sum(column + literal)</code>.</p>
<p><strong>Understanding the memory bottleneck</strong></p>
<p>ClickBench query Q29 contains multiple sum expressions with literals:</p>
<pre><code><span>SELECT</span> <span>SUM</span>(ResolutionWidth), <span>SUM</span>(ResolutionWidth <span>+</span> <span>1</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>2</span>), 
       <span>SUM</span>(ResolutionWidth <span>+</span> <span>3</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>4</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>5</span>), 
       <span>SUM</span>(ResolutionWidth <span>+</span> <span>6</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>7</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>8</span>), 
       <span>SUM</span>(ResolutionWidth <span>+</span> <span>9</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>10</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>11</span>), 
       <span>SUM</span>(ResolutionWidth <span>+</span> <span>12</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>13</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>14</span>), 
       <span>SUM</span>(ResolutionWidth <span>+</span> <span>15</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>16</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>17</span>), 
       <span>SUM</span>(ResolutionWidth <span>+</span> <span>18</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>19</span>), <span>SUM</span>(ResolutionWidth <span>+</span> <span>20</span>),
       <span>-- ... continues up to SUM(ResolutionWidth + 89)</span>
<span>FROM</span> hits;
</code></pre>
<p>The original query execution</p>
<ol>
<li><strong>Loaded column</strong> “ResolutionWidth” from storage once,</li>
<li><strong>Compute expressions</strong> - 90 times, creating 90 temporary columns (one per expression),</li>
<li><strong>Sum values</strong> performing 90 separate aggregation operations on each computed column.</li>
</ol>
<p>Creating 90 temporary columns and running 90 redundant aggregations obviously created massive memory pressure.</p>
<p><strong>Frontend query optimization for memory efficiency</strong></p>
<p>This optimization demonstrates how better optimizer rules can reduce memory pressure by eliminating redundant computations. The key insight is that many analytical queries contain patterns that can be algebraically simplified.</p>
<p>The optimization recognizes that <code>sum(column + literal)</code> can be rewritten to <code>sum(column) + count(column) * literal</code>.</p>
<p><strong>Performance impact</strong></p>
<ul>
<li>ClickBench query Q29 sped up by 11.5x on a 2×80 vCPU system.</li>
<li>The geometric mean of all ClickBench queries saw a 5.3% improvement overall.</li>
</ul>
<p>More intelligent query plans can be more effective than optimizing execution itself. Avoiding work is better than doing work efficiently.</p>

<p>Fast aggregation is a core promise of any analytical database. From a database perspective, aggregating data in parallel threads is only one part of the equation. It is equally important to merge the local results in parallel.</p>
<p>ClickHouse's aggregation operator has two phases: In the first phase, each thread processes its portion of the data in parallel, creating a local and partial result. In the second phase, all partial results must be merged. If the merge phase is not properly parallelized, it becomes a bottleneck. More threads can actually make this issue worse by creating more partial results to merge.</p>
<p>Solving this issue requires careful algorithm design, smart data structure choices, and a deep understanding how hash tables behave under different load patterns. The goal is to eliminate the serial merge phase and enable linear scaling even for the most complex aggregation queries.</p>

<p>ClickBench query Q5 showed a severe performance degradation as the core count increased from 80 to 112 threads. Our pipeline analysis revealed serial processing in the hash table conversion.</p>
<p><strong>Understanding hash tables in ClickHouse</strong></p>
<p>ClickHouse uses two types of hash tables for hash aggregation:</p>
<ol>
<li><strong>Single-level hash tables</strong>: This is a flat hash table that is suitable (= faster) for smaller datasets.</li>
<li><strong>Two-level hash tables</strong>: This is a hierarchical hash table with 256 buckets. Two-level hash tables are more amendable to large datasets.</li>
</ol>
<p>The database chooses the right hash table type based on the size of the processed data: Once a single-level hash table reaches a certain threshold during aggregation, it is automatically converted to a two-level hash table. The code to merge hash tables of different types was serialized.</p>
<p><strong>The serial bottleneck</strong></p>
<p>When merging hash tables from different threads,</p>
<ul>
<li><strong>single-level hash tables</strong> were serially merged in a pair-wise manner, e.g. ht1 / ht2 → result, then result / ht3, etc.</li>
<li><strong>two-level hash tables</strong> are merged one-by-one as well but the merge is parallelized across buckets.</li>
</ul>
<p>In the case of mixed single/two-level hash tables, the single-level hash tables had to be converted to two-level hash tables first (this was a serial process). Once the was done, the resulting two-level hash tables could be merged in parallel.</p>
<p>With Q5, increasing the number of threads from 80 to 112 meant that each thread processes less data. With 80 threads, all hash tables were two-level. With 112 threads, the aggregation ended up with the mixed scenario: some hash tables remained single-level while others became two-level. This caused serialization - all single-level hash tables had to be converted to two-level before parallel merging could take place.</p>
<p>To diagnose the issue, pipeline visualization was a crucial tool. The telltale sign was that the merge phase duration increased with thread count - this is the opposite of what should happen.</p>
<p><img src="https://clickhouse.com/uploads/intel_img_1_1481af3982.png" alt="intel_img_1.png" loading="lazy"></p>
<p><em>Performance degradation with increased core count</em></p>
<p><img src="https://clickhouse.com/uploads/intel_img_2_d019431938.png" alt="intel_img_2.png" loading="lazy"></p>
<p><em>Pipeline visualization (max_threads=80) - the merge phase is reasonable</em></p><p><img src="https://clickhouse.com/uploads/intel_img_3_b28b847281.png" alt="intel_img_3.png" loading="lazy"></p>
<p><em>Pipeline visualization (max_threads=112) - the merge phase takes 3.2x longer</em></p><p>Our optimization parallelizes the conversion phase: instead of converting all single-level hash tables to two-level hash tables one by one (serially), we now convert them in parallel. As each hash table can be converted independently, this eliminates the serial bottleneck.</p>
<pre><code><span>/// Original code</span>
<span>void</span> <span>mergeHashTable</span><span>(left_table, right_table)</span>
{
    <span>if</span> (left_table.is_single_level() &amp;&amp; right_table.is_two_level())    
        left_table.convert_to_two_level();  <span>/// Serial conversion blocks threads</span>

    <span>/// Now merge</span>
    merge_sets(left_table, right_table);
}
</code></pre>
<pre><code><span>/// Optimized code</span>
<span>void</span> <span>mergeHashTableParallel</span><span>(all_tables)</span>
{
    <span>/// Phase 1: Parallel conversion</span>
    parallel_tasks = [];
    <span>for</span> (<span>const</span> <span>auto</span> &amp; table : all_tables)
    {
        <span>if</span> (table.is_single_level())
        {
            <span>/// Parallel conversion!</span>
            task = create_parallel_task(table.convert_to_two_level());
            parallel_tasks.add(task);
        }
    }

    <span>/// Wait for all conversions to complete</span>
    wait_for_all_tasks(parallel_tasks);

    <span>/// Phase 2: Now all sets are two-level, merge efficiently.</span>
    <span>for</span> (<span>const</span> <span>auto</span> &amp; <span>pair</span> : all_tables)
        merge_sets(<span>pair</span>.left_table, <span>pair</span>.right_table);
}
</code></pre>
<p><strong>Performance impact</strong></p>
<p>The performance did not improve only for Q5 - the optimization enabled linear scaling for any aggregation-heavy query on ultra-high core count systems.</p>
<p><img src="https://clickhouse.com/uploads/intel_img_4_c4f403312b.png" alt="intel_img_4.png" loading="lazy"></p>
<p><em>Performance improvement after parallel conversion - Q5 achieves 264% improvement</em></p>
<ul>
<li>ClickBench query Q5 improved by a 264% on a 2×112 vCPU system,</li>
<li>24 queries achieved &gt;5% improvement,</li>
<li>the overall geometric mean improved by 7.4%</li>
</ul>
<p>The optimization demonstrates that scalability isn't just about making things more parallel - it's about eliminating serial sections that grow with parallelism. Sometimes you need to restructure algorithms on a more deep level, not just add more threads.</p>

<p>We noticed that the performance was also subpar when all hash tables were single-level.</p>
<p><strong>Extending parallel merge to single-level cases</strong></p>
<p>Building on <a href="https://github.com/ClickHouse/ClickHouse/pull/50748">PR #50748</a>, this optimization recognizes that the benefits of parallel merging are not limited to mixed hash tables. Even when all hash tables are single-level, parallel merging can improve performance if the total data size is large enough.</p>
<p>The challenge was to determine when single-level hash tables should be merged in parallel parallel:</p>
<ul>
<li>If datasets are too small, parallelization introduces extra overhead.</li>
<li>If datasets are too large, parallelization may not be beneficial enough.</li>
</ul>
<p>Based on the implementation in <a href="https://github.com/ClickHouse/ClickHouse/pull/52973/files">PR #52973</a>, the optimization added parallel merges to all single-level cases:</p>
<pre><code><span>/// Before: Only parallelize mixed-level merges</span>
<span>void</span> <span>parallelizeMergePrepare</span><span>(hash_tables)</span>
{
    single_level_count = <span>0</span>;

    <span>for</span> (<span>const</span> <span>auto</span> &amp; hash_table : hash_tables)
        <span>if</span> hash_table.is_single_level():
            single_level_count++;

    <span>/// Only convert if mixed levels (some single, some two-level)</span>
    <span>if</span> single_level_count &gt; <span>0</span> and single_level_count &lt; hash_tables.size():
        convert_to_two_level_parallel(hash_tables);
}
</code></pre>
<pre><code><span>/// Optimized code</span>
<span>void</span> <span>parallelizeMergePrepare</span><span>(hash_tables)</span>:
{
    single_level_count = <span>0</span>;
    all_single_hash_size = <span>0</span>;

    <span>for</span> (<span>const</span> <span>auto</span> &amp; hash_table : hash_tables)
        <span>if</span> (hash_table.is_single_level())
            single_level_count++

    <span>/// Calculate total size if all hash tables are single-level</span>
    <span>if</span> (single_level_count == hash_tables.size())
        <span>for</span> (<span>const</span> <span>auto</span> &amp; hash_table : hash_tables)
            all_single_hash_size += hash_table.size();

    <span>/// Convert if mixed levels OR if all single-level with average size &gt; THRESHOLD</span>
    <span>if</span> (single_level_count &gt; <span>0</span> and single_level_count &lt; hash_tables.size())
        ||
       (all_single_hash_size / hash_tables.size() &gt; THRESHOLD)
        convert_to_two_level_parallel(hash_tables);
}
</code></pre>
<p><strong>Performance impact</strong></p>
<ul>
<li>Performance for single-level merge scenarios improved by 235%</li>
<li>The optimal threshold was determined through systematic testing</li>
<li>There were no regressions on small datasets</li>
</ul>

<p>GROUP BY operations with large hash tables were merged serially.</p>
<p><strong>Extending parallelization to keyed aggregations</strong></p>
<p>The previous two optimizations (3.1 and 3.2) addressed merges without key - simple hash table operations like <code>COUNT(DISTINCT)</code>. We applied the same optimization to merges with key where hash tables contain both keys and aggregated values that must be combined, e.g. general <code>GROUP BY</code> semantics.</p>
<p><strong>Performance Impact</strong>:</p>
<ul>
<li>ClickBench query Q8 improved by 10.3%, Q9 by 7.6%</li>
<li>There were no regressions in other queries</li>
<li>CPU utilization during the merge phase improved</li>
</ul>
<p>Parallel merging can be extended to complex aggregation scenarios with careful attention to cancellation and error handling.</p>

<p>Harnessing the full potential of SIMD instructions is notoriously difficult. Compilers are conservative about vectorization, and database workloads often have complex control flows that inhibit auto-vectorization.</p>
<p>Effective usage of SIMD instructions in databases requires thinking beyond traditional vectorization. Besides processing N data items simultaneously instead of one, one can also utilize parallel SIMD comparisons for smart pruning strategies which lead to less work done overall. This idea is particularly powerful for string operations. These are at the same time frequently used in practice and computationally expensive.</p>

<p>String search (e.g. plain substring search or LIKE pattern search) is a bottleneck in a lot of queries, for example in ClickBench query Q20.</p>
<p><strong>Understanding string search in analytical queries</strong></p>
<p>Clickbench query 20 evaluates a LIKE pattern on millions of URLs, making fast string search crucial.</p>
<pre><code><span>SELECT</span> <span>COUNT</span>(<span>*</span>) <span>FROM</span> hits <span>WHERE</span> URL <span>LIKE</span> <span>'%google%'</span>
</code></pre>
<p><strong>Reducing false positives with two-character filtering</strong></p>
<p><a href="https://github.com/ClickHouse/ClickHouse/pull/46289/files">PR #46289</a> is based on the insight that SIMD instructions can be used in a smart way beyond brute-force parallelization. The original code already leveraged SIMD instructions but it only considered the search pattern’s first character, leading to expensive false positives. We rewrite the code to check the second character as well. This improved selectivity dramatically while adding only a negligible amount of new SIMD operations.</p>
<pre><code><span>/// Original code</span>
<span><span>class</span> <span>StringSearcher</span>
{</span>
    first_needle_character = needle[<span>0</span>];
    first_needle_character_vec = broadcast_to_simd_vector(first_needle_character);

    <span>void</span> <span>search</span><span>()</span>
    {
        <span>for</span> (position in haystack; step by <span>16</span> bytes)
        {
            haystack_chunk = load_16_bytes(haystack + position);
            first_matches = simd_compare_equal(haystack_chunk, first_needle_character_vec);
            match_mask = extract_match_positions(first_matches);

            <span>for</span> (<span>const</span> <span>auto</span> &amp; match : match_mask)
                <span>/// High false positive rate - many expensive verifications</span>
                <span>if</span> (full_string_match(haystack + match_pos, needle))
                    <span>return</span> match_pos;
        }
    }
}
</code></pre>
<pre><code><span>// Optimized code</span>
<span><span>class</span> <span>StringSearcher</span>
{</span>
    first_needle_character = needle[<span>0</span>];
    second_needle_character = needle[<span>1</span>];  <span>/// Second character</span>
    first_needle_character_vec = broadcast_to_simd_vector(first_needle_character);
    second_needle_character_vec = broadcast_to_simd_vector(second_needle_character);

    <span>void</span> <span>search</span><span>()</span>
    {
        <span>for</span> (position : haystack, step by <span>16</span> bytes)
        {
            haystack_chunk1 = load_16_bytes(haystack + position);
            haystack_chunk2 = load_16_bytes(haystack + position + <span>1</span>);

            <span>/// Compare both characters simultaneously</span>
            first_matches = simd_compare_equal(haystack_chunk1, first_needle_character_vec);
            second_matches = simd_compare_equal(haystack_chunk2, second_needle_character_vec);
            combined_matches = simd_and(first_matches, second_matches);

            match_mask = extract_match_positions(combined_matches);

            <span>for</span> (<span>const</span> <span>auto</span> &amp; match : match_mask)
                <span>// Dramatically fewer false positives - fewer expensive verifications</span>
                <span>if</span> <span>full_string_match</span><span>(haystack + match_pos, needle)</span>:
                    <span>return</span> match_pos;
        }
    }
}
</code></pre>
<p><strong>Performance impact</strong></p>
<p>Two-character SIMD filtering improved performance significantly:</p>
<ul>
<li>ClickBench query Q20 sped up by 35%</li>
<li>Other queries which perform substring matching saw an overall improvement of ~10%</li>
<li>The geometric mean of all queries improved by 4.1%</li>
</ul>
<p>The performance improvements are a result of fewer false positives, better cache locality and more efficient branch prediction.</p>
<p>Two-character SIMD filtering demonstrates that effective SIMD optimization isn't just about processing more data per instruction - it's about using SIMD's parallel comparison capabilities to improve the algorithmic efficiency. The two-character approach shows how a small number of additional SIMD operations can in some cases yield massive performance gains.</p>

<p>False sharing occurs when multiple threads access variables in the same cache. The CPU's cache coherence protocol works at cache line granularity, meaning that any cache line modifications - including modifications of two different variables - are treated as conflicts which require expensive synchronization between cores. On a 2 x 240 vCPUs system, false sharing can turn simple counter increments into system-wide performance disasters.</p>
<p>Eliminating false sharing requires how CPU cache coherence is implemented at the hardware level. It's not enough to optimize algorithms - to avoid false sharing, one must also optimize the memory layout to make sure that frequently-accessed data structures don't accidentally interfere with each other through cache line conflicts. This involves for example a strategic data layout and use of alignment and padding.</p>

<p>ClickBench query Q3 showed 36.6% of CPU cycles spent in <code>ProfileEvents::increment</code> on a 2×240 vCPU system. Performance profiling revealed a severe cache line contention.</p>
<p><strong>ProfileEvents counters at scale</strong></p>
<p>Profile event counters refer to ClickHouse's internal eventing system - profile events track all internal operations, from detailed query execution steps to memory allocations. In a typical analytical query, these counters are incremented millions of times across all threads. The original implementation organized multiple counters in the same memory region without considering cache line boundaries.</p>
<p>This creates three challenges:</p>
<ol>
<li>
<p><strong>Cache line physics</strong>: Modern Intel processors use 64-byte cache lines. When any byte in a cache line is modified, the entire line must be invalidated in the other cores' caches.</p>
</li>
<li>
<p><strong>False sharing amplification</strong>: With 240 threads, each counter update triggers a cache line invalidation across potentially dozens of cores. What should be independent operations become serialized through the cache coherence protocol.</p>
</li>
<li>
<p><strong>Exponential degradation</strong>: As the number of cores increases, the probability of a simultaneous access to the same cache line grows exponentially, compounding the impact of cache misses.</p>
</li>
</ol>
<p>Using perf, I discovered that <code>ProfileEvents::increment</code> was generating massive cache coherence traffic. The smoking gun was the cache line utilization report that showed eight different counters packed into a single cache line. We also added new capabilities to Linux’s perf c2c tool and worked with the community to help developers more easily identify false sharing issues like this.</p>
<p><img src="https://clickhouse.com/uploads/intel_img_5_64dd7ef454.png" alt="intel_img_5.png" loading="lazy"></p>
<p><em>Perf analysis showing 36.6% cycles in ProfileEvents::increment</em></p><p>Proper cache line alignment ensures that each counter gets its own 64-byte cache line. This transforms false sharing (bad) into true sharing (manageable). When a thread updates its counter, now only a single cache line wil be affected.</p>
<p>Based on our implementation in <a href="https://github.com/ClickHouse/ClickHouse/pull/82697/files">PR #82697</a>, the fix improved the cache line alignment for the profile event counters:</p>
<pre><code><span>// Before: Counters packed without alignment</span>
<span><span>struct</span> <span>ProfileEvents</span>:</span>
    <span>atomic_value</span> counters[NUM_EVENTS]  <span>// Multiple counters per cache line</span>
    <span>// 8 counters sharing single 64-byte cache lines</span>

<span>// After: Cache line aligned counters  </span>
<span><span>struct</span> <span>ProfileEvents</span>:</span>
    <span>struct</span> <span>alignas</span><span>(<span>64</span>)</span> AlignedCounter:
        <span>atomic_value</span> value
        <span>// Padding automatically added to reach 64 bytes</span>
    
    AlignedCounter counters[NUM_EVENTS]  <span>// Each counter gets own cache line</span>
    <span>// Now each counter has exclusive cache line ownership</span>
</code></pre>
<p><strong>Performance impact</strong></p>
<p>This optimization pattern applies to any frequently updated shared and compact data structure. The lesson is that the memory layout becomes critical at scale - what works fine on eight cores can be excruciatingly slow on 240 cores.</p>
<p><img src="https://clickhouse.com/uploads/intel_img_6_d32f81bea1.png" alt="intel_img_6.png" loading="lazy"></p>
<p><em>After optimization: ProfileEvents::increment drops to 8.5% (from 36.6%)</em></p><p>As a result of our optimization, ClickBench query Q3 saw a 27.4% improvement on ultra-high core count systems. The performance gain increases with the number of cores because the cache coherence overhead grows super-linearly. This optimization therefore doesn't merely fix a bottleneck - it changes the scalability curve.</p>
<p><img src="https://clickhouse.com/uploads/intel_img_7_651eaa2f76.png" alt="intel_img_7.png" loading="lazy"></p>
<p><em>ClickBench Q3: 27.4% improvement, with larger gains on higher core count systems</em></p>
<p>In this post I covered optimizations for five performance bottlenecks:</p>
<ol>
<li><strong>Lock contention</strong> - The coordination overhead grows exponentially with core count.</li>
<li><strong>Memory optimization</strong> - The memory bandwidth per core decreases as the core count increases.</li>
<li><strong>Increased parallelism</strong> - Serial phases become the dominant bottleneck.</li>
<li><strong>SIMD optimization</strong> - Smarter algorithms like two-character filtering beyond brute-force vectorization can improve performance significantly.</li>
<li><strong>False sharing</strong> - False sharing is caused by the granularity of cache line size.</li>
</ol>
<p>The bottlenecks and optimizations presented here are not just about ClickHouse - they represent a fundamental shift in how we must approach database optimization in the ultra-high core count era. As processors continue to evolve toward higher core counts, these techniques will become essential for any system that needs to scale.</p>
<p>Our optimizations enable ClickHouse to achieve close-to-linear scalability as the core count increases. This enables ClickHouse to thrive as an analytics database in a future world where Intel and other hardware manufacturers push the core count into the thousands.</p>
<p><img src="https://clickhouse.com/uploads/Team2_16ed51dacb.jpg" alt="Team2.jpg" loading="lazy"></p>
<hr>
<h2 id="references-and-resources"><strong>References and Resources</strong> </h2>
<ul>
<li><strong>Source Code</strong>: All optimizations available in ClickHouse main branch</li>
<li><strong>Slide Deck</strong>: <a href="https://github.com/ClickHouse/clickhouse-presentations/blob/master/2025-meetup-Shanghai-1/Talk%204%20-%20Intel%20-%20Shanghai%20Meetup_01Mar25.pdf">2025 Shanghai Meetup Presentation</a></li>
<li><strong>Pull Requests</strong>: Individual PRs linked throughout this post with detailed performance analysis</li>
<li><strong>Intel Intrinsics Guide</strong>: <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html">Intel® Intrinsics Guide</a></li>
</ul>

<p>Special thanks to the ClickHouse community for rigorous code review and performance validation. These optimizations represent collaborative effort between Intel and ClickHouse teams to unlock the full potential of modern ultra-high core count processors.</p>
<hr>
<p><em>For questions about implementation details or performance reproduction, please refer to the individual PR discussions linked throughout this post.</em></p></div></article></div>]]></description>
        </item>
    </channel>
</rss>