<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 23 Aug 2024 13:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Government report proves that we need to liberate the Postcode Address File (224 pts)]]></title>
            <link>https://takes.jamesomalley.co.uk/p/secret-paf-report</link>
            <guid>41326604</guid>
            <pubDate>Fri, 23 Aug 2024 06:36:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://takes.jamesomalley.co.uk/p/secret-paf-report">https://takes.jamesomalley.co.uk/p/secret-paf-report</a>, See on <a href="https://news.ycombinator.com/item?id=41326604">Hacker News</a></p>
Couldn't get https://takes.jamesomalley.co.uk/p/secret-paf-report: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Claude's API now supports CORS requests, enabling client-side applications (197 pts)]]></title>
            <link>https://simonwillison.net/2024/Aug/23/anthropic-dangerous-direct-browser-access/</link>
            <guid>41325889</guid>
            <pubDate>Fri, 23 Aug 2024 03:05:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2024/Aug/23/anthropic-dangerous-direct-browser-access/">https://simonwillison.net/2024/Aug/23/anthropic-dangerous-direct-browser-access/</a>, See on <a href="https://news.ycombinator.com/item?id=41325889">Hacker News</a></p>
Couldn't get https://simonwillison.net/2024/Aug/23/anthropic-dangerous-direct-browser-access/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[What's Going on in Machine Learning? Some Minimal Models (115 pts)]]></title>
            <link>https://writings.stephenwolfram.com/2024/08/whats-really-going-on-in-machine-learning-some-minimal-models/</link>
            <guid>41323454</guid>
            <pubDate>Thu, 22 Aug 2024 19:05:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://writings.stephenwolfram.com/2024/08/whats-really-going-on-in-machine-learning-some-minimal-models/">https://writings.stephenwolfram.com/2024/08/whats-really-going-on-in-machine-learning-some-minimal-models/</a>, See on <a href="https://news.ycombinator.com/item?id=41323454">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p><img src="https://content.wolfram.com/sites/43/2024/08/sw-ml-hero-v3.png" max-width="650px" height="auto" alt="What's Really Going On in Machine Learning? Some Minimal Models" title="What's Really Going On in Machine Learning? Some Minimal Models"></p>
<h2 id="the-mystery-of-machine-learning">The Mystery of Machine Learning</h2>
<p>It’s surprising how little is known about the foundations of machine learning. Yes, from an engineering point of view, an immense amount has been figured out about how to build neural nets that do all kinds of impressive and <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/">sometimes almost magical things</a>. But at a fundamental level we still don’t really know why neural nets “work”—and we don’t have any kind of “scientific big picture” of what’s going on inside them. </p>
<p>The basic structure of neural networks can be pretty simple. But by the time they’re trained up with all their weights, etc. it’s been hard to tell what’s going on—or even to get any good visualization of it. And indeed it’s far from clear even what aspects of the whole setup are actually essential, and what are just “details” that have perhaps been “grandfathered” all the way from when computational neural nets were first invented in the 1940s.</p>
<p>Well, what I’m going to try to do here is to get “underneath” this—and to “strip things down” as much as possible. I’m going to explore some very minimal models—that, among other things, are more directly amenable to visualization. At the outset, I wasn’t at all sure that these minimal models would be able to reproduce any of the kinds of things we see in machine learning. But, rather surprisingly, it seems they can.<span id="more-61728"></span></p>
<p>And the simplicity of their construction makes it much easier to “see inside them”—and to get more of a sense of what essential phenomena actually underlie machine learning. One might have imagined that even though the training of a machine learning system might be circuitous, somehow in the end the system would do what it does through some kind of identifiable and “explainable” mechanism. But we’ll see that in fact that’s typically not at all what happens. </p>
<p>Instead it looks much more as if the training manages to home in on some quite wild computation that “just happens to achieve the right results”. Machine learning, it seems, isn’t building structured mechanisms; rather, it’s basically just sampling from the typical complexity one sees in the computational universe, picking out pieces whose behavior turns out to overlap what’s needed. And in a sense, therefore, the possibility of machine learning is ultimately yet another consequence of the phenomenon of <a href="https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence#sect-12-6--computational-irreducibility">computational irreducibility</a>. </p>
<p>Why is that? Well, it’s only because of computational irreducibility that there’s all that richness in the computational universe. And, more than that, it’s because of computational irreducibility that things end up being effectively random enough that the adaptive process of training a machine learning system can reach success without getting stuck. </p>
<p>But the presence of computational irreducibility also has another important implication: that even though we can expect to find limited pockets of computational reducibility, we can’t expect a “general narrative explanation” of what a machine learning system does. In other words, there won’t be a traditional (say, mathematical) “general science” of machine learning (or, for that matter, probably also neuroscience). Instead, the story will be much closer to the fundamentally computational “<a href="https://www.wolframscience.com/nks/">new kind of science</a>” that I’ve explored for so long, and that has brought us our <a href="https://www.wolframphysics.org/" target="_blank" rel="noopener">Physics Project</a> and <a href="https://writings.stephenwolfram.com/2021/11/the-concept-of-the-ruliad/">the ruliad</a>.</p>
<p>In many ways, the problem of machine learning is a version of the <a href="https://writings.stephenwolfram.com/2024/05/why-does-biological-evolution-work-a-minimal-model-for-biological-evolution-and-other-adaptive-processes/">general problem of adaptive evolution</a>, as encountered <a href="https://writings.stephenwolfram.com/2024/05/why-does-biological-evolution-work-a-minimal-model-for-biological-evolution-and-other-adaptive-processes/">for example in biology</a>. In biology we typically imagine that we want to adaptively optimize some overall “fitness” of a system; in machine learning we typically try to adaptively “train” a system to make it align with certain goals or behaviors, most often defined by examples. (And, yes, in practice this is often done by trying to minimize a quantity normally called the “loss”.)</p>
<p>And while in biology there’s a general sense that “things arise through evolution”, quite how this works has always been rather mysterious. But (rather to my surprise) I recently <a href="https://writings.stephenwolfram.com/2024/05/why-does-biological-evolution-work-a-minimal-model-for-biological-evolution-and-other-adaptive-processes/">found a very simple model</a> that seems to do well at capturing at least some of the most essential features of biological evolution. And while the model isn’t the same as what we’ll explore here for machine learning, it has some definite similarities. And in the end we’ll find that the core phenomena of machine learning and of biological evolution appear to be remarkably aligned—and both fundamentally connected to the phenomenon of computational irreducibility.</p>
<p>Most of what I’ll do here focuses on foundational, theoretical questions. But in understanding more about what’s really going on in machine learning—and what’s essential and what’s not—we’ll also be able to begin to see how in practice machine learning might be done differently, potentially with more efficiency and more generality. </p>
<h2 id="traditional-neural-nets">Traditional Neural Nets</h2>

<p><em>Note: Click any diagram to get Wolfram Language code to reproduce it.</em></p>
<p>To begin the process of understanding the essence of machine learning, let’s start from a very traditional—and familiar—example: a <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/#machine-learning-and-the-training-of-neural-nets">fully connected (“multilayer perceptron”) neural net</a> that’s been trained to compute a certain function <em>f</em>[<em>x</em>]:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192024traditionalimg1.png" alt="" title="" width="445" height="283"> </p>
</div>
<p>If one gives a value <em>x</em> as input at the top, then after “rippling through the layers of the network” one gets a value at the bottom that (almost exactly) corresponds to our function <em>f</em>[<em>x</em>]:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192024traditionalimg2.png" alt="" title="" width="546" height="321"> </p>
</div>
<p>Scanning through different inputs <em>x</em>, we see different patterns of intermediate values inside the network: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192024traditionalimg3.png" alt="" title="" width="683" height="281"> </p>
</div>
<p>And here’s (on a linear and log scale) how each of these intermediate values changes with <em>x</em>. And, yes, the way the final value (highlighted here) emerges looks very complicated: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192024traditionalimg4.png" alt="" title="" width="682" height="292"> </p>
</div>
<p>So how is the neural net ultimately put together? How are these values that we’re plotting determined? We’re using the <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/#neural-nets">standard setup for a fully connected multilayer network</a>. Each node (“neuron”) on each layer is connected to all nodes on the layer above—and values “flow” down from one layer to the next, being multiplied by the (positive or negative) “weight” (indicated by color in our pictures) associated with the connection through which they flow. The value of a given neuron is found by totaling up all its (weighted) inputs from the layer before, adding a “bias” value for that neuron, and then applying to the result a certain (nonlinear) “<a href="https://reference.wolfram.com/language/ref/ElementwiseLayer.html">activation function</a>” (here ReLU or <tt><a href="http://reference.wolfram.com/language/ref/Ramp.html">Ramp</a></tt>[<em>z</em>], i.e. <tt><a href="http://reference.wolfram.com/language/ref/If.html">If</a></tt>[<em>z</em> &lt; 0, 0, <em>z</em>]).</p>
<p>What overall function a given neural net will compute is determined by the collection of weights and biases that appear in the neural net (along with its overall connection architecture, and the activation function it’s using). The idea of machine learning is to find weights and biases that produce a particular function by adaptively “learning” from examples of that function. Typically we might start from a random collection of weights, then successively tweak weights and biases to <a href="https://reference.wolfram.com/language/ref/NetTrain.html">“train” the neural net</a> to reproduce the function: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192024traditionalimg5.png" alt="" title="" width="664" height="205"> </p>
</div>
<p>We can get a sense of how this progresses (and, yes, it’s complicated) by plotting successive changes in individual weights over the course of the training process (the spikes near the end come from “neutral changes” that don’t affect the overall behavior):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192024traditionalimg6.png" alt="" title="" width="614" height="180"> </p>
</div>
<p>The overall objective in the training is progressively to decrease <a href="https://reference.wolfram.com/language/ref/LossFunction.html">the “loss”</a>—the average (squared) difference between true values of <em>f</em>[<em>x</em>] and those generated by the neural net. The evolution of the loss defines a “learning curve” for the neural net, with the downward glitches corresponding to points where the neural net in effect “made a breakthrough” in being able to represent the function better: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192024traditionalimg7.png" alt="" title="" width="348" height="120"> </p>
</div>
<p>It’s important to note that typically there’s randomness injected into neural net training. So if one runs the training multiple times, one will get different networks—and different learning curves—every time: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192024traditionalimg8.png" alt="" title="" width="600" height="136"> </p>
</div>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192024traditionalimg9.png" alt="" title="" width="356" height="123"> </p>
</div>
<p>But what’s really going on in neural net training? Effectively we’re finding a way to “compile” a function (at least to some approximation) into a neural net with a certain number of (real-valued) parameters. And in the example here we happen to be using about 100 parameters.</p>
<p>But what happens if we use a different number of parameters, or set up the architecture of our neural net differently? Here are a few examples, indicating that for the function we’re trying to generate, the network we’ve been using so far is pretty much the smallest that will work:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192024traditionalimg10.png" alt="" title="" width="526" height="421"> </p>
</div>
<p>And, by the way, here’s what happens if we change our activation function from ReLU<br>
<img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192024traditionalimg11.png" alt="" title="" width="37" height="23"> to the smoother ELU <img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192024traditionalimg12.png" alt="" title="" width="44" height="28">:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192024traditionalimg13.png" alt="" title="" width="525" height="420"> </p>
</div>
<p>Later we’ll talk about what happens when we do machine learning with discrete systems. And in anticipation of that, it’s interesting to see what happens if we take a neural net of the kind we’ve discussed here, and “quantize” its weights (and biases) in discrete levels:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192024traditionalimg14.png" alt="" title="" width="553" height="197"> </p>
</div>
<p>The result is that (as recent experience with large-scale neural nets has also shown) the basic “operation” of the neural net does not require precise real numbers, but survives even when the numbers are at least somewhat discrete—as this 3D rendering as a function of the discreteness level <em>δ</em> also indicates:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192024traditionalimg15.png" alt="" title="" width="341" height="364"> </p>
</div>
<h2 id="simplifying-the-topology-mesh-neural-nets">Simplifying the Topology: Mesh Neural Nets</h2>
<p>So far we’ve been discussing very traditional neural nets. But to do machine learning, do we really need systems that have all those details? For example, do we really need every neuron on each layer to get an input from every neuron on the previous layer? What happens if instead every neuron just gets input from at most two others—say with the neurons effectively laid out in a simple mesh? Quite surprisingly, it turns out that such a network is still perfectly able to generate a function like the one we’ve been using as an example: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08202024topologyimg1.png" alt="" title="" width="403" height="270"> </p>
</div>
<p>And one advantage of such a “mesh neural net” is that—like a cellular automaton—its “internal behavior” can readily be visualized in a rather direct way. So, for example, here are visualizations of “how the mesh net generates its output”, stepping through different input values <em>x</em>:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08202024topologyimg2.png" alt="" title="" width="642" height="531"> </p>
</div>
<p>And, yes, even though we can visualize it, it’s still hard to understand “what’s going on inside”. Looking at the intermediate values of each individual node in the network as a function of <em>x</em> doesn’t help much, though we can “see something happening” at places where our function <em>f</em>[<em>x</em>] has jumps:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08222024MLupdatesAimg1.png" alt="" title="" width="612" height="324"> </p>
</div>
<p>So how do we train a mesh neural net? Basically we can use the same procedure as for a fully connected network of the kind we saw above (ReLU activation functions don’t seem to work well for mesh nets, so we’re using ELU here):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08202024topologyimg4.png" alt="" title="" width="616" height="173"> </p>
</div>
<p>Here’s the evolution of differences in each individual weight during the training process: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08202024topologyimg5.png" alt="" title="" width="615" height="180"> </p>
</div>
<p>And here are results for different random seeds: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08202024topologyimg6.png" alt="" title="" width="591" height="108"> </p>
</div>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08202024topologyimg7.png" alt="" title="" width="412" height="136"> </p>
</div>
<p>At the size we’re using, our mesh neural nets have about the same number of connections (and thus weights) as our main example of a fully connected network above. And we see that if we try to reduce the size of our mesh neural net, it doesn’t do well at reproducing our function:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08202024topologyimg8.png" alt="" title="" width="593" height="459"> </p>
</div>
<h2 id="making-everything-discrete-a-biological-evolution-analog">Making Everything Discrete: A Biological Evolution Analog</h2>
<p>Mesh neural nets simplify the topology of neural net connections. But, somewhat surprisingly at first, it seems as if we can go much further in simplifying the systems we’re using—and still successfully do versions of machine learning. And in particular we’ll find that we can make our systems completely discrete. </p>
<p>The typical methodology of neural net training involves progressively tweaking real-valued parameters, usually using methods based on calculus, and on finding derivatives. And one might imagine that any successful adaptive process would ultimately have to rely on being able to make arbitrarily small changes, of the kind that are possible with real-valued parameters. </p>
<p>But in <a href="https://writings.stephenwolfram.com/2024/05/why-does-biological-evolution-work-a-minimal-model-for-biological-evolution-and-other-adaptive-processes/">studying simple idealizations of biological evolution</a> I recently found striking examples where this isn’t the case—and where completely discrete systems seemed able to capture the essence of what’s going on. </p>
<p>As an example consider a (3-color) cellular automaton. The rule is shown on the left, and the behavior one generates by repeatedly applying that rule (starting from a single-cell initial condition) is shown on the right:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192027discreteimg1.png" alt="" title="" width="476" height="349"> </p>
</div>
<p>The rule has the property that the pattern it generates (from a single-cell initial condition) survives for exactly 40 steps, and then dies out (i.e. every cell becomes white). And the important point is that this rule can be found by a discrete adaptive process. The idea is to start, say, from a null rule, and then at each step to randomly change a single outcome out of the 27 in the rule (i.e. make a “single-point mutation” in the rule). Most such changes will cause the “lifetime” of the pattern to get further from our target of 40—and these we discard. But gradually we can build up “beneficial mutations”</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192027discreteimg2.png" alt="" title="" width="213" height="186"> </p>
</div>
<p>that through “progressive adaptation” eventually get to our original lifetime-40 rule:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192027discreteimg3.png" alt="" title="" width="595" height="140"> </p>
</div>
<p>We can make a plot of all the attempts we made that eventually let us reach lifetime 40—and we can think of this progressive “fitness” curve as being directly analogous to the loss curves in machine learning that we saw before: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192027discreteimg4.png" alt="" title="" width="392" height="139"> </p>
</div>
<p>If we make different sequences of random mutations, we’ll get different paths of adaptive evolution, and different “solutions” for rules that have lifetime 40:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192027discreteimg5.png" alt="" title="" width="567" height="664"> </p>
</div>
<p>Two things are immediately notable about these. First, that they essentially all seem to be “using different ideas” to reach their goal (presumably analogous to the phenomenon of different branches in the tree of life). And second, that none of them seem to be using a clear “mechanical procedure” (of the kind we might construct through traditional engineering) to reach their goal. Instead, they seem to be finding “natural” complicated behavior that just “happens” to achieve the goal.</p>
<p>It’s nontrivial, of course, that this behavior can achieve a goal like the one we’ve set here, as well as that simple selection based on random point mutations can successfully reach the necessary behavior. But <a href="https://writings.stephenwolfram.com/2024/05/why-does-biological-evolution-work-a-minimal-model-for-biological-evolution-and-other-adaptive-processes/">as I discussed in connection with biological evolution</a>, this is ultimately a story of <a href="https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence#sect-12-6--computational-irreducibility">computational irreducibility</a>—particularly in generating diversity both in behavior, and in the paths necessary to reach it. </p>
<p>But, OK, so how does this model of adaptive evolution relate to systems like neural nets? In the standard language of neural nets, our model is like a discrete analog of a recurrent convolutional network. It’s “convolutional” because at any given step the same rule is applied—locally—throughout an array of elements. It’s “recurrent” because in effect data is repeatedly “passed through” the same rule. The kinds of procedures (like “backpropagation”) typically used to train traditional neural nets wouldn’t be able to train such a system. But it turns out that—essentially as a consequence of computational irreducibility—the very simple method of successive random mutation can be successful.</p>
<h2 id="machine-learning-in-discrete-rule-arrays">Machine Learning in Discrete Rule Arrays</h2>
<p>Let’s say we want to set up a system like a neural net—or at least a mesh neural net—but we want it to be completely discrete. (And I mean “born discrete”, not just discretized from an existing continuous system.) How can we do this? One approach (that, as it happens, <a href="https://content.wolfram.com/sw-publications/2020/07/approaches-complexity-engineering.pdf">I first considered in the mid-1980s</a>—but never seriously explored) is to make what we can call a “rule array”. Like in a cellular automaton there’s an array of cells. But instead of these cells always being updated according to the same rule, each cell at each place in the cellular automaton analog of “spacetime” can make a different choice of what rule it will use. (And although it’s a fairly extreme idealization, we can potentially imagine that these different rules represent a discrete analog of different local choices of weights in a mesh neural net.)</p>
<p>As a first example, let’s consider a rule array in which there are two possible choices of rules: <nobr><em>k </em>= 2, <em>r </em>= 1</nobr> <a href="https://www.wolframscience.com/nks/chap-3--the-world-of-simple-programs#sect-3-2--more-cellular-automata">cellular automaton rules 4 and 146</a> (which are respectively <a href="https://www.wolframscience.com/nks/chap-6--starting-from-randomness#sect-6-2--four-classes-of-behavior">class 2 and class 3</a>):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192024arraysimg1.png" alt="" title="" width="366" height="116"> </p>
</div>
<p>A particular rule array is defined by which of these rules is going to be used at each (“spacetime”) position in the array. Here are a few examples. In all cases we’re starting from the same single-cell initial condition. But in each case the rule array has a different arrangement of rule choices—with cells “running” rule 4 being given a <img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08202024arraysaquabox.png" alt="" title="" width="15" height="15"> background, and those running rule 146 a <img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192024arrayspinkbox.png" alt="" title="" width="15" height="15"> one:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08202024arraysBimg2.png" alt="" title="" width="675" height="171"> </p>
</div>
<p>We can see that different choices of rule array can yield very different behaviors. But (in the spirit of machine learning) can we in effect “invert this”, and find a rule array that will give some particular behavior we want?</p>
<p>A simple approach is to do the direct analog of what we did in our minimal modeling of biological evolution: progressively make random “single-point mutations”—here “flipping” the identity of just one rule in the rule array—and then keeping only those mutations that don’t make things worse. </p>
<p>As our sample objective, let’s ask to find a rule array that makes the pattern generated from a single cell using that rule array “survive” for exactly 50 steps. At first it might not be obvious that we’d be able to find such a rule array. But in fact our simple adaptive procedure easily manages to do this:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08202024arraysBimg3.png" alt="" title="" width="595" height="505"> </p>
</div>
<p>As the dots here indicate, many mutations don’t lead to longer lifetimes. But every so often, the adaptive process has a “breakthrough” that increases the lifetime—eventually reaching 50:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08202024arraysBimg4.png" alt="" title="" width="337" height="119"> </p>
</div>
<p>Just as in our model of biological evolution, different random sequences of mutations lead to different “solutions”, here to the problem of “living for exactly 50 steps”:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08202024arraysBimg5.png" alt="" title="" width="618" height="406"> </p>
</div>
<p>Some of these are in effect “simple solutions” that require only a few mutations. But most—like most of our examples in biological evolution—seem more as if they just “happen to work”, effectively by tapping into just the right, fairly complex behavior.</p>
<p>Is there a sharp distinction between these cases? Looking at the collection of “fitness” (AKA “learning”) curves for the examples above, it doesn’t seem so:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08202024arraysBimg6.png" alt="" title="" width="612" height="163"> </p>
</div>
<p>It’s not too difficult to see how to “construct a simple solution” just by strategically placing a single instance of the second rule in the rule array:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08202024arraysBimg7.png" alt="" title="" width="309" height="57"> </p>
</div>
<p>But the point is that adaptive evolution by repeated mutation normally won’t “discover” this simple solution. And what’s significant is that the adaptive evolution can nevertheless still successfully find some solution—even though it’s not one that’s “understandable” like this.</p>
<p>The cellular automaton rules we’ve been using so far take 3 inputs. But it turns out that we can make things even simpler by just putting ordinary <a href="https://www.wolframscience.com/nks/p806--implications-for-mathematics-and-its-foundations/">2-input Boolean functions</a> into our rule array. For example, we can make a rule array from <tt><a href="https://reference.wolfram.com/language/ref/And.html">And</a></tt> and <tt><a href="https://reference.wolfram.com/language/ref/Xor.html">Xor</a></tt> functions (<a href="https://www.wolframscience.com/nks/p806--implications-for-mathematics-and-its-foundations/"><em>r</em> = 1/2 rules 8 and 6</a>):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08202024arraysBimg8.png" alt="" title="" width="309" height="40"> </p>
</div>
<p>Different <tt>And</tt>+<tt>Xor</tt> (<img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08202024arraysmintbox.png" alt="" title="" width="14" height="14"> + <img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192024arraysimg10.png" alt="" title="" width="11" height="13">) rule arrays show different behavior:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08202024arraysBimg11.png" alt="" title="" width="594" height="168"> </p>
</div>
<p>But are there for example <tt>And</tt>+<tt>Xor</tt> rule arrays that will compute any of the 16 possible (2-input) functions? We can’t get <tt><a href="https://reference.wolfram.com/language/ref/Not.html">Not</a></tt> or any of the 8 other functions with <img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192024arrayshexes.png" alt="" title="" width="52" height="26">—but it turns out we can get all 8 functions with <img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192024arraysimg13.png" alt="" title="" width="50" height="13"> (additional inputs here are assumed to be <img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192024arraysimg14.png" alt="" title="" width="12" height="13">):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08202024arraysBimg15.png" alt="" title="" width="662" height="179"> </p>
</div>
<p>And in fact we can also set up <tt>And</tt>+<tt>Xor</tt> rule arrays for all other “even” Boolean functions. For example, here are rule arrays for the 3-input rule 30 and rule 110 Boolean functions:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08202024arraysBimg16.png" alt="" title="" width="573" height="53"> </p>
</div>
<p>It may be worth commenting that the ability to set up such rule arrays is related to <a href="https://www.wolframscience.com/nks/p807--implications-for-mathematics-and-its-foundations/">functional completeness</a> of the underlying rules we’re using—though it’s not quite the same thing. Functional completeness is about setting up arbitrary formulas, that can in effect allow long-range connections between intermediate results. Here, all information has to explicitly flow through the array. But for example the functional completeness of <tt><a href="http://reference.wolfram.com/language/ref/Nand.html">Nand</a></tt> (<em>r</em> = 1/2 rule 7, <img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08222024arraysorangehex.png" alt="" title="" width="15" height="15">) allows it to generate all Boolean functions when combined for example with <tt><a href="http://reference.wolfram.com/language/ref/First.html">First</a></tt> (<em>r</em> = 1/2 rule 12, <img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08222024arraysbluehex.png" alt="" title="" width="15" height="15">), though sometimes the rule arrays required are quite large: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08222024MLupdatesAimg2.png" alt="" title="" width="632" height="165"> </p>
</div>
<p>OK, but what happens if we try to use our adaptive evolution process—say to solve the problem of finding a pattern that survives for exactly 30 steps? Here’s a result for <tt>And</tt>+<tt>Xor</tt> rule arrays:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08202024arraysBimg20.png" alt="" title="" width="591" height="397"> </p>
</div>
<p>And here are examples of other “solutions” (none of which in this case look particularly “mechanistic” or “constructed”):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024arraysBimg21.png" alt="" title="" width="596" height="311"> </p>
</div>
<p>But what about learning our original <em>f</em>[<em>x</em>] = <img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08192024arraysimg22.png" alt="" title="" width="51" height="13"> function? Well, first we have to decide how we’re going to represent the numbers <em>x</em> and <em>f</em>[<em>x</em>] in our discrete rule array system. And one approach is to do this simply in terms of the position of a black cell (“one-hot encoding”). So, for example, in this case there’s an initial black cell at a position corresponding to about <em>x</em> = –1.1. And then the result after passing through the rule array is a black cell at a position corresponding to <em>f</em>[<em>x</em>] = 1.0:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024arraysCimg1.png" alt="" title="" width="188" height="304"> </p>
</div>
<p>So now the question is whether we can find a rule array that successfully maps initial to final cell positions according to the mapping <em>x</em> <img src="https://content.wolfram.com/uploads/sites/32/2022/10/rightarrow.png" width="18" height="12"> <em>f</em>[<em>x</em>] we want. Well, here’s an example that comes at least close to doing this (note that the array is taken to be cyclic):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024arraysBimg24.png" alt="" title="" width="687" height="414"> </p>
</div>
<p>So how did we find this? Well, we just used a simple adaptive evolution process. In direct analogy to the way it’s usually done in machine learning, we set up “training examples”, here of the form:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08222024MLupdatesAimg3.png" alt="" title="" width="573" height="76"> </p>
</div>
<p>Then we repeatedly made single-point mutations in our rule array, keeping those mutations where the total difference from all the training examples didn’t increase. And after 50,000 mutations this gave the final result above. </p>
<p>We can get some sense of “how we got there” by showing the sequence of intermediate results where we got closer to the goal (as opposed to just not getting further from it):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024arraysBimg26.png" alt="" title="" width="597" height="246"> </p>
</div>
<p>Here are the corresponding rule arrays, in each case highlighting elements that have changed (and showing the computation of <em>f</em>[0] in the arrays):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024arraysBimg27.png" alt="" title="" width="609" height="289"> </p>
</div>
<p>Different sequences of random mutations will lead to different rule arrays. But with the setup defined here, the resulting rule arrays will almost always succeed in accurately computing <em>f</em>[<em>x</em>]. Here are a few examples—in which we’re specifically showing the computation of <em>f</em>[0]:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024arraysBimg28.png" alt="" title="" width="642" height="109"> </p>
</div>
<p>And once again an important takeaway is that we don’t see “identifiable mechanism” in what’s going on. Instead, it looks more as if the rule arrays we’ve got just “happen” to do the computations we want. Their behavior is complicated, but somehow we can manage to “tap into it” to compute our <em>f</em>[<em>x</em>].</p>
<p>But how robust is this computation? A key feature of typical machine learning is that it can “generalize” away from the specific examples it’s been given. It’s never been clear just how to characterize that generalization (when does an image of a cat in a dog suit start <a href="https://writings.stephenwolfram.com/2015/05/wolfram-language-artificial-intelligence-the-image-identification-project/">being identified as an image of a dog</a>?). But—at least when we’re talking about classification tasks—we can think of what’s going on in terms of <a href="https://www.wolframscience.com/nks/p624--human-thinking/">basins of attraction</a> that lead to attractors corresponding to our classes.</p>
<p>It’s all considerably easier to analyze, though, in the kind of discrete system we’re exploring here. For example, we can readily enumerate all our training inputs (i.e. all initial states containing a single black cell), and then see how frequently these cause any given cell to be black:</p>
<div>
<p><img src="https://content.wolfram.com/sites/43/2024/08/sw08242024arraysupdateFimg2.png" alt="" title="" width="186" height="302"> </p>
</div>
<p>By the way, here’s what happens to this plot at successive “breakthroughs” during training:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024arraysBimg30.png" alt="" title="" width="660" height="309"> </p>
</div>
<p>But what about all possible inputs, including ones that don’t just contain a single black cell? Well, we can enumerate all of them, and compute the overall frequency for each cell in the array to be black:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08222024MLupdatesAimg4.png" alt="" title="" width="188" height="304"> </p>
</div>
<p>As we would expect, the result is considerably “fuzzier” than what we got purely with our training inputs. But there’s still a strong trace of the discrete values for <em>f</em>[<em>x</em>] that appeared in the training data. And if we plot the overall probability for a given final cell to be black, we see peaks at positions corresponding to the values 0 and 1 that <em>f</em>[<em>x</em>] takes on:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024arraysBimg32.png" alt="" title="" width="276" height="76"> </p>
</div>
<p>But because our system is discrete, we can explicitly look at what outcomes occur: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024arraysBimg33.png" alt="" title="" width="348" height="193"> </p>
</div>
<p>The most common overall is the “meaningless” all-white state—that basically occurs when the computation from the input “never makes it” to the output. But the next most common outcomes correspond exactly to <em>f</em>[<em>x</em>] = 0 and <em>f</em>[<em>x</em>] = 1. After that is the “superposition” outcome where <em>f</em>[<em>x</em>] is in effect “both 0 and 1”. </p>
<p>But, OK, so what initial states are “in the basins of attraction of” (i.e. will evolve to) the various outcomes here? The fairly flat plots in the last column above indicate that the overall density of black cells gives little information about what attractor a particular initial state will evolve to. </p>
<p>So this means we have to look at specific configurations of cells in the initial conditions. As an example, start from the initial condition </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024arraysBimg34.png" alt="" title="" width="195" height="12"> </p>
</div>
<p>which evolves to:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024arraysBimg35.png" alt="" title="" width="195" height="12"> </p>
</div>
<p>Now we can ask what happens if we look at a sequence of slightly different initial conditions. And here we show in black and white initial conditions that still evolve to the original “attractor” state, and in pink ones that evolve to some different state:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024arraysBimg36.png" alt="" title="" width="475" height="398"> </p>
</div>
<p>What’s actually going on inside here? Here are a few examples, highlighting cells whose values change as a result of changing the initial condition:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024arraysBimg37.png" alt="" title="" width="596" height="154"> </p>
</div>
<p>As is typical in machine learning, there doesn’t seem to be any simple characterization of the form of the basin of attraction. But now we have a sense of what the reason for this is: it’s another consequence of computational irreducibility. Computational irreducibility gives us the effective randomness that allows us to find useful results by adaptive evolution, but it also leads to changes having what seem like random and unpredictable effects. (It’s worth noting, by the way, that we could probably dramatically improve the robustness of our attractor basins by specifically including our training data examples that have “noise” injected.)</p>
<h2 id="multiway-mutation-graphs">Multiway Mutation Graphs</h2>
<p>In doing machine learning in practice, the goal is typically to find some collection of weights, etc. that successfully solve a particular problem. But in general there will be many such collections of weights, etc. With typical continuous weights and random training steps it’s very difficult to see what the whole “ensemble” of possibilities is. But in our discrete rule array systems, this becomes more feasible.</p>
<p>Consider a tiny 2×2 rule array with two possible rules. We can <a href="https://writings.stephenwolfram.com/2024/05/why-does-biological-evolution-work-a-minimal-model-for-biological-evolution-and-other-adaptive-processes/#the-multiway-graph-of-all-possible-mutation-histories">make a graph whose edges represent all possible “point mutations”</a> that can occur in this rule array:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024mutationimg1.png" alt="" title="" width="258" height="243"> </p>
</div>
<p>In our adaptive evolution process, we’re always moving around a graph like this. But typically most “moves” will end up in states that are rejected because they increase whatever loss we’ve defined. </p>
<p>Consider the problem of generating an <tt>And</tt>+<tt>Xor</tt> rule array in which we end with lifetime-4 patterns. Defining the loss as how far we are from this lifetime, we can draw a graph that shows all possible adaptive evolution paths that always progressively decrease the loss:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024mutationimg2.png" alt="" title="" width="641" height="226"> </p>
</div>
<p>The result is a multiway graph of the type we’ve now seen in a <a href="https://writings.stephenwolfram.com/2023/11/aggregation-and-tiling-as-multicomputational-processes/">great many kinds of situations</a>—notably our <a href="https://writings.stephenwolfram.com/2024/05/why-does-biological-evolution-work-a-minimal-model-for-biological-evolution-and-other-adaptive-processes/#the-multiway-graph-of-all-possible-mutation-histories">recent study of biological evolution</a>. </p>
<p>And although this particular example is quite trivial, the idea in general is that different parts of such a graph represent “different strategies” for solving a problem. And—in direct analogy to our <a href="https://www.wolframphysics.org/" target="_blank" rel="noopener">Physics Project</a> and <a href="https://writings.stephenwolfram.com/2022/06/games-and-puzzles-as-multicomputational-systems/">our studies of things like game graphs</a>—one can imagine such strategies being laid out in a “<a href="https://www.wolframphysics.org/technical-introduction/the-updating-process-for-string-substitution-systems/the-concept-of-branchial-graphs/" target="_blank" rel="noopener">branchial space</a>” defined by common ancestry of configurations in the multiway graph. </p>
<p>And one can expect that while in some cases the branchial graph will be fairly uniform, in other cases it will have quite separated pieces—that represent fundamentally different strategies. Of course, the fact that underlying strategies may be different doesn’t mean that the overall behavior or performance of the system will be noticeably different. And indeed one expects that in most cases computational irreducibility will lead to enough effective randomness that there’ll be no discernable difference.</p>
<p>But in any case, here’s an example starting with a rule array that contains both <tt>And </tt>and <tt>Xor</tt>—where we observe distinct branches of adaptive evolution that lead to different solutions to the problem of finding a configuration with a lifetime of exactly 4:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024mutationimg3.png" alt="" title="" width="612" height="220"> </p>
</div>
<h2 id="optimizing-the-learning-process">Optimizing the Learning Process</h2>
<p>How should one actually do the learning in machine learning? In practical work with traditional neural nets, learning is normally done using systematic algorithmic methods like backpropagation. But so far, all we’ve done here is something much simpler: we’ve “learned” by successively making random point mutations, and keeping only ones that don’t lead us further from our goal. And, yes, it’s interesting that such a procedure can work at all—and (<a href="https://writings.stephenwolfram.com/2024/05/why-does-biological-evolution-work-a-minimal-model-for-biological-evolution-and-other-adaptive-processes/">as we’ve discussed elsewhere</a>) this is presumably very relevant to understanding phenomena like biological evolution. But, as we’ll see, there are more efficient (and probably much more efficient) methods of doing machine learning, even for the kinds of discrete systems we’re studying.</p>
<p>Let’s start by looking again at our earlier example of finding an <tt>And</tt>+<tt>Xor</tt> rule array that gives a “lifetime” of exactly 30. At each step in our adaptive (“learning”) process we make a single-point mutation (changing a single rule in the rule array), keeping the mutation if it doesn’t take us further from our goal. The mutations gradually accumulate—every so often reaching a rule array that gives a lifetime closer to 30. Just as above, here’s a plot of the lifetime achieved by successive mutations—with the “internal” red dots corresponding to rejected mutations:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg1.png" alt="" title="" width="349" height="123"> </p>
</div>
<p>We see a series of “plateaus” at which mutations are accumulating but not changing the overall lifetime. And between these we see occasional “breakthroughs” where the lifetime jumps. Here are the actual rule array configurations for these breakthroughs, with mutations since the last breakthrough highlighted:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg2.png" alt="" title="" width="592" height="397"> </p>
</div>
<p>But in the end the process here is quite wasteful; in this example, we make a total of 1705 mutations, but only 780 of them actually contribute to generating the final rule array; all the others are discarded along the way.</p>
<p>So how can we do better? One strategy is to try to figure out at each step which mutation is “most likely to make a difference”. And one way to do this is to try every possible mutation in turn at every step (as in multiway evolution)—and see what effect each of them has on the ultimate lifetime. From this we can construct a “change map” in which we give the change of lifetime associated with a mutation at every particular cell. The results will be different for every configuration of rule array, i.e. at every step in the adaptive evolution. But for example here’s what they are for the particular “breakthrough” configurations shown above (elements in regions that are colored gray won’t affect the result if they are changed; ones colored red will have a positive effect (with more intense red being more positive), and ones colored blue a negative one:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg3.png" alt="" title="" width="542" height="335"> </p>
</div>
<p>Let’s say we start from a random rule array, then repeatedly construct the change map and apply the mutation that it implies gives the most positive change—in effect at each step following the “path of steepest descent” to get to the lifetime we want (i.e. reduce the loss). Then the sequence of “breakthrough” configurations we get is:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg4.png" alt="" title="" width="536" height="171"> </p>
</div>
<p>And this in effect corresponds to a slightly more direct “path to a solution” than our sequence of pure single-point mutations. </p>
<p>By the way, the particular problem of reaching a certain lifetime has a simple enough structure that this “steepest descent” method—when started from a simple uniform rule array—finds a very “mechanical” (if slow) path to a solution:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg5.png" alt="" title="" width="640" height="124"> </p>
</div>
<p>What about the problem of learning <em>f</em>[<em>x</em>] = <img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg6.png" alt="" title="" width="51" height="13">? Once again we can make a change map based on the loss we define. Here are the results for a sequence of “breakthrough” configurations. The gray regions are ones where changes will be “neutral”, so that there’s still exploration that can be done without affecting the loss. The red regions are ones that are in effect “locked in” and where any changes would be deleterious in terms of loss:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg7.png" alt="" title="" width="644" height="224"> </p>
</div>
<p>So what happens in this case if we follow the “path of steepest descent”, always making the change that would be best according to the change map? Well, the results are actually quite unsatisfactory. From almost any initial condition the system quickly gets stuck, and never finds any satisfactory solution. In effect it seems that deterministically following the path of steepest descent leads us to a “local minimum” from which we cannot escape. So what are we missing in just looking at the change map? Well, the change map as we’ve constructed it has the limitation that it’s separately assessing the effect of each possible individual mutation. It doesn’t deal with multiple mutations at a time—which could well be needed in general if one’s going to find the “fastest path to success”, and avoid getting stuck.</p>
<p>But even in constructing the change map there’s already a problem. Because at least the direct way of computing it scales quite poorly. In an <em>n</em>×<em>n</em> rule array we have to check the effect of flipping about <em>n</em><sup>2</sup> values, and for each one we have to run the whole system—taking altogether about <em>n</em><sup>4</sup> operations. And one has to do this separately for each step in the learning process.</p>
<p>So how do traditional neural nets avoid this kind of inefficiency? The answer in a sense involves a mathematical trick. And at least as it’s usually presented it’s all based on the continuous nature of the weights and values in neural nets—which allow us to use methods from calculus. </p>
<p>Let’s say we have a neural net like this</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg10.png" alt="" title="" width="94" height="173"> </p>
</div>
<p>that computes some particular function <em>f</em>[<em>x</em>]:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg11.png" alt="" title="" width="148" height="95"> </p>
</div>
<p>We can ask how this function changes as we change each of the weights in the network:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg12.png" alt="" title="" width="625" height="231"> </p>
</div>
<p>And in effect this gives us something like our “change map” above. But there’s an important difference. Because the weights are continuous, we can think about infinitesimal changes to them. And then we can ask questions like “How does <em>f</em>[<em>x</em>] change when we make an infinitesimal change to a particular weight <em>w</em><sub><em>i</em></sub>?”—or equivalently, “What is the partial derivative of <em>f</em> with respect to <em>w</em><sub><em>i</em></sub> at the point <em>x</em>?” But now we get to use a key feature of infinitesimal changes: that they can always be thought of as just “adding linearly” (essentially because ε<sup>2</sup> can always be ignored to ε). Or, in other words, we can summarize any infinitesimal change just by giving its “direction” in weight space, i.e. a vector that says how much of each weight should be (infinitesimally) changed. So if we want to change <em>f</em>[<em>x</em>] (infinitesimally) as quickly as possible, we should go in the direction of steepest descent defined by all the derivatives of <em>f</em> with respect to the weights.</p>
<p>In machine learning, we’re typically trying in effect to set the weights so that the form of <em>f</em>[<em>x</em>] we generate successfully minimizes whatever loss we’ve defined. And we do this by incrementally “moving in weight space”—at every step computing the direction of steepest descent to know where to go next. (In practice, there are all sorts of tricks like “ADAM” that try to optimize the way to do this.)</p>
<p>But how do we efficiently compute the partial derivative of <em>f</em> with respect to each of the weights? Yes, we could do the analog of generating pictures like the ones above, separately for each of the weights. But it turns out that a standard result from calculus gives us a vastly more efficient procedure that in effect “maximally reuses” parts of the computation that have already been done. </p>
<p>It all starts with the textbook chain rule for the derivative of nested (i.e. composed) functions:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg16.png" alt="" title="" width="145" height="44"> </p>
</div>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg17.png" alt="" title="" width="239" height="44"> </p>
</div>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg18.png" alt="" title="" width="361" height="44"> </p>
</div>
<p>This basically says that the (infinitesimal) change in the value of the “whole chain” <em>d</em>[<em>c</em>[<em>b</em>[<em>a</em>[<em>x</em>]]]] can be computed as a product of (infinitesimal) changes associated with each of the “links” in the chain. But the key observation is then that when we get to the computation of the change at a certain point in the chain, we’ve already had to do a lot of the computation we need—and so long as we stored those results, we always have only an incremental computation to perform.</p>
<p>So how does this apply to neural nets? Well, each layer in a neural net is in effect doing a function composition. So, for example, our <em>d</em>[<em>c</em>[<em>b</em>[<em>a</em>[<em>x</em>]]]] is like a trivial neural net:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg19.png" alt="" title="" width="53" height="167"> </p>
</div>
<p>But what about the weights, which, after all, are what we are trying to find the effect of changing? Well, we could include them explicitly in the function we’re computing:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg20.png" alt="" title="" width="207" height="14"> </p>
</div>
<p>And then we could in principle symbolically compute the derivatives with respect to these weights:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg21.png" alt="" title="" width="671" height="94"> </p>
</div>
<p>For our network above</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg22.png" alt="" title="" width="52" height="74"> </p>
</div>
<p>the corresponding expression (ignoring biases) is </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg23.png" alt="" title="" width="655" height="37"> </p>
</div>
<p>where ϕ denotes our activation function. Once again we’re dealing with nested functions, and once again—though it’s a bit more intricate in this case—the computation of derivatives can be done by incrementally evaluating terms in the chain rule and in effect using the standard neural net method of “backpropagation”. </p>
<p>So what about the discrete case? Are there similar methods we can use there? We won’t discuss this in detail here, but we’ll give some indications of what’s likely to be involved.</p>
<p>As a potentially simpler case, let’s consider ordinary cellular automata. The analog of our change map asks how the value of a particular “output” cell is affected by changes in other cells—or in effect what the “partial derivative” of the output value is with respect to changes in values of other cells. </p>
<p>For example, consider the highlighted “output” cell in this cellular automaton evolution:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg24.png" alt="" title="" width="178" height="98"> </p>
</div>
<p>Now we can look at each cell in this array, and make a change map based on seeing whether flipping the value of just that cell (and then running the cellular automaton forwards from that point) would change the value of the output cell:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg25.png" alt="" title="" width="183" height="100"> </p>
</div>
<p>The form of the change map is different if we look at different “output cells”:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg26.png" alt="" title="" width="630" height="125"> </p>
</div>
<p>Here, by the way, are some larger change maps for this and a couple of other cellular automaton rules:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg27.png" alt="" title="" width="580" height="154"> </p>
</div>
<p>But is there a way to construct such change maps incrementally? One might have thought that there would immediately be at least for <a href="https://www.wolframscience.com/nks/chap-9--fundamental-physics#sect-9-2--the-notion-of-reversibility">cellular automata that (unlike the cases here) are fundamentally reversible</a>. But actually such reversibility doesn’t seem to help much—because although it allows us to “backtrack” whole states of the cellular automaton, it doesn’t allow us to trace the separate effects of individual cells. </p>
<p>So how about using discrete analogs of derivatives and the chain rule? Let’s for example call the function computed by one step in rule 30 cellular automaton evolution <em>w</em>[<em>x</em>, <em>y</em>, <em>z</em>]. We can think of the “partial derivative” of this function with respect to <em>x</em> at the point <em>x</em> as representing whether the output of <em>w</em> changes when <em>x</em> is flipped starting from the value given:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg28.png" alt="" title="" width="631" height="184"> </p>
</div>
<p>(Note that “no change” is indicated as <tt><a href="http://reference.wolfram.com/language/ref/False.html">False</a></tt> or <img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg29.png" alt="" title="" width="9" height="9">, while a change is indicated as <tt><a href="http://reference.wolfram.com/language/ref/True.html">True</a></tt> or <img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg30.png" alt="" title="" width="9" height="9">. And, yes, one can either explicitly compute the rule outcomes here, and then deduce from them the functional form, or one can use symbolic rules to directly deduce the functional form.)</p>
<p>One can compute a discrete analog of a derivative for any Boolean function. For example, we have</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg31.png" alt="" title="" width="71" height="13"> </p>
</div>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg32.png" alt="" title="" width="120" height="14"> </p>
</div>
<p>and</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg33.png" alt="" title="" width="147" height="14"> </p>
</div>
<p>which we can write as: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg34.png" alt="" title="" width="212" height="24"> </p>
</div>
<p>We also have:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg35.png" alt="" title="" width="158" height="14"> </p>
</div>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg36.png" alt="" title="" width="212" height="24"> </p>
</div>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg37.png" alt="" title="" width="192" height="14"> </p>
</div>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg38.png" alt="" title="" width="212" height="24"> </p>
</div>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg39.png" alt="" title="" width="222" height="14"> </p>
</div>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg40.png" alt="" title="" width="212" height="24"> </p>
</div>
<p>And here is a table of “Boolean derivatives” for all 2-input Boolean functions:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg41.png" alt="" title="" width="572" height="329"> </p>
</div>
<p>And indeed there’s a whole “Boolean calculus” one can set up for these kinds of derivatives. And in particular, there’s a direct analog of the chain rule:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg42.png" alt="" title="" width="262" height="14"> </p>
</div>
<p>where <tt><a href="http://reference.wolfram.com/language/ref/Xnor.html">Xnor</a></tt><tt>[x,y]</tt> is effectively the equality test <em>x</em> == <em>y</em>:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg43.png" alt="" title="" width="114" height="32"> </p>
</div>
<p>But, OK, how do we use this to create our change maps? In our simple cellular automaton case, we can think of our change map as representing how a <a href="https://www.wolframscience.com/nks/p604--cryptography-and-cryptanalysis/">change in an output cell “propagates back”</a> to previous cells. But if we just try to apply our discrete calculus rules we run into a problem: different “chain rule chains” can imply different changes in the value of the same cell. In the continuous case this path dependence doesn’t happen because of the way infinitesimals work. But in the discrete case it does. And ultimately we’re doing a kind of backtracking that can really be represented faithfully only as a multiway system. (Though if we just want probabilities, for example, we can consider <a href="https://www.wolframphysics.org/bulletins/2021/02/multiway-turing-machines/" target="_blank" rel="noopener">averaging over branches of the multiway system</a>—and the change maps we showed above are effectively the result of thresholding over the multiway system.)</p>
<p>But despite the appearance of such difficulties in the “simple” cellular automaton case, such methods typically seem to work better in our original, more complicated rule array case. There’s a bunch of subtlety associated with the fact that we’re finding derivatives not only with respect to the values in the rule array, but also with respect to the choice of rules (which are the analog of weights in the continuous case). </p>
<p>Let’s consider the <tt>And</tt>+<tt>Xor</tt> rule array:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg44.png" alt="" title="" width="171" height="162"> </p>
</div>
<p>Our loss is the number of cells whose values disagree with the row shown at the bottom. Now we can construct a change map for this rule array both in a direct “forward” way, and “backwards” using our discrete derivative methods (where we effectively resolve the small amount of “multiway behavior” by always picking “majority” values): </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg45.png" alt="" title="" width="293" height="149"> </p>
</div>
<p>The results are similar, though in this case not exactly the same. Here are a few other examples:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg46.png" alt="" title="" width="596" height="374"> </p>
</div>
<p>And, yes, in detail there are essentially always local differences between the results from the forward and backward methods. But the backward method—like in the case of backpropagation in ordinary neural nets—can be implemented much more efficiently. And for purposes of practical machine learning it’s actually likely to be perfectly satisfactory—especially given that the forward method is itself only providing an approximation to the question of which mutations are best to do. </p>
<p>And as an example, here are the results of the forward and backward methods for the problem of learning the function <em>f</em>[<em>x</em>] = <img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg47.png" alt="" title="" width="51" height="13">, for the “breakthrough” configurations that we showed above:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024optimizingimg48.png" alt="" title="" width="639" height="222"> </p>
</div>
<h2 id="what-can-be-learned">What Can Be Learned?</h2>
<p>We’ve now shown quite a few examples of machine learning in action. But a fundamental question we haven’t yet addressed is what kind of thing can actually be learned by machine learning. And even before we get to this, there’s another question: given a particular underlying type of system, what kinds of functions can it even represent?</p>
<p>As a first example consider a minimal neural net of the form (essentially a single-layer perceptron):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08222024MLupdatesAimg5.png" alt="" title="" width="318" height="139"> </p>
</div>
<p>With ReLU (AKA <tt><a href="http://reference.wolfram.com/language/ref/Ramp.html">Ramp</a></tt>) as the activation function and the first set of weights all taken to be 1, the function computed by such a neural net has the form:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedimg2.png" alt="" title="" width="204" height="14"> </p>
</div>
<p>With enough weights and biases this form can represent any piecewise linear function—essentially just by moving around ramps using biases, and scaling them using weights. So for example consider the function:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedimg3.png" alt="" title="" width="261" height="75"> </p>
</div>
<p>This is the function computed by the neural net above—and here’s how it’s built up by adding in successive ramps associated with the individual intermediate nodes (neurons):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedimg4.png" alt="" title="" width="598" height="200"> </p>
</div>
<p>(It’s similarly possible to get all smooth functions from activation functions like ELU, etc.)</p>
<p>Things get slightly more complicated if we try to represent functions with more than one argument. With a single intermediate layer we can only get “piecewise (hyper)planar” functions (i.e. functions that change direction only at linear “fault lines”):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedimg5.png" alt="" title="" width="291" height="241"> </p>
</div>
<p>But already with a total of two intermediate layers—and sufficiently many nodes in each of these layers—we can generate any piecewise function of any number of arguments. </p>
<p>If we limit the number of nodes, then roughly we limit the number of boundaries between different linear regions in the values of the functions. But as we increase the number of layers with a given number of nodes, we basically increase the number of sides that polygonal regions within the function values can have:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedimg6.png" alt="" title="" width="492" height="298"> </p>
</div>
<p>So what happens with the mesh nets that we discussed earlier?  Here are a few random examples, showing results very similar to shallow, fully connected networks with a comparable total number of nodes:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedimg7.png" alt="" title="" width="443" height="352"> </p>
</div>
<p>OK, so how about our fully discrete rule arrays? What functions can they represent? We already saw part of the answer earlier when we generated rule arrays to represent various Boolean functions. It turns out that there is a fairly efficient procedure based on <a href="https://reference.wolfram.com/language/ref/SatisfiabilityInstances.html">Boolean satisfiability</a> for explicitly finding rule arrays that can represent a given function—or determine that no rule array (say of a given size) can do this. </p>
<p>Using this procedure, we can find minimal <tt>And</tt>+<tt>Xor</tt> rule arrays that represent all (“even”) 3-input Boolean functions (i.e. <em>r</em> = 1 cellular automaton rules):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedimg8.png" alt="" title="" width="641" height="462"> </p>
</div>
<p>It’s always possible to specify any <em>n</em>-input Boolean function by an array of 2<sup><em>n</em></sup> bits, as in:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedimg10.png" alt="" title="" width="206" height="22"> </p>
</div>
<p>But we see from the pictures above that when we “compile” Boolean functions into <tt>And</tt>+<tt>Xor</tt> rule arrays, they can take different numbers of bits (i.e. different numbers of elements in the rule array). (In effect, the “algorithmic information content” of the function varies with the “language” we’re using to represent them.) And, for example, in the <em>n </em>= 3 case shown here, the distribution of minimal rule array sizes is:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedimg11.png" alt="" title="" width="216" height="114"> </p>
</div>
<p>There are some functions that are difficult to represent as <tt>And</tt>+<tt>Xor</tt> rule arrays (and seem to require 15 rule elements)—and others that are easier. And this is similar to what happens if we represent Boolean functions as Boolean expressions (say in conjunctive normal form) and <a href="https://www.wolframscience.com/nks/notes-10-11--boolean-formula-sizes/">count the total number of (unary and binary) operations used</a>:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedimg12.png" alt="" title="" width="366" height="116"> </p>
</div>
<p>OK, so we know that there is in principle an <tt>And</tt>+<tt>Xor</tt> rule array that will compute any (even) Boolean function. But now we can ask whether an adaptive evolution process can actually find such a rule array—say with a sequence of single-point mutations. Well, if we do such adaptive evolution—with a loss that counts the number of “wrong outputs” for, say, rule 254—then here’s a sequence of successive breakthrough configurations that can be produced:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedimg13.png" alt="" title="" width="578" height="320"> </p>
</div>
<p>The results aren’t as compact as the minimal solution above. But it seems to always be possible to find at least some <tt>And</tt>+<tt>Xor</tt> rule array that “solves the problem” just by using adaptive evolution with single-point mutations.</p>
<p>Here are results for some other Boolean functions:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedimg14.png" alt="" title="" width="531" height="579"> </p>
</div>
<p>And so, yes, not only are all (even) Boolean functions representable in terms of <tt>And</tt>+<tt>Xor</tt> rule arrays, they’re also learnable in this form, just by adaptive evolution with single-point mutations.</p>
<p>In what we did above, we were looking at how machine learning works with our rule arrays in specific cases like for the <img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedimg15.png" alt="" title="" width="51" height="13"> function. But now we’ve got a case where we can explicitly enumerate all possible functions, at least of a given class. And in a sense what we’re seeing is evidence that machine learning tends to be very broad—and capable at least in principle of learning pretty much any function. </p>
<p>Of course, there can be specific restrictions. Like the <tt>And</tt>+<tt>Xor</tt> rule arrays we’re using here can’t represent (“odd”) functions where <img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedimg16.png" alt="" title="" width="69" height="9">. (The <tt><a href="https://reference.wolfram.com/language/ref/Nand.html">Nand</a></tt>+<tt><a href="https://reference.wolfram.com/language/ref/First.html">First</a></tt> rule arrays we discussed above nevertheless can.) But in general it seems to be a reflection of the <a href="https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence/">Principle of Computational Equivalence</a> that pretty much any setup is capable of representing any function—and also adaptively “learning” it. </p>
<p>By the way, it’s a lot easier to discuss questions about representing or learning “any function” when one’s dealing with discrete (countable) functions—because one can expect to either be able to “exactly get” a given function, or not. But for continuous functions, it’s more complicated, because one’s pretty much inevitably dealing with approximations (unless one can use symbolic forms, which are basically discrete). So, for example, while we can say (as we did above) that (ReLU) neural nets can represent any piecewise-linear function, in general we’ll only be able to imagine successively approaching an arbitrary function, much like when you progressively add more terms in a simple Fourier series:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedimg17.png" alt="" title="" width="639" height="74"> </p>
</div>
<p>Looking back at our results for discrete rule arrays, one notable observation that is that while we can successfully reproduce all these different Boolean functions, the actual rule array configurations that achieve this tend to look quite messy. And indeed it’s much the same as we’ve seen throughout: machine learning can find solutions, but they’re not “structured solutions”; they’re in effect just solutions that “happen to work”.</p>
<p>Are there more structured ways of representing Boolean functions with rule arrays? Here are the two possible minimum-size <tt>And</tt>+<tt>Xor</tt> rule arrays that represent rule 30:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedimg18.png" alt="" title="" width="144" height="60"> </p>
</div>
<p>At the next-larger size there are more possibilities for rule 30:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedimg19.png" alt="" title="" width="457" height="45"> </p>
</div>
<p>And there are also rule arrays that can represent rule 110:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedimg20.png" alt="" title="" width="595" height="45"> </p>
</div>
<p>But in none of these cases is there obvious structure that allows us to immediately see how these computations work, or what function is being computed. But what if we try to explicitly construct—effectively by standard engineering methods—a rule array that computes a particular function? We can start by taking something like the function for rule 30 and writing it in terms of <tt>And </tt>and <tt>Xor</tt> (i.e. in <a href="https://reference.wolfram.com/language/ref/BooleanConvert.html">ANF, or “algebraic normal form”</a>):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedimg21.png" alt="" title="" width="225" height="14"> </p>
</div>
<p>We can imagine implementing this using an “evaluation graph”:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08222024MLupdatesAimg6A.png" alt="" title="" width="108" height="112"> </p>
</div>
<p>But now it’s easy to turn this into a rule array (and, yes, we haven’t gone all the way and arranged to copy inputs, etc.):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08222024MLupdatesAimg7.png" alt="" title="" width="101" height="108"> </p>
</div>
<p>“Evaluating” this rule array for different inputs, we can see that it indeed gives rule 30:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedAimg24.png" alt="" title="" width="533" height="58"> </p>
</div>
<p>Doing the same thing for rule 110, the <tt>And</tt>+<tt>Xor</tt> expression is</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedimg28.png" alt="" title="" width="342" height="14"> </p>
</div>
<p>the evaluation graph is</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08222024MLupdatesAimg8A.png" alt="" title="" width="112" height="114"> </p>
</div>
<p>and the rule array is:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08222024MLupdatesAimg9.png" alt="" title="" width="100" height="102"> </p>
</div>
<p>And at least with the evaluation graph as a guide, we can readily “see what’s happening” here. But the rule array we’re using is considerably larger than our minimal solutions above—or even than the solutions we found by adaptive evolution.</p>
<p>It’s a typical situation that one sees in many other kinds of systems (like for example <a href="https://www.wolframscience.com/nks/notes-12-8--sorting-networks/">sorting networks</a>): it’s possible to have a “constructed solution” that has clear structure and regularity and is “understandable”. But minimal solutions—or ones found by adaptive evolution—tend to be much smaller. But they almost always look in many ways random, and aren’t readily understandable or interpretable.</p>
<p>So far, we’ve been looking at rule arrays that compute specific functions. But in getting a sense of what rule arrays can do, we can consider rule arrays that are “<a href="https://www.wolframscience.com/nks/chap-11--the-notion-of-computation/">programmable</a>”, in that their input specifies what function they should compute. So here, for example, is an <tt>And</tt>+<tt>Xor</tt> rule array—found by adaptive evolution—that takes the “bit pattern” of any (even) Boolean function as input on the left, then applies that Boolean function to the inputs on the right:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedimg33.png" alt="" title="" width="269" height="250"> </p>
</div>
<p>And with this same rule array we can now compute any possible (even) Boolean function. So here, for example, it’s evaluating <tt><a href="https://reference.wolfram.com/language/ref/And.html">Or</a></tt>:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024learnedimg34.png" alt="" title="" width="648" height="142"> </p>
<h2 id="other-kinds-of-models-and-setups">Other Kinds of Models and Setups</h2>
<p>Our general goal here has been to set up models that capture the most essential features of neural nets and machine learning—but that are simple enough in their structure that we can readily “look inside” and get a sense of what they are doing. Mostly we’ve concentrated on rule arrays as a way to provide a minimal analog of standard “perceptron-style” feed-forward neural nets. But what about other architectures and setups?</p>
<p>In effect, our rule arrays are “spacetime-inhomogeneous” generalizations of cellular automata—in which adaptive evolution determines which rule (say from a finite set) should be used at every (spatial) position and every (time) step. A different idealization (that in fact we already used in <a href="https://writings.stephenwolfram.com/2024/08/whats-really-going-on-in-machine-learning-some-minimal-models/#making-everything-discrete-a-biological-evolution-analog">one section above</a>) is to have an ordinary homogeneous cellular automaton—but with a single “global rule” determined by adaptive evolution. Rule arrays are the analog of feed-forward networks in which a given rule in the rule array is in effect used only once as data “flows through” the system. Ordinary homogeneous cellular automata are like recurrent networks in which a single stream of data is in effect subjected over and over again to the same rule.</p>
<p>There are various interpolations between these cases. For example, we can imagine a “layered rule array” in which the rules at different steps can be different, but those on a given step are all the same. Such a system can be viewed as an idealization of a convolutional neural net in which a given layer applies the same kernel to elements at all positions, but different layers can apply different kernels.</p>
<p>A layered rule array can’t encode as much information as a general rule array. But it’s still able to show machine-learning-style phenomena. And here, for example, is adaptive evolution for a layered <tt>And</tt>+<tt>Xor</tt> rule array progressively solving the problem of generating a pattern that lives for exactly 30 steps:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024modelsimg1.png" alt="" title="" width="591" height="329"> </p>
</div>
<p>One could also imagine “vertically layered” rule arrays, in which different rules are used at different positions, but any given position keeps running the same rule forever. However, at least for the kinds of problems we’ve considered here, it doesn’t seem sufficient to just be able to pick the positions at which different rules are run. One seems to either need to change rules at different (time) steps, or one needs to be able to adaptively evolve the underlying rules themselves.</p>
<p>Rule arrays and ordinary cellular automata share the feature that the value of each cell depends only on the values of neighboring cells on the step before. But in neural nets it’s standard for the value at a given node to depend on the values of lots of nodes on the layer before. And what makes this straightforward in neural nets is that (weighted, and perhaps otherwise transformed) values from previous nodes are taken to be combined just by simple numerical addition—and addition (being <em>n</em>-ary and associative) can take any number of “inputs”. In a cellular automaton (or Boolean function), however, there’s always a definite number of inputs, determined by the structure of the function. In the most straightforward case, the inputs come only from nearest-neighboring cells. But there’s no requirement that this is how things need to work—and for example we can pick any “local template” to bring in the inputs for our function. This template could either be the same at every position and every step, or it could be picked from a certain set differently at different positions—in effect giving us “template arrays” as well as rule arrays.</p>
<p>So what about having a fully connected network, as we did in our very first neural net examples above? To set up a discrete analog of this we first need some kind of discrete <em>n</em>-ary associative “accumulator” function to fill the place of numerical addition. And for this <a href="https://www.wolframscience.com/nks/notes-12-9--properties-of-logical-primitives/">we could pick a function like</a> <tt><a href="http://reference.wolfram.com/language/ref/And.html">And</a></tt>, <tt><a href="http://reference.wolfram.com/language/ref/Or.html">Or</a></tt>, <tt><a href="http://reference.wolfram.com/language/ref/Xor.html">Xor</a></tt>—or <tt><a href="http://reference.wolfram.com/language/ref/Majority.html">Majority</a></tt>. And if we’re not just going to end up with the same value at each node on a given layer, we need to set up some analog of a weight associated with each connection—which we can achieve by applying either <tt><a href="http://reference.wolfram.com/language/ref/Identity.html">Identity</a></tt> or <tt><a href="http://reference.wolfram.com/language/ref/Not.html">Not</a></tt> (i.e. flip or not) to the value flowing through each connection. </p>
<p>Here’s an example of a network of this type, trained to compute the <img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024modelsimg2.png" alt="" title="" width="51" height="13"> function we discussed above:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024modelsimg3.png" alt="" title="" width="591" height="138"> </p>
</div>
<p>There are just two kinds of connections here: flip and not. And at each node we’re computing the majority function—giving value 1 if the majority of its inputs are 1, and 0 otherwise. With the “one-hot encoding” of input and output that we used before, here are a few examples of how this network evaluates our function:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024modelsimg4.png" alt="" title="" width="613" height="126"> </p>
</div>
<p>This was trained just using 1000 steps of single-point mutation applied to the connection types. The loss systematically goes down—but the configuration of the connection types continues to look quite random even as it achieves zero loss (i.e. even after the function has been completely learned):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024modelsimg5.png" alt="" title="" width="501" height="200"> </p>
</div>
<p>In what we’ve just done we assume that all connections continue to be present, though their types (or effectively signs) can change. But we can also consider a network where connections can end up being zeroed out during training—so that they are effectively no longer present. </p>
<p>Much of what we’ve done here with machine learning has centered around trying to learn transformations of the form <em>x </em><img src="https://content.wolfram.com/uploads/sites/32/2022/10/rightarrow2.png" width="15" height="11"> <em>f</em>[<em>x</em>]. But another typical application of machine learning is autoencoding—or in effect learning how to compress data representing a certain set of examples. And once again it’s possible to do such a task using rule arrays, with learning achieved by a series of single-point mutations. </p>
<p>As a starting point, consider training a rule array (of cellular automaton rules 4 and 146) to reproduce unchanged a block of black cells of any width. One might have thought this would be trivial. But it’s not, because in effect the initial data inevitably gets “ground up” inside the rule array, and has to be reconstituted at the end. But, yes, it’s nevertheless possible to train a rule array to at least roughly do this—even though once again the rule arrays we find that manage to do this look quite random:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024modelsimg6.png" alt="" title="" width="668" height="261"> </p>
</div>
<p>But to set up a nontrivial autoencoder let’s imagine that we progressively “squeeze” the array in the middle, creating an increasingly narrow “bottleneck” through which the data has to flow. At the bottleneck we effectively have a compressed version of the original data. And we find that at least down to some width of bottleneck, it’s possible to create rule arrays that—with reasonable probability—can act as successful autoencoders of the original data:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024modelsimg7.png" alt="" title="" width="679" height="701"> </p>
</div>
<p>The success of LLMs has highlighted the use of machine learning for sequence continuation—and the effectiveness of transformers for this. But just as with other neural nets, the forms of transformers that are used in practice are typically very complicated. But can one find a minimal model that nevertheless captures the “essence of transformers”?</p>
<p>Let’s say that we have a sequence that we want to continue, like:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024modelsimg8.png" alt="" title="" width="315" height="87"> </p>
</div>
<p>We want to encode each possible value by a vector, as in</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024modelsimg9.png" alt="" title="" width="198" height="57"> </p>
</div>
<p>so that, for example, our original sequence is encoded as:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024modelsimg10.png" alt="" title="" width="304" height="32"> </p>
</div>
<p>Then we have a “head” that reads a block of consecutive vectors, picking off certain values and feeding pairs of them into <tt>And</tt> and <tt>Xor</tt> functions, to get a vector of Boolean values:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024modelsimg11.png" alt="" title="" width="224" height="147"> </p>
</div>
<p>Ultimately this head is going to “slide” along our sequence, “predicting” what the next element in the sequence will be. But somehow we have to go from our vector of Boolean values to (probabilities of) sequence elements. Potentially we might be able to do this just with a rule array. But for our purposes here we’ll use a fully connected single-layer <tt><a href="https://reference.wolfram.com/language/ref/Identity.html">Identity</a></tt>+<tt><a href="https://reference.wolfram.com/language/ref/Not.html">Not</a></tt> network in which at each output node we just find the sum of the number of values that come to it—and treat this as determining (through a <a href="https://reference.wolfram.com/language/ref/SoftmaxLayer.html">softmax</a>) the probability of the corresponding element:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024modelsimg12.png" alt="" title="" width="548" height="115"> </p>
</div>
<p>In this case, the element with the maximum value is 5, so at “zero temperature” this would be our “best prediction” for the next element. </p>
<p>To train this whole system we just make a sequence of random point mutations to everything, keeping mutations that don’t increase the loss (where the loss is basically the difference between predicted next values and actual next values, or, more precisely, the “<a href="https://reference.wolfram.com/language/ref/CrossEntropyLossLayer.html">categorical cross-entropy</a>”). Here’s how this loss progresses in a typical such training:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08222024MLupdatesAimg10.png" alt="" title="" width="358" height="142"> </p>
</div>
<p>At the end of this training, here are the components of our minimal transformer:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024modelsimg14.png" alt="" title="" width="616" height="444"> </p>
</div>
<p>First come the encodings of the different possible elements in the sequence.  Then there’s the head, here shown applied to the encoding of the first elements of the original sequence.  Finally there’s a single-layer discrete network that takes the output from the head, and deduces relative probabilities for different elements to come next.  In this case the highest-probability prediction  for the next element is that it should be element 6.</p>
<p>To do the analog of an LLM we start from some initial “prompt”, i.e. an initial sequence that fits within the width (“context window”) of the head.  Then we progressively apply our minimal transformer, for example at each step taking the next element to be the one with the highest predicted probability (i.e. operating “at zero temperature”). With this setup the collection of “prediction strengths” is shown in gray, with the “best prediction” shown in red:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024modelsimg15.png" alt="" title="" width="671" height="86"> </p>
</div>
<p>Running this even far beyond our original training data, we see that we get a “prediction” of a continued sine wave:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024modelsimg16.png" alt="" title="" width="665" height="42"> </p>
</div>
<p>As we might expect, the fact that our minimal transformer can make such a plausible prediction relies on the simplicity of our sine curve.  If we use “more complicated” training data, such as the “mathematically defined” (<span><img src="https://content.wolfram.com/sites/43/2024/08/sw08212024modelsimg17.png" width="128" height="23"></span>) blue curve in</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024modelsimg18.png" alt="" title="" width="659" height="93"> </p>
</div>
<p>the result of training and running a minimal transformer is now:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024modelsimg19.png" alt="" title="" width="671" height="86"> </p>
</div>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024modelsimg20.png" alt="" title="" width="665" height="42"> </p>
</div>
<p>And, not surprisingly, it can’t “figure out the computation” to correctly continue the curve.  By the way, different training runs will involve different sequences of mutations, and will yield different predictions (often with periodic “hallucinations”):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08212024modelsimg21.png" alt="" title="" width="648" height="307"> </p>
</div>
<p>In looking at “perceptron-style” neural nets we wound up using rule arrays<tt>—</tt>or, in effect, spacetime-inhomogeneous cellular automata<tt>—</tt>as our minimal models. Here we’ve ended up with a slightly more complicated minimal model for transformer neural nets. But if we were to simplify it further, we would end up not with something like a cellular automaton but instead with something like a <a href="https://www.wolframscience.com/nks/p93--tag-systems/">tag system</a>, in which one has a sequence of elements, and at each step removes a block from the beginning, and<tt>—</tt>depending on its form<tt>—</tt>adds a certain block at the end, as in:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/08/sw08222024modelsAimg22.png" alt="" title="" width="632" height="402"> </p>
</div>
<p>And, yes, such systems <a href="https://writings.stephenwolfram.com/2021/03/after-100-years-can-we-finally-crack-posts-problem-of-tag-a-story-of-computational-irreducibility-and-more/">can generate extremely complex behavior</a><tt>—</tt>reinforcing the idea (that we have repeatedly seen here) that machine learning works by selecting complexity that aligns with goals that have been set.</p>
<p>And along these lines, one can consider all sorts of different computational systems as foundations for machine learning. Here we’ve been looking at cellular-automaton-like and tag-system-like examples. But for example our <a href="https://www.wolframphysics.org/" target="_blank" rel="noopener">Physics Project</a> has shown us the power and flexibility of systems based on <a href="https://www.wolframphysics.org/technical-introduction/basic-form-of-models/" target="_blank" rel="noopener">hypergraph rewriting</a>. And from what we’ve seen here, it seems very plausible that something like hypergraph rewriting can serve as a yet more powerful and flexible substrate for machine learning.</p>
<h2 id="so-in-the-end-whats-really-going-on-in-machine-learning">So in the End, What’s Really Going On in Machine Learning?</h2>
<p>There are, I think, several quite striking conclusions from what we’ve been able to do here. The first is just that models much simpler than traditional neural nets seem capable of capturing the essential features of machine learning—and indeed these models may well be the basis for a new generation of practical machine learning.</p>
<p>But from a scientific point of view, one of the things that’s important about these models is that they are simple enough in structure that it’s immediately possible to produce visualizations of what they’re doing inside. And studying these visualizations, the most immediately striking feature is how complicated they look. </p>
<p>It could have been that machine learning would somehow “crack systems”, and find simple representations for what they do. But that doesn’t seem to be what’s going on at all. Instead what seems to be happening is that machine learning is in a sense just “hitching a ride” on the <a href="https://www.wolframscience.com/nks/chap-3--the-world-of-simple-programs/">general richness of the computational universe</a>. It’s not “specifically building up behavior one needs”; rather what it’s doing is to harness behavior that’s “already out there” in the computational universe.</p>
<p>The fact that this could possibly work relies on the crucial—and at first unexpected—fact that in the computational universe even very simple programs can ubiquitously produce all sorts of complex behavior. And the point then is that this behavior has enough richness and diversity that it’s possible to find instances of it that align with machine learning objectives one’s defined. In some sense what machine learning is doing is to “mine” the computational universe for programs that do what one wants. </p>
<p>It’s not that machine learning nails a specific precise program. Rather, it’s that in typical successful applications of machine learning there are lots of programs that “do more or less the right thing”. If what one’s trying to do involves something computationally irreducible, machine learning won’t typically be able to “get well enough aligned” to correctly “get through all the steps” of the irreducible computation. But it seems that <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/#models-for-human-like-tasks">many “human-like tasks”</a> that are the particular focus of modern machine learning can successfully be done. </p>
<p>And by the way, one can expect that with the minimal models explored here, it becomes more feasible to get a real characterization of what kinds of objectives can successfully be achieved by machine learning, and what cannot. Critical to the operation of machine learning is not only that there exist programs that can do particular kinds of things, but also that they can realistically be found by adaptive evolution processes.</p>
<p>In what we’ve done here we’ve often used what’s essentially the very simplest possible process for adaptive evolution: a sequence of point mutations. And what we’ve discovered is that even this is usually sufficient to lead us to satisfactory machine learning solutions. It could be that our paths of adaptive evolution would always be getting stuck—and not reaching any solution. But the fact that this doesn’t happen seems <a href="https://writings.stephenwolfram.com/2024/05/why-does-biological-evolution-work-a-minimal-model-for-biological-evolution-and-other-adaptive-processes/#the-fitness-landscape">crucially connected to the computational irreducibility</a> that’s ubiquitous in the systems we’re studying, and that leads to effective randomness that with overwhelming probability will “give us a way out” of anywhere we got stuck.</p>
<p>In some sense computational irreducibility “levels the playing field” for different processes of adaptive evolution, and lets even simple ones be successful. Something similar seems to happen for the whole framework we’re using. Any of a wide class of systems seem capable of successful machine learning, even if they don’t have the detailed structure of traditional neural nets. We can see this as a typical reflection of the <a href="https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence/">Principle of Computational Equivalence</a>: that even though systems may differ in their details, they are ultimately all equivalent in the computations they can do.</p>
<p>The phenomenon of computational irreducibility leads to a fundamental tradeoff, of <a href="https://writings.stephenwolfram.com/2023/03/will-ais-take-all-our-jobs-and-end-human-history-or-not-well-its-complicated/">particular importance in thinking about things like AI</a>. If we want to be able to know in advance—and broadly guarantee—what a system is going to do or be able to do, we have to set the system up to be computationally reducible. But if we want the system to be able to make the richest use of computation, it’ll inevitably be capable of computationally irreducible behavior. And it’s the same story with machine learning. If we want machine learning to be able to do the best it can, and perhaps give us the impression of “achieving magic”, then we have to allow it to show computational irreducibility. And if we want machine learning to be “understandable” it has to be computationally reducible, and not able to access the full power of computation.</p>
<p>At the outset, though, it’s not obvious whether machine learning actually has to access such power. It could be that there are computationally reducible ways to solve the kinds of problems we want to use machine learning to solve. But what we’ve discovered here is that even in solving very simple problems, the adaptive evolution process that’s at the heart of machine learning will end up sampling—and using—what we can expect to be computationally irreducible processes. </p>
<p>Like biological evolution, machine learning is fundamentally about finding things that work—without the constraint of “understandability” that’s forced on us when we as humans explicitly engineer things step by step. Could one imagine constraining machine learning to make things understandable? To do so would effectively prevent machine learning from having access to the power of computationally irreducible processes, and from the evidence here it seems unlikely that with this constraint the kind of successes we’ve seen in machine learning would be possible.</p>
<p>So what does this mean for the “science of machine learning”? One might have hoped that one would be able to “look inside” machine learning systems and get detailed narrative explanations for what’s going on; that in effect one would be able to “explain the mechanism” for everything. But what we’ve seen here suggests that in general nothing like this will work. All one will be able to say is that somewhere out there in the computational universe there’s some (typically computationally irreducible) process that “happens” to be aligned with what we want. </p>
<p>Yes, we can make general statements—strongly based on computational irreducibility—about things like the findability of such processes, say by adaptive evolution. But if we ask “How in detail does the system work?”, there won’t be much of an answer to that. Of course we can trace all its computational steps and see that it behaves in a certain way. But we can’t expect what amounts to a “global human-level explanation” of what it’s doing. Rather, we’ll basically just be reduced to looking at some computationally irreducible process and observing that it “happens to work”—and we won’t have a high-level explanation of “why”.</p>
<p>But there is one important loophole to all this. Within any computationally irreducible system, there are always inevitably pockets of computational reducibility. And—as I’ve <a href="https://writings.stephenwolfram.com/2023/12/observer-theory/">discussed at length particularly in connection with our Physics Project</a>—it’s these pockets of computational reducibility that allow computationally bounded observers like us to identify things like “laws of nature” from which we can build “human-level narratives”.</p>
<p>So what about machine learning? What pockets of computational reducibility show up there, from which we might build “human-level scientific laws”? Much as with the emergence of “simple continuum behavior” from computationally irreducible processes happening at the level of molecules in a gas or ultimate discrete elements of space, we can expect that at least certain computationally reducible features will be more obvious when one’s dealing with larger numbers of components. And indeed in sufficiently large machine learning systems, it’s routine to see smooth curves and apparent regularity when one’s looking at the kind of aggregated behavior that’s probed by things like training curves.</p>
<p>But the question about pockets of reducibility is always whether they end up being aligned with things we consider interesting or useful. Yes, it could be that machine learning systems would exhibit some kind of collective (“EEG-like”) behavior. But what’s not clear is whether this behavior will tell us anything about the actual “information processing” (or whatever) that’s going on in the system. And if there is to be a “science of machine learning” what we have to hope for is that we can find in machine learning systems pockets of computational reducibility that are aligned with things we can measure, and care about.</p>
<p>So given what we’ve been able to explore here about the foundations of machine learning, what can we say about the <a href="https://writings.stephenwolfram.com/2024/03/can-ai-solve-science/">ultimate power of machine learning systems</a>? A key observation has been that machine learning works by “piggybacking” on computational irreducibility—and in effect by finding “natural pieces of computational irreducibility” that happen to fit with the objectives one has. But what if those objectives involve computational irreducibility—as they often do when one’s dealing with a process that’s been successfully formalized in computational terms (as in math, exact science, computational X, etc.)? Well, it’s not enough that our machine learning system “uses some piece of computational irreducibility inside”. To achieve a particular computationally irreducible objective, the system would have to do something closely aligned with that actual, specific objective. </p>
<p>It has to be said, however, that by laying bare more of the essence of machine learning here, it becomes easier to at least define the issues of merging typical “formal computation” with machine learning. Traditionally there’s been a tradeoff between the computational power of a system and its trainability. And indeed in terms of what we’ve seen here this seems to reflect the sense that “larger chunks of computational irreducibility” are more difficult to fit into something one’s incrementally building up by a process of adaptive evolution.</p>
<p>So how should we ultimately think of machine learning? In effect its power comes from leveraging the “natural resource” of computational irreducibility. But when it uses computational irreducibility it does so by “foraging” pieces that happen to advance its objectives. Imagine one’s building a wall. One possibility is to fashion bricks of a particular shape that one knows will fit together. But another is just to look at stones one sees lying around, then to build the wall by fitting these together as best one can. </p>
<p>And if one then asks “Why does the wall have such-and-such a pattern?” the answer will end up being basically “Because that’s what one gets from the stones that happened to be lying around”. There’s no overarching theory to it in itself; it’s just a reflection of the resources that were out there. Or, in the case of machine learning, one can expect that what one sees will be to a large extent a reflection of the raw characteristics of computational irreducibility. In other words, the foundations of machine learning are as much as anything rooted in the <a href="https://writings.stephenwolfram.com/2021/09/charting-a-course-for-complexity-metamodeling-ruliology-and-more/#the-pure-basic-science-of-ruliology">science of ruliology</a>. And it’s in large measure to that science we should look in our efforts to understand more about “what’s really going on” in machine learning, and quite possibly also in neuroscience.</p>
<h2 id="historical--personal-notes">Historical &amp; Personal Notes</h2>
<p>In some ways it seems like a quirk of intellectual history that the kinds of foundational questions I’ve been discussing here weren’t already addressed long ago—and in some ways it seems like an inexorable consequence of the only rather recent development of certain intuitions and tools.</p>
<p>The idea that the brain is fundamentally made of connected nerve cells was considered in the latter part of the nineteenth century, and took hold in the first decades of the twentieth century—with the <a href="https://www.wolframscience.com/nks/notes-10-12--history-of-ideas-about-thinking/">formalized concept of a neural net</a> that operates in a computational way emerging in full form in the work of Warren McCulloch and Walter Pitts in 1943. By the late 1950s there were hardware implementations of neural nets (typically for image processing) in the form of “perceptrons”. But despite early enthusiasm, practical results were mixed, and at the end of the 1960s it was announced that simple cases amenable to mathematical analysis had been “solved”—leading to a general belief that “neural nets couldn’t do anything interesting”.</p>
<p>Ever since the 1940s there had been a trickle of general analyses of neural nets, particularly using methods from physics. But typically these analyses ended up with things like continuum approximations—that could say little about the information-processing aspects of neural nets. Meanwhile, there was an ongoing undercurrent of belief that somehow neural networks would both explain and reproduce how the brain works—but no methods seemed to exist to say quite how. Then at the beginning of the 1980s there was a resurgence of interest in neural networks, coming from several directions. Some of what was done concentrated on very practical efforts to get neural nets to do particular “human-like” tasks. But some was more theoretical, typically using methods from statistical physics or dynamical systems. </p>
<p>Before long, however, the buzz died down, and for several decades only a few groups were left working with neural nets. Then in 2011 came a surprise breakthrough in using neural nets for image analysis. It was an important practical advance. But it was driven by technological ideas and development—not any significant new theoretical analysis or framework. </p>
<p>And this was also the pattern for almost all of what followed. People spent great effort to come up with neural net systems that worked—and <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/#the-practice-and-lore-of-neural-net-training">all sorts of folklore</a> grew up about how this should best be done. But there wasn’t really even an attempt at an underlying theory; this was a domain of engineering practice, not basic science. </p>
<p>And it was in this tradition that <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/">ChatGPT burst onto the scene</a> in late 2022. Almost everything about LLMs seemed to be complicated. Yes, there were empirically some large-scale regularities (like scaling laws). And I quickly suspected that the success of LLMs was a <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/#what-really-lets-chatgpt-work">strong hint of general regularities in human language</a> that hadn’t been clearly identified before. But beyond a few outlier examples, almost nothing about “what’s going on inside LLMs” has seemed easy to decode. And efforts to put “strong guardrails” on the operation of the system—in effect so as to make it in some way “predictable” or “understandable”—typically seem to substantially decrease its power (a point that now makes sense in the context of computational irreducibility).</p>
<p>My own interaction with machine learning and neural nets <a href="https://writings.stephenwolfram.com/2015/05/wolfram-language-artificial-intelligence-the-image-identification-project/#personal-backstory">began in 1980</a> when I was developing my <a href="https://writings.stephenwolfram.com/2013/06/there-was-a-time-before-mathematica/">SMP symbolic computation system</a>, and wondering whether it might be possible to generalize the symbolic pattern-matching foundations of the system to some kind of “fuzzy pattern matching” that would be closer to human thinking. I was aware of neural nets but thought of them as semi-realistic models of brains, not for example as potential sources of algorithms of the kind I imagined might “solve” fuzzy matching. </p>
<p>And it was partly as a result of trying to understand the essence of systems like neural nets that <a href="https://www.wolframscience.com/nks/p17--the-personal-story-of-the-science-in-this-book/">in 1981 I came up with</a> what I later learned could be thought of as one-dimensional cellular automata. Soon I was deeply involved in studying cellular automata and developing a new intuition about how complex behavior could arise even from simple rules. But when I learned about recent efforts to make idealized models of neural nets using ideas from statistical mechanics, I was at least curious enough to set up simulations to try to understand more about these models.</p>
<p>But what I did wasn’t a success. I could neither get the models to do anything of significant practical interest—nor did I manage to derive any good theoretical understanding of them. I kept wondering, though, what relationship there might be between cellular automata that “just run”, and systems like neural nets that can also “learn”. And in fact <a href="https://content.wolfram.com/sw-publications/2020/07/approaches-complexity-engineering.pdf">in 1985 I tried to make a minimal cellular-automaton-based model</a> to explore this. It was what I’m now calling a “vertically layered rule array”. And while in many ways I was already asking the right questions, this was an unfortunate specific choice of system—and my experiments on it didn’t reveal the kinds of phenomena we’re now seeing.</p>
<p>Years went by. I wrote a <a href="https://www.wolframscience.com/nks/chap-10--processes-of-perception-and-analysis/#sect-10-12--human-thinking">section on “Human Thinking”</a> in <em><a href="https://www.wolframscience.com/nks/">A New Kind of Science</a></em>, that discussed the possibility of simple foundational rules for the essence of thinking, and even included a minimal discrete analog of a neural net. At the time, though, I didn’t develop these ideas. By 2017, though, 15 years after the book was published—and knowing about the breakthroughs in deep learning—I had begun to <a href="https://writings.stephenwolfram.com/2017/05/a-new-kind-of-science-a-15-year-view/#machine-learning-and-the-neural-net-renaissance">think more concretely about neural nets as getting their power</a> by sampling programs from across the computational universe. But still I didn’t see quite how this would work. </p>
<p>Meanwhile, there was a new intuition emerging from practical experience with machine learning: that if you “bashed” almost any system “hard enough”, it would learn. Did that mean that perhaps one didn’t need all the details of neural networks to successfully do machine learning? And could one perhaps make a system whose structure was simple enough that its operation would for example be accessible to visualization? I particularly wondered about this when I was writing <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/">an exposition of ChatGPT and LLMs in early 2023</a>. And I kept talking about “LLM science”, but didn’t have much of a chance to work on it.</p>
<p>But then, a few months ago, as part of an <a href="https://writings.stephenwolfram.com/2024/03/can-ai-solve-science/">effort to understand the relation between what science does and what AI does</a>, I tried a kind of <a href="https://writings.stephenwolfram.com/2024/03/can-ai-solve-science/#exploring-spaces-of-systems">“throwaway experiment”</a>—which, to my considerable surprise, seemed to successfully capture some of the essence of what <a href="https://writings.stephenwolfram.com/2024/05/why-does-biological-evolution-work-a-minimal-model-for-biological-evolution-and-other-adaptive-processes/">makes biological evolution possible</a>. But what about other adaptive evolution—and in particular, machine learning? The models that seemed to be needed were embarrassingly close to <a href="https://content.wolfram.com/sw-publications/2020/07/approaches-complexity-engineering.pdf">what I’d studied in 1985</a>. But now I had a new intuition—and, thanks to <a href="https://www.wolfram.com/language/">Wolfram Language</a>, vastly better tools. And the result has been my effort here. </p>
<p>Of course this is only a beginning. But I’m excited to be able to see what I consider to be the beginnings of foundational science around machine learning. Already there are clear directions for practical applications (which, needless to say, I plan to explore). And there are signs that perhaps we may finally be able to understand just why—and when—the “magic” of machine learning works.</p>
<h2 id="thanks">Thanks</h2>
<p>Thanks to Richard Assar of the <a href="https://www.wolframinstitute.org/">Wolfram Institute</a> for extensive help. Thanks also to Brad Klee, Tianyi Gu, Nik Murzin and Max Niederman for specific results, to George Morgan and others at <a href="https://www.symbolica.ai/" target="_blank" rel="noopener">Symbolica</a> for their early interest, and to Kovas Boguta for suggesting many years ago to link machine learning to the ideas in <em>A New Kind of Science</em>.</p>
        </div>

        
        

        

    <!-- wrapper -->

        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Python's Preprocessor – Pydong (322 pts)]]></title>
            <link>https://pydong.org/posts/PythonsPreprocessor/</link>
            <guid>41322758</guid>
            <pubDate>Thu, 22 Aug 2024 17:54:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pydong.org/posts/PythonsPreprocessor/">https://pydong.org/posts/PythonsPreprocessor/</a>, See on <a href="https://news.ycombinator.com/item?id=41322758">Hacker News</a></p>
Couldn't get https://pydong.org/posts/PythonsPreprocessor/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: InstantDB – A Modern Firebase (880 pts)]]></title>
            <link>https://github.com/instantdb/instant</link>
            <guid>41322281</guid>
            <pubDate>Thu, 22 Aug 2024 17:08:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/instantdb/instant">https://github.com/instantdb/instant</a>, See on <a href="https://news.ycombinator.com/item?id=41322281">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a href="https://instantdb.com/" rel="nofollow">
    <themed-picture data-catalyst-inline="true"><picture>
      <source media="(prefers-color-scheme: dark)" srcset="https://camo.githubusercontent.com/2af63e8197df473d79408acf76aa06b72d1bd76f0a851f8983a8b487ee7faf6b/68747470733a2f2f696e7374616e7464622e636f6d2f726561646d65732f6c6f676f5f776974685f746578745f6461726b5f6d6f64652e737667" data-canonical-src="https://instantdb.com/readmes/logo_with_text_dark_mode.svg">
      <img alt="Shows the Instant logo" src="https://camo.githubusercontent.com/fcbab3cc92cfb8b401f1b4ecbc4d31ac2e0323c368b1425689b3e294f5f24ad6/68747470733a2f2f696e7374616e7464622e636f6d2f726561646d65732f6c6f676f5f776974685f746578745f6c696768745f6d6f64652e737667" data-canonical-src="https://instantdb.com/readmes/logo_with_text_light_mode.svg">
    </picture></themed-picture>
  </a>
</p>
<p dir="auto">
  <a href="https://discord.com/invite/VU53p7uQcE" rel="nofollow">
    <img height="20" src="https://camo.githubusercontent.com/8fc18af31e1f1939c19c29462d1e29a0a9bbacb77c7a3ec147be30efde001827/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f31303331393537343833323433313838323335" data-canonical-src="https://img.shields.io/discord/1031957483243188235">
  </a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/89f509177e307d8b573fe98e36deae42e2498fa84230420f8f26ab09359f0a2c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f696e7374616e7464622f696e7374616e74"><img src="https://camo.githubusercontent.com/89f509177e307d8b573fe98e36deae42e2498fa84230420f8f26ab09359f0a2c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f696e7374616e7464622f696e7374616e74" alt="stars" data-canonical-src="https://img.shields.io/github/stars/instantdb/instant"></a>
</p>
<p dir="auto">
   <a href="https://instantdb.com/docs" rel="nofollow">Get Started</a> · 
   <a href="https://instantdb.com/examples" rel="nofollow">Examples</a> · 
   <a href="https://instantdb.com/tutorial" rel="nofollow">Try the Demo</a> · 
   <a href="https://instantdb.com/docs" rel="nofollow">Docs</a> · 
   <a href="https://discord.com/invite/VU53p7uQcE" rel="nofollow">Discord</a>
</p><p dir="auto">Instant is a client-side database that makes it easy to build real-time and collaborative apps like Notion or Figma.</p>
<p dir="auto">You write <a href="https://www.instantdb.com/docs/instaql" rel="nofollow">relational queries</a> in the shape of the data you want and Instant handles all the data fetching, permission checking, and offline caching. When you <a href="https://www.instantdb.com/docs/instaml" rel="nofollow">change data</a>, optimistic updates and rollbacks are handled for you as well. Plus, every query is multiplayer by default.</p>
<p dir="auto">We also support <a href="https://www.instantdb.com/docs/presence-and-topics" rel="nofollow">ephemeral</a> updates, like cursors, or who's online. Currently we have SDKs for <a href="https://www.instantdb.com/docs/start-vanilla" rel="nofollow">Javascript</a>, <a href="https://www.instantdb.com/docs/" rel="nofollow">React</a>, and <a href="https://www.instantdb.com/docs/start-rn" rel="nofollow">React Native</a>.</p>
<p dir="auto">How does it look? Here's a barebones chat app in about 10 lines:</p>
<div dir="auto" data-snippet-clipboard-copy-content="// ༼ つ ◕_◕ ༽つ Real-time Chat
// ----------------------------------
// * Updates instantly
// * Multiplayer
// * Works offline
function Chat() {
  // 1. Read
  const { isLoading, error, data } = useQuery({
    messages: {},
  });

  // 2. Write
  const addMessage = (message) => {
    transact(tx.messages[id()].update(message));
  }

  // 3. Render!
  return <UI data={data} onAdd={addMessage} />
}"><pre><span>// ༼ つ ◕_◕ ༽つ Real-time Chat</span>
<span>// ----------------------------------</span>
<span>// * Updates instantly</span>
<span>// * Multiplayer</span>
<span>// * Works offline</span>
<span>function</span> <span>Chat</span><span>(</span><span>)</span> <span>{</span>
  <span>// 1. Read</span>
  <span>const</span> <span>{</span> isLoading<span>,</span> error<span>,</span> data <span>}</span> <span>=</span> <span>useQuery</span><span>(</span><span>{</span>
    <span>messages</span>: <span>{</span><span>}</span><span>,</span>
  <span>}</span><span>)</span><span>;</span>

  <span>// 2. Write</span>
  <span>const</span> <span>addMessage</span> <span>=</span> <span>(</span><span>message</span><span>)</span> <span>=&gt;</span> <span>{</span>
    <span>transact</span><span>(</span><span>tx</span><span>.</span><span>messages</span><span>[</span><span>id</span><span>(</span><span>)</span><span>]</span><span>.</span><span>update</span><span>(</span><span>message</span><span>)</span><span>)</span><span>;</span>
  <span>}</span>

  <span>// 3. Render!</span>
  <span>return</span> <span>&lt;</span><span>UI</span> <span>data</span><span>=</span><span>{</span><span>data</span><span>}</span> <span>onAdd</span><span>=</span><span>{</span><span>addMessage</span><span>}</span> <span>/</span><span>&gt;</span>
<span>}</span></pre></div>
<p dir="auto">Want to see for yourself? <a href="https://instantdb.com/tutorial" rel="nofollow">try a demo in your browser.</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Motivation</h2><a id="user-content-motivation" aria-label="Permalink: Motivation" href="#motivation"></a></p>
<p dir="auto">Writing modern apps are full of schleps. Most of the time you start with the server: stand up databases, caches, ORMs, and endpoints. Then you write client-side code: stores, selectors, mutators. Finally you paint a screen. If you add multiplayer you need to think about stateful servers, and if you support offline mode, you need to think about IndexedDB and transaction queues.</p>
<p dir="auto">To make things worse, whenever you add a new feature, you go through the same song and dance over and over again: add models, write endpoints, stores, selectors, and finally the UI.</p>
<p dir="auto">Could it be better?</p>
<p dir="auto">In 2021, <strong>we realized that most of the schleps we face as UI engineers are actually database problems problems in disguise.</strong> (We got into greater detail <a href="https://instantdb.com/essays/next_firebase" rel="nofollow">in this essay</a>)</p>
<p dir="auto">
  <a href="#">
    <img alt="Shows how Instant compresses schleps" src="https://camo.githubusercontent.com/5767f96f66d7e1d87f07bfeab64f3590a30e11792bdeca79001247f8c70e6412/68747470733a2f2f696e7374616e7464622e636f6d2f726561646d65732f636f6d7072657373696f6e2e737667" data-canonical-src="https://instantdb.com/readmes/compression.svg">
  </a>
</p>
<p dir="auto">If you had a database on the client, you wouldn't need to think about stores, selectors, endpoints, or local caches: just write queries. If these queries were multiplayer by default, you wouldn't have to worry about stateful servers. And if your database supported rollback, you'd get optimistic updates for free.</p>
<p dir="auto">So we built Instant. Instant gives you a database you can use in the client, so you can focus on what’s important: building a great UX for your users, and doing it quickly.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architectural Overview</h2><a id="user-content-architectural-overview" aria-label="Permalink: Architectural Overview" href="#architectural-overview"></a></p>
<p dir="auto">Here's how Instant works at a high level:</p>
<p dir="auto">
  <a href="#">
    <img alt="Shows how Instant compresses schleps" src="https://camo.githubusercontent.com/9f9534e626714562bc04bbfc0d2d2a2295a9375ec44c5467b9ff7474395aba86/68747470733a2f2f696e7374616e7464622e636f6d2f726561646d65732f6172636869746563747572652e737667" data-canonical-src="https://instantdb.com/readmes/architecture.svg">
  </a>
</p>
<p dir="auto">Under the hood, we store all user data as triples in one big Postgres database. A multi-tenant setup lets us offer a free tier that never pauses.</p>
<p dir="auto">A sync server written in Clojure talks to Postgres. We wrote a query engine that understands datalog and <a href="https://www.instantdb.com/docs/instaql" rel="nofollow">InstaQL</a>, a relational language that looks a lot like GraphQL:</p>
<div dir="auto" data-snippet-clipboard-copy-content="// give me all users, their posts and comments
{ users: { posts: { comments: {} } } }"><pre><span>// give me all users, their posts and comments</span>
<span>{</span> <span>users</span>: <span>{</span> <span>posts</span>: <span>{</span> <span>comments</span>: <span>{</span><span>}</span> <span>}</span> <span>}</span> <span>}</span></pre></div>
<p dir="auto">Taking inspiration from <a href="https://asana.com/inside-asana/worldstore-distributed-caching-reactivity-part-1" rel="nofollow">Asana’s WorldStore</a> and <a href="https://www.figma.com/blog/how-figmas-multiplayer-technology-works/#syncing-object-properties" rel="nofollow">Figma’s LiveGraph</a>, we tail postgres’ WAL to detect novelty and invalidate relevant queries.</p>
<p dir="auto">For the frontend, we wrote a client-side triple store. The SDK handles persisting a cache of recent queries to IndexedDB on web, and AsyncStorage in React Native.</p>
<p dir="auto">All data goes through a permission system powered by Google's <a href="https://github.com/google/cel-java">CEL library</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">The easiest way to get started with Instant is by signing up on instantdb.com. <a href="https://instantdb.com/docs" rel="nofollow">You can create a functional app in 5 minute or less.</a>.</p>
<p dir="auto">If you have any questions, you can jump in on our <a href="https://discord.com/invite/VU53p7uQcE" rel="nofollow">discord</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">You can start by joining our <a href="https://discord.com/invite/VU53p7uQcE" rel="nofollow">discord</a> and introducing yourself. Even if you don't contribute code, we always love feedback.</p>
<p dir="auto">If you want to make changes, start by reading the <a href="https://github.com/instantdb/instant/blob/main/client"><code>client</code></a> and <a href="https://github.com/instantdb/instant/blob/main/server"><code>server</code></a> READMEs. There you'll find instructions to start Instant locally.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Peloton to charge $95 activation fee for used bikes (114 pts)]]></title>
            <link>https://www.cnbc.com/2024/08/22/peloton-to-charge-95-activation-fee-for-used-bikes-.html</link>
            <guid>41322266</guid>
            <pubDate>Thu, 22 Aug 2024 17:06:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2024/08/22/peloton-to-charge-95-activation-fee-for-used-bikes-.html">https://www.cnbc.com/2024/08/22/peloton-to-charge-95-activation-fee-for-used-bikes-.html</a>, See on <a href="https://news.ycombinator.com/item?id=41322266">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/PTON/">Peloton</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> on Thursday said it will <a href="https://www.cnbc.com/2024/08/22/peloton-pton-earnings-q4-2024.html">start charging new subscribers</a> a one-time $95 activation fee if they bought their hardware on the secondary market as more consumers snag lightly used equipment for a fraction of the typical retail price.</p><p>The used equipment activation fee for subscribers in the U.S. and Canada comes as Peloton starts to see a meaningful increase in new members who bought used Bikes or Treads from peer-to-peer markets such as Facebook Marketplace.&nbsp;</p><p>During its fiscal fourth quarter, which ended June 30, Peloton said it saw a "steady stream of paid connected fitness subscribers" who bought hardware on the secondary market. The company said the segment grew 16% year over year.</p><p>"We believe a meaningful share of these subscribers are incremental, and they exhibit lower net churn rates than rental subscribers," the company said in a letter to shareholders.&nbsp;</p><p>"It's also worth highlighting that this activation fee will be a source of incremental revenue and gross profit for us, helping to support our investments in improving the fitness experience for our members," interim co-CEO Christopher Bruzzo later added on a call with analysts.&nbsp;</p><p>While plenty of Peloton subscribers are avid users of the home workout machines, some have likened them to glorified clothes racks because so many people stop using the equipment. Those people paid Peloton for that hardware originally, but importantly, many of them have canceled their monthly subscription, which is how Peloton <a href="https://www.cnbc.com/2024/07/02/peloton-staves-off-liquidity-crunch-in-global-refinance.html">makes the bulk of its money</a>.&nbsp;</p><p>The ability to attract new, budget-conscious members from the secondary market who are willing to pay for a monthly subscription is a unique opportunity for Peloton to grow revenue without any upfront cost, on top of the revenue from the original sale.&nbsp;</p><p>Ari Kimmelfeld — whose startup Trade My Stuff, formerly known as Trade My Spin, sells used Peloton equipment — estimates there are around a million Bikes collecting dust in homes around the world that could be a source of new revenue for the company.&nbsp;</p><p>He told CNBC he previously met with Peloton executives to discuss ways to collaborate, because every time he sells a used piece of equipment, it could lead to more than $500 in new revenue per year for Peloton. With the new used equipment activation fee, that number could grow to more than $600 for the first year.&nbsp;</p><p>"We save the customer a lot more than $95," Kimmelfeld told CNBC on Thursday after the new activation fee was announced. "I don't think it'll stop or slow down people from buying secondary equipment … because you can get a bike delivered faster and cheaper on the secondary market, even with the $95, let's call it a tax, from Peloton."&nbsp;</p><p>Trade My Stuff sells first-generation Bikes for $499, compared with $1,445 new. It offers the Bike+ for $1,199, compared with $2,495 new. It also sells used Treads for $1,999, compared with $2,995 new.&nbsp;</p><p>Since launching his business, Kimmelfeld has worked with people looking to sell their used Peloton equipment and has since sold a "few thousand" Bikes. In 14 cities around the country, including Los Angeles, Denver and New York City, the company offers same- or next-day delivery. Outside of those locales, it provides delivery within three to five days. That compares with a new Peloton purchase, which can take significantly longer to deliver.&nbsp;</p><p>The used equipment activation fee is designed to ensure that new members "receive the same high-quality onboarding experience Peloton is known for," the company said. Bruzzo said that those who buy a used Bike or Bike+ have access to a virtual custom fitting ahead of their first ride, as well as a history summary that shows how many rides those bikes had before they were resold.&nbsp;</p><p>"We're also offering these new members discounts on accessories such as bike shoes, bike mats and spare parts," said Bruzzo. "We'll continue to lean into this important channel and find additional ways to improve the new member experience, for example, providing early education about the broad range of fitness modalities that we offer and the many series and programs our instructors provide to new members."</p></div><div id="RegularArticle-RelatedContent-1"><h2>Don’t miss these insights from CNBC PRO</h2><div><ul><li><a href="https://www.cnbc.com/2024/08/16/the-60/40-portfolio-shined-during-the-market-turbulence-.html">The 60/40 portfolio excelled during the market storm — and Vanguard sees a strong decade ahead</a></li><li><a href="https://www.cnbc.com/2024/08/20/investor-mark-mobius-names-one-risk-that-could-set-back-us-markets.html">Veteran investor Mark Mobius says this 'historically significant' factor could set back U.S. stocks</a></li><li><a href="https://www.cnbc.com/2024/08/14/jefferies-names-3-chip-stocks-to-buy-after-the-sell-off-giving-all-over-50percent-upside.html">Jefferies names 3 chip stocks to buy after the sell-off, giving all over 50% upside</a></li><li><a href="https://www.cnbc.com/2024/08/20/novo-nordisk-vs-eli-lilly-analysts-weigh-in-as-the-obesity-drug-battle-heats-up.html">Novo Nordisk vs. Eli Lilly: Analysts weigh in as the obesity-drug battle heats up</a><br></li></ul></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Aerc: A well-crafted TUI for email (255 pts)]]></title>
            <link>https://blog.sergeantbiggs.net/posts/aerc-a-well-crafted-tui-for-email/</link>
            <guid>41321981</guid>
            <pubDate>Thu, 22 Aug 2024 16:34:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.sergeantbiggs.net/posts/aerc-a-well-crafted-tui-for-email/">https://blog.sergeantbiggs.net/posts/aerc-a-well-crafted-tui-for-email/</a>, See on <a href="https://news.ycombinator.com/item?id=41321981">Hacker News</a></p>
Couldn't get https://blog.sergeantbiggs.net/posts/aerc-a-well-crafted-tui-for-email/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Continuous reinvention: A brief history of block storage at AWS (326 pts)]]></title>
            <link>https://www.allthingsdistributed.com/2024/08/continuous-reinvention-a-brief-history-of-block-storage-at-aws.html</link>
            <guid>41321063</guid>
            <pubDate>Thu, 22 Aug 2024 14:59:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.allthingsdistributed.com/2024/08/continuous-reinvention-a-brief-history-of-block-storage-at-aws.html">https://www.allthingsdistributed.com/2024/08/continuous-reinvention-a-brief-history-of-block-storage-at-aws.html</a>, See on <a href="https://news.ycombinator.com/item?id=41321063">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header></header><hr><section><p><time itemprop="datePublished" datetime="2024-08-22">August 22, 2024</time> • 4800 words</p><span itemprop="articleBody"><p><em><a href="https://www.linkedin.com/in/msolson/">Marc Olson</a> has been part of the team shaping Elastic Block Store (EBS) for over a decade. In that time, he’s helped to drive the dramatic evolution of EBS from a simple block storage service relying on shared drives to a massive network storage system that delivers over 140 trillion daily operations.</em></p><p><em>In this post, Marc provides a fascinating insider’s perspective on the journey of EBS. He shares hard-won lessons in areas such as queueing theory, the importance of comprehensive instrumentation, and the value of incrementalism versus radical changes. Most importantly, he emphasizes how constraints can often breed creative solutions. It’s an insightful look at how one of AWS’s foundational services has evolved to meet the needs of our customers (and the pace at which they’re innovating).</em></p><p><em>–W</em></p><hr><center><h2>Continuous reinvention: A brief history of block storage at AWS</h2></center><p>I’ve built system software for most of my career, and before joining AWS it was mostly in the networking and security spaces. When I joined AWS nearly 13 years ago, I entered a new domain—storage—and stepped into a new challenge. Even back then the scale of AWS dwarfed anything I had worked on, but many of the same techniques I had picked up until that point remained applicable—distilling problems down to first principles, and using successive iteration to incrementally solve problems and improve performance.</p><p>If you look around at AWS services today, you’ll find a mature set of core building blocks, but it wasn’t always this way. <a href="https://www.allthingsdistributed.com/2008/08/amazon_ebs_elastic_block_store.html">EBS launched on August 20, 2008</a>, nearly two years after EC2 became available in beta, with a simple idea to provide network attached block storage for EC2 instances. We had one or two storage experts, and a few distributed systems folks, and a solid knowledge of computer systems and networks. How hard could it be? In retrospect, if we knew at the time how much we didn’t know, we may not have even started the project!</p><p>Since I’ve been at EBS, I’ve had the opportunity to be part of the team that’s evolved EBS from a product built using shared hard disk drives (HDDs), to one that is capable of delivering hundreds of thousands of IOPS (IO operations per second) to a single EC2 instance. It’s remarkable to reflect on this because EBS is capable of delivering more IOPS to a single instance today than it could deliver to an entire Availability Zone (AZ) in the early years on top of HDDs. Even more amazingly, today EBS in aggregate delivers over 140 trillion operations daily across a distributed SSD fleet. But we definitely didn’t do it overnight, or in one big bang, or even perfectly. When I started on the EBS team, I initially worked on the EBS client, which is the piece of software responsible for converting instance IO requests into EBS storage operations. Since then I’ve worked on almost every component of EBS and have been delighted to have had the opportunity to participate so directly in the evolution and growth of EBS.</p><p>As a storage system, EBS is a bit unique. It’s unique because our primary workload is system disks for EC2 instances, motivated by the hard disks that used to sit inside physical datacenter servers. A lot of storage services place durability as their primary design goal, and are willing to degrade performance or availability in order to protect bytes. EBS customers care about durability, and we provide the primitives to help them achieve high durability with io2 Block Express volumes and volume snapshots, but they also care a lot about the performance and availability of EBS volumes. EBS is so closely tied as a storage primitive for EC2, that the performance and availability of EBS volumes tends to translate almost directly to the performance and availability of the EC2 experience, and by extension the experience of running applications and services that are built using EC2. The story of EBS is the story of understanding and evolving performance in a very large-scale distributed system that spans layers from guest operating systems at the top, all the way down to custom SSD designs at the bottom. In this post I’d like to tell you about the journey that we’ve taken, including some memorable lessons that may be applicable to your systems. After all, systems performance is a complex and really challenging area, and it’s a complex language across many domains.</p><h2 id="queueing-theory-briefly">Queueing theory, briefly <a href="#queueing-theory-briefly"></a></h2><p>Before we dive too deep, let’s take a step back and look at how computer systems interact with storage. The high-level basics haven’t changed through the years—a storage device is connected to a bus which is connected to the CPU. The CPU queues requests that travel the bus to the device. The storage device either retrieves the data from CPU memory and (eventually) places it onto a durable substrate, or retrieves the data from the durable media, and then transfers it to the CPU’s memory.</p><figure><img src="https://www.allthingsdistributed.com/images/mo-computer-arch.png" alt="Architecture with direct attached disk" loading="lazy"><figcaption>High-level computer architecture with direct attached disk</figcaption></figure><p>You can think of this like a bank. You walk into the bank with a deposit, but first you have to traverse a queue before you can speak with a bank teller who can help you with your transaction. In a perfect world, the number of patrons entering the bank arrive at the exact rate at which their request can be handled, and you never have to stand in a queue. But the real world isn’t perfect. The real world is asynchronous. It’s more likely that a few people enter the bank at the same time. Perhaps they have arrived on the same streetcar or train. When a group of people all walk into the back at the same time, some of them are going to have to wait for the teller to process the transactions ahead of them.</p><p>As we think about the time to complete each transaction, and empty the queue, the average time waiting in line (latency) across all customers may look acceptable, but the first person in the queue had the best experience, while the last had a much longer delay. There are a number of things the bank can do to improve the experience for all customers. The bank could add more tellers to process more requests in parallel, it could rearrange the teller workflows so that each transaction takes less time, lowering both the total time and the average time, or it could create different queues for either latency insensitive customers or consolidating transactions that may be faster to keep the queue low. But each of these options comes at an additional cost—hiring more tellers for a peak that may never occur, or adding more real estate to create separate queues. While imperfect, unless you have infinite resources, queues are necessary to absorb peak load.</p><figure><img src="https://www.allthingsdistributed.com/images/mo-simplified-ec2-ebs-queueing.png" alt="Simple diagram of EC2 and EBS queueing from 2012" loading="lazy"><figcaption>Simplified diagram of EC2 and EBS queueing (c. 2012)</figcaption></figure><p>In network storage systems, we have several queues in the stack, including those between the operating system kernel and the storage adapter, the host storage adapter to the storage fabric, the target storage adapter, and the storage media. In legacy network storage systems, there may be different vendors for each component, and different ways that they think about servicing the queue. You may be using a dedicated, lossless network fabric like fiber channel, or using iSCSI or NFS over TCP, either with the operating system network stack, or a custom driver. In either case, tuning the storage network often takes specialized knowledge, separate from tuning the application or the storage media.</p><p>When we first built EBS in 2008, the storage market was largely HDDs, and the latency of our service was dominated by the latency of this storage media. Last year, Andy Warfield went in-depth about the <a href="https://www.allthingsdistributed.com/2023/07/building-and-operating-a-pretty-big-storage-system.html#technical-scale-scale-and-the-physics-of-storage">fascinating mechanical engineering behind HDDs</a>. As an engineer, I still marvel at everything that goes into a hard drive, but at the end of the day they are mechanical devices and physics limits their performance. There’s a stack of platters that are spinning at high velocity. These platters have tracks that contain the data. Relative to the size of a track (&lt;100 nanometers), there’s a large arm that swings back and forth to find the right track to read or write your data. Because of the physics involved, the IOPS performance of a hard drive has remained relatively constant for the last few decades at approximately 120-150 operations per second, or 6-8 ms average IO latency. One of the biggest challenges with HDDs is that tail latencies can easily drift into the hundreds of milliseconds with the impact of queueing and command reordering in the drive.</p><p>We didn’t have to worry much about the network getting in the way since end-to-end EBS latency was dominated by HDDs and measured in the 10s of milliseconds. Even our early data center networks were beefy enough to handle our user’s latency and throughput expectations. The addition of 10s of microseconds on the network was a small fraction of overall latency.</p><p>Compounding this latency, hard drive performance is also variable depending on the other transactions in the queue. Smaller requests that are scattered randomly on the media take longer to find and access than several large requests that are all next to each other. This random performance led to wildly inconsistent behavior. Early on, we knew that we needed to spread customers across many disks to achieve reasonable performance. This had a benefit, it dropped the peak outlier latency for the hottest workloads, but unfortunately it spread the inconsistent behavior out so that it impacted many customers.</p><p>When one workload impacts another, we call this a “noisy neighbor.” Noisy neighbors turned out to be a critical problem for the business. As AWS evolved, we learned that we had to focus ruthlessly on a high-quality customer experience, and that inevitably meant that we needed to achieve strong performance isolation to avoid noisy neighbors causing interference with other customer workloads.</p><p>At the scale of AWS, we often run into challenges that are hard and complex due to the scale and breadth of our systems, and our focus on maintaining the customer experience. Surprisingly, the fixes are often quite simple once you deeply understand the system, and have enormous impact due to the scaling factors at play. We were able to make some improvements by changing scheduling algorithms to the drives and balancing customer workloads across even more spindles. But all of this only resulted in small incremental gains. We weren’t really hitting the breakthrough that truly eliminated noisy neighbors. Customer workloads were too unpredictable to achieve the consistency we knew they needed. We needed to explore something completely different.</p><h2 id="set-long-term-goals-but-dont-be-afraid-to-improve-incrementally">Set long term goals, but don’t be afraid to improve incrementally <a href="#set-long-term-goals-but-dont-be-afraid-to-improve-incrementally"></a></h2><p>Around the time I started at AWS in 2011, solid state disks (SSDs) became more mainstream, and were available in sizes that started to make them attractive to us. In an SSD, there is no physical arm to move to retrieve data—random requests are nearly as fast as sequential requests—and there are multiple channels between the controller and NAND chips to get to the data. If we revisit the bank example from earlier, replacing an HDD with an SSD is like building a bank the size of a football stadium and staffing it with superhumans that can complete transactions orders of magnitude faster. A year later we started using SSDs, and haven’t looked back.</p><p>We started with a small, but meaningful milestone: we built a new storage server type built on SSDs, and a new EBS volume type called Provisioned IOPS. Launching a new volume type is no small task, and it also limits the workloads that can take advantage of it. For EBS, there was an immediate improvement, but it wasn’t everything we expected.</p><p>We thought that just dropping SSDs in to replace HDDs would solve almost all of our problems, and it certainly did address the problems that came from the mechanics of hard drives. But what surprised us was that the system didn’t improve nearly as much as we had hoped and noisy neighbors weren’t automatically fixed. We had to turn our attention to the rest of our stack—the network and our software—that the improved storage media suddenly put a spotlight on.</p><p>Even though we needed to make these changes, we went ahead and launched in August 2012 with a maximum of 1,000 IOPS, 10x better than existing EBS standard volumes, and ~2-3 ms average latency, a 5-10x improvement with significantly improved outlier control. Our customers were excited for an EBS volume that they could begin to build their mission critical applications on, but we still weren’t satisfied and we realized that the performance engineering work in our system was really just beginning. But to do that, we had to measure our system.</p><h2 id="if-you-cant-measure-it-you-cant-manage-it">If you can’t measure it, you can’t manage it <a href="#if-you-cant-measure-it-you-cant-manage-it"></a></h2><p>At this point in EBS’s history (2012), we only had rudimentary telemetry. To know what to fix, we had to know what was broken, and then prioritize those fixes based on effort and rewards. Our first step was to build a method to instrument every IO at multiple points in every subsystem—in our client initiator, network stack, storage durability engine, and in our operating system. In addition to monitoring customer workloads, we also built a set of canary tests that run continuously and allowed us to monitor impact of changes—both positive and negative—under well-known workloads.</p><p>With our new telemetry we identified a few major areas for initial investment. We knew we needed to reduce the number of queues in the entire system. Additionally, the Xen hypervisor had served us well in EC2, but as a general-purpose hypervisor, it had different design goals and many more features than we needed for EC2. We suspected that with some investment we could reduce complexity of the IO path in the hypervisor, leading to improved performance. Moreover, we needed to optimize the network software, and in our core durability engine we needed to do a lot of work organizationally and in code, including on-disk data layout, cache line optimization, and fully embracing an asynchronous programming model.</p><p>A really consistent lesson at AWS is that system performance issues almost universally span a lot of layers in our hardware and software stack, but even great engineers tend to have jobs that focus their attention on specific narrower areas. While the much celebrated ideal of a “full stack engineer” is valuable, in deep and complex systems it’s often even more valuable to create cohorts of experts who can collaborate and get really creative across the entire stack and all their individual areas of depth.</p><p>By this point, we already had separate teams for the storage server and for the client, so we were able to focus on these two areas in parallel. We also enlisted the help of the EC2 hypervisor engineers and formed a cross-AWS network performance cohort. We started to build a blueprint of both short-term, tactical fixes and longer-term architectural changes.</p><h2 id="divide-and-conquer">Divide and conquer <a href="#divide-and-conquer"></a></h2><figure><img src="https://www.allthingsdistributed.com/images/mo-physalia.png" alt="Whiteboard showing how the team removed the contronl from from the IO path with Physalia" loading="lazy"><figcaption>Removing the control plane from the IO path with Physalia</figcaption></figure><p>When I was an undergraduate student, while I loved most of my classes, there were a couple that I had a love-hate relationship with. “Algorithms” was taught at a graduate level at my university for both undergraduates and graduates. I found the coursework intense, but I eventually fell in love with the topic, and <a href="https://www.amazon.com/dp/026204630X">Introduction to Algorithms</a>, commonly referred to as CLR, is one of the few textbooks I retained, and still occasionally reference. What I didn’t realize until I joined Amazon, and seems obvious in hindsight, is that you can design an organization much the same way you can design a software system. Different algorithms have different benefits and tradeoffs in how your organization functions. Where practical, Amazon chooses a divide and conquer approach, and keeps teams small and focused on a self-contained component with well-defined APIs.</p><p>This works well when applied to components of a retail website and control plane systems, but it’s less intuitive in how you could build a high-performance data plane this way, and at the same time improve performance. In the EBS storage server, we reorganized our monolithic development team into small teams focused on specific areas, such as data replication, durability, and snapshot hydration. Each team focused on their unique challenges, dividing the performance optimization into smaller sized bites. These teams are able to iterate and commit their changes independently—made possible by rigorous testing that we’ve built up over time. It was important for us to make continual progress for our customers, so we started with a blueprint for where we wanted to go, and then began the work of separating out components while deploying incremental changes.</p><p>The best part of incremental delivery is that you can make a change and observe its impact before making the next change. If something doesn’t work like you expected, then it’s easy to unwind it and go in a different direction. In our case, the blueprint that we laid out in 2013 ended up looking nothing like what EBS looks like today, but it gave us a direction to start moving toward. For example, back then we never would have imagined that Amazon would one day <a href="https://aws.amazon.com/blogs/aws/aws-nitro-ssd-high-performance-storage-for-your-i-o-intensive-applications/">build its own SSDs</a>, with a technology stack that could be tailored specifically to the needs of EBS.</p><h2 id="always-question-your-assumptions">Always question your assumptions! <a href="#always-question-your-assumptions"></a></h2><p>Challenging our assumptions led to improvements in every single part of the stack.</p><p>We started with software virtualization. Until late 2017 all EC2 instances ran on the Xen hypervisor. With devices in Xen, there is a ring queue setup that allows guest instances, or domains, to share information with a privileged driver domain (dom0) for the purposes of IO and other emulated devices. The EBS client ran in dom0 as a kernel block device. If we follow an IO request from the instance, just to get off of the EC2 host there are many queues: the instance block device queue, the Xen ring, the dom0 kernel block device queue, and the EBS client network queue. In most systems, performance issues are compounding, and it’s helpful to focus on components in isolation.</p><p>One of the first things that we did was to write several “loopback” devices so that we could isolate each queue to gauge the impact of the Xen ring, the dom0 block device stack, and the network. We were almost immediately surprised that with almost no latency in the dom0 device driver, when multiple instances tried to drive IO, they would interact with each other enough that the goodput of the entire system would slow down. We had found another noisy neighbor! Embarrassingly, we had launched EC2 with the Xen defaults for the number of block device queues and queue entries, which were set many years prior based on the limited storage hardware that was available to the Cambridge lab building Xen. This was very unexpected, especially when we realized that it limited us to only 64 IO outstanding requests for an entire host, not per device—certainly not enough for our most demanding workloads.</p><p>We fixed the main issues with software virtualization, but even that wasn’t enough. In 2013, we were well into the development of our first <a href="https://www.allthingsdistributed.com/2020/09/reinventing-virtualization-with-nitro.html">Nitro offload card</a> dedicated to networking. With this first card, we moved the processing of VPC, our software defined network, from the Xen dom0 kernel, into a dedicated hardware pipeline. By isolating the packet processing data plane from the hypervisor, we no longer needed to steal CPU cycles from customer instances to drive network traffic. Instead, we leveraged Xen’s ability to pass a virtual PCI device directly to the instance.</p><p>This was a fantastic win for latency and efficiency, so we decided to do the same thing for EBS storage. By moving more processing to hardware, we removed several operating system queues in the hypervisor, even if we weren’t ready to pass the device directly to the instance just yet. Even without passthrough, by offloading more of the interrupt driven work, the hypervisor spent less time servicing the requests—the hardware itself had dedicated interrupt processing functions. This second Nitro card also had hardware capability to handle EBS encrypted volumes with no impact to EBS volume performance. Leveraging our hardware for encryption also meant that the encryption key material is kept separate from the hypervisor, which further protects customer data.</p><figure><img src="https://www.allthingsdistributed.com/images/mo-network-tuning.png" alt="Diagram showing experiments in network tuning to improve throughput and reduce latency" loading="lazy"><figcaption>Experimenting with network tuning to improve throughput and reduce latency</figcaption></figure><p>Moving EBS to Nitro was a huge win, but it almost immediately shifted the overhead to the network itself. Here the problem seemed simple on the surface. We just needed to tune our wire protocol with the latest and greatest data center TCP tuning parameters, while choosing the best congestion control algorithm. There were a few shifts that were working against us: AWS was experimenting with different data center cabling topology, and our AZs, once a single data center, were growing beyond those boundaries. Our tuning would be beneficial, as in the example above, where adding a small amount of random latency to requests to storage servers counter-intuitively reduced the average latency and the outliers due to the smoothing effect it has on the network. These changes were ultimately short lived as we continuously increased the performance and scale of our system, and we had to continually measure and monitor to make sure we didn’t regress.</p><p>Knowing that we would need something better than TCP, in 2014 we started laying the foundation for Scalable Relatable Diagram (SRD) with “<a href="https://ieeexplore.ieee.org/document/9167399">A Cloud-Optimized Transport Protocol for Elastic and Scalable HPC</a>”. Early on we set a few requirements, including a protocol that could improve our ability to recover and route around failures, and we wanted something that could be easily offloaded into hardware. As we were investigating, we made two key observations: 1/ we didn’t need to design for the general internet, but we could focus specifically on our data center network designs, and 2/ in storage, the execution of IO requests that are in flight could be reordered. We didn’t need to pay the penalty of TCP’s strict in-order delivery guarantees, but could instead send different requests down different network paths, and execute them upon arrival. Any barriers could be handled at the client before they were sent on the network. What we ended up with is a protocol that’s useful not just for storage, but for networking, too. When used in <a href="https://aws.amazon.com/about-aws/whats-new/2022/11/elastic-network-adapter-ena-express-amazon-ec2-instances/">Elastic Network Adapter (ENA) Express</a>, SRD improves the performance of your TCP stacks in your guest. SRD can drive the network at higher utilization by taking advantage of multiple network paths and reducing the overflow and queues in the intermediate network devices.</p><p>Performance improvements are never about a single focus. It’s a discipline of continuously challenging your assumptions, measuring and understanding, and shifting focus to the most meaningful opportunities.</p><h2 id="constraints-breed-innovation">Constraints breed innovation <a href="#constraints-breed-innovation"></a></h2><p>We weren’t satisfied that only a relatively small number of volumes and customers had better performance. We wanted to bring the benefits of SSDs to everyone. This is an area where scale makes things difficult. We had a large fleet of thousands of storage servers running millions of non-provisioned IOPS customer volumes. Some of those same volumes still exist today. It would be an expensive proposition to throw away all of that hardware and replace it.</p><p>There was empty space in the chassis, but the only location that didn’t cause disruption in the cooling airflow was between the motherboard and the fans. The nice thing about SSDs is that they are typically small and light, but we couldn’t have them flopping around loose in the chassis. After some trial and error—and help from our material scientists—we found heat resistant, industrial strength hook and loop fastening tape, which also let us service these SSDs for the remaining life of the servers.</p><figure><img src="https://www.allthingsdistributed.com/images/mo-manual-ssd.png" alt="An SSD in one of our servers" loading="lazy"><figcaption>Yes, we manually put an SSD into every server!</figcaption></figure><p>Armed with this knowledge, and a lot of human effort, over the course of a few months in 2013, EBS was able to put a single SSD into each and every one of those thousands of servers. We made a small change to our software that staged new writes onto that SSD, allowing us to return completion back to your application, and then flushed the writes to the slower hard disk asynchronously. And we did this with no disruption to customers—we were converting a propeller aircraft to a jet while it was in flight. The thing that made this possible is that we designed our system from the start with non-disruptive maintenance events in mind. We could retarget EBS volumes to new storage servers, and update software or rebuild the empty servers as needed.</p><p>This ability to migrate customer volumes to new storage servers has come in handy several times throughout EBS’s history as we’ve identified new, more efficient data structures for our on-disk format, or brought in new hardware to replace the old hardware. There are volumes still active from the first few months of EBS’s launch in 2008. These volumes have likely been on hundreds of different servers and multiple generations of hardware as we’ve updated and rebuilt our fleet, all without impacting the workloads on those volumes.</p><h2 id="reflecting-on-scaling-performance">Reflecting on scaling performance <a href="#reflecting-on-scaling-performance"></a></h2><p>There’s one more journey over this time that I’d like to share, and that’s a personal one. Most of my career prior to Amazon had been in either early startup or similarly small company cultures. I had built managed services, and even distributed systems out of necessity, but I had never worked on anything close to the scale of EBS, even the EBS of 2011, both in technology and organization size. I was used to solving problems by myself, or maybe with one or two other equally motivated engineers.</p><p>I really enjoy going super deep into problems and attacking them until they’re complete, but there was a pivotal moment when a colleague that I trusted pointed out that I was becoming a performance bottleneck for our organization. As an engineer who had grown to be an expert in the system, but also who cared really, really deeply about all aspects of EBS, I found myself on every escalation and also wanting to review every commit and every proposed design change. If we were going to be successful, then I had to learn how to scale myself–I wasn’t going to solve this with just ownership and bias for action.</p><p>This led to even more experimentation, but not in the code. I knew I was working with other smart folks, but I also needed to take a step back and think about how to make them effective. One of my favorite tools to come out of this was peer debugging. I remember a session with a handful of engineers in one of our lounge rooms, with code and a few terminals projected on a wall. One of the engineers exclaimed, “Uhhhh, there’s no way that’s right!” and we had found something that had been nagging us for a while. We had overlooked where and how we were locking updates to critical data structures. Our design didn’t usually cause issues, but occasionally we would see slow responses to requests, and fixing this removed one source of jitter. We don’t always use this technique, but the neat thing is that we are able to combine our shared systems knowledge when things get really tricky.</p><p>Through all of this, I realized that empowering people, giving them the ability to safely experiment, can often lead to results that are even better than what was expected. I’ve spent a large portion of my career since then focusing on ways to remove roadblocks, but leave the guardrails in place, pushing engineers out of their comfort zone. There’s a bit of psychology to engineering leadership that I hadn’t appreciated. I never expected that one of the most rewarding parts of my career would be encouraging and nurturing others, watching them own and solve problems, and most importantly celebrating the wins with them!</p><h2 id="conclusion">Conclusion <a href="#conclusion"></a></h2><p>Reflecting back on where we started, we knew we could do better, but we weren’t sure how much better. We chose to approach the problem, not as a big monolithic change, but as a series of incremental improvements over time. This allowed us to deliver customer value sooner, and course correct as we learned more about changing customer workloads. We’ve improved the shape of the EBS latency experience from one averaging more than 10 ms per IO operation to consistent sub-millisecond IO operations with our highest performing io2 Block Express volumes. We accomplished all this without taking the service offline to deliver a new architecture.</p><p>We know we’re not done. Our customers will always want more, and that challenge is what keeps us motivated to innovate and iterate.</p><ul><li><a href="https://www.allthingsdistributed.com/2023/07/building-and-operating-a-pretty-big-storage-system.html?utm_campaign=related+posts&amp;utm_source=brief-history-ebs">Building and operating a pretty big storage system called S3</a></li><li><a href="https://www.allthingsdistributed.com/2023/11/standing-on-the-shoulders-of-giants-colm-on-constant-work.html?utm_campaign=related+posts&amp;utm_source=brief-history-ebs">Reliability, constant work, and a good cup of coffee</a></li><li><a href="https://www.allthingsdistributed.com/2022/11/amazon-1998-distributed-computing-manifesto.html?utm_campaign=related+posts&amp;utm_source=brief-history-ebs">The Distributed Computing Manifesto</a></li></ul></span></section><hr></div></div>]]></description>
        </item>
    </channel>
</rss>