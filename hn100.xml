<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 15 Oct 2024 00:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Zamba2-7B (108 pts)]]></title>
            <link>https://www.zyphra.com/post/zamba2-7b</link>
            <guid>41842975</guid>
            <pubDate>Mon, 14 Oct 2024 22:45:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.zyphra.com/post/zamba2-7b">https://www.zyphra.com/post/zamba2-7b</a>, See on <a href="https://news.ycombinator.com/item?id=41842975">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-animation="default" data-collapse="tiny" data-duration="400" data-easing="ease" data-easing2="ease" role="banner"><p><a href="https://www.zyphra.com/"><img width="123" sizes="(max-width: 479px) 123px, (max-width: 767px) 22vw, 123px" alt="" src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch.png" loading="lazy" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch.png 1505w"></a></p></div><div><p><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash.jpg" loading="lazy" width="726" height="Auto" alt="" sizes="(max-width: 479px) 90vw, (max-width: 1279px) 80vw, 90vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-500.jpg 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-800.jpg 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-1080.jpg 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-1600.jpg 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-2000.jpg 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-2600.jpg 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-3200.jpg 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash.jpg 5760w"></p><p><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670700157cd6aceff5b383ed_zamba27b.png" loading="lazy" width="648" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670700157cd6aceff5b383ed_zamba27b-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670700157cd6aceff5b383ed_zamba27b-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670700157cd6aceff5b383ed_zamba27b.png 1470w"></p></div><div><p>October 14, 2024</p><p>PALO ALTO, CALIFORNIA</p><p>Zyphra is excited to release Zamba2-7B, a state-of-the-art small language model. At the 7B scale, we outperform the leading models of Mistral, Google’s Gemma and Meta’s Llama3 series in both quality and performance. We believe Zamba2-7B is the leading model for running on-device and on consumer GPUs as well as for many enterprise applications which require a powerful but compact and efficient model for natural-language tasks.</p><p>Authors</p><p>Zyphra Team</p><p>Collaborators</p><p>Daniel A Roberts (Sequoia Capital &amp; MIT), Andrey Gromov (Meta FAIR), Kushal Tirumala (Meta FAIR) and Hassan Shapourian (Cisco)</p></div><div id="introduction"><div><div id="zamba1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="zamba2"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="Quality vs. Inference Speed" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="zamba3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div></section></div><div><div id="zyda1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="zyda2"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="zyda3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div></section><section id="zyda4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section></div><div><section id="small1"><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><div><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div></section><section id="small2"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="small3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section><section id="small4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a></section><section id="small5"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><div><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div></section></div><div><div id="rag1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="rag2"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="rag3"><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section><section id="rag4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a></section><div id="rag5"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></div><div><div id="tree1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="tree2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div></section><section id="tree3"><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" width="674" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" width="586" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section></div><div><div id="layer1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="layer2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" width="619" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" width="658" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" width="680" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div></section><div id="layer3"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></div><div><section id="edge1"><div><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" width="455" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><div id="edge2"><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><section id="edge3"><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section><div id="edge4"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><div id="edge5"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></div><div><div id="hop1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="hop2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="hop3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" width="581" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div></section><section id="hop4"><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section><div id="hop5"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><section id="hop6"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" width="367" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section><section id="hop7"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" width="479" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a></section><section id="hop8"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" width="458" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w"></a></section><section id="hop9"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png" loading="lazy" width="360" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png 1389w"></a></section></div><div><div target="_blank" id="cook1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="cook2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="cook3"><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a></section><div id="cook4"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><div id="cook5"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div><section id="cook7"><p>What is Annealing?</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a></section><section id="cook8"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a></section><section id="hop9"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png 1389w"></a></section></div><div><div target="_blank" id="mini1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="mini2"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="mini3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a></section><section id="mini4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a></section><section id="mini5"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png" loading="lazy" width="384" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div></section></div><div><section id="noc1"><div target="_blank"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" width="429" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="noc2"><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" width="267" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" width="319" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a><div><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section><section id="noc3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" width="400" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a></section></div><div><div target="_blank" id="longrag1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="longrag2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><div target="_blank" id="longrag3"><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><section id="longrag4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><div target="_blank"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section><section id="longrag5"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div target="_blank"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section></div><div><div target="_blank" id="zamba2_1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="zamba2_2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" width="570" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" width="570" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div target="_blank"><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" width="570" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" width="319" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a></section><section id="zamba2_3"><div target="_blank"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" width="571" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png" loading="lazy" width="570" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png" loading="lazy" width="571" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png 1389w"></a><div target="_blank"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section></div><div><div target="_blank" id="zyda2_1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="zyda2_2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="zyda2_3"><div target="_blank"><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div target="_blank"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div target="_blank"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section><section id="zyda2_4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a></section><section id="zyda2_5"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png 1389w"></a></section><section id="zyda2_7"><h3>Analysis of Global Duplicates</h3><p>We present histograms depicting distribution of cluster sizes in all the datasets (see Fig. 7-11). Please, note that all the figures are in log-log scale. We see a significant drop in the number of clusters starting from the size of around 100. This drop is present both in DCLM and FineWeb-Edu2 (see Fig. 8 and 9 respectively), and most likely is explained by a combination of&nbsp; the deduplication strategy and quality when creating both datasets: DCLM deduplication was done individually within 10 shards, while FineWeb-Edu2 was deduplicated within every Common Crawl snapshot. We find that large clusters usually contain low quality material (repeated advertisements, license agreements templates, etc), so it’s not surprising that such documents were removed. Notably, DCLM still contained one cluster with the size close to 1 million documents, containing low quality documents seemingly coming from the advertisements (see Appendix).We find both Zyda-1and Dolma-CC contain a small amount of duplicates, which is expected, since both datasets were deduplicated globally by their authors. Remaining duplicates are likely false negatives from the initial deduplication procedure. Note, that distribution of duplicates clusters sizes of these two datasets (Fig. 10 and 11) don’t contain any sharp drops, but rather hyper exponentially decreases with cluster size.&nbsp;</p><a href="#"><img src="https://cdn.prod.website-files.com/img/placeholder-thumb.svg" loading="lazy" alt=""></a><h5><em>Figure 7: Distribution of cluster sizes of duplicates in global dataset (log-log scale).</em></h5><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9.png" loading="lazy" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9.png 1600w" alt=""></a><h5><em>Figure 8: Distribution of cluster sizes of duplicates in DCLM (log-log scale).</em></h5><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10.png" loading="lazy" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10.png 1600w" alt=""></a><h5><em>Figure 9: Distribution of cluster sizes of duplicates in FineWeb-Edu2 (log-log scale).</em></h5><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11.png" loading="lazy" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11.png 1600w" alt=""></a><h5><em>Figure 10: Distribution of cluster sizes of duplicates in Zyda-1 (log-log scale).</em></h5><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12.png" loading="lazy" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12.png 1600w" alt=""></a><h5><em>Figure 11: Distribution of cluster sizes of duplicates in Dolma-CC (log-log scale).</em></h5><h3>Largest cluster in DCLM</h3><p>Below is an example of the document from the largest cluster (~1M documents) of duplicates in DCLM (quality score 0.482627):<br>‍<em>Is safe? Is scam?<br>Is safe for your PC?<br>Is safe or is it scam?<br>Domain is SafeSafe score: 1</em>‍<br>‍<em>The higher the number, the more dangerous the website.Any number higher than 1 means DANGER.</em>‍<br>‍<em>Positive votes:<br>Negative votes:<br>Vote Up Vote Down review</em>‍<br>‍<em>Have you had bad experience with Warn us, please!</em></p><h3>Examples of varying quality score in DCLM in a cluster</h3><p>Below one will find a few documents with different quality scores from DCLM coming from the same duplicates cluster. Quality score varies from ~0.2 to ~0.04.</p></section></div></div>
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bike Manufacturers Are Making Bikes Less Repairable (160 pts)]]></title>
            <link>https://www.ifixit.com/News/101675/bike-manufacturers-are-making-bikes-less-repairable</link>
            <guid>41840971</guid>
            <pubDate>Mon, 14 Oct 2024 19:27:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ifixit.com/News/101675/bike-manufacturers-are-making-bikes-less-repairable">https://www.ifixit.com/News/101675/bike-manufacturers-are-making-bikes-less-repairable</a>, See on <a href="https://news.ycombinator.com/item?id=41840971">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
<p>The bicycle is probably the canonical example of something that anyone can fix. Spares from all brands are mostly interchangeable, and you can do most repairs with wrenches, screwdrivers, and Allen keys, or some fairly standard tools for bottom brackets and chainrings. But that’s all changing.</p>



<p>Just like cars, tractors, computers, and seemingly every other product category, bikes—and especially e-bikes—are going all black box on us. Instead of using standard parts that can easily be swapped or upgraded, bike makers are using more and more proprietary parts. At the same time,<a href="https://www.vice.com/en/article/mechanics-ask-walmart-major-bike-manufacturers-to-stop-making-and-selling-built-to-fail-bikes/"> cheap bikes are getting worse</a> and are designed to fail, or rather, they are not designed to last, which is pretty much the same thing.</p>


  <div>
      <p><span>Featured Guide</span></p><div>
          <div>
            <h3>How to Replace a Bike Chain With a Master Link</h3>
            <p>Bicycle chains sometimes need to be removed for…</p>
            <p><a href="https://www.ifixit.com/Guide/How+to+Replace+a+Bike+Chain+With+a+Master+Link/140441">Follow this Guide</a></p>
          </div>
          <p><a href="https://www.ifixit.com/Guide/How+to+Replace+a+Bike+Chain+With+a+Master+Link/140441"><img decoding="async" loading="lazy" src="https://guide-images.cdn.ifixit.com/igi/DdhLc1N4sRZfdMOW.medium" alt="How to Replace a Bike Chain With a Master Link"></a>
          </p>
        </div>
          </div>



<h3>Riding Away From Standardization</h3>



<p>For example, the bottom bracket—the tubular bearing assembly at the bottom of the frame that the pedal axle threads through—has long been a fairly standard part.</p>



<p>Over the years, and on different continents, there may have been a few thread sizes, but a cyclist could easily buy the right part for a surprisingly reasonable price. Just as important, you’ve been able to remove the bottom bracket with one of a few simple tools. Now, though, a bike shop has to keep 20+ tools on hand to deal with all the proprietary fittings.</p>



<figure><img fetchpriority="high" loading="lazy" decoding="async" width="4000" height="2672" src="https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12.jpg 4000w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-1536x1026.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-2048x1368.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-1347x900.jpg 1347w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-450x300.jpg 450w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-768x512.jpg 768w" sizes="(max-width: 4000px) 100vw, 4000px"><figcaption>Standard parts make modding and repair easier. Photo Charlie Sorrel.</figcaption></figure>



<p>On electric bikes, things are even worse. Batteries are as non-standard as they are on cell phones. Instead of creating a standard, a kind of giant li-ion AA-equivalent for all bikes, you’re stuck buying non-standard sizes that you won’t be able to use on a new bike. This creates its own kind of lock-in, like the batteries on power tools, perhaps making you more likely to stick with the same brand within a family.</p>



<p>Then there are the apps. If you’re considering an e-bike that requires an app to function, or to change settings, do not buy that bike. When (not if) that app is abandoned, the bike will become at best a hobbled version of itself.</p>



<p>The result is that possibly the greenest, most efficient form of transport is turning into yet another source of landfill and e-waste. But why?</p>



<p>The cynical—and probably correct—take is that it boosts sales. By using proprietary parts, a bike manufacturer guarantees you have to go back to them for spares. And if those spares are not readily available, or are too expensive, then maybe you’ll just give up and buy a new bike instead.</p>



<p>Couple this with the explosion in new bike tech in recent years, which is itself designed to drive the desire to “upgrade” a perfectly good bike by replacing it with a new one, and you can see the attraction for the bean counters. Electronic, wireless gear shifters. Carbon-fiber seat posts. Active, self-adjusting suspension. Proprietary apps for changing key features like the power mode. All of these are superfluous for most riders, and add complexity to what is essentially a very simple, and pretty much perfect, machine.</p>



<h3>Buy Cheap, Buy Twice</h3>



<p>Bikes are getting ever more popular, in large part thanks to e-bikes, which make riding easy for people who would not otherwise consider cycling. That’s good news! Alas, to service this popularity, cheap and crappy bikes have proliferated.</p>



<p>“Budget bikes from ‘big box’ stores […] cost little ($150 to $250) because manufacturers cut corners. These bikes are built to fail: badly engineered, constructed from low-grade materials and fabricated in countries with inhumane labor standards,” writes cycling advocate <a href="https://nyc.streetsblog.org/2021/03/24/opinion-how-the-budget-bike-trap-creates-inequality-in-nyc">Josh Bicker on StreetsBlog NYC</a>.</p>



<p>These bikes are often broken out of the box. That’s bad if the buyer is a bike shop, and possibly deadly if the buyer is an inexperienced rider buying off the internet. Buying a used bike is a much better way to get a well-made machine for a good price. The downside of that is that you need to know what to look for, and how to bring that bike up to correct working order so that it is reliable and safe.</p>



<figure><img loading="lazy" decoding="async" width="4256" height="2832" src="https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733.jpg 4256w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-1536x1022.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-2048x1363.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-1353x900.jpg 1353w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-450x300.jpg 450w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-768x512.jpg 768w" sizes="(max-width: 4256px) 100vw, 4256px"></figure>



<p>Fortunately, that’s possible with local bike kitchens and co-ops, or by asking your bike shop to look over the bike for you. This works best if that bike isn’t using a bunch of proprietary parts. You can’t reach into the spare parts bin for a brake caliper if your bike uses a proprietary disk-brake design, whether that’s a super-high-end model, or a closed unit with more plastic than metal.</p>



<p>Ideally all bikes would continue to draw on a pool of standard part-types, but manufacturers seem set on the opposite. This makes it all the more important that we have legislation to force them to make proprietary parts available for riders to buy themselves, not just selling to repair shops (if at all). And with the increase of technology in bikes, public repair information is also essential. You’re definitely not going to find powered-hub servicing guides on <a href="https://sheldonbrown.com/">Sheldon Brown</a>. Fingers crossed, but that legislation may indeed be on the way.</p>


  <div>
      <p><span>Featured Guide</span>

              <a href="https://www.ifixit.com/Guide/How+to+Repair+Electric+Bikes/75971"><img decoding="async" loading="lazy" src="https://guide-images.cdn.ifixit.com/igi/Od2deYTwnAFriE1g.medium" alt="How to Repair Electric Bikes"></a></p><h3>How to Repair Electric Bikes</h3>
        <p>What to inspect and do when looking to resolve…</p>
        <p><a href="https://www.ifixit.com/Guide/How+to+Repair+Electric+Bikes/75971">Follow this Guide</a></p>
          </div>



<h3>Batteries Should Be Interoperable</h3>



<p>Let’s get to batteries. After tires, tubes, cables and chains, the one thing on an electric bike that will 100% wear out and need replacing is the battery. Unlike most laptops, you can easily remove the battery from the bike. But forget about ordering up a standard replacement, because there isn’t one. The batteries are often shaped to fit the bike, but even those that clip into a section below the rear rack, or are otherwise independently-mounted vary in capacity, voltage, and current delivery.</p>



<p>That keeps replacement costs higher, but it also means that you are stuck if the manufacturer discontinues your battery. A bike that is otherwise in perfect working order might end up prematurely useless, or you will end up in the world of shonky spares from Amazon or another unreliable source.</p>



<figure><img loading="lazy" decoding="async" width="4000" height="3000" src="https://valkyrie.cdn.ifixit.com/media/2024/10/11013742/2010-09-26-4.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/10/11013742/2010-09-26-4.jpg 4000w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013742/2010-09-26-4-1536x1152.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013742/2010-09-26-4-2048x1536.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013742/2010-09-26-4-1200x900.jpg 1200w" sizes="(max-width: 4000px) 100vw, 4000px"><figcaption>Sure, why not? Photo Charlie Sorrel.</figcaption></figure>



<p>The standard excuses apply. Original parts are designed to work safely together. Using non-official parts can be dangerous, etc. That may be true, but if so, it’s only because the parts were designed that way. The blame is with the manufacturer. It’s totally possible to design around standard batteries. Ask anyone who’s ever made a device that runs on AA batteries, or swapped a new 12-Volt lead-acid battery into a car.</p>



<p>We are 100% against this trend. A bike is an almost perfect machine, and e-bikes combined with public transit are probably the best way to get cars out of cities, and to make personal transport sustainable.</p>



<p>“There’s no machine known that is more efficient than a human on a bicycle,” Bill Nye, the science guy,<a href="https://bigthink.com/articles/bill-nye-the-city-of-the-future/"> told Big Think</a>. “Bowl of oatmeal, 30 miles — you can’t come close to that.”</p>



<p>And yet all that is being ruined in an effort to make us buy a new bike every few years, instead of repairing the ones we have. Newer, more exotic specs and components encourage us to “upgrade,” just like with smartphones, laptops, and cameras, and they also turn the perfect machine into an unknowable black box that is often not worth the cost of repair.</p>



<figure><img loading="lazy" decoding="async" width="695" height="460" src="https://valkyrie.cdn.ifixit.com/media/2024/10/11084604/zcqutape5vm2kvphcxlu.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/10/11084604/zcqutape5vm2kvphcxlu.jpg 695w, https://valkyrie.cdn.ifixit.com/media/2024/10/11084604/zcqutape5vm2kvphcxlu-300x200.jpg 300w" sizes="(max-width: 695px) 100vw, 695px"><figcaption>The Infinite Battery is endlessly repairable, and even looks cool.</figcaption></figure>



<p>One ray of hope here is the Infinite Battery by Gouach, <a href="https://www.indiegogo.com/projects/infinite-the-repairable-universal-ebike-battery#/">currently seeking development funding via Indiegogo</a>. It’s compatible with all major brands’ setups, and offers the usual power capacity and safety features, but it is totally user serviceable. All parts can be swapped out individually, and when the cells inside start to wear out, you can replace them individually, almost as if your bike ran on around 30 AA cells</p>



<p>If you can repair a bike, and use standard spares, either new or harvested from dead bikes, then a bike can essentially live forever. If the growing anti-repair practices of the bike industry are allowed to threaten that, then we no longer own our machines. We are essentially renting disposable gadgets instead.</p>
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How I Experience Web Today (257 pts)]]></title>
            <link>https://how-i-experience-web-today.com</link>
            <guid>41840931</guid>
            <pubDate>Mon, 14 Oct 2024 19:22:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://how-i-experience-web-today.com">https://how-i-experience-web-today.com</a>, See on <a href="https://news.ycombinator.com/item?id=41840931">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <p>
                    https://example.com
                </p>
                <p><a href="https://how-i-experience-web-today.com/detail">
                    Then it shows me something
                </a></p><p>
                    Example Domain. This domain is for use in illustrative examples in documents. You may use this domain in literature without prior coordination or asking for ...
                </p>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Play 3.0 mini – A lightweight, reliable, cost-efficient Multilingual TTS model (102 pts)]]></title>
            <link>https://play.ht/news/introducing-play-3-0-mini/</link>
            <guid>41840872</guid>
            <pubDate>Mon, 14 Oct 2024 19:16:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://play.ht/news/introducing-play-3-0-mini/">https://play.ht/news/introducing-play-3-0-mini/</a>, See on <a href="https://news.ycombinator.com/item?id=41840872">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Today we’re releasing our most capable and conversational voice model that can speak in 30+ languages using any voice or accent, with industry leading speed and accuracy. We’re also releasing 50+ new conversational AI voices across languages.</p>
<p>Our mission is to make voice AI accessible, personal and capable for all. Part of that mission is to advance the current state of interactive voice technology in conversational AI and elevate user experience.</p>
<p>When you’re building real time applications using TTS, a few things really matter – latency, reliability, quality and naturalness of speech. While we’ve been leading on latency and naturalness of speech with our previous generation models, Play 3.0 mini makes significant improvements to reliability and audio quality while still being the fastest and most conversational voice model.</p>
<p>Play3.0 mini is the first in a series of efficient multi-lingual AI text-to-speech models we plan to release over the coming months. Our goal is to make the models smaller and cost-efficient so they can be run on devices and at scale.</p>
<h2>Play 3.0 mini is our fastest, most conversational speech model yet</h2>
<p>3.0 mini achieves a mean latency of 189 milliseconds for TTFB, making it our fastest AI Text to Speech model. It supports text-in streaming from LLMs and audio-out streaming, and can be used via our HTTP REST API, websockets API or SDKs. 3.0 mini is also more efficient than Play 2.0, and runs inference 28% faster.</p>
<figure><p>
<iframe title="Introducing Play 3.0 mini - a new compact Text to Speech model for realtime Voice AI" width="640" height="360" src="https://www.youtube.com/embed/DusTj5NLC9w?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p></figure>
<h2>Play 3.0 mini supports 30+ languages across any voice</h2>
<p>Play 3.0 mini now supports more than 30+ languages, many with multiple male and female voice options out of the box.&nbsp; Our English, Japanese, Hindi, Arabic, Spanish, Italian, German, French, and Portuguese voices are available now for production use cases, and are available through our <a href="https://docs.play.ht/reference/api-getting-started">API</a> and on our <a href="https://www.play.ht/playground">playground</a>.&nbsp; Additionally, Afrikaans, Bulgarian, Croatian, Czech, Hebrew, Hungarian, Indonesian, Malay, Mandarin, Polish, Serbian, Swedish, Tagalog, Thai, Turkish, Ukrainian, Urdu, and Xhosa are available for testing.</p>
<h2>Play 3.0 mini is more accurate</h2>
<p>Our goal with Play 3.0 mini was to build the best TTS model for conversational AI. To achieve this, the model had to outperform competitor models in latency and accuracy while generating speech in the most conversational tone.</p>
<p>LLMs hallucinate and voice LLMs are no different. Hallucinations in voice LLMs can be in the form of extra or missed words or numbers in the output audio not part of the input text. Sometimes they can just be random sounds in the audio. This makes it difficult to use generative voice models reliably.</p>
<p>Here are some challenging text prompts that most TTS models struggle to get right –</p>
<blockquote>
<p><em>“Okay, so your flight UA2390 from San Francisco to Las Vegas on November 3rd is confirmed. And, your ticket number is F X 2, 3 9 A, 7 R T. The flight is scheduled to depart at 2:45 p.m. Is there anything else I can assist you with?”</em></p>
</blockquote>
<figure><audio controls="" src="https://playtht-website-assets.s3.amazonaws.com/voice-samples/t1.wav"></audio></figure>
<blockquote>
<p><em>“Now, when people RSVP, they can call the event coordinator at&nbsp;<strong>555 342 1234</strong>, but if they need more details, they can also call the backup number, which is&nbsp;<strong>416 789 0123</strong>.”</em></p>
</blockquote>
<figure><audio controls="" src="https://playtht-website-assets.s3.amazonaws.com/voice-samples/t2+(1).mp3"></audio></figure>
<blockquote>
<p>“<em>I’ve successfully processed your order and I’d like to confirm your product ID. It is A as in Alpha, 1, 2, 3, B as in Bravo, 5, 6, 7,&nbsp; Z as in Zulu, 8, 9, 0,&nbsp; X as in X-ray.</em>“</p>
</blockquote>
<figure><audio controls="" src="https://playtht-website-assets.s3.amazonaws.com/voice-samples/t3.mp3"></audio></figure>
<p>3.0 mini was finetuned specifically on a diverse dataset of alpha-numeric phrases to make it reliable for critical use cases where important information such as phone numbers, passport numbers, dates, currencies, etc. can’t be misread.</p>
<h2>Play 3.0 mini reads alphanumeric sequences more naturally</h2>
<p>We’ve trained the model to read numbers and acronyms just like humans do. The model adjusts its pace and slows down any alpha-numeric characters. Phone numbers for instance are read out with more natural pacing, and similarly all acronyms and abbreviations. This makes the overall conversational experience more natural.</p>
<blockquote>
<p><em>“Alright, let’s troubleshoot your laptop issue. First, let’s confirm your device’s ID so we’re on the same page. The I D is 894-d94-774-496-438-9b0-d2. Did I get that right?</em>“</p>
</blockquote>
<figure><audio controls="" src="https://playtht-website-assets.s3.amazonaws.com/voice-samples/t4.mp3"></audio></figure>
<h2>Play 3.0 mini achieves the best voice similarity for voice cloning</h2>
<p>When cloning voices, close often isn’t good enough.&nbsp; Play 3.0 voice cloning achieves state-of-the-art performance when cloning voices, ensuring accurate reproduction of accent, tone, and inflection of cloned voices.&nbsp; In benchmarking using a popular open source embedding model, we lead competitor models by a wide margin for similarity to the original voice.&nbsp; Try it for yourself by cloning your own voice, and talking to yourself on <a href="https://play.ai/" target="_blank" rel="noopener">https://play.ai</a>&nbsp;</p>
<h2>Websockets API Support</h2>
<p>3.0 mini’s API now supports websockets, which significantly reduces the overhead of opening and closing HTTP connections, and makes it easier than ever to enable text-in streaming from LLMs or other sources.</p>
<h2>Play 3.0 mini is a cost efficient model</h2>
<p>We’re happy to announce reduced pricing for our higher volume Startup and Growth tiers, and have now introduced a new Pro tier at $49 a month for businesses with more modest requirements.&nbsp; Check out our new pricing table <a href="https://play.ht/pricing/?planType=api">here</a>.</p>
<p>We look forward to seeing what you build with us!&nbsp; If you’ve custom, high volume requirements, feel free to <a href="https://play.ht/contact-us/">contact our sales team</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Funding Construction of Seven U.S. Nuclear Reactors (336 pts)]]></title>
            <link>https://www.wsj.com/business/energy-oil/google-nuclear-power-artificial-intelligence-87966624</link>
            <guid>41840769</guid>
            <pubDate>Mon, 14 Oct 2024 19:06:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/business/energy-oil/google-nuclear-power-artificial-intelligence-87966624">https://www.wsj.com/business/energy-oil/google-nuclear-power-artificial-intelligence-87966624</a>, See on <a href="https://news.ycombinator.com/item?id=41840769">Hacker News</a></p>
Couldn't get https://www.wsj.com/business/energy-oil/google-nuclear-power-artificial-intelligence-87966624: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Commonly used arm positions can overestimate blood pressure readings: study (144 pts)]]></title>
            <link>https://medicalxpress.com/news/2024-10-commonly-arm-positions-substantially-overestimate.html</link>
            <guid>41840023</guid>
            <pubDate>Mon, 14 Oct 2024 17:57:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medicalxpress.com/news/2024-10-commonly-arm-positions-substantially-overestimate.html">https://medicalxpress.com/news/2024-10-commonly-arm-positions-substantially-overestimate.html</a>, See on <a href="https://news.ycombinator.com/item?id=41840023">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
									    
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2024/johns-hopkins-medicine-1.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2024/johns-hopkins-medicine-1.jpg" data-sub-html="Researchers say their study findings underscore the importance of adhering to clinical guidelines calling for firm arm support on a desk or other surface when measuring blood pressure. Credit: Tammy Brady">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2024/johns-hopkins-medicine-1.jpg" alt="Johns Hopkins Medicine study finds commonly used arm positions can substantially overestimate blood pressure readings" title="Researchers say their study findings underscore the importance of adhering to clinical guidelines calling for firm arm support on a desk or other surface when measuring blood pressure. Credit: Tammy Brady" width="800" height="529">
             <figcaption>
                Researchers say their study findings underscore the importance of adhering to clinical guidelines calling for firm arm support on a desk or other surface when measuring blood pressure. Credit: Tammy Brady
            </figcaption>        </figure>
    </div><p>A study led by Johns Hopkins Medicine researchers concludes that commonly used ways of positioning the patient's arm during blood pressure (BP) screenings can substantially overestimate test results and may lead to a misdiagnosis of hypertension.</p>

                                        
                                                                                  
                                         

                                                                                                                                    <p>In a report on the <a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/10.1001/jamainternmed.2024.5213" target="_blank">study</a>, which was published Oct. 7 in <i>JAMA Internal Medicine</i>, investigators examined the effects of three different arm positions: an arm supported on a desk, arm supported on a lap, and an unsupported arm hanging at the patient's side.</p>
<p>Researchers found that lap support overestimated <a href="https://medicalxpress.com/tags/systolic+pressure/" rel="tag">systolic pressure</a> (the top number in a BP reading) by nearly 4 mmHg, and an unsupported arm hanging at the side overestimated systolic pressure by nearly 7 mmHg.</p>
<p>The findings confirm that arm position makes a "huge difference" when it comes to an accurate <a href="https://medicalxpress.com/tags/blood+pressure/" rel="tag">blood pressure</a> measurement, says Tammy Brady, M.D., Ph.D., vice chair for <a href="https://medicalxpress.com/tags/clinical+research/" rel="tag">clinical research</a> in the Department of Pediatrics at the Johns Hopkins University School of Medicine, medical director of the pediatric hypertension program at Johns Hopkins Children's Center, deputy director of the Welch Center for Prevention, Epidemiology, and Clinical Research and senior author of the study.</p>
<p>And they underscore the importance of adhering to clinical guidelines calling for firm support on a desk or other surface when measuring blood pressure, the investigators add.</p>

                                                                                                                                                         
                                                                                                                                                                                                <p>According to the American Heart Association, nearly half of U.S. adults have elevated blood pressure, a diagnosis made when the measured force of blood flowing through blood vessels is higher than what is generally considered normal, on average 120/80.*</p>
<p>Untreated, <a href="https://medicalxpress.com/tags/high+blood+pressure/" rel="tag">high blood pressure</a> increases the risk of stroke, heart attack and other serious cardiovascular conditions. Because hypertension may cause minimal or no symptoms, early and frequent screening during routine checkups is considered the cornerstone of hypertension management.</p>
<p>In most cases, lifestyle changes such as weight loss, healthy diets and exercise, as well as therapy with any of a variety of medications, can keep BP under control.</p>
<p>The latest clinical practice guidelines from the American Heart Association emphasize several key steps for an <a href="https://medicalxpress.com/tags/accurate+measurement/" rel="tag">accurate measurement</a>—including <a href="https://clinicalconnection.hopkinsmedicine.org/videos/cuff-size-matters-for-blood-pressure-monitoring-and-device-accuracy" target="_blank">appropriate cuff size</a>, back support, feet flat on the floor with legs uncrossed, and an appropriate arm position, in which the middle of an adjustable BP cuff is positioned at mid-heart level on an arm supported on a desk or table.</p>
<p>Despite these recommendations, the researchers say BP is too often measured with patients seated on an exam table without any, or inadequate, arm support. In some cases, a clinician holds the arm, or the patient holds an arm in their lap.</p>

                                                                                                                                            <p>In the new Johns Hopkins study, the researchers recruited 133 adult participants (78% Black, 52% female) between Aug. 9, 2022, and June 1, 2023. Study participants, who ranged from age 18 to 80, were sorted at random into one of six possible groups that differed by order of the three seated arm positions.</p>
<p>Measurements were taken during a single visit between 9 a.m. and 6 p.m. Before BP measures were taken, all participants first emptied their bladders and then walked for two minutes to mimic a typical clinical scenario in which people walk into a clinic or office before screening takes place. They then underwent a five-minute, seated rest period with their backs and feet supported.</p>
<p>Each person, wearing an upper arm BP cuff selected and sized based on their upper arm size, had three sets of triplicate measurements taken with a digital blood pressure device 30 seconds apart.</p>
<p>Upon completion of each set of three measurements, the cuff was removed, participants walked for two minutes and rested for five minutes. On the same visit, they then underwent a fourth set of triplicate measurements with their arm supported on a desk, a set used to account for well-known variations in BP readings. All of the measurements were conducted in a quiet and private space, and participants were asked not to talk to researchers or use their phones during the screening.</p>
<p>Researchers found that BP measurements obtained with arm positions frequently used in clinical practice—an arm on the lap or unsupported at the side—were markedly higher than those obtained when the arm was supported on a desk, the standard, recommended arm position.</p>
<p>Supporting the arm on the lap overestimated systolic BP—the top number of a reading, or the force of blood flow when pumped out of the heart, by 3.9 mmHg and diastolic blood pressure—the bottom number, or the pressure in the arteries when the heart rests between beats, by 4.0 mmHg. An unsupported arm at the side overestimated systolic by 6.5 mmHg and diastolic by 4.4 mmHg.</p>
<p>"If you are consistently measuring blood pressure with an unsupported arm, and that gives you an overestimated BP of 6.5 mmHg, that's a potential difference between a systolic BP of 123 and 130, or 133 and 140—which is considered stage 2 hypertension," says Sherry Liu, M.H.S., an epidemiology research coordinator at the Welch Center for Prevention, Epidemiology, and Clinical Research, Department of Epidemiology, at Johns Hopkins Bloomberg School of Public Health and study author.</p>
<p>Investigators caution that their study results may only apply during screenings with automated BP devices, and may not apply to readings done with other BP devices.</p>
<p>However, Brady says, the findings suggest that clinicians need to pay better attention to best practice guidelines, and that patients "must advocate for themselves in the clinical setting and when measuring their BP at home."</p>
<h2>*Current blood pressure guidelines and variability</h2>
<p>According to the 2017 guidelines from the American Heart Association, normal blood pressure is less than 120/80 mmHg. Blood pressure readings between 120-129/80 mmHg are classified as elevated, with hypertension being diagnosed at 130/80 mmHg or higher.</p>
<p>It's important to note that blood pressure can vary due to factors such as stress, diet, caffeine intake, and smoking. Therefore, to obtain an accurate reading, it is crucial to measure blood pressure under consistent conditions, following clinical guidelines.</p>

                                                                                                                                                                            
                                        											<div>
												                                                    <p><strong>More information:</strong>
                                                    Hairong Liu et al, Arm Position and Blood Pressure Readings, <i>JAMA Internal Medicine</i> (2024). DOI: 10.1001/jamainternmed.2024.5213 , <a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/10.1001/jamainternmed.2024.5213" target="_blank">jamanetwork.com/journals/jamai … ainternmed.2024.5213</a>
																								
																								</p>
																							</div>
                                        											
																					
                                                                                                                        
                                        <!-- print only -->
                                        <div>
                                            <p><strong>Citation</strong>:
                                                 Commonly used arm positions can substantially overestimate blood pressure readings, study finds (2024, October 7)
                                                 retrieved 14 October 2024
                                                 from https://medicalxpress.com/news/2024-10-commonly-arm-positions-substantially-overestimate.html
                                            </p>
                                            <p>
                                            This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
                                            part may be reproduced without the written permission. The content is provided for information purposes only.
                                            </p>
                                        </div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Response to DHH (126 pts)]]></title>
            <link>https://ma.tt/2024/10/on-dhh/</link>
            <guid>41839864</guid>
            <pubDate>Mon, 14 Oct 2024 17:42:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ma.tt/2024/10/on-dhh/">https://ma.tt/2024/10/on-dhh/</a>, See on <a href="https://news.ycombinator.com/item?id=41839864">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page" role="main">

			
				
<article id="post-128140">
	<!-- .entry-header -->

		<div>
		
<p>I’ll just remind everyone at the start that this is a respectful debate, and DHH and I tried to get on a call but couldn’t because we were both traveling.</p>



<p>However, “<a href="https://world.hey.com/dhh/automattic-is-doing-open-source-dirty-b95cf128">Automattic is doing open source dirty</a>” is an abomination of a headline, and David’s second post <a href="https://world.hey.com/dhh/open-source-royalty-and-mad-kings-a8f79d16">Open source royalty and mad kings</a>, is just sloppy. So I’m forced to reply publicly:</p>



<p>DHH claims to be an expert on open source, but his toxic personality and inability to scale teams means that although he has invented about half a trillion dollars worth of good ideas, most of the value has been captured by others. Let’s look at 37signals portfolio:</p>



<ul>
<li><a href="https://www.hey.com/">Hey</a>, proprietary, some sort of email / calendar / blogging thing that almost no one uses. It’s trying to be Gmail/Workspace and <a href="https://medium.com/">Medium</a> at the same time. And you can arbitrarily cut off anyone publishing with Hey, they have no open source rights.</li>



<li><a href="https://once.com/campfire">Campfire</a>, proprietary, you invented <a href="https://slack.com/">Slack</a> but they took the idea and built a $900M/ARR business with it, while you are trying to make shrinkwrap licensing a “thing” with <a href="https://once.com/">Once</a>.</li>



<li><a href="https://once.com/writebook">Writebook</a>, proprietary. Pretty cool.</li>



<li><a href="https://basecamp.com/">Basecamp</a>, proprietary. Great software. You invented the ideas <a href="https://www.atlassian.com/">Atlassian</a> ran with and built a $4.4B/revenue and growing business.</li>



<li><a href="https://rubyonrails.org/">Rails</a>, finally some open source! Looks like ~943k lines of code, 143k from Basecamp org. Automattic publishes 6.58M lines of open source code, 6.9x more than you. Yet, we’re “doing open source dirty”? <a href="https://www.shopify.com/">Shopify</a> used Rails to build a $7B/revenue and growing business, why didn’t you?</li>
</ul>



<p>David, perhaps it would be good to explore with a therapist or coach why you keep having these great ideas but cannot scale them beyond a handful of niche customers. I will give full credit and respect. 37signals inspired tons of what Automattic does! We’re now half a billion in revenue. Why are you still so small?</p>



<p>I was surprised someone as smart as DHH would fall for WP Engine’s lame deferral to make this about “GPL code” or forking, rather than trademarks. We have no problem with their use of GPL code, our beef is with their trademark abuse.</p>



<p>Let’s talk about trademarks! I don’t own the WordPress trademark personally, it belongs to a foundation on which I’m one of three votes. Rails? </p>



<blockquote>
<p><strong>“Rails”</strong>,&nbsp;<strong>“Ruby on Rails”</strong>, and the&nbsp;<strong>Rails logo</strong>&nbsp;are registered trademarks of&nbsp;<strong>David Heinemeier Hansson</strong>, but are under exclusive license to&nbsp;<a href="https://rubyonrails.org/foundation">The Rails Foundation</a>, which is responsible for administering their use and permission. You may not use these trademarks in a commercial setting to imply that your product or service is endorsed or associated with Ruby on Rails without permission. You may use these marks to refer to Ruby on Rails in a way where it’s clear that you’re simply referring to the project, not claiming endorsement or association.</p>
</blockquote>



<p>Huh, sounds like if I wanted to start RailsEngine I would need a trademark license. You are  ignoring WP Engine’s trademark abuse while retaining the same for your Rails trademark. The same as Drupal, where “Drupal is a registered trademark of Dries Buytaert, who retains sole ownership and control of this policy and any trademark licensing.” (<a href="https://dri.es/solving-the-maker-taker-problem">Dries has also decided to drop in on this debate</a>.)</p>



<p>Dries or David could arbitrarily withdraw their trademarks from the foundations / etc. at any time and for any reason or no reason. If they die, it’s not clear what happens to the trademarks. Their communities should look into that and consider a different name or taking over the trademark into a Foundation with multiple board members. </p>



<p>David, perhaps instead of spending <a href="https://www.autoblog.com/news/pagani-zonda-hh-commissioner-revealed-as-30-year-old-chicago-sof">$2M on a race car</a>, you should do some <a href="https://ma.tt/2024/09/charitable-contributions/">philanthropy</a>.</p>



<p>Instead of <a href="https://world.hey.com/dhh/how-it-started-how-it-s-going-baefaf09">bragging about your beautiful office in the clouds</a>, you should question why you can’t scale teams.</p>



<p>When you did a (less generous) buy-out offer <a href="https://www.theverge.com/2021/4/30/22412714/basecamp-employees-memo-policy-hansson-fried-controversy">33% of your team left</a>, vs <a href="https://ma.tt/2024/10/alignment/">8.4% of mine</a>.</p>



<p>I’m unsure why you felt you had to insert yourself into this fight with Silver Lake / WP Engine and take their side, but here we are.</p>



<p>Respectfully,<br>Matt</p>

	</div><!-- .entry-content -->
	
	<!-- .entry-meta -->
</article><!-- #post -->
						<nav>
		<h2>
			Post navigation		</h2>
		<!-- .nav-links -->
	</nav><!-- .navigation -->
						
<!-- #comments -->
			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Vortex – a high-performance columnar file format (126 pts)]]></title>
            <link>https://github.com/spiraldb/vortex</link>
            <guid>41839773</guid>
            <pubDate>Mon, 14 Oct 2024 17:34:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/spiraldb/vortex">https://github.com/spiraldb/vortex</a>, See on <a href="https://news.ycombinator.com/item?id=41839773">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Vortex</h2><a id="user-content-vortex" aria-label="Permalink: Vortex" href="#vortex"></a></p>
<p dir="auto"><a href="https://github.com/spiraldb/vortex/actions"><img src="https://github.com/fulcrum-so/vortex/actions/workflows/ci.yml/badge.svg" alt="Build Status"></a>
<a href="https://crates.io/crates/vortex-array" rel="nofollow"><img src="https://camo.githubusercontent.com/5e4c7b2bac2564f0b28006fdc5832127a05ecfe4c324a299deca48c113f2539a/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f766f727465782d61727261792e737667" alt="Crates.io" data-canonical-src="https://img.shields.io/crates/v/vortex-array.svg"></a>
<a href="https://docs.rs/vortex-array" rel="nofollow"><img src="https://camo.githubusercontent.com/bb82ef82b46a0d868cb2eaa855fb1724ffd3d9d614d5ff26be508aec779e9792/68747470733a2f2f646f63732e72732f766f727465782d61727261792f62616467652e737667" alt="Documentation" data-canonical-src="https://docs.rs/vortex-array/badge.svg"></a>
<a href="https://pypi.org/project/vortex-array/" rel="nofollow"><img src="https://camo.githubusercontent.com/506d8e3db54d4420bdf98ef197df095cb9bf0b76bf29d032704d00cb6e4f4198/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f766f727465782d6172726179" alt="PyPI - Python Version" data-canonical-src="https://img.shields.io/pypi/pyversions/vortex-array"></a></p>
<p dir="auto">Vortex is a toolkit for working with compressed Apache Arrow arrays in-memory, on-disk, and over-the-wire.</p>
<p dir="auto">Vortex is designed to be to columnar file formats what Apache DataFusion is to query engines (or, analogously,
what LLVM + Clang are to compilers): a highly extensible &amp; extremely fast <em>framework</em> for building a modern
columnar file format, with a state-of-the-art, "batteries included" reference implementation.</p>
<p dir="auto">Vortex is an aspiring successor to Apache Parquet, with dramatically faster random access reads (100-200x faster)
and scans (2-10x faster), while preserving approximately the same compression ratio and write throughput. It will also support very wide
tables (at least 10s of thousands of columns) and (eventually) on-device decompression on GPUs.</p>
<div dir="auto"><p dir="auto">Caution</p><p dir="auto">This library is still under rapid development and is a work in progress!</p>
<p dir="auto">Some key features are not yet implemented, both the API and the serialized format are likely to change in breaking ways,
and we cannot yet guarantee correctness in all cases.</p>
</div>
<p dir="auto">The major features of Vortex are:</p>
<ul dir="auto">
<li><strong>Logical Types</strong> - a schema definition that makes no assertions about physical layout.</li>
<li><strong>Zero-Copy to Arrow</strong> - "canonicalized" (i.e., fully decompressed) Vortex arrays can be zero-copy converted to/from Apache Arrow arrays.</li>
<li><strong>Extensible Encodings</strong> - a pluggable set of physical layouts. In addition to the builtin set of Arrow-compatible encodings,
the Vortex repository includes a number of state-of-the-art encodings (e.g., FastLanes, ALP, FSST, etc.) that are implemented
as extensions. While arbitrary encodings can be implemented as extensions, we have intentionally chosen a small set
of encodings that are highly data-parallel, which in turn allows for efficient vectorized decoding, random access reads,
and (in the future) decompression on GPUs.</li>
<li><strong>Cascading Compression</strong> - data can be recursively compressed with multiple nested encodings.</li>
<li><strong>Pluggable Compression Strategies</strong> - the built-in Compressor is based on BtrBlocks, but other strategies can trivially be used instead.</li>
<li><strong>Compute</strong> - basic compute kernels that can operate over encoded data (e.g., for filter pushdown).</li>
<li><strong>Statistics</strong> - each array carries around lazily computed summary statistics, optionally populated at read-time.
These are available to compute kernels as well as to the compressor.</li>
<li><strong>Serialization</strong> - Zero-copy serialization of arrays, both for IPC and for file formats.</li>
<li><strong>Columnar File Format (in progress)</strong> - A modern file format that uses the Vortex serde library to store compressed array data.
Optimized for random access reads and extremely fast scans; an aspiring successor to Apache Parquet.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview: Logical vs Physical</h2><a id="user-content-overview-logical-vs-physical" aria-label="Permalink: Overview: Logical vs Physical" href="#overview-logical-vs-physical"></a></p>
<p dir="auto">One of the core design principles in Vortex is strict separation of logical and physical concerns.</p>
<p dir="auto">For example, a Vortex array is defined by a logical data type (i.e., the type of scalar elements) as well as a physical encoding
(the type of the array itself). Vortex ships with several built-in encodings, as well as several extension encodings.</p>
<p dir="auto">The built-in encodings are primarily designed to model the Apache Arrow in-memory format, enabling us to construct
Vortex arrays with zero-copy from Arrow arrays. There are also several built-in encodings (e.g., <code>sparse</code> and
<code>chunked</code>) that are useful building blocks for other encodings. The included extension encodings are mostly designed
to model compressed in-memory arrays, such as run-length or dictionary encoding.</p>
<p dir="auto">Analogously, <code>vortex-serde</code> is designed to handle the low-level physical details of reading and writing Vortex arrays. Choices
about which encodings to use or how to logically chunk data are left up to the <code>Compressor</code> implementation.</p>
<p dir="auto">One of the unique attributes of the (in-progress) Vortex file format is that it encodes the physical layout of the data within the
file's footer. This allows the file format to be effectively self-describing and to evolve without breaking changes to
the file format specification.</p>
<p dir="auto">In fact, the format is designed to support forward compatibility by optionally embedding WASM decoders directly into the files
themselves. This should help avoid the rapid calcification that has plagued other columnar file formats.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Components</h2><a id="user-content-components" aria-label="Permalink: Components" href="#components"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Logical Types</h3><a id="user-content-logical-types" aria-label="Permalink: Logical Types" href="#logical-types"></a></p>
<p dir="auto">The Vortex type-system is still in flux. The current set of logical types is:</p>
<ul dir="auto">
<li>Null</li>
<li>Bool</li>
<li>Integer(8, 16, 32, 64)</li>
<li>Float(16, b16, 32, 64)</li>
<li>Binary</li>
<li>UTF8</li>
<li>Struct</li>
<li>List (partially implemented)</li>
<li>Date/Time/DateTime/Duration (implemented as an extension type)</li>
<li>Decimal: TODO</li>
<li>FixedList: TODO</li>
<li>Tensor: TODO</li>
<li>Union: TODO</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Canonical/Flat Encodings</h3><a id="user-content-canonicalflat-encodings" aria-label="Permalink: Canonical/Flat Encodings" href="#canonicalflat-encodings"></a></p>
<p dir="auto">Vortex includes a base set of "flat" encodings that are designed to be zero-copy with Apache Arrow. These are the
canonical representations of each of the logical data types. The canonical encodings currently supported are:</p>
<ul dir="auto">
<li>Null</li>
<li>Bool</li>
<li>Primitive (Integer, Float)</li>
<li>Struct</li>
<li>VarBin (Binary, UTF8)</li>
<li>VarBinView (Binary, UTF8)</li>
<li>Extension</li>
<li>...with more to come</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Compressed Encodings</h3><a id="user-content-compressed-encodings" aria-label="Permalink: Compressed Encodings" href="#compressed-encodings"></a></p>
<p dir="auto">Vortex includes a set of highly data-parallel, vectorized encodings. These encodings each correspond to a compressed
in-memory array implementation, allowing us to defer decompression. Currently, these are:</p>
<ul dir="auto">
<li>Adaptive Lossless Floating Point (ALP)</li>
<li>BitPacked (FastLanes)</li>
<li>Constant</li>
<li>Chunked</li>
<li>Delta (FastLanes)</li>
<li>Dictionary</li>
<li>Fast Static Symbol Table (FSST)</li>
<li>Frame-of-Reference</li>
<li>Run-end Encoding</li>
<li>RoaringUInt</li>
<li>RoaringBool</li>
<li>Sparse</li>
<li>ZigZag</li>
<li>...with more to come</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Compression</h3><a id="user-content-compression" aria-label="Permalink: Compression" href="#compression"></a></p>
<p dir="auto">Vortex's default compression strategy is based on the
<a href="https://www.cs.cit.tum.de/fileadmin/w00cfj/dis/papers/btrblocks.pdf" rel="nofollow">BtrBlocks</a> paper.</p>
<p dir="auto">Roughly, for each chunk of data, a sample of at least ~1% of the data is taken. Compression is then attempted (
recursively) with a set of lightweight encodings. The best-performing combination of encodings is then chosen to encode
the entire chunk. This sounds like it would be very expensive, but given basic statistics about a chunk, it is
possible to cheaply prune many encodings and ensure the search space does not explode in size.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Compute</h3><a id="user-content-compute" aria-label="Permalink: Compute" href="#compute"></a></p>
<p dir="auto">Vortex provides the ability for each encoding to specialize the implementation of a compute function to avoid
decompressing where possible. For example, filtering a dictionary-encoded UTF8 array can be more cheaply performed by
filtering the dictionary first.</p>
<p dir="auto">Note--as mentioned above--that Vortex does not intend to become a full-fledged compute engine, but rather to implement
basic compute operations as may be required for efficient scanning &amp; pushdown.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Statistics</h3><a id="user-content-statistics" aria-label="Permalink: Statistics" href="#statistics"></a></p>
<p dir="auto">Vortex arrays carry lazily-computed summary statistics. Unlike other array libraries, these statistics can be populated
from disk formats such as Parquet and preserved all the way into a compute engine. Statistics are available to compute
kernels as well as to the compressor.</p>
<p dir="auto">The current statistics are:</p>
<ul dir="auto">
<li>BitWidthFreq</li>
<li>TrailingZeroFreq</li>
<li>IsConstant</li>
<li>IsSorted</li>
<li>IsStrictSorted</li>
<li>Max</li>
<li>Min</li>
<li>RunCount</li>
<li>TrueCount</li>
<li>NullCount</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Serialization / Deserialization (Serde)</h3><a id="user-content-serialization--deserialization-serde" aria-label="Permalink: Serialization / Deserialization (Serde)" href="#serialization--deserialization-serde"></a></p>
<p dir="auto">The goals of the <code>vortex-serde</code> implementation are:</p>
<ul dir="auto">
<li>Support scanning (column projection + row filter) with zero-copy and zero heap allocation.</li>
<li>Support random access in constant or near-constant time.</li>
<li>Forward statistical information (such as sortedness) to consumers.</li>
<li>Provide IPC format for sending arrays between processes.</li>
<li>Provide an extensible, best-in-class file format for storing columnar data on disk or in object storage.</li>
</ul>
<p dir="auto">TODO: insert diagram here</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Integration with Apache Arrow</h2><a id="user-content-integration-with-apache-arrow" aria-label="Permalink: Integration with Apache Arrow" href="#integration-with-apache-arrow"></a></p>
<p dir="auto">Apache Arrow is the de facto standard for interoperating on columnar array data. Naturally, Vortex is designed to
be maximally compatible with Apache Arrow. All Arrow arrays can be converted into Vortex arrays with zero-copy,
and a Vortex array constructed from an Arrow array can be converted back to Arrow, again with zero-copy.</p>
<p dir="auto">It is important to note that Vortex and Arrow have different--albeit complementary--goals.</p>
<p dir="auto">Vortex explicitly separates logical types from physical encodings, distinguishing it from Arrow. This allows
Vortex to model more complex arrays while still exposing a logical interface. For example, Vortex can model a UTF8
<code>ChunkedArray</code> where the first chunk is run-length encoded and the second chunk is dictionary encoded.
In Arrow, <code>RunLengthArray</code> and <code>DictionaryArray</code> are separate incompatible types, and so cannot be combined in this way.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Usage</h3><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">For best performance we recommend using <a href="https://github.com/microsoft/mimalloc">MiMalloc</a> as the application's
allocator.</p>
<div dir="auto" data-snippet-clipboard-copy-content="#[global_allocator]
static GLOBAL_ALLOC: MiMalloc = MiMalloc;"><pre><span>#<span>[</span>global_allocator<span>]</span></span>
<span>static</span> <span>GLOBAL_ALLOC</span><span>:</span> <span>MiMalloc</span> = <span>MiMalloc</span><span>;</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Please see <a href="https://github.com/spiraldb/vortex/blob/develop/CONTRIBUTING.md">CONTRIBUTING.md</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<p dir="auto">In order to build vortex, you may also need to install the flatbuffer compiler (flatc):</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mac</h3><a id="user-content-mac" aria-label="Permalink: Mac" href="#mac"></a></p>

<p dir="auto">This repo uses rye to manage the combined Rust/Python monorepo build. First, make sure to run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install Rye from https://rye-up.com, and setup the virtualenv
rye sync"><pre><span><span>#</span> Install Rye from https://rye-up.com, and setup the virtualenv</span>
rye sync</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Licensed under the Apache License, Version 2.0 (the "License").</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Governance</h2><a id="user-content-governance" aria-label="Permalink: Governance" href="#governance"></a></p>
<p dir="auto">Vortex is and will remain an open-source project. Our intent is to model its governance structure after the
<a href="https://substrait.io/governance/" rel="nofollow">Substrait project</a>, which in turn is based on the model of the Apache Software Foundation.
Expect more details on this in Q4 2024.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgments 🏆</h2><a id="user-content-acknowledgments-" aria-label="Permalink: Acknowledgments 🏆" href="#acknowledgments-"></a></p>
<p dir="auto">This project is inspired by and--in some cases--directly based upon the existing, excellent work of many researchers
and OSS developers.</p>
<p dir="auto">In particular, the following academic papers greatly influenced the development:</p>
<ul dir="auto">
<li>Maximilian Kuschewski, David Sauerwein, Adnan Alhomssi, and Viktor Leis.
2023. <a href="https://www.cs.cit.tum.de/fileadmin/w00cfj/dis/papers/btrblocks.pdf" rel="nofollow">BtrBlocks: Efficient Columnar Compression
for Data Lakes</a>. Proc. ACM Manag. Data 1,
2,
Article 118 (June 2023), 14 pages. <a href="https://doi.org/10.1145/3589263" rel="nofollow">https://doi.org/10.1145/3589263</a></li>
<li>Azim Afroozeh and Peter
Boncz. <a href="https://www.vldb.org/pvldb/vol16/p2132-afroozeh.pdf" rel="nofollow">The FastLanes Compression Layout: Decoding &gt;100 Billion Integers per Second with Scalar
Code</a>. PVLDB, 16(9): 2132 - 2144, 2023.</li>
<li>Peter Boncz, Thomas Neumann, and Viktor Leis. <a href="https://www.vldb.org/pvldb/vol13/p2649-boncz.pdf" rel="nofollow">FSST: Fast Random Access String
Compression</a>.
PVLDB, 13(11): 2649-2661, 2020.</li>
<li>Azim Afroozeh, Leonardo X. Kuffo, and Peter Boncz. 2023. <a href="https://ir.cwi.nl/pub/33334/33334.pdf" rel="nofollow">ALP: Adaptive Lossless floating-Point
Compression</a>. Proc. ACM
Manag. Data 1, 4 (SIGMOD), Article 230 (December 2023), 26 pages. <a href="https://doi.org/10.1145/3626717" rel="nofollow">https://doi.org/10.1145/3626717</a></li>
</ul>
<p dir="auto">Additionally, we benefited greatly from:</p>
<ul dir="auto">
<li>the existence, ideas, &amp; implementation of <a href="https://arrow.apache.org/" rel="nofollow">Apache Arrow</a>.</li>
<li>likewise for the excellent <a href="https://github.com/apache/datafusion">Apache DataFusion</a> project.</li>
<li>the <a href="https://github.com/jorgecarleitao/parquet2">parquet2</a> project by <a href="https://github.com/jorgecarleitao">Jorge Leitao</a>.</li>
<li>the public discussions around choices of compression codecs, as well as the C++ implementations thereof,
from <a href="https://github.com/duckdb/duckdb">duckdb</a>.</li>
<li>the <a href="https://github.com/facebookincubator/velox">Velox</a> and <a href="https://github.com/facebookincubator/nimble">Nimble</a> projects,
and discussions with their maintainers.</li>
</ul>
<p dir="auto">Thanks to all of the aforementioned for sharing their work and knowledge with the world! 🚀</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek: Advancing theorem proving in LLMs through large-scale synthetic data (116 pts)]]></title>
            <link>https://arxiv.org/abs/2405.14333</link>
            <guid>41838589</guid>
            <pubDate>Mon, 14 Oct 2024 15:44:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2405.14333">https://arxiv.org/abs/2405.14333</a>, See on <a href="https://news.ycombinator.com/item?id=41838589">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2405.14333">View PDF</a>
    <a href="https://arxiv.org/html/2405.14333v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Proof assistants like Lean have revolutionized mathematical proof verification, ensuring high accuracy and reliability. Although large language models (LLMs) show promise in mathematical reasoning, their advancement in formal theorem proving is hindered by a lack of training data. To address this issue, we introduce an approach to generate extensive Lean 4 proof data derived from high-school and undergraduate-level mathematical competition problems. This approach involves translating natural language problems into formal statements, filtering out low-quality statements, and generating proofs to create synthetic data. After fine-tuning the DeepSeekMath 7B model on this synthetic dataset, which comprises 8 million formal statements with proofs, our model achieved whole-proof generation accuracies of 46.3% with 64 samples and 52% cumulatively on the Lean 4 miniF2F test, surpassing the baseline GPT-4 at 23.0% with 64 samples and a tree search reinforcement learning method at 41.0%. Additionally, our model successfully proved 5 out of 148 problems in the Lean 4 Formalized International Mathematical Olympiad (FIMO) benchmark, while GPT-4 failed to prove any. These results demonstrate the potential of leveraging large-scale synthetic data to enhance theorem-proving capabilities in LLMs. Both the synthetic dataset and the model will be made available to facilitate further research in this promising field.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Huajian Xin [<a href="https://arxiv.org/show-email/a0e31718/2405.14333">view email</a>]      <br>    <strong>[v1]</strong>
        Thu, 23 May 2024 09:03:42 UTC (4,425 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NASA's Europa Clipper Launch (180 pts)]]></title>
            <link>https://www.youtube.com/watch?v=lQToTWKwtuw</link>
            <guid>41838420</guid>
            <pubDate>Mon, 14 Oct 2024 15:28:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=lQToTWKwtuw">https://www.youtube.com/watch?v=lQToTWKwtuw</a>, See on <a href="https://news.ycombinator.com/item?id=41838420">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Busy Status Bar from Flipper Devices (803 pts)]]></title>
            <link>https://busy.bar/?hn</link>
            <guid>41838337</guid>
            <pubDate>Mon, 14 Oct 2024 15:19:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://busy.bar/?hn">https://busy.bar/?hn</a>, See on <a href="https://news.ycombinator.com/item?id=41838337">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-artboard-recid="784896582" data-artboard-screens="320,420,1200,1440,1720,1920" data-artboard-height="1111" data-artboard-valign="center" data-artboard-upscale="grid" data-artboard-height-res-320="2140" data-artboard-height-res-420="2490" data-artboard-height_vh-res-420="100" data-artboard-upscale-res-420="window" data-artboard-height-res-1200="831" data-artboard-height-res-1440="871" data-artboard-height-res-1720="1091" id="rec784896582" data-animationappear="off" data-record-type="396">             <div data-elem-id="1719228202659" data-elem-type="image" data-field-top-value="265" data-field-left-value="668" data-field-width-value="1127" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-filewidth-value="3000" data-field-fileheight-value="1066" data-field-top-res-320-value="454" data-field-left-res-320-value="-163" data-field-width-res-320-value="646" data-field-top-res-420-value="515" data-field-left-res-420-value="-201" data-field-width-res-420-value="802" data-field-top-res-1200-value="213" data-field-left-res-1200-value="413" data-field-width-res-1200-value="726" data-field-top-res-1440-value="223" data-field-left-res-1440-value="493" data-field-width-res-1440-value="881" data-field-top-res-1720-value="289" data-field-left-res-1720-value="590" data-field-width-res-1720-value="1016"> <p><img data-original="img/tild6163-3961-4233-a438-663566386561__bv_orange_3_1.png" alt="" imgfield="tn_img_1719228202659" src="https://busy.bar/img/tild6163-3961-4233-a438-663566386561__bv_orange_3_1.png"> </p> </div> <div data-elem-id="1722857618734" data-elem-type="text" data-field-top-value="905" data-field-left-value="372" data-field-width-value="198" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1692" data-field-top-res-420-value="1682" data-field-left-res-420-value="-612" data-field-width-res-420-value="331" data-field-top-res-1200-value="655" data-field-left-res-1200-value="197" data-field-width-res-1200-value="152" data-field-top-res-1440-value="740" data-field-left-res-1440-value="249" data-field-width-res-1440-value="159" data-field-top-res-1720-value="864" data-field-left-res-1720-value="333" data-field-width-res-1720-value="178"> <p>Cloud-based Python/JavaScript/Go apps</p> </div>     <div data-elem-id="1722514507053" data-elem-type="text" data-field-top-value="836" data-field-left-value="1174" data-field-width-value="235" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1899" data-field-left-res-320-value="32" data-field-width-res-320-value="123" data-field-top-res-420-value="2192" data-field-left-res-420-value="38" data-field-width-res-420-value="157" data-field-top-res-1200-value="616" data-field-left-res-1200-value="741" data-field-width-res-1200-value="183" data-field-top-res-1440-value="680" data-field-left-res-1440-value="892" data-field-width-res-1440-value="189" data-field-top-res-1720-value="802" data-field-left-res-1720-value="1051" data-field-width-res-1720-value="211"> <p>&gt; Free JavaScript apps SDK</p> </div> <p data-elem-id="1722857660392" data-elem-type="text" data-field-top-value="798" data-field-left-value="665" data-field-width-value="331" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1601" data-field-left-res-320-value="34" data-field-width-res-320-value="251" data-field-top-res-420-value="1851" data-field-left-res-420-value="40" data-field-width-res-420-value="303" data-field-top-res-1200-value="578" data-field-left-res-1200-value="397" data-field-width-res-1200-value="271" data-field-top-res-1440-value="650" data-field-left-res-1440-value="483" data-field-width-res-1440-value="265" data-field-top-res-1720-value="766" data-field-left-res-1720-value="595" data-field-width-res-1720-value="339"> <h3 field="tn_text_1722857660392">Time management technique based on short intervals of focused work broken by five-minute breaks.</h3> </p> <div data-elem-id="1722514534198" data-elem-type="text" data-field-top-value="862" data-field-left-value="1174" data-field-width-value="253" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1937" data-field-left-res-320-value="32" data-field-width-res-320-value="109" data-field-top-res-420-value="2239" data-field-left-res-420-value="38" data-field-width-res-420-value="137" data-field-top-res-1200-value="636" data-field-left-res-1200-value="741" data-field-width-res-1200-value="209" data-field-top-res-1440-value="702" data-field-left-res-1440-value="892" data-field-width-res-1440-value="230" data-field-top-res-1720-value="826" data-field-left-res-1720-value="1051" data-field-width-res-1720-value="243"> <p>&gt; Libs for Python/JavaScript/Go</p> </div>  <div data-elem-id="1722857675736" data-elem-type="text" data-field-top-value="905" data-field-left-value="859" data-field-width-value="197" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1704" data-field-left-res-320-value="43" data-field-width-res-320-value="245" data-field-top-res-420-value="1974" data-field-left-res-420-value="53" data-field-width-res-420-value="348" data-field-top-res-1200-value="655" data-field-left-res-1200-value="526" data-field-width-res-1200-value="130" data-field-top-res-1440-value="740" data-field-left-res-1440-value="639" data-field-width-res-1440-value="158" data-field-top-res-1720-value="864" data-field-left-res-1720-value="769" data-field-width-res-1720-value="177"> <p>Integration with hourly payment time trackers</p> </div> <div data-elem-id="1722514566656" data-elem-type="text" data-field-top-value="914" data-field-left-value="1174" data-field-width-value="261" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="2013" data-field-left-res-320-value="32" data-field-width-res-320-value="125" data-field-top-res-420-value="2333" data-field-left-res-420-value="38" data-field-width-res-420-value="146" data-field-top-res-1200-value="676" data-field-left-res-1200-value="741" data-field-width-res-1200-value="211" data-field-top-res-1440-value="746" data-field-left-res-1440-value="892" data-field-width-res-1440-value="210" data-field-top-res-1720-value="872" data-field-left-res-1720-value="1051" data-field-width-res-1720-value="252"> <p>&gt; Self-hosted cloud provisioning</p> </div>  <div data-elem-id="1722857692064" data-elem-type="text" data-field-top-value="905" data-field-left-value="681" data-field-width-value="150" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1678" data-field-left-res-320-value="43" data-field-width-res-320-value="260" data-field-top-res-420-value="1939" data-field-left-res-420-value="53" data-field-width-res-420-value="348" data-field-top-res-1200-value="655" data-field-left-res-1200-value="410" data-field-width-res-1200-value="100" data-field-top-res-1440-value="740" data-field-left-res-1440-value="497" data-field-width-res-1440-value="121" data-field-top-res-1720-value="864" data-field-left-res-1720-value="609" data-field-width-res-1720-value="134"> <p>Configure your own focus intervals</p> </div>  <div data-elem-id="1722514579043" data-elem-type="text" data-field-top-value="809" data-field-left-value="1461" data-field-width-value="222" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1860" data-field-left-res-320-value="168" data-field-width-res-320-value="114" data-field-top-res-420-value="2161" data-field-left-res-420-value="220" data-field-width-res-420-value="153" data-field-top-res-1200-value="595" data-field-left-res-1200-value="961" data-field-width-res-1200-value="207" data-field-top-res-1440-value="657" data-field-left-res-1440-value="1122" data-field-width-res-1440-value="178" data-field-top-res-1720-value="778" data-field-left-res-1720-value="1308" data-field-width-res-1720-value="199"> <p>&gt; Serial COM port over USB</p> </div>  <div data-elem-id="1722514579041" data-elem-type="text" data-field-top-value="835" data-field-left-value="1461" data-field-width-value="338" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1898" data-field-left-res-320-value="168" data-field-width-res-320-value="130" data-field-top-res-420-value="2208" data-field-left-res-420-value="220" data-field-width-res-420-value="174" data-field-top-res-1200-value="615" data-field-left-res-1200-value="961" data-field-width-res-1200-value="233" data-field-top-res-1440-value="679" data-field-left-res-1440-value="1122" data-field-width-res-1440-value="271" data-field-top-res-1720-value="802" data-field-left-res-1720-value="1308" data-field-width-res-1720-value="330"> <p>&gt; IoT integrations: IFTTT, HomeAssistant</p> </div>  <div data-elem-id="1722514579035" data-elem-type="text" data-field-top-value="887" data-field-left-value="1461" data-field-width-value="364" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1960" data-field-left-res-320-value="168" data-field-width-res-320-value="123" data-field-top-res-420-value="2285" data-field-left-res-420-value="220" data-field-width-res-420-value="146" data-field-top-res-1200-value="667" data-field-left-res-1200-value="961" data-field-width-res-1200-value="189" data-field-top-res-1440-value="723" data-field-left-res-1440-value="1122" data-field-width-res-1440-value="292" data-field-top-res-1720-value="848" data-field-left-res-1720-value="1308" data-field-width-res-1720-value="326"> <p>&gt; Offline API (no internet required)</p> </div>    <p data-elem-id="1722857591333" data-elem-type="text" data-field-top-value="798" data-field-left-value="152" data-field-width-value="374" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1362" data-field-left-res-320-value="33" data-field-width-res-320-value="246" data-field-top-res-420-value="1585" data-field-left-res-420-value="39" data-field-width-res-420-value="329" data-field-top-res-1200-value="578" data-field-left-res-1200-value="63" data-field-width-res-1200-value="268" data-field-top-res-1440-value="650" data-field-left-res-1440-value="72" data-field-width-res-1440-value="300" data-field-top-res-1720-value="766" data-field-left-res-1720-value="136" data-field-width-res-1720-value="360"> <h3 field="tn_text_1722857591333">Built-in apps: clock, weather, social media metrics, currency chart, pixel art wallpapers, and more.</h3> </p> <div data-elem-id="1722857608514" data-elem-type="text" data-field-top-value="905" data-field-left-value="171" data-field-width-value="164" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1441" data-field-left-res-320-value="44" data-field-width-res-320-value="260" data-field-top-res-420-value="1674" data-field-left-res-420-value="53" data-field-width-res-420-value="348" data-field-top-res-1200-value="655" data-field-left-res-1200-value="75" data-field-width-res-1200-value="113" data-field-top-res-1440-value="740" data-field-left-res-1440-value="87" data-field-width-res-1440-value="141" data-field-top-res-1720-value="864" data-field-left-res-1720-value="147" data-field-width-res-1720-value="146"> <p>Install JavaScript apps from community</p> </div>     <div data-elem-id="1722872865168" data-elem-type="text" data-field-top-value="1038" data-field-left-value="-100" data-field-width-value="160" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1465" data-field-left-res-320-value="43" data-field-width-res-320-value="247" data-field-top-res-420-value="1707" data-field-left-res-420-value="53" data-field-width-res-420-value="331" data-field-top-res-1440-value="888" data-field-left-res-1440-value="229" data-field-width-res-1440-value="159" data-field-top-res-1720-value="982" data-field-left-res-1720-value="-89" data-field-width-res-1720-value="143"> <p>Remote hosted Python/Javascript/Go apps via API</p> </div> <p data-elem-id="1722514020064" data-elem-type="text" data-field-top-value="386" data-field-left-value="146" data-field-width-value="398" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1013" data-field-left-res-320-value="31" data-field-width-res-320-value="260" data-field-top-res-420-value="1208" data-field-left-res-420-value="37" data-field-width-res-420-value="296" data-field-top-res-1200-value="266" data-field-left-res-1200-value="59" data-field-width-res-1200-value="259" data-field-top-res-1440-value="314" data-field-left-res-1440-value="68" data-field-width-res-1440-value="326" data-field-top-res-1720-value="372" data-field-left-res-1720-value="130" data-field-width-res-1720-value="326"> <h3 field="tn_text_1722514020064">Customizable busy status message to match your own workflow.</h3> </p> <div data-elem-id="1722514063649" data-elem-type="text" data-field-top-value="462" data-field-left-value="164" data-field-width-value="453" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1072" data-field-left-res-320-value="41" data-field-width-res-320-value="215" data-field-top-res-420-value="1271" data-field-left-res-420-value="50" data-field-width-res-420-value="288" data-field-top-res-1200-value="344" data-field-left-res-1200-value="71" data-field-width-res-1200-value="278" data-field-top-res-1440-value="374" data-field-left-res-1440-value="82" data-field-width-res-1440-value="364" data-field-top-res-1720-value="440" data-field-left-res-1720-value="147" data-field-width-res-1720-value="342"> <p>Set any busy message, expiry timer and activation trigger</p> </div> <div data-elem-id="1722857497787" data-elem-type="text" data-field-top-value="500" data-field-left-value="164" data-field-width-value="453" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1116" data-field-left-res-320-value="41" data-field-width-res-320-value="253" data-field-top-res-420-value="1326" data-field-left-res-420-value="50" data-field-width-res-420-value="338" data-field-top-res-1200-value="385" data-field-left-res-1200-value="71" data-field-width-res-1200-value="300" data-field-top-res-1440-value="404" data-field-left-res-1440-value="82" data-field-width-res-1440-value="364" data-field-top-res-1720-value="501" data-field-left-res-1720-value="147" data-field-width-res-1720-value="405"> <p>Upload custom busy graphics or choose from gallery</p> </div> <div data-elem-id="1722857506371" data-elem-type="text" data-field-top-value="535" data-field-left-value="164" data-field-width-value="371" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1160" data-field-left-res-320-value="41" data-field-width-res-320-value="232" data-field-top-res-420-value="1361" data-field-left-res-420-value="50" data-field-width-res-420-value="310" data-field-top-res-1200-value="408" data-field-left-res-1200-value="71" data-field-width-res-1200-value="245" data-field-top-res-1440-value="433" data-field-left-res-1440-value="82" data-field-width-res-1440-value="298" data-field-top-res-1720-value="534" data-field-left-res-1720-value="147" data-field-width-res-1720-value="333"> <p>Activate manually from device or remotely from PC, Mobile App or via API</p> </div> <div data-elem-id="1722857515571" data-elem-type="text" data-field-top-value="594" data-field-left-value="164" data-field-width-value="371" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1204" data-field-left-res-320-value="41" data-field-width-res-320-value="220" data-field-top-res-420-value="1417" data-field-left-res-420-value="50" data-field-width-res-420-value="295" data-field-top-res-1200-value="450" data-field-left-res-1200-value="71" data-field-width-res-1200-value="245" data-field-top-res-1440-value="483" data-field-left-res-1440-value="82" data-field-width-res-1440-value="298" data-field-top-res-1720-value="587" data-field-left-res-1720-value="147" data-field-width-res-1720-value="333"> <p>Automatic activation by Zoom, Discord, Microsoft Teams, Google Calendar</p> </div>      </div><div data-artboard-recid="796388741" data-artboard-screens="320,420,1200,1440,1720,1920" data-artboard-height="1010" data-artboard-valign="center" data-artboard-upscale="grid" data-artboard-height-res-320="1080" data-artboard-height-res-420="1250" data-artboard-height_vh-res-420="100" data-artboard-upscale-res-420="window" data-artboard-height-res-1200="740" data-artboard-height-res-1440="790" id="rec796388741" data-animationappear="off" data-record-type="396">    <div data-elem-id="1723051146284" data-elem-type="image" data-field-top-value="162" data-field-left-value="2238" data-field-width-value="200" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-filewidth-value="1600" data-field-fileheight-value="1107" data-field-top-res-320-value="199" data-field-left-res-320-value="-85" data-field-width-res-320-value="500" data-field-top-res-420-value="242" data-field-left-res-420-value="-90" data-field-width-res-420-value="640"> <p><img data-original="img/tild3262-3138-4461-a632-346233363732__mobile_headset.jpg" alt="" imgfield="tn_img_1723051146284" src="https://busy.bar/img/tild3262-3138-4461-a632-346233363732__mobile_headset.jpg"> </p> </div>  <p data-elem-id="1723037385419" data-elem-type="text" data-field-top-value="208" data-field-left-value="111" data-field-width-value="713" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-animate-prx="scroll" data-field-top-res-320-value="60" data-field-left-res-320-value="25" data-field-top-res-420-value="80" data-field-left-res-420-value="71" data-field-width-res-420-value="270" data-field-axisy-res-420-value="top" data-field-top-res-1200-value="153" data-field-left-res-1200-value="40" data-field-top-res-1440-value="160" data-field-left-res-1440-value="41" data-field-width-res-1440-value="641"> <h2 field="tn_text_1723037385419">Live Busy status</h2> </p>  <p data-elem-id="1723037385412" data-elem-type="text" data-field-top-value="306" data-field-left-value="111" data-field-width-value="622" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="546" data-field-left-res-320-value="15" data-field-width-res-320-value="288" data-field-top-res-420-value="656" data-field-left-res-420-value="15" data-field-width-res-420-value="383" data-field-top-res-1200-value="225" data-field-left-res-1200-value="40" data-field-width-res-1200-value="468" data-field-top-res-1440-value="241" data-field-left-res-1440-value="42" data-field-width-res-1440-value="524"> <h3 field="tn_text_1723037385412">Busy Status Bar can integrate with desktop software and automatically activate when you’re on a call, live on stream, recording audio or when a certain program is active.</h3> </p> <div data-elem-id="1723037476748" data-elem-type="text" data-field-top-value="488" data-field-left-value="191" data-field-width-value="361" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="713" data-field-left-res-320-value="68" data-field-width-res-320-value="263" data-field-top-res-420-value="855" data-field-left-res-420-value="80" data-field-width-res-420-value="352" data-field-top-res-1200-value="345" data-field-left-res-1200-value="92" data-field-width-res-1200-value="278" data-field-top-res-1440-value="374" data-field-left-res-1440-value="102" data-field-width-res-1440-value="356"> <p>Automatic “On Call” status</p> </div>  <div data-elem-id="1723037523349" data-elem-type="text" data-field-top-value="530" data-field-left-value="191" data-field-width-value="375" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="746" data-field-left-res-320-value="68" data-field-width-res-320-value="239" data-field-top-res-420-value="891" data-field-left-res-420-value="80" data-field-width-res-420-value="303" data-field-top-res-1200-value="377" data-field-left-res-1200-value="92" data-field-width-res-1200-value="336" data-field-top-res-1440-value="409" data-field-left-res-1440-value="102" data-field-width-res-1440-value="305"> <p>When the microphone is activated on the computer, the device will automatically display an “on call” status.</p> </div> <div data-elem-id="1723037779285" data-elem-type="text" data-field-top-value="656" data-field-left-value="191" data-field-width-value="451" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="865" data-field-left-res-320-value="68" data-field-width-res-320-value="244" data-field-top-res-420-value="1018" data-field-left-res-420-value="80" data-field-width-res-420-value="299" data-field-top-res-1200-value="471" data-field-left-res-1200-value="92" data-field-width-res-1200-value="375" data-field-top-res-1440-value="507" data-field-left-res-1440-value="102" data-field-width-res-1440-value="370"> <p>When you are streaming through any software like OBS (Open Broadcaster Software), Busy Status Bar will automatically turn on.</p> </div>    <div data-elem-id="1723037822598" data-elem-type="text" data-field-top-value="777" data-field-left-value="191" data-field-width-value="368" data-field-axisy-value="top" data-field-axisx-value="left" data-field-container-value="grid" data-field-topunits-value="px" data-field-leftunits-value="px" data-field-heightunits-value="" data-field-widthunits-value="px" data-field-top-res-320-value="1005" data-field-left-res-320-value="68" data-field-width-res-320-value="263" data-field-top-res-420-value="1148" data-field-left-res-420-value="80" data-field-width-res-420-value="352" data-field-top-res-1200-value="567" data-field-left-res-1200-value="92" data-field-width-res-1200-value="278" data-field-top-res-1440-value="609" data-field-left-res-1440-value="102" data-field-width-res-1440-value="356"> <p>Supports Windows / macOS / Linux.</p> </div>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scientists successfully breed corals to improve their heat tolerance (132 pts)]]></title>
            <link>https://phys.org/news/2024-10-scientists-successfully-corals-tolerance.html</link>
            <guid>41837925</guid>
            <pubDate>Mon, 14 Oct 2024 14:34:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phys.org/news/2024-10-scientists-successfully-corals-tolerance.html">https://phys.org/news/2024-10-scientists-successfully-corals-tolerance.html</a>, See on <a href="https://news.ycombinator.com/item?id=41837925">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
										
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2024/scientists-have-succes.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2024/scientists-have-succes.jpg" data-sub-html="Credit: Dr. James Guest">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2024/scientists-have-succes.jpg" alt="Scientists have successfully bred corals to improve their heat tolerance" title="Credit: Dr. James Guest" width="800" height="530">
             <figcaption>
                Credit: Dr. James Guest
            </figcaption>        </figure>
    </div><p>A new study has shown that selective breeding can lead to a modest rise in coral heat tolerance.</p>


										      
																																	<p>Led by experts at Newcastle University's Coralassist Lab, the study documents the world's first effort to selectively breed adult corals for enhanced <a href="https://phys.org/tags/heat+tolerance/" rel="tag">heat tolerance</a>, i.e. the ability of adult corals to survive intense marine heat waves. The breeding effort was a success, showing that it is possible to improve the heat tolerance of adult coral offspring, even in a single generation.</p>
<p>However, the improvement was modest in comparison to future marine heat waves expected under climate change. The authors stress that rapid reductions of global greenhouse gas emissions are an absolute requirement to mitigate warming and give corals an opportunity to adapt.</p>
<p>The study, published in the journal <i>Nature Communications</i>, was carried out in partnership with the University of Victoria, Horniman Museum and Gardens, Palau International Coral Reef Center, University of Derby, and the University of Exeter.</p>
<p>The publication is the result of a five-year project which was launched by Dr. James Guest.</p>
<h2>Not a silver bullet solution</h2>
<p>"This work shows that <a href="https://phys.org/tags/selective+breeding/" rel="tag">selective breeding</a> is feasible but not a silver bullet solution and that more research is needed to maximize breeding outcomes," says study lead author, Liam Lachs, a Postdoctoral Research Associate at Newcastle University. He continues, reflecting that "in parallel, rapid reductions of global greenhouse gas emissions are an absolute requirement to mitigate warming and give corals an opportunity to adapt.</p>
<p>Dr. Guest, Reader in Coral Reef Ecology at Newcastle University's School of Natural and Environmental Sciences, explains that "the results show that selective breeding could be a viable tool to improve population resilience. Yet, there are still many challenges that need to be overcome.</p>
<p>"How many corals need to outplanted to benefit wild populations? Can we ensure there are no trade-offs (evidence so far suggests this is not a large risk)? How can we avoid dilution of selected traits once added to the wild? How can we maximize responses to selection?</p>
<p>"Given the moderate levels of enhancement we achieved in this study, the effectiveness of such interventions will also depend on urgent climate action."</p>

																																						
																																			<h2>Successful breeding trial</h2>
<p>Selective breeding has been practiced by humans for thousands of years to produce animals and plants with desirable characteristics. Now it is being considered as a tool for nature conservation, particularly for coral reefs.</p>
<p>These <a href="https://phys.org/tags/marine+ecosystems/" rel="tag">marine ecosystems</a> are at the forefront of climate change impacts, as reef-building corals are highly sensitive to marine heat waves. These can trigger mass coral bleaching and mortality events which have already led to considerable reef declines globally.</p>
<p>The experts conducted selective breeding trials for two different traits, either the tolerance to a short intense heat exposure (10 days, reaching +3.5°C) or a less-intense but long-term exposure more typical of natural marine heat waves (1 month, reaching +2.5°C).</p>
<p>The team found that selecting parent colonies for high rather than low heat tolerance increased the tolerance of adult offspring. This result held for the response to both 10-day and one-month exposures. Heat tolerance could in theory be enhanced by approximately 1°C/week within one generation. However, this level of enhancement is likely insufficient to keep pace with unabated warming.</p>
<h2>What's next?</h2>
<p>Selectively breeding for short-stress tolerance did not show evidence of enhancing the ability of offspring to survive the long heat stress exposure. With no genetic correlation detected, it is plausible that these traits are under independent genetic controls.</p>
<p>This would have important implications, as interventions would benefit from cheap and rapid assays that can effectively identify heat tolerant colonies for breeding. However, if these assays do not predict adult colony survival to natural marine heat waves, it presents a serious challenge for management interventions.</p>
<p>Study lead author, Dr. Adriana Humanes, Postdoctoral Research Associate at the Coralassist Lab, Newcastle University, highlights that "considerable work remains before selective breeding can be successfully implemented. A deeper understanding is needed to determine which traits to prioritize and how these traits are genetically correlated."</p>
<h2>Take home message</h2>
<p>The authors say that this work is an important proof of concept: selective breeding corals for adult heat wave survival is possible. Now, they call for more research and development to understand how to operationalize breeding interventions and maximize outcomes to hopefully keep pace with the lower levels of warming that can be achieved with concurrent climate action.</p>

																																																					
																				<div>
																						<p><strong>More information:</strong>
												Selective breeding enhances coral heat tolerance to marine heatwaves, <i>Nature Communications</i> (2024). <a data-doi="1" href="https://dx.doi.org/10.1038/s41467-024-52895-1" target="_blank">DOI: 10.1038/s41467-024-52895-1</a>
																						
																						</p>
																					</div>
                               											
																					
                              										                                        
										<!-- print only -->
										<div>
											 <p><strong>Citation</strong>:
												Scientists successfully breed corals to improve their heat tolerance (2024, October 14)
												retrieved 14 October 2024
												from https://phys.org/news/2024-10-scientists-successfully-corals-tolerance.html
											 </p>
											 <p>
											 This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
											 part may be reproduced without the written permission. The content is provided for information purposes only.
											 </p>
										</div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Stallman Report (127 pts)]]></title>
            <link>https://stallman-report.org/</link>
            <guid>41837782</guid>
            <pubDate>Mon, 14 Oct 2024 14:19:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stallman-report.org/">https://stallman-report.org/</a>, See on <a href="https://news.ycombinator.com/item?id=41837782">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<h3>October 14th, 2024</h3>
<p>Richard Stallman (aka “RMS”) is the founder of <a href="https://www.gnu.org/">GNU</a> and the
<a href="https://www.fsf.org/">Free Software Foundation</a> and present-day voting member of the Free Software
Foundation (FSF) board of directors and “Chief GNUisance” of the GNU project. He
is responsible for innumerable contributions to the free software movement,
setting its guiding principles, organizing political action, and directly
contributing to a flourishing free software ecosystem. The majority of
Stallman’s political activity has been of priceless value to society at large.</p>
<p>However, Stallman has been the subject of numerous allegations of misconduct.
Stallman has also incited numerous controversies for advancing a political
agenda which normalizes sexual misconduct and advocates for reforming our social
and legal understanding of sexual conduct in a manner which benefits the
perpetrators of abuse.</p>
<p>On the basis that Stallman has not demonstrated an understanding of his
misconduct; has not apologized for allegations of misconduct, alleged or
corroborated; continues to publish his harmful political program; and does not
acknowledge or apologize for harm done in the course of this program, this
report reiterates the position that Stallman should be removed from the board of
directors at the Free Software Foundation.</p>
<p>To support this case, we have catalogued the following:</p>
<ol>
<li>Primary sources documenting Stallman’s political advocacy for:
<ul>
<li>The normalization of sexual relations between adults and minors
<a href="#topicref-1">[1]</a>
</li>
<li>Defense of individuals both accused and convicted of sexual crimes,
including the rape of minors, sexual assault, and sexual harassment
<a href="#topicref-2">[2]</a>
</li>
<li>Dismissal of legal norms regarding sexual assault
<a href="#topicref-3">[3]</a>
</li>
<li>Dismissal of legal norms regarding sexual harassment
<a href="#topicref-4">[4]</a>
</li>
<li>Support for the possession of child sexual abuse material
<a href="#topicref-5">[5]</a>
</li>
<li>Legal and social normalization of sex between humans and animals
<a href="#topicref-6">[6]</a>
</li>
<li>Legal and social normalization of sex with corpses (necrophilia)
<a href="#topicref-7">[7]</a>
</li>
</ul>
</li>
<li>Credible allegations of sexual misconduct regarding Stallman
<a href="#topicref-8">[8]</a>
</li>
<li>Misconduct of the Free Software Foundation board of directors
<a href="#topicref-9">[9]</a>
</li>
<li>Calls from the free software community for Stallman’s removal
<a href="#topicref-10">[10]</a>
</li>
<li>Recommendations for reconciliation and closure
<a href="#topicref-11">[11]</a>
</li>
</ol>
<div>
	<p><strong>Content warning</strong>: This report catalogues and directly quotes hundreds of
statements from Richard Stallman of an extremely offensive nature on subjects
including rape, sexual assault, child sexual abuse, sexual exploitation of
children, and more. Sensitive readers are <strong>strongly advised</strong> to proceed with
caution.</p>
<p>If you or someone you know has been the victim of sexual violence, help is
available. The United States National Sexual Assault Hotline can be reached at
800-656-HOPE (4673), and is available for <a href="https://hotline.rainn.org/online">live chat
online</a> 24/7. You can speak to a trained
expert for confidential support at any time.</p>
<p>International readers are directed to
<a href="https://www.hotpeachpages.net/">HotPeachPages</a> for resources in your location
and your language, and <a href="https://childhelplineinternational.org/helplines/">Child Helpline International</a>
provides international resources specifically aimed at the needs of children and
young people.</p>

</div>

<h2 id="statement-regarding-stallmans-medical-situation">
	Statement regarding Stallman’s medical situation
	
	<a href="#statement-regarding-stallmans-medical-situation"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h2>
<p>We understand that Richard Stallman was diagnosed with follicular
lymphoma in 2023.<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> We are pleased to hear that his prognosis is good and
his cancer is in remission. We wish Stallman good health, a peaceful recovery,
and many more years of health.</p>
<p>We are not of the opinion that Stallman’s cancer diagnosis absolves him of
responsibility for his actions, past and present. We urge Stallman to reconsider
his controversial political positions and issue retractions and/or apologies to
the extent that his health permits him to do so, and draw attention to the fact
that Stallman continues to forward his controversial views following his
diagnosis. We also urge the free software community to hold Stallman accountable
for his actions and to contend with our history of sexism and tolerating abuse.</p>
<h2 id="why-publish-this-report">
	Why publish this report?
	
	<a href="#why-publish-this-report"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h2>
<p>Richard Stallman has a profound influence on the free software community and our
movement. He is responsible for defining the four freedoms that steer us, he has
written all of our principal philosophy, he founded our foundational software
projects, and he is venerated as our ideological leader.</p>
<p>Richard Stallman has also embarked upon a decades-long political project to
normalize sexual violence. Under his ideological leadership, the free software
movement is unsafe, particularly for women. Women represent just 3% of the free
software community,<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> compared to 23% of industry
programmers generally.<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> This is no accident. There is a
pervasive culture of sexism and a stark lack of accountability in free software,
and it begins with Stallman’s unchallenged and reprehensible behavior.</p>
<p>The case against Stallman is clear, and yet the free software community has
failed to act, in particular at the level of institutions and leadership but
also in the form of grassroots support for Stallman. Many defenses of Stallman
rely on a comfortable ignorance: ignorance of the scope and depth of Stallman’s
political campaign against women and victims of sexual violence, or a
comfortable belief that Stallman ceased his problematic behavior following his
2021 re-instatement in the Free Software Foundation. Some believe that
Stallman’s speech has not caused material harm, or that his fringe views are not
taken seriously; we provide evidence to dismiss all of these arguments in this
report.</p>
<p>Ignorance of the case against Stallman is due in part to the scattered and
disorganized nature of information regarding Stallman’s misconduct. Many of
those who raise a defense of Stallman have heard one or two uncorroborated
allegations of misconduct or one or two examples of years-old problematic
quotes, and understandably find it easier to excuse it as such. Furthermore,
those most directly accountable for Stallman’s behavior are the members of the
Free Software Foundation board of directors, and their misconduct in handling
the case is not widely known; this report brings this misconduct to light. By
carefully organizing information about Stallman’s misconduct and the misconduct
of the FSF board of directors into a single, comprehensive and exhaustively
cited report, the appeal to ignorance is no longer applicable.</p>
<p>This report collects hundreds of primary sources from 2003 to 2024 which clearly
demonstrate Stallman’s harmful political program and misconduct, meticulously
cataloged, analyzed, and subject to factual rebuttals. If the free software
community cannot address the blatant misconduct of Richard Stallman in the face
of overwhelming evidence, the free software community is not safe, and cannot be
made safe. Our institutions and our community must act. We have made several
recommendations for such actions at the end of the report.</p>
<p>First, we will justify our unqualified condemnation of Richard Stallman.</p>
<h2 id="stallmans-political-statements">
	Stallman’s political statements
	
	<a href="#stallmans-political-statements"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h2>
<p>Richard Stallman maintains a collection of political notes on his
website.<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup> He frequently publishes short political opinions on
his website here on a wide variety of topics. In this report we draw attention
to his political program on sex, drawing from his political notes as a primary
source.</p>
<p>Note that all quotes sourced from Stallman’s website for this report are direct
quotes of material publicly available at the time this report was prepared in
September 2024.<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup> You may click any citation to
view it on Stallman’s website.</p>

<p>We have catalogued comments published by Stallman, mostly in the political notes
section of his personal website, and categorized comments of interest. Each link
leads to a page which provides a complete list of comments applicable to each
category. Some comments have been reproduced in multiple categories.</p>
<table>
  <thead>
    <tr>
      <th>Category</th>
      <th>Dates applicable</th>
      <th colspan="3">Occurrences</th>
    </tr>
    <tr>
      <th></th>
      <th></th>
      <th>Total</th>
      <th colspan="2">Retracted</th>
    </tr>
    <tr>
      <th></th>
      <th></th>
      <th></th>
      <th>Yes</th>
      <th>No</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://stallman-report.org/on-children">Distinction between "children" and other minors</a></td>
      <td>2003-2024</td>
      <td>124</td>
      <td colspan="2">n/a</td>
    </tr>
    <tr>
      <td><a href="https://stallman-report.org/on-child-pornography">Support of child sexual abuse material</a></td>
      <td>2003-2019</td>
      <td>55</td>
      <td>0</td>
      <td>55</td>
    </tr>
    <tr>
      <td><a href="https://stallman-report.org/defense-of-sexual-misconduct">Defense of sexual misconduct</a></td>
      <td>2006-2023</td>
      <td>37</td>
      <td>1</td>
      <td>36</td>
    </tr>
    <tr>
      <td><a href="https://stallman-report.org/on-sex-with-minors">Support of sex between adults and minors</a></td>
      <td>2006-2019</td>
      <td>34</td>
      <td>5</td>
      <td>29</td>
    </tr>
    <tr>
      <td><a href="https://stallman-report.org/on-sexual-assault">Misrepresentation of sexual assault</a></td>
      <td>2015-2024</td>
      <td>24</td>
      <td>1</td>
      <td>23</td>
    </tr>
    <tr>
      <td><a href="https://stallman-report.org/on-sexual-harassment">Misrepresentation of sexual harassment</a></td>
      <td>2014-2018</td>
      <td>13</td>
      <td>0</td>
      <td>13</td>
    </tr>
    <tr>
      <td><a href="https://stallman-report.org/on-bestiality/">Support of bestiality</a></td>
      <td>2003-2018</td>
      <td>12</td>
      <td>0</td>
      <td>12</td>
    </tr>
    <tr>
      <td><a href="https://stallman-report.org/on-necrophilia">Support of necrophilia</a></td>
      <td>2003-2013</td>
      <td>3</td>
      <td>0</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

<h3 id="normalization-of-sexual-relations-between-adults-and-minors">
	Normalization of sexual relations between adults and minors
	
	<a href="#normalization-of-sexual-relations-between-adults-and-minors"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>Richard Stallman has consistently advanced the political position that minors
can consent to sex with adults. Stallman’s position on minors having sex with
adults is the only position addressed in this report for which Stallman has
issued a retraction:</p>
<blockquote>
<p>Many years ago I posted that I could not see anything wrong about sex between
an adult and a child, if the child accepted it.</p>
<p>Through personal conversations in recent years, I’ve learned to understand how
sex with a child can harm per<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup> psychologically. This changed my mind
about the matter: I think adults should not do that. I am grateful for the
conversations that enabled me to understand why.</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2019-jul-oct.html#14_September_2019_(Sex_between_an_adult_and_a_child_is_wrong)">stallman.org, 14 September 2019 “Sex between an adult and a child is wrong”</a></p>
<p>However, as we will show, Stallman’s retraction is misleading and does not cover
the majority of his past statements on the subject. In short, we will show that
Stallman’s 2019 retraction only addresses his views regarding sex between adults
and pre-pubescent minors, and this retraction does not account for minors above
the age of 12 or 13.</p>
<p>Our report catalogues 34 political comments from Stallman making a political
case for sexual relationships between adults and minors. A strict reading of the
retraction applies it only to the singular political note that it references,
which Stallman has updated accordingly:</p>
<details>
	<summary>This quote is covered by Stallman's 2019 retraction. Click to show.</summary>
	<blockquote>
<p>Dutch pedophiles have formed a political party to campaign for legalization.</p>
<p>I am skeptical of the claim that voluntarily pedophilia harms children. The
arguments that it causes harm seem to be based on cases which aren’t
voluntary, which are then stretched by parents who are horrified by the idea
that their little baby is maturing.</p>
<p>[Many years after posting this note, I had conversations with people who had
been sexually abused as children and had suffered harmful effects. These
conversations eventually convinced me that the practice is harmful and adults
should not do it.]</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2006-mar-jun.html#05%20June%202006%20(Dutch%20paedophiles%20form%20political%20party)">stallman.org, 05 June 2006 “Dutch paedophiles form political party”</a></p>

</details>

<p>Stallman has not updated similar comments with the same retraction. However, in
good faith we have assumed that the retraction applies to any comments which
would apply to minors under the age of 12 or 13, or explicitly refers to
“children”. Accounting for this, our catalog of Stallman’s 34 comments in
support of sex between adults and minors indicates that four have been fully
retracted and one has been partially retracted.</p>
<p><a href="https://stallman-report.org/on-sex-with-minors">Appendix: Stallman on sexual relations between adults and minors</a></p>
<hr>
<p>We justify our interpretation by citing 124 primary sources in which Stallman
insists on a distinction between “children” and other minors, in particular
teenagers. Our sources are dated from 2003 to 2024; Stallman has emphasized that
teenagers are distinct from “children”, on average, once every 9 weeks since
2003. Stallman has made this distinction 42 times following his 2019 retraction,
an average of once every 6½ weeks since the retraction.</p>
<p><a href="https://stallman-report.org/on-children">Appendix: Stallman’s idiolectical use of “child”</a></p>
<p>To understand Stallman’s remarks on sexual relationships between adults and
minors, we must show how Stallman distinguishes between “children” and other
minors.</p>
<p>Stallman is particular about his use of language, and a reading of his political
notes must be paired with an understanding of his idiolect. Stallman uses
numerous unconventional definitions and terminology in his political notes and
does so with rigour and consistency. In order to assist the reader in
interpreting Stallman’s political notes, he provides two resources on his
website: a glossary<sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup> and an “anti-glossary”<sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup>.</p>
<p>Consider the following political note:</p>
<blockquote>
<p>The company Dataminr tries to scan all posted tweets to find anything
suggestive of possible violent intent, and report it to the thugs.</p>
</blockquote>
<p>– <a href="https://stallman.org/notes/2020-jul-oct.html#26_October_2020_(BIBO:_company_reports_tweets_to_thugs)">stallman.org, 16 October 2020, “BIBO: company reports tweets to thugs”</a></p>
<p>Out of context, the reader may be confused as to who the “thugs” are. The
glossary answers:</p>
<blockquote>
<p><strong>Thugs</strong>: the armed, usually uniformed marauders that attack protesters and
blacks, and make false accusations against them.</p>
<p>Usually only a few thugs commit the physical violence, but when one thug makes
a false accusation, the rest lie to support it. That’s why they deserve the
term “thugs” as a group. They are so habituated to perjury that they have
their own word for it: “testilying”.</p>
<p>A few members of thug departments are upright and refuse to support the
others’ lies. They are the honorable exceptions, and I express my respect for
them by calling them “police officers”.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/glossary.html#thug">stallman.org, “Glossary”</a></p>
<p>It follows that when Stallman says the word “thug”, he is referring to the
police, and indeed his use of the word “thug” is consistent with this in
thousands of his political notes. Many banal words and phrases are given a
similar treatment in his political notes and the two glossaries serve as a guide
for readers to interpret and understand his political notes as such.</p>
<p>Notably, the following definition also appears in Stallman’s anti-glossary:</p>
<blockquote>
<p><strong>Children</strong>: Humans up to age 12 or 13 are children. After that, they become
adolescents or teenagers. Let’s resist the practice of infantilizing
teenagers, by not calling them “children”.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/antiglossary.html#children">stallman.org, “Anti-Glossary”</a></p>
<p>That Stallman’s use of the word “child” is consistent with this idiolectical
definition and is re-enforced throughout Stallman’s political notes. He has
drawn a distinction between children and teenagers numerous times, which this
report catalogues in an appendix.</p>
<p><a href="https://stallman-report.org/on-children">Appendix: Stallman’s idiolectical use of “child”</a></p>
<p>Stallman has made this distinction most recently in June 2024:</p>
<blockquote>
<p>Please do not use the word “children” or “child” to refer to anyone under age
18. A 17-year-old is not a child. A 13-year-old is a teenager.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2024-mar-jun.html#16_June_2024_(Online_addictive-feeds_law,_NY)">stallman.org, 16 June 2024 “Online addictive-feeds law NY”</a></p>
<p>Stallman’s insistence on distinguishing children from other minors, in
particular teenagers, is often made with sexual overtures. For example, in
December 2023:</p>
<blockquote>
<p>The intended purpose of that law is to prevent minors from accessing porn
sites. To exclude everyone under 18 is unreasonably strict. They try to
justify this by referring to all minors as “children”. Even a person of age
17 is a “child” according to them.</p>
<p>To exclude only children – real children – from porn sites might be ok in
principle. But how to determine whether a given user is under the specified
age? The methods mentioned in the article either directly require a user to
identify perself, or indirectly require per to make perself vulnerable to
being identified.</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2023-sep-dec.html#6_December_2023_(UK_age_verification_for_porn_sites)">stallman.org, 6 December 2023 “UK age verification for porn sites”</a></p>
<p>Or in April 2018, explicitly making this distinction in support of sexual
activity between adults and minors (denying the experience of two rape victims
in the process, see <a href="#defense-of-sexual-misconduct">defense of sexual misconduct</a>):</p>
<blockquote>
<p>It sounds horrible: “UN peacekeepers accused of child rape in South Sudan.”
But the article makes it pretty clear that the “children” involved were not
children. They were teenagers.</p>
<p>What about “rape”? Was this really rape? Or did they have sex willingly, and
prudes want to call it “rape” to make it sound like an injustice? We can’t
tell from the article which one it is.</p>
<p>Rape means coercing someone to have sex. Precisely because that is a grave and
clear wrong, using the same name for something much less grave is a
distortion.</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2018-mar-jun.html#30_April_2018_(UN_peacekeepers_in_South_Sudan)">stallman.org, 30 April 2018 “UN peacekeepers in South Sudan”</a></p>
<p>This report considers Stallman’s idiolectical use of “child”, established in his
“anti-glossary” and re-enforced throughout his political notes, supports the
interpretation that his retraction only addresses sexual relationships between
adults and children up to the age of “12 or 13”, and that political notes which
remark upon sexual relationships between minors above the age of 12 or 13 are
not covered by his 2019 retraction.</p>
<hr>

<p>We now offer a rebuttal of Stallman’s political position regarding sexual
relations between adults and minors.</p>
<p><a href="https://stallman-report.org/on-sex-with-minors">Appendix: Stallman on sexual relations between adults and minors</a></p>
<p>Sexual relationships between adults and minors are prohibited by social and
legal norms because a differential of life and sexual experiences between adults
and minors enables adults to manipulate minors for the purpose of sexual
gratification. This bears out in statistics that highlight the risks sexual
relationships with adults impose on young girls in particular.</p>
<p>Older men often manipulate minors into unsafe sexual practices, leading to
undesirable outcomes for their victims. Young girls are often unprepared to
negotiate the use of contraception with an older partner, resulting in teenage
girls having unprotected sex at a rate that increases by 11% for each year older
their partner is. (Manlove, Ryan, Franzetta 2007)<sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup>
Minors who have sexual relationships with
partners 5 or more years older are 3.7 times more likely to experience an
unwanted pregnancy (Planned Parenthood, 2004; Darroch et al.,
1999)<sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup> <sup id="fnref:11"><a href="#fn:11" role="doc-noteref">11</a></sup> and twice as likely to acquire a
sexually transmitted infection (STI) than peers who have partners similar in age
(Ryan, Franzetta 2008).<sup id="fnref:12"><a href="#fn:12" role="doc-noteref">12</a></sup></p>
<p>Young women aged 15 to 17 who have had a relationship with a partner five years
or older than themselves have been forced to have sex at twice the rate of young
women who have only had similar-age relationships (Darroch 1999).<sup id="fnref1:11"><a href="#fn:11" role="doc-noteref">11</a></sup>
Minors who experience these rapes have poorer life outcomes than their peers;
women who are raped before the age of 18 are twice as likely to be raped in
adulthood (Tjaden, Thoennes 2000)<sup id="fnref:13"><a href="#fn:13" role="doc-noteref">13</a></sup> and experience
significantly higher incidences of domestic abuse, mental health problems, low
self-esteem, and long-term intimacy problems in adulthood (Flemming et al
1999).<sup id="fnref:14"><a href="#fn:14" role="doc-noteref">14</a></sup> Abuse involving sexual intercourse increases this risk by
a factor of two (Flemming et al 1999).<sup id="fnref1:14"><a href="#fn:14" role="doc-noteref">14</a></sup></p>
<hr>
<p><em>Note on the sexual abuse of young men:</em></p>
<p><em>This report does not deny the experiences of young men who are victims of
sexual abuse. However, young heterosexual women are at a much higher risk of
exploitation than young heterosexual men (about 5× higher). The academic
literature tends to focus on the experiences of heterosexual young women as a
result, creating a gender bias that is unfortunately reproduced in our report
due to a lack of reliable sources. However, it is noted by Manlove et al. that
young boys who have sex before the age of 16 with an older partner are more than
twice as likely to father a child as a teen than young boys with similar aged
partners.</em><sup id="fnref1:9"><a href="#fn:9" role="doc-noteref">9</a></sup></p>
<hr>
<p>Stallman often makes the claim that it is normal for adults to be sexually
attracted to minors. On one occasion he has likened condemnation of this
attraction to homosexual conversion therapy:</p>
<blockquote>
<p>Research found that men generally find females of age 18 the most attractive.</p>
<p>This accords with the view that Stendhal reported in France in the 1800s, that
a woman’s most beautiful years were from 16 to 20.</p>
<p>Although this attitude on men’s part is normal, the author still wants to
present it as wrong or perverted, and implicitly demands men somehow control
their attraction to direct it elsewhere. Which is as absurd, and as
potentially oppressive, as claiming that homosexuals should control their
attraction and direct it towards to the other sex. Will men be pressured to
undergo “age conversion therapy” intended to brainwash them to feel attracted
mainly to women of their own age?</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2018-jul-oct.html#21_August_2018_(Age_and_attraction)">stallman.org, 21 August 2018 “Age and attraction”</a></p>
<p>The presumption that adult attraction to minors is “normal” is difficult to
characterize. The prevalence of adults with an attraction to post-pubescent
minors is unknown. The prevalence of adults with an attraction to pre-pubescent
minors is better studied, but poorly estimated; estimates for men are generally
around 5% (Seto 2009).<sup id="fnref:15"><a href="#fn:15" role="doc-noteref">15</a></sup> However, the editors note that pedophilia is
understood as a psychological pathology by the medical literature and is noted
as such by its inclusion in the DSM-5 (American Psychiatric Association, 2013).<sup id="fnref:16"><a href="#fn:16" role="doc-noteref">16</a></sup></p>
<p>Additionally, it is factually incorrect to assume that sexual abuse of minors is
motivated by a sexual attraction to minors. Studies show that only about 50% of
sexual exploitation of minors is motivated by sexual attraction.</p>
<blockquote>
<p>Although this preference increases the risk of engaging in CSA, only about 50%
of all individuals who do sexually abuse children are pedophilic (Blanchard et
al., 2001; Schaefer et al., 2010) and not every pedophilic individual actually
has abused children. The other 50% of individuals that have abused children
are those who do so without a sexual attraction to children; i.e., they lack
the necessary social skills to develop and maintain emotional and sexual
relationships with appropriately aged peers and look to “replacement partners”
in children as a kind of “surrogate” (Beier, 1998; Seto, 2008; Mokros et al.,
2012b). (Tenbergen et al, 2015)<sup id="fnref:17"><a href="#fn:17" role="doc-noteref">17</a></sup></p>
</blockquote>

<h3 id="defense-of-sexual-misconduct">
	Defense of sexual misconduct
	
	<a href="#defense-of-sexual-misconduct"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>Stallman’s political notes frequently respond to news articles about sexual
crimes by downplaying the severity of the crime and advocating on behalf of the
offender. Stallman’s political notes consistently contribute to a broader
harmful discourse which silences the experiences of victims of sexual violence.</p>
<p>This report catalogues 37 examples of Stallman expressing a defense of
individuals accused of or convicted of sexual harassment, sexual assault, or
rape (statutory or otherwise). The report only considers occasions where
Stallman acknowledges or assumes that the sexual act took place, or presents his
arguments as if it had taken place. We have omitted other occasions where
Stallman does not presume the act had taken place, for instance occasions where
Stallman emphasizes the presumption of innocence in legal proceedings.</p>
<p><a href="https://stallman-report.org/defense-of-sexual-misconduct">Appendix: Stallman’s defense of sexual misconduct</a></p>
<p>Among these sources we have identified at least 567 separate victims of sexual
misconduct whose experience was downplayed or dismissed by
Stallman.<sup id="fnref:18"><a href="#fn:18" role="doc-noteref">18</a></sup></p>

<p>It is demonstrable that Stallman’s defenses of sexual misconduct cause material
harm to victims. Rhetoric which denies or downplays a victim’s experience of
sexual misconduct causes harm:</p>
<blockquote>
<p>One of the most important factors that predicts severity of post-trauma
symptomatology in any rape victim is the post-trauma response received from
the environment. For example, where a victims’ experience of rape is ignored
(deliberately or as a result of people simply not knowing), not recognised,
minimised, or both; and where victims are blamed, judged as culpable, met with
further violence, violation, or both. Lack of empathy and understanding can,
therefore, reduce the prospects for a recovery. (Mason, Lodrick 2013)<sup id="fnref:19"><a href="#fn:19" role="doc-noteref">19</a></sup></p>
</blockquote>
<p>The symptoms of complex post-traumatic stress disorder (CPTSD) in victims of
sexual assault which are exacerbated by rhetoric similar to Stallman’s are
severe. It is also noted that the symptoms of post-traumatic stress disorder in
sexual assault victims are exacerbated if the victim is very young.</p>
<blockquote>
<p>Post-traumatic stress disorder is an extremely distressing and disabling
condition. Intrusive symptoms such as ﬂashbacks, nightmares and feeling as
though the assault is reoccurring are profoundly upsetting to individuals who
experience them. Their psychological response is often to become avoidant of
thoughts, feelings, places and other reminders of the assault.
(Mason, Lodrick 2013)<sup id="fnref1:19"><a href="#fn:19" role="doc-noteref">19</a></sup></p>
</blockquote>
<p>Exposure to Stallman’s rhetoric not only harms the victims of the incidents to
which he refers, but also harms victims of similar experiences which are exposed
to his remarks.</p>
<p>Stallman’s defenses of sexual misconduct rely on a number of recurring premises.
A common defense relies on Stallman’s insistence that minors over the age of 12
or 13 are sexually mature and can meaningfully consent to having sex with an
adult. Consider Stallman’s remarks on the case of Cody Wilson:</p>
<blockquote>
<p>Cody Wilson has been charged with “sexual assault” on a “child” after a
session with a sex worker of age 16. (…)</p>
<p>The article refers to the sex worker as a “child”, but that is not so.
Elsewhere it has been published that she is 16 years old. That is late
adolescence, not childhood.</p>
<p>Calling teenagers “children” encourages treating teenagers as children, a
harmful practice which retards their development into capable adults.</p>
<p>In this case, the effect of that mislabeling is to smear Wilson. It is rare,
and considered perverse, for adults to be physically attracted to children.
However, it is normal for adults to be physically attracted to adolescents.
Since the claim sbout[<em>sic</em>] Wilson is the latter, it is wrong to present it
as the former.</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2018-jul-oct.html#23_September_2018_(Cody_Wilson)">stallman.org, 23 September 2018 “Cody Wilson”</a></p>
<p>Laws regarding rape are more general than cases of outwardly apparent coercion.
We have provided a general rebuttal of Stallman’s political position on sexual
relations between adults and minors <a href="#minor-adult-sex-rebuttal">elsewhere in the
report</a>.</p>
<p>Other defenses of sexual misconduct by Stallman focus on an insistence on
appropriate use of language in order to establish the “gravity” of the crime in
order to determine how the public should “judge” an offender. This defense is
often associated with Stallman’s fixation on the term “sexual assault”, which we
cover <a href="#dismissal-of-legal-norms-regarding-sexual-assault">elsewhere in this report</a>.</p>
<blockquote>
<p>Jelani Maraj (who I had never heard of) could be imprisoned for a long time
for “sexual assault”. What does that concretely mean?</p>
<p>Due to the vagueness of the term “sexual assault” together with the dishonest
law that labels sex with adolescents as “rape” even if they are willing, we
cannot tell from this article what sort of acts Maraj was found to have
committed. So we can’t begin to judge whether those acts were wrong.</p>
<p>I see at least three possibilities. Perhaps those acts really constituted
rape — it is a possibility. Or perhaps the two had sex willingly, but her
parents freaked out and demanded prosecution. Or, intermediate between those
two, perhaps he pressured her into having sex, or got her drunk.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2017-nov-feb.html#13_November_2017_(Jelani_Maraj)">stallman.org, 13 November 2017 “Jelani Maraj”</a></p>
<p>In this comment, Stallman belittles the experience of a rape
victim<sup id="fnref:20"><a href="#fn:20" role="doc-noteref">20</a></sup> and argues for the following positions:</p>
<ol>
<li>Adolescents can consent to sex with adults (<a href="#minor-adult-sex-rebuttal">rebuttal</a>)</li>
<li>Pressuring someone into sex is an “intermediate” offense between overtly
consensual sex and sexual assault.</li>
<li>Making someone drunk for the purpose of sexual assault is “intermediate”
offense between overtly consensual sex and sexual assault.</li>
</ol>
<p>This is an example of Stallman’s regular insistence that sexual crimes be
discussed in highly specific language for the purpose of establishing the
gravity of the crime so that the public may judge the offender by measures.</p>
<p>Moreover, in this respect Stallman’s defenses of sexual misconduct are based on
a dismissal of the importance of consent. Stallman consistently defends
scenarios where he presumes consent due to a perceived absence of violent
coercion.</p>
<p>This form of defense also appears in Stallman’s frequent defenses of Julian
Assange.</p>
<blockquote>
<p>Personal attacks against Julian Assange are used to distract attention from
the heroic achievements of Wikileaks.</p>
<p>Ironically, this article itself exaggerates criticism of Assange by stating
that the allegations against him consist of “rape” — they do not.</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2012-may-aug.html#2_July_2012_(Attacks)">stallman.org, 2 July 2012 “Attacks”</a></p>
<p>The editors acknowledge that the political circumstances surrounding allegations
of sexual misconduct by Julian Assange may represent due cause to doubt the
allegations. However, we draw attention to the fact that Stallman does not argue
from a presumption of Assange’s innocence, but rather from an objection to the
presumed act being classified as rape.</p>
<p>Among other accusations, one of the presumed acts is that a woman woke up to
discover Assange having unprotected sex with her as she slept. The two had had
consensual sex the prior evening on the condition that Assange used a condom. We
can conclude that Stallman dismisses the conditional nature of the victim’s
consent regarding condoms, and argues that the consent agreed upon on the prior
evening “carries over” to sex with a sleeping victim the following morning.
Stallman made this clear on August 12th, 2012:</p>
<blockquote>
<p>If Assange had sex with a sleeping woman, the morning after they had sex and
then slept together, was that rape? MP George Galloway says no.</p>
<p>Waking up your lover with sex is a tradition that has given pleasure to many,
and prohibiting it by designating it as rape is absurd. If that’s what the law
says in some country, that law is absurd.</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2012-may-aug.html#21_August_2012_(Assange)">stallman.org, 21 August 2012 “Assange”</a></p>
<p>We also identify one additional theme in Stallman’s defenses of sexual
misconduct, which are based on an outright misrepresentation of the events
concerned. For example, in response to a case where Ohio State athletics teacher
Dr Richard Strauss was revealed to have sexually abused 177 students over the
course of 20 years, Stallman writes the following:</p>
<blockquote>
<p>Should we accept stretching the terms “sexual abuse” and “molestation” to
include looking without touching?</p>
<p>I do not accept it.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2019-may-aug.html#11_June_2019_(Stretching_meaning_of_terms)">stallman.org, 11 June 2019 “Stretching meaning of terms”</a></p>
<p>The article Stallman cites includes the following quote:</p>
<blockquote>
<p>Many of Strauss’s accusers who have spoken publicly said they were masturbated
or otherwise touched inappropriately during physical exams or leered at in the
locker rooms.</p>
</blockquote>
<p>– <a href="https://www.theguardian.com/us-news/2019/may/17/ohio-state-report-reveals-team-doctor-sexually-abused-at-least-177-athletes">The Guardian, 17 May 2019</a></p>
<p>We also cite the following example from December 2017:</p>
<blockquote>
<p>Mormon feminists are challenging sexual abuse in the Mormon church, which
combines with scorn for women that aren’t “chaste” to cause great suffering.</p>
<p>There are fathers that rape their daughters — and there are also “recovered
memory therapists” that implant false memories of childhood sexual abuse that
didn’t happen. A priori, either one could have happened here. The fact that
Carol did not remember the abuse until she worked with a therapist makes me
suspect the latter. It seems that Carol’s sister also need “help” to remember.</p>
<p>I hope there is a way to determine which one really occurred.</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2017-sep-dec.html#1_December_2017_(Mormon_sexual_abuse)">stallman.org, 1 December 2017 “Mormon sexual abuse”</a></p>
<p>In this example, Stallman invokes “recovered memory therapists”, which is not
referenced in the cited text, to sow doubt on the stories of Mormon survivors of
rape.</p>

<h3 id="dismissal-of-legal-norms-regarding-sexual-assault">
	Dismissal of legal norms regarding sexual assault
	
	<a href="#dismissal-of-legal-norms-regarding-sexual-assault"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>We have catalogued 24 primary sources in which Stallman misrepresents or
downplays sexual assault between 2015 and 2024 as part of a broader political
program that aims to dismiss the experiences of victims and erode social and
legal norms around sexual assault.</p>
<p><a href="https://stallman-report.org/on-sexual-assault">Appendix: Stallman on sexual assault</a></p>
<p>Like “children”, in Stallman’s speech “sexual assault” is an idiolectical term
which Stallman defines in his “anti-glossary”:</p>
<blockquote>
<p><strong>Sexual assault</strong>: The term is applied to a broad range of actions, from rape
on one end, to the least physical contact on the other, as well as everything
in between. It acts as propaganda for treating them all the same. That would
be wrong.</p>
<p>The term is further stretched to include sexual harassment, which does not
refer to a single act, but rather to a series of acts that amounts to a form
of gender bias. Gender bias is rightly prohibited in certain situations for
the sake of equal opportunity, but that is a different issue.</p>
<p>I don’t think that rape should be treated the same as a momentary touch.
People we accuse have a right to those distinctions, so I am careful not to
use the term “sexual assault” to categorize the actions of any person on any
specific occasion.<sup id="fnref1:8"><a href="#fn:8" role="doc-noteref">8</a></sup></p>
</blockquote>
<p>The gross misrepresentation of sexual harassment in this quote is not lost on
the editors, and is covered in the next section in detail.</p>
<p>Sexual assault is more accurately defined as an assault of a sexual nature. It
refers to an act of assault – an unwanted physical interactions – with
sexual motivations. The United States National Center for Victims of Crime
provides the following explanation:</p>
<blockquote>
<p>Sexual assault is an act of forcing another person into sexual activity
against his or her will. Sexual assault takes many forms, including rape or
attempted rape, as well as any unwanted sexual contact. The crime includes
forced sexual intercourse (rape), sodomy (oral or anal sexual acts), child
molestation, incest, fondling, and attempted rape.<sup id="fnref:21"><a href="#fn:21" role="doc-noteref">21</a></sup></p>
</blockquote>
<p>Stallman has misrepresented sexual assault many times. For instance, in October
2023, Stallman writes the following:</p>
<blockquote>
<p>I warned that the stretchable term “sexual assault”, which extends from grave
crimes such as rape through significant crimes such as groping and down to no
clear lower bound, could be stretched to criminalize minor things, perhaps
even stealing a kiss. Now this has happened.</p>
<p>What next? Will a pat on the arm or a hug be criminalized? There is no clear
limit to how far this can go, when a group builds up enough outrage to push
it.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2023-jul-oct.html#15_October_2023_(Sexual_assault_for_stealing_a_kiss)">stallman.org, 15 October 2023 “Sexual assault for stealing a kiss”</a></p>
<p>In this note, Stallman cites the case of Luis Rubiales, who was under
investigation for kissing a female football player on television as part of a
series of incidents called the “Rubiales affair”, for which Rubiales was
indicted on criminal charges of sexual assault and faces a potential prison
sentence.<sup id="fnref:22"><a href="#fn:22" role="doc-noteref">22</a></sup></p>
<p>The idea of “stealing a kiss” is a familiar refrain for Stallman’s program
speaking against social and legal norms around sexual assault. In 2019, he
writes:</p>
<blockquote>
<p>If it is true that he persistently pressured her to kiss him, on stage and
off, if he stuck his tongue into her mouth despite her objections, that could
well be sexual harassment. He should have accepted no for an answer the first
time she said it. However, calling a kiss “sexual assault” is an exaggeration,
an attempt to equate it to much graver acts, that are crimes.</p>
<p>The term “sexual assault” encourages that injustice, and I believe it has been
popularized specifically with that intention. That is why I reject that term.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2019-jul-oct.html#30_July_2019_(Al_Franken)">stallman.org, 30 July 2019 “Al Franken”</a></p>
<p>“Stealing a kiss” is the least “grave” of the acts Stallman questions the
legitimacy of the label of sexual assault, but Stallman has also questioned its
use for incidents such as the rape of minors.<sup id="fnref:23"><a href="#fn:23" role="doc-noteref">23</a></sup> It is this
“gravity” that Stallman fixes on when questioning sexual assault, which he
wishes to understand for the purpose of how he, and the reader, should “judge”
the offender.</p>
<blockquote>
<p>Due to the vagueness of the term “sexual assault” together with the dishonest
law that labels sex with adolescents as “rape” even if they are willing, we
cannot tell from this article what sort of acts Maraj was found to have
committed. So we can’t begin to judge whether those acts were wrong.<sup id="fnref1:20"><a href="#fn:20" role="doc-noteref">20</a></sup></p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2017-nov-feb.html#13_November_2017_(Jelani_Maraj)">stallman.org, 13 November 2017 “Jelani Maraj”</a></p>
<p>This line of questioning is precisely the wrong response to news of sexual
assault. The vagueness of the term as presented to the public is deliberate; it
protects the privacy of both the victim and the accused. It is the concern of
the legal system to determine the severity of the offense, not the general
public.</p>
<p>The Rape, Abuse &amp; Incest National Network (RAINN) provides resources on
appropriate ways to respond to sexual assault, for instance their “TALK”
framework specifically advises against minimizing the victim’s experiences,
pressing them for details, or challenging their experience (“Are you sure that
counts as assault?” is an example given in their resources).<sup id="fnref:24"><a href="#fn:24" role="doc-noteref">24</a></sup> RAINN
also offers the following advice:<sup id="fnref:25"><a href="#fn:25" role="doc-noteref">25</a></sup></p>
<blockquote>
<p>It can be extremely difficult for survivors to come forward and share their
story. They may feel ashamed, concerned that they won’t be believed, or
worried they’ll be blamed. Leave any “why” questions or investigations to the
experts—your job is to support this person. Be careful not to interpret
calmness as a sign that the event did not occur—everyone responds to traumatic
events differently. The best thing you can do is to believe them.</p>
</blockquote>
<p>We re-iterate the earlier position that Stallman’s rhetoric <a href="#rhetorical-harm">causes material
harm</a> to victims of sexual assault, and we have illustrated
that his political program contributes to an environment where the harm
suffered by sexual assault victims is exacerbated by creating an atmosphere of
confusion and doubt in which sexual violence can thrive.</p>
<p>On the subject of sexual assault, Stallman advances a political agenda which
systematically undermines the importance of consent in sexual and intimate
interactions, objectifying women as subjects of men’s desires, enabling men to
force their sexual desires on women, and dismissing womens’ agency in choosing
how to express intimacy and interact with others.</p>

<h3 id="dismissal-of-legal-norms-regarding-sexual-harassment">
	Dismissal of legal norms regarding sexual harassment
	
	<a href="#dismissal-of-legal-norms-regarding-sexual-harassment"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>We have catalogued 13 primary sources in which Stallman misrepresents or
downplays sexual harassment between 2014 and 2018 as part of a broader political
program that aims to erode social and legal norms around sexual harassment.</p>
<p><a href="https://stallman-report.org/on-sexual-harassment">Appendix: Stallman on sexual harassment</a></p>
<p>Stallman’s “anti-glossary” indirectly defines sexual harassment in its
definition of sexual assault:<sup id="fnref2:8"><a href="#fn:8" role="doc-noteref">8</a></sup></p>
<blockquote>
<p>The term [sexual assault] is further stretched to include sexual
harassment, which does not refer to a single act, but rather to a series of
acts that amounts to a form of gender bias. Gender bias is rightly prohibited
in certain situations for the sake of equal opportunity, but that is a
different issue.</p>
</blockquote>
<p>We also note the following quote from November 2017:</p>
<blockquote>
<p>The term “sexual assault” is not suitable for a serious discussion, because it
covers crimes of varying severities which call for different responses, plus
sexual harassment which is not a crime.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2017-nov-feb.html#3_November_2017_(Saboteur_of_Energy)">stallman.org, 3 November 2017 “Saboteur of Energy”</a></p>
<p>Both of these examples are gross misrepresentations of sexual harassment. The US
Equal Employment Opportunity Commission provides a more appropriate explanation:</p>
<blockquote>
<p>It is unlawful to harass a person (an applicant or employee) because of that
person’s sex. Harassment can include “sexual harassment” or unwelcome sexual
advances, requests for sexual favors, and other verbal or physical harassment
of a sexual nature.</p>
<p>Harassment does not have to be of a sexual nature, however, and can include
offensive remarks about a person’s sex. For example, it is illegal to harass a
woman by making offensive comments about women in general.</p>
<p>Both victim and the harasser can be either a woman or a man, and the victim
and harasser can be the same sex.<sup id="fnref:26"><a href="#fn:26" role="doc-noteref">26</a></sup></p>
</blockquote>
<p>Stallman’s remarks on sexual harassment are antifactual. Sexual harassment is a
crime, and it is not reducible to a kind of gender bias. It may consist of
several actions forming a pattern of behavior, but isolated events of sufficient
severity may also constitute sexual harassment.</p>
<p>We also note that the definition given in Stallman’s anti-glossary changed
sometime between February and March 2019. Previously it read as follows:</p>
<blockquote>
<p>[Acts that constitute sexual assault] are not merely different in degree. They
are different in kind. Rape is a grave crime. Being groped is unpleasant but
not as grave as robbery. Sexual harassment is a not an action at all, but
rather a pattern of actions that constitutes economic unfairness. How can it
make sense to group these behaviors things together?</p>
</blockquote>
<p>– <a href="https://web.archive.org/web/20190214062917/http://www.stallman.org:80/antiglossary.html">stallman.org, “Anti-glossary”, archived February 2019 by archive.org</a></p>

<p>Our report finds this change noteworthy on the basis that Stallman completed a
mandatory course on sexual harassment in his role at MIT in September 2018, five
months prior to the edit:</p>
<blockquote>
<p>In September MIT demanded that I take an online course about sexual
harassment; although I don’t teach classes or even meet undergraduates, they
treated me like a professor on the “be overcautious at every opportunity”
principle. But I was unable to do so until MIT arranged to let me log in on
an MIT kiosk terminal and bypassed the two-factor requirement for me.</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2019-may-aug.html#7_May_2019_(Mobile_phone_numbers_for_Facebook)">stallman.org, 7 May 2019 “Mobile phone numbers for Facebook”</a></p>
<p>The editors of this report sought to ascertain the nature of the training that
Stallman received. The 2018 Annual Report from MIT’s Committee on Sexual
Misconduct Prevention and Response<sup id="fnref:27"><a href="#fn:27" role="doc-noteref">27</a></sup> states that MIT faculty
and staff members were required to complete the <em>Haven for Faculty and Staff</em>
course provided by <a href="https://everfi.com/">Everfi</a>, a US training provider which
is relied upon by many educational institutions for training its faculty
members. The editors reached out to the MIT committe and to Everfi for comment
regarding the cirriculum and did not receive a response. However, MIT’s annual
reports provide some insight into the training material. The committe’s inagural
report in 2016 describes this program as follows:</p>
<blockquote>
<p>Haven’s introduction is completely customizable, including a welcome video
(e.g., from the President, Provost, or Chancellor), a list of campus resources
and contact information, and any other desired materials. The program
continues with videos on supporting survivors, encouraging bystander
intervention, and recognizing the potential for violence on campus or in the
workplace, and then provides details on Title IX and other legislation. Four
additional videos follow: “A Student Disclosure” about how to respond when a
student or employee initiates a discussion about sexual misconduct; “Always
Around” on policies and responses to stalking; “A Concerned Co‐worker” about
intimate partner violence that affects the workplace; and “Unwanted Attention”
about addressing inappropriate behavior from a supervisor. Questions are posed
before and after each video; incorrect answers trigger a gentle steering
toward the most appropriate response.<sup id="fnref:28"><a href="#fn:28" role="doc-noteref">28</a></sup></p>
</blockquote>
<p>The 2018 report also provides insights into the cirruclum:</p>
<blockquote>
<p>All faculty and staff were required to complete Haven for Faculty and Staff,
an online education program that includes examples and scenarios that faculty
and staff may face around sexual assault, domestic violence, stalking, and
sexual harassment.<sup id="fnref1:27"><a href="#fn:27" role="doc-noteref">27</a></sup></p>
</blockquote>
<p>The committee also summarizes the outcomes of the training program in their 2019
report, which provides a few examples of specific goals associated with the
training program.<sup id="fnref:29"><a href="#fn:29" role="doc-noteref">29</a></sup> In particular, this report draws attention
to the survey results enumerated in Appendix C, which indicate that the
program’s goals were in part to obtain favorable responses to the following
questions of note:</p>
<ul>
<li>I have a good understanding of what constitutes sexual assault, relationship
violence, stalking, and sexual harassment.</li>
<li>I am aware of strategies for preventing sexual assault, relationship violence,
stalking, and sexual harassment.</li>
<li>I am confident in my ability to respond to disclosures of sexual assault,
relationship violence, stalking, and sexual harassment.</li>
<li>A person should never be blamed for being the victim of sexual assault, abuse,
or harassment.</li>
<li>I think sexist jokes and langauge contribute to the issues of sexual assault,
relationship violence, stalking, and sexual harassment.</li>
<li>I plan to play an active role in addressing sexual assault, relationship
violence, stalking, and sexual harassment at my institution.</li>
</ul>
<p>From this we conclude that from September 2018 onwards, Stallman should have
posessed a working understanding of sexual assault and sexual harassment, as
well as the appropriate language and tone for discussing the matter,
particularly with respect to the best interests of victims. Our sources note
that Stallman’s website maintains misleading definitions of sexual assault and
sexual harassment to the present day, and we cite 15 examples of Stallman
misrepresenting sexual assault following his 2018 training.</p>
<p><a href="https://stallman-report.org/on-sexual-assault">Appendix: Stallman on sexual assault</a></p>
<p>Our report notes that the sort of sexual harassment that Stallman consistently
defends has long-term effects on the well-being of victims. Young women who are
subjected to a man in a position of power “stealing a kiss” are objectified and
reduced to a sexual object with no agency over consent in their interactions, an
experience which prevents them from accessing education and employment
opportunities on equal terms with respect to their male peers. Experiences of
sexual harassment have long-term consequences for victims, including increased
rates of symptoms of anxiety and depression for months following the incident,
including in relatively “less grave” cases that Stallman defends such as sexual
jokes or remarks and unwelcome advances. (Johansson et al,
2024)<sup id="fnref:30"><a href="#fn:30" role="doc-noteref">30</a></sup></p>
<p>A 2019 study by Pinchevsky et al also characterizes the harmful effects of
sexual harassment by distinguishing “non-contact” and “contact” harassment,
where the former does not involve physical contact between the perpetrator and
the victim.</p>
<blockquote>
<p>McGinley et al. (2016) found that experiences of non-contact SH [Sexual
Harassment] undermined the health of college students. Non-contact SH is
associated with decreased mental health (i.e., depression, anxiety) and
increased health-risk behavior such as substance use as a coping mechanism,
particularly among White females and sexual minorities (McGinley et al. 2016).
Paludi et al. (2006) identified other research that noted the consequences of
non-contact SH including changes in physical and mental health. Additionally,
victims of non-contact SH are more likely to experience future SH (Petersen
and Hyde 2009).<sup id="fnref:31"><a href="#fn:31" role="doc-noteref">31</a></sup></p>
</blockquote>

<h3 id="support-for-the-possession-of-child-sexual-abuse-material">
	Support for the possession of child sexual abuse material
	
	<a href="#support-for-the-possession-of-child-sexual-abuse-material"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>We have catalogued 55 primary sources in which Stallman advocates for the
possession and/or distribution of child sexual abuse material (aka “child
pornography”) between 2003 and 2019, none of which are addressed by <a href="https://stallman-report.org/on-children#2019-retraction">Stallman’s
2019 retraction</a> on sexual relationships between
adults and minors under the age of 12 or 13.</p>
<p><a href="https://stallman-report.org/on-child-pornography">Appendix: Stallman on child sexual abuse material</a></p>
<p>The following quote from Stallman in June 2017 is a typical example:</p>
<blockquote>
<p>In the US, people convicted for having copies of child pornography tend to get
longer prison sentences than those convicted of having sex with children.</p>
<p>Mere possession of child pornography should not be a crime at all. To
prosecute people for possessing something published, no matter what it may be,
is a big threat to human rights.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2017-mar-jun.html#5_June_2017_(Possession_of_child_porn)">stallman.org, 5 June 2017 “Possession of child porn”</a></p>
<p>Stallman’s discourse on child sexual abuse material (CSAM) rely on several
recurring key points:</p>
<ol>
<li>A blanket objection to censorship in any form</li>
<li>Objections to legal norms regarding CSAM on the basis that CSAM depicting
minors over the age of 12-13 do not depict an objectionable act, as adults
having sex with minors in this age range is not a form of abuse (<a href="#minor-adult-sex-rebuttal">rebuttal</a>)</li>
<li>The assertion that minors distributing explicit images with similar-aged
peers should not be prosecuted, often paired with a defense of adults who
distribute images produced in this manner</li>
</ol>
<p>We will first address Stallman’s political positions on CSAM which the editors
do not find unreasonable. First, Stallman often expresses concern that law
enforcement tools and technologies developed to curtail the production and
distribution of CSAM will be applied more generally and infringe on legitimate
freedoms; the editors find this concern reasonable but disagree with Stallman’s
conclusion that CSAM possession and/or distribution should be legalized on this
premise.</p>
<p>Stallman also defends the practice of “sexting”, or exchange of sexually
explicit material, between consenting minors of similar age; the editors find
this argumentation reasonable. However, we object to Stallman’s use of this
argument as the basis for a more general argument in favor of the legalization
of CSAM possession and distribution.</p>
<p>Stallman has also defended “sexting” cases which involve the sexual exploitation
of a minor by an adult. In 2016, Stallman writes the following in response to
the case of a 21 year-old man soliciting a 16 year-old girl for explicit images:</p>
<blockquote>
<p>A Pennsylvania man has been imprisoned for receiving nude photos from his
16-year-old girlfriend, and will have to register as a sex offender, but
“only” for 15 years.</p>
<p>The willfully blind law pretends there is no difference between a teenager and
a child.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2016-jul-oct.html#30_August_2016_(Man_imprisoned_for_receiving_nude_photos)">stallman.org, 30 August 2016 “Man imprisoned for receiving nude photos”</a></p>
<p>We now present a general rebuttal of Stallman’s position on CSAM. Our report
indicates four groups of people who experience material harm as a consequence
of CSAM distribution:</p>
<ul>
<li>Victims of child sexual abuse</li>
<li>Consumers of child sexual assault material</li>
<li>Criminal investigators exposed to CSAM in their work</li>
<li>Online content moderators tasked with CSAM removal</li>
</ul>
<p>The possession and distribution of CSAM exacerbates the harm done to victims of
sexual abuse. Experts on sexual violence assert that the distribution of CSAM
causes children to be victimized twice: first by the perpetrator of their abuse,
and again by the person who view it.<sup id="fnref:32"><a href="#fn:32" role="doc-noteref">32</a></sup> Jennifer Martin explains how
CSAM impacts survivors of abuse:</p>
<blockquote>
<p>This persistent shame means that traumatic stress symptoms may continue
indefinitely in cases of [child sexual abuse material]. Children’s ongoing
efforts to make meaning of their abuse experience may be ineffective, and they
may continually fear what parents, caregivers, and others may think if they
discover, or are shown, images of the abuse (Palmer, 2006). Victims may be
further subjected to shame by the knowledge that images of the abuse are
stored in law enforcement databases and may be accessed and shared
indefinitely among and between legal agencies globally (Muir, 2005). They are
powerless over the distribution or accessibility of the images of abuse, and
must contend with the fact that they may be gazed upon by anyone at any time
because their abuse images are “out there” in the public arena of cyberspace
and can be forever shared and downloaded. The child may internalize the shame
and humiliation of the “global gaze” thereby adding to the child’s traumatic
burden. The never-ending threat of this gaze can influence – have power over -
how the child thinks, feels, and behaves. Ainley(1998) referred to the
“constant torture of the random but ever possible gaze”.<sup id="fnref:33"><a href="#fn:33" role="doc-noteref">33</a></sup></p>
</blockquote>
<p>Consumers of CSAM also experience harm. According to Kothari et al,
consumers of CSAM are at a high risk of suicide and experience symptoms
associated with post-traumatic stress disorder and adjustment disorder.
Offenders experience extreme feelings of stress, shame, and self-hatred, which
impacts their ability to seek help (Kothari et al 2021).<sup id="fnref:34"><a href="#fn:34" role="doc-noteref">34</a></sup></p>
<p>We also draw attention to the experiences of the online content moderators and
criminal investigators who are tasked with combating the distribution of CSAM.
Members of law enforcement who are exposed to CSAM in their work experience
experience an elevated risk of sexual post-traumatic stress symptoms (sexual
PTSS) (Gewirtz-Meydan et al, 2023).<sup id="fnref:35"><a href="#fn:35" role="doc-noteref">35</a></sup> Online content
moderators are susceptible to similar risks, and often experience symptoms
associated with post-traumatic and secondary traumatic stress (Spence et al,
2021).<sup id="fnref:36"><a href="#fn:36" role="doc-noteref">36</a></sup></p>
<p>This report also draws attention to a particularly disturbing source from
Richard Stallman’s website, entitled “Suggestion to the target of a witch hunt”,
dated February 2015.<sup id="fnref:37"><a href="#fn:37" role="doc-noteref">37</a></sup> This article is listed on the front page of
Stallman’s website in a section on political articles outside of the scope of
Stallman’s free software political program. In this article, Stallman reveals
that someone had emailed him asking for advice because they were “drawn to look
at images of sex with children”. Stallman published his response as follows:</p>
<blockquote>
<p>I don’t think it is wrong to distribute “child porn” images, even when they
[depict] children rather than adolescents. However, making them is wrong if it
involves real sex with a child. For the sake of opposing sexual abuse of real
children, I suggest that you boycott the images that involve real children.
Imaginary children can’t be hurt by drawing them.</p>
<p>I can’t suggest any way you could talk publicly about your prediliction
without being the object of a witch hunt. Americans go nuts where they imagine
that children are in danger, and in their frenzy they exaggerate tiny risks —
look at how they jail parents for letting children go to the park or stay home
without an escort.</p>
<p>To be sure, a child faces the danger of sexual abuse mainly while at home. But
not while home alone with no members or friends of the family present.</p>
</blockquote>
<p>Stallman updated the page in 2016 with an additional note:</p>
<blockquote>
<p>2016 note: I support prosecution of those that perpetrate real abuse (sexual
or not) of real children. By “real” I mean specifically that I do not follow
states’ definitions of these terms. In fact, some states stretch the terms to
the point of absurd injustice. There is a tendency to define adolescents as
“children” and define all sex involving adolescents as “sexual abuse”.
Infantilizing adolescents is harmful to society in many ways.</p>
<p>Since this is an ethical question, not a legal one, the question of the right
definitions is for us to consider, not for states to dictate.</p>
</blockquote>
<p>The editors were particularly alarmed by this page. The person who reached out
to Stallman for advice is subject to all of the harm faced by consumers of CSAM
as discussed earlier in our report, and faces the risk of arrest and criminal
prosecution if caught. Rather than providing this person with resources to seek
help, Stallman states that there is nothing wrong with his behavior and uses the
opportunity to publicly re-enforce his political program regarding CSAM and the
sexual abuse of minors.</p>
<p>The editors cite this as an example of direct harm caused by Stallman and as
evidence that Stallman’s remarks are taken seriously, that he is viewed as an
authority on sexual matters by some of those who read his work, and that he has
been consulted for his opinion on these matters by his readers.</p>

<h3 id="legal-and-social-normalization-of-sex-between-humans-and-animals">
	Legal and social normalization of sex between humans and animals
	
	<a href="#legal-and-social-normalization-of-sex-between-humans-and-animals"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>We have catalogued 12 primary sources from 2003-2018 in which Stallman advocates
for humans having sex with animals (bestiality) or the possession and
distribution of pornography featuring humans having sex with animals. None of
Stallman’s remarks on bestiality have been retracted.</p>
<p><a href="https://stallman-report.org/on-bestiality">Appendix: Stallman on bestiality</a></p>
<p>Stallman remarked most recently on the subject of pornography featuring humans
and animals in 2018:</p>
<blockquote>
<p>Prudish censorship attacks again in the UK, convicting someone for possessing
“extreme pornography”, including images of sex with animals.</p>
<p>I can’t imagine a possible reason to punish people for this. The article does
not report that the animals were harmed, or that they objected to the
experience, or that they thought of it as sexual. The law does not consider
these questions pertinent.</p>
<p>What is, however, clear is that prohibiting the possession of copies of some
image or text — no matter what that image or text may be — threatens human
rights. It creates excuses to search through people’s possessions and files.
It creates ways to make people vulnerable to criminal charges without their
cooperation or even their knowledge. All such laws must be repealed.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2018-sep-dec.html#14_December_2018_('Extreme_pornography'_conviction)">stallman.org, 14 December 2018 “‘Extreme pornography’ conviction”</a></p>
<p>The editors note that Stallman’s remarks here are consistent with <a href="#support-for-the-possession-of-child-sexual-abuse-material">his rhetoric
on child sexual abuse material</a>.
In addition to arguing for the legal normalization of this kind of pornographic
images, Stallman has explicitly supported the act depicted therein, for instance
in 2017:</p>
<blockquote>
<p>European countries are passing laws against having sex with an animal. (We are
talking about sex practices that don’t physically hurt the animal.)</p>
<p>These laws have no rational basis. We know that some animals enjoy sex with
humans. Others don’t. But really, if you smear something on your genitals that
tastes good to dogs, and have a dog lick you off, it harms no one. Why should
this be illegal except mindless religion?</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2017-sep-dec.html#10_October_2017_(Laws_against_having_sex_with_an_animal)">stallman.org, 10 October 2017 “Laws against having sex with an animal”</a></p>
<p>Stallman explained his views in more detail in 2016:</p>
<blockquote>
<p>A national campaign seeks to make all US states prohibit sex between humans
and nonhuman animals.</p>
<p>This campaign seems to be sheer bull-headed prudery, using the perverse
assumption that sex between a human and an animal hurts the animal. That’s
true for some ways of having sex, and false for others.</p>
<p>For instance, I’ve heard that some women get dogs to lick them off. That
doesn’t hurt the dog at all. Why should it be prohibited?</p>
<p>When male dolphins have sex with people, that doesn’t hurt the dolphins. Quite
the contrary, they like it very much. Why should it be prohibited?</p>
<p>I’ve also read that female gorillas sometimes express desire for sex with men.
If they both like it, who is harmed? Why should this be prohibited?</p>
<p>The proponents of this law claim that any kind of sex between humans and other
species implies that the human is a “predator” that we need to lock up. That’s
clearly false, for the cases listed above. Making a prohibition based on
prejudice, writing it in an overbroad way, is what prissy governments tend to
do where sex is concerned. The next step is to interpret it too strongly with
“zero tolerance”.</p>
<p>Will people convicted of having dogs lick them off be required to live at
least 1000 feet from any dogs?</p>
<p>This law should be changed to prohibit only acts in which the animal is
physically forced to have sex, or physically injured.</p>
</blockquote>
<p>– <a href="https://www.stallman.org/archives/2016-sep-dec.html#14_December_2016_(Campaign_of_bull-headed_prudery)">stallman.org, 14 December 2016 “Campaign of bull-headed prudery”</a></p>
<p>It is straightforwardly understood that animals cannot express consent; they are
not capable of meaningfully communicating “yes” or “no” and they cannot explain
their subjective experiences or advocate for themselves following sexual abuse.
It is also understood that animal victims of sexual abuse experience symptoms
similar to human victims; animal victims of sexual abuse commonly display signs
of depression, anxiety, and aggression (Kunz 2019).<sup id="fnref:38"><a href="#fn:38" role="doc-noteref">38</a></sup></p>
<p>The actors involved in pornographic films depicting bestiality are often coerced
and humiliated by the act. Linda Lovelace is a famous pornographic actress who
became widely known for her appearance in the 1972 film “Deep Throat”, later
stating that she was coerced and raped on screen in this film. In 1969, she
appeared in a film where she was coerced into performing sexual acts with a dog.
She was forced to perform these acts at gunpoint and later explained the lasting
effect of this experience:</p>
<blockquote>
<p>I am able to handle almost everything that has happened to me in my life… but
I’m still not able to handle that day. A dog. An animal. I’ve been raped by
men who were no better than animals, but this was an actual animal and that
represented a huge dividing line. (…)</p>
<p>There were no greater humiliations left for me. The memory of that day and
that dog does not fade the way other memories do. The overwhelming sadness
that I felt on that day is with me at this moment, stronger than ever. It was
a bad day, such a bad day.<sup id="fnref:39"><a href="#fn:39" role="doc-noteref">39</a></sup></p>
</blockquote>
<p>Stallman’s remarks on bestiality are consistent with his broader dismissal of
the importance of consent with respect to sexual interactions, be they animals,
minors, subordinates, women whose consent is contingent on the use of
contraception, or women who have previously consented to sex; in all of these
cases Stallman absolves the perpetrator of wrongdoing and argues that images of
these acts of sexual violence should not be subject to censorship.</p>

<h3 id="legal-and-social-normalization-of-necrophilia">
	Legal and social normalization of necrophilia
	
	<a href="#legal-and-social-normalization-of-necrophilia"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>We have catalogued 3 primary sources from 2003-2013 in which Stallman advocates
for humans having sex with corpses (necrophilia) or the possession and
distribution of pornography featuring humans having sex with corpses. We have
also identified several more contemporary sources where Stallman remarks on the
abuse of corpses generally in a non-sexual context, using the same argumentation
used to advocate for necrophilia, as recently as 2023. None of Stallman’s
remarks on the abuse of corpses, for sexual purposes or otherwise, have been
retracted.</p>
<p><a href="https://stallman-report.org/on-necrophilia">Appendix: Stallman on necrophilia</a></p>
<p>For example, Stallman writes the following in April 2008 in a statement calling
for pornography featuring living individuals having sex with corpses to be
legalized:</p>
<blockquote>
<p>It is true that victims of real violence suffer. (Never mind that in making
movies of violence, typically nobody is actually hurt.) The true oppressive
spirit of this law starts to show in the prohibition of images of sex with
corpses. Are we supposed to believe that corpses can suffer? Or are some cruel
prudes trying to impose their prejudices by force?</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2008-mar-jun.html#30%20April%202008%20(Possession%20of%20extreme%20pornography)">30 April 2008 (Possession of “extreme pornography”)</a></p>
<p>The editors acknowledge that Stallman has not made any explicit statements in
support of necrophilia since 2010. However, Stallman has often used the same
line of argumentation in statements on the abuse of corpses more generally than
for the purpose of sexual gratification since 2010. For example, in December
2023:</p>
<blockquote>
<p>Brittany Watts, of Ohio, had a miscarriage at home and disposed of the
nonviable fetus as people often do. Now she faces possible charges of “abuse
of a corpse”.</p>
<p>The very idea of sentencing someone to prison for “abuse of a corpse” is
absurd, since whatever is done to a corpse can’t injure any person.</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2023-sep-dec.html#28_December_2023_(Brittany_Watts)">stallman.org, 28 December 2023 “Brittany Watts”</a></p>
<p>Legal and social norms regarding the treatment of corpses acknowledge the agency
of the deceased individual and their right to self-determination and bodily
autonomy following their death. Moreover, the editors point out that a corpse is
unable to express consent.</p>
<p>The treatment of a corpse is also a matter of respect for friends and family of
the deceased, who should be allowed to grieve in peace without the knowledge
that their loved one’s corpse is being exploited for sexual gratification. The
editors of this report cannot imagine a more traumatic grieving process than one
which contends with the knowledge that images of the desecration of your loved
one’s corpse are being distributed for the sexual gratification of others.</p>
<p>It is noted that the rape of corpses is a war crime which has been reported in
several conflicts and has been employed for the purpose of subjecting the
population to terror, humiliation, and trauma.<sup id="fnref:40"><a href="#fn:40" role="doc-noteref">40</a></sup></p>

<h2 id="credible-allegations-of-sexual-misconduct">
	Credible allegations of sexual misconduct
	
	<a href="#credible-allegations-of-sexual-misconduct"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h2>
<p>There are numerous allegations of misconduct regarding Richard Stallman. Most of
these are hearsay recounts of individual experiences with Stallman. This report
only includes allegations which have been corroborated or are otherwise
considered verifiable.</p>
<h3 id="testimony-of-betsy-s">
	Testimony of Betsy S.
	
	<a href="#testimony-of-betsy-s"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>The testimony of “Betsy S.” recalls the following interaction with Richard
Stallman, which would have taken place in the early 1980’s.</p>
<blockquote>
<p>When I was a teen freshman, I went to a buffet lunch at an Indian restaurant
in Central Square with a graduate student friend and others from the AI lab. I
don’t know if he and I were the last two left, but at a table with only the
two of us, Richard Stallman told me of his misery and that he’d kill himself
if I didn’t go out with him.</p>
<p>I felt bad for him and also uncomfortable and manipulated. I did not like
being put in that position — suddenly responsible for an “important” man. What
had I done to get into this situation? I decided I could not be responsible
for his living or dying, and would have to accept him killing himself. I
declined further contact.</p>
<p>He was not a man of his word or he’d be long dead.</p>
</blockquote>
<p>We consider the report verifiable on the basis that Stallman has corroborated
Betsy’s recollection of events in a July 2020 statement on the subject:</p>
<blockquote>
<p>A note to Betsy S.</p>
<p>Betsy S met me at a lunch around 40 years ago. I am sure her recounting of her
recollections is sincere, but she must have misunderstood the last thing I
said to her. She said she didn’t want an acquaintance with me. That no, on top
of so many noes from others, impelled me to express despair; she seems to have
misconstrued that as a demand.</p>
<p>Betsy S, I regret that this misunderstanding caused you distress. I never
intended to demand anything of you. I only ever wished you well.</p>
</blockquote>
<p>– <a href="https://stallman.org/archives/2020-jul-oct.html#19_July_2020_(A_note_to_Betsy_S.)">stallman.org, 19 July 2020 “A note to Betsy S.”</a></p>
<p>Betsy’s testimony describes an experience of sexual coercion, wherein Stallman
threatens violence (to himself) if Betsy does not date him.</p>
<p>At the time of this incident, Betsy would have been a freshman at MIT, no older
than 19, and Stallman would have been approximately 27 years old, a graduate
student having been established at the AI lab for about nine years at this time.
Stallman exploited this power differential in an attempt to take advantage of
this young woman, coercing her into dating him.</p>
<p>Stallman’s 2020 response is lacking in several respects. The editors point out
that at the time this response was written, Stallman should have been equipped
with the requisite training to understand the gravity of this incident given his
September 2019 course on sexual harassment and sexual violence at MIT, which is
discussed in detail <a href="#ref-sexual-harassment-training">earlier in this report</a>.</p>
<p>We also draw attention to the phrasing of Stallman’s apology. Stallman blames
Betsy for misunderstanding his intent when he threatened suicide if Betsy did
not agree to date him. Stallman also excuses his behavior by shifting
responsibility to Betsy and to women collectively, citing both that Betsy did
not want an acquaintance with Stallman and that his actions were motivated by a
series of romantic rejections. Stallman does not demonstrate an understanding of
why his behavior was wrong, and does not take responsibility for his behavior;
instead he “apologises” for Betsy’s behavior (i.e. misunderstanding him).</p>
<h3 id="emacs-virgin-incidents">
	“Emacs virgin” incidents
	
	<a href="#emacs-virgin-incidents"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>Richard Stallman has often performed a satirical routine as “St. IGNUcius” of
the “Church of EMACS” at numerous events. The routine includes a ceremony
regarding the “EMACS virgin” (a person who has not used EMACS before) with
sexualized overtones. Prior to a 2009, Stallman emphasized in his routine that
the virgin must be female, after 2009 Stallman referred to the EMACS virgin as a
“person” who has not used EMACS.</p>
<p>We have several uncorroborated testimonies of women, including minors, being
overtly sexualized during this routine, some without consent. In the course of
our research we discovered that one of these routines was recorded, in which
Stallman brings a 13 year-old girl on stage and makes sexually suggestive
remarks about her in front of a crowd at FKFT 2008 in Barcelona.</p>

<p>We highlight the following quote from the transcript of this event:</p>
<blockquote>
<p>I saw her experiment once. She actually typed Ctrl+V to scroll the screen. But
I think that– at that point that’s like having kissed, so she’s still a
virgin for now. [Stallman approaches the girl and places a hand on her
shoulder.] But I hope to do something about that. And, by the way, that
reminds me that one of the other advantages of the Church of EMACS is that
being a saint in this church does not require celibacy.</p>
</blockquote>
<p>Following a particularly controversial performance of this routine at the 2009
Gran Canaria Desktop Summit, Stallman made the following statement:</p>
<blockquote>
<p>Some of the people in the audience in my speech in the Gran Canaria Desktop
Summit thought that my joke about the Virgin of Emacs was intended to make
some kind of statement about women.</p>
<p>I was surprised by that reaction, since I had told the same joke dozens of
times and this is the first report of interpreting it that way.  In any case,
it was a misunderstanding: the only intended meaning of the Cult of the Virgin
of Emacs is to parody another Cult of the Virgin. The whole St IGNUius routine
makes fun of me, the free software movement and religion, through parody.</p>
<p>To be abundantly clear, my views about women in connection with free software
are simply that they deserve freedom in using computers, just as men do.  Some
women already appreciate this freedom and have become free software activists.
We need more people, regardless of sex, to do this, so that someday all women,
and all men, will enjoy the freedom that free software offers.</p>
<p>Misunderstanding is not a good outcome.  To help avoid misunderstandings of
this kind in the future, since August I have changed the joke so that the
Virgin of Emacs can be of either sex.<sup id="fnref:41"><a href="#fn:41" role="doc-noteref">41</a></sup></p>
</blockquote>
<p>Stallman has also <a href="https://stallman.org/articles/virgin-of-emacs.html">issued a statement on stallman.org</a>
about the routine.</p>
<h3 id="pleasure-cards">
	Pleasure cards
	
	<a href="#pleasure-cards"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>In lieu of a traditional business card, it has been reported that Richard
Stallman employs a “pleasure card”, which solicits “tender embraces” from the
recipient.</p>
<p><img src="https://stallman-report.org/old-card.png" alt="A picture of Richard Stallman’s original “pleasure card” with sensitive details retracted. The headline reads “Sharing good books, good food and exotic music and dance; tender embraces; unusual sense of humor”"></p>
<p>It has been suggested that Stallman gives these cards to people regardless of
gender, but that when Stallman hands this card to women he often does so to
supplement a romantic or sexual proposition. The editors reached out to a woman
who received a “pleasure card” from Stallman for an interview, who we will refer
to as Ms. W. The editors have independently corroborated Ms. W’s testimony.</p>
<blockquote>
<p>I was at one of my first events as a speaker, at the speaker’s dinner.
Stallman was there and he approached me to chat. He spent a few minutes
talking about himself, showing no interest in me or what I was working on. He
didn’t ask about my work, or my situation, he just wanted to… pick me up.
I got the impression that he just assumed that he was entitled to my attention
based on his fame and reputation.</p>
<p>He hit on me for a few minutes and handed me his “pleasure card”, then told me
he would be around later that evening – I just thought, “yeah, and I’ll be
hanging out with my spouse and kid”. He moved to touch my arm, and I backed
off and avoided him for the rest of the event.</p>
<p>It didn’t even feel like he was particularly attracted to me – it felt like I
was just a woman under 40 and that was enough.</p>
</blockquote>
<p>The editors note that the event in question had an anti-harassment policy. Ms. W
elaborated on her thoughts after the fact:</p>
<blockquote>
<p>I mean, I’m middle aged, I brought my kid – this wasn’t my first rodeo. And
it just felt so inappropriate in a professional context. If I had met Stallman
in another context and we weren’t “coworkers”, in a way, it would have been…
unwanted, but not inappropriate. There’s a difference between, like, some of
us clicked and went out for drinks and it was flirty, and this taking place at
the speaker’s dinner. And what made it so inappropriate was the power
differential, of handing this to a relatively unknown woman when you’re
Richard Stallman and you’re giving the keynote.</p>
<p>I’ve been hit on at events before – but it often felt like a peer-to-peer
sort of thing, whereas the proposition from Stallman was more… “do you want
to be an acolyte?” There was some kind of power dynamic at play. He was 20, 25
years older than me – it was like your dad was hitting on you. It didn’t feel
like he was my peer, and we hadn’t talked enough to register if I was actually
interested in him at all.</p>
</blockquote>
<p>Ms. W notes that she attended a second event with Richard Stallman a year later,
where she indicates that Stallman participated in writing the code of conduct,
then violated that code of conduct when performing his <a href="#emacs-virgin-incidents">“St. Ignutius”
routine</a>, as well as at other occasions. Ms. W reported
her concerns to the event organizers, her testimony corroborated by Matthew
Garrett (member of the FSF board of directors at the time), and was met with
disbelief. Ms. W explains that the organizers were aware of Stallman’s
reputation at the time, and they stated “He’s one of those neckbeardy guys, but
we think he’ll behave himself. We talked about it.”</p>
<p>The editors have found a photograph which is alleged to be Stallman’s new
business card circa 2023, with the reference to “tender embraces” removed:</p>
<p><img src="https://stallman-report.org/new-card.png" alt="A picture of Richard Stallman’s current business card with sensitive details retracted. The headline now reads “sharing good books, good food, and far-away music &amp; dance; thoughtful and emotional conversation; unusual sense of humor”"></p>
<h3 id="testimonies-of-former-fsf-staff">
	Testimonies of former FSF staff
	
	<a href="#testimonies-of-former-fsf-staff"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>Georgia Young was the Program Manager for the FSF between 2015 and 2018 and
testified to Stallman’s conduct and character on Twitter in March 2021.</p>
<blockquote>
<p>I worked at the FSF from 2015-2018 &amp; was shop steward for a while. I recall
having a months-long conversation with [Executive Director] John Sullivan
about why racist &amp; sexist ‘hacker humor’ from the 90s needed to be removed
from gnu.org. RMS didn’t get why it was harmful.</p>
<p>The abortion joke<sup id="fnref:42"><a href="#fn:42" role="doc-noteref">42</a></sup> (‘contributed’ by RMS) in a technical
manual? He threw a fit when it was removed. (…)</p>
<p>The thing that (people) who have never had to actually work with RMS don’t
understand is that MANY people who deeply respected him tried to help him
learn to not objectify women, shout over others at Libreplanet as if it was
his birthday party, (and) stop shit like ’emacs virgins’.</p>
</blockquote>
<p>– <a href="https://x.com/georgialyle/status/1374504389155508232">@georgialyle on Twitter, 24 March 2021</a></p>
<p>Paul Fisher worked for 3 years on the staff of the Free Software Foundation and
worked as a volunteer for 6 years, ceasing his involvement in 2004. Paul
testified to his experiences on Twitter in March 2021:</p>
<blockquote>
<p>I worked at the FSF for 3 years and volunteered for over 6 years — that ended
in 2004. I witnessed misogyny, sexual objectification, and abuse carried out
by RMS. I banded together with my coworkers, formed a union, negotiated a
contract, and was elected shop steward.</p>
<p>While RMS started the free software movement and the GNU GPL was a
groundbreaking document, the community still has a right to hold him to
account for his abhorrent actions and harmful speech. RMS should not be part
of the FSF.</p>
</blockquote>
<p>– <a href="https://x.com/paulnivin/status/1374499598853545986">@paulnivin on Twitter, 24 March 2021</a></p>
<p>Paul also explained a few days later that the formation of the FSF staff union
was motivated by Stallman’s poor conduct.</p>
<blockquote>
<p>RMS created non-safe spaces at both MIT &amp; the FSF. When I was at the FSF, RMS
had little to no empathy for the staff. The FSF was not a healthy, functional
workplace. We formed a union to help protect ourselves from RMS — he
controlled our pay, benefits, and workplace conditions.</p>
<p>Everything was controlled by RMS — not the executive director, and not the
board. The union helped turn FSF employment into what most people think of as
a “normal” office job. It didn’t fix everything. Some of the issues that we
did fix:</p>
<p>RMS did not believe in providing raises — prior cost of living adjustments
were a battle and not annual. RMS believed that if a precedent was created for
increasing wages, the logical conclusion would be that employees would be paid
infinity dollars and the FSF would go bankrupt.</p>
<p>RMS did not believe in providing bereavement leave. What if all your close
friends and family die one after another? It’s conceivable you would be gone
from the office for days, or weeks, if not months. What if you lie about who
is dying?</p>
<p>RMS would often throw tantrums and threaten to fire employees for perceived
infractions. FSF staff had to show up to work each day, not knowing if RMS had
eliminated their position the night before.</p>
<p>Respectively, the union provided a formula for allocating a portion of any
budget surplus to COLAs and wage increases, bereavement leave, and progressive
discipline for workers, ensuring that union employees could not be fired at
RMS’ whim.</p>
<p>RMS has not apologized for the harm he’s caused. Both MIT &amp; the FSF
successfully separated themselves from RMS in 2019. Why did the secret group
of voting FSF members reelect him to the board? Why.</p>
</blockquote>
<p>– <a href="https://x.com/paulnivin/status/1377079987950395393">@paulnivin on Twitter, 31 March 2021</a></p>
<p>The allegation that the FSF staff union was formed due to Stallman’s conduct is
corroborated by David Turner, founder of the FSF’s GPL Compliance Labs:</p>
<blockquote>
<p>Funny confluence of RMS and tech union tweets today. We unionized FSF, in
large part, because RMS.</p>
</blockquote>
<p>– <a href="https://x.com/NovalisDMT/status/1172573166956437505">@NovalisDMT on Twitter, 13 September 2019</a></p>
<p>Matthew Garret, member of the FSF board of directors between 2014 and 2017 and
winner of the FSF Award for the Advancement of Free Software wrote the
following:</p>
<blockquote>
<p>I know of at least one other case where Stallman has decided to protect an
abuser. (…)</p>
<p>Free software is an amazing thing, and [Richard Stallman] is a liability
towards it.</p>
<p>His refusal to take action and insistence on making excuses for an abuser is
why I quit the FSF board. The FSF’s former general counsel threatened a board
member at an FSF event. [Stallman] threatened to overrule staff if they
attempted to enforce the event code of conduct and refused to tell the
abuser’s employer.</p>
</blockquote>
<p>– <a href="https://x.com/mjg59/status/1172286576082214912">@mjg59 on Twitter, 13 September 2019</a></p>

<h2 id="misconduct-of-the-fsf-board-of-directors">
	Misconduct of the FSF board of directors
	
	<a href="#misconduct-of-the-fsf-board-of-directors"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h2>
<p>This report alleges misconduct on the part of the 2019 FSF board of directors,
and that the present-day board of directors is responsible both for enabling
Richard Stallman and for failing to provide a workplace free of sexual
harassment under federal and Massachusetts law.</p>
<p>The composition of the board in 2019, when the question of Stallman’s continued
role in the FSF was under discussion, was as follows:<sup id="fnref:43"><a href="#fn:43" role="doc-noteref">43</a></sup></p>
<ul>
<li>Alexandre Oliva</li>
<li>Benjamin Mako Hill</li>
<li>Bradley Kuhn</li>
<li>Geoffrey Knauth</li>
<li>Gerald Jay Sussman</li>
<li>Henry Poole</li>
<li>Kat Walsh</li>
<li>Richard Stallman</li>
</ul>
<p>This report has reason to believe that Bradley Kuhn, Kat Walsh, and Benjamin
Mako Hill were not party to the misconduct of the 2019 board of directors. Mr.
Kuhn’s public statements following his ejection from the board of directors have
been a valuable source in the preparation of this report. Ms. Walsh voted
against Stallman’s return to office and resigned from her position on the Board
of Directors a few days following Stallman’s return.<sup id="fnref:44"><a href="#fn:44" role="doc-noteref">44</a></sup> Mr. Hill also
publicly spoke against Stallman’s re-instatement and quit his positions at the
FSF.<sup id="fnref:45"><a href="#fn:45" role="doc-noteref">45</a></sup></p>
<p>Present-day members of the FSF Board of Directors are as
follows:<sup id="fnref:46"><a href="#fn:46" role="doc-noteref">46</a></sup></p>
<ul>
<li>Christina Haralanova</li>
<li>Geoffrey Knauth</li>
<li>Gerald Jay Sussman</li>
<li>Henry Poole</li>
<li>Ian Kelling</li>
<li>John Gilmore</li>
<li>Maria Chiara Pievatolo</li>
<li>Richard M. Stallman</li>
</ul>
<h3 id="lack-of-transparency-in-governance">
	Lack of transparency in governance
	
	<a href="#lack-of-transparency-in-governance"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>The Free Software Foundation, like most non-profits, maintains a Board of
Directors which is responsible for directing its activities. The members of the
Board of Directors are disclosed in the FSF’s annual filings<sup id="fnref:47"><a href="#fn:47" role="doc-noteref">47</a></sup> and are
clearly enumerated on the FSF’s website.<sup id="fnref1:46"><a href="#fn:46" role="doc-noteref">46</a></sup></p>
<p>The governance of the FSF is also subject to a group of “Voting
Members”.<sup id="fnref:48"><a href="#fn:48" role="doc-noteref">48</a></sup> At the time of writing, this group is composed of the
following members:</p>
<ul>
<li>Alexandre Oliva</li>
<li>Christina Haralanoa</li>
<li>Geoffrey Knauth</li>
<li>Gerald Jay Sussman</li>
<li>Henry Poole</li>
<li>Ian Kelling</li>
<li>John Gilmore</li>
<li>Maria Chiara Pievatolo</li>
<li>Odile Bénassy</li>
<li>Richard M. Stallman</li>
</ul>
<p>Information about the Voting Members on the FSF’s website and has only been
available since 2021, following Richard Stallman’s return to the Board of
Directors. According to the FSF, the primary function of the Voting Members is
electing the Board of Directors.</p>
<p>According to public statements from the FSF, Richard Stallman resigned from all
positions of governance on September 17th, 2019.<sup id="fnref:49"><a href="#fn:49" role="doc-noteref">49</a></sup> From this point
until Stallman’s return to the Board of Directors on April 12th, 2021, all
public statements from the FSF supported the conclusion that Stallman was no
longer involved in the governance of the Free Software Foundation.</p>
<p>However, the testimony of Bradley Kuhn alleges that Stallman never resigned as a
Voting Member, and remained a Voting Member throughout the period of his
resignation.<sup id="fnref:50"><a href="#fn:50" role="doc-noteref">50</a></sup></p>
<p>This report alleges that the FSF has maintained, particularly between the events
of 2019 and 2021, a “shadow government” which is subject to a lack of
transparency in their role and operations, and alleges misconduct in misleading
the public on the nature of Stallman’s role in the FSF between 2019 and 2021.</p>
<h3 id="knowledge-of-stallmans-misconduct">
	Knowledge of Stallman’s misconduct
	
	<a href="#knowledge-of-stallmans-misconduct"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>Bradley Kuhn was the Executive Director of the Free Software Foundation between
2001 and 2005, and served on the board of directors from March 2010 to October
2019.<sup id="fnref:51"><a href="#fn:51" role="doc-noteref">51</a></sup> Following <a href="#ejection-of-bradley-kuhn">Mr. Kuhn’s expulsion</a>,
he issued a public statement on the 2019 controversy<sup id="fnref1:50"><a href="#fn:50" role="doc-noteref">50</a></sup> where he
asserts that Stallman’s behavior was well-known to the FSF for at least two
years prior to the public outcry; other sources suggest it was known for longer:</p>
<blockquote>
<p>For the last two years, I had been a loud internal voice in the FSF leadership
regarding RMS’ Free-Software-unrelated public statements; I felt strongly that
it was in the best interest of the FSF to actively seek to limit such
statements, and that it was my duty to FSF to speak out about this within the
organization. (…)</p>
<p>I attempted to argue with him at length to convince him that some of his
positions were harmful to sexual assault survivors and those who are
sex-trafficked, and to the people who devote their lives in service to such
individuals. More importantly to the FSF, I attempted to persuade RMS that
launching a controversial campaign on sexual behavior and morality was counter
to his and FSF’s mission to advance software freedom, and told RMS that my
duty as an FSF Director was to assure the best outcome for the FSF, which
<abbr title="in my opinion">IMO</abbr> didn’t include having a leader who made
such statements.</p>
</blockquote>
<h3 id="ejection-of-bradley-kuhn">
	Ejection of Bradley Kuhn
	
	<a href="#ejection-of-bradley-kuhn"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>In October 2019, Bradley Kuhn was removed from the Free Software Foundation
Board of Directors and Voting Members. Mr. Kuhn’s public remarks on the matter
provide insight into the misconduct of the board during the scandal of
2019.<sup id="fnref2:50"><a href="#fn:50" role="doc-noteref">50</a></sup> Quoting Mr. Kuhn:</p>
<blockquote>
<p>I was narrowly (by exactly one vote) voted out (of all my FSF roles) by FSF’s
Voting Members.</p>
<p>I was voted out for various reasons. The most relevant reason was a
fundamental disagreement about the criteria and requirements for RMS’ return
to the FSF Board of Directors. In particular, during September-October 2019, I
was insisting that one qualification for reinstatement was a complete,
unqualified apology for RMS’ September 2019 statements that (a) “she [Virginia
Giuffre] presented herself to him [Marvin Minksy][sic] as entirely willing”,
and (b) Giuffre (who was sex-trafficed by Jeffrey Epstein) committed “an
injustice” by accusing Minksy[sic] of sexual assault in her deposition.</p>
</blockquote>
<p>The FSF’s “Voting Members” are responsible for electing the FSF Board of
Directors. Mr. Kuhn reports that Richard Stallman was a Voting Member at this
time.</p>
<p>We conclude from this testimony that Mr. Kuhn was ejected from the Free Software
Foundation governance for the apparent purpose of facilitating Richard
Stallman’s eventual return to the Board of Directors, in particular that his
return not be contingent on apologizing for his behavior, disenfranchising Mr.
Kuhn of his legitimate vote on the matter of the membership of the Board of
Directors, and demonstrates that the FSF was preparing for Stallman’s
re-instatement even as they were facilitating his resignation.</p>
<h3 id="failure-to-account-for-sexual-harassment">
	Failure to account for sexual harassment
	
	<a href="#failure-to-account-for-sexual-harassment"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>We assert that the Free Software Foundation’s consistent protection for Richard
Stallman despite prior knowledge of allegations of his misconduct signals
incompetence with respect to their legal obligation to maintain a workplace free
of sexual harassment. Allegations of misconduct while Stallman was conducting
official FSF business are enumerated above, but did not appear to instigate an
investigation by FSF leadership. We contend that an appropriate response to the
allegations would have been to perform an investigation similar to one
undertaken by the editors of this report, which would have presented a clear
case for Stallman’s removal.</p>
<p>We also argue that the FSF has created a “hostile work environment” under US
and Massachusetts law. In one respect, we hold that retaining in leadership an
individual who does not understand sexual harassment or sexual assault and
continuously makes public statements to this effect constitutes a hostile work
environment. We also argue that the 2019 expulsion of Bradley Kuhn constitutes
illegal retaliation under Title VII of the United States Civil Rights Act of
1964.</p>
<p>Moreover, regardless of the conclusion of proceedings following the 2019 scandal
involving Richard Stallman, we feel that the FSF leadership failed to implement
prudent steps to address sexual harassment in its workplace at a moment when it
would have been obvious to do so.</p>
<p>It is unknown to the authors of this report if the FSF is in full compliance
with Massachusetts law regarding sexual harassment, in particular if they have
prepared a policy regarding sexual harassment, have established processes for
reporting sexual harassment, or annually provide materials to this effect to all
employees. However, we note that Massachusetts provides optional recommendations
that the FSF does not appear to have implemented in the aftermath of the 2019
scandal:</p>
<blockquote>
<p>Employers and labor organizations are encouraged to conduct an education and
training program for new employees and members, within one year of
commencement of employment or membership, which includes at a minimum the
information set forth in this section. Employers are encouraged to conduct
additional training for new supervisory and managerial employees and members
within one year of commencement of employment or membership, which shall
include at a minimum the information set forth in subsection (b), the specific
responsibilities of supervisory and managerial employees and the methods that
such employees should take to ensure immediate and appropriate corrective
action in addressing sexual harassment complaints. Employers, labor
organizations and appropriate state agencies are encouraged to cooperate in
making such training available.<sup id="fnref:52"><a href="#fn:52" role="doc-noteref">52</a></sup></p>
</blockquote>
<p>We note that Richard Stallman recieved mandatory sexual harassment training at
MIT in September 2018, which we discuss <a href="#ref-sexual-harassment-training">earlier in the
report</a>. Everfi, the company responsible for
the training program Stallman recieved, provides course materials which are
compliant with California AB 1825 requirements on mandatory
training.<sup id="fnref:53"><a href="#fn:53" role="doc-noteref">53</a></sup> These requirements include that training
materials cover, among other things, identifying retalitory behavior under
federal law and an employer’s obligation to complete an investigation upon
receiving a report of sexual harassment. It is reasonable to assume that
Stallman is familiar with these legal norms.</p>
<p>Gerald Sussman, also on the FSF Board of Directors during the 2019 scandal, was
also a member of the MIT faculty during its 2018 mandatory training program and
presumably received similar training.</p>
<h3 id="2020-form-990-irs-filing">
	2020 Form 990 IRS filing
	
	<a href="#2020-form-990-irs-filing"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>This report also draws attention to the Free Software Foundation’s 2020 Form 990
tax filing.<sup id="fnref:54"><a href="#fn:54" role="doc-noteref">54</a></sup></p>
<p>Richard Stallman has been reported on the FSF’s Form 990 filings as an officer
of the organization in every year except for 2020, following Stallman’s
ostensible removal from the governance of the FSF. However, if Bradley Kuhn’s
allegations that Stallman’s was as a Voting Member in this year are true, the
absence of Stallman in this filing may be fraudulent.</p>
<p>The IRS instructions for Form 990 in 2020 provide the following instructions for
supplying the list of Directors and other notable members of the organization:</p>
<blockquote>
<p>A “director or trustee” is a member of the organization’s governing body, but
only if the member has voting rights.<sup id="fnref:55"><a href="#fn:55" role="doc-noteref">55</a></sup></p>
</blockquote>
<p>The report also notes that several present-day FSF Voting Members are not
included on the Free Software Foundation’s Form 990 filing in 2022.</p>
<h3 id="regarding-the-fsf-codes-of-ethics">
	Regarding the FSF codes of ethics
	
	<a href="#regarding-the-fsf-codes-of-ethics"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>The report notes that the FSF has published two codes of ethics, respectively
applying to the Board of Directors<sup id="fnref:56"><a href="#fn:56" role="doc-noteref">56</a></sup> and its Voting
Members,<sup id="fnref:57"><a href="#fn:57" role="doc-noteref">57</a></sup> and respectively appearing in December 2021 and
July 2022. As such, neither were in force at the time the above reported
misconduct took place.</p>
<p>However, this report takes this opportunity to offer a retrospective analysis of
FSF board members’ and voting members’ 2019 conduct with respect to these codes
of ethics, as well as a contemporary analysis of their conduct.</p>
<p>On the subject of erroneous IRS filings, we find that treasurer Geoffrey Knauth
actions have contravened the following provision:</p>
<blockquote>
<p>Members of the board of directors will conduct the business affairs of the
organization in good faith and with honesty, integrity, due diligence, and
reasonable competence.</p>
</blockquote>
<p>If the failure to report the Voting Members on the FSF’s Form 990, previously
and to the present day, is part of a larger program of deliberately obscuring
the governance of the Free Software Foundation, we assert this contravenes the
principles of good faith, honesty, and integrity; if the filings are simply a
mistake and there is no broader objective to obscure the governance of the FSF,
we assert that this contravenes the principles of due diligence and reasonable
competence. To assist the reader in choosing a suitable interpretation, we
acknowledge that Bradley Kuhn’s statement reports that demands for transparency
were among the likely reasons he was expelled from his role.<sup id="fnref3:50"><a href="#fn:50" role="doc-noteref">50</a></sup></p>
<p>On the subject of Mr. Kuhn’s expulsion from the FSF governing bodies, we
consider the following principle of the Voting Member’s code of ethics:</p>
<blockquote>
<p>A Voting Member must act in good faith in accord with the regulations of the
Free Software Foundation, including its articles of incorporation and its
bylaws.</p>
</blockquote>
<p>We assert that the removal of Mr. Kuhn for the purpose of installing an
electorate that would re-instate Richard Stallman on favorable terms is not
acting in good faith and hold the quorum accountable to this.</p>
<p>It is noted by Mr. Kuhn that Richard Stallman was among the Voting Members that
voted for Mr. Kuhn’s expulsion. We find that this contravenes the following
provision of the code of ethics for board members:</p>
<blockquote>
<p>Board members shall all avoid placing–and the appearance of placing–one’s
own self interest or any third-party interest, including the interests of
associate members, above that of the organization as a whole.</p>
</blockquote>
<p>We argue that the continued support of the Free Software Foundation Board of
Directors for Richard Stallman’s platform places the interests of Richard
Stallman above that of the organization as a whole, in particular with respect
to the formal policy of non-cooperation many institutions in the free software
community have adopted with respect to the FSF so long as Stallman remains on
the board.</p>
<p>We also note that Richard Stallman’s prolonged political program in defense of
sexual violence contravenes the following provision:</p>
<blockquote>
<p>Members of the FSF’s board of directors acknowledge that their statements and
actions have greater potential to reflect broadly on the organization because
of their leadership position and will take seriously their position of public
visibility and trust.</p>
</blockquote>
<p>This report notes that the FSF defines no particular recourse for violations of
its codes of ethics.</p>


<p>Following the 2021 re-instatement of Richard Stallman to his position on the
Free Software Foundation board of directors, numerous individuals and
institutions in the free software community spoke out in protest. Our report
focuses on institutions, on the basis that institutional policies of
non-cooperation with the FSF over the role of Richard Stallman is a significant
obstacle to the objectives of Free Software Foundation.</p>
<p>In 2021, 61 institutions and 3,003 individuals signed an open letter calling for
Stallman to be removed from all leadership positions, and calling for the board
of directors of the Free Software Foundation to resign.<sup id="fnref:58"><a href="#fn:58" role="doc-noteref">58</a></sup> An
additional 33 GNU project maintainers and developers collectively called for
Stallman’s removal in 2019.<sup id="fnref:59"><a href="#fn:59" role="doc-noteref">59</a></sup></p>
<p>This report highlights the following institutions that have explicitly withdrawn
financial support and/or adopted a policy of non-cooperation with the Free
Software Foundation over concerns regarding Richard Stallman:</p>
<ul>
<li><a href="https://codema.in/d/Xdi7EPS9/statement-on-richard-stallman-rejoining-the-fsf-board">Free Software Community of India</a></li>
<li><a href="https://fsfe.org/news/2021/news-20210324-01.html">Free Software Foundation Europe</a></li>
<li><a href="https://x.com/bad_packets/status/1374081329340456962">Okta Bad Packets</a></li>
<li><a href="https://www.outreachy.org/blog/2021-03-23/fsf-participation-barred/">Outreachy</a></li>
<li><a href="https://www.redhat.com/en/blog/red-hat-statement-about-richard-stallmans-return-free-software-foundation-board">Red Hat</a></li>
<li><a href="https://fedoramagazine.org/fedora-council-statement-on-richard-stallman-rejoining-fsf-board/">The Fedora Project</a></li>
<li><a href="https://opensource.org/blog/OSI_Response">The Open Source Initiative</a></li>
<li><a href="https://x.com/torproject/status/1374754834050654212">Tor Foundation</a></li>
<li><a href="https://news.opensuse.org/2021/04/12/a-message-from-the-opensuse-board/">openSUSE</a></li>
</ul>

<h2 id="recommendations-for-reconciliation-and-closure">
	Recommendations for reconciliation and closure
	
	<a href="#recommendations-for-reconciliation-and-closure"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h2>
<p>This report provides the following recommendations to parties involved for
seeking reconciliation and closure for the problems enumerated herein.</p>
<h3 id="recommendations-to-richard-stallman">
	Recommendations to Richard Stallman
	
	<a href="#recommendations-to-richard-stallman"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>To Mr. Stallman, we offer the following advice:</p>
<ol>
<li>We urge you to issue a detailed retraction of the positions enumerated in
this publication and cease all future statements of this nature, and to
demonstrate your renewed understanding of the subject matter.</li>
<li>We urge you to meaningfully apologize for the material harm you’ve done in
the course of your political work to defend sexual violence and undermine the
experiences of victims of sexual violence.</li>
<li>We urge you to remove all political notes and articles cited by this report
from your website, or update them with a link to your retraction.</li>
<li>We ask you to step down from all positions at the FSF and the GNU project and
entrust it to a new generation of leaders.</li>
</ol>
<p>We are of the unfortunate opinion that the scope and extent of your misconduct
disqualifies you from formal positions of power within our community
indefinitely. Your influence on the free software community is profound and
immeasurable, as is the harm you have done to victims of sexual violence. If you
wish to cement the positive parts of your legacy, you must contend with the
consequences of your violent political program.</p>
<h3 id="recommendations-to-the-fsf-leadership">
	Recommendations to the FSF leadership
	
	<a href="#recommendations-to-the-fsf-leadership"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>We offer the following recommendations to the FSF Board of Directors and Voting
Members.</p>
<ol>
<li>
<p>To Voting Members, if Richard Stallman fails to step down of his own accord,
we urge you to convene a meeting of voting members at the earliest possible
occasion in accordance with section 7 of the FSF by-laws for the purpose of
removing Richard Stallman from both the Voting Members and the Board of
Directors.</p>
</li>
<li>
<p>If Richard Stallman fails to retract his political statements on sexual
violence, we encourage you to release a statement denouncing them.</p>
</li>
<li>
<p>To all members of the present-day Voting Members and Board of Directors who
were contemporaneous with the 2019 scandal and the associated patterns of
misconduct, we urge you to step down from your posts and allow new leaders to
fill your roles. Namely:</p>
<ul>
<li>Alexandre Oliva</li>
<li>Geoffrey Knauth</li>
<li>Gerald Sussman</li>
<li>Henry Poole</li>
</ul>
<p>In particular we call upon Mr. Knauth to uphold his 2021 pledge to resign “as
soon as there is a clear path for new leadership assuring continuity of the
FSF’s mission and compliance with fiduciary
requirements”.<sup id="fnref:60"><a href="#fn:60" role="doc-noteref">60</a></sup></p>
</li>
<li>
<p>The leadership is called upon to improve the FSF codes of ethics to prevent
future errors of this sort, including the institution of reasonable measures
of recourse in the event of violations of the codes of ethics.</p>
</li>
<li>
<p>The leadership is called upon to implement a comprehensive program of sexual
harassment training within the Free Software Foundation, as well as policies
and procedures for handling allegations of sexual harassment.</p>
</li>
<li>
<p>All parties complicit in the platforming of Richard Stallman are encouraged
to consider publishing a written apology for their conduct.</p>
</li>
</ol>
<p>We strongly urge you to take these actions in the best interests of the free
software community and the future of the Free Software Foundation.</p>
<h3 id="recommendations-to-the-gnu-project">
	Recommendations to the GNU project
	
	<a href="#recommendations-to-the-gnu-project"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h3>
<p>To the leadership of the GNU project, we recommend the following steps:</p>
<ol>
<li>The removal of Stallman as the “chief GNUisance” of the GNU project if he
fails to step down of his own accord.</li>
<li>Replacing the transphobic <a href="https://www.gnu.org/philosophy/kind-communication.en.html">GNU Kind Communication Guidelines</a>
authored by Stallman with a Code of Conduct which better addresses the needs
and safety of the community.</li>
</ol>

<p>If the leadership of the Free Software Foundation fails to account for these
problems, we call upon the community to boycott the FSF. Consider cancelling
your membership fees. We also encourage members of the community to voice their
support for these calls to action, disseminate our report as broadly as
possible, and raise our voices in condemnation of sexual violence and those who
protect perpetrators of it.</p>
<h2 id="acknowledgements">
	Acknowledgements
	
	<a href="#acknowledgements"><img src="https://stallman-report.org/link.svg" alt=""></a>
</h2>
<p>This report was prepared in collaboration with a number of researchers,
advisors, and victims of sexual violence. The editors acknowledge that the
process of researching the material for this report or testifying to experiences
of sexual violence and harassment was traumatic for those involved, and we thank
them for their bravery and cooperation.</p>

<p>The editors of this report may be reached by email to
<a href="mailto:editors@stallman-report.org">editors@stallman-report.org</a>.</p>
<p>Our PGP public key is:</p>
<pre tabindex="0"><code>-----BEGIN PGP PUBLIC KEY BLOCK-----

mDMEZwPpMxYJKwYBBAHaRw8BAQdAExd8GOPBecIUkRGuuH7EN/z5bxSmGVexI6MC
1+Ko4XW0NVN0YWxsbWFuIFJlcG9ydCBlZGl0b3JzIDxlZGl0b3JzQHN0YWxsbWFu
LXJlcG9ydC5vcmc+iJkEExYKAEEWIQQUUS1wAhze9vx2OMAoMLLhOrisUAUCZwPp
MwIbAwUJBaOagAULCQgHAgIiAgYVCgkICwIEFgIDAQIeBwIXgAAKCRAoMLLhOris
UPNyAQCQVLGVof5sv+gV77p1GFnw6Uw8sE821378OCC+uUxo9AEA1WgrLe74OMd3
TSnJnK+PFDHwMhtCdvlLwirNXoE8Hwy4OARnA+kzEgorBgEEAZdVAQUBAQdA/cqY
UGagKg2nmyiI0EvItMKZMo9NF3bSa+rtrvoel2MDAQgHiH4EGBYKACYWIQQUUS1w
Ahze9vx2OMAoMLLhOrisUAUCZwPpMwIbDAUJBaOagAAKCRAoMLLhOrisUMu+AP4x
k10EFSC6DndhVSaI7CsuTZaopnxDUn5a+Tw+PSWtfQD+P5v9ydYqFZLKs+YALvb1
D1YiaJ+qmhQ+TEF64FX7xwM=
=DOmk
-----END PGP PUBLIC KEY BLOCK-----
</code></pre>


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: X11 tool to share a screen area in any video meeting (217 pts)]]></title>
            <link>https://github.com/splitbrain/clipscreen</link>
            <guid>41837204</guid>
            <pubDate>Mon, 14 Oct 2024 13:11:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/splitbrain/clipscreen">https://github.com/splitbrain/clipscreen</a>, See on <a href="https://news.ycombinator.com/item?id=41837204">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">clipscreen</h2><a id="user-content-clipscreen" aria-label="Permalink: clipscreen" href="#clipscreen"></a></p>
<p dir="auto">clipscreen is a simple application that creates a virtual monitor that mirrors a portion of your screen. A green rectangle highlights the specified area.</p>
<p dir="auto">Why's this useful? You can use any screen sharing tool (Google Meet, Microsoft Teams, Jitsi Meet, etc.) to share the virtual monitor instead of your entire screen. No need to share individual windows and having to switch between them, just move any window you wannt to share into the green border.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Compile</h2><a id="user-content-compile" aria-label="Permalink: Compile" href="#compile"></a></p>
<p dir="auto">Ensure you have the following installed on your system:</p>
<ul dir="auto">
<li>X11 development libraries</li>
<li>Cairo graphics library</li>
<li>A C++ compiler (e.g., g++)</li>
</ul>
<p dir="auto">Then simply run the following command to compile the application:</p>

<p dir="auto">Note: The application has only been tested on Linux and xorg. I doubt it will work on any other system.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Run the compiled executable with the following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./clipscreen <width> <height> <x> <y>"><pre>./clipscreen <span>&lt;</span>width<span>&gt;</span> <span>&lt;</span>height<span>&gt;</span> <span>&lt;</span>x<span>&gt;</span> <span>&lt;</span>y<span>&gt;</span></pre></div>
<ul dir="auto">
<li><code>&lt;width&gt;</code>: The width of the overlay and virtual monitor.</li>
<li><code>&lt;height&gt;</code>: The height of the overlay and virtual monitor.</li>
<li><code>&lt;x&gt;</code>: The x-coordinate of the top-left corner of the overlay and virtual monitor.</li>
<li><code>&lt;y&gt;</code>: The y-coordinate of the top-left corner of the overlay and virtual monitor.</li>
</ul>
<p dir="auto">For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./clipscreen 800 600 100 100"><pre>./clipscreen 800 600 100 100</pre></div>
<p dir="auto">This command will create an 800x600 overlay window starting at position (100, 100) on your screen.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Termination</h2><a id="user-content-termination" aria-label="Permalink: Termination" href="#termination"></a></p>
<p dir="auto">To terminate the application, press <code>Ctrl+C</code> in the terminal where the application is running.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Copyright 2024 Andreas Gohr <a href="mailto:andi@splitbrain.org">andi@splitbrain.org</a></p>
<p dir="auto">Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p>
<p dir="auto">The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p>
<p dir="auto">THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Upgrading Uber's MySQL Fleet (205 pts)]]></title>
            <link>https://www.uber.com/en-JO/blog/upgrading-ubers-mysql-fleet/</link>
            <guid>41836748</guid>
            <pubDate>Mon, 14 Oct 2024 12:08:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.uber.com/en-JO/blog/upgrading-ubers-mysql-fleet/">https://www.uber.com/en-JO/blog/upgrading-ubers-mysql-fleet/</a>, See on <a href="https://news.ycombinator.com/item?id=41836748">Hacker News</a></p>
Couldn't get https://www.uber.com/en-JO/blog/upgrading-ubers-mysql-fleet/: Error: Request failed with status code 404]]></description>
        </item>
        <item>
            <title><![CDATA[Blocking the "Sign in with Google" Prompt (2023) (122 pts)]]></title>
            <link>https://superuser.com/questions/1773208/how-can-i-block-the-sign-in-with-google-prompt-on-websites</link>
            <guid>41835217</guid>
            <pubDate>Mon, 14 Oct 2024 07:48:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://superuser.com/questions/1773208/how-can-i-block-the-sign-in-with-google-prompt-on-websites">https://superuser.com/questions/1773208/how-can-i-block-the-sign-in-with-google-prompt-on-websites</a>, See on <a href="https://news.ycombinator.com/item?id=41835217">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
                    

<p>On many websites that require login, those annoying Google login prompts appear:</p>
<p><a href="https://i.sstatic.net/gY1Yj.png" rel="noreferrer"><img src="https://i.sstatic.net/gY1Yj.png" alt="&quot;Sign in to unlock deals and discounts!&quot; popup"></a></p>
<p>There are several tutorials on the Internet on how to avoid this, for example, <a href="https://www.howtogeek.com/735152/how-to-turn-off-the-sign-in-with-google-prompt-on-websites/" rel="noreferrer">this one on How-To Geek</a>, which suggest disabling an option in the Google account. However, this doesn't work, since mine is not enabled and never was:</p>
<p><a href="https://i.sstatic.net/R84pR.png" rel="noreferrer"><img src="https://i.sstatic.net/R84pR.png" alt="&quot;Allow Google to offer a faster way to sign in with your Google Account on supported third-party sites&quot; (in German)"></a></p>
<p>Note, that this only happens with Firefox (110.0.1 (64-bit), AdBlocker ultimate v 3.7.21 installed); if I use the Brave browser (version 1.49.120 Chromium: 111.0.5563.64 (Official Build) (64-bit)), which is known to block ads by default, they are not displayed. I'm on <a href="https://en.wikipedia.org/wiki/Ubuntu_version_history#Ubuntu_22.04_LTS_(Jammy_Jellyfish)" rel="noreferrer">Ubuntu 22.04.2</a> LTS (Jammy Jellyfish) with Linux kernel 5.19.0-35-generic x86_64.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gosub – An open-source browser engine (131 pts)]]></title>
            <link>https://github.com/gosub-io/gosub-engine</link>
            <guid>41835040</guid>
            <pubDate>Mon, 14 Oct 2024 07:17:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/gosub-io/gosub-engine">https://github.com/gosub-io/gosub-engine</a>, See on <a href="https://news.ycombinator.com/item?id=41835040">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Gosub: Gateway to Optimized Searching and Unlimited Browsing</h2><a id="user-content-gosub-gateway-to-optimized-searching-and-unlimited-browsing" aria-label="Permalink: Gosub: Gateway to Optimized Searching and Unlimited Browsing" href="#gosub-gateway-to-optimized-searching-and-unlimited-browsing"></a></p>
<p dir="auto">This repository holds the Gosub browser engine. It will become a standalone library that can be used by other projects
but will ultimately be used by the Gosub browser user-agent. See the <a href="#about">About</a> section for more information.</p>
<p dir="auto">Join us at our development <a href="https://chat.developer.gosub.io/" rel="nofollow">Zulip chat</a>!</p>
<p dir="auto">For more general information you can also join our <a href="https://chat.gosub.io/" rel="nofollow">Discord server</a>.</p>
<p dir="auto">If you are interested in contributing to Gosub, please check out the <a href="https://github.com/gosub-io/gosub-engine/blob/main/CONTRIBUTING.md">contribution guide</a>!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">About</h2><a id="user-content-about" aria-label="Permalink: About" href="#about"></a></p>
<p dir="auto">This repository is part of the Gosub browser engine project. This is the main engine that holds the following components:</p>
<ul dir="auto">
<li>HTML5 tokenizer / parser</li>
<li>CSS3 tokenizer / parser</li>
<li>Document tree</li>
<li>Several APIs for connecting to javascript</li>
<li>Configuration store</li>
<li>Networking stack</li>
<li>Rendering engine</li>
<li>JS bridge</li>
</ul>
<p dir="auto">More will follow as the engine grows. The idea is that this engine will receive some kind of stream of bytes (most likely
from a socket or file) and parse this into a valid HTML5 document tree and CSS stylesheets.
From that point, it can be fed to a renderer engine that will render the document tree into a window, or it can be fed
to a more simplistic engine that will render it in a terminal. JS can be executed on the document tree and the document
tree can be modified by JS.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Status</h2><a id="user-content-status" aria-label="Permalink: Status" href="#status"></a></p>
<blockquote>
<p dir="auto">This project is in its infancy. There is no usable browser yet. However, you can look at simple html pages and parse
them into a document tree and do some initial rendering.</p>
</blockquote>
<p dir="auto">We can parse HTML5 and CSS3 files into a document tree or the respective css tree. This tree can be shown in the terminal
or be rendered in a very unfinished renderer. Our renderer cannot render everything yet, but it can render simple html
pages, sort of.</p>
<p dir="auto">We already implemented other parts of the engine, for a JS engine, networking stack, a configuration store and other
things however these aren't integrated yet. You can try these out by running the respective binary.</p>
<p dir="auto">We can render a part for our own <a href="https://gosub.io/" rel="nofollow">site</a>:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/gosub-io/gosub-engine/blob/main/resources/images/current_progress.png"><img src="https://github.com/gosub-io/gosub-engine/raw/main/resources/images/current_progress.png" alt="Gosub.io"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to run</h2><a id="user-content-how-to-run" aria-label="Permalink: How to run" href="#how-to-run"></a></p>
<details>
<summary> Installing dependencies </summary>
<p dir="auto">This project uses <a href="https://doc.rust-lang.org/cargo/" rel="nofollow">cargo</a> and <a href="https://www.rust-lang.org/tools/install" rel="nofollow">rustup</a>. First
you must install <code>rustup</code> at the link provided. After installing <code>rustup</code>, run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ rustup toolchain install 1.73
$ rustc --version
rustc 1.73.0 (cc66ad468 2023-10-03)"><pre>$ rustup toolchain install 1.73
$ rustc --version
rustc 1.73.0 (cc66ad468 2023-10-03)</pre></div>
<p dir="auto">Once Rust is installed, run this command to pre-build the dependencies:</p>

</details>
<p dir="auto">You can run the following binaries:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Command</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>cargo run -r --bin config-store</code></td>
<td>bin</td>
<td>A simple test application of the config store for testing purposes</td>
</tr>
<tr>
<td><code>cargo run -r --bin css3-parser</code></td>
<td>bin</td>
<td>Show the parsed css tree</td>
</tr>
<tr>
<td><code>cargo run -r --bin display-text-tree</code></td>
<td>bin</td>
<td>A simple parser that will try and return a textual presentation of the website</td>
</tr>
<tr>
<td><code>cargo run -r --bin gosub-parser</code></td>
<td>bin</td>
<td>The actual html5 parser/tokenizer that allows you to convert html5 into a document tree.</td>
</tr>
<tr>
<td><code>cargo run -r --bin html5-parser-test</code></td>
<td>test</td>
<td>A test suite that tests all html5lib tests for the treebuilding</td>
</tr>
<tr>
<td><code>cargo run -r --bin parser-test</code></td>
<td>test</td>
<td>A test suite for the parser that tests specific tests. This will be removed as soon as the parser is completely finished as this tool is for developement only.</td>
</tr>
<tr>
<td><code>cargo run -r --bin renderer</code></td>
<td>bin</td>
<td>Render a html page (WIP)</td>
</tr>
<tr>
<td><code>cargo run -r --bin run-js</code></td>
<td>bin</td>
<td>Run a JS file (Note: console and event loop are not yet implemented)</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">For running the binaries, take a look at a quick introduction at <a href="https://github.com/gosub-io/gosub-engine/blob/main/docs/binaries.md">/docs/binaries.md</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Benchmark and test suites</h2><a id="user-content-benchmark-and-test-suites" aria-label="Permalink: Benchmark and test suites" href="#benchmark-and-test-suites"></a></p>
<p dir="auto">To run the tests and benchmark suite, do:</p>
<div dir="auto" data-snippet-clipboard-copy-content="make test
cargo bench
ls target/criterion/report
index.html"><pre>make <span>test</span>
cargo bench
ls target/criterion/report
index.html</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Wasm</h2><a id="user-content-wasm" aria-label="Permalink: Wasm" href="#wasm"></a></p>
<p dir="auto">Our engine can also be compiled to WebAssembly. You need to use WasmPack for this. To build the Wasm version, run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="wasm-pack build --target web"><pre>wasm-pack build --target web</pre></div>
<p dir="auto">Afterwards you need to serve the small useragent around the wasm version in the <code>wasm/</code> directory. You can do this by</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd wasm
npm run dev  # you can also use `bun run dev`"><pre><span>cd</span> wasm
npm run dev  <span><span>#</span> you can also use `bun run dev`</span></pre></div>
<p dir="auto">To use this demo, you need to enable webgpu in chromium and disable the same origin policy.</p>
<div dir="auto" data-snippet-clipboard-copy-content="chromium --disable-web-security --enable-features=Vulkan --enable-unsafe-webgpu --user-data-dir=/tmp/chromium-temp-profile "><pre>chromium --disable-web-security --enable-features=Vulkan --enable-unsafe-webgpu --user-data-dir=/tmp/chromium-temp-profile </pre></div>
<p dir="auto">This command works on Linux only, if someone uses Windows or macOS, please open an PR!</p>
<p dir="auto">And then you have it! A browser in a browser:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/gosub-io/gosub-engine/blob/main/resources/images/browser-wasm-hackernews.png"><img src="https://github.com/gosub-io/gosub-engine/raw/main/resources/images/browser-wasm-hackernews.png" alt="Browser in browser"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing to the project</h2><a id="user-content-contributing-to-the-project" aria-label="Permalink: Contributing to the project" href="#contributing-to-the-project"></a></p>
<p dir="auto">We welcome contributions to this project but the current status makes that we are spending a lot of time researching,
building small proof-of-concepts and figuring out what needs to be done next. Much time of a contributor at this stage
of the project will be non-coding.</p>
<p dir="auto">We do like to hear from you if you are interested in contributing to the project and you can join us currently at
our <a href="https://chat.developer.gosub.io/" rel="nofollow">Zulip chat</a>!</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A review after using Rust on embedded in production for over a year (193 pts)]]></title>
            <link>https://blog.lohr.dev/embedded-rust</link>
            <guid>41834662</guid>
            <pubDate>Mon, 14 Oct 2024 06:06:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.lohr.dev/embedded-rust">https://blog.lohr.dev/embedded-rust</a>, See on <a href="https://news.ycombinator.com/item?id=41834662">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-content-parent"><p>When I mention that we use Rust on the embedded ESP32 platform the most common reaction is a jaw-dropping "This is possible?!". Yes, it is indeed possible! - And we at STABL Energy have used it successfully for over a year now. And because so many people seem interested in why we use Rust and how it is going for us, I decided to write this blog post.</p>
<blockquote>
<p>"Thanks to its direct access to both hardware and memory, Rust is well suited for embedded systems and bare-metal development." - <a target="_blank" href="https://github.blog/2023-08-30-why-rust-is-the-most-admired-language-among-developers/">GitHub</a></p>
</blockquote>
<p>It all started in 2022, with me getting annoyed by our C implementation of a small piece of software running on an ESP32 (a microcontroller with built-in internet connectivity). The purpose of the software was to read messages via UART (serial interface/protocol, often used to communicate between embedded devices) and send them to some cloud services via MQTT (messaging protocol, often used to communicate between IoT devices). Seems simple enough, right? Well, we had some additional requirements in terms of reliability, regarding the layout of the resulting JSON MQTT messages and concerning the content of the UART messages which is rather difficult to parse.</p>
<p>To give you some more context: I work for a startup named <a target="_blank" href="https://stabl.com/?ref=ml-blog">STABL Energy</a> where we build revolutionary energy storage systems, that are based on second-life batteries (batteries that were used in electric cars before). Instead of recycling them just now, we can use those perfectly fine batteries to (for example) back large PV farms and prolong their lives just a little bit more. The device I was talking about before is used to connect our battery storage systems to the cloud for monitoring and remote control. We also use it during development, to test new features in our system - so it really needs to be reliable, because we don't want to lose any data.</p>
<p><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1706812711815/53a4d26a-2e21-486c-b052-245e017cd584.png?auto=compress,format&amp;format=webp" alt=""></p>
<p>Our C implementation was a small (very prototype) software running on the <a target="_blank" href="https://www.espressif.com/en/products/socs/esp32">ESP32</a> platform. It had some serious runtime issues, preventing it from working reliably, that were hard to debug. And debugging on embedded is a lot more complicated than debugging software targeting desktop architectures. I have much respect for C (and even more for people programming in C), but I think its era is coming to an end. As I wrote in <a target="_blank" href="https://blog.lohr.dev/after-a-day-of-programming-in-zig">a previous blog post</a>, Zig could be a quite good modern replacement in the future. But Zig is rather new and at the time we worked with the C implementation, I didn't even know that Zig existed. However, I did a lot with Rust in personal projects at that time. The developer experience in Rust is just on the next level and the software you write is reliable without giving reliability, memory allocation etc. too much thought in the first place.</p>
<p><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1706813402492/571c6001-33b8-4c77-a2d8-ec6cc95bd21c.webp?auto=compress,format&amp;format=webp" alt=""></p>
<p>At STABL Energy, we didn't use any Rust before. We mainly used C for embedded and Python for anything else. But since I got to lead this project and had a great time with Rust, we ended up writing a prototype using <a target="_blank" href="https://esp-rs.github.io/book/overview/using-the-standard-library.html">ESP IDF</a>, which even allowed us to use the Rust standard library. Long story short: Our Rust prototype ended up much more reliable than the C implementation. We spent a little bit more time writing the software (Rust takes longer to write than C) to achieve the same functionality but spent basically zero time debugging (since there weren't that many bugs) - so we stuck with it. At that time the Rust support from Espressif (the company behind the ESP32) was rather new and experimental (and it still is), but it kept improving and we noticed quite the investment from Espressif in terms of work spent working on Rust support for their platform.</p>
<p>Fast forward to 2024: We now used the ESP32 with our custom software written in Rust for over a year in production, with great success. The devices transmit data 24/7 and we have no known bugs (I don't even remember the last bug we had). There is just one problem: Who maintains and further develops this project? While there are some pretty passionate Rust developers (&amp; consultancies) out there (even in Germany) and even more that would be willing to learn Rust, it is not viable to hire one sole Rust developer for this (small) project. Since Rust, and especially embedded Rust (lots of FFI &amp; unsafe), is quite hard to learn, it is not viable (for us) to retrain a C developer to Rust. Luckily we <a target="_blank" href="https://github.com/Shemnei/">found a Rust developer</a> who was willing to learn C as well.</p>
<p>So what's the state of embedded Rust outside of our small project? Dion, from Tweedegolf, recently published <a target="_blank" href="https://tweedegolf.nl/en/blog/101/are-we-embedded-yet">a great article</a> about the current state: He says that what works and does not work heavily depends on the specific use case and willingness to build the missing pieces yourself or rely on open-source software. There are still some rough edges, as visualised in his infographic, but overall Rust is a viable programming language choice for embedded projects.</p>
<p><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1706813382842/c04013ce-7a07-4e46-9bec-71c8d45b3ad7.png?auto=compress,format&amp;format=webp" alt="The state of embedded Rust by https://tweedegolf.nl/en/blog/101/are-we-embedded-yet"></p>
<p>If, in the future, I were faced with a choice between C and Rust for embedded development again, I would most likely choose Rust because of how successfully we used it in the past. Let me know if you heard about some similar projects - I am always excited to hear about embedded Rust!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Huly – Open-source project management platform (433 pts)]]></title>
            <link>https://github.com/hcengineering/platform</link>
            <guid>41833902</guid>
            <pubDate>Mon, 14 Oct 2024 03:16:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/hcengineering/platform">https://github.com/hcengineering/platform</a>, See on <a href="https://news.ycombinator.com/item?id=41833902">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Huly Platform</h2><a id="user-content-huly-platform" aria-label="Permalink: Huly Platform" href="#huly-platform"></a></p>
<p dir="auto"><a href="https://x.com/huly_io" rel="nofollow"><img src="https://camo.githubusercontent.com/dd04868728364da7e93e7005a13600fe97009391b86c92e7a498296d4768282b/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f68756c795f696f3f7374796c653d666f722d7468652d6261646765" alt="X (formerly Twitter) Follow" data-canonical-src="https://img.shields.io/twitter/follow/huly_io?style=for-the-badge"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d7e7baa784ff194de13a336fccc97de8e442d8f041cb52bf389a135d5fbdb5ad/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6863656e67696e656572696e672f706c6174666f726d3f7374796c653d666f722d7468652d6261646765"><img src="https://camo.githubusercontent.com/d7e7baa784ff194de13a336fccc97de8e442d8f041cb52bf389a135d5fbdb5ad/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6863656e67696e656572696e672f706c6174666f726d3f7374796c653d666f722d7468652d6261646765" alt="GitHub License" data-canonical-src="https://img.shields.io/github/license/hcengineering/platform?style=for-the-badge"></a></p>
<p dir="auto">⭐️ Your star shines on us. Star us on GitHub!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">About</h2><a id="user-content-about" aria-label="Permalink: About" href="#about"></a></p>
<p dir="auto">The Huly Platform is a robust framework designed to accelerate the development of business applications, such as CRM systems.
This repository includes several applications, such as Chat, Project Management, CRM, HRM, and ATS.
Various teams are building products on top of the Platform, including <a href="https://huly.io/" rel="nofollow">Huly</a> and <a href="https://tracex.co/" rel="nofollow">TraceX</a>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://repository-images.githubusercontent.com/392073243/6d27d5cc-38cd-4d88-affe-bb88b393180c"><img src="https://repository-images.githubusercontent.com/392073243/6d27d5cc-38cd-4d88-affe-bb88b393180c" alt="Huly"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Self-Hosting</h2><a id="user-content-self-hosting" aria-label="Permalink: Self-Hosting" href="#self-hosting"></a></p>
<p dir="auto">If you're primarily interested in self-hosting Huly without the intention to modify or contribute to its development, please use <a href="https://github.com/hcengineering/huly-selfhost">huly-selfhost</a>.
This project offers a convenient method to host Huly using <code>docker</code>, designed for ease of use and quick setup. Explore this option to effortlessly enjoy Huly on your own server.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Activity</h2><a id="user-content-activity" aria-label="Permalink: Activity" href="#activity"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/336051a8bce09028c886ea6c1206a0ed668c3db37f0c342341f4f8a5b7551556/68747470733a2f2f7265706f62656174732e6178696f6d2e636f2f6170692f656d6265642f633432633939653231363931666136306561363162356364663131633265303634373632313533342e737667"><img src="https://camo.githubusercontent.com/336051a8bce09028c886ea6c1206a0ed668c3db37f0c342341f4f8a5b7551556/68747470733a2f2f7265706f62656174732e6178696f6d2e636f2f6170692f656d6265642f633432633939653231363931666136306561363162356364663131633265303634373632313533342e737667" alt="Alt" title="Repobeats analytics image" data-canonical-src="https://repobeats.axiom.co/api/embed/c42c99e21691fa60ea61b5cdf11c2e0647621534.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Content</h2><a id="user-content-table-of-content" aria-label="Permalink: Table of Content" href="#table-of-content"></a></p>
<ul dir="auto">
<li><a href="#huly-platform">Huly Platform</a>
<ul dir="auto">
<li><a href="#about">About</a></li>
<li><a href="#self-hosting">Self-Hosting</a></li>
<li><a href="#activity">Activity</a></li>
<li><a href="#table-of-content">Table of Content</a></li>
<li><a href="#pre-requisites">Pre-requisites</a></li>
<li><a href="#verification">Verification</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#build-and-run">Build and run</a></li>
<li><a href="#run-in-development-mode">Run in development mode</a></li>
<li><a href="#update-project-structure-and-database">Update project structure and database</a></li>
<li><a href="#troubleshooting">Troubleshooting</a></li>
<li><a href="#build--watch">Build &amp; Watch</a></li>
<li><a href="#tests">Tests</a>
<ul dir="auto">
<li><a href="#unit-tests">Unit tests</a></li>
<li><a href="#ui-tests">UI tests</a></li>
</ul>
</li>
<li><a href="#package-publishing">Package publishing</a></li>
<li><a href="#additional-testing">Additional testing</a></li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Pre-requisites</h2><a id="user-content-pre-requisites" aria-label="Permalink: Pre-requisites" href="#pre-requisites"></a></p>
<ul dir="auto">
<li>Before proceeding, ensure that your system meets the following requirements:
<ul dir="auto">
<li><a href="https://nodejs.org/en/download/" rel="nofollow">Node.js</a> (v20.11.0 is required)</li>
<li><a href="https://docs.docker.com/get-docker/" rel="nofollow">Docker</a></li>
<li><a href="https://docs.docker.com/compose/install/" rel="nofollow">Docker Compose</a></li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Verification</h2><a id="user-content-verification" aria-label="Permalink: Verification" href="#verification"></a></p>
<p dir="auto">To verify the installation, perform the following checks in your terminal:</p>
<ul dir="auto">
<li>Ensure that the <code>docker</code> commands are available:
<div dir="auto" data-snippet-clipboard-copy-content="docker --version
docker compose version"><pre>docker --version
docker compose version</pre></div>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Fast start</h2><a id="user-content-fast-start" aria-label="Permalink: Fast start" href="#fast-start"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="sh ./scripts/fast-start.sh"><pre>sh ./scripts/fast-start.sh</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">You need Microsoft's <a href="https://rushjs.io/" rel="nofollow">rush</a> to install application.</p>
<ol dir="auto">
<li>Install Rush globally using the command:
<div dir="auto" data-snippet-clipboard-copy-content="npm install -g @microsoft/rush"><pre>npm install -g @microsoft/rush</pre></div>
</li>
<li>Navigate to the repository root and run the following commands:

</li>
</ol>
<p dir="auto">Alternatively, you can just execute:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sh ./scripts/presetup-rush.sh"><pre>sh ./scripts/presetup-rush.sh</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Build and run</h2><a id="user-content-build-and-run" aria-label="Permalink: Build and run" href="#build-and-run"></a></p>
<p dir="auto">Development environment setup requires Docker to be installed on system.</p>
<p dir="auto">Support is available for both amd64 and arm64 containers on Linux and macOS.</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd ./dev/
rush build    # Will build all the required packages. 
# rush rebuild  # could be used to omit build cache.
rush bundle   # Will prepare bundles.
rush package  # Will build all webpack packages.
rush validate # Will validate all sources with typescript and generate d.ts files required for ts-node execution.
rush svelte-check # Optional. svelte files validation using svelte-check.
rush docker:build   # Will build Docker containers for all applications in the local Docker environment.
rush docker:up # Will set up all the containers"><pre><span>cd</span> ./dev/
rush build    <span><span>#</span> Will build all the required packages. </span>
<span><span>#</span> rush rebuild  # could be used to omit build cache.</span>
rush bundle   <span><span>#</span> Will prepare bundles.</span>
rush package  <span><span>#</span> Will build all webpack packages.</span>
rush validate <span><span>#</span> Will validate all sources with typescript and generate d.ts files required for ts-node execution.</span>
rush svelte-check <span><span>#</span> Optional. svelte files validation using svelte-check.</span>
rush docker:build   <span><span>#</span> Will build Docker containers for all applications in the local Docker environment.</span>
rush docker:up <span><span>#</span> Will set up all the containers</span></pre></div>
<p dir="auto">Be aware <code>rush docker:build</code> will automatically execute all required phases like build, bundle, package.</p>
<p dir="auto">Alternatively, you can just execute:</p>

<p dir="auto">By default, Docker volumes named dev_db, dev_elastic, and dev_files will be created for the MongoDB, Elasticsearch, and MinIO instances.</p>
<p dir="auto">Before you can begin, you need to create a workspace and an account and associate it with the workspace.</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd ./tool # dev/tool in the repository root
rushx run-local create-workspace ws1 -w DevWorkspace # Create workspace
rushx run-local create-account user1 -p 1234 -f John -l Appleseed # Create account
rushx run-local configure ws1 --list --enable '*' # Enable all modules, even if they are not yet intended to be used by a wide audience.
rushx run-local assign-workspace user1 ws1 # Assign workspace to user.
rushx run-local confirm-email user1 # To allow the creation of additional test workspaces.
"><pre><span>cd</span> ./tool <span><span>#</span> dev/tool in the repository root</span>
rushx run-local create-workspace ws1 -w DevWorkspace <span><span>#</span> Create workspace</span>
rushx run-local create-account user1 -p 1234 -f John -l Appleseed <span><span>#</span> Create account</span>
rushx run-local configure ws1 --list --enable <span><span>'</span>*<span>'</span></span> <span><span>#</span> Enable all modules, even if they are not yet intended to be used by a wide audience.</span>
rushx run-local assign-workspace user1 ws1 <span><span>#</span> Assign workspace to user.</span>
rushx run-local confirm-email user1 <span><span>#</span> To allow the creation of additional test workspaces.</span>
</pre></div>
<p dir="auto">Alternatively, you can just execute:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sh ./scripts/create-workspace.sh"><pre>sh ./scripts/create-workspace.sh</pre></div>
<p dir="auto">Add the following line to your /etc/hosts file</p>
<div data-snippet-clipboard-copy-content="127.0.0.1 host.docker.internal"><pre><code>127.0.0.1 host.docker.internal
</code></pre></div>
<p dir="auto">Accessing the URL <a href="http://host.docker.internal:8087/" rel="nofollow">http://host.docker.internal:8087</a> will lead you to the app in development mode.</p>
<p dir="auto">Limitations:</p>
<ul dir="auto">
<li>Local installation does not support sending emails, thus disabling functionalities such as password recovery and email notifications.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Run in development mode</h2><a id="user-content-run-in-development-mode" aria-label="Permalink: Run in development mode" href="#run-in-development-mode"></a></p>
<p dir="auto">Development mode allows for live reloading and a smoother development process.</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd dev/prod
rush validate
rushx dev-server"><pre><span>cd</span> dev/prod
rush validate
rushx dev-server</pre></div>
<p dir="auto">Then go to <a href="http://localhost:8080/" rel="nofollow">http://localhost:8080</a></p>
<p dir="auto">Click on "Login with password" link on the bottom of the right panel and use the following login credentials:</p>
<div data-snippet-clipboard-copy-content="Email: user1
Password: 1234
Workspace: ws1"><pre lang="plain"><code>Email: user1
Password: 1234
Workspace: ws1
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Update project structure and database</h2><a id="user-content-update-project-structure-and-database" aria-label="Permalink: Update project structure and database" href="#update-project-structure-and-database"></a></p>
<p dir="auto">If the project's structure is updated, it may be necessary to relink and rebuild the projects.</p>

<p dir="auto">It may also be necessary to upgrade the running database.</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd ./dev/tool
rushx upgrade -f"><pre><span>cd</span> ./dev/tool
rushx upgrade -f</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshooting</h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto">If a build fails, but the code is correct, try to delete the <a href="https://rushjs.io/pages/maintainer/build_cache/" rel="nofollow">build cache</a> and retry.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# from the project root
rm -rf common/temp/build-cache"><pre><span><span>#</span> from the project root</span>
rm -rf common/temp/build-cache</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Build &amp; Watch</h2><a id="user-content-build--watch" aria-label="Permalink: Build &amp; Watch" href="#build--watch"></a></p>
<p dir="auto">For development purpose <code>rush build:watch</code> action could be used.</p>
<p dir="auto">It includes build and validate phases in watch mode.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tests</h2><a id="user-content-tests" aria-label="Permalink: Tests" href="#tests"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Unit tests</h3><a id="user-content-unit-tests" aria-label="Permalink: Unit tests" href="#unit-tests"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="rush test # To execute all tests

rushx test # For individual test execution inside a package directory"><pre>rush <span>test</span> <span><span>#</span> To execute all tests</span>

rushx <span>test</span> <span><span>#</span> For individual test execution inside a package directory</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">UI tests</h3><a id="user-content-ui-tests" aria-label="Permalink: UI tests" href="#ui-tests"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="cd ./tests
rush build
rush bundle
rush docker:build
## creates test Docker containers and sets up test database
./prepare.sh
## runs UI tests
rushx uitest"><pre><span>cd</span> ./tests
rush build
rush bundle
rush docker:build
<span><span>#</span># creates test Docker containers and sets up test database</span>
./prepare.sh
<span><span>#</span># runs UI tests</span>
rushx uitest</pre></div>
<p dir="auto">To execute tests in the development environment, please follow these steps:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd ./tests
./create-local.sh ## use ./restore-local.sh if you only want to restore the workspace to a predefined initial state for sanity.
cd ./sanity
rushx dev-uitest # To execute all tests against the development environment.
rushx dev-debug -g 'pattern' # To execute tests in debug mode with only the matching test pattern."><pre><span>cd</span> ./tests
./create-local.sh <span><span>#</span># use ./restore-local.sh if you only want to restore the workspace to a predefined initial state for sanity.</span>
<span>cd</span> ./sanity
rushx dev-uitest <span><span>#</span> To execute all tests against the development environment.</span>
rushx dev-debug -g <span><span>'</span>pattern<span>'</span></span> <span><span>#</span> To execute tests in debug mode with only the matching test pattern.</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Package publishing</h2><a id="user-content-package-publishing" aria-label="Permalink: Package publishing" href="#package-publishing"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="node ./common/scripts/bump.js -p projectName"><pre>node ./common/scripts/bump.js -p projectName</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Additional testing</h2><a id="user-content-additional-testing" aria-label="Permalink: Additional testing" href="#additional-testing"></a></p>
<p dir="auto">This project is tested with BrowserStack.</p>
<p dir="auto"><sub><sup>© 2024 <a href="https://hardcoreeng.com/" rel="nofollow">Hardcore Engineering Inc</a>.</sup></sub></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why pay for a search engine (155 pts)]]></title>
            <link>https://help.kagi.com/kagi/why-kagi/why-pay-for-search.html</link>
            <guid>41833833</guid>
            <pubDate>Mon, 14 Oct 2024 03:00:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://help.kagi.com/kagi/why-kagi/why-pay-for-search.html">https://help.kagi.com/kagi/why-kagi/why-pay-for-search.html</a>, See on <a href="https://news.ycombinator.com/item?id=41833833">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-6b87e69f=""><h2 id="why-pay-for-search" tabindex="-1">Why pay for search <a href="#why-pay-for-search" aria-label="Permalink to &quot;Why pay for search&quot;">​</a></h2><p>Search advertising is becoming increasingly aggressive and harder to distinguish from organic results. In 2022 alone, <a href="https://www.statista.com/statistics/267056/paid-search-advertising-expenditure-worldwide/" target="_blank" rel="noreferrer">search advertising spending</a> reached a staggering <strong>185.35 billion U.S. dollars</strong> worldwide, and this is forecast to grow by six percent annually until 2028, hitting nearly <strong>261 billion U.S. dollars</strong>.</p><p>This explosion in spending has directly contributed to the cluttering of search results with intrusive ads, slowing down the experience and making it harder for users to find what they’re looking for.</p><p>Beyond just the clutter, these ads are powered by invasive tracking technologies that follow users across the web, exploiting their data to maximize ad revenue.</p><p>This is where Kagi comes in. By charging a nominal fee for searches, Kagi ensures that its search results are faster, more accurate, and completely respectful of the user's privacy:</p><p><img src="https://help.kagi.com/assets/why_pay_for_search_kagi.c8abd60b.png" alt="Why_pay_for_search"></p><p>And when you pay for search, you’re supporting a product that doesn't need ad revenue or access to user data because it gets its support directly from its users.</p><p>By aligning our incentives with those of our users, Kagi is committed to building a <strong>better, more ethical web</strong>.</p><p>You can learn more in <a href="https://blog.kagi.com/age-pagerank-over" target="_blank" rel="noreferrer">The Age of PageRank is Over</a> post on our blog.</p><h2 id="ad-supported-search-engines-the-future-of-the-web" tabindex="-1">Ad-supported search engines &amp; the future of the web <a href="#ad-supported-search-engines-the-future-of-the-web" aria-label="Permalink to &quot;Ad-supported search engines &amp; the future of the web&quot;">​</a></h2><p>In the early days of Google, in 1998, its founders emphasized the importance of maintaining an ad-free search engine:</p><blockquote><p>"<strong>The goals of the advertising business model do not always correspond to providing quality search to users.</strong> ... <strong>we expect that advertising funded search engines will be inherently biased towards the advertisers and away from the needs of the consumers.</strong>"</p><p>— <a href="http://infolab.stanford.edu/pub/papers/google.pdf" target="_blank" rel="noreferrer">The Anatomy of a Large-Scale Hypertextual Web Search Engine</a>, Sergey Brin and Lawrence Page, 1998</p></blockquote><p>Unfortunately, this is exactly how it played out, and over the last 20 years we proceeded to witness the <a href="https://blog.kagi.com/age-pagerank-over" target="_blank" rel="noreferrer">great detoriation of web and search</a> as a result. In many ways, Google made a pivot from "organizing the world's information" to "organizing the world's ads" and we believe that this leads to a sub-optimal experience for the end user.</p><h2 id="the-numbers-tell-a-tale" tabindex="-1">The numbers tell a tale <a href="#the-numbers-tell-a-tale" aria-label="Permalink to &quot;The numbers tell a tale&quot;">​</a></h2><p>Ad based search engines make almost $300 a year off their users.</p><p>Google generated <a href="https://www.statista.com/statistics/469821/google-annual-ad-revenue-usa/" target="_blank" rel="noreferrer">$76 billion</a> in US ad revenue in 2023. Google had <a href="https://www.statista.com/topics/1001/google/" target="_blank" rel="noreferrer">274 million</a> unique visitors in the US as of February 2023.</p><p>To estimate the revenue per user, we can divide the 2023 US ad revenue by the 2023 number of users: $76 billion / 274 million = $277 revenue per user in the US or $23 USD per month, on average! That means there is someone, somewhere, a third party and a complete stranger, an advertiser, paying $23 per month for your searches.</p><p>Choosing to subscribe to Kagi means that while you are now paying for your search you are getting a fair value for your money, you are getting more relevant results, are able to personalize your experience and take advantage of all the tools and features we built, all while protecting your and your family's privacy and data.</p><h2 id="kagi-offers-a-choice" tabindex="-1">Kagi offers a choice <a href="#kagi-offers-a-choice" aria-label="Permalink to &quot;Kagi offers a choice&quot;">​</a></h2><p>All search engines have search costs, development costs, and administrative costs. Most search engines cover this by advertising, tracking, and selling your data. And for 25 years we did not have any choice.</p><p>Kagi brings a new model to the market - pay for your search with your wallet instead. For only $5/mo (Starter plan) or $10/mo (unlimited search plan) you can now search with a peace of mind, knowing the results are always shown with your best interest in mind.</p><h2 id="pick-a-kagi-plan-that-is-right-for-you" tabindex="-1">Pick a Kagi plan that is right for you <a href="#pick-a-kagi-plan-that-is-right-for-you" aria-label="Permalink to &quot;Pick a Kagi plan that is right for you&quot;">​</a></h2><p>We make it easy to pick the plan that is right for you, take a look at our <a href="https://help.kagi.com/kagi/plans/plan-types.html">Plan Types</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Python client for the $20 Colmi R02 smart ring (316 pts)]]></title>
            <link>https://tahnok.github.io/colmi_r02_client/colmi_r02_client.html</link>
            <guid>41833200</guid>
            <pubDate>Mon, 14 Oct 2024 01:06:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tahnok.github.io/colmi_r02_client/colmi_r02_client.html">https://tahnok.github.io/colmi_r02_client/colmi_r02_client.html</a>, See on <a href="https://news.ycombinator.com/item?id=41833200">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Open source python client to read your data from the Colmi R02 family of Smart Rings. 100% open source, 100% offline.</p>

<h2 id="what-is-the-colmi-r02">What is the Colmi R02?</h2>

<p><img src="https://cdn.tahnok.ca/u/banner_colmi_r02.png" alt="picture of the colmi r02 smart ring in shiny black. The electronics can be seen through the epoxy inside the ring" width="100%"></p>

<p>It's a cheap (as in $20) "smart ring" / fitness wearable that includes the following sensors:</p>

<ul>
<li>Accelerometer
<ul>
<li>step tracking</li>
<li>sleep tracking</li>
<li>gestures (maybe...?)</li>
</ul></li>
<li>Heart Rate (HR)</li>
<li>Blood Oxygen (SPO2)</li>
</ul>

<p>I found out about the ring from atc1441 and his work on <a href="https://github.com/atc1441/ATC_RF03_Ring/">ATC_RF03</a> and the 
<a href="https://hackaday.com/2024/06/16/new-part-day-a-hackable-smart-ring/">Hackaday coverage</a></p>

<p>Got questions or ideas? <a href="mailto:tahnok+colmir02@gmail.com">Send me an email</a> or <a href="https://github.com/tahnok/colmi_r02_client/issues/new">open an issue</a></p>

<p>Are you hiring? <a href="mailto:tahnok+colmir02@gmail.com">Send me an email</a></p>

<h2 id="how-to-buy">How to buy</h2>

<p>You can get it on <a href="https://www.aliexpress.com/item/1005006631448993.html">here on AliExpress</a>. If that link is dead try searching for "COLMI R02", I got mine from "Colmi official store". It cost me $CAD 22 shipped.</p>

<h2 id="reverse-engineering-status">Reverse engineering status</h2>

<ul>
<li> Real time heart rate and SPO2</li>
<li> Step logs (still don't quite understand how the day is split up)</li>
<li> Heart rate logs (aka periodic measurement)</li>
<li> Set ring time</li>
<li> Set HR log frequency</li>
<li> SPO2 logs</li>
<li> Sleep tracking</li>
<li> "Stress" measurement</li>
</ul>

<h2 id="planned-feature">Planned Feature</h2>

<ul>
<li>add more CLI functionlity</li>
<li>pretty print HR and steps</li>
<li>sync all data to a file or sqlite db</li>
<li>simple web interface</li>
</ul>

<h2 id="getting-started">Getting started</h2>

<h3 id="using-the-command-line">Using the command line</h3>

<p>If you don't know python that well, I <strong>highly</strong> recommend you install <a href="https://pipx.pypa.io/stable/installation/">pipx</a>. It's puprpose built for managing python packages intended to be used as standalone programs and it will keep your computer safe from the pitfalls of python packaging. Once installed you can do</p>

<div>
<pre><span></span><code>pipx<span> </span>install<span> </span>git+https://github.com/tahnok/colmi_r02_client
</code></pre>
</div>

<p>Once that is done you can look for nearby rings using</p>



<pre><code>Found device(s)
                Name  | Address
--------------------------------------------
            R02_341C  |  70:CB:0D:D0:34:1C
</code></pre>

<p>Once you have your address you can use it to do things like get real time heart rate</p>

<div>
<pre><span></span><code>colmi_r02_client<span> </span>--address<span>=</span><span>70</span>:CB:0D:D0:34:1C<span> </span>get-real-time-heart-rate
</code></pre>
</div>

<pre><code>Starting reading, please wait.
[81, 81, 79, 79, 79, 79]
</code></pre>

<p>The most up to date and comprehensive help for the command line can be found running</p>



<pre><code>Usage: colmi_r02_client [OPTIONS] COMMAND [ARGS]...

Options:
  --debug / --no-debug
  --record / --no-record  Write all received packets to a file
  --address TEXT          Bluetooth address
  --name TEXT             Bluetooth name of the device, slower but will work
                          on macOS
  --help                  Show this message and exit.

Commands:
  get-heart-rate-log           Get heart rate for given date
  get-heart-rate-log-settings  Get heart rate log settings
  get-real-time-heart-rate     Get real time heart rate.
  info                         Get device info and battery level
  set-heart-rate-log-settings  Get heart rate log settings
  set-time                     Set the time on the ring, required if you...
</code></pre>

<h3 id="with-the-library-sdk">With the library / sdk</h3>

<p>You can use the <code><a href="https://tahnok.github.io/colmi_r02_client/colmi_r02_client/client.html">colmi_r02_client.client</a></code> class as a library to do your own stuff in python. I've tried to write a lot of docstrings, which are visible on <a href="https://tahnok.github.io/colmi_r02_client/">the docs site</a></p>

<h2 id="communication-protocol-details">Communication Protocol Details</h2>

<p>I've kept a lab notebook style stream of consciousness notes on <a href="https://notes.tahnok.ca/">https://notes.tahnok.ca/</a>, starting with <a href="https://notes.tahnok.ca/blog/2024-07-07+Smart+Ring+Hacking">2024-07-07 Smart Ring Hacking</a> and eventually getting put under one folder. That's the best source for all the raw stuff.</p>

<p>At a high level though, you can talk to and read from the ring using BLE. There's no binding or security keys required to get started. (that's kind of bad, but the range on the ring is really tiny and I'm not too worried about someone getting my steps or heart rate information. Up to you).</p>

<p>The ring has a ble GATT service with the UUID <code>6E40FFF0-B5A3-F393-E0A9-E50E24DCCA9E</code>. It has two important characteristics:</p>

<ol>
<li>RX: <code>6E400002-B5A3-F393-E0A9-E50E24DCCA9E</code>, which you write to</li>
<li>TX: <code>6E400003-B5A3-F393-E0A9-E50E24DCCA9E</code>, which you can "subscribe" to and is where the ring responds to packets you have sent.</li>
</ol>

<p>This closely ressembles the <a href="https://docs.nordicsemi.com/bundle/ncs-latest/page/nrf/libraries/bluetooth_services/services/nus.html">Nordic UART Service</a> and UART/Serial communications in general.</p>

<h3 id="packet-structure">Packet structure</h3>

<p>The ring communicates in 16 byte packets for both sending and receiving. The first byte of the packet is always a command/tag/type. For example, the packet you send to ask for the battery level starts with <code>0x03</code> and the response packet also starts with <code>0x03</code>.</p>

<p>The last byte of the packet is always a checksum/crc. This value is calculated by summing up the other 15 bytes in the packet and taking the result modulo 255. See <code><a href="https://tahnok.github.io/colmi_r02_client/colmi_r02_client/packet.html#checksum">colmi_r02_client.packet.checksum</a></code></p>

<p>The middle 14 bytes are the "subdata" or payload data. Some requests (like <code><a href="https://tahnok.github.io/colmi_r02_client/colmi_r02_client/set_time.html#set_time_packet">colmi_r02_client.set_time.set_time_packet</a></code>) include additional data. Almost all responses use the subdata to return the data you asked for.</p>

<p>Some requests result in multiple responses that you have to consider together to get the data. <code><a href="https://tahnok.github.io/colmi_r02_client/colmi_r02_client/steps.html#SportDetailParser">colmi_r02_client.steps.SportDetailParser</a></code> is an example of this behaviour.</p>

<p>If you want to know the actual packet structure for a given feature's request or response, take a look at the source code for that feature. I've tried to make it pretty easy to follow even if you don't know python very well. There are also some tests that you can refer to for validated request/response pairs and human readable interpretations of that data.</p>

<p>Got questions or ideas? <a href="mailto:tahnok+colmir02@gmail.com">Send me an email</a> or <a href="https://github.com/tahnok/colmi_r02_client/issues/new">open an issue</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A VSCode Extension to edit HTML visually in real-time (208 pts)]]></title>
            <link>https://github.com/urin/vscode-web-visual-editor</link>
            <guid>41833198</guid>
            <pubDate>Mon, 14 Oct 2024 01:06:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/urin/vscode-web-visual-editor">https://github.com/urin/vscode-web-visual-editor</a>, See on <a href="https://news.ycombinator.com/item?id=41833198">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_product_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>GitHub Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;security&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;security_link_product_navbar&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_product_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_product_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_product_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_product_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code Review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_product_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_product_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
        <p>Code Search</p><p>
        Find more, search less
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>

                  <li>
      
      
</li>

                  <li>
      
      <div>
                    <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;white_papers_ebooks_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;white_papers_ebooks_webinars_link_resources_navbar&quot;}" href="https://resources.github.com/">
      White papers, Ebooks, Webinars

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

                </ul>
              </div>
</li>


                <li>
      
      <div>
              <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

                </ul>
              </div>
              <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

                </ul>
              </div>
              
          </div>
</li>


                <li>
      
      <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
        <p>Enterprise platform</p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;pricing&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;pricing_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:urin/vscode-web-visual-editor" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="EdKlY9rSamYqREzNOCysGCV_VR1SGFMNBtbuiQdGKeZlTEtI2Wnznvs1deFPEf3K-cKEAVA5AxVNBWFbXZi3qg" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="urin/vscode-web-visual-editor" data-current-org="" data-current-owner="urin" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=urin%2Fvscode-web-visual-editor" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/urin/vscode-web-visual-editor&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="6233d79a658a382344e3bca3b4d7d45518ebb6d7e1c81beda26a1b8ba611c983" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a>
          
        </p></div>
      </div></div>]]></description>
        </item>
    </channel>
</rss>