<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 24 Feb 2024 17:00:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Institutions try to preserve the problem to which they are the solution (253 pts)]]></title>
            <link>https://effectiviology.com/shirky-principle/</link>
            <guid>39491863</guid>
            <pubDate>Sat, 24 Feb 2024 14:53:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://effectiviology.com/shirky-principle/">https://effectiviology.com/shirky-principle/</a>, See on <a href="https://news.ycombinator.com/item?id=39491863">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text"><p><img decoding="async" src="https://effectiviology.com/wp-content/uploads/Shirky-Principle.jpg" alt="" width="1000" height="667" srcset="https://effectiviology.com/wp-content/uploads/Shirky-Principle.jpg 1000w, https://effectiviology.com/wp-content/uploads/Shirky-Principle-300x200.jpg 300w, https://effectiviology.com/wp-content/uploads/Shirky-Principle-768x512.jpg 768w" sizes="(max-width: 1000px) 100vw, 1000px"></p><p>The <em>Shirky principle</em> is the adage that “institutions will try to preserve the problem to which they are the solution”. More broadly, it can also be characterized as the adage that “every entity tends to prolong the problem it is solving”.</p><p>For example, the Shirky principle means that a government agency that’s meant to address a certain societal issue may hinder attempts by others to address the issue, in order to ensure that the agency remains relevant. Alternatively, the agency may become so focused on the current way in which it addresses the issue that it will fail to adopt better new solutions as they become available, thus prolonging the issue.</p><p>The Shirky principle has important implications in various domains, so it’s important to understand it. As such, in the following article you will learn more about this principle, and see what you can do about it in practice.</p><div id="ez-toc-container"><nav><ul><li><a href="#Examples_of_the_Shirky_principle" title="Examples of the Shirky principle">Examples of the Shirky principle</a></li><li><a href="#Origin_and_formulations_of_the_Shirky_principle" title="Origin and formulations of the Shirky principle">Origin and formulations of the Shirky principle</a></li><li><a href="#Caveats_about_the_Shirky_principle" title="Caveats about the Shirky principle">Caveats about the Shirky principle</a></li><li><a href="#Accounting_for_the_Shirky_principle" title="Accounting for the Shirky principle">Accounting for the Shirky principle</a></li><li><a href="#Related_concepts" title="Related concepts">Related concepts</a></li><li><a href="#Summary_and_conclusions" title="Summary and conclusions">Summary and conclusions</a></li></ul></nav></div><h2><span id="Examples_of_the_Shirky_principle"></span>Examples of the Shirky principle<span></span></h2><p>An example of the Shirky principle are tax-filing companies who <a href="http://web.archive.org/web/20210509181435/https:/www.propublica.org/article/inside-turbotax-20-year-fight-to-stop-americans-from-filing-their-taxes-for-free">lobby</a> the government to <a href="http://web.archive.org/web/20210509175424/https:/www.propublica.org/article/filing-taxes-could-be-free-simple-hr-block-intuit-lobbying-against-it">prevent</a> it from <a href="http://web.archive.org/web/20210509181647/https:/www.politico.com/agenda/story/2018/07/18/tax-filing-congress-irs-000683/">offering</a> a free and easy way to <a href="http://web.archive.org/web/20210416223809/https:/www.nytimes.com/2015/04/16/technology/personaltech/turbotax-or-irs-as-tax-preparer-intuit-has-a-favorite.html">file taxes</a>, to ensure that the companies can continue to make a profit. A similar example of this are private prison companies who <a href="http://web.archive.org/web/20210509182458/https:/www.washingtonpost.com/posteverything/wp/2015/04/28/how-for-profit-prisons-have-become-the-biggest-lobby-no-one-is-talking-about/">lobby</a> the government to <a href="http://web.archive.org/web/20210509184717/https:/www.theguardian.com/commentisfree/2012/sep/27/lawmakers-lobbyists-keep-lock-private-prison-business">support</a> policies that <a href="http://web.archive.org/web/20210509185251/https:/escholarship.org/uc/item/3qj7q63d">increase</a> the number of <a href="http://web.archive.org/web/20210509185809/https:/papers.ssrn.com/sol3/papers.cfm?abstract_id=2794145">incarcerated</a> people and&nbsp;the <a href="https://doi.org/10.1002/9781118519639.wbecpx175">duration</a> of their incarceration.</p><p>Another <a href="https://sagepub.com/en-us/nam/encyclopedia-of-social-media-and-politics/book239101">well-known</a> example of the Shirky principle is described in “<a href="https://amzn.to/3tqsrZz">Cognitive Surplus</a>”, a book by Clay Shirky that contained one of the first discussions of this principle:</p><blockquote><p>“PickupPal.com is… a carpooling site designed to coordinate drivers and riders planning to travel along the same route.</p><p>In May 2008 the Ontario-based bus company Trentway-Wagar… petitioned the Ontario Highway Transport Board (OHTB) to shut PickupPal down on the grounds that, by helping coordinate drivers and riders, it worked too well to be a carpool. Trentway-Wagar invoked Section 11 of the Ontario Public Vehicles Act, which stipulated that carpooling could happen only between home and work (rather than, say, school or hospital.) It had to happen within municipal lines. It had to involve the same driver each day. And gas or travel expense could be reimbursed no more frequently than weekly.</p><p>Trentway-Wagar was arguing that because carpooling used to be inconvenient, it should always be inconvenient, and if that inconvenience disappeared, then it should be reinserted by legal fiat. Curiously, an organization that commits to helping society manage a problem also commits itself to the preservation of that same problem, as its institutional existence hinges on society’s continued need for its management. Bus companies provide a critical service—public transportation—but they also commit themselves, as Trentway-Wagar did, to fending off competition from alternative ways of moving people from one place to another.</p><p>The OHTB upheld Trentway-Wagar’s complaint and ordered PickupPal to stop operating in Ontario. PickupPal decided to fight the case—and lost in the hearing. But public attention became focused on the issue, and in a year of high gas prices, burgeoning environmental concern, and a financial downturn, almost no one took Trentway-Wagar’s side. The public reaction, channeled through everything from an online petition to T-shirt sales, had one message: Save PickupPal. The idea that people couldn’t use such a service was too hot for the politicians in Ontario to ignore. Within weeks of Trentway-Wagar’s victory, the Ontario legislature amended the Public Vehicles Act to make PickupPal legal again.”</p></blockquote><p>In addition, the Shirky principle can also apply to entities other than institutions. For example, an individual employee who’s in charge of a certain process in their workplace might resist attempts to automate that process, in order to ensure that the employee remains necessary to their employer.</p><p>A well-known example of the Shirky effect in this context is the <em>cobra effect</em>. It <a href="https://doi.org/10.1057/s41302-021-00187-7">describes</a> a <a href="http://web.archive.org/web/20210510110107/https:/freakonomics.com/podcast/the-cobra-effect-a-new-freakonomics-radio-podcast/">case</a> where British colonial officials in Delhi (India), set a bounty on dead cobras, in order to reduce the cobra population. However, this led citizens to breed the cobras for profit, and eventually to release them when the bounty was canceled.</p><p>A similar incident occurred circa 1902 in Hanoi (Vietnam), which was under French colonial rule at the time, when French officials sought to reduce the rat population in the city:</p><blockquote><p>“To fight the infestation citywide, the colonial administration added vigilantes to its team of professional killers. Appealing to both civic duty and to the pocketbook, a one-cent bounty was paid for each rat tail brought to the authorities (it was decided that the handing in of an entire rat corpse would create too much of a burden for the already taxed municipal health authorities).</p><p>Unfortunately, this scheme backfired. Despite initial apparent success, the authorities soon discovered that the best laid plans of mice and men often go awry. As soon the municipal administrators publicized the reward program, Vietnamese residents began to bring in thousands of tails. While many desk-bound administrators delighted in the numbers of apparently eliminated rats, more alert officials in the field began to notice a disturbing development. There were frequent sightings of rats without tails going about their business in the city streets. After some perplexity, the authorities realized that less-than-honest but quite resourceful characters were catching rats, but merely cutting off the tails and letting the still-living pests go free (perhaps to breed and produce more valuable tails).</p><p>Later, things became even more serious as health inspectors discovered a disturbing development in the suburbs of Hanoi. These officials found that more enterprising but equally deceptive individuals were actually raising rats to collect the bounty. One can only imagine the frustration of the municipal authorities, who realized that their best efforts at <em>dératisation</em> [extermination of rats] had actually increased the rodent population by indirectly encouraging rat-farming.”</p><p>— From “Of rats, rice, and race: The great Hanoi rat massacre, an episode in French colonial history” (Vann, <a href="https://doi.org/10.1353/fch.2003.0027">2003</a>)</p></blockquote><p>Finally, note that the phenomenon described by the Shirky principle—entities prolonging a problem to which they are the solution—isn’t necessarily the result of intentional actions. For example, a company may inadvertently perpetuate the problem that it solves, because its processes are so focused on the mediocre solution that they’re currently selling, that they don’t realize a better solution exists. Similarly, a company may discourage the use of a certain approach to solving a problem because it previously failed for them, even after technological advancements make this approach viable.</p><h2><span id="Origin_and_formulations_of_the_Shirky_principle"></span>Origin and formulations of the Shirky principle<span></span></h2><p>The Shirky principle was proposed in a <a href="http://web.archive.org/web/20210508141233/https:/kk.org/thetechnium/the-shirky-prin/">2010 blog post</a> by Kevin Kelly, editor of <em>Wired</em> magazine, who based it on the speaking and writing of scholar Clay Shirky.</p><p>Specifically, Kelly attributed the adage that “Institutions will try to preserve the problem to which they are the solution” to a statement that Shirky made in a recent talk, and noted that similar statements were made by Shirky in an associated blog post (“<a href="http://web.archive.org/web/20100404013927/http:/www.shirky.com/weblog/2010/04/the-collapse-of-complex-business-models/">The Collapse of Complex Business Models</a>”) and book (“<a href="https://amzn.to/3tqsrZz">Cognitive Surplus</a>”). There, Shirky states that “an organization that commits to helping society manage a problem also commits itself to the preservation of that same problem, as its institutional existence hinges on society’s continued need for its management”.</p><p>In addition to mentioning the key quote that is now known as the Shirky principle, Kelly also says the following in his <a href="http://web.archive.org/web/20210508141233/https:/kk.org/thetechnium/the-shirky-prin/">blog post</a>:</p><blockquote><p>“The Shirky Principle declares that complex solutions (like a company, or an industry) can become so dedicated to the problem they are the solution to, that often they inadvertently perpetuate the problem.”</p></blockquote><p>Later, he also says the following with regard to this principle (bold added here for emphasis):</p><blockquote><p>“In a strong sense we are defined by the problems we are solving. Yin/Yang, problem/solution, both sides form one unit. <strong>Because of the Shirky Principle, which says that every entity tends to prolong the problem it is solving</strong>, progress sometimes demands that we let go of problems.”</p></blockquote><p>Essentially, in his writing on the topic, Kelly offers three formulations of the Shirky principle, which differ in subtle but important ways:</p><ul><li>The first formulation—“Institutions will try to preserve the problem to which they are the solution”—refers to <em>institutions</em>, and states that they will <em>try</em> to preserve problems, which implies that they do so intentionally.</li><li>The second formulation—“Complex solutions (like a company, or an industry) can become so dedicated to the problem they are the solution to, that often they inadvertently perpetuate the problem”—refers to <em>complex solutions</em>, and states that they often <em>inadvertently</em> perpetuate the problem, which implies that they do so unintentionally.</li><li>The third formulation—”Every entity tends to prolong the problem it is solving”—refers to <em>entities</em>, and states that they <em>tend to</em> prolong problems, without making any claim about their intentions.</li></ul><p>The first formulation is the one that’s most commonly used when people discuss the Shirky principle, though Kelly does not actually refer to it as the Shirky principle in his original blog post. The third formulation, on the other hand, is the most general, though one issue with it is that it states that “every” entity engages in this kind of behavior, which is too absolute of a claim. However, this issue can be addressed by slightly changing this formulation, into “entities tend to prolong the problems they are solving”.</p><p><em>Note</em>: In <a href="http://web.archive.org/web/20210508141233/https:/kk.org/thetechnium/the-shirky-prin/">his post</a>, Kelly states that Shirky’s observation reminds him “of the clarity of the Peter Principle, which says that a person in an organization will be promoted to the level of their incompetence. At which point their past achievements will prevent them from being fired, but their incompetence at this new level will prevent them from being promoted again, so they stagnate in their incompetence.”.</p><h2><span id="Caveats_about_the_Shirky_principle"></span>Caveats about the Shirky principle<span></span></h2><p>There are some caveats about the Shirly principle that are important to keep in mind:</p><ul><li><strong>The Shirky principle is just a general observation.</strong> As such, there are many situations where it’s incorrect. For example, an institution may successfully solve the problem to which they are the solution because there’s greater profit to be made that way than by prolonging the problem.</li><li><strong>The Shirky principle can involve various types of entities.</strong> Though the best-known formulation of the Shirky principle refers to “institutions”, this principle can apply to various types of entities, including individuals and small social groups. This is noted in the general formulation of the principle (“every entity tends to prolong the problem it is solving”).</li><li><strong>The Shirky principle can involve various causes.</strong> For example, one company may prolong a problem unintentionally, due to passivity or inertia, whereas another company may prolong a problem intentionally, due to greed or self-preservation. This is reflected in the general formulation of this principle, which doesn’t make any claims regarding the causes or intentionality of this phenomenon.</li><li><strong>The Shirky principle can involve various patterns of behavior.</strong> For example, one company may prolong an existing problem by not dedicating resources to developing new solutions, whereas another company may actively prevent others from developing such solutions.</li></ul><p>In addition, the behaviors associated with the Shirly principle can vary in other ways. For example:</p><ul><li>An entity may not just preserve an existing problem, but also exacerbate it.</li><li>An entity may create a problem that did not previously exist, if they can be the solution to it.</li><li>An entity may perpetuate a problem that it benefits from, even if the entity is not actually a solution to the problem, though the entity may pretend that it is.</li></ul><p>Based on this, a broader version of Shirky’s principle can be expressed as:</p><blockquote><p>“Entities often promote problems that they benefit from”.</p></blockquote><h2><span id="Accounting_for_the_Shirky_principle"></span>Accounting for the Shirky principle<span></span></h2><p>Accounting for the Shirky principle can be beneficial when it comes to several things:</p><ul><li><strong>Understanding past and current behavior.</strong> For example, it can help you understand why certain institutions are seemingly so bad at solving certain problems, despite all the resources—like time, effort, and money—that they dedicate to those problems.</li><li><strong>Predicting future behavior.</strong> For example, it can help you predict that an executive will keep perpetuating a certain problem, in order to improve their own status within a company, even though this leads to worse outcomes for the company itself.</li><li><strong>Modifying behavior.</strong> For example, if this makes you aware of someone’s incentive to prolong a problem, that could lead you to either eliminate the perverse incentive or create a stronger disincentive. Similarly, this could lead you to point out the issue to the entity in question, in order to encourage them to try and change their behavior themselves if doing so can benefit them in the long term.</li></ul><p>When deciding how and whether to use your understanding of the Shirky principle in practice, it can help to assess relevant factors pertaining to your situation, such as what’s causing someone to act in accordance with this principle, and what outcomes their behavior leads to. For example, you will likely respond differently to a government agency that’s perpetuating a problem due to inefficient bureaucracy, than to a private company that’s perpetuating a problem out of greed, or to an individual who’s acting out of desperate self-preservation.</p><p>Finally, there are also two useful concepts worth keeping in mind when accounting for Shirky’s principle:</p><ul><li><a href="https://effectiviology.com/cui-bono/"><em><strong>Cui bono</strong></em></a>, which is a Latin phrase that means “who benefits?”, and which is used to suggest that there’s a high probability that those responsible for a certain event are the ones who stand to gain from it.</li><li><a href="https://effectiviology.com/hanlons-razor/"><em><strong>Hanlon’s razor</strong></em></a>, which is the adage that you should “never attribute to malice that which is adequately explained by stupidity”, and which, when applied broadly, suggests that when assessing people’s actions, you should not assume that they acted out of a desire to cause harm, as long as there is a reasonable alternative explanation.</li></ul><h2>Related concepts<span></span></h2><p><a href="https://effectiviology.com/parkinsons-law/"><em>Parkinson’s law</em></a> is the adage that “work expands so as to fill the time which is available for its completion” (or more generally, that “work expands to consume the resources available for its completion”). It relates to Shirky’s principle, since both concepts present a common way in which entities are inefficient or ineffective in dealing with problems that they’re supposed to solve.</p><p>Shirky’s principle also relates to another phenomenon that was <a href="https://doi.org/10.1088/1742-5468/2009/03/p03008">identified</a> by Parkinson, whereby the growth of a bureaucratic or administrative body is often associated with a substantial decrease in its overall efficiency. This is <a href="http://web.archive.org/web/20130331045219/http:/www.economist.com/node/14116121">attributed</a> to the desire of officials to increase the number of their subordinates, and to officials’ tendency to create work for each other.</p><p>In addition, a similar famous concept that’s related to Shirky’s principle has been expressed by novelist and social reformer Upton Sinclair, who <a href="http://web.archive.org/web/20210507155626/https:/www.oxfordreference.com/view/10.1093/acref/9780191826719.001.0001/q-oro-ed4-00010168">said</a> that “It is difficult to get a man to understand something when his salary depends on his not understanding it.”</p><h2><span id="Summary_and_conclusions"></span>Summary and conclusions<span></span></h2><ul><li>The <em>Shirky principle</em> is the adage that “institutions will try to preserve the problem to which they are the solution”.</li><li>For example, the Shirky principle means that a government agency that’s meant to address a certain societal issue may hinder attempts by others to address the issue, in order to ensure that the agency remains relevant.</li><li>This principle can be expressed more broadly as “every entity tends to prolong the problem it is solving”, since it can involve entities other than institutions (e.g., individuals), and various patterns of behavior (e.g., unintentionally focusing on an outdated solution vs. intentionally interfering with competition).</li><li>This principle can also be extended to say that “entities often promote problems that they benefit from”, since entities can also create new problems, exacerbate existing ones, and perpetuate problems that they don’t actually solve.</li><li>Accounting for this principle can help understand past and current behavior, predict future behavior, and modify problematic behaviors (e.g., by removing perverse incentives).</li></ul><hr> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Everyone seems to forget why GNOME and GNOME 3 and Unity happened (2022) (134 pts)]]></title>
            <link>https://liam-on-linux.dreamwidth.org/85359.html</link>
            <guid>39490879</guid>
            <pubDate>Sat, 24 Feb 2024 11:51:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://liam-on-linux.dreamwidth.org/85359.html">https://liam-on-linux.dreamwidth.org/85359.html</a>, See on <a href="https://news.ycombinator.com/item?id=39490879">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>&nbsp;That is *what* it came from, yes, but not *why*.</p><p>The "why" part seems to be forgotten now: because Microsoft was threatening to sue all the Linux vendors shipping Windows 95-like desktops.</p><p>https://www.theregister.com/2006/11/20/microsoft_claims_linux_code</p><p>Microsoft invented the Win95 desktop from scratch. Its own previous Ones (e.g. Windows for Workgroups 3.11, Windows NT 3.51 and OS/2 1.x) looked nothing like it.</p><p>The task bar, the Start menu, the system tray, "My Computer", "Network Neighbourhood", all that: all original, *patented* Microsoft designs. There was nothing like it before.&nbsp;</p><p>(The closest was Acorn's RISC OS, with an "icon bar" that works very differently, on the Archimedes computer. A handful of those were imported to North America, and right after, NeXT "invented" the Dock, and then Microsoft invented the task bar which is quite a bit more sophisticated.</p><p>One source: the team that programmed it. Here's me moderating a panel discussion by most of the surviving members of Acorn's programming team, on video from a month ago:</p><p>https://www.youtube.com/watch?v=P_SDL0IwbCc</p><p>SUSE signed a patent-sharing deal:</p><p>https://www.theregister.com/2006/11/03/microsoft_novell_suse_linux/</p><p>Note: SUSE is the biggest German Linux company. (Source: I worked for them until last year.) KDE is a German project. SUSE developers did a lot of the work on KDE.&nbsp;</p><p>So, when SUSE signed up, KDE was safe.</p><p>Red Hat and Ubuntu refused to sign.</p><p>So, both needed *non* Windows like desktops, ASAP, without a Start menu, without a taskbar, without a window menu at top left and minimize/maximize/close at top right, and so on.</p><p>Red Hat is the main sponsor of GNOME development. (When KDE was first launched, Qt was not GPL, so Red Hat refused to bundle it or support it, and wrote its own environment instead.)</p><p>Ubuntu tried to get involved with the development of GNOME 3, and was rebuffed. So it went its own way with Unity instead: basically, a Mac OS X rip-off, only IMHO done better. Myself, I still use both Unity and macOS every day. They are like twins, and switching between them is very easy.</p><p>So both RH and Ubuntu switched to non-Windows-like desktops by default.</p><p>In the end MS did not sue anyone... but it got what it wanted: total chaos in the Linux desktop world.</p><p>Before the threats, almost everyone used GNOME 2. Even SUSE bundled GNOME because its corporate owner bought the main GNOME 3rd party developers, Ximian, and forcibly merged the company into SUSE:</p><p>https://www.theregister.com/2004/01/07/novell_marries_suse_to_ximian/</p><p>SUSE, Red Hat, Debian, Ubuntu, even Sun Solaris used GNOME 2. Everyone liked GNOME 2.</p><p>Then Microsoft rattled its sabre, and the FOSS UNIX world splintered in all directions.</p><p>RH uses GNOME 3. Ubuntu used Unity, alienated a lot of people who only knew how to use Windows-like desktops, and that made Mint a huge success. GNOME 2 got forked as MATE, and Mint adopted it, helping a lot. Mint also built its own fork of GNOME 3, Cinnamon. Formerly tiny niche desktops like Xfce and LXDE got a *huge* boost. Debian adopted GNOME 3 and systemd, annoying lots of its developers and causing the Devuan fork to happen.</p><p>Here's an analysis I wrote at the time:</p><p>https://www.theregister.com/2013/06/03/thank_microsoft_for_linux_desktop_fail/</p><p>Yes, Unity evolved out of the Ubuntu netbook desktop, but the reason _why_ it did is that Ubuntu was getting threatened.</p><p>(Xubuntu and Lubuntu and Kubuntu are not official and not the defaults, so they don't endanger it.)</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Power Metal: is it really about dragons? (2018) (200 pts)]]></title>
            <link>https://notes.atomutek.org/power-metal-and-dragons.html</link>
            <guid>39489920</guid>
            <pubDate>Sat, 24 Feb 2024 07:47:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://notes.atomutek.org/power-metal-and-dragons.html">https://notes.atomutek.org/power-metal-and-dragons.html</a>, See on <a href="https://news.ycombinator.com/item?id=39489920">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <!-- summary: One day a friend described Power Metal as the love child of opera and metal singing about dragons. I decided to check if it was really about dragons. -->

<p>Some years ago, Matt Daniels <a href="https://pudding.cool/2017/02/vocabulary/index.html">[1]</a> wrote a great article on who has the largest vocabulary in hip-hop. I thought it was brilliant: comparing Shakespeare to famous hip-hop artists. It got me thinking and gave me the inspiration of doing something similar with a genre I love: Power Metal. People know metal or heavy metal but not necessarily power metal. For me, power metal has speed (remember that last song on Guitar Hero), clean vocals (you can understand the words, looking at you grind metal) and lyrics about dragons. Here I will just confirm or infirm my assumption about dragons.</p>
<p>I used the lyrics of 58 bands, which should fall under the label of Power Metal, at least according to <a href="https://reddit.com/r/powermetal/">/r/powermetal</a> <a href="https://www.reddit.com/r/PowerMetal/wiki/essential">[2]</a> and myself. This article will dive into the vocabulary of the power metal bands, what makes a song power metal or not and who wrote the most positive and negative power metal songs ever (at least from my dataset).</p>
<h2>TL;DR</h2>
<ul>
<li>Running Wild (2949 unique words) has the biggest vocabulary, followed by Helloween (2641) and  Elvenking (2505),</li>
<li>Bands from Spain, Germany and Finland have an average of more than 1600 words vocabulary; in comparison native countries like UK, US and Scotland have an average of 925, 1383 and 1501 words respectively,</li>
<li>The most metal words are <strong>deliverance, defender, honour, forevermore, realm</strong> and the least are <strong>shit, baby, fuck, girl, verse</strong>,</li>
<li>The most negative song is Condemned To Hell by Gamma Ray and the most positive There's Something In The Skies by Dark Moor,</li>
<li>Source code: to be released soon,</li>
<li>Dataset: not available but you can find lyrics all over the internet ;)</li>
</ul>
<h2>The dataset</h2>
<p>The dataset includes 58 bands: Alestorm, Angra, At Vance, Avantasia, Blind Guardian, Borealis, Cain’s Offering, Concerto Moon, Dark Moor, Demons &amp; Wizards, Dragonforce, Dragonland, Dream Evil, Edguy, Elegy, Elvenking, Fairyland, Falconer, Firewind, Freedom Call, Gamma Ray, Gloryhammer, Grave Digger, Hammerfall, Heavenly, Helloween, Hibria, Highland Glory, Iron Savior, Judicator, Kamelot, Keldian, Labyrinth, Lost Horizon, Manowar, Masterplan, Nightwish, Nocturnal Rites, Orden Ogan, Pagan’s Mind, Pathfinder, Persuader, Power Quest, Powerwolf, Primal Fear, Rhapsody, Rhapsody Of Fire, Running Wild, Sabaton, Secret Sphere, Seventh Wonder, Sonata Arctica, Stratovarius, Theocracy, Twilight Force, Twilightning, Unisonic, Wisdom; and a total of 4808 songs in English.</p>
<h3>Distribution of bands by country</h3>
<table>
<thead>
<tr>
<th>Country</th>
<th>Number of bands</th>
</tr>
</thead>
<tbody>
<tr>
<td>Brazil</td>
<td>2</td>
</tr>
<tr>
<td>Canada</td>
<td>1</td>
</tr>
<tr>
<td>Finland</td>
<td>5</td>
</tr>
<tr>
<td>France</td>
<td>1</td>
</tr>
<tr>
<td>Germany</td>
<td>16</td>
</tr>
<tr>
<td>Greece</td>
<td>1</td>
</tr>
<tr>
<td>Hungary</td>
<td>1</td>
</tr>
<tr>
<td>Italy</td>
<td>5</td>
</tr>
<tr>
<td>Japan</td>
<td>1</td>
</tr>
<tr>
<td>Netherlands</td>
<td>1</td>
</tr>
<tr>
<td>Norway</td>
<td>3</td>
</tr>
<tr>
<td>Poland</td>
<td>1</td>
</tr>
<tr>
<td>Scotland</td>
<td>1</td>
</tr>
<tr>
<td>Spain</td>
<td>1</td>
</tr>
<tr>
<td>Sweden</td>
<td>10</td>
</tr>
<tr>
<td>UK</td>
<td>3</td>
</tr>
<tr>
<td>US</td>
<td>5</td>
</tr>
</tbody>
</table>
<p>Bands from Germany and Sweden are over-represented compared to other countries. This is due to the dataset creation, which was mostly based on the albums recommendation of <a href="https://reddit.com/r/powermetal/">/r/powermetal</a>. Therefore I won’t go deeper in regions difference.</p>
<h2>Text analysis of the lyrics</h2>
<h2>Vocabulary</h2>
<p>Calculating the vocabulary length could be done in several ways: counting the unique words (with inflections, meaning singular, plural nouns, conjugated verbs, etc.), counting the lemma form of the words (more or less the dictionary form) and finally the stemmed version which is when you get the stem of a word (we reduce a word to the its minimal non-changing parts). There is no right way to do the counting. All the methods have their pros and cons:</p>
<ul>
<li>stemming should be fast but the cutting might reduce to a stem common to two different words (ex: markets/marketing would give the stem market);</li>
<li>lemmatization requires knowing the part of speech (POS) which might slow your processing but would be more accurate.</li>
</ul>
<p>Since I preferred fast results over a possible better accuracy, I chose the stemmed version (Porter) of the words for the vocabulary processing. Based on this stemming transformation, I could plot the following chart describing the growth of vocabulary range along the years (from 1982 to 2018).</p>
<p>Play with the slider it's interactive!</p>


<p><strong>Largest vocabulary in PowerMetal in unique words</strong>* (in 2018)</p>
<ol>
<li>Running Wild (2949)</li>
<li>Helloween (2641)</li>
<li>Elvenking (2502)</li>
<li>Sonata Arctica (2467)</li>
<li>Edguy (2385)</li>
</ol>
<p>* <sub>at least in my dataset</sub></p>
<h2>What makes a power metal song power metal?</h2>
<p>We need to look at the importance of the words used in the song. Common metrics are <strong>tf</strong> (term frequency of a word), <strong>idf</strong> (inverse document frequency, how rare a word is in a document), <strong>tf-idf</strong> (term frequency-inverse document frequency, measuring how important a word is in a corpus)<a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">[3]</a>.</p>
<p>Iain Barr chose the <strong>tf</strong> approach and measured the <em>Metalness</em> of a word / lyrics as follow:</p>
<p>$$ M_w = \log \frac{freq^{metal-corpus}_w}{freq^{corpus}_w} $$</p>
<p>where $M_w$ is the Metalness of a word, $freq^{metal-corpus}_w$ the frequency of the word $w$ in the metal corpus, $freq^{corpus}_w$ the frequency of the word $w$ in another corpus.</p>
<p>He looked at the distribution of a specific word in two corpus - his metal lyrics dataset and the Brown corpus, an ensemble of literary texts from the 60s <a href="https://www.degeneratestate.org/posts/2016/Apr/20/heavy-metal-and-natural-language-processing-part-1/">[4]</a>.</p>
<p>Nonetheless using a literary corpus might not be ideal since the type of text and choice of words between a novel and a lyric are quite different. I chose to use a dataset already prepared called Metrolyrics dataset, available on Kaggle <a href="https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics">[5]</a>.</p>
<p>After a bit of cleaning and processing, I got the following results (interactive chart):</p>


<p>A power metal is not necessarily about <strong>dragons</strong> (in the 47th position in the Metalness ranking): it’s all about deliverance, honour, defender, realm with a forevermore touch. Looking at the top 50 words, all these words wouldn’t appear out of place in a heroic fantasy novel or a dungeons &amp; dragons handbook.</p>
<p>On the opposite, words like <em>shit, baby, fuck, girl, verse</em> are not that metal.</p>
<h3>PowerMetalness ranking</h3>
<p><strong>The most power metal songs</strong> 🤘🤘🤘🤘🤘</p>
<ol>
<li>Freedom Call    -    66 Warriors</li>
<li>Grave Digger   -      The Emperor’s Death</li>
<li>Manowar        -     Hail And Kill</li>
<li>Dream Evil     -        Made Of Metal</li>
<li>Primal Fear    -     Evil Spell</li>
</ol>
<p><strong>The least power metal songs</strong> 🤘</p>
<ol>
<li>Gamma Ray    -     Money</li>
<li>Helloween    -     Anything My Mama Don't Like</li>
<li>Iron Savior   -      Dance With Somebody (Mando Diao cover)</li>
<li>Highland Glory  -   Love Gun (KISS cover)</li>
<li>Masterplan     -    Black Dog (Led Zeppelin cover)</li>
</ol>
<p>As you notice, the least power metal songs are mostly covers of non power metal bands, which makes sense.</p>
<p>If we look also at what words are the most important for each band, you will be able to distinguish what makes these bands unique. Let’s have a look at some iconic bands such as Alestorm, Manowar, Rhapsody and Sabaton.</p>
<h3>Alestorm</h3>
<p><strong>Words</strong>: drink, sail, sea, pirate, quest<br>
For a band which is part of the pirate metal genre, these words are the perfect representation of pirate-related vocabulary.</p>
<h3>Manowar</h3>
<p><strong>Words</strong>: die, ride, metal, live, fight<br>
My knowledge of Manowar is a bit lacking but this iconic band seems to give importance to equally iconic words.</p>
<h3>Rhapsody</h3>
<p><strong>Words</strong>: holy, ancient, king, wind, land<br>
One of my favorite bands. Their lyrics are epic and the most important words for them represent that.</p>
<h3>Sabaton</h3>
<p><strong>Words</strong>: war, death, army, strike, way<br>
Sabaton writes a lot about real wars and this is reflected in their top words.</p>
<h2>Cluster bands by their lyrics</h2>
<p>Lyrics can be used to find similarities between bands. To do so, I’m using hierarchical clustering, a clustering technique where you don’t need to already know the number of clusters in your data in advance.</p>
<p><a href="https://notes.atomutek.org/img/band-clustering.png"><img alt="Band clustering" src="https://notes.atomutek.org/img/band-clustering.png"></a></p>
<p>Note that Rhapsody and Rhapsody of Fire are clustered together. For people who might not know, Rhapsody of Fire broke into in two bands, one keeping the original name and the other becoming Rhapsody. From a lyric point of view, as Luca Turilli wrote most if not all the songs for the bands, these two bands are basically the same which is confirmed in the clustering graph.</p>
<p>Out of this clustering emerge four clusters (interpretations my own, happy to hear yours):</p>
<ul>
<li><span><strong>Rhapsody-ish</strong></span>: uplifting epic and fantasy lyrics</li>
<li><span><strong>Edguy-ish</strong></span>: wide range of lyrics (?)</li>
<li><span><strong>Blind Guardian-ish</strong></span>: dark epic lyrics</li>
<li><span><strong>Manowar-ish</strong></span>: brutal, powerful lyrics</li>
</ul>
<p>Some bands are remotely affiliated with others:</p>
<ul>
<li><strong>Affiliated but independent</strong>: Gloryhammer with Rhapsody-ish, Alestorm with Manowar-ish</li>
<li><strong>The outsider</strong>: Concerto Moon (Japanese band)</li>
</ul>
<p>The cluster might be improved by looking at the topics of the lyrics and work from that.</p>
<h2>Sentiment analysis</h2>
<p>After having looked into the words, let’s feel them. I use VADER (Valence Aware Dictionary and sEntiment Reasoner) to analyze the sentiments in the lyrics. The datasets VADER was built upon include tweets, movie/amazon reviews and New York Times editorials. The authors claim that it’s “specifically attuned to sentiments expressed in social media” <a href="https://github.com/cjhutto/vaderSentiment">[6]</a> but it shouldn’t impact much my analysis.</p>
<h3>Positive sentiments</h3>
<p>Out of 58 bands, only 20 bands were considered overall as positive. I’m not surprised by this as the lyrics in power metal are about epic themes which are not per se the most joyous theme.</p>
<h4>Songs</h4>
<p><strong>The most positive songs in the dataset</strong></p>
<ol>
<li>Dark Moor   -      There's Something In The Skies</li>
<li>Sonata Arctica  -   I Have A Right</li>
<li>Sonata Arctica  -   Half A Marathon Man</li>
<li>Power Quest     -    Sacred Land</li>
<li>Freedom Call    -     A Perfect Day</li>
</ol>
<h4>Bands</h4>
<ol>
<li>Twilight Force</li>
<li>Freedom Call</li>
<li>Nightwish</li>
<li>Fairyland</li>
<li>Power Quest</li>
</ol>
<h3>Negative sentiments</h3>
<h4>Songs</h4>
<p><strong>The most negative songs in the dataset</strong></p>
<ul>
<li>Gamma Ray    -     Condemned To Hell</li>
<li>Dragonforce   -      War!</li>
<li>Sabaton     -    Burn In Hell</li>
<li>Edguy       -      Sacred Hell</li>
<li>Running Wild     -    Genocide</li>
</ul>
<p>Just looking at the titles of the songs are enough to say we are not talking about bunnies and sweet cakes.</p>
<h4>Bands</h4>
<ul>
<li>Sabaton</li>
<li>Judicator</li>
<li>Hibria</li>
<li>Dragonforce</li>
<li>Powerwolf</li>
</ul>
<h2>Conclusion</h2>
<p>Power Metal is not that into dragons (47th rank). Indeed Power Metal has a fantasy and epic style where of course dragons don't clash with the theme.
The project was fun to do and I discovered new bands along the way. There will be some updates on the analysis in the future, as I might add new bands and build on people’s feedbacks. I plan to release the source code I used for this analysis after some cleaning.</p>
<p>If you want me to add your favorite band(s), <a href="mailto:notes@atomutek.org?subject=[power%20metal]What%20about%20my%20favorite%20band?">send me an email</a> and I will try my best to add it/them!</p>
<p>Meanwhile keep 🤘!</p>
<h3>Acknowledgement</h3>
<ul>
<li><a href="https://jabalazs.github.io/">Jorge</a>, for the great comments and advice</li>
</ul>
<h2>Further reading</h2>
<ul>
<li><a href="https://helda.helsinki.fi/bitstream/handle/10138/136524/keywords.pdf">Jesse Taina - Keywords in heavy metal lyrics (2014)</a></li>
<li><a href="https://paulelvers.com/post/emotionsineuropeanmusic/">Sentiment analysis of musical taste: a cross-European comparison (2018)</a></li>
<li><a href="https://www.reddit.com/r/PowerMetal/comments/4dlw6v/">Dragonforce lyrical analysis (2016)</a></li>
<li><a href="https://www.reddit.com/r/PowerMetal/comments/4bxppd/">Most commonly used words in power metal lyrics (2016)</a></li>
</ul>
<h2>References</h2>
<ol>
<li><a href="https://pudding.cool/2017/02/vocabulary/index.html">https://pudding.cool/2017/02/vocabulary/index.html</a></li>
<li><a href="https://www.reddit.com/r/PowerMetal/wiki/essential">https://www.reddit.com/r/PowerMetal/wiki/essential</a></li>
<li><a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">https://en.wikipedia.org/wiki/Tf-idf</a></li>
<li><a href="https://www.degeneratestate.org/posts/2016/Apr/20/heavy-metal-and-natural-language-processing-part-1/">https://www.degeneratestate.org/posts/2016/Apr/20/heavy-metal-and-natural-language-processing-part-1/</a></li>
<li><a href="https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics">https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics</a></li>
<li><a href="https://github.com/cjhutto/vaderSentiment">https://github.com/cjhutto/vaderSentiment</a></li>
</ol>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Quality is a hard sell in big tech (131 pts)]]></title>
            <link>https://www.pcloadletter.dev/blog/big-tech-quality/</link>
            <guid>39489519</guid>
            <pubDate>Sat, 24 Feb 2024 06:09:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pcloadletter.dev/blog/big-tech-quality/">https://www.pcloadletter.dev/blog/big-tech-quality/</a>, See on <a href="https://news.ycombinator.com/item?id=39489519">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="skip">
			


<ul>
	<li><time datetime="2024-02-23">23 February 2024</time></li>
</ul>

<p>I have noticed a trend in a handful of products I've worked on at big tech companies. I have friends at other big tech companies that have noticed a similar trend: The products are kind of crummy.</p>
<p>Here are some experiences that I have often encountered:</p>
<ul>
<li>the UI is flakey and/or unintuitive</li>
<li>there is a lot of cruft in the codebase that has never been cleaned up</li>
<li>bugs that have "acceptable" workarounds that never get fixed</li>
<li>packages/dependencies are badly out of date</li>
<li>the developer experience is crummy (bad build times, easily breakable processes)</li>
</ul>
<p>One of the reasons I have found for these issues is that we simply aren't investing enough time to increase product quality: we have poorly or nonexistent quality metrics, invest minimally in testing infrastructure (and actually writing tests), and don't invest in improving the inner loop. But why is this?</p>
<p>My experience has been that quality is simply a hard sell in bigh tech.</p>
<p>Let's first talk about something that's an <em>easy</em> sell right now: AI everything. Why is this an easy sell? Well, Microsoft could announce they put ChatGPT in a toaster and their stock price would jump $5/share. The sad truth is that big tech is hyper-focused on doing the things that make their stock prices go up in the short-term.</p>
<p>It's hard to make this connection with quality initiatives. If your software is slightly less shitty, the stock price won't jump next week. So instead of being able to sell the obvious benefit of shiny new features, you need to have an Engineering Manager willing to risk having lower <a href="https://www.pcloadletter.dev/blog/impact-based-performance-evaluation">impact</a> for the sake of having a better product. Even if there is broad consensus in your team, group, org that these quality improvements are necessary, there's a point up the corporate hierarchy where it simply doesn't matter to them. Certainly not as much as shipping some feature to great fanfare.</p>
<h2 id="part-of-a-bigger-strategy" tabindex="-1">Part of a bigger strategy? <a href="#part-of-a-bigger-strategy">#</a></h2>
<p>Cory Doctorow has <a href="https://pluralistic.net/2023/11/22/who-wins-the-argument/#corporations-are-people-my-friend">said some interesting things</a> about <em>enshittification</em> in big tech:</p>
<p><em>"enshittification is a three-stage process: first, surpluses are allocated to users until they are locked in. Then they are withdrawn and given to business-customers until they are locked in. Then all the value is harvested for the company's shareholders, leaving just enough residual value in the service to keep both end-users and business-customers glued to the platform."</em></p>
<p>At a macro level, it's possible this is the strategy: hook users initially, make them dependent on your product, and then cram in superficial features that make the stock go up but don't offer real value, and keep the customers simply because they really have no choice but to use your product (an enterprise Office 365 customer probably isn't switching anytime soon).</p>
<p>This does seem to have been a good strategy in the <em>short-term</em>: look at Microsoft's stock ever since they started cranking out AI everything. But how can the quality corner-cutting work long-term?</p>
<h2 id="i-hope-the-hubris-will-backfire" tabindex="-1">I hope the hubris will backfire <a href="#i-hope-the-hubris-will-backfire">#</a></h2>
<p>Something will have to give. Big tech products can't just keep getting shittier—can they? I'd like to think some smaller competitors will come eat their lunch, but I'm not sure. Hopefully we're not all too entrenched in the big tech ecosystem for this to happen.</p>

<ul><li>Previous: <a href="https://www.pcloadletter.dev/blog/coding-interviews/">Coding interviews are effective</a></li>
</ul>



		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Please Make Your Table Headings Sticky (267 pts)]]></title>
            <link>https://btxx.org/posts/Please_Make_Your_Table_Headings_Sticky/</link>
            <guid>39488836</guid>
            <pubDate>Sat, 24 Feb 2024 03:16:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://btxx.org/posts/Please_Make_Your_Table_Headings_Sticky/">https://btxx.org/posts/Please_Make_Your_Table_Headings_Sticky/</a>, See on <a href="https://news.ycombinator.com/item?id=39488836">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="pagebody" role="main" class="page">
<p>I often stumble upon large data sets or table layouts across the web. When these tables contain hundreds of rows of content, things become problematic once you start to scroll...</p>

<p><video width="100%" controls="">
  <source src="https://btxx.org/posts/Please_Make_Your_Table_Headings_Sticky/not-fixed-header-tables.mp4" type="video/mp4">
Your browser does not support the video tag.
</video> </p>

<p>Look at that table header disappear! Now, if I scroll all the way down to item #300 (for example) will I remember what each column's data is associated with? If this is my first time looking at this table - probably not. Luckily we can fix this (no pun intended!) with a tiny amount of CSS.</p>



<p>Check it out:</p>

<p><video width="100%" controls="">
  <source src="https://btxx.org/ikiwiki/git/fixed-header-tables.mp4" type="video/mp4">
Your browser does not support the video tag.
</video> </p>

<p>Pretty awesome, right? It might look like magic but it's actually very easy to implement. You only need to add 2 CSS properties on your <code>thead</code>:</p>

<pre><code>position: sticky;
top: 0;
</code></pre>

<p>That's it! Best of all, <code>sticky</code> has <a href="https://caniuse.com/?search=sticky">~96% global support</a> which means this isn't some "bleeding-edge" property and can safely support a ton of browsers. Not to mention the improved experience for your end-users!</p>

<p>You can view a live demo of this table on the <a href="https://codepen.io/bradleytaunt/pen/bGZyJBj">CodePen example pen</a>.</p>

<p>If you found this interesting, feel free to check out my other table-focused post: <a href="https://btxx.org/posts/tables/">Making Tables Responsive With Minimal CSS</a></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to lose two jobs in one year (213 pts)]]></title>
            <link>https://jbennetcodes.medium.com/how-to-lose-two-jobs-in-one-year-e8e428702b91</link>
            <guid>39488833</guid>
            <pubDate>Sat, 24 Feb 2024 03:16:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jbennetcodes.medium.com/how-to-lose-two-jobs-in-one-year-e8e428702b91">https://jbennetcodes.medium.com/how-to-lose-two-jobs-in-one-year-e8e428702b91</a>, See on <a href="https://news.ycombinator.com/item?id=39488833">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a rel="noopener follow" href="https://jbennetcodes.medium.com/?source=post_page-----e8e428702b91--------------------------------"><div aria-hidden="false"><p><img alt="Irina Truong" src="https://miro.medium.com/v2/resize:fill:88:88/1*WG91mjIEby9z6Yu0ehXv9Q.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div><p id="3b79">and learn to accept imperfection</p><figure><figcaption>generated with <a href="https://replicate.com/stability-ai/stable-diffusion" rel="noopener ugc nofollow" target="_blank">https://replicate.com/stability-ai/stable-diffusion</a></figcaption></figure><h2 id="4b17">Elastic</h2><p id="c325">The year 2022 was stressful. My family was trying and failing to buy a house, because the housing market was insane. Of course, this was a first-world problem. On February 24, 2022, the Russian army invaded Ukraine, my home country, which I left in 2010, but still consider home. Friends and relatives lost homes, jobs, and any security in their future.</p><p id="22cc">But to me personally, what happened on November 30 of that year was the biggest shock of all.</p><p id="5d30">I was in the middle of my work day. I noticed an email dropped in my inbox, and I opened Gmail to read it. The email cut right to the chase:</p><blockquote><p id="d887">Hello,</p><p id="7eac">Earlier today, Elastic announced that we are reducing our team by 13%, and unfortunately, you’ve been included in this action.</p></blockquote><p id="c730">The rest of it was details. I love details. The details are great. They give you the sense of being grounded in reality, even though the brain is trying to reject it. However, I was not able to process the details just yet. I had too many feelings to get over. So I screamed and cried.</p><p id="d2a6">My husband ran over and started asking questions; he probably thought someone died. To me, it was almost like someone did: my identity as a software engineer. I had been a software engineer for twenty years, most of them in senior and lead positions, and I believed that good engineers don’t get laid off.</p><p id="5b2e">I was naive.</p><p id="d8eb">As layoffs go, Elastic was great. I was out of work immediately, but I was kept on payroll and benefits for the month of December. I also received 14 weeks of severance pay and 6 months of healthcare coverage.</p><p id="a9c9">I cried for a few days, but I had to pull myself together and start looking for a new job ASAP. Of course, we postponed our house search. We had savings, but I didn’t want to dip into them, and I also was the one to provide medical insurance coverage for my whole family, because my husband worked as a contractor with no benefits.</p><p id="1759">I had a lot of connections with my ex-coworkers. One of my coworkers from <a href="https://www.parse.ly/" rel="noopener ugc nofollow" target="_blank">Parse.ly</a> was just hired at <a href="https://www.coiled.io/" rel="noopener ugc nofollow" target="_blank">Coiled</a>, a data engineering startup. Coiled is the company behind Dask, a distributed data processing framework written in Python. I have been interested in Dask for a long time, so I applied for one of their open positions. I was able to pass their interviews, and I started a new job as a backend engineer in Coiled in January 2023.</p><h2 id="eec2">Coiled</h2><p id="e42f">I learned a lot while working at Coiled, but it was very different from Elastic. Elastic was a large company, where every process was structured and formalized. Coiled was a small startup where directions changed all the time, and nothing was ever clear-cut. In addition, I still felt shell-shocked after the Elastic layoff. I no longer had confidence that I was a good engineer. So I thought, perhaps I should keep a low profile, listen more, speak less, work hard, and concentrate on being as useful as possible.</p><p id="d99e">To this day, I don’t know what went wrong with Coiled. Perhaps the decision to keep a low profile was wrong. Perhaps I wasn’t useful enough. I never felt that the more senior engineers on the team fully accepted me; I didn’t have a feeling of belonging. Still, some of the teammates I worked with were great, we collaborated very well together, and I believed that the rest of the team would accept me too, once I became more experienced with the project and could deliver more value.</p><p id="96a0">In July 2023, my manager started a regularly scheduled 1:1 (on Zoom, because everyone was remote) with the following phrase:</p><blockquote><p id="b1d8">I have bad news. We have to let you go.</p></blockquote><p id="eb31">I wish I had taken the news well (I didn’t). Still, the second time around the blow was less sharp. I guess humans can get used to anything.</p><h2 id="9b9b">A mass</h2><p id="b458">This time, I didn’t have as much protection in terms of severance pay. Coiled gave me 6 weeks —not too bad for a startup. I started searching for a new job immediately, but I was not as lucky. It took me a lot longer to find a new job. It was August 2023, more and more companies embraced the frugal mindset, and the market was flooded with engineers laid off from Twitter, Google, Meta, etc.</p><p id="3f14">While I was looking, something else happened.</p><p id="3d51">I needed to have surgery (not life-threatening). With Coiled, I was covered by medical insurance until the end of August, and I asked the surgeon to please try and schedule it while still within coverage. As part of a pre-surgery checkup, the doctor sent me to have a lung X-ray. An hour after the X-ray appointment, when I had barely got back, I received a call from the doctor.</p><blockquote><p id="0684">“The X-ray shows a 3 cm mass in your lung. You need to schedule a lung CT as soon as you can. You might have to postpone this surgery.”</p></blockquote><p id="1a12">I asked what kind of mass it was; she said it was not clear without further testing. I called the X-ray place. The earliest CT date they could give me was ten days away.</p><p id="30ba">They were the longest ten days of my life. I kept thinking how, if the worst came to pass, my kids (daughter and son, then 9 and 3 years old) would have to grow up without their mom, and how hard it would be for them to see me dying. I was still studying, doing job interviews, and handling recruiter phone calls.</p><p id="2c59">Fun stuff, eh?</p><p id="0f51">After the CT, I received a phone call from my doctor within hours:</p><blockquote><p id="9a00">“Ms Truong, can you talk? It’s good news.”</p><p id="5dc2">“Yes, of course”.</p><p id="bc00">“The mass in your lung, it’s scar tissue. Have you ever had pneumonia?”</p><p id="8b2a">“Yes, I had it as a student, around twenty years old.”</p><p id="431c">“Well, that’s probably what did it. Everything is well. You are clear to have your surgery.”</p></blockquote><p id="0c4e">It’s hard to describe what I felt. I was very happy, but it was more than just being happy for myself. I was (irrationally) happy for my kids. They won’t have to grow up orphans. Not this time, Universe. Not this time.</p><p id="2c5c">The surgery happened on schedule, within the insurance coverage, and was a success. The recovery was painful and took weeks, but that was expected.</p><p id="a2ce">Something else changed. Suddenly, the two layoffs stopped being of any consequence. They quite frankly didn’t matter anymore.</p><blockquote><p id="d018">“Frankly, my dear, I don’t give a damn.”</p><p id="12cd">Rhett Butler, Gone with the Wind (1939 movie)</p></blockquote><h2 id="7986">Silence</h2><p id="29eb">The ups and downs of 2022 and 2023 shook my confidence badly. I didn’t feel like I could or should write articles anymore. I was still wondering if I could be of any value to any company again, or anyone else, for that matter. The happiness of not dying gave me some peace of mind, but it wasn’t enough to overcome all the other things that happened. So I stopped writing about software development, data analysis, or anything else; I didn’t sign up for any conferences or meetups; I didn’t regain the ability or desire to speak in public. I hid.</p><p id="6ab2">This is not a story of a hero. This is not a story about conquering all obstacles. This is a story about imperfection. About struggling, stumbling, and blundering through life.</p><p id="d505">I accepted a few axioms:</p><ul><li id="28f0">Anything can happen to anyone. War? It can happen to me. Cancer? It can happen to me. I can’t control everything.</li><li id="f2b4">I will never be able to protect my kids from everything either.</li><li id="6cde">My work is only part of my identity.</li><li id="30bc">I am not, and will never be perfect.</li><li id="95b5">Change is inevitable.</li><li id="fee9">I’m not alone.</li></ul><p id="cd58">A lot of friends stepped up and helped with advice, referrals, or simply words of support and encouragement during this time. I’m immensely grateful to all of them.</p><p id="ce35">The war in Ukraine is still ongoing. My family still doesn’t own a home. However, I did eventually find a new job. I have a great team, a challenging product to work on, and a source of income again. I’m absorbed in my work and starting to breathe again. And the feeling that I have something to say that’s worth listening to is starting to return.</p><p id="c7ca">Thank you for listening.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Ultimate Guide to PostgreSQL Data Change Tracking (106 pts)]]></title>
            <link>https://exaspark.medium.com/the-ultimate-guide-to-postgresql-data-change-tracking-c3fa88779572</link>
            <guid>39488719</guid>
            <pubDate>Sat, 24 Feb 2024 02:54:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://exaspark.medium.com/the-ultimate-guide-to-postgresql-data-change-tracking-c3fa88779572">https://exaspark.medium.com/the-ultimate-guide-to-postgresql-data-change-tracking-c3fa88779572</a>, See on <a href="https://news.ycombinator.com/item?id=39488719">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a rel="noopener follow" href="https://exaspark.medium.com/?source=post_page-----c3fa88779572--------------------------------"><div aria-hidden="false"><p><img alt="exAspArk" src="https://miro.medium.com/v2/resize:fill:88:88/1*xykLd-j1bsw7K_ZFcSxe1Q.png" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div><p id="0b01">PostgreSQL, one of the most popular databases, was named DBMS of the Year 2023 by <a href="https://db-engines.com/en/blog_post/106" rel="noopener ugc nofollow" target="_blank">DB-Engines Ranking</a> and is used more than any other database among startups according to <a href="https://www.hntrends.com/2024/january.html?compare=SQL+Server" rel="noopener ugc nofollow" target="_blank">HN Hiring Trends</a>.</p><figure><figcaption>PostgreSQL is the most popular database among startups</figcaption></figure><p id="c97f">The SQL standard has included features related to <a href="https://en.wikipedia.org/wiki/Temporal_database" rel="noopener ugc nofollow" target="_blank">temporal databases</a> since 2011, which allow storing data changes over time rather than just the current data state. However, relational databases don’t completely follow the standards. In the case of PostgreSQL, it doesn’t support these features, even though there has been a submitted <a href="https://www.postgresql.org/message-id/flat/CALAY4q-cXCD0r4OybD%3Dw7Hr7F026ZUY6%3DLMsVPUe6yw_PJpTKQ%40mail.gmail.com" rel="noopener ugc nofollow" target="_blank">patch</a> with some discussions.</p><p id="59ed">Let’s explore five alternative methods of data change tracking in PostgreSQL available to us in 2024.</p><h2 id="82a0">Triggers and Audit Table</h2><figure><figcaption>A PostgreSQL trigger with an audit table</figcaption></figure><p id="c0ea">PostgreSQL allows adding triggers with custom procedural SQL code performed on row changes with <code>INSERT</code>, <code>UPDATE</code>, and <code>DELETE</code> queries. The official PostgreSQL wiki describes a generic <a href="https://wiki.postgresql.org/wiki/Audit_trigger" rel="noopener ugc nofollow" target="_blank">audit trigger function</a>. Let’s have a quick look at a simplified example.</p><p id="b5ab">First, create a table called <code>logged_actions</code> in a separate schema called <code>audit</code>:</p><pre><span id="ca96">CREATE schema audit;<p>CREATE TABLE audit.logged_actions (<br>  schema_name TEXT NOT NULL,<br>  table_name TEXT NOT NULL,<br>  user_name TEXT,<br>  action_tstamp TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT current_timestamp,<br>  action TEXT NOT NULL CHECK (action IN ('I','D','U')),<br>  original_data TEXT,<br>  new_data TEXT,<br>  query TEXT<br>);</p></span></pre><p id="1c7d">Next, create a function to insert audit records and establish a trigger on a table you wish to track, such as <code>my_table</code>:</p><pre><span id="8630">CREATE OR REPLACE FUNCTION audit.if_modified_func() RETURNS TRIGGER AS $body$<br>BEGIN<br>  IF (TG_OP = 'UPDATE') THEN<br>    INSERT INTO audit.logged_actions (schema_name,table_name,user_name,action,original_data,new_data,query)<br>    VALUES (TG_TABLE_SCHEMA::TEXT,TG_TABLE_NAME::TEXT,session_user::TEXT,substring(TG_OP,1,1),ROW(OLD.*),ROW(NEW.*),current_query());<br>    RETURN NEW;<br>  elsif (TG_OP = 'DELETE') THEN<br>    INSERT INTO audit.logged_actions (schema_name,table_name,user_name,action,original_data,query)<br>    VALUES (TG_TABLE_SCHEMA::TEXT,TG_TABLE_NAME::TEXT,session_user::TEXT,substring(TG_OP,1,1),ROW(OLD.*),current_query());<br>    RETURN OLD;<br>  elsif (TG_OP = 'INSERT') THEN<br>    INSERT INTO audit.logged_actions (schema_name,table_name,user_name,action,new_data,query)<br>    VALUES (TG_TABLE_SCHEMA::TEXT,TG_TABLE_NAME::TEXT,session_user::TEXT,substring(TG_OP,1,1),ROW(NEW.*),current_query());<br>    RETURN NEW;<br>  END IF;<br>END;<br>$body$<br>LANGUAGE plpgsql;<p>CREATE TRIGGER my_table_if_modified_trigger<br>AFTER INSERT OR UPDATE OR DELETE ON my_table<br>FOR EACH ROW EXECUTE PROCEDURE if_modified_func();</p></span></pre><p id="dd2d">Once it’s done, row changes made in <code>my_table</code> will create records in <code>audit.logged_actions</code>:</p><pre><span id="ca4a">INSERT INTO my_table(x,y) VALUES (1, 2);<br>SELECT * FROM audit.logged_actions;</span></pre><p id="9d98">If you want to further improve this solution by using JSONB columns instead of TEXT, ignoring changes in certain columns, pausing auditing a table, and so on, check out the SQL example in this <a href="https://github.com/2ndQuadrant/audit-trigger" rel="noopener ugc nofollow" target="_blank">audit-trigger</a> repo and its forks.</p><h2 id="ef6d">Downsides</h2><ul><li id="3a13">Performance. Triggers add performance overhead by inserting additional records synchronously on every <code>INSERT</code>, <code>UPDATE</code>, and <code>DELETE</code> operation.</li><li id="54dc">Security. Anyone with superuser access can modify the triggers and make unnoticed data changes. It is also recommended to make sure that records in the audit table cannot be modified or removed.</li><li id="f890">Maintenance. Managing complex triggers across many constantly changing tables can become cumbersome. Making a small mistake in an SQL script can break queries or data change tracking functionality.</li></ul><h2 id="cc80">Triggers and Notify/Listen</h2><figure><figcaption>A PostgreSQL trigger with Notify</figcaption></figure><p id="570f">This approach is similar to the previous one but instead of writing data changes in the audit table directly, we pass them through a pub/sub mechanism through a trigger to another system dedicated to reading and storing these data changes:</p><pre><span id="b228">CREATE OR REPLACE FUNCTION if_modified_func() RETURNS TRIGGER AS $body$<br>BEGIN<br>  IF (TG_OP = 'UPDATE') THEN<br>    PEFORM pg_notify('data_changes', json_build_object(<br>      'schema_name', TG_TABLE_SCHEMA::TEXT,<br>      'table_name', TG_TABLE_NAME::TEXT,<br>      'user_name', session_user::TEXT,<br>      'action', substring(TG_OP,1,1),<br>      'original_data', jsonb_build(OLD),<br>      'new_data', jsonb_build(NEW)<br>    )::TEXT);<br>    RETURN NEW;<br>  elsif (TG_OP = 'DELETE') THEN<br>    PEFORM pg_notify('data_changes', json_build_object(<br>      'schema_name', TG_TABLE_SCHEMA::TEXT,<br>      'table_name', TG_TABLE_NAME::TEXT,<br>      'user_name', session_user::TEXT,<br>      'action', substring(TG_OP,1,1),<br>      'original_data', jsonb_build(OLD)<br>    )::TEXT);<br>    RETURN OLD;<br>  elsif (TG_OP = 'INSERT') THEN<br>    PEFORM pg_notify('data_changes', json_build_object(<br>      'schema_name', TG_TABLE_SCHEMA::TEXT,<br>      'table_name', TG_TABLE_NAME::TEXT,<br>      'user_name', session_user::TEXT,<br>      'action', substring(TG_OP,1,1),<br>      'new_data', jsonb_build(NEW)<br>    )::TEXT);<br>    RETURN NEW;<br>  END IF;<br>END;<br>$body$<br>LANGUAGE plpgsql;<p>CREATE TRIGGER my_table_if_modified_trigger<br>AFTER INSERT OR UPDATE OR DELETE ON my_table<br>FOR EACH ROW EXECUTE PROCEDURE if_modified_func();</p></span></pre><p id="7b86">Now it’s possible to run a separate process running as a worker that listens to messages containing data changes and stores them separately:</p><pre><span id="eeb5">LISTEN data_changes;</span></pre><h2 id="cffa">Downsides</h2><ul><li id="9bfb">“At most once” delivery<strong>.</strong> Listen/notify notifications are not persisted meaning if a listener disconnects, it may miss updates that happened before it reconnected again.</li><li id="5371">Payload size limit. Listen/notify messages have a maximum payload size of 8000 bytes by default. For larger payloads, it is recommended to store them in the DB audit table and send only references of the records.</li><li id="242a">Debugging. Troubleshooting issues related to triggers and listen/notify in a production environment can be challenging due to their asynchronous and distributed nature.</li></ul><h2 id="7a5b">Application-Level Tracking</h2><figure><figcaption>Application-level tracking with a PostgreSQL audit table</figcaption></figure><p id="17ed">If you have control over the codebase that connects and makes data changes in a PostgreSQL database, then one of the following options is also available to you:</p><ul><li id="5a3c">Manually record all data changes when issuing <code>INSERT</code>, <code>UPDATE</code>, and <code>DELETE</code> queries</li><li id="d590">Use existing open-source libraries that integrate with popular ORMs</li></ul><p id="5af4">For example, there is <a href="https://github.com/paper-trail-gem/paper_trail" rel="noopener ugc nofollow" target="_blank">paper_trail</a> for Ruby on Rails with ActiveRecord and <a href="https://github.com/jazzband/django-simple-history" rel="noopener ugc nofollow" target="_blank">django-simple-history</a> for Django. At a high level, they use callbacks or middlewares to insert additional records into an audit table. Here is a simplified example written in Ruby:</p><pre><span id="2aa4">class User &lt; ApplicationRecord<br>  after_commit :track_data_changes<p>  private</p><p>  def track_data_changes<br>    AuditRecord.create!(auditable: self, changes: changes)<br>  end<br>end</p></span></pre><p id="9927">On the application level, <a href="https://martinfowler.com/eaaDev/EventSourcing.html" rel="noopener ugc nofollow" target="_blank">Event Sourcing</a> can also be implemented with an append-only log as the source of truth. But it’s a separate, big, and exciting topic that deserves a separate blog post.</p><h2 id="13aa">Downsides</h2><ul><li id="7281">Reliability. Application-level data change tracking is not as accurate as database-level change tracking. For example, data changes made outside an app will not be tracked, developers may accidentally skip callbacks, or there could be data inconsistencies if a query changing the data has succeeded but a query inserting an audit record failed.</li><li id="d197">Performance. Manually capturing changes and inserting them in the database via callbacks leads to both runtime application and database overhead.</li><li id="f949">Scalability. These audit tables are usually stored in the same database and can quickly become unmanageable, which can require separating the storage, implementing declarative partitioning, and continuous archiving.</li></ul><h2 id="df40">Change Data Capture</h2><p id="7fef"><a href="https://en.wikipedia.org/wiki/Change_data_capture" rel="noopener ugc nofollow" target="_blank">Change Data Capture</a> (CDC) is a pattern of identifying and capturing changes made to data in a database and sending those changes to a downstream system. Most often it is used for <a href="https://en.wikipedia.org/wiki/Extract,_transform,_load" rel="noopener ugc nofollow" target="_blank">ETL</a> to send data to a data warehouse for analytical purposes.</p><p id="3fae">There are multiple approaches to implementing CDC. One of them, which doesn’t intersect with what we have already discussed, is a log-based CDC. With PostgreSQL, it is possible to connect to the <a href="https://www.postgresql.org/docs/current/wal-intro.html" rel="noopener ugc nofollow" target="_blank">Write-Ahead Log</a> (WAL) that is used for data durability, recovery, and replication to other instances.</p><figure><figcaption>CDC with PostgreSQL logical replication</figcaption></figure><p id="2f27">PostgreSQL supports two types of replications: physical replication and logical replication. The latter allows decoding WAL changes on a row level and filtering them out, for example, by table name. This is exactly what we need to implement data change tracking with CDC.</p><p id="5cad">Here are the basic steps necessary for retrieving data changes by using logical replication:</p><p id="af59">1. Set <code>wal_level</code> to <code>logical</code> in <code>postgresql.conf</code> and restart the database.</p><p id="258c">2. Create a publication like a “pub/sub channel” for receiving data changes:</p><pre><span id="0d95">CREATE PUBLICATION my_publication FOR ALL TABLES;</span></pre><p id="8f8c">3. Create a logical replication slot like a “cursor position” in the WAL:</p><pre><span id="dfec">SELECT * FROM pg_create_logical_replication_slot('my_replication_slot', 'wal2json')</span></pre><p id="11b9">4. Fetch the latest unread changes:</p><pre><span id="ea6c">SELECT * FROM pg_logical_slot_get_changes('my_replication_slot', NULL, NULL)</span></pre><p id="a137">To implement log-based CDC with PostgreSQL, I would recommend using the existing open-source solutions. The most popular one is <a href="https://github.com/debezium/debezium" rel="noopener ugc nofollow" target="_blank">Debezium</a>.</p><h2 id="0a5e">Downsides</h2><ul><li id="6b4d">Limited context. PostgreSQL WAL contains only low-level information about row changes and doesn’t include information about an SQL query that triggered the change, information about a user, or any application-specific context.</li><li id="aefd">Complexity. Implementing CDC adds a lot of system complexity. This involves running a server that connects to PostgreSQL as a replica, consumes data changes, and stores them somewhere.</li><li id="e7e7">Tuning. Running it in a production environment may require a deeper understanding of PostgreSQL internals and properly configuring the system. For example, periodically flushing the position for a replication slot to reclaim WAL disk space.</li></ul><h2 id="6aa3">Integrated Change Data Capture</h2><figure><figcaption>Integrated CDC with application context</figcaption></figure><p id="5539">To overcome the challenge of limited information about data changes stored in the WAL, we can use a clever approach of passing additional context to the WAL directly.</p><p id="6fbd">Here is a simple example of passing additional context on row changes:</p><pre><span id="8e85">CREATE OR REPLACE FUNCTION if_modified_func() RETURNS TRIGGER AS $body$<br>BEGIN<br>  PERFORM pg_logical_emit_message(true, 'my_message', 'ADDITIONAL_CONTEXT');<p>  IF (TG_OP = 'DELETE') THEN<br>    RETURN OLD;<br>  ELSE<br>    RETURN NEW;<br>  END IF;<br>END;<br>$body$<br>LANGUAGE plpgsql;</p><p>CREATE TRIGGER my_table_if_modified_trigger<br>AFTER INSERT OR UPDATE OR DELETE ON my_table<br>FOR EACH ROW EXECUTE PROCEDURE if_modified_func();</p></span></pre><p id="525c">Notice the <code>pg_logical_emit_message</code> function that was added to PostgreSQL as an internal function for plugins. It allows namespacing and emitting messages that will be stored in the WAL. Reading these messages became possible with the standard logical decoding plugin <code>pgoutput</code> since PostgreSQL v14.</p><p id="b783">There is an open-source project called <a href="https://github.com/BemiHQ/bemi" rel="noopener ugc nofollow" target="_blank">Bemi</a> which allows tracking not only low-level data changes but also reading any custom context with CDC and stitching everything together. Full disclaimer, I’m one of the core contributors.</p><p id="ee57">For example, it can integrate with popular ORMs and adapters to pass application-specific context with all data changes:</p><pre><span id="dca3">import { setContext } from "@bemi-db/prisma";<br>import express, { Request } from "express";<p>const app = express();</p><p>app.use(<br>  // Customizable context<br>  setContext((req: Request) =&gt; ({<br>    userId: req.user?.id,<br>    endpoint: req.url,<br>    params: req.body,<br>  }))<br>);</p></span></pre><h2 id="b6d1">Downsides</h2><ul><li id="2b7b">Complexity and tuning related to implementing CDC.</li></ul><p id="59c2">If you need a ready-to-use cloud solution that can be integrated and connected to PostgreSQL in a few minutes, check out <a href="https://bemi.io/" rel="noopener ugc nofollow" target="_blank">bemi.io</a>.</p><h2 id="14d1">Conclusion</h2><figure><figcaption>PostgreSQL data change tracking approach comparison</figcaption></figure><ol><li id="6feb">If you need basic data change tracking, <strong>triggers with an audit table</strong> are a great initial solution.</li><li id="776f"><strong>Triggers with listen/notify</strong> are a good option for simple testing in a development environment.</li><li id="6b4e">If you value application-specific context (information about a user, API endpoint, etc.) over reliability, you can use <strong>application-level tracking</strong>.</li><li id="f97e"><strong>Change Data Capture</strong> is a good option if you prioritize reliability and scalability as a unified solution that can be reused, for example, across many databases.</li><li id="329d">Finally, <strong>integrated Change Data Capture </strong>is your best bet if you need a robust data change tracking system that can also be integrated into your application. Go with <a href="https://bemi.io/" rel="noopener ugc nofollow" target="_blank">bemi.io</a> if you need a cloud-managed solution.</li></ol></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT in 500 Lines of SQL (786 pts)]]></title>
            <link>https://explainextended.com/2023/12/31/happy-new-year-15/</link>
            <guid>39488668</guid>
            <pubDate>Sat, 24 Feb 2024 02:45:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://explainextended.com/2023/12/31/happy-new-year-15/">https://explainextended.com/2023/12/31/happy-new-year-15/</a>, See on <a href="https://news.ycombinator.com/item?id=39488668">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	<p><small><strong>Translations</strong>: <a href="https://habr.com/ru/articles/786276/" rel="noopener" target="_blank">Russian</a></small></p>
<p>This year, the talk of the town was AI and how it can do everything for you.</p>
<p>I like it when someone or something does everything for me. To this end, I decided to ask ChatGPT to write my New Year's post:</p>
<blockquote><p>
"Hey ChatGPT. Can you implement a large language model in SQL?"<br>
"No, SQL is not suitable for implementing large language models. SQL is a language for managing and querying data in relational databases. Implementing a language model requires complex algorithms, large datasets, and often deep learning frameworks, which are beyond the capabilities of SQL."
</p></blockquote>
<p>It just proves that if you want something done right, you have to do it yourself.</p>
<p>Encouraged by this optimistic forecast, today we will implement a large language model in SQL.</p>
<p><img fetchpriority="high" decoding="async" width="700" height="539" data-attachment-id="7315" data-permalink="https://explainextended.com/2023/12/31/happy-new-year-15/kandinsky-download-1703639350496/" data-orig-file="https://explainextended.com/wp-content/uploads/2023/12/kandinsky-download-1703639350496.jpg" data-orig-size="700,539" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Scribe" data-image-description="" data-image-caption="" data-medium-file="https://explainextended.com/wp-content/uploads/2023/12/kandinsky-download-1703639350496-300x231.jpg" data-large-file="https://explainextended.com/wp-content/uploads/2023/12/kandinsky-download-1703639350496.jpg" src="https://explainextended.com/wp-content/uploads/2023/12/kandinsky-download-1703639350496.jpg" alt="" srcset="https://explainextended.com/wp-content/uploads/2023/12/kandinsky-download-1703639350496.jpg 700w, https://explainextended.com/wp-content/uploads/2023/12/kandinsky-download-1703639350496-300x231.jpg 300w" sizes="(max-width: 700px) 100vw, 700px"></p>
<h3>Theory</h3>
<p>While writing this post, I used the wonderful article <a href="https://jaykmody.com/blog/gpt-from-scratch/" rel="noopener" target="_blank">GPT in 60 Lines of NumPy</a> by Jay Mody. This article explains the inner workings of a GPT model much better than I can hope to do. Still, a little recap is in order.</p>
<h4>What is a generative large language model from a technical perspective?</h4>
<p>A generative LLM is a function. It takes a text string as input (called "prompt" in AI parlance), and returns an array of strings and numbers. Here's what the signature of this function looks like:</p>
<p><code>llm(prompt: str) -&gt; list[tuple[str, float]]</code></p>
<p>This function is deterministic. It does a lot of math under the hood, but all this math is hardwired. If you call it repeatedly with the same input, it will always return the same output.</p>
<p>It may come as a surprise to anyone who's been using ChatGPT and similar products because they can give different answers to the same question. Yet, it's true. We will shortly see how it works.</p>
<h4>What are the values this function returns?</h4>
<p>Something like this:</p>
<pre title="">llm("I wish you a happy New")

0       (' Year', 0.967553)
1       (' Years', 0.018199688)
2       (' year', 0.003573329)
3       (' York', 0.003114716)
4       (' New', 0.0009022804)
…
50252   (' carbohyd', 2.3950911e-15)
50253   (' volunte', 2.2590102e-15)
50254   ('pmwiki', 1.369229e-15)
50255   (' proport', 1.1198108e-15)
50256   (' cumbers', 7.568147e-17)
</pre>
<p>It returns an array of tuples. Each tuple consists of a word (or, rather, a string) and a number. The number is the probability that this word will continue the prompt. The model "thinks" that the phrase "I wish you a happy New" will be followed by the character sequence " Year" with a probability of 96.7%, " Years" of 1.8% and so on.</p>
<p>The word "think" above is quoted because, of course, the model doesn't really think. It mechanically returns arrays of words and numbers according to some hardwired internal logic.</p>
<h4>If it's that dumb and deterministic, how can it generate different texts?</h4>
<p>Large language models are used in text applications (chatbots, content generators, code assistants etc). These applications repeatedly call the model and select the word suggested by it (with some degree of randomness). The next suggested word is added to the prompt and the model is called again. This continues in a loop until enough words are generated.</p>
<p>The accrued sequence of words will look like a text in a human language, complete with grammar, syntax and even what appears to be intelligence and reasoning. In this aspect, it is not unlike a <a href="https://en.wikipedia.org/wiki/Discrete-time_Markov_chain" rel="noopener" target="_blank">Markov chain</a> which works on the same principle.</p>
<p>The internals of a large language model are wired up so that the next suggested word will be a natural continuation of the prompt, complete with its grammar, semantics and sentiment. Equipping a function with such a logic became possible through a series of scientific breakthroughs (and programming drudgery) that have resulted in the development of the family of algorithms known as GPT, or Generative Pre-trained Transformer.</p>
<h4>What does "Generative Pre-trained Transformer" mean?</h4>
<p>"Generative" means that it generates text (by adding continuations to the prompt recursively, as we saw earlier).</p>
<p>"Transformer" means that it uses a particular type of neural network, first developed by Google and described in <a href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener" target="_blank">this paper</a>.</p>
<p>"Pre-trained" is a little bit historical. Initially, the ability for the model to continue text was thought of as just a prerequisite for a more specialized task: inference (finding logical connections between phrases), classification (for instance, guessing the number of stars in a hotel rating from the text of the review), machine translation and so on. It was thought that these two parts should have been trained separately, the language part being just a <em>pre-</em>training for a "real" task that would follow.</p>
<p>As the original GPT paper puts it:</p>
<blockquote><p>
We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task.
</p></blockquote>
<p>It was not until later that people realized that, with a model large enough, the second step was often not necessary. A Transformer model, trained to do nothing else than generate texts, turned out to be able to follow human language instructions that were contained in these texts, with no additional training ("fine-tuning" in AI parlance) required.</p>
<p>With that out of the way, let's focus on the implementation.<br>
<span id="more-7143"></span></p>
<h3>Generation</h3>
<p>Here is what happens when we try to generate text from the prompt using GPT2:</p>
<pre title="">def generate(prompt: str) -&gt; str:
  # Transforms a string into a list of tokens.
  tokens = tokenize(prompt) # tokenize(prompt: str) -&gt; list[int]

  while True:

    # Runs the algorithm.
    # Returns tokens' probabilities: a list of 50257 floats, adding up to 1.
    candidates = gpt2(tokens) # gpt2(tokens: list[int]) -&gt; list[float]

    # Selects the next token from the list of candidates
    next_token = select_next_token(candidates)
    # select_next_token(candidates: list[float]) -&gt; int

    # Append it to the list of tokens
    tokens.append(next_token)

    # Decide if we want to stop generating.
    # It can be token counter, timeout, stopword or something else.
    if should_stop_generating():
      break

  # Transform the list of tokens into a string
  completion = detokenize(tokens) # detokenize(tokens: list[int]) -&gt; str
  return completion
</pre>
<p>Let's implement all these pieces one by one in SQL.</p>
<h3>Tokenizer</h3>
<p>Before a text can be fed to a neural network, it needs to be converted into a list of numbers. Of course, that's barely news: that's what text encodings like Unicode do. Plain Unicode, however, doesn't really work well with neural networks.</p>
<p>Neural networks, at their core, do a lot of matrix multiplications and capture whatever predictive powers they have in the coefficients of these matrixes. Some of these matrixes have one row per every possible value in the "alphabet"; others have one row per "character".</p>
<p>Here, the words "alphabet" and "character" don't have the usual meaning. In Unicode, the "alphabet" is 149186 characters long (this is how many different Unicode points there are at the time of this writing), and a "character" can be something like this: ﷽ (yes, that's a single Unicode point number 65021, encoding <a href="https://en.wikipedia.org/wiki/Basmala">a whole phrase in Arabic</a> that is particularly important for the Muslims). Note that the very same phrase could have been written in usual Arabic letters. It means that the same text can have many encodings.</p>
<p>As an illustration, let's take the word "PostgreSQL". If we were to encode it (convert to an array of numbers) using Unicode, we would get 10 numbers that could potentially be from 1 to 149186. It means that our neural network would need to store a matrix with 149186 rows in it and perform a number of calculations on 10 rows from this matrix. Some of these rows (corresponding to the letters of the English alphabet) would be used a lot and pack a lot of information; others, like poop emoji and obscure symbols from dead languages, would hardly be used at all, but still take up space.</p>
<p>Naturally, we want to keep both these numbers, the "alphabet" length and the "character" count, as low as possible. Ideally, all the "characters" in our alphabet should be distributed uniformly, and we still want our encoding to be as powerful as Unicode.</p>
<p>The way we can do that, intuitively, is to assign unique numbers to sequences of words that occur often in the texts we work with. In Unicode, the same religious phrase in Arabic can be encoded using either a single code point, or letter by letter. Since we are rolling our own encoding, we can do the same for the words and phrases that are important for the model (i.e. show up often in texts).</p>
<p>For instance, we could have separate numbers for "Post", "greSQL" and "ing". This way, the words "PostgreSQL" and "Posting" would both have a length of 2 in our representation. And of course, we would still maintain separate code points for shorter sequences and individual bytes. Even if we come across gibberish or a text in a foreign language, it would still be encodable, albeit longer.</p>
<p>GPT2 uses a variation of the algorithm called <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding" rel="noopener" target="_blank">Byte pair encoding</a> to do precisely that. Its tokenizer uses a dictionary of 50257 code points (in AI parlance, "tokens") that correspond to different byte sequences in UTF-8 (plus the "end of text" as a separate token).</p>
<p>This dictionary was built by statistical analysis performed like this:</p>
<ol>
<li>Start with a simple encoding of 256 tokens: one token per byte.</li>
<li>Take a large corpus of texts (preferably the one the model will be trained on).</li>
<li>Encode it.</li>
<li>Calculate which pair of tokens is the most frequent. Let's assume it's 0x20 0x74 (space followed by the lowercase "t").</li>
<li>Assign the next available value (257) to this pair of bytes.</li>
<li>Repeat the steps 3-5, now paying attention to the byte sequences. If a sequence of bytes can be encoded with a complex token, use the complex token. If there are ambiguities (say, "abc" can, at some point, be encoded as "a" + "bc" or "ab" + "c"), use the one with the lowest number (because it was added earlier and hence is more frequent). Do this recursively until all sequences that can collapse into a single token will collapse into a single token.</li>
<li>Perform the collapse 50000 times over.</li>
</ol>
<p>The number 50000 was chosen more or less arbitrarily by the developers. Other models keep the number of tokens in a similar range (from 30k to 100k).</p>
<p>At every iteration of this algorithm, a new token that is a concatenation of two previous ones will be added to the dictionary. Ultimately, we will end up with 50256 tokens. Add a fixed-number token for "end-of-text", and we're done.</p>
<p>The GPT2 version of BTE has another layer of encoding: the token dictionary maps tokens to strings and not arrays of bytes. Mapping from bytes to string characters is defined in <a href="https://github.com/openai/gpt-2/blob/a74da5d99abaaba920de8131d64da2862a8f213b/src/encoder.py#L9-L28" rel="noopener" target="_blank">this function</a>. We will save the dictionary it produces in the table <code>encoder</code>.</p>
<p>Let's see how we can implement the tokenizer in SQL.</p>
<p>The tokenizer is an integral part of GPT2, and the token dictionary can be downloaded from OpenAI's website along with the rest of the model. We will need to import it into the table <code>tokenizer</code>. At the bottom of this post, you will find a link to the code repository. Its code will automate populating database tables needed for the model.</p>
<p>In a recursive CTE, we will split this word into tokens (starting with single bytes) and merge the best adjacent pairs, until there is nothing left to merge. The merging itself happens in a nested recursive CTE.</p>
<p>For the demo, I will use the word "Mississippilessly". Each record in the resultset shows the best pair to collapse found so far, and also the progress through the query.</p>
<pre title="">WITH    RECURSIVE
        bpe AS
        (
        SELECT  (n + 1)::BIGINT AS position, character, TRUE AS continue, 1 AS step,
                NULL::INT AS token, NULL::TEXT AS combined
        FROM    CONVERT_TO('Mississippilessly', 'UTF-8') AS bytes
        CROSS JOIN LATERAL
                GENERATE_SERIES(0, LENGTH(bytes) - 1) AS n
        JOIN    encoder
        ON      byte = GET_BYTE(bytes, n)
        UNION ALL
        (
        WITH    RECURSIVE
                base AS
                (
                SELECT  *
                FROM    bpe
                WHERE   continue
                ),
                bn AS
                (
                SELECT  ROW_NUMBER() OVER (ORDER BY position) AS position,
                        continue,
                        character,
                        character || LEAD(character) OVER (ORDER BY position) AS cluster
                FROM    base
                ),
                top_rank AS
                (
                SELECT  tokenizer.*
                FROM    bn
                CROSS JOIN LATERAL
                        (
                        SELECT  *
                        FROM    tokenizer
                        WHERE   tokenizer.cluster = bn.cluster
                        LIMIT   1
                        ) tokenizer
                ORDER BY
                        token
                LIMIT   1
                ),
                breaks AS
                (
                SELECT  0::BIGINT AS position, 1 AS length
                UNION ALL
                SELECT  bn.position,
                        CASE WHEN token IS NULL THEN 1 ELSE 2 END
                FROM    breaks
                JOIN    bn
                ON      bn.position = breaks.position + length
                LEFT JOIN
                        top_rank
                USING   (cluster)
                )
        SELECT  position, character, token IS NOT NULL,
                (SELECT step + 1 FROM base LIMIT 1), token, top_rank.cluster
        FROM    breaks
        LEFT JOIN
                top_rank
        ON      1 = 1
        CROSS JOIN LATERAL
                (
                SELECT  STRING_AGG(character, '' ORDER BY position) AS character
                FROM    bn
                WHERE   bn.position &gt;= breaks.position
                        AND bn.position &lt; breaks.position + length
                ) bn
        WHERE   position &gt; 0
        )
        )
SELECT  step, MAX(token) AS token, MAX(combined) AS combined, ARRAY_AGG(character ORDER BY position)
FROM    bpe
WHERE   continue
GROUP BY
        step
ORDER BY
        step
</pre>
<div>
<table>
<tbody><tr>
<th>step</th>
<th>token</th>
<th>combined</th>
<th>array_agg</th>
</tr>
<tr>
<td>1</td>
<td>None</td>
<td>None</td>
<td>['M', 'i', 's', 's', 'i', 's', 's', 'i', 'p', 'p', 'i', 'l', 'e', 's', 's', 'l', 'y']</td>
</tr>
<tr>
<td>2</td>
<td>271</td>
<td>is</td>
<td>['M', 'is', 's', 'is', 's', 'i', 'p', 'p', 'i', 'l', 'e', 's', 's', 'l', 'y']</td>
</tr>
<tr>
<td>3</td>
<td>274</td>
<td>es</td>
<td>['M', 'is', 's', 'is', 's', 'i', 'p', 'p', 'i', 'l', 'es', 's', 'l', 'y']</td>
</tr>
<tr>
<td>4</td>
<td>306</td>
<td>ly</td>
<td>['M', 'is', 's', 'is', 's', 'i', 'p', 'p', 'i', 'l', 'es', 's', 'ly']</td>
</tr>
<tr>
<td>5</td>
<td>346</td>
<td>il</td>
<td>['M', 'is', 's', 'is', 's', 'i', 'p', 'p', 'il', 'es', 's', 'ly']</td>
</tr>
<tr>
<td>6</td>
<td>381</td>
<td>pp</td>
<td>['M', 'is', 's', 'is', 's', 'i', 'pp', 'il', 'es', 's', 'ly']</td>
</tr>
<tr>
<td>7</td>
<td>408</td>
<td>ess</td>
<td>['M', 'is', 's', 'is', 's', 'i', 'pp', 'il', 'ess', 'ly']</td>
</tr>
<tr>
<td>8</td>
<td>747</td>
<td>iss</td>
<td>['M', 'iss', 'iss', 'i', 'pp', 'il', 'ess', 'ly']</td>
</tr>
<tr>
<td>9</td>
<td>3974</td>
<td>ipp</td>
<td>['M', 'iss', 'iss', 'ipp', 'il', 'ess', 'ly']</td>
</tr>
<tr>
<td>10</td>
<td>17140</td>
<td>Miss</td>
<td>['Miss', 'iss', 'ipp', 'il', 'ess', 'ly']</td>
</tr>
<tr>
<td>11</td>
<td>30608</td>
<td>iless</td>
<td>['Miss', 'iss', 'ipp', 'iless', 'ly']</td>
</tr>
</tbody></table>
</div>
<p>On each step, the BPE algorithm finds the best pair of tokens to merge and merges them (you can see the merged pair and its rank in the output). This procedure brings down the token space size from Unicode's 150k to 50k, and the number of tokens (in this particular word) from 17 to 5. Both are great improvements.</p>
<p>When working with multiple words, the tokenizer first splits the text into separate words using <a href="https://github.com/openai/gpt-2/blob/master/src/encoder.py#L53" rel="noopener" target="_blank">this regexp</a> and merges the tokens inside each word separately. Unfortunately, PostgreSQL doesn't support Unicode character properties in regexps, so I had to tweak it a little bit (probably killing proper Unicode support in the process). Here's how it looks in SQL:</p>
<pre title="">WITH    input AS
        (
        SELECT  'PostgreSQL is great' AS prompt
        ),
        clusters AS
        (
        SELECT  part_position, bpe.*
        FROM    input
        CROSS JOIN LATERAL
                REGEXP_MATCHES(prompt, '''s|''t|''re|''ve|''m|''ll|''d| ?\w+| ?\d+| ?[^\s\w\d]+|\s+(?!\S)|\s+', 'g') WITH ORDINALITY AS rm (part, part_position)
        CROSS JOIN LATERAL
                (
                WITH    RECURSIVE
                        bpe AS
                        (
                        SELECT  (n + 1)::BIGINT AS position, character, TRUE AS continue
                        FROM    CONVERT_TO(part[1], 'UTF-8') AS bytes
                        CROSS JOIN LATERAL
                                GENERATE_SERIES(0, LENGTH(bytes) - 1) AS n
                        JOIN    encoder
                        ON      byte = GET_BYTE(bytes, n)
                        UNION ALL
                        (
                        WITH    RECURSIVE
                                base AS
                                (
                                SELECT  *
                                FROM    bpe
                                WHERE   continue
                                ),
                                bn AS
                                (
                                SELECT  ROW_NUMBER() OVER (ORDER BY position) AS position,
                                        continue,
                                        character,
                                        character || LEAD(character) OVER (ORDER BY position) AS cluster
                                FROM    base
                                ),
                                top_rank AS
                                (
                                SELECT  tokenizer.*
                                FROM    bn
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  *
                                        FROM    tokenizer
                                        WHERE   tokenizer.cluster = bn.cluster
                                        LIMIT   1
                                        ) tokenizer
                                ORDER BY
                                        token
                                LIMIT   1
                                ),
                                breaks AS
                                (
                                SELECT  0::BIGINT AS position, 1 AS length
                                UNION ALL
                                SELECT  bn.position,
                                        CASE WHEN token IS NULL THEN 1 ELSE 2 END
                                FROM    breaks
                                JOIN    bn
                                ON      bn.position = breaks.position + length
                                LEFT JOIN
                                        top_rank
                                USING   (cluster)
                                )
                        SELECT  position, character, token IS NOT NULL
                        FROM    breaks
                        LEFT JOIN
                                top_rank
                        ON      1 = 1
                        CROSS JOIN LATERAL
                                (
                                SELECT  STRING_AGG(character, '' ORDER BY position) AS character
                                FROM    bn
                                WHERE   bn.position &gt;= breaks.position
                                        AND bn.position &lt; breaks.position + length
                                ) bn
                        WHERE   position &gt; 0
                        )
                        )
                SELECT  position, character AS cluster
                FROM    bpe
                WHERE   NOT continue
                ) bpe
        ),
        tokens AS
        (
        SELECT  token, cluster
        FROM    clusters
        JOIN    tokenizer
        USING   (cluster)
        )
SELECT  *
FROM    tokens
</pre>
<div>
<table>
<tbody><tr>
<th>token</th>
<th>cluster</th>
</tr>
<tr>
<td>6307</td>
<td>Post</td>
</tr>
<tr>
<td>47701</td>
<td>greSQL</td>
</tr>
<tr>
<td>318</td>
<td>Ġis</td>
</tr>
<tr>
<td>1049</td>
<td>Ġgreat</td>
</tr>
</tbody></table>
</div>
<p>The weird character Ġ is the whitespace.</p>
<p>This query tokenizes the prompt and converts it into an array of numbers. This way, the prompt is ready for its journey through the layers of the model.</p>
<h3>Embeddings</h3>
<p>The tokens represent parts of the human languages (about 0.75 words per token, in general), so any model that is trying to succeed at text completion should somehow encode the relationships between these parts. Even in isolation, the parts of the speech have sets of orthogonal properties.</p>
<p>Let's take the word "subpoena" (which happens to have a whole token in itself in the GPT2 tokenizer). Is it a noun? Yes, very much so. Is it a verb? Well, sort of. Is it an adjective? Not that much, but it can be if you squint hard enough. Is it legalese? Hell yes. And so on.</p>
<p>All these properties are orthogonal, i.e. independent of each other. A word can be a legalese noun but not an adjective or a verb. In English, any combination thereof can happen.</p>
<p>Things with orthogonal properties are best encoded using vectors. Instead of having a single property (like a token number), we can have many. And it helps if we can wiggle them as we want. For instance, for a word to continue the phrase "A court decision cited by the lawyer mentions the …" we would probably want something that's heavy on the legalese dimension and at the same time heavy on being a noun. We don't really care if it has a side hustle being an adjective, a verb, or a flower.</p>
<p>In math, mapping narrower values into wider spaces (such as token IDs to vectors) is called an <a href="https://en.wikipedia.org/wiki/Embedding" rel="noopener" target="_blank">embedding</a>. This is exactly what we are doing here.</p>
<p>How do we decide which properties these vectors represent? We don't. We just provide enough vector space for every token and hope that the model during its training phase will populate these dimensions with something meaningful. GPT2 uses 768 dimensions for its vectors. There is no telling in advance (and, actually, even in the retrospective) what property of the word will, say, the dimension 247 encode. Surely it would encode something, but it's not easy to tell what it is.</p>
<p>What properties of each token do we want to embed in the vector space? Anything that has any bearing on what the next token would be.</p>
<p>Token id? Of course. Different tokens mean different things.</p>
<p>Position of the token in the text? Yes, please. "Blue violet" and "violet blue" are not the same thing.</p>
<p>Relationships of tokens to each other? Sure! That's, probably, the most important part of the job, and the Attention block of the Transformer architecture was the first one to get it right.</p>
<p>Tokens and positions are easy to embed. Let's say we have the phrase "PostgreSQL is great", which, as we already know, maps to four tokens: <code>[6307, 47701, 318, 1049]</code>.</p>
<p>Among other parameters of GPT2, there are two matrixes called WTE (word token embedding) and WPE (word position embedding). As the names suggest, the former stores embeddings of the tokens, and the latter stores embeddings of the positions. The actual values of these embeddings have been populated ("learned") during the training of GPT2. As far as we are concerned, they are constants that live in the database tables <code>wte</code> and <code>wpe</code>.</p>
<p>WTE is 50257×768 and WPE is 1024×768. The latter means that the maximum number of tokens that we can use in a prompt to GPT2 is 1024. If we provide more tokens in the prompt, we just won't be able to pull positional embeddings for them. It's an architectural aspect ("hyperparameter" in AI parlance) of the model that is set at design time and cannot be changed by training. When people talk about the "context window" of an LLM, they mean this number.</p>
<p>We have the token 6307 at place 0, 47701 at 1, 318 at 2, and 1049 at 3. For each of these tokens and positions, we have two vectors: one from WTE and another one from WPE. We need to <a href="https://github.com/jaymody/picoGPT/blob/29e78cc52b58ed2c1c483ffea2eb46ff6bdec785/gpt2.py#L75" rel="noopener" target="_blank">add them together</a>. The four resulting vectors will be the inputs for the next part of the algorithm: the feed-forward neural network with the attention mechanism.</p>
<p>For the SQL part, we will use <a href="https://github.com/pgvector/pgvector" rel="noopener" target="_blank">pgvector</a>, a PostgreSQL extension.</p>
<p><em>A little disclaimer: normally, I write code for my New Year posts in vanilla SQL, sometimes with pure SQL functions as helpers. It would be perfectly possible to do it for this post as well by defining vector operations on arrays, at the cost of some performance decrease (it was done in version 1 and worked, albeit slowly). With the advent of the AI and growing importance of vector databases, pgvector or its equivalent will definitely make it into the core of PostgreSQL within two or three releases. I just decided to ride the wave of the future.</em></p>
<p>Here's how we do that in SQL:</p>
<pre title="">WITH    embeddings AS
        (
        SELECT  place, values
        FROM    UNNEST(ARRAY[6307, 47701, 318, 1049]) WITH ORDINALITY AS tokens (token, ordinality)
        CROSS JOIN LATERAL
                (
                SELECT  ordinality - 1 AS place
                ) o
        CROSS JOIN LATERAL
                (
                SELECT  wte.values + wpe.values AS values
                FROM    wte
                CROSS JOIN
                        wpe
                WHERE   wte.token = tokens.token
                        AND wpe.place = o.place
                ) embedding
        )
SELECT  place, (values::REAL[])[0:5]
FROM    embeddings
</pre>
<div>
<table>
<tbody><tr>
<th>place</th>
<th>values</th>
</tr>
<tr>
<td>0</td>
<td>[0.1035146, -0.22879261, 0.18413992, -0.29924694, 0.18642524]</td>
</tr>
<tr>
<td>1</td>
<td>[0.10757777, -0.0011023134, -0.0077463835, 0.03656415, -0.14654925]</td>
</tr>
<tr>
<td>2</td>
<td>[-0.005507436, -0.07471258, 0.11009377, -0.11708109, -0.14026159]</td>
</tr>
<tr>
<td>3</td>
<td>[-0.04785268, -0.0792546, 0.1628486, -0.3598496, 0.11462127]</td>
</tr>
</tbody></table>
</div>
<p>(To keep the output short, this query only shows the first 5 dimensions for each vector)</p>
<h3>Attention</h3>
<p>The part that really makes the Transformer architecture tick is the self-attention mechanism. It was first described in the 2017 paper <a href="https://arxiv.org/abs/1706.03762" rel="noopener" target="_blank">"Attention is all you need"</a> by Vasmani et al., probably <em>the</em> most famous AI paper, whose name has since become a <a href="https://en.wikipedia.org/wiki/Snowclone" rel="noopener" target="_blank">snowclone</a> (a cliché for naming other papers).</p>
<p>So far, we have several vectors that, hopefully, encode some syntactic and semantic properties of the words in our prompt. We need these properties to somehow transfer to the last vector. A little spoiler alert: at the end of the day, it will be the last vector that will store the embedding for the continuation word.</p>
<p>In a phrase like "I looked at the violet and saw that it was not the usual …", the ellipsis has to be something you see (and this notion has to jump from "saw"), something that's a property of a violet (jumping from "violet" to "it" and then to the ellipsis), and something that is "unusual" (jumping from "not" and "usual" and flipping the sign in the dimensions responsible for the usualness). The analogy in the real world would be a person reading a book in a foreign language that they kind of have a basic command of, but don't quite know very well. They would need to consciously trace their way from one word to another, and if they don't <em>pay attention</em> to the crucial part of the phrase, their understanding would be wrong.</p>
<p>To enable this transfer of meaning from one token to another, we need to allow the vectors of all the tokens to influence each other. If we want to populate the word "it" with some concrete semantics, how much of the semantics should come from the previous vectors in the prompt, and how much should remain from the word "it" itself?</p>
<p>To solve this problem, the model uses 12 sets of matrixes called Q (query), K (key) and V (value). Each of them has 64 columns. They are obtained from the vector embeddings through a 768×2304 linear transformation <code>c_attn</code>, whose weights and biases are stored in the tables <code>c_attn_w</code> and <code>c_attn_b</code>.</p>
<p>The result of <code>c_attn</code> is a matrix with <code>n_token</code> rows and 2304 columns (3×12×64). It consists of 12 Q matrixes, 12 K matrixes and 12 V matrixes stacked horizontally, in this order.</p>
<p>Each set of Q, K and V is called a "head". They are used to perform the step known as "multi-headed causal self-attention", by calculating the attention function.</p>
<p>Here's the formula for the attention function:</p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=A+%3D+%5Cmathrm%7Bsoftmax%7D%28%5Cdfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D+%2B+M%29V&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="A = \mathrm{softmax}(\dfrac{QK^T}{\sqrt{d_k}} + M)V">,</p>
<p>where softmax is the weight normalization function. It's defined like this:</p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bsoftmax_n%7D%28%5Ctextbf%7BR%7D%29+%3D+%5Cdfrac%7Be%5E%7BR_n%7D%7D%7B%5Csum%5Climits_n+e%5E%7BR_n%7D+%7D&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\mathrm{softmax_n}(\textbf{R}) = \dfrac{e^{R_n}}{\sum\limits_n e^{R_n} }"></p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=M+&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="M "> is a constant matrix called a "causal mask". It is defined like this: <img decoding="async" src="https://s0.wp.com/latex.php?latex=M+%3D+%5Cbegin%7Bbmatrix%7D++++++0+%26+-inf+%26+-inf+%26+%5Cdots++%26+-inf+%5C%5C++++++0+%26+0+%26+-inf+%26+%5Cdots++%26+-inf+%5C%5C++++++%5Cvdots+%26+%5Cvdots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%5C%5C++++++0+%26+0+%26+0+%26+%5Cdots+%26+-inf+%5C%5C++++++0+%26+0+%26+0+%26+%5Cdots+%26+0++%5Cend%7Bbmatrix%7D+&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="M = \begin{bmatrix}      0 &amp; -inf &amp; -inf &amp; \dots  &amp; -inf \\      0 &amp; 0 &amp; -inf &amp; \dots  &amp; -inf \\      \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\      0 &amp; 0 &amp; 0 &amp; \dots &amp; -inf \\      0 &amp; 0 &amp; 0 &amp; \dots &amp; 0  \end{bmatrix} "></p>
<p>Softmax turns negative infinities into zeros.</p>
<h4>Why do we need masking?</h4>
<p>The prompt in our previous examples had 4 tokens, and the first thing the model did was calculate the 4 embeddings for these 4 tokens. As the model progresses, these vectors will undergo a lot of calculations, but for the most part, they will be independent and parallel. Changes in one vector will not affect the other vectors, as if they had not existed. The self-attention block is the only place in the whole model where the vectors affect each other.</p>
<p>Once the model is done with the math, the candidates for the next token will be decided solely from the last embedding. All the information flow should be directed towards this last vector and not from it. The transient values of the last embedding should not affect the transient values of the previous embeddings during the forward pass of the model.</p>
<p>That's why we "mask" the latter embeddings so that they don't influence the earlier embeddings through this particular channel. Hence the word "causal" in "multi-headed causal self-attention".</p>
<h4>Why are the matrixes called "query", "key" and "value"?</h4>
<p>To be honest, I'm not sure it's even a good analogy. But I'll still do my take on the intuition behind it.</p>
<p>In machine learning, generally, calculations should not involve variable-length loops or statement branching. Everything should be done through the composition of simple analytic functions (additions, multiplications, powers, logarithms and trig). It allows backpropagation, which relies on technologies like <a href="https://en.wikipedia.org/wiki/Automatic_differentiation" rel="noopener" target="_blank">automatic differentiation</a>, to work efficiently. </p>
<p>The mathematical model of the key-value store is the expression</p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbegin%7Bcases%7Dv%2C+%26+k+%3D+q+%5C%5C+0%2C+%26+%5Ctext%7Botherwise%7D+%5Cend%7Bcases%7D&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\displaystyle \begin{cases}v, &amp; k = q \\ 0, &amp; \text{otherwise} \end{cases}"></p>
<p>, but it's not a smooth, differentiable function and it will not work well with backpropagation. To make it work, we would need to turn it into a smooth function that would be <em>close</em> to <img decoding="async" src="https://s0.wp.com/latex.php?latex=v&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="v"> when <img decoding="async" src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="k"> is close to <img decoding="async" src="https://s0.wp.com/latex.php?latex=q&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="q">, and <em>close</em> to <img decoding="async" src="https://s0.wp.com/latex.php?latex=0&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="0"> otherwise.</p>
<p>The Gaussian distribution ("bell curve"), scaled to <img decoding="async" src="https://s0.wp.com/latex.php?latex=v&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="v">, with the expectation of <img decoding="async" src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="k"> and a small enough standard deviation would do perfectly for this purpose:</p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cfrac%7Bv%7D%7B%5Csigma%5Csqrt%7B2%5Cpi%7D%7D+%5C%2C+%5Cmathrm%7Bexp%7D%5Cleft%28-%5Cfrac%7B%5Cleft%28q+-+k%5Cright%29%5E2%7D%7B2%5Csigma%5E2%7D%5Cright%29&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\displaystyle \frac{v}{\sigma\sqrt{2\pi}} \, \mathrm{exp}\left(-\frac{\left(q - k\right)^2}{2\sigma^2}\right)"></p>
<p>, where <img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\sigma"> is an arbitrary parameter, defining how sharp the bell curve is.</p>
<p>In a vector space with many enough dimensions, if we take a fixed vector <img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+K&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\mathbf K"> and several vectors <img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\mathbf Q"> that randomly and uniformly deviate from <img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+K&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\mathbf K"> on every dimension, their dot products will naturally form the bell curve. So, in the vector space, the concept of a "differentiable key-value store" can be modeled by the expression <img decoding="async" src="https://s0.wp.com/latex.php?latex=%28%5Ctextbf+Q+%5Ccdot+%5Ctextbf+K%29+%5Ctextbf+V&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="(\textbf Q \cdot \textbf K) \textbf V">, which is what we are using in our attention function.</p>
<p>Again, this analogy is far-fetched. It's best not to pay too much attention (no pun intended) to these concepts of attention, meaning flow, hash tables and so on. Just think of them as an inspiration for a math trick that has been put to the test and proved to work really well.</p>
<p>Let's illustrate this step:</p>
<pre title="">WITH    embeddings AS
        (
        SELECT  place, values
        FROM    UNNEST(ARRAY[6307, 47701, 318, 1049]) WITH ORDINALITY AS tokens (token, ordinality)
        CROSS JOIN LATERAL
                (
                SELECT  ordinality - 1 AS place
                ) o
        CROSS JOIN LATERAL
                (
                SELECT  wte.values + wpe.values AS values
                FROM    wte
                CROSS JOIN
                        wpe
                WHERE   wte.token = tokens.token
                        AND wpe.place = o.place
                ) embedding
        ),
        c_attn_w AS
        (
        SELECT  *
        FROM    c_attn_w
        WHERE   block = 0
        ),
        c_attn_b AS
        (
        SELECT  *
        FROM    c_attn_b
        WHERE   block = 0
        ),
        ln_1_g AS
        (
        SELECT  *
        FROM    ln_1_g
        WHERE   block = 0
        ),
        ln_1_b AS
        (
        SELECT  *
        FROM    ln_1_b
        WHERE   block = 0
        ),
        mha_norm AS
        (
        SELECT  place, mm.values + c_attn_b.values AS values
        FROM    (
                SELECT  place, ARRAY_AGG(INNER_PRODUCT(c_attn_w.values, layer_norm.values) ORDER BY y)::VECTOR(2304) AS values
                FROM    (
                        SELECT  place, agg.values * ln_1_g.values + ln_1_b.values AS values
                        FROM    (
                                SELECT  place, norm.values
                                FROM    embeddings
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  AVG(value) AS mean,
                                                VAR_POP(value) AS variance
                                        FROM    UNNEST(values::REAL[]) value
                                        ) agg
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                                        FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n(value, ordinality)
                                        ) norm
                                ) agg
                        CROSS JOIN
                                ln_1_b
                        CROSS JOIN
                                ln_1_g
                        ) layer_norm
                CROSS JOIN
                        c_attn_w
                GROUP BY
                        place
                ) mm
        CROSS JOIN
                c_attn_b
        ),
        head AS
        (
        SELECT  place,
                (values::REAL[])[1:64]::VECTOR(64) AS q,
                (values::REAL[])[1 + 768:64 + 768]::VECTOR(64) AS k,
                (values::REAL[])[1 + 1536:64 + 1536]::VECTOR(64) AS v
        FROM    mha_norm
        ),
        sm_input AS
        (
        SELECT  h1.place AS x, h2.place AS y, INNER_PRODUCT(h1.q, h2.k) / 8 + CASE WHEN h2.place &gt; h1.place THEN -1E10 ELSE 0 END AS value
        FROM    head h1
        CROSS JOIN
                head h2
        ),
        sm_diff AS
        (
        SELECT  x, y, value - MAX(value) OVER (PARTITION BY x) AS diff
        FROM    sm_input
        ),
        sm_exp AS
        (
        SELECT  x, y, CASE WHEN diff &lt; -745.13 THEN 0 ELSE EXP(diff) END AS e
        FROM    sm_diff
        ),
        softmax AS
        (
        SELECT  x, y AS place, e / SUM(e) OVER (PARTITION BY x) AS value
        FROM    sm_exp
        ),
        attention AS
        (
        SELECT  place, (ARRAY_AGG(value ORDER BY ordinality))[:3] AS values
        FROM    (
                SELECT  x AS place, SUM(ARRAY_FILL(softmax.value, ARRAY[64])::VECTOR(64) * head.v) AS values
                FROM    softmax
                JOIN    head
                USING   (place)
                GROUP BY
                        x
                ) q
        CROSS JOIN LATERAL
                UNNEST(values::REAL[]) WITH ORDINALITY v (value, ordinality)
        GROUP BY
                place
        )
SELECT  place,
        (SELECT STRING_AGG(TO_CHAR(n, 'S0.000'), ' ') || ' …' FROM UNNEST((q::REAL[])[:3]) AS n) AS q,
        (SELECT STRING_AGG(TO_CHAR(n, 'S0.000'), ' ') || ' …' FROM UNNEST((k::REAL[])[:3]) AS n) AS k,
        (SELECT STRING_AGG(TO_CHAR(n, 'S0.000'), ' ') || ' …' FROM UNNEST((v::REAL[])[:3]) AS n) AS v,
        matrix,
        (SELECT STRING_AGG(TO_CHAR(n, 'S0.000'), ' ') || ' …' FROM UNNEST((values::REAL[])[:3]) AS n) AS attention
FROM    head
JOIN    attention
USING   (place)
JOIN    (
        SELECT  x AS place, STRING_AGG(CASE WHEN value &gt; 0 THEN TO_CHAR(value, '0.00') ELSE '    0' END, ' ' ORDER BY place) AS matrix
        FROM    softmax
        GROUP BY
                x
        ) softmax_grouped
USING   (place)
</pre>
<div>
<table>
<tbody><tr>
<th>place</th>
<th>q</th>
<th>k</th>
<th>v</th>
<th>matrix</th>
<th>attention</th>
</tr>
<tr>
<td>0</td>
<td>+0.381 -0.579 +0.073 …</td>
<td>-1.395 +2.367 +0.332 …</td>
<td>-0.006 +0.192 +0.047 …</td>
<td> 1.00     0     0     0</td>
<td>-0.006 +0.192 +0.047 …</td>
</tr>
<tr>
<td>1</td>
<td>+1.518 +0.827 -0.388 …</td>
<td>-2.380 +3.714 +0.659 …</td>
<td>-0.315 -0.062 +0.018 …</td>
<td> 0.73  0.27     0     0</td>
<td>-0.089 +0.124 +0.039 …</td>
</tr>
<tr>
<td>2</td>
<td>+0.238 -0.226 +0.344 …</td>
<td>-1.952 +2.404 +1.953 …</td>
<td>+0.256 -0.268 +0.301 …</td>
<td> 0.67  0.26  0.07     0</td>
<td>-0.069 +0.095 +0.057 …</td>
</tr>
<tr>
<td>3</td>
<td>+1.130 -0.011 -0.103 …</td>
<td>-2.855 +2.053 +2.813 …</td>
<td>+0.176 +0.019 -0.099 …</td>
<td> 0.59  0.19  0.12  0.10</td>
<td>-0.016 +0.071 +0.058 …</td>
</tr>
</tbody></table>
</div>
<p>Here is what we did:</p>
<ol>
<li>Before calculating the attention function, we normalized the vectors by applying the linear transformation <img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+R%5E%5Cprime+%3D+%5Cmathbf%7BR%5CGamma_1%7D+%2B+%5Cmathbf%7BB_1%7D&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\mathbf R^\prime = \mathbf{R\Gamma_1} + \mathbf{B_1}">. The matrix <img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5CGamma_1%7D+&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\mathbf{\Gamma_1} "> and the vector <img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BB_1%7D+&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\mathbf{B_1} "> are called "scale" and "shift", accordingly. They are learned parameters of the model, which are stored in the tables <code>ln_1_g</code> and <code>ln_1_b</code></li>
<li>We are only showing the first head of the first layer of the algorithm. After we multiplied the vectors by the learned coefficients from <code>c_attn_w</code> and <code>c_attn_b</code> ("weight" and "bias"), we sliced the resulting 2304-vectors, taking 64-vectors starting at the positions 0, 768 and 1536. They correspond to the vectors Q, K and V for the first head.</li>
<li><code>EXP</code> in PostgreSQL fails on really small numbers, that's why we shortcut to zero if the argument to <code>EXP</code> is less than -745.13.</li>
<li>We are only showing the first three elements for each vector. The attention matrix we show in full.</li>
</ol>
<p>As we can see, the first value vector got copied to the output as is (as it will do in every other layer of the algorithm). It means that once the model has been trained, the output embedding for the first token will be only defined by the value of the first token. In general, during the recursive inference phase, where tokens only get added to the prompt, only the last embedding in the output will ever change compared to the previous iteration. This is what the causal mask does.</p>
<p>Looking a bit forward: the attention block is the <em>only</em> place in the entire algorithm where tokens can influence each other during the forward pass. Since we have disabled the ability of later tokens to influence the previous ones in this step, all the calculations done on the previous tokens can be reused between the forward passes of the model.</p>
<p>Remember, the model operates by appending tokens to the prompt. If our original (tokenized) prompt is "Post greSQL Ġis Ġgreat" and the next one will be (for instance) "Post greSQL Ġis Ġgreat Ġfor", all the results of the calculations made on the first four tokens can be reused for the new prompt; they will never change, regardless of what is appended to them.</p>
<p>Jay Mody's illustrative article doesn't make use of this fact (and neither do we, for the sake of simplicity), but the original GPT2 implementation <a href="https://github.com/openai/gpt-2/blob/a74da5d99abaaba920de8131d64da2862a8f213b/src/model.py#L161" rel="noopener" target="_blank">does</a>.</p>
<p>Once all the heads are done, we will end up with 12 matrixes, each 64 columns wide and <code>n_tokens</code> rows tall. To map it back to the dimension of embedding vectors (768), we just need to stack these matrixes horizontally.</p>
<p>The final step of multi-headed attention involves projecting the values through a learned linear transformation of the same dimension. Its weights and biases are stored in the tables <code>c_proj_w</code> and <code>c_proj_b</code>.</p>
<p>Here's what the code for a complete multi-headed attention step in the first layer looks like:</p>
<pre title="">WITH    embeddings AS
        (
        SELECT  place, values
        FROM    UNNEST(ARRAY[6307, 47701, 318, 1049]) WITH ORDINALITY AS tokens (token, ordinality)
        CROSS JOIN LATERAL
                (
                SELECT  ordinality - 1 AS place
                ) o
        CROSS JOIN LATERAL
                (
                SELECT  wte.values + wpe.values AS values
                FROM    wte
                CROSS JOIN
                        wpe
                WHERE   wte.token = tokens.token
                        AND wpe.place = o.place
                ) embedding
        ),
        c_proj_w AS
        (
        SELECT  *
        FROM    c_proj_w
        WHERE   block = 0
        ),
        c_proj_b AS
        (
        SELECT  *
        FROM    c_proj_b
        WHERE   block = 0
        ),
        mlp_c_fc_w AS
        (
        SELECT  *
        FROM    mlp_c_fc_w
        WHERE   block = 0
        ),
        mlp_c_fc_b AS
        (
        SELECT  *
        FROM    mlp_c_fc_b
        WHERE   block = 0
        ),
        mlp_c_proj_w AS
        (
        SELECT  *
        FROM    mlp_c_proj_w
        WHERE   block = 0
        ),
        mlp_c_proj_b AS
        (
        SELECT  *
        FROM    mlp_c_proj_b
        WHERE   block = 0
        ),
        c_attn_w AS
        (
        SELECT  *
        FROM    c_attn_w
        WHERE   block = 0
        ),
        c_attn_b AS
        (
        SELECT  *
        FROM    c_attn_b
        WHERE   block = 0
        ),
        ln_1_g AS
        (
        SELECT  *
        FROM    ln_1_g
        WHERE   block = 0
        ),
        ln_1_b AS
        (
        SELECT  *
        FROM    ln_1_b
        WHERE   block = 0
        ),
        mha_norm AS
        (
        SELECT  place, mm.values + c_attn_b.values AS values
        FROM    (
                SELECT  place, ARRAY_AGG(INNER_PRODUCT(c_attn_w.values, layer_norm.values) ORDER BY y)::VECTOR(2304) AS values
                FROM    (
                        SELECT  place, agg.values * ln_1_g.values + ln_1_b.values AS values
                        FROM    (
                                SELECT  place, norm.values
                                FROM    embeddings
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  AVG(value) AS mean,
                                                VAR_POP(value) AS variance
                                        FROM    UNNEST(values::REAL[]) value
                                        ) agg
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                                        FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n(value, ordinality)
                                        ) norm
                                ) agg
                        CROSS JOIN
                                ln_1_b
                        CROSS JOIN
                                ln_1_g
                        ) layer_norm
                CROSS JOIN
                        c_attn_w
                GROUP BY
                        place
                ) mm
        CROSS JOIN
                c_attn_b
        ),
        heads AS
        (
        SELECT  place, head,
                (values::REAL[])[(head * 64 + 1):(head * 64 + 64)]::VECTOR(64) AS q,
                (values::REAL[])[(head * 64 + 1 + 768):(head * 64 + 64 + 768)]::VECTOR(64) AS k,
                (values::REAL[])[(head * 64 + 1 + 1536):(head * 64 + 64 + 1536)]::VECTOR(64) AS v
        FROM    mha_norm
        CROSS JOIN
                GENERATE_SERIES(0, 11) head
        ),
        sm_input AS
        (
        SELECT  head, h1.place AS x, h2.place AS y, INNER_PRODUCT(h1.q, h2.k) / 8 + CASE WHEN h2.place &gt; h1.place THEN -1E10 ELSE 0 END AS value
        FROM    heads h1
        JOIN    heads h2
        USING   (head)
        ),
        sm_diff AS
        (
        SELECT  head, x, y, value - MAX(value) OVER (PARTITION BY head, x) AS diff
        FROM    sm_input
        ),
        sm_exp AS
        (
        SELECT  head, x, y, CASE WHEN diff &lt; -745.13 THEN 0 ELSE EXP(diff) END AS e
        FROM    sm_diff
        ),
        softmax AS
        (
        SELECT  head, x, y AS place, e / SUM(e) OVER (PARTITION BY head, x) AS value
        FROM    sm_exp
        ),
        attention AS
        (
        SELECT  place, ARRAY_AGG(value ORDER BY head * 64 + ordinality)::VECTOR(768) AS values
        FROM    (
                SELECT  head, x AS place, SUM(ARRAY_FILL(softmax.value, ARRAY[64])::VECTOR(64) * heads.v) AS values
                FROM    softmax
                JOIN    heads
                USING   (head, place)
                GROUP BY
                        head, x
                ) q
        CROSS JOIN LATERAL
                UNNEST(values::REAL[]) WITH ORDINALITY v (value, ordinality)
        GROUP BY
                place
        ),
        mha AS
        (
        SELECT  place, w.values + c_proj_b.values AS values
        FROM    (
                SELECT  attention.place, ARRAY_AGG(INNER_PRODUCT(attention.values, c_proj_w.values) ORDER BY c_proj_w.place)::VECTOR(768) AS values
                FROM    attention
                CROSS JOIN
                        c_proj_w
                GROUP BY
                        attention.place
                ) w
        CROSS JOIN
                c_proj_b
        )
SELECT  place,
        (SELECT STRING_AGG(TO_CHAR(n, 'S0.000'), ' ') || ' …' FROM UNNEST((values::REAL[])[:10]) AS n) AS q
FROM    mha
</pre>
<div>
<table>
<tbody><tr>
<th>place</th>
<th>q</th>
</tr>
<tr>
<td>0</td>
<td>+0.814 -1.407 +0.171 +0.008 +0.065 -0.049 -0.407 +1.178 -0.234 -0.061 …</td>
</tr>
<tr>
<td>1</td>
<td>+1.150 -0.430 +0.083 +0.030 +0.010 +0.015 -0.245 +3.778 -0.445 -0.004 …</td>
</tr>
<tr>
<td>2</td>
<td>-0.219 -0.745 -0.116 +0.032 +0.064 -0.044 +0.290 +3.187 -0.074 -0.003 …</td>
</tr>
<tr>
<td>3</td>
<td>-0.526 -0.757 -0.510 -0.008 +0.027 -0.017 +0.302 +2.842 +0.188 -0.028 …</td>
</tr>
</tbody></table>
</div>
<p>Before the results of multi-headed attention are passed to the next step, the original inputs are added to them. This trick was described in the original transformer paper. It's supposed to help with vanishing and exploding gradients.</p>
<p>It's a common problem during training: sometimes the gradients of the parameters turn out too big or too small. Changing them on the training iteration either has very little effect on the loss function (and so the model converges very slowly), or, on the opposite, has such a big effect that even a small change throws the loss function too far away from its local minimum, negating the training efforts.</p>
<h3>Feedforward</h3>
<p>This is what the deep neural networks do. The larger part of the model parameters is actually used at this step.</p>
<p>This step is a <a href="https://en.wikipedia.org/wiki/Feedforward_neural_network#Multilayer_perceptron" rel="noopener" target="_blank">multi-layer perceptron</a> with three layers (768, 3072, 768), using the Gaussian Error Linear Unit (GELU) as an activation function:</p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BGELU%7D%28x%29+%3D+%5Cdisplaystyle+%5Cfrac+x+2+%5Cleft%281+%2B+%5Cmathrm%7Berf%7D%5C%2C%5Cfrac+x+%7B%5Csqrt+2%7D%5Cright%29&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\mathrm{GELU}(x) = \displaystyle \frac x 2 \left(1 + \mathrm{erf}\,\frac x {\sqrt 2}\right)"><br>
<img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Berf%7D%5C%2Cx+%3D+%5Cdisplaystyle+%5Cfrac%7B2%7D%7B%5Csqrt+%5Cpi%7D%5Cint_0%5Ex%7Be%5E%7B-t%5E2%7D%7D%5C%2Cdt&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\mathrm{erf}\,x = \displaystyle \frac{2}{\sqrt \pi}\int_0^x{e^{-t^2}}\,dt"></p>
<p>This function <a href="https://arxiv.org/abs/1606.08415" rel="noopener" target="_blank">has been observed</a> to yield good results in deep neural networks. It can be analytically approximated like this:</p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BGELU%7D%28x%29+%5Cdisplaystyle+%5Capprox+0.5x+%5Cleft%281+%2B+%5Cmathrm%7Btanh%7D%5Cleft%5B0.797884%5Cleft%28x+%2B+0.044715x%5E3%5Cright%29+%5Cright%5D%5Cright%29+&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\mathrm{GELU}(x) \displaystyle \approx 0.5x \left(1 + \mathrm{tanh}\left[0.797884\left(x + 0.044715x^3\right) \right]\right) "></p>
<p>The learned linear transformation parameters for layer connections are called <code>c_fc</code> (768 → 3072) and <code>c_proj</code> (3072 → 768). The values for the first layer are first normalized using the coefficients in the learned parameter <code>ln_2</code>. After the feedforward step is completed, its input is again added to the output. This, too, is a part of the original transformer design.</p>
<p>The whole feedforward step looks like this:</p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathrm%7BFFN%7D%28%5Cmathbf+R%29+%3D+%5Cmathbf+R+%2B+%5Cmathrm%7Bc%5C_proj%7D%5Cleft%28%5Cmathrm%7BGELU%7D%5Cleft%28%5Cmathrm%7Bc%5C_fc%7D%5Cleft%28%5Cmathrm%7Bln%5C_2%7D%5Cleft%28%5Cmathbf+R%5Cright%29%5Cright%29%5Cright%29%5Cright%29+&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\displaystyle \mathrm{FFN}(\mathbf R) = \mathbf R + \mathrm{c\_proj}\left(\mathrm{GELU}\left(\mathrm{c\_fc}\left(\mathrm{ln\_2}\left(\mathbf R\right)\right)\right)\right) "></p>
<p>And here's how we do this in SQL:</p>
<pre title="">WITH    embeddings AS
        (
        SELECT  place, values
        FROM    UNNEST(ARRAY[6307, 47701, 318, 1049]) WITH ORDINALITY AS tokens (token, ordinality)
        CROSS JOIN LATERAL
                (
                SELECT  ordinality - 1 AS place
                ) o
        CROSS JOIN LATERAL
                (
                SELECT  wte.values + wpe.values AS values
                FROM    wte
                CROSS JOIN
                        wpe
                WHERE   wte.token = tokens.token
                        AND wpe.place = o.place
                ) embedding
        ),
        c_proj_w AS
        (
        SELECT  *
        FROM    c_proj_w
        WHERE   block = 0
        ),
        c_proj_b AS
        (
        SELECT  *
        FROM    c_proj_b
        WHERE   block = 0
        ),
        mlp_c_fc_w AS
        (
        SELECT  *
        FROM    mlp_c_fc_w
        WHERE   block = 0
        ),
        mlp_c_fc_b AS
        (
        SELECT  *
        FROM    mlp_c_fc_b
        WHERE   block = 0
        ),
        mlp_c_proj_w AS
        (
        SELECT  *
        FROM    mlp_c_proj_w
        WHERE   block = 0
        ),
        mlp_c_proj_b AS
        (
        SELECT  *
        FROM    mlp_c_proj_b
        WHERE   block = 0
        ),
        c_attn_w AS
        (
        SELECT  *
        FROM    c_attn_w
        WHERE   block = 0
        ),
        c_attn_b AS
        (
        SELECT  *
        FROM    c_attn_b
        WHERE   block = 0
        ),
        ln_1_g AS
        (
        SELECT  *
        FROM    ln_1_g
        WHERE   block = 0
        ),
        ln_1_b AS
        (
        SELECT  *
        FROM    ln_1_b
        WHERE   block = 0
        ),
        ln_2_b AS
        (
        SELECT  *
        FROM    ln_2_b
        WHERE   block = 0
        ),
        ln_2_g AS
        (
        SELECT  *
        FROM    ln_2_g
        WHERE   block = 0
        ),
        mha_norm AS
        (
        SELECT  place, mm.values + c_attn_b.values AS values
        FROM    (
                SELECT  place, ARRAY_AGG(INNER_PRODUCT(c_attn_w.values, layer_norm.values) ORDER BY y)::VECTOR(2304) AS values
                FROM    (
                        SELECT  place, agg.values * ln_1_g.values + ln_1_b.values AS values
                        FROM    (
                                SELECT  place, norm.values
                                FROM    embeddings
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  AVG(value) AS mean,
                                                VAR_POP(value) AS variance
                                        FROM    UNNEST(values::REAL[]) value
                                        ) agg
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                                        FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n(value, ordinality)
                                        ) norm
                                ) agg
                        CROSS JOIN
                                ln_1_b
                        CROSS JOIN
                                ln_1_g
                        ) layer_norm
                CROSS JOIN
                        c_attn_w
                GROUP BY
                        place
                ) mm
        CROSS JOIN
                c_attn_b
        ),
        heads AS
        (
        SELECT  place, head,
                (values::REAL[])[(head * 64 + 1):(head * 64 + 64)]::VECTOR(64) AS q,
                (values::REAL[])[(head * 64 + 1 + 768):(head * 64 + 64 + 768)]::VECTOR(64) AS k,
                (values::REAL[])[(head * 64 + 1 + 1536):(head * 64 + 64 + 1536)]::VECTOR(64) AS v
        FROM    mha_norm
        CROSS JOIN
                GENERATE_SERIES(0, 11) head
        ),
        sm_input AS
        (
        SELECT  head, h1.place AS x, h2.place AS y, INNER_PRODUCT(h1.q, h2.k) / 8 + CASE WHEN h2.place &gt; h1.place THEN -1E10 ELSE 0 END AS value
        FROM    heads h1
        JOIN    heads h2
        USING   (head)
        ),
        sm_diff AS
        (
        SELECT  head, x, y, value - MAX(value) OVER (PARTITION BY head, x) AS diff
        FROM    sm_input
        ),
        sm_exp AS
        (
        SELECT  head, x, y, CASE WHEN diff &lt; -745.13 THEN 0 ELSE EXP(diff) END AS e
        FROM    sm_diff
        ),
        softmax AS
        (
        SELECT  head, x, y AS place, e / SUM(e) OVER (PARTITION BY head, x) AS value
        FROM    sm_exp
        ),
        attention AS
        (
        SELECT  place, ARRAY_AGG(value ORDER BY head * 64 + ordinality)::VECTOR(768) AS values
        FROM    (
                SELECT  head, x AS place, SUM(ARRAY_FILL(softmax.value, ARRAY[64])::VECTOR(64) * heads.v) AS values
                FROM    softmax
                JOIN    heads
                USING   (head, place)
                GROUP BY
                        head, x
                ) q
        CROSS JOIN LATERAL
                UNNEST(values::REAL[]) WITH ORDINALITY v (value, ordinality)
        GROUP BY
                place
        ),
        mha AS
        (
        SELECT  place, w.values + c_proj_b.values + embeddings.values AS values
        FROM    (
                SELECT  attention.place, ARRAY_AGG(INNER_PRODUCT(attention.values, c_proj_w.values) ORDER BY c_proj_w.place)::VECTOR(768) AS values
                FROM    attention
                CROSS JOIN
                        c_proj_w
                GROUP BY
                        attention.place
                ) w
        CROSS JOIN
                c_proj_b
        JOIN    embeddings
        USING   (place)
        ),
        ffn_norm AS
        (
        SELECT  place, agg.values * ln_2_g.values + ln_2_b.values AS values
        FROM    (
                SELECT  place, norm.values
                FROM    mha
                CROSS JOIN LATERAL
                        (
                        SELECT  AVG(value) AS mean,
                                VAR_POP(value) AS variance
                        FROM    UNNEST(values::REAL[]) value
                        ) agg
                CROSS JOIN LATERAL
                        (
                        SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                        FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n(value, ordinality)
                        ) norm
                ) agg
        CROSS JOIN
                ln_2_b
        CROSS JOIN
                ln_2_g
        ),
        ffn_a AS
        (
        SELECT  gelu.place, gelu.values
        FROM    (
                SELECT  place, w.values + mlp_c_fc_b.values AS values
                FROM    (
                        SELECT  ffn_norm.place, ARRAY_AGG(INNER_PRODUCT(ffn_norm.values, mlp_c_fc_w.values) ORDER BY mlp_c_fc_w.place)::VECTOR(3072) AS values
                        FROM    ffn_norm
                        CROSS JOIN
                                mlp_c_fc_w
                        GROUP BY
                                ffn_norm.place
                        ) w
                CROSS JOIN
                        mlp_c_fc_b
                ) v
        CROSS JOIN LATERAL
                (
                SELECT  place, ARRAY_AGG(0.5 * value * (1 + TANH(0.797884560802 * (value + 0.044715 * value*value*value))) ORDER BY ordinality)::VECTOR(3072) AS values
                FROM    UNNEST(values::REAL[]) WITH ORDINALITY n (value, ordinality)
                GROUP BY
                        place
                ) gelu
        ),
        ffn AS
        (
        SELECT  place, w.values + mlp_c_proj_b.values + mha.values AS values
        FROM    (
                SELECT  ffn_a.place, ARRAY_AGG(INNER_PRODUCT(ffn_a.values, mlp_c_proj_w.values) ORDER BY mlp_c_proj_w.place)::VECTOR(768) AS values
                FROM    ffn_a
                CROSS JOIN
                        mlp_c_proj_w
                GROUP BY
                        ffn_a.place
                ) w
        CROSS JOIN
                mlp_c_proj_b
        JOIN    mha
        USING   (place)
        )
SELECT  place,
        (SELECT STRING_AGG(TO_CHAR(n, 'S0.000'), ' ') || ' …' FROM UNNEST((values::REAL[])[:10]) AS n) AS q
FROM    ffn
</pre>
<div>
<table>
<tbody><tr>
<th>place</th>
<th>q</th>
</tr>
<tr>
<td>0</td>
<td>+0.309 -1.267 -0.250 -1.111 -0.226 +0.549 -0.346 +0.645 -1.603 -0.501 …</td>
</tr>
<tr>
<td>1</td>
<td>+0.841 -1.081 +0.227 -1.029 -1.554 +1.061 -0.070 +5.258 -1.892 -0.973 …</td>
</tr>
<tr>
<td>2</td>
<td>-1.256 -0.528 -0.846 -0.288 +0.166 +0.409 +0.019 +3.393 +0.085 -0.212 …</td>
</tr>
<tr>
<td>3</td>
<td>-1.007 -1.719 -0.725 -1.417 -0.086 -0.144 +0.605 +3.272 +1.051 -0.666 …</td>
</tr>
</tbody></table>
</div>
<p>This output is what comes out of the first block of GPT2.</p>
<h3>Blocks</h3>
<p>What we saw in the previous steps is repeated in layers (called "blocks"). The blocks are set up in a pipeline so that the output of a previous block goes straight to the next one. Each block has its own set of learned parameters.</p>
<p>In SQL, we would need to connect the blocks using a recursive CTE.</p>
<p>Once the final block produces the values, we need to normalize it using the learned parameter <code>ln_f</code>.</p>
<p>Here's what the model ultimately looks like:</p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathrm%7BGPT%7D%28tokens%29+%3D+%5Cmathrm%7Bln%5C_f%7D%28%5Cmathbf+R_%7B12%7D%29&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\displaystyle \mathrm{GPT}(tokens) = \mathrm{ln\_f}(\mathbf R_{12})"></p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathbf+R_%7Bn%7D+%3D+%5Cmathrm%7Bblock_n%7D%28%5Cmathbf+R_%7Bn-1%7D%29%2C+n+%3E+0&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\displaystyle \mathbf R_{n} = \mathrm{block_n}(\mathbf R_{n-1}), n > 0"></p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathrm%7Bblock_n%7D%28%5Cmathbf+R%29+%3D+%5Cmathrm%7Bffn_n%7D%28%5Cmathrm%7Bmha_n%7D%28%5Cmathbf+R%29%29&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\displaystyle \mathrm{block_n}(\mathbf R) = \mathrm{ffn_n}(\mathrm{mha_n}(\mathbf R))"></p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathbf+R_0+%3D+%5Cmathrm%7Bwte%7D%28tokens%29+%2B+%5Cmathrm%7Bwpe%7D%28%5B1+%5Cldots+%5Cmathrm%7Bdim%7D%28tokens%29%5D%29&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\displaystyle \mathbf R_0 = \mathrm{wte}(tokens) + \mathrm{wpe}([1 \ldots \mathrm{dim}(tokens)])"></p>
<p>And here's how it looks in SQL:</p>
<pre title="">WITH    RECURSIVE
        initial AS
        (
        SELECT  ARRAY[6307, 47701, 318, 1049] AS input
        ),
        hparams AS
        (
        SELECT  12 AS n_block
        ),
        embeddings AS
        (
        SELECT  place, values
        FROM    initial
        CROSS JOIN
                hparams
        CROSS JOIN LATERAL
                UNNEST(input) WITH ORDINALITY AS tokens (token, ordinality)
        CROSS JOIN LATERAL
                (
                SELECT  ordinality - 1 AS place
                ) o
        CROSS JOIN LATERAL
                (
                SELECT  wte.values + wpe.values AS values
                FROM    wte
                CROSS JOIN
                        wpe
                WHERE   wte.token = tokens.token
                        AND wpe.place = o.place
                ) embedding
        ),
        transform AS
        (
        SELECT  0 AS block, place, values
        FROM    embeddings
        UNION ALL
        (
        WITH    previous AS
                (
                SELECT  *
                FROM    transform
                )
        SELECT  block + 1 AS block, transformed_layer.*
        FROM    hparams
        CROSS JOIN LATERAL
                (
                SELECT  block
                FROM    previous
                WHERE   block &lt; 12
                LIMIT   1
                ) q
        CROSS JOIN LATERAL
                (
                WITH    ln_2_b AS
                        (
                        SELECT  *
                        FROM    ln_2_b
                        WHERE   block = q.block
                        ),
                        ln_2_g AS
                        (
                        SELECT  *
                        FROM    ln_2_g
                        WHERE   block = q.block
                        ),
                        c_proj_w AS
                        (
                        SELECT  *
                        FROM    c_proj_w
                        WHERE   block = q.block
                        ),
                        c_proj_b AS
                        (
                        SELECT  *
                        FROM    c_proj_b
                        WHERE   block = q.block
                        ),
                        mlp_c_fc_w AS
                        (
                        SELECT  *
                        FROM    mlp_c_fc_w
                        WHERE   block = q.block
                        ),
                        mlp_c_fc_b AS
                        (
                        SELECT  *
                        FROM    mlp_c_fc_b
                        WHERE   block = q.block
                        ),
                        mlp_c_proj_w AS
                        (
                        SELECT  *
                        FROM    mlp_c_proj_w
                        WHERE   block = q.block
                        ),
                        mlp_c_proj_b AS
                        (
                        SELECT  *
                        FROM    mlp_c_proj_b
                        WHERE   block = q.block
                        ),
                        c_attn_w AS
                        (
                        SELECT  *
                        FROM    c_attn_w
                        WHERE   block = q.block
                        ),
                        c_attn_b AS
                        (
                        SELECT  *
                        FROM    c_attn_b
                        WHERE   block = q.block
                        ),
                        ln_1_g AS
                        (
                        SELECT  *
                        FROM    ln_1_g
                        WHERE   block = q.block
                        ),
                        ln_1_b AS
                        (
                        SELECT  *
                        FROM    ln_1_b
                        WHERE   block = q.block
                        ),
                        mha_norm AS
                        (
                        SELECT  place, mm.values + c_attn_b.values AS values
                        FROM    (
                                SELECT  place, ARRAY_AGG(INNER_PRODUCT(c_attn_w.values, layer_norm.values) ORDER BY y)::VECTOR(2304) AS values
                                FROM    (
                                        SELECT  place, agg.values * ln_1_g.values + ln_1_b.values AS values
                                        FROM    (
                                                SELECT  place, norm.values
                                                FROM    previous
                                                CROSS JOIN LATERAL
                                                        (
                                                        SELECT  AVG(value) AS mean,
                                                                VAR_POP(value) AS variance
                                                        FROM    UNNEST(values::REAL[]) value
                                                        ) agg
                                                CROSS JOIN LATERAL
                                                        (
                                                        SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                                                        FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n(value, ordinality)
                                                        ) norm
                                                ) agg
                                        CROSS JOIN
                                                ln_1_b
                                        CROSS JOIN
                                                ln_1_g
                                        ) layer_norm
                                CROSS JOIN
                                        c_attn_w
                                GROUP BY
                                        place
                                ) mm
                        CROSS JOIN
                                c_attn_b
                        ),
                        heads AS
                        (
                        SELECT  place, head,
                                (values::REAL[])[(head * 64 + 1):(head * 64 + 64)]::VECTOR(64) AS q,
                                (values::REAL[])[(head * 64 + 1 + 768):(head * 64 + 64 + 768)]::VECTOR(64) AS k,
                                (values::REAL[])[(head * 64 + 1 + 1536):(head * 64 + 64 + 1536)]::VECTOR(64) AS v
                        FROM    mha_norm
                        CROSS JOIN
                                GENERATE_SERIES(0, 11) head
                        ),
                        sm_input AS
                        (
                        SELECT  head, h1.place AS x, h2.place AS y, INNER_PRODUCT(h1.q, h2.k) / 8 + CASE WHEN h2.place &gt; h1.place THEN -1E10 ELSE 0 END AS value
                        FROM    heads h1
                        JOIN    heads h2
                        USING   (head)
                        ),
                        sm_diff AS
                        (
                        SELECT  head, x, y, value - MAX(value) OVER (PARTITION BY head, x) AS diff
                        FROM    sm_input
                        ),
                        sm_exp AS
                        (
                        SELECT  head, x, y, CASE WHEN diff &lt; -745.13 THEN 0 ELSE EXP(diff) END AS e
                        FROM    sm_diff
                        ),
                        softmax AS
                        (
                        SELECT  head, x, y AS place, e / SUM(e) OVER (PARTITION BY head, x) AS value
                        FROM    sm_exp
                        ),
                        attention AS
                        (
                        SELECT  place, ARRAY_AGG(value ORDER BY head * 64 + ordinality)::VECTOR(768) AS values
                        FROM    (
                                SELECT  head, x AS place, SUM(ARRAY_FILL(softmax.value, ARRAY[64])::VECTOR(64) * heads.v) AS values
                                FROM    softmax
                                JOIN    heads
                                USING   (head, place)
                                GROUP BY
                                        head, x
                                ) q
                        CROSS JOIN LATERAL
                                UNNEST(values::REAL[]) WITH ORDINALITY v (value, ordinality)
                        GROUP BY
                                place
                        ),
                        mha AS
                        (
                        SELECT  place, w.values + c_proj_b.values + previous.values AS values
                        FROM    (
                                SELECT  attention.place, ARRAY_AGG(INNER_PRODUCT(attention.values, c_proj_w.values) ORDER BY c_proj_w.place)::VECTOR(768) AS values
                                FROM    attention
                                CROSS JOIN
                                        c_proj_w
                                GROUP BY
                                        attention.place
                                ) w
                        CROSS JOIN
                                c_proj_b
                        JOIN    previous
                        USING   (place)
                        ),
                        ffn_norm AS
                        (
                        SELECT  place, agg.values * ln_2_g.values + ln_2_b.values AS values
                        FROM    (
                                SELECT  place, norm.values
                                FROM    mha
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  AVG(value) AS mean,
                                                VAR_POP(value) AS variance
                                        FROM    UNNEST(values::REAL[]) value
                                        ) agg
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                                        FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n(value, ordinality)
                                        ) norm
                                ) agg
                        CROSS JOIN
                                ln_2_b
                        CROSS JOIN
                                ln_2_g
                        ),
                        ffn_a AS
                        (
                        SELECT  gelu.place, gelu.values
                        FROM    (
                                SELECT  place, w.values + mlp_c_fc_b.values AS values
                                FROM    (
                                        SELECT  ffn_norm.place, ARRAY_AGG(INNER_PRODUCT(ffn_norm.values, mlp_c_fc_w.values) ORDER BY mlp_c_fc_w.place)::VECTOR(3072) AS values
                                        FROM    ffn_norm
                                        CROSS JOIN
                                                mlp_c_fc_w
                                        GROUP BY
                                                ffn_norm.place
                                        ) w
                                CROSS JOIN
                                        mlp_c_fc_b
                                ) v
                        CROSS JOIN LATERAL
                                (
                                SELECT  place, ARRAY_AGG(0.5 * value * (1 + TANH(0.797884560802 * (value + 0.044715 * value*value*value))) ORDER BY ordinality)::VECTOR(3072) AS values
                                FROM    UNNEST(values::REAL[]) WITH ORDINALITY n (value, ordinality)
                                GROUP BY
                                        place
                                ) gelu
                        ),
                        ffn AS
                        (
                        SELECT  place, w.values + mlp_c_proj_b.values + mha.values AS values
                        FROM    (
                                SELECT  ffn_a.place, ARRAY_AGG(INNER_PRODUCT(ffn_a.values, mlp_c_proj_w.values) ORDER BY mlp_c_proj_w.place)::VECTOR(768) AS values
                                FROM    ffn_a
                                CROSS JOIN
                                        mlp_c_proj_w
                                GROUP BY
                                        ffn_a.place
                                ) w
                        CROSS JOIN
                                mlp_c_proj_b
                        JOIN    mha
                        USING   (place)
                        )
                SELECT  *
                FROM    ffn
                ) transformed_layer
        )
        ),
        block_output AS
        (
        SELECT  *
        FROM    hparams
        JOIN    transform
        ON      transform.block = n_block
        ),
        ln_f AS
        (
        SELECT  place, norm.values * ln_f_g.values + ln_f_b.values AS values
        FROM    block_output
        CROSS JOIN LATERAL
                (
                SELECT  AVG(value) AS mean,
                        VAR_POP(value) AS variance
                FROM    UNNEST(values::REAL[]) AS n(value)
                ) agg
        CROSS JOIN LATERAL
                (
                SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n (value, ordinality)
                ) norm
        CROSS JOIN
                ln_f_b
        CROSS JOIN
                ln_f_g
        )
SELECT  place,
        (SELECT STRING_AGG(TO_CHAR(n, 'S0.000'), ' ') || ' …' FROM UNNEST((values::REAL[])[:10]) AS n) AS q
FROM    ln_f
</pre>
<div>
<table>
<tbody><tr>
<th>place</th>
<th>q</th>
</tr>
<tr>
<td>0</td>
<td>-0.153 -0.126 -0.368 +0.028 -0.013 -0.198 +0.661 +0.056 -0.228 -0.001 …</td>
</tr>
<tr>
<td>1</td>
<td>-0.157 -0.314 +0.291 -0.386 -0.273 -0.054 +3.397 +0.440 -0.137 -0.243 …</td>
</tr>
<tr>
<td>2</td>
<td>-0.912 -0.220 -0.886 -0.661 +0.491 -0.050 +0.693 +1.128 +0.031 -0.577 …</td>
</tr>
<tr>
<td>3</td>
<td>-0.098 -0.323 -1.479 -0.736 +0.235 -0.608 +1.774 +0.566 -0.057 -0.211 …</td>
</tr>
</tbody></table>
</div>
<p>This is the output of the model. </p>
<p>The fourth vector is the actual embedding of the next token predicted by the model. We just need to map it back to the tokens.</p>
<h3>Tokens</h3>
<p>We have an embedding (a 768-vector) which, according to the model, captures the semantics and the grammar of the most likely continuation of the prompt. Now we need to map it back to the token.</p>
<p>One of the first steps the model makes is mapping the tokens to their embeddings. It is done through the 50257×768 matrix <code>wpe</code>. We will need to use the same matrix to map the embedding back to the token.</p>
<p>The problem is that the exact reverse mapping is not possible: the embedding will not (likely) be equal to any of the rows in the matrix. So we will need to find the "closest" token to the embedding.</p>
<p>Since the dimensions of embeddings capture (as we hope) some semantic and grammatical aspects of the token, we need them to match as closely as possible. One way to consolidate the closeness of each dimension would be to just calculate the dot product of the two embeddings. The higher the dot product, the closer the token is to the prediction.</p>
<p>To do this, we will multiply the embedding by the matrix <code>wte</code>. The result will be a single-column matrix, 50257 rows tall. Each value in this result will be the dot product of the predicted embedding and the token embedding. The higher this number, the more likely it is for the token to continue the prompt.</p>
<p>To pick the next token, we will need to convert the similarities to probabilities. To do this, we will use our good friend softmax (the same function that we used to normalize attention weights).</p>
<h4>Why use softmax for probabilities?</h4>
<p>Softmax has the nice property of satisfying <a href="https://en.wikipedia.org/wiki/Luce%27s_choice_axiom" rel="noopener" target="_blank">Luce's choice axiom</a>. It means that the relative probabilities of two options don't depend on the presence or probability of other options. If A is twice as probable as B, then the presence or absence of other options will not change this ratio (although it of course can change the absolute values).</p>
<p>The vector of dot products ("logit" in AI parlance) contains arbitrary scores that don't have an intrinsic scale. If A has a larger score than B, we know that it's more likely, but that's about it. We can tweak the inputs to softmax as we please, as long as they keep their order (i.e. larger scores stay larger).</p>
<p>One common way to do that is to normalize the scores by subtracting the greatest value from the set from them (so that the biggest score becomes 0 and the rest become negative numbers). Then we take some fixed number (let's say five or ten) top scores. Finally, we multiply each score by a constant before feeding it to softmax.</p>
<p>The number of top scores that we take is usually called <img decoding="async" src="https://s0.wp.com/latex.php?latex=top%5C_n&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="top\_n"> and the multiplication constant (or, rather, its reverse) is called "temperature" (<img decoding="async" src="https://s0.wp.com/latex.php?latex=T&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="T">). The higher the temperature, the more smoothed out the probabilities, and the bigger the chance that the next picked token will not be just the first one.</p>
<p>The formula for tokens' probabilities is <img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+p_n+%3D+%5Cmathrm%7Bsoftmax_n%5Cleft%28%5Cfrac%7B%5Cmathbf%7Bscores%7D%7D%7BT%7D%5Cright%29%7D&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\displaystyle p_n = \mathrm{softmax_n\left(\frac{\mathbf{scores}}{T}\right)}">, where <img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bscores%7D&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\mathbf{scores}"> is the set of <img decoding="async" src="https://s0.wp.com/latex.php?latex=top%5C_n&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="top\_n"> scores.</p>
<h4>Why is it called "temperature"?</h4>
<p>The softmax function has another name: <a href="https://en.wikipedia.org/wiki/Boltzmann_distribution" rel="noopener" target="_blank">Boltzmann distribution</a>. It's extensively used in physics. Among other things, it serves as a base for the <a href="https://en.wikipedia.org/wiki/Barometric_formula" rel="noopener" target="_blank">barometric formula</a>, which tells how density or air varies with altitude.</p>
<p>Intuitively, hot air rises. It spreads further away from the Earth. When air is hot, it's more likely for an air molecule to bounce off its neighbors and jump at an otherwise impossible height. Compared to colder temperatures, air density increases at higher altitudes and drops at sea level.</p>
<p>See how air behaves at different temperatures:</p>


<p><em>Courtesy of Dominic Ford, <a href="https://sciencedemos.org.uk/balls.php" rel="noopener" target="_blank">Bouncing Balls and the Boltzmann Distribution</a></em></p>
<p>By analogy, a large "temperature" increases the probability of second-choice tokens being selected (at the expense of the first-choice tokens, of course). The inference becomes less predictable and more "creative".</p>
<p>Let's put this all into SQL. The prompt was "PostgreSQL is great". Here are the top 5 tokens that, according to the model, are most likely to continue this phrase, and their probabilities at different temperatures:</p>
<pre title="">WITH    RECURSIVE
        initial AS
        (
        SELECT  ARRAY[6307, 47701, 318, 1049] AS input
        ),
        hparams AS
        (
        SELECT  12 AS n_block,
                5 AS top_n,
                ARRAY_LENGTH(input, 1) AS n_seq
        FROM    initial
        ),
        embeddings AS
        (
        SELECT  place, values
        FROM    initial
        CROSS JOIN
                hparams
        CROSS JOIN LATERAL
                UNNEST(input) WITH ORDINALITY AS tokens (token, ordinality)
        CROSS JOIN LATERAL
                (
                SELECT  ordinality - 1 AS place
                ) o
        CROSS JOIN LATERAL
                (
                SELECT  wte.values + wpe.values AS values
                FROM    wte
                CROSS JOIN
                        wpe
                WHERE   wte.token = tokens.token
                        AND wpe.place = o.place
                ) embedding
        ),
        transform AS
        (
        SELECT  0 AS block, place, values
        FROM    embeddings
        UNION ALL
        (
        WITH    previous AS
                (
                SELECT  *
                FROM    transform
                )
        SELECT  block + 1 AS block, transformed_layer.*
        FROM    hparams
        CROSS JOIN LATERAL
                (
                SELECT  block
                FROM    previous
                WHERE   block &lt; 12
                LIMIT   1
                ) q
        CROSS JOIN LATERAL
                (
                WITH    ln_2_b AS
                        (
                        SELECT  *
                        FROM    ln_2_b
                        WHERE   block = q.block
                        ),
                        ln_2_g AS
                        (
                        SELECT  *
                        FROM    ln_2_g
                        WHERE   block = q.block
                        ),
                        c_proj_w AS
                        (
                        SELECT  *
                        FROM    c_proj_w
                        WHERE   block = q.block
                        ),
                        c_proj_b AS
                        (
                        SELECT  *
                        FROM    c_proj_b
                        WHERE   block = q.block
                        ),
                        mlp_c_fc_w AS
                        (
                        SELECT  *
                        FROM    mlp_c_fc_w
                        WHERE   block = q.block
                        ),
                        mlp_c_fc_b AS
                        (
                        SELECT  *
                        FROM    mlp_c_fc_b
                        WHERE   block = q.block
                        ),
                        mlp_c_proj_w AS
                        (
                        SELECT  *
                        FROM    mlp_c_proj_w
                        WHERE   block = q.block
                        ),
                        mlp_c_proj_b AS
                        (
                        SELECT  *
                        FROM    mlp_c_proj_b
                        WHERE   block = q.block
                        ),
                        c_attn_w AS
                        (
                        SELECT  *
                        FROM    c_attn_w
                        WHERE   block = q.block
                        ),
                        c_attn_b AS
                        (
                        SELECT  *
                        FROM    c_attn_b
                        WHERE   block = q.block
                        ),
                        ln_1_g AS
                        (
                        SELECT  *
                        FROM    ln_1_g
                        WHERE   block = q.block
                        ),
                        ln_1_b AS
                        (
                        SELECT  *
                        FROM    ln_1_b
                        WHERE   block = q.block
                        ),
                        mha_norm AS
                        (
                        SELECT  place, mm.values + c_attn_b.values AS values
                        FROM    (
                                SELECT  place, ARRAY_AGG(INNER_PRODUCT(c_attn_w.values, layer_norm.values) ORDER BY y)::VECTOR(2304) AS values
                                FROM    (
                                        SELECT  place, agg.values * ln_1_g.values + ln_1_b.values AS values
                                        FROM    (
                                                SELECT  place, norm.values
                                                FROM    previous
                                                CROSS JOIN LATERAL
                                                        (
                                                        SELECT  AVG(value) AS mean,
                                                                VAR_POP(value) AS variance
                                                        FROM    UNNEST(values::REAL[]) value
                                                        ) agg
                                                CROSS JOIN LATERAL
                                                        (
                                                        SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                                                        FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n(value, ordinality)
                                                        ) norm
                                                ) agg
                                        CROSS JOIN
                                                ln_1_b
                                        CROSS JOIN
                                                ln_1_g
                                        ) layer_norm
                                CROSS JOIN
                                        c_attn_w
                                GROUP BY
                                        place
                                ) mm
                        CROSS JOIN
                                c_attn_b
                        ),
                        heads AS
                        (
                        SELECT  place, head,
                                (values::REAL[])[(head * 64 + 1):(head * 64 + 64)]::VECTOR(64) AS q,
                                (values::REAL[])[(head * 64 + 1 + 768):(head * 64 + 64 + 768)]::VECTOR(64) AS k,
                                (values::REAL[])[(head * 64 + 1 + 1536):(head * 64 + 64 + 1536)]::VECTOR(64) AS v
                        FROM    mha_norm
                        CROSS JOIN
                                GENERATE_SERIES(0, 11) head
                        ),
                        sm_input AS
                        (
                        SELECT  head, h1.place AS x, h2.place AS y, INNER_PRODUCT(h1.q, h2.k) / 8 + CASE WHEN h2.place &gt; h1.place THEN -1E10 ELSE 0 END AS value
                        FROM    heads h1
                        JOIN    heads h2
                        USING   (head)
                        ),
                        sm_diff AS
                        (
                        SELECT  head, x, y, value - MAX(value) OVER (PARTITION BY head, x) AS diff
                        FROM    sm_input
                        ),
                        sm_exp AS
                        (
                        SELECT  head, x, y, CASE WHEN diff &lt; -745.13 THEN 0 ELSE EXP(diff) END AS e
                        FROM    sm_diff
                        ),
                        softmax AS
                        (
                        SELECT  head, x, y AS place, e / SUM(e) OVER (PARTITION BY head, x) AS value
                        FROM    sm_exp
                        ),
                        attention AS
                        (
                        SELECT  place, ARRAY_AGG(value ORDER BY head * 64 + ordinality)::VECTOR(768) AS values
                        FROM    (
                                SELECT  head, x AS place, SUM(ARRAY_FILL(softmax.value, ARRAY[64])::VECTOR(64) * heads.v) AS values
                                FROM    softmax
                                JOIN    heads
                                USING   (head, place)
                                GROUP BY
                                        head, x
                                ) q
                        CROSS JOIN LATERAL
                                UNNEST(values::REAL[]) WITH ORDINALITY v (value, ordinality)
                        GROUP BY
                                place
                        ),
                        mha AS
                        (
                        SELECT  place, w.values + c_proj_b.values + previous.values AS values
                        FROM    (
                                SELECT  attention.place, ARRAY_AGG(INNER_PRODUCT(attention.values, c_proj_w.values) ORDER BY c_proj_w.place)::VECTOR(768) AS values
                                FROM    attention
                                CROSS JOIN
                                        c_proj_w
                                GROUP BY
                                        attention.place
                                ) w
                        CROSS JOIN
                                c_proj_b
                        JOIN    previous
                        USING   (place)
                        ),
                        ffn_norm AS
                        (
                        SELECT  place, agg.values * ln_2_g.values + ln_2_b.values AS values
                        FROM    (
                                SELECT  place, norm.values
                                FROM    mha
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  AVG(value) AS mean,
                                                VAR_POP(value) AS variance
                                        FROM    UNNEST(values::REAL[]) value
                                        ) agg
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                                        FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n(value, ordinality)
                                        ) norm
                                ) agg
                        CROSS JOIN
                                ln_2_b
                        CROSS JOIN
                                ln_2_g
                        ),
                        ffn_a AS
                        (
                        SELECT  gelu.place, gelu.values
                        FROM    (
                                SELECT  place, w.values + mlp_c_fc_b.values AS values
                                FROM    (
                                        SELECT  ffn_norm.place, ARRAY_AGG(INNER_PRODUCT(ffn_norm.values, mlp_c_fc_w.values) ORDER BY mlp_c_fc_w.place)::VECTOR(3072) AS values
                                        FROM    ffn_norm
                                        CROSS JOIN
                                                mlp_c_fc_w
                                        GROUP BY
                                                ffn_norm.place
                                        ) w
                                CROSS JOIN
                                        mlp_c_fc_b
                                ) v
                        CROSS JOIN LATERAL
                                (
                                SELECT  place, ARRAY_AGG(0.5 * value * (1 + TANH(0.797884560802 * (value + 0.044715 * value*value*value))) ORDER BY ordinality)::VECTOR(3072) AS values
                                FROM    UNNEST(values::REAL[]) WITH ORDINALITY n (value, ordinality)
                                GROUP BY
                                        place
                                ) gelu
                        ),
                        ffn AS
                        (
                        SELECT  place, w.values + mlp_c_proj_b.values + mha.values AS values
                        FROM    (
                                SELECT  ffn_a.place, ARRAY_AGG(INNER_PRODUCT(ffn_a.values, mlp_c_proj_w.values) ORDER BY mlp_c_proj_w.place)::VECTOR(768) AS values
                                FROM    ffn_a
                                CROSS JOIN
                                        mlp_c_proj_w
                                GROUP BY
                                        ffn_a.place
                                ) w
                        CROSS JOIN
                                mlp_c_proj_b
                        JOIN    mha
                        USING   (place)
                        )
                SELECT  *
                FROM    ffn
                ) transformed_layer
        )
        ),
        block_output AS
        (
        SELECT  *
        FROM    hparams
        JOIN    transform
        ON      transform.block = n_block
        ),
        ln_f AS
        (
        SELECT  place, norm.values * ln_f_g.values + ln_f_b.values AS values
        FROM    block_output
        CROSS JOIN LATERAL
                (
                SELECT  AVG(value) AS mean,
                        VAR_POP(value) AS variance
                FROM    UNNEST(values::REAL[]) AS n(value)
                ) agg
        CROSS JOIN LATERAL
                (
                SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n (value, ordinality)
                ) norm
        CROSS JOIN
                ln_f_b
        CROSS JOIN
                ln_f_g
        ),
        logits AS
        (
        SELECT  logits.*
        FROM    hparams
        CROSS JOIN LATERAL
                (
                SELECT  token, INNER_PRODUCT(ln_f.values, wte.values) AS value
                FROM    ln_f
                CROSS JOIN
                        wte
                WHERE   ln_f.place = n_seq - 1
                ORDER BY
                        value DESC
                LIMIT   (top_n)
                ) logits
        ),
        temperatures (temperature) AS
        (
        VALUES
        (0.5),
        (1),
        (2)
        ),
        tokens AS
        (
        SELECT  token, value, softmax, temperature
        FROM    temperatures
        CROSS JOIN LATERAL
                (
                SELECT  *, (e / SUM(e) OVER ()) AS softmax
                FROM    (
                        SELECT  *,
                                (value - MAX(value) OVER ()) / temperature AS diff
                        FROM    logits
                        ) exp_x
                CROSS JOIN LATERAL
                        (
                        SELECT  CASE WHEN diff &lt; -745.13 THEN 0 ELSE EXP(diff) END AS e
                        ) exp
                ) q
        )
SELECT  token,
        cluster,
        TO_CHAR(t1.value, 'S00.000') AS score,
        TO_CHAR(t1.softmax, '0.00') AS "temperature = 0.5",
        TO_CHAR(t2.softmax, '0.00') AS "temperature = 1",
        TO_CHAR(t3.softmax, '0.00') AS "temperature = 2"
FROM    (
        SELECT  *
        FROM    tokens
        WHERE   temperature = 0.5
        ) t1
JOIN    (
        SELECT  *
        FROM    tokens
        WHERE   temperature = 1
        ) t2
USING   (token)
JOIN    (
        SELECT  *
        FROM    tokens
        WHERE   temperature = 2
        ) t3
USING   (token)
JOIN    tokenizer
USING   (token)
</pre>
<div>
<table>
<tbody><tr>
<th>token</th>
<th>cluster</th>
<th>score</th>
<th>temperature = 0.5</th>
<th>temperature = 1</th>
<th>temperature = 2</th>
</tr>
<tr>
<td>329</td>
<td>Ġfor</td>
<td>-85.435</td>
<td> 0.74</td>
<td> 0.48</td>
<td> 0.33</td>
</tr>
<tr>
<td>11</td>
<td>,</td>
<td>-86.232</td>
<td> 0.15</td>
<td> 0.22</td>
<td> 0.22</td>
</tr>
<tr>
<td>13</td>
<td>.</td>
<td>-86.734</td>
<td> 0.05</td>
<td> 0.13</td>
<td> 0.17</td>
</tr>
<tr>
<td>379</td>
<td>Ġat</td>
<td>-86.785</td>
<td> 0.05</td>
<td> 0.12</td>
<td> 0.17</td>
</tr>
<tr>
<td>284</td>
<td>Ġto</td>
<td>-87.628</td>
<td> 0.01</td>
<td> 0.05</td>
<td> 0.11</td>
</tr>
</tbody></table>
</div>
<h3>Inference</h3>
<p>Finally, we are ready to do some real inference: run the model, select a token according to its probability, add it to the prompt and repeat until enough tokens are generated.</p>
<p>The LLM itself, as we saw before, is deterministic: it's just a series of matrix multiplications and other math operations on predefined constants. As long as the prompt and the hyperparameters like temperature and top_n are the same, the output will also be the same.</p>
<p>The only non-deterministic process is token selection. There is randomness involved in it (to a variable degree). That's why GPT-based chatbots can give different answers to the same prompt.</p>
<p>We will use the phrase "Happy New Year! I wish" as the prompt and make the model generate 10 new tokens for this prompt. The temperature will be set to 2, and top_n will be set to 5.</p>
<p>The query runs for 2:44 minutes on my machine. Here's its output:</p>
<pre title="">SELECT SETSEED(0.20231231);

WITH    RECURSIVE
        input AS
        (
        SELECT  'Happy New Year! I wish you' AS prompt,
                10 AS threshold,
                2 AS temperature,
                1 AS top_n
        ),
        clusters AS
        (
        SELECT  part_position, bpe.*
        FROM    input
        CROSS JOIN LATERAL
                REGEXP_MATCHES(prompt, '''s|''t|''re|''ve|''m|''ll|''d| ?\w+| ?\d+| ?[^\s\w\d]+|\s+(?!\S)|\s+', 'g') WITH ORDINALITY AS rm (part, part_position)
        CROSS JOIN LATERAL
                (
                WITH    RECURSIVE
                        bpe AS
                        (
                        SELECT  (n + 1)::BIGINT AS position, character, TRUE AS continue
                        FROM    CONVERT_TO(part[1], 'UTF-8') AS bytes
                        CROSS JOIN LATERAL
                                GENERATE_SERIES(0, LENGTH(bytes) - 1) AS n
                        JOIN    encoder
                        ON      byte = GET_BYTE(bytes, n)
                        UNION ALL
                        (
                        WITH    RECURSIVE
                                base AS
                                (
                                SELECT  *
                                FROM    bpe
                                WHERE   continue
                                ),
                                bn AS
                                (
                                SELECT  ROW_NUMBER() OVER (ORDER BY position) AS position,
                                        continue,
                                        character,
                                        character || LEAD(character) OVER (ORDER BY position) AS cluster
                                FROM    base
                                ),
                                top_rank AS
                                (
                                SELECT  tokenizer.*
                                FROM    bn
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  *
                                        FROM    tokenizer
                                        WHERE   tokenizer.cluster = bn.cluster
                                        LIMIT   1
                                        ) tokenizer
                                ORDER BY
                                        token
                                LIMIT   1
                                ),
                                breaks AS
                                (
                                SELECT  0::BIGINT AS position, 1 AS length
                                UNION ALL
                                SELECT  bn.position,
                                        CASE WHEN token IS NULL THEN 1 ELSE 2 END
                                FROM    breaks
                                JOIN    bn
                                ON      bn.position = breaks.position + length
                                LEFT JOIN
                                        top_rank
                                USING   (cluster)
                                )
                        SELECT  position, character, token IS NOT NULL
                        FROM    breaks
                        LEFT JOIN
                                top_rank
                        ON      1 = 1
                        CROSS JOIN LATERAL
                                (
                                SELECT  STRING_AGG(character, '' ORDER BY position) AS character
                                FROM    bn
                                WHERE   bn.position &gt;= breaks.position
                                        AND bn.position &lt; breaks.position + length
                                ) bn
                        WHERE   position &gt; 0
                        )
                        )
                SELECT  position, character AS cluster
                FROM    bpe
                WHERE   NOT continue
                ) bpe
        ),
        tokens AS
        (
        SELECT  ARRAY_AGG(token ORDER BY part_position, position) AS input
        FROM    clusters
        JOIN    tokenizer
        USING   (cluster)
        ),
        gpt AS
        (
        SELECT  input, ARRAY_LENGTH(input, 1) AS original_length
        FROM    tokens
        UNION ALL
        SELECT  input || next_token.token, original_length
        FROM    gpt
        CROSS JOIN
                input
        CROSS JOIN LATERAL
                (
                WITH    RECURSIVE
                        hparams AS
                        (
                        SELECT  ARRAY_LENGTH(input, 1) AS n_seq,
                                12 AS n_block
                        ),
                        embeddings AS
                        (
                        SELECT  place, values
                        FROM    hparams
                        CROSS JOIN LATERAL
                                UNNEST(input) WITH ORDINALITY AS tokens (token, ordinality)
                        CROSS JOIN LATERAL
                                (
                                SELECT  ordinality - 1 AS place
                                ) o
                        CROSS JOIN LATERAL
                                (
                                SELECT  wte.values + wpe.values AS values
                                FROM    wte
                                CROSS JOIN
                                        wpe
                                WHERE   wte.token = tokens.token
                                        AND wpe.place = o.place
                                ) embedding
                        ),
                        transform AS
                        (
                        SELECT  0 AS block, place, values
                        FROM    embeddings
                        UNION ALL
                        (
                        WITH    previous AS
                                (
                                SELECT  *
                                FROM    transform
                                )
                        SELECT  block + 1 AS block, transformed_layer.*
                        FROM    hparams
                        CROSS JOIN LATERAL
                                (
                                SELECT  block
                                FROM    previous
                                WHERE   block &lt; 12
                                LIMIT   1
                                ) q
                        CROSS JOIN LATERAL
                                (
                                WITH    ln_2_b AS
                                        (
                                        SELECT  *
                                        FROM    ln_2_b
                                        WHERE   block = q.block
                                        ),
                                        ln_2_g AS
                                        (
                                        SELECT  *
                                        FROM    ln_2_g
                                        WHERE   block = q.block
                                        ),
                                        c_proj_w AS
                                        (
                                        SELECT  *
                                        FROM    c_proj_w
                                        WHERE   block = q.block
                                        ),
                                        c_proj_b AS
                                        (
                                        SELECT  *
                                        FROM    c_proj_b
                                        WHERE   block = q.block
                                        ),
                                        mlp_c_fc_w AS
                                        (
                                        SELECT  *
                                        FROM    mlp_c_fc_w
                                        WHERE   block = q.block
                                        ),
                                        mlp_c_fc_b AS
                                        (
                                        SELECT  *
                                        FROM    mlp_c_fc_b
                                        WHERE   block = q.block
                                        ),
                                        mlp_c_proj_w AS
                                        (
                                        SELECT  *
                                        FROM    mlp_c_proj_w
                                        WHERE   block = q.block
                                        ),
                                        mlp_c_proj_b AS
                                        (
                                        SELECT  *
                                        FROM    mlp_c_proj_b
                                        WHERE   block = q.block
                                        ),
                                        c_attn_w AS
                                        (
                                        SELECT  *
                                        FROM    c_attn_w
                                        WHERE   block = q.block
                                        ),
                                        c_attn_b AS
                                        (
                                        SELECT  *
                                        FROM    c_attn_b
                                        WHERE   block = q.block
                                        ),
                                        ln_1_g AS
                                        (
                                        SELECT  *
                                        FROM    ln_1_g
                                        WHERE   block = q.block
                                        ),
                                        ln_1_b AS
                                        (
                                        SELECT  *
                                        FROM    ln_1_b
                                        WHERE   block = q.block
                                        ),
                                        mha_norm AS
                                        (
                                        SELECT  place, mm.values + c_attn_b.values AS values
                                        FROM    (
                                                SELECT  place, ARRAY_AGG(INNER_PRODUCT(c_attn_w.values, layer_norm.values) ORDER BY y)::VECTOR(2304) AS values
                                                FROM    (
                                                        SELECT  place, agg.values * ln_1_g.values + ln_1_b.values AS values
                                                        FROM    (
                                                                SELECT  place, norm.values
                                                                FROM    previous
                                                                CROSS JOIN LATERAL
                                                                        (
                                                                        SELECT  AVG(value) AS mean,
                                                                                VAR_POP(value) AS variance
                                                                        FROM    UNNEST(values::REAL[]) value
                                                                        ) agg
                                                                CROSS JOIN LATERAL
                                                                        (
                                                                        SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                                                                        FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n(value, ordinality)
                                                                        ) norm
                                                                ) agg
                                                        CROSS JOIN
                                                                ln_1_b
                                                        CROSS JOIN
                                                                ln_1_g
                                                        ) layer_norm
                                                CROSS JOIN
                                                        c_attn_w
                                                GROUP BY
                                                        place
                                                ) mm
                                        CROSS JOIN
                                                c_attn_b
                                        ),
                                        heads AS
                                        (
                                        SELECT  place, head,
                                                (values::REAL[])[(head * 64 + 1):(head * 64 + 64)]::VECTOR(64) AS q,
                                                (values::REAL[])[(head * 64 + 1 + 768):(head * 64 + 64 + 768)]::VECTOR(64) AS k,
                                                (values::REAL[])[(head * 64 + 1 + 1536):(head * 64 + 64 + 1536)]::VECTOR(64) AS v
                                        FROM    mha_norm
                                        CROSS JOIN
                                                GENERATE_SERIES(0, 11) head
                                        ),
                                        sm_input AS
                                        (
                                        SELECT  head, h1.place AS x, h2.place AS y, INNER_PRODUCT(h1.q, h2.k) / 8 + CASE WHEN h2.place &gt; h1.place THEN -1E10 ELSE 0 END AS value
                                        FROM    heads h1
                                        JOIN    heads h2
                                        USING   (head)
                                        ),
                                        sm_diff AS
                                        (
                                        SELECT  head, x, y, value - MAX(value) OVER (PARTITION BY head, x) AS diff
                                        FROM    sm_input
                                        ),
                                        sm_exp AS
                                        (
                                        SELECT  head, x, y, CASE WHEN diff &lt; -745.13 THEN 0 ELSE EXP(diff) END AS e
                                        FROM    sm_diff
                                        ),
                                        softmax AS
                                        (
                                        SELECT  head, x, y AS place, e / SUM(e) OVER (PARTITION BY head, x) AS value
                                        FROM    sm_exp
                                        ),
                                        attention AS
                                        (
                                        SELECT  place, ARRAY_AGG(value ORDER BY head * 64 + ordinality)::VECTOR(768) AS values
                                        FROM    (
                                                SELECT  head, x AS place, SUM(ARRAY_FILL(softmax.value, ARRAY[64])::VECTOR(64) * heads.v) AS values
                                                FROM    softmax
                                                JOIN    heads
                                                USING   (head, place)
                                                GROUP BY
                                                        head, x
                                                ) q
                                        CROSS JOIN LATERAL
                                                UNNEST(values::REAL[]) WITH ORDINALITY v (value, ordinality)
                                        GROUP BY
                                                place
                                        ),
                                        mha AS
                                        (
                                        SELECT  place, w.values + c_proj_b.values + previous.values AS values
                                        FROM    (
                                                SELECT  attention.place, ARRAY_AGG(INNER_PRODUCT(attention.values, c_proj_w.values) ORDER BY c_proj_w.place)::VECTOR(768) AS values
                                                FROM    attention
                                                CROSS JOIN
                                                        c_proj_w
                                                GROUP BY
                                                        attention.place
                                                ) w
                                        CROSS JOIN
                                                c_proj_b
                                        JOIN    previous
                                        USING   (place)
                                        ),
                                        ffn_norm AS
                                        (
                                        SELECT  place, agg.values * ln_2_g.values + ln_2_b.values AS values
                                        FROM    (
                                                SELECT  place, norm.values
                                                FROM    mha
                                                CROSS JOIN LATERAL
                                                        (
                                                        SELECT  AVG(value) AS mean,
                                                                VAR_POP(value) AS variance
                                                        FROM    UNNEST(values::REAL[]) value
                                                        ) agg
                                                CROSS JOIN LATERAL
                                                        (
                                                        SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                                                        FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n(value, ordinality)
                                                        ) norm
                                                ) agg
                                        CROSS JOIN
                                                ln_2_b
                                        CROSS JOIN
                                                ln_2_g
                                        ),
                                        ffn_a AS
                                        (
                                        SELECT  gelu.place, gelu.values
                                        FROM    (
                                                SELECT  place, w.values + mlp_c_fc_b.values AS values
                                                FROM    (
                                                        SELECT  ffn_norm.place, ARRAY_AGG(INNER_PRODUCT(ffn_norm.values, mlp_c_fc_w.values) ORDER BY mlp_c_fc_w.place)::VECTOR(3072) AS values
                                                        FROM    ffn_norm
                                                        CROSS JOIN
                                                                mlp_c_fc_w
                                                        GROUP BY
                                                                ffn_norm.place
                                                        ) w
                                                CROSS JOIN
                                                        mlp_c_fc_b
                                                ) v
                                        CROSS JOIN LATERAL
                                                (
                                                SELECT  place, ARRAY_AGG(0.5 * value * (1 + TANH(0.797884560802 * (value + 0.044715 * value*value*value))) ORDER BY ordinality)::VECTOR(3072) AS values
                                                FROM    UNNEST(values::REAL[]) WITH ORDINALITY n (value, ordinality)
                                                GROUP BY
                                                        place
                                                ) gelu
                                        ),
                                        ffn AS
                                        (
                                        SELECT  place, w.values + mlp_c_proj_b.values + mha.values AS values
                                        FROM    (
                                                SELECT  ffn_a.place, ARRAY_AGG(INNER_PRODUCT(ffn_a.values, mlp_c_proj_w.values) ORDER BY mlp_c_proj_w.place)::VECTOR(768) AS values
                                                FROM    ffn_a
                                                CROSS JOIN
                                                        mlp_c_proj_w
                                                GROUP BY
                                                        ffn_a.place
                                                ) w
                                        CROSS JOIN
                                                mlp_c_proj_b
                                        JOIN    mha
                                        USING   (place)
                                        )
                                SELECT  *
                                FROM    ffn
                                ) transformed_layer
                        )
                        ),
                        block_output AS
                        (
                        SELECT  *
                        FROM    hparams
                        JOIN    transform
                        ON      transform.block = n_block
                        ),
                        ln_f AS
                        (
                        SELECT  place, norm.values * ln_f_g.values + ln_f_b.values AS values
                        FROM    block_output
                        CROSS JOIN LATERAL
                                (
                                SELECT  AVG(value) AS mean,
                                        VAR_POP(value) AS variance
                                FROM    UNNEST(values::REAL[]) AS n(value)
                                ) agg
                        CROSS JOIN LATERAL
                                (
                                SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                                FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n (value, ordinality)
                                ) norm
                        CROSS JOIN
                                ln_f_b
                        CROSS JOIN
                                ln_f_g
                        ),
                        logits AS
                        (
                        SELECT  token, INNER_PRODUCT(ln_f.values, wte.values) AS value
                        FROM    hparams
                        JOIN    ln_f
                        ON      ln_f.place = n_seq - 1
                        CROSS JOIN
                                wte
                        ORDER BY
                                value DESC
                        LIMIT   (top_n)
                        ),
                        tokens AS
                        (
                        SELECT  token,
                                high - softmax AS low,
                                high
                        FROM    (
                                SELECT  *,
                                        SUM(softmax) OVER (ORDER BY softmax) AS high
                                FROM    (
                                        SELECT  *, (e / SUM(e) OVER ()) AS softmax
                                        FROM    (
                                                SELECT  *,
                                                        (value - MAX(value) OVER ()) / temperature AS diff
                                                FROM    logits
                                                ) exp_x
                                        CROSS JOIN LATERAL
                                                (
                                                SELECT  CASE WHEN diff &lt; -745.13 THEN 0 ELSE EXP(diff) END AS e
                                                ) exp
                                        ) q
                                ) q
                        ),
                        next_token AS
                        (
                        SELECT  *
                        FROM    (
                                SELECT  RANDOM() AS rnd
                                ) r
                        CROSS JOIN LATERAL
                                (
                                SELECT  *
                                FROM    tokens
                                WHERE   rnd &gt;= low
                                        AND rnd &lt; high
                                ) nt
                        )
                SELECT  *
                FROM    next_token
                ) next_token
        WHERE   ARRAY_LENGTH(input, 1) &lt; original_length + threshold
                AND next_token.token &lt;&gt; 50256
        ),
        output AS
        (
        SELECT  CONVERT_FROM(STRING_AGG(SET_BYTE('\x00', 0, byte), '' ORDER BY position), 'UTF8') AS response
        FROM    (
                SELECT  STRING_AGG(cluster, '' ORDER BY ordinality) AS response
                FROM    input
                JOIN    gpt
                ON      ARRAY_LENGTH(input, 1) = original_length + threshold
                CROSS JOIN LATERAL
                        UNNEST(input) WITH ORDINALITY n (token, ordinality)
                JOIN    tokenizer
                USING   (token)
                ) q
        CROSS JOIN LATERAL
                STRING_TO_TABLE(response, NULL) WITH ORDINALITY n (character, position)
        JOIN    encoder
        USING   (character)
        )
SELECT  *
FROM    output
</pre>
<div>
<table>
<tbody><tr>
<th>response</th>
</tr>
<tr>
<td>Happy New Year! I wish you all the best in your new year!
</td>
</tr>
</tbody></table>
</div>
<p>This part the AI got right. I do wish you all the best in your new year!</p>
<p>You can find the queries and the installation code in the GitHub repository: <a href="https://github.com/quassnoi/explain-extended-2024" rel="noopener" target="_blank">quassnoi/explain-extended-2024</a></p>
<p>
<big><strong>Happy New Year!</strong></big>
</p>
<p>Previous New Year posts:</p>
<ul>
<li><a href="https://explainextended.com/2009/12/31/happy-new-year/">2010: SQL graphics in Oracle, MySQL, SQL Server and PostgreSQL</a></li>
<li><a href="https://explainextended.com/2010/12/31/happy-new-year-2/">2011: Drawing a clock in SQL</a></li>
<li><a href="https://explainextended.com/2011/12/31/happy-new-year-3/">2012: Drawing snowflakes in SQL</a></li>
<li><a href="https://explainextended.com/2012/12/31/happy-new-year-4/">2013: View of Earth from space in SQL</a></li>
<li><a href="https://explainextended.com/2013/12/31/happy-new-year-5/">2014: Drawing fractals in SQL</a></li>
<li><a href="https://explainextended.com/2014/12/31/happy-new-year-6/">2015: Composing music in SQL</a></li>
<li><a href="https://explainextended.com/2015/12/31/happy-new-year-7/">2016: Conway’s Game of Life in SQL</a></li>
<li><a href="https://explainextended.com/2016/12/31/happy-new-year-8/">2017: The Sultan’s Riddle in SQL</a></li>
<li><a href="https://explainextended.com/2017/12/31/happy-new-year-9/">2018: Settlers of Catan in SQL</a></li>
<li><a href="https://explainextended.com/2018/12/31/happy-new-year-10/">2019: GIF decoder in SQL</a></li>
<li><a href="https://explainextended.com/2019/12/31/happy-new-year-11/">2020: A stereogram in SQL</a></li>
<li><a href="https://explainextended.com/2020/12/31/happy-new-year-12/">2021: 3D picture of the coronavirus in SQL</a></li>
<li><a href="https://explainextended.com/2021/12/31/happy-new-year-13/">2022: Quantum computer emulator in SQL</a></li>
<li><a href="https://explainextended.com/2022/12/31/happy-new-year-14/">2023: Solving the Rubik’s Cube in SQL</a></li>
</ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Chronic stress spreads cancer (106 pts)]]></title>
            <link>https://www.cshl.edu/chronic-stress-spreads-cancer-heres-how/</link>
            <guid>39488653</guid>
            <pubDate>Sat, 24 Feb 2024 02:43:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cshl.edu/chronic-stress-spreads-cancer-heres-how/">https://www.cshl.edu/chronic-stress-spreads-cancer-heres-how/</a>, See on <a href="https://news.ycombinator.com/item?id=39488653">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-68688">
    <!-- HEADLINE -->
    <header>
		
    </header><!-- .entry-header -->

    <!-- // FEATURE IMAGE -->
<div>
                 <p><img src="https://www.cshl.edu/wp-content/uploads/2024/02/Lung_Cancer_Metastasis.jpg" alt="Image of lung cancer metastasis in a mouse"></p><figcaption>For a recent CSHL Cancer Center study, Adjunct Professor Mikala Egeblad (now a Bloomberg Distinguished Professor with Johns Hopkins University) and postdoc Xue-Yan He (now Assistant Professor of Cell Biology &amp; Physiology at Washington University School of Medicine in St. Louis) teamed with CSHL Professor Linda Van Aelst. Above: lung cancer metastasis in a mouse that underwent experiments designed to simulate the stress that cancer patients experience.</figcaption></div>

	<div>
		<div><p><em>
Read time 3 minutes | <span><time datetime="Thursday, 22 February  2024">Thursday, 22 February 2024</time></span>				</em></p>           
           
				</div><!--/.row-->	

<!-- MAIN POST CONTENT AREA -->
		<div>
			<div>


				

<p><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img decoding="async" src="https://www.cshl.edu/wp-content/uploads/2023/03/print_pdf_icon.png" alt="Print Friendly, PDF &amp; Email"></a></p><p>Stress is inevitable. But too much of it can be terrible for our health. Chronic stress can increase our risk for heart disease and strokes. It may also help cancer spread. How this works has remained a mystery—a challenge for cancer care. Xue-Yan He, a former postdoc in Cold Spring Harbor Laboratory (CSHL) Adjunct Professor <a href="https://www.cshl.edu/research/faculty-staff/mikala-egeblad/">Mikala Egeblad</a>’s lab, explains:</p>
<p><span data-url="https://www.cshl.edu/wp-content/uploads/2024/02/He_Stress_Cancer_Patients.mp3" data-plays="1">“Stress is something we cannot really avoid in cancer patients. You can imagine if you are diagnosed, you cannot stop thinking about the disease or insurance or family. So it is very important to understand how stress works on us.”</span></p>
<p>Now, He and Egeblad may have reached a breakthrough in understanding exactly that. Working with CSHL Professor <a href="https://www.cshl.edu/research/faculty-staff/linda-van-aelst/">Linda Van Aelst</a>, they discovered that stress causes certain white blood cells called neutrophils to form sticky web-like structures that make body tissues more susceptible to metastasis. The finding could point to new treatment strategies that stop cancer’s spread before it starts.</p>
<p>The team arrived at their discovery by mimicking chronic stress in mice with cancer. They first removed tumors that had been growing in mice’s breasts and spreading cancer cells to their lungs. Next, they exposed the mice to stress. What He observed was shocking. Egeblad recalls:</p>
<p><span data-url="https://www.cshl.edu/wp-content/uploads/2024/02/Egeblad_Increase_Metastatic_Lesions.mp3" data-plays="1">“She saw this scary increase in metastatic lesions in these animals. It was up to a fourfold increase in metastasis.”</span></p>
<p>The team found that stress hormones called glucocorticoids acted on the neutrophils. These “stressed” neutrophils formed spider-web-like structures called <a href="https://www.cshl.edu/how-an-antiviral-immune-reaction-can-go-too-far/">NETs (neutrophil extracellular traps)</a>. NETs form when neutrophils expel DNA. Normally, they can defend us against invading microorganisms. However, in cancer, NETs create a metastasis-friendly environment. </p>
<figure id="attachment_68683"><a href="https://www.cshl.edu/wp-content/uploads/2024/02/Cancer_stress_Dnase_I_Comparison.jpg"><img decoding="async" fetchpriority="high" data-headline="Cancer spread faster and more furiously in stressed mice (middle column) than in a control group (left column). By comparison, cancer cells in stressed mice treated with an enzyme called DNase I (right column) were largely non-proliferating, and the treatment caused a significant reduction in stress-induced metastasis." src="https://www.cshl.edu/wp-content/uploads/2024/02/Cancer_stress_Dnase_I_Comparison.jpg" alt="Image of comparisons in cancer growth with low stress, high stress, and DNase I" width="1920" height="1080" srcset="https://www.cshl.edu/wp-content/uploads/2024/02/Cancer_stress_Dnase_I_Comparison.jpg 1920w, https://www.cshl.edu/wp-content/uploads/2024/02/Cancer_stress_Dnase_I_Comparison-814x458.jpg 814w, https://www.cshl.edu/wp-content/uploads/2024/02/Cancer_stress_Dnase_I_Comparison-1249x703.jpg 1249w, https://www.cshl.edu/wp-content/uploads/2024/02/Cancer_stress_Dnase_I_Comparison-768x432.jpg 768w, https://www.cshl.edu/wp-content/uploads/2024/02/Cancer_stress_Dnase_I_Comparison-1536x864.jpg 1536w" sizes="(max-width: 1920px) 100vw, 1920px"></a><figcaption>Cancer spread faster and more furiously in stressed mice (middle column) than in a control group (left column). By comparison, cancer cells in stressed mice treated with an enzyme called DNase I (right column) were largely non-proliferating, and the treatment caused a significant reduction in stress-induced metastasis.</figcaption></figure>
<p>To confirm that stress triggers NET formation, leading to increased metastasis, He performed three tests. First, she removed neutrophils from the mice using antibodies. Next, she injected a NET-destroying drug into the animals. Lastly, she used mice whose neutrophils couldn’t respond to glucocorticoids. Each test achieved similar results. “The stressed mice no longer developed more metastasis,” He says.</p>
<p>Notably, the team found that chronic stress caused <a href="https://www.cshl.edu/drug-halts-immune-reactions-to-save-damaged-lungs/">NET formation</a> to modify lung tissue even in mice without cancer. “It’s almost preparing your tissue for getting cancer,” Egeblad explains. </p>
<p>To Van Aelst, the implication, though startling, is clear. “Reducing stress should be a component of cancer treatment <em>and</em> prevention,” she says.</p>
<p>The team also speculates that future drugs preventing NET formation could benefit patients whose cancer hasn’t yet metastasized. Such new treatments could slow or stop cancer’s spread, offering much-needed relief.</p>
<p><strong>Written by</strong>: <a href="https://www.cshl.edu/author/osborne/">Margaret Osborne</a>, <em>Science Writer</em> | <a href="mailto:publicaffairs@cshl.edu">publicaffairs@cshl.edu</a> | 516-367-8455</p><hr><section id="funding"><p><strong>Funding</strong></p><p><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img decoding="async" src="https://www.cshl.edu/wp-content/uploads/2023/03/print_pdf_icon.png" alt="Print Friendly, PDF &amp; Email"></a></p>
<p>National Institutes of Health, Department of Defense Breast Cancer Research Program, American Association for Cancer Research, Cancer Research Institute, German Research Foundation</p>
</section><section id="citation"><p><strong>Citation</strong></p><p><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img decoding="async" src="https://www.cshl.edu/wp-content/uploads/2023/03/print_pdf_icon.png" alt="Print Friendly, PDF &amp; Email"></a></p>
<p>He, X. Y., <em>et al</em>., “Chronic stress increases metastasis via neutrophil-mediated changes to the microenvironment”, <em>Cancer Cell</em>, February 22, 2024. DOI: <a href="https://doi.org/10.1016/j.ccell.2024.01.013" rel="noopener" target="_blank">10.1016/j.ccell.2024.01.013</a></p>
</section><!-- Core facilities-->
<div><h4>Core Facilites</h4></div>
<div id="newsletter">
	<p>
		<h4>
			Stay informed
		</h4>
	</p>

<div>
<p>Sign up for our newsletter to get the latest discoveries, upcoming events, videos, podcasts, and a news roundup delivered straight to your inbox every month.
</p>
<p><a href="https://www.cshl.edu/news-stand/newsletter/"><span></span> &nbsp; Newsletter Signup</a></p>
</div></div>
			</div><!-- /.col-->
			<!-- SIDEBAR SECTION -->
			<div>


				

<h3>Tags</h3>			 <!-- /.row -->     
	</div><!-- /.entry-content -->
</div></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Consol3 – A 3D engine for the terminal that executes on the CPU (143 pts)]]></title>
            <link>https://github.com/Victormeriqui/Consol3</link>
            <guid>39488529</guid>
            <pubDate>Sat, 24 Feb 2024 02:17:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Victormeriqui/Consol3">https://github.com/Victormeriqui/Consol3</a>, See on <a href="https://news.ycombinator.com/item?id=39488529">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><h2 tabindex="-1" dir="auto">Consol3</h2>
<p dir="auto">A graphics engine that executes entirely on the CPU and uses the console as the display</p>
<h2 tabindex="-1" dir="auto">Intro</h2>
<h2 tabindex="-1" dir="auto">Videos</h2>
<p dir="auto"><a href="https://www.youtube.com/watch?v=khu1oPdL6ww" rel="nofollow"><img src="https://camo.githubusercontent.com/582aa9b313454e4313bd5ce2433523fd57976e3a6d7b219636b691716e2019cd/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f6b6875316f50644c3677772f302e6a7067" alt="Rasterization" data-canonical-src="https://img.youtube.com/vi/khu1oPdL6ww/0.jpg"></a><br>
<a href="https://www.youtube.com/watch?v=IXVWcb05Z5U" rel="nofollow"><img src="https://camo.githubusercontent.com/09fa87b569c5f967553f5d88c5457a0d968aa60b8975d98ece1b2cfd5be0a5dd/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f49585657636230355a35552f302e6a7067" alt="Ray Marching" data-canonical-src="https://img.youtube.com/vi/IXVWcb05Z5U/0.jpg"></a></p>
<h3 tabindex="-1" dir="auto">Software Rendering</h3>
<p dir="auto">Consol3 is a 3D graphics engine that doesn't use the graphics card to render any frame, instead the same calculations that would typically be made by the GPU hardware is done in software, every single vertex transformation, matrix calculation, etc is calculated on the CPU</p>
<p dir="auto">To make the engine more flexible, some concepts typically used for programming GPUs are implemented, for example Shaders - However these are still entirely handled on the CPU</p>
<h3 tabindex="-1" dir="auto">Dependencies</h3>
<p dir="auto">No external dependencies will ever be used in this engine, the goal is to do everything using only what the OS already provides, that means no external math libraries, window managers, resource loaders, etc</p>
<h4 tabindex="-1" dir="auto">Older Versions</h4>
<p dir="auto">Building this engine is a hobby of mine, and I've been working on it infrequently for some years now, as such it has gone through many refactors, partial and complete rewrites, this is the latest version of the engine</p>
<h2 tabindex="-1" dir="auto">Building</h2>
<p dir="auto">This project uses CMake, to build it simply create a build folder on the cloned repository, cd into it and run <code>cmake ..</code>, followed by <code>make</code> - after compiling 2
executables should be generated inside the build folder:<br>
- Consol3_raster<br>
- Consol3_voxel</p>
<p dir="auto">Consol3_raster will have a scene with only rasterized meshes, lights &amp; other experiments<br>
Consol3_voxel will have a scene with ray marched voxels, along with a particle-like simulation for sand, water, lava, steam and ice using the voxels</p>
<p dir="auto">The project can be built for either Windows or Linux, on Linux no mouse input is supported yet (use the arrow keys to control the look direction), and only a few frame drawers are supported</p>
<h2 tabindex="-1" dir="auto">Controls</h2>
<h3 tabindex="-1" dir="auto">Raster</h3>
<p dir="auto">Mouse 2,3,4,5 - Control lights<br>
Numbers 1, 2, 3, 4, 5, 6, 7, 8, 9, 0 - Enable different floor showcases<br>
P - play animations</p>
<h3 tabindex="-1" dir="auto">Voxel</h3>
<p dir="auto">Mouse2 - Spawn currently selected element<br>
Numbers 1, 2, 3, 4, 5, 6, 7, 8, 9, 0 - Select different elements<br>
Q and E - Control cursor distance<br>
R and T - Control cursor size</p>
<h3 tabindex="-1" dir="auto">Common for both</h3>
<p dir="auto">WASD - Move<br>
Capslock - Toggle mouse camera<br>
Page Up and Down - Change frame drawer<br>
Arrow keys - Change camera direction<br>
Shift - Slow down movement</p>
<h2 tabindex="-1" dir="auto">Rendering</h2>
<h3 tabindex="-1" dir="auto">Rasterization</h3>
<p dir="auto">The engine has a flexible rasterization pipeline that can be controlled by using different "Shaders", these shaders are similar in concept to GPU shaders, in the sense that they can modify the data that is passed on to the next stage</p>
<p dir="auto">The pipeline to render a mesh is as follows:<br>
1. The first step of the pipeline is calling the vertex shader for each triangle, giving it the triangle vertices and the mesh transformations, the shader then applies the transformations and projection, and decides whether the triangle should be culled (backface culling)<br>
2. Then the resulting triangle is clipped of any offscreen vertices, for this the triangles are clipped against different planes, 2 per each axis<br>
3. The resulting vertices from clipping are transformed to screen space and then sent to the rasterizer<br>
4. The rasterizer then calculates the coordinates that are inside the triangle and calls the fragment shader for each coordinate<br>
5. The fragment shader then decides which color to output on each coordinate, using solid colors, textures, and shading techniques</p>
<p dir="auto">The engine uses barycentric rasterization to determine which pixels are inside a triangle</p>
<h4 tabindex="-1" dir="auto">Rasterization Features</h4>
<h6 tabindex="-1" dir="auto">OBJ file loading</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/obj.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/obj.png" width="200" height="200"></a></p>
<h6 tabindex="-1" dir="auto">MD2 file loading</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/md2.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/md2.png" width="200" height="200"></a></p>
<h6 tabindex="-1" dir="auto">Directional lights</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/directionallight.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/directionallight.png" width="200" height="200"></a></p>
<h6 tabindex="-1" dir="auto">Point lights</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/pointlight.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/pointlight.gif" width="200" height="200" data-animated-image=""></a></p>
<h6 tabindex="-1" dir="auto">Spot lights</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/spotlight.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/spotlight.png" width="200" height="200"></a></p>
<h6 tabindex="-1" dir="auto">Textures</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/textures.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/textures.gif" width="200" height="200" data-animated-image=""></a></p>
<h6 tabindex="-1" dir="auto">Shadow maps</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/shadows.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/shadows.png" width="200" height="200"></a></p>
<h6 tabindex="-1" dir="auto">Shadows from multiple sources</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/multishadows.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/multishadows.png" width="200" height="200"></a></p>
<h6 tabindex="-1" dir="auto">MD2 Animations</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/animation1.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/animation1.gif" width="200" height="200" data-animated-image=""></a>  <a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/animation2.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/animation2.gif" width="200" height="200" data-animated-image=""></a></p>
<h6 tabindex="-1" dir="auto">Ico-sphere generation</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/sphere.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/sphere.gif" width="200" height="200" data-animated-image=""></a></p>
<h6 tabindex="-1" dir="auto">Normal maps</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/normal_map.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/normal_map.png" width="200" height="200"></a>  <a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/normal_map.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/normal_map.gif" width="200" height="200" data-animated-image=""></a></p>
<h6 tabindex="-1" dir="auto">Specular highlights</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/specular.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/specular.gif" width="200" height="200" data-animated-image=""></a></p>
<h6 tabindex="-1" dir="auto">Colored lighting</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/colored_lights.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/colored_lights.png" width="200" height="200"></a></p>
<h3 tabindex="-1" dir="auto">Ray Marching</h3>
<p dir="auto">The engine also has a different rendering technique based on ray marching instead of rasterization. For this a regular 3D grid is defined, where each cell represents a voxel, and can either be filled with a specific particle type, or empty (Air)</p>
<p dir="auto">When rendering a frame, rays are marched from the camera origin towards the looking direction, and are stopped in case they hit one of the non-air voxels</p>
<p dir="auto">For this ray marching experiment, a simple physics simulation was also implemented, where different elements can be spawned and played around with, the currently supported elements are:<br>
- Sand<br>
- Ice<br>
- Water<br>
- Steam<br>
- Stone<br>
- Lava<br>
- Steel</p>
<h4 tabindex="-1" dir="auto">Ray Marching Features</h4>
<h5 tabindex="-1" dir="auto">Voxel Shading</h5>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/voxel_shading.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/voxel_shading.png" width="200" height="200"></a></p>
<h5 tabindex="-1" dir="auto">Voxel Shadows</h5>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/voxel_shadows.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/voxel_shadows.png" width="200" height="200"></a></p>
<h5 tabindex="-1" dir="auto">Rasterization &amp; Ray Marching on the same scene</h5>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/voxel_raster.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/voxel_raster.gif" width="200" height="200" data-animated-image=""></a></p>
<h5 tabindex="-1" dir="auto">Physics Simulation</h5>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/sand.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/sand.gif" width="200" height="200" data-animated-image=""></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/water.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/water.gif" width="200" height="200" data-animated-image=""></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/lava.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/lava.gif" width="200" height="200" data-animated-image=""></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/lava%20+%20water%20floor.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/lava%20+%20water%20floor.gif" width="200" height="200" data-animated-image=""></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/tankers%20draining.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/tankers%20draining.gif" width="200" height="200" data-animated-image=""></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/frozen%20tanker.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/frozen%20tanker.gif" width="200" height="200" data-animated-image=""></a></p>
<h2 tabindex="-1" dir="auto">Frame Drawers</h2>
<p dir="auto">After rendering a full frame, in order to actually draw to the console output the engine has a flexible system where different "frame drawers" can be used<br>
These are the components in charge of making a specific RGB color pixel be represented in the console</p>
<p dir="auto">For this, different techniques are employed for different effects/quality, some techniques allow more colors, some allow more performance</p>
<h5 tabindex="-1" dir="auto">Greyscale Frame Drawer</h5>
<p dir="auto">Overrides the palette with 16 shades from black to white<br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/greyscale.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/greyscale.png" width="200" height="200"></a></p>
<h6 tabindex="-1" dir="auto">Dithered Greyscale Frame Drawer</h6>
<p dir="auto">Similar to the previous one, but also takes advantage of the dithering block characters (░▒▓) to dither different combinations of the 16 shades, expands the original 16 to 80 shades<br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/ditheredgreyscale.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/ditheredgreyscale.png" width="200" height="200"></a></p>
<h6 tabindex="-1" dir="auto">Dithered Frame Drawer</h6>
<p dir="auto">Uses the same mechanism from the previous Frame Drawer but with the default palette, giving more depth to the default colors (10 shades per color)<br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/dithered.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/dithered.png" width="200" height="200"></a></p>
<h6 tabindex="-1" dir="auto">VT Escape Sequence Frame Drawer</h6>
<p dir="auto">Uses escape sequences to set the colors of each pixel, allowing for full 32 bit real RGB colors, or indexed colors (256 color palette)<br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/vtescapesequence.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/vtescapesequence.png" width="200" height="200"></a></p>
<h6 tabindex="-1" dir="auto">Text Only Frame Drawer</h6>
<p dir="auto">Does not use any attribute change, thus the only color is white, the lightness of each pixel is controlled through the character in the cell<br>
The current characters used are: " ·;%░≡¥▒▓█"<br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/ascii.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/ascii.png" width="200" height="200"></a></p>
<h3 tabindex="-1" dir="auto">Shaders</h3>
<p dir="auto">Vertex and Fragment shaders can be created, they are basically classes that implement a vertex and fragment stage in the rasterization pipeline, and can pass data around via the class members</p>
<p dir="auto">A simple shader:</p>
<div dir="auto" data-snippet-clipboard-copy-content="bool PlainShader::VertexShader(Vertex&amp; v0, Vertex&amp; v1, Vertex&amp; v2, const MVPTransform&amp; mvp_mats)
{
	TransformVertexMVP(v0, mvp_mats);
	TransformVertexMVP(v1, mvp_mats);
	TransformVertexMVP(v2, mvp_mats);

	vert_v0_texture_coord = v0.GetTextureCoords();
	vert_v1_texture_coord = v1.GetTextureCoords();
	vert_v2_texture_coord = v2.GetTextureCoords();

	return !IsBackface(v0.GetPosition(), v1.GetPosition(), v2.GetPosition());
}

RGBColor PlainShader::FragmentShader(RGBColor color, const Triangle&amp; triangle, float barcoord0, float barcoord1, float barcoord2)
{
	Vector2 frag_texture_coord = PerspectiveCorrectInterpolate<Vector2>(vert_v0_texture_coord,
									    vert_v1_texture_coord,
									    vert_v2_texture_coord,
									    triangle,
									    barcoord0,
									    barcoord1,
									    barcoord2);

	RGBColor final_color = texture->GetColorFromTextureCoords(frag_texture_coord.x, frag_texture_coord.y);
	final_color.BlendMultiply(color);

	return final_color;
}"><pre><span>bool</span> <span>PlainShader::VertexShader</span>(Vertex&amp; v0, Vertex&amp; v1, Vertex&amp; v2, <span>const</span> MVPTransform&amp; mvp_mats)
{
	<span>TransformVertexMVP</span>(v0, mvp_mats);
	<span>TransformVertexMVP</span>(v1, mvp_mats);
	<span>TransformVertexMVP</span>(v2, mvp_mats);

	vert_v0_texture_coord = v0.<span>GetTextureCoords</span>();
	vert_v1_texture_coord = v1.<span>GetTextureCoords</span>();
	vert_v2_texture_coord = v2.<span>GetTextureCoords</span>();

	<span>return</span> !<span>IsBackface</span>(v0.<span>GetPosition</span>(), v1.<span>GetPosition</span>(), v2.<span>GetPosition</span>());
}

RGBColor <span>PlainShader::FragmentShader</span>(RGBColor color, <span>const</span> Triangle&amp; triangle, <span>float</span> barcoord0, <span>float</span> barcoord1, <span>float</span> barcoord2)
{
	Vector2 frag_texture_coord = PerspectiveCorrectInterpolate&lt;Vector2&gt;(vert_v0_texture_coord,
									    vert_v1_texture_coord,
									    vert_v2_texture_coord,
									    triangle,
									    barcoord0,
									    barcoord1,
									    barcoord2);

	RGBColor final_color = texture-&gt;<span>GetColorFromTextureCoords</span>(frag_texture_coord.<span>x</span>, frag_texture_coord.<span>y</span>);
	final_color.<span>BlendMultiply</span>(color);

	<span>return</span> final_color;
}</pre></div>
<h3 tabindex="-1" dir="auto">Planned Features</h3>
<ul dir="auto">
<li>Faster vertex transformations with SIMD</li>
<li>Faster rasterizer with multipixel filling</li>
<li>Faster rasterizer with binning</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DVD's New Cousin Can Store More Than a Petabit (129 pts)]]></title>
            <link>https://spectrum.ieee.org/data-storage-petabit-optical-disc</link>
            <guid>39488375</guid>
            <pubDate>Sat, 24 Feb 2024 01:44:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/data-storage-petabit-optical-disc">https://spectrum.ieee.org/data-storage-petabit-optical-disc</a>, See on <a href="https://news.ycombinator.com/item?id=39488375">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="DVD’s New Cousin Can Store More Than a Petabit" data-elid="2667335462" data-post-url="https://spectrum.ieee.org/data-storage-petabit-optical-disc" data-authors="Charles Q. Choi" data-page-title="DVD’s New Cousin Can Store More Than a Petabit - IEEE Spectrum"><p>A novel disc the size of a DVD can hold more than 1 million gigabits—roughly as much as is <a href="https://www.washingtonpost.com/technology/2022/10/27/laser-powered-chip-internet-data-transfer/" target="_blank">transmitted per second over the entire world’s Internet</a>—by storing data in three dimensions as opposed to two, a new study finds.<br></p><p>Optical discs such as CDs and <a href="https://spectrum.ieee.org/fivedimensional-dvd-could-store-16-terabytes" target="_self">DVDs</a> encode data using a series of microscopic pits. These pits, and the islands between them, together represent the 0s and 1s of binary code that computers use to symbolize information. CD, DVD, and Blu-ray players use lasers to read the data encoded in these discs.</p><p>“The use of ultrahigh-density optical data storage technology in big data centers is now possible.” <strong>—Min Gu, University of Shanghai for Science and Technology</strong></p><p>Although optical discs are low in cost and highly durable, they are limited by the amount of data they can hold, which is usually stored in a single layer. Previously, scientists investigated encoding data on optical discs in many layers in <a href="https://spectrum.ieee.org/racetrack-memory" target="_self">three dimensions</a> to boost their capacity. However, a key barrier that prior research faced was how the optics used to read and write this data were limited to roughly the size of the wavelengths of light they used.</p><p>Now scientists in China have developed a way to encode data on 100 layers in optical discs. In addition, the data is recorded using spots as small as 54 nanometers wide, roughly a tenth of the size of the wavelengths of visible light used to read and write the data.</p><p>All in all, a DVD-size version of the new disc has a capacity of up to 1.6 <a href="https://spectrum.ieee.org/frequency-comb" target="_self">petabits</a>—that is, 1.6 million gigabits. This is some 4,000 times as much data density as a <a href="https://spectrum.ieee.org/the-consumer-electronics-hall-of-fame-samsung-bdp1000" target="_self">Blu-ray disc</a> and 24 times as much as the currently most advanced hard disks. The researchers suggest their new optical disc can enable a data center capable of exabit storage—a billion gigabits—to fit inside a room instead of a stadium-size space.</p><p>“The use of ultrahigh-density optical data storage technology in big data centers is now possible,” says <a href="https://en.wikipedia.org/wiki/Min_Gu" target="_blank">Min Gu</a>, professor of <a href="https://www.atse.org.au/news-and-events/article/min-gu-appointed-executive-chancellor-in-shanghai/" target="_blank">optical-electrical and computer engineering</a> at the <a href="https://en.wikipedia.org/wiki/University_of_Shanghai_for_Science_and_Technology" target="_blank">University of Shanghai for Science and Technology</a>.</p><h3>How to store a petabit on one disc</h3><p>The strategy the researchers used to write the data relies on a pair of lasers. The first, a green 515-nanometer laser, triggers spot formation, whereas the second, a red 639-nm laser, switches off the writing process. By controlling the time between firing of the lasers, the scientists could produce spots smaller than the wavelengths of light used to create them.</p><p>The procedure used to create blank discs is compatible with conventional DVD mass production and can be completed within 6 minutes.</p><p>To read the data, the researchers again depended on a pair of lasers. The first, a blue 480-nm beam, can make spots fluoresce, while the second, an orange 592-nm light, switches off the fluorescence process. Precise control over the firing of these lasers can single out which specific nanometer-scale spot ends up fluorescing.</p><p>This new strategy depends on a novel light-sensitive material called AIE-DDPR that is capable of all these varied responses to different wavelengths of light. “It has been a 10-year effort searching for this kind of material,” Gu says. “The difficulty has been how the writing and reading processes affect each other in a given material—in particular, in a three-dimensional geometry.”</p><p>The scientists encoded data on layers each separated by 1 micrometer. They found that the writing quality stayed comparable across all the layers. “Personally, I was surprised that nanoscale writing-recoding and reading processes both work well in our newly invented material,” Gu says.</p><p>The researchers note that the entire procedure used to create blank discs made using AIE-DDPR films is compatible with conventional DVD mass production and can be completed within 6 minutes. Gu says these new discs may therefore prove to be manufacturable at commercial scales.</p><p>Currently, he says, the new discs have a writing speed of about 100 milliseconds and an energy consumption of microjoules to millijoules. </p><p>Still, Gu says, the researchers would like to see their new discs used in big data centers. As a result, they’re working to improve their new method’s writing speed and energy consumption. He suggests this may be possible using new, more energy-efficient recording materials. He says more layers in each disc may be possible in the future, using better lenses and fewer aberrations in their optics.</p><p>The scientists detailed <u><a href="https://www.nature.com/articles/s41586-023-06980-y" target="_blank">their findings</a></u> online 21 February in the journal <em>Nature</em>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RTO doesn't improve company value, but does make employees miserable: Study (110 pts)]]></title>
            <link>https://arstechnica.com/science/2024/02/rto-doesnt-improve-company-value-but-does-make-employees-miserable-study/</link>
            <guid>39487622</guid>
            <pubDate>Fri, 23 Feb 2024 23:48:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/science/2024/02/rto-doesnt-improve-company-value-but-does-make-employees-miserable-study/">https://arstechnica.com/science/2024/02/rto-doesnt-improve-company-value-but-does-make-employees-miserable-study/</a>, See on <a href="https://news.ycombinator.com/item?id=39487622">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      WFH FTW    —
</h4>
            
            <h2 itemprop="description">Data is consistent with bosses using RTO to reassert control and scapegoat workers.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/02/GettyImages-182629853-800x534.jpeg" alt="Empty cubicles">
      <figcaption><div><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2024/02/GettyImages-182629853-scaled.jpeg" data-height="1708" data-width="2560">Enlarge</a> <span>/</span> Empty cubicles</p></div></figcaption>  </figure>

  




<!-- cache hit 286:single/related:18c9375f1c776a4d2db6a313a7a14292 --><!-- empty -->
<p>For some, having to work from home during the COVID-19 pandemic was stressful. Parents balanced job duties while caring for children. Some struggled to set up a home office and adjust to new tools, like video conferencing. Lonely workdays at home added to social isolation. The line between work and life blurred.</p>
<p>For others, working from home was a boon—comfort, convenience, flexibility, no commuting or rush-hour traffic, no office-environment distractions. When the acute aspects of the pandemic receded, some who at first struggled began to settle into a work-from-home (WFH) groove and appreciated the newfound flexibility.</p>
<p>Then, bosses began calling their employees back to the office. Many made the argument that the return-to-office (RTO) policies and mandates were better for their companies; workers are more productive at the office, and face-to-face interactions promote collaboration, many suggested. But there's little data to support that argument. Pandemic-era productivity is tricky to interpret, given that the crisis disrupted every aspect of life. Research from before the pandemic generally suggested remote work <em>improves</em> worker performance—though it often included workers who volunteered to WFH, potentially biasing the finding.</p>
<p>For a clearer look at the effect of RTO policies after the pandemic, two business researchers at the University of Pittsburgh examined a sample of firms on the S&amp;P 500 list—137 of which had RTO mandates and 320 that clearly did not between June 2019 and January 2023. The researchers collected publicly available data on each company, including financial data and employee reviews. They then looked at what factors were linked to whether a firm implemented an RTO policy—such as the company's size, financial constraints, and CEO characteristics—as well as the consequences of the RTO mandates—employee satisfaction and financial metrics of the firms.</p>                                            
                                                        
<p>Overall, the analysis, <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4675401#maincontent">released as a pre-print</a>, found that RTO mandates did not improve a firm's financial metrics, but they did decrease employee satisfaction.</p>
<p>Drilling down, the data indicated that RTO mandates were linked to firms with male CEOs who had greater power in the company. Here, power is measured as the CEO’s total compensation divided by the average total compensation paid to the four highest-paid executives in the firm.</p>
<p>Before the analysis, the researchers hypothesized that RTO mandates may be used to blame employees for poor firm performance. But, companies that have institutional ownership—such as hedge funds or endowments—would not fall for such a "blame game" and would thus would be less likely to implement an RTO mandate. The data supported those hypotheses. Firms with weaker stock performance before employees were able to return to the office were more likely to enforce RTO mandates. However, institutional ownership decreased the probability of RTO mandates.</p>
<p>Although CEOs often justified RTO mandates by arguing it will improve the company's performance, "Results of our determinant analyses are consistent with managers using RTO mandates to reassert control over employees and blame employees as a scapegoat for bad firm performance," the researchers concluded.</p>
<p>Unsurprisingly, the researchers also found that RTO mandates were linked to decreases in employee satisfaction. Specifically, after an RTO mandate, employees' ratings significantly declined on overall job satisfaction, work-life balance, senior management, and corporate culture. But their ratings of factors unrelated to RTO did not change, indicating that the RTO mandate was driving dissatisfaction.</p>
<p>The study has limitations, including a short time frame to look at long-term outcomes of RTO policies and a time frame that overlapped with a labor shortage. Worker responses may be different in a tight labor market. Still, the study adds some data to the ongoing debate—and feuds—over RTO policies.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A former Gizmodo writer changed name to 'Slackbot', stayed undetected for months (323 pts)]]></title>
            <link>https://www.theverge.com/2024/2/23/24081249/slack-slackbot-gizmodo-tom-mckay</link>
            <guid>39487341</guid>
            <pubDate>Fri, 23 Feb 2024 23:10:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/2/23/24081249/slack-slackbot-gizmodo-tom-mckay">https://www.theverge.com/2024/2/23/24081249/slack-slackbot-gizmodo-tom-mckay</a>, See on <a href="https://news.ycombinator.com/item?id=39487341">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Hiding on Slack isn’t all that hard, apparently; you just have to pretend you’re a bot. That’s what <em>IT Brew</em>’s Tom McKay did when he left <em>Gizmodo</em> in 2022, and he went undetected by the site’s management for months.</p><p><a href="https://twitter.com/thetomzone/status/1760833981904228508?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1760833981904228508%7Ctwgr%5E%7Ctwcon%5Es1_&amp;ref_url=about%3Asrcdoc">In a post on X</a>, McKay shared some screenshots of the new “Slackbot” persona he took on after he officially left <em>Gizmodo</em>. He also confirmed to <em>The Verge</em> that this silly prank really happened.</p><p>If you’re not glued to Slack for most of the day like I am, then you might not know that Slackbot is the friendly robot that lives in the messaging service. It helps you do things like set reminders, find out your office’s Wi-Fi password, or let you know when you’ve been mentioned in a channel that you’re not a part of.</p><p>When it was his time to leave, McKay swapped out his existing profile picture for one that resembled an angrier version of <a href="https://slack.com/resources/using-slack/a-guide-to-slackbot-custom-responses">Slackbot’s actual icon</a>. He also changed his name to “Slackbot.” You can’t just change your name on Slack to “Slackbot,” by the way, as the service will tell you that name’s already been taken. It <em>does</em> work if you use a special character that resembles one of the letters inside Slackbot, though, <a href="https://x.com/fromdanielwei15/status/1760860498780622947?s=20">such as replacing</a> “o” with the Unicode character “о.”</p><p>The move camouflaged McKay’s active Slack account for months, letting his account evade deletion. It also allowed him to send bot-like messages to his colleagues such as, “Slackbot fact of the day: Hi, I’m Slackbot! That’s a fact. Have a Slack-ly day!” My colleague Victoria Song, who previously worked at <em>Gizmodo</em>, isn’t all that surprised that this situation unfolded, and says, “As Tom’s former coworker and a <a href="https://www.theverge.com/2023/7/8/23788162/gizmodo-g-o-media-ai-generated-articles-star-wars">G/O Media survivor</a>, this tracks.” </p><p>Of course, not <em>every</em> company will fall for this trick, as some have security measures in place to prevent this kind of thing. But perhaps <em>Gizmodo</em>’s management thought that McKay’s account had already been deleted. Or maybe they just weren’t eagle-eyed enough to spot a duplicate Slackbot with a suspicious pair of brows.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Generative Models: What do they know? Do they know things? Let's find out (323 pts)]]></title>
            <link>https://intrinsic-lora.github.io/</link>
            <guid>39487124</guid>
            <pubDate>Fri, 23 Feb 2024 22:46:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://intrinsic-lora.github.io/">https://intrinsic-lora.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=39487124">Hacker News</a></p>
<div id="readability-page-1" class="page">

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<div>
          <h2>Generative Models: What do they know? <br> Do they know things? Let's find out!</h2>
          

          <p><span><sup>1</sup>Toyota Technological Institute at Chicago,</span>
            <span><sup>2</sup>Adobe</span>
          </p>

          
        </div>

<div>
      <p><img id="teaser" src="https://intrinsic-lora.github.io/static/images/teaser.png" alt="Teaser"></p><h2>
        <span>INTRINSIC LoRA (I-LoRA)</span> uncovers the hidden capabilities of generative models like VQGAN, StyleGAN-XL, StyleGAN-v2, and Stable Diffusion. I-LoRA modulates key feature maps to extract intrinsic scene properties such as normals, depth, albedo, and shading, using the models' existing decoders without additional layers, revealing their deep understanding of scene intrinsics.
      </h2>
    </div>


<div>
        <h2>Abstract</h2>
        <p>
            Generative models have been shown to be capable of synthesizing highly detailed and realistic images. It is natural to suspect that they implicitly learn to model some image intrinsics such as surface normals, depth, or shadows. In this paper, we present compelling evidence that generative models indeed internally produce high-quality scene intrinsic maps. We introduce <span>INTRINSIC LoRA (I-LoRA)</span>, a universal, plug-and-play approach that transforms any generative model into a scene intrinsic predictor, capable of extracting intrinsic scene maps directly from the original generator network without needing additional decoders or fully fine-tuning the original network. Our method employs a Low-Rank Adaptation (LoRA) of key feature maps, with newly learned parameters that make up less than 0.6% of the total parameters in the generative model. Optimized with a small set of labeled images, our model-agnostic approach adapts to various generative architectures, including Diffusion models, GANs, and Autoregressive models.  We show that the scene intrinsic maps produced by our method compare well with, and in some cases surpass those generated by leading supervised techniques.
          </p>
      </div>





<div id="BibTeX">
    <h2>BibTeX</h2>
    <pre><code>@article{du2023generative,
  author    = {Du, Xiaodan and Kolkin, Nicholas and Shakhnarovich, Greg and Bhattad, Anand},
  title     = {Generative Models: What do they know? Do they know things? Let's find out!},
  journal   = {arXiv},
  year      = {2023},
}</code></pre>
  </div>






</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta's new LLM-based test generator is a sneak peek to the future of development (337 pts)]]></title>
            <link>https://read.engineerscodex.com/p/metas-new-llm-based-test-generator</link>
            <guid>39486717</guid>
            <pubDate>Fri, 23 Feb 2024 22:04:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://read.engineerscodex.com/p/metas-new-llm-based-test-generator">https://read.engineerscodex.com/p/metas-new-llm-based-test-generator</a>, See on <a href="https://news.ycombinator.com/item?id=39486717">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><em>Engineer’s Codex is a publication about real-world software engineering.</em></p><p><span>Meta recently released a </span><a href="https://arxiv.org/abs/2402.09171" rel="">paper called “Automated Unit Test Improvement using Large Language Models at Meta”</a><span>. It’s a good look at how Big Tech is using AI internally to make development faster and software less buggy. For example, </span><a href="https://read.engineerscodex.com/i/139414745/critique-googles-code-review-tool" rel="">Google is using AI to speed up code reviews</a><span>.</span></p><p><span>A major win of this paper is that while it integrates LLMs into a developer’s workflow, it also recommends fully-formed software improvements that are verified to be both correct and an improvement to current code coverage. Compare this to GitHub Copilot, where suggestions still have to be manually verified to work by the human - and we all know that </span><a href="https://read.engineerscodex.com/p/clever-code-is-probably-the-worst" rel="">debugging code is twice as hard as writing it</a><span>.</span></p><p>Meta claims that this “this is the first paper to report on LLM-generated code that has been developed independent of human intervention (other than final review sign off), and landed into large scale industrial production systems with guaranteed assurances for improvement over the existing code base.”</p><p>Furthermore, there are solid principles that developers can take away in order to use AI effectively themselves.</p><p><strong>Table of Contents (total read time: 7 minutes):</strong></p><ul><li><p>Key Points (1 minute read)</p></li><li><p>Stats (1 minute read)</p></li><li><p><span>Actionable Takeaways </span><strong>←</strong><span> </span><strong>if you’re short on time, just read this!</strong><span> (3 minute read)</span></p></li><li><p>How TestGen-LLM Works (2 minute read)</p></li></ul><p><em><a href="http://swequiz.com/" rel="">SWE Quiz</a><span> is for ambitious developers who want to make sure their software fundamentals are rock solid. Reveal gaps in your knowledge and make sure you really know what you’re doing both at work and while interviewing.</span></em></p><p><em><span>Take the tests for </span><a href="https://www.swequiz.com/learn/authentication-roadmap" rel="">authentication</a><span>, </span><a href="https://swequiz.com/learn/caching-roadmap" rel="">caching</a><span>, </span><a href="https://www.swequiz.com/learn/databases-roadmap" rel="">databases</a><span>, </span><a href="https://www.swequiz.com/learn/api-design-roadmap" rel="">API Design</a><span>, and </span><a href="https://www.swequiz.com/learn" rel="">more</a><span>.</span></em></p><p data-attrs="{&quot;url&quot;:&quot;https://swequiz.com&quot;,&quot;text&quot;:&quot;Check out SWE Quiz&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://swequiz.com/" rel=""><span>Check out SWE Quiz</span></a></p><p><span>TestGen-LLM uses an approach called ‘Assured LLM-based Software Engineering’ (Assured LLMSE), using private, internal LLMs that are probably fine-tuned with Meta’s codebase. This means that it uses LLMs to generate code improvements that are backed by </span><strong>verifiable guarantees of improvement and non-regression.</strong></p><p><span>TestGen-LLM uses an </span><strong>ensemble approach </strong><span>to generate code improvements. This means that it uses </span><strong>multiple LLMs, prompts, and hyper-parameters</strong><span> to generate a set of candidate improvements, and then selects the best one. This approach can help to improve the quality of the generated improvements.</span></p><p><span>TestGen-LLM is specifically designed to </span><strong>improve existing human-written tests</strong><span> rather than </span><strong>generate code from scratch</strong><span>.&nbsp;</span></p><p><span>TestGen-LLM has been </span><strong>integrated into Meta's software engineering workflows</strong><span>. This means that it can be used to automatically improve tests as part of the development process. It would be cool to see some screenshots of how exactly it’s integrated, but the paper doesn’t provide any.</span></p><p>These stats are either direct quotes or paraphrased quotes from the paper.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a8257b1-bce4-41e3-b88b-42e7d71bcc7b_533x465.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a8257b1-bce4-41e3-b88b-42e7d71bcc7b_533x465.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a8257b1-bce4-41e3-b88b-42e7d71bcc7b_533x465.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a8257b1-bce4-41e3-b88b-42e7d71bcc7b_533x465.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a8257b1-bce4-41e3-b88b-42e7d71bcc7b_533x465.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a8257b1-bce4-41e3-b88b-42e7d71bcc7b_533x465.png" width="471" height="410.90994371482174" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7a8257b1-bce4-41e3-b88b-42e7d71bcc7b_533x465.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:465,&quot;width&quot;:533,&quot;resizeWidth&quot;:471,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a8257b1-bce4-41e3-b88b-42e7d71bcc7b_533x465.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a8257b1-bce4-41e3-b88b-42e7d71bcc7b_533x465.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a8257b1-bce4-41e3-b88b-42e7d71bcc7b_533x465.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a8257b1-bce4-41e3-b88b-42e7d71bcc7b_533x465.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><ul><li><p><span>The image above shows that: in an evaluation on Reels and Stories products for Instagram, 75% of TestGen-LLM test cases that were generated built correctly, 57% passed reliably, and </span><em>25% increased coverage</em><span>.</span></p></li><li><p><span>TestGen-LLM was able to improve </span><strong>10% of all classes to which it was applied and 73% of its test improvements were accepted by developers</strong><span>, and landed into production.</span></p></li><li><p><span>In a “test-a-thon” between engineers, where various Meta engineers created tests in order to increase Instagram’s test coverage, “</span><strong>the median number of lines of code added by a TestGen-LLM test was 2.5</strong><span>.”</span></p></li><li><p><span>However, one test case “hit the jackpot” and covered 1,326 lines. This is a </span><em>really important stat, which I iterate upon below.</em></p></li><li><p>All improved cases generated during the “test-a-thon” did “cover at least one additional valid corner case, such as an early return and/or special processing for special values such as null and empty list.”</p></li></ul><p><strong>TestGen-LLM is a good example of how LLMs can be used to improve dev productivity and software reliability in a time-efficient manner.</strong><span> There are a few takeaways I got from reading this paper that give us a look both to how Big Tech is implementing LLMs internally and to how any developer or engineering manager reading this can use LLMs in a more productive manner. (Note: many of these are my own personal opinions that I’ve taken away from the paper.)</span></p><p>Small context windows and scattered dependencies make LLMs nearly unusable for non-boilerplate solutions in large codebases. Aside from any privacy concerns, it’s not feasible to paste in multiple files of code into an LLM when there could be 20+ dependencies from across a codebase in a C++ header file (as an example). Even if you do paste in multiple files, there is a time and cognitive cost to actually using and trying the code outputted by an LLM in a chat window or even in the code editor by GitHub Copilot.</p><p><span>The price of extra cognitive load cannot be understated. </span><a href="https://news.ycombinator.com/item?id=39460788" rel="">Hacker News commenters find the inaccuracies of GPT-based tooling exhausting and unreliable.</a><span> This is where the </span><strong>verification of outputs being both valid and non-regressive is extremely important.</strong></p><p><span>This means that for a long-term productivity boost in large codebases,</span><strong> improvements will probably come in incremental, specialized use cases</strong><span>, like test generation and </span><a href="https://blog.research.google/2023/05/resolving-code-review-comments-with-ml.html" rel="">automatic suggestions during code reviews</a><span>. These are also low risk ways to save cumulative developer time. Basically, “GPT wrappers” will continue to be useful 🙂.</span></p><p><strong>The real value of LLMs here are displayed through the edge cases</strong><span>. The paradox of writing good code is that </span><a href="https://web.mit.edu/nelsonr/www/Repenning=Sterman_CMR_su01_.pdf" rel="">nobody ever gets credit for fixing problems that never happened</a><span>.</span></p><p><a href="https://antithesis.com/blog/is_something_bugging_you/" rel="">Will Wilson writes</a><span>:</span></p><blockquote><p><em>“The fundamental problem of software testing… is that software has to handle many situations that the developer has never thought of or will never anticipate. This limits the value of testing, because if you had the foresight to write a test for a particular case, then you probably had the foresight to make the code handle that case too. This makes conventional testing great for catching regressions, but really terrible at catching all the “unknown unknowns” that life, the universe, and your endlessly creative users will throw at you.”</em></p></blockquote><p><span>Most of the test cases created by Meta’s TestGen-LLM only covered an extra 2.5 lines. However, one test case covered </span><em>1326 lines</em><span>! The value of that one test case is exponentially more valuable than most of the previous test cases and exponentially improves the value of TestGen-LLM. LLMs can vigorously “think outside the box” and the value of catching unexpected edge cases is very high here. </span></p><p><span>In fact, it’s so high that the creator of FoundationDB’s startup, </span><a href="https://antithesis.com/" rel="">Antithesis</a><span>, is entirely based on the fact that software testing edge cases are best found by AI. </span><a href="https://read.engineerscodex.com/p/how-apple-built-icloud-to-store-billions" rel="">For reference, FoundationDB was acquired by Apple and is the basis for Apple iCloud’s billions of databases.</a></p><p><span>Base model LLMs aren’t “plug-n-play" and shouldn’t reasonably ever be expected to. Sure, they might output pristine React and Tailwind CSS code, but that’s a narrow use case in most production codebases. They need a fair amount of processing and filtering for code generation tasks that require correctness. Part of this processing means grounding LLMs with examples. Google and Meta both make </span><strong>suggestions based on existing code</strong><span>, where the results are much, much better than raw generation.</span><strong> </strong><span>LLMs used in production should take ideas from how Meta processes and filters LLM outputs, and most outputs should be expected to be discarded.</span></p><p>LLMs do work best integrated into workflows. This is a reason why GitHub Copilot is so popular and another reason why Google’s Workspace integrations are a great idea. Asking a chatbot works great for certain use cases, like debugging and boilerplate code generation, but chatbots can often fail at more complex use cases.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F105bb1eb-0ad0-4555-8721-58714b76bcdf_1600x416.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F105bb1eb-0ad0-4555-8721-58714b76bcdf_1600x416.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F105bb1eb-0ad0-4555-8721-58714b76bcdf_1600x416.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F105bb1eb-0ad0-4555-8721-58714b76bcdf_1600x416.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F105bb1eb-0ad0-4555-8721-58714b76bcdf_1600x416.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F105bb1eb-0ad0-4555-8721-58714b76bcdf_1600x416.png" width="1456" height="379" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/105bb1eb-0ad0-4555-8721-58714b76bcdf_1600x416.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:379,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F105bb1eb-0ad0-4555-8721-58714b76bcdf_1600x416.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F105bb1eb-0ad0-4555-8721-58714b76bcdf_1600x416.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F105bb1eb-0ad0-4555-8721-58714b76bcdf_1600x416.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F105bb1eb-0ad0-4555-8721-58714b76bcdf_1600x416.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>TestGen-LLM applies a series of semantic filters to candidate solutions generated by Meta’s internal LLMs, making sure that only the most valuable tests are preserved. Here’s how it works:</p><p><strong>Filter 1: Buildability:</strong><span> Initially, TestGen-LLM checks if the generated code can be built within the app's existing infrastructure. Any code that fails to build is immediately discarded.</span></p><p><strong>Filter 2: Execution (does the test pass?): </strong><span>Next, the system runs the tests that passed the buildability filter. Any test that doesn't pass is discarded. This step is crucial because, without a way to automatically determine the validity of a failing test (whether it's due to a bug or an incorrect assertion), TestGen-LLM opts to keep only those tests that can be used for regression testing (aka making sure they can protect current code against future regressions).&nbsp;</span></p><p><strong>Filter 3: Flakiness:</strong><span> To address the issue of </span><a href="https://www.swequiz.com/learn/what-are-flaky-tests" rel="">flakiness</a><span> (tests that pass or fail inconsistently under the same conditions), TestGen-LLM employs repeated execution. A test must pass consistently across multiple (five) executions to be considered non-flaky.</span></p><p><strong>Filter 3: Coverage Improvement:</strong><span> Finally, to ensure that new tests actually add value, TestGen-LLM evaluates them for their contribution to test coverage. Tests that do not enhance coverage by exploring new code paths or conditions are discarded. Only tests that provide new insights or protect against regressions are kept.</span></p><p>These processing filters are pretty important as they guarantee improvements to a test suite. It also shows that LLMs are very far from being “plug-and-play.” </p><p>The tests that successfully pass through all these filters are guaranteed to enhance the existing test suite, offering reliable regression testing capabilities without duplicating effort or wasting resources. Pre- and post-processing steps in TestGen-LLM facilitate the extraction and reconstruction of test classes, streamlining the integration of new tests into the software development workflow.</p><p>This paper is a good formalization of an use case that many devs probably already use LLMs like ChatGPT, Gemini, and Mistral/LLaMA for. Keeping it in writing is a good way of tracking the progress of future improvements on LLMs in the software reliability space. Unit tests are probably the lowest, most basic level of code generation where LLMs have the most immediate value, but as time goes on, we’ll definitely see LLMs be able to catch and test for bugs in increasingly complex software systems.</p><p>The question is - will that make software easier to develop in the long run or will it lead to a proliferation of software complexity in the future?</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Tell HN: Equifax free credit report dark patterns (213 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=39485259</link>
            <guid>39485259</guid>
            <pubDate>Fri, 23 Feb 2024 19:49:03 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=39485259">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="39485259">
      <td><span></span></td>      <td><center><a id="up_39485259" href="https://news.ycombinator.com/vote?id=39485259&amp;how=up&amp;goto=item%3Fid%3D39485259"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=39485259">Tell HN: Equifax free credit report dark patterns</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_39485259">109 points</span> by <a href="https://news.ycombinator.com/user?id=PopAlongKid">PopAlongKid</a> <span title="2024-02-23T19:49:03"><a href="https://news.ycombinator.com/item?id=39485259">4 hours ago</a></span> <span id="unv_39485259"></span> | <a href="https://news.ycombinator.com/hide?id=39485259&amp;goto=item%3Fid%3D39485259">hide</a> | <a href="https://hn.algolia.com/?query=Tell%20HN%3A%20Equifax%20free%20credit%20report%20dark%20patterns&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=39485259&amp;auth=4b33295ed7f38a66a86aca6733f3abb5666d8dbd">favorite</a> | <a href="https://news.ycombinator.com/item?id=39485259">41&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><div><p>For years, I have been obtaining free annual credit reports (annualcreditreport.com) which must be provided by law. Recently, for the first time, when I tried to obtain my Equifax report, I was prompted for an email address and a mobile phone number, a new requirement that apparently cannot be bypassed.  The other two bureaus, and Equifax previously, confirmed identity by asking knowledge based questions.  They do not need my phone or email for anything.</p><p>Next, trying to obtain the report by phone instead of web site per instructions, I got to the point where I entered my ZIP code on the phone keypad, the voice menu system correctly repeated the number back to me, but every time I press 1 to indicate it is correct, the system acts like it got an invalid response and only gives me the option to enter further information by voice, not by the phone keypad.  Just as with my phone number and email, they do not need to record my voice to provide my report.</p><p>I wrote a complaint to the annualcreditreport firm earlier this week, no response yet.</p></div></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tauri 2.0 tries to make mobile apps crossplatform (129 pts)]]></title>
            <link>https://beta.tauri.app/guides/</link>
            <guid>39485098</guid>
            <pubDate>Fri, 23 Feb 2024 19:33:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://beta.tauri.app/guides/">https://beta.tauri.app/guides/</a>, See on <a href="https://news.ycombinator.com/item?id=39485098">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <div> 
<p>Tauri is a framework for building tiny, fast binaries for all major desktop and mobile platforms. Developers can integrate any frontend framework that compiles to HTML, JavaScript, and CSS for building their user experience while leveraging languages such as Rust, Swift, and Kotlin for backend logic when needed.</p>
<p>Get started building with <a href="https://github.com/tauri-apps/create-tauri-app"><code dir="auto">create-tauri-app</code></a> by using one of the below commands. Be sure to follow the <a href="https://beta.tauri.app/guides/prerequisites/">prerequisites guide</a> to install all of the dependencies required by Tauri and then view the <a href="https://beta.tauri.app/guides/frontend/">Frontend Configuration guides</a> for recommended frontend configurations.</p>

<starlight-tabs> <div> <ul role="tablist"> <li role="presentation"> <a role="tab" href="#tab-panel-245" id="tab-245" aria-selected="true" tabindex="0"> Bash </a> </li><li role="presentation"> <a role="tab" href="#tab-panel-246" id="tab-246" tabindex="-1"> PowerShell </a> </li><li role="presentation"> <a role="tab" href="#tab-panel-247" id="tab-247" tabindex="-1"> npm </a> </li><li role="presentation"> <a role="tab" href="#tab-panel-248" id="tab-248" tabindex="-1"> Yarn </a> </li><li role="presentation"> <a role="tab" href="#tab-panel-249" id="tab-249" tabindex="-1"> pnpm </a> </li><li role="presentation"> <a role="tab" href="#tab-panel-250" id="tab-250" tabindex="-1"> Cargo </a> </li> </ul> </div> <div id="tab-panel-245" aria-labelledby="tab-245" role="tabpanel"><figure><pre tabindex="0" dir="ltr"><code><p><span>sh</span><span> </span><span>&lt;(</span><span><span>curl</span><span> https://create.tauri.app/sh</span></span><span>)</span><span> </span><span>--alpha</span></p></code></pre></figure></div> </starlight-tabs>  
<p>After you’ve created your first app you can explore the different features and recipes of Tauri in the <a href="https://beta.tauri.app/features/">List of Features &amp; Recipes</a>.</p>
<h2 id="why-tauri"></h2>
<p>Tauri has 3 main advantages for developers to build upon:</p>
<ul>
<li>Secure foundation for building apps</li>
<li>Smaller bundle size by using the system’s native webview</li>
<li>Flexibility for developers to use any frontend and bindings for multiple languages</li>
</ul>
<p>Learn more about the Tauri philosophy in the <a href="https://beta.tauri.app/blog/tauri-1-0">Tauri 1.0 blog post</a>.</p>
<h3 id="secure-foundation"></h3>
<p>By being built on Rust, Tauri is able to take advantage of the memory, thread, and type-safety offered by Rust. Apps built on Tauri can automatically get those benefits even without needing to be developed by Rust experts.</p>
<p>Tauri also undergoes a security audit for major and minor releases. This not only covers code in the Tauri organization, but also for upstream dependencies that Tauri relies on. Of course this doesn’t mitigate all risks, but it provides a solid foundation for developers to build on top of.</p>
<p>Read the <a href="https://github.com/tauri-apps/tauri/security/policy">Tauri security policy</a> and the <a href="https://github.com/tauri-apps/tauri/blob/dev/audits/Radically_Open_Security-v1-report.pdf">Tauri 1.0 audit report</a>.</p>
<h3 id="smaller-app-size"></h3>
<p>Tauri apps take advantage of the web view already available on every user’s system. A Tauri app only contains the code and assets specific for that app and doesn’t need to bundle a browser engine with every app. This means that a minimal Tauri app can be less than 600KB in size.</p>
<p>Learn more about creating optimized apps in the <a href="https://beta.tauri.app/concepts/size">App Size concept</a>.</p>
<h3 id="flexible-architecture"></h3>
<p>Since Tauri uses web technologies that means that virtually any frontend framework is compatible with Tauri. The <a href="https://beta.tauri.app/guides/frontend/">Frontend Configuration guide</a> contains common configurations for popular frontend frameworks and the <a href="https://beta.tauri.app/concepts/rendering">Rendering concept</a> discusses which rendering techniques work best with Tauri (such as SPAs and SSGs).</p>
<p>Bindings between JavaScript and Rust are available to developers using the <code dir="auto">invoke</code> function in JavaScript and Swift and Kotlin bindings are available for <a href="https://beta.tauri.app/guides/plugins/">Tauri Plugins</a>.</p>
<p><a href="https://github.com/tauri-apps/tao">TAO</a> is responsible for Tauri window creation and <a href="https://github.com/tauri-apps/wry">WRY</a> is responsible for web view rendering. These are libraries maintained by Tauri and can be consumed directly if deeper system integration is required outside of what Tauri exposes.</p>
<p>In addition, Tauri maintains a number of plugins to extend what core Tauri exposes. You can find those plugins alongside those provided by the community in the <a href="https://beta.tauri.app/features">Features and Recipes section</a>.</p>  </div>    <hr> <p>© 2024 Tauri Contributors. CC-BY / MIT</p>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scuttlebutt social network: a decentralised platform (212 pts)]]></title>
            <link>https://scuttlebutt.nz/</link>
            <guid>39484907</guid>
            <pubDate>Fri, 23 Feb 2024 19:14:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://scuttlebutt.nz/">https://scuttlebutt.nz/</a>, See on <a href="https://news.ycombinator.com/item?id=39484907">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <section>
      
      <p>
        
        <h2>social network</h2>
        <h3>a decentralised platform</h3>
      </p>

      <div>
          <p><img src="https://scuttlebutt.nz/images/app-icons/hermiepatchwork.svg" alt="double hermie!!">
          </p>
          <div>
            <p><a href="https://scuttlebutt.nz/get-started">Get Started</a>
          </p></div>
          <p><img src="https://scuttlebutt.nz/images/app-icons/hermiepatchbay.svg" alt="hermie!">
          </p>
      </div>
    </section>



    <section>
      <h2><mark>A Scuttlebutt </mark></h2>
      <h2><mark>Love Story</mark></h2>
      
    </section>

    <section id="proof-we-legit">
      <header>
        <h2>The Decentralised Web</h2>
        <h3>It's time to build our own Internet</h3>
        <p><a href="https://scuttlebutt.nz/docs/talks/">We're talking about it and gossiping on it</a></p>
      </header>

      <div>
        <figure>
            
            <a href="https://letstalkbitcoin.com/blog/post/epicenter-dominic-tarr-secure-scuttlebutt-the-localized-but-distributed-social-network">
            <p>The Localized but Distributed Social Network - 2019</p>
            </a>
        </figure>
        <figure>
            
            <a href="https://youtu.be/JSWWkzsHhjk" target="_blank">
            <p>Scuttlebutt and a Decentralized Future - 2018</p>
          </a>
        </figure>
        <figure>
          
          <a href="https://www.youtube.com/watch?v=8GE5C9-RUpg" target="_blank">
            <p>Reinvent the Social Web <br> André Staltz @ FullStackFest - 2018</p>
          </a>
        </figure>
      </div>
      <div>
        <p><a href="https://scuttlebutt.nz/docs/talks/">see more videos</a></p>
      </div>
    </section>

    <section id="quote">
      <span>❝</span>
      <h2><mark>
        Scuttlebutt can be transformative for society, decentralizing and enabling
        local community development free of big corp. It is a fast growing
        decentralized social network. As an alternative to the large corporate social
        networks it enables autonomy for the users and a free zone from big data
        harvesting...</mark>
      </h2>
       <span>❞</span>
    </section>

    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Little Fixes – a spatial forum to improve your city (106 pts)]]></title>
            <link>https://littlefixes.xyz/</link>
            <guid>39484685</guid>
            <pubDate>Fri, 23 Feb 2024 18:54:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://littlefixes.xyz/">https://littlefixes.xyz/</a>, See on <a href="https://news.ycombinator.com/item?id=39484685">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><section data-svelte-h="svelte-tuvg9x"> <p>Our cities and towns are each a collaborative project between all their residents.
        The voices of the everyday people that wait to cross the street, honk their horns,
        drink coffee on benches, and carry in their groceries are critical in every 
        community's mission to build a successful city.</p> <p>This project's goal is to help those voices be heard.</p></section> <section>  <h2 data-svelte-h="svelte-7m84ju">Find your city</h2> <ul><a href="https://littlefixes.xyz/city/san-francisco"><li>San Francisco</li></a><a href="https://littlefixes.xyz/city/palo-alto"><li>Palo Alto</li></a><a href="https://littlefixes.xyz/city/boston"><li>Boston</li></a><a href="https://littlefixes.xyz/city/homer-ak"><li>Homer, AK</li></a><a href="https://littlefixes.xyz/city/portland-or"><li>Portland, OR</li></a><a href="https://littlefixes.xyz/city/vancouver-bc"><li>Vancouver, BC</li></a></ul> <p data-svelte-h="svelte-u570ty">Don't see your city? We're starting with a limited set of cities and will 
        gradually add more. If you want to see your city here, let me know at 
        <a href="https://littlefixes.xyz/cdn-cgi/l/email-protection#ee9d9b89898b9d9aae82879a9a828b8887968b9dc0969794"><span data-cfemail="087b7d6f6f6d7b7c4864617c7c646d6e61706d7b26707172">[email&nbsp;protected]</span></a>.</p></section> <section data-svelte-h="svelte-1930zmd"><blockquote><div><p>I walked with a mother who was pushing a stroller in the ditch. She told me
            that she needed to go to the store, didn't have a car that day, and didn't
            feel safe walking along the street, so she was taking the ditch, knee-high
            weeds and all. I observed the well-worn path she was treading and realized
            this was a struggle being shared with others.</p> <p>I met an elderly woman going down the street using a walker, climbing 
            over mounds of snow left by the snowplow. She told me she had no 
            choice but to get to the pharmacy that day. She pointed out that the
            street was cleared of snow but the sidewalk wasn't, so she was walking
            where she had to.</p> <p>...</p> <p>[These] are the kind of thing people accept, the gradually diminishing
                expectations of a long decline.</p></div> </blockquote></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Refractify – Optical software against myopia (141 pts)]]></title>
            <link>https://refractify.io/</link>
            <guid>39484590</guid>
            <pubDate>Fri, 23 Feb 2024 18:47:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://refractify.io/">https://refractify.io/</a>, See on <a href="https://news.ycombinator.com/item?id=39484590">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">

						<!-- Introduction -->
							<section id="intro">
										<header>
											<h2>Can a screen filter match the effectiveness of myopia prevention contacts?</h2>
										</header>
								<div>

									


									<div>

										<p>Recent studies have demonstrated that Myopia(known as near-sightedness) which was previously believed to be a hereditary condition is partially caused by increased times spent indoors or working on a near-screen. <sup>[
										<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9512310/" title="The Impact of the COVID-19 Pandemic on Myopia Progression in Children: A Systematic Review">1</a>,
										<a href="https://onlinelibrary.wiley.com/doi/10.1111/j.1475-1313.2008.00550.x" title="Nearwork-induced transient myopia (NITM) and permanent myopia – is there a link?">2</a>
										]</sup></p>
										 

										<p>Researchers are beginning to understand the mechanism in the human eye that is causing the abnormal axial growth resulting in near-sightedness, and multiple non-invasive treatment/prevention methods have been proposed, including <a href="https://kubotaglass.com/" title="Kubota Glass">smart glasses</a> and <a href="https://www.misight.com/" title="MiSight 1 day">myopia control contact lenses</a>.</p>
										<ul>
											<!--<li><a href="further_reading.html" class="button">Learn More</a></li>-->
										</ul>
									</div>
									<p>
									<iframe width="100%" height="100%" src="https://www.youtube.com/embed/KakLn3g9mlw?si=c7QbIM1zHVfMRW4G" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
									</p>
								</div>
							</section>

							<section id="first">
								<header>
									<h2>What is Refractify?</h2>
								</header>
									<p>
										Refractify is the worlds first software to apply myopic defocus effect on the screen. Pre-clinical studies suggest that it may slow the progression of myopia or even prevent it.<sup>[
										<a href="https://www.nature.com/articles/s41598-022-26323-7" title="Human myopic defocus">3</a>,
										<a href="https://www.sciencedirect.com/science/article/abs/pii/S0014483522002676?via%3Dihub" title="Tree shrews">4</a>
										]</sup></p>
									

									<p>
										This makes the screen look on the retina naturally as if it was at a greater distance. This is possible because there are slight detectable differences in the statistical properties of the light depending on how far it is coming from due to Longitudinal Chromatic Aberration(LCA) and other effects. LCA simulation is being used in computer graphics since at least 2017 to enchance depth perception, but only recently has it gained research interest for its myopia prevention properties.<sup>[
										<a href="https://www.nature.com/articles/s41598-022-26323-7" title="Human myopic defocus">3</a>,
										<a href="https://www.sciencedirect.com/science/article/abs/pii/S0014483522002676?via%3Dihub" title="Tree shrews">4</a>
										]</sup></p>
									

									<p>
										 Refractify is a software platform for effective simulation of LCA based myopic defocus and other refractive effects on computer screens.
									</p>

									<!--
								<footer class="major">
									<ul class="actions special">
										<li><a data-umami-event="learn-history" href="history.html" class="button">Learn More</a></li>
									</ul>
								</footer>
									-->
							</section>

							<section id="cta">
								<header>
									<h2>Downloads</h2>
								</header>
								<p><span>Note:</span> Refractify LLC's goal is to develop medical software based on this tech, but important steps are needed to get there including research, refinement, clinical testing for specific medical conditions, significant changes or rewrites to the software, usage recommendations, etc. Please do not attempt to cure, alter or prevent any medical condition without consulting with your doctor/eye care specialist and especially do not cease any treatment or management that has been approved for you by your doctors.</p>
								<section>
								<h3>Refractify MDL browser extension</h3>

									<div>
										<p><span><img src="https://refractify.io/images/chrome_ext_shot.png" alt="screenshot showing myopic blurred browser tab" onclick="openFullscreen(this)"></span>Free(open source) for <b>Google Chrome</b>, <b>Microsoft Edge</b>, <b>Brave</b>, and other browsers. To experience Myopic Defocus on mobile, try <b>Kiwi</b> browser. Firefox does not support Refractify yet. Remember to set effect strength and screen parameters.</p>

										<p><a data-umami-event="get-extension" href="https://chromewebstore.google.com/detail/refractify-myopic-defocus/dpnfdlnkgojjihdmgmacnmheflkojijm"><img src="https://refractify.io/images/ChromeWebStore_noborder.png" alt="Chrome Web Store icon"></a>
										<a data-umami-event="get-github" href="https://github.com/refractify/myopic_defocus"><img src="https://refractify.io/images/GitHub_Logo_White.png" alt="Github logo"></a>

										</p>
									</div>
								</section>
								
								
								
								<hr>

								<section>
								<h3>Refractify MDL desktop experience</h3>
									<div>
										<p><span><img src="https://refractify.io/images/desktop_shot_blur.png" alt="screenshot showing myopic blurred desktop" onclick="openFullscreen(this)"></span>Integrates into <b>Microsoft Windows 10/11</b> Automatically detects monitor size and resolution. Select up to 8 monitors. Calculates the best live myopic defocus effect on screen. Get the the fully optimized software with autostart from <b>Gumroad Store</b> for best experience. The desktop version is not free, but a small one-time payment is requested that supports future development testing and research on the project.</p>

										<p><a data-umami-event="get-gumroad" href="https://refractify.gumroad.com/l/product"><img src="https://refractify.io/images/gumroad.png" alt="Gumroad icon"></a></p>
									</div>
								</section>


							</section>


							<section id="faq">
								<header>
									<h2>Frequently Asked Questions</h2>
								</header>




<details open="">
  <summary>What is Myopic Defocus?</summary>
  <p>When the object you look at is at a greater distance than the far point of your eye, you experience myopic defocus naturally. This can be simulated on-screen. It is being researched for its myopia prevention properties.</p>
</details>
<details>
  <summary>Will it reverse my myopia?</summary>
  <p>Probably not, but maybe it will slow the progression of myopia, or prevent myopia for children.</p>
</details>
<details>
  <summary>Why do you think it will work?</summary>
  <p>Myopic defocus was tested on tree shrews in 2022, in a controlled test, successfully preventing myopia in a "near work" environment.<sup>[
										<a href="https://www.sciencedirect.com/science/article/abs/pii/S0014483522002676?via%3Dihub" title="Tree shrews">1</a>
										]</sup></p>
</details>
<details>
  <summary>Why do you think it will work on humans?</summary>
  <div>
    <p>Myopic defocus is currently being used on humans to slow progression of myopia in the form of bifocal contact lenses. In some cases it has been demonstrated that the human eye reacts to on-scren simulated myopic defocus. <sup>[
										<a href="https://www.nature.com/articles/s41598-022-26323-7" title="Human myopic defocus">2</a>
										]</sup></p>
    
  </div>
</details>
<details>
  <summary>Is this FDA or EU MDSW approved, etc?</summary>
  <p>Not yet. But we believe a future version will be registered for myopia prevention. May need significant changes by then. This version is capable of applying myopic defocus to make the screen appear on the retina as if it was more distant. Some people report that it is more convenient to look at.</p>
</details>
<details>
  <summary>What do eye care specialists or doctors say if this project will succeed?</summary>
  <p>Some of the doctors we talked to encouraged us to continue working on this, while others say myopia is 100% genetically determined, therefore it will never be more than a cool effect.</p>
</details>
<details>
  <summary>How myopic defocus differs from blue light filtes?</summary>
  <p>Blue light filters change the spectum of the light emulating the daily natural variation of athmospheric light. Myopic defocus blurs the blue and green colors according to the Longitudinal Chromatic Aberration function, creating an appearance of light coming from a distance. You can use either or both at the same time.</p>
</details>



							</section>


							<section id="second">
								<header>
									<h2>Roadmap</h2>
									<!-- The ultimate goal of the project if to control myopia by a screen altering solution that is:  -->
								</header>
								<div>
											<p><span><img src="https://refractify.io/images/roadmap3.png" alt=""></span></p>
										</div>
							</section>


						<!--

							<section id="second" class="main special">
								<header class="major">
									<h2>Ipsum consequat</h2>
									<p>Donec imperdiet consequat consequat. Suspendisse feugiat congue<br />
									posuere. Nulla massa urna, fermentum eget quam aliquet.</p>
								</header>
								<ul class="statistics">
									<li class="style1">
										<span class="icon solid fa-code-branch"></span>
										<strong>5,120</strong> Etiam
									</li>
									<li class="style2">
										<span class="icon fa-folder-open"></span>
										<strong>8,192</strong> Magna
									</li>
									<li class="style3">
										<span class="icon solid fa-flask"></span>
										<strong>2,048</strong> Tempus
									</li>
									<li class="style4">
										<span class="icon solid fa-umbrella"></span>
										<strong>4,096</strong> Aliquam
									</li>
									<li class="style5">
										<span class="icon fa-gem"></span>
										<strong>1,024</strong> Nullam
									</li>
								</ul>
								<p class="content">Nam elementum nisl et mi a commodo porttitor. Morbi sit amet nisl eu arcu faucibus hendrerit vel a risus. Nam a orci mi, elementum ac arcu sit amet, fermentum pellentesque et purus. Integer maximus varius lorem, sed convallis diam accumsan sed. Etiam porttitor placerat sapien, sed eleifend a enim pulvinar faucibus semper quis ut arcu. Ut non nisl a mollis est efficitur vestibulum. Integer eget purus nec nulla mattis et accumsan ut magna libero. Morbi auctor iaculis porttitor. Sed ut magna ac risus et hendrerit scelerisque. Praesent eleifend lacus in lectus aliquam porta. Cras eu ornare dui curabitur lacinia.</p>
								<footer class="major">
									<ul class="actions special">
										<li><a href="generic.html" class="button">Learn More</a></li>
									</ul>
								</footer>
							</section>

						-->


						<!--
							<section id="third" class="main special">
								<header class="major">
									<h2>Refractify MDL</h2>
									<h3>Apply a live Myopic Defocus Layer anywhere.</h3>
								</header>
								<p>Applies a Myopic Defocus Layer. This makes the screen look on the retina naturally as if it was at a greater distance. This effect is based on the physical refractive properties of the human eye. This effect is being researched for its myopia prevention properties.</p>
								<footer>

									<div>

										<script
											defer
											src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/index.js"
										></script>
										<link
											rel="stylesheet"
											href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/styles.css"
										/>

										<img-comparison-slider>
											<img slot="first" src="images/images_colors_N.png" width=100%/>
											<img slot="second" src="images/images_colors_R.png" width=100%/>
										</img-comparison-slider>

									</div>
									<p>This screen was captured on a screen of resolution 2560x1440 with screen size of 309x174mm, physical viewing distance set to 50cm and average pupil size. Alpha is 30%.</p>
								</footer>

							</section>

							-->

								<section id="feedback">
									<h2>Subscribe for updates</h2>
									<p>Get notified on future versions(including Linux) and clinical studies!</p>
									
								</section>

								<section id="feedback">
									<h2>Support</h2>
									<p>If you think this project is making a positive impact, consider supporting it. Thank you.</p>

									

									<stripe-buy-button data-umami-event="support-money" buy-button-id="buy_btn_1OUaphIqzWoUILVH8M4jIIan" publishable-key="pk_live_51OUaKYIqzWoUILVHG4mQJ0iYeOV5uXW7jWFUfQPPlyC9LKCk9N0UWJ2EUwnSRyCKUo8tw3NQg3QRVMMxb35gkAcq00xFfzI5b3">
									</stripe-buy-button>

								</section>


					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD ROCm Software Blogs (114 pts)]]></title>
            <link>https://rocm.blogs.amd.com/</link>
            <guid>39484321</guid>
            <pubDate>Fri, 23 Feb 2024 18:28:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rocm.blogs.amd.com/">https://rocm.blogs.amd.com/</a>, See on <a href="https://news.ycombinator.com/item?id=39484321">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      
      
      <main id="main-content">
        
        



          <div>
              
              
              
              

<div id="jb-print-docs-body">
    <h2><no title=""></no></h2>
    <!-- Table of contents -->
    <div id="print-main-content">
            
            <p>
                <h2> Contents </h2>
            </p>
            <nav aria-label="Page">
                <ul>
</ul>

            </nav>
        </div>
</div>

              
                

                <article role="main">
                   
  <meta charset="UTF-8">
  <meta name="description" content="AMD ROCm™ software blogs">
  <meta name="keywords" content="AMD GPU, MI300, MI250, ROCm, blog">
  <title>ROCm Blogs</title>


<h2>Applications &amp; models </h2>
<div>
<div>
<p>
Stable Diffusion (ONNX Runtime)</p>
<p>Efficient image generation with Stable Diffusion models and ONNX Runtime using AMD GPUs</p>
</div>
<div>
<p>
3D scene mapping using NeRF</p>
<p>Two-dimensional images to three-dimensional scene mapping using NeRF on an AMD GPU</p>
</div>
<div>
<p>
LLM fine-tuning with JAX</p>
<p>LLM distributed supervised fine-tuning with JAX</p>
</div>
<div>
<p>
Jacobi solver</p>
<p>Simplifying deep learning: A guide to PyTorch Lightning</p>
</div>
<div>
<p>
Finite difference: Laplacian, part 4</p>
<p>Performance benchmarking across various AMD GPUs and cache size limitations</p>
</div>

</div>
<h2>Software tools &amp; optimizations</h2>
<div>
<div>
<p>
Register pressure</p>
<p>Register pressure in AMD CDNA2 GPUs</p>
</div>
<div>
<p>
Jacobi solver</p>
<p>Implementation of the Jacobi solver with both HIP and OpenMP offloading</p>
</div>
<div>
<p>
Matrix cores</p>
<p>Accelerating GEMM computations using AMD’s matrix core technology</p>
</div>
<div>
<p>
GPU-aware MPI</p>
<p>Leveraging various GPU-aware MPI implementations with ROCm</p>
</div>
<div>
<p>
MI200 memory space</p>
<p>Overview of the AMD Instinct MI200 memory space</p>
</div>

</div>
<h2> Stay informed</h2>
<ul>
  <li><a href="https://rocm.blogs.amd.com/blog/atom.xml"> Subscribe to our  RSS feed</a></li>
  <li><a href="https://github.com/ROCm/rocm-blogs"> Watch our GitHub repo </a></li>
</ul>







                </article>
              

              
              
                
              
            </div>
          
        

      </main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: OK-Robot: open, modular home robot framework for pick-and-drop anywhere (476 pts)]]></title>
            <link>https://ok-robot.github.io/</link>
            <guid>39483482</guid>
            <pubDate>Fri, 23 Feb 2024 17:23:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ok-robot.github.io/">https://ok-robot.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=39483482">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                            <figure>
                                <img src="https://ok-robot.github.io/mfiles/images/sankey_failure_modes.png" alt="A sankey diagram showing the analysis of success and failure modes of OK-Robot.">
                            </figure>
                            <p>
                                While our method can show zero-shot generalization in
                                completely new environments, we probe OK-Robot to better
                                understand when and how it succeeds and fails.
                                While we find a 58.5% success rate at completely novel homes, at a closer look, we
                                also notice a long tail of failure causes, which is presented in the figure above.
                                We see that the leading three cause of failures are failing
                                to retrieve the right object to navigate to from the semantic
                                memory (9.3%), getting a difficult pose from the manipulation
                                module (8.0%), and hardware difficulties (7.5%).
                            </p>
                            <p>
                                In the "Understanding the performance
                                of OK-Robot" section of the paper,
                                we go over the analysis of the failure modes presented in
                                the figure above and discuss the most frequent cases.
                            </p>
                        </div></div>]]></description>
        </item>
    </channel>
</rss>