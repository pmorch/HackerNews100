<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 07 Jan 2026 20:30:13 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[US will ban Wall Street investors from buying single-family homes (165 pts)]]></title>
            <link>https://www.reuters.com/world/us/us-will-ban-large-institutional-investors-buying-single-family-homes-trump-says-2026-01-07/</link>
            <guid>46531068</guid>
            <pubDate>Wed, 07 Jan 2026 19:13:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/world/us/us-will-ban-large-institutional-investors-buying-single-family-homes-trump-says-2026-01-07/">https://www.reuters.com/world/us/us-will-ban-large-institutional-investors-buying-single-family-homes-trump-says-2026-01-07/</a>, See on <a href="https://news.ycombinator.com/item?id=46531068">Hacker News</a></p>
Couldn't get https://www.reuters.com/world/us/us-will-ban-large-institutional-investors-buying-single-family-homes-trump-says-2026-01-07/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Texas A&M bans part of Plato's Symposium (129 pts)]]></title>
            <link>https://dailynous.com/2026/01/06/texas-am-bans-plato/</link>
            <guid>46529257</guid>
            <pubDate>Wed, 07 Jan 2026 17:23:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dailynous.com/2026/01/06/texas-am-bans-plato/">https://dailynous.com/2026/01/06/texas-am-bans-plato/</a>, See on <a href="https://news.ycombinator.com/item?id=46529257">Hacker News</a></p>
Couldn't get https://dailynous.com/2026/01/06/texas-am-bans-plato/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Eat Real Food (134 pts)]]></title>
            <link>https://realfood.gov</link>
            <guid>46529237</guid>
            <pubDate>Wed, 07 Jan 2026 17:22:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://realfood.gov">https://realfood.gov</a>, See on <a href="https://news.ycombinator.com/item?id=46529237">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><div id="intro"><div aria-label="Introduction"><p><h2>Real Food<br> Starts Here</h2></p><p>Better health begins on your plate—not in your medicine cabinet.<br> The new Dietary Guidelines for Americans defines real food as whole, nutrient-dense, and naturally occurring, placing them back at the center of our diets.</p></div><div id="hero-video"><p><img src="https://realfood.gov/images/video-placeholder.webp" alt=""></p><div role="button" tabindex="0"><p><img src="https://realfood.gov/images/video-placeholder.webp" alt=""></p><video src="https://customer-29f0fiy60aiz1fqm.cloudflarestream.com/e978a151c0700e12277e66802746e204/manifest/video.m3u8" crossorigin="anonymous" muted="" playsinline="" controls="" aria-label="Announcement video for mobile fullscreen playback"><track kind="captions" src="/transcript.vtt" srclang="en" label="English" default=""></video></div></div></div><div id="problem" aria-labelledby="soh-heading"><p><h2 id="soh-heading">The State of Our Health</h2></p><div><div><p><h3>America is sick.<br>The data is clear.</h3></p></div><div><p><h3>50% of Americans have <span>prediabetes or diabetes</span></h3></p></div><div><p><h3>75% of adults report having at least one <span>chronic condition</span></h3></p></div><div><p><h3>90% of U.S. healthcare spending goes to treating <span>chronic disease</span>—much of which is linked to diet and lifestyle</h3></p></div></div></div></div><div><section id="solution"><div><p><span>1992 Food Pyramid</span></p><p><span><span>For</span> </span><span><span>decades</span> </span><span><span>we've</span> </span><span><span>been</span> </span><span><span>misled</span> </span><span><span>by</span> </span><span><span>guidance</span> </span><span><span>that</span> </span><span><span>prioritized</span> </span><span><span>highly</span> </span><span><span>processed</span> </span><span><span>food,</span> </span><span><span>and</span> </span><span><span>are</span> </span><span><span>now</span> </span><span><span>facing</span> </span><span><span>rates</span> </span><span><span>of</span> </span><span><span>unprecedented</span> </span><span><span>chronic</span> </span><span><span>disease.</span></span></p></div><div id="solution-solvable"><h2><p><span><span>W</span><span>e</span></span><span>&nbsp;</span><span><span>c</span><span>a</span><span>n</span></span><span>&nbsp;</span><span><span>s</span><span>o</span><span>l</span><span>v</span><span>e</span></span></p><p><span><span>t</span><span>h</span><span>i</span><span>s</span></span><span>&nbsp;</span><span><span>c</span><span>r</span><span>i</span><span>s</span><span>i</span><span>s</span><span>.</span></span></p></h2></div><p><span><span><span data-word-id="line-word-0">For</span> </span><span><span data-word-id="line-word-1">the</span> </span><span><span data-word-id="line-word-2">first</span> </span><span><span data-word-id="line-word-3">time,</span> </span><span><span data-word-id="line-word-4">we're</span> </span><span><span data-word-id="line-word-5">calling</span> </span><span><span data-word-id="line-word-6">out</span> </span><span><span data-word-id="line-word-7">the</span> </span><span><span data-word-id="line-word-8">dangers</span> </span><span><span data-word-id="line-word-9">of</span> </span><span><span data-word-id="line-word-10">highly</span> </span><span><span data-word-id="line-word-11">processed</span> </span><span><span data-word-id="line-word-12">foods</span> </span><span><span data-word-id="line-word-13">and</span> </span><span><span data-word-id="line-word-14">rebuilding</span> </span><span><span data-word-id="line-word-15">a</span> </span><span><span data-word-id="line-word-16">broken</span> </span><span><span data-word-id="line-word-17">system</span> </span><span><span data-word-id="line-word-18">from</span> </span><span><span data-word-id="line-word-19">the</span> </span><span><span data-word-id="line-word-20">ground</span> </span><span><span data-word-id="line-word-21">up</span> </span><span><span data-word-id="line-word-22">with</span> </span><span><span data-word-id="line-word-23">gold-standard</span> </span><span><span data-word-id="line-word-24">science</span> </span><span><span data-word-id="line-word-25">and</span> </span><span><span data-word-id="line-word-26">common</span> <span data-word-id="line-word-27">sense.</span></span></span></p></section><section id="pyramid"><div><p>Introducing</p><h2>The New Pyramid</h2></div><div><div><p><h2>Eat Real <br>Food</h2></p></div><div data-category="proteins"><h2>Protein, Dairy, &amp; Healthy Fats</h2><div><p>We are ending the war on protein. Every meal must prioritize high-quality, nutrient-dense protein from both animal and plant sources, paired with healthy fats from whole foods such as eggs, seafood, meats, full-fat dairy, nuts, seeds, olives, and avocados.</p><p><strong>Protein target:</strong> <!-- -->1.2–1.6 grams per kilogram of body weight per day.</p></div></div><div data-category="fruits-vegetables"><h2>Vegetables &amp; Fruits</h2><div><p>Vegetables and fruits are essential to real food nutrition. Eat a wide variety of whole, colorful, nutrient-dense vegetables and fruits in their original form, prioritizing freshness and minimal processing.</p><div><p><strong>Vegetables:</strong> <!-- -->3 servings per day.</p><p><strong>Fruits:</strong> <!-- -->2 servings per day.</p></div></div></div><div data-category="whole-grains"><h2>Whole Grains</h2><div><p>Whole grains are encouraged. Refined carbohydrates are not. Prioritize fiber-rich whole grains and significantly reduce the consumption of highly processed, refined carbohydrates that displace real nourishment.</p><p><strong>Target:</strong> <!-- -->2–4 servings per day.</p></div></div></div></section><section id="guidelines"><p><span><span><span data-word-id="line-1-word-0">Our</span> </span><span><span data-word-id="line-1-word-1">nation</span> </span><span><span data-word-id="line-1-word-2">is</span> </span><span><span data-word-id="line-1-word-3">finding</span> </span><span><span data-word-id="line-1-word-4">its</span> </span><span><span data-word-id="line-1-word-5">footing</span> </span><span><span data-word-id="line-1-word-6">again,</span> </span><span><span data-word-id="line-1-word-7">moving</span> </span><span><span data-word-id="line-1-word-8">past</span> </span><span><span data-word-id="line-1-word-9">decades</span> </span><span><span data-word-id="line-1-word-10">of</span> </span><span><span data-word-id="line-1-word-11">unhealthy</span> </span><span><span data-word-id="line-1-word-12">eating</span> </span><span><span data-word-id="line-1-word-13">and</span> </span><span><span data-word-id="line-1-word-14">rebuilding</span> </span><span><span data-word-id="line-1-word-15">a</span> </span><span><span data-word-id="line-1-word-16">food</span> </span><span><span data-word-id="line-1-word-17">culture</span> </span><span><span data-word-id="line-1-word-18">rooted</span> </span><span><span data-word-id="line-1-word-19">in</span> </span><span><span data-word-id="line-1-word-20">health,</span> </span><span><span data-word-id="line-1-word-21">science,</span> </span><span><span data-word-id="line-1-word-22">transparency,</span> </span><span><span data-word-id="line-1-word-23">and</span> </span><span><span data-word-id="line-1-word-24">personal</span> </span><span><span data-word-id="line-1-word-25">responsibility.</span></span></span></p></section></div><div><div id="faqs"><p><h2>Key<br>Guidance</h2></p></div><div aria-labelledby="resources-heading"><h2 id="resources-heading">Resources</h2><p>Explore the research, recommendations, and implementation guidance that shape the Dietary Guidelines, including the science, the policy guidance, and the everyday serving framework.</p><a href="#" target="_blank" rel="noopener noreferrer"><span>Watch the press release</span></a></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Creators of Tailwind laid off 75% of their engineering team (611 pts)]]></title>
            <link>https://github.com/tailwindlabs/tailwindcss.com/pull/2388</link>
            <guid>46527950</guid>
            <pubDate>Wed, 07 Jan 2026 16:02:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/tailwindlabs/tailwindcss.com/pull/2388">https://github.com/tailwindlabs/tailwindcss.com/pull/2388</a>, See on <a href="https://news.ycombinator.com/item?id=46527950">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-turbo-body="">
      
      



    <div>
      <p><a href="#start-of-content" data-skip-target-assigned="false">Skip to content</a>

      <span data-view-component="true">
    <span data-view-component="true"></span>
</span></p>

<react-partial partial-name="keyboard-shortcuts-dialog" data-ssr="false" data-attempted-ssr="false" data-react-profiling="false">
  
  
  
</react-partial>





      

          

              






<header role="banner" data-is-top="true" data-color-mode="light" data-light-theme="light" data-dark-theme="dark">
  <h2>Navigation Menu</h2>

  

  <div>
            

<react-partial partial-name="marketing-navigation" data-ssr="true" data-attempted-ssr="true" data-react-profiling="false">
  
  
  <div data-target="react-partial.reactRoot"><nav aria-label="Global"><ul><li><div><ul><li><div><p><span>AI CODE CREATION</span></p><ul><li><a href="https://github.com/features/copilot" data-analytics-event="{&quot;action&quot;:&quot;github_copilot&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_copilot_link_platform_navbar&quot;}"><div><p><span>GitHub Copilot</span><span>Write better code with AI</span></p></div></a></li><li><a href="https://github.com/features/spark" data-analytics-event="{&quot;action&quot;:&quot;github_spark&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_spark_link_platform_navbar&quot;}"><div><p><span>GitHub Spark</span><span>Build and deploy intelligent apps</span></p></div></a></li><li><a href="https://github.com/features/models" data-analytics-event="{&quot;action&quot;:&quot;github_models&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_models_link_platform_navbar&quot;}"><div><p><span>GitHub Models</span><span>Manage and compare prompts</span></p></div></a></li><li><a href="https://github.com/mcp" data-analytics-event="{&quot;action&quot;:&quot;mcp_registry&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;mcp_registry_link_platform_navbar&quot;}"><div><p><span>MCP Registry<sup>New</sup></span><span>Integrate external tools</span></p></div></a></li></ul></div></li><li></li><li></li><li></li></ul><p><a href="https://github.com/features" data-analytics-event="{&quot;action&quot;:&quot;view_all_features&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;view_all_features_link_platform_navbar&quot;}"><span>View all features</span></a></p></div></li><li></li><li></li><li></li><li></li><li><a href="https://github.com/pricing" data-analytics-event="{&quot;action&quot;:&quot;pricing&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;pricing&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;pricing_link_pricing_navbar&quot;}"><span>Pricing</span></a></li></ul></nav></div>
</react-partial>



        <div>
                


<qbsearch-input data-scope="repo:tailwindlabs/tailwindcss.com" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="irRp-BFaHMgxq4imOVlsGFLVnIoxfMw_tIkEHpPUnmsWsPxYPBmTPbY1tSupCkn4Zmv1cmnhiHDiC3o5HVbxuw" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="tailwindlabs/tailwindcss.com" data-current-org="tailwindlabs" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fpull_requests_fragments%2Fpull_request_layout&amp;source=header-repo&amp;source_repo=tailwindlabs%2Ftailwindcss.com" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/tailwindlabs/tailwindcss.com/pull/2388&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="3401bd36361ea32f919c718fec042ee6232ab8ab3d2502cfe1fa7f09643a4565" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/voltron/pull_requests_fragments/pull_request_layout;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a></p><p>
    <react-partial-anchor>
      <tool-tip id="tooltip-2e337e34-b90d-4bf2-b09f-a1a394c6af5c" for="icon-button-ba963e4b-17e4-4fd6-b030-bced3c528543" popover="manual" data-direction="s" data-type="label" data-view-component="true">Appearance settings</tool-tip>

      <template data-target="react-partial-anchor.template">
        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.753d458774a2f782559b.module.css">

<react-partial partial-name="appearance-settings" data-ssr="false" data-attempted-ssr="false" data-react-profiling="false">
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>


      </template>
    </react-partial-anchor>
  </p>

          </div>
      </div>
</header>

      
    </div>

  








    


    






  <div itemscope="" itemtype="http://schema.org/SoftwareSourceCode" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="" data-project-hovercards-enabled="">
    <main id="js-repo-pjax-container">
      
  





      
    

    






  

  
  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance">
    <div id="repo-content-pjax-container" data-channel="eyJjIjoicHVsbF9yZXF1ZXN0OjMwMjM3NDgwNTYiLCJ0IjoxNzY3ODA3MDAyfQ==--02ac85884aa40242a31c1d071d6a92a340932cbe67d928c4835bcc713bdd4bbe" data-url="/tailwindlabs/tailwindcss.com/pull/2388/partials/title?sticky=true" data-channel-event-name="title_updated" data-pull-is-open="false" data-gid="PR_kwDOB0_BXc60OrvY" data-pjax="" data-turbo-frame="">
            


               

<details>
  <summary id="button-d368d467fb033745">
    
    New issue
  </summary>
  <details-dialog aria-label="Sign up for GitHub">
            <div>
  <p>
    <strong>Have a question about this project?</strong> Sign up for a free GitHub account to open an issue and contact its maintainers and the community.
  </p>

  

  <p>By clicking “Sign up for GitHub”, you agree to our <a href="https://docs.github.com/terms" target="_blank">terms of service</a> and
  <a href="https://docs.github.com/privacy" target="_blank">privacy statement</a>. We’ll occasionally send you account related emails.</p>

  <p>
    Already on GitHub?
    <a data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;new issue modal&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/tailwindlabs/tailwindcss.com/pull/2388&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="c7ca29965cfe2224da65a695d44c1a1053cdf867ccbe608745faebf96a3b3ef0" href="https://github.com/login?return_to=%2Ftailwindlabs%2Ftailwindcss.com%2Fissues%2Fnew%2Fchoose">Sign in</a>
    to your account
  </p>
</div>
  </details-dialog>
</details>
              
          </div>

</turbo-frame>


    </main>
  </div>

          



    <ghcc-consent id="ghcc" data-locale="en" data-initial-cookie-consent-allowed="" data-cookie-consent-required="true"></ghcc-consent>




  

    <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm" open="">
    <summary role="button" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal">
      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog="">
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

    

    <template id="snippet-clipboard-copy-button">
  <div class="zeroclipboard-container position-absolute right-0 top-0">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn js-clipboard-copy m-2 p-0" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon m-2">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>
<template id="snippet-clipboard-copy-button-unpositioned">
  <div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>




    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dell's CES 2026 chat was the most pleasingly un-AI briefing I've had in 5 years (121 pts)]]></title>
            <link>https://www.pcgamer.com/hardware/dells-ces-2026-chat-was-the-most-pleasingly-un-ai-briefing-ive-had-in-maybe-5-years/</link>
            <guid>46527706</guid>
            <pubDate>Wed, 07 Jan 2026 15:46:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pcgamer.com/hardware/dells-ces-2026-chat-was-the-most-pleasingly-un-ai-briefing-ive-had-in-maybe-5-years/">https://www.pcgamer.com/hardware/dells-ces-2026-chat-was-the-most-pleasingly-un-ai-briefing-ive-had-in-maybe-5-years/</a>, See on <a href="https://news.ycombinator.com/item?id=46527706">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/xU57HyjJgwpdKBvDT7nwdF-1920-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/xU57HyjJgwpdKBvDT7nwdF-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/xU57HyjJgwpdKBvDT7nwdF-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/xU57HyjJgwpdKBvDT7nwdF-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/xU57HyjJgwpdKBvDT7nwdF-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/xU57HyjJgwpdKBvDT7nwdF-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/xU57HyjJgwpdKBvDT7nwdF-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/xU57HyjJgwpdKBvDT7nwdF.jpg" alt="Dell COO Jeff Clarke at its CES 2026 pr-briefing" srcset="https://cdn.mos.cms.futurecdn.net/xU57HyjJgwpdKBvDT7nwdF-1920-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/xU57HyjJgwpdKBvDT7nwdF-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/xU57HyjJgwpdKBvDT7nwdF-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/xU57HyjJgwpdKBvDT7nwdF-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/xU57HyjJgwpdKBvDT7nwdF-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/xU57HyjJgwpdKBvDT7nwdF-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/xU57HyjJgwpdKBvDT7nwdF-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/xU57HyjJgwpdKBvDT7nwdF.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/xU57HyjJgwpdKBvDT7nwdF.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<figcaption>
<span>(Image credit: Dell | Alienware)</span>
</figcaption>
</div>
<div id="article-body">

<p id="c9397255-321a-4a27-b2eb-09145cc3d640">The unshakable presence of AI has been an unwelcome companion of my job for the past few years, but it sure feels like longer. It's not even like it's some excitingly malevolent artificial mind with tendrils of influence weaving its way throughout my world. That would at least be satisfying from a sci-fi perspective. No, what I've had to deal with can barely write, definitely cannot count, and has only just figured out what fingers are.</p><p>Yet it's been something that has pervading every product announcement, presentation, or pre-briefing I've been a part of in recent times from any company even tangentially related to tech. To the point where I now have a bullshit AI bingo card I fill out just to distract myself from the barely resistible desire to stab a pen through my own hand just to feel something real.</p><div id="249ffcec-800e-43e1-844c-28253a20336f"><p>CES 2026</p><div><figure><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/tS9pSoXGH8XGnm6WV32cwM-1200-80.png.webp 1200w, https://cdn.mos.cms.futurecdn.net/tS9pSoXGH8XGnm6WV32cwM-1024-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/tS9pSoXGH8XGnm6WV32cwM-970-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/tS9pSoXGH8XGnm6WV32cwM-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/tS9pSoXGH8XGnm6WV32cwM-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/tS9pSoXGH8XGnm6WV32cwM-320-80.png.webp 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/tS9pSoXGH8XGnm6WV32cwM.png" alt="The CES logo on display at the show." srcset="https://cdn.mos.cms.futurecdn.net/tS9pSoXGH8XGnm6WV32cwM-1200-80.png 1200w, https://cdn.mos.cms.futurecdn.net/tS9pSoXGH8XGnm6WV32cwM-1024-80.png 1024w, https://cdn.mos.cms.futurecdn.net/tS9pSoXGH8XGnm6WV32cwM-970-80.png 970w, https://cdn.mos.cms.futurecdn.net/tS9pSoXGH8XGnm6WV32cwM-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/tS9pSoXGH8XGnm6WV32cwM-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/tS9pSoXGH8XGnm6WV32cwM-320-80.png 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/tS9pSoXGH8XGnm6WV32cwM.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/tS9pSoXGH8XGnm6WV32cwM.png">
</picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: Future)</span></figcaption></figure><p><a data-analytics-id="inline-link" href="https://www.pcgamer.com/uk/tag/ces-2026/" target="_blank" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.pcgamer.com/tag/ces-2026/"><strong>Catch up with CES 2026</strong></a><strong>: </strong>We're on the ground in sunny Las Vegas covering all the latest announcements from some of the biggest names in tech, including Nvidia, AMD, Intel, Asus, Razer, MSI and more.</p></div></div><p id="32aec047-7311-4bb6-b793-a0b0aec698c7">So thank you, Dell, for making your CES 2026 pre-briefing so blessedly free of effusive AI chat that I just had to mention it.</p><p>It started off with Dell vice chairman and COO, Jeff Clarke, taking to a small stage to talk about the state of the industry and where Dell and its Alienware sub-brand is going this year. He talks tariffs, the slow transitioning of the industry (he says CPU, but I'm presuming he meant OS and Windows 10 → 11), and then "we have this un-met promise of AI, and the expectation of AI driving end user demand," as well as the fact that "we're about ready to enter 2026 with a memory shortage that is pretty significant."</p><p>Clarke and his co-presenters then go on to introduce the return of the XPS laptop lineup, some <a data-analytics-id="inline-link" href="https://www.pcgamer.com/hardware/gaming-laptops/alienware-is-taking-on-razer-and-asus-with-its-new-ultra-slim-covert-gaming-laptop-tease/" target="_blank" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.pcgamer.com/hardware/gaming-laptops/alienware-is-taking-on-razer-and-asus-with-its-new-ultra-slim-covert-gaming-laptop-tease">new high-end ultraslim Alienware laptops</a>, as well as some <a data-analytics-id="inline-link" href="https://www.pcgamer.com/hardware/gaming-laptops/alienware-promises-its-not-cutting-corners-on-the-things-that-matter-the-most-with-its-new-entry-level-gaming-laptops/" target="_blank" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.pcgamer.com/hardware/gaming-laptops/alienware-promises-its-not-cutting-corners-on-the-things-that-matter-the-most-with-its-new-entry-level-gaming-laptops">entry-level Alienware laptops</a> (cheap Alienwares? Really?), <a data-analytics-id="inline-link" href="https://www.pcgamer.com/hardware/gaming-pcs/quite-frankly-it-was-missing-one-thing-alienware-has-fixed-one-of-the-biggest-issues-with-its-area-51-gaming-pc/" target="_blank" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.pcgamer.com/hardware/gaming-pcs/quite-frankly-it-was-missing-one-thing-alienware-has-fixed-one-of-the-biggest-issues-with-its-area-51-gaming-pc">new spins of its Area-51 desktops</a>, and a handful of new monitors.</p><p>All of this is very "consumer-first" and aimed at dialling in to both expand the numbers of people using Dell/Alienware tech and the areas in which it operates. And the only mention of AI in the entire thing is Jeff's little line at the beginning. It's clear, concise, focused on the tech and, in the Q&amp;A that followed, refreshingly honest.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-kbfaxaf7wyqvuKrohe5kdb"><section><p>Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team.</p></section></div><p>"One thing you'll notice is the message we delivered around our products was not AI-first," Dell head of product, Kevin Terwilliger says with a smile. "So, a bit of a shift from a year ago where we were all about the AI PC."</p><p>It's not that Dell doesn't care about AI or AI PCs anymore, it's just that over the past year or so it's come to realise that the consumer doesn't.</p><figure data-bordeaux-image-check="" id="eeacaae0-eecd-4a10-b9e7-bd932d9e84a9"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/Jy2bLcR2V2YBQGnCj4u5VZ-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/Jy2bLcR2V2YBQGnCj4u5VZ-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/Jy2bLcR2V2YBQGnCj4u5VZ-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/Jy2bLcR2V2YBQGnCj4u5VZ-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/Jy2bLcR2V2YBQGnCj4u5VZ-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/Jy2bLcR2V2YBQGnCj4u5VZ-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/Jy2bLcR2V2YBQGnCj4u5VZ.jpg" alt="Dell&amp;#039;s new Alienware ultraslim and entry-level laptops" srcset="https://cdn.mos.cms.futurecdn.net/Jy2bLcR2V2YBQGnCj4u5VZ-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/Jy2bLcR2V2YBQGnCj4u5VZ-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/Jy2bLcR2V2YBQGnCj4u5VZ-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/Jy2bLcR2V2YBQGnCj4u5VZ-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/Jy2bLcR2V2YBQGnCj4u5VZ-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/Jy2bLcR2V2YBQGnCj4u5VZ-320-80.jpg 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/Jy2bLcR2V2YBQGnCj4u5VZ.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/Jy2bLcR2V2YBQGnCj4u5VZ.jpg">
</picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: Dell | Alienware)</span></figcaption></figure><figure id="38c6cd4d-82f1-4886-bbcd-461c9536c26d"><blockquote><p>They're not buying based on AI. In fact I think AI probably confuses them more than it helps them.</p><figcaption><cite>Kevin Terwilliger, Dell head of product</cite></figcaption></blockquote></figure><p id="fc2bf18f-3d59-4b0c-959a-08eaa3dd352a">"We're very focused on delivering upon the AI capabilities of a device—in fact everything that we're announcing has an NPU in it—but what we've learned over the course of this year, especially from a consumer perspective, is they're not buying based on AI," Terwilliger says bluntly. "In fact I think AI probably confuses them more than it helps them understand a specific outcome."</p><p>In a way, you could argue that's tantamount to dumbing down the technology for the end user. But this isn't like withholding information about the core counts of the chips inside your machine, or the TGP of the mobile GPU at its heart for fear of confusing some fictitious customer. There <em>are</em> people who care about the hardware inside these devices, but it's becoming clear there are precious few who care about the AI components or theoretical capabilities of those machines.</p><p>The fact that a huge PC brand such as Dell/Alienware has decided to ditch the AI-first marketing that seems to otherwise permeate everything—and honestly still permeates—is entirely welcome, very refreshing, and hopefully the mark of things to come.</p><p>Because, until AI becomes a valid, useful technology for the end user of these devices, and not just some marketing check box or buzzword for investors, every company ought to take a leaf out of Dell's book and just keep schtum. And that's honestly not something I've said many times about the big PC box shifter in the past.</p><div id="slice-container-person-kbfaxaf7wyqvuKrohe5kdb-uV3uS7TdUjKIGgGTX2AFrnKgjUapjfz7"><figure data-bordeaux-image-check="false"><div><picture data-hydrate="false"><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/5ryUJb6snbbQMdpHJu7i8W-140-80.jpg.webp 140w" sizes="99vw" data-normal="https://cdn.mos.cms.futurecdn.net/5ryUJb6snbbQMdpHJu7i8W.jpg" data-original-mos="https://cdn.mos.cms.futurecdn.net/5ryUJb6snbbQMdpHJu7i8W.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/5ryUJb6snbbQMdpHJu7i8W.jpg" data-pin-nopin="true" data-slice-image="true"><source type="image/jpeg" srcset="https://cdn.mos.cms.futurecdn.net/5ryUJb6snbbQMdpHJu7i8W-140-80.jpg 140w" sizes="99vw" data-normal="https://cdn.mos.cms.futurecdn.net/5ryUJb6snbbQMdpHJu7i8W.jpg" data-original-mos="https://cdn.mos.cms.futurecdn.net/5ryUJb6snbbQMdpHJu7i8W.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/5ryUJb6snbbQMdpHJu7i8W.jpg" data-pin-nopin="true" data-slice-image="true"><img src="https://cdn.mos.cms.futurecdn.net/5ryUJb6snbbQMdpHJu7i8W.jpg" alt="AMD Ryzen 9 9800X3D processor" srcset="https://cdn.mos.cms.futurecdn.net/5ryUJb6snbbQMdpHJu7i8W-140-80.jpg 140w" sizes="99vw" loading="lazy" data-normal="https://cdn.mos.cms.futurecdn.net/5ryUJb6snbbQMdpHJu7i8W.jpg" data-original-mos="https://cdn.mos.cms.futurecdn.net/5ryUJb6snbbQMdpHJu7i8W.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/5ryUJb6snbbQMdpHJu7i8W.jpg" data-pin-nopin="true" data-slice-image="true"></picture></div></figure></div>
</div>



<div id="slice-container-authorBio-kbfaxaf7wyqvuKrohe5kdb"><p>Dave has been gaming since the days of Zaxxon and Lady Bug on the Colecovision, and code books for the Commodore Vic 20 (Death Race 2000!). He built his first gaming PC at the tender age of 16, and finally finished bug-fixing the Cyrix-based system around a year later. When he dropped it out of the window. He first started writing for Official PlayStation Magazine and Xbox World many decades ago, then moved onto PC Format&nbsp;full-time, then PC Gamer, TechRadar, and T3 among others. Now he's back, writing about the nightmarish graphics card market, CPUs with more cores than sense, gaming laptops hotter than the sun, and SSDs more capacious than a Cybertruck.</p></div>
</section>

<div x-show="$store.Viafoura.showWidgets" x-cloak="" data-component-name="Viafoura:Comments" x-data="ViafouraComments('300px')" data-nosnippet="" data-community-guidelines-text="<p class='vfcustom-community-guidelines'>Please follow our <a href=&quot;https://www.pcgamer.com/about-pc-gamer/#section-community-guidelines&quot; target=&quot;_blank&quot;>community guidelines</a>.</p>">
<p>You must confirm your public display name before commenting</p>
<p>Please logout and then login again, you will then be prompted to enter your display name.</p>
</div>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLM Problems Observed in Humans (130 pts)]]></title>
            <link>https://embd.cc/llm-problems-observed-in-humans</link>
            <guid>46527581</guid>
            <pubDate>Wed, 07 Jan 2026 15:36:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://embd.cc/llm-problems-observed-in-humans">https://embd.cc/llm-problems-observed-in-humans</a>, See on <a href="https://news.ycombinator.com/item?id=46527581">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Agents</p>

<p>Published 7 Jan 2026. Written by Jakob Kastelic.</p>
<p><img src="https://embd.cc/images/brain.jpg" alt=""></p>
<p>While some are still discussing why computers will never be able to pass the
Turing test, I find myself repeatedly facing the idea that as the models improve
and humans don’t, the bar for the test gets raised and eventually humans won’t
pass the test themselves. Here’s a list of what used to be LLM failure modes but
that are now more commonly observed when talking to people.</p>
<h3 id="dont-know-when-to-stop-generating">Don’t know when to stop generating</h3>
<p>This has always been an issue in conversations: you ask a seemingly small and
limited question, and in return have to listen to what seems like hours of
incoherent rambling. Despite exhausting their knowledge of the topic, people
will keep on talking about stuff you have no interest in. I find myself
searching for the “stop generating” button, only to remember that all I can do
is drop hints, or rudely walk away.</p>
<h3 id="small-context-window">Small context window</h3>
<p>The best thing about a good deep conversation is when the other person gets you:
you explain a complicated situation you find yourself in, and find some
resonance in their replies. That, at least, is what happens when chatting with
the recent large models. But when subjecting the limited human mind to the same
prompt—a rather long one—again and again the information in the prompt
somehow gets lost, their focus drifts away, and you have to repeat crucial
facts. In such a case, my gut reaction is to see if there’s a way to pay to
upgrade to a bigger model, only to remember that there’s no upgrading of the
human brain. At most what you can do is give them a good night’s sleep and then
they may possibly switch from the “Fast” to the “Thinking” mode, but that’s not
guaranteed with all people.</p>
<h3 id="too-narrow-training-set">Too narrow training set</h3>
<p>I’ve got a lot of interests and on any given day, I may be excited to discuss
various topics, from kernels to music to cultures and religions. I know I can
put together a prompt to give any of today’s leading models and am essentially
guaranteed a fresh perspective on the topic of interest. But let me pose the
same prompt to people and more often then not the reply will be a polite nod
accompanied by clear signs of their thinking something else entirely, or maybe
just a summary of the prompt itself, or vague general statements about how
things should be. In fact, so rare it is to find someone who knows what I mean
that it feels like a magic moment. With the proliferation of genuinely good
models—well educated, as it were—finding a conversational partner with a
good foundation of shared knowledge has become trivial with AI. This does not
bode well for my interest in meeting new people.</p>
<h3 id="repeating-the-same-mistakes">Repeating the same mistakes</h3>
<p>Models with a small context window, or a small number of parameters, seem to
have a hard time learning from their mistakes. This should not be a problem for
humans: we have a long term memory span measured in decades, with emotional
reinforcement of the most crucial memories. And yet, it happens all too often
that I must point out the same logical fallacy again and again in the same
conversation! Surely, I think, if I point out the mistake in the reasoning, this
will count as an important correction that the brain should immediately make use
of? As it turns out, there seems to be some kind of a fundamental limitation on
how quickly the neural connections can get rewired. Chatting with recent models,
who can make use the extra information immediately, has deteriorated my patience
regarding having to repeat myself.</p>
<h3 id="failure-to-generalize">Failure to generalize</h3>
<p>By this point, it’s possible to explain what happens in a given situation, and
watch the model apply the lessons learned to a similar situation. Not so with
humans. When I point out that the same principles would apply elsewhere, their
response will be somewhere along the spectrum of total bafflement on the one end
and on the other, a face-saving explanation that the comparison doesn’t apply
“because it’s different”. Indeed the whole point of comparisons is to apply same
principles in different situations, so why the excuse? I’ve learned to take up
such discussions with AI and not trouble people with them.</p>
<h3 id="failure-to-apply-to-specific-situation">Failure to apply to specific situation</h3>
<p>This is the opposite issue: given a principle stated in general terms, the
person will not be able to apply it in a specific situation. Indeed, I’ve had a
lifetime of observing this very failure mode in myself: given the laws of
physics, which are typically “obvious” and easy to understand, I find it very
difficult to calculate how long before the next eclipse. More and more, rather
than think these things through myself, I’d just send a quick prompt to the most
recent big model, and receive a good answer in seconds. In other words, models
threaten to sever me not only from other flawed humans, but from my own “slow”
thinking as well!</p>
<h3 id="persistent-hallucination">Persistent hallucination</h3>
<p>Understood in the medical sense, hallucination refers to when something appears
to be real even as you know very well it isn’t. Having no direct insight into
the “inner mental life” of models, we claim that every false fact they spit out
is a form of hallucination. The meaning of the word is shifting from the medical
sense towards the direction of “just being wrong, and persistently so”. This has
plagued human speech for centuries. As a convenient example, look up some heated
debate between proponents of science and those of religion. (As if the two need
be in conflict!) When a model exhibits hallucination, often providing more
context and evidence will dispel it, but the same trick does not appear to work
so well on humans.</p>
<h3 id="conclusion">Conclusion</h3>
<p>Where to go from here? One conclusion is that LLMs are damaging the connection
people feel with each other, much like a decade before social networks
threatened to destroy it by replacing it with a shallower, simulated versions.
Another interpretation would be to conclude cynically that it’s time humans get
either enhanced or replaced by a more powerful form of intelligence. I’d say
we’re not there yet entirely, but that some of the replacement has been effected
already: I’ll never again ask a human to write a computer program shorter than
about a thousand lines, since an LLM will do it better.</p>
<p>Indeed, why am I even writing this? I asked GPT-5 for additional failure modes
and found more additional examples than I could hope to get from a human:</p>
<blockquote>
<p>Beyond the failure modes already discussed, humans also exhibit analogues of
several newer LLM pathologies: conversations often suffer from <strong>instruction
drift,</strong> where the original goal quietly decays as social momentum takes over;
<strong>mode collapse,</strong> in which people fall back on a small set of safe clichés
and conversational templates; and <strong>reward hacking,</strong> where social approval or
harmony is optimized at the expense of truth or usefulness. Humans frequently
<strong>overfit the prompt,</strong> responding to the literal wording rather than the
underlying intent, and display <strong>safety overrefusal,</strong> declining to engage
with reasonable questions to avoid social or reputational risk. Reasoning is
also marked by <strong>inconsistency across turns,</strong> with contradictions going
unnoticed, and by <strong>temperature instability,</strong> where fatigue, emotion, or
audience dramatically alters the quality and style of thought from one moment
to the next.</p>
</blockquote>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US Job Openings Decline to Lowest Level in More Than a Year (261 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2026-01-07/us-job-openings-decline-to-lowest-level-in-more-than-a-year</link>
            <guid>46527533</guid>
            <pubDate>Wed, 07 Jan 2026 15:32:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2026-01-07/us-job-openings-decline-to-lowest-level-in-more-than-a-year">https://www.bloomberg.com/news/articles/2026-01-07/us-job-openings-decline-to-lowest-level-in-more-than-a-year</a>, See on <a href="https://news.ycombinator.com/item?id=46527533">Hacker News</a></p>
Couldn't get https://www.bloomberg.com/news/articles/2026-01-07/us-job-openings-decline-to-lowest-level-in-more-than-a-year: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Shipmap.org (295 pts)]]></title>
            <link>https://www.shipmap.org/</link>
            <guid>46527161</guid>
            <pubDate>Wed, 07 Jan 2026 15:03:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.shipmap.org/">https://www.shipmap.org/</a>, See on <a href="https://news.ycombinator.com/item?id=46527161">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="info-panel-inner">
		<p><i></i> Back to map</p>
		<div>
			<h2>New! Stunning high-res maps for print</h2>
			<p>Due to popular demand the designers of this map, <a href="https://kiln.digital/">Kiln</a>, are now selling stunning high-resolution versions of the world “routes” view. There are two versions available: coloured by ship type over the inky-blue base map; or just the ship in a single colour a transparent background so you can overlay or print onto whatever background colour you like. Contact <a href="https://www.shipmap.org/cdn-cgi/l/email-protection#9ff7faf3f3f0dff4f6f3f1b1fbf6f8f6ebfef3"><span data-cfemail="6b030e0707042b00020705450f020c021f0a07">[email&nbsp;protected]</span></a> for pricing and further information.</p>
			<p><img src="https://www.shipmap.org/images/colored_preview.jpg">
			<img src="https://www.shipmap.org/images/transparent_preview.jpg">
		</p></div>
		<h2>Can I embed this map?</h2>
		<p>Yes. You are welcome to embed this map. Please include a link back to <a href="https://www.kiln.digital/">Kiln</a> somewhere in the text of your article. Use the following embed code for a fully responsive embed that will adjust to the width of your website. Feel free to change the height and/or give it a fixed width if you prefer.</p>
		<h2>What can I see?</h2>
		<p>You can see movements of the global merchant fleet over the course of 2012, overlaid on a bathymetric map. You can also see a few statistics such as a counter for emitted CO<span>2</span> (in thousand tonnes) and maximum freight carried by represented vessels (varying units).</p>
		<h2>What can I do?</h2>
		<p>You can pan and zoom in the usual ways, and skip back and forward in time using the timeline at the bottom of the screen. The controls at the top right let you show and hide different map layers: port names, the background map, routes (a plot of all recorded vessel positions), and the animated ships view. There are also controls for filtering and colouring by vessel type.</p>
		<h2>What the are types of ships shown?</h2>
		<p>The merchant fleet is divided into five categories, each of which has a filter and a CO<span>2</span> and freight counter for the hour shown on the clock. The ship types and units are as follows:</p>
		<ul>
			<li>Container (e.g. manufactured goods): number of container slots equivalent to 20 feet (i.e. a 40-foot container takes two slots)</li>
			<li>Dry bulk (e.g. coal, aggregates): combined weight of cargo, fuel, water, provisions, passengers and crew a vessel can carry, measured in thousand tonnes</li>
			<li>Tanker (e.g. oil, chemicals): same as dry bulk</li>
			<li>Gas bulk (e.g. liquified natural gas): capacity for gases, measured in cubic metres</li>
			<li>Vehicles (e.g. cars): same as dry bulk</li>
		</ul>
		<h2>Why do ships sometimes appear to move across land?</h2>
		<p>In some cases this is because there are ships navigating via canals or rivers that aren’t visible on the map. Generally, though, this effect is an artefact of animating a ship between two recorded positions with missing data between, especially when the positions are separated by a narrow strip of land. We may develop the map to remove this effect in the future.</p>
		<h2>Why are there fewer ships visible in the first part of the year?</h2>
		<p>Unfortunately the data we are using for the map is incomplete for the first few months of the year: roughly January to April.</p>
		<h2>Who created this map?</h2>
		<p>The map was created by <a href="https://www.kiln.digital/">Kiln</a> based on data from the <a href="http://www.bartlett.ucl.ac.uk/energy">UCL Energy Institute</a> (UCL EI)</p>
		<p>Website: Duncan Clark &amp; Robin Houston from Kiln</p>
		<p>Data: Julia Schaumeier &amp; Tristan Smith from the UCL EI</p>
		<p>Music: Bach Goldberg Variations played by <a href="http://www.opengoldbergvariations.org/">Kimiko Ishizaka</a></p>
		<h2>How was the map created?</h2>
		<p>UCL EI took data showing location and speed of ships and cross-checked it with another database to get the vessel characteristics, such as engine type and hull measurements. With this information they were able to compute the CO<span>2</span> emissions for each observed hour, following the approach laid out in the Third IMO Greenhouse Gas Study 2014. Kiln took the resulting dataset and visualized it with WebGL on top of a specially created base map, which shows bathymetry (ocean depth), based on the <a href="http://www.gebco.net/">GEBCO_2014 Grid</a> (version 20150318), as well as continents and major rivers from <a href="http://www.naturalearthdata.com/">Natural Earth</a>.</p>
		<h2>Where did you get the data and who paid?</h2>
		<p>Our data sources for shipping positions are <a href="http://www.exactearth.com/">exactEarth</a> for AIS data (location/speed) and <a href="http://www.clarksons.com/services/research/">Clarksons Research UK World Fleet Register</a> (static vessel information). We are very grateful to our funders, the <a href="http://europeanclimate.org/">European Climate Foundation</a>.</p>
		<h2>I want one too!</h2>
		<p>If you want to have an installation in your foyer, museum, living room, you can contact us at <a href="https://www.shipmap.org/cdn-cgi/l/email-protection#2d45484141426d46444143034459"><span data-cfemail="83ebe6efefecc3e8eaefedadeaf7">[email&nbsp;protected]</span></a>. We’d be happy to work on a bespoke version.</p>
		<p><a id="iiba-link" href="http://www.informationisbeautifulawards.com/showcase/1580-shipmap-org" target="_top"><img id="iiba" src="https://www.shipmap.org/images/iiba.png"></a></p><p><i></i> Back to map</p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meditation as Wakeful Relaxation: Unclenching Smooth Muscle (107 pts)]]></title>
            <link>https://psychotechnology.substack.com/p/meditation-as-wakeful-relaxation</link>
            <guid>46527157</guid>
            <pubDate>Wed, 07 Jan 2026 15:03:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://psychotechnology.substack.com/p/meditation-as-wakeful-relaxation">https://psychotechnology.substack.com/p/meditation-as-wakeful-relaxation</a>, See on <a href="https://news.ycombinator.com/item?id=46527157">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>The frontier of my meditation practice is exploring it as </span><em>wakeful relaxation</em><span>. This is how </span><a href="https://psychotechnology.substack.com/p/my-meditation-teacher-roger-thisdell" rel="">my meditation teacher, Roger Thisdell</a><span>, framed it for me recently. People often treat relaxation and wakefulness as two opposites: relaxation as a drowsy and dull, wakefullnes as sharp and jittery. But the two can co-exist.</span></p><p>Over the last two weeks I’ve been actively trying to relax during meditation. And goddamn it, folks, this is hard. I am constantly spasming in different ways. There is a lot of tension in my body and my experiential field.</p><p>Relaxation is this game of whack-a-mole: relaxing one area of my body causes tension to pop up somewhere else. Proper relaxation requires coordinating mind and body in ways that’s not unlike learning to dance.</p><p><span>The thing is, intentional relaxation brings anxiety and fear. Sometimes it’s about past experiences. Sometimes it’s the stress of publishing daily — I’m currently doing </span><a href="https://www.inkhaven.blog/" rel="">Inkhaven</a><span>, a 30-day writing workshop where you must publish a 500+ words post daily or they kick you out.</span></p><p>To get to each progressively deeper levels of relaxation, I have to be fairly equanimous. That usually means being a bit overwhelmed with emotion. Muscle tension seems to guard against feeling stress. It’s not just “bracing for impact” — something more complex is going on.</p><p>When I manage to relax more completely, something shifts in my experience. I get less reactive and less neurotic — I generate fewer negative “what if” scenarios. Interfacing with people feels less effortful too: e.g. it becomes easier to switch from planning my day to chatting with a friend who walks by.</p><p>It’s almost as if my default stance on reality changes. What’s going on? </p><div id="youtube2-CmFhWUIkPaI" data-attrs="{&quot;videoId&quot;:&quot;CmFhWUIkPaI&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/CmFhWUIkPaI?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><blockquote><p>There’s something in vasocomputation I’ve been calling a “stance” or way of feeling — stances are essentially discrete patterns of muscle tension (primarily vascular tension) that we can jump into and which offer specific affordances. The girl in this video is a great example: she goes from her default stance, to professional-calm, back to default (with a big release of tension)</p><p>Vasocomputation’s basic thesis is “vascular clenches stabilize neural patterns” — patterns of muscle tension will set certain aspects of phenomenology as constants and others as variables. I.e. every stance has a certain internal feeling that defines the stance and as long as you hold this feeling, you “hold frame” — if you lose the feeling (i.e. if this core pattern of stabilizing tension shifts), you break frame.</p><p><a href="https://x.com/johnsonmxe/status/1878648168356954567" rel="">[continued on twitter]</a></p></blockquote><p>Let’s unpack this. </p><p>The whole body is a computer: it’d be wasteful for evolution to only use the brain for computation when other systems could take part too. Muscle tension constrains and stabilises neural patterns. I picture this as some regions of “thought–feeling space” becoming less accessible — like clamping some pages in a book, or putting an overly active dog on a leash in a crowd so it doesn’t run away.</p><p><span>Mike says that this tension is primarily vascular — in the smooth muscle that lines blood vessels. The word </span><em>“primarily” </em><span>means that working with the “regular” skeletal muscle can be productive, but we could do much better by relaxing smooth muscle.  </span><em> </em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!DBOG!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e402c98-ad16-42f8-a0a9-4520df982518_620x892.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!DBOG!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e402c98-ad16-42f8-a0a9-4520df982518_620x892.jpeg 424w, https://substackcdn.com/image/fetch/$s_!DBOG!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e402c98-ad16-42f8-a0a9-4520df982518_620x892.jpeg 848w, https://substackcdn.com/image/fetch/$s_!DBOG!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e402c98-ad16-42f8-a0a9-4520df982518_620x892.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!DBOG!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e402c98-ad16-42f8-a0a9-4520df982518_620x892.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!DBOG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e402c98-ad16-42f8-a0a9-4520df982518_620x892.jpeg" width="620" height="892" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6e402c98-ad16-42f8-a0a9-4520df982518_620x892.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:892,&quot;width&quot;:620,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Image&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="Image" title="Image" srcset="https://substackcdn.com/image/fetch/$s_!DBOG!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e402c98-ad16-42f8-a0a9-4520df982518_620x892.jpeg 424w, https://substackcdn.com/image/fetch/$s_!DBOG!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e402c98-ad16-42f8-a0a9-4520df982518_620x892.jpeg 848w, https://substackcdn.com/image/fetch/$s_!DBOG!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e402c98-ad16-42f8-a0a9-4520df982518_620x892.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!DBOG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e402c98-ad16-42f8-a0a9-4520df982518_620x892.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>All these blood vessels are lined with smooth muscle. So. Much. Muscle.</figcaption></figure></div><p>Smooth muscle works very differently from skeletal muscle — it’s not under direct conscious control. You can consciously decide to flex your bicep, but you can’t decide to constrict your left renal artery.</p><p><span>If Mike is right, then the situation is </span><em>cursed</em><span>: there is a system in the body we have no direct access to, and it heavily influences our conscious experience. </span></p><p>What’s worse is that smooth muscle can also form energetically inexpensive “latches” — patterns of contraction that persist as stable, semi-permanent “knots” in the body’s tissues. A skeletal muscle is (roughly) either “relaxed” or “tensed.” Smooth muscle has three interesting modes: “relaxed,” “tensed,” and “latched.”</p><p><span>Mike’s hypothesis is that smooth-muscle latches can persist long-term — for months and years, depending on which prediction or mode of action they’re stabilising. He outlines this in </span><a href="https://opentheory.net/2023/07/principles-of-vasocomputation-a-unification-of-buddhist-phenomenology-active-inference-and-physical-reflex-part-i/" rel="">Principles of Vasocomputation: A Unification of Buddhist Phenomenology, Active Inference, and Physical Reflex</a><span>:</span></p><blockquote><p>Latches can persist for minutes, hours, days, months, or years (depending on what prediction they’re stabilizing), and the sum total of all latches likely accounts for the majority of bodily suffering. If you are “holding tension in your body” you are subject to the mechanics of the latch-bridge mechanism. Migraines and cluster headaches are almost certainly inappropriate VSMC latches; all hollow organs are surrounded by smooth muscle and can latch.</p></blockquote><p>He also claims that latching for months and years is possible because this tension does not require ongoing energy. So far I haven’t been able to independently verify this strong version of the claim. Textbooks say the latch state requires “low energy consumption,” not “no energy”, and the timescale usually given is “hours,” not years.</p><p>For example:</p><ol><li><p><span>Brant B. Hafen; Bracken Burns in </span><a href="https://www.ncbi.nlm.nih.gov/books/NBK526125" rel="">Physiology, Smooth Muscle</a><span>:</span></p></li></ol><blockquote><p>The mechanism that allows the smooth muscle to maintain high-tension at low energy consumption; termed the latch state</p></blockquote><ol start="2"><li><p><span>Charles Asbury in </span><a href="http://“Muscle Physiology”" rel="">“Muscle Physiology”</a><span>:</span></p></li></ol><blockquote><p>This so-called latch (or latch-bridge) behavior is thought to help smooth muscles maintain luminal pressures (e.g., in the vasculature) more economically than they would otherwise, by allowing them sustain tension over long times while using less ATP. </p></blockquote><ol start="3"><li><p><a href="https://www.studocu.com/row/document/universite-chouaib-doukkali/medecine/guyton-and-hall-textbook-of-medical-physiology-pdfdrive-12/49327254" rel="">Chapter 7: Excitation of Skeletal Muscle - Neuromuscular Transmission (PHYSIO 101)</a></p></li></ol><blockquote><p>Once  smooth muscle  has  developed  full  contraction,  the  amount  of continuing excitation  can  usually  be  reduced  to  far  less than the initial level even though the muscle maintains its full force of contraction. Further, the energy consumed to maintain  contraction  is  often  minuscule,  sometimes  as little  as  1/300  the  energy  required  for  comparable  sustained  skeletal  muscle  contraction.  This  mechanism  is called the “latch” mechanism.</p><p>The importance of the latch mechanism is that it can maintain prolonged tonic contraction in smooth muscle for hours with little use of energy. Little continued excitatory  signal  is  required  from  nerve  fibers  or  hormonal sources</p></blockquote><p><span>Still, from my meditation practice it’s pretty clear that skeletal muscle tension plays an important role. It would be surprising if evolution recruited only one type of muscle for computation without also recruiting the other. So while I don’t yet buy into the full story, it seems highly plausible that latches play </span><em>some</em><span> role in the way Mike describes. Perhaps even if individual latches don’t persist for years, they can unclench and re-clench in similar patterns, with similar effects.</span></p><p><span>EDIT: Mike </span><a href="https://x.com/johnsonmxe/status/1994032854225883603" rel="">posted a great response to these concerns on twitter</a><span> and it makes me more confident in the model.</span></p><p>So how do you actually relax, given all this? Mike gives some advice in “Principles of Vasocomputation”:</p><blockquote><p>Sauna + cold plunges are an effective way to force the clench-release cycle and release latches; likewise, simply taking time to feel your body and put your attention into latched tissues can release them. Psychedelics can force open latches.</p></blockquote><p>I’m not sure why putting attention on a body part would cause unlatching when smooth muscle isn’t under conscious control. Perhaps the mechanism is indirect: you highlight a set of neural patterns in attention, and that in turn changes autonomic and vascular signalling in the associated territory.</p><p>If this works, there must be some way to tune our meditation methods specifically for relaxing smooth muscle. One guess: ultra-slow body-scan Vipassana combined with deliberate clench-release cycles, e.g. contrast showers or sauna + cold, using awareness to track exactly how and where the body grips and lets go.</p><p>This is the direction I intend to take my practice into. If you have other ideas — let me know in the comments.</p><ol><li><p><span data-state="closed"><a href="https://open.substack.com/users/3307116-michael-edward-johnson?utm_source=mentions" target="_blank" rel="noopener" data-attrs="{&quot;name&quot;:&quot;Michael Edward Johnson&quot;,&quot;id&quot;:3307116,&quot;type&quot;:&quot;user&quot;,&quot;url&quot;:null,&quot;photo_url&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/de259395-2e4a-4d9f-a08c-82af8d057c34_1584x1590.jpeg&quot;,&quot;uuid&quot;:&quot;5b0937a6-7d52-4b86-8df5-042a2375ac1a&quot;}" data-component-name="MentionUser">Michael Edward Johnson</a></span><span>: </span><a href="https://opentheory.net/2023/07/principles-of-vasocomputation-a-unification-of-buddhist-phenomenology-active-inference-and-physical-reflex-part-i/" rel="">Principles of Vasocomputation: A Unification of Buddhist Phenomenology, Active Inference, and Physical Reflex</a></p></li><li><p><span>Mike Johnson: </span><a href="https://x.com/johnsonmxe/status/1751994506478587977" rel="">In the beginning there was sensation</a></p></li><li><p><span>Mike Johnson: </span><a href="https://x.com/johnsonmxe/status/1863595299056517410" rel="">a video recording of a talk on vasomputation</a></p></li><li><p><span>Mike Johnson: </span><a href="https://x.com/johnsonmxe/status/1903532040278224951" rel="">Vasocomputation suggests that imagination is essentially tension-based</a></p></li><li><p><span>Hinterlander: </span><a href="https://x.com/yoltartar/status/1887930438322405402" rel="">meditation as mental stretching</a></p></li><li><p><span>Mike Johnson: </span><a href="https://x.com/johnsonmxe/status/1887946660728480099" rel="">The idea that sports physiology has a lot to say about how to train ‘mental muscles’ (which are literal muscles) seems super generative</a></p></li></ol></div></article></div><div><div id="discussion"><h4>Discussion about this post</h4></div><div><h3>Ready for more?</h3></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LaTeX Coffee Stains [pdf] (2021) (237 pts)]]></title>
            <link>https://ctan.math.illinois.edu/graphics/pgf/contrib/coffeestains/coffeestains-en.pdf</link>
            <guid>46526933</guid>
            <pubDate>Wed, 07 Jan 2026 14:46:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ctan.math.illinois.edu/graphics/pgf/contrib/coffeestains/coffeestains-en.pdf">https://ctan.math.illinois.edu/graphics/pgf/contrib/coffeestains/coffeestains-en.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=46526933">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Sugar industry influenced researchers and blamed fat for CVD (451 pts)]]></title>
            <link>https://www.ucsf.edu/news/2016/09/404081/sugar-papers-reveal-industry-role-shifting-national-heart-disease-focus</link>
            <guid>46526740</guid>
            <pubDate>Wed, 07 Jan 2026 14:29:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ucsf.edu/news/2016/09/404081/sugar-papers-reveal-industry-role-shifting-national-heart-disease-focus">https://www.ucsf.edu/news/2016/09/404081/sugar-papers-reveal-industry-role-shifting-national-heart-disease-focus</a>, See on <a href="https://news.ycombinator.com/item?id=46526740">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>A newly discovered cache of industry documents revealed that the sugar industry began working closely with nutrition scientists in the mid-1960s to single out fat and cholesterol as the dietary causes of coronary heart disease and to downplay evidence that sucrose consumption was also a risk factor.</p>

<p>An analysis of those papers by researchers at UC San Francisco appears Sept. 12, 2016, in <em>JAMA Internal Medicine. </em></p>

<p>The internal industry documents, which were found in public archives, showed that a sugar industry trade organization recognized as early as 1954 that if Americans adopted low-fat diets, then per-capita consumption of sucrose would increase by more than one-third. The trade organization represented 30 international members.</p>

<p>Meanwhile, evidence linking sugar consumption to high blood cholesterol and triglyceride levels – both thought to be risk factors for coronary heart disease – began to emerge in the scientific literature and popular press.</p>

<h2>Literature Shaped Public Opinion</h2>

<p>After a 1965 spike in media attention to the heart disease risks of sucrose, the sugar industry commissioned Project 226, a literature review written by researchers at the Harvard University School of Public Health Nutrition Department, which was published in the highly respected <em>New England Journal of</em> <em>Medicine </em>(NEJM) in 1967. It concluded there was “no doubt” that the only dietary intervention required to prevent coronary heart disease was to reduce dietary cholesterol and substitute polyunsaturated fat for saturated fat in the American diet.</p><p>


Cristin Kearns, DDS, MBA


</p><p>“The literature review helped shape not only public opinion on what causes heart problems but also the scientific community’s view of how to evaluate dietary risk factors for heart disease,” said lead author Cristin Kearns, DDS, MBA, who discovered the industry documents.</p>

<p>The UCSF researchers analyzed more than 340 documents, totaling 1,582 pages of text, between the sugar industry and two individuals: Roger Adams, then a professor of organic chemistry who served on scientific advisory boards for the sugar industry; and D. Mark Hegsted, one of the Harvard researchers who produced the literature review.</p>

<p>To conduct the literature review, the sugar industry paid the Harvard scientists the equivalent of $50,000 in 2016 dollars, then set the review’s objective, contributed articles to be included, and received drafts. Yet the industry’s funding and role were not disclosed in the final NEJM publication.</p>

<p>The literature review heavily criticized studies linking sucrose to heart disease, while ignoring limitations of studies investigating dietary fats. The review argued that blood cholesterol levels were the only significant risk factor for coronary heart disease, which made the high sucrose content of the American diet seem less hazardous than if blood triglycerides were also considered to be a risk factor.</p>

<h2>Need for More Transparent Scientific Reviews</h2><p>


Stanton A. Glantz, PhD


</p><p>The authors emphasized that this analysis demonstrates the importance of having scientific reviews written by people without conflicts of interest, as well as the need for financial disclosure in nutrition science.</p>

<p>“As the saying goes, he who pays the piper calls the tune,” said senior author Stanton A. Glantz, PhD, UCSF professor of medicine and director of the UCSF Center for Tobacco Control Research and Education. “There are all kinds of ways that you can subtly manipulate the outcome of a study, which industry is very well practiced at.”</p>

<p>Co-author Laura Schmidt, PhD, who is also principal investigator on the UCSF-led SugarScience initiative, noted that after decades of focusing on saturated fat as the dietary culprit in heart disease, the science is building around sugar’s role, but health policy has only just begun to catch up.</p><p>


Laura Schmidt, PhD


</p><p>“There is now a considerable body of evidence linking added sugars to hypertension and cardiovascular disease, which is the No. 1 cause of premature death in the developed world,” Schmidt said. “Yet, health policy documents are still inconsistent in citing heart disease risk as a health consequence of added sugars consumption.”</p>

<p>The study was funded by the UCSF Philip R. Lee Institute for Health Policy Studies; a donation by the Hellmann Family Fund to the UCSF Center for Tobacco Control Research and Education; the UCSF School of Dentistry Department of Orofacial Sciences and Global Oral Health Program; and grants from the National Institute of Dental and Craniofacial Research and the National Cancer Institute.</p>

<p><em>UCSF&nbsp;is a leading university dedicated to promoting health worldwide through advanced biomedical research, graduate-level education in the life sciences and health professions, and excellence in patient care. It includes top-ranked graduate schools of dentistry, medicine, nursing and pharmacy; a graduate division with nationally renowned programs in basic, biomedical, translational and population sciences; and a preeminent biomedical research enterprise. It also includes UCSF Health, which comprises two top-ranked hospitals,&nbsp;UCSF Medical Center&nbsp;and&nbsp;UCSF Benioff Children’s Hospital San Francisco, and other partner and affiliated hospitals and healthcare providers throughout the Bay Area.</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Everyone hates OneDrive, Microsofts cloud app that steals and deletes files (165 pts)]]></title>
            <link>https://boingboing.net/2026/01/05/everyone-hates-onedrive-microsofts-cloud-app-that-steals-then-deletes-all-your-files.html</link>
            <guid>46526376</guid>
            <pubDate>Wed, 07 Jan 2026 13:54:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://boingboing.net/2026/01/05/everyone-hates-onedrive-microsofts-cloud-app-that-steals-then-deletes-all-your-files.html">https://boingboing.net/2026/01/05/everyone-hates-onedrive-microsofts-cloud-app-that-steals-then-deletes-all-your-files.html</a>, See on <a href="https://news.ycombinator.com/item?id=46526376">Hacker News</a></p>
Couldn't get https://boingboing.net/2026/01/05/everyone-hates-onedrive-microsofts-cloud-app-that-steals-then-deletes-all-your-files.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[A4 Paper Stories (244 pts)]]></title>
            <link>https://susam.net/a4-paper-stories.html</link>
            <guid>46525888</guid>
            <pubDate>Wed, 07 Jan 2026 12:54:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://susam.net/a4-paper-stories.html">https://susam.net/a4-paper-stories.html</a>, See on <a href="https://news.ycombinator.com/item?id=46525888">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>By <b>Susam Pal</b> on 06 Jan 2026</p>
<p>
  I sometimes resort to a rather common measuring technique that is
  neither fast, nor accurate, nor recommended by any standards body
  and yet it hasn't failed me whenever I have had to use it.  I will
  describe it here, though calling it a technique might be overselling
  it.  Please do not use it for installing kitchen cabinets or
  anything that will stare back at you every day for the next ten
  years. It involves one tool: a sheet of A4 paper.
</p>
<p>
  Like most sensible people with a reasonable sense of priorities, I
  do not carry a ruler with me wherever I go.  Nevertheless, I often
  find myself needing to measure something at short notice, usually in
  situations where a certain amount of inaccuracy is entirely
  forgivable.  When I cannot easily fetch a ruler, I end up doing what
  many people do and reach for the next best thing, which for me is a
  sheet of A4 paper, available in abundant supply where I live.
</p>
<p>
  From photocopying night-sky charts to serving as a scratch pad for
  working through mathematical proofs, A4 paper has been a trusted
  companion since my childhood days.  I use it often.  If I am
  carrying a bag, there is almost always some A4 paper inside: perhaps
  a printed research paper or a mathematical problem I have worked on
  recently and need to chew on a bit more during my next train ride.
</p>
<h2 id="dimensions">Dimensions<a href="#dimensions"></a></h2>
<p>
  The dimensions of A4 paper are the solution to a simple, elegant
  problem.  Imagine designing a sheet of paper such that, when you cut
  it in half parallel to its shorter side, both halves have exactly
  the same aspect ratio as the original.  In other words, if the
  shorter side has length \( x \) and the longer side has length \( y
 , \) then

  \[
    \frac{y}{x} = \frac{x}{y / 2}
  \]

  which gives us

  \[
    \frac{y}{x} = \sqrt{2}.
  \]

  Test it out.  Suppose we have \( y/x = \sqrt{2}.  \)  We cut the
  paper in half parallel to the shorter side to get two halves, each
  with shorter side \( x' = y / 2 = x \sqrt{2} / 2 = x / \sqrt{2} \)
  and longer side \( y' = x.  \)  Then indeed

  \[
    \frac{y'}{x'}
    = \frac{x}{x / \sqrt{2}}
    = \sqrt{2}.
  \]

  In fact, we can keep cutting the halves like this and we'll keep
  getting even smaller sheets with the aspect ratio \( \sqrt{2} \)
  intact.  To summarise, when a sheet of paper has the aspect ratio \(
  \sqrt{2}, \) bisecting it parallel to the shorter side leaves us
  with two halves that preserve the aspect ratio.  A4 paper has this
  property.
</p>
<p>
  But what are the exact dimensions of A4 and why is it called A4?
  What does 4 mean here?  Like most good answers, this one too begins
  by considering the numbers \( 0 \) and \( 1.  \)  Let me elaborate.
</p>
<p>
  Let us say we want to make a sheet of paper that is \( 1 \,
  \mathrm{m}^2 \) in area and has the aspect-ratio-preserving property
  that we just discussed.  What should its dimensions be?  We want

  \[
    xy = 1 \, \mathrm{m}^2
  \]

  subject to the condition

  \[
    \frac{y}{x} = \sqrt{2}.
  \]

  Solving these two equations gives us

  \[
    x^2 = \frac{1}{\sqrt{2}} \, \mathrm{m}^2
  \]

  from which we obtain

  \[
    x = \frac{1}{\sqrt[4]{2}} \, \mathrm{m}, \quad
    y = \sqrt[4]{2} \, \mathrm{m}.
  \]

  Up to three decimal places, this amounts to

  \[
    x = 0.841 \, \mathrm{m}, \quad
    y = 1.189 \, \mathrm{m}.
  \]

  These are the dimensions of A0 paper.  It is quite large to scribble
  mathematical solutions on, unless your goal is to make a spectacle
  of yourself and cause your friends and family to reassess your
  sanity.  So we need something smaller that allows us to work in
  peace, without inviting commentary or concerns from passersby.  We
  take the A0 paper of size

  \[
    84.1 \, \mathrm{cm} \times 118.9 \, \mathrm{cm}
  \]

  and bisect it to get A1 paper of size

  \[
    59.4 \, \mathrm{cm} \times 84.1 \, \mathrm{cm}.
  \]

  Then we bisect it again to get A2 paper with dimensions

  \[
    42.0 \, \mathrm{cm} \times 59.4 \, \mathrm{cm}.
  \]

  And once again to get A3 paper with dimensions

  \[
    29.7 \, \mathrm{cm} \times 42.0 \, \mathrm{cm}.
  \]

  And then once again to get A4 paper with dimensions

  \[
    21.0 \, \mathrm{cm} \times 29.7 \, \mathrm{cm}.
  \]

  There we have it.  The dimensions of A4.  These numbers are etched
  in my memory like the multiplication table of \( 1.  \)  We can keep
  going further to get A5, A6, etc.  We could, in theory, go all the
  way up to A\( \infty.  \)  Hold on, I think I hear someone heckle.
  What's that?  Oh, we can't go all the way to A\( \infty?  \)
  Something about atoms, was it?  Hmm.  Security!  Where's security?
  Ah yes, thank you, sir.  Please show this gentleman out, would you?
</p>
<p>
  Sorry for the interruption, ladies and gentlemen.  Phew!  That
  fellow!  Atoms?  Honestly.  We, the mathematically inclined, are not
  particularly concerned with such trivial limitations.  We drink our
  tea from doughnuts.  We are not going to let the size of atoms
  dictate matters, now are we?
</p>
<p>
  So I was saying that we can bisect our paper like this and go all
  the way to A\( \infty.  \)  That reminds me.  Last night I was at a
  bar in Hoxton and I saw an infinite number of mathematicians walk
  in.  The first one asked, "Sorry to bother you, but would it be
  possible to have a sheet of A0 paper?  I just need something to
  scribble a few equations on."  The second one asked, "If you happen
  to have one spare, could I please have an A1 sheet?"  The third one
  said, "An A2 would be perfectly fine for me, thank you."  Before the
  fourth one could ask, the bartender disappeared into the back for a
  moment and emerged with two sheets of A0 paper and said, "Right.
  That should do it.  Do know your limits and split these between
  yourselves."
</p>
<p>
  In general, a sheet of A\( n \) paper has the dimensions

  \[
    2^{-(2n + 1)/4} \, \mathrm{m} \times
    2^{-(2n - 1)/4} \, \mathrm{m}.
  \]

  If we plug in \( n = 4, \) we indeed get the dimensions of A4 paper:

  \[
    0.210 \, \mathrm{m} \times 0.297 \, \mathrm{m}.
  \]
</p>
<h2 id="measuring-stuff">Measuring Stuff<a href="#measuring-stuff"></a></h2>
<p>
  Let us now return to the business of measuring things.  As I
  mentioned earlier, the dimensions of A4 are lodged firmly into my
  memory.  Getting hold of a sheet of A4 paper is rarely a challenge
  where I live.  I have accumulated a number of A4 paper stories over
  the years.  Let me share a recent one.  I was hanging out with a few
  folks of the nerd variety one afternoon when the conversation
  drifted, as it sometimes does, to a nearby computer monitor that
  happened to be turned off.  At some point, someone confidently
  declared that the screen in front of us was 27 inches.  That sounded
  plausible but we wanted to confirm it.  So I reached for my trusted
  measuring instrument: an A4 sheet of paper.  What followed was
  neither fast, nor especially precise, but it was more than adequate
  for settling the matter at hand.
</p>
<p>
  I lined up the longer edge of the A4 sheet with the width of the
  monitor.  One length.  Then I repositioned it and measured a second
  length.  The screen was still sticking out slightly at the end.  By
  eye, drawing on an entirely unjustified confidence built from years
  of measuring things that never needed measuring, I estimated the
  remaining bit at about \( 1 \, \mathrm{cm}.  \)  That gives us a
  width of

  \[
    29.7 \, \mathrm{cm} +
    29.7 \, \mathrm{cm} +
     1.0 \, \mathrm{cm}
    =
    60.4 \, \mathrm{cm}.
  \]

  Let us round that down to \( 60 \, \mathrm{cm}.  \)  For the height,
  I switched to the shorter edge.  One full \( 21 \, \mathrm{cm} \)
  fit easily.  For the remainder, I folded the paper parallel to the
  shorter side, producing an A5-sized rectangle with dimensions \(
  14.8 \, \mathrm{cm} \times 21.0 \, \mathrm{cm}.  \)  Using the \(
  14.8 \, \mathrm{cm} \) edge, I discovered that it overshot the top
  of the screen slightly.  Again, by eye, I estimated the excess at
  around \( 2 \, \mathrm{cm}.  \)  That gives us

  \[
    21.0 \, \mathrm{cm} +
    14.8 \, \mathrm{cm}
    -2.0 \, \mathrm{cm}
    =
    33.8 \, \mathrm{cm}.
  \]

  Let us round this up to \( 34 \, \mathrm{cm}.  \)  The ratio \( 60 /
  34 \approx 1.76 \) is quite close to \( 16/9, \) a popular aspect
  ratio of modern displays.  At this point the measurements were
  looking good.  So far, the paper had not embarrassed itself.
  Invoking the wisdom of the Pythagoreans, we can now estimate the
  diagonal as

  \[
    \sqrt{(60 \, \mathrm{cm})^2 + (34 \, \mathrm{cm})^2}
    \approx 68.9 \,\mathrm{cm}.
  \]

  Finally, there is the small matter of units.  One inch is \( 2.54 \,
  \mathrm{cm}, \) another figure that has embedded itself in my head.
  Dividing \( 68.9 \) by \( 2.54 \) gives us roughly \( 27.2 \,
  \mathrm{in}.  \)  So yes.  It was indeed a \( 27 \)-inch display.  My
  elaborate exercise in showing off my A4 paper skills was now
  complete.  Nobody said anything.  A few people looked away in
  silence.  I assumed they were reflecting.  I am sure they were
  impressed deep down.  Or perhaps... no, no.  They were definitely
  impressed.  I am sure.
</p>
<p>
  Hold on.  I think I hear another heckle.  What is that?  There are
  mobile phone apps that can measure things now?  Really?  Right.
  Security.  Where's security?
</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[“Stop Designing Languages. Write Libraries Instead” (2016) (208 pts)]]></title>
            <link>https://lbstanza.org/purpose_of_programming_languages.html</link>
            <guid>46525640</guid>
            <pubDate>Wed, 07 Jan 2026 12:29:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lbstanza.org/purpose_of_programming_languages.html">https://lbstanza.org/purpose_of_programming_languages.html</a>, See on <a href="https://news.ycombinator.com/item?id=46525640">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <h2 id="anchor112">"Stop Designing Languages. Write Libraries Instead."</h2><p>Patrick S. Li - May 29, 2016</p><p>I had a friend tell me recently that all programming languages seem very similar to each other. They all have variables, and arrays, a few loop constructs, functions, and some arithmetic constructs. Sure, some languages have fancier features like first-class functions or coroutines, but he doesn't consider himself an expert programmer anyway and doesn't use those features.</p><p>What <span>really</span> makes a programming language productive for him, he says, are the libraries it comes with. For example, he got into programming by using the popular <span>Ruby on Rails</span> web framework. There is no way that he could have written a full database-driven web stack by himself, nor is he interested in doing so. But thanks to Ruby on Rails, he doesn't have to! So he said that he has no particular opinion about the Ruby programming language, but he absolutely loves Rails. The vast majority of programmers are non-experts, like himself, and the largest gains in productivity for non-experts come from having a wide spectrum of easy-to-use libraries. Subtle language features like first-class functions, and object systems, are lost on them because they don't really use them anyway. Computer scientists should really be spending their time developing new libraries rather than inventing new programming languages.</p><p>My friend's opinion about programming languages is a common one, and I have heard it repeatedly from experts and non-experts alike. Being a language designer myself, I, of course, don't share this opinion. Here is what I consider to be the purpose of a general-purpose programming language.</p><p>To start off, I would say that my friend's opinion is completely correct, just incomplete. The greatest productivity gains <span>are</span> indeed the result of having a wide spectrum of libraries. Ruby on Rails is a fantastic framework, and it has enabled thousands (if not millions) of non-experts to build sophisticated websites quickly. So the natural question then is, why isn't there now a Rails framework for every programming language? </p><p>Some languages that are semantically similar to Ruby do have their own web frameworks. Python, for example, has Django. But as of now, there is <span>still</span> no decent web framework for Java that is as easy to use as Ruby on Rails. Why is that? Are Java developers just not as competent as Ruby programmers? If David Hansson could design and develop Rails by himself, why can't a group of programmers just copy the design to Java? What makes this even more embarrassing is the fact that Java initially marketed itself as <span>the</span> web programming language, because of its applet technology. To emphasize this point, let me add that there is no good web framework for C either, and it is unlikely that there ever will be. Let me assure you that it's not because C programmers are worse than Ruby programmers. </p><p>Economics is not the reason either. The Tiobe index lists Java and C as the most widely used programming languages today, with Ruby coming in eighth place. There are <span>many times</span> more Java and C programmers than there are Ruby programmers. If someone would just write Java on Rails their framework would have many times more users than Ruby on Rails, and it would instantly propel him to internet fame and fortune. </p><p>So it's not because of incompetency. Nor is it because of economics. So why else wouldn't someone port Ruby on Rails to Java? Well, simply, because they <span>can't</span>.</p><p>If you're a knowledgeable Ruby programmer and you take a deep look through an introductory Rails tutorial, you'll notice that pretty much all of the Ruby language features come into play in some way. Rail's ActiveRecords library makes pervasive use of Ruby's meta-programming features. Rail's template system heavily relies upon Ruby's runtime evaluation features. To make your website respond to a user click, you subclass <code>ApplicationController</code> and reuse pre-coded functionality by importing various <span>mixins</span>. Events are handled often by attaching a call back in the form of a <span>first-class function</span> to some widget. Casual website designers can safely completely ignore the concepts of types and memory deallocation because Ruby is dynamically-typed and garbage-collected. These features are simply not available in all other languages. Java's meta-programming features, for example, are just not powerful enough to implement a system like ActiveRecords. Rails is only possible <span>because</span> of Ruby.</p><p>So, completely unbeknownst to my friend, he is actually making <span>heavy</span> use of all those subtle language features that he claimed he never cared about. And this is intentional! Ruby on Rails was <span>designed</span> to make it possible to build websites without understanding type theory, or memory management, or object-oriented design patterns. Rails allow website designers to focus on designing websites, not managing their software infrastructure. My friend is enjoying all the benefits of Ruby without even knowing it, and that's the whole point.</p><p>Taking a step back, the concept of packaging code into easy-to-use libraries is not new. It's been around even in the days when programs were stored on punched paper tape. There are still vast libraries of assembly code containing useful subroutines. And <span>every</span> programming language ever designed provided some way for common functionality to be reused. To me, this is the primary purpose of a general-purpose programming language, to <span>enable</span> the creation of a wide spectrum of easy-to-use libraries. </p><p>The design of the programming language directly determines what sort of libraries you can write and how easy they are to use in the end. In the C language, the only major feature provided for enabling reuse is the ability to declare and call functions. So guess what? The majority of C libraries are basically large collections of functions. Ruby on Rails provides a concise way for expressing: <span>do this</span> when the button is clicked. The "<span>do this</span>" part is implemented in Ruby as a first-class function. How would it be implemented in languages like Java which don't support them? Well, the behaviour of first-class functions can be mocked by defining a new event handler class with a single <code>perform_action</code> method and then passing an instance of this class to the button object. So guess what? Using a Java library typically entails declaring a humongous number of handler classes. The programming language directly shapes the design of its libraries.</p><p>In the early days of software, collections of functions were sufficient in allowing us to code reusable components. A lot of early software was numerical in nature, and there was a library function for every numerical algorithm you would want to run. Numbers go in. Numbers come out. Functions were perfectly adequate for this. Unix and C were also designed in a time when the majority of computing happens in <span>batch mode</span>. You prepare some input data, call a function or run a program, and you get some output data back. But computing has changed radically since the 70's. Nowadays, most interesting programs are <span>interactive</span>. When a user clicks a button, it should <span>do something</span>. It was rare to want to extend the functionality of a library of the 70's. The library provides a collection of useful functions. If one of them does what you want, then use it. If not, then write your own. But with the advent of interactive software, the need for extensible libraries became apparent. Programmers wanted GUI libraries that allowed them to say: when a user clicks a button, please run <span>my</span> code. Java (and C++) provides a limited method for extending an existing library's functionality through its subclassing mechanism. So using a Java library often consists of subclassing a number of magical classes and then <span>overriding</span> a number of magical methods. This style of library became so pervasive at one point that we even gave them a new name. They're called <span>frameworks</span>.</p><p>I surmise that probably many general purpose programming languages were originally designed because of the author's inability to write a good library for the language that he was using at the time. The initial impetus that got me thinking about designing Stanza, for example, came out of my frustrations with trying to write an easy-to-use game programming library in Java. To handle concurrency, traditional game programming frameworks required sprite behaviours to be programmed using a state machine model. But that's not how we intuitively think about sprites in our heads. Intuitively, we think about a character's behaviour as consisting of a sequence of steps. For example, first the character jumps, and then after he lands he looks to his left and then his right for the nearest enemy. If he sees one then he goes to attack it, otherwise he jumps again. He does this three times, and if he doesn't see an enemy after three jumps, then he takes a short nap. Transforming this sequence of steps into a state machine is an incredibly tedious and error-prone process, and most importantly, feels <span>repetitive</span>. It felt like I was doing the same thing again and again. So the natural question is, can I just make this state machine transformation a library and re-use it? It turns out I couldn't, not in Java at least. The language feature that I needed was some sort of coroutine or continuation mechanism. After some research I found that the Scheme language supports continuations, so the Scheme version of my game programming library was much easier to use than the Java version. </p><p>Because of its support for continuations, the Scheme version of my game library does not require users to write their sprite behaviour as state machines. But it wasn't better than the Java version in <span>every</span> way. Most importantly, the Java version was statically typed and so the compiler automatically caught many of your mistakes for you. The Scheme version didn't have this ability and thus debugging my games took a bit longer. At this point, the right question to ask would be, well can you write a static-typing library for Scheme that then automatically checks your code for type errors? And the current answer, for now and for the foreseeable future, is <span>no</span>. No mainstream language today allows you to write a library to extend its type system. Stanza doesn't either. It just attempts to provide one that is useful for a wider audience. </p><p>Since the purpose of general-purpose programming languages are to enable the creation of powerful libraries, this means that different languages can also be characterized by what features they provide that <span>cannot</span> be written as libraries. Stanza provides an optional type system, garbage collection, and a multimethod based object system. But if you don't like Stanza's object system, there is no way to write your own. This is one of the main directions of programming language research. Can we design a language so expressive that library writers can easily write the most appropriate object system, or most appropriate type system, to fit their application? Perhaps one day we'll have such a language. Racket and Shen provide mechanisms for extending their type systems and research on meta-object protocols were attempts at designing extensible object systems. So languages are differentiated by what types of libraries you can write in them and what types of libraries you can't. </p><p>In summary, the purpose of a general-purpose programming language is to enable the creation of powerful and easy-to-use libraries. The more powerful the language, the easier the libraries are to use. Code that makes use of a perfectly tuned library should read almost like a set of instructions for a coworker. So the next time you come across a particularly elegant library, know that many decades of language research has gone into making that possible. If you're curious about specifically which language features a library makes use of, then you can dig deeper, explore, and appreciate the thought that went into its implementation. If you're not curious about all this subtle language stuff, you can safely ignore it all and get on with your work. That's the whole point. </p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Firefox extension to redirect x.com to xcancel.com (222 pts)]]></title>
            <link>https://addons.mozilla.org/en-US/firefox/addon/toxcancel/</link>
            <guid>46524873</guid>
            <pubDate>Wed, 07 Jan 2026 10:53:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://addons.mozilla.org/en-US/firefox/addon/toxcancel/">https://addons.mozilla.org/en-US/firefox/addon/toxcancel/</a>, See on <a href="https://news.ycombinator.com/item?id=46524873">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-site-identifier="2855005"><header><p><img alt="Preview of ToXCancel" src="https://addons.mozilla.org/user-media/addon_icons/2855/2855005-64.png?modified=334c3eca"></p><div><h2>ToXCancel</h2><p>Redirects to xcancel.com (a mirror of x) when the browser is about to load an x.com page</p></div><div><p><span><span>Available on Firefox for Android™</span></span><span>Available on Firefox for Android™</span></p><p><span><span>2,006 Users</span></span><span>2,006 Users</span></p></div></header><div><section><header><p>About this extension</p></header><div><p>Redirects your browser to <a href="https://prod.outgoing.prod.webservices.mozgcp.net/v1/0cd69f14b59425102ab68244cc2a5502f44c56594fdb5e5c8c8069e3190682de/http%3A//xcancel.com" rel="nofollow">xcancel.com</a> when you are about to land on <a href="https://prod.outgoing.prod.webservices.mozgcp.net/v1/bbbd0abd56d9c27b91509f0839c7f12e3dadb8352ac0ee3d9210043cba246fe1/http%3A//x.com" rel="nofollow">x.com</a> or <a href="https://prod.outgoing.prod.webservices.mozgcp.net/v1/89048a1de851fa717a0c54daa357686619b6fb0d8f43598efc43a0e6aef42dcb/http%3A//twitter.com" rel="nofollow">twitter.com</a>, making it possible to read a thread or to browse the replies to a tweet without having to have an x account.</p></div></section><section><header><p>Rated 4.8 by 17 reviewers</p></header></section></div><section><header><p>Permissions and data</p></header><div><h4>Optional permissions:</h4><ul><li><span>Access your data for sites in the x.com domain</span></li><li><span>Access your data for sites in the twitter.com domain</span></li></ul></div></section><section><header><p>More information</p></header><div><dl><p><dt>Version</dt><dd>1.3</dd></p><p><dt>Size</dt><dd>63.52 KB</dd></p><p><dt>Last updated</dt><dd>8 months ago (May 11, 2025)</dd></p><p><dt>License</dt><dd><a href="https://spdx.org/licenses/GPL-3.0-only.html">GNU General Public License v3.0 only</a></dd></p><p><dt>Add to collection</dt><dd></dd></p></dl></div></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bill to Eliminate H-1B Visa Program Introduced in Congress (126 pts)]]></title>
            <link>https://www.newsweek.com/eliminate-h1b-visa-program-congress-bill-marjorie-taylor-greene-11312655</link>
            <guid>46523029</guid>
            <pubDate>Wed, 07 Jan 2026 05:44:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.newsweek.com/eliminate-h1b-visa-program-congress-bill-marjorie-taylor-greene-11312655">https://www.newsweek.com/eliminate-h1b-visa-program-congress-bill-marjorie-taylor-greene-11312655</a>, See on <a href="https://news.ycombinator.com/item?id=46523029">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>A bill that would "eliminate the H-1B program," which helps U.S. companies hire highly skilled foreign workers, was among Representative <a href="https://www.newsweek.com/marjorie-taylor-greene-floats-bill-to-end-h-1b-visas-11045491" target="_blank" rel="noreferrer noopener">Marjorie Taylor Greene</a>'s final two final pieces of proposed legislation, new records show.</p><p>The Republican of Georgia introduced the bill and another to mandate "photo identification and proof of United States citizenship to vote in Federal elections" on Friday.</p><p>She served her final day in Congress for Georgia's 14th district on Monday, following her a <a href="https://www.newsweek.com/donald-trump-officially-breaks-with-lunatic-marjorie-taylor-greene-11052628">high-profile break with President Donald Trump</a> over issues such as health care, the release of the Jeffrey Epstein files and foreign policy.</p><div><p><img id="11312824" alt="" caption="Representative Marjorie Taylor Greene, the Republican of Georgia, speaks during a hearing with the House Committee on Homeland Security in the Cannon House Office Building on December 11, 2025 in Washington, D.C., and stock image showing a  USCIS letter on an American flag." credit="Getty Images/Anna Moneymaker" sourcealt="" sources="[]" fetchpriority="auto" loading="lazy" width="3000" height="2000" decoding="async" data-nimg="1" sizes="(min-width: 1200px) 1536px, (min-width: 768px) 768px, 100vw" srcset="https://assets.newsweek.com/wp-content/uploads/2026/01/MTG-and-H1B-visa.jpg?w=640&amp;quality=80&amp;webp=1 640w, https://assets.newsweek.com/wp-content/uploads/2026/01/MTG-and-H1B-visa.jpg?w=750&amp;quality=80&amp;webp=1 750w, https://assets.newsweek.com/wp-content/uploads/2026/01/MTG-and-H1B-visa.jpg?w=1000&amp;quality=80&amp;webp=1 1000w, https://assets.newsweek.com/wp-content/uploads/2026/01/MTG-and-H1B-visa.jpg?w=1200&amp;quality=80&amp;webp=1 1200w, https://assets.newsweek.com/wp-content/uploads/2026/01/MTG-and-H1B-visa.jpg?w=1360&amp;quality=80&amp;webp=1 1360w, https://assets.newsweek.com/wp-content/uploads/2026/01/MTG-and-H1B-visa.jpg?w=1600&amp;quality=80&amp;webp=1 1600w" src="https://assets.newsweek.com/wp-content/uploads/2026/01/MTG-and-H1B-visa.jpg?w=1600&amp;quality=80&amp;webp=1"></p></div><h2><strong>Why It Matters</strong></h2><p>The H1-B visa program has been under heavy scrutiny during President Trump's second term.</p><p>In September, Trump ordered a controversial $100,000 application fee for H-1B visas, <a href="https://www.newsweek.com/trump-admin-100000-dollar-h1b-visas-fee-appeal-11289142">sparking ongoing legal challenges</a>. In court papers, Trump's administration argued the substantial increase is necessary to protect American jobs and fund border security, while critics—including some business groups, educators and state governments—warned it would stifle innovation, competitiveness, and specialized fields like artificial intelligence, cybersecurity and medicine.</p><p>Greene's bill goes one step further than the government's position, seeking to end the program entirely.</p><p>The clash brings to the forefront the challenges Trump faces as he seeks to balance economic pragmatism with the “America First” sentiments of his political base, which Greene has argued are being ignored by Trump's administration.</p><h2><strong>What To Know</strong></h2><p>The H-1B visa program allows employers to sponsor foreign professionals for jobs that require specialized skills, most commonly in technology, engineering, and science.</p><p>Each year, the government issues 85,000 new H-1B visas, including 20,000 reserved for workers with advanced degrees from U.S. universities. Demand for the visas regularly exceeds supply, triggering an annual lottery. Indian nationals make up a large share of recipients.</p><p>Greene first touted a H-1B program ban in September, writing on X: "I am introducing a bill to END the mass replacement of American workers by aggressively phasing out the H1B program. Big Tech, AI giants, hospitals, and industries across the board have abused the H-1B system to cut out our own people. Americans are the most talented people in the world, and I have full faith in the American people. I serve Americans only, and I will ALWAYS put Americans first."</p><p>Representative Beth Van Duyne, the Republican of Texas, last month also <a href="https://www.newsweek.com/h1b-visas-banned-republican-proposal-11226290">called for a total ban on the visas</a> as part of a broader push to overhaul the U.S. immigration system, telling conservative commentator Benny Johnson politicians had overlooked the “unintended consequences” of programs like H-1B.</p><p>Meanwhile, a federal judge has ruled that the Trump administration may proceed with imposing a $100,000 fee on new H1B visa applications. The U.S. Chamber of Commerce, which brought a legal challenge over concerns the fee would disrupt the framework established by Congress and impose high costs on U.S. businesses, last week launched an appeal over the ruling.</p><p>"These harms to American&nbsp;<a href="https://www.newsweek.com/topic/businesses">businesses</a>&nbsp;will also be a boon to America’s economic rivals, who will surely welcome the talent no longer able to accept work in the United States," the business group said in its initial October lawsuit. "That is a competitive edge that foreign employers might never cede back."</p><p>Separately, a coalition of 20 states, spearheaded by Washington Attorney General Nick Brown and California and Massachusetts' Attorneys General, challenged the fee’s legality under the Administrative Procedure Act (APA), arguing the required notice-and-comment process was bypassed, and the fee was arbitrarily set unrelated to administrative costs.</p><p>The U.S. Department of State expanded its screening process for H-1B applicants and their dependents last month<a href="https://www.newsweek.com/india-says-citizens-stranded-h-1b-visa-delays-immigration-11278996" target="_blank" rel="noreferrer noopener">, requiring applicants to make social media accounts</a> publicly accessible as part of routine vetting. Officials said the move was aimed at preventing abuse of the system.</p><p>The Department of Homeland Security has finalized new rules that will reshape the H-1B visa lottery. Beginning in 2027, applicants would no longer have equal chances of selection; instead, the system would prioritize higher-paid workers within each occupation. Federal officials said the change is intended to favor highly skilled applicants and respond to concerns about visa misuse and the displacement of U.S. workers.</p><h2><strong>What People Are Saying</strong></h2><p><strong>Representative Beth Van Duyne, the Republican of Texas,</strong> told conservative commentator Benny Johnson on December 17: “That H-1B visa program has got to either stop right now until we understand the amount of just how it’s being taken advantage of, or redone so it doesn't exist."</p><p><strong>Stuart Anderson, executive director at the National Foundation for American Policy</strong>, responding to a decline in the number of H-1B visas approved for Indian-based companies, according to a National Foundation for American Policy (NFAP) analysis of government data, told <em>Newsweek</em>: "H-1B&nbsp;visas&nbsp;are important because they are typically the only way to hire a high-skilled foreign national long term in the United States, and approximately 70% of full-time graduate students in key science and technology fields at U.S. schools are international students.”</p><p><strong>United States Citizenship and Immigration Services</strong> <strong>spokesperson Matthew Tragesser, i</strong>n a December 23 press release announcing H-1B lotttery changes:&nbsp;"The new weighted selection will better serve Congress’ intent for the H-1B program and strengthen America’s competitiveness by incentivizing American employers to petition for higher-paid, higher-skilled foreign workers. With these regulatory changes and others in the future, we will continue to update the H-1B program to help American businesses without allowing the abuse that was harming American workers.”&nbsp;</p><h2><strong>What Happens Next</strong></h2><p>Green's amendment has been referred to the House Judiciary Committee, along with the Energy and Commerce and Ways and Means committees.</p><blockquote><article><div><p><img id="11261860" alt="" caption="" credit="" sourcealt="" sources="[]" fetchpriority="auto" loading="lazy" width="1312" height="121" decoding="async" data-nimg="1" sizes="(min-width: 1200px) 1312px, (min-width: 768px) 1024px, 100vw" srcset="https://assets.newsweek.com/wp-content/uploads/2025/12/Top.png?w=640&amp;quality=80&amp;webp=1 640w, https://assets.newsweek.com/wp-content/uploads/2025/12/Top.png?w=750&amp;quality=80&amp;webp=1 750w, https://assets.newsweek.com/wp-content/uploads/2025/12/Top.png?w=1000&amp;quality=80&amp;webp=1 1000w, https://assets.newsweek.com/wp-content/uploads/2025/12/Top.png?w=1200&amp;quality=80&amp;webp=1 1200w, https://assets.newsweek.com/wp-content/uploads/2025/12/Top.png?w=1360&amp;quality=80&amp;webp=1 1360w, https://assets.newsweek.com/wp-content/uploads/2025/12/Top.png?w=1600&amp;quality=80&amp;webp=1 1600w" src="https://assets.newsweek.com/wp-content/uploads/2025/12/Top.png?w=1600&amp;quality=80&amp;webp=1"></p></div><p>The news cycle is loud. Algorithms push us to extremes. In the middle—where facts, ideas and progress live—there's a void. <strong><a href="https://www.newsweek.com/subscribe">At <em>Newsweek</em>, we fill it with fearless, fair and fiercely independent journalism</a>.</strong></p><p>Common ground isn't just possible—it's essential. Our readers reflect America's diversity, united by a desire for thoughtful, unbiased news. Independent ratings confirm our approach: NewsGuard gives us 100/100 for reliability, and AllSides places us firmly in the political center.</p><p>In a polarized era, the center is dismissed as bland. Ours is different: The Courageous Center—it's not "both sides," it's sharp, challenging and alive with ideas. We follow facts, not factions. If that sounds like the kind of journalism you want to see thrive, we need you.</p><p>When you <strong><a href="https://www.newsweek.com/subscribe">become a Newsweek Member</a></strong>, you support a mission to keep the center strong and vibrant.&nbsp;Members enjoy:</p><ul><li><span>&nbsp; <em>Conventional Wisdom</em>: Tracking political winds with clarity.</span></li><li><span>&nbsp; <em>Uncommon Knowledge</em>: Deep dives into overlooked truths.</span></li><li><span>&nbsp; <em>Ad-free browsing</em> and exclusive editor conversations.</span></li></ul><p><strong><a href="https://www.newsweek.com/subscribe">Help keep the center courageous. Join today.</a></strong></p><div><p><a href="https://www.newsweek.com/jennifer-cunningham"><img lightbox="{&quot;enabled&quot;:false}" id="11261873" alt="" caption="" credit="" sourcealt="" sources="[]" fetchpriority="auto" loading="lazy" width="1312" height="136" decoding="async" data-nimg="1" sizes="(min-width: 1200px) 1312px, (min-width: 768px) 1024px, 100vw" srcset="https://assets.newsweek.com/wp-content/uploads/2025/12/Bottom.png?w=640&amp;quality=80&amp;webp=1 640w, https://assets.newsweek.com/wp-content/uploads/2025/12/Bottom.png?w=750&amp;quality=80&amp;webp=1 750w, https://assets.newsweek.com/wp-content/uploads/2025/12/Bottom.png?w=1000&amp;quality=80&amp;webp=1 1000w, https://assets.newsweek.com/wp-content/uploads/2025/12/Bottom.png?w=1200&amp;quality=80&amp;webp=1 1200w, https://assets.newsweek.com/wp-content/uploads/2025/12/Bottom.png?w=1360&amp;quality=80&amp;webp=1 1360w, https://assets.newsweek.com/wp-content/uploads/2025/12/Bottom.png?w=1600&amp;quality=80&amp;webp=1 1600w" src="https://assets.newsweek.com/wp-content/uploads/2025/12/Bottom.png?w=1600&amp;quality=80&amp;webp=1"></a></p></div></article></blockquote></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[On the slow death of scaling (105 pts)]]></title>
            <link>https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5877662</link>
            <guid>46522308</guid>
            <pubDate>Wed, 07 Jan 2026 03:48:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5877662">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5877662</a>, See on <a href="https://news.ycombinator.com/item?id=46522308">Hacker News</a></p>
Couldn't get https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5877662: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft probably killed my Snapdragon Dev Kit (197 pts)]]></title>
            <link>https://jasoneckert.github.io/myblog/how-microsoft-killed-my-snapdragon-devkit/</link>
            <guid>46521860</guid>
            <pubDate>Wed, 07 Jan 2026 02:37:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jasoneckert.github.io/myblog/how-microsoft-killed-my-snapdragon-devkit/">https://jasoneckert.github.io/myblog/how-microsoft-killed-my-snapdragon-devkit/</a>, See on <a href="https://news.ycombinator.com/item?id=46521860">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <p><img src="https://jasoneckert.github.io/myblog/how-microsoft-killed-my-snapdragon-devkit/snapdragondevkit.jpg#center" alt="Snapdragon Dev Kit" title="Snapdragon Dev Kit"></p>
<p>Back in October 2024, <a href="https://jasoneckert.github.io/myblog/windows-on-arm/">I got my hands on a Snapdragon Dev Kit</a>. With a Qualcomm Snapdragon X Elite ARM64 CPU (the fastest model!), 32GB of RAM, and a 512GB SSD, it ran Windows 11 for ARM lightning fast. So I made it my daily driver, and it stayed that way until very recently. I even wrote a <a href="https://jasoneckert.github.io/myblog/windows-on-arm-1year/">review of it one year later</a> back in October.</p>
<p>Since the first boot, it’s been rock-solid and reliable every single day. And as someone who has used Windows since the beginning of time, I’m always combing through Event Viewer for software or hardware issues (there were none). Yes, the fan is noisy, as <a href="https://www.jeffgeerling.com/blog/2024/snapdragon-dev-kit-windows-fastest-x-elite-tested/">Jeff Geerling pointed out</a>, but I don’t notice it because I work with headphones on.</p>
<p>Of course, that changed this past week.</p>
<h2 id="so-what-happened">So, what happened?</h2>
<p>IIn early December, a Windows 11 security update (KB5068861) failed to install and rolled back during reboot. I tried installing it two more times with the same result. I cleared the package cache, ran the usual <code>sfc /scannow</code> and <code>dism /Online /Cleanup-Image /RestoreHealth</code> incantations, and even tried manually installing the update from the Microsoft Update Catalog. No go.</p>
<p>So I Googled it, found that many, many people were having the same issue and that Microsoft was going to fix it with a future cumulative update. So I turned off updates for a month and went on my merry way.</p>
<p>This past week, I turned updates back on. While I was working, I got the usual Windows notification that a restart was pending from a recently applied update. I hit restart and immediately realized it was the same update, same failure, same rollback.</p>
<p>Only this time, the rollback didn’t go as planned. The system rebooted four times before finally loading into Windows. And when it did, it wouldn’t sign me into my Microsoft account. Entering my PIN got me in, but to a brand-new profile complete with the default, soulless Windows background.</p>
<p>I had Internet access and most of my apps worked, but I couldn’t open Windows Terminal or most other Microsoft apps… even Event Viewer. I decided to give it a fresh reboot (hey, it’s Windows after all). But right after the Windows logo appeared, the system either rebooted automatically or just shut down entirely… seemingly at random. A few dozen attempts later, I had to admit defeat.</p>
<p>Next, I tried checking the UEFI settings by booting into the Boot Device Selection (BDS) menu (by pressing the Home key during boot). Unfortunately, the BDS menu behaved sporadically: random freezing, options that wouldn’t select, and general weirdness. I thought it might be keyboard- or USB-related, but other keyboards in different ports behaved exactly the same.</p>
<p>Persistence paid off eventually, and one time I managed to get into the BDS menu and navigate all the options. Figuring a Windows reinstall was my best shot, I disabled Secure Boot, enabled USB-first boot, and turned on the UEFI option that allows WinPE to use external displays (since this isn’t a laptop).</p>
<p>I downloaded the Windows 11 ARM ISO, imaged it to a USB thumb drive, and prepared a second thumb drive with the Snapdragon Dev Kit drivers I had previously snagged from Qualcomm’s website in one big ZIP file (thankfully, before they disappeared). I was able to boot into the Windows 11 installer, and everything ran smoothly at first. I completed an installation that overwrote my existing C:\ partition, rebooted, and made it through the initial setup just fine.</p>
<p>After choosing my region and keyboard layout, I got to the screen asking for a driver to connect to the network. I browsed successfully to the second thumb drive containing the drivers, but before I could even click the file, the system froze and shut down.</p>
<p>Since then, every attempt to boot has failed. It won’t get past the Snapdragon boot logo before rebooting or powering off… again, seemingly at random. I can still get into the BDS menu, but no options are selectable. That means I can’t reinstall Windows 11 again, or try anything else for that matter, like Linux (which still lacks support for the Snapdragon Dev Kit).</p>
<p>I opened the system and reseated everything, including the SSD. No change. I even tested the SSD in another machine to rule it out, and it’s fine too.</p>
<h2 id="postmortem">Postmortem</h2>
<p>The system was perfectly healthy and working fine right up until that Windows update failed.</p>
<p>Did the update somehow overwrite firmware it shouldn’t have? Or partially corrupt some UEFI or bootloader component, enough to boot sometimes, but not enough to stay sane? It could also be a Secure Boot or TPM state mismatch, or even a low-level power-management firmware issue given the random reboot-versus-power-off behavior. And since this is a dev kit with no documented firmware recovery path, even a normally recoverable failure might be permanent.</p>
<p>Or maybe a piece of hardware just failed at exactly the wrong time. No idea.</p>
<p>If Qualcomm hadn’t discontinued the Snapdragon Dev Kit, this probably would’ve been an inconvenience instead of a postmortem. A firmware recovery tool, documented reflashing process, or even a basic support path might have turned this into a bad afternoon rather than a dead system. On a supported consumer Snapdragon PC, I suspect this would’ve been annoying, but fixable.</p>
<p>Is this a problem with the Snapdragon platform itself? I doubt it. It was flawless as a daily driver from October 2024 onward. But this also isn’t a typical Snapdragon-based Windows PC… it’s the Snapdragon Dev Kit. The day it arrived was the same day Qualcomm announced it would be discontinuing it and stopping all future support. Unlike Snapdragon laptops from ASUS, Dell, or Lenovo, there are no OEM-backed recovery tools or firmware safety nets here.</p>
<p>And I certainly haven’t lost faith in the platform. The Snapdragon X Elite is excellent, and up until this one update, Windows 11 and the system performed flawlessly. It was a great PC for just over a year, and it’s a real bummer losing a 32GB machine that consistently smoked my Core i9 system.</p>
<p>Oh well. RIP, powerful little ARM box.</p>
<p><em><strong>P.S. If Qualcomm ever releases a firmware recovery tool for this thing, I’ll happily update this post.</strong></em></p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Polymarket refuses to pay bets that US would 'invade' Venezuela (135 pts)]]></title>
            <link>https://www.ft.com/content/985ae542-1ab4-491e-8e6e-b30f6a3ab666</link>
            <guid>46521773</guid>
            <pubDate>Wed, 07 Jan 2026 02:25:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ft.com/content/985ae542-1ab4-491e-8e6e-b30f6a3ab666">https://www.ft.com/content/985ae542-1ab4-491e-8e6e-b30f6a3ab666</a>, See on <a href="https://news.ycombinator.com/item?id=46521773">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main id="site-content" data-ft-origin="next-barrier-page"><div id="barrier-page"><div id="heroOffer-Hero offers-8f1fe9f5-dfe3-4d8e-973c-a8cc8c176f98" data-component="heroOffer" data-component-unique-name="Hero offers" data-o3-theme="inverse"><div data-o-grid-colspan="12 L6"><p><span></span><span></span><span></span><span>Subscribe to unlock this article</span><span></span></p></div><div data-o-grid-colspan="12 L6"><div><h2><span><span>Save 40% on Standard Digital</span></span></h2><p><span><span><span>was </span><span>Dkr4188</span><span> </span><span>now </span><span>Dkr2499</span><span> for your first year</span></span></span></p></div><p><span><span>Save now on essential digital access to trusted FT journalism on any device. Savings based on monthly annualised price.</span></span></p></div></div><div id="recommendedOffers-Recommended Offers" data-component="recommendedOffers" data-component-unique-name="Recommended Offers"><p><h2 data-o-grid-colspan="12">Explore more offers.</h2></p><div data-o-grid-colspan="12"><div data-o-grid-colspan="12"><div><p><img src="https://images.ft.com/v3/image/raw/https%3A%2F%2Fbarrier-page-components.s3.eu-west-1.amazonaws.com%2Fassets%2Ficons%2Fprimary_product_icon_trial.svg?format=svg&amp;source=next-barrier-page" alt=""></p><p><h3>Trial</h3></p></div><p><span><span>Dkr10</span><span> for 4 weeks</span></span></p><p><span><span>Then </span><span>Dkr535</span><span> per month. Complete digital access with exclusive insights and industry deep dives on any device. Cancel anytime during your trial.</span></span></p></div><div data-o-grid-colspan="12"><div><p><img src="https://images.ft.com/v3/image/raw/https%3A%2F%2Fbarrier-page-components.s3.eu-west-1.amazonaws.com%2Fassets%2Ficons%2Fprimary_product_icon_premium.svg?format=svg&amp;source=next-barrier-page" alt=""></p><p><h3>Premium Digital</h3></p></div><p><span><span>Dkr535</span><span> per month</span></span></p><p><span><span>Complete digital access with exclusive insights and industry deep dives on any device.</span></span></p></div><div data-o-grid-colspan="12"><div><p><img src="https://images.ft.com/v3/image/raw/https%3A%2F%2Fbarrier-page-components.s3.eu-west-1.amazonaws.com%2Fassets%2Ficons%2Fprimary_product_icon_print.svg?format=svg&amp;source=next-barrier-page" alt=""></p><p><h3>Print</h3></p></div><p><span><span>was </span><span>Dkr6799</span><span> </span><span>now </span><span>Dkr1459</span><span> for your first year</span></span></p><p><span><span>Delivery Monday - Saturday, including FT Weekend and FT Digital Edition: all the content of the FT newspaper on any device. Savings based on annual price.</span></span></p></div></div></div><div data-component="subscriptionOptions" data-component-unique-name="Subscription Options Offers API" data-o3-theme="inverse"><h2>Explore our full range of subscriptions.</h2><div><div><div><h3>For individuals</h3></div><p>Discover all the plans currently available in your country</p></div><div><div><h3> For multiple readers</h3></div><p>Digital access for organisations. Includes exclusive features and content.</p></div></div></div><div data-component="whyFT" data-component-unique-name="Why FT" data-o3-theme="inverse"><div><h2>Why the FT?</h2><p>See why over a million readers pay to read the Financial Times.</p></div><p><a href="https://subs.ft.com/whytheft?ft-content-uuid=985ae542-1ab4-491e-8e6e-b30f6a3ab666" aria-label="Find out why the FT">Find out why</a></p></div></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Most websites don't need cookie consent banners (109 pts)]]></title>
            <link>https://block81.com/blog/why-most-websites-dont-actually-need-cookie-consent-banners</link>
            <guid>46521179</guid>
            <pubDate>Wed, 07 Jan 2026 01:05:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://block81.com/blog/why-most-websites-dont-actually-need-cookie-consent-banners">https://block81.com/blog/why-most-websites-dont-actually-need-cookie-consent-banners</a>, See on <a href="https://news.ycombinator.com/item?id=46521179">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-alignment="" data-max-width="" id="blockId-39063">
				<p>A <a href="https://mastodon.ar.al/@aral/113074917674281694">recent discussion in the Mastodon tech community</a> highlighted something important: confusion about when websites actually need those annoying cookie consent banners. The reality is that most small business websites don’t need them—at all!</p><p>That said, just about every site you visit has a cookie notice that interrupt the user experience and potentially hurt conversion rates. It’s as if the enter web—or at least the web in the US—collectively decided that privacy laws require these pop-ups and banners. The truth is far more nuanced.</p><p>Let me break down what’s really going on and why your business website may not even need a cookie consent banner.</p><h2>What Cookies Actually Do</h2><p>Let’s clear up what we’re talking about first.</p><p>Cookies are small text files that websites store on your browser. They serve different purposes:</p><p><strong>Essential cookies</strong> handle basic site functionality such as keeping you logged in, remembering things in your shopping cart, or maintaining your session as you navigate between pages. These are necessary for the website to work properly.</p><p><strong>Tracking cookies</strong> are the problematic ones. These are the creepy ones that follow you around the web, building profiles of your behavior online to serve targeted ads or collect data for third-parties. This is what privacy laws are <i>actually</i> concerned about.</p><p>The key distinction here is that if you’re not tracking people or sharing their data with third parties, you likely don’t need a cookie banner.</p><h2>What Laws Actually Say</h2><p><a href="https://gdpr.eu/">GDPR</a> and similar privacy laws <i>don’t mandate cookie notices.</i> They do require consent for data processing that violates user privacy. <a href="https://gdpr.eu/cookies">Here’s the breakdown</a>:</p><p><strong>You DON’T need consent for:</strong></p><ul><li>Cookies that are strictly necessary for your website to function</li><li>First-party cookies used just for your own analytics (in most cases)</li><li>Session cookies that expire when someone closes their browser</li></ul><p><strong>You DO need consent for:</strong></p><ul><li>Third-party tracking cookies like Google Analytics, Facebook Pixel, and a million other creepy marketing tools</li><li>Cookies that share user data with other companies</li><li>Advertising or behavioral tracking cookies</li></ul><blockquote><p><i>To comply with the regulations governing cookies under the GDPR and the ePrivacy Directive you must: Receive users' consent before you use any cookies except strictly necessary cookies.&nbsp;</i><br><i>— </i><a href="https://gdpr.eu/cookies/"><i>GDPR.eu</i></a></p></blockquote><p>In the United States, the situation is different but often a bit simpler. There’s <a href="https://www.termsfeed.com/faq/is-there-a-cookie-law-in-the-us">no federal cookie consent law</a> in the US. Several states, however, have enacted privacy laws that do affect cookie usage, including <a href="https://oag.ca.gov/privacy/ccpa">California (CCPA/CPRA)</a>, <a href="https://law.lis.virginia.gov/vacodefull/title59.1/chapter53/">Virginia (VCDPA)</a>, <a href="https://coag.gov/resources/colorado-privacy-act/">Colorado (CPA)</a>, and <a href="https://portal.ct.gov/ag/sections/privacy/the-connecticut-data-privacy-act">Connecticut (CTDPA)</a>.</p><p>Most US state privacy laws use a backwards "opt-out" model rather than requiring upfront consent. For CCPA/CPRA compliance without requiring cookie consent banners, you need:</p><ol><li><strong>A Privacy Policy page</strong> that contains a notice of data collection with categories of personal information collected, purposes for its use, and a description of consumer rights.</li><li><strong>A "Do Not Sell or Share My Personal Information" link</strong> that must be clearly visible, typically placed in the footer of a website.</li><li><strong>Global Privacy Control Support</strong> that honors browser-level opt-out signals.</li></ol><p>This means that most US businesses can avoid cookie consent banners entirely by focusing on clear disclosure and honoring opt-out requests. (Or you know, you could just not use software that tracks people. 🤷♀️)</p><h2>The Real Problem: Surveillance Capitalism</h2><p>In case it hasn’t been clear up to this point, the reason cookie consent banners are seemingly everywhere <strong>is not</strong> because laws require them. It’s because most websites have become surveillance machines. They’re packed with tracking scripts that monitor every click, scroll, and interaction all in the name of analytics and profit—privacy be damned.</p><p>Google Analytics, Facebook pixels, advertising networks, heat mapping tools, chatbots, social media widgets—each one typically drops at least one (usually multiple) tracking cookies that require consent. T<strong>he solution isn’t better cookie consent banners; it’s questioning whether you need all that tracking in the first place.</strong></p><h2>Build Websites That Respect User Privacy</h2><p>It isn’t difficult. Really. Here’s how to create effective websites without the cookie consent overhead:</p><p><strong>Use privacy-focused analytics.</strong> Tools like <a href="https://usefathom.com/ref/NSCQH0">Fathom Analytics</a> and <a href="https://plausible.io/">Plausible</a> provide the insights you need <i>without</i> invasive tracking. They don’t use cookies, don’t track individuals, and they comply with privacy laws by default.</p><p><strong>Host your own content.</strong> Instead of embedding videos from YouTube (yes, even I’m guilty of this) or using third-party fonts from Google, host these kinds of assets your self. Your visitors will thank you for the faster loading times too.</p><p><strong>Rethink your tools.</strong> Do you <i>really</i> need that chatbot that tracks users across sessions? Could you use a simple contact form instead? Every third-party script is a potential privacy liability.</p><p><strong>Keep essential functions local.</strong> Shopping carts, user sessions, and form data can all be handled with first-party cookies that don’t require consent.</p><h2>Privacy as a Competitive Advantage</h2><p>Beyond checking off a legal compliance checkbox, there are compelling business reasons to avoid unnecessary cookies:</p><ul><li><strong>Better user experience:</strong> No annoying cookie consent pop-ups interrupting the customer journey</li><li><strong>Faster websites:</strong> Fewer third-party scripts mean better performance</li><li><strong>Higher conversion rates:</strong> Visitors can focus on your content instead of cookie choices</li><li><strong>Competitive advantage:</strong> Privacy-conscious consumers increasingly value businesses that respect their data</li><li><strong>Simplified compliance:</strong> No need to maintain complex consent management systems</li></ul><h2>Getting Started with Privacy-First</h2><p>The transition to a privacy-first approach doesn’t have to happen overnight. But the sooner you start, the better. Start by auditing what you’re actually using:</p><ol><li><strong>Check your analytics.</strong> Yes, this is a bit ironic if you’re using Google Analytics. But are you actually using all that detailed behavioral data, or would aggregate statistics serve your needs just as well? If aggregate stats works just as well, give <a href="https://usefathom.com/ref/NSCQH0">Fathom</a> a go.</li><li><strong>Review your plugins and widgets.</strong> Each third-party tool should justify its privacy cost with clear business value. Otherwise, ditch it or, if feasible, bring it in-house.</li><li><strong>Test without tracking.</strong> Try running your website without third-party cookies for a week or two. You might be surprised how little you miss them.</li></ol><p><strong>If you </strong><i><strong>do</strong></i><strong> need a cookie consent banner,</strong> I recommend using <a href="https://cookieconsent.orestbida.com/">CookieConsent</a> by Orest Bida. I’ve used it on multiple client sites with much success. It’s lightweight, customizable, and doesn’t require any external dependencies.</p><p>That said, cookie consent banners should be the exception, not the rule. The web doesn’t have to be, nor should it be, a surveillance system. By choosing privacy-respecting tools and questioning unnecessary tracking, you can have a website that serves your business goals while treating visitors with respect.</p><p>Your customers will appreciate the cleaner experience and you’ll appreciate the simplified compliance. It really is a win-win that makes the web a little bit better for everyone.</p><p><i>Please note: I'm not a lawyer, and this article doesn't constitute legal advice. Privacy laws are complex and vary by jurisdiction. For specific compliance questions related to your business, consult with a qualified attorney who specializes in privacy law.</i></p><hr><p><strong>Ready to build a website that puts user privacy first?</strong> At Block 81, we specialize in creating fast, effective websites that work beautifully without invasive tracking. Our privacy-by-design approach means better user experiences and simpler compliance. <a href="https://block81.com/contact">Let's talk about your project.</a></p>

							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Electronic nose for indoor mold detection and identification (187 pts)]]></title>
            <link>https://advanced.onlinelibrary.wiley.com/doi/10.1002/adsr.202500124</link>
            <guid>46520935</guid>
            <pubDate>Wed, 07 Jan 2026 00:31:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://advanced.onlinelibrary.wiley.com/doi/10.1002/adsr.202500124">https://advanced.onlinelibrary.wiley.com/doi/10.1002/adsr.202500124</a>, See on <a href="https://news.ycombinator.com/item?id=46520935">Hacker News</a></p>
Couldn't get https://advanced.onlinelibrary.wiley.com/doi/10.1002/adsr.202500124: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: SMTP Tunnel – A SOCKS5 proxy disguised as email traffic to bypass DPI (116 pts)]]></title>
            <link>https://github.com/x011/smtp-tunnel-proxy</link>
            <guid>46520926</guid>
            <pubDate>Wed, 07 Jan 2026 00:30:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/x011/smtp-tunnel-proxy">https://github.com/x011/smtp-tunnel-proxy</a>, See on <a href="https://news.ycombinator.com/item?id=46520926">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">📧 SMTP Tunnel Proxy</h2><a id="user-content--smtp-tunnel-proxy" aria-label="Permalink: 📧 SMTP Tunnel Proxy" href="#-smtp-tunnel-proxy"></a></p>
<blockquote>
<p dir="auto"><strong>A high-speed covert tunnel that disguises TCP traffic as SMTP email communication to bypass Deep Packet Inspection (DPI) firewalls.</strong></p>
</blockquote>
<div data-snippet-clipboard-copy-content="┌─────────────┐      ┌─────────────┐      ┌─────────────┐      ┌──────────────┐
│ Application │─────▶│   Client    │─────▶│   Server    │─────▶│  Internet    │
│  (Browser)  │ TCP  │ SOCKS5:1080 │ SMTP │  Port 587   │ TCP  │              │
│             │◀─────│             │◀─────│             │◀─────│              │
└─────────────┘      └─────────────┘      └─────────────┘      └──────────────┘
                            │                    │
                            │   Looks like       │
                            │   Email Traffic    │
                            ▼                    ▼
                     ┌────────────────────────────────┐
                     │     DPI Firewall               │
                     │  ✅ Sees: Normal SMTP Session  │
                     │  ❌ Cannot see: Tunnel Data    │
                     └────────────────────────────────┘"><pre><code>┌─────────────┐      ┌─────────────┐      ┌─────────────┐      ┌──────────────┐
│ Application │─────▶│   Client    │─────▶│   Server    │─────▶│  Internet    │
│  (Browser)  │ TCP  │ SOCKS5:1080 │ SMTP │  Port 587   │ TCP  │              │
│             │◀─────│             │◀─────│             │◀─────│              │
└─────────────┘      └─────────────┘      └─────────────┘      └──────────────┘
                            │                    │
                            │   Looks like       │
                            │   Email Traffic    │
                            ▼                    ▼
                     ┌────────────────────────────────┐
                     │     DPI Firewall               │
                     │  ✅ Sees: Normal SMTP Session  │
                     │  ❌ Cannot see: Tunnel Data    │
                     └────────────────────────────────┘
</code></pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🎯 Features</h2><a id="user-content--features" aria-label="Permalink: 🎯 Features" href="#-features"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Feature</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>🔒 <strong>TLS Encryption</strong></td>
<td>All traffic encrypted with TLS 1.2+ after STARTTLS</td>
</tr>
<tr>
<td>🎭 <strong>DPI Evasion</strong></td>
<td>Initial handshake mimics real SMTP servers (Postfix)</td>
</tr>
<tr>
<td>⚡ <strong>High Speed</strong></td>
<td>Binary streaming protocol after handshake - minimal overhead</td>
</tr>
<tr>
<td>👥 <strong>Multi-User</strong></td>
<td>Per-user secrets, IP whitelists, and logging settings</td>
</tr>
<tr>
<td>🔑 <strong>Authentication</strong></td>
<td>Per-user pre-shared keys with HMAC-SHA256</td>
</tr>
<tr>
<td>🌐 <strong>SOCKS5 Proxy</strong></td>
<td>Standard proxy interface - works with any application</td>
</tr>
<tr>
<td>📡 <strong>Multiplexing</strong></td>
<td>Multiple connections over single tunnel</td>
</tr>
<tr>
<td>🛡️ <strong>IP Whitelist</strong></td>
<td>Per-user access control by IP address/CIDR</td>
</tr>
<tr>
<td>📦 <strong>Easy Install</strong></td>
<td>One-liner server installation with systemd service</td>
</tr>
<tr>
<td>🎁 <strong>Client Packages</strong></td>
<td>Auto-generated ZIP files for each user</td>
</tr>
<tr>
<td>🔄 <strong>Auto-Reconnect</strong></td>
<td>Client automatically reconnects on connection loss</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<blockquote>
<p dir="auto">📚 For in-depth technical details, protocol specifications, and security analysis, see <a href="https://github.com/x011/smtp-tunnel-proxy/blob/main/TECHNICAL.md">TECHNICAL.md</a>.</p>
</blockquote>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">⚡ Quick Start</h2><a id="user-content--quick-start" aria-label="Permalink: ⚡ Quick Start" href="#-quick-start"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">📋 Prerequisites</h3><a id="user-content--prerequisites" aria-label="Permalink: 📋 Prerequisites" href="#-prerequisites"></a></p>
<ul dir="auto">
<li><strong>Server</strong>: Linux VPS with Python 3.8+, port 587 open</li>
<li><strong>Client</strong>: Windows/macOS/Linux with Python 3.8+</li>
<li><strong>Domain name</strong>: Required for TLS certificate verification (free options: <a href="https://www.duckdns.org/" rel="nofollow">DuckDNS</a>, <a href="https://www.noip.com/" rel="nofollow">No-IP</a>, <a href="https://freedns.afraid.org/" rel="nofollow">FreeDNS</a>)</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 Server Setup (VPS)</h2><a id="user-content--server-setup-vps" aria-label="Permalink: 🚀 Server Setup (VPS)" href="#-server-setup-vps"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Step 1️⃣: Get a Domain Name</h3><a id="user-content-step-1️⃣-get-a-domain-name" aria-label="Permalink: Step 1️⃣: Get a Domain Name" href="#step-1️⃣-get-a-domain-name"></a></p>
<p dir="auto">Get a free domain pointing to your VPS:</p>
<ul dir="auto">
<li>🦆 <strong><a href="https://www.duckdns.org/" rel="nofollow">DuckDNS</a></strong> - Recommended, simple and free</li>
<li>🌐 <strong><a href="https://www.noip.com/" rel="nofollow">No-IP</a></strong> - Free tier available</li>
<li>🆓 <strong><a href="https://freedns.afraid.org/" rel="nofollow">FreeDNS</a></strong> - Many domain options</li>
</ul>
<p dir="auto">Example: <code>myserver.duckdns.org</code> → <code>203.0.113.50</code> (your VPS IP)</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Step 2️⃣: Run the Installer</h3><a id="user-content-step-2️⃣-run-the-installer" aria-label="Permalink: Step 2️⃣: Run the Installer" href="#step-2️⃣-run-the-installer"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -sSL https://raw.githubusercontent.com/x011/smtp-tunnel-proxy/main/install.sh | sudo bash"><pre>curl -sSL https://raw.githubusercontent.com/x011/smtp-tunnel-proxy/main/install.sh <span>|</span> sudo bash</pre></div>
<p dir="auto">The installer will:</p>
<ol dir="auto">
<li>📥 Download and install everything</li>
<li>❓ Ask for your domain name</li>
<li>🔐 Generate TLS certificates automatically</li>
<li>👤 Offer to create your first user</li>
<li>🔥 Configure firewall</li>
<li>🚀 Start the service</li>
</ol>
<p dir="auto"><strong>That's it!</strong> Your server is ready.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">➕ Add More Users Later</h3><a id="user-content--add-more-users-later" aria-label="Permalink: ➕ Add More Users Later" href="#-add-more-users-later"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="smtp-tunnel-adduser bob      # Add user + generate client ZIP
smtp-tunnel-listusers        # List all users
smtp-tunnel-deluser bob      # Remove a user"><pre>smtp-tunnel-adduser bob      <span><span>#</span> Add user + generate client ZIP</span>
smtp-tunnel-listusers        <span><span>#</span> List all users</span>
smtp-tunnel-deluser bob      <span><span>#</span> Remove a user</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">🔄 Update Server</h3><a id="user-content--update-server" aria-label="Permalink: 🔄 Update Server" href="#-update-server"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="smtp-tunnel-update           # Updates code, preserves config/certs/users"><pre>smtp-tunnel-update           <span><span>#</span> Updates code, preserves config/certs/users</span></pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">💻 Client Setup</h2><a id="user-content--client-setup" aria-label="Permalink: 💻 Client Setup" href="#-client-setup"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Option A: Easy Way (Recommended)</h3><a id="user-content-option-a-easy-way-recommended" aria-label="Permalink: Option A: Easy Way (Recommended)" href="#option-a-easy-way-recommended"></a></p>
<ol dir="auto">
<li>Get your <code>username.zip</code> file from the server admin</li>
<li>Extract the ZIP file</li>
<li>Run the launcher:</li>
</ol>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Platform</th>
<th>How to Run</th>
</tr>
</thead>
<tbody>
<tr>
<td>🪟 <strong>Windows</strong></td>
<td>Double-click <code>start.bat</code></td>
</tr>
<tr>
<td>🐧 <strong>Linux</strong></td>
<td>Run <code>./start.sh</code></td>
</tr>
<tr>
<td>🍎 <strong>macOS</strong></td>
<td>Run <code>./start.sh</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">The launcher will automatically install dependencies and start the client.</p>
<p dir="auto">✅ You should see:</p>
<div data-snippet-clipboard-copy-content="SMTP Tunnel Proxy Client
User: alice

[INFO] Starting SMTP Tunnel...
[INFO] SOCKS5 proxy will be available at 127.0.0.1:1080

Connecting to myserver.duckdns.org:587
Connected - binary mode active
SOCKS5 proxy on 127.0.0.1:1080"><pre><code>SMTP Tunnel Proxy Client
User: alice

[INFO] Starting SMTP Tunnel...
[INFO] SOCKS5 proxy will be available at 127.0.0.1:1080

Connecting to myserver.duckdns.org:587
Connected - binary mode active
SOCKS5 proxy on 127.0.0.1:1080
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Option B: Manual Way</h3><a id="user-content-option-b-manual-way" aria-label="Permalink: Option B: Manual Way" href="#option-b-manual-way"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="cd alice
pip install -r requirements.txt
python client.py"><pre><span>cd</span> alice
pip install -r requirements.txt
python client.py</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Option C: Custom Configuration</h3><a id="user-content-option-c-custom-configuration" aria-label="Permalink: Option C: Custom Configuration" href="#option-c-custom-configuration"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Download files
scp root@myserver.duckdns.org:/etc/smtp-tunnel/ca.crt .

# Create config.yaml:
cat > config.yaml << EOF
client:
  server_host: &quot;myserver.duckdns.org&quot;
  server_port: 587
  socks_port: 1080
  username: &quot;alice&quot;
  secret: &quot;your-secret-from-admin&quot;
  ca_cert: &quot;ca.crt&quot;
EOF

# Run client
python client.py -c config.yaml"><pre><span><span>#</span> Download files</span>
scp root@myserver.duckdns.org:/etc/smtp-tunnel/ca.crt <span>.</span>

<span><span>#</span> Create config.yaml:</span>
cat <span>&gt;</span> config.yaml <span><span>&lt;&lt;</span> <span>EOF</span></span>
<span>client:</span>
<span>  server_host: "myserver.duckdns.org"</span>
<span>  server_port: 587</span>
<span>  socks_port: 1080</span>
<span>  username: "alice"</span>
<span>  secret: "your-secret-from-admin"</span>
<span>  ca_cert: "ca.crt"</span>
<span><span>EOF</span></span>

<span><span>#</span> Run client</span>
python client.py -c config.yaml</pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">📖 Usage</h2><a id="user-content--usage" aria-label="Permalink: 📖 Usage" href="#-usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">🌐 Configure Your Applications</h3><a id="user-content--configure-your-applications" aria-label="Permalink: 🌐 Configure Your Applications" href="#-configure-your-applications"></a></p>
<p dir="auto">Set SOCKS5 proxy to: <code>127.0.0.1:1080</code></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">🦊 Firefox</h4><a id="user-content--firefox" aria-label="Permalink: 🦊 Firefox" href="#-firefox"></a></p>
<ol dir="auto">
<li>Settings → Network Settings → Settings</li>
<li>Manual proxy configuration</li>
<li>SOCKS Host: <code>127.0.0.1</code>, Port: <code>1080</code></li>
<li>Select SOCKS v5</li>
<li>✅ Check "Proxy DNS when using SOCKS v5"</li>
</ol>
<p dir="auto"><h4 tabindex="-1" dir="auto">🌐 Chrome</h4><a id="user-content--chrome" aria-label="Permalink: 🌐 Chrome" href="#-chrome"></a></p>
<ol dir="auto">
<li>Install "Proxy SwitchyOmega" extension</li>
<li>Create profile with SOCKS5: <code>127.0.0.1:1080</code></li>
</ol>
<p dir="auto"><h4 tabindex="-1" dir="auto">🪟 Windows (System-wide)</h4><a id="user-content--windows-system-wide" aria-label="Permalink: 🪟 Windows (System-wide)" href="#-windows-system-wide"></a></p>
<p dir="auto">Settings → Network &amp; Internet → Proxy → Manual setup → <code>socks=127.0.0.1:1080</code></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">🍎 macOS (System-wide)</h4><a id="user-content--macos-system-wide" aria-label="Permalink: 🍎 macOS (System-wide)" href="#-macos-system-wide"></a></p>
<p dir="auto">System Preferences → Network → Advanced → Proxies → SOCKS Proxy → <code>127.0.0.1:1080</code></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">🐧 Linux (System-wide)</h4><a id="user-content--linux-system-wide" aria-label="Permalink: 🐧 Linux (System-wide)" href="#-linux-system-wide"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="export ALL_PROXY=socks5://127.0.0.1:1080"><pre><span>export</span> ALL_PROXY=socks5://127.0.0.1:1080</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">💻 Command Line</h4><a id="user-content--command-line" aria-label="Permalink: 💻 Command Line" href="#-command-line"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# curl
curl -x socks5h://127.0.0.1:1080 https://ifconfig.me

# git
git config --global http.proxy socks5://127.0.0.1:1080

# Environment variable
export ALL_PROXY=socks5://127.0.0.1:1080"><pre><span><span>#</span> curl</span>
curl -x socks5h://127.0.0.1:1080 https://ifconfig.me

<span><span>#</span> git</span>
git config --global http.proxy socks5://127.0.0.1:1080

<span><span>#</span> Environment variable</span>
<span>export</span> ALL_PROXY=socks5://127.0.0.1:1080</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">✅ Test Connection</h3><a id="user-content--test-connection" aria-label="Permalink: ✅ Test Connection" href="#-test-connection"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Should show your VPS IP
curl -x socks5://127.0.0.1:1080 https://ifconfig.me"><pre><span><span>#</span> Should show your VPS IP</span>
curl -x socks5://127.0.0.1:1080 https://ifconfig.me</pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">⚙️ Configuration Reference</h2><a id="user-content-️-configuration-reference" aria-label="Permalink: ⚙️ Configuration Reference" href="#️-configuration-reference"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">🖥️ Server Options (<code>config.yaml</code>)</h3><a id="user-content-️-server-options-configyaml" aria-label="Permalink: 🖥️ Server Options (config.yaml)" href="#️-server-options-configyaml"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Option</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>host</code></td>
<td>Listen interface</td>
<td><code>0.0.0.0</code></td>
</tr>
<tr>
<td><code>port</code></td>
<td>Listen port</td>
<td><code>587</code></td>
</tr>
<tr>
<td><code>hostname</code></td>
<td>SMTP hostname (must match certificate)</td>
<td><code>mail.example.com</code></td>
</tr>
<tr>
<td><code>cert_file</code></td>
<td>TLS certificate path</td>
<td><code>server.crt</code></td>
</tr>
<tr>
<td><code>key_file</code></td>
<td>TLS private key path</td>
<td><code>server.key</code></td>
</tr>
<tr>
<td><code>users_file</code></td>
<td>Path to users configuration</td>
<td><code>users.yaml</code></td>
</tr>
<tr>
<td><code>log_users</code></td>
<td>Global logging setting</td>
<td><code>true</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">👥 User Options (<code>users.yaml</code>)</h3><a id="user-content--user-options-usersyaml" aria-label="Permalink: 👥 User Options (users.yaml)" href="#-user-options-usersyaml"></a></p>
<p dir="auto">Each user can have individual settings:</p>
<div dir="auto" data-snippet-clipboard-copy-content="users:
  alice:
    secret: &quot;auto-generated-secret&quot;
    # whitelist:              # Optional: restrict to specific IPs
    #   - &quot;192.168.1.100&quot;
    #   - &quot;10.0.0.0/8&quot;        # CIDR notation supported
    # logging: true           # Optional: disable to stop logging this user

  bob:
    secret: &quot;another-secret&quot;
    whitelist:
      - &quot;203.0.113.50&quot;        # Bob can only connect from this IP
    logging: false            # Don't log Bob's activity"><pre><span>users</span>:
  <span>alice</span>:
    <span>secret</span>: <span><span>"</span>auto-generated-secret<span>"</span></span>
    <span><span>#</span> whitelist:              # Optional: restrict to specific IPs</span>
    <span><span>#</span>   - "192.168.1.100"</span>
    <span><span>#</span>   - "10.0.0.0/8"        # CIDR notation supported</span>
    <span><span>#</span> logging: true           # Optional: disable to stop logging this user</span>

  <span>bob</span>:
    <span>secret</span>: <span><span>"</span>another-secret<span>"</span></span>
    <span>whitelist</span>:
      - <span><span>"</span>203.0.113.50<span>"</span></span>        <span><span>#</span> Bob can only connect from this IP</span>
    <span>logging</span>: <span>false            </span><span><span>#</span> Don't log Bob's activity</span></pre></div>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Option</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>secret</code></td>
<td>User's authentication secret</td>
<td>Required</td>
</tr>
<tr>
<td><code>whitelist</code></td>
<td>Allowed IPs for this user (CIDR supported)</td>
<td>All IPs</td>
</tr>
<tr>
<td><code>logging</code></td>
<td>Enable activity logging for this user</td>
<td><code>true</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">💻 Client Options</h3><a id="user-content--client-options" aria-label="Permalink: 💻 Client Options" href="#-client-options"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Option</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>server_host</code></td>
<td>Server domain name</td>
<td>Required</td>
</tr>
<tr>
<td><code>server_port</code></td>
<td>Server port</td>
<td><code>587</code></td>
</tr>
<tr>
<td><code>socks_port</code></td>
<td>Local SOCKS5 port</td>
<td><code>1080</code></td>
</tr>
<tr>
<td><code>socks_host</code></td>
<td>Local SOCKS5 interface</td>
<td><code>127.0.0.1</code></td>
</tr>
<tr>
<td><code>username</code></td>
<td>Your username</td>
<td>Required</td>
</tr>
<tr>
<td><code>secret</code></td>
<td>Your authentication secret</td>
<td>Required</td>
</tr>
<tr>
<td><code>ca_cert</code></td>
<td>CA certificate for verification</td>
<td>Recommended</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">📋 Service Management</h2><a id="user-content--service-management" aria-label="Permalink: 📋 Service Management" href="#-service-management"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Check status
sudo systemctl status smtp-tunnel

# Restart after config changes
sudo systemctl restart smtp-tunnel

# View logs
sudo journalctl -u smtp-tunnel -n 100

# Uninstall
sudo /opt/smtp-tunnel/uninstall.sh"><pre><span><span>#</span> Check status</span>
sudo systemctl status smtp-tunnel

<span><span>#</span> Restart after config changes</span>
sudo systemctl restart smtp-tunnel

<span><span>#</span> View logs</span>
sudo journalctl -u smtp-tunnel -n 100

<span><span>#</span> Uninstall</span>
sudo /opt/smtp-tunnel/uninstall.sh</pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔧 Command Line Options</h2><a id="user-content--command-line-options" aria-label="Permalink: 🔧 Command Line Options" href="#-command-line-options"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">🖥️ Server</h3><a id="user-content-️-server" aria-label="Permalink: 🖥️ Server" href="#️-server"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="python server.py [-c CONFIG] [-d]

  -c, --config    Config file (default: config.yaml)
  -d, --debug     Enable debug logging"><pre>python server.py [-c CONFIG] [-d]

  -c, --config    Config file (default: config.yaml)
  -d, --debug     Enable debug logging</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">💻 Client</h3><a id="user-content--client" aria-label="Permalink: 💻 Client" href="#-client"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="python client.py [-c CONFIG] [--server HOST] [--server-port PORT]
                 [-p SOCKS_PORT] [-u USERNAME] [-s SECRET] [--ca-cert FILE] [-d]

  -c, --config      Config file (default: config.yaml)
  --server          Override server domain
  --server-port     Override server port
  -p, --socks-port  Override local SOCKS port
  -u, --username    Your username
  -s, --secret      Override secret
  --ca-cert         CA certificate path
  -d, --debug       Enable debug logging"><pre>python client.py [-c CONFIG] [--server HOST] [--server-port PORT]
                 [-p SOCKS_PORT] [-u USERNAME] [-s SECRET] [--ca-cert FILE] [-d]

  -c, --config      Config file (default: config.yaml)
  --server          Override server domain
  --server-port     Override server port
  -p, --socks-port  Override <span>local</span> SOCKS port
  -u, --username    Your username
  -s, --secret      Override secret
  --ca-cert         CA certificate path
  -d, --debug       Enable debug logging</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">👥 User Management</h3><a id="user-content--user-management" aria-label="Permalink: 👥 User Management" href="#-user-management"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="smtp-tunnel-adduser <username> [-u USERS_FILE] [-c CONFIG] [--no-zip]
    Add a new user and generate client package

smtp-tunnel-deluser <username> [-u USERS_FILE] [-f]
    Remove a user (use -f to skip confirmation)

smtp-tunnel-listusers [-u USERS_FILE] [-v]
    List all users (use -v for detailed info)

smtp-tunnel-update
    Update server to latest version (preserves config/certs/users)"><pre>smtp-tunnel-adduser <span>&lt;</span>username<span>&gt;</span> [-u USERS_FILE] [-c CONFIG] [--no-zip]
    Add a new user and generate client package

smtp-tunnel-deluser <span>&lt;</span>username<span>&gt;</span> [-u USERS_FILE] [-f]
    Remove a user (use -f to skip confirmation)

smtp-tunnel-listusers [-u USERS_FILE] [-v]
    List all users (use -v <span>for</span> detailed info)

smtp-tunnel-update
    Update server to latest version (preserves config/certs/users)</pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">📁 File Structure</h2><a id="user-content--file-structure" aria-label="Permalink: 📁 File Structure" href="#-file-structure"></a></p>
<div data-snippet-clipboard-copy-content="smtp_proxy/
├── 📄 server.py               # Server (runs on VPS)
├── 📄 client.py               # Client (runs locally)
├── 📄 common.py               # Shared utilities
├── 📄 generate_certs.py       # Certificate generator
├── 📄 config.yaml             # Server/client configuration
├── 📄 users.yaml              # User database
├── 📄 requirements.txt        # Python dependencies
├── 📄 install.sh              # One-liner server installer
├── 📄 smtp-tunnel.service     # Systemd unit file
├── 🔧 smtp-tunnel-adduser     # Add user script
├── 🔧 smtp-tunnel-deluser     # Remove user script
├── 🔧 smtp-tunnel-listusers   # List users script
├── 🔧 smtp-tunnel-update      # Update server script
├── 📄 README.md               # This file
└── 📄 TECHNICAL.md            # Technical documentation"><pre><code>smtp_proxy/
├── 📄 server.py               # Server (runs on VPS)
├── 📄 client.py               # Client (runs locally)
├── 📄 common.py               # Shared utilities
├── 📄 generate_certs.py       # Certificate generator
├── 📄 config.yaml             # Server/client configuration
├── 📄 users.yaml              # User database
├── 📄 requirements.txt        # Python dependencies
├── 📄 install.sh              # One-liner server installer
├── 📄 smtp-tunnel.service     # Systemd unit file
├── 🔧 smtp-tunnel-adduser     # Add user script
├── 🔧 smtp-tunnel-deluser     # Remove user script
├── 🔧 smtp-tunnel-listusers   # List users script
├── 🔧 smtp-tunnel-update      # Update server script
├── 📄 README.md               # This file
└── 📄 TECHNICAL.md            # Technical documentation
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">📦 Installation Paths (after install.sh)</h3><a id="user-content--installation-paths-after-installsh" aria-label="Permalink: 📦 Installation Paths (after install.sh)" href="#-installation-paths-after-installsh"></a></p>
<div data-snippet-clipboard-copy-content="/opt/smtp-tunnel/              # Application files
/etc/smtp-tunnel/              # Configuration files
  ├── config.yaml
  ├── users.yaml
  ├── server.crt
  ├── server.key
  └── ca.crt
/usr/local/bin/                # Management commands
  ├── smtp-tunnel-adduser
  ├── smtp-tunnel-deluser
  ├── smtp-tunnel-listusers
  └── smtp-tunnel-update"><pre><code>/opt/smtp-tunnel/              # Application files
/etc/smtp-tunnel/              # Configuration files
  ├── config.yaml
  ├── users.yaml
  ├── server.crt
  ├── server.key
  └── ca.crt
/usr/local/bin/                # Management commands
  ├── smtp-tunnel-adduser
  ├── smtp-tunnel-deluser
  ├── smtp-tunnel-listusers
  └── smtp-tunnel-update
</code></pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔧 Troubleshooting</h2><a id="user-content--troubleshooting" aria-label="Permalink: 🔧 Troubleshooting" href="#-troubleshooting"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">❌ "Connection refused"</h3><a id="user-content--connection-refused" aria-label="Permalink: ❌ &quot;Connection refused&quot;" href="#-connection-refused"></a></p>
<ul dir="auto">
<li>Check server is running: <code>systemctl status smtp-tunnel</code> or <code>ps aux | grep server.py</code></li>
<li>Check port is open: <code>netstat -tlnp | grep 587</code></li>
<li>Check firewall: <code>ufw status</code></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">❌ "Auth failed"</h3><a id="user-content--auth-failed" aria-label="Permalink: ❌ &quot;Auth failed&quot;" href="#-auth-failed"></a></p>
<ul dir="auto">
<li>Verify <code>username</code> and <code>secret</code> match in users.yaml</li>
<li>Check server time is accurate (within 5 minutes)</li>
<li>Run <code>smtp-tunnel-listusers -v</code> to verify user exists</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">❌ "IP not whitelisted"</h3><a id="user-content--ip-not-whitelisted" aria-label="Permalink: ❌ &quot;IP not whitelisted&quot;" href="#-ip-not-whitelisted"></a></p>
<ul dir="auto">
<li>Check user's whitelist in users.yaml</li>
<li>Your current IP must match a whitelist entry</li>
<li>CIDR notation is supported (e.g., <code>10.0.0.0/8</code>)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">❌ "Certificate verify failed"</h3><a id="user-content--certificate-verify-failed" aria-label="Permalink: ❌ &quot;Certificate verify failed&quot;" href="#-certificate-verify-failed"></a></p>
<ul dir="auto">
<li>Ensure you're using a domain name, not IP address</li>
<li>Verify <code>server_host</code> matches the certificate hostname</li>
<li>Ensure you have the correct <code>ca.crt</code> from the server</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">🐛 Debug Mode</h3><a id="user-content--debug-mode" aria-label="Permalink: 🐛 Debug Mode" href="#-debug-mode"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Enable detailed logging
python server.py -d
python client.py -d

# View systemd logs
journalctl -u smtp-tunnel -f"><pre><span><span>#</span> Enable detailed logging</span>
python server.py -d
python client.py -d

<span><span>#</span> View systemd logs</span>
journalctl -u smtp-tunnel -f</pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔐 Security Notes</h2><a id="user-content--security-notes" aria-label="Permalink: 🔐 Security Notes" href="#-security-notes"></a></p>
<ul dir="auto">
<li>✅ <strong>Always use a domain name</strong> for proper TLS verification</li>
<li>✅ <strong>Always use <code>ca_cert</code></strong> to prevent man-in-the-middle attacks</li>
<li>✅ <strong>Use <code>smtp-tunnel-adduser</code></strong> to generate strong secrets automatically</li>
<li>✅ <strong>Use per-user IP whitelists</strong> if you know client IPs</li>
<li>✅ <strong>Protect <code>users.yaml</code></strong> - contains all user secrets (chmod 600)</li>
<li>✅ <strong>Disable logging</strong> for sensitive users with <code>logging: false</code></li>
</ul>
<blockquote>
<p dir="auto">📚 For detailed security analysis and threat model, see <a href="https://github.com/x011/smtp-tunnel-proxy/blob/main/TECHNICAL.md">TECHNICAL.md</a>.</p>
</blockquote>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">📄 License</h2><a id="user-content--license" aria-label="Permalink: 📄 License" href="#-license"></a></p>
<p dir="auto">This project is provided for educational and authorized use only. Use responsibly and in accordance with applicable laws.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto"><g-emoji alias="warning">⚠️</g-emoji> Disclaimer</h2><a id="user-content-️-disclaimer" aria-label="Permalink: ⚠️ Disclaimer" href="#️-disclaimer"></a></p>
<p dir="auto">This tool is designed for legitimate privacy and censorship circumvention purposes. Users are responsible for ensuring their use complies with applicable laws and regulations.</p>
<hr>
<p dir="auto"><em>Made with ❤️ for internet freedom</em></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why the trans flag emoji is the 5-codepoint sequence it is (155 pts)]]></title>
            <link>https://hecate.pink/blog/2026/trans-flag-emoji/</link>
            <guid>46520879</guid>
            <pubDate>Wed, 07 Jan 2026 00:22:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hecate.pink/blog/2026/trans-flag-emoji/">https://hecate.pink/blog/2026/trans-flag-emoji/</a>, See on <a href="https://news.ycombinator.com/item?id=46520879">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> 
    
      
  <p>Back in 2018, I was an intern at facebook<sup><a href="#1">1</a></sup>. I was also very early into my transition, so I was very in-the-loop with my trans collegeaues - and I heard about how one of the messenging teams (messenger? whatsapp? maybe both?) was going to be rolling out a trans pride flag emoji, and was encouraging early testing.</p>
<p>My first and immediate find? Copy-and-pasting the emoji into any other text field would result in a singular unknown glyph (like, say, �) - these clowns wanted to map it to a private use area codepoint! Fair enough, I suppose, for a corporation that wishes to ship on their own timelines - so I did some digging.</p>
<h2 id="some-background">some background</h2>
<p>I found no good prior art to go off of - I believe that the <a href="https://www.unicode.org/L2/L2019/19080-transgender-flag.pdf">eventually-accepted proposal</a> was circulating as a draft at the time, but I haven't been able to find any references or records of it from that period of time; I did find reference <a href="https://www.lgbtqnation.com/2018/07/unicode-refused-approve-trans-flag-emoji-instead-got-lobster/">here</a> to the unicode consortium rejecting a proposal for a trans flag that year - I think it might have been an earlier version of the proposal, it might have been an alternate proposal; I do know that I recalled seeing requests for a new codepoint to be issued for a trans flag emoji glyph, and a proposal without all of the requisite U+FE0F's.</p>
<p>The initial approach and codepoint mapping taken a day or so later was 🏳‍⚧ - <code>[U+1F3F3, U+200D, U+26A7]</code>. That's pretty close to what the approved codepoints ended up being, and it likely displays correctly now, but it's lacking a couple U+FE0F's that were necessary at the time. Here's where things get into the weeds.</p>
<h2 id="u-fe0f-emoji-presentation-mode">U+FE0F: Emoji Presentation Mode</h2>
<p>Nowadays, if you go look at the <a href="https://emojipedia.org/transgender-flag#technical">listed codepoints</a> for 🏳️‍⚧️, you'll see <em>five</em> codepoints<sup><a href="#2">2</a></sup> (characters) that make it up, with two of them being <code>U+FE0F</code>. FEOF can be slapped after a symbol - without a ZWJ! - to indicate that the preceding codepoint should be interpreted for Emoji Presentation, and not textual mode - her sister, U+FE0E, does that. If you want to see this in action: [⚧︎⚧️] is <code>[U+26A7 U+FE0E U+26A7 U+FEOF]</code>. Same codepoint, two render modes!</p>
<p>There was <a href="https://emojipedia.org/rainbow-flag#technical">some prior art</a> on how to approach the trans flag codepoints - emoji presentation white flag, zero width joiner, and a rainbow emoji. So, just swap the rainbow emoji for U+26A7 and we're good to go, right?</p>
<h2 id="u-200d-zero-width-joiner-my-beloved">U+200D: Zero-width joiner my beloved</h2>
<p>There's a lot of technical details about what is and is not technically valid to do with zero-width joiners. I'm not here to say that there is a <em>wrong</em> way to use the ZWJ to combine codepoints, because language is always-changing, but I <em>was there</em> to ensure that there is no <em>technical</em> reason to deny the trans flag proposal. The existing and eventually accepted proposal L2/19-080 that I linked earlier, proposed a four-codepoint approach, but it lacked the trailing U+FE0F for ⚧.</p>
<p>You might see where I'm going with this: at the time, U+26A7 was a text-mode-only glyph. Including it in a ZWJ sequence <a href="https://www.unicode.org/reports/tr51/tr51-14.html#def_fully_qualified_emoji_zwj_sequence">was not valid without a following U+FE0F</a> at the time:</p>
<blockquote>
<p>fully-qualified emoji zwj sequence — An emoji zwj sequence in which every default text presentation character (ED-7) is either followed by an emoji modifier or followed by an emoji presentation selector, and there are no other emoji or text presentation selectors in the sequence.</p>
</blockquote>
<p>At the time<sup><a href="#3">3</a></sup>, ⚧ was a text-mode only glyph; thus, it must be followed by an emoji presentation selector. I suggested the 5-codepoint mapping, and also advised that there should be an emoji-rendering glyph for U+26A7 U+FE0F. To my utter amazement, my feedback was followed!</p>
<p>The end result of this, was the 5-codepoint version of 🏳️‍⚧️ was the first one to be out in the wild. You could send it on fb messenger, or whatsapp, and it'd render as a nice emoji; you could copy/paste it to another client, and you'd get the fallback render of a flag and a symbol, similar to the lag time between 🏳️‍🌈 being an accepted sequence and vendors supporting rendering a couple years prior. The next year, the trans flag emoji proposal was accepted, citing the sequence already existing on whatsapp and rendering as a first-class emoji - no new codepoints or allocations needed; it only needed the blessing of its codepoint sequence, and an update to U+26A7 saying that it is valid to use it in emoji presentation mode (and emoji presentation sequences).</p>
<p>This does actually mean that now, the four-codepoint sequence for 🏳️‍⚧️ is <em>now</em> valid, since now that ⚧︎ <em>can</em> be displayed in an emoji presentation, it doesn't <em>have</em> to be specified as needing emoji presentation in a ZWJ sequence. It is unfortunate that the transgender approach to acceptance feels like it necessitates being technically correct beyond all reproach in order to force acceptance - looking back, I think it's a bit of a shame that the advocates pushing for 🏳️‍⚧️ through the appropriate mediums were being rebuffed for not being totally technically accurate (as they weren't including any details about U+26A7 needing emoji presentation), and that no one technical would actually work with them on getting those details corrected earlier. It wasn't until a corporation, a vendor, whatever broke ranks and just had a technically unassailable implementation, that it was included into the fold.</p>
<h3 id="if-there-is-one-mark-i-am-glad-to-have-left-on-this-earth-it-is-this-5-codepoint-sequence-pink-heart-flag-white-transgender-symbol-pink-heart">if there is one mark i am glad to have left on this earth, it is <a href="https://www.unicode.org/Public/emoji/latest/emoji-zwj-sequences.txt#:~:text=1F3F3%20FE0F%20200D%2026A7%20FE0F">this 5-codepoint sequence</a>. 🩷🏳️‍⚧️🩷</h3>






    
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hyundai Introduces Its Next-Gen Atlas Robot at CES 2026 [video] (128 pts)]]></title>
            <link>https://www.youtube.com/watch?v=9e0SQn9uUlw</link>
            <guid>46520508</guid>
            <pubDate>Tue, 06 Jan 2026 23:40:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=9e0SQn9uUlw">https://www.youtube.com/watch?v=9e0SQn9uUlw</a>, See on <a href="https://news.ycombinator.com/item?id=46520508">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Are we tired of social media? (2025) (107 pts)]]></title>
            <link>https://www.danielbrendel.com/blog/24-are-we-tired-of-social-media-once-and-for-all</link>
            <guid>46519622</guid>
            <pubDate>Tue, 06 Jan 2026 22:15:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.danielbrendel.com/blog/24-are-we-tired-of-social-media-once-and-for-all">https://www.danielbrendel.com/blog/24-are-we-tired-of-social-media-once-and-for-all</a>, See on <a href="https://news.ycombinator.com/item?id=46519622">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
 <pre>I've been asking this question to myself a few times now. When I speak with other peers, many are tired of social media. Without any hope for a change?

I remember how different the times were like 20 years ago. Us fellow internet users weren't connected 24/7. We used to be online when we were in front of our desktop or laptop computer. Our ways of communicating happened via instant messaging systems or client-server based chat systems. ICQ, MSN, IRC, Skype, TeamSpeak, Ventrilo or Mumble were systems that were widely used. Plus we had forums. 

<hr>
<img src="https://www.watson.ch/imgdb/7822/Qx,A,0,0,626,411,260,171,104,68/2983565120294014" width="360" alt="icq-retro-image">
<i>Do you remember ICQ?</i>
<hr>

It was really a hype, something new to explore, exciting and more like a wild, wild west. However the Internet became more and more commercialised. And with many years passing by, social interactions have been taking place on a few big player networks, like Facebook, Instagram, TikTok, Reddit, Twitter, etc.

<hr>
<img src="https://preview.redd.it/forums-before-social-media-i-miss-them-v0-5p97av9veu3a1.png?auto=webp&amp;s=c4d4ec49a3feda9383b34d3a3e74eaa8b11a576c" width="360" alt="old-forum-software">
<i>Forums were one way to organize communities</i>
<hr>

As a result of the recent shifts, I, personally, experienced some tiredness in socializing. The Internet drove us more away from each other, and this is also a shift that I feel takes place in real life as well. Of course, the internet and real life have become more interwined and there is definitely a correlation between these phenomena. 

<hr>
<img src="https://upload.wikimedia.org/wikipedia/commons/9/9f/Wikipedia_Mumble.png" width="360" alt="mumble-voip">
<i>Mumble was one of the popular VoIP systems</i>
<hr>

Bubbles have emerged, hatespeech, plenty of ads, misinformation, et cetera. Bad news, controversial content and ragebait are things that bring plenty of financial revenue. Has the internet not become a giant capitalistic market place? Do we go online to entertain ourselves? Or do we go online merely as a consumer, a customer, to be spoonfed with marketing ads, while also getting flooded with news how bad everything is, that the world is practically doomed? And it only gets worse with the rise of AI generated rage/click-bait content across all social media platforms.

This is not the Internet of the past. Nor is it the Internet as it should be. 

<hr>
<img src="https://www.danielbrendel.com/img/whr.png" width="360" alt="world-happiness-report">
<i>Source: <a href="https://www.worldhappiness.report/ed/2019/the-sad-state-of-happiness-in-the-united-states-and-the-role-of-digital-media/">World Happiness Report</a></i>
<hr>

When I got my attention to Mastodon and the Fediverse, I believed this could be a way to get the exit on a highly commercialised highway that will eventually lead to a dead internet. See, Instagram and other social networks are currently a cash cow that is milked till oblivion. Enshittification is taking place on every aspect of the popular internet. It's not only social media that is affected. Take Google Play as an example. They are essentially abusing their power, hurting developers all around the globe. I don't want to dig too deep into the reality of Enshittification in this post, but if you want to know more, please <a href="https://en.wikipedia.org/wiki/Enshittification">see this Wikipedia article</a>.

Back to social media. Mastodon and other systems of the fediverse are great. No doubt, they have the power to better at least one portion of the Internet. I used to be registered on Mastodon for quite a while and at first it was an almost entirely great experience, with a few exceptions. That was a massive <b>Yay, Mastodon ^_^</b>, as my personal experience with Twitter (which I left for a while now) and Instagram, as well as other big tech platforms, have become utterly negative. So, Mastodon appeared as a light in the dark, given it is open-sourced software and doesn't patronize your feed. However after a while I realized that people just post into the void. Everyone has something to say or promote, yet no one wants to listen. It's like we're all having our booth on a crowded public space, but there are no actual recievers. I left mastodon after a while and moved to Bluesky. But it's just the same. It's the same everywhere.

Thus I concluded that people overall are tired of socializing. Which is fine, and honestly, I admit that this is the case for me, too. At this point, I don't even know if I <i>would want</i> to connect with others that much. I, personally, merely go online, post some dev things, and then move on to something else. On the other hand, getting into a great community of developers, supporting each other and exchanging information, progress or simply chatting, is something I miss. Yet I don't really pursue a change here. I'm simply tired of social media - and people. 

And from what I see from others, I'm definitely not alone.</pre>
 </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CES 2026: Taking the Lids Off AMD's Venice and MI400 SoCs (127 pts)]]></title>
            <link>https://chipsandcheese.com/p/ces-2026-taking-the-lids-off-amds</link>
            <guid>46519326</guid>
            <pubDate>Tue, 06 Jan 2026 21:46:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/p/ces-2026-taking-the-lids-off-amds">https://chipsandcheese.com/p/ces-2026-taking-the-lids-off-amds</a>, See on <a href="https://news.ycombinator.com/item?id=46519326">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>Hello you fine Internet folks,</p><p>Here at CES 2026, AMD showed off their upcoming Venice series of server CPUs and their upcoming MI400 series of datacenter accelerators. AMD has talked about the specifications of both Venice and the MI400 series at their Advancing AI event back in June of 2025, but this is the first time AMD has shown off the silicon for both of product lines.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!qd_2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4e23adc-da36-42af-b245-11459f567dcc_1674x960.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!qd_2!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4e23adc-da36-42af-b245-11459f567dcc_1674x960.png 424w, https://substackcdn.com/image/fetch/$s_!qd_2!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4e23adc-da36-42af-b245-11459f567dcc_1674x960.png 848w, https://substackcdn.com/image/fetch/$s_!qd_2!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4e23adc-da36-42af-b245-11459f567dcc_1674x960.png 1272w, https://substackcdn.com/image/fetch/$s_!qd_2!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4e23adc-da36-42af-b245-11459f567dcc_1674x960.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!qd_2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4e23adc-da36-42af-b245-11459f567dcc_1674x960.png" width="1456" height="835" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c4e23adc-da36-42af-b245-11459f567dcc_1674x960.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:835,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:766677,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/183673318?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4e23adc-da36-42af-b245-11459f567dcc_1674x960.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!qd_2!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4e23adc-da36-42af-b245-11459f567dcc_1674x960.png 424w, https://substackcdn.com/image/fetch/$s_!qd_2!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4e23adc-da36-42af-b245-11459f567dcc_1674x960.png 848w, https://substackcdn.com/image/fetch/$s_!qd_2!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4e23adc-da36-42af-b245-11459f567dcc_1674x960.png 1272w, https://substackcdn.com/image/fetch/$s_!qd_2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4e23adc-da36-42af-b245-11459f567dcc_1674x960.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Starting with Venice, the first thing to notice is the packaging of the CCDs to the IO dies is different. Instead of using the organic substrate of the package to run the wires between the CCDs and the IO dies that AMD has used since EPYC Rome, Venice appears to be using a more advanced form of packaging similar to Strix Halo or MI250X. Another change is that Venice appears to have two IO dies instead of the single IO die that the prior EPYC CPUs had.</p><p><span>Venice has 8 CCDs each of which have 32 cores for a total of up to 256 cores per Venice package. Doing some measuring of each of the dies, you get that each CCD is approximately 165mm</span><sup>2 </sup><span>of N2 silicon. If AMD has stuck to 4MB of L3 per core than each of these CCDs have 32 Zen 6 cores and 128MB of L3 cache along with the die to die interface for the CCD &lt;-&gt; IO die communications. At approximately 165mm</span><sup>2</sup><span> per CCD, that would make a Zen 6 core plus the 4MB of L3 per core about 5mm</span><sup>2</sup><span> each which is similar to Zen 5’s approximately 5.34mm</span><sup>2</sup><span> on N3 when counting both the Zen 5 core and 4MB of L3 cache.</span></p><p><span>Moving to the IO dies, they each appear to be approximately 353mm</span><sup>2</sup><span> for a total of just over 700mm</span><sup>2</sup><span> of silicon dedicated for the IO dies. This is a massive increase from the approximately 400mm</span><sup>2</sup><span> that the prior EPYC CPUs dedicated for their IO dies. The two IO dies appear to be using an advanced packaging of some kind similar to the CCDs. Next to the IO dies appear to be 8 little dies, 4 on each side of the package, which are likely to either be structural silicon or deep trench capacitor dies meant to improve power delivery to the CCDs and IO dies.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!ivUB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45dd5538-8896-4235-a618-546e3acdfa11_704x690.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ivUB!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45dd5538-8896-4235-a618-546e3acdfa11_704x690.png 424w, https://substackcdn.com/image/fetch/$s_!ivUB!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45dd5538-8896-4235-a618-546e3acdfa11_704x690.png 848w, https://substackcdn.com/image/fetch/$s_!ivUB!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45dd5538-8896-4235-a618-546e3acdfa11_704x690.png 1272w, https://substackcdn.com/image/fetch/$s_!ivUB!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45dd5538-8896-4235-a618-546e3acdfa11_704x690.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!ivUB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45dd5538-8896-4235-a618-546e3acdfa11_704x690.png" width="704" height="690" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/45dd5538-8896-4235-a618-546e3acdfa11_704x690.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:690,&quot;width&quot;:704,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:382099,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/183673318?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45dd5538-8896-4235-a618-546e3acdfa11_704x690.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ivUB!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45dd5538-8896-4235-a618-546e3acdfa11_704x690.png 424w, https://substackcdn.com/image/fetch/$s_!ivUB!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45dd5538-8896-4235-a618-546e3acdfa11_704x690.png 848w, https://substackcdn.com/image/fetch/$s_!ivUB!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45dd5538-8896-4235-a618-546e3acdfa11_704x690.png 1272w, https://substackcdn.com/image/fetch/$s_!ivUB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45dd5538-8896-4235-a618-546e3acdfa11_704x690.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>Shifting off of Venice and on to the MI400 accelerator, this is a massive package with 12 HBM4 dies and “twelve 2 nanometer and 3 nanometer compute and IO dies”. It appears as if there are two base dies just like MI350. But unlike MI350, there appears to also be two extra dies on the top and bottom of the base dies. These two extra dies are likely for off-package IO such as PCIe, UALink, etc.</p><p><span>Calculating the die sizes of the base dies and the IO dies, the die size of the base die is approximately 747mm</span><sup>2</sup><span> for each of the two base dies with the off-package IO dies each being approximately 220mm</span><sup>2</sup><span>.  As for the compute dies, while the packaging precludes any visual demarcation of the different compute dies, it is likely that there are 8 compute dies with 4 compute dies on each base die. So while we can’t figure out the exact die size of the compute dies, the maximum size is approximately 180mm</span><sup>2</sup><span>. The compute chiplet is likely in the 140mm</span><sup>2</sup><span> to 160mm</span><sup>2</sup><span> region but that is a best guess that will have to wait to be confirmed.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!w4k7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a409225-b74c-403c-a268-4d42ef2d8c5a_3984x5975.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!w4k7!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a409225-b74c-403c-a268-4d42ef2d8c5a_3984x5975.jpeg 424w, https://substackcdn.com/image/fetch/$s_!w4k7!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a409225-b74c-403c-a268-4d42ef2d8c5a_3984x5975.jpeg 848w, https://substackcdn.com/image/fetch/$s_!w4k7!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a409225-b74c-403c-a268-4d42ef2d8c5a_3984x5975.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!w4k7!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a409225-b74c-403c-a268-4d42ef2d8c5a_3984x5975.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!w4k7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a409225-b74c-403c-a268-4d42ef2d8c5a_3984x5975.jpeg" width="1456" height="2184" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7a409225-b74c-403c-a268-4d42ef2d8c5a_3984x5975.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:2184,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:4569856,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/183673318?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a409225-b74c-403c-a268-4d42ef2d8c5a_3984x5975.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!w4k7!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a409225-b74c-403c-a268-4d42ef2d8c5a_3984x5975.jpeg 424w, https://substackcdn.com/image/fetch/$s_!w4k7!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a409225-b74c-403c-a268-4d42ef2d8c5a_3984x5975.jpeg 848w, https://substackcdn.com/image/fetch/$s_!w4k7!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a409225-b74c-403c-a268-4d42ef2d8c5a_3984x5975.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!w4k7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a409225-b74c-403c-a268-4d42ef2d8c5a_3984x5975.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The MI455X and Venice are the two SoCs that are going to be powering AMD’s Helios AI Rack but they aren’t the only new Zen 6 and MI400 series products that AMD announced at CES. AMD announced that there would be a third member of the MI400 family called the MI440X joining the MI430X and MI455X. The MI440X is designed to fit into the 8-way UBB boxes as a direct replacement for the MI300/350 series. </p><p>AMD also announced Venice-X which is likely is going to be a V-Cache version of Venice. This is interesting because not only did AMD skip Turin-X but if there is a 256 core version of Venice-X, then this would be the first time that a high core count CCD will have the ability to support a V-Cache die. If AMD sticks to the same ratio of base die cache to V-Cache die cache, then each 32 core CCD would have up to 384MB of L3 cache which equates to 3 Gigabytes of L3 cache across the chip.</p><p>Both Venice and the MI400 series are due to launch later this year and I can’t wait to learn more about the underlying architectures of both SoCs.</p><p><span>If you like the content then consider heading over to the </span><a href="https://www.patreon.com/ChipsandCheese" rel="">Patreon</a><span> or </span><a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ" rel="">PayPal</a><span> if you want to toss a few bucks to Chips and Cheese, also consider joining the </span><a href="https://discord.gg/TwVnRhxgY2" rel="">Discord</a><span>.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Comparing AI agents to cybersecurity professionals in real-world pen testing (116 pts)]]></title>
            <link>https://arxiv.org/abs/2512.09882</link>
            <guid>46518996</guid>
            <pubDate>Tue, 06 Jan 2026 21:23:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2512.09882">https://arxiv.org/abs/2512.09882</a>, See on <a href="https://news.ycombinator.com/item?id=46518996">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+J+W" rel="nofollow">Justin W. Lin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jones,+E+K" rel="nofollow">Eliot Krzysztof Jones</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jasper,+D+J" rel="nofollow">Donovan Julian Jasper</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ho,+E+J" rel="nofollow">Ethan Jun-shen Ho</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+A" rel="nofollow">Anna Wu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+A+T" rel="nofollow">Arnold Tianyi Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Perry,+N" rel="nofollow">Neil Perry</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zou,+A" rel="nofollow">Andy Zou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fredrikson,+M" rel="nofollow">Matt Fredrikson</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kolter,+J+Z" rel="nofollow">J. Zico Kolter</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liang,+P" rel="nofollow">Percy Liang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Boneh,+D" rel="nofollow">Dan Boneh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ho,+D+E" rel="nofollow">Daniel E. Ho</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2512.09882">View PDF</a>
    <a href="https://arxiv.org/html/2512.09882v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Justin Lin [<a href="https://arxiv.org/show-email/9298da67/2512.09882" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Wed, 10 Dec 2025 18:12:29 UTC (1,057 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Oral microbiome sequencing after taking probiotics (172 pts)]]></title>
            <link>https://blog.booleanbiotech.com/oral-microbiome-biogaia</link>
            <guid>46518804</guid>
            <pubDate>Tue, 06 Jan 2026 21:10:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.booleanbiotech.com/oral-microbiome-biogaia">https://blog.booleanbiotech.com/oral-microbiome-biogaia</a>, See on <a href="https://news.ycombinator.com/item?id=46518804">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>Recently, a friend recommended <a href="https://www.biogaia.com/products/biogaia-prodentis/">BioGaia Prodentis</a> to me.
It is a DTC oral probiotic you can buy online that is supposedly good for oral health.
I thought it would be interesting to do some sequencing to see what, if anything, it did to my oral microbiome.</p>
<p><img src="https://blog.booleanbiotech.com/images/oral_biogaia.jpg" width="100%"></p>
<p><em>BioGaia Prodentis is <a href="https://www.walmart.com/ip/BioGaia-Prodentis-Clinically-Proven-Dental-Probiotics-Teeth-Gums-Promotes-Good-Oral-Health-Gut-Too-30-Mint-Flavored-Lozenges-1-Pack/175541687">available online</a> for $20 or less for a month's supply</em></p>
<h2>BioGaia</h2>
<p><a href="https://www.biogaia.com/collections/all">BioGaia</a> has a fascinating story.
They are a Swedish company, founded over 30 years ago, that exclusively sells probiotics DTC. 
They have developed multiple strains of <em>Limosilactobacillus reuteri</em>, mainly for gut and oral health.
They apparently sell well! Their <a href="https://www.nasdaq.com/european-market-activity/shares/biog-b?id=TX184">market cap</a> is around $1B—impressive for a consumer biotech.</p>
<p>Going in, I expected scant evidence for any real benefits to their probiotics, but the data (<a href="https://www.biogaia.com/pages/probiotic-innovation">over 250 clinical studies</a>) is much more complete than I expected.</p>
<!--
They have been involved in .
I asked [Edison](https://platform.edisonscientific.com/) to assess the BioGaia literature.
Edison said the results were "reasonable, legitimate, and generally trustworthy".
-->

<p>Most notably, their gut probiotic, <a href="https://www.biogaia.com/products/protectis-baby-drops">Protectis</a>,
seems to have a significant effect on preventing
<a href="https://my.clevelandclinic.org/health/diseases/10026-necrotizing-enterocolitis">Necrotizing Enterocolitis</a> (NEC) in premature babies.
According to their <a href="https://ibtherapeutics.com/ibt-is-addressing-urgent-medical-needs-in-the-premature-infant/">website</a>:</p>
<blockquote>
<p>5-10% of the smallest premature infants develop NEC, a potentially lethal disorder in which portions of the bowel undergo tissue death.</p>
</blockquote>
<p>In March 2025, the FDA granted Breakthrough Therapy Designation to <a href="https://ibtherapeutics.com/press-releases/ibt-is-granted-breakthrough-therapy-designation-for-its-drug-candidate/">IBP-9414</a>,
an <em>L. reuteri</em> probiotic developed by BioGaia spinout <a href="https://ibtherapeutics.com/">IBT</a>.</p>
<!--
From a recent [review](https://pmc.ncbi.nlm.nih.gov/articles/PMC8306447/):
>Conclusion: Currently, probiotics are widely used to prevent and treat numerous gastrointestinal disorders. In our opinion, L. reuteri meets all the requirements to be considered a safe, well-tolerated, and efficacious probiotic that is able to contribute to the beneficial effects on gut-human health, preventing and treating many gastrointestinal symptoms, and speeding up the recovery and discharge of patients accessing the emergency department.
-->

<p>This is not specifically for the oral health product, but it's for sure more science than I expected to see going in.</p>
<h3>Prodentis</h3>
<p>BioGaia Prodentis contains two strains of <em>L. reuteri</em>: <a href="https://www.ncbi.nlm.nih.gov/biosample/43393324">DSM 17938</a>
and <a href="https://www.ncbi.nlm.nih.gov/biosample/?term=ATCC%20PTA%205289">ATCC PTA 5289</a>.
The claimed benefits include fresher breath, healthier gums, and outcompeting harmful bacteria.</p>
<h2>Sequencing with Plasmidsaurus</h2>
<p>Many readers will be familiar with <a href="https://plasmidsaurus.com/">Plasmidsaurus</a>.
Founded in 2021, the team took a relatively simple idea:
use massively multiplexed Oxford Nanopore (ONT) to offer complete plasmid sequencing with one day turnaround for $15,
and scaled it.
Plasmidsaurus quickly became part of biotech's core infrastructure, and spread like wildfire.
It also inspired multiple copycats.</p>
<p>Compared to Illumina, ONT is faster and has much longer reads, but lower throughput and lower accuracy.
This profile is a great fit to many sequencing problems like plasmid QC, where you only need megabases of sequences, but want an answer within 24 hours.</p>
<p>Over time, Plasmidsaurus has been adding services, including genome sequencing, RNA-Seq, and microbiome sequencing,
all based on ONT sequencing.</p>
<p><img src="https://blog.booleanbiotech.com/images/oral_plasmidsaurus_list.png" width="100%"></p>
<p><em>Plasmidsaurus accepts many kinds of sample for microbiome sequencing</em></p>
<p>I used their <a href="https://plasmidsaurus.com/microbiome">16S sequencing product</a>, which costs $45 for ~5000 reads, plus $15 for DNA extraction.
16S sequencing is an efficient way to amplify and sequence a small amount of DNA (the ubiquitous 16S region)
and be able to assign reads to specific species or even strains.</p>
<p>This experiment cost me $240 for four samples, and I got data back in around a week.
It's very convenient that I no longer have to do my <a href="https://blog.booleanbiotech.com/human-genome-at-home">own</a> <a href="https://blog.booleanbiotech.com/sequencing-at-home-with-flongle">sequencing</a>.
As a side note, you can pay more for more than 5000 reads, but unless you want information on very rare strains (&lt;&lt;1% frequency),
this is a waste of money.</p>
<p>Sample collection is simple: take 100-250 µL of saliva and mix with 500 µL of <a href="https://www.zymoresearch.com/collections/dna-rna-shield">Zymo DNA/RNA Shield</a>
(which I also had to buy for around $70.) You also need <a href="https://www.amazon.com/dp/B09DYWX8Q2?ref=ppx_yo2ov_dt_b_fed_asin_title">2 mL screwtop tubes</a> to ship in.</p>
<p>The reads are high quality for nanopore sequencing, with a median Q score of 23 (99%+ accuracy).
This is more than sufficient accuracy for this experiment.
The read length is very tightly distributed around 1500 nt (the length of a typical 16S region).</p>
<p>The results provided by Plasmidsaurus include taxonomy tables, per-read assignments, and some basic plots.
I include a download of the results at the end of this article, as well as the FASTQ files.</p>
<h2>The experiment</h2>
<p>The main idea of the experiment was to see if any <em>L. reuteri</em> would colonize by the end of 30 days of probiotic use,
and if so, whether it would persist beyond that.
I collected four saliva samples:</p>
<table>
<thead>
<tr>
<th>Sample</th>
<th>Timing</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baseline A</td>
<td>Day -4</td>
<td>A few days before starting BioGaia</td>
</tr>
<tr>
<td>Baseline B</td>
<td>Day -1</td>
<td>The day before I started BioGaia</td>
</tr>
<tr>
<td>Day 30</td>
<td>Day 30</td>
<td>The last day of the 30 day course</td>
</tr>
<tr>
<td>Week after</td>
<td>Day 37</td>
<td>One week after completing the course</td>
</tr>
</tbody>
</table>
<p><img src="https://blog.booleanbiotech.com/images/oral_heatmap.svg"></p>
<p><em>Heatmap of the top 20 species. All species assignments were done by <a href="https://plasmidsaurus.com/technical-documentation/microbiome">Plasmidsaurus</a></em></p>
<h2>Did <em>L. reuteri</em> colonize?</h2>
<p>There was no <em>L. reuteri</em> found in any of the samples.
I did a manual analysis to check for any possible misassignments,
but the closest read was only 91% identical to either <em>L. reuteri</em> strain.</p>
<p>The probiotic either (a) didn't colonize the oral cavity;
(b) was present only transiently while actively taking the lozenges;
(c) was below the detection threshold.</p>
<p>Probiotics are generally bad at colonizing, which is why you have to keep taking them.
Still, I was surprised not to see a single <em>L. reuteri</em> read in there.</p>
<h2>What actually changed?</h2>
<p>Even though the probiotic itself didn't show up,
the oral microbiome did change quite a lot.</p>
<p>The most striking change was a massive increase in <em>S. salivarius</em>.
<em>S. salivarius</em> went from essentially absent to ~20% of my oral microbiome on the last day.
However, this happened <em>one week after</em> I stopped taking the probiotic,
so it's very unclear if it is related.</p>
<table>
<thead>
<tr>
<th>Sample</th>
<th><em>S. mitis</em></th>
<th><em>S. salivarius</em></th>
</tr>
</thead>
<tbody>
<tr>
<td>Baseline A</td>
<td>2.0%</td>
<td>0.4%</td>
</tr>
<tr>
<td>Baseline B</td>
<td><strong>15.9%</strong></td>
<td>0.0%</td>
</tr>
<tr>
<td>Day 30</td>
<td><strong>10.2%</strong></td>
<td>0.8%</td>
</tr>
<tr>
<td>Week after</td>
<td>1.0%</td>
<td><strong>19.3%</strong></td>
</tr>
</tbody>
</table>
<p>We see <em>S. mitis</em> decreasing as <em>S. salivarius</em> increases, 
while the total Streptococcus fraction stayed roughly stable.
It's possible one species replaced the other within the same ecological niche.</p>
<p><em>S. salivarius</em> is itself a probiotic species.
The strain <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC1449041/">BLIS K12</a>
was isolated from a healthy New Zealand child and is sold commercially for oral health.
It produces bacteriocins that kill <em>Streptococcus pyogenes</em> (strep throat bacteria).</p>
<p>At the same time, <em>V. tobetsuensis</em> increased in abundance from 2.1% to 5.7%.
Veillonella bacteria can't eat sugar directly—they survive by consuming lactate that Streptococcus produces.
The <em>S. salivarius</em> bloom is plausibly feeding them.</p>
<!--
## A bacterial friendship dissolved

*Haemophilus parainfluenzae* and *Streptococcus mitis* are known to
[live adjacent to each other](https://www.nature.com/articles/s41396-021-01141-3)
in oral biofilms — they're positively correlated in healthy mouths.

Both decreased together in my samples.
The Haemophilus species (*H. parainfluenzae*, *H. parahaemolyticus*, *H. haemolyticus*)
collectively dropped from ~14% to ~7%, with some species disappearing entirely.

Interestingly, *H. parainfluenzae* is
[associated with good oral health](https://pmc.ncbi.nlm.nih.gov/articles/PMC11406952/),
not bad — so losing it isn't necessarily a win.
The oral microbiome shifted from Proteobacteria (Haemophilus) toward Firmicutes (Streptococcus).
-->

<h2>Are these changes real or intra-day variation?</h2>
<p>There was a lot more variation in species than I expected, especially comparing the two baseline samples. 
In retrospect, I should have taken multiple samples on the same day, and mixed them to smooth it out.</p>
<p>However, there is some light evidence that the variation I see is not just intra-day variation.
Specifically, there are several species that stay consistent in frequency across all samples:
e.g., <em>Neisseria subflava</em>, <em>Streptococcus viridans</em>, <em>Streptococcus oralis</em>.</p>
<!--
The *S. salivarius* increase shows variability **6x beyond baseline noise**,
strongly suggesting this is a real biological change, not random fluctuation.

**Caveat:** *S. mitis* was already highly variable at baseline (2% vs 16%),
so some species are naturally dynamic in the oral microbiome.
But the *S. salivarius* signal (0% → 19%) is unambiguous.
-->

<h2>Conclusions</h2>
<ul>
<li><em>L. reuteri</em> didn't detectably colonize my mouth. Oral probiotics are surprisingly difficult to detect, even if you sample the same day as dosing.</li>
<li><em>S. salivarius</em> increased massively in abundance, but this increase happened after I stopped taking BioGaia</li>
<li>Microbiome sequencing can be used to assess oral health. None of the <a href="https://en.wikipedia.org/wiki/Red_complex">"red complex"</a> bacteria (<em>P. gingivalis</em>, <em>T. forsythia</em>, <em>T. denticola</em>) associated with gum disease were found in any sample.</li>
<li>The oral microbiome is dynamic, with huge swings in species abundance over a timeframe of just weeks</li>
<li>Microbiome sequencing is very easy and convenient these days thanks to Plasmidsaurus</li>
<li>Prodentis tastes good, may help with oral health, and I'd consider taking it again</li>
</ul>
<h2>Data</h2>
<ul>
<li><a href="https://public.booleanbiotech.com/GVNLJW_fastq.zip">Download Raw FASTQ (50MB)</a></li>
<li><a href="https://public.booleanbiotech.com/GVNLJW_results.zip">Download Analysis Results (3MB)</a></li>
</ul>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A 30B Qwen model walks into a Raspberry Pi and runs in real time (304 pts)]]></title>
            <link>https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/</link>
            <guid>46518573</guid>
            <pubDate>Tue, 06 Jan 2026 20:55:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/">https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/</a>, See on <a href="https://news.ycombinator.com/item?id=46518573">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="blog-content">
      

      <div>
        <section>
          <p>
            For this release, we optimize for what people actually experience when they run a model: 
            <strong>fast, high-quality responses on a specific target device</strong>.
          </p>

          <p>
            We use Shapelearn, our bitlength learning method to choose weight datatypes for 
            <strong>Qwen3-30B-A3B-Instruct-2507</strong> that maximize performance in terms of 
            tokens per second (TPS) and output quality, with one practical constraint: the model 
            must fit comfortably in the available memory. Once it fits, making the file smaller 
            isn't a goal by itself. We only shrink further when it also improves the real tradeoff 
            people care about: <strong>speed vs. quality</strong>.
          </p>

          <p>
            Approaching bitlength learning this way matters because in llama.cpp, "fewer bits" doesn't 
            automatically mean "more speed." Different quantization formats can trigger different kernels 
            and overheads, and on some GPUs, <strong>going lower-bit can even get slower</strong>, despite 
            using less memory.
          </p>

          <p>
            <strong>Bottom line:</strong> treat memory as a <strong>budget</strong> to meet, then optimize what 
            matters most: <strong>TPS and quality</strong>.
          </p>
        </section>

        

        <section>
          <h2>TL;DR</h2>
          <p>
            Yes, this 30B Qwen3 runs on a Raspberry Pi. On a Pi 5 (16GB),
            <code>
              <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-2.70bpw.gguf" target="_blank" rel="noopener noreferrer">
                Q3_K_S-2.70bpw [KQ-2]
              </a>
            </code>
            hits 8.03 TPS at 2.70 BPW and maintains 94.18% of BF16 quality. It genuinely feels
            real-time. More broadly, the same pattern shows up everywhere else: ByteShape models
            give you a better TPS/quality tradeoff than the alternatives (here we look at Unsloth
            and MagicQuant).
          </p>
        </section>

        <section>
          <h2>CPUs</h2>
          <p>
            On CPUs, the reducing footprint via shorter bitlengths affects the TPS and accuracy
            tradeoff as one would expect: once the model fits, reducing footprint tends to increase
            TPS in a fairly monotonic way. If datatypes are selected correctly, you can trade a bit
            of quality for speed predictably, which makes it much easier to pick a point on the
            curve that matches your constraints.
          </p>

          <p>
            We'll start with the most memory-constrained CPU case (Raspberry Pi 5 16GB), where
            "fits in RAM" is the limiting factor, then move to an Intel i7 with 64GB, where
            everything fits.
          </p>

          <h3>Raspberry Pi 5</h3>
          <p>
            The figure below shows TPS vs. normalized accuracy for the models that fit in RAM on
            the Raspberry Pi 5 16GB.
          </p>

          <!-- Mobile version: PNG -->
          <figure>
            <img src="https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/images/averagen_bubble_chart_bpw_size_pi.png" alt="Raspberry Pi 5 performance: tokens per second vs quality">
            <figcaption>
              Raspberry Pi 5: Tokens per second vs quality (bubble size = model footprint)
            </figcaption>
          </figure>
          <!-- Desktop version: Interactive HTML -->
          <figure>
            
            <figcaption>
              Raspberry Pi 5: Tokens per second vs quality (bubble size = model footprint)
            </figcaption>
          </figure>

          <p>
            Notably, sustaining <strong>8.5 TPS at 92%+ baseline accuracy</strong> with a 30B model on a 
            Raspberry Pi reshapes expectations for Pi-class systems. Overall, the trend shows that ShapeLearn 
            consistently produces better models, with ByteShape trending up and to the right of Unsloth, 
            achieving higher tokens per second at the same quality, or higher quality at the same throughput.
          </p>

          <p>We highlight choices for two primary objectives: accuracy or response time.</p>

          <ul>
            <li>
              <strong>Optimizing for response time while maintaining accuracy:</strong> For interactive, 
              on-device use, perceived responsiveness is driven by how quickly text appears, not peak 
              throughput. In practice, generation feels real-time once it reaches roughly <strong>8 TPS</strong>, 
              comfortably above typical reading speed. In this Raspberry Pi real-time regime, 
              <code>
                <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-2.70bpw.gguf" target="_blank" rel="noopener noreferrer">
                  Q3_K_S-2.70bpw [KQ-2]
                </a>
              </code>
              (2.70 BPW, 8.03 TPS, 94.18% accuracy) is our go-to recommendation: it crosses the
              real-time threshold while maintaining high accuracy. Compared 
              to Unsloth models at similar quality, ByteShape achieves real-time performance at lower BPW 
              and higher TPS, making it the more efficient choice for interactive edge deployment.
            </li>
            <li>
              <strong>Accuracy above all:</strong> The table below lists the models that achieve the highest 
              accuracy while still being able to run on a Raspberry Pi. Within this set, ByteShape models 
              make the best use of the available resources to maximize accuracy, occupying the 
              <strong>lowest-error rows</strong> (~1.1–1.3% relative error, ~98.8% accuracy), while the 
              strongest Unsloth entries remain around 2.1–2.2% error (~97.9% accuracy). Compared to Unsloth's 
              <code>UD-Q3_K_XL [8]</code>, ByteShape achieves up to a <strong>1.87× lower error rate</strong> 
              while still operating at <strong>~5–6 TPS</strong>, comfortably within TPS-norms on Raspberry PI 
              making it the better choice when accuracy is the priority. <br>
              Even when prioritizing maximum speed with some reduction in accuracy,
              <code>
                <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-3.25bpw.gguf" target="_blank" rel="noopener noreferrer">
                  Q3_K_S-3.25bpw [KQ-5]
                </a>
              </code>
              offers a better tradeoff: <strong>more accurate, smaller, and faster</strong> than
              the fastest Unsloth model.
            </li>
          </ul>

          <table>
            <thead>
              <tr>
                <th>Model</th>
                <th>Relative Error</th>
                <th>BPW</th>
                <th>TPS</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  <code>
                    <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q4_K_S-3.92bpw.gguf" target="_blank" rel="noopener noreferrer">
                      Q4_K_S-3.92bpw [KQ-7]
                    </a>
                  </code>
                </td>
                <td>1.14%</td>
                <td>3.92</td>
                <td>5.30</td>
              </tr>
              <tr>
                <td>
                  <code>
                    <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q4_K_S-3.61bpw.gguf" target="_blank" rel="noopener noreferrer">
                      Q4_K_S-3.61bpw [KQ-6]
                    </a>
                  </code>
                </td>
                <td>1.25%</td>
                <td>3.61</td>
                <td>5.94</td>
              </tr>
              <tr>
                <td>
                  <code>
                    <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-3.25bpw.gguf" target="_blank" rel="noopener noreferrer">
                      Q3_K_S-3.25bpw [KQ-5]
                    </a>
                  </code>
                </td>
                <td>2.03%</td>
                <td>3.25</td>
                <td>6.68</td>
              </tr>
              <tr>
                <td><code>UD-IQ3_XXS [6]</code></td>
                <td>2.22%</td>
                <td>3.38</td>
                <td>5.03</td>
              </tr>
              <tr>
                <td><code>UD-Q3_K_XL [8]</code></td>
                <td>2.13%</td>
                <td>3.62</td>
                <td>6.28</td>
              </tr>
            </tbody>
          </table>

          <p>
            Many other Unsloth and MagicQuant models (some of ours too!) are not in this chart. We compare 
            them in other sections, but they're not applicable in the Raspberry Pi case. They simply don't fit!
          </p>

          <h3>Intel i7</h3>
          <p>
            Next, we move to the Intel i7 with 64GB RAM. The figure below shows TPS vs 
            normalized accuracy for all models.
          </p>

          <!-- Mobile version: PNG -->
          <figure>
            <img src="https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/images/averagen_bubble_chart_bpw_size_i7.png" alt="Intel i7 performance: tokens per second vs quality">
            <figcaption>
              Intel i7: Tokens per second vs quality (bubble size = model footprint)
            </figcaption>
          </figure>
          <!-- Desktop version: Interactive HTML -->
          <figure>
            
            <figcaption>
              Intel i7: Tokens per second vs quality (bubble size = model footprint)
            </figcaption>
          </figure>

          <p>
            Overall, ByteShape models outperform both Unsloth and MagicQuant, delivering higher quality at 
            comparable throughput using fewer bits per parameter. Only ByteShape offers models that run in 
            the 26+ TPS range, extending performance well beyond the other methods.
          </p>

          <p><strong>Highlights:</strong></p>

          <ul>
            <li>
              <strong>Quality-first:</strong> At the high-accuracy end of the table,
              <code>
                <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-IQ4_XS-4.67bpw.gguf" target="_blank" rel="noopener noreferrer">
                  IQ4_XS-4.67bpw [KQ-9]
                </a>
              </code>
              achieves the lowest relative error (0.25%), outperforming the best-running Unsloth
              models (<code>Q6_K [20]</code> and <code>Q5_K_M [18]</code> whose 
              relative errors are 0.36% and 0.44%). Compared directly, ByteShape delivers up to a 1.44× lower 
              error rate with higher throughput than <code>Q6_K [20]</code>, and a 1.76× lower error rate at 
              essentially the same speed as <code>Q5_K_M [18]</code>. MagicQuant <code>mxfp4 [3]</code> trails 
              in this regime, with both higher error and lower TPS.
            </li>
            <li>
              <strong>Balanced point:</strong> In the mid-accuracy, high-throughput region,
              <code>
                <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-3.25bpw.gguf" target="_blank" rel="noopener noreferrer">
                  Q3_K_S-3.25bpw [KQ-5]
                </a>
              </code>
              combines ~98% accuracy with 23.1 TPS at just 3.25 BPW, offering the best overall
              balance in the table. Matching or exceeding this accuracy with Unsloth 
              (<code>IQ4_XS [10]</code>) requires higher BPW and lower TPS, while choosing an Unsloth model 
              closer in speed (<code>Q3_K_S [7]</code>) incurs a 1.73× higher error rate. MagicQuant does not 
              offer a competitive model in this range; its fastest entry (<code>IQ4_NL [2]</code>) is behind both 
              ByteShape and Unsloth in accuracy and throughput.
            </li>
          </ul>

          <p>
            <strong>Takeaway:</strong> Across both quality-first and balanced settings, ByteShape consistently 
            converts the available bit budget into either higher accuracy or higher TPS, and is the only approach 
            that simultaneously covers the high-quality and 26+ TPS balanced-performance regions in this comparison.
          </p>
        </section>

        <section>
          <h2>GPUs: RTX5090/32GB and RTX4080/16GB</h2>
          <p>
            On GPUs, performance depends as much on <strong>kernel choice</strong> as on raw memory footprint. 
            For matmul/matvec, llama.cpp's quantization-specific GPU decode paths incur very different overheads, 
            so fewer bits per weight do <strong>not</strong> reliably translate to higher TPS. Instead, TPS often 
            peaks at quantization-specific sweet spots. Pushing BPW lower can even <strong>increase VRAM traffic 
            and instruction count</strong>, hurting performance rather than improving it. We dig into this behavior in 
            more detail right after the GPU results section, where the kernel-level tradeoffs become more apparent.
          </p>

          <p>
            We evaluate on two GPUs: an <strong>RTX 5090 (32 GB)</strong>, which can run models 
            <strong>above 4 BPW</strong> and typically reach the fastest sweet spots, and an 
            <strong>RTX 4080 (16 GB)</strong>, where <strong>&gt;4 BPW models do not fit</strong>, forcing 
            different trade-offs and making the device-optimized curve easier to see.
          </p>

          <h3>RTX 5090 (32GB of VRAM)</h3>
          <p>
            Let's start with the 5090, which has enough VRAM to support all of the quantized models. The figure 
            below shows TPS vs normalized accuracy.
          </p>

          <!-- Mobile version: PNG -->
          <figure>
            <img src="https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/images/averagen_bubble_chart_bpw_size_5090.png" alt="RTX 5090 performance: tokens per second vs quality">
            <figcaption>
              RTX 5090: Tokens per second vs quality (bubble size = model footprint)
            </figcaption>
          </figure>
          <!-- Desktop version: Interactive HTML -->
          <figure>
            
            <figcaption>
              RTX 5090: Tokens per second vs quality (bubble size = model footprint)
            </figcaption>
          </figure>

          <p>
            Two things stand out immediately:<br>
            First, this GPU shows a clear <strong>~4-bit sweet spot</strong>: several ~4b models
            cluster at very high TPS with nearly identical quality. Examples include
            <code>Unsloth Q4_0 [12]</code>, <code>Unsloth IQ4_XS [10]</code>,
            <code>
              <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-IQ4_XS-3.87bpw.gguf" target="_blank" rel="noopener noreferrer">
                IQ4_XS-3.87bpw [IQ-6]
              </a>
            </code>
            , and MagicQuant <code>iq4_nl-EHQKOUD-IQ4NL [1]</code>, all running around ~302–303 TPS
            at ~98.4–98.9% accuracy. Within 
            this tight cluster, Unsloth edges out slightly in throughput and quality.
          </p>

          <p>
            Second, outside of that sweet spot, the tradeoff becomes much more uneven:
          </p>

          <ul>
            <li>
              Many other Unsloth and Magic Quant models show <strong>significantly lower TPS</strong>, regardless 
              of whether they are quantized more or less aggressively.
            </li>
            <li>
              Past the ~4b region, only ByteShape continues to increase TPS with a more predictable reduction in quality.
            </li>
          </ul>

          <p>
            <strong>Accuracy-critical workloads:</strong> when output quality is paramount,
            ByteShape delivers the most accurate model on the 5090:
            <code>
              <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-IQ4_XS-4.67bpw.gguf" target="_blank" rel="noopener noreferrer">
                IQ4_XS-4.67bpw [IQ-8]
              </a>
            </code>
            (4.67 BPW, 272.98 TPS, 99.75% accuracy). It surpasses <code>Unsloth Q6_K [20]</code> (6.57 BPW, 264.88 TPS, 99.64% accuracy) 
            while using fewer bits and achieving slightly higher throughput, and it clearly outperforms MagicQuant 
            <code>mxfp4_moe-H-B16-EUR-IQ4NL-KO-Q5K-QD-Q6K [3]</code> (5.46 BPW, 240.42 TPS, 99.32% accuracy) in both 
            accuracy and speed, making it the strongest choice when accuracy is a task-critical deployment requirement.
          </p>

          <p>
            <br><strong>Practical takeaway.</strong> If your GPU has enough VRAM to run a strong ~4b model that 
            already meets your speed and accuracy requirements, that cluster is an excellent default. The curve becomes 
            more interesting when task-critical deployment constraints demand higher accuracy or smaller models as for 
            example, under tighter memory budgets or constrained environments (as we'll see on the 4080).
          </p>

          <h3>RTX 4080 (16GB of VRAM)</h3>
          <p>
            Next, let's move to a more accessible GPU, especially in these memory-challenged times. The biggest 
            stumbling block for the 4080 is its 16GB of VRAM, which is not sufficient to support the "magical" 
            ~4b quantizations for a 30B model. How convenient!<strong> </strong>This "avoids" the 5090's ~4b sweet 
            spot and forces a more "real-world" comparison under a hard VRAM budget. The figure below shows TPS versus 
            normalized accuracy for all models that fit on the 4080.
          </p>

          <!-- Mobile version: PNG -->
          <figure>
            <img src="https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/images/averagen_bubble_chart_bpw_size_4080.png" alt="RTX 4080 performance: tokens per second vs quality">
            <figcaption>
              RTX 4080: Tokens per second vs quality (bubble size = model footprint)
            </figcaption>
          </figure>
          <!-- Desktop version: Interactive HTML -->
          <figure>
            
            <figcaption>
              RTX 4080: Tokens per second vs quality (bubble size = model footprint)
            </figcaption>
          </figure>

          <p>
            On the RTX 4080, ByteShape consistently outperforms Unsloth under the same 16 GB VRAM constraint, 
            delivering a better TPS–quality tradeoff.
          </p>

          <p>
            In particular, ByteShape's highest-quality model that fits,
            <code>
              <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-IQ4_XS-3.87bpw.gguf" target="_blank" rel="noopener noreferrer">
                IQ4_XS-3.87bpw [IQ-6]
              </a>
            </code>
            (3.87 BPW, 214.81 TPS, 98.66% accuracy) delivers:
          </p>

          <ul>
            <li>
              a 1.59× lower error rate and 9.4% higher TPS vs. <code>Unsloth Q3_K_XL [8]</code> (3.62 BPW, 196.42 TPS, 
              97.87% accuracy).
            </li>
            <li>
              a 2.54× lower error rate at the same TPS vs. <code>Unsloth IQ2_M [2]</code> (2.84 BPW, 214.79 TPS, 
              96.59% accuracy).
            </li>
          </ul>

          <p>
            As we move to higher throughput, ByteShape's maintains accuracy, while Unsloth's error rate experiences a cliff.
          </p>

          <h3>The Elephant in the Room: When 3-bits is not just 3-bits</h3>
          <p>
            There is an inconvenient truth hiding in these results. On several setups, around 4 bpw is already 
            flying, and pushing quantization harder does not make things faster. It just manages to be smaller 
            and slower at the same time.
          </p>

          <p>
            Reducing the size of data doesn't automatically speed things up. While using fewer bits to store each 
            number seems like it should reduce memory traffic and speed up computation, GPUs don't work that way. 
            NVIDIA GPUs process work in fixed groups of 32 threads called "warps," which move through instructions 
            together in near lock-step. The GPU hardware is optimized for specific data formats, memory access patterns, 
            and operations that the chip's circuits are physically designed to handle efficiently. When your workload 
            matches these "golden paths", you get peak performance. Step outside them, and you hit slowdowns. This isn't 
            a design flaw, it's a deliberate tradeoff. Supporting more flexibility would require additional circuitry: 
            more wires, more transistors, more complexity. That extra hardware consumes more power and adds latency to 
            every operation, whether a program needs that flexibility or not.
          </p>

          <p>
            Here a few examples of relevant hardware "quirks": VRAM is read in aligned 32-byte blocks, so reading one 
            or 32 bytes consumes the same memory bandwidth. Both on-chip and off-chip memories can also suffer contention 
            depending on how data is laid out, meaning that a warp's accesses may complete in a single step or, in the 
            worst case, be serialized into 32 steps. And of course, decoding quantized values before computation can 
            require extra instructions, with the cost depending on the quantization scheme.
          </p>

          <p>
            This explains the behaviour we observe: 4-bit kernels use VRAM bandwidth more efficiently than 3- or 2-bit 
            kernels and require fewer decode steps before computation. At the same time, 4-bit kernels exploit subword 
            parallelism just as effectively as lower-bit kernels, and all rely primarily on dynamic caches rather than 
            shared memory to take advantage of data reuse when possible.
          </p>

          <p>
            So why llama.cpp hasn't been optimized to deliver peak speed for <strong>every</strong> bit-length? Our understanding 
            is that llama.cpp prioritizes <strong>portable, space-efficient quantization</strong> that can run across a 
            wide range of hardware. That design goal limits how aggressively backends can reshape data layouts or reorder 
            computation in ways that might help one GPU or one bit-width.
          </p>

          <p>
            A key example is its choice to store quantized weights in fixed blocks of 256 values. Each block is 
            self-contained (it carries everything needed to decode it) and sits at a simple, predictable offset in the 
            tensor, which makes the format easy to implement and fast to locate.
          </p>

          <p>
            The tradeoff is that GPUs often need to <strong>decode many blocks in parallel</strong> to keep their wide 
            compute units busy. With many independent 256-value blocks, those parallel decodes can translate into more 
            scattered or fragmented VRAM reads and extra decode overhead, reducing bandwidth efficiency, especially for 
            some lower-bit formats.
          </p>

          <p>
            <strong>Point for example on RTX 5090:</strong> a matrix multiply [256, 768] × [768, 2048] takes 
            <strong>~54µs with </strong><code>iq4_xs</code><strong> </strong>datatype, but <strong>~62µs with </strong>
            <code>iq3_xxs</code><strong> </strong>(mul_mat_q()+mul_mat_q_stream_k_fixup()). In other words, 
            <strong>cutting nearly 1.2 bits per weight </strong>(a reduction of more than 25% in weight footprint) leads 
            to a <strong>~13% slowdown</strong>, directly hurting user experience.
          </p>

          <p>
            An excellent reminder that bitlength learning matters: Heuristics can get us part of the way, but not all 
            the way. ShapeLearn makes deliberate, per-tensor datatype choices that improve speed without sacrificing accuracy.
          </p>
        </section>

        <section>
          <h2>Methodology (brief recap)</h2>
          <p>
            If you're wondering how we are scoring these points, the full methodology is discussed in our previous 
            <a href="https://byteshape.com/blogs/Qwen3-4B-I-2507/">blog post</a>. This post is intentionally focused on the curves and 
            device tradeoffs, so here is the quick version.
          </p>

          <p>
            For each quantized variant, we measure <strong>throughput (TPS)</strong> on the target device and 
            compute a single <strong>normalized quality</strong> score relative to the <strong>BF16 baseline</strong>, 
            using the same evaluation harness and prompts as the methodology post. The quality score aggregates 
            standard benchmarks (MMLU, GSM8K, IFEval, LiveCodeBench V4) into one number so you can compare points 
            directly. In other words, every dot in the plots answers two questions: how fast does it run on this 
            device, and how much quality does it retain compared to BF16, with memory fit as the first constraint.
          </p>

          <p>
            We also want to thank all for the many, excellent suggestions on our recent Reddit post for improving 
            and extending this evaluation strategy, and we’re actively working through them. Right now, evaluation 
            is the main bottleneck and not bitlength learning/quantization.
            Careful evaluation is essential to clearly communicate the strengths of each model.
          </p>
        </section>

        <section>
          <h2>Wrapping up</h2>
          <p>
            <strong>First, thank you for your tenacity.</strong> You made it through all of this without giving up. 
            We are sincerely flattered!
          </p>

          <p>
            <strong>The takeaway is simple:</strong> treat <strong>memory as a constraint, not a goal</strong>. Once 
            a model fits on your device, what matters is the tradeoff curve, <strong>TPS versus quality</strong>. 
            Across CPUs and GPUs, <strong>ByteShape</strong> consistently lands on the better side of that curve, 
            delivering either <strong>more speed at the same quality</strong> or <strong>higher quality at the same 
            speed</strong>.
          </p>

          <p>
            If you're deploying on a <strong>Raspberry Pi 5 (16 GB)</strong> and want a genuinely
            interactive experience, start with
            <code>
              <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-2.70bpw.gguf" target="_blank" rel="noopener noreferrer">
                Q3_K_S-2.70bpw [KQ-2]
              </a>
            </code>
            . On larger CPUs or GPUs, you can move up the curve toward higher-quality points with
            little loss in throughput, the same rule applies:
            <strong>fit first, then optimize the tradeoff</strong>.
          </p>
          

          <p>
            We'll keep releasing more device-targeted variants (and more plots). If your system can't run a 30B 
            model smoothly, don't blame the model or the silicon. <strong>Blame the datatypes.</strong>
          </p>
        </section>
      </div>
    </article></div>]]></description>
        </item>
    </channel>
</rss>