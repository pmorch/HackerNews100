<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 19 Aug 2023 08:00:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Morris Chang founded TSMC, the most valuable company in Asia (106 pts)]]></title>
            <link>https://www.abc.net.au/news/2023-08-19/tsmc-the-most-important-company-in-the-world/102728172</link>
            <guid>37184117</guid>
            <pubDate>Sat, 19 Aug 2023 00:36:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.abc.net.au/news/2023-08-19/tsmc-the-most-important-company-in-the-world/102728172">https://www.abc.net.au/news/2023-08-19/tsmc-the-most-important-company-in-the-world/102728172</a>, See on <a href="https://news.ycombinator.com/item?id=37184117">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Microchips run everything. They're in your phone, your car, your microwave — there might even be one in your kettle.</p><p>The best microchips in the world are made by TSMC, the Taiwan Semiconductor Manufacturing Company.</p><p>TSMC is so good, they've put Western tech companies and militaries streets ahead of China — and China is not happy about it.</p><p>So how did TSMC become the most important company in the world, and could it bring the United States and China to the brink of war?</p><h2 data-component="Heading">The birth of an idea</h2><p>In 1958, a then 27-year-old Morris Chang was working at a microchip factory in Dallas belonging to Texas Instruments, and he had a problem.</p><p>The process of making them was so complex, almost none of the chips were working correctly.</p><p>This was a Harvard-educated man who fled from China as a teen. It was time to prove himself.</p><figure role="group" data-component="VerticalArticleFigure" aria-labelledby="102732206" data-uri="coremedia://imageproxy/102732206"><img alt="Morris Chang" sizes="(max-width: 543px) 543px," srcset="https://live-production.wcms.abc-cdn.net.au/f26a241d6bfd55faa58db6c0fe49e0e0?impolicy=wcms_crop_resize&amp;cropH=1650&amp;cropW=2200&amp;xPos=0&amp;yPos=36&amp;width=862&amp;height=647 543w, https://live-production.wcms.abc-cdn.net.au/f26a241d6bfd55faa58db6c0fe49e0e0?impolicy=wcms_crop_resize&amp;cropH=1467&amp;cropW=2200&amp;xPos=0&amp;yPos=128&amp;width=862&amp;height=575" src="https://live-production.wcms.abc-cdn.net.au/f26a241d6bfd55faa58db6c0fe49e0e0?impolicy=wcms_crop_resize&amp;cropH=1467&amp;cropW=2200&amp;xPos=0&amp;yPos=128&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"><p><figcaption id="102732206" data-component="VerticalArticleFigure__figcaption" aria-live="polite"> <!-- -->Morris Chang founded TSMC, the world's largest contract chipmaker.<span data-component="Byline"><span data-component="Text">(<span>Reuters: Nicky Loh</span>)</span></span></figcaption></p></figure><p>He started fiddling with the factory's settings, and within weeks, the factory's yield was up to 20 per cent, then 30 per cent.</p><p>His bosses noticed how successful he'd been, and promoted him. Texas Instruments paid for him to get his PhD. They gave him bigger and bigger responsibilities.</p><figure role="group" data-component="VerticalArticleFigure" aria-labelledby="102732264" data-uri="coremedia://imageproxy/102732264"><img alt="Man with white hair and glasses wearing a suit smokes a pipe in side profile." sizes="(max-width: 543px) 543px," srcset="https://live-production.wcms.abc-cdn.net.au/193f0d5014f187e4a71f2b5b0cac7b8c?impolicy=wcms_crop_resize&amp;cropH=3333&amp;cropW=4444&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=647 543w, https://live-production.wcms.abc-cdn.net.au/193f0d5014f187e4a71f2b5b0cac7b8c?impolicy=wcms_crop_resize&amp;cropH=3333&amp;cropW=5000&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=575" src="https://live-production.wcms.abc-cdn.net.au/193f0d5014f187e4a71f2b5b0cac7b8c?impolicy=wcms_crop_resize&amp;cropH=3333&amp;cropW=5000&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"><p><figcaption id="102732264" data-component="VerticalArticleFigure__figcaption" aria-live="polite"> <!-- -->Morris Chang has weathered decades at the valuable microchip company.<span data-component="Byline"><span data-component="Text">(<span>Reuters: Eason Lam</span>)</span></span></figcaption></p></figure><p>Years went by, and Chang developed an idea that would change the course of his life, and the future of the world.</p><p>You see, just like Texas Instruments, all tech companies at the time were expending enormous amounts of time and effort figuring out how to run good in-house microchip factories.</p><p>But Chang thought that was silly.</p><p>Microchips are probably the hardest thing in the world to manufacture. It requires unbelievably complicated and expensive machines, running in the cleanest environment on earth.</p><p>Chang thought the tech companies should focus on designing the microchips, and outsource the actual manufacture to a company that specialises in it.</p><p>Chang suggested this to Texas Instruments over and over, but they never budged.</p><p>Eventually, he&nbsp;was shifted sideways into a dead end role, and he quit.</p><figure role="group" data-component="VerticalArticleFigure" aria-labelledby="102736950" data-uri="coremedia://imageproxy/102736950"><img alt="Man wearing glasses in a suit looks serious with hands clasped in front of his chest." sizes="(max-width: 543px) 543px," srcset="https://live-production.wcms.abc-cdn.net.au/fd79f0b583fbd0aae9770057b57f4d25?impolicy=wcms_crop_resize&amp;cropH=3333&amp;cropW=4444&amp;xPos=406&amp;yPos=0&amp;width=862&amp;height=647 543w, https://live-production.wcms.abc-cdn.net.au/fd79f0b583fbd0aae9770057b57f4d25?impolicy=wcms_crop_resize&amp;cropH=3333&amp;cropW=5000&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=575" src="https://live-production.wcms.abc-cdn.net.au/fd79f0b583fbd0aae9770057b57f4d25?impolicy=wcms_crop_resize&amp;cropH=3333&amp;cropW=5000&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"><p><figcaption id="102736950" data-component="VerticalArticleFigure__figcaption" aria-live="polite"> <!-- -->Morris&nbsp;Chang, 92, has retired from TSMC but still appears at public events.<span data-component="Byline"><span data-component="Text">(<span>Reuters: Ann Wang</span>)</span></span></figcaption></p></figure><p>While Texas Instruments did not see Chang's vision, Taiwan did.</p><p>The Taiwanese government offered him a blank cheque to build a company that would make Taiwan the microchip capital of the world — the place all tech companies go to get their chips made.</p><p>They weren't going to design a perfect microchip. They were going to design the perfect microchip factory.</p><p>And thus, the Taiwanese Semiconductor Manufacturing Company was born, with Chang at its head.</p><h2 data-component="Heading">TSMC dominates the industry</h2><figure role="group" data-component="VerticalArticleFigure" aria-labelledby="102732412" data-uri="coremedia://imageproxy/102732412"><img alt="A person walks pass a&nbsp;building with a red and white TSMC sign." sizes="(max-width: 543px) 543px," srcset="https://live-production.wcms.abc-cdn.net.au/1259a096e9aee1337a635bf18f25b0c5?impolicy=wcms_crop_resize&amp;cropH=2561&amp;cropW=3415&amp;xPos=136&amp;yPos=0&amp;width=862&amp;height=647 543w, https://live-production.wcms.abc-cdn.net.au/1259a096e9aee1337a635bf18f25b0c5?impolicy=wcms_crop_resize&amp;cropH=2560&amp;cropW=3840&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=575" src="https://live-production.wcms.abc-cdn.net.au/1259a096e9aee1337a635bf18f25b0c5?impolicy=wcms_crop_resize&amp;cropH=2560&amp;cropW=3840&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"><p><figcaption id="102732412" data-component="VerticalArticleFigure__figcaption" aria-live="polite"> <!-- -->The TSMC factories in Taiwan are incredibly sterile and efficient.<span data-component="Byline"><span data-component="Text">(<span>Reuters: Ann Wang</span>)</span></span></figcaption></p></figure><p>Forty years on from when it was first founded, TSMC is now making the smallest chips in the world.</p><p>In the chip world, smaller means not only that they take up less space, but they're also more energy efficient and powerful.</p><p>And TSMC can make them faster than anyone else.</p><p>TSMC is the most valuable company in Asia, and last year, the 8th most valuable in the world.</p><p>It's responsible for 5 per cent of Taiwan's GDP, and 7 per cent of its electricity consumption.</p><figure role="group" data-component="VerticalArticleFigure" aria-labelledby="102728240" data-uri="coremedia://imageproxy/102728240"><img alt="A microchip." sizes="(max-width: 543px) 543px," srcset="https://live-production.wcms.abc-cdn.net.au/580d66856d9a019565ebc8a03f5feeee?impolicy=wcms_crop_resize&amp;cropH=1988&amp;cropW=2648&amp;xPos=180&amp;yPos=0&amp;width=862&amp;height=647 543w, https://live-production.wcms.abc-cdn.net.au/580d66856d9a019565ebc8a03f5feeee?impolicy=wcms_crop_resize&amp;cropH=1988&amp;cropW=2991&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=575" src="https://live-production.wcms.abc-cdn.net.au/580d66856d9a019565ebc8a03f5feeee?impolicy=wcms_crop_resize&amp;cropH=1988&amp;cropW=2991&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"><p><figcaption id="102728240" data-component="VerticalArticleFigure__figcaption" aria-live="polite"> <!-- -->TSMC makes minuscule microchips that power phones, laptops and missile systems.<span data-component="Byline"><span data-component="Text">(<span>Pexels: Poko Rie</span>)</span></span></figcaption></p></figure><p>TSMC pumps out close to 60 per cent of the semiconductor chips used around the world and makes 90 per cent of the most advanced technology used in phones and laptops all the way to missile systems.</p><p>It's an extraordinary situation — you've got a resource that's needed for basically everything in the modern world coming from one place, making it the most valuable and indispensable chip company in the world.</p><p>TSMC is so essential to the global economy that Taiwanese media calls it the "sacred mountain protecting the island" from invasion by Communist China.</p><p>But is it actually protecting them, or is it a giant target?</p><h2 data-component="Heading">China-Taiwan tensions cause global anxiety</h2><figure role="group" data-component="VerticalArticleFigure" aria-labelledby="102732498" data-uri="coremedia://imageproxy/102732498"><img alt="Joe Biden sits at a desk while Xi Jinping can be seen on a TV screen." sizes="(max-width: 543px) 543px," srcset="https://live-production.wcms.abc-cdn.net.au/701050da12672627dd732c06091afbdf?impolicy=wcms_crop_resize&amp;cropH=3310&amp;cropW=4413&amp;xPos=276&amp;yPos=0&amp;width=862&amp;height=647 543w, https://live-production.wcms.abc-cdn.net.au/701050da12672627dd732c06091afbdf?impolicy=wcms_crop_resize&amp;cropH=3310&amp;cropW=4965&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=575" src="https://live-production.wcms.abc-cdn.net.au/701050da12672627dd732c06091afbdf?impolicy=wcms_crop_resize&amp;cropH=3310&amp;cropW=4965&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"><p><figcaption id="102732498" data-component="VerticalArticleFigure__figcaption" aria-live="polite"> <!-- -->US President&nbsp;Joe&nbsp;Biden&nbsp;and Chinese leader&nbsp;Xi&nbsp;Jinping&nbsp;both see TSMC microchips as a crucial asset.<span data-component="Byline"><span data-component="Text">(<span>Reuters: Jonathan Ernst</span>)</span></span></figcaption></p></figure><p>Beijing has conducted massive military exercises close to Taiwan, raising fears that Chinese President Xi Jinping will fulfil his promise to take control of Taiwan — either with an invasion, or a blockade.</p><p>Should China take Taiwan, it would have control of TSMC — one of the world's most valuable resources.</p><p>China could stop selling the West chips or sell them at such high prices that it makes non-Chinese companies unprofitable.</p><p>To try and get around that, the US, Germany and Japan have convinced TSMC to start construction on factories in their countries, to ensure a supply of chips to the US and its allies.</p><p>Some pundits have suggested that the US should threaten to destroy TSMC's Taiwanese factories if China tries to invade, hoping this would be a strong disincentive.</p><p>Chang is now 92, and told the New York Times that he doesn't think invasion is likely.</p><p>But as the Chinese military increases its presence in the sea and sky around Taiwan, some American investors, like billionaire Warren Buffett, also 92, have decided the risk is too great, and have sold their stake in TSMC.</p></div><p><span data-component="Text">Posted<!-- -->&nbsp;</span><time data-component="ScreenReaderOnly" datetime="2023-08-18T19:38:44.000Z">12 hours ago</time><time data-component="Text">Fri 18 Aug 2023 at 7:38pm</time>, <span data-component="Text">updated<!-- -->&nbsp;</span><time data-component="ScreenReaderOnly" datetime="2023-08-19T00:31:47.000Z">7 hours ago</time><time data-component="Text">Sat 19 Aug 2023 at 12:31am</time></p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[2009scape (183 pts)]]></title>
            <link>https://2009scape.org/</link>
            <guid>37183069</guid>
            <pubDate>Fri, 18 Aug 2023 22:40:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://2009scape.org/">https://2009scape.org/</a>, See on <a href="https://news.ycombinator.com/item?id=37183069">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
          <div id="left">
            <!--MAIN BUTTON: LEFT (DISCORD)-->
            <p><a href="https://forum.2009scape.org/" target="_blank">
              <img src="https://2009scape.org/site/2009scape-resources/img/buttons/btn-forums2.webp" alt="Join us on the forums" width="268" height="151">
              <span></span>
            </a></p><!--WEBSITE FEATURES-->
            <div id="features">
                <div>
                  <p><a href="https://2009scape.org/site/game_guide/play.html"><img src="https://2009scape.org/site/img/2009img/main/home/feature_kbsearch_icon.webp" width="57" height="57" alt="Getting Started"></a></p><p>Getting Started</p>
                  
                </div>
                <div>
                  <p><a href="https://2009scape.org/services/m=hiscore/hiscores.html?world=2"><img src="https://2009scape.org/site/img/2009img/main/home/feature_poll_icon.webp" width="57" height="57" alt="Hiscores"></a></p><p>Community Hiscores</p>
                  <p>See who's on top &amp; check your skill levels! View the <a href="https://2009scape.org/services/m=hiscore/hiscores.html?world=2">Hiscores</a>.</p>
                </div>
                <div>
                  <p><a href="https://gitlab.com/2009scape/2009scape/-/issues"><img src="https://2009scape.org/site/2009scape-resources/img/icons/feature-bugreport.webp" width="57" height="57" alt="Report a Bug"></a></p><p>Report a Bug</p>
                  <p>Found a bug in game? Send us a <a href="https://gitlab.com/2009scape/2009scape/-/issues" target="_blank">Bug report</a>!</p>
                </div>
                <div>
                  <p><a href="https://2009scape.org/site/classicapplet/playclassic.html"><img src="https://2009scape.org/site/2009scape-resources/img/icons/feature-openrsc.webp" width="57" height="57" alt="Open RuneScape Classic"></a></p><p>OpenRSC</p>
                  <p>Experience RuneScape Classic in its original glory: <a href="https://2009scape.org/site/classicapplet/playclassic.html">OpenRSC</a>!</p>
                </div>
                <div>
                  <p><a target="_blank" href="https://github.com/2009scape/2009Scape-mobile#readme"><img src="https://2009scape.org/site/2009scape-resources/img/icons/aog_icon.jpeg" width="57" height="57" alt="AOG"></a></p><p>2009Scape Mobile</p>
                  <p>Enjoy our project on the go with the Android <a target="_blank" href="https://github.com/2009scape/2009Scape-mobile#readme">
                    mobile client!</a>
                  </p>
                </div>
              </div>
            <!--GAME GUIDE-->
            <div id="articles">
                  <p><img alt="Featured Article" src="https://2009scape.org/site/2009scape-resources/img/titles/title-article-featured.webp" width="164" height="9">
                  </p>
                  <div>
                      <p><a href="https://discord.gg/YY7WSttN7H"><img alt="" src="https://2009scape.org/site/img/2009img/main/kbase/aow_icons/discord-promo.webp" width="249" height="87"></a></p>
                      <p>Join us on Discord</p>
                      <p> 2009Scape has an active community on our Discord Server. Discuss the game, ask questions, or just hang out! 
                        <a href="https://discord.gg/YY7WSttN7H">Join Now!</a>
                      </p>
                    </div>
                  
                </div>
          </div>
          <div id="right">
            <!--MAIN BUTTON: PLAY/CLIENT DOWNLOAD-->
            <p><a href="https://2009scape.org/site/game_guide/play.html" id="playbutton">
              <img src="https://2009scape.org/site/2009scape-resources/img/buttons/btn-play.webp" alt="Play 2009Scape" width="480" height="151">
              <span></span>
            </a></p><div id="recentnews">
                
                
                
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                
                  
                    
                  
                
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                
                  
                    
                  
                
                  
                
                  
                    
                      
                        
                          <!--LATEST UPDATE ITEM-->
                          <div>
                                <!--UPDATE TITLE & DATE-->
                                <div>
                                  <h3>Clock Tower Quest</h3>
                                  <p><span>12-August-2023</span>
                                </p></div>
                                <!--UPDATE IMAGE-->
                                
                                <p><a href="https://2009scape.org/services/m=news/archives/2023-08-12.html"><img alt="Latest update image" src="https://2009scape.org/site/2009scape-resources/img/updates/update-monthly-08.webp" width="215" height="137"></a></p><!--UPDATE CONTENT TEASER-->
                              <p>  Clock Tower quest, combat sounds and deep wilderness balancing...
                                <!--READ MORE LINK-->
                                <br><a href="https://2009scape.org/services/m=news/archives/2023-08-12.html">Read more...</a>
                              </p>
                            </div>
                        <!--END LATEST UPDATE ITEM-->
                      
                    
                  
                
                  
                    
                      
                        
                        <!--RECENT UPDATE ITEM-->
                        <div>
                            <!--UPDATE TITLE & DATE-->
                            <div>
                              <h3>Death Plateau, New Random Events and Deep Wilderness PvP</h3>
                              <p><span>06-August-2023</span>
                            </p></div>
                            <!--UPDATE ICON (Use most relevant to this post!)
                            --
                            GENERIC // generic1.webp, generic2.webp, generic3.webp
                            SETTINGS & TWEAKS // settings1.webp, settings2.webp, settings3.webp
                            OTHER // account.webp, award.webp, bugfix.webp, combat.webp, hiscores.webp, money,webp, random.webp, website.webp -->
                            <p><img src="https://2009scape.org/site/2009scape-resources/img/updates/icons/generic1.webp" width="65" height="70" alt="Generic feature icon"></p><!--UPDATE CONTENT TEASER-->
                              <p>A new quest, new random events, deep wilderness PvP zone, champion challenge and a HUGE list of other changes...
                                <!--READ MORE LINK-->
                                <br><a href="https://2009scape.org/services/m=news/archives/2023-08-06.html">Read more..</a>
                              </p>
                            </div>
                        
                    
                  
                
                  
                    
                      
                        
                        <!--RECENT UPDATE ITEM-->
                        <div>
                            <!--UPDATE TITLE & DATE-->
                            <div>
                              <h3>Small Changes in Gielinor</h3>
                              <p><span>06-June-2023</span>
                            </p></div>
                            <!--UPDATE ICON (Use most relevant to this post!)
                            --
                            GENERIC // generic1.webp, generic2.webp, generic3.webp
                            SETTINGS & TWEAKS // settings1.webp, settings2.webp, settings3.webp
                            OTHER // account.webp, award.webp, bugfix.webp, combat.webp, hiscores.webp, money,webp, random.webp, website.webp -->
                            <p><img src="https://2009scape.org/site/2009scape-resources/img/updates/icons/generic1.webp" width="65" height="70" alt="Generic feature icon"></p><!--UPDATE CONTENT TEASER-->
                              <p>A small hotfix update.
                                <!--READ MORE LINK-->
                                <br><a href="https://2009scape.org/services/m=news/archives/2023-06-06.html">Read more..</a>
                              </p>
                            </div>
                        
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
              </div>
          </div>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Radiation Spikes at Chernobyl (115 pts)]]></title>
            <link>https://zetter.substack.com/p/radiation-spikes-at-chernobyl-a-mystery</link>
            <guid>37182828</guid>
            <pubDate>Fri, 18 Aug 2023 22:11:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zetter.substack.com/p/radiation-spikes-at-chernobyl-a-mystery">https://zetter.substack.com/p/radiation-spikes-at-chernobyl-a-mystery</a>, See on <a href="https://news.ycombinator.com/item?id=37182828">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8de3f126-77cb-4f77-8da9-29a857577a61_1200x688.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8de3f126-77cb-4f77-8da9-29a857577a61_1200x688.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8de3f126-77cb-4f77-8da9-29a857577a61_1200x688.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8de3f126-77cb-4f77-8da9-29a857577a61_1200x688.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8de3f126-77cb-4f77-8da9-29a857577a61_1200x688.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8de3f126-77cb-4f77-8da9-29a857577a61_1200x688.png" width="1200" height="688" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8de3f126-77cb-4f77-8da9-29a857577a61_1200x688.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:688,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1528742,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8de3f126-77cb-4f77-8da9-29a857577a61_1200x688.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8de3f126-77cb-4f77-8da9-29a857577a61_1200x688.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8de3f126-77cb-4f77-8da9-29a857577a61_1200x688.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8de3f126-77cb-4f77-8da9-29a857577a61_1200x688.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Map showing the location of radiation sensors in the Chernobyl Exclusion Zone. The red dots indicate sensors that recorded spikes at 10:20 pm on February 24, 2022, hours after Russian troops seized control of the Chernobyl nuclear power plant. You can see from this how far apart the sensors that spiked were (in some cases tens of miles away from each other), while other sensors near the spiked sensors showed no elevated levels — a suspicious pattern that suggested the spikes might not have been real. (Image courtesy of Ruben Santamarta)</figcaption></figure></div><p>Shortly after Russian troops seized control of the Chernobyl nuclear power plant following the invasion of Ukraine last year, more than forty radiation sensors positioned in the 1,000-square-mile forested area around the plant — known as the Chernobyl Exclusion Zone (CEZ) — began reporting alarming spikes in radiation levels. Ukrainian authorities attributed it to heavy military equipment that they said was stirring up radioactive dirt that had settled in the ground following the 1986 nuclear accident at the plant and condemned Russia for creating a scenario that posed a radiological risk not only to Ukraine but also to Europe.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff06d25bd-e686-4377-90ab-3f672a0ec3d0_1782x858.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff06d25bd-e686-4377-90ab-3f672a0ec3d0_1782x858.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff06d25bd-e686-4377-90ab-3f672a0ec3d0_1782x858.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff06d25bd-e686-4377-90ab-3f672a0ec3d0_1782x858.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff06d25bd-e686-4377-90ab-3f672a0ec3d0_1782x858.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff06d25bd-e686-4377-90ab-3f672a0ec3d0_1782x858.png" width="586" height="282.13324175824175" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f06d25bd-e686-4377-90ab-3f672a0ec3d0_1782x858.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:701,&quot;width&quot;:1456,&quot;resizeWidth&quot;:586,&quot;bytes&quot;:2665582,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff06d25bd-e686-4377-90ab-3f672a0ec3d0_1782x858.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff06d25bd-e686-4377-90ab-3f672a0ec3d0_1782x858.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff06d25bd-e686-4377-90ab-3f672a0ec3d0_1782x858.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff06d25bd-e686-4377-90ab-3f672a0ec3d0_1782x858.png 1456w" sizes="100vw"></picture></div></a><figcaption>CCTV footage captured the Russian military convoy passing through checkpoint Leliv, about nine miles southeast of the Chernobyl nuclear power plant on the afternoon of February 24, 2022. (Image taken from CCTV footage supplied to Greenpeace by staff at the Chernobyl plant)</figcaption></figure></div><p><span>As I reported in </span><a href="https://www.wired.com/story/chernobyl-radiation-spike-mystery/" rel="">a story for WIRED</a><span> published yesterday, that explanation didn’t wash with a number of experts I spoke with, including security researcher Ruben Santamarta. After a group of scientists in the United Kingdom published a paper last year discounting the soil theory, someone approached Santamarta about whether it was possible the sensors had been hacked.</span></p><p><span>A few years ago, Santamarta found </span><a href="https://www.blackhat.com/docs/us-17/wednesday/us-17-Santamarta-Go-Nuclear-Breaking%20Radition-Monitoring-Devices-wp.pdf" rel="">unpatched vulnerabilities</a><span> in radiation sensors (different from the ones used at Chernobyl), that could have allowed someone to manipulate data recorded by the sensors. Intrigued by the mystery at Chernobyl, Santamarta wondered if the Chernobyl sensors might have been vulnerable in similar ways. He spent months last year studying the history of Chernobyl, digging deeply into the technology behind radiation sensors in general and more specifically the equipment setup in the CEZ, and eliminated a number of theories about what might have caused the spikes, before concluding that the data was likely manipulated.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadfa063b-402f-4aa6-97f4-852cd99d0fc0_1700x1074.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadfa063b-402f-4aa6-97f4-852cd99d0fc0_1700x1074.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadfa063b-402f-4aa6-97f4-852cd99d0fc0_1700x1074.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadfa063b-402f-4aa6-97f4-852cd99d0fc0_1700x1074.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadfa063b-402f-4aa6-97f4-852cd99d0fc0_1700x1074.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadfa063b-402f-4aa6-97f4-852cd99d0fc0_1700x1074.png" width="540" height="341.2087912087912" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/adfa063b-402f-4aa6-97f4-852cd99d0fc0_1700x1074.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:920,&quot;width&quot;:1456,&quot;resizeWidth&quot;:540,&quot;bytes&quot;:2767102,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadfa063b-402f-4aa6-97f4-852cd99d0fc0_1700x1074.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadfa063b-402f-4aa6-97f4-852cd99d0fc0_1700x1074.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadfa063b-402f-4aa6-97f4-852cd99d0fc0_1700x1074.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadfa063b-402f-4aa6-97f4-852cd99d0fc0_1700x1074.png 1456w" sizes="100vw"></picture></div></a><figcaption>An amalgamation of satellite imagery showing fires and other “thermal anomalies” in Ukraine (some caused naturally, some due to the war) between February 24 and July 14. (Courtesy of NASA and Greenpeace)</figcaption></figure></div><p>The key was a database he obtained that contained the raw data transmitted by the sensors before, during and after the invasion. Examining this data, which included timestamps indicating exactly when each sensor recorded a spike, he discovered very distinct and suspicious patterns to the spikes.</p><ul><li><p>They occurred in clusters between 8:40 pm the night of February 24 and 10:50 am the next morning.</p></li><li><p>The spike clusters often involved 7, 9, or 11 sensors all spiking at the same time, before dropping back to normal radiation levels within 30 minutes to two hours after a spike.</p></li><li><p>Most significantly, sensors that spiked simultaneously were miles away from one another, while sensors closer to them showed no spike at all.</p></li></ul><p>These and other patterns he found in the data didn’t match anything resembling what should occur if the spikes were caused by an actual increase in radiation levels. The patterns led him to conclude that the data was likely manipulated — either by a remote hacker or someone with direct access to the server that processed the data. </p><p><span>I’d recommend reading the </span><a href="https://www.wired.com/story/chernobyl-radiation-spike-mystery/" rel="">WIRED story</a><span> to learn everything Santamarta uncovered and, if you’re at the BlackHat security conference this week in Las Vegas, Santamarta will be giving a </span><a href="https://www.blackhat.com/us-23/briefings/schedule/#seeing-through-the-invisible-radiation-spikes-detected-in-chernobyl-during-the-russian-invasion-show-possible-evidence-of-fabrication-32941" rel="">presentation about his findings</a><span> on Thursday, August 10. He’ll also release a 100-page paper laying out his detailed research.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F499bcec9-fb5d-469a-98ed-be8809afa82e_1040x662.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F499bcec9-fb5d-469a-98ed-be8809afa82e_1040x662.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F499bcec9-fb5d-469a-98ed-be8809afa82e_1040x662.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F499bcec9-fb5d-469a-98ed-be8809afa82e_1040x662.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F499bcec9-fb5d-469a-98ed-be8809afa82e_1040x662.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F499bcec9-fb5d-469a-98ed-be8809afa82e_1040x662.png" width="448" height="285.16923076923075" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/499bcec9-fb5d-469a-98ed-be8809afa82e_1040x662.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:662,&quot;width&quot;:1040,&quot;resizeWidth&quot;:448,&quot;bytes&quot;:672753,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F499bcec9-fb5d-469a-98ed-be8809afa82e_1040x662.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F499bcec9-fb5d-469a-98ed-be8809afa82e_1040x662.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F499bcec9-fb5d-469a-98ed-be8809afa82e_1040x662.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F499bcec9-fb5d-469a-98ed-be8809afa82e_1040x662.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Research paper Santamarta will release on August 10.</figcaption></figure></div><p>But in the meantime, I’d like to highlight a few things that I was only able to briefly cover in the WIRED story. You should probably read the story first to understand the context for what I’m going to discuss here.</p><p>There are about 68 battery-powered radiation sensors at Chernobyl, called GammaTRACERs, that are spread throughout the CEZ. They’re managed by the State Specialized Enterprise Ecocenter (Ecocenter for short). These detectors continuously record ambient gamma radiation levels and calculate an average before transmitting that figure wirelessly (using a SkyLINK module) to a base station in an Ecocenter office in the town of Chernobyl. Those readings, transmitted once an hour, get analyzed by software and then are posted to the Ecocenter’s web site. </p><p>Santamarta considered a number of ways the data might have been manipulated:  Someone could have hacked individual sensors to alter the data before it was transmitted, or intercepted and altered it while it was in transit to the base station. But this would have been too much work and been inefficient. Instead, based on the patterns, he concluded that the most likely way for someone to alter the data was to write malicious code that altered it on the Ecocenter’s server, after it was aggregated from all of the sensors. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a88d931-c28c-499b-89e3-60b5033db0bd_1480x842.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a88d931-c28c-499b-89e3-60b5033db0bd_1480x842.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a88d931-c28c-499b-89e3-60b5033db0bd_1480x842.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a88d931-c28c-499b-89e3-60b5033db0bd_1480x842.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a88d931-c28c-499b-89e3-60b5033db0bd_1480x842.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a88d931-c28c-499b-89e3-60b5033db0bd_1480x842.png" width="524" height="297.989010989011" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4a88d931-c28c-499b-89e3-60b5033db0bd_1480x842.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:828,&quot;width&quot;:1456,&quot;resizeWidth&quot;:524,&quot;bytes&quot;:320671,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a88d931-c28c-499b-89e3-60b5033db0bd_1480x842.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a88d931-c28c-499b-89e3-60b5033db0bd_1480x842.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a88d931-c28c-499b-89e3-60b5033db0bd_1480x842.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a88d931-c28c-499b-89e3-60b5033db0bd_1480x842.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Model of a GammaTRACER radiation detector.</figcaption></figure></div><p>During the period in question, forty-two sensors spiked, most of them simultaneously in clusters. To give you an idea of how the spike clusters looked, see the image below.</p><p><span>Radiation levels are recorded in ambient dose rates, known as microSieverts per hour (or µSv/h). This represents the amount of energy a human body would absorb from a specific level of ionizing radiation in an area. If a sensor recorded that the ambient dose rate in a particular area was .992 µSv/h, this means a human body, if exposed for an hour to the energy produced by that level of ionizing radiation, would absorb about .992 microSieverts of radiation during that hour. The acceptable exposure limit for humans for a year is </span><a href="https://www.admnucleartechnologies.com.au/blog/what-safe-level-radiation-exposure" rel="">1000 µSv</a><span>.</span></p><p>The list below shows the first spike cluster that occurred at 8:40 pm the night of February 24.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eb03865-72fb-44ef-9824-f14ca495c5cb_1010x1382.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eb03865-72fb-44ef-9824-f14ca495c5cb_1010x1382.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eb03865-72fb-44ef-9824-f14ca495c5cb_1010x1382.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eb03865-72fb-44ef-9824-f14ca495c5cb_1010x1382.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eb03865-72fb-44ef-9824-f14ca495c5cb_1010x1382.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eb03865-72fb-44ef-9824-f14ca495c5cb_1010x1382.png" width="340" height="465.2277227722772" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9eb03865-72fb-44ef-9824-f14ca495c5cb_1010x1382.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1382,&quot;width&quot;:1010,&quot;resizeWidth&quot;:340,&quot;bytes&quot;:179218,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eb03865-72fb-44ef-9824-f14ca495c5cb_1010x1382.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eb03865-72fb-44ef-9824-f14ca495c5cb_1010x1382.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eb03865-72fb-44ef-9824-f14ca495c5cb_1010x1382.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eb03865-72fb-44ef-9824-f14ca495c5cb_1010x1382.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Radiation sensors that recorded elevated radiation levels at 8:40 pm on February 24. The name of the sensor is in black and the normal baseline level of microSieverts that the sensor traditionally recorded prior to the spike is in parentheses. The red line data shows the time and level of the spike, as well as a comparison to the normal baseline level (most of these sensors spiked 3x what was normal for that sensor). Some of the entries have another timestamp below this. This shows when  the spike decreased. If the spikes had truly been caused by trucks stirring up radioactive dirt, the radiation levels would not have dropped so quickly.</figcaption></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F961e2627-d0cc-4a59-a052-267c1b629209_832x1418.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F961e2627-d0cc-4a59-a052-267c1b629209_832x1418.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F961e2627-d0cc-4a59-a052-267c1b629209_832x1418.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F961e2627-d0cc-4a59-a052-267c1b629209_832x1418.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F961e2627-d0cc-4a59-a052-267c1b629209_832x1418.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F961e2627-d0cc-4a59-a052-267c1b629209_832x1418.png" width="312" height="531.75" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/961e2627-d0cc-4a59-a052-267c1b629209_832x1418.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1418,&quot;width&quot;:832,&quot;resizeWidth&quot;:312,&quot;bytes&quot;:174547,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F961e2627-d0cc-4a59-a052-267c1b629209_832x1418.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F961e2627-d0cc-4a59-a052-267c1b629209_832x1418.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F961e2627-d0cc-4a59-a052-267c1b629209_832x1418.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F961e2627-d0cc-4a59-a052-267c1b629209_832x1418.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>A second set of ten sensors spiked at 9:50 pm. Note that in the 8:40 pm spike cluster, the levels only increased three to five times normal. But in the 9:50 pm spike cluster, two of the sensors increased twenty-seven and fifty-five times normal levels.</figcaption></figure></div><p>Anyone who looks at the data and maps showing the spikes in the CEZ would likely think something was amiss. Aside from the geographical distance between spiking sensors, it should have been obvious something was wrong with the data when  sensors that spiked quickly returned to normal levels.</p><p><span>In fact, a number of people outside Ukraine were expressing doubt about the authenticity of the data — including a radiation expert in France who told a reporter the data </span><a href="https://www.reuters.com/world/europe/ukraine-nuclear-agency-reports-higher-chernobyl-radiation-levels-due-heavy-2022-02-25/" rel="">might have been hacked</a><span>. </span></p><p>Yet officials in Ukraine, and at the International Atomic Energy Agency in Vienna — the United Nations body that monitors nuclear facilities around the world — didn’t investigate or even notice something was wrong with the data. Why?</p><p>The war was raging, Ukrainians were in a fight for their lives and country and were uncertain if money and military equipment would come from abroad. And staff at the Chernobyl plant were being held hostage by Russian troops. These are all good reasons why neither Ukrainian officials nor the IAEA would have taken time to notice or investigate the suspicious data patterns when they occurred. And if the IAEA suspected the data was false, this would be another reason not to become alarmed and hurry an investigation.</p><p>Yet its public statements indicated the IAEA believed the data was accurate. So why not, once Russian soldiers departed Chernobyl at the end of March, conduct an investigation then?</p><p><span>The IAEA sent experts to do a radiological survey in the CEZ in April, but their focus wasn’t on determining the cause of the spikes. Investigators instead wanted to determine if Russian soldiers who dug trenches in a small section of the CEZ had released dangerous levels of radiation in that area that could have sickened the soldiers or anyone else who passed through that area. (The IAEA determined the radiation levels from trench-digging could not have made the soldiers ill.) Three months later, the Ukrainian government invited Greenpeace to conduct its own radiological survey of the areas where Russian soldiers had camped, and they also did not look into the cause or veracity of the February spikes. (They concluded that the radiation levels where the soldiers camped were three times what the IAEA reported and </span><a href="https://www.greenpeace.org/international/press-release/54762/greenpeace-investigation-challenges-nuclear-agency-on-chornobyl-radiation-levels/" rel="">accused the IAEA of being untrustworthy</a><span> because IAEA Deputy Director Mikhail Chudakov, a Russian, is a long-term official with ROSATOM, Russia’s state atomic corporation.) </span></p><p>It seems strange, Santamarta told me, that both the IAEA and Greenpeace would put effort into determining if Russian soldiers were endangered from digging trenches in one small part of the CEZ and not be concerned about much higher radiation levels that sensors had recorded in other parts of the CEZ — including parts where Chernobyl staff were working.</p><p>One can understand on the surface why the IAEA might not have investigated — the agency had issued a statement after the spikes occurred saying the radiation levels had only reached 9 µSv/h, a level that posed no danger to the workers at Chernobyl or the public. But as I note in the WIRED story, the IAEA was basing this assessment on incomplete information Ukraine provided. In fact, the spikes had already reached more than 90 µSv/h by the time the agency released its statement, but the IAEA never mentioned this in its statement. </p><p>Tero Karhunen, senior inspector with Finland’s radiation and nuclear safety authority, known as STUK, told me that if ambient dose rates rise above 100 µSv/h for more than 48 hours, this normally triggers an evacuation of the affected region. The data from Chernobyl was all posted to the Ecocenter web site for everyone to see, including the IAEA, yet the agency ignored the higher numbers. Why?</p><p>Did the IAEA know or suspect the spikes were false and therefore wanted to calm the public by citing only the lower numbers? The agency wouldn’t answer any questions I sent. </p><p>The implications, if it knew or suspected the data was false and didn’t investigate, are concerning. The implications if it believed the data was accurate and told the public the radiation posed no danger to the public, when in fact 90 µSv/h did potentially pose a danger, are also concerning.</p><p><strong>Why Did Sensor Data Stop Getting Updated?</strong></p><p>The spikes occurred over a 14-hour period, with a number of the sensors spiking multiple times throughout this time. With some sensors that spiked more than once, the second spike was lower than the first. But other sensors that spiked more than once showed dramatic increases with the second spike — including two sensors that spiked to nearly 100 µSv/h. These two sensors, known as DGS-2 and HZHTO, initially spiked at 9:50 pm on the night of February 24 and then again at 10:40 am the next morning. </p><p>DGS-2 (which normally recorded a radiation dose level of 7.67 µSv/h) spiked to 58.8 µSv/h at 9:50 pm on February 24th, and then to 93 µSv/h at 10:40 am the next morning. And HZHTO (which normally recorded a radiation dose level of 2.98 µSv/h spiked to 65.5 µSv/h at 9:50 pm and then to 93 at 10:40 am the next morning.</p><p>Both DGS-2 and HZHTO got very close to 100 µSv/h — the level which Karhunen says generally triggers an evacuation if it lasts for 48 hours. Given the upward trajectory of the spikes between 9:40 pm and 10:50 am, it’s possible they would have climbed higher with subsequent spikes. We’ll never know if they would have reached higher levels, however, because the Ecocenter’s online map stopped updating data after 10:50 am on February 25th. This meant nuclear experts outside Ukraine, and others who were monitoring the Ecocenter web site to track the radiation levels in the CEZ, were suddenly blind to what was occurring there.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3f8afe9-5509-4ea1-8dec-3770b0f09c4f_892x1234.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3f8afe9-5509-4ea1-8dec-3770b0f09c4f_892x1234.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3f8afe9-5509-4ea1-8dec-3770b0f09c4f_892x1234.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3f8afe9-5509-4ea1-8dec-3770b0f09c4f_892x1234.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3f8afe9-5509-4ea1-8dec-3770b0f09c4f_892x1234.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3f8afe9-5509-4ea1-8dec-3770b0f09c4f_892x1234.png" width="374" height="517.3946188340807" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b3f8afe9-5509-4ea1-8dec-3770b0f09c4f_892x1234.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1234,&quot;width&quot;:892,&quot;resizeWidth&quot;:374,&quot;bytes&quot;:157054,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3f8afe9-5509-4ea1-8dec-3770b0f09c4f_892x1234.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3f8afe9-5509-4ea1-8dec-3770b0f09c4f_892x1234.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3f8afe9-5509-4ea1-8dec-3770b0f09c4f_892x1234.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3f8afe9-5509-4ea1-8dec-3770b0f09c4f_892x1234.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Image showing the two sensors — DGS-2 and HZHTO — whose ambient dose rates were nearing 100 µSv/h before the Ecocenter’s web site stopped updating data from all of the sensors.</figcaption></figure></div><p>It’s not clear why the data suddenly stopped, but there are a couple of possibilities. The following is speculation.</p><ol><li><p>Ukraine was experiencing intermittent internet and electricity outages on the day of the invasion, and it’s possible the Ecocenter’s internet service went out. In this case, the radiation sensors in the CEZ would still have recorded radiation levels (they are battery operated after all) and they still would have transmitted the data to the Ecocenter base station. With the internet out, however, the data could not get posted to the Ecocenter’s web site. But once internet connectivity was restored, data held in reserve should have posted. Indeed, data from the sensors all came back online the following Monday, February 28 (at this point they were all reporting normal radiation levels again). The sensors all went offine again by March 3, however, and remained that way until long after Russian soldiers ended their occupation of the Chernobyl plant and CEZ. </p></li><li><p>But there’s an alternate theory. If internet outages didn’t prevent the data from getting uploaded to the Ecocenter web site, and the sensors themselves were still recording and transmitting data, there is a possibility that someone pulled the plug on the Ecocenter server that was used to process and upload the data to the web site. Ukrainian authorities say that Russian soldiers at the Ecocenter’s office in Chernobyl stole the hard drive from the server used to process the radiation data, but couldn’t say when that occurred — on March 3 when the web site stopped posting data or later as the soldiers were departing Chernobyl. If they stole it on March 3, this could explain why the data stopped updating. But if they stole it later than this, then this doesn’t explain why data stopped updating to the web site. And it suggests that somene intentionally stopped the data from posting — possibly because they knew the data was false and feared that radiation levels higher than 100 µSv/h would get posted to the web site and cause panic.</p></li><li><p>There’s another mystery, however. Ordinarily the Ecocenter’s system automatically transmits the sensor data to the IAEA around the same time it’s posted to the Ecocenter’s online map. But at some point on February 24, the IAEA stopped receiving this direct data transmission, even though data continued to get updated to the Ecocenter’s web site until February 25th, and then again on the 28th when all of the sensor data came back online. Did someone know the data was false and halt the transmission to prevent the IAEA from acting on false data if the levels increased to 100 µSv/h? There are a lot of unanswered questions.</p></li></ol><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb3bfe1e-bdf3-4fea-8524-c9aafef3d955_1256x936.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb3bfe1e-bdf3-4fea-8524-c9aafef3d955_1256x936.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb3bfe1e-bdf3-4fea-8524-c9aafef3d955_1256x936.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb3bfe1e-bdf3-4fea-8524-c9aafef3d955_1256x936.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb3bfe1e-bdf3-4fea-8524-c9aafef3d955_1256x936.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb3bfe1e-bdf3-4fea-8524-c9aafef3d955_1256x936.png" width="518" height="386.02547770700636" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bb3bfe1e-bdf3-4fea-8524-c9aafef3d955_1256x936.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:936,&quot;width&quot;:1256,&quot;resizeWidth&quot;:518,&quot;bytes&quot;:1693272,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb3bfe1e-bdf3-4fea-8524-c9aafef3d955_1256x936.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb3bfe1e-bdf3-4fea-8524-c9aafef3d955_1256x936.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb3bfe1e-bdf3-4fea-8524-c9aafef3d955_1256x936.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb3bfe1e-bdf3-4fea-8524-c9aafef3d955_1256x936.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The GammaTRACERs store data in their memory. This data could be used to verify if the data the Ecocenter posted to its web site was accurate. If someone had altered the data on the Ecocenter server, it wouldn’t match the data stored in the memory of the sensors. The memory of these devices only hold so much data, however, before it’s overwritten, and it’s not clear if the data from February 2022 still remains in them.</p><p>In addition to the digital GammaTRACERs that measure energy produced by ionizing radiation, some of the sensor stations also have an aerosol filter that measures ambient dose levels in the air. These filters are much more sensitive to radiation changes than the digital sensors and therefore are more accurate in detecting radiation dose levels. The filters would have been a great way for Ukraine to verify that the levels recorded by the GammaTRACERs were accurate.</p><p>Prior to the invasion, Chernobyl workers would collect these filters once a week to test them in a lab. But during the occupation of Chernobyl, it appears workers were prevented from collecting and testing the filters. Once Russian soldiers left Chernobyl and the CEZ, however, it would have been a great time for workers to collect the filters and test them. Unfortunately, according to a nuclear expert from Greenpeace who entered the CEZ a few months after the Russians left, there were landmines littered throughout the CEZ and it would have been dangerous for workers to collect the filters. It’s unclear if the filters have been collected since then.  </p><p>One other way to verify the accuracy of the sensor data — either after the Russians left or today — would be to do a forensic investigation of the sensors themselves and the Ecocenter server used to process and publish the data online. Unfortunately, the Ecocenter server — or its hard drive — is gone. According to Ukrainian authorities, the Russians took it. They also took specialized software used to process and analyze the sensor data.</p><p>The server is an important piece of the puzzle and could help determine if someone manipulated the sensor data. The Ukrainians say that Russian soldiers absconded with the entire server when they left Chernobyl. Workers at the Ecocenter even appeared to show a French TV station the empty cabinet where the server had been stored.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89bff27a-ca25-4ca4-9f62-d39dcd840cb8_618x694.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89bff27a-ca25-4ca4-9f62-d39dcd840cb8_618x694.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89bff27a-ca25-4ca4-9f62-d39dcd840cb8_618x694.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89bff27a-ca25-4ca4-9f62-d39dcd840cb8_618x694.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89bff27a-ca25-4ca4-9f62-d39dcd840cb8_618x694.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89bff27a-ca25-4ca4-9f62-d39dcd840cb8_618x694.png" width="398" height="446.94498381877025" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/89bff27a-ca25-4ca4-9f62-d39dcd840cb8_618x694.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:694,&quot;width&quot;:618,&quot;resizeWidth&quot;:398,&quot;bytes&quot;:634053,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89bff27a-ca25-4ca4-9f62-d39dcd840cb8_618x694.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89bff27a-ca25-4ca4-9f62-d39dcd840cb8_618x694.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89bff27a-ca25-4ca4-9f62-d39dcd840cb8_618x694.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89bff27a-ca25-4ca4-9f62-d39dcd840cb8_618x694.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Image reportedly showing the looted cabinet that once held the server used to analyze and process data from the radiation sensors in the CEZ.</figcaption></figure></div><p>When I asked Ukrainian authorities in charge of the Ecocenter lab what the Russians took from that facility, they told me they didn’t take entire components. Instead they only took parts, such as hard drives. </p><p>“Most computers [were] stolen not in assembled form (such as a monoblock, laptop, or system unit), but in disassembled form, as parts,” Maksym Shevchuk, deputy head of the State Agency for the Management of the Exclusion ZoneIn — which oversees the Ecocenter — told me in an email. “The Russian occupiers stole many hard drives of computers, and the rest of their components were scattered around the offices and territory of the nuclear power plant.”</p><p>Here’s are photos showing some of the damage left behind by Russian soldiers at Chernobyl. The images were provided to me by the State Agency for the Management of the Exclusion Zone.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71b97c6d-0aa0-43cc-96ee-cc639a96795b_4032x3024.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71b97c6d-0aa0-43cc-96ee-cc639a96795b_4032x3024.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71b97c6d-0aa0-43cc-96ee-cc639a96795b_4032x3024.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71b97c6d-0aa0-43cc-96ee-cc639a96795b_4032x3024.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71b97c6d-0aa0-43cc-96ee-cc639a96795b_4032x3024.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71b97c6d-0aa0-43cc-96ee-cc639a96795b_4032x3024.jpeg" width="316" height="421.260989010989" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/71b97c6d-0aa0-43cc-96ee-cc639a96795b_4032x3024.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1941,&quot;width&quot;:1456,&quot;resizeWidth&quot;:316,&quot;bytes&quot;:2410575,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71b97c6d-0aa0-43cc-96ee-cc639a96795b_4032x3024.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71b97c6d-0aa0-43cc-96ee-cc639a96795b_4032x3024.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71b97c6d-0aa0-43cc-96ee-cc639a96795b_4032x3024.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71b97c6d-0aa0-43cc-96ee-cc639a96795b_4032x3024.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce27ac01-48b3-4f2e-8ee0-3b1f0892f65e_4624x3472.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce27ac01-48b3-4f2e-8ee0-3b1f0892f65e_4624x3472.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce27ac01-48b3-4f2e-8ee0-3b1f0892f65e_4624x3472.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce27ac01-48b3-4f2e-8ee0-3b1f0892f65e_4624x3472.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce27ac01-48b3-4f2e-8ee0-3b1f0892f65e_4624x3472.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce27ac01-48b3-4f2e-8ee0-3b1f0892f65e_4624x3472.jpeg" width="350" height="262.74038461538464" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ce27ac01-48b3-4f2e-8ee0-3b1f0892f65e_4624x3472.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1093,&quot;width&quot;:1456,&quot;resizeWidth&quot;:350,&quot;bytes&quot;:8761339,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce27ac01-48b3-4f2e-8ee0-3b1f0892f65e_4624x3472.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce27ac01-48b3-4f2e-8ee0-3b1f0892f65e_4624x3472.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce27ac01-48b3-4f2e-8ee0-3b1f0892f65e_4624x3472.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce27ac01-48b3-4f2e-8ee0-3b1f0892f65e_4624x3472.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Why would the Russians remove hard drives and not the entire towers? Perhaps the answer is as simple as space. They didn’t have enough space in their military vehicles to carry entire computer towers back to Russia, so they just took the hard drives. But why take the hard drives?</p><p>If the radiation data at Chernobyl was manipulated, this raises questions about who had the means to do it and why. As I note in the WIRED story, there are two obvious possibilities — Russia and Ukraine — both of whom had means and motive, both of whom had access to the Ecocenter server and radiation sensor data. </p><p>It makes sense to consider what occurred from the viewpoint of either scenario, though all we can do is speculate at this point.</p><p>If Russia falsified the data as part of a disinformation campaign, as I discuss in the WIRED story, to bolster its claims that Ukraine was building a dirty bomb at Chernobyl, then it would have made sense if Ukrainian workers at Ecocenter grew alarmed by the spikes and pulled the plug on the server to thwart the disinformation campaign. But once Russian soldiers had control of the Ecocenter facilities, why remove the hard drives? To remove evidence of Russia’s manipulation?</p><p>If Ukraine was responsible for manipulating the data and falsifying the radiation spikes — to alarm the West or control the movement of Russian soldiers at Chernobyl, as I discuss in the WIRED story — the scenario becomes more interesting. If workers at Chernobyl created the spikes, and someone in authority realized the implications of deceiving the IAEA and the public with false data and ordered them to stop, this could explain why the data suddenly stopped transmitting to the IAEA and stopped being updated to the Ecocenter web site. Alternatively, Russia could have realized that Ukraine was falsifying the data to drum up support from Europe and the US by showing that the invasion had created a dangrous nuclear risk and ordered soldiers at Chernobyl to seize the Ecocenter hard drives to halt the Ukrainian disinformation campaign. Who knows?</p><p>Without an investigation, we may never know the truth. </p><p>Shevchuk told me in an email that his agency would consider investigating the radiation spikes if merited, but would do so only after the war has ended. With the Ecocenter hard drives missing, and the unlikelihood that any 2022 data in the memory of the GammaTRACERs would still remain at the end of the war, it’s not clear an investigation would even be possible. </p><p>The mystery of Chernobyl may remain a mystery.</p><p><em>Thank you for reading. If you found this article useful or interesting, feel free to share it with others.</em></p><p data-attrs="{&quot;url&quot;:&quot;https://zetter.substack.com/p/software-maker-3cx-was-compromised?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&amp;token=eyJ1c2VyX2lkIjo5MTM0Mzg0LCJwb3N0X2lkIjoxMTU5NjkwODMsImlhdCI6MTY4NzkyMDc4MCwiZXhwIjoxNjkwNTEyNzgwLCJpc3MiOiJwdWItMzEyMjM4Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.sSuOL5LnCXYH8VjU5qBinLFaeNACRiMSWP6__8dylyY&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:&quot;button-wrapper&quot;}" data-component-name="ButtonCreateButton"><a href="https://zetter.substack.com/p/software-maker-3cx-was-compromised?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&amp;token=eyJ1c2VyX2lkIjo5MTM0Mzg0LCJwb3N0X2lkIjoxMTU5NjkwODMsImlhdCI6MTY4NzkyMDc4MCwiZXhwIjoxNjkwNTEyNzgwLCJpc3MiOiJwdWItMzEyMjM4Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.sSuOL5LnCXYH8VjU5qBinLFaeNACRiMSWP6__8dylyY" rel=""><span>Share</span></a></p><p><em>Zero Day is a reader-supported publication. You can support my work by becoming a paid subscriber; or if you prefer, you can subscribe for free:</em></p><p><em>You can also give a gift subscription to someone else:</em></p><p data-attrs="{&quot;url&quot;:&quot;https://zetter.substack.com/subscribe?&amp;gift=true&quot;,&quot;text&quot;:&quot;Give a gift subscription&quot;,&quot;action&quot;:null,&quot;class&quot;:&quot;button-wrapper&quot;}" data-component-name="ButtonCreateButton"><a href="https://zetter.substack.com/subscribe?&amp;gift=true" rel=""><span>Give a gift subscription</span></a></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Analysis: Health care CEOs hauled in $4B last year as inflation pinched workers (112 pts)]]></title>
            <link>https://www.statnews.com/2023/08/17/health-ceo-salaries-compensation/</link>
            <guid>37182600</guid>
            <pubDate>Fri, 18 Aug 2023 21:48:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.statnews.com/2023/08/17/health-ceo-salaries-compensation/">https://www.statnews.com/2023/08/17/health-ceo-salaries-compensation/</a>, See on <a href="https://news.ycombinator.com/item?id=37182600">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
	<p><span><span>T</span></span>he health care industry didn’t just provide a safe haven for jittery stock investors in 2022, a year defined by inflation and higher interest rates. It also provided a stable stream of wealth for top executives, who collectively pocketed billions of dollars in what was otherwise a rough patch for the economy.</p>
<p>By almost every measure, 2022 was a <a href="https://www.wsj.com/articles/for-u-s-stocks-2022-is-a-year-with-almost-no-record-highs-11672443247" target="_blank" rel="noopener">bad year</a> for the stock market. But health care stocks <a href="https://www.barrons.com/articles/healthcare-stocks-defensive-buy-51672951649" target="_blank" rel="noopener">fell significantly less</a> than other companies as the amount of care received and prescriptions filled returned closer to pre-pandemic norms.</p>
<p>As a result, the CEOs of more than 300 publicly traded health care companies combined to make $4 billion in 2022, according to a STAT analysis of financial filings. That amount of money could buy Costco memberships for more than 66 million people, and it’s equivalent to the entire economic output of <a href="https://data.worldbank.org/indicator/NY.GDP.MKTP.CD?locations=SL" target="_blank" rel="noopener">Sierra Leone</a>.</p>
<p>That CEO haul was down 11% from the <a href="https://www.statnews.com/2022/07/18/health-care-ceo-compensation-2021/">$4.5 billion recorded in 2021</a>. But the sizable paydays highlight how every niche of health care — from Covid-19 vaccines and obscure technology to orthopedic implants and providing coverage to the nation’s poor — continued to supply its leaders with substantial sums of money even as more people struggled to afford food, housing, and, yes, <a href="https://news.gallup.com/poll/468053/record-high-put-off-medical-care-due-cost-2022.aspx" target="_blank" rel="noopener">health care</a>.</p>
<p>“No matter how you slice it, the people at the top — the CEOs of these companies — are making enormous gains every year compared to ordinary Americans,” said John McDonough, a health policy professor at Harvard who has studied health care for nearly four decades. “This is the bitter fruit that we reap from telling the health care industry to act more like a business.”</p>
		
		
<p>No CEO made more than Moderna’s Stéphane Bancel, who took in <a href="https://www.statnews.com/2023/03/17/moderna-stephane-bancel-compensation/">nearly $400 million</a> after governments around the world quickly bought the biotech company’s Covid-19 shot. Bancel is in the process of donating a vast majority of his income to charities. The 10 highest-paid CEOs — a list that also includes the CEOs of Thermo Fisher and McKesson — made a combined $1.4 billion, or about a third of the total studied. That amount is roughly what the National Institutes of Health spends annually to <a href="https://nida.nih.gov/about-nida/legislative-activities/budget-information/fiscal-year-2024-budget-information-congressional-justification-national-institute-drug-abuse" target="_blank" rel="noopener">study drug abuse and addiction</a>.</p>
<p>The irony with health care’s massive payouts, according to experts, is how the industry praises itself for attempting to find ways to lower costs. But health care still represents more than 18% of the U.S. economy — similar to what it was before the pandemic.</p>
<p>“Individual companies’ incentive is not to bring system costs down,” McDonough said.</p>
		
		
<p>Similar to last year, STAT analyzed executive compensation found in the annual proxy filings of more than 300 companies across all health care sectors — drugmakers and biotech firms, health insurers, hospitals, other providers, medical device firms, health tech companies, suppliers, and more. The analysis focused on companies that were worth at least $1 billion in March, based on data from financial database provider AlphaSense.</p>
<p>The data offer a detailed view of how top executives are incentivized, like how stock continues to comprise most of their compensation and how that drives the wide pay disparities between those in charge and those who work on the ground, like nurses, home health aides, and others. The analysis also highlights outliers within each sector, such as one biopharma CEO who made more than $90 million — all of which came from so-called “management fees.”</p>
		<div>
					<p>
						Unlock this article by subscribing to STAT+ and enjoy your first 30 days free!					</p>
					<p><a href="https://www.statnews.com/stat-plus/" data-stat-paywall-cta="breaker subscribe cta 1">GET STARTED</a>
				</p></div>
		</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dirty downside of 'return to office'; ending WFH could make climate crisis worse (167 pts)]]></title>
            <link>https://www.businessinsider.com/return-to-office-remote-work-from-home-commute-companies-climate-2023-8</link>
            <guid>37180392</guid>
            <pubDate>Fri, 18 Aug 2023 18:53:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.businessinsider.com/return-to-office-remote-work-from-home-commute-companies-climate-2023-8">https://www.businessinsider.com/return-to-office-remote-work-from-home-commute-companies-climate-2023-8</a>, See on <a href="https://news.ycombinator.com/item?id=37180392">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component-type="content-lock" data-load-strategy="exclude">
                                  <p>Rachel really hates sitting in traffic. Knowing she's about to spend up to an hour and a half in her car that day staring at taillights in front of her makes it all the more difficult to wake up in the morning.</p><p>She had a remote position over the past few years, but landed a new job in February at a nonprofit, which came with a catch: The job required her to make the hike to its Silver Spring, Maryland, office at least two days a week — forcing her to commute 30 to 45 minutes each way.&nbsp;</p><p>Rachel, who was granted a pseudonym to speak freely about her employer's policy, is among millions of workers getting back into the swing of commuting thanks to return-to-office mandates. Like many of them, Rachel said she doesn't see the point. But it's not just the time suck that bothers her, it's the environmental impact of her new routine.</p><p>"I get kind of furious when I drive to work and see the road choked with traffic," Rachel told me. "It's incredibly harmful to the environment. And offices also generate so much waste, like paper and plastic cups and utensils."&nbsp;</p><p>Rachel's concerns about the environment have been largely ignored in <a href="https://www.businessinsider.com/companies-making-workers-employees-return-to-office-rto-wfh-hybrid-2023-1" data-analytics-product-module="body_link" rel="">the battle over return-to-office mandates</a> — at least publicly. CEOs at Amazon, Google, and JPMorgan Chase argue that in-person collaboration and random watercooler conversations keep people more engaged. Opponents of the shift have emphasized the ability of workers to be more productive when they set their own schedule. But grappling with the broader planetary effects of these decisions is a failure on the part of corporations, James Elfer, the founder of More Than Now, told me.&nbsp;</p><p>"It's shocking that this isn't part of the conversation, especially at companies that claim to care about sustainability," Elfer, whose firm conducts behavioral-science experiments to improve workplaces, said. "It's a missed opportunity to explore an employer's colossal influence on our behavior."&nbsp;</p><p>Whether mandatory return-to-office policies will make the climate crisis worse is an important question, especially as scientists predict that <a href="https://www.insider.com/summer-2023-heat-wave-tracker-temperature-records-2023-5" data-analytics-product-module="body_link" rel="">2023 will be the hottest year on record</a>. Transportation accounts for about 15% of greenhouse-gas emissions warming the planet, with gas-powered cars, trucks, and buses contributing an outsize amount. But determining whether working from an office is worse for the planet isn't that simple. There are myriad factors that could tip the scales the other way, such as where people live, the amount of energy people use at home, the food they eat, the things they buy, and the extra trips they take. We are in the midst of a major upheaval in how people work, and Fortune 500 companies <a href="https://www.prnewswire.com/news-releases/fortune-announces-2022-fortune-500-list-301552608.html#:~:text=The%20revenue%20threshold%20for%202022,employ%2029.7%20million%20people%20worldwide." target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">employ nearly 30 million people around the world</a>. Failing to find a balance between boosting productivity at the office and protecting the planet risks making the crisis worse.</p><p>"Collectively, these decisions mean millions of employees working in profoundly different ways," Elfer said. "The lack of attention feels irresponsible."</p><h2>Commuting cost</h2><p>In the US, the transportation sector belches out the most greenhouse gases of any industry, and <a href="https://www.epa.gov/greenvehicles/fast-facts-transportation-greenhouse-gas-emissions" target="_blank" rel="noopener" data-analytics-product-module="body_link">passenger vehicles are largely responsible</a>, generating the equivalent of 374 million metric tons of carbon dioxide in 2021. While all that driving isn't just from commuting, about three-fourths of the 154 million Americans who do travel to work go by car. On average, they spend nearly an hour on the road each day, according to US Census data from 2021. The <a href="https://www.gov.uk/government/statistics/transport-statistics-great-britain-2022/transport-statistics-great-britain-2022-domestic-travel" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">trends are similar in the UK</a>, where cars and taxis account for half of the transportation sector's emissions, and some 68% percent of people who commute to work did so by car for an average travel time of one hour a day.&nbsp;</p><p>So it stands to reason that cutting out commutes would help keep the air cleaner. Ty Colman, a cofounder and the chief revenue officer at Optera, a carbon-accounting firm that helps organizations quantify their emissions, said that in general, a fully remote company with no offices has the lowest impact-per-employee per year, at less than 1 metric ton of carbon-dioxide equivalent. That includes the uptick in energy used to power computers, keep the lights on, and maintain a comfortable temperature at home. Employees with a hybrid-work policy that allows them to be remote three days a week emit about 1.4 metric tons per year, which increases to 1.7 metric tons under a fully in-office policy. The firm's method is based on the Greenhouse Gas Protocol, the most widely used standard among organizations, cities, and countries to track their emissions. The company adjusted a wide array of variables to see how different behaviors affected the amount of emissions per employee, Colman said, and fully in-office always produced the highest emissions unless a large majority of people made their commute by public transit — then a hybrid schedule could actually have a greater environmental impact.&nbsp;</p><p>The <a href="https://www.iea.org/commentaries/working-from-home-can-save-energy-and-reduce-emissions-but-how-much" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">International Energy Agency in 2020 found</a> that for people who commute more than 4 miles per day by car, their emissions would decrease if they worked from home. Overall, if people who can telework did so just one day a week, it would cut 24 million metric tons of emissions a year — equivalent to what the Greater London area produces each year. The IEA said that represented a "notable decline" but was small in the context of the amount emissions must drop to meet global climate goals.&nbsp;</p><blockquote><q>"I get kind of furious when I drive to work and see the road choked with traffic. It's incredibly harmful to the environment."</q></blockquote><p>The onset of the pandemic in 2020 provided a real-world opportunity to research what happens when millions of people stop commuting and work from home. Ralf Martin, an associate professor at Imperial College Business School in London, did just that. His team collected smart-meter data from a representative sample of 1,164 UK households and surveyed 452 of them about how their daily patterns changed during July and August 2020. Most people reported a commute of fewer than 20 miles and traveled by car. On average, they worked from home three days a week and commuted two, as some were essential workers and had to go in. Even as people sat at home charging laptops and taking video calls, overall household emissions dropped by 33% — electricity use went up by 6%, while gas consumption decreased by 9.5%. That rise in electricity didn't generate as big a spike in emissions as expected, Martin said, for two reasons: First, demand was more spread out during the day. In the pre-COVID world, household emissions were clustered in the early morning and the evenings, forcing more inefficient oil and gas plants to come online to meet the spikes in demand.</p><p>"During these morning and evening peaks is when energy generation is the dirtiest," Martin said. "The high demand leads to some of the more marginal, dirtier power plants getting switched on so the carbon intensity of the power grid goes up."</p><p>The second factor was a bit more odd: Since people weren't going into the office and spending time around colleagues, they probably care less about hygiene. "It's also possible people didn't shower so much," Martin added. "That's less energy for heating water."&nbsp;</p><p>Even though the study only covered the early part of the pandemic, it illustrated how a shift away from office work can have some positive effects for the planet. But since we are no longer in lockdown, global emissions have rebounded to higher than pre-pandemic levels because people <a href="https://www.businessinsider.com/delta-airlines-expects-memorial-day-travel-beat-pre-pandemic-levels-2023-5" data-analytics-product-module="body_link" rel="">are driving and flying again</a>, the IEA said. The added trouble is that many people used the pandemic as a reason to <a href="https://www.businessinsider.com/millennials-gen-z-leaving-cities-for-suburbs-amid-pandemic-2020-11" data-analytics-product-module="body_link" rel="">move to the suburbs</a>. That potentially means longer and more expensive commutes as employers demand people be in the office again — making a strong case for more flexibility to work from home.</p><p>"The bottom line here is that the immediate effect was this big savings in emissions," Martin said. "But what seems to be emerging is a danger that transport emissions are actually increasing as people move further away from their place of work and engage in hybrid working."</p><h2>Keeping the lights on</h2><p>Despite the intuitive nature of the argument, most experts said that sending everyone back to home offices is not a silver bullet for combating the climate crisis. If a person's commute was shorter than 4 miles or they relied on public transportation, the IEA report found, teleworking could actually increase their emissions because of the extra energy used at home. Other behaviors make a difference in this calculation, as well: What time of year is it? During the winter, energy use tends to be higher as people turn on the heat, though in the US it's actually higher in the summer because air conditioning is so common. Are people wasting more food or running their electronic devices unnecessarily? Running more errands? Taking more weekend trips?</p><p>In 2021, More Than Now outlined a road map for how companies could <a href="https://hbr.org/2022/03/is-remote-work-actually-better-for-the-environment" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">determine the optimal mix of commuting and WFH</a> for their employees. The firm's initial research found that employees' net-sustainability impact depended on travel, the energy and digital devices they use, waste management, and local infrastructure.</p><p>"Each was dramatically influenced by remote work and had pros and cons when it comes to our environmental footprint," Elfer said. "There was no clear answer to whether work from home was better or worse for the environment in general terms."</p><blockquote><q>It's shocking that this isn't part of the conversation, especially at companies that claim to care about sustainability</q></blockquote><p>Tailoring people's behavior to ensure that working from home actually cuts emissions can be hard, especially without robust data from employees on how they spend their time. Even without this information, some institutions are encouraging people to be more sustainable at home, including American University in Washington, DC. Megan Litke, the director of sustainability at American, said her office conducts a survey each year of faculty and staff to track their commuting habits. However, they haven't found a way to accurately measure what people are doing at home.</p><p>"We've taken the approach that we don't need a firm number to start taking action on it," Litke said. "We provide people with a list of actions they can do, broken out by morning routine, lunch break, and afternoon stretch."</p><p>A <a href="https://www.american.edu/about/sustainability/get-involved/green-home-guide.cfm" target="_blank" rel="noopener" data-analytics-product-module="body_link">Green Home Guide suggests</a> people avoid plastic K-Cups for coffee and eat meat- and dairy-free meals to lower their carbon footprint, and it offers tips for recycling, composting, and buying green cleaning products. Energy-saving techniques are also included, such as putting desks near a window to take advantage of daylight and setting electronic devices to go into sleep mode after a certain window of time.</p><p>"It's important to recognize that working from home isn't a perfect environmental solution," Litke said. "We have to think about it holistically."</p><h2>Head in the sand</h2><p>While scientists and experts are trying to find the perfect mix of work-from-home and in-office time to help the planet, CEOs seem less concerned. Many companies — even ones that say <a href="https://www.businessinsider.com/climate-crisis-inequality-racial-justice-energy-technology-innovation-2022-11" data-analytics-product-module="body_link" rel="">they are dedicated to helping the planet</a> — don't seem interested in trying to figure out the answer, Efler said. More Than Now published its road map in 2021 in hopes that companies would want to partner to study the trade-offs, but in the two years since his organization started looking into the issue, it's been mostly crickets from the corporate world.</p><p>Operta's Colman said that companies are quietly considering "what emissions will be relative to various work policies." It may not be center stage in the debate because employees' commutes can represent a small portion of a company's overall emissions, Colman said, but Optera, which has clients including Target, Dell, and Williams-Sonoma, has run the numbers for dozens of companies.&nbsp;</p><p>Given the importance of the question at hand and the volume of the environmental promises made by many large corporations, the relative hush when it comes to commutes is perplexing.</p><p>Insider asked Amazon, Apple, Google, and JPMorgan Chase — all of which have ambitious climate goals — whether they considered the potential environmental trade-offs in crafting their return-to-office mandates. All declined to comment or didn't respond<strong>.</strong> In their latest sustainability reports, Amazon, Apple, and Google account for employee commutes in their overall carbon footprint. JPMorgan does not. Google <a href="https://www.gstatic.com/gumdrop/sustainability/google-2023-environmental-report.pdf" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">said commuting and teleworking contributes</a> 2% of the total emissions created by the company. Amazon <a href="https://affiliate.insider.com/?h=7648d69efeb3a0b75f2d9a47cc6073534dd497afbfcd48f8d275d6e4bd385f6f&amp;postID=64d2530e29ea7163e0de2990&amp;site=bi&amp;u=https%3A%2F%2Fsustainability.aboutamazon.com%2Fcarbon-methodology.pdf&amp;amazonTrackingID=null&amp;platform=browser&amp;sc=false&amp;disabled=false" target="_blank" rel="noopener" data-analytics-product-module="body_link">said it counts emissions</a> from company-provided shuttles and certain employer-subsidized transit but doesn't break out a specific total. Apple <a href="https://affiliate.insider.com/?h=de5fff59f18e5d767aaeb71155e925f76f0af4afa82283a29fec5087968a5e89&amp;postID=64d2530e29ea7163e0de2990&amp;site=bi&amp;u=https%3A%2F%2Fwww.apple.com%2Fenvironment%2Fpdf%2FApple_Environmental_Progress_Report_2022.pdf&amp;amazonTrackingID=null&amp;platform=browser&amp;sc=false&amp;disabled=false" target="_blank" rel="noopener" data-analytics-product-module="body_link">said employee commute and business travel</a> accounts for 0.5% of its emissions, which it offsets by purchasing carbon credits from projects that protect and restore nature. While these numbers may seem small, they do add up, and optimizing how employees commute could be an easy win for companies that ostensibly care about the future of the planet.&nbsp;</p><blockquote><q>It's important to recognize that working from home isn't a perfect environmental solution. We have to think about it holistically.</q></blockquote><p>Schneider Electric, which makes hardware and software to manage energy in buildings, is focused on making its own offices as sustainable as possible and offering employees flexibility to work from home. Tony Johnson, the director of hub sites and workplace strategy, said these efforts began around 2015. Since then, Schneider Electric has shrunk its offices from 300 to about 200 in North America.&nbsp;</p><p>"We recognized that not everybody wants or needs to be in the office every day of the week," he said. "At the same time, it's hard to build relationships that drive innovation virtually."&nbsp;</p><p>Schneider asks its 35,000 employees in North America to be in the office at least two days a week and is pursuing electric vehicles for those who regularly drive to meet with customers. The company's <a href="https://www.se.com/ww/en/assets/564/document/396659/2022-sustainability-report.pdf" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">2022 sustainability report shows</a> that emissions from its own offices and manufacturing sites around the world are decreasing, while emissions from employee commuting increased. Both categories account for a small portion of Schneider Electric's overall carbon footprint.&nbsp;&nbsp;&nbsp;&nbsp;</p><p>"This is a complicated problem, and we're learning as we go," Johnson said. "Several years ago, no one was talking about how their vendors were impacting the planet. Now, we're trying to make sure our furniture supplier recycles or refurbishes."&nbsp;</p><p>For Rachel, finding another job that is fully remote would be ideal. She already negotiated with her current employer to be in the office two days a week — as opposed to five — and doesn't think they will budge any further. That's left Rachel thinking about the ripple effects of commuting to work — the environmental impact, the weight on her mental health, even the wear and tear on her cars and the roads. For all these reasons, she's determined that going into the office simply isn't worth it.&nbsp;</p><p>"I think we go in because our CEO likes to have people to talk to," she said. "I don't think our presence is generating amazing ideas or networking. My boss just likes a more traditional work environment."</p><hr><p><em><a href="https://www.businessinsider.com/author/catherine-boudreau" data-analytics-product-module="body_link" rel="">Catherine Boudreau</a> is senior sustainability reporter at Insider.&nbsp;</em></p>
                      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dude,Where's My Donations? Wikimedia gives another $1M to non-Wikimedia projects (118 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-08-15/News_and_notes</link>
            <guid>37179587</guid>
            <pubDate>Fri, 18 Aug 2023 17:59:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-08-15/News_and_notes">https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-08-15/News_and_notes</a>, See on <a href="https://news.ycombinator.com/item?id=37179587">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
<h3><span id="Wikimedia_Foundation_gives_away_about_.241_million_in_grants_to_counter_racial_bias_and_discrimination"></span><span id="Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bias_and_discrimination" data-mw-thread-id="h-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia-signpost-article-title"><span data-mw-comment-start="" id="h-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia-signpost-article-title"></span>Wikimedia Foundation gives away about $1 million in grants to counter racial bias and discrimination<span data-mw-comment-end="h-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia-signpost-article-title"></span></span></h3>
</p><div>
<p>In 2021, the Wikimedia Foundation <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2021-09-26/News_from_Diff" title="Wikipedia:Wikipedia Signpost/2021-09-26/News from Diff">announced</a> the first grants of a "<a href="https://meta.wikimedia.org/wiki/Knowledge_Equity_Fund" title="m:Knowledge Equity Fund">Knowledge Equity Fund</a>" created in June of the previous year. This involved about a million dollars of WMF funds being given, in the form of grants, to a number of external charitable and advocacy organizations.
</p><p>This proved controversial; as we <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2022-10-31/News_and_notes" title="Wikipedia:Wikipedia Signpost/2022-10-31/News and notes">covered last October</a>, a "somewhat-viral Twitter thread" questioned the relevance of these organizations to Wikimedia projects and values, and while some of the controversy was certainly political in nature, many of the grant recipients seemed unrelated to Wikimedia projects, prompting further discussion on mailing lists. One concern was the lack of community input into the process that led to the fund's creation. Another was the use of money which was generally solicited on the grounds of being necessary to fund Wikimedia projects, meaning that many donors likely did not know or intend for their funds to be given to unrelated organizations. <a href="https://meta.wikimedia.org/wiki/Knowledge_Equity_Fund#Round_1_-_Narrative_and_Financial_reports" title="meta:Knowledge Equity Fund">Two of the grant recipients</a> from the first round seem to have not shared financial reports detailing how the money was spent.
</p><p>The Wikimedia Foundation has <a href="https://diff.wikimedia.org/2023/08/03/announcing-the-second-round-of-grantees-from-the-wikimedia-foundation-knowledge-equity-fund/">announced</a> a second round of grantees this month, saying in its announcement:
</p>
<blockquote><p>Equity – more specifically, knowledge equity – underpins our movement's vision of a world in which every human can share in the sum of all knowledge. It encourages us to consider the knowledge and communities that have been left out of the historical record, both intentionally and unintentionally. This is an important pillar of the Wikimedia movement’s strategic direction, our forward-looking approach to prepare for the Wikimedia of 2030. 
</p><p>There can be many reasons behind these gaps in knowledge, derived from systemic social, political and technical challenges that prevent all people from being able to access and contribute to free knowledge projects like Wikimedia equally. In 2021, the Wikimedia Foundation launched the Knowledge Equity Fund specifically to address gaps in the Wikimedia movement's vision of free knowledge caused by racial bias and discrimination, that have prevented populations around the world from participating equally. The fund is a part of the Wikimedia Foundation’s Annual Plan for the 2023-24 fiscal year to support knowledge equity by supporting regional and thematic strategies, and helping close knowledge gaps. Building on learnings from its first round of grants, today the Equity Fund is welcoming its second round of grantees.
</p><p>This second round includes seven grantees that span four regions, including the Fund's first-ever grantees in Asia. This diverse group of grantees was chosen from an initial pool of 42 nominations, which were received from across the Wikimedia movement through an open survey in 2022 and 2023. Each grantee aligns with one of Fund's five focus areas, identified to address persistent structural barriers that prevent equitable access and participation in open knowledge. They are also recognized nonprofits with a proven track record of impact in their region. The Knowledge Equity Fund was initially conceived in response to global demands for racial equity, and the global reach of these new grantees is testament to and in recognition of the systemic impact of racial inequity in affecting participation in knowledge across the world.
</p>
</blockquote>
<p>The grants announced are as follows:
</p>
<h4><span id=".24290.2C000_USD_to_Black_Cultural_Archives.2C_United_Kingdom"></span><span id="$290,000_USD_to_Black_Cultural_Archives,_United_Kingdom" data-mw-thread-id="h-$290,000_USD_to_Black_Cultural_Archives,_United_Kingdom-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"><span data-mw-comment-start="" id="h-$290,000_USD_to_Black_Cultural_Archives,_United_Kingdom-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span>$290,000 USD to Black Cultural Archives, United Kingdom<span data-mw-comment-end="h-$290,000_USD_to_Black_Cultural_Archives,_United_Kingdom-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span></span></h4>
<p>Black Cultural Archives is a Black-led archive and heritage center that preserves and gives access to the histories of African and Caribbean people in the UK. Their goals with this grant for the coming year include increasing research into their collections, as well as increasing the breadth of their collections for research. 
</p>
<h4><span id=".24200.2C000_USD_to_Aliansi_Masyarakat_Adat_Nusantara.2C_Indonesia"></span><span id="$200,000_USD_to_Aliansi_Masyarakat_Adat_Nusantara,_Indonesia" data-mw-thread-id="h-$200,000_USD_to_Aliansi_Masyarakat_Adat_Nusantara,_Indonesia-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"><span data-mw-comment-start="" id="h-$200,000_USD_to_Aliansi_Masyarakat_Adat_Nusantara,_Indonesia-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span>$200,000 USD to Aliansi Masyarakat Adat Nusantara, Indonesia<span data-mw-comment-end="h-$200,000_USD_to_Aliansi_Masyarakat_Adat_Nusantara,_Indonesia-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span></span></h4>
<p>The Aliansi Masyarakat Adat Nusantara, or the Alliance of the Indigenous Peoples of the Archipelago (AMAN for short), is a non-profit organization based in Indonesia that works on human rights and advocacy issues for indigenous people. 
</p>
<h4><span id=".24160.2C000_USD_to_Criola.2C_Brazil"></span><span id="$160,000_USD_to_Criola,_Brazil" data-mw-thread-id="h-$160,000_USD_to_Criola,_Brazil-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"><span data-mw-comment-start="" id="h-$160,000_USD_to_Criola,_Brazil-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span>$160,000 USD to Criola, Brazil<span data-mw-comment-end="h-$160,000_USD_to_Criola,_Brazil-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span></span></h4>
<p>Criola is a civil society organization, based in Rio de Janeiro, dedicated to advocating for the rights of Black women in Brazilian society. They prioritize knowledge production, research, and skills development as part of their work. They are also part of a national and international network of human rights, justice and advocacy organization focused on promoting racial equity.
</p>
<h4><span id=".24100.2C000_USD_to_Data_for_Black_Lives.2C_United_States"></span><span id="$100,000_USD_to_Data_for_Black_Lives,_United_States" data-mw-thread-id="h-$100,000_USD_to_Data_for_Black_Lives,_United_States-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"><span data-mw-comment-start="" id="h-$100,000_USD_to_Data_for_Black_Lives,_United_States-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span>$100,000 USD to Data for Black Lives, United States<span data-mw-comment-end="h-$100,000_USD_to_Data_for_Black_Lives,_United_States-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span></span></h4>
<p>Data for Black Lives is a movement of activists, organizers, and scientists committed to the mission of using data to create concrete and measurable change in the lives of Black people. They will use the grant in part to launch a Movement Scientists Fellowship matching racial justice leaders with machine learning research engineers to develop data-based machine learning applications to drive change in the areas of climate, genetics, and economic justice.
</p>
<h4><span id=".2475.2C000_USD_to_Create_Caribbean_Research_Institute.2C_Commonwealth_of_Dominica"></span><span id="$75,000_USD_to_Create_Caribbean_Research_Institute,_Commonwealth_of_Dominica" data-mw-thread-id="h-$75,000_USD_to_Create_Caribbean_Research_Institute,_Commonwealth_of_Dominica-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"><span data-mw-comment-start="" id="h-$75,000_USD_to_Create_Caribbean_Research_Institute,_Commonwealth_of_Dominica-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span>$75,000 USD to Create Caribbean Research Institute, Commonwealth of Dominica<span data-mw-comment-end="h-$75,000_USD_to_Create_Caribbean_Research_Institute,_Commonwealth_of_Dominica-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span></span></h4>
<p>Create Caribbean Research Institute is the first digital humanities center in the Caribbean. The grant will be used to expand Create Caribbean’s Create and Code technology education program to enable children ages 5-16 to develop information and digital literacy as well as coding skills.
</p>
<h4><span id=".2470.2C000_USD_to_Filipino_American_National_Historical_Society.2C_United_States"></span><span id="$70,000_USD_to_Filipino_American_National_Historical_Society,_United_States" data-mw-thread-id="h-$70,000_USD_to_Filipino_American_National_Historical_Society,_United_States-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"><span data-mw-comment-start="" id="h-$70,000_USD_to_Filipino_American_National_Historical_Society,_United_States-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span>$70,000 USD to Filipino American National Historical Society, United States<span data-mw-comment-end="h-$70,000_USD_to_Filipino_American_National_Historical_Society,_United_States-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span></span></h4>
<p>The Filipino American National Historical Society, or FANHS, has a mission to gather, document and share Filipino American history through its 42 community based chapters. The grant will support continuing and growing FANHS’ scholarship and advocacy on accurate historical representations of Filipino Americans and counter distorted and effaced ethnic history.
</p>
<h4><span id=".2450.2C000_USD_to_Project_Multatuli.2C_Indonesia"></span><span id="$50,000_USD_to_Project_Multatuli,_Indonesia" data-mw-thread-id="h-$50,000_USD_to_Project_Multatuli,_Indonesia-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"><span data-mw-comment-start="" id="h-$50,000_USD_to_Project_Multatuli,_Indonesia-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span>$50,000 USD to Project Multatuli, Indonesia<span data-mw-comment-end="h-$50,000_USD_to_Project_Multatuli,_Indonesia-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span></span></h4>
<p>Project Multatuli is an organization dedicated to non-profit journalism, especially for underreported topics, ranging from indigenous people to marginalized issues. Their goal is to produce data-based, deeply researched news stories to promote inclusive journalism and amplify the voices of marginalized communities. 
</p><p>For further background on the grantees, see the <a href="https://diff.wikimedia.org/2023/08/03/announcing-the-second-round-of-grantees-from-the-wikimedia-foundation-knowledge-equity-fund/">Wikimedia announcement</a>. –&nbsp;<span><a href="https://en.wikipedia.org/wiki/User:Jayen466" title="User:Jayen466">AK</a>, <a href="https://en.wikipedia.org/wiki/User:JPxG" title="User:JPxG">JG</a></span>
</p>
<h3><span id="Jimbo_promises_improvements_for_Wikimedia_Endowment.27s_lackluster_transparency"></span><span id="Jimbo_promises_improvements_for_Wikimedia_Endowment's_lackluster_transparency" data-mw-thread-id="h-Jimbo_promises_improvements_for_Wikimedia_Endowment's_lackluster_transparency-signpost-article-title"><span data-mw-comment-start="" id="h-Jimbo_promises_improvements_for_Wikimedia_Endowment's_lackluster_transparency-signpost-article-title"></span>Jimbo promises improvements for Wikimedia Endowment's lackluster transparency<span data-mw-comment-end="h-Jimbo_promises_improvements_for_Wikimedia_Endowment's_lackluster_transparency-signpost-article-title"></span></span></h3>
<p>On the subject of financial transparency regarding the Wikimedia Endowment: here is what the <a href="https://meta.wikimedia.org/wiki/Wikimedia_Endowment/Meetings/January_27,_2022" title="m:Wikimedia Endowment/Meetings/January 27, 2022">minutes of the January 2022</a> board meeting had to say about it. Not exactly a wealth of detail, but we do at least get a financial summary:
</p>
<blockquote><p><b>8) Fundraising update</b>
</p><ul><li>Overview, lead by Caitlin Virtue</li>
<li>Review of Fundraising Report, lead by Amy Parker</li>
<li>Summary: As of December 31, 2021, the Endowment held $105.4 million. There is currently $99.33 million in the investment account and $6.07 million in cash. An additional $8 million raised in December will be transferred to the Endowment in January 2022.</li></ul>
</blockquote>
<p>This summary was the last time the Endowment Board meeting minutes contained a dollar figure for the Endowment's total value (cash plus investments). Requests for an updated figure in February <span><a href="https://meta.wikimedia.org/w/index.php?title=Talk:Wikimedia_Endowment&amp;oldid=25284029#January_2023_board_meeting">remained unanswered in July</a></span>. 
</p><p>A couple of weeks ago, the Wikimedia Foundation's <a href="https://en.wikipedia.org/wiki/User:JAntonio_(WMF)" title="User:JAntonio (WMF)">Jayde Antonio</a> <span><a href="https://meta.wikimedia.org/w/index.php?title=Wikimedia_Endowment/Meetings/January_19,_2023&amp;oldid=24524042">posted</a></span> the approved minutes for the January 19, 2023 Endowment Board meeting to the its page on <a href="https://en.wikipedia.org/wiki/Meta-Wiki" title="Meta-Wiki">Meta</a>. Noticeable here is the lack of any substantial new information – apart from noting the approval of the Endowment grants which were <a href="https://diff.wikimedia.org/2023/04/13/launching-the-first-grants-from-the-wikimedia-endowment-to-support-technical-innovation-in-wikimedia-projects/">announced publicly back in April</a>, they essentially just repeat the boilerplate meeting agenda posted months ago. 
</p><p>For example, the meeting's <a href="https://meta.wikimedia.org/wiki/Wikimedia_Endowment/Meetings/January_19,_2023#Agenda" title="meta:Wikimedia Endowment/Meetings/January 19, 2023">agenda</a> (posted in February 2023) contained the following item:
</p>
<blockquote><p>6:25 - 6:55 pm UTC: Fundraising Update (Board Chair, Jimmy Wales and Endowment Director, Amy Parker)
</p><ul><li>FY22-23 year to date update</li>
<li>Campaign strategy</li></ul>
</blockquote>
<p>The <a href="https://meta.wikimedia.org/wiki/Wikimedia_Endowment/Meetings/January_19,_2023#Minutes" title="meta:Wikimedia Endowment/Meetings/January 19, 2023">minutes</a> approved by the Endowment's board, led by Jimbo Wales, repeated the same point almost verbatim when they were added in July:
</p>
<blockquote><p><b>Fundraising Update</b> (Amy Parker)
</p><ul><li>FY22-23 year to date update</li>
<li>Presentation of campaign strategy</li></ul>
</blockquote>
<p>Following a <a href="https://en.wikipedia.org/wiki/Special:PermanentLink/1169861874#Endowment" title="Special:PermanentLink/1169861874">query</a> on his user talk page about the Endowment's apparent secrecy, Jimbo <a href="https://en.wikipedia.org/wiki/Special:Diff/1169793872" title="Special:Diff/1169793872">appeared to criticize</a> the minutes approved by him and his board:
</p>
<blockquote><p>At the meeting we discussed, to universal agreement, that we should publish more information and more often [...] the discussion about publishing more information and more often came about in no small part because the January minutes were something that I felt were not good enough in terms of being open and informative. (A financial report is forthccoming – I haven't seen it yet – but delayed because the relevant person creating it has taken a bit of family leave.)
</p></blockquote>
<p>This is a strange comment, as it would seem entirely within the power of the board to determine what information the minutes of its own meetings should contain. He later clarified: "The minutes of the previous board meetings are not written in realtime in the board meeting. They are a legal document prepared in advance and reviewed by the legal team and staff."
</p><p>Following that discussion, however, Wales did provide a <a href="https://meta.wikimedia.org/wiki/Talk:Wikimedia_Endowment#Update_on_endowment" title="m:Talk:Wikimedia Endowment">more meaningful update on Meta-Wiki</a>:
</p>
<blockquote><p>In official business, the Board moved to hire KPMG as our independent auditor for the new entity, approved a spending policy for the Endowment, approved an operational budget of $2.09 million, and approved a grantmaking budget of $2.91 million for FY 2023-24.  We also set the target of $11.5 million in revenue between fundraising and investment income this fiscal year. We ended the last fiscal year with $118 million in the Wikimedia Endowment and are projecting to grow the corpus by approximately $6.5 million depending on market performance and after expenses.
</p></blockquote>
<p>How much of this $118 million is held by the Tides Foundation, and how much by the new 501(c)(3) organization, is unknown. The Wikimedia Foundation has been keen to emphasize that the Endowment is now a transparent 501(c)(3) non-profit, fulfilling a promise first made in 2017, but the Endowment website itself continues to say:
</p>
<blockquote><p>The Endowment has been established, with an initial contribution by Wikimedia Foundation, as a Collective Action Fund at Tides Foundation (Tax ID# 51-0198509).
</p></blockquote>
<p>Jimmy Wales also uploaded a <a href="https://meta.wikimedia.org/wiki/File:Wikimedia_Endowment_2023-24_Plan_-_2023-08-12_release_date.pdf" title="m:File:Wikimedia Endowment 2023-24 Plan - 2023-08-12 release date.pdf">document</a> to Meta-Wiki titled "Wikimedia Endowment 2023-24 Plan". This provides information on fundraising goals, an operational timeline, and the Endowment's budget for 2023–2024. It <a href="https://meta.wikimedia.org/w/index.php?title=File%3AWikimedia_Endowment_2023-24_Plan_-_2023-08-12_release_date.pdf&amp;page=11">mentions</a> $1.8 million in annual expenses in the most recent financial year (similar to the figure mentioned in the <a href="https://meta.wikimedia.org/wiki/Wikimedia_Endowment/Meetings/July_21,_2022" title="m:Wikimedia Endowment/Meetings/July 21, 2022">minutes for the July 2022 board meeting</a>), including $400,000 for unspecified professional services. It envisages the Endowment standing at $130.4 million by the end of the 2023–2024 fiscal year. 
</p><p>Even with the information now provided, the Wikimedia Endowment has never published a statement detailing its revenue and expenses for any year of its existence. Its actual receipts and spending from 2016 to the present day, including the fees paid to Tides, are completely opaque. The Wikimedia Endowment, the Wikimedia movement's richest affiliate, remains some way away from delivering the level of transparency ordinarily expected of Wikimedia affiliates. 
</p><p><i>See also:</i>
</p>
<ul><li><a href="https://en.wikipedia.org/wiki/User_talk:Jimbo_Wales#Endowment" title="User talk:Jimbo Wales">User talk:Jimbo Wales#Endowment</a></li>
<li><a href="https://meta.wikimedia.org/wiki/Talk:Wikimedia_Endowment#Update_on_endowment" title="m:Talk:Wikimedia Endowment">m:Talk:Wikimedia Endowment#Update on endowment</a></li></ul>
<p>–&nbsp;<span><a href="https://en.wikipedia.org/wiki/User:Jayen466" title="User:Jayen466">AK</a></span>
</p>
<h3><span id="Wikifunctions_goes_live" data-mw-thread-id="h-Wikifunctions_goes_live-signpost-article-title"><span data-mw-comment-start="" id="h-Wikifunctions_goes_live-signpost-article-title"></span>Wikifunctions goes live<span data-mw-comment-end="h-Wikifunctions_goes_live-signpost-article-title"></span></span></h3>
</div><div>
<p>The Wikimedia Foundation has <a href="https://diff.wikimedia.org/2023/08/07/wikifunctions-is-starting-up/">announced</a> that after three years of development, its <a href="https://en.wikipedia.org/wiki/Wikifunctions" title="Wikifunctions">Wikifunctions</a> project is slowly beginning to roll out.
</p>
<blockquote><p>Wikifunctions, the newest Wikimedia project, is a new space to collaboratively create and maintain a library of functions. You can think of these functions like recipes for a meal—they take inputs and produce an output (a reliable answer). You might have experienced something similar when using a search engine to find the distance between two locations, the volume of an object, converting two units, and more.
</p></blockquote>
<p>The announcement describes Wikifunctions as "a core component of the larger" <a href="https://en.wikipedia.org/wiki/Abstract_Wikipedia" title="Abstract Wikipedia">Abstract Wikipedia</a>, a project designed to have volunteers writing simple Wikipedia articles in code that can then be translated into human languages. Both projects are spearheaded by <a href="https://en.wikipedia.org/wiki/Denny_Vrande%C4%8Di%C4%87" title="Denny Vrandečić">Denny Vrandečić</a>, the former project lead of <a href="https://en.wikipedia.org/wiki/Wikidata" title="Wikidata">Wikidata</a> and a past Google employee. You can learn more about how Wikifunctions works in this short video on <a href="https://commons.wikimedia.org/wiki/File:Wikifunctions_in_7_minutes.webm" title="commons:File:Wikifunctions in 7 minutes.webm">Commons</a> and <a rel="nofollow" href="https://www.youtube.com/watch?v=bHy63VOp0RQ">YouTube</a>.
</p><p>A technical evaluation published in December 2022 had criticized this "decision to make Abstract Wikipedia depend on Wikifunctions, a new programming language and runtime environment, invented by the Abstract Wikipedia team, with design goals that exceed the scope of Abstract Wikipedia itself, and architectural issues that are incompatible with the standards of correctness, performance, and usability that Abstract Wikipedia requires." However, Vrandečić's team disputed such criticisms and rejected the evaluation's recommendations, which had included decoupling Wikifunctions from Abstract Wikipedia, and having it based on the existing <a href="https://en.wikipedia.org/wiki/Wikipedia:Lua" title="Wikipedia:Lua">Lua</a> programming language that is already integrated into MediaWiki and widely used by Wikipedia editors (see <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-01-01/Technology_report" title="Wikipedia:Wikipedia Signpost/2023-01-01/Technology report">detailed <i>Signpost</i> coverage</a>).
– <span>AK</span>, <span><a href="https://en.wikipedia.org/wiki/User:HaeB" title="User:HaeB">H</a></span>
</p>
<h3><span id="Wikimania_Singapore" data-mw-thread-id="h-Wikimania_Singapore-signpost-article-title"><span data-mw-comment-start="" id="h-Wikimania_Singapore-signpost-article-title"></span>Wikimania Singapore<span data-mw-comment-end="h-Wikimania_Singapore-signpost-article-title"></span></span></h3>
<figure typeof="mw:File"><a href="https://en.wikipedia.org/wiki/File:Wikimania_2023_Singapore_Header_logo.svg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Wikimania_2023_Singapore_Header_logo.svg/800px-Wikimania_2023_Singapore_Header_logo.svg.png" decoding="async" width="800" height="272" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Wikimania_2023_Singapore_Header_logo.svg/1200px-Wikimania_2023_Singapore_Header_logo.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Wikimania_2023_Singapore_Header_logo.svg/1600px-Wikimania_2023_Singapore_Header_logo.svg.png 2x" data-file-width="2560" data-file-height="870"></a><figcaption></figcaption></figure>
<p><a href="https://en.wikipedia.org/wiki/Wikimania" title="Wikimania">Wikimania</a> 2023 is taking place in Singapore this week, from 16 to 19 August, with some workshop, hackathon and pre-conference activities happening on 15 August. Event partners include <a href="https://en.wikipedia.org/wiki/UNESCO" title="UNESCO">UNESCO</a>, <a href="https://en.wikipedia.org/wiki/Google" title="Google">Google</a>, <a href="https://en.wikipedia.org/wiki/Creative_Commons" title="Creative Commons">Creative Commons</a> and <a href="https://en.wikipedia.org/wiki/Mozilla" title="Mozilla">Mozilla</a> as well as a number of Singaporean partners like <a href="https://en.wikipedia.org/wiki/NETS_(company)" title="NETS (company)">NETS</a> and the <a href="https://en.wikipedia.org/wiki/National_Library_Board" title="National Library Board">National Library Board</a>.
</p><p>While this year is the first time since 2019 that the Wikimedia movement's annual conference is happening as an in-person event again, it is also open to <a href="https://wikimania.wikimedia.org/wiki/2023:Registration">remote participation</a>.  The full schedule can be found <a rel="nofollow" href="https://pretalx.com/wm2023/schedule/">here</a>.
</p><p><i>The Signpost</i> wishes all those who travel to Wikimania safe journey and a great conference!
</p>
<h3><span id="Brief_notes" data-mw-thread-id="h-Brief_notes-signpost-article-title"><span data-mw-comment-start="" id="h-Brief_notes-signpost-article-title"></span>Brief notes<span data-mw-comment-end="h-Brief_notes-signpost-article-title"></span></span></h3>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Earth’s hottest month: these charts show what happened in July; what comes next (101 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-023-02552-2</link>
            <guid>37179110</guid>
            <pubDate>Fri, 18 Aug 2023 17:29:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-023-02552-2">https://www.nature.com/articles/d41586-023-02552-2</a>, See on <a href="https://news.ycombinator.com/item?id=37179110">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-02552-2/d41586-023-02552-2_25925002.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-02552-2/d41586-023-02552-2_25925002.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="A damaged saguaro cactus stands with a recently fallen arm resting on the sidewalk in Mesa, Arizona." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-02552-2/d41586-023-02552-2_25925002.jpg">
  <figcaption>
   <p><span>Intense heatwaves in the US desert southwest have been killing off the iconic saguaro cactus.</span><span>Credit: Mario Tama/Getty</span></p>
  </figcaption>
 </picture>
</figure><p>From wilting saguaros in Arizona and hot-tub-like temperatures off the coast of Florida to increased heat-related hospitalizations in Europe and agricultural losses in China, last month felt unusually hot. It was: several teams have now confirmed that July 2023 was the hottest month in recorded history. And there’s more to come.</p><p>July is typically the hottest month of the year, and this July shattered records going back as far as 1850 by around 0.25 °C. Overall, the average global temperature was 1.54 °C above the preindustrial average for July, according to Berkeley Earth, a non-profit group in California that is one of several organizations tracking global warming. It’s a seemingly small increase, but what many people across the world actually experienced was a bout of long and often <a href="https://www.nature.com/articles/d41586-023-02233-0" data-track="click" data-label="https://www.nature.com/articles/d41586-023-02233-0" data-track-category="body text link">brutal heat waves</a>.</p><p>“We’re in a particularly extreme period on top of a long-term warming trend, and the view from the top is a little scary,” says Zeke Hausfather, a climate scientist at Berkeley Earth.</p><h2>Loading the dice</h2><p>Multiple factors might have played a small part in the record-breaking temperatures, including a budding El Niño warming event in the equatorial Pacific Ocean and a volcanic eruption last year on the island of Tonga that injected water vapour, itself a powerful greenhouse gas, into the stratosphere. New regulations have also curbed the release of sulphur dioxide pollution from ships, which tends to have a cooling effect. But the biggest driver by far, scientists say, is increasing <a href="https://www.nature.com/articles/d41586-023-02233-0" data-track="click" data-label="https://www.nature.com/articles/d41586-023-02233-0" data-track-category="body text link">greenhouse-gas concentrations</a> in the atmosphere, which have been steadily raising average global temperatures and have loaded the dice in favour of extreme weather and climate events (see ‘Going up’).</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-02552-2/d41586-023-02552-2_25932032.png?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-02552-2/d41586-023-02552-2_25932032.png?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Going up. Chart showing global mean temperature increasing since 1850 to 2023." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-02552-2/d41586-023-02552-2_25932032.png">
  <figcaption>
   <p><span>Source: Berkeley Earth</span><span></span></p>
  </figcaption>
 </picture>
</figure><p>An analysis by scientists at the World Weather Attribution initiative found that the heatwave in China last month would have been expected only <a href="https://www.worldweatherattribution.org/extreme-heat-in-north-america-europe-and-china-in-july-2023-made-much-more-likely-by-climate-change/" data-track="click" data-label="https://www.worldweatherattribution.org/extreme-heat-in-north-america-europe-and-china-in-july-2023-made-much-more-likely-by-climate-change/" data-track-category="body text link">once every 250 years in a world without human influence</a>. Temperatures in southern Europe and North America, meanwhile, would have been “virtually impossible” in the preindustrial era. But such extremes are becoming the norm: last month’s events can now be expected every 5–15 years, and could happen as often as every 2–5 years if global temperatures increase to 2 °C above preindustrial levels, the upper limit imposed by the 2015 Paris climate agreement.</p><p>“It only takes a small change in average temperature for the frequency of extremes to completely blow out, which is what we’ve seen in the Northern Hemisphere recently,” says Sarah Perkins-Kirkpatrick, a climate scientist at the University of New South Wales in Sydney, Australia.</p><h2>Local troubles</h2><p>Global average temperature, often measured on a rolling ten-year basis, is a metric that scientists use to track broad trends in a noisy, complex system. Thus far, the world has warmed by 1.14 °C using that metric. But no one actually lives in an average world. And although 90% of the excess heat due to the presence of greenhouse gases has gone into the oceans, the fact is that temperatures over land are both warmer and rising faster than those of the ocean surface. Many parts of the Earth’s land surface have already warmed by more than 1.5 °C in at least one season, and temperatures in numerous places last month were as much as 8 °C above the average for July (see ‘Hot spots’).</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-02552-2/d41586-023-02552-2_25932034.png?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-02552-2/d41586-023-02552-2_25932034.png?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Hot spots. World map showing temperature anomaly during July 2023." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-02552-2/d41586-023-02552-2_25932034.png">
  <figcaption>
   <p><span>Source: Berkeley Earth</span><span></span></p>
  </figcaption>
 </picture>
</figure><p>To some extent, this should come as no surprise. The Paris agreement limits of 1.5–2 °C were intended to establish a relatively safe zone that, if maintained, would prevent many of the most severe impacts of a warming world. But a key message from the <a href="https://www.ipcc.ch/assessment-report/ar6/" data-track="click" data-label="https://www.ipcc.ch/assessment-report/ar6/" data-track-category="body text link">2021–22 assessment of the Intergovernmental Panel on Climate Change</a> is that every tenth of a degree of warming at the global level comes with additional — and often extreme — impacts at the local and regional level.</p><p>A few decades ago, many of those impacts were theoretical, but a growing body of research suggests that the planet is beginning to breach important ecological thresholds, says Jofre Carnicer, an ecologist at the University of Barcelona in Spain. Carnicer says that temperature and precipitation trends are already pushing many parts of Europe into entirely new fire regimes, evidenced by extreme wildfires in Greece and elsewhere this year<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup>.</p><h2>Heat waves rising</h2><p>Global temperature trends have tracked fairly well with projections from climate models going back more than two decades, but research into what that means at the local level is just beginning, Carnicer says (see ‘Heatwave projections’). “This is really new science,” he says, and it suggests that even the low threshold of a 1.5 °C average — which could be breached for the first time in the next several years — might be a significant challenge for the world.</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-02552-2/d41586-023-02552-2_25932036.gif?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-02552-2/d41586-023-02552-2_25932036.gif?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Heatwave projections. World map showing areas where number of days above 35 C is projected to increase." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-02552-2/d41586-023-02552-2_25932036.gif">
  <figcaption>
   <p><span>Source: IPCC</span><span></span></p>
  </figcaption>
 </picture>
</figure><p>The science makes one thing clear: the warming shows no sign of stopping. This year’s El Niño event is just getting started, and many scientists suspect that 2023 could be the hottest on record. Next year is likely to be even warmer.</p><p>“July 2023 is just the latest in a long run of extremely warm months and years,” says Sarah Kapnick, chief scientist for the National Oceanic and Atmospheric Administration. “The long-term increase in global temperature marches on and on and on.”</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Police Are Getting DNA Data from People Who Think They Opted Out (204 pts)]]></title>
            <link>https://theintercept.com/2023/08/18/gedmatch-dna-police-forensic-genetic-genealogy/</link>
            <guid>37179094</guid>
            <pubDate>Fri, 18 Aug 2023 17:28:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theintercept.com/2023/08/18/gedmatch-dna-police-forensic-genetic-genealogy/">https://theintercept.com/2023/08/18/gedmatch-dna-police-forensic-genetic-genealogy/</a>, See on <a href="https://news.ycombinator.com/item?id=37179094">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    
<p><u>CeCe Moore, an</u> actress and director-turned-genetic genealogist, stood behind a lectern at New Jersey’s Ramapo College in late July. Propelled onto the national stage by the popular PBS show “<a href="https://www.pbs.org/weta/finding-your-roots">Finding Your Roots</a>,” Moore was delivering the keynote address for the inaugural conference of forensic genetic genealogists at Ramapo, one of only two institutions of higher education in the U.S. that offer instruction in the field. It was a new era, Moore told the audience, a turning point for solving crime, and they were in on the ground floor. “We’ve created this tool that can accomplish so much,” she said.</p>



<p>Genealogists like Moore hunt for relatives and build family trees just as traditional genealogists do, but with a twist: They work with law enforcement agencies and use commercial DNA databases to search for people who can help them identify unknown human remains or perpetrators who left DNA at a crime scene.</p>



<p>The field exploded in 2018 after the <a href="https://www.vcstar.com/story/news/local/communities/ventura/2018/04/26/golden-state-killer-joseph-deangelo-dna-ventura-double-homicide/554069002/">arrest</a> of Joseph James DeAngelo as the notorious Golden State Killer, responsible for more than a dozen murders across California. DNA evidence collected from a 1980 double murder was analyzed and uploaded to a commercial database; a hit to a distant relative helped a genetic genealogist build an elaborate family tree that ultimately coalesced on DeAngelo. Since then, <a href="https://data.mendeley.com/datasets/jcycgvhm96/1">hundreds</a> of cold cases have been solved using the technique. Moore, among the field’s biggest evangelists, boasts of having personally helped close more than 200 cases.</p>



<p>The practice is not without controversy. It involves combing through the genetic information of hundreds of thousands of innocent people in search of a perpetrator. And its practitioners operate without meaningful guardrails, save for “interim” guidance <a href="https://www.justice.gov/olp/page/file/1204386/download">published</a> by the Department of Justice in 2019.</p>



<p>The last five years have been like the “Wild West,” Moore acknowledged, but she was proud to be among the founding members of the <a href="https://www.iggab.org/">Investigative Genetic Genealogy Accreditation Board</a>, which is developing professional standards for practitioners. “With this incredibly powerful tool comes immense responsibility,” she solemnly told the audience. The practice relies on public trust to convince people not only to upload their private genetic information to commercial databases, but also to allow police to rifle through that information. If you’re doing something you wouldn’t want blasted on the front page of the New York Times, Moore said, you should probably rethink what you’re doing. “If we lose public trust, we will lose this tool.” </p>



<p>Despite those words of caution, Moore is one of several high-profile genetic genealogists who exploited a loophole in a commercial database called GEDmatch, allowing them to search the DNA of individuals who explicitly opted out of sharing their genetic information with police.</p>



<!-- BLOCK(newsletter)[0](%7B%22componentName%22%3A%22NEWSLETTER%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%7D) -->

<!-- END-BLOCK(newsletter)[0] -->



<p>The loophole, which a source demonstrated for The Intercept, allows genealogists working with police to manipulate search fields within a DNA comparison tool to trick the system into showing opted-out profiles. In records of communications reviewed by The Intercept, Moore and two other forensic genetic genealogists discussed the loophole and how to trigger it. In a separate communication, one of the genealogists described hiding the fact that her organization had made an identification using an opted-out profile.</p>



<p>The communications are a disturbing example of how genetic genealogists and their law enforcement partners, in their zeal to close criminal cases, skirt privacy rules put in place by DNA database companies to protect their customers. How common these practices are remains unknown, in part because police and prosecutors have fought to keep details of genetic investigations from being turned over to criminal defendants. As commercial DNA databases grow, and the use of forensic genetic genealogy as a crime-fighting tool expands, experts say the genetic privacy of millions of Americans is in jeopardy.</p>



<p>Moore did not respond to The Intercept’s requests for comment.</p>



<!-- BLOCK(pullquote)[1](%7B%22componentName%22%3A%22PULLQUOTE%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%22pull%22%3A%22left%22%7D) --><blockquote data-shortcode-type="pullquote" data-pull="left"><!-- CONTENT(pullquote)[1] -->“If we can’t trust these practitioners, we certainly cannot trust law enforcement.”<!-- END-CONTENT(pullquote)[1] --></blockquote><!-- END-BLOCK(pullquote)[1] -->



<p>To Tiffany Roy, a DNA expert and lawyer, the fact that genetic genealogists have accessed private profiles — while simultaneously preaching about ethics — is troubling. “If we can’t trust these practitioners, we certainly cannot trust law enforcement,” she said. “These investigations have serious consequences; they involve people who have never been suspected of a crime.” At the very least, law enforcement actors should have a warrant to conduct a genetic genealogy search, she said. “Anything less is a serious violation of privacy.”</p>


<!-- BLOCK(photo)[2](%7B%22componentName%22%3A%22PHOTO%22%2C%22entityType%22%3A%22RESOURCE%22%7D)(%7B%22scroll%22%3Afalse%2C%22align%22%3A%22bleed%22%2C%22bleed%22%3A%22large%22%2C%22width%22%3A%22auto%22%7D) --><div><!-- CONTENT(photo)[2] --> <p><img decoding="async" fetchpriority="high" width="5514" height="3676" src="https://theintercept.com/wp-content/uploads/2023/08/GettyImages-1017209950.jpg" alt="MEGYN KELLY TODAY -- Pictured: (l-r) CeCe Moore and Megyn Kelly on Tuesday, August 14, 2018 -- (Photo by: Zach Pagano/NBCU Photo Bank/NBCUniversal via Getty Images via Getty Images)" srcset="https://theintercept.com/wp-content/uploads/2023/08/GettyImages-1017209950.jpg 5514w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-1017209950.jpg?resize=300,200 300w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-1017209950.jpg?resize=768,512 768w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-1017209950.jpg?resize=1024,683 1024w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-1017209950.jpg?resize=1536,1024 1536w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-1017209950.jpg?resize=2048,1365 2048w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-1017209950.jpg?resize=540,360 540w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-1017209950.jpg?resize=1000,667 1000w" sizes="(max-width: 5514px) 100vw, 5514px"></p><p>CeCe Moore appears as a guest on “Megyn Kelly Today” on Aug. 14, 2018.</p>
<p>
Photo: Zach Pagano/NBCU Photo Bank/NBCUniversal via Getty Images</p><!-- END-CONTENT(photo)[2] --></div><!-- END-BLOCK(photo)[2] -->


<h2 id="h-the-wild-west">The Wild West</h2>



<p>Forensic genetic genealogy evolved from the direct-to-consumer DNA testing craze that took hold roughly a decade ago. Companies like 23andMe and Ancestry offered DNA analysis and a database where results could be uploaded and searched against millions of other profiles, offering consumers a powerful new tool to dig into their heritage through genetics.</p>



<p>It wasn’t long before entrepreneurial genealogists realized this information could also be used to solve criminal cases, especially those that had gone cold. While the arrest of the Golden State Killer captured national attention, it was not the first case solved by forensic genetic genealogy. Two weeks earlier, genetic genealogists Margaret Press and Colleen Fitzpatrick joined officials in Ohio to <a href="https://www.limaohio.com/top-stories/2018/04/16/buckskin-girl-identified/">announce</a> that “groundbreaking work” had allowed authorities to identify a young woman whose body was found by the side of a road back in 1981. Formerly known as “Buckskin Girl” for the handmade pullover she wore, Marcia King was given her name back through genetic genealogy. “Everyone said it couldn’t be done,” Press <a href="https://www.youtube.com/watch?v=dYTI1zvfCfk">said</a>.</p>



<!-- BLOCK(cta)[3](%7B%22componentName%22%3A%22CTA%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%7D) -->

<!-- END-BLOCK(cta)[3] -->



<p>The type of consumer DNA information used in forensic genetic genealogy is far different from that uploaded to the <a href="https://www.fbi.gov/how-we-can-help-you/dna-fingerprint-act-of-2005-expungement-policy/codis-and-ndis-fact-sheet">Combined DNA Index System</a>, or CODIS, a decades-old network administered by the FBI. The DNA entered in CODIS comes from individuals convicted of or arrested for serious crimes and is often referred to as “junk” DNA: short pieces of unique genetic code that don’t carry any individual health or trait information. “It’s not telling us how the person looks. It’s not telling us about their heritage or their phenotypic traits,” Roy said. “It’s a string of numbers, like a telephone number.”</p>



<p>In contrast, the DNA testing offered by direct-to-consumer companies is “as sensitive as it gets,” Roy said. “It tells you about your origins. It tells you about your relatives and your parentage, and it tells you about your disease propensity.” And it has serious reach: While CODIS searches the DNA of people already identified by the criminal justice system, the commercial databases have the potential to search through the DNA of everyone else.</p>



<p>Individuals can upload their test results to any number of databases; at present, there are five main commercial portals. Ancestry and 23andMe are the biggest players in the field, with databases containing roughly 23 million and 14 million profiles. Individuals must test with the companies to gain access to their databases; neither allow DNA results obtained from a different testing service. Both <a href="https://www.ancestry.com/c/legal/privacystatement#:~:text=We%20do%20not%20allow%20law,the%20law%20from%20doing%20so.">Ancestry</a> and <a href="https://www.23andme.com/privacy/">23andMe</a> forbid police, and the genetic genealogists who work with them, from accessing their data for crime-fighting purposes. “We do not allow law enforcement to use Ancestry’s service to investigate crimes or to identify human remains” absent a valid court order, Ancestry’s privacy policy notes. The two companies provide regular <a href="https://www.ancestry.com/c/transparency">transparency</a> reports <a href="https://www.23andme.com/transparency-report/">documenting</a> law enforcement requests for user information.</p>



<p>MyHeritage, home to some 7 million DNA profiles, similarly bars law enforcement searches, but it does allow individuals to upload DNA results obtained from other sources.</p>



<p>And then there are <a href="https://www.familytreedna.com/">FamilyTreeDNA</a> and <a href="https://www.gedmatch.com/about/">GEDmatch</a>, which grant police access but give users the choice of opting in or out. Both allow anyone to upload their DNA results and have <a href="https://www.wired.com/story/genetic-genealogy-nonprofit-dna-database/">upward</a> of 1.8 million profiles. But neither company routinely publicizes the number of customers who have opted in, said Leah Larkin, a veteran genetic genealogist and privacy advocate from California. Larkin writes about issues in the field — including forensic genetic genealogy, which she does not practice — on her website <a href="https://thednageek.com/">the DNA Geek</a>. Larkin estimates that roughly 700,000 GEDmatch profiles are opted in. She suspects that even more are opted in on FamilyTreeDNA; opting in is the default for the company’s U.S. customers and “it’s not obvious how to opt out.”</p>



<p>But even opting out of law enforcement searches doesn’t guarantee that a profile won’t be accessed: A loophole in GEDmatch offers users working with law enforcement agencies a back door to accessing protected profiles. A source showed The Intercept how to exploit the loophole; it was not an obvious weakness or one that could be triggered mistakenly. Rather, it was a back door that required experience with the platform’s various tools to open.</p>



<p>GEDmatch’s parent company, Verogen, did not respond to a request for comment.</p>


<!-- BLOCK(photo)[4](%7B%22componentName%22%3A%22PHOTO%22%2C%22entityType%22%3A%22RESOURCE%22%7D)(%7B%22scroll%22%3Afalse%2C%22align%22%3A%22none%22%2C%22width%22%3A%22auto%22%7D) --><div><!-- CONTENT(photo)[4] --> <p><img decoding="async" width="3000" height="1907" src="https://theintercept.com/wp-content/uploads/2023/08/GettyImages-951591636.jpg" alt="CITRUS HEIGHTS, CA - APRIL 25:  Law enforcement officials leave the home of accused rapist and killer Joseph James DeAngelo on April 24, 2018 in Citrus Heights, California. Sacramento District Attorney Anne Marie Schubert was joined by law enforcement officials from across California to announce the arrest of 72 year-old Joseph James DeAngelo who is believed to be the the East Area Rapist, also known as the Golden State Killer, who killed at least 12, raped over 45 people and burglarized hundreds of homes throughout California in the 1970s and 1980s.  (Photo by Justin Sullivan/Getty Images)" srcset="https://theintercept.com/wp-content/uploads/2023/08/GettyImages-951591636.jpg 3000w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-951591636.jpg?resize=300,191 300w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-951591636.jpg?resize=768,488 768w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-951591636.jpg?resize=1024,651 1024w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-951591636.jpg?resize=1536,976 1536w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-951591636.jpg?resize=2048,1302 2048w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-951591636.jpg?resize=540,343 540w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-951591636.jpg?resize=1000,636 1000w" sizes="(max-width: 3000px) 100vw, 3000px"></p><p>Law enforcement officials leave the home of accused serial killer Joseph James DeAngelo in Citrus Heights, Calif., on April 24, 2018.</p>
<p>
Photo: Justin Sullivan/Getty Images</p><!-- END-CONTENT(photo)[4] --></div><!-- END-BLOCK(photo)[4] -->


<h2 id="h-an-open-secret">An Open Secret</h2>



<p>In forensic genetic genealogy circles, the GEDmatch loophole had long been an open secret, sources told The Intercept, one that finally surfaced publicly during the Ramapo College conference in late July.</p>



<p>Roy, the DNA expert, was giving a presentation titled “In the Hot Seat,” a primer for genealogists on what to expect if called to testify in a criminal case. There was a clear and simple theme: “Do not lie,” Roy said. “The minute you’re caught in a lie is the minute that it’s going to be difficult for people to use your work.”</p>



<p>As part of the session, David Gurney, a professor of law and society at Ramapo and director of the <a href="https://www.forensicmag.com/592484-NJ-College-Launches-World-s-First-Investigative-Genetic-Genealogy-Center/">college’s</a> nascent <a href="https://www.ramapo.edu/igg/">Investigative Genetic Genealogy Center</a>, joined Roy for a mock questioning of Cairenn Binder, a genealogist who heads up the center’s <a href="https://www.ramapo.edu/igg/certificate-program/">certificate program</a>.</p>



<p>Gurney, simulating direct examination, walked Binder through a series of friendly questions. Did she have access to DNA evidence or genetic code during her investigations? No, she replied. Could she see everyone who’d uploaded DNA to the databases? No, she said, only those who’d opted in to law enforcement searches.</p>



<p>Roy, playing the part of opposing counsel, was pointed in her cross-examination: Was Binder aware of the GEDmatch loophole? And had she used it? Yes, Binder said. “How many times?” Roy asked.</p>



<p>“A handful,” Binder replied. “Maybe up to a dozen.”</p>



<p>Binder’s answers quickly made their way into a private Facebook group for genetic genealogy enthusiasts, prompting a response from the DNA Doe Project, a volunteer-driven organization led by Press, one of the women who identified the Buckskin Girl. Before joining Ramapo College, Binder had worked for the DNA Doe Project.</p>



<p>In a statement posted to the Facebook group, Pam Lauritzen, the project’s communications director, said the loophole was an artifact of changes GEDmatch implemented in 2019, when it made opting out the default for all profiles. “While we knew that the intent of the change was to make opted-out users unavailable, some volunteers with the DNA Doe Project continued to use the reports that allowed access to profiles that were opted out,” she wrote. That use was neither “encouraged nor discouraged,” she continued. Still, she claimed the access was somehow “in compliance” with GEDmatch’s terms of service — which at the time promised that DNA uploaded for law enforcement purposes would only be matched with customers who’d opted in — and that the loophole was closed “years ago.”</p>



<p>It was a curious statement, particularly given that Press, the group’s co-founder, was among the genealogists who discussed the GEDmatch loophole in communications reviewed by The Intercept. In 2020, she described the DNA Doe Project using an opted-out profile to make an identification — and devising a way to keep that quiet.</p>



<p>Press referred The Intercept’s questions to the DNA Doe Project, which declined to comment.</p>



<p>In July 2020, GEDmatch was hacked, which resulted in all 1.45 million profiles then contained in the database to be briefly opted in to law enforcement matching; at the time, BuzzFeed News <a href="https://www.buzzfeednews.com/article/peteraldhous/hackers-gedmatch-dna-privacy">reported</a>, just 280,000 profiles had opted in. GEDmatch was taken offline “until such time that we can be absolutely sure that user data is protected against potential attacks,” Verogen <a href="https://www.facebook.com/officialGEDmatch/posts/226701508802584">wrote</a> on Facebook.</p>



<p>In the wake of the hack, a genetic genealogist named Joan Hanlon was asked by Verogen to beta test a new version of the site. According to records of a conversation reviewed by The Intercept, Press and Moore, the featured speaker at the Ramapo conference, discussed with Hanlon their tricks to access opted-out profiles and whether the new website had plugged all backdoor access. It hadn’t. It’s unclear if anyone told Verogen; as of this month, the back door was still open.</p>



<p>Hanlon did not respond to The Intercept’s requests for comment.</p>



<p>In January 2021, GEDmatch changed its terms of service to opt everyone in for searches involving unidentified human remains, making the back door irrelevant for genealogists who only worked on Doe cases, but not those working with authorities to identify perpetrators of violent crimes.<br></p>



<h2 id="h-undisclosed-methods">Undisclosed Methods</h2>



<p>Exploitation of the GEDmatch loophole isn’t the only example of genetic genealogists and their law enforcement partners playing fast and loose with the rules.</p>



<p>Law enforcement officers have used genetic genealogy to solve crimes that aren’t eligible for genetic investigation per company terms of service and Justice Department guidelines, which say the practice should be reserved for violent crimes like rape and murder only when all other “reasonable” avenues of investigation have failed. In May, CNN <a href="https://edition.cnn.com/2023/05/09/australia/australia-william-leslie-arnold-cold-case-intl-hnk-dst/index.html">reported</a> on a U.S. marshal who used genetic genealogy to solve a decades-old prison break in Nebraska. There is no prison break exception to the eligibility rules, Larkin noted in a <a href="https://thednageek.com/rules-we-dont-need-no-stinkin-rules/">post</a> on her website. “This case should never have used forensic genetic genealogy in the first place.”</p>



<!-- BLOCK(pullquote)[5](%7B%22componentName%22%3A%22PULLQUOTE%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%22pull%22%3A%22right%22%7D) --><blockquote data-shortcode-type="pullquote" data-pull="right"><!-- CONTENT(pullquote)[5] -->“This case should never have used forensic genetic genealogy in the first place.”<!-- END-CONTENT(pullquote)[5] --></blockquote><!-- END-BLOCK(pullquote)[5] -->



<p>A month later, Larkin wrote about another violation, this time in a California case. The FBI and the Riverside County Regional Cold Case Homicide Team had identified the victim of a 1996 homicide using the MyHeritage database — an explicit violation of the company’s terms of service, which <a href="https://mobileapi.myheritage.com/terms-and-conditions">make clear</a> that using the database for law enforcement purposes is “strictly prohibited” absent a court order.</p>



<p>“The case presents an example of ‘noble cause bias,’” Larkin <a href="https://thednageek.com/noble-cause-no-rules/">wrote</a>, “in which the investigators seem to feel that their objective is so worthy that they can break the rules in place to protect others.”</p>



<p>MyHeritage did not respond to a request for comment. The Riverside County Sheriff’s Office referred questions to the Riverside district attorney’s office, which declined to comment on an ongoing investigation. The FBI also declined to comment.</p>



<p>Violations have even come from inside the DNA testing companies. Back in 2019, GEDmatch co-founder Curtis Rogers unilaterally made an exception to the terms of service, without notifying the site’s users, to allow police to search for someone suspected of assault in Utah. It was a tough call, Rogers <a href="https://www.buzzfeednews.com/article/peteraldhous/genetic-genealogy-parabon-gedmatch-assault">told</a> BuzzFeed News, but the case in question “was as close to a homicide as you can get.”</p>



<p>It appears that violations have also spread to Ancestry, which prohibits the use of its DNA data for law enforcement purposes unless the company is legally compelled to provide access. Genetic genealogists told The Intercept that they are aware of examples in which genealogists working with police have provided AncestryDNA testing kits to the possible relatives of suspects — what’s known as “target testing” — or asked customers for access to preexisting accounts as a way to unlock the off-limits data.</p>



<p>A spokesperson for Ancestry did not answer The Intercept’s questions about efforts to unlock DNA data for law enforcement purposes via a third party. Instead, in a statement, the company reiterated its commitment to maintaining the privacy of its users. “Protecting our customers’ privacy and being good stewards of their data is Ancestry’s highest priority,” it read. The company did not respond to follow-up questions.</p>



<p>As it turns out, the genetic genealogy work in the Golden State Killer case was also questionable: The break that led to DeAngelo came after genealogist Barbara Rae-Venter uploaded DNA from the double murder to MyHeritage, according to the <a href="https://www.latimes.com/california/story/2020-12-08/man-in-the-window">Los Angeles Times</a>. Rae-Venter told the Times that she didn’t notify the company about what she was doing but that her actions were approved by Steve Kramer, the FBI’s Los Angeles division counsel at the time. “In his opinion, law enforcement is entitled to go where the public goes,” Rae-Venter told the paper.</p>



<p>Just how prevalent these practices are may never fully be known, in part because police and prosecutors regularly seek to shield genetic investigations from being vetted in court. They argue that what they obtain from forensic genetic genealogy is merely a tip, like information provided by an informant, and is exempt from disclosure to criminal defendants.</p>



<p>That’s exactly what’s happening in Idaho, where Bryan Kohberger is awaiting trial for the 2022 <a href="https://www.idahostatesman.com/news/local/crime/article275698001.html">murder</a> of four university students. <a href="https://www.idahostatesman.com/news/local/crime/article276611776.html">For months</a>, the state failed to disclose that it had used forensic genetic genealogy to identify Kohberger as a suspect. A probable cause <a href="https://s3.us-west-2.amazonaws.com/isc.coi/CR29-22-2805/122922+Affidavit+-+Exhibit+A+-+Statement+of+Brett-Payne.pdf">statement</a> methodically laying out the evidence that led cops to his door conspicuously omitted any mention of genetic genealogy. Kohberger’s defense team has asked to see documents related to the genealogy work as it prepares for an October trial, but the state has refused, saying the defense has no right to any information about the genetic genealogy it used to crack the case.</p>



<p>Prosecutors said it was the FBI that did the genetic genealogy work, and few records were created in the process, leaving little to turn over. But the state also <a href="https://s3.us-west-2.amazonaws.com/isc.coi/CR29-22-2805/061623+States+Motion+for+Protective+Order.pdf">argued</a> that it couldn’t turn over information because the family tree the FBI created was extensive — including “the names and personal information of … hundreds of innocent relatives” — and the privacy of those individuals needed to be maintained. According to the state, it shouldn’t even have to say which genetic database — or databases — it used.</p>



<p>Kohberger’s attorneys argue that the state’s position is preposterous and keeps them from ensuring that the work undertaken to find Kohberger was above board. “It would appear that the state is acknowledging that the companies are providing personal information to the state and that those companies and the government would suffer if the public were to realize it,” one of Kohberger’s attorneys <a href="https://s3.us-west-2.amazonaws.com/isc.coi/CR29-22-2805/062323+Objection+to+States+Motion+for+Protective+Order.pdf">wrote</a>. “The statement by the government implies that the databases searched may be ones that law enforcement is specifically barred from, which explains why they do not want to disclose their methods.”</p>



<p>A hearing on the issue is scheduled for August 18.</p>


<!-- BLOCK(photo)[6](%7B%22componentName%22%3A%22PHOTO%22%2C%22entityType%22%3A%22RESOURCE%22%7D)(%7B%22scroll%22%3Afalse%2C%22align%22%3A%22bleed%22%2C%22bleed%22%3A%22large%22%2C%22width%22%3A%22auto%22%7D) --><div><!-- CONTENT(photo)[6] --> <p><img decoding="async" width="4633" height="3141" src="https://theintercept.com/wp-content/uploads/2023/08/GettyImages-543306496.jpg" alt="LITTLETON, CO - JUNE 27: Patrick Meeker show his family tree on Ancestry.com, June 24, 2016. Meeker used Ancestry.com's DNA test to track down his birth parents. (Photo by RJ Sangosti/The Denver Post via Getty Images)" srcset="https://theintercept.com/wp-content/uploads/2023/08/GettyImages-543306496.jpg 4633w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-543306496.jpg?resize=300,203 300w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-543306496.jpg?resize=768,521 768w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-543306496.jpg?resize=1024,694 1024w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-543306496.jpg?resize=1536,1041 1536w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-543306496.jpg?resize=2048,1388 2048w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-543306496.jpg?resize=540,366 540w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-543306496.jpg?resize=1000,678 1000w" sizes="(max-width: 4633px) 100vw, 4633px"></p><p>An AncestryDNA user points to his family tree on Ancestry.com on June 24, 2016.</p>
<p>
Photo: RJ Sangosti/The Denver Post via Getty Images</p><!-- END-CONTENT(photo)[6] --></div><!-- END-BLOCK(photo)[6] -->


<h2>“A Search of All of Us”</h2>



<p>Natalie Ram, a law professor at the University of Maryland Carey School of Law and an expert in genetic privacy, believes forensic genetic genealogy is a giant fishing expedition that fails the particularity requirement of the Fourth Amendment: that law enforcement searches be targeted and based on individualized suspicion. Finding a match to crime scene DNA by searching through millions of genetic profiles is the opposite of targeted. Forensic genetic genealogy, according to Ram, “is fundamentally a search of all of us every time they do it.”</p>



<p>While proponents of forensic genetic genealogy say the individuals they’re searching have willingly uploaded their genetic information and opted in to law enforcement access, Ram and others aren’t so sure that’s the case, even when practitioners adhere to terms of service. If the consent is truly informed and voluntary, “then I think that it would be ethical, lawful, permissible for law enforcement to use that DNA … to identify those individuals who did the volunteering,” Ram said. But that’s not who is being identified in these cases. Instead, it’s relatives — and sometimes very distant relatives. “Our genetic associations are involuntary. They’re profoundly involuntary. They’re involuntary in a way that almost nothing else is. And they’re also immutable,” she said. “I can estrange myself from my family and my siblings and deprive them of information about what I’m doing in my life. And yet their DNA is informative on me.”</p>



<p>Jennifer Lynch, general counsel at the Electronic Frontier Foundation, agrees. “We’re putting other people’s privacy on the line when we’re trying to upload our own genetic information,” she said. “You can’t consent for another person. And there’s just not an argument that you have consented for your genetic information to be in a database when it’s your brother who’s uploaded the information, or when it’s somebody you don’t even know who is related to you.”</p>



<!-- BLOCK(promote-related-post)[7](%7B%22componentName%22%3A%22PROMOTE_RELATED_POST%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%22relatedPostNumber%22%3A1%7D) -->

<!-- END-BLOCK(promote-related-post)[7] -->



<p>To date, efforts to rein in the practice as a violation of the Fourth Amendment have presented some problems. A person whose arrest was built on a foundation of genetic genealogy, for example, might have been harmed by the genealogical fishing expedition but lack standing to bring a case; in the strictest sense, it wasn’t their DNA that was searched. In contrast, a third cousin whose DNA was used to identify a suspect could have standing to bring a suit, but they might be hard-pressed to prove they were harmed by the search.</p>



<p>If police are getting hits to suspects by violating companies’ terms of service — using databases that bar police searching — that “raises some serious Fourth Amendment questions” because no expectation of privacy has been waived, Ram said. Of course, ferreting out such violations would require that the information be disclosed in court, which isn’t happening.</p>



<p>At present, the only real regulators of the practice are the database owners: private companies that can change hands or terms of service with little notice. GEDmatch, which has at least once bent its terms to accommodate police, was started by two genealogy hobbyists and then sold to the biotech company Verogen, which in turn was acquired last winter by another biotech company, Qiagen. Experts like Ram and Lynch worry about the implications of so much sensitive information held in for-profit hands — and readily exploited by police. The “platforms right now are the most powerful regulators we have for most Americans,” Ram said. Police regulate “after a fashion, in a fashion, by what they do. They tell us what they’re willing to do by what they actually do,” she added. “But by the way, that’s like law enforcement making rules for itself, so not exactly a diverse group of stakeholders.”</p>



<p>For now, Ram said, the best way to regulate forensic genetic genealogy is by statute. In 2021, Maryland lawmakers passed a <a href="https://mgaleg.maryland.gov/2021RS/chapters_noln/Ch_681_hb0240E.pdf">comprehensive law</a> to restrain the practice. It requires police to obtain a warrant before conducting a genetic genealogy search — certifying that the case is an eligible violent felony and that all other reasonable avenues of investigation have failed — and notify the court before gathering DNA evidence to confirm the suspect identified via genetic genealogy is, in fact, the likely perpetrator. Currently, police use surreptitious methods to collect DNA without judicial oversight: mining a person’s garbage, for example, for items expected to contain biological evidence. In the Golden State Killer case, DeAngelo was implicated by DNA on a discarded tissue.</p>



<p>The Maryland law also requires police to obtain consent from any third party whose DNA might help solve a crime. In the Kohberger case, police searched his parents’ garbage, <a href="https://slate.com/technology/2023/01/bryan-kohberger-university-idaho-murders-forensic-genealogy.html">collecting trash</a> with DNA on it that the lab believed belonged to Kohberger’s father. In a notorious Florida case, <a href="https://www.nbcnews.com/news/us-news/they-lied-us-mom-says-police-deceived-her-get-her-n1140696">police lied</a> to a suspect’s parents to get a DNA sample from the mother, telling her they were trying to identify a person found dead whom they believed was her relative. Those methods are barred under the Maryland law.</p>



<p><a href="https://leg.mt.gov/bills/2021/billhtml/HB0602.htm">Montana</a> and <a href="https://le.utah.gov/xcode/Title53/Chapter10/53-10-S403.7.html?v=C53-10-S403.7_2023050320230503">Utah</a> have also passed laws governing forensic genetic genealogy, though neither is as strict as Maryland’s.</p>


<!-- BLOCK(photo)[8](%7B%22componentName%22%3A%22PHOTO%22%2C%22entityType%22%3A%22RESOURCE%22%7D)(%7B%22scroll%22%3Afalse%2C%22align%22%3A%22none%22%2C%22width%22%3A%22auto%22%7D) --><div><!-- CONTENT(photo)[8] --> <p><img decoding="async" loading="lazy" width="4920" height="3551" src="https://theintercept.com/wp-content/uploads/2023/08/GettyImages-634432394.jpg" alt="MyHeritage UK Ltd. DNA kits are displayed for sale at the 2017 RootsTech Conference in Salt Lake City, Utah, U.S., on Thursday, Feb. 9, 2017. The four-day conference is a genealogy event focused on discovering and sharing family connections across generations through technology. Photographer: George Frey/Bloomberg via Getty Images" srcset="https://theintercept.com/wp-content/uploads/2023/08/GettyImages-634432394.jpg 4920w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-634432394.jpg?resize=300,217 300w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-634432394.jpg?resize=768,554 768w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-634432394.jpg?resize=1024,739 1024w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-634432394.jpg?resize=1536,1109 1536w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-634432394.jpg?resize=2048,1478 2048w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-634432394.jpg?resize=540,390 540w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-634432394.jpg?resize=1000,722 1000w" sizes="(max-width: 4920px) 100vw, 4920px"></p><p>MyHeritage DNA kits are displayed at the RootsTech conference in Salt Lake City on Feb. 9, 2017.</p>
<p>
Photo: George Frey/Bloomberg via Getty Images</p><!-- END-CONTENT(photo)[8] --></div><!-- END-BLOCK(photo)[8] -->


<h2>Solving Crime Before It Happens</h2>



<p>The rise of direct-to-consumer DNA testing and forensic genetic genealogy raises another issue: the looming reality of a de facto national DNA database that can identify large swaths of the U.S. population, regardless of whether those individuals have uploaded their genetic information. In 2018, researchers led by the former chief science officer at MyHeritage <a href="https://www.science.org/doi/10.1126/science.aau4832">predicted</a> that a database of roughly 3 million people could identify nearly 100 percent of U.S. citizens of European descent. “Such a database scale is foreseeable for some third-party websites in the near future,” they concluded.</p>



<!-- BLOCK(pullquote)[9](%7B%22componentName%22%3A%22PULLQUOTE%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%22pull%22%3A%22right%22%7D) --><blockquote data-shortcode-type="pullquote" data-pull="right"><!-- CONTENT(pullquote)[9] -->“All of a sudden, we have a national DNA database, and we didn’t ever have any kind of debate about whether we wanted that in our society.”<!-- END-CONTENT(pullquote)[9] --></blockquote><!-- END-BLOCK(pullquote)[9] -->



<p>“All of a sudden, we have a national DNA database,” said Lynch, “and we didn’t ever have any kind of debate about whether we wanted that in our society.” A national database in “private hands,” she added.</p>



<p>By the time people started worrying about this as a policy issue, it was “too late,” Moore said during her address at the Ramapo conference. “By the time the vast majority of the public learned about genetic genealogy, we’d been quietly building this incredibly powerful tool for human identification behind the scenes,” she said. “People sort of laughed, like, ‘Oh, hobbyists … you do your genealogy, you do your adoption,’ and we were allowed to build this tool without interference.”</p>



<p>Moore advocated for involving forensic genetic genealogy earlier in the investigative process. Doing so, she argued, could focus police on guilty parties more quickly and save innocent people from needless law enforcement scrutiny. In fact, she told the audience, she believes that forensic genetic genealogy can help to eradicate crime. “We can stop criminals in their tracks,” she said. “I really believe we can stop serial killers from existing, stop serial rapists from existing.”</p>



<p>“We are an army. We can do this! So repeat after me,” Moore said, before leading the audience in a chant. “No more serial killers!”</p>



<p><strong>Update: August 18, 2023, 3:55 p.m. ET</strong></p>



<p><em>After this article was published, Margaret Press, founder of the DNA Doe Project, released a <a href="https://dnadoeproject.org/statement-from-Margaret-Press/">statement</a> in response to The Intercept’s findings. Press acknowledged that between May 2019 and January 2021, the organization’s leadership and volunteers made use of GEDmatch tools that provided access to DNA profiles that were opted out of law enforcement searches, which she described as “a bug in the software.” Press stated:</em></p>



<blockquote>
<p><em>We have always been committed to abide by the Terms of Service for the databases we used, and take our responsibility to our law enforcement and medical examiner partner agencies extremely seriously. In hindsight, it’s clear we failed to consider the critically important need for the public to be able to trust that their DNA data will only be shared and used with their permission and under the restrictions they choose. We should have reported these bugs to GEDmatch and stopped using the affected reports until the bugs were fixed. Instead, on that first day when we found that all of the profiles were set to opt-out, I discouraged our team from reporting them at all. I now know I was wrong and I regret my words and actions.</em></p>
</blockquote>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Plumber Problem (111 pts)]]></title>
            <link>https://hypercritical.co/2023/08/18/the-plumber-problem</link>
            <guid>37179066</guid>
            <pubDate>Fri, 18 Aug 2023 17:26:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hypercritical.co/2023/08/18/the-plumber-problem">https://hypercritical.co/2023/08/18/the-plumber-problem</a>, See on <a href="https://news.ycombinator.com/item?id=37179066">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="container">

<div id="nav">
<ul>
<li><a href="https://hypercritical.co/apps/">Apps</a></li>
<li><a href="https://hypercritical.co/about/">About</a></li>
<li><a href="https://hypercritical.co/archive/">Archive</a></li>
<li><a href="https://hypercritical.co/contact/">Contact</a></li>
<li><a href="https://hypercritical.co/feeds/main">RSS</a></li>
</ul>
</div>

<h2><a href="https://hypercritical.co/">Hypercritical<span><img src="https://hypercritical.co/images/tiny-mac.gif" width="16" height="16" alt=""></span></a></h2> 

<hr>



<div><p><time datetime="2023-08-18T12:44:19-04:00">August 18, 2023 at 12:44 PM</time>
</p></div>



<p>“The Plumber Problem” is a phrase I coined to describe the experience of watching a movie that touches on some subject area that you know way more about than the average person, and then some inaccuracy in what’s depicted distracts you and takes you out of the movie. (This can occur in any work of fiction, of course: movies, TV, books, etc.)</p>

<p>Here’s an example. A plumber is watching a movie with a scene where something having to do with pipes is integral to the plot. But it’s all wrong, and the plumber’s mind rebels. No one else in the audience is bothered. They’re all still wrapped up in the narrative. But the plumber has a problem.</p>

<p>I’m not sure how long ago I came up with this phrase. The earliest recorded occurrence I can find is from 2021, in <a href="https://www.relay.fm/rd/153">episode #153 of Reconcilable Differences</a> (<a href="https://overcast.fm/+E5IOGS27o/47:02">at 47:02</a>) where I explain it to my cohost, <a href="http://www.merlinmann.com/">Merlin</a>, so it obviously predates that.</p>

<p>The Plumber Problem is loosely related to the “<a href="https://en.wikipedia.org/wiki/Michael_Crichton#GellMannAmnesiaEffect">Gell-Mann amnesia effect</a>” which is “the phenomenon of experts believing news articles written on topics outside of their fields of expertise, yet acknowledging that articles written in the same publication within their fields of expertise are error-ridden and full of misunderstanding.”

</p><p>Anyway, I was thinking about this today thanks to some people on <a href="https://mastodon.social/">Mastodon</a> sending <a href="https://mastodon.social/@siracusa">me</a> examples of The Plumber Problem. Here are a few (lightly edited):</p>

<blockquote><a href="https://mastodon.social/@snowdolphin/110905581273831250">Simon Orrell</a>: My first exposure to “The Plumber Problem” was sitting in a theatre with my dad in 1973 watching “Emperor of the North” and my dad leans over to whisper, “They didn’t make culvert pipe like that back in the ’30s. It was plate, not corrugated.”</blockquote>

<br>

<blockquote><a href="https://sfba.social/@kalong/110908035186703085">Tim Allen</a>: In Speed 2, a plot point involves a laden oil tanker about to collide explosively. My wife, native to a major oil port city, couldn’t follow the plot because she could tell the tanker was empty just by looking at it, so she didn’t understand why everyone was saying it would explode.</blockquote>

<br>

<blockquote><a href="https://vmst.io/@DanMorgan/110910521055493784">Dan Morgan</a>: Interstellar’s farming scenes were just <a href="https://www.reddit.com/r/MovieDetails/comments/ek1ln7/in_interstellar_2014_there_are_numerous_scenes_of/">SO BAD</a>. I’m not going to <a href="https://www.reddit.com/r/MovieDetails/comments/ek1ln7/in_interstellar_2014_there_are_numerous_scenes_of/">detail</a> them here, but this retired farmer and agronomist found it hard to watch. I’m sure the physics were fine though. 😂</blockquote>

<p>Someone also <a href="https://vis.social/@gretared/110907538291037006">mentioned</a> that “The Plumber Problem” is not an easy phrase to look up online, so here’s hoping this post remedies that situation.</p>

<hr>

<p>Here’s one more bonus post that I enjoyed:</p>

<blockquote><a href="https://social.lol/@daisy55/110911857615301788">magic</a>: In Star Wars, Luke turns off his targeting computer to use the Force for his attack run on the Death Star. I’ve flown from one side of this galaxy to the other. I’ve seen a lot of strange stuff, but I’ve never seen anything to make me believe there’s one all-powerful Force controlling everything. There’s no mystical energy field that controls my destiny.</blockquote>

<!--
<iframe src="https://mastodon.social/@snowdolphin/110905581273831250/embed" class="mastodon-embed" style="max-width: 100%; border: 0" width="400" allowfullscreen="allowfullscreen"></iframe><script src="https://mastodon.social/embed.js" async="async"></script>

https://mastodon.social/@snowdolphin/110905581273831250
Simon Orrell (@snowdolphin)
@siracusa my first exposure to ‘The Plumber’s Problem’ was sitting in a theatre with my dad in 1973 watching ‘Emperor of the North’ and my dad leans over to whisper “they didn’t make culvert pipe like that back in the 30’s, it was plate not corrugated”

<iframe src="https://vmst.io/@DanMorgan/110910521055493784/embed" width="400" allowfullscreen="allowfullscreen" sandbox="allow-scripts allow-same-origin allow-popups allow-popups-to-escape-sandbox allow-forms"></iframe>

https://sfba.social/@kalong/110908035186703085
Tim Allen
@kalong@sfba.social
@siracusa @gretared @snowdolphin eg Speed 2, where a plot point is a laden oil tanker about to collide explosively. My wife, native to a major oil port city, couldn’t follow the plot, because she could tell the tanker was empty just by looking at it so didn’t understand why everyone was saying it would explode.

<iframe src="https://sfba.social/@kalong/110908035186703085/embed" width="400" allowfullscreen="allowfullscreen" sandbox="allow-scripts allow-same-origin allow-popups allow-popups-to-escape-sandbox allow-forms"></iframe>

https://vmst.io/@DanMorgan/110910521055493784
Dan Morgan :powercat: (@DanMorgan@vmst.io)
@kalong @siracusa @gretared @snowdolphin Intersteller’s farming scenes were just SO BAD. Not going to detail them here, but this retired farmer and agronomist found it hard to watch. I’m sure the physics were fine though. 😂

<iframe src="https://social.lol/@daisy55/110911857615301788/embed" width="400" allowfullscreen="allowfullscreen" sandbox="allow-scripts allow-same-origin allow-popups allow-popups-to-escape-sandbox allow-forms"></iframe>

https://social.lol/@daisy55/110911857615301788
magic💫 (@daisy55@social.lol)
@siracusa @gretared @snowdolphin in Star Wars, Luke turns off his targeting computer to use the Force for his attack run on the Death Star. I’ve flown from one side of this galaxy to the other. I’ve seen a lot of strange stuff, but I’ve never seen anything to make me believe there’s one all-powerful force controlling everything. There’s no mystical energy field that controls my destiny.
-->


<hr>



<p>© 2010-2023 John Siracusa</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zaum (142 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Zaum</link>
            <guid>37178251</guid>
            <pubDate>Fri, 18 Aug 2023 16:41:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Zaum">https://en.wikipedia.org/wiki/Zaum</a>, See on <a href="https://news.ycombinator.com/item?id=37178251">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							

						<p>From Wikipedia, the free encyclopedia</p>
					</div><div id="mw-content-text" lang="en" dir="ltr">
<p>This article is about the Russian Futurist concept. For the American band, see <a href="https://en.wikipedia.org/wiki/Zaum_(band)" title="Zaum (band)">Zaum (band)</a>.</p>
<p>Not to be confused with <a href="https://en.wikipedia.org/wiki/ZA/UM" title="ZA/UM">ZA/UM</a>.</p>
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Zangezi.jpg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Zangezi.jpg/220px-Zangezi.jpg" decoding="async" width="220" height="321" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Zangezi.jpg/330px-Zangezi.jpg 1.5x, https://upload.wikimedia.org/wikipedia/commons/7/71/Zangezi.jpg 2x" data-file-width="411" data-file-height="600"></a><figcaption><a href="https://en.wikipedia.org/wiki/Velimir_Khlebnikov" title="Velimir Khlebnikov">Khlebnikov's</a> book <i><a href="https://en.wikipedia.org/wiki/Zangezi" title="Zangezi">Zangezi</a></i> (1922)</figcaption></figure>
<p><b>Zaum</b> (<a href="https://en.wikipedia.org/wiki/Russian_language" title="Russian language">Russian</a>: <span lang="ru">за́умь</span>, <small><a href="https://en.wikipedia.org/wiki/Literal_translation" title="Literal translation">lit.</a> </small>'transrational') are the linguistic experiments in <a href="https://en.wikipedia.org/wiki/Sound_symbolism" title="Sound symbolism">sound symbolism</a> and <a href="https://en.wikipedia.org/wiki/Artistic_language" title="Artistic language">language creation</a> of <a href="https://en.wikipedia.org/wiki/Cubo-Futurism" title="Cubo-Futurism">Russian Cubo-Futurist</a> poets such as <a href="https://en.wikipedia.org/wiki/Velimir_Khlebnikov" title="Velimir Khlebnikov">Velimir Khlebnikov</a> and <a href="https://en.wikipedia.org/wiki/Aleksei_Kruchenykh" title="Aleksei Kruchenykh">Aleksei Kruchenykh</a>. Zaum is a non-referential <a href="https://en.wikipedia.org/wiki/Phonetics" title="Phonetics">phonetic</a> entity with its own <a href="https://en.wikipedia.org/wiki/Ontology" title="Ontology">ontology</a>. The language consists of <a href="https://en.wikipedia.org/wiki/Neologism" title="Neologism">neologisms</a> that mean nothing. Zaum is a language organized through phonetic analogy and rhythm.<sup id="cite_ref-:0_1-0"><a href="#cite_note-:0-1">[1]</a></sup> Zaum literature cannot contain any <a href="https://en.wikipedia.org/wiki/Onomatopoeia" title="Onomatopoeia">onomatopoeia</a> or <a href="https://en.wikipedia.org/wiki/Psychopathology" title="Psychopathology">psychopathological states</a>.<sup id="cite_ref-:1_2-0"><a href="#cite_note-:1-2">[2]</a></sup>
</p>
<meta property="mw:PageProp/toc">
<h2><span id="Usage">Usage</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Zaum&amp;action=edit&amp;section=1" title="Edit section: Usage">edit</a><span>]</span></span></h2>
<p>Aleksei Kruchenykh created Zaum in order to show that language was indefinite and indeterminate.<sup id="cite_ref-:1_2-1"><a href="#cite_note-:1-2">[2]</a></sup>
</p><p>Kruchenykh stated that when creating Zaum, he decided to forgo <a href="https://en.wikipedia.org/wiki/Grammar" title="Grammar">grammar</a> and <a href="https://en.wikipedia.org/wiki/Syntax" title="Syntax">syntax</a> rules. He wanted to convey the disorder of life by introducing disorder into the language. Kruchenykh considered Zaum to be the manifestation of a spontaneous non-codified language.<sup id="cite_ref-:0_1-1"><a href="#cite_note-:0-1">[1]</a></sup>
</p><p>Khelinbov believed that the purpose of Zaum was to find the essential meaning of <a href="https://en.wikipedia.org/wiki/Root_(linguistics)" title="Root (linguistics)">word roots</a> in <a href="https://en.wikipedia.org/wiki/Consonant" title="Consonant">consonantal sounds</a>. He believed such knowledge could help create a new <a href="https://en.wikipedia.org/wiki/Universal_language" title="Universal language">universal language</a> based on reason.<sup id="cite_ref-:0_1-2"><a href="#cite_note-:0-1">[1]</a></sup>
</p><p>Examples of zaum include Kruchenykh's poem "<a href="https://en.wikipedia.org/wiki/Dyr_bul_shchyl" title="Dyr bul shchyl">Dyr bul shchyl</a>",<sup id="cite_ref-FOOTNOTEJanecek199649_3-0"><a href="#cite_note-FOOTNOTEJanecek199649-3">[3]</a></sup> Kruchenykh's libretto for the Futurist opera <i><a href="https://en.wikipedia.org/wiki/Victory_over_the_Sun" title="Victory over the Sun">Victory over the Sun</a></i> with music by <a href="https://en.wikipedia.org/wiki/Mikhail_Matyushin" title="Mikhail Matyushin">Mikhail Matyushin</a> and stage design by <a href="https://en.wikipedia.org/wiki/Kazimir_Malevich" title="Kazimir Malevich">Kazimir Malevich</a>,<sup id="cite_ref-FOOTNOTEJanecek1996111_4-0"><a href="#cite_note-FOOTNOTEJanecek1996111-4">[4]</a></sup> and Khlebnikov's so-called "<a href="https://en.wikipedia.org/wiki/Language_of_the_birds" title="Language of the birds">language of the birds</a>", "<a href="https://en.wikipedia.org/wiki/Language_of_the_gods" title="Language of the gods">language of the gods</a>" and "language of the stars".<sup id="cite_ref-FOOTNOTEJanecek1996137–138_5-0"><a href="#cite_note-FOOTNOTEJanecek1996137–138-5">[5]</a></sup> The poetic output is perhaps comparable to that of the contemporary <a href="https://en.wikipedia.org/wiki/Dadaism" title="Dadaism">Dadaism</a> but the linguistic theory or metaphysics behind zaum was entirely devoid of the gentle reflexive irony of that movement and in all seriousness intended to recover the <a href="https://en.wikipedia.org/wiki/Sound_symbolism" title="Sound symbolism">sound symbolism</a> of a lost <a href="https://en.wikipedia.org/wiki/Glottogony" title="Glottogony">aboriginal tongue</a>.<sup id="cite_ref-FOOTNOTEJanecek199679_6-0"><a href="#cite_note-FOOTNOTEJanecek199679-6">[6]</a></sup> Exhibiting traits of a Slavic <a href="https://en.wikipedia.org/wiki/National_mysticism" title="National mysticism">national mysticism</a>, Kruchenykh aimed at recovering the primeval Slavic mother-tongue in particular.
</p><p>Kruchenykh would author many poems and <a href="https://en.wikipedia.org/wiki/Mimeograph" title="Mimeograph">mimeographed</a> <a href="https://en.wikipedia.org/wiki/Pamphlet" title="Pamphlet">pamphlets</a> written in Zaum. These pamphlets combine poetry, illustrations, and theory.<sup id="cite_ref-:0_1-3"><a href="#cite_note-:0-1">[1]</a></sup>
</p><p>In modern times, since 1962 <a href="https://en.wikipedia.org/wiki/Serge_Segay" title="Serge Segay">Serge Segay</a> was creating zaum poetry.<sup id="cite_ref-У_Голубой_Лагуны_7-0"><a href="#cite_note-У_Голубой_Лагуны-7">[7]</a></sup> <a href="https://en.wikipedia.org/wiki/Ry_Nikonova" title="Ry Nikonova">Rea Nikonova</a> started creating zaum verses probably a bit later, around 1964.<sup id="cite_ref-Zhumati_8-0"><a href="#cite_note-Zhumati-8">[8]</a></sup> Their zaum poetry can be seen e.g. in issues of the famous "Transponans" <a href="https://en.wikipedia.org/wiki/Samizdat" title="Samizdat">samizdat</a> magazine.<sup id="cite_ref-Transponans_9-0"><a href="#cite_note-Transponans-9">[9]</a></sup> In 1990, contemporary <a href="https://en.wikipedia.org/wiki/Avant-garde" title="Avant-garde">avant-garde</a> poet Sergei Biriukov has founded an association of poets called the "Academy of Zaum" in <a href="https://en.wikipedia.org/wiki/Tambov" title="Tambov">Tambov</a>.
</p><p>The use of Zaum peaked from 1916 to 1920 during <a href="https://en.wikipedia.org/wiki/World_War_I" title="World War I">World War I</a>. At this time, Zaumism took root as a movement primarily involved in <a href="https://en.wikipedia.org/wiki/Visual_arts" title="Visual arts">visual arts</a>, <a href="https://en.wikipedia.org/wiki/Literature" title="Literature">literature</a>, <a href="https://en.wikipedia.org/wiki/Poetry" title="Poetry">poetry</a>, <a href="https://en.wikipedia.org/wiki/Art_manifesto" title="Art manifesto">art manifestoes</a>, <a href="https://en.wikipedia.org/wiki/Art_theory" title="Art theory">art theory</a>, <a href="https://en.wikipedia.org/wiki/Theatre" title="Theatre">theatre</a>, and <a href="https://en.wikipedia.org/wiki/Graphic_design" title="Graphic design">graphic design</a>,<sup id="cite_ref-FOOTNOTEJanecek1984149–206_10-0"><a href="#cite_note-FOOTNOTEJanecek1984149–206-10">[10]</a></sup> and concentrated its <a href="https://en.wikipedia.org/wiki/Anti_war" title="Anti war">anti war</a> politic through a rejection of the prevailing standards in <a href="https://en.wikipedia.org/wiki/Art" title="Art">art</a> through <a href="https://en.wikipedia.org/wiki/Anti-art" title="Anti-art">anti-art</a> cultural works. Zaum activities included public gatherings, demonstrations, and publications. The movement influenced later styles, <a href="https://en.wikipedia.org/wiki/Avant-garde" title="Avant-garde">Avant-garde</a> and <a href="https://en.wikipedia.org/wiki/Downtown_music" title="Downtown music">downtown music</a> movements, and groups including <a href="https://en.wikipedia.org/wiki/Surrealism" title="Surrealism">surrealism</a>, <a href="https://en.wikipedia.org/wiki/Nouveau_r%C3%A9alisme" title="Nouveau réalisme">nouveau réalisme</a>, <a href="https://en.wikipedia.org/wiki/Pop_Art" title="Pop Art">Pop Art</a> and <a href="https://en.wikipedia.org/wiki/Fluxus" title="Fluxus">Fluxus</a>.<sup id="cite_ref-FOOTNOTEKnowlson1996217_11-0"><a href="#cite_note-FOOTNOTEKnowlson1996217-11">[11]</a></sup>
</p>
<h2><span id="Etymology_and_meaning">Etymology and meaning</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Zaum&amp;action=edit&amp;section=2" title="Edit section: Etymology and meaning">edit</a><span>]</span></span></h2>
<p>Coined by Kruchenykh in 1913,<sup id="cite_ref-FOOTNOTEJanecek19962_12-0"><a href="#cite_note-FOOTNOTEJanecek19962-12">[12]</a></sup> the word <i>zaum</i> is made up of the Russian prefix <a href="https://en.wiktionary.org/wiki/%D0%B7%D0%B0" title="wikt:за">за</a> "beyond, behind" and noun <a href="https://en.wiktionary.org/wiki/%D1%83%D0%BC" title="wikt:ум">ум</a> "the mind, <i><a href="https://en.wikipedia.org/wiki/Nous" title="Nous">nous</a></i>" and has been translated as "transreason", "transration" or "beyonsense."<sup id="cite_ref-FOOTNOTEJanecek19961_13-0"><a href="#cite_note-FOOTNOTEJanecek19961-13">[13]</a></sup> According to scholar Gerald Janecek, <i>zaum</i> can be defined as experimental poetic language characterized by indeterminacy in meaning.<sup id="cite_ref-FOOTNOTEJanecek19961_13-1"><a href="#cite_note-FOOTNOTEJanecek19961-13">[13]</a></sup>
</p><p>Kruchenykh, in "Declaration of the Word as Such (1913)", declares zaum "a language which does not have any definite meaning, a transrational language" that "allows for fuller expression" whereas, he maintains, the common language of everyday speech "binds".<sup id="cite_ref-FOOTNOTEJanecek199678_14-0"><a href="#cite_note-FOOTNOTEJanecek199678-14">[14]</a></sup> He further maintained, in "Declaration of Transrational Language (1921)", that zaum "can provide a universal poetic language, born organically, and not artificially, like <a href="https://en.wikipedia.org/wiki/Esperanto" title="Esperanto">Esperanto</a>."<sup id="cite_ref-FOOTNOTEKruchenykh2005183_15-0"><a href="#cite_note-FOOTNOTEKruchenykh2005183-15">[15]</a></sup>
</p>
<h2><span id="Major_zaumiks">Major zaumiks</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Zaum&amp;action=edit&amp;section=3" title="Edit section: Major zaumiks">edit</a><span>]</span></span></h2>
<ul><li><a href="https://en.wikipedia.org/wiki/Velimir_Khlebnikov" title="Velimir Khlebnikov">Velimir Khlebnikov</a><sup id="cite_ref-:1_2-2"><a href="#cite_note-:1-2">[2]</a></sup></li>
<li><a href="https://en.wikipedia.org/wiki/Aleksei_Kruchenykh" title="Aleksei Kruchenykh">Aleksei Kruchenykh</a><sup id="cite_ref-:1_2-3"><a href="#cite_note-:1-2">[2]</a></sup></li>
<li><a href="https://en.wikipedia.org/wiki/Ilia_Zdanevich" title="Ilia Zdanevich">Ilia Zdanevich</a><sup id="cite_ref-:1_2-4"><a href="#cite_note-:1-2">[2]</a></sup></li>
<li><a href="https://en.wikipedia.org/wiki/Igor_Terentiev" title="Igor Terentiev">Igor Terentev</a><sup id="cite_ref-:1_2-5"><a href="#cite_note-:1-2">[2]</a></sup></li>
<li>Aleksandr Tufanov<sup id="cite_ref-:1_2-6"><a href="#cite_note-:1-2">[2]</a></sup></li>
<li><a href="https://en.wikipedia.org/wiki/Kazimir_Malevich" title="Kazimir Malevich">Kazimir Malevich</a><sup id="cite_ref-:1_2-7"><a href="#cite_note-:1-2">[2]</a></sup></li>
<li><a href="https://en.wikipedia.org/wiki/Olga_Rozanova" title="Olga Rozanova">Olga Razanova</a><sup id="cite_ref-:1_2-8"><a href="#cite_note-:1-2">[2]</a></sup></li>
<li><a href="https://en.wikipedia.org/wiki/Varvara_Stepanova" title="Varvara Stepanova">Varvara Stepanova</a><sup id="cite_ref-:1_2-9"><a href="#cite_note-:1-2">[2]</a></sup></li></ul>
<h2><span id="Notes">Notes</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Zaum&amp;action=edit&amp;section=4" title="Edit section: Notes">edit</a><span>]</span></span></h2>
<div><ol>
<li id="cite_note-:0-1"><span>^ <a href="#cite_ref-:0_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:0_1-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:0_1-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-:0_1-3"><sup><i><b>d</b></i></sup></a></span> <span><cite id="CITEREFTerras1985">Terras, Victor (1985). <i>Handbook of Russian Literature</i>. London: Yale University Press. p.&nbsp;530. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-030-004-868-1" title="Special:BookSources/978-030-004-868-1"><bdi>978-030-004-868-1</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Handbook+of+Russian+Literature&amp;rft.place=London&amp;rft.pages=530&amp;rft.pub=Yale+University+Press&amp;rft.date=1985&amp;rft.isbn=978-030-004-868-1&amp;rft.aulast=Terras&amp;rft.aufirst=Victor&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AZaum"></span></span>
</li>
<li id="cite_note-:1-2"><span>^ <a href="#cite_ref-:1_2-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:1_2-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:1_2-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-:1_2-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-:1_2-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-:1_2-5"><sup><i><b>f</b></i></sup></a> <a href="#cite_ref-:1_2-6"><sup><i><b>g</b></i></sup></a> <a href="#cite_ref-:1_2-7"><sup><i><b>h</b></i></sup></a> <a href="#cite_ref-:1_2-8"><sup><i><b>i</b></i></sup></a> <a href="#cite_ref-:1_2-9"><sup><i><b>j</b></i></sup></a></span> <span><cite id="CITEREFKostelanetz2013">Kostelanetz, Richard (2013). <i>A Dictionary of the Avant-Gardes</i>. New York: Taylor&amp;Francis. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-113-680-619-3" title="Special:BookSources/978-113-680-619-3"><bdi>978-113-680-619-3</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=A+Dictionary+of+the+Avant-Gardes&amp;rft.place=New+York&amp;rft.pub=Taylor%26Francis&amp;rft.date=2013&amp;rft.isbn=978-113-680-619-3&amp;rft.aulast=Kostelanetz&amp;rft.aufirst=Richard&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AZaum"></span></span>
</li>
<li id="cite_note-FOOTNOTEJanecek199649-3"><span><b><a href="#cite_ref-FOOTNOTEJanecek199649_3-0">^</a></b></span> <span><a href="#CITEREFJanecek1996">Janecek 1996</a>, p.&nbsp;49.</span>
</li>
<li id="cite_note-FOOTNOTEJanecek1996111-4"><span><b><a href="#cite_ref-FOOTNOTEJanecek1996111_4-0">^</a></b></span> <span><a href="#CITEREFJanecek1996">Janecek 1996</a>, p.&nbsp;111.</span>
</li>
<li id="cite_note-FOOTNOTEJanecek1996137–138-5"><span><b><a href="#cite_ref-FOOTNOTEJanecek1996137–138_5-0">^</a></b></span> <span><a href="#CITEREFJanecek1996">Janecek 1996</a>, pp.&nbsp;137–138.</span>
</li>
<li id="cite_note-FOOTNOTEJanecek199679-6"><span><b><a href="#cite_ref-FOOTNOTEJanecek199679_6-0">^</a></b></span> <span><a href="#CITEREFJanecek1996">Janecek 1996</a>, p.&nbsp;79.</span>
</li>
<li id="cite_note-У_Голубой_Лагуны-7"><span><b><a href="#cite_ref-У_Голубой_Лагуны_7-0">^</a></b></span> <span><i><a href="https://en.wikipedia.org/w/index.php?title=%D0%9A%D1%83%D0%B7%D1%8C%D0%BC%D0%B8%D0%BD%D1%81%D0%BA%D0%B8%D0%B9,_%D0%9A%D0%BE%D0%BD%D1%81%D1%82%D0%B0%D0%BD%D1%82%D0%B8%D0%BD_%D0%9A%D0%BE%D0%BD%D1%81%D1%82%D0%B0%D0%BD%D1%82%D0%B8%D0%BD%D0%BE%D0%B2%D0%B8%D1%87&amp;action=edit&amp;redlink=1" title="Кузьминский, Константин Константинович (page does not exist)">Кузьминский К.</a>, Ковалёв Г.</i> <a rel="nofollow" href="http://www.kkk-bluelagoon.ru/tom5b/transpoety.htm">Антология новейшей русской поэзии у Голубой Лагуны</a>. — Т. 5Б.</span>
</li>
<li id="cite_note-Zhumati-8"><span><b><a href="#cite_ref-Zhumati_8-0">^</a></b></span> <span><cite id="CITEREFЖумати1999">Жумати, Т. П. (1999). "<span></span>"Уктусская школа" (1965-1974)&nbsp;: К истории уральского андеграунда". <i>Известия Уральского государственного университета</i>. <b>13</b>: 125–127.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=%D0%98%D0%B7%D0%B2%D0%B5%D1%81%D1%82%D0%B8%D1%8F+%D0%A3%D1%80%D0%B0%D0%BB%D1%8C%D1%81%D0%BA%D0%BE%D0%B3%D0%BE+%D0%B3%D0%BE%D1%81%D1%83%D0%B4%D0%B0%D1%80%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE+%D1%83%D0%BD%D0%B8%D0%B2%D0%B5%D1%80%D1%81%D0%B8%D1%82%D0%B5%D1%82%D0%B0&amp;rft.atitle=%22%D0%A3%D0%BA%D1%82%D1%83%D1%81%D1%81%D0%BA%D0%B0%D1%8F+%D1%88%D0%BA%D0%BE%D0%BB%D0%B0%22+%281965-1974%29+%3A+%D0%9A+%D0%B8%D1%81%D1%82%D0%BE%D1%80%D0%B8%D0%B8+%D1%83%D1%80%D0%B0%D0%BB%D1%8C%D1%81%D0%BA%D0%BE%D0%B3%D0%BE+%D0%B0%D0%BD%D0%B4%D0%B5%D0%B3%D1%80%D0%B0%D1%83%D0%BD%D0%B4%D0%B0&amp;rft.volume=13&amp;rft.pages=125-127&amp;rft.date=1999&amp;rft.aulast=%D0%96%D1%83%D0%BC%D0%B0%D1%82%D0%B8&amp;rft.aufirst=%D0%A2.+%D0%9F.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AZaum"></span></span>
</li>
<li id="cite_note-Transponans-9"><span><b><a href="#cite_ref-Transponans_9-0">^</a></b></span> <span><cite><a rel="nofollow" href="https://samizdatcollections.library.utoronto.ca/islandora/object/samizdat:transponans">"Журнал теории и практики "Транспонанс": Комментированное электронное издание / Под ред. И. Кукуя. - A Work in Progress | Project for the Study of Dissidence and Samizdat"</a>. <i>samizdatcollections.library.utoronto.ca</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=samizdatcollections.library.utoronto.ca&amp;rft.atitle=%D0%96%D1%83%D1%80%D0%BD%D0%B0%D0%BB+%D1%82%D0%B5%D0%BE%D1%80%D0%B8%D0%B8+%D0%B8+%D0%BF%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D0%BA%D0%B8+%22%D0%A2%D1%80%D0%B0%D0%BD%D1%81%D0%BF%D0%BE%D0%BD%D0%B0%D0%BD%D1%81%22%3A+%D0%9A%D0%BE%D0%BC%D0%BC%D0%B5%D0%BD%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%BD%D0%BE%D0%B5+%D1%8D%D0%BB%D0%B5%D0%BA%D1%82%D1%80%D0%BE%D0%BD%D0%BD%D0%BE%D0%B5+%D0%B8%D0%B7%D0%B4%D0%B0%D0%BD%D0%B8%D0%B5+%2F+%D0%9F%D0%BE%D0%B4+%D1%80%D0%B5%D0%B4.+%D0%98.+%D0%9A%D1%83%D0%BA%D1%83%D1%8F.+-+A+Work+in+Progress+%26%23124%3B+Project+for+the+Study+of+Dissidence+and+Samizdat&amp;rft_id=https%3A%2F%2Fsamizdatcollections.library.utoronto.ca%2Fislandora%2Fobject%2Fsamizdat%3Atransponans&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AZaum"></span></span>
</li>
<li id="cite_note-FOOTNOTEJanecek1984149–206-10"><span><b><a href="#cite_ref-FOOTNOTEJanecek1984149–206_10-0">^</a></b></span> <span><a href="#CITEREFJanecek1984">Janecek 1984</a>, pp.&nbsp;149–206.</span>
</li>
<li id="cite_note-FOOTNOTEKnowlson1996217-11"><span><b><a href="#cite_ref-FOOTNOTEKnowlson1996217_11-0">^</a></b></span> <span><a href="#CITEREFKnowlson1996">Knowlson 1996</a>, p.&nbsp;217.</span>
</li>
<li id="cite_note-FOOTNOTEJanecek19962-12"><span><b><a href="#cite_ref-FOOTNOTEJanecek19962_12-0">^</a></b></span> <span><a href="#CITEREFJanecek1996">Janecek 1996</a>, p.&nbsp;2.</span>
</li>
<li id="cite_note-FOOTNOTEJanecek19961-13"><span>^ <a href="#cite_ref-FOOTNOTEJanecek19961_13-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-FOOTNOTEJanecek19961_13-1"><sup><i><b>b</b></i></sup></a></span> <span><a href="#CITEREFJanecek1996">Janecek 1996</a>, p.&nbsp;1.</span>
</li>
<li id="cite_note-FOOTNOTEJanecek199678-14"><span><b><a href="#cite_ref-FOOTNOTEJanecek199678_14-0">^</a></b></span> <span><a href="#CITEREFJanecek1996">Janecek 1996</a>, p.&nbsp;78.</span>
</li>
<li id="cite_note-FOOTNOTEKruchenykh2005183-15"><span><b><a href="#cite_ref-FOOTNOTEKruchenykh2005183_15-0">^</a></b></span> <span><a href="#CITEREFKruchenykh2005">Kruchenykh 2005</a>, p.&nbsp;183.</span>
</li>
</ol></div>
<h2><span id="References">References</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Zaum&amp;action=edit&amp;section=5" title="Edit section: References">edit</a><span>]</span></span></h2>
<ul><li><cite id="CITEREFJanecek1984">Janecek, Gerald (1984), <i>The Look of Russian Literature: Avant-Garde Visual Experiments 1900-1930</i>, Princeton: Princeton University Press, <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0691014579" title="Special:BookSources/978-0691014579"><bdi>978-0691014579</bdi></a></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Look+of+Russian+Literature%3A+Avant-Garde+Visual+Experiments+1900-1930&amp;rft.place=Princeton&amp;rft.pub=Princeton+University+Press&amp;rft.date=1984&amp;rft.isbn=978-0691014579&amp;rft.aulast=Janecek&amp;rft.aufirst=Gerald&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AZaum"></span></li>
<li><cite id="CITEREFJanecek1996">Janecek, Gerald (1996), <i>Zaum: The Transrational Poetry of Russian Futurism</i>, San Diego: San Diego State University Press, <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-1879691414" title="Special:BookSources/978-1879691414"><bdi>978-1879691414</bdi></a></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Zaum%3A+The+Transrational+Poetry+of+Russian+Futurism&amp;rft.place=San+Diego&amp;rft.pub=San+Diego+State+University+Press&amp;rft.date=1996&amp;rft.isbn=978-1879691414&amp;rft.aulast=Janecek&amp;rft.aufirst=Gerald&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AZaum"></span></li>
<li><cite id="CITEREFKruchenykh2005">Kruchenykh, Aleksei (2005), Anna Lawton; Herbert Eagle (eds.), "Declaration of Transrational Language", <i>Words in Revolution: Russian Futurist Manifestoes 1912-1928</i>, Washington: New Academia Publishing, <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0974493473" title="Special:BookSources/978-0974493473"><bdi>978-0974493473</bdi></a></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Words+in+Revolution%3A+Russian+Futurist+Manifestoes+1912-1928&amp;rft.atitle=Declaration+of+Transrational+Language&amp;rft.date=2005&amp;rft.isbn=978-0974493473&amp;rft.aulast=Kruchenykh&amp;rft.aufirst=Aleksei&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AZaum"></span></li>
<li><cite id="CITEREFKnowlson1996">Knowlson, J. (1996), <i>The Continuing Influence of Zaum</i>, London: Bloomsbury</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Continuing+Influence+of+Zaum&amp;rft.place=London&amp;rft.pub=Bloomsbury&amp;rft.date=1996&amp;rft.aulast=Knowlson&amp;rft.aufirst=J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AZaum"></span></li></ul>
<h2><span id="External_links">External links</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Zaum&amp;action=edit&amp;section=6" title="Edit section: External links">edit</a><span>]</span></span></h2>
<ul><li><a rel="nofollow" href="http://www.thing.net/~grist/l%26d/kruch/lkrucht1.htm">Chapter Nine of G. Janecek, <i>Zaum: The Transrational Poetry of Russian Futurism</i></a></li>
<li><a rel="nofollow" href="http://sdsupress.sdsu.edu/codex.html">Janecek's <i>Zaum</i>, published by San Diego State University Press</a></li>
<li><a rel="nofollow" href="http://users.belgacom.net/nachtschimmen/zaumpaper.htm">Lecture by Z. Laskewicz: "Zaum: Words Without Meaning or Meaning Without Words? Towards a Musical Understanding of Language"</a></li>
<li><a rel="nofollow" href="http://jacketmagazine.com/27/reed.html">'Locating Zaum: Mnatsakanova on Khlebnikov'</a> an essay by Brian Reed</li>
<li><a rel="nofollow" href="http://www.vavilon.ru/texts/purin3-8.html">Article by A. Purin: "Meaning and Zaum"</a> <span>(in Russian)</span></li>
<li><a rel="nofollow" href="http://www.tstu.ru/koi/kultur/literary/lit_tamb/az.htm">Tambov Academy of Zaum</a>, Cyrillic KOI8-R encoding <span>(in Russian)</span></li>
<li><a rel="nofollow" href="http://www.transfurism.com/serge-segay-books">Samizdat books and artist' books by Serge Segay, some with zaum and visual poetry</a></li>
<li><a rel="nofollow" href="http://www.transfurism.com/ry-nikonova-books">Samizdat books and artist' books by Ry Nikonova, some with zaum and visual poetry</a></li></ul>


<!-- 
NewPP limit report
Parsed by mw1379
Cached time: 20230818235036
Cache expiry: 1814400
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.568 seconds
Real time usage: 0.693 seconds
Preprocessor visited node count: 1564/1000000
Post‐expand include size: 71743/2097152 bytes
Template argument size: 1448/2097152 bytes
Highest expansion depth: 9/100
Expensive parser function count: 2/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 42062/5000000 bytes
Lua time usage: 0.380/10.000 seconds
Lua memory usage: 22397093/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  569.425      1 -total
 21.89%  124.644      3 Template:Navbox
 21.44%  122.061      1 Template:Reflist
 18.11%  103.134      1 Template:Futurism
 17.23%   98.085      1 Template:Lang-ru
 16.01%   91.181      2 Template:Cite_book
 14.54%   82.821     11 Template:Sfn
 12.29%   69.970      1 Template:Short_description
  7.33%   41.746      2 Template:Pagetype
  4.53%   25.778      4 Template:Citation
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:915530-0!canonical and timestamp 20230818235044 and revision id 1171086231. Rendering was triggered because: edit-page
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[C and C++ prioritize performance over correctness (234 pts)]]></title>
            <link>https://research.swtch.com/ub</link>
            <guid>37178009</guid>
            <pubDate>Fri, 18 Aug 2023 16:27:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://research.swtch.com/ub">https://research.swtch.com/ub</a>, See on <a href="https://news.ycombinator.com/item?id=37178009">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <h2>C and C++ Prioritize Performance over Correctness
        
        <div>
        <p>
          
            Posted on Friday, August 18, 2023.
            
           <span size="-1"><a href="https://research.swtch.com/ub.pdf">PDF</a></span>
        </p>
        </div>
        </h2>
        

<p>
The original ANSI C standard, C89, introduced the concept of “undefined behavior,”
which was used both to describe the effect of outright bugs like
accessing memory in a freed object
and also to capture the fact that existing implementations differed about
handling certain aspects of the language,
including use of uninitialized values,
signed integer overflow, and null pointer handling.

</p><p>
The C89 spec defined undefined behavior (in section 1.6) as:</p><blockquote>

<p>
Undefined behavior—behavior, upon use of a nonportable or
erroneous program construct, of erroneous data, or of
indeterminately-valued objects, for which the Standard imposes no
requirements.  Permissible undefined behavior ranges from ignoring the
situation completely with unpredictable results, to behaving during
translation or program execution in a documented manner characteristic
of the environment (with or without the issuance of a diagnostic
message), to terminating a translation or execution (with the issuance
of a diagnostic message).</p></blockquote>

<p>
Lumping both non-portable and buggy code into the same category was a mistake.
As time has gone on, the way compilers treat undefined behavior
has led to more and more unexpectedly broken programs,
to the point where it is becoming difficult to tell whether any program
will compile to the meaning in the original source.
This post looks at a few examples and then tries to make some general observations.
In particular, today’s C and C++ prioritize
performance to the clear detriment of correctness.
<a href="#uninit"></a></p><h2 id="uninit"><a href="#uninit">Uninitialized variables</a></h2>


<p>
C and C++ do not require variables to be initialized
on declaration (explicitly or implicitly) like Go and Java.
Reading from an uninitialized variable is undefined behavior.

</p><p>
In a <a href="http://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html">blog post</a>,
Chris Lattner (creator of LLVM and Clang) explains the rationale:</p><blockquote>

<p>
<b>Use of an uninitialized variable</b>:
This is commonly known as source of problems in C programs
and there are many tools to catch these:
from compiler warnings to static and dynamic analyzers.
This improves performance by not requiring that all variables
be zero initialized when they come into scope (as Java does).
For most scalar variables, this would cause little overhead,
but stack arrays and malloc’d memory would incur
a memset of the storage, which could be quite costly,
particularly since the storage is usually completely overwritten.</p></blockquote>

<p>
Early C compilers were too crude to detect
use of uninitialized basic variables like integers and pointers,
but modern compilers are dramatically more sophisticated.
They could absolutely react in these cases by
“terminating a translation or execution (with the issuance
of a diagnostic message),”
which is to say reporting a compile error.
Or, if they were worried about not rejecting old programs,
they could insert a zero initialization with, as Lattner admits, little overhead.
But they don’t do either of these.
Instead, they just do whatever they feel like during code generation.

</p><p>
For example, here’s a simple C++ program with an uninitialized variable (a bug):
</p><pre>#include &lt;stdio.h&gt;

int main() {
    for(int i; i &lt; 10; i++) {
        printf("%d\n", i);
    }
    return 0;
}
</pre>


<p>
If you compile this with <code>clang++</code> <code>-O1</code>, it deletes the loop entirely:
<code>main</code> contains only the <code>return</code> <code>0</code>.
In effect, Clang has noticed the uninitialized variable and chosen
not to report the error to the user but instead
to pretend <code>i</code> is always initialized above 10, making the loop disappear.

</p><p>
It is true that if you compile with <code>-Wall</code>, then Clang does report the
use of the uninitialized variable as a warning.
This is why you should always build with and fix warnings in C and C++ programs.
But not all compiler-optimized undefined behaviors
are reliably reported as warnings.
<a href="#overflow"></a></p><h2 id="overflow"><a href="#overflow">Arithmetic overflow</a></h2>


<p>
At the time C89 was standardized, there were still legacy
<a href="https://en.wikipedia.org/wiki/Ones%27_complement">ones’-complement computers</a>,
so ANSI C could not assume the now-standard two’s-complement representation
for negative numbers.
In two’s complement, an <code>int8</code> −1 is 0b11111111;
in ones’ complement that’s −0, while −1 is 0b11111110.
This meant that operations like signed integer overflow could not be defined,
because</p><blockquote>

<p>
<code>int8</code> 127+1 = 0b01111111+1 = 0b10000000</p></blockquote>

<p>
is −127 in ones’ complement but −128 in two’s complement.
That is, signed integer overflow was non-portable.
Declaring it undefined behavior let compilers escalate the behavior
from “non-portable”, with one of two clear meanings,
to whatever they feel like doing.
For example, a common thing programmers expect is that you can test
for signed integer overflow by checking whether the result is
less than one of the operands, as in this program:
</p><pre>#include &lt;stdio.h&gt;

int f(int x) {
    if(x+100 &lt; x)
        printf("overflow\n");
    return x+100;
}
</pre>


<p>
Clang optimizes away the <code>if</code> statement.
The justification is that since signed integer overflow is undefined behavior,
the compiler can assume it never happens, so <code>x+100</code> must never be less than <code>x</code>.
Ironically, this program would correctly detect overflow
on both ones’-complement and two’s-complement machines
if the compiler would actually emit the check.

</p><p>
In this case, <code>clang++</code> <code>-O1</code> <code>-Wall</code> prints no warning while it deletes the <code>if</code> statement,
and neither does <code>g++</code>,
although I seem to remember it used to, perhaps in subtly different situations
or with different flags.

</p><p>
For C++20, the <a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0907r0.html">first version of proposal P0907</a>
suggested standardizing that signed integer overflow
wraps in two’s complement. The original draft gave a very clear statement of the history
of the undefined behavior and the motivation for making a change:</p><blockquote>

<p>
[C11] Integer types allows three representations for signed integral types:
</p><ul>
<li>
Signed magnitude
</li><li>
Ones’ complement
</li><li>
Two’s complement</li></ul>


<p>
See §4 C Signed Integer Wording for full wording.

</p><p>
C++ inherits these three signed integer representations from C. To the author’s knowledge no modern machine uses both C++ and a signed integer representation other than two’s complement (see §5 Survey of Signed Integer Representations). None of [MSVC], [GCC], and [LLVM] support other representations. This means that the C++ that is taught is effectively two’s complement, and the C++ that is written is two’s complement. It is extremely unlikely that there exist any significant code base developed for two’s complement machines that would actually work when run on a non-two’s complement machine.

</p><p>
The C++ that is spec’d, however, is not two’s complement. Signed integers currently allow for trap representations, extra padding bits, integral negative zero, and introduce undefined behavior and implementation-defined behavior for the sake of this extremely abstract machine.

</p><p>
Specifically, the current wording has the following effects:
</p><ul>
<li>
Associativity and commutativity of integers is needlessly obtuse.
</li><li>
Naïve overflow checks, which are often security-critical, often get eliminated by compilers. This leads to exploitable code when the intent was clearly not to and the code, while naïve, was correctly performing security checks for two’s complement integers. Correct overflow checks are difficult to write and equally difficult to read, exponentially so in generic code.
</li><li>
Conversion between signed and unsigned are implementation-defined.
</li><li>
There is no portable way to generate an arithmetic right-shift, or to sign-extend an integer, which every modern CPU supports.
</li><li>
constexpr is further restrained by this extraneous undefined behavior.
</li><li>
Atomic integral are already two’s complement and have no undefined results, therefore even freestanding implementations already support two’s complement in C++.</li></ul>


<p>
Let’s stop pretending that the C++ abstract machine should represent integers as signed magnitude or ones’ complement. These theoretical implementations are a different programming language, not our real-world C++. Users of C++ who require signed magnitude or ones’ complement integers would be better served by a pure-library solution, and so would the rest of us.</p></blockquote>

<p>
In the end, the C++ standards committee put up “strong resistance against” the idea of defining
signed integer overflow the way every programmer expects; the undefined behavior remains.
<a href="#loops"></a></p><h2 id="loops"><a href="#loops">Infinite loops</a></h2>


<p>
A programmer would never accidentally cause a program to execute an infinite loop, would they?
Consider this program:
</p><pre>#include &lt;stdio.h&gt;

int stop = 1;

void maybeStop() {
    if(stop)
        for(;;);
}

int main() {
    printf("hello, ");
    maybeStop();
    printf("world\n");
}
</pre>


<p>
This seems like a completely reasonable program to write. Perhaps you are debugging and want the program to stop so you can attach a debugger. Changing the initializer for <code>stop</code> to <code>0</code> lets the program run to completion.
But it turns out that, at least with the latest Clang, the program runs to completion anyway:
the call to <code>maybeStop</code> is optimized away entirely, even when <code>stop</code> is <code>1</code>.

</p><p>
The problem is that C++ defines that every side-effect-free loop may be assumed by the compiler to terminate.
That is, a loop that does not terminate is therefore undefined behavior.
This is purely for compiler optimizations, once again treated as more important than correctness.
The rationale for this decision played out in the C standard and was more or less adopted in the C++ standard as well.

</p><p>
John Regehr pointed out this problem in his post
“<a href="https://blog.regehr.org/archives/140">C Compilers Disprove Fermat’s Last Theorem</a>,”
which included this entry in a FAQ:</p><blockquote>

<p>
Q: Does the C standard permit/forbid the compiler to terminate infinite loops?

</p><p>
A: The compiler is given considerable freedom in how it implements the C program,
but its output must have the same externally visible behavior that the program would have when interpreted by the “C abstract machine” that is described in the standard.  Many knowledgeable people (including me) read this as saying that the termination behavior of a program must not be changed.  Obviously some compiler writers disagree, or else don’t believe that it matters.  The fact that reasonable people disagree on the interpretation would seem to indicate that the C standard is flawed.</p></blockquote>

<p>
A few months later, Douglas Walls wrote <a href="http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1509.pdf">WG14/N1509: Optimizing away infinite loops</a>,
making the case that the standard should <i>not</i> allow this optimization.
In response, Hans-J. Boehm wrote
<a href="http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1528.htm">WG14/N1528: Why undefined behavior for infinite loops?</a>,
arguing for allowing the optimization.

</p><p>
Consider the potential optimization of this code:
</p><pre>for (p = q; p != 0; p = p-&gt;next)
    ++count;
for (p = q; p != 0; p = p-&gt;next)
    ++count2;
</pre>


<p>
A sufficiently smart compiler might reduce it to this code:
</p><pre>for (p = q; p != 0; p = p-&gt;next) {
        ++count;
        ++count2;
}
</pre>


<p>
Is that safe? Not if the first loop is an infinite loop. If the list at <code>p</code> is cyclic and another thread is modifying <code>count2</code>,
then the first program has no race, while the second program does.
Compilers clearly can’t turn correct, race-free programs into racy programs.
But what if we declare that infinite loops are not correct programs?
That is, what if infinite loops were undefined behavior?
Then the compiler could optimize to its robotic heart’s content.
This is exactly what the C standards committee decided to do.

</p><p>
The rationale, paraphrased, was:
</p><ul>
<li>
It is very difficult to tell if a given loop is infinite.
</li><li>
Infinite loops are rare and typically unintentional.
</li><li>
There are many loop optimizations that are only valid for non-infinite loops.
</li><li>
The performance wins of these optimizations are deemed important.
</li><li>
Some compilers already apply these optimizations, making infinite loops non-portable too.
</li><li>
Therefore, we should declare programs with infinite loops undefined behavior, enabling the optimizations.</li></ul>
<a href="#null"><h2 id="null">Null pointer usage</h2></a>


<p>
We’ve all seen how dereferencing a null pointer causes a crash on modern operating systems:
they leave page zero unmapped by default precisely for this purpose.
But not all systems where C and C++ run have hardware memory protection.
For example, I wrote my first C and C++ programs using Turbo C on an MS-DOS system.
Reading or writing a null pointer did not cause any kind of fault:
the program just touched the memory at location zero and kept running.
The correctness of my code improved dramatically when I moved to
a Unix system that made those programs crash at the moment of the mistake.
Because the behavior is non-portable, though, dereferencing a null pointer is undefined behavior.

</p><p>
At some point, the justification for keeping the undefined behavior became performance.
<a href="http://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html">Chris Lattner explains</a>:</p><blockquote>

<p>
In C-based languages, NULL being undefined enables a large number of simple scalar optimizations that are exposed as a result of macro expansion and inlining.</p></blockquote>

<p>
In <a href="https://research.swtch.com/plmm#ub">an earlier post</a>, I showed this example, lifted from <a href="https://twitter.com/andywingo/status/903577501745770496">Twitter in 2017</a>:
</p><pre>#include &lt;cstdlib&gt;

typedef int (*Function)();

static Function Do;

static int EraseAll() {
    return system("rm -rf slash");
}

void NeverCalled() {
    Do = EraseAll;
}

int main() {
    return Do();
}
</pre>


<p>
Because calling <code>Do()</code> is undefined behavior when <code>Do</code> is null, a modern C++ compiler like Clang
simply assumes that can’t possibly be what’s happening in <code>main</code>.
Since <code>Do</code> must be either null or <code>EraseAll</code> and since null is undefined behavior,
we might as well assume <code>Do</code> is <code>EraseAll</code> unconditionally,
even though <code>NeverCalled</code> is never called.
So this program can be (and is) optimized to:
</p><pre>int main() {
    return system("rm -rf slash");
}
</pre>


<p>
Lattner gives <a href="https://blog.llvm.org/2011/05/what-every-c-programmer-should-know_14.html">an equivalent example</a> (search for <code>FP()</code>)
and then this advice:</p><blockquote>

<p>
The upshot is that it is a fixable issue: if you suspect something weird is going on like this, try building at -O0, where the compiler is much less likely to be doing any optimizations at all.</p></blockquote>

<p>
This advice is not uncommon: if you cannot debug the correctness problems in your C++ program, disable optimizations.
<a href="#sort"></a></p><h2 id="sort"><a href="#sort">Crashes out of sorts</a></h2>


<p>
C++’s <code>std::sort</code> sorts a collection of values
(abstracted as a random access iterator, but almost always an array)
according to a user-specified comparison function.
The default function is <code>operator&lt;</code>, but you can write any function.
For example if you were sorting instances of class <code>Person</code> your
comparison function might sort by the <code>LastName</code> field, breaking
ties with the <code>FirstName</code> field.
These comparison functions end up being subtle yet boring to write,
and it’s easy to make a mistake.
If you do make a mistake and pass in a comparison function that
returns inconsistent results or accidentally reports that any value
is less than itself, that’s undefined behavior:
<code>std::sort</code> is now allowed to do whatever it likes,
including walking off either end of the array
and corrupting other memory.
If you’re lucky, it will pass some of this memory to your comparison
function, and since it won’t have pointers in the right places,
your comparison function will crash.
Then at least you have a chance of guessing the comparison function is at fault.
In the worst case, memory is silently corrupted and the crash happens much later,
with <code>std::sort</code> is nowhere to be found.

</p><p>
Programmers make mistakes, and when they do, <code>std::sort</code> corupts memory.
This is not hypothetical. It happens enough in practice to be a
<a href="https://stackoverflow.com/questions/18291620/why-will-stdsort-crash-if-the-comparison-function-is-not-as-operator">popular question on StackOverflow</a>.

</p><p>
As a final note, it turns out that <code>operator&lt;</code> is not a valid comparison function
on floating-point numbers if NaNs are involved, because:
</p><ul>
<li>
1 &lt; NaN and NaN &lt; 1 are both false, implying NaN == 1.
</li><li>
2 &lt; NaN and NaN &lt; 2 are both false, implying NaN == 2.
</li><li>
Since NaN == 1 and NaN == 2, 1 == 2, yet 1 &lt; 2 is true.</li></ul>


<p>
Programming with NaNs is never pleasant, but it seems particularly extreme
to allow <code>std::sort</code> to crash when handed one.
<a href="#reveal"></a></p><h2 id="reveal"><a href="#reveal">Reflections and revealed preferences</a></h2>


<p>
Looking over these examples,
it could not be more obvious that in modern C and C++,
performance is job one and correctness is job two.
To a C/C++ compiler, a programmer making a mistake and (gasp!)
compiling a program containing a bug is just not a concern.
Rather than have the compiler point out the bug or at least
compile the code in a clear, understandable, debuggable manner,
the approach over and over again is
to let the compiler do whatever it likes,
in the name of performance.

</p><p>
This may not be the wrong decision for these languages.
There are undeniably power users for whom every last bit of performance
translates to very large sums of money, and I don’t claim
to know how to satisfy them otherwise.
On the other hand, this performance comes at a significant
development cost, and there are probably plenty of people and companies
who spend more than their performance savings
on unnecessarily difficult debugging sessions
and additional testing and sanitizing.
It also seems like there must be a middle ground where
programmers retain most of the control they have in C and C++
but the program doesn’t crash when sorting NaNs or
behave arbitrarily badly if you accidentally dereference a null pointer.
Whatever the merits, it is important to see clearly the choice that C and C++ are making.

</p><p>
In the case of arithmetic overflow, later drafts of the
proposal removed the defined behavior for wrapping, explaining:</p><blockquote>

<p>
The main change between [P0907r0] and the subsequent revision is to maintain undefined behavior when signed integer overflow occurs, instead of defining wrapping behavior. This direction was motivated by:
</p><ul>
<li>
Performance concerns, whereby defining the behavior prevents optimizers from assuming that overflow never occurs;
</li><li>
Implementation leeway for tools such as sanitizers;
</li><li>
Data from Google suggesting that over 90% of all overflow is a bug, and defining wrapping behavior would not have solved the bug.</li></ul>
</blockquote>

<p>
Again, performance concerns rank first.
I find the third item in the list particularly telling.
I’ve known C/C++ compiler authors who got excited about a 0.1% performance improvement,
and incredibly excited about 1%.
Yet here we have an idea that would change 10% of affected programs from incorrect to correct,
and it is rejected, because performance is more important.

</p><p>
The argument about sanitizers is more nuanced.
Leaving a behavior undefined allows any implementation at all, including reporting the
behavior at runtime and stopping the program.
True, the widespread use of undefined behavior enables sanitizers like ThreadSanitizer, MemorySanitizer, and UBSan,
but so would defining the behavior as “either this specific behavior, or a sanitizer report.”
If you believed correctness was job one, you could
define overflow to wrap, fixing the 10% of programs outright
and making the 90% behave at least more predictably,
and then at the same time define that overflow is still
a bug that can be reported by sanitizers.
You might object that requiring wrapping in the absence of a sanitizer
would hurt performance, and that’s fine: it’s just more evidence that
performance trumps correctness.

</p><p>
One thing I find surprising, though, is that correctness gets ignored even
when it clearly doesn’t hurt performance.
It would certainly not hurt performance to emit a compiler warning
about deleting the <code>if</code> statement testing for signed overflow,
or about optimizing away the possible null pointer dereference in <code>Do()</code>.
Yet I could find no way to make compilers report either one; certainly not <code>-Wall</code>.

</p><p>
The explanatory shift from non-portable to optimizable also seems revealing.
As far as I can tell, C89 did not use performance as a justification for any of
its undefined behaviors.
They were non-portabilities, like signed overflow and null pointer dereferences,
or they were outright bugs, like use-after-free.
But now experts like Chris Lattner and Hans Boehm point to optimization potential,
not portability, as justification for undefined behaviors.
I conclude that the rationales really have shifted from the mid-1980s to today:
an idea that meant to capture non-portability has been preserved for performance,
trumping concerns like correctness and debuggability.

</p><p>
Occasionally in Go we have <a href="https://go.dev/blog/compat#input">changed library functions to remove surprising behavior</a>,
It’s always a difficult decision, but we are willing
to break existing programs depending on a mistake
if correcting the mistake fixes a much larger number of programs.
I find it striking that the C and C++ standards committees are
willing in some cases to break existing programs if doing so
merely <i>speeds up</i> a large number of programs.
This is exactly what happened with the infinite loops.

</p><p>
I find the infinite loop example telling for a second reason:
it shows clearly the escalation from non-portable to optimizable.
In fact, it would appear that if you want to break C++ programs in
service of optimization, one possible approach is to just do that in a
compiler and wait for the standards committee to notice.
The de facto non-portability of whatever programs you have broken
can then serve as justification for undefining their behavior,
leading to a future version of the standard in which your optimization is legal.
In the process, programmers have been handed yet another footgun
to try to avoid setting off.

</p><p>
(A common counterargument is that the standards committee cannot
force existing implementations to change their compilers.
This doesn’t hold up to scrutiny: every new feature that gets added
is the standards committee forcing existing implementations
to change their compilers.)

</p><p>
I am not claiming that anything should change about C and C++.
I just want people to recognize that the current versions of these
sacrifice correctness for performance.
To some extent, all languages do this: there is almost always a tradeoff
between performance and slower, safer implementations.
Go has data races in part for performance reasons:
we could have done everything by message copying
or with a single global lock instead, but the performance wins of
shared memory were too large to pass up.
For C and C++, though, it seems no performance win is too small
to trade against correctness.

</p><p>
As a programmer, you have a tradeoff to make too,
and the language standards make it clear which side they are on.
In some contexts, performance is the dominant priority and
nothing else matters quite as much.
If so, C or C++ may be the right tool for you.
But in most contexts, the balance flips the other way.
If programmer productivity, debuggability, reproducible bugs,
and overall correctness and understandability
are more important than squeezing every last little bit of performance,
then C and C++ are not the right tools for you.
I say this with some regret, as I spent many years happily writing C programs.

</p><p>
I have tried to avoid exaggerated, hyperbolic language in this post,
instead laying out the tradeoff and the preferences revealed
by the decisions being made.
John Regehr wrote a less restrained series of posts about undefined behavior
a decade ago, and in <a href="https://blog.regehr.org/archives/226">one of them</a> he concluded:</p><blockquote>

<p>
It is basically evil to make certain program actions wrong, but to not give developers any way to tell whether or not their code performs these actions and, if so, where. One of C’s design points was “trust the programmer.” This is fine, but there’s trust and then there’s trust. I mean, I trust my 5 year old but I still don’t let him cross a busy street by himself. Creating a large piece of safety-critical or security-critical code in C or C++ is the programming equivalent of crossing an 8-lane freeway blindfolded.</p></blockquote>

<p>
To be fair to C and C++,
if you set yourself the goal of crossing an 8-lane freeway blindfolded,
it does make sense to focus on doing it as fast as you possibly can.
      </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Learning async Rust with entirely too many web servers (203 pts)]]></title>
            <link>https://ibraheem.ca/posts/too-many-web-servers/</link>
            <guid>37176960</guid>
            <pubDate>Fri, 18 Aug 2023 15:28:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ibraheem.ca/posts/too-many-web-servers/">https://ibraheem.ca/posts/too-many-web-servers/</a>, See on <a href="https://news.ycombinator.com/item?id=37176960">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
    
    <p>I've found that one of the best ways to understand a new concept is to start from the very beginning. Start from a place where it doesn't exist yet and recreate it yourself, learning in the process not just how it works, but <em>why</em> it was designed the way it was.</p>
<p>This isn't a practical guide to async, but hopefully some of the background knowledge it covers will help you <em>think</em> about asynchronous problems, or at least fulfill your curiosity, without boring you with too many details.</p>
<p>..It's still really long.</p>
<blockquote>
<h2>Contents</h2>
<ul>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#a-simple-web-server">A Simple Web Server</a></li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#a-multithreaded-server">A Multithreaded Server</a></li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#a-non-blocking-server">A Non-Blocking Server</a></li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#a-multiplexed-server">A Multiplexed Server</a></li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#futures">Futures</a>
<ul>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#a-reactor">A Reactor</a></li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#scheduling-tasks">Scheduling Tasks</a></li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#an-async-server">An Async Server</a></li>
</ul>
</li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#a-functional-server">A Functional Server</a></li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#a-graceful-server">A Graceful Server</a></li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#back-to-reality">Back To Reality</a>
<ul>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#pinning">Pinning</a></li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#async-await">async/await</a></li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#a-tokio-server">A Tokio Server</a></li>
</ul>
</li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#afterword">Afterword</a></li>
</ul>
</blockquote>
<h2 id="a-simple-web-server"><a href="#a-simple-web-server" aria-label="Anchor link for: a-simple-web-server">A Simple Web Server</a></h2>
<p>We'll start our journey into async programming with the simplest of web servers, using nothing but the standard networking types in <code>std::net</code>. Our server just needs to accept HTTP requests and reply with a basic response. We'll ignore most of the HTTP specification, or write any useful application code, for the entirety of this post, focusing instead on the basic flow of the server.</p>
<p>HTTP is a text-based protocol built on top of TCP, so to start we have to accept TCP connections. We can do that by creating a <code>TcpListener</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>std::net::TcpListener;
</span><span>
</span><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).</span><span>unwrap</span><span>();
</span><span>}
</span></code></pre>
<p>And listening for incoming connections, handling them one by one.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>std::net::{TcpListener, TcpStream};
</span><span>use </span><span>std::io;
</span><span>
</span><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>// ...
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>let </span><span>(connection, </span><span>_</span><span>) </span><span>=</span><span> listener.</span><span>accept</span><span>().</span><span>unwrap</span><span>();
</span><span>
</span><span>        </span><span>if </span><span>let Err</span><span>(e) </span><span>= </span><span>handle_connection</span><span>(connection) {
</span><span>            println!(</span><span>"failed to handle connection: </span><span>{e}</span><span>"</span><span>)
</span><span>        }
</span><span>    }
</span><span>}
</span><span>
</span><span>fn </span><span>handle_connection</span><span>(</span><span>connection</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>TCP connections are represented by the <code>TcpStream</code> type, a bidirectional stream of data between us and the client. It implements the <code>Read</code> and <code>Write</code> traits, abstracting away the internal details of TCP and allowing us to read or write plain old bytes.</p>
<p>As a server, we need to receive the HTTP request. We'll initialize a small buffer to hold the request.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>handle_connection</span><span>(</span><span>connection</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>let </span><span>mut</span><span> request </span><span>= </span><span>[</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>];
</span><span>    </span><span>// ...
</span><span>
</span><span>    </span><span>Ok</span><span>(())
</span><span>}
</span></code></pre>
<p>And then call <code>read</code> on the connection, reading the request bytes into the buffer. <code>read</code> will fill the buffer with an arbitrary number of bytes, not necessarily the entire request at once. So we have to keep track of the total number of bytes we've read and call it in a loop, reading the rest of the request into the unfilled part of the buffer.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>std::io::Read;
</span><span>
</span><span>fn </span><span>handle_connection</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>let </span><span>mut</span><span> read </span><span>= </span><span>0</span><span>;
</span><span>    </span><span>let </span><span>mut</span><span> request </span><span>= </span><span>[</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>];
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>// try reading from the stream
</span><span>        </span><span>let</span><span> num_bytes </span><span>=</span><span> connection.</span><span>read</span><span>(</span><span>&amp;mut</span><span> request[read</span><span>..</span><span>])</span><span>?</span><span>;
</span><span>
</span><span>        </span><span>// keep track of how many bytes we've read
</span><span>        read </span><span>+=</span><span> num_bytes;
</span><span>    }
</span><span>
</span><span>    </span><span>Ok</span><span>(())
</span><span>}
</span></code></pre>
<p>Finally, we have to check for the byte sequence <code>\r\n\r\n</code>, which indicates the end of the request.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>handle_connection</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>let </span><span>mut</span><span> read </span><span>= </span><span>0</span><span>;
</span><span>    </span><span>let </span><span>mut</span><span> request </span><span>= </span><span>[</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>];
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>// try reading from the stream
</span><span>        </span><span>let</span><span> num_bytes </span><span>=</span><span> connection.</span><span>read</span><span>(</span><span>&amp;mut</span><span> request[read</span><span>..</span><span>])</span><span>?</span><span>;
</span><span>
</span><span>        </span><span>// keep track of how many bytes we've read
</span><span>        read </span><span>+=</span><span> num_bytes;
</span><span>
</span><span>        </span><span>// have we reached the end of the request?
</span><span>        </span><span>if</span><span> request.</span><span>get</span><span>(read </span><span>- </span><span>4</span><span>..</span><span>read) </span><span>== </span><span>Some</span><span>(</span><span>b</span><span>"</span><span>\r\n\r\n</span><span>"</span><span>) {
</span><span>            </span><span>break</span><span>;
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span>Ok</span><span>(())
</span><span>}
</span></code></pre>
<p>It's also possible for <code>read</code> to return zero bytes, which can happen when the client disconnects. If the client disconnects without sending an entire request we can simply return and move on to the next connection.</p>
<p>Again, don't worry too much about sticking to the HTTP specification, we're just trying to get something working.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>handle_connection</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>let </span><span>mut</span><span> read </span><span>= </span><span>0</span><span>;
</span><span>    </span><span>let </span><span>mut</span><span> request </span><span>= </span><span>[</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>];
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>// try reading from the stream
</span><span>        </span><span>let</span><span> num_bytes </span><span>=</span><span> connection.</span><span>read</span><span>(</span><span>&amp;mut</span><span> request[read</span><span>..</span><span>])</span><span>?</span><span>;
</span><span>
</span><span>        </span><span>// the client disconnected
</span><span>        </span><span>if</span><span> num_bytes </span><span>== </span><span>0 </span><span>{ </span><span>// 👈
</span><span>            println!(</span><span>"client disconnected unexpectedly"</span><span>);
</span><span>            </span><span>return </span><span>Ok</span><span>(());
</span><span>        }
</span><span>
</span><span>        </span><span>// keep track of how many bytes we've read
</span><span>        read </span><span>+=</span><span> num_bytes;
</span><span>
</span><span>        </span><span>// have we reached the end of the request?
</span><span>        </span><span>if</span><span> request.</span><span>get</span><span>(read </span><span>- </span><span>4</span><span>..</span><span>read) </span><span>== </span><span>Some</span><span>(</span><span>b</span><span>"</span><span>\r\n\r\n</span><span>"</span><span>) {
</span><span>            </span><span>break</span><span>;
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span>Ok</span><span>(())
</span><span>}
</span></code></pre>
<p>Once we've read in the entire request, we can convert it to a string and log it to the console.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>handle_connection</span><span>(</span><span>stream</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>let </span><span>mut</span><span> read </span><span>= </span><span>0</span><span>;
</span><span>    </span><span>let </span><span>mut</span><span> request </span><span>= </span><span>[</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>];
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>// ...
</span><span>    }
</span><span>
</span><span>    </span><span>let</span><span> request </span><span>= </span><span>String</span><span>::from_utf8_lossy(</span><span>&amp;</span><span>request[</span><span>..</span><span>read]);
</span><span>    println!(</span><span>"</span><span>{request}</span><span>"</span><span>);
</span><span>
</span><span>    </span><span>Ok</span><span>(())
</span><span>}
</span></code></pre>
<p>Now we have to write our response.</p>
<p>Just like <code>read</code>, a call to <code>write</code> may not write the entire buffer at once. We need a second loop to ensure the entire response is written to the client, with each call to <code>write</code> continuing from where the previous left off.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>std::io::Write;
</span><span>
</span><span>fn </span><span>handle_connection</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>// ...
</span><span>
</span><span>    </span><span>// "Hello World!" in HTTP
</span><span>    </span><span>let</span><span> response </span><span>= </span><span>concat!(
</span><span>        </span><span>"HTTP/1.1 200 OK</span><span>\r\n</span><span>"</span><span>,
</span><span>        </span><span>"Content-Length: 12</span><span>\n</span><span>"</span><span>,
</span><span>        </span><span>"Connection: close</span><span>\r\n\r\n</span><span>"</span><span>,
</span><span>        </span><span>"Hello world!"
</span><span>    );
</span><span>
</span><span>    </span><span>let </span><span>mut</span><span> written </span><span>= </span><span>0</span><span>;
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>// write the remaining response bytes
</span><span>        </span><span>let</span><span> num_bytes </span><span>=</span><span> connection.</span><span>write</span><span>(response[written</span><span>..</span><span>].</span><span>as_bytes</span><span>())</span><span>?</span><span>;
</span><span>
</span><span>        </span><span>// the client disconnected
</span><span>        </span><span>if</span><span> num_bytes </span><span>== </span><span>0 </span><span>{
</span><span>            println!(</span><span>"client disconnected unexpectedly"</span><span>);
</span><span>            </span><span>return </span><span>Ok</span><span>(());
</span><span>        }
</span><span>
</span><span>        written </span><span>+=</span><span> num_bytes;
</span><span>
</span><span>        </span><span>// have we written the whole response yet?
</span><span>        </span><span>if</span><span> written </span><span>==</span><span> response.</span><span>len</span><span>() {
</span><span>            </span><span>break</span><span>;
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>And finally, we'll call <code>flush</code> to ensure that the response is written to the client <sup><a href="#0">1</a></sup>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>
</span><span>fn </span><span>handle_connection</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>let </span><span>mut</span><span> written </span><span>= </span><span>0</span><span>;
</span><span>    
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>// ...
</span><span>    }
</span><span>
</span><span>    </span><span>// flush the response
</span><span>    connection.</span><span>flush</span><span>()
</span><span>}
</span></code></pre>
<p>There you go, a working HTTP server!</p>
<pre data-lang="sh"><code data-lang="sh"><span>$ curl localhost:3000
</span><span># =&gt; Hello world!
</span></code></pre>
<h2 id="a-multithreaded-server"><a href="#a-multithreaded-server" aria-label="Anchor link for: a-multithreaded-server">A Multithreaded Server</a></h2>
<p>Alright, so our server <em>works</em>. But there's a problem.</p>
<p>Take a look at our accept loop.</p>
<pre data-lang="rust"><code data-lang="rust"><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>loop </span><span>{
</span><span>    </span><span>let </span><span>(connection, </span><span>_</span><span>) </span><span>=</span><span> listener.</span><span>accept</span><span>().</span><span>unwrap</span><span>();
</span><span>
</span><span>    </span><span>if </span><span>let Err</span><span>(e) </span><span>= </span><span>handle_connection</span><span>(connection) {
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span></code></pre>
<p>See the problem?</p>
<p>Our server can only serve a single request at a time.</p>
<p>Reading and writing from/to a network connection isn't instantaneous, there's a lot of infrastructure between us and the user. What would happen if two users made a request to our server at the same time, or ten, or ten thousand? Obviously this isn't going to scale, so what do we do?</p>
<p>We have a couple of options, but by far the simplest one is to spawn some threads. Spawn a thread for each request and our server becomes infinitely faster, right?</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>// ...
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>let </span><span>(connection, </span><span>_</span><span>) </span><span>=</span><span> listener.</span><span>accept</span><span>().</span><span>unwrap</span><span>();
</span><span>
</span><span>        </span><span>// spawn a thread to handle each connection
</span><span>        std::thread::spawn(|| {
</span><span>            </span><span>if </span><span>let Err</span><span>(e) </span><span>= </span><span>handle_connection</span><span>(connection) {
</span><span>                </span><span>// ...
</span><span>            }
</span><span>        });
</span><span>    }
</span><span>}
</span></code></pre>
<p>In fact, it probably does! Maybe not infinitely, but with each request being handled in a separate thread, the potential throughput of our server increases significantly.</p>
<p>How exactly does that work?</p>
<p>On Linux, as well as most other modern operation systems, every program is run as a separate process. While it seems like every active program is run at the same time, it's only physically possible for a single CPU core to execute a single task at a time, or possibly two with hyperthreading. To allow all programs to make progress, the kernel constantly switches between them, pausing one program to give another a chance to run. These <em>context switches</em> happen on the order of milliseconds, providing the illusion of paralellism.</p>
<p>The kernel scheduler can take advantage of multiple cores by distributing its workload across them. Each core manages a subset of processes <sup><a href="#1">2</a></sup>, meaning that some programs do in fact get to run in parallel.</p>
<pre><code><span> cpu1 cpu2 cpu3 cpu4
</span><span>|----|----|----|----|
</span><span>| p1 | p3 | p5 | p7 |
</span><span>|    |____|    |____|
</span><span>|    |    |____|    |
</span><span>|____| p4 |    | p8 |
</span><span>|    |    | p6 |____|
</span><span>| p2 |____|    |    |
</span><span>|    | p3 |    | p7 |
</span><span>|    |    |    |    |
</span></code></pre>
<p>This type of scheduling is known as <em>preemptive multitasking</em>. The kernel decides when one process has been running for too long and preempts it, switching to someone else.</p>
<p>Processes work well for distinct programs because the kernel ensures that separate processes aren't allowed to access each other's memory. However, this makes context switching more expensive because the kernel has to flush certain parts of memory before performing a context switch to ensure that memory is properly isolated <sup><a href="#2">3</a></sup>.</p>
<p><em>Threads</em> are similar to processes <sup><a href="#3">4</a></sup> except they are allowed to share memory with other threads in the parent process, which is what allows us to share state between threads within the same program. Scheduling works much the same way.</p>
<p>The key insight regarding thread-per-request is that our server is <em>I/O bound</em>. Most of the time inside <code>handle_connection</code> is not spent doing compute work, it's spent waiting to send or receive some data across the network. Functions like <code>read</code>, <code>write</code>, and <code>flush</code> perform <em>blocking</em> I/O. We submit an I/O request, yielding control to the kernel, and it returns control to us when the operation completes. In the meantime, the kernel can execute other runnable threads, which is exactly what we want!</p>
<p>In general, most of the time it takes to serve a web request is spent waiting for other tasks to complete, like database queries or HTTP requests. Multithreading works great because we can utilize that time to handle other requests.</p>
<h2 id="a-non-blocking-server"><a href="#a-non-blocking-server" aria-label="Anchor link for: a-non-blocking-server">A Non-Blocking Server</a></h2>
<p>It seems like threads do exactly what we need, and they're easy to use, so why not stop here?</p>
<p>You may have heard that threads are too heavyweight and context switching is very expensive. Nowadays, that's not really true. Modern servers can manage tens of thousands of threads without breaking a sweat.</p>
<p>The issue is that blocking I/O yields complete control of our program to the kernel until the requested operation completes. We have no say in when we get to run again. This is problematic because it makes it very difficult to model two operations: <em>cancellation</em>, and <em>selection</em>.</p>
<p>Imagine we wanted to implement graceful shutdown for our server. When someone hits ctrl+c, instead of killing the program abruptly, we should stop accepting new connections but still wait for any active requests to complete. Any requests that take more than thirty seconds to finish are killed as the server exits.</p>
<p>This poses a problem when dealing with blocking I/O. Our accept loop blocks until the next connection comes in. We can check for the ctrl+c signal before or after a new connection comes in, but if the signal is triggered <em>during</em> a call to <code>accept</code>, we have no choice but to wait until the next connection is accepted. The kernel has complete control over the execution of our program.</p>
<pre data-lang="rust"><code data-lang="rust"><span>loop </span><span>{
</span><span>    </span><span>// check before we call `accept`
</span><span>    </span><span>if </span><span>got_ctrl_c</span><span>() {
</span><span>        </span><span>break</span><span>;
</span><span>    }
</span><span>
</span><span>    </span><span>// **what if ctrl+c happens here?**
</span><span>    </span><span>let </span><span>(connection, </span><span>_</span><span>) </span><span>=</span><span> listener.</span><span>accept</span><span>().</span><span>unwrap</span><span>();
</span><span>
</span><span>    </span><span>// this won't be checked until *after* we accept a new connection
</span><span>    </span><span>if </span><span>got_ctrl_c</span><span>() {
</span><span>        </span><span>break</span><span>;
</span><span>    }
</span><span>
</span><span>    std::thread::spawn(|| </span><span>/* ... */</span><span>);
</span><span>}
</span></code></pre>
<p>What we want is to listen for both the incoming connection and the ctrl+c signal, <em>at the same time</em>. Like a <code>match</code> statement, but for I/O operations.</p>
<pre data-lang="rust"><code data-lang="rust"><span>loop </span><span>{
</span><span>    </span><span>// if only...
</span><span>    </span><span>match </span><span>{
</span><span>        </span><span>ctrl_c</span><span>() </span><span>=&gt; </span><span>{
</span><span>            </span><span>break</span><span>;
</span><span>        },
</span><span>        </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=</span><span> listener.</span><span>accept</span><span>() </span><span>=&gt; </span><span>{
</span><span>            std::thread::spawn(|| </span><span>...</span><span>);
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>It turns out that this is almost impossible to do with blocking I/O. The best thing we can do is open up a TCP connection to <em>ourselves</em> to free up the loop, which feels extremely hacky to say the least.</p>
<p>And what about timing out long running requests after thirty seconds? We could set a flag that tells threads to stop, but how often would they check it? We again run into the problem that we lose control of our program during I/O and have no choice but to wait until it completes. There's really no good way of force cancelling a thread.</p>
<p>Problems like these are where threads and blocking I/O fall apart. Expressing event-based logic becomes very difficult when the kernel holds so much control over the execution of our program.</p>
<p>But what if there was a way to perform I/O <em>without</em> yielding to the kernel?</p>
<hr>
<p>It turns out there <em>is</em> a second way to perform I/O, known as non-blocking I/O. As the name suggests, a non-blocking operation will never block the calling thread. Instead it returns immediately, returning an error if the given resource was not available.</p>
<p>We can switch to using non-blocking I/O by putting our TCP listener and streams into <em>non-blocking mode</em>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).</span><span>unwrap</span><span>();
</span><span>listener.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>loop </span><span>{
</span><span>    </span><span>let </span><span>(connection, </span><span>_</span><span>) </span><span>=</span><span> listener.</span><span>accept</span><span>().</span><span>unwrap</span><span>();
</span><span>    connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>Non-blocking I/O works a little differently. If the I/O request cannot be fulfilled immediately, instead of blocking, the kernel simply returns the <code>WouldBlock</code> error code. Despite being represented as an error, <code>WouldBlock</code> isn't <em>really</em> an error condition. It just means the operation could not be performed immediately, giving us the chance to decide what to do instead of blocking.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>std::io;
</span><span>
</span><span>// ...
</span><span>listener.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>loop </span><span>{
</span><span>    </span><span>let</span><span> connection </span><span>= match</span><span> listener.</span><span>accept</span><span>() {
</span><span>        </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt;</span><span> connection,
</span><span>        </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{
</span><span>            </span><span>// the operation was not performed
</span><span>            </span><span>// ...
</span><span>        }
</span><span>        </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>    };
</span><span>
</span><span>    connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>Imagine we called <code>accept</code> when there were no incoming connections. With blocking I/O, the <code>accept</code> <em>would</em> have blocked until a new connection came in. Now instead of yielding control to the kernel, <code>WouldBlock</code> puts the control back in our hands.</p>
<p>Our I/O doesn't block, great! But what do we actually <em>do</em> when something isn't ready?</p>
<p><code>WouldBlock</code> is a temporary state, meaning at some point in the future the socket should become ready to read or write from. So technically, we could just spin until the socket becomes ready.</p>
<pre data-lang="rust"><code data-lang="rust"><span>loop </span><span>{
</span><span>    </span><span>let</span><span> connection </span><span>= match</span><span> listener.</span><span>accept</span><span>() {
</span><span>        </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt;</span><span> connection,
</span><span>        </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{
</span><span>            </span><span>continue</span><span>; </span><span>// 👈
</span><span>        }
</span><span>        </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>    };
</span><span>}
</span></code></pre>
<p>But spinning is really just worse than blocking directly. When we block for I/O, the OS gives other threads a chance to run. So what we really need is to build some sort of a scheduler for all of our tasks, doing what the operating system used to handle for us.</p>
<p>Let's walk things through from the beginning again.</p>
<p>We create a TCP listener.</p>
<pre data-lang="rust"><code data-lang="rust"><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).</span><span>unwrap</span><span>();
</span></code></pre>
<p>Set it to non-blocking mode.</p>
<pre data-lang="rust"><code data-lang="rust"><span>listener.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span></code></pre>
<p>And then start our main loop. The first thing we'll do is try accepting a new TCP connection.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// ...
</span><span>
</span><span>loop </span><span>{
</span><span>    </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>        </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>            connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>            </span><span>// ...
</span><span>        },
</span><span>        </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{}
</span><span>        </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>    }
</span><span>}
</span></code></pre>
<p>Now, we can't just continue serving that connection directly and forget about everyone else. Instead, we have to keep track of all our active connections.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// ...
</span><span>
</span><span>let </span><span>mut</span><span> connections </span><span>= </span><span>Vec</span><span>::new(); </span><span>// 👈
</span><span>
</span><span>loop </span><span>{
</span><span>    </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>        </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>            connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>            connections.</span><span>push</span><span>(connection); </span><span>// 👈
</span><span>        },
</span><span>        </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{}
</span><span>        </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>    }
</span><span>}
</span></code></pre>
<p>But we can't keep accepting connections forever. We don't have the luxury of OS scheduling anymore, so we need to handle running a little bit of everything in every iteration of the main loop. After trying to <code>accept</code> once, we need to deal with the active connections.</p>
<p>For each connection, we have to perform whatever operation is needed to move the processing of the request forward, whether that means reading the request, or writing the response.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// ...
</span><span>loop </span><span>{
</span><span>    </span><span>// try accepting a new connection
</span><span>    </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>        </span><span>// ...
</span><span>    }
</span><span>
</span><span>    </span><span>// attempt to make progress on active connections 
</span><span>    </span><span>for</span><span> connection </span><span>in</span><span> connections.</span><span>iter_mut</span><span>() {
</span><span>        </span><span>// 🤔
</span><span>    }
</span><span>}
</span></code></pre>
<p>Uhh...</p>
<p>If you remember the <code>handle_connection</code> function from before:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>handle_connection</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>let </span><span>mut</span><span> request </span><span>= </span><span>[</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>];
</span><span>    </span><span>let </span><span>mut</span><span> read </span><span>= </span><span>0</span><span>;
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>let</span><span> num_bytes </span><span>=</span><span> connection.</span><span>read</span><span>(</span><span>&amp;mut</span><span> request[read</span><span>..</span><span>])</span><span>?</span><span>;  </span><span>// 👈
</span><span>        </span><span>// ...
</span><span>    }
</span><span>
</span><span>    </span><span>let</span><span> request </span><span>= </span><span>String</span><span>::from_utf8_lossy(</span><span>&amp;</span><span>request[</span><span>..</span><span>read]);
</span><span>    println!(</span><span>"</span><span>{request}</span><span>"</span><span>);
</span><span>
</span><span>    </span><span>let</span><span> response </span><span>= </span><span>/* ... */</span><span>;
</span><span>    </span><span>let </span><span>mut</span><span> written </span><span>= </span><span>0</span><span>;
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>let</span><span> num_bytes </span><span>=</span><span> connection.</span><span>write</span><span>(</span><span>&amp;</span><span>response[written</span><span>..</span><span>])</span><span>?</span><span>; </span><span>// 👈
</span><span>
</span><span>        </span><span>// ...
</span><span>    }
</span><span>
</span><span>    connection.</span><span>flush</span><span>().</span><span>unwrap</span><span>(); </span><span>// 👈
</span><span>}
</span></code></pre>
<p>We perform three different I/O operations, <code>read</code>, <code>write</code>, and <code>flush</code>. With blocking I/O we could write our code sequentially, but now we have to deal with the fact that at any point when performing I/O, we could face <code>WouldBlock</code> and won't be able to make progress.</p>
<p>We can't simply drop everything and move on to the next active connection, we need to keep track of its current <em>state</em> in order to resume from the correct point when we come back.</p>
<p>We can represent the three possible states of <code>handle_connection</code> in an enum.</p>
<pre data-lang="rust"><code data-lang="rust"><span>enum </span><span>ConnectionState {
</span><span>    Read,
</span><span>    Write,
</span><span>    Flush
</span><span>}
</span></code></pre>
<p>Remember, we don't need separate states for things like converting the request to a string, we only need states for places where we might encounter <code>WouldBlock</code>.</p>
<p>The <code>Read</code> and <code>Write</code> states also need to hold on to some local state for the request/response buffers and the number of bytes that have already been read/written. These used to be local variables in our function, but now we need them to persist across iterations of our main loop.</p>
<pre data-lang="rust"><code data-lang="rust"><span>enum </span><span>ConnectionState {
</span><span>    Read {
</span><span>        request: [</span><span>u8</span><span>; </span><span>1024</span><span>],
</span><span>        read: </span><span>usize
</span><span>    },
</span><span>    Write {
</span><span>        response: </span><span>&amp;'static </span><span>[</span><span>u8</span><span>],
</span><span>        written: </span><span>usize</span><span>,
</span><span>    },
</span><span>    Flush,
</span><span>}
</span></code></pre>
<p>Connections start in the <code>Read</code> state with an empty buffer and zero bytes read, the same variables we used to initialize at the very start of <code>handle_connection</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// ...
</span><span>
</span><span>let </span><span>mut</span><span> connections </span><span>= </span><span>Vec</span><span>::new();
</span><span>
</span><span>loop </span><span>{
</span><span>    </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>        </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>            connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>
</span><span>            </span><span>let</span><span> state </span><span>= </span><span>ConnectionState::Read { </span><span>// 👈
</span><span>                request: [</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>],
</span><span>                read: </span><span>0</span><span>,
</span><span>            };
</span><span>
</span><span>            connections.</span><span>push</span><span>((connection, state));
</span><span>        },
</span><span>        </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{}
</span><span>        </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>    }
</span><span>}
</span></code></pre>
<p>Now we can try to drive each connection forward from its current state.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// ...
</span><span>loop </span><span>{
</span><span>    </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>        </span><span>// ...
</span><span>    }
</span><span>
</span><span>    </span><span>for </span><span>(connection, state) </span><span>in</span><span> connections.</span><span>iter_mut</span><span>() {
</span><span>        </span><span>if </span><span>let </span><span>ConnectionState::Read { request, read } </span><span>=</span><span> state {
</span><span>            </span><span>// ...
</span><span>        }
</span><span>
</span><span>        </span><span>if </span><span>let </span><span>ConnectionState::Write { response, written } </span><span>=</span><span> state {
</span><span>            </span><span>// ...
</span><span>        }
</span><span>
</span><span>        </span><span>if </span><span>let </span><span>ConnectionState::Flush </span><span>=</span><span> state {
</span><span>            </span><span>// ...
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>If the connection is still in the read state, we can continue reading the request same as we did before. The only difference is that when we receive <code>WouldBlock</code>, we have to move on to the next connection.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// ...
</span><span>
</span><span>'next: </span><span>for </span><span>(connection, state) </span><span>in</span><span> connections.</span><span>iter_mut</span><span>() {
</span><span>    </span><span>if </span><span>let </span><span>ConnectionState::Read { request, read } </span><span>=</span><span> state {
</span><span>        </span><span>loop </span><span>{
</span><span>            </span><span>// try reading from the stream
</span><span>            </span><span>match</span><span> connection.</span><span>read</span><span>(</span><span>&amp;mut</span><span> request[</span><span>*</span><span>read</span><span>..</span><span>]) {
</span><span>                </span><span>Ok</span><span>(n) </span><span>=&gt; </span><span>{
</span><span>                    </span><span>// keep track of how many bytes we've read
</span><span>                    </span><span>*</span><span>read </span><span>+=</span><span> n
</span><span>                }
</span><span>                </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{
</span><span>                    </span><span>// not ready yet, move on to the next connection
</span><span>                    </span><span>continue 'next</span><span>; </span><span>// 👈
</span><span>                }
</span><span>                </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>            }
</span><span>
</span><span>            </span><span>// did we reach the end of the request?
</span><span>            </span><span>if</span><span> request.</span><span>get</span><span>(</span><span>*</span><span>read </span><span>- </span><span>4</span><span>..*</span><span>read) </span><span>== </span><span>Some</span><span>(</span><span>b</span><span>"</span><span>\r\n\r\n</span><span>"</span><span>) {
</span><span>                </span><span>break</span><span>;
</span><span>            }
</span><span>        }
</span><span>
</span><span>        </span><span>// we're done, print the request
</span><span>        </span><span>let</span><span> request </span><span>= </span><span>String</span><span>::from_utf8_lossy(</span><span>&amp;</span><span>request[</span><span>..*</span><span>read]);
</span><span>        println!(</span><span>"</span><span>{request}</span><span>"</span><span>);
</span><span>    }
</span><span>
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>We also have to deal with the case where we read zero bytes. Before we could simply return from the connection handler and the state would be cleaned up for us, but now we have to remove the connection ourselves. Because we're currently iterating through the connections list, we'll store a separate list of indices to remove after we finish.</p>
<pre data-lang="rust"><code data-lang="rust"><span>let </span><span>mut</span><span> completed </span><span>= </span><span>Vec</span><span>::new(); </span><span>// 👈
</span><span>
</span><span>'next: </span><span>for </span><span>(i, (connection, state)) </span><span>in</span><span> connections.</span><span>iter_mut</span><span>().</span><span>enumerate</span><span>() {
</span><span>    </span><span>if </span><span>let </span><span>ConnectionState::Read { request, read } </span><span>=</span><span> state {
</span><span>        </span><span>loop </span><span>{
</span><span>            </span><span>// try reading from the stream
</span><span>            </span><span>match</span><span> connection.</span><span>read</span><span>(</span><span>&amp;mut</span><span> request[</span><span>*</span><span>read</span><span>..</span><span>]) {
</span><span>                </span><span>Ok</span><span>(</span><span>0</span><span>) </span><span>=&gt; </span><span>{
</span><span>                    println!(</span><span>"client disconnected unexpectedly"</span><span>);
</span><span>                    completed.</span><span>push</span><span>(i); </span><span>// 👈
</span><span>                    </span><span>continue 'next</span><span>;
</span><span>                }
</span><span>                </span><span>Ok</span><span>(n) </span><span>=&gt; *</span><span>read </span><span>+=</span><span> n,
</span><span>                </span><span>// not ready yet, move on to the next connection
</span><span>                </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; continue 'next</span><span>,
</span><span>                </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>            }
</span><span>
</span><span>            </span><span>// ...
</span><span>        }
</span><span>
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span><span>
</span><span>// iterate in reverse order to preserve the indices
</span><span>for</span><span> i </span><span>in</span><span> completed.</span><span>into_iter</span><span>().</span><span>rev</span><span>() {
</span><span>    connections.</span><span>remove</span><span>(i); </span><span>// 👈
</span><span>}
</span></code></pre>
<p>Once we finish reading the request, we have to transition into the <code>Write</code> state and attempt to write the response. The control flow around writing the response is very similar to reading, transitioning to the <code>Flush</code> state once we finish.</p>
<pre data-lang="rust"><code data-lang="rust"><span>if </span><span>let </span><span>ConnectionState::Read { request, read } </span><span>=</span><span> state {
</span><span>    </span><span>// ...
</span><span>
</span><span>    </span><span>// move into the write state
</span><span>    </span><span>let</span><span> response </span><span>= </span><span>concat!(
</span><span>        </span><span>"HTTP/1.1 200 OK</span><span>\r\n</span><span>"</span><span>,
</span><span>        </span><span>"Content-Length: 12</span><span>\n</span><span>"</span><span>,
</span><span>        </span><span>"Connection: close</span><span>\r\n\r\n</span><span>"</span><span>,
</span><span>        </span><span>"Hello world!"
</span><span>    );
</span><span>
</span><span>    </span><span>*</span><span>state </span><span>= </span><span>ConnectionState::Write { </span><span>// 👈
</span><span>        response: response.</span><span>as_bytes</span><span>(),
</span><span>        written: </span><span>0</span><span>,
</span><span>    };
</span><span>}
</span><span>
</span><span>if </span><span>let </span><span>ConnectionState::Write { response, written } </span><span>=</span><span> state {
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>match</span><span> connection.</span><span>write</span><span>(</span><span>&amp;</span><span>response[</span><span>*</span><span>written</span><span>..</span><span>]) {
</span><span>            </span><span>Ok</span><span>(</span><span>0</span><span>) </span><span>=&gt; </span><span>{
</span><span>                println!(</span><span>"client disconnected unexpectedly"</span><span>);
</span><span>                completed.</span><span>push</span><span>(i);
</span><span>                </span><span>continue 'next</span><span>;
</span><span>            }
</span><span>            </span><span>Ok</span><span>(n) </span><span>=&gt; </span><span>{
</span><span>                </span><span>*</span><span>written </span><span>+=</span><span> n;
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{
</span><span>                </span><span>// not ready yet, move on to the next connection
</span><span>                </span><span>continue 'next</span><span>;
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        }
</span><span>
</span><span>        </span><span>// did we write the whole response yet?
</span><span>        </span><span>if *</span><span>written </span><span>==</span><span> response.</span><span>len</span><span>() {
</span><span>            </span><span>break</span><span>;
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span>// successfully wrote the response, try flushing next
</span><span>    </span><span>*</span><span>state </span><span>= </span><span>ConnectionState::Flush;
</span><span>}
</span></code></pre>
<p>And after we successfully flush the response, we can mark the connection as completed and have it removed from the list.</p>
<pre data-lang="rust"><code data-lang="rust"><span>if </span><span>let </span><span>ConnectionState::Flush </span><span>=</span><span> ConnectionState {
</span><span>    </span><span>match</span><span> connection.</span><span>flush</span><span>() {
</span><span>        </span><span>Ok</span><span>(</span><span>_</span><span>) </span><span>=&gt; </span><span>{
</span><span>            completed.</span><span>push</span><span>(i); </span><span>// 👈
</span><span>        },
</span><span>        </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{
</span><span>            </span><span>// not ready yet, move on to the next connection
</span><span>            </span><span>continue 'next</span><span>;
</span><span>        }
</span><span>        </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>    }
</span><span>}
</span></code></pre>
<p>That's it! Here's the new high-level flow of the server:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>// bind the listener
</span><span>    </span><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).</span><span>unwrap</span><span>();
</span><span>    listener.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>    </span><span>let </span><span>mut</span><span> connections </span><span>= </span><span>Vec</span><span>::new();
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>// try accepting a connection
</span><span>        </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>                </span><span>// keep track of connection state
</span><span>                </span><span>let</span><span> state </span><span>= </span><span>ConnectionState::Read {
</span><span>                    request: </span><span>Vec</span><span>::new(),
</span><span>                    read: </span><span>0</span><span>,
</span><span>                };
</span><span>
</span><span>                connections.</span><span>push</span><span>((connection, state));
</span><span>            },
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{}
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        }
</span><span>
</span><span>        </span><span>let </span><span>mut</span><span> completed </span><span>= </span><span>Vec</span><span>::new();
</span><span>
</span><span>        </span><span>// try to drive connnections forward
</span><span>        'next: </span><span>for </span><span>(i, (connection, state)) </span><span>in</span><span> connections.</span><span>iter_mut</span><span>().</span><span>enumerate</span><span>() {
</span><span>            </span><span>if </span><span>let </span><span>ConnectionState::Read { request, read } </span><span>=</span><span> state {
</span><span>                </span><span>// ...
</span><span>                </span><span>*</span><span>state </span><span>= </span><span>ConnectionState::Write { response, written };
</span><span>            }
</span><span>
</span><span>            </span><span>if </span><span>let </span><span>ConnectionState::Write { response, written } </span><span>=</span><span> state {
</span><span>                </span><span>// ...
</span><span>                </span><span>*</span><span>state </span><span>= </span><span>ConnectionState::Flush;
</span><span>            }
</span><span>
</span><span>            </span><span>if </span><span>let </span><span>ConnectionState::Flush </span><span>=</span><span> state {
</span><span>                </span><span>// ...
</span><span>            }
</span><span>        }
</span><span>
</span><span>        </span><span>// remove any connections that completed, iterating in reverse order
</span><span>        </span><span>// to preserve the indices
</span><span>        </span><span>for</span><span> i </span><span>in</span><span> completed.</span><span>into_iter</span><span>().</span><span>rev</span><span>() {
</span><span>            connections.</span><span>remove</span><span>(i);
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>Now that we have to manage scheduling ourselves.. things are a lot more complicated.</p>
<p>And now for the moment of truth...</p>
<pre data-lang="sh"><code data-lang="sh"><span>$ curl localhost:3000
</span><span># =&gt; Hello world!
</span></code></pre>
<p>It works!</p>
<h2 id="a-multiplexed-server"><a href="#a-multiplexed-server" aria-label="Anchor link for: a-multiplexed-server">A Multiplexed Server</a></h2>
<p>Our server can now handle running multiple requests concurrently on a single thread. Nothing ever blocks. If some operation <em>would</em> have blocked, it remembers the current state and moves on to run something else, much like the kernel scheduler was doing for us. However, our new design introduces two new problems.</p>
<p>The first problem is that everything runs on the main thread, utilizing only a single CPU core. We're doing the best we can to use that one core efficiently, but we're still only running a single thing at a time. With threads spread across multiple cores, we could be doing much more.</p>
<p>There's a bigger problem though.</p>
<p>Our main loop isn't actually very efficient.</p>
<p>We're making an I/O request to the kernel for <em>every</em> single active connection, <em>every</em> single iteration of the loop, to check if it's ready. A call to <code>read</code> or <code>write</code>, even if it returns <code>WouldBlock</code> and doesn't actually perform any I/O, is still a syscall. Syscalls aren't cheap. We might have 10k active connections but only 500 of them are ready. Calling <code>read</code> or <code>write</code> 10k times when only 500 of them will actually do anything is a massive waste of CPU cycles.</p>
<p>As the number of connections scales, our loop becomes less and less efficient, wasting more time doing useless work.</p>
<p>How do we fix this? With blocking I/O the kernel was able to schedule things efficiently because it <em>knows</em> when resources become ready. With non-blocking I/O, we don't know without checking. But checking is expensive.</p>
<p>What we need is an efficient way to keep track of all of our active connections, and somehow get notified when they become ready.</p>
<p>It turns out, we aren't the first to run into this problem. Every operating system provides a solution for exactly this. On Linux, it's called <code>epoll</code>.</p>
<blockquote>
<p><code>epoll(7)</code> - I/O event notification facility</p>
<p>The epoll API performs a similar task to <code>poll(2)</code>: monitoring multiple file descriptors to see if I/O is possible on any of them. The epoll API can be used either as an edge-triggered or a level-triggered interface and scales well to large numbers of watched file descriptors.</p>
</blockquote>
<p>Sounds perfect! Let's try using it.</p>
<p><code>epoll</code> is a family of Linux system calls that let us work with a set of non-blocking sockets. It isn't terribly ergonomic to use directly, so we'll be using the <code>epoll</code> crate, a thin wrapper around the C interface.</p>
<p>To start, we'll initialize an epoll instance using the <code>create</code> function.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// ```toml
</span><span>// [dependencies]
</span><span>// epoll = "4.3"
</span><span>// ```
</span><span>
</span><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>let</span><span> epoll </span><span>= </span><span>epoll::create(</span><span>false</span><span>).</span><span>unwrap</span><span>(); </span><span>// 👈
</span><span>}
</span></code></pre>
<p><code>epoll::create</code> returns a file descriptor that represents the newly created epoll instance. You can think of it as a set of file descriptors that we can add or remove from.</p>
<blockquote>
<p>In Linux/Unix, everything is considered a file. Actual files on the file system, TCP sockets, and external devices are all files that you can <code>read</code>/<code>write</code> to. A <em>file descriptor</em> is an integer that represents an open "file" in the system. We'll be working with file descriptors a lot throughout the rest of the article.</p>
</blockquote>
<p>The first file descriptor we have to add is the TCP listener. We can modify the epoll set with the <code>epoll::ctl</code> command. To add to it, we'll use the <code>EPOLL_CTL_ADD</code> flag.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>epoll::{Event, Events, ControlOptions::</span><span>*</span><span>};
</span><span>use </span><span>std::os::fd::AsRawFd;
</span><span>
</span><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).</span><span>unwrap</span><span>();
</span><span>    listener.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>    </span><span>// add the listener to epoll
</span><span>    </span><span>let</span><span> event </span><span>= </span><span>Event::new(Events::</span><span>EPOLLIN</span><span>, listener.</span><span>as_raw_fd</span><span>() </span><span>as _</span><span>);
</span><span>    epoll::ctl(epoll, </span><span>EPOLL_CTL_ADD</span><span>, listener.</span><span>as_raw_fd</span><span>(), event).</span><span>unwrap</span><span>(); </span><span>// 👈
</span><span>}
</span></code></pre>
<p>We pass in the file descriptor of the resource we are registering, the TCP listener, along with an <code>Event</code>. An event has two parts, the <em>interest</em> flag, and the data field. The interest flag gives us a way to tell epoll which I/O events we are interested in. In the case of the TCP listener, we want to be notified when new connections come <em>in</em>, so we pass the <code>EPOLLIN</code> flag.</p>
<p>The data field lets us store an ID that will uniquely identify each resource. Remember, a file descriptor is a unique integer for a given file, so we can just use that. You'll see why this is important in the next step.</p>
<p>Now for the main loop. This time, no spinning. Instead we can call <code>epoll::wait</code>.</p>
<blockquote>
<p><code>epoll_wait(2)</code>- wait for an I/O event on an epoll file descriptor</p>
<p>The <code>epoll_wait()</code> system call waits for events on the <code>epoll(7)</code> instance referred to by the file descriptor epfd. The buffer pointed to by events is used to return information from the ready list about file descriptors in the interest list that have some events available.</p>
<p>A call to <code>epoll_wait()</code> will block until either:</p>
<ul>
<li>a file descriptor delivers an event;</li>
<li>the call is interrupted by a signal handler; or</li>
<li>the timeout expires.</li>
</ul>
</blockquote>
<p><code>epoll::wait</code> is the magical part of epoll. It lets us block until <em>any</em> of the events we registered become ready, and tells us which ones did. Right now that's just until a new connection comes in, but soon we'll use this same call to block for read, write, and flush events that we were previously spinning for.</p>
<p>The fact that <code>epoll::wait</code> is "blocking" might put you off, but remember, it only blocks if there is nothing else to do, where previously we would have been spinning and making pointless syscalls. This idea of blocking on multiple operations simultaneously is known as <em>I/O multiplexing</em>.</p>
<p><code>epoll::wait</code> accepts a list of events that it will populate with information about the file descriptors that became ready. It then returns the number of events that were added.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// ...
</span><span>loop </span><span>{
</span><span>    </span><span>let </span><span>mut</span><span> events </span><span>= </span><span>[Event::new(Events::empty(), </span><span>0</span><span>); </span><span>1024</span><span>];
</span><span>    </span><span>let</span><span> timeout </span><span>= -</span><span>1</span><span>; </span><span>// block forever, until something happens
</span><span>    </span><span>let</span><span> num_events </span><span>= </span><span>epoll::wait(epoll, timeout, </span><span>&amp;mut</span><span> events).</span><span>unwrap</span><span>(); </span><span>// 👈
</span><span>
</span><span>    </span><span>for</span><span> event </span><span>in &amp;</span><span>events[</span><span>..</span><span>num_events] {
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span></code></pre>
<p>Each event contains the data field associated with the resource that became ready.</p>
<pre data-lang="rust"><code data-lang="rust"><span>for</span><span> event </span><span>in &amp;</span><span>events[</span><span>..</span><span>num_events] {
</span><span>    </span><span>let</span><span> fd </span><span>=</span><span> event.data </span><span>as </span><span>i32</span><span>;
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>Remember when we used the file descriptor for the data field? We can use it to check whether the event is for the TCP listener, which means there's an incoming connection ready to accept:</p>
<pre data-lang="rust"><code data-lang="rust"><span>for</span><span> event </span><span>in &amp;</span><span>events[</span><span>..</span><span>num_events] {
</span><span>    </span><span>let</span><span> fd </span><span>=</span><span> event.data </span><span>as </span><span>i32</span><span>;
</span><span>
</span><span>    </span><span>// is the listener ready?
</span><span>    </span><span>if</span><span> fd </span><span>==</span><span> listener.</span><span>as_raw_fd</span><span>() {
</span><span>        </span><span>// try accepting a connection
</span><span>        </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>                </span><span>// ...
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{}
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>If the call still returns <code>WouldBlock</code> for whatever reason, we can just move one and wait for the next event.</p>
<p>Now we have to register the new connection in epoll, just like we did the listener.</p>
<pre data-lang="rust"><code data-lang="rust"><span>for</span><span> event </span><span>in &amp;</span><span>events[</span><span>..</span><span>num_events] {
</span><span>    </span><span>let</span><span> fd </span><span>=</span><span> event.data </span><span>as </span><span>i32</span><span>;
</span><span>
</span><span>    </span><span>// is the listener ready?
</span><span>    </span><span>if</span><span> fd </span><span>==</span><span> listener.</span><span>as_raw_fd</span><span>() {
</span><span>        </span><span>// try accepting a connection
</span><span>        </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>                 </span><span>let</span><span> fd </span><span>=</span><span> connection.</span><span>as_raw_fd</span><span>();
</span><span>
</span><span>                 </span><span>// register the connection with epoll
</span><span>                 </span><span>let</span><span> event </span><span>= </span><span>Event::new(Events::</span><span>EPOLLIN </span><span>| </span><span>Events::</span><span>EPOLLOUT</span><span>, fd </span><span>as _</span><span>);
</span><span>                 epoll::ctl(epoll, </span><span>EPOLL_CTL_ADD</span><span>, fd, event).</span><span>unwrap</span><span>(); </span><span>// 👈
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{}
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>This time we set both <code>EPOLLIN</code> and <code>EPOLLOUT</code>, because we are interested in both read and write events, depending on the state of the connection.</p>
<p>Now that we register connections, we'll get events for both the TCP listener and individual connections. We need to store connections and their states in a way that we can look up by file descriptor.</p>
<p>Instead of a list, we can use a <code>HashMap</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>let </span><span>mut</span><span> connections </span><span>= </span><span>HashMap::new();
</span><span>
</span><span>loop </span><span>{
</span><span>    </span><span>// ...
</span><span>    'next: </span><span>for</span><span> event </span><span>in &amp;</span><span>events[</span><span>..</span><span>num_events] {
</span><span>        </span><span>let</span><span> fd </span><span>=</span><span> event.data </span><span>as </span><span>i32</span><span>;
</span><span>
</span><span>        </span><span>// is the listener ready?
</span><span>        </span><span>if</span><span> fd </span><span>==</span><span> listener.</span><span>as_raw_fd</span><span>() {
</span><span>            </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>                </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                    </span><span>// ...
</span><span>
</span><span>                    </span><span>let</span><span> state </span><span>= </span><span>ConnectionState::Read {
</span><span>                        request: [</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>],
</span><span>                        read: </span><span>0</span><span>,
</span><span>                    };
</span><span>
</span><span>                    connections.</span><span>insert</span><span>(fd, (connection, state)); </span><span>// 👈
</span><span>                }
</span><span>                </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{}
</span><span>                </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>            }
</span><span>
</span><span>            </span><span>continue 'next</span><span>;
</span><span>        }
</span><span>
</span><span>        </span><span>// otherwise, a connection must be ready
</span><span>        </span><span>let </span><span>(connection, state) </span><span>=</span><span> connections.</span><span>get_mut</span><span>(</span><span>&amp;</span><span>fd).</span><span>unwrap</span><span>(); </span><span>// 👈
</span><span>    }
</span><span>}
</span></code></pre>
<p>Once we have the ready connection and it's state, we can try to make progress the exact same way we did last time. Nothing changes about the way we read or write from the stream, the only difference is that we only ever <em>do</em> I/O when epoll tells us to.</p>
<p>Before we had to check every single connection to see if something became ready, but now <code>epoll</code> handles that for us, so we avoid any useless syscalls.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// ...
</span><span>
</span><span>// epoll told us this connection is ready
</span><span>let </span><span>(connection, state) </span><span>=</span><span> connections.</span><span>get_mut</span><span>(</span><span>&amp;</span><span>fd).</span><span>unwrap</span><span>();
</span><span>
</span><span>if </span><span>let </span><span>ConnectionState::Read { request, read } </span><span>=</span><span> state {
</span><span>    </span><span>// connection.read...
</span><span>    </span><span>*</span><span>state </span><span>= </span><span>ConnectionState::Write { response, written };
</span><span>}
</span><span>
</span><span>if </span><span>let </span><span>ConnectionState::Write { response, written } </span><span>=</span><span> state {
</span><span>    </span><span>// connection.write...
</span><span>    </span><span>*</span><span>state </span><span>= </span><span>ConnectionState::Flush;
</span><span>}
</span><span>
</span><span>if </span><span>let </span><span>ConnectionState::Flush </span><span>=</span><span> state {
</span><span>    </span><span>// connection.flush...
</span><span>}
</span></code></pre>
<p>Once we've finished reading, writing, and flushing the response, we remove the connection from our map and drop it, which automatically unregisters it from epoll.</p>
<pre data-lang="rust"><code data-lang="rust"><span>for</span><span> fd </span><span>in</span><span> completed {
</span><span>    </span><span>let </span><span>(connection, _state) </span><span>=</span><span> connections.</span><span>remove</span><span>(</span><span>&amp;</span><span>fd).</span><span>unwrap</span><span>();
</span><span>    </span><span>// unregister from epoll
</span><span>    </span><span>drop</span><span>(connection);
</span><span>}
</span></code></pre>
<p>And that's it! Here's the new high-level flow of our server:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>// create epoll
</span><span>    </span><span>let</span><span> epoll </span><span>= </span><span>epoll::create(</span><span>false</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>    </span><span>// bind the listener
</span><span>    </span><span>let</span><span> listener </span><span>= </span><span>/* ... */</span><span>.
</span><span>
</span><span>    </span><span>// add the listener to epoll
</span><span>    </span><span>let</span><span> event </span><span>= </span><span>Event::new(Events::</span><span>EPOLLIN</span><span>, listener.</span><span>as_raw_fd</span><span>() </span><span>as _</span><span>);
</span><span>    epoll::ctl(epoll, </span><span>EPOLL_CTL_ADD</span><span>, listener.</span><span>as_raw_fd</span><span>(), event).</span><span>unwrap</span><span>();
</span><span>
</span><span>    </span><span>let </span><span>mut</span><span> connections </span><span>= </span><span>HashMap::new();
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>let </span><span>mut</span><span> events </span><span>= </span><span>[Event::new(Events::empty(), </span><span>0</span><span>); </span><span>1024</span><span>];
</span><span>
</span><span>        </span><span>// block until epoll wakes us up
</span><span>        </span><span>let</span><span> num_events </span><span>= </span><span>epoll::wait(epoll, </span><span>0</span><span>, </span><span>&amp;mut</span><span> events).</span><span>unwrap</span><span>();
</span><span>        </span><span>let </span><span>mut</span><span> completed </span><span>= </span><span>Vec</span><span>::new();
</span><span>
</span><span>        'next: </span><span>for</span><span> event </span><span>in &amp;</span><span>events[</span><span>..</span><span>num_events] {
</span><span>            </span><span>let</span><span> fd </span><span>=</span><span> event.data </span><span>as </span><span>i32</span><span>;
</span><span>
</span><span>            </span><span>// is the listener ready?
</span><span>            </span><span>if</span><span> fd </span><span>==</span><span> listener.</span><span>as_raw_fd</span><span>() {
</span><span>                </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>                    </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                        </span><span>// ...
</span><span>
</span><span>                        </span><span>// add the connection to epoll
</span><span>                        </span><span>let</span><span> event </span><span>= </span><span>Event::new(Events::</span><span>EPOLLIN </span><span>| </span><span>Events::</span><span>EPOLLOUT</span><span>, fd </span><span>as _</span><span>);
</span><span>                        epoll::ctl(epoll, </span><span>EPOLL_CTL_ADD</span><span>, fd, event).</span><span>unwrap</span><span>();
</span><span>
</span><span>                        </span><span>// keep track of connection state
</span><span>                        </span><span>let</span><span> state </span><span>= </span><span>ConnectionState::Read {
</span><span>                            request: [</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>],
</span><span>                            read: </span><span>0</span><span>,
</span><span>                        };
</span><span>
</span><span>                        connections.</span><span>insert</span><span>(fd, (connection, state));
</span><span>                }
</span><span>
</span><span>                </span><span>continue 'next</span><span>;
</span><span>            }
</span><span>
</span><span>            </span><span>// otherwise, a connection is ready
</span><span>            </span><span>let </span><span>(connection, state) </span><span>=</span><span> connections.</span><span>get_mut</span><span>(</span><span>&amp;</span><span>fd).</span><span>unwrap</span><span>();
</span><span>
</span><span>            </span><span>// try to drive it forward based on its state
</span><span>            </span><span>if </span><span>let </span><span>ConnectionState::Read { request, read } </span><span>=</span><span> state {
</span><span>                </span><span>// connection.read...
</span><span>                </span><span>*</span><span>state </span><span>= </span><span>ConnectionState::Write {
</span><span>                    response: response.</span><span>as_bytes</span><span>(),
</span><span>                    written: </span><span>0</span><span>,
</span><span>                };
</span><span>            }
</span><span>
</span><span>            </span><span>if </span><span>let </span><span>ConnectionState::Write { response, written } </span><span>=</span><span> state {
</span><span>                </span><span>// connection.write...
</span><span>                </span><span>*</span><span>state </span><span>= </span><span>ConnectionState::Flush;
</span><span>            }
</span><span>
</span><span>            </span><span>if </span><span>let </span><span>ConnectionState::Flush </span><span>=</span><span> state {
</span><span>                </span><span>// connection.flush...
</span><span>            }
</span><span>        }
</span><span>
</span><span>        </span><span>for</span><span> fd </span><span>in</span><span> completed {
</span><span>            </span><span>let </span><span>(connection, _state) </span><span>=</span><span> connections.</span><span>remove</span><span>(</span><span>&amp;</span><span>fd).</span><span>unwrap</span><span>();
</span><span>            </span><span>// unregister from epoll
</span><span>            </span><span>drop</span><span>(connection);
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>And...</p>
<pre data-lang="sh"><code data-lang="sh"><span>$ curl localhost:3000
</span><span># =&gt; Hello world!
</span></code></pre>
<p>It works!</p>
<h2 id="futures"><a href="#futures" aria-label="Anchor link for: futures">Futures</a></h2>
<p>Alright, our server can now process multiple requests concurrently on a single thread. And thanks to epoll, it's pretty efficient at doing so. But there's still a problem.</p>
<p>We got so caught up in gaining control over the execution of our tasks, and then scheduling them efficiently ourselves, that in the process the complexity of our code has increased dramatically.</p>
<p>What went from a simple, sequential accept loop has become a massive event loop managing multiple state machines.</p>
<p>And it's not pretty.</p>
<p>Making our original server multi-threaded was as simple as adding a single line of code in <code>thread::spawn</code>. If you think about it, our server is still a set of concurrent tasks, we just manage them all messily in a giant loop.</p>
<p>This doesn't seem very scalable. The more features we add to our program, the more complex the loop becomes, because everything is so tightly coupled together.</p>
<p>What if we could write an abstraction like <code>thread::spawn</code> that let us write our tasks as individual units, and handle the scheduling and event handling for all tasks in a single place, regaining some of that sequential control flow?</p>
<blockquote>
<p>This idea is generally referred to as <em>asynchronous programming</em>.</p>
</blockquote>
<p>Let's take a look at the signature of <code>thread::spawn</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>pub </span><span>fn </span><span>spawn</span><span>&lt;F, T&gt;(</span><span>f</span><span>: F) -&gt; JoinHandle&lt;T&gt;
</span><span>where
</span><span>    F: FnOnce() -&gt; T + Send + </span><span>'static</span><span>,
</span><span>    T: Send + </span><span>'static</span><span>;
</span></code></pre>
<p><code>thread::spawn</code> takes a closure. Our version of <code>thread::spawn</code> however, could not take a closure, because we aren't the operating system and can't arbitrarily preempt code at will. We need to somehow represent a non-blocking, resumable task.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// fn spawn&lt;T: Task&gt;(task: T);
</span><span>
</span><span>trait </span><span>Task {}
</span></code></pre>
<p>Handling a request is a task. Reading or writing from/to the connection is also a task. A task is really just a piece of code that needs to be run, representing a value that will resolve sometime in the <em>future</em>.</p>
<p><code>Future</code>, that's a nice name isn't it.</p>
<pre data-lang="rust"><code data-lang="rust"><span>trait </span><span>Future {
</span><span>    </span><span>type </span><span>Output;
</span><span>
</span><span>    </span><span>fn </span><span>run</span><span>(</span><span>self</span><span>) -&gt; </span><span>Self::</span><span>Output;
</span><span>}
</span></code></pre>
<p>Hmm.. that signature doesn't really work. Having <code>run</code> return the output directly means it must be blocking, which is what we're trying so hard to avoid. We instead want a way to attempt to drive the future forward without blocking, like we've been doing with all our state machines in the event loop.</p>
<p>What we're really doing when we try to run a future is asking it if the value is ready yet, <em>polling</em> it, and giving it a chance to make progress.</p>
<pre data-lang="rust"><code data-lang="rust"><span>trait </span><span>Future {
</span><span>    </span><span>type </span><span>Output;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>self</span><span>) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt;;
</span><span>}
</span></code></pre>
<p>That looks more like it.</p>
<p>Except wait, <code>poll</code> can't take <code>self</code> if we want to call it multiple times, it should probably take a reference. A mutable reference, if we want to mutate the internal state of the task as it makes progress, like <code>ConnectionState</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>trait </span><span>Future {
</span><span>    </span><span>type </span><span>Output;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt;;
</span><span>}
</span></code></pre>
<p>Alright, imagine a scheduler that runs these new futures.</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>Scheduler {
</span><span>    </span><span>fn </span><span>run</span><span>(</span><span>&amp;</span><span>self</span><span>) {
</span><span>        </span><span>loop </span><span>{
</span><span>            </span><span>for</span><span> future </span><span>in &amp;</span><span>self.tasks {
</span><span>                future.</span><span>poll</span><span>();
</span><span>            }
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>That doesn't look right.</p>
<p>After initiating the future, the scheduler should only try to call <code>poll</code> when the given future is able to make progress, like when epoll returns an event. But how do we know when that happens?</p>
<p>If the future represents an I/O operation, we know it's able to make progress when epoll tells us it is. The problem is the scheduler won't know which epoll event corresponds to which future, because the future handles everything internally in <code>poll</code>.</p>
<p>What we need is for the scheduler to pass each future an ID, so that the future can register any I/O resources with epoll using the same ID, instead of their file descriptors. That way the scheduler has a way of mapping epoll events to runnable futures.</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>Scheduler {
</span><span>    </span><span>fn </span><span>spawn</span><span>&lt;T&gt;(</span><span>&amp;</span><span>self</span><span>, </span><span>mut </span><span>future</span><span>: T) {
</span><span>        </span><span>let</span><span> id </span><span>= </span><span>rand</span><span>();
</span><span>        </span><span>// poll the future once to get it started, passing in it's ID
</span><span>        future.</span><span>poll</span><span>(event.id);
</span><span>        </span><span>// store the future
</span><span>        self.tasks.</span><span>insert</span><span>(id, future);
</span><span>    }
</span><span>
</span><span>    </span><span>fn </span><span>run</span><span>(</span><span>self</span><span>) {
</span><span>        </span><span>// ...
</span><span>
</span><span>        </span><span>for</span><span> event </span><span>in</span><span> epoll_events {
</span><span>            </span><span>// poll the future associated with this event 
</span><span>            </span><span>let</span><span> future </span><span>= </span><span>self.tasks.</span><span>get</span><span>(</span><span>&amp;</span><span>event.id).</span><span>unwrap</span><span>();
</span><span>            future.</span><span>poll</span><span>(event.id);
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>You know, it would be nice if there was a more generic way to tell the scheduler about progress than tying every future to epoll. We might have different types of futures that make progress in other ways, like a timer running on a background thread, or a channel that needs to notify tasks that a message is available.</p>
<p>What if we gave the futures themselves more control? Instead of just an ID, what if we give every future a way to wake itself up, notifying the scheduler that it's ready to make progress?</p>
<p>A simple callback should do the trick.</p>
<pre data-lang="rust"><code data-lang="rust"><span>#[derive(Clone)]
</span><span>struct </span><span>Waker(Arc&lt;dyn </span><span>Fn</span><span>() </span><span>+ </span><span>Send </span><span>+ </span><span>Sync</span><span>&gt;</span><span>);
</span><span>
</span><span>impl </span><span>Waker {
</span><span>    </span><span>fn </span><span>wake</span><span>(</span><span>&amp;</span><span>self</span><span>) {
</span><span>        (self.</span><span>0</span><span>)()
</span><span>    }
</span><span>}
</span><span>
</span><span>trait </span><span>Future {
</span><span>    </span><span>type </span><span>Output;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt;;
</span><span>}
</span></code></pre>
<p>The scheduler can provide each future a callback, that when called, updates the scheduler's state for that future, marking it as ready. That way our scheduler is completely disconnected from epoll, or any other individual notification system.</p>
<p><code>Waker</code> is thread-safe, allowing us to use background threads to wake futures. Right now all of our tasks are connected to epoll anyways, but this will come in handy later.</p>
<h2 id="a-reactor"><a href="#a-reactor" aria-label="Anchor link for: a-reactor">A Reactor</a></h2>
<p>Consider a future that reads from a TCP connection. It receives a <code>Waker</code> that needs to be called when epoll returns the relevant <code>EPOLLIN</code> event, but the future won't be running when that happens, it will be idle in the scheduler's queue. Obviously, the future can't wake itself up, someone else has to.</p>
<p>All I/O futures need a way to give their wakers to epoll. In fact, they need more than that, they need some sort of background service that drives epoll, so we can register wakers with it.</p>
<p>This service is commonly known as a <em>reactor</em>.</p>
<p>The reactor is a simple object holding the epoll descriptor and a map of tasks keyed by file descriptor, just like we had before. The difference is that instead of the map holding the TCP connections themselves, it holds the wakers.</p>
<pre data-lang="rust"><code data-lang="rust"><span>thread_local! {
</span><span>    </span><span>static </span><span>REACTOR</span><span>: Reactor </span><span>= </span><span>Reactor::new();
</span><span>}
</span><span>
</span><span>struct </span><span>Reactor {
</span><span>    epoll: RawFd,
</span><span>    tasks: RefCell&lt;HashMap&lt;RawFd, Waker&gt;&gt;,
</span><span>}
</span><span>
</span><span>impl </span><span>Reactor {
</span><span>    </span><span>pub </span><span>fn </span><span>new</span><span>() -&gt; Reactor {
</span><span>        Reactor {
</span><span>            epoll: epoll::create(</span><span>false</span><span>).</span><span>unwrap</span><span>(),
</span><span>            tasks: RefCell::new(HashMap::new()),
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>To keep things simple, the reactor is a thread-local object, mutated through a <code>RefCell</code>. This is important because the reactor will be modified and accessed by different tasks throughout the program.</p>
<p>The reactor needs to support a couple simple operations.</p>
<p>Adding a task:</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>Reactor {
</span><span>    </span><span>// Add a file descriptor with read and write interest.
</span><span>    </span><span>//
</span><span>    </span><span>// `waker` will be called when the descriptor becomes ready.
</span><span>    </span><span>pub </span><span>fn </span><span>add</span><span>(</span><span>&amp;</span><span>self</span><span>, </span><span>fd</span><span>: RawFd, </span><span>waker</span><span>: Waker) {
</span><span>        </span><span>let</span><span> event </span><span>= </span><span>epoll::Event::new(Events::</span><span>EPOLLIN </span><span>| </span><span>Events::</span><span>EPOLLOUT</span><span>, fd </span><span>as </span><span>u64</span><span>);
</span><span>        epoll::ctl(self.epoll, </span><span>EPOLL_CTL_ADD</span><span>, fd, event).</span><span>unwrap</span><span>();
</span><span>        self.tasks.</span><span>borrow_mut</span><span>().</span><span>insert</span><span>(fd, waker);
</span><span>    }
</span><span>}
</span></code></pre>
<p>Removing a task:</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>Reactor {
</span><span>    </span><span>// Remove the given descriptor from epoll.
</span><span>    </span><span>//
</span><span>    </span><span>// It will no longer receive any notifications.
</span><span>    </span><span>pub </span><span>fn </span><span>remove</span><span>(</span><span>&amp;</span><span>self</span><span>, </span><span>fd</span><span>: RawFd) {
</span><span>        self.tasks.</span><span>borrow_mut</span><span>().</span><span>remove</span><span>(</span><span>&amp;</span><span>fd);
</span><span>    }
</span><span>}
</span></code></pre>
<p>And driving epoll.</p>
<p>We'll be running the reactor in a loop, just like we were running epoll in a loop before. It works exactly the same way, except all the reactor has to do is wake the associated future for every event. Remember, this will trigger the scheduler to run the future later, and continue the cycle.</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>Reactor {
</span><span>    </span><span>// Drive tasks forward, blocking forever until an event arrives.
</span><span>    </span><span>pub </span><span>fn </span><span>wait</span><span>(</span><span>&amp;</span><span>self</span><span>) {
</span><span>       </span><span>let </span><span>mut</span><span> events </span><span>= </span><span>[Event::new(Events::empty(), </span><span>0</span><span>); </span><span>1024</span><span>];
</span><span>       </span><span>let</span><span> timeout </span><span>= -</span><span>1</span><span>; </span><span>// forever
</span><span>       </span><span>let</span><span> num_events </span><span>= </span><span>epoll::wait(self.epoll, timeout, </span><span>&amp;mut</span><span> events).</span><span>unwrap</span><span>();
</span><span>
</span><span>       </span><span>for</span><span> event </span><span>in &amp;</span><span>events[</span><span>..</span><span>num_events] {
</span><span>           </span><span>let</span><span> fd </span><span>=</span><span> event.data </span><span>as </span><span>i32</span><span>;
</span><span>
</span><span>           </span><span>// wake the task
</span><span>           </span><span>if </span><span>let Some</span><span>(waker) </span><span>= </span><span>self.tasks.</span><span>borrow</span><span>().</span><span>get</span><span>(</span><span>&amp;</span><span>fd) {
</span><span>               waker.</span><span>wake</span><span>();
</span><span>           }
</span><span>       }
</span><span>    }
</span><span>}
</span></code></pre>
<p>Great, now we have a simple reactor interface.</p>
<p>But all of this is still a little abstract. What does it really mean to call <code>wake</code>?</p>
<h2 id="scheduling-tasks"><a href="#scheduling-tasks" aria-label="Anchor link for: scheduling-tasks">Scheduling Tasks</a></h2>
<p>We have a reactor, now we need a scheduler to run our tasks.</p>
<p>One thing to keep in mind is that the scheduler must be global and thread-safe because wakers are <code>Send</code>, meaning <code>wake</code> may be called concurrently from other threads.</p>
<pre data-lang="rust"><code data-lang="rust"><span>static </span><span>SCHEDULER</span><span>: Scheduler </span><span>=</span><span> Scheduler { </span><span>/* ... */ </span><span>};
</span><span>
</span><span>#[derive(Default)]
</span><span>struct </span><span>Scheduler {
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>We want to be able to spawn tasks onto our scheduler, just like we could spawn threads. For now, we'll only handle spawning tasks that don't return anything, to avoid having to implement a version of <code>JoinHandle</code>.</p>
<p>To start, we'll probably need some sort of list of tasks to run, guarded by a <code>Mutex</code> to be thread-safe.</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>Scheduler {
</span><span>    tasks: Mutex&lt;</span><span>Vec</span><span>&lt;</span><span>Box</span><span>&lt;dyn Future </span><span>+ </span><span>Send</span><span>&gt;&gt;&gt;
</span><span>}
</span><span>
</span><span>impl </span><span>Scheduler {
</span><span>    </span><span>pub </span><span>fn </span><span>spawn</span><span>(</span><span>&amp;</span><span>self</span><span>, </span><span>task</span><span>: impl Future&lt;Output = ()&gt; + Send + </span><span>'static</span><span>) {
</span><span>        self.tasks.</span><span>lock</span><span>().</span><span>unwrap</span><span>().</span><span>push</span><span>(</span><span>Box</span><span>::new(task));
</span><span>    }
</span><span>
</span><span>    </span><span>pub </span><span>fn </span><span>run</span><span>(</span><span>&amp;</span><span>self</span><span>) {
</span><span>        </span><span>for</span><span> task </span><span>in</span><span> tasks.</span><span>borrow_mut</span><span>().</span><span>iter_mut</span><span>() {
</span><span>            </span><span>// ...
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>Remember, futures are only polled when they are able to make progress. They should always be able to make progress at the start, but after that we don't touch them until someone calls <code>wake</code>.</p>
<p>There are a couple of ways we could go about this. We could just store a <code>HashMap</code> of tasks with a status flag that indicates whether or not the task was woken, but that means we would have to iterate through the entire map to find out which tasks are runnable. While this isn't incredibly expensive, there is a better way.</p>
<p>Instead of storing every spawned task in the map, we'll only store runnable tasks in a queue.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>std::collections::VecDeque;
</span><span>
</span><span>type </span><span>SharedTask </span><span>= </span><span>Arc&lt;Mutex&lt;dyn Future&lt;Output = ()&gt; </span><span>+ </span><span>Send</span><span>&gt;&gt;;
</span><span>
</span><span>#[derive(Default)]
</span><span>struct </span><span>Scheduler {
</span><span>    runnable: Mutex&lt;VecDeque&lt;SharedTask&gt;&gt;,
</span><span>}
</span></code></pre>
<p>The types will make sense soon.</p>
<p>When a task is spawned, it's pushed onto the back of the queue:</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>Scheduler {
</span><span>    </span><span>pub </span><span>fn </span><span>spawn</span><span>(</span><span>&amp;</span><span>self</span><span>, </span><span>task</span><span>: impl Future&lt;Output = ()&gt; + Send + </span><span>'static</span><span>) {
</span><span>        self.runnable.</span><span>lock</span><span>().</span><span>unwrap</span><span>().</span><span>push_back</span><span>(Arc::new(Mutex::new(task)));
</span><span>    }
</span><span>}
</span></code></pre>
<p>The scheduler pops tasks off the queue one by one and calls <code>poll</code>:</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>Scheduler {
</span><span>    </span><span>fn </span><span>run</span><span>(</span><span>&amp;</span><span>self</span><span>) {
</span><span>        </span><span>loop </span><span>{
</span><span>            </span><span>// pop a runnable task off the queue
</span><span>            </span><span>let</span><span> task </span><span>= </span><span>self.runnable.</span><span>lock</span><span>().</span><span>unwrap</span><span>().</span><span>pop_front</span><span>();
</span><span>
</span><span>            </span><span>if </span><span>let Some</span><span>(task) </span><span>=</span><span> task {
</span><span>                </span><span>// and poll it
</span><span>                task.</span><span>try_lock</span><span>().</span><span>unwrap</span><span>().</span><span>poll</span><span>(waker);
</span><span>            }
</span><span>        }
</span><span>    }
</span></code></pre>
<blockquote>
<p>Notice that we don't even really need a <code>Mutex</code> around the task because it's only going to be accessed by the main thread, but removing it would mean <code>unsafe</code>. We'll settle with <code>try_lock().unwrap()</code> for now.</p>
</blockquote>
<p>Now for the important bit, the waker. The beautiful part of our run queue is that when a task is woken, it's simply pushed back onto the queue.</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>Scheduler {
</span><span>    </span><span>fn </span><span>run</span><span>(</span><span>&amp;</span><span>self</span><span>) {
</span><span>        </span><span>loop </span><span>{
</span><span>            </span><span>// pop a runnable task off the queue
</span><span>            </span><span>let</span><span> task </span><span>= </span><span>self.runnable.</span><span>lock</span><span>().</span><span>unwrap</span><span>().</span><span>pop_front</span><span>();
</span><span>
</span><span>            </span><span>if </span><span>let Some</span><span>(task) </span><span>=</span><span> task {
</span><span>                </span><span>let</span><span> t2 </span><span>=</span><span> task.</span><span>clone</span><span>();
</span><span>
</span><span>                </span><span>// create a waker that pushes the task back on
</span><span>                </span><span>let</span><span> wake </span><span>= </span><span>Arc::new(</span><span>move || </span><span>{
</span><span>                    </span><span>SCHEDULER</span><span>.runnable.</span><span>lock</span><span>().</span><span>unwrap</span><span>().</span><span>push_back</span><span>(t2.</span><span>clone</span><span>());
</span><span>                });
</span><span>
</span><span>                </span><span>// poll the task
</span><span>                task.</span><span>try_lock</span><span>().</span><span>unwrap</span><span>().</span><span>poll</span><span>(Waker(wake));
</span><span>            }
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>This is why the task needed to be reference counted — it's not owned by the scheduler, it's referenced by the queue, as well as wherever the waker is being stored. In fact the same task might be on the queue multiple times at once, and the waker might be cloned all over the place.</p>
<p>Once we've dealt with all runnable tasks, we need to block on the reactor until another task becomes ready <sup><a href="#4">5</a></sup>. Once a task becomes ready, the reactor will call <code>wake</code> and push the future back onto our queue for us to run it again, continuing the cycle.</p>
<pre data-lang="rust"><code data-lang="rust"><span>pub </span><span>fn </span><span>run</span><span>(</span><span>&amp;</span><span>self</span><span>) {
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>loop </span><span>{
</span><span>            </span><span>// pop a runnable task off the queue
</span><span>            </span><span>let Some</span><span>(task) </span><span>= </span><span>self.runnable.</span><span>lock</span><span>().</span><span>unwrap</span><span>().</span><span>pop_front</span><span>() </span><span>else </span><span>{ </span><span>break </span><span>};
</span><span>            </span><span>let</span><span> t2 </span><span>=</span><span> task.</span><span>clone</span><span>();
</span><span>
</span><span>            </span><span>// create a waker that pushes the task back on
</span><span>            </span><span>let</span><span> wake </span><span>= </span><span>Arc::new(</span><span>move || </span><span>{
</span><span>                </span><span>SCHEDULER</span><span>.runnable.</span><span>lock</span><span>().</span><span>unwrap</span><span>().</span><span>push_back</span><span>(t2.</span><span>clone</span><span>());
</span><span>            });
</span><span>
</span><span>            </span><span>// poll the task
</span><span>            task.</span><span>lock</span><span>().</span><span>unwrap</span><span>().</span><span>poll</span><span>(Waker(wake));
</span><span>        }
</span><span>
</span><span>        </span><span>// if there are no runnable tasks, block on epoll until something becomes ready
</span><span>        </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| reactor.</span><span>wait</span><span>()); </span><span>// 👈
</span><span>    }
</span><span>}
</span></code></pre>
<p>Beautiful.</p>
<p>...ignoring the <code>Arc&lt;Mutex&lt;T&gt;&gt;</code> clutter.</p>
<p>Alright! Together, the scheduler and reactor form a <em>runtime</em> for our futures. The scheduler keeps tracks of which tasks are runnable and polls them, and the reactor marks tasks as runnable when epoll tells us something they are interested in becomes ready.</p>
<pre data-lang="rust"><code data-lang="rust"><span>trait </span><span>Future {
</span><span>    </span><span>type </span><span>Output;
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt;;
</span><span>}
</span><span>
</span><span>static </span><span>SCHEDULER</span><span>: Scheduler </span><span>=</span><span> Scheduler { </span><span>/* ... */ </span><span>};
</span><span>
</span><span>// The scheduler.
</span><span>#[derive(Default)]
</span><span>struct </span><span>Scheduler {
</span><span>    runnable: Mutex&lt;VecDeque&lt;SharedTask&gt;&gt;,
</span><span>}
</span><span>
</span><span>type </span><span>SharedTask </span><span>= </span><span>Arc&lt;Mutex&lt;dyn Future&lt;Output = ()&gt; </span><span>+ </span><span>Send</span><span>&gt;&gt;;
</span><span>
</span><span>impl </span><span>Scheduler {
</span><span>    </span><span>pub </span><span>fn </span><span>spawn</span><span>(</span><span>&amp;</span><span>self</span><span>, </span><span>task</span><span>: impl Future&lt;Output = ()&gt; + Send + </span><span>'static</span><span>);
</span><span>    </span><span>pub </span><span>fn </span><span>run</span><span>(</span><span>&amp;</span><span>self</span><span>);
</span><span>}
</span><span>
</span><span>thread_local! {
</span><span>    </span><span>static </span><span>REACTOR</span><span>: Reactor </span><span>= </span><span>Reactor::new();
</span><span>}
</span><span>
</span><span>// The reactor.
</span><span>struct </span><span>Reactor {
</span><span>    epoll: RawFd,
</span><span>    tasks: RefCell&lt;HashMap&lt;RawFd, Waker&gt;&gt;,
</span><span>}
</span><span>
</span><span>impl </span><span>Reactor {
</span><span>    </span><span>pub </span><span>fn </span><span>new</span><span>() -&gt; Reactor;
</span><span>    </span><span>pub </span><span>fn </span><span>add</span><span>(</span><span>&amp;</span><span>self</span><span>, </span><span>fd</span><span>: RawFd, </span><span>waker</span><span>: Waker);
</span><span>    </span><span>pub </span><span>fn </span><span>remove</span><span>(</span><span>&amp;</span><span>self</span><span>, </span><span>fd</span><span>: RawFd);
</span><span>    </span><span>pub </span><span>fn </span><span>wait</span><span>(</span><span>&amp;</span><span>self</span><span>);
</span><span>}
</span></code></pre>
<p>We've written the runtime, now let's try to use it!</p>
<h2 id="an-async-server"><a href="#an-async-server" aria-label="Anchor link for: an-async-server">An Async Server</a></h2>
<p>It's time to actually write the tasks that our scheduler is going to run. Like before, we'll use enums as state machines to manage the different states of our program. The difference is that this time, each task will manage it's own state independent from other tasks, instead of having the entire program revolve around a messy event loop.</p>
<p>To start everything off, we need to write the main task. This task will be pushed on and off the scheduler's run queue for the entirety of our program.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(Main::Start);
</span><span>    </span><span>SCHEDULER</span><span>.</span><span>run</span><span>();
</span><span>}
</span><span>
</span><span>enum </span><span>Main {
</span><span>    Start,
</span><span>}
</span><span>
</span><span>impl </span><span>Future </span><span>for </span><span>Main {
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>();
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;()&gt; {
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span></code></pre>
<p>Our task starts off just like before, creating the TCP listener and setting it to non-blocking mode.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// impl Future for Main {
</span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;()&gt; {
</span><span>    </span><span>if </span><span>let </span><span>Main::Start </span><span>= </span><span>self {
</span><span>        </span><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).</span><span>unwrap</span><span>();
</span><span>        listener.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>    }
</span><span>
</span><span>    </span><span>None
</span><span>}
</span></code></pre>
<p>Now we need to register the listener with epoll. We can do that using our new <code>Reactor</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// impl Future for Main {
</span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;()&gt; {
</span><span>    </span><span>if </span><span>let </span><span>Main::Start </span><span>= </span><span>self {
</span><span>        </span><span>// ...
</span><span>
</span><span>        </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>            reactor.</span><span>add</span><span>(listener.</span><span>as_raw_fd</span><span>(), waker);
</span><span>        });
</span><span>    }
</span><span>}
</span></code></pre>
<p>Notice how we give the reactor the waker provided to us by the scheduler. When a connection comes, epoll will return an event and the <code>Reactor</code> will wake the task, causing the scheduler to push our task back onto the queue and <code>poll</code> us again. The waker keeps everything connected.</p>
<p>We now need a second state for the next time we're run, <code>Accept</code>. The main task will stay in the <code>Accept</code> state for the rest of the program, attempting to accept new connections.</p>
<pre data-lang="rust"><code data-lang="rust"><span>enum </span><span>Main {
</span><span>    Start,
</span><span>    Accept { listener: TcpListener }, </span><span>// 👈
</span><span>}
</span><span>
</span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;()&gt; {
</span><span>    </span><span>if </span><span>let </span><span>Main::Start </span><span>= </span><span>self {
</span><span>        </span><span>// ...
</span><span>        </span><span>*</span><span>self </span><span>= </span><span>Main::Accept { listener };
</span><span>    }
</span><span>
</span><span>    </span><span>if </span><span>let </span><span>Main::Accept { listener } </span><span>= </span><span>self {
</span><span>        </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                </span><span>// ...
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{
</span><span>                </span><span>return </span><span>None</span><span>;
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span>None
</span><span>}
</span></code></pre>
<p>If the listener is not ready, we can simply return <code>None</code>. Remember, this tells the scheduler the future is not yet ready, and it will be rescheduled once the reactor wakes us.</p>
<p>If we do accept a new connection, we need to again set it to non-blocking mode.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;()&gt; {
</span><span>    </span><span>if </span><span>let </span><span>Main::Start </span><span>= </span><span>self {
</span><span>        </span><span>// ...
</span><span>    }
</span><span>
</span><span>    </span><span>if </span><span>let </span><span>Main::Accept { listener } </span><span>= </span><span>self {
</span><span>        </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>(); </span><span>// 👈 
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; return </span><span>None</span><span>,
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span>None
</span><span>}
</span></code></pre>
<p>And now we need to spawn a new task to handle the request.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;()&gt; {
</span><span>    </span><span>if </span><span>let </span><span>Main::Start </span><span>= </span><span>self {
</span><span>        </span><span>// ...
</span><span>    }
</span><span>
</span><span>    </span><span>if </span><span>let </span><span>Main::Accept { listener } </span><span>= </span><span>self {
</span><span>        </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>                </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(Handler { </span><span>// 👈
</span><span>                    connection,
</span><span>                    state: HandlerState::Start,
</span><span>                });
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; return </span><span>None</span><span>,
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>The handler task looks similar to before, but now it manages the connection itself along with its current state, which is identical to <code>ConnectionState</code> from earlier.</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>Handler {
</span><span>    connection: TcpStream,
</span><span>    state: HandlerState,
</span><span>}
</span><span>
</span><span>enum </span><span>HandlerState {
</span><span>    Start,
</span><span>    Read {
</span><span>        request: [</span><span>u8</span><span>; </span><span>1024</span><span>],
</span><span>        read: </span><span>usize</span><span>,
</span><span>    },
</span><span>    Write {
</span><span>        response: </span><span>&amp;'static </span><span>[</span><span>u8</span><span>],
</span><span>        written: </span><span>usize</span><span>,
</span><span>    },
</span><span>    Flush,
</span><span>}
</span></code></pre>
<p>The handler task starts by registering its connection with the reactor to be notified when the connection is ready to read/write to. Again, it passes the waker so that the scheduler knows when to run it again.</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>Future </span><span>for </span><span>Handler {
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>();
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>        </span><span>if </span><span>let </span><span>HandlerState::Start </span><span>= </span><span>self.state {
</span><span>            </span><span>// start by registering our connection for notifications
</span><span>            </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>                reactor.</span><span>add</span><span>(self.connection.</span><span>as_raw_fd</span><span>(), waker);
</span><span>            });
</span><span>
</span><span>            self.state </span><span>= </span><span>HandlerState::Read {
</span><span>                request: [</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>],
</span><span>                read: </span><span>0</span><span>,
</span><span>            };
</span><span>        }
</span><span>
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span></code></pre>
<p>The <code>Read</code>, <code>Write</code>, and <code>Flush</code> states work exactly the same as before, but now when we encounter <code>WouldBlock</code>, we can simply return <code>None</code>, knowing that we'll be run again once our future is woken.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// impl Future for Handler {
</span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>    </span><span>if </span><span>let </span><span>HandlerState::Start </span><span>= </span><span>self.state {
</span><span>        </span><span>// ...
</span><span>    }
</span><span>
</span><span>    </span><span>// read the request
</span><span>    </span><span>if </span><span>let </span><span>HandlerState::Read { request, read } </span><span>= &amp;mut </span><span>self.state {
</span><span>        </span><span>loop </span><span>{
</span><span>            </span><span>match </span><span>self.connection.</span><span>read</span><span>(</span><span>&amp;mut</span><span> request[read</span><span>..</span><span>]) {
</span><span>                </span><span>Ok</span><span>(</span><span>0</span><span>) </span><span>=&gt; </span><span>{
</span><span>                    println!(</span><span>"client disconnected unexpectedly"</span><span>);
</span><span>                    </span><span>return </span><span>Some</span><span>(());
</span><span>                }
</span><span>                </span><span>Ok</span><span>(n) </span><span>=&gt; *</span><span>read </span><span>+=</span><span> n,
</span><span>                </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; return </span><span>None</span><span>, </span><span>// 👈
</span><span>                </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>            }
</span><span>
</span><span>            </span><span>// did we reach the end of the request?
</span><span>            </span><span>let</span><span> read </span><span>= *</span><span>read;
</span><span>            </span><span>if</span><span> read </span><span>&gt;= </span><span>4 </span><span>&amp;&amp; &amp;</span><span>request[read </span><span>- </span><span>4</span><span>..</span><span>read] </span><span>== </span><span>b</span><span>"</span><span>\r\n\r\n</span><span>" </span><span>{
</span><span>                </span><span>break</span><span>;
</span><span>            }
</span><span>        }
</span><span>
</span><span>        </span><span>// we're done, print the request
</span><span>        </span><span>let</span><span> request </span><span>= </span><span>String</span><span>::from_utf8_lossy(</span><span>&amp;</span><span>request[</span><span>..*</span><span>read]);
</span><span>        println!(</span><span>"</span><span>{}</span><span>"</span><span>, request);
</span><span>
</span><span>        </span><span>// and move into the write state
</span><span>        </span><span>let</span><span> response </span><span>= </span><span>/* ... */</span><span>;
</span><span>
</span><span>        self.state </span><span>= </span><span>HandlerState::Write {
</span><span>            response: response.</span><span>as_bytes</span><span>(),
</span><span>            written: </span><span>0</span><span>,
</span><span>        };
</span><span>    }
</span><span>
</span><span>    </span><span>// write the response
</span><span>    </span><span>if </span><span>let </span><span>HandlerState::Write { response, written } </span><span>= &amp;mut </span><span>self.state {
</span><span>        </span><span>// self.connection.write...
</span><span>
</span><span>        </span><span>// successfully wrote the response, try flushing next
</span><span>        self.state </span><span>= </span><span>HandlerState::Flush;
</span><span>    }
</span><span>
</span><span>    </span><span>// flush the response
</span><span>    </span><span>if </span><span>let </span><span>HandlerState::Flush </span><span>= </span><span>self.state {
</span><span>        </span><span>match </span><span>self.connection.</span><span>flush</span><span>() {
</span><span>            </span><span>Ok</span><span>(</span><span>_</span><span>) </span><span>=&gt; </span><span>{}
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; return </span><span>None</span><span>, </span><span>// 👈
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>Notice how much nicer things are when tasks are independent, encapsulated objects?</p>
<p>At the end of the task's lifecycle, it removes its connection from the reactor and returns <code>Some</code>. It will never be run again after that point.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>    </span><span>// ...
</span><span>
</span><span>    </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>        reactor.</span><span>remove</span><span>(self.connection.</span><span>as_raw_fd</span><span>());
</span><span>    });
</span><span>
</span><span>    </span><span>Some</span><span>(())
</span><span>}
</span></code></pre>
<p>Perfect! Our new server looks a lot nicer. Individual tasks are completely independent of each other, and we can spawn new tasks just like threads.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(Main::Start);
</span><span>    </span><span>SCHEDULER</span><span>.</span><span>run</span><span>();
</span><span>}
</span><span>
</span><span>// main task: accept loop
</span><span>enum </span><span>Main {
</span><span>    Start,
</span><span>    Accept { listener: TcpListener },
</span><span>}
</span><span>
</span><span>impl </span><span>Future </span><span>for </span><span>Main {
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>();
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;()&gt; {
</span><span>        </span><span>if </span><span>let </span><span>Main::Start </span><span>= </span><span>self {
</span><span>            </span><span>// ...
</span><span>            </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>                reactor.</span><span>add</span><span>(listener.</span><span>as_raw_fd</span><span>(), waker);
</span><span>            });
</span><span>
</span><span>            </span><span>*</span><span>self </span><span>= </span><span>Main::Accept { listener };
</span><span>        }
</span><span>
</span><span>        </span><span>if </span><span>let </span><span>Main::Accept { listener } </span><span>= </span><span>self {
</span><span>            </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>                </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                    </span><span>// ...
</span><span>                    </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(Handler {
</span><span>                        connection,
</span><span>                        state: HandlerState::Start,
</span><span>                    });
</span><span>                }
</span><span>                </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; return </span><span>None</span><span>,
</span><span>                </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>            }
</span><span>        }
</span><span>
</span><span>        </span><span>None
</span><span>    }
</span><span>}
</span><span>
</span><span>// handler task: handles every connection
</span><span>struct </span><span>Handler {
</span><span>    connection: TcpStream,
</span><span>    state: HandlerState,
</span><span>}
</span><span>
</span><span>enum </span><span>HandlerState {
</span><span>    Start,
</span><span>    Read {
</span><span>        request: [</span><span>u8</span><span>; </span><span>1024</span><span>],
</span><span>        read: </span><span>usize</span><span>,
</span><span>    },
</span><span>    Write {
</span><span>        response: </span><span>&amp;'static </span><span>[</span><span>u8</span><span>],
</span><span>        written: </span><span>usize</span><span>,
</span><span>    },
</span><span>    Flush,
</span><span>}
</span><span>
</span><span>impl </span><span>Future </span><span>for </span><span>Handler {
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>();
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>        </span><span>if </span><span>let </span><span>HandlerState::Start </span><span>= </span><span>self.state {
</span><span>            </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>                reactor.</span><span>add</span><span>(self.connection.</span><span>as_raw_fd</span><span>(), waker);
</span><span>            });
</span><span>
</span><span>            self.state </span><span>= </span><span>HandlerState::Read { </span><span>/* .. */ </span><span>};
</span><span>        }
</span><span>
</span><span>        </span><span>if </span><span>let </span><span>HandlerState::Read { request, read } </span><span>= &amp;mut </span><span>self.state {
</span><span>            </span><span>// ...
</span><span>            self.state </span><span>= </span><span>HandlerState::Write { </span><span>/* .. */ </span><span>};
</span><span>        }
</span><span>
</span><span>        </span><span>if </span><span>let </span><span>HandlerState::Write { response, written } </span><span>= &amp;mut </span><span>self.state {
</span><span>            </span><span>// ...
</span><span>            self.state </span><span>= </span><span>HandlerState::Flush;
</span><span>        }
</span><span>
</span><span>        </span><span>if </span><span>let </span><span>HandlerState::Flush </span><span>= </span><span>self.state {
</span><span>            </span><span>// ...
</span><span>        }
</span><span>
</span><span>        </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>            reactor.</span><span>remove</span><span>(self.connection.</span><span>as_raw_fd</span><span>());
</span><span>        });
</span><span>
</span><span>        </span><span>Some</span><span>(())
</span><span>    }
</span><span>}
</span></code></pre>
<p>Andd....</p>
<pre data-lang="sh"><code data-lang="sh"><span>$ curl localhost:3000
</span><span># =&gt; Hello world!
</span></code></pre>
<p>It works!</p>
<h2 id="a-functional-server"><a href="#a-functional-server" aria-label="Anchor link for: a-functional-server">A Functional Server</a></h2>
<p>With this new future abstraction, our server is <em>much</em> nicer than before. Futures get to manage their state independently, the scheduler gets to run tasks without worrying about epoll, and tasks can be spawned and woken without worrying about any of the lower level details of the scheduler. It really is a much nicer programming model.</p>
<p>It is nice that tasks are encapsulated, but we still have to write everything in a state-machine like way. Granted, Rust makes this pretty easy to do with enums, but could we do better?</p>
<p>Looking at the two futures we've written, they have a lot in common. Each future has a number of states. At each state, some code is run. If that code completes successfully, we transition into the next state. If it encounters <code>WouldBlock</code>, we return <code>None</code>, indicating that the future is not yet ready.</p>
<p>This seems like something we can abstract over.</p>
<p>What we need is a way to create a future from some block of code, and a way to <em>combine</em> two futures, chaining them together.</p>
<p>Given a block of code, we need to be able to construct a future... sound like a job for a closure?</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>future_fn</span><span>(</span><span>f</span><span>: F) -&gt; impl Future
</span><span>where
</span><span>    F: Fn(),
</span><span>{
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>The closure probably also needs to mutate local state.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>future_fn</span><span>(</span><span>f</span><span>: F) -&gt; impl Future
</span><span>where
</span><span>    F: FnMut(),
</span><span>{
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>And it also needs access to the waker.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>future_fn</span><span>(</span><span>f</span><span>: F) -&gt; impl Future
</span><span>where
</span><span>    F: FnMut(Waker),
</span><span>{
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>And.. it needs to return a value. An optional value, in case it's not ready yet. In fact, we can just copy the signature of <code>poll</code>, because that's really what this closure is 😅</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>poll_fn</span><span>&lt;F, T&gt;(</span><span>f</span><span>: F) -&gt; impl Future&lt;Output = T&gt;
</span><span>where
</span><span>    F: FnMut(Waker) -&gt; </span><span>Option</span><span>&lt;T&gt;,
</span><span>{
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>Implementing <code>poll_fn</code> doesn't seem too hard, we just need a wrapper struct that implements <code>Future</code> and delegates <code>poll</code> to the closure.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>poll_fn</span><span>&lt;F, T&gt;(</span><span>f</span><span>: F) -&gt; impl Future&lt;Output = T&gt;
</span><span>where
</span><span>    F: FnMut(Waker) -&gt; </span><span>Option</span><span>&lt;T&gt;,
</span><span>{
</span><span>    </span><span>struct </span><span>FromFn&lt;F&gt;(F);
</span><span>
</span><span>    </span><span>impl</span><span>&lt;F, T&gt; Future </span><span>for </span><span>FromFn&lt;F&gt;
</span><span>    </span><span>where
</span><span>        F: FnMut(Waker) -&gt; </span><span>Option</span><span>&lt;T&gt;,
</span><span>    {
</span><span>        </span><span>type </span><span>Output </span><span>=</span><span> T;
</span><span>
</span><span>        </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>            (self.</span><span>0</span><span>)(waker)
</span><span>        }
</span><span>    }
</span><span>
</span><span>    FromFn(f)
</span><span>}
</span></code></pre>
<p>Alright. Let's try reworking the main task to use the new <code>poll_fn</code> helper. We can easily stick the code of the <code>Main::Start</code> state into a closure.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(</span><span>listen</span><span>());
</span><span>    </span><span>SCHEDULER</span><span>.</span><span>run</span><span>();
</span><span>}
</span><span>
</span><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> start </span><span>= </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>        listener.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>        </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>            reactor.</span><span>add</span><span>(listener.</span><span>as_raw_fd</span><span>(), waker);
</span><span>        });
</span><span>
</span><span>        </span><span>Some</span><span>(listener)
</span><span>    });
</span><span>
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>Remember, <code>Main::Start</code> never waits on any I/O, so it's immediately ready with the listener.</p>
<p>We can also use <code>poll_fn</code> to write the <code>Main::Accept</code> future.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> start </span><span>= </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>// ...
</span><span>        </span><span>Some</span><span>(listener)
</span><span>    });
</span><span>
</span><span>    </span><span>let</span><span> accept </span><span>= </span><span>poll_fn</span><span>(|_| </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>        </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>            connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>            </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(Handler {
</span><span>                connection,
</span><span>                state: HandlerState::Start,
</span><span>            });
</span><span>
</span><span>            </span><span>None
</span><span>        }
</span><span>        </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>None</span><span>,
</span><span>        </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>    });
</span><span>}
</span></code></pre>
<p>On the other hand, <code>accept</code> always returns <code>None</code> because we <em>want</em> it to be called every time a new connection comes in. It runs for the entirety of our program.</p>
<p>We have our two task states, now we need a way to chain them together.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>chain</span><span>&lt;F1, F2&gt;(</span><span>future1</span><span>: F1, </span><span>future2</span><span>: F2) -&gt; impl Future&lt;Output = F2::Output&gt;
</span><span>where
</span><span>    F1: Future,
</span><span>    F2: Future
</span><span>{
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>Hm, that doesn't really work.</p>
<p>The second future will need to access the output of the first, the TCP listener.</p>
<p>Instead of chaining the second future directly, we have to chain a closure over the first future's output. That way the closure can use the output of the first future to construct the second.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>chain</span><span>&lt;T1, F, T2&gt;(</span><span>future1</span><span>: T1, </span><span>chain</span><span>: F) -&gt; impl Future&lt;Output = T2::Output&gt;
</span><span>where
</span><span>    T1: Future,
</span><span>    F: FnOnce(T1::Output) -&gt; T2,
</span><span>    T2: Future
</span><span>{
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>That seems better.</p>
<p>We might as well be fancy and have <code>chain</code> be a method on the <code>Future</code> trait. That way we can call <code>.chain</code> as a postfix method on any future.</p>
<pre data-lang="rust"><code data-lang="rust"><span>trait </span><span>Future {
</span><span>    </span><span>// ...
</span><span>
</span><span>    </span><span>fn </span><span>chain</span><span>&lt;F, T&gt;(</span><span>self</span><span>, </span><span>chain</span><span>: F) -&gt; Chain&lt;</span><span>Self</span><span>, F, T&gt;
</span><span>    </span><span>where
</span><span>        F: FnOnce(</span><span>Self::</span><span>Output) -&gt; T,
</span><span>        T: Future,
</span><span>        </span><span>Self</span><span>: Sized,
</span><span>    {
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span><span>
</span><span>enum </span><span>Chain&lt;T1, F, T2&gt; {
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>That looks right, let's try implementing it!</p>
<p>The <code>Chain</code> future is a generalization of our state machines, so it itself is a mini state machine. It starts off by polling the first future, holding onto the transition closure for when it finishes.</p>
<pre data-lang="rust"><code data-lang="rust"><span>enum </span><span>Chain&lt;T1, F, T2&gt; {
</span><span>    First { future1: </span><span>T1</span><span>, transition: F },
</span><span>}
</span><span>
</span><span>impl</span><span>&lt;T1, F, T2&gt; Future </span><span>for </span><span>Chain&lt;T1, F, T2&gt;
</span><span>where
</span><span>    T1: Future,
</span><span>    F: FnOnce(T1::Output) -&gt; T2,
</span><span>    T2: Future,
</span><span>{
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>T2</span><span>::Output;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>        </span><span>if </span><span>let </span><span>Chain::First { future1, transition } </span><span>= </span><span>self {
</span><span>            </span><span>// poll the first future
</span><span>            </span><span>match</span><span> future1.</span><span>poll</span><span>(waker) {
</span><span>                </span><span>// ...
</span><span>            }
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>Once the first future is finished, it constructs the second future using the <code>transition</code> closure, and starts polling it:</p>
<pre data-lang="rust"><code data-lang="rust"><span>enum </span><span>Chain&lt;T1, F, T2&gt; {
</span><span>    First { future1: </span><span>T1</span><span>, transition: F },
</span><span>    Second { future2: </span><span>T2 </span><span>},
</span><span>}
</span><span>
</span><span>impl</span><span>&lt;T1, F, T2&gt; Future </span><span>for </span><span>Chain&lt;T1, F, T2&gt;
</span><span>where
</span><span>    T1: Future,
</span><span>    F: FnOnce(T1::Output) -&gt; T2,
</span><span>    T2: Future,
</span><span>{
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>T2</span><span>::Output;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>        </span><span>if </span><span>let </span><span>Chain::First { future1, transition } </span><span>= </span><span>self {
</span><span>            </span><span>// poll the first future
</span><span>            </span><span>match</span><span> future1.</span><span>poll</span><span>(waker.</span><span>clone</span><span>()) {
</span><span>                </span><span>Some</span><span>(value) </span><span>=&gt; </span><span>{
</span><span>                    </span><span>// first future is done, transition into the second
</span><span>                    </span><span>let</span><span> future2 </span><span>= </span><span>(transition)(value); </span><span>// 👈
</span><span>                    </span><span>*</span><span>self </span><span>= </span><span>Chain::Second { future2 };
</span><span>                }
</span><span>                </span><span>// first future is not ready, return
</span><span>                </span><span>None </span><span>=&gt; return </span><span>None</span><span>,
</span><span>            }
</span><span>        }
</span><span>
</span><span>        </span><span>if </span><span>let </span><span>Chain::Second { future2 } </span><span>= </span><span>self {
</span><span>            </span><span>// first future is already done, poll the second
</span><span>            </span><span>return</span><span> future2.</span><span>poll</span><span>(waker); </span><span>// 👈
</span><span>        }
</span><span>
</span><span>        </span><span>None
</span><span>    }
</span><span>}
</span></code></pre>
<p>Notice how the same <code>waker</code> is used to poll both futures. This means that notifications for both futures will be propagated to the <code>Chain</code> parent future, depending on its state.</p>
<p>Hm... that doesn't actually seem to work:</p>
<pre data-lang="rust"><code data-lang="rust"><span>error[</span><span>E0507</span><span>]: cannot </span><span>move</span><span> out of `</span><span>*</span><span>transition` which is behind a mutable reference
</span><span>   </span><span>-</span><span>-&gt; src</span><span>/</span><span>main.rs:</span><span>182</span><span>:</span><span>33
</span><span>    </span><span>|
</span><span>182 </span><span>|    </span><span>let</span><span> future2 </span><span>= </span><span>(transition)(value);
</span><span>    |                  ^^^^^^^^^^^^ move occurs because `*transition` has type `F`,
</span><span>                                    which does not implement the `</span><span>Copy</span><span>` </span><span>trait
</span></code></pre>
<p>Oh right, <code>transition</code> is an <code>FnOnce</code> closure, meaning it is consumed the first time it is called. We only ever call it once based on our state machine, but the compiler doesn't know that.</p>
<p>We can wrap it in an <code>Option</code> and use <code>take</code> to call it, replacing it with <code>None</code> and allowing us to get an owned value. This is a common pattern when working with state machines.</p>
<pre data-lang="rust"><code data-lang="rust"><span>enum </span><span>Chain&lt;T1, F, T2&gt; {
</span><span>    First { future1: </span><span>T1</span><span>, transition: </span><span>Option</span><span>&lt;F&gt; }, </span><span>// 👈
</span><span>    Second { future2: </span><span>T2 </span><span>},
</span><span>}
</span><span>
</span><span>// ...
</span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>    </span><span>if </span><span>let </span><span>Chain::First { future1, transition } </span><span>= </span><span>self {
</span><span>        </span><span>match</span><span> future1.</span><span>poll</span><span>(waker.</span><span>clone</span><span>()) {
</span><span>            </span><span>Some</span><span>(value) </span><span>=&gt; </span><span>{
</span><span>                </span><span>let</span><span> future2 </span><span>= </span><span>(transition.</span><span>take</span><span>().</span><span>unwrap</span><span>())(value); </span><span>// 👈
</span><span>                </span><span>*</span><span>self </span><span>= </span><span>Chain::future2 { future2 };
</span><span>            }
</span><span>            </span><span>None </span><span>=&gt; return </span><span>None</span><span>,
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>Perfect. Now the <code>chain</code> method can simply construct our <code>Chain</code> future in it's starting state.</p>
<pre data-lang="rust"><code data-lang="rust"><span>trait </span><span>Future {
</span><span>    </span><span>// ...
</span><span>    </span><span>fn </span><span>chain</span><span>&lt;F, T&gt;(</span><span>self</span><span>, </span><span>transition</span><span>: F) -&gt; Chain&lt;</span><span>Self</span><span>, F, T&gt;
</span><span>    </span><span>where
</span><span>        F: FnOnce(</span><span>Self::</span><span>Output) -&gt; T,
</span><span>        T: Future,
</span><span>        </span><span>Self</span><span>: Sized,
</span><span>    {
</span><span>        Chain::First {
</span><span>            future1: self,
</span><span>            transition: </span><span>Some</span><span>(transition),
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>Alright. Where were we... oh right, the main future!</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> start </span><span>= </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>// ...
</span><span>        </span><span>Some</span><span>(listener)
</span><span>    });
</span><span>
</span><span>    </span><span>let</span><span> accept </span><span>= </span><span>poll_fn</span><span>(|_| </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>        </span><span>// ...
</span><span>    });
</span><span>}
</span></code></pre>
<p>We can combine the two futures using our new <code>chain</code> method:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>// ...
</span><span>        </span><span>Some</span><span>(listener)
</span><span>    })
</span><span>    .</span><span>chain</span><span>(|</span><span>listener</span><span>| { </span><span>// 👈
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>// ...
</span><span>        })
</span><span>    })
</span><span>}
</span></code></pre>
<p>Huh, that seems really nice! Gone is our manual state machine, our listen method can now be expressed in terms of simple closures!</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(</span><span>listen</span><span>());
</span><span>    </span><span>SCHEDULER</span><span>.</span><span>run</span><span>();
</span><span>}
</span><span>
</span><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).</span><span>unwrap</span><span>();
</span><span>        </span><span>// ...
</span><span>
</span><span>        </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>            reactor.</span><span>add</span><span>(listener.</span><span>as_raw_fd</span><span>(), waker);
</span><span>        });
</span><span>
</span><span>        </span><span>Some</span><span>(listener)
</span><span>    })
</span><span>    .</span><span>chain</span><span>(|</span><span>listener</span><span>| {
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                </span><span>// ...
</span><span>                </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(Handler {
</span><span>                    connection,
</span><span>                    state: HandlerState::Start,
</span><span>                });
</span><span>
</span><span>                </span><span>None
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>None</span><span>,
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        })
</span><span>    })
</span><span>}
</span></code></pre>
<p>It's too good to be true!</p>
<p>We can convert the connection handler to a closure-based future just like we did with the main one. To start we'll separate it out into a function that returns a <code>Future</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>// ...
</span><span>    })
</span><span>    .</span><span>chain</span><span>(|</span><span>listener</span><span>| {
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                </span><span>// ...
</span><span>                </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(</span><span>handle</span><span>(connection)); </span><span>// 👈
</span><span>                </span><span>None
</span><span>            }
</span><span>            </span><span>// ...
</span><span>        })
</span><span>    })
</span><span>}
</span><span>
</span><span>fn </span><span>handle</span><span>(</span><span>connection</span><span>: TcpStream) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>The first state, <code>HandlerState::Start</code>, is a simple <code>poll_fn</code> closure that registers the connection with the reactor and immediately returns.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>handle</span><span>(</span><span>connection</span><span>: TcpStream) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>        </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>            reactor.</span><span>add</span><span>(connection.</span><span>as_raw_fd</span><span>(), waker);
</span><span>        });
</span><span>
</span><span>        </span><span>Some</span><span>(())
</span><span>    })
</span><span>}
</span></code></pre>
<p>The second state, <code>HandlerState::Read</code>, can be chained on quite easily. It initializes its local request state on the stack and <em>moves</em> it into the future, allowing the future to own its state.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>handle</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>        </span><span>// ...
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>        </span><span>let </span><span>mut</span><span> read </span><span>= </span><span>0</span><span>;
</span><span>        </span><span>let </span><span>mut</span><span> request </span><span>= </span><span>[</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>]; </span><span>// 👈
</span><span>
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>loop </span><span>{
</span><span>                </span><span>// try reading from the stream
</span><span>                </span><span>match</span><span> connection.</span><span>read</span><span>(</span><span>&amp;mut</span><span> request[read</span><span>..</span><span>]) {
</span><span>                    </span><span>Ok</span><span>(</span><span>0</span><span>) </span><span>=&gt; </span><span>{
</span><span>                        println!(</span><span>"client disconnected unexpectedly"</span><span>);
</span><span>                        </span><span>return </span><span>Some</span><span>(());
</span><span>                    }
</span><span>                    </span><span>Ok</span><span>(n) </span><span>=&gt;</span><span> read </span><span>+=</span><span> n,
</span><span>                    </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; return </span><span>None</span><span>,
</span><span>                    </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>                }
</span><span>
</span><span>                </span><span>// did we reach the end of the request?
</span><span>                </span><span>let</span><span> read </span><span>=</span><span> read;
</span><span>                </span><span>if</span><span> read </span><span>&gt;= </span><span>4 </span><span>&amp;&amp; &amp;</span><span>request[read </span><span>- </span><span>4</span><span>..</span><span>read] </span><span>== </span><span>b</span><span>"</span><span>\r\n\r\n</span><span>" </span><span>{
</span><span>                    </span><span>break</span><span>;
</span><span>                }
</span><span>            }
</span><span>
</span><span>            </span><span>// we're done, print the request
</span><span>            </span><span>let</span><span> request </span><span>= </span><span>String</span><span>::from_utf8_lossy(</span><span>&amp;</span><span>request[</span><span>..</span><span>read]);
</span><span>            println!(</span><span>"</span><span>{request}</span><span>"</span><span>);
</span><span>
</span><span>            </span><span>Some</span><span>(())
</span><span>        })
</span><span>    })
</span><span>}
</span></code></pre>
<p><code>HandlerState::Write</code> and <code>HandlerState::Flush</code> can be chained on the same way.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>handle</span><span>(</span><span>connection</span><span>: TcpStream) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>        </span><span>// REACTOR.register...
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>        </span><span>// connection.read...
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>        </span><span>let</span><span> response </span><span>= </span><span>/* ... */</span><span>;
</span><span>        </span><span>let </span><span>mut</span><span> written </span><span>= </span><span>0</span><span>;
</span><span>
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>loop </span><span>{
</span><span>                </span><span>match</span><span> connection.</span><span>write</span><span>(response[written</span><span>..</span><span>].</span><span>as_bytes</span><span>()) {
</span><span>                    </span><span>Ok</span><span>(</span><span>0</span><span>) </span><span>=&gt; </span><span>{
</span><span>                        println!(</span><span>"client disconnected unexpectedly"</span><span>);
</span><span>                        </span><span>return </span><span>Some</span><span>(());
</span><span>                    }
</span><span>                    </span><span>Ok</span><span>(n) </span><span>=&gt;</span><span> written </span><span>+=</span><span> n,
</span><span>                    </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; return </span><span>None</span><span>,
</span><span>                    </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>                }
</span><span>
</span><span>                </span><span>// did we write the whole response yet?
</span><span>                </span><span>if</span><span> written </span><span>==</span><span> response.</span><span>len</span><span>() {
</span><span>                    </span><span>break</span><span>;
</span><span>                }
</span><span>            }
</span><span>
</span><span>            </span><span>Some</span><span>(())
</span><span>        })
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>match</span><span> connection.</span><span>flush</span><span>() {
</span><span>                </span><span>Ok</span><span>(</span><span>_</span><span>) </span><span>=&gt; </span><span>{}
</span><span>                </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{
</span><span>                    </span><span>return </span><span>None</span><span>;
</span><span>                }
</span><span>                </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>            };
</span><span>
</span><span>            </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| reactor.</span><span>remove</span><span>(connection.</span><span>as_raw_fd</span><span>());
</span><span>            </span><span>Some</span><span>(())
</span><span>        })
</span><span>    })
</span><span>}
</span></code></pre>
<p>It's beautiful.</p>
<p>Uhhhh...</p>
<pre data-lang="rust"><code data-lang="rust"><span>error[</span><span>E0382</span><span>]: </span><span>use</span><span> of moved value: `connection`
</span><span>  </span><span>-</span><span>-&gt; src</span><span>/</span><span>main.rs:</span><span>59</span><span>:</span><span>12
</span><span>   </span><span>|
</span><span>51 </span><span>| </span><span>fn </span><span>handle</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>   </span><span>|           -------------- move</span><span> occurs because `connection` has </span><span>type</span><span> `TcpStream`, which does not implement the `</span><span>Copy</span><span>` </span><span>trait
</span><span>52 </span><span>|     </span><span>poll_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>   |             ------------- value moved into closure here
</span><span>53 </span><span>|         </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>54 </span><span>|</span><span>             reactor.</span><span>add</span><span>(connection.</span><span>as_raw_fd</span><span>(), waker);
</span><span>   |                          ---------- variable moved due to use in closure
</span><span>...
</span><span>59 </span><span>|     </span><span>.</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>   |            ^^^^^^^^ value used here after move
</span><span>...
</span><span>66 </span><span>|                 match</span><span> connection.</span><span>read</span><span>(</span><span>&amp;mut</span><span> request[read</span><span>..</span><span>]) {
</span><span>   |                       ---------- use occurs due to use in closure
</span><span>
</span><span>error[</span><span>E0382</span><span>]: </span><span>use</span><span> of moved value: `connection`
</span><span>// ...
</span></code></pre>
<p>Hmm....</p>
<p>All of our futures use <code>move</code> closures, meaning they take ownership of the connection. There can only be one owner of the connection though. Guess they shouldn't be <code>move</code> closures?</p>
<pre data-lang="rust"><code data-lang="rust"><span>error[</span><span>E0373</span><span>]: closure may outlive the current function, but it borrows `connection`, which is owned by the current function
</span><span>   </span><span>-</span><span>-&gt; src</span><span>/</span><span>main.rs:</span><span>52</span><span>:</span><span>13
</span><span>    </span><span>|
</span><span>52  </span><span>|     </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>    |             ^^^^^^^^ may outlive borrowed value `connection`
</span><span>53  </span><span>|         </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>54  </span><span>|</span><span>             reactor.</span><span>add</span><span>(connection.</span><span>as_raw_fd</span><span>(), waker);
</span><span>    |                          ---------- `connection` is borrowed here
</span><span>    </span><span>|
</span><span>note: closure is returned here
</span><span>   </span><span>-</span><span>-&gt; src</span><span>/</span><span>main.rs:</span><span>52</span><span>:</span><span>5
</span><span>    </span><span>|
</span><span>52  </span><span>| /     </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>53  </span><span>| |         </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>54  </span><span>| |</span><span>             reactor.</span><span>add</span><span>(connection.</span><span>as_raw_fd</span><span>(), waker);
</span><span>55  </span><span>| |         </span><span>});
</span><span>...   |
</span><span>128 </span><span>| |         </span><span>})
</span><span>129 </span><span>| |     </span><span>})
</span><span>    </span><span>| |</span><span>______</span><span>^
</span><span>help: to force the closure to take ownership of `connection` (and any other referenced variables), </span><span>use</span><span> the `</span><span>move</span><span>` keyword
</span><span>    </span><span>|
</span><span>52  </span><span>|     </span><span>poll_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>    |             ++++
</span></code></pre>
<p>That doesn't seem to work either. The <code>connection</code> needs to live <em>somewhere</em>. What if we only move it into the first future, and have the rest of the futures borrow it?</p>
<pre data-lang="rust"><code data-lang="rust"><span>error[</span><span>E0382</span><span>]: </span><span>use</span><span> of moved value: `connection`
</span><span>  </span><span>-</span><span>-&gt; src</span><span>/</span><span>main.rs:</span><span>59</span><span>:</span><span>12
</span><span>   </span><span>|
</span><span>51 </span><span>| </span><span>fn </span><span>handle</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>   </span><span>|           -------------- move</span><span> occurs because `connection` has </span><span>type</span><span> `TcpStream`, which does not implement the `</span><span>Copy</span><span>` </span><span>trait
</span><span>52 </span><span>|     </span><span>poll_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>   |             ------------- value moved into closure here
</span><span>53 </span><span>|         </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>54 </span><span>|</span><span>             reactor.</span><span>add</span><span>(connection.</span><span>as_raw_fd</span><span>(), waker);
</span><span>   |                          ---------- variable moved due to use in closure
</span><span>...
</span><span>59 </span><span>|     </span><span>.</span><span>chain</span><span>(|_| {
</span><span>   |            ^^^ value used here after move
</span><span>...
</span><span>66 </span><span>|                 match</span><span> connection.</span><span>read</span><span>(</span><span>&amp;mut</span><span> request[read</span><span>..</span><span>]) {
</span><span>   |                       ---------- use occurs due to use in closure
</span></code></pre>
<p>Nope, that doesn't work either.</p>
<p>Under the hood, our chained futures look something like this. The first future owns the connection, and the rest borrow from it.</p>
<pre data-lang="rust"><code data-lang="rust"><span>enum </span><span>Handle {
</span><span>    Start {
</span><span>        connection: TcpStream,
</span><span>    }
</span><span>    Read {
</span><span>        connection: </span><span>&amp;'???</span><span> TcpStream
</span><span>    }
</span><span>}
</span></code></pre>
<p>Which of course, doesn't make much sense. Once the state transitions into <code>Read</code>, the connection from <code>Start</code> is dropped, and we have nothing to reference.</p>
<p>So how did this work when we were writing futures manually?</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>Handler {
</span><span>    connection: TcpStream,
</span><span>    state: HandlerState,
</span><span>}
</span><span>
</span><span>enum </span><span>HandlerState { </span><span>/* ... */ </span><span>}
</span></code></pre>
<p>Right, the connection lived in the outer struct. Maybe we can write another one of those future helpers that allows us to reference some data stored in an outer future?</p>
<p>Something like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>WithData&lt;D, F&gt; {
</span><span>    data: D,
</span><span>    future: F,
</span><span>}
</span></code></pre>
<p>Seems simple enough. We should be able to construct the future such that it can capture a reference to the data. We can use a closure, just like we did with <code>chain</code>:</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl</span><span>&lt;D, F&gt; WithData&lt;D, F&gt; {
</span><span>    </span><span>pub </span><span>fn </span><span>new</span><span>(</span><span>data</span><span>: D, </span><span>construct</span><span>: impl Fn(&amp;</span><span>D</span><span>) -&gt; F) -&gt; WithData&lt;D, F&gt; {
</span><span>        </span><span>let</span><span> future </span><span>= </span><span>construct</span><span>(</span><span>&amp;</span><span>data);
</span><span>        WithData { data, future }
</span><span>    }
</span><span>}
</span></code></pre>
<p><code>WithData</code> can implement <code>Future</code> by simply delegating to the inner future:</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl</span><span>&lt;D, F&gt; Future </span><span>for </span><span>WithData&lt;D, F&gt;
</span><span>where
</span><span>    F: Future,
</span><span>{
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>F::Output;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>        self.future.</span><span>poll</span><span>(waker)
</span><span>    }
</span><span>}
</span></code></pre>
<p>Now we should be able to wrap our future in <code>WithData</code>, giving <code>connection</code> a place to live even after it is returned.. and everything should work!</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>handle</span><span>(</span><span>connection</span><span>: TcpStream) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    WithData::new(connection, |</span><span>connection</span><span>| {
</span><span>        </span><span>from_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>            </span><span>// ...
</span><span>        })
</span><span>        .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>let </span><span>mut</span><span> request </span><span>= </span><span>[</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>];
</span><span>            </span><span>let </span><span>mut</span><span> read </span><span>= </span><span>0</span><span>;
</span><span>
</span><span>            </span><span>from_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>                </span><span>// ...
</span><span>            })
</span><span>        })
</span><span>        .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>let</span><span> response </span><span>= </span><span>/* ... */</span><span>;
</span><span>            </span><span>let </span><span>mut</span><span> written </span><span>= </span><span>0</span><span>;
</span><span>
</span><span>            </span><span>from_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>                </span><span>// ...
</span><span>            })
</span><span>        })
</span><span>        .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>from_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>                </span><span>// ...
</span><span>            })
</span><span>        })
</span><span>    })
</span><span>}
</span></code></pre>
<p>A little funky, but if it works...</p>
<pre data-lang="rust"><code data-lang="rust"><span>error: lifetime may not live long enough
</span><span>   </span><span>-</span><span>-&gt; src</span><span>/</span><span>main.rs:</span><span>53</span><span>:</span><span>9
</span><span>    </span><span>|
</span><span>52  </span><span>|       </span><span>WithData::new(connection, |</span><span>connection</span><span>| {
</span><span>    |                                  ----------- return type of closure `Chain&lt;Chain&lt;Chain&lt;impl Future&lt;Output = ()&gt;, [closure@src/bin/play.</span><span>rs</span><span>:60:16: 86:10], impl Future&lt;Output = ()&gt;&gt;, [closure@src/bin/play.</span><span>rs</span><span>:87:16: 113:10], impl Future&lt;Output = ()&gt;&gt;, [closure@src/bin/play.</span><span>rs</span><span>:114:16: 130:10], impl Future&lt;Output = ()&gt;&gt;` contains a lifetime `'2`
</span><span>    </span><span>|                                  |
</span><span>    </span><span>|</span><span>                                  has </span><span>type</span><span> `</span><span>&amp;'</span><span>1</span><span> TcpStream`
</span><span>53  </span><span>| /         </span><span>from_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>54  </span><span>| |             </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>55  </span><span>| |</span><span>                 reactor.</span><span>add</span><span>(connection.</span><span>as_raw_fd</span><span>(), waker);
</span><span>56  </span><span>| |             </span><span>});
</span><span>...   |
</span><span>129 </span><span>| |             </span><span>})
</span><span>130 </span><span>| |         </span><span>})
</span><span>    </span><span>| |</span><span>__________</span><span>^</span><span> returning this value requires that `</span><span>'</span><span>1</span><span>` must outlive `</span><span>'</span><span>2</span><span>`
</span></code></pre>
<p>It couldn't be that easy could it.</p>
<p>What a weird error message too.</p>
<blockquote>
<pre><code><span>return type of closure `Chain&lt;Chain&lt;Chain&lt;impl Future&lt;Output = ()&gt;, [closure
</span><span>@src/bin/play.rs:60:16: 86:10], impl Future&lt;Output = ()&gt;&gt;, [closure@src/bin/
</span><span>play.rs:87:16: 113: 10], impl Future&lt;Output = ()&gt;&gt;, [closure@src/bin/play.rs
</span><span>:114:16: 130:10], impl Future&lt;Output = ()&gt;&gt;` contains a lifetime `'2`
</span></code></pre>
</blockquote>
<p>Alright, so, the giant chained future we pass to <code>WithData</code> contains a reference to the connection.</p>
<p>That's what we want, for the future to borrow the connection, right?</p>
<blockquote>
<pre><code><span>`connection` has type `&amp;'1 TcpStream` ... returning this value requires
</span><span>that `'1` must outlive `'2`
</span></code></pre>
</blockquote>
<p>Hmmm, nowhere in our <code>WithData</code> struct did we actually specify that the future borrows from the data. It seems Rust can't figure out the lifetimes without that. So.. we should probably add a lifetime to <code>WithData</code> right?</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>WithData&lt;</span><span>'data</span><span>, D, F&gt; {
</span><span>    data: D,
</span><span>    future: F,
</span><span>}
</span></code></pre>
<pre data-lang="rust"><code data-lang="rust"><span>error[</span><span>E0392</span><span>]: parameter `</span><span>'data</span><span>` is never used
</span><span>   </span><span>-</span><span>-&gt; src</span><span>/</span><span>bin</span><span>/</span><span>play.rs:</span><span>160</span><span>:</span><span>17
</span><span>    </span><span>|
</span><span>160 </span><span>| </span><span>struct </span><span>WithData&lt;</span><span>'data</span><span>, D, F&gt; {
</span><span>    |                 ^^^^^ unused parameter
</span><span>    |
</span><span>    = help: consider removing `</span><span>'data</span><span>`, referring to it in a field, or using a marker such as `PhantomData`
</span></code></pre>
<p>Adding <code>PhantomData</code> seems like an easy fix.</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>WithData&lt;</span><span>'data</span><span>, D, F&gt; {
</span><span>    data: D,
</span><span>    future: F,
</span><span>    _data: PhantomData&lt;</span><span>&amp;'data</span><span> D&gt;,
</span><span>}
</span></code></pre>
<p>The future does reference <code>&amp;'data D</code>, so that sort of makes sense. Now in the constructor, we should say that the future borrows from the data:</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl</span><span>&lt;</span><span>'data</span><span>, D, F&gt; WithData&lt;</span><span>'data</span><span>, D, F&gt;
</span><span>where
</span><span>    F: Future + </span><span>'data</span><span>, </span><span>// 👈
</span><span>{
</span><span>    </span><span>pub </span><span>fn </span><span>new</span><span>(
</span><span>        </span><span>data</span><span>: D,
</span><span>        </span><span>construct</span><span>: impl Fn(&amp;'</span><span>data D</span><span>) -&gt; F, </span><span>// 👈
</span><span>    ) -&gt; WithData&lt;</span><span>'data</span><span>, D, F&gt; {
</span><span>        </span><span>let</span><span> future </span><span>= </span><span>construct</span><span>(</span><span>&amp;</span><span>data);
</span><span>
</span><span>        WithData {
</span><span>            data,
</span><span>            future,
</span><span>            _data: PhantomData,
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>And that should work, right? All the lifetimes are written out and make sense:</p>
<pre data-lang="rust"><code data-lang="rust"><span>error[</span><span>E0597</span><span>]: `data` does not live long enough
</span><span>   </span><span>-</span><span>-&gt; src</span><span>/</span><span>bin</span><span>/</span><span>play.rs:</span><span>172</span><span>:</span><span>30
</span><span>    </span><span>|
</span><span>167 </span><span>| </span><span>impl</span><span>&lt;</span><span>'data</span><span>, D, F&gt; WithData&lt;</span><span>'data</span><span>, D, F&gt;
</span><span>    |      ----- lifetime `</span><span>'data</span><span>` defined here
</span><span>...
</span><span>172 |         let future = construct(</span><span>&amp;</span><span>data);
</span><span>    |                      ----------^^^^^-
</span><span>    |                      |         |
</span><span>    |                      |         borrowed value does not live long enough
</span><span>    |                      argument requires that `data` is borrowed for `</span><span>'data</span><span>`
</span><span>...
</span><span>178 |     }
</span><span>    |     - `data` dropped here while still borrowed
</span><span>
</span><span>error[E0505]: cannot move out of `data` because it is borrowed
</span><span>   --&gt; src/bin/play.rs:174:13
</span><span>    |
</span><span>167 | impl&lt;</span><span>'data</span><span>, D, F&gt; WithData&lt;</span><span>'data</span><span>, D, F&gt;
</span><span>    |      ----- lifetime `</span><span>'data</span><span>` defined here
</span><span>...
</span><span>172 |         let future = construct(</span><span>&amp;</span><span>data);
</span><span>    |                      ----------------
</span><span>    |                      |         |
</span><span>    |                      |         borrow of `data` occurs here
</span><span>    |                      argument requires that `data` is borrowed for `</span><span>'data</span><span>`
</span><span>173 |         WithData {
</span><span>174 </span><span>|</span><span>             data,
</span><span>    |             ^^^^ move out of `data` occurs here
</span></code></pre>
<p>Or... not.</p>
<p>Why doesn't this work?</p>
<blockquote>
<pre><code><span>`data` dropped here while still borrowed
</span></code></pre>
</blockquote>
<p>Wait a minute, that's the same error message we got when we removed <code>move</code> from our future closures?! But the data <em>does</em> have a place to live now... doesn't it?</p>
<p>Hmm.. actually, the second error is telling us that moving <code>data</code> is wrong too:</p>
<blockquote>
<pre><code><span>cannot move out of `data` because it is borrowed
</span></code></pre>
</blockquote>
<p>That... actually makes sense. The future we construct borrows the <code>data</code> that lives on the stack. Once we move it, it's <strong>no longer in the same place on the stack</strong>. Its address changes, so the future's reference to the data is actually invalidated.</p>
<pre><code><span>## Before Moving
</span><span>
</span><span>       ┌─────────────┐
</span><span>       │0101001010010│
</span><span>data:  │001...       │ ◄──────── &amp;future.data
</span><span>       │             │
</span><span>       │             │
</span><span>       └─────────────┘
</span><span>
</span><span>
</span><span>## After Moving
</span><span>
</span><span>             ???       ◄──────── &amp;future.data
</span><span>
</span><span>
</span><span>       ┌─────────────┐
</span><span>       │0101001010010│
</span><span>data:  │001...       │
</span><span>       │             │
</span><span>       │             │
</span><span>       └─────────────┘
</span></code></pre>
<p>We gave the data a place to live, but we didn't give it a <em>stable</em> place to live. It turns out, this is a well-known problem in Rust. What we're trying to create is called a <em>self-referential struct</em>, and it's not possible to do safely.</p>
<p>Back when our entire future state was in the <code>Handler</code> struct, there was no self-referencing going on. Everything just worked off the <code>Handler</code>. But now that we're trying to split our futures up into subtasks, we need a way for them to access the data independently.</p>
<p>So is it not possible?</p>
<p>Well...</p>
<p>We <em>could</em> allocate the data l using <code>Rc</code> and clone the pointer into each of the futures. That way the futures get a stable pointer to the data on the heap, and it's only deallocated after all the futures complete.</p>
<p>The code is about to get pretty ugly.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>handle</span><span>(</span><span>connection</span><span>: TcpStream) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> connection </span><span>= </span><span>Rc::new(connection); </span><span>// 👈
</span><span>    </span><span>let</span><span> read_connection_ref </span><span>=</span><span> connection.</span><span>clone</span><span>();
</span><span>    </span><span>let</span><span> write_connection_ref </span><span>=</span><span> connection.</span><span>clone</span><span>();
</span><span>    </span><span>let</span><span> flush_connection_ref </span><span>=</span><span> connection.</span><span>clone</span><span>();
</span><span>
</span><span>    </span><span>poll_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>        </span><span>// ...
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>        </span><span>// ...
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>let</span><span> connection </span><span>= &amp;*</span><span>read_connection_ref;
</span><span>
</span><span>            </span><span>loop </span><span>{
</span><span>                </span><span>match </span><span>(</span><span>&amp;mut</span><span> connection).</span><span>read</span><span>(</span><span>&amp;mut</span><span> request) {
</span><span>                    </span><span>// ...
</span><span>                }
</span><span>            }
</span><span>            </span><span>// ...
</span><span>        })
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>        </span><span>// ...
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>let</span><span> connection </span><span>= &amp;*</span><span>write_connection_ref;
</span><span>            </span><span>// ...
</span><span>        })
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>let</span><span> connection </span><span>= &amp;*</span><span>flush_connection_ref;
</span><span>            </span><span>// ...
</span><span>        })
</span><span>    })
</span><span>}
</span></code></pre>
<p>Oh no..</p>
<pre><code><span>error[E0277]: `Rc&lt;TcpStream&gt;` cannot be sent between threads safely
</span><span>   --&gt; src/main.rs:90:37
</span><span>    |
</span><span>90  |     SCHEDULER.spawn(handle(connection));
</span><span>    |               ----- ^^^^^^^^^^^^^^^^^^ `Rc&lt;TcpStream&gt;` cannot be sent between threads safely
</span><span>    |               |
</span><span>    |               required by a bound introduced by this call
</span><span>...
</span><span>100 |     fn handle(mut connection: TcpStream) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    |                                             ------------------------ within this `impl Future&lt;Output = ()&gt;`
</span></code></pre>
<p>Using <code>Rc</code> in our handler makes it <code>!Send</code>. Even though the connection is only used internally within the future and futures are only ever run by the main thread, we need to use an <code>Arc</code> to satisfy the compiler.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>handle</span><span>(</span><span>connection</span><span>: TcpStream) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> connection </span><span>= </span><span>Arc::new(connection); </span><span>// 👈
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>A little sad, but at least it compiles.</p>
<p>Our server has no more manual state machines and is looking pretty clean.</p>
<p>A <em>lot</em> cleaner than when we started with epoll manually, even with the messy <code>Arc</code> business.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(</span><span>listen</span><span>());
</span><span>    </span><span>SCHEDULER</span><span>.</span><span>run</span><span>();
</span><span>}
</span><span>
</span><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>        </span><span>// ...
</span><span>
</span><span>        </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>            reactor.</span><span>add</span><span>(listener.</span><span>as_raw_fd</span><span>(), waker);
</span><span>        });
</span><span>
</span><span>        </span><span>Some</span><span>(listener)
</span><span>    })
</span><span>    .</span><span>chain</span><span>(|</span><span>listener</span><span>| {
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                </span><span>// ...
</span><span>                </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(</span><span>handle</span><span>(connection));
</span><span>                </span><span>None
</span><span>            }
</span><span>            </span><span>// ...
</span><span>        })
</span><span>    })
</span><span>}
</span><span>
</span><span>fn </span><span>handle</span><span>(</span><span>connection</span><span>: TcpStream) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> connection </span><span>= </span><span>Arc::new(connection);
</span><span>    </span><span>let</span><span> read_connection_ref </span><span>=</span><span> connection.</span><span>clone</span><span>();
</span><span>    </span><span>let</span><span> write_connection_ref </span><span>=</span><span> connection.</span><span>clone</span><span>();
</span><span>    </span><span>let</span><span> flush_connection_ref </span><span>=</span><span> connection.</span><span>clone</span><span>();
</span><span>
</span><span>    </span><span>poll_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>        </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>            reactor.</span><span>add</span><span>(connection.</span><span>as_raw_fd</span><span>(), waker);
</span><span>        });
</span><span>
</span><span>        </span><span>Some</span><span>(())
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>        </span><span>let </span><span>mut</span><span> request </span><span>= </span><span>[</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>];
</span><span>        </span><span>let </span><span>mut</span><span> read </span><span>= </span><span>0</span><span>;
</span><span>
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>// ...
</span><span>        })
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>        </span><span>let</span><span> response </span><span>= </span><span>/* ... */</span><span>;
</span><span>        </span><span>let </span><span>mut</span><span> written </span><span>= </span><span>0</span><span>;
</span><span>
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>// ...
</span><span>        })
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>// ...
</span><span>            </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>                reactor.</span><span>remove</span><span>(flush_connection_ref.</span><span>as_raw_fd</span><span>());
</span><span>            });
</span><span>
</span><span>            </span><span>Some</span><span>(())
</span><span>        })
</span><span>    })
</span><span>}
</span></code></pre>
<p>And of course...</p>
<pre data-lang="sh"><code data-lang="sh"><span>$ curl localhost:3000
</span><span># =&gt; Hello world!
</span></code></pre>
<p>It works!</p>
<h2 id="a-graceful-server"><a href="#a-graceful-server" aria-label="Anchor link for: a-graceful-server">A Graceful Server</a></h2>
<p>Whew, that was a lot.</p>
<p>One last thing before we finish. To put our task model to the test, we can finally implement the graceful shutdown mechanism we discussed earlier.</p>
<blockquote>
<p>Imagine we wanted to implement graceful shutdown for our server. When someone hits the keys ctrl+c, instead of killing the program abruptly, we should stop accepting new connections and wait for any active requests to complete. Any requests that take more than 30 seconds to handle are killed as the server exits.</p>
</blockquote>
<p>There are a couple things we have to do to set this up. Firstly, we have to actually detect the signal. On Linux, ctrl+c triggers the <code>SIGINT</code> signal, so we can use the <code>signal_hook</code> crate to wait until the signal is received.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>signal_hook::consts::signal::</span><span>SIGINT</span><span>;
</span><span>use </span><span>signal_hook::iterator::Signals;
</span><span>
</span><span>fn </span><span>ctrl_c</span><span>() {
</span><span>    </span><span>let </span><span>mut</span><span> signal </span><span>= </span><span>Signals::new(</span><span>&amp;</span><span>[</span><span>SIGINT</span><span>]).</span><span>unwrap</span><span>();
</span><span>    </span><span>let</span><span> _ctrl_c </span><span>=</span><span> signal.</span><span>forever</span><span>().</span><span>next</span><span>().</span><span>unwrap</span><span>();
</span><span>}
</span></code></pre>
<p>There's a problem though. <code>forever().next()</code> blocks the thread until the signal is received. Now that our server is async, that means calling <code>ctrl_c()</code> on the main thread will block the entire program.</p>
<p>Instead, we need to represent the ctrl+c signal as a <em>future</em> that resolves when it is received. Something like this.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>ctrl_c</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>        </span><span>// ...
</span><span>    })
</span><span>}
</span></code></pre>
<p>So how do we listen for the signal asynchronously?</p>
<p>We could register a signal handler with epoll, but we could also use this as an opportunity to learn about handling blocking tasks in an async program. There will be times when the only way to get what you want is through a blocking API, but you can't simply call it on the main thread. Instead, you can run the blocking work on a separate thread, and notify the main thread when it completes.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>spawn_blocking</span><span>(</span><span>blocking_work</span><span>: impl FnOnce()) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>// run the blocking work on a separate thread
</span><span>    std::thread::spawn(</span><span>move || </span><span>{
</span><span>        </span><span>blocking_work</span><span>();
</span><span>    });
</span><span>
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>// ???
</span><span>    }))
</span><span>}
</span></code></pre>
<p>The question is, how do we know when the work is done?</p>
<p>The blocking work is run on a separate thread, outside of the future. It needs access to the waker so it can notify the future when it completes. We only get access to the waker when the future is first polled, so the state needs to start out as <code>None</code>.</p>
<p>We also need a flag that tells the future that the work has completed, in case the work completes before the future is even polled.</p>
<p>These two pieces of state can be stored inside a <code>Mutex</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>let</span><span> state: Arc&lt;Mutex&lt;(</span><span>bool</span><span>, </span><span>Option</span><span>&lt;Waker&gt;)&gt;&gt; </span><span>= </span><span>Arc::default();
</span></code></pre>
<p>Once the thread completes the work, it must set the flag to true and call <code>wake</code> if a waker has been stored. It's fine if a waker hasn't been stored yet, the future will see the flag when it's first polled and return immediately.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>spawn_blocking</span><span>(</span><span>blocking_work</span><span>: impl FnOnce() + Send + </span><span>'static</span><span>) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> state: Arc&lt;Mutex&lt;(</span><span>bool</span><span>, </span><span>Option</span><span>&lt;Waker&gt;)&gt;&gt; </span><span>= </span><span>Arc::default();
</span><span>    </span><span>let</span><span> state_handle </span><span>=</span><span> state.</span><span>clone</span><span>();
</span><span>
</span><span>    </span><span>// run the blocking work on a separate thread
</span><span>    std::thread::spawn(</span><span>move || </span><span>{
</span><span>        </span><span>// run the work
</span><span>        </span><span>blocking_work</span><span>();
</span><span>
</span><span>        </span><span>// mark the task as done
</span><span>        </span><span>let </span><span>(done, waker) </span><span>= &amp;mut *</span><span>state_handle.</span><span>lock</span><span>().</span><span>unwrap</span><span>();
</span><span>        </span><span>*</span><span>done </span><span>= </span><span>true</span><span>;
</span><span>
</span><span>        </span><span>// wake the waker
</span><span>        </span><span>if </span><span>let Some</span><span>(waker) </span><span>=</span><span> waker.</span><span>take</span><span>() {
</span><span>            waker.</span><span>wake</span><span>();
</span><span>        }
</span><span>    });
</span><span>
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>// ...
</span><span>    }))
</span><span>}
</span></code></pre>
<p>Now the future needs to access the state and check if the work has completed yet. If not, it stores its waker and returns <code>None</code>, to be woken later when the work does complete.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>spawn_blocking</span><span>(</span><span>blocking_work</span><span>: impl FnOnce() + Send + </span><span>'static</span><span>) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> state: Arc&lt;Mutex&lt;(</span><span>bool</span><span>, </span><span>Option</span><span>&lt;Waker&gt;)&gt;&gt; </span><span>= </span><span>Arc::default();
</span><span>    </span><span>let</span><span> state_handle </span><span>=</span><span> state.</span><span>clone</span><span>();
</span><span>
</span><span>    </span><span>// run the blocking work on a separate thread
</span><span>    std::thread::spawn(</span><span>move || </span><span>{
</span><span>        </span><span>// ...
</span><span>    });
</span><span>
</span><span>    </span><span>poll_fn</span><span>(</span><span>move |</span><span>waker</span><span>| match &amp;mut *</span><span>state.</span><span>lock</span><span>().</span><span>unwrap</span><span>() {
</span><span>        </span><span>// work is not completed, store our waker and come back later
</span><span>        (</span><span>false</span><span>, state) </span><span>=&gt; </span><span>{
</span><span>            </span><span>*</span><span>state </span><span>= </span><span>Some</span><span>(waker);
</span><span>            </span><span>None
</span><span>        }
</span><span>        </span><span>// the work is completed
</span><span>        (</span><span>true</span><span>, </span><span>_</span><span>) </span><span>=&gt; </span><span>Some</span><span>(()),
</span><span>    })
</span><span>
</span><span>}
</span></code></pre>
<p>The future returned by <code>spawn_blocking</code> serves as an asynchronous version of <code>JoinHandle</code>. We can wait asynchronously on the main thread while the blocking work is run on a separate thread.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>ctrl_c</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>spawn_blocking</span><span>(|| {
</span><span>        </span><span>let </span><span>mut</span><span> signal </span><span>= </span><span>Signals::new(</span><span>&amp;</span><span>[</span><span>SIGINT</span><span>]).</span><span>unwrap</span><span>();
</span><span>        </span><span>let</span><span> _ctrl_c </span><span>=</span><span> signal.</span><span>forever</span><span>().</span><span>next</span><span>().</span><span>unwrap</span><span>();
</span><span>    })
</span><span>}
</span></code></pre>
<p><code>spawn_blocking</code> is an extremely convenient abstraction for dealing with blocking APIs in an async program.</p>
<p>Alright, we now have a future that waits for the ctrl+c signal!</p>
<p>If you remember from back when our server used blocking I/O, we wondered how to watch for the signal in a way that aborts the connection listener loop immediately after the signal arrives. We realized we needed some way to listen for both incoming connections, and the ctrl+c signal, <em>at the same time</em>.</p>
<p>Because <code>accept</code> was blocking, it wasn't that simple. But with futures, it's actually possible!</p>
<p>We can implement this as another future wrapper. Given two futures, we should be able to create a wrapper future that selects between either of the future's outputs, depending on which future completed first.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>select</span><span>&lt;L, R&gt;(</span><span>left</span><span>: L, </span><span>right</span><span>: R) -&gt; Select&lt;L, R&gt; {
</span><span>    Select { left, right }
</span><span>}
</span><span>
</span><span>struct </span><span>Select&lt;L, R&gt; {
</span><span>    left: L,
</span><span>    right: R
</span><span>}
</span><span>
</span><span>enum </span><span>Either&lt;L, R&gt; {
</span><span>    Left(L),
</span><span>    Right(R)
</span><span>}
</span><span>
</span><span>impl</span><span>&lt;L, R&gt; Future </span><span>for </span><span>Select&lt;L, R&gt; {
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>Either&lt;L, R&gt;;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Self::</span><span>Output {
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span></code></pre>
<p>It turns out that the implementation of the select future is really simple. We just attempt to poll both futures and return when the first one resolves.</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl</span><span>&lt;L, R&gt; Future </span><span>for </span><span>Select&lt;L, R&gt; {
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>Either&lt;</span><span>L::</span><span>Output, </span><span>R::</span><span>Output&gt;;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>        </span><span>if </span><span>let Some</span><span>(output) </span><span>= </span><span>self.left.</span><span>poll</span><span>(waker.</span><span>clone</span><span>()) {
</span><span>            </span><span>return </span><span>Some</span><span>(Either::Left(output));
</span><span>        }
</span><span>
</span><span>        </span><span>if </span><span>let Some</span><span>(output) </span><span>= </span><span>self.right.</span><span>poll</span><span>(waker) {
</span><span>            </span><span>return </span><span>Some</span><span>(Either::Right(output));
</span><span>        }
</span><span>
</span><span>        </span><span>None
</span><span>    }
</span><span>}
</span></code></pre>
<p>Because we pass the same waker to both futures, any progress in either future will notify us, and we can check if either of them completed.</p>
<p>It really is that simple.</p>
<p>Now back to our main program.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).</span><span>unwrap</span><span>();
</span><span>        </span><span>// ...
</span><span>    })
</span><span>    .</span><span>chain</span><span>(|</span><span>listener</span><span>| {
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                </span><span>// ...
</span><span>                </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(</span><span>handle</span><span>(connection));
</span><span>                </span><span>None
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>None</span><span>,
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        })
</span><span>    })
</span><span>}
</span></code></pre>
<p>We can combine the TCP listener task with the ctrl+c listener using our new <code>select</code> combinator. This way we can listen for both at the same time:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>// ...
</span><span>    })
</span><span>    .</span><span>chain</span><span>(|</span><span>listener</span><span>| {
</span><span>        </span><span>let</span><span> listen </span><span>= </span><span>poll_fn</span><span>(</span><span>move |_| match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                </span><span>// ...
</span><span>                </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(</span><span>handle</span><span>(connection));
</span><span>                </span><span>None
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>None</span><span>,
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        });
</span><span>
</span><span>        </span><span>select</span><span>(listen, </span><span>ctrl_c</span><span>())
</span><span>    })
</span><span>}
</span></code></pre>
<p>The TCP listener task never resolves — remember, it represents the loop from our synchronous server. <code>ctrl_c()</code> can resolve though, so we need to chain on another task to handle the shutdown signal.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>// ...
</span><span>    })
</span><span>    .</span><span>chain</span><span>(|</span><span>listener</span><span>| {
</span><span>        </span><span>let</span><span> listen </span><span>= </span><span>/* ... */</span><span>;
</span><span>        </span><span>select</span><span>(listen, </span><span>ctrl_c</span><span>())
</span><span>    })
</span><span>    .</span><span>chain</span><span>(|</span><span>_ctrl_c</span><span>| </span><span>graceful_shutdown</span><span>())
</span><span>}
</span><span>
</span><span>fn </span><span>graceful_shutdown</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>Now we need to implement the rest of our shutdown logic. Once the shutdown signal is received, we wait at most thirty seconds for any active requests to complete before shutting down.</p>
<p>This sounds like another use case for <code>select</code>! Either thirty seconds elapse, or all active requests complete.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>graceful_shutdown</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> timer </span><span>= </span><span>/* ... */</span><span>;
</span><span>    </span><span>let</span><span> request_counter </span><span>= </span><span>/* ... */</span><span>;
</span><span>
</span><span>    </span><span>select</span><span>(timer, request_counter).</span><span>chain</span><span>(|_| {
</span><span>        </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>            </span><span>// graceful shutdown process complete, now we actually exit
</span><span>            println!(</span><span>"Graceful shutdown complete"</span><span>);
</span><span>            std::process::exit(</span><span>0</span><span>)
</span><span>        })
</span><span>    })
</span><span>}
</span></code></pre>
<p>All we need to do now is create the two futures for our shutdown conditions.</p>
<p>First we need a timer. Of course, we can't simply call <code>thread::sleep</code> because it's a blocking function. But we <em>could</em> run it through <code>spawn_blocking</code>, and use the handle to represent our timer future.</p>
<blockquote>
<p>Note that there <em>are</em> ways to build async timers around epoll, but that's out of scope for this article.</p>
</blockquote>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>std::thread;
</span><span>use </span><span>std::time::Duration;
</span><span>
</span><span>fn </span><span>graceful_shutdown</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> timer </span><span>= </span><span>spawn_blocking</span><span>(|| thread::sleep(Duration::from_secs(</span><span>30</span><span>)));
</span><span>    </span><span>let</span><span> request_counter </span><span>= </span><span>/* ... */</span><span>;
</span><span>
</span><span>    </span><span>select</span><span>(timer, request_counter).</span><span>chain</span><span>(|_| {
</span><span>        </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>            </span><span>// graceful shutdown process complete, now we actually exit
</span><span>            println!(</span><span>"Graceful shutdown complete"</span><span>);
</span><span>            std::process::exit(</span><span>0</span><span>)
</span><span>        })
</span><span>    })
</span><span>}
</span></code></pre>
<p>That was simple enough.</p>
<p>Now for the main shutdown condition. For us to know when all active requests are completed, we'll need a counter for active requests.</p>
<p>We can keep the counter local to our <code>listen</code> future, and increment/decrement it whenever tasks are spawned, or complete.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> tasks </span><span>= </span><span>Arc::new(Mutex::new(</span><span>0</span><span>));
</span><span>
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>// ...
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |</span><span>listener</span><span>| </span><span>{
</span><span>        </span><span>let</span><span> listen </span><span>= </span><span>poll_fn</span><span>(</span><span>move |_| match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                </span><span>// increment the counter
</span><span>                </span><span>*</span><span>tasks.</span><span>lock</span><span>().</span><span>unwrap</span><span>() </span><span>+= </span><span>1</span><span>; </span><span>// 👈
</span><span>
</span><span>                </span><span>let</span><span> handle_connection </span><span>= </span><span>handle</span><span>(connection).</span><span>chain</span><span>(|_| {
</span><span>                    </span><span>poll_fn</span><span>(|_| {
</span><span>                        </span><span>// decrement the counter
</span><span>                        </span><span>*</span><span>tasks.</span><span>lock</span><span>().</span><span>unwrap</span><span>() </span><span>-= </span><span>1</span><span>; </span><span>// 👈
</span><span>                        </span><span>Some</span><span>(())
</span><span>                    })
</span><span>                });
</span><span>
</span><span>                </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(handle_connection);
</span><span>                </span><span>None
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>None</span><span>,
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        });
</span><span>
</span><span>        </span><span>select</span><span>(listen, </span><span>ctrl_c</span><span>())
</span><span>    })
</span><span>    .</span><span>chain</span><span>(|</span><span>_ctrl_c</span><span>| </span><span>graceful_shutdown</span><span>())
</span><span>}
</span></code></pre>
<p>Notice how the decrement of the counter is chained onto the connection handler, so the decrement actually happens from within the spawned task, after it completes.</p>
<p>A task counter is great, but we need a little more than that. We can't just check for <code>tasks == 0</code> in a loop, we need the shutdown handler to be <em>notified</em> when the last task completes.</p>
<p>And for that, the connection handler that completes needs access to the shutdown handler's waker.</p>
<p>What we need is similar to the <code>spawn_blocking</code> solution we created earlier, except instead of a boolean flag, we need a counter. We can wrap up all this state into a small struct.</p>
<pre data-lang="rust"><code data-lang="rust"><span>#[derive(Default)]
</span><span>struct </span><span>Counter {
</span><span>    state: Mutex&lt;(</span><span>usize</span><span>, </span><span>Option</span><span>&lt;Waker&gt;)&gt;,
</span><span>}
</span><span>
</span><span>impl </span><span>Counter {
</span><span>    </span><span>fn </span><span>increment</span><span>(</span><span>&amp;</span><span>self</span><span>) {
</span><span>        </span><span>let </span><span>(count, </span><span>_</span><span>) </span><span>= &amp;mut *</span><span>self.state.</span><span>lock</span><span>().</span><span>unwrap</span><span>();
</span><span>        </span><span>*</span><span>count </span><span>+= </span><span>1</span><span>;
</span><span>    }
</span><span>
</span><span>    </span><span>fn </span><span>decrement</span><span>(</span><span>&amp;</span><span>self</span><span>) {
</span><span>        </span><span>let </span><span>(count, waker) </span><span>= &amp;mut *</span><span>self.state.</span><span>lock</span><span>().</span><span>unwrap</span><span>();
</span><span>        </span><span>*</span><span>count </span><span>-= </span><span>1</span><span>;
</span><span>
</span><span>        </span><span>// we were the last task
</span><span>        </span><span>if *</span><span>count </span><span>== </span><span>0 </span><span>{
</span><span>            </span><span>// wake the waiting task
</span><span>            </span><span>if </span><span>let Some</span><span>(waker) </span><span>=</span><span> waker.</span><span>take</span><span>() {
</span><span>                waker.</span><span>wake</span><span>();
</span><span>            }
</span><span>        }
</span><span>    }
</span><span>
</span><span>
</span><span>    </span><span>fn </span><span>wait_for_zero</span><span>(</span><span>self</span><span>: Arc&lt;</span><span>Self</span><span>&gt;) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>            </span><span>match &amp;mut *</span><span>self.state.</span><span>lock</span><span>().</span><span>unwrap</span><span>() {
</span><span>                </span><span>// work is completed
</span><span>                (</span><span>0</span><span>, </span><span>_</span><span>) </span><span>=&gt; </span><span>Some</span><span>(()),
</span><span>                </span><span>// work is not completed, store our waker and come back later
</span><span>                (</span><span>_</span><span>, state) </span><span>=&gt; </span><span>{
</span><span>                    </span><span>*</span><span>state </span><span>= </span><span>Some</span><span>(waker);
</span><span>                    </span><span>None
</span><span>                }
</span><span>            }
</span><span>        })
</span><span>    }
</span><span>}
</span></code></pre>
<p>When <code>wait_for_zero</code> is first called it stores its waker in the counter state before returning. Now the task that calls <code>decrement</code> and sees that it was the last active task can simply call <code>wake</code>, notifying the caller of <code>wait_for_zero</code>.</p>
<p>When the shutdown handler is woken, it will see that the counter is at zero and shut down the program.</p>
<p>Now we can replace our manual counter with the <code>Counter</code> object.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> tasks </span><span>= </span><span>Arc::new(Counter::default()); </span><span>// 👈
</span><span>    </span><span>let</span><span> tasks_ref </span><span>= </span><span>Arc::new(Counter::default());
</span><span>
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>// ...
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |</span><span>listener</span><span>| </span><span>{
</span><span>        </span><span>let</span><span> listen </span><span>= </span><span>poll_fn</span><span>(</span><span>move |_| match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>                </span><span>// increment the counter
</span><span>                tasks.</span><span>increment</span><span>(); </span><span>// 👈
</span><span>
</span><span>                </span><span>let</span><span> tasks </span><span>=</span><span> tasks.</span><span>clone</span><span>();
</span><span>                </span><span>let</span><span> handle_connection </span><span>= </span><span>handle</span><span>(connection).</span><span>chain</span><span>(|_| {
</span><span>                    </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>                        </span><span>// decrement the counter
</span><span>                        tasks.</span><span>decrement</span><span>(); </span><span>// 👈
</span><span>                        </span><span>Some</span><span>(())
</span><span>                    })
</span><span>                });
</span><span>
</span><span>                </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(handle_connection);
</span><span>                </span><span>None
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>None</span><span>::&lt;()&gt;,
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        });
</span><span>
</span><span>        </span><span>select</span><span>(listen, </span><span>ctrl_c</span><span>())
</span><span>    })
</span><span>    .</span><span>chain</span><span>(|</span><span>_ctrl_c</span><span>| </span><span>graceful_shutdown</span><span>(tasks_ref)) </span><span>// 👈
</span><span>}
</span></code></pre>
<p>And our graceful shutdown handler can use <code>wait_for_zero</code> to wait until all the active tasks are complete. Once they are, or the timer elapses, the graceful shutdown is met and the program will exit.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>graceful_shutdown</span><span>(</span><span>tasks</span><span>: Arc&lt;Counter&gt;) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>let</span><span> timer </span><span>= </span><span>spawn_blocking</span><span>(|| thread::sleep(Duration::from_secs(</span><span>30</span><span>)));
</span><span>        </span><span>let</span><span> request_counter </span><span>=</span><span> tasks.</span><span>wait_for_zero</span><span>(); </span><span>// 👈
</span><span>        </span><span>select</span><span>(timer, request_counter)
</span><span>    }).</span><span>chain</span><span>(|_| {
</span><span>        </span><span>// graceful shutdown process complete, now we actually exit
</span><span>        println!(</span><span>"Graceful shutdown complete"</span><span>);
</span><span>        std::process::exit(</span><span>0</span><span>)
</span><span>    })
</span><span>}
</span></code></pre>
<p>And that's it!</p>
<p>Now if you start the server and hit ctrl+c, it will exit immediately, without blocking for another connection.</p>
<pre data-lang="sh"><code data-lang="sh"><span>$ cargo run
</span><span>^C
</span><span># =&gt; Graceful shutdown complete
</span><span>$ </span><span>|
</span></code></pre>
<h2 id="looking-back"><a href="#looking-back" aria-label="Anchor link for: looking-back">Looking Back</a></h2>
<p>Well, that was quite the journey.</p>
<p>Our server is looking pretty good now. From threads, to an epoll event loop, to futures and closure combinators, we've come a long way. There is some manual work that we could abstract over even further, but overall our program is relatively clean.</p>
<p>Compared to our original multithreaded program, our code is still clearly more complex. However, it's also a lot more powerful. Composing futures is trivial, and we were able to express complex control flow that would have been very difficult to do with threads. We can even still call out to blocking functions without interrupting our async runtime.</p>
<p>There must be a price to pay for all this power, right?</p>
<h2 id="back-to-reality"><a href="#back-to-reality" aria-label="Anchor link for: back-to-reality">Back To Reality</a></h2>
<p>Now that we've thoroughly explored concurrency and async ourselves, let's see how it works in the real world.</p>
<p>The standard library defines a trait like <code>Future</code>, which looks remarkably similar to the trait we designed.</p>
<pre data-lang="rust"><code data-lang="rust"><span>pub </span><span>trait </span><span>Future {
</span><span>    </span><span>type </span><span>Output;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>self</span><span>: Pin&lt;</span><span>&amp;mut </span><span>Self</span><span>&gt;, </span><span>cx</span><span>: </span><span>&amp;mut </span><span>Context&lt;'</span><span>_</span><span>&gt;) -&gt; Poll&lt;</span><span>Self::</span><span>Output&gt;;
</span><span>}
</span></code></pre>
<p>However, there are a few noticeable differences.</p>
<p>The first is that instead of a <code>Waker</code> argument, <code>poll</code> takes a <code>&amp;mut Context</code>. It turns out this isn't much of a difference at all, because <code>Context</code> is simply a wrapper around a <code>Waker</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>Context&lt;'</span><span>_</span><span>&gt; {
</span><span>    </span><span>pub </span><span>fn </span><span>from_waker</span><span>(</span><span>waker</span><span>: </span><span>&amp;'a</span><span> Waker) -&gt; Context&lt;</span><span>'a</span><span>&gt;  { </span><span>/* ... */ </span><span>}
</span><span>    </span><span>pub </span><span>fn </span><span>waker</span><span>(</span><span>&amp;</span><span>self</span><span>) -&gt; </span><span>&amp;'a</span><span> Waker  { </span><span>/* ... */ </span><span>}
</span><span>}
</span></code></pre>
<p>And <code>Waker</code>, along with a few other utility methods, has the familiar <code>wake</code> method.</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>Waker {
</span><span>    </span><span>pub </span><span>fn </span><span>wake</span><span>(</span><span>self</span><span>) { </span><span>/* ... */ </span><span>}
</span><span>}
</span></code></pre>
<p>Constructing a <code>Waker</code> is a little more complicated, but it's essentially a manual trait object just like the <code>Arc&lt;dyn Fn()&gt;</code> we used for our version. All of that happens <a href="https://doc.rust-lang.org/std/task/struct.Waker.html#method.from_raw">through the <code>RawWaker</code> type</a>, which you can check out yourself.</p>
<p>The second difference is that instead of returning an <code>Option</code>, <code>poll</code> returns a new type called <code>Poll</code>.. which is really just a rebranding of <code>Option</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>pub </span><span>enum </span><span>Poll&lt;T&gt; {
</span><span>    Ready(T),
</span><span>    Pending,
</span><span>}
</span></code></pre>
<p>The final difference is a little more complicated.</p>
<h2 id="pinning"><a href="#pinning" aria-label="Anchor link for: pinning">Pinning</a></h2>
<p>Instead of <code>poll</code> taking a mutable reference to <code>self</code>, it takes a <em>pinned</em> mutable reference to self — <code>Pin&lt;&amp;mut Self&gt;</code>.</p>
<p>What is <code>Pin</code>, you ask?</p>
<pre data-lang="rust"><code data-lang="rust"><span>#[derive(Copy, Clone)]
</span><span>pub </span><span>struct </span><span>Pin&lt;P&gt; {
</span><span>    pointer: P,
</span><span>}
</span></code></pre>
<p>Huh. That doesn't seem very useful.</p>
<p>It turns out, what makes <code>Pin</code> special is how you create one:</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl</span><span>&lt;P: Deref&gt; Pin&lt;P&gt; {
</span><span>    </span><span>pub </span><span>fn </span><span>new</span><span>(</span><span>pointer</span><span>: P) -&gt; Pin&lt;P&gt; </span><span>where </span><span>P::</span><span>Target: Unpin { </span><span>/* ... */ </span><span>}
</span><span>    </span><span>pub unsafe </span><span>fn </span><span>new_unchecked</span><span>(</span><span>pointer</span><span>: P) -&gt; Pin&lt;P&gt;  { </span><span>/* ... */ </span><span>}
</span><span>}
</span><span>
</span><span>impl</span><span>&lt;P: Deref&gt; Deref </span><span>for </span><span>Pin&lt;P&gt; {
</span><span>    </span><span>type </span><span>Target </span><span>= </span><span>P::Target;
</span><span>}
</span><span>
</span><span>impl</span><span>&lt;P: Deref&gt; DerefMut </span><span>for </span><span>Pin&lt;P&gt;
</span><span>where
</span><span>    </span><span>P::</span><span>Target: Unpin
</span><span>{
</span><span>    </span><span>type </span><span>Target </span><span>= </span><span>P::Target;
</span><span>}
</span></code></pre>
<p>So you can only create a <code>Pin&lt;&amp;mut T&gt;</code> safely if <code>T</code> is <code>Unpin</code>... what's <code>Unpin</code>?</p>
<pre data-lang="rust"><code data-lang="rust"><span>pub</span><span> auto </span><span>trait </span><span>Unpin {}
</span><span>
</span><span>/// A marker type which does not implement `Unpin`.
</span><span>pub </span><span>struct </span><span>PhantomPinned;
</span><span>impl </span><span>!Unpin for PhantomPinned {}
</span></code></pre>
<p><code>Unpin</code> seems to be automatically implemented for all types except <code>PhantomPinned</code>. So creating a <code>Pin</code> is safe, except for types that contain <code>PhantomPinned</code>? And <code>Pin</code> just dereferences to <code>T</code> normally? All of this seems a little useless. </p>
<p>There is a point to it all though, and it goes back to a problem we ran into earlier. Remember when we tried creating a self-referential struct to hold our task state but it wouldn't work, so we ended up having to allocate our task state with an <code>Arc</code>? It was a bit unfortunate, and it turns out that you actually <em>can</em> create self-referential structs with a little bit of unsafe code, and avoid that <code>Arc</code> allocation.</p>
<p>The problem is that you can't just go handing out a self-referential struct in general, because as we realized, moving a self-referential struct breaks its internal references and is <em>unsound</em>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>SelfReferential {
</span><span>    counter: </span><span>u8</span><span>, </span><span>// (X)
</span><span>    state: FutureState 
</span><span>}
</span><span>
</span><span>enum </span><span>FutureState  {
</span><span>    First { counter_ptr: </span><span>*mut </span><span>u8 </span><span>} </span><span>// self-referentially points to `counter` (X)
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<pre data-lang="rust"><code data-lang="rust"><span>let </span><span>mut</span><span> future1 </span><span>= </span><span>SelfReferential::new();
</span><span>future1.</span><span>poll</span><span>(</span><span>/* ... */</span><span>);
</span><span>
</span><span>let </span><span>mut</span><span> moved </span><span>=</span><span> future1; </span><span>// move it
</span><span>// unsound! `counter_ptr` still point to the old stack location of `counter`
</span><span>moved.</span><span>poll</span><span>(</span><span>/* ... */</span><span>);
</span></code></pre>
<p>This is where <code>Pin</code> comes in. You can only create a <code>Pin&lt;&amp;mut T&gt;</code> if you <strong>guarantee that the <code>T</code> will stay in a stable location until it is dropped</strong>, meaning that any self-references will remain valid.</p>
<p>For most types, <code>Pin</code> doesn't mean anything, which is why <code>Unpin</code> exists. <code>Unpin</code> essentially tells <code>Pin</code> that a type is not self-referential, so pinning it is completely safe and always valid. <code>Pin</code> will even hand out mutable references to <code>Unpin</code> types and let you use <code>mem::swap</code> or <code>mem::replace</code> to move them around. Because you can't safely create a self-referential struct, <code>Unpin</code> is the default and implemented by types automatically.</p>
<p>If you did want to create a self-referential future though, you can use the <code>PhantomPinned</code> marker struct to make it <code>!Unpin</code>. Pinning a <code>!Unpin</code> type requires <code>unsafe</code>, so because <code>poll</code> requires <code>Pin&lt;&amp;mut Self&gt;</code>, it cannot be called safely on a self-referential future.</p>
<pre data-lang="rust"><code data-lang="rust"><span>let </span><span>mut</span><span> future </span><span>= </span><span>SelfReferential::new();
</span><span>
</span><span>// SAFETY: we never move `future`
</span><span>let</span><span> pinned </span><span>= unsafe </span><span>{ Pin::new_unchecked(</span><span>&amp;mut</span><span> future) };
</span><span>pinned.</span><span>poll</span><span>(</span><span>/* ... /*);
</span></code></pre>
<p>Notice that you can move around the future all you want before pinning it because the self-references are only created after you first call <code>poll</code>. Once you do pin it though, you <em>must</em> uphold the <code>Pin</code> safety contract.</p>
<p>There are a couple safe ways of creating a pin though, even for <code>!Unpin</code> types.</p>
<p>The first way is with <code>Box::pin</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>let </span><span>mut</span><span> future1: Pin&lt;</span><span>Box</span><span>&lt;SelfReferential&gt;&gt; </span><span>= </span><span>Box</span><span>::pin(SelfReferential::new());
</span><span>future1.</span><span>as_mut</span><span>().</span><span>poll</span><span>(</span><span>/* ... */</span><span>);
</span><span>
</span><span>let </span><span>mut</span><span> moved </span><span>=</span><span> future1;
</span><span>moved.</span><span>as_mut</span><span>().</span><span>poll</span><span>(</span><span>/* ... */</span><span>);
</span></code></pre>
<p>At first glance this may seem unsound, but remember, <code>Box</code> is an allocation. Once the future is allocated it has a stable location on the heap, so you can move around the <code>Box</code> pointer all you want, the internal references will remain stable.</p>
<p>The second way you can safely create a pin is with the <code>pin!</code> macro.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>std::pin::pin;
</span><span>
</span><span>let </span><span>mut</span><span> future1: Pin&lt;</span><span>&amp;mut</span><span> SelfReferential&gt; </span><span>= </span><span>pin!(SelfReferential::new());
</span><span>future1.</span><span>as_mut</span><span>().</span><span>poll</span><span>(</span><span>/* ... */</span><span>);
</span></code></pre>
<p>With <code>pin!</code>, you can safely pin a struct without even allocating it! The trick is that <code>pin!</code> takes ownership of the future, making it impossible to access except through the <code>Pin&lt;&amp;mut T&gt;</code>, which remember, will never give you a mutable reference if <code>T</code> isn't <code>Unpin</code>. The <code>T</code> is completely hidden and thus safe from being tampered with.</p>
<p><code>Pin</code> is a common point of confusion around futures, but once you understand why it exists, the solution is pretty ingenious.</p>
<h2 id="async-await"><a href="#async-await" aria-label="Anchor link for: async-await">async/await</a></h2>
<p>Alright, that's the standard <code>Future</code> trait.</p>
<pre data-lang="rust"><code data-lang="rust"><span>pub </span><span>trait </span><span>Future {
</span><span>    </span><span>type </span><span>Output;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>self</span><span>: Pin&lt;</span><span>&amp;mut </span><span>Self</span><span>&gt;, </span><span>cx</span><span>: </span><span>&amp;mut </span><span>Context&lt;'</span><span>_</span><span>&gt;) -&gt; Poll&lt;</span><span>Self::</span><span>Output&gt;;
</span><span>}
</span></code></pre>
<p>So how do we use it?</p>
<p>The <a href="https://docs.rs/futures"><code>futures</code></a> crate is where all the useful helpers live. Functions like <code>poll_fn</code> that we wrote before, and combinators like <code>map</code> and <code>and_then</code>, which we called <code>chain</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>futures::{FutureExt, future::{ready, poll_fn}};
</span><span>
</span><span>let</span><span> future </span><span>= </span><span>poll_fn</span><span>(|_| Poll::Ready(</span><span>1</span><span>))
</span><span>    .</span><span>and_then</span><span>(|</span><span>x</span><span>| </span><span>poll_fn</span><span>(|_| Poll::Ready(x </span><span>+ </span><span>1</span><span>)))
</span><span>    .</span><span>and_then</span><span>(|</span><span>x</span><span>| </span><span>poll_fn</span><span>(|_| println!(</span><span>"</span><span>{x}</span><span>"</span><span>)));
</span></code></pre>
<p>But even with these helpers, as we found out, it's a little cumbersome to write async code. It's still a shift from the simple synchronous code we're used to. Maybe not as drastic as a manual <code>epoll</code> event loop, but still a big change.</p>
<p>It turns out there's actually another way to write futures in Rust, with the async/await syntax.</p>
<p>Instead of using <code>poll_fn</code> to create futures, you can attach the <code>async</code> keyword to functions:</p>
<pre data-lang="rust"><code data-lang="rust"><span>async </span><span>fn </span><span>foo</span><span>() -&gt; </span><span>usize </span><span>{
</span><span>    </span><span>1
</span><span>}
</span></code></pre>
<p>An <em>async</em> function is really just a function that returns an async block:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>foo</span><span>() -&gt; impl Future&lt;Output = </span><span>usize</span><span>&gt; {
</span><span>    async { </span><span>1 </span><span>}
</span><span>}
</span></code></pre>
<p>Which is really just a function that returns a <code>poll_fn</code> future:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>foo</span><span>() -&gt; impl Future&lt;Output = </span><span>usize</span><span>&gt; {
</span><span>    </span><span>poll_fn</span><span>(|| Poll::Ready(</span><span>1</span><span>))
</span><span>}
</span></code></pre>
<p>The magic comes with the <code>await</code> keyword. <code>await</code> waits for the completion of another future, propagating <code>Poll::Pending</code> until the future is resolved.</p>
<pre data-lang="rust"><code data-lang="rust"><span>async </span><span>fn </span><span>foo</span><span>() {
</span><span>    </span><span>let</span><span> one </span><span>= </span><span>one</span><span>().await;
</span><span>    </span><span>let</span><span> two </span><span>= </span><span>two</span><span>().await;
</span><span>    assert_eq!(one </span><span>+ </span><span>1</span><span>, two);
</span><span>}
</span><span>
</span><span>async </span><span>fn </span><span>two</span><span>() -&gt; </span><span>usize </span><span>{
</span><span>    </span><span>one</span><span>().await </span><span>+ </span><span>1
</span><span>}
</span><span>
</span><span>async </span><span>fn </span><span>one</span><span>() -&gt; </span><span>usize </span><span>{
</span><span>    </span><span>1
</span><span>}
</span></code></pre>
<p>Under the hood, the compiler transforms this into manual state machines, similar to the ones we created with those combinators:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>foo</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>one</span><span>()
</span><span>        .</span><span>and_then</span><span>(|</span><span>one</span><span>| </span><span>two</span><span>().</span><span>and_then</span><span>(</span><span>move |</span><span>two</span><span>| </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>Poll::Ready((one, two)))))
</span><span>        .</span><span>and_then</span><span>(|(</span><span>one</span><span>, </span><span>two</span><span>)| </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>Poll::Ready(assert_eq!(one, two </span><span>+ </span><span>1</span><span>))))
</span><span>}
</span><span>
</span><span>fn </span><span>two</span><span>() -&gt; impl Future&lt;Output = </span><span>usize</span><span>&gt; {
</span><span>    </span><span>one</span><span>().</span><span>and_then</span><span>(|</span><span>one</span><span>| </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>Poll::Ready(one </span><span>+ </span><span>1</span><span>)))
</span><span>}
</span><span>
</span><span>fn </span><span>one</span><span>() -&gt; impl Future&lt;Output = </span><span>usize</span><span>&gt; {
</span><span>    </span><span>poll_fn</span><span>(|_| Poll::Ready(</span><span>1</span><span>))
</span><span>}
</span></code></pre>
<p>Which, as we know all too well, translates into a huge manual state machine that looks something like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span>enum </span><span>FooFuture {
</span><span>    One(OneFuture),
</span><span>    Two(</span><span>usize</span><span>, TwoFuture),
</span><span>}
</span><span>
</span><span>impl </span><span>Future </span><span>for </span><span>FooFuture {
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>();
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>cx</span><span>: </span><span>&amp;mut </span><span>Context&lt;'</span><span>_</span><span>&gt;) -&gt; Poll&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>        </span><span>if </span><span>let </span><span>FooFuture::One(f) </span><span>= </span><span>self {
</span><span>            </span><span>match</span><span> f.</span><span>poll</span><span>(cx) {
</span><span>                Poll::Ready(one) </span><span>=&gt; *</span><span>self </span><span>= </span><span>Self</span><span>::Two(one, TwoFuture(OneFuture)),
</span><span>                Poll::Pending </span><span>=&gt; return </span><span>Poll::Pending,
</span><span>            }
</span><span>        }
</span><span>
</span><span>        </span><span>if </span><span>let </span><span>FooFuture::Two(one, f) </span><span>= </span><span>self {
</span><span>            </span><span>match</span><span> f.</span><span>poll</span><span>(cx) {
</span><span>                Poll::Ready(two) </span><span>=&gt; </span><span>{
</span><span>                    assert_eq!(</span><span>*</span><span>one </span><span>+ </span><span>1</span><span>, two);
</span><span>                    </span><span>return </span><span>Poll::Ready(());
</span><span>                }
</span><span>                Poll::Pending </span><span>=&gt; return </span><span>Poll::Pending,
</span><span>            }
</span><span>        }
</span><span>
</span><span>        </span><span>None
</span><span>    }
</span><span>}
</span><span>
</span><span>struct </span><span>TwoFuture(OneFuture);
</span><span>
</span><span>impl </span><span>Future </span><span>for </span><span>TwoFuture {
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>usize</span><span>;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>cx</span><span>: </span><span>&amp;mut </span><span>Context&lt;'</span><span>_</span><span>&gt;) -&gt; Poll&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>        </span><span>match </span><span>self.</span><span>0.</span><span>poll</span><span>(waker) {
</span><span>            Poll::Ready(one) </span><span>=&gt; </span><span>Poll::Ready(one </span><span>+ </span><span>1</span><span>),
</span><span>            Poll::Pending </span><span>=&gt; </span><span>Poll::Pending,
</span><span>        }
</span><span>    }
</span><span>}
</span><span>
</span><span>struct </span><span>OneFuture;
</span><span>
</span><span>impl </span><span>Future </span><span>for </span><span>OneFuture {
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>usize</span><span>;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>cx</span><span>: </span><span>&amp;mut </span><span>Context&lt;'</span><span>_</span><span>&gt;) -&gt; Poll&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>        Poll::Ready(</span><span>1</span><span>)
</span><span>    }
</span><span>}
</span></code></pre>
<p>... but async/await removes all of that headache. Of course, we aren't actually doing any I/O here so the futures are mostly useless, but you can imagine how helpful this would be for our web server.</p>
<p>In fact, it's even better than the combinators. With <code>async</code> functions, you can hold onto local state across <code>await</code> points!</p>
<pre data-lang="rust"><code data-lang="rust"><span>async </span><span>fn </span><span>foo</span><span>() {
</span><span>    </span><span>let</span><span> x </span><span>= </span><span>vec![</span><span>1</span><span>, </span><span>2</span><span>, </span><span>3</span><span>];
</span><span>    </span><span>bar</span><span>(</span><span>&amp;</span><span>x).await;
</span><span>    </span><span>baz</span><span>(</span><span>&amp;</span><span>x).await;
</span><span>    println!(</span><span>"</span><span>{x:?}</span><span>"</span><span>);
</span><span>}
</span></code></pre>
<p>After going through implementing futures ourselves, we can really appreciate the convenience of this. Under the hood the compiler has to generate a self-referential future to give <code>bar</code> and <code>baz</code> access to the state.</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>FooFuture {
</span><span>    x: </span><span>Vec</span><span>&lt;</span><span>i32</span><span>&gt;, </span><span>// (X)
</span><span>    state: FooFutureState,
</span><span>}
</span><span>
</span><span>enum </span><span>FooFutureState {
</span><span>    Bar(BarFuture),
</span><span>    Baz(BazFuture),
</span><span>}
</span><span>
</span><span>struct </span><span>BarFuture { x: </span><span>*mut </span><span>Vec</span><span>&lt;</span><span>i32</span><span>&gt; </span><span>/* pointer to (X)! */ </span><span>}
</span><span>struct </span><span>BazFuture { x: </span><span>*mut </span><span>Vec</span><span>&lt;</span><span>i32</span><span>&gt; </span><span>/* pointer to (X)! */ </span><span>}
</span></code></pre>
<p>The compiler takes care of all the unsafe code involved in this, allowing us to work with local state just like we would in a regular function. For this reason, the futures generated by <code>async</code> blocks or functions are <code>!Unpin</code>.</p>
<p>async/await removes any complexity that remained with writing futures compared to synchronous code. After implementing futures manually, it almost feels like magic!</p>
<h2 id="a-tokio-server"><a href="#a-tokio-server" aria-label="Anchor link for: a-tokio-server">A Tokio Server</a></h2>
<p>So far we've only been looking at how <code>Future</code> <em>works</em>, we haven't discussed how to actually run one, or do any I/O. The thing is, the standard library doesn't provide any of that, it only provides the bare essential types and traits to get started.</p>
<p>If you want to actually write an async application, you have to use an external runtime. The most popular general purpose runtime is <code>tokio</code>. <code>tokio</code> provides a task scheduler, a reactor, and a pool to run blocking tasks, just like we wrote earlier, but it also provides timers, async channels, and various other useful types and utilities for async code. On top of that, <code>tokio</code> is multi-threaded, distributing async tasks to take advantage of all your CPU cores. The core ideas behind <code>tokio</code> are very similar to the async runtime we wrote ourselves, but you can read more about it's design <a href="https://tokio.rs/blog/2019-10-scheduler">in this excellent blog post</a>.</p>
<p>It's time to write our final web server, this time using the standard <code>Future</code> trait and tokio.</p>
<p>Tokio applications begin with the <code>#[tokio::main]</code> macro. Under the hood, this macro spins up the runtime and runs the async code in <code>main</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>#[tokio::main]
</span><span>async </span><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>Tokio does it's best to mirror the standard library for most of it's types. For example, <code>tokio::net::TcpListener</code> works exactly like <code>std::net::TcpListener</code>, except with <code>async</code> methods. Any interactions with epoll and the reactor are hidden under the hood.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>tokio::net::{TcpListener, TcpStream};
</span><span>
</span><span>#[tokio::main]
</span><span>async </span><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).await.</span><span>unwrap</span><span>();
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>let </span><span>(connection, </span><span>_</span><span>) </span><span>=</span><span> listener.</span><span>accept</span><span>().await.</span><span>unwrap</span><span>();
</span><span>
</span><span>        </span><span>if </span><span>let Err</span><span>(e) </span><span>= </span><span>handle_connection</span><span>(connection).await {
</span><span>            println!(</span><span>"failed to handle connection: </span><span>{e}</span><span>"</span><span>)
</span><span>        }
</span><span>    }
</span><span>}
</span><span>
</span><span>async </span><span>fn </span><span>handle_connection</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>That isn't exactly right though, we need to spawn the connection handler. We can do so with the <code>tokio::spawn</code> function, which takes a future.</p>
<pre data-lang="rust"><code data-lang="rust"><span>#[tokio::main]
</span><span>async </span><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).await.</span><span>unwrap</span><span>();
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>let </span><span>(connection, </span><span>_</span><span>) </span><span>=</span><span> listener.</span><span>accept</span><span>().await.</span><span>unwrap</span><span>();
</span><span>
</span><span>        tokio::spawn(async </span><span>move </span><span>{ </span><span>// 👈
</span><span>            </span><span>if </span><span>let Err</span><span>(e) </span><span>= </span><span>handle_connection</span><span>(connection).await {
</span><span>                println!(</span><span>"failed to handle connection: </span><span>{e}</span><span>"</span><span>)
</span><span>            }
</span><span>        });
</span><span>    }
</span><span>}
</span></code></pre>
<p>Now for the connection handler. With the <code>AsyncReadExt</code> trait and the <code>await</code> keyword, we can read from the TCP stream almost exactly like we did before.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>tokio::io::AsyncRead;
</span><span>
</span><span>async </span><span>fn </span><span>handle_connection</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>let </span><span>mut</span><span> read </span><span>= </span><span>0</span><span>;
</span><span>    </span><span>let </span><span>mut</span><span> request </span><span>= </span><span>[</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>];
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>// try reading from the stream
</span><span>        </span><span>let</span><span> num_bytes </span><span>=</span><span> connection.</span><span>read</span><span>(</span><span>&amp;mut</span><span> request[read</span><span>..</span><span>]).await</span><span>?</span><span>; </span><span>// 👈
</span><span>
</span><span>        </span><span>// the client disconnected
</span><span>        </span><span>if</span><span> num_bytes </span><span>== </span><span>0 </span><span>{
</span><span>            println!(</span><span>"client disconnected unexpectedly"</span><span>);
</span><span>            </span><span>return </span><span>Ok</span><span>(());
</span><span>        }
</span><span>
</span><span>        </span><span>// keep track of how many bytes we've read
</span><span>        read </span><span>+=</span><span> num_bytes;
</span><span>
</span><span>        </span><span>// have we reached the end of the request?
</span><span>        </span><span>if</span><span> request.</span><span>get</span><span>(read </span><span>- </span><span>4</span><span>..</span><span>read) </span><span>== </span><span>Some</span><span>(</span><span>b</span><span>"</span><span>\r\n\r\n</span><span>"</span><span>) {
</span><span>            </span><span>break</span><span>;
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span>let</span><span> request </span><span>= </span><span>String</span><span>::from_utf8_lossy(</span><span>&amp;</span><span>request[</span><span>..</span><span>read]);
</span><span>    println!(</span><span>"</span><span>{request}</span><span>"</span><span>);
</span><span>
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>Writing the response works the same as well.</p>
<pre data-lang="rust"><code data-lang="rust"><span>async </span><span>fn </span><span>handle_connection</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>// ...
</span><span>
</span><span>    </span><span>// "Hello World!" in HTTP
</span><span>    </span><span>let</span><span> response </span><span>= </span><span>/* ... */</span><span>;
</span><span>    </span><span>let </span><span>mut</span><span> written </span><span>= </span><span>0</span><span>;
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>// write the remaining response bytes
</span><span>        </span><span>let</span><span> num_bytes </span><span>=</span><span> connection.</span><span>write</span><span>(response[written</span><span>..</span><span>].</span><span>as_bytes</span><span>()).await</span><span>?</span><span>; </span><span>// 👈
</span><span>
</span><span>        </span><span>// the client disconnected
</span><span>        </span><span>if</span><span> num_bytes </span><span>== </span><span>0 </span><span>{
</span><span>            println!(</span><span>"client disconnected unexpectedly"</span><span>);
</span><span>            </span><span>return </span><span>Ok</span><span>(());
</span><span>        }
</span><span>
</span><span>        written </span><span>+=</span><span> num_bytes;
</span><span>
</span><span>        </span><span>// have we written the whole response yet?
</span><span>        </span><span>if</span><span> written </span><span>==</span><span> response.</span><span>len</span><span>() {
</span><span>            </span><span>break</span><span>;
</span><span>        }
</span><span>    }
</span><span>
</span><span>    connection.</span><span>flush</span><span>().await
</span><span>}
</span></code></pre>
<p>Well that was easy.</p>
<p>If you notice, our program is exactly the same as our original server, with the exception of a couple uses of the <code>async</code> and <code>await</code> keywords. With async/await, we really can have our cake and eat it too.</p>
<p>Now to implement graceful shutdown.</p>
<p>The first step is to identify the ctrl+c signal. With tokio, this is as simple as using the <code>tokio::signal::ctrl_c</code> function, an async function that returns once the ctrl+c signal is received. We can also use tokio's <code>select!</code> macro, a more powerful version of the <code>select</code> combinator we implemented earlier.</p>
<pre data-lang="rust"><code data-lang="rust"><span>pub</span><span> async </span><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).await.</span><span>unwrap</span><span>();
</span><span>    </span><span>let</span><span> state </span><span>= </span><span>Arc::new((AtomicUsize::new(</span><span>0</span><span>), Notify::new()));
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        select! {
</span><span>            </span><span>// new incoming connection
</span><span>            result </span><span>=</span><span> listener.</span><span>accept</span><span>() </span><span>=&gt; </span><span>{
</span><span>                </span><span>let </span><span>(connection, </span><span>_</span><span>) </span><span>=</span><span> result.</span><span>unwrap</span><span>();
</span><span>
</span><span>                tokio::spawn(async </span><span>move </span><span>{
</span><span>                    </span><span>// ..
</span><span>                });
</span><span>            }
</span><span>            </span><span>// ctrl+c signal
</span><span>            shutdown </span><span>= </span><span>ctrl_c</span><span>() </span><span>=&gt; </span><span>{
</span><span>                </span><span>let</span><span> timer </span><span>= </span><span>/* ... */</span><span>;
</span><span>                </span><span>let</span><span> request_counter </span><span>= </span><span>/* .. */</span><span>;
</span><span>
</span><span>                select! {
</span><span>                    </span><span>_ =</span><span> timer </span><span>=&gt; </span><span>{}
</span><span>                    </span><span>_ =</span><span> request_counter </span><span>=&gt; </span><span>{}
</span><span>                }
</span><span>
</span><span>                println!(</span><span>"Gracefully shutting down."</span><span>);
</span><span>                </span><span>return</span><span>;
</span><span>            }
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p><code>select!</code> runs the branch for whichever future completes first and cancels the other branches, allowing us to select between incoming connections and the ctrl+c signal, and run the appropriate code for each.</p>
<p>Now we need to create the graceful shutdown condition.</p>
<p>For the timer, we can use tokio's asynchronous <code>sleep</code> function. Under the hood this hooks into a custom timer system, a much more efficient version of our <code>spawn_blocking</code> timers. You can read more about how that works <a href="https://tokio.rs/blog/2018-03-timers">in this other excellent post</a>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>select! {
</span><span>    </span><span>// new incoming connection
</span><span>    result </span><span>=</span><span> listener.</span><span>accept</span><span>() </span><span>=&gt; </span><span>{
</span><span>        </span><span>// ...
</span><span>    }
</span><span>    </span><span>// ctrl+c signal
</span><span>    shutdown </span><span>= </span><span>ctrl_c</span><span>() </span><span>=&gt; </span><span>{
</span><span>        </span><span>let</span><span> timer </span><span>= </span><span>tokio::time::sleep(Duration::from_secs(</span><span>30</span><span>));
</span><span>        </span><span>let</span><span> request_counter </span><span>= </span><span>/* .. */</span><span>;
</span><span>
</span><span>        select! {
</span><span>            </span><span>_ =</span><span> timer </span><span>=&gt; </span><span>{}
</span><span>            </span><span>_ =</span><span> request_counter </span><span>=&gt; </span><span>{}
</span><span>        }
</span><span>
</span><span>        println!(</span><span>"Gracefully shutting down."</span><span>);
</span><span>        </span><span>return</span><span>;
</span><span>    }
</span><span>}
</span></code></pre>
<p>Now for the active request counter. Instead of managing wakers manually, we can use a simple counter and take advantage of tokio's <code>Notify</code> type, which allows tasks to notify each other, or wait to be notified.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>tokio::sync::Notify;
</span><span>
</span><span>let</span><span> state </span><span>= </span><span>Arc::new((AtomicUsize::new(</span><span>0</span><span>), Notify::new()));
</span></code></pre>
<p>When a request comes in, we increment the counter, and when it completes, we decrement it. If the counter reaches zero, the last active task calls <code>notify_one</code>, which will wake up the main thread, letting it know that all active tasks have completed.</p>
<pre data-lang="rust"><code data-lang="rust"><span>select! {
</span><span>    result </span><span>=</span><span> listener.</span><span>accept</span><span>() </span><span>=&gt; </span><span>{
</span><span>        </span><span>let </span><span>(connection, </span><span>_</span><span>) </span><span>=</span><span> result.</span><span>unwrap</span><span>();
</span><span>        </span><span>let</span><span> state </span><span>=</span><span> state.</span><span>clone</span><span>();
</span><span>
</span><span>    </span><span>// increment the counter
</span><span>    state.</span><span>0.</span><span>fetch_add</span><span>(</span><span>1</span><span>, Ordering::Relaxed);
</span><span>
</span><span>    tokio::spawn(async </span><span>move </span><span>{
</span><span>        </span><span>if </span><span>let Err</span><span>(e) </span><span>= </span><span>handle_connection</span><span>(connection).await {
</span><span>            </span><span>// ...
</span><span>        }
</span><span>
</span><span>        </span><span>// decrement the counter
</span><span>        </span><span>let</span><span> count </span><span>=</span><span> state.</span><span>0.</span><span>fetch_sub</span><span>(</span><span>1</span><span>, Ordering::Relaxed);
</span><span>        </span><span>if</span><span> count </span><span>== </span><span>1 </span><span>{
</span><span>            </span><span>// we were the last active task
</span><span>            state.</span><span>1.</span><span>notify_one</span><span>();
</span><span>        }
</span><span>    });
</span><span>    }
</span><span>
</span><span>    shutdown </span><span>= </span><span>ctrl_c</span><span>() </span><span>=&gt; </span><span>{
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span></code></pre>
<p>The shutdown handler can now simply select between the timer and <code>Notify::notified</code>, which will resolve when someone calls <code>notify_one</code>, indicating that the last active request has completed.</p>
<pre data-lang="rust"><code data-lang="rust"><span>select! {
</span><span>    result </span><span>=</span><span> listener.</span><span>accept</span><span>() </span><span>=&gt; </span><span>{
</span><span>        </span><span>// ...
</span><span>    }
</span><span>    shutdown </span><span>= </span><span>ctrl_c</span><span>() </span><span>=&gt; </span><span>{
</span><span>        </span><span>// a 30 second timer
</span><span>        </span><span>let</span><span> timer </span><span>= </span><span>tokio::time::sleep(Duration::from_secs(</span><span>30</span><span>));
</span><span>        </span><span>// notified by the last active task
</span><span>        </span><span>let</span><span> notification </span><span>=</span><span> state.</span><span>1.</span><span>notified</span><span>();
</span><span>
</span><span>        </span><span>// if the count isn't zero, we have to wait
</span><span>        </span><span>if</span><span> state.</span><span>0.</span><span>load</span><span>(Ordering::Relaxed) </span><span>!= </span><span>0 </span><span>{
</span><span>            </span><span>// wait for either the timer or notification to resolve
</span><span>            select! {
</span><span>                </span><span>_ =</span><span> timer </span><span>=&gt; </span><span>{}
</span><span>                </span><span>_ =</span><span> notification </span><span>=&gt; </span><span>{}
</span><span>            }
</span><span>        }
</span><span>
</span><span>        println!(</span><span>"Gracefully shutting down."</span><span>);
</span><span>        </span><span>return</span><span>;
</span><span>    }
</span><span>}
</span></code></pre>
<p>Beautiful, isn't it?</p>
<p>With <code>tokio</code> and async/await we don't even have to think about wakers, reactors, or anything else that goes on under the hood. All the building blocks are provided for us, we just have to put them together.</p>
<h2 id="afterword"><a href="#afterword" aria-label="Anchor link for: afterword">Afterword</a></h2>
<p>Whew, that was quite the journey!</p>
<p>We started from the simplest web server, tried multithreading, and then worked our way up to a custom asynchronous runtime built on epoll. All to implement graceful shutdown.</p>
<p>Then we circled back and implemented graceful shutdown with <code>tokio</code> in just a few extra lines of code.</p>
<p>Hopefully this article has helped you appreciate the power of async Rust, as well as taught you a little more about how it works under the hood. All the code for this repository <a href="https://github.com/ibraheemdev/too-many-web-servers">is available on GitHub</a>.</p>
<br>
<hr>






</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meaningful exits for founders (2016) (120 pts)]]></title>
            <link>https://medium.com/strong-words/meaningful-exits-for-founders-4c3b2baba6b4</link>
            <guid>37176842</guid>
            <pubDate>Fri, 18 Aug 2023 15:16:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.com/strong-words/meaningful-exits-for-founders-4c3b2baba6b4">https://medium.com/strong-words/meaningful-exits-for-founders-4c3b2baba6b4</a>, See on <a href="https://news.ycombinator.com/item?id=37176842">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a rel="noopener follow" href="https://medium.com/@bryce?source=post_page-----4c3b2baba6b4--------------------------------"><div aria-hidden="false"><p><img alt="Bryce Roberts" src="https://miro.medium.com/v2/resize:fill:88:88/0*53AyJNc3J_vnpZXf.jpg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a><a href="https://medium.com/strong-words?source=post_page-----4c3b2baba6b4--------------------------------" rel="noopener follow"><div aria-hidden="false"><p><img alt="Strong Words" src="https://miro.medium.com/v2/resize:fill:48:48/1*pt3cKQuyPT2A2Fz97lx80Q.jpeg" width="24" height="24" loading="lazy" data-testid="publicationPhoto"></p></div></a></div><p id="c7d8">For an industry that doesn’t do it for the money, we sure talk about money an awful lot in the world of startups.</p><p id="69ca">A few posts were written this past week diving deeper into the numbers that drive VC returns which, in turn, drive behavior in startups who’ve raised money from VCs.</p><p id="f066">One <a href="http://t.umblr.com/redirect?z=https%3A%2F%2Fmedium.com%2Fjme-venture-capital%2Fmeaningful-vc-exits-2bb5702776e2%23.ewfj7kn73&amp;t=NmY4YWY0NTJjMzZjMTljNjhhYmJhODA0NzgwNmI1Y2Q5Njk3MzJhMyxYeEYza3ZlNQ%3D%3D&amp;b=t%3APMcMAc3jokc9Lc15hdr0gw&amp;p=http%3A%2F%2Fbryce.vc%2Fpost%2F154821809555%2Fmeaningful-exits-for-founders&amp;m=1" rel="noopener ugc nofollow" target="_blank">post</a>, written by <a href="https://twitter.com/samuelgil" rel="noopener ugc nofollow" target="_blank">Samuel Gil</a>, outlined what is considered a meaningful exit for VCs. According to Sam’s math, a meaningful exit for a fund should have the ability to return 33% of a given VCs fund, a “home run” exit should be able to return their entire fund in a single investment.</p><p id="916d">This is a helpful visual he put together to show the resulting math behind meaningful exits at various fund sizes:</p><figure></figure><p id="cb20">Across all of these scenarios, the smallest meaningful exit for the tiniest fund is a purchase price of $85M.</p><p id="a8e3">Now, we don’t know much in this scenario beyond the 20% owned by one fund. We don’t know how many rounds this company has raised, how many other VCs are on the cap table, nor how much the founders own.</p><p id="5ed4">(sidebar- most $50M funds would kill for 20% ownership these days. 5%-10% range is much more likely after the initial round of funding and dilutes rather quickly in successive rounds as reserves for pro rata vary wildly by firm).</p><p id="b14d">Another <a href="http://t.umblr.com/redirect?z=https%3A%2F%2Fwww.capshare.com%2Fblog%2F4-key-insights-from-5000-cap-tables%2F&amp;t=Y2VjZjg4NDkxNDRmY2ZlYmEzMDUzMjYzNmJiNGE1YWJhYmJlYmEyNixYeEYza3ZlNQ%3D%3D&amp;b=t%3APMcMAc3jokc9Lc15hdr0gw&amp;p=http%3A%2F%2Fbryce.vc%2Fpost%2F154821809555%2Fmeaningful-exits-for-founders&amp;m=1" rel="noopener ugc nofollow" target="_blank">post</a> from <a href="http://t.umblr.com/redirect?z=https%3A%2F%2Fwww.capshare.com%2F&amp;t=N2JiNmFiYjg3ODJlOTZmOWVhYWZlYTVkYzc1NWZiODk3YzJiNjU5YixYeEYza3ZlNQ%3D%3D&amp;b=t%3APMcMAc3jokc9Lc15hdr0gw&amp;p=http%3A%2F%2Fbryce.vc%2Fpost%2F154821809555%2Fmeaningful-exits-for-founders&amp;m=1" rel="noopener ugc nofollow" target="_blank">Capshare</a> surfaced this week touching on that last bit about founder ownership at various levels of VC funding. Specifically, this post highlighted, after having analyzed over 5,000 VC backed cap tables, that founder dilution is fairly predictable based on the rounds raised by a given company. There are plenty of graphs to pour over in the post, but the summary reads as follows:</p><blockquote><p id="fcab">If your company exits around Series D, you can expect the following splits:</p><p id="2f7c">Founder ownership: 11–17%<br>Other employees: 17–21%<br>Investors: 66–68%</p></blockquote><p id="1233">The data suggests startups generally have 2–3 founders so divide that number by the number of founders, and that’s the predictable founder ownership level of a Series D funded company (in the Capshare post the Series D valuation was $210M).</p><p id="ee4b">Reframing those numbers another way, a founder selling at the Series D price of $210M, would make the same amount of money at exit as they would have if they’d sold for $38M after having only raised a seed round.</p><figure></figure><p id="2eb0">Lifetimes of work and risk lie between a Seed round and a Series D round. And, despite increasing the value of the underlying business 7x, the dollars at exit for the founder remain roughly the same. It is also worth noting that an exit at $210M would not even qualify as a home run for even the smallest fund in Sam’s examples.</p><p id="e89c">There are many paths to managing dilution- be it raising fewer rounds at higher and higher valuations, or raising one round and scaling a business on revenue and profits generated from customers.</p><p id="79fb">Through our work on <a href="http://t.umblr.com/redirect?z=http%3A%2F%2FIndie.vc&amp;t=Zjg0OWM1ZTBlNmJkNTlkMDMyNGM5ZmRmMmRhMjFhZTliMTRkOWVmZCxYeEYza3ZlNQ%3D%3D&amp;b=t%3APMcMAc3jokc9Lc15hdr0gw&amp;p=http%3A%2F%2Fbryce.vc%2Fpost%2F154821809555%2Fmeaningful-exits-for-founders&amp;m=1" rel="noopener ugc nofollow" target="_blank">Indie.vc</a> we’ve met many founders who’ve raised successive rounds of funding and had opportunities to exit their businesses for life changing sums of money; however, those acqusition offers were not meaningful exits or homeruns for their investors.</p><p id="b561">Many are looking to avoid, or at least be more aligned, with investors on the companies they are working on now. As Marc Hedlund wrote in his <a href="http://t.umblr.com/redirect?z=https%3A%2F%2Fblog.skyliner.io%2Findie-vc-and-focus-8e833d8680d4%23.w0ovk2125&amp;t=NzYzNWExOTMyOTIwOTk1N2QzOGI1NmIzMTZlNmIzMzhjYjQxOWVlMyxYeEYza3ZlNQ%3D%3D&amp;b=t%3APMcMAc3jokc9Lc15hdr0gw&amp;p=http%3A%2F%2Fbryce.vc%2Fpost%2F154821809555%2Fmeaningful-exits-for-founders&amp;m=1" rel="noopener ugc nofollow" target="_blank">post</a> about why <a href="http://t.umblr.com/redirect?z=http%3A%2F%2Fskyliner.io&amp;t=YzdkOTBjNTVkZmE4OWEwM2JhMmM3NDcyZTgxODczZmE2YTg0MDA2MSxYeEYza3ZlNQ%3D%3D&amp;b=t%3APMcMAc3jokc9Lc15hdr0gw&amp;p=http%3A%2F%2Fbryce.vc%2Fpost%2F154821809555%2Fmeaningful-exits-for-founders&amp;m=1" rel="noopener ugc nofollow" target="_blank">Skyliner</a> chose to work with us:</p><blockquote><p id="85c5">It should not be surprising that we chose to work with <a href="http://t.umblr.com/redirect?z=http%3A%2F%2FIndie.vc&amp;t=Zjg0OWM1ZTBlNmJkNTlkMDMyNGM5ZmRmMmRhMjFhZTliMTRkOWVmZCxYeEYza3ZlNQ%3D%3D&amp;b=t%3APMcMAc3jokc9Lc15hdr0gw&amp;p=http%3A%2F%2Fbryce.vc%2Fpost%2F154821809555%2Fmeaningful-exits-for-founders&amp;m=1" rel="noopener ugc nofollow" target="_blank">Indie.vc</a>. It’s a choice many more entrepreneurs <a href="http://t.umblr.com/redirect?z=http%3A%2F%2Fwww.indie.vc%2Fapplication&amp;t=ZGMwMTM0NjQ0YmFmMmI1NGRmZjg5YzQ4YTU0MWVmYmEzZDYxYjI5OSxYeEYza3ZlNQ%3D%3D&amp;b=t%3APMcMAc3jokc9Lc15hdr0gw&amp;p=http%3A%2F%2Fbryce.vc%2Fpost%2F154821809555%2Fmeaningful-exits-for-founders&amp;m=1" rel="noopener ugc nofollow" target="_blank">should make</a>, whether you have tons of experience or are just starting out. Don’t let the normal VC business model drive your work into a low-probability, high-reward outcome, when a higher-probability outcome is well within your reach, especially one that does nothing to limit your growth.</p></blockquote><p id="ecc8">So, let’s keep saying we don’t do it for the money.</p><p id="06da">Fine.</p><p id="7c33">But, it might be worth looking more closely at the numbers before so quickly trading what’s a meaningful exit to a founder for what’s a meaningful exit to a VC.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deja Vu: The FBI Proves Again It Can’t Be Trusted with Section 702 (325 pts)]]></title>
            <link>https://www.eff.org/deeplinks/2023/07/deja-vu-fbi-proves-again-it-cant-be-trusted-section-702</link>
            <guid>37176717</guid>
            <pubDate>Fri, 18 Aug 2023 15:07:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/deeplinks/2023/07/deja-vu-fbi-proves-again-it-cant-be-trusted-section-702">https://www.eff.org/deeplinks/2023/07/deja-vu-fbi-proves-again-it-cant-be-trusted-section-702</a>, See on <a href="https://news.ycombinator.com/item?id=37176717">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article role="article">
  
  
  <div><p><span>We all deserve privacy in our communications, and part of that is trusting that the government will only access them within the limits of the law. But at this point, it’s crystal clear that the FBI doesn’t believe that either our rights nor the limitations that Congress has placed upon the bureau matter when it comes to the vast amount of information about us collected under FISA Section 702.&nbsp; <br></span></p>
<p><span>How many times will the FBI get caught with their hand in the cookie jar of our </span><span>constitutionally protected private communications </span><span>without losing these invasive and unconstitutional powers?</span></p>
<p><span>The latest exhibit in this is in yet another newly declassified opinion of the Foreign Intelligence Surveillance Court (FISC). This opinion further reiterates </span><a href="https://www.eff.org/deeplinks/2023/05/newly-public-fisc-opinion-best-evidence-why-congress-must-end-section-702"><span>what we already know</span></a><span>, that the Federal Bureau of Investigation simply cannot be trusted with conducting foreign intelligence queries on American persons.&nbsp; Regardless of the rules, or consistent FISC disapprovals, the FBI continues to act in a way that shows no regard for privacy and civil liberties. <br></span></p>
<p><span>According to the </span><a href="https://www.intel.gov/assets/documents/702%20Documents/declassified/2023/FISC_2023_FISA_702_Certifications_Opinion_April11_2023.pdf"><span>declassified FISC ruling</span></a><span>, despite paper reforms which the FBI has touted that it put into place to respond to the last time it was caught violating U.S. law, the Bureau conducted four queries for the communications of a state senator and a U.S. senator.&nbsp; And they did so without even meeting their own&nbsp; already-inadequate standards for these kinds of searches. <br></span></p>
<p><span>How many times will the FBI get caught with their hand in the cookie jar of our </span><a href="https://www.eff.org/deeplinks/2023/06/foreign-intelligence-surveillance-court-has-made-mockery-constitutional-right"><span>constitutionally protected private communications</span></a><span> </span><span>without losing these invasive and unconstitutional powers?</span></p>
<p><span>Specifically, this disclosure concerns Section 702 of the 2008 Foreign Intelligence Surveillance Amendments Act, which authorizes the collection of overseas communications that can be queried by intelligence agencies in national security investigations under the oversight of the FISC. The FBI has access to the collected information, but only for limited purposes—purposes which it routinely and grossly oversteps.</span></p>
<p><span>Apart from the FBI’s apparent failure to even abide by its own rules, the bigger problem with this arrangement—even under the law—is that we live in a globalized world where U.S. persons regularly communicate with people in other countries. This creates a massive pool of digital communications in which one side of the conversation is an American on U.S. soil. The FBI, investigating crimes in the U.S., has spent the better part of 15 years sifting through these communications without even a warrant. So the fact that they cannot even abide by their own rules, much less the ones set by Congress, is a big deal.<br></span></p>
<p><a href="https://www.eff.org/deeplinks/2023/03/section-702s-unconstitutional-domestic-spying-program-must-end"><span>But now we have a chance to close this unconstitutional loophole</span></a><span> and block the FBI—or any other government agency—from searching any of our communications without a warrant. Section 702 is set to expire in December 2023.&nbsp; Sadly, both the FBI and Biden Administration have signaled that they are all in when it comes to trying to keep open the FBI’s warrantless backdoor searches of 702 data. They like their hands fully in the cookie jar and at this point are likely confident that, even when they get caught, the FISC won’t take any serious steps to stop them.</span></p>
<p><span>But they won’t get that renewal without a fight. After several hearings in the House Judiciary Committee, it is clear that there is </span><a href="https://cyberscoop.com/congress-fbi-section-702/"><span>bipartisan support</span></a><span> </span><span>for the idea that Section 702 must drastically change, or else face termination (called sunsetting in DC) entirely. The Privacy and Civil Liberties Oversight Board (PCLOB), which has been unwilling to seriously take on 702 violations, even suggested before Congress that some </span><a href="https://www.eff.org/deeplinks/2023/04/congressional-hearing-pclob-members-suggest-bare-minimum-702-reforms"><span>bare minimum of changes</span></a><span> </span><span>should be made to the surveillance programs in order to protect the privacy rights of Americans. <br></span></p>
<p><span>While we think it’s time for 702 to end entirely, and for any future programs to start from scratch in protecting the privacy of digital communications. EFF will continue to fight to make sure that any bill that does renew Section 702 closes the government’s warrantless access to U.S. communications, minimizes the amount of data collected, and increases transparency. Anything less than that would signal a continued indifference, or contempt, to our right to privacy.<br></span></p>
<p><span>This recent disclosure proves, in a </span><i><span>Groundhog Day</span></i><span>-like fashion, that the FBI is not going to suddenly become good at self-control when it comes to access to our data. If the privacy of our communications—including communications with people abroad—is going to actually matter, Section 702 must be irrevocably changed or jettisoned entirely.</span></p>

</div>

          </article>
    </div><div>
          <h2>Join EFF Lists</h2>
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ask vs. Guess Culture (734 pts)]]></title>
            <link>https://jeanhsu.substack.com/p/ask-vs-guess-culture</link>
            <guid>37176703</guid>
            <pubDate>Fri, 18 Aug 2023 15:06:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jeanhsu.substack.com/p/ask-vs-guess-culture">https://jeanhsu.substack.com/p/ask-vs-guess-culture</a>, See on <a href="https://news.ycombinator.com/item?id=37176703">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Have you had someone ask you for a favor that seemed unreasonable — a referral you didn’t want to make, a long-term stay at your place, a sizable cash loan? But because they asked, you felt obliged to seriously consider it, to try to meet their request, even if it put you in a space of discomfort? Maybe you carry out the favor, but it sours your relationship, and when it all comes out, that person says, “Well why’d you agree to it? You could have just said no!”</p><p>But you feel resentful that that person even put you in a position to have to say, “Sorry we’re a bit busy that week so don’t have space for you to stay with us,” or “I can’t loan you that money at the moment”?</p><p>Congratulations, you’ve just encountered a clash between ask culture and guess culture.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d15f5ae-994c-49b5-99a6-5efff02b7eca_1456x816.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d15f5ae-994c-49b5-99a6-5efff02b7eca_1456x816.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d15f5ae-994c-49b5-99a6-5efff02b7eca_1456x816.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d15f5ae-994c-49b5-99a6-5efff02b7eca_1456x816.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d15f5ae-994c-49b5-99a6-5efff02b7eca_1456x816.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d15f5ae-994c-49b5-99a6-5efff02b7eca_1456x816.png" width="1456" height="816" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5d15f5ae-994c-49b5-99a6-5efff02b7eca_1456x816.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:816,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Image&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="Image" title="Image" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d15f5ae-994c-49b5-99a6-5efff02b7eca_1456x816.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d15f5ae-994c-49b5-99a6-5efff02b7eca_1456x816.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d15f5ae-994c-49b5-99a6-5efff02b7eca_1456x816.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d15f5ae-994c-49b5-99a6-5efff02b7eca_1456x816.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>The idea of ask vs guess culture was </span><a href="https://ask.metafilter.com/55153/Whats-the-middle-ground-between-FU-and-Welcome#830421" rel="">shared online</a><span> in 2007 by a user </span><em>tangerine</em><span> on Metafilter. When I first read it years ago, a lightbulb moment went off, and many frustrations and conflicts I had while growing up made much more sense in this framework. </span></p><p>Despite this idea’s longevity, I find that it’s still a new-to-many and incredibly useful concept to revisit, so here’s a little exploration of ask vs guess culture at home and at work.</p><p><span>Ask culture and guess culture are </span><em>vastly</em><span> different in behavior and expectations. Here are some highlights:</span></p><ul><li><p>Ask for what you want, even if it seems out of reach or like a big unreasonable request</p></li><li><p>Take care of your own needs, and others will take care of theirs</p></li><li><p>It’s fine to make requests that people will probably say no to</p></li><li><p>People say yes to requests that you truly feel good about, say no to ones they don’t</p></li></ul><ul><li><p>Only ask for something if you’re already pretty sure the other person will say yes</p></li><li><p>Read an abundance of indirect contextual cues to determine if your request is reasonable to make</p></li><li><p>It’s rude to put someone in a position where they have to say no to you</p></li><li><p>If the appropriate feelers and context are set, you will never have to make your request at all.</p></li></ul><p>It’s easiest to understand the differences between ask culture and guess culture through examples, so here are two examples with a moving situation — you’re moving soon and hope to save a few bucks with the help of your friends.</p><p>You post on Facebook sharing that you’re moving and make a list of things you could use help with: moving boxes and tape, packing help, usage of a truck/van, and physical labor on moving day. You reach out to a few local friends asking if they’re available on moving day. A few people respond on Facebook with moving supplies, and a friend comes over to help with packing, but no one is available to help on moving day, so you end up renting a moving van and hiring a few movers.</p><p>Your friend is typically free on the weekends, so you ask them if they’re available to help you on moving day. You ask another friend what they’re up to, and they have family visiting, so you don’t mention that you need help with moving. Another friend has access to a pickup truck, and you dropped off some soup recently when they were sick, so you mention that you’re moving next weekend. They ask if you’d like to borrow their truck, which you defer saying you don’t want to inconvenience them, but when they offer again, you accept.</p><p>Depending on whether you gravitate more towards ask or guess culture as your default, one of these scenarios may sound very uncomfortable. </p><p>If you’re more a guess-culture person, asking people for help without knowing their circumstances can feel rude or intrusive. Broadcasting publicly your need for help can feel awkward and vulnerable.</p><p>If you’re more of an ask-culture person, the guess-culture example of juggling everyone’s specific scenarios and the historical context of favors probably seems exhausting. Dropping hints in the hopes that you won’t even have to make your request can feel extra passive and manipulative.</p><p>I probably operate somewhere in-between ask and guess culture&nbsp;— defaulting to guess culture when I’m low-functioning but aspiring to be more and more ask-culture. </p><p>I was raised deeply in guess culture, as many Asians and Asian-Americans are. The Japanese proverb that “the nail that sticks up gets nailed down” reinforces the idea of social collectivism and keeping your individual needs and wants to yourself&nbsp;— values that are shared by many Asian culture. My parents rarely had to make explicit asks of me, because the expectations around values and behavior were communicated through indirect messaging, often by tone of voice or through stories about other people. </p><p>Western society is very much ask culture. A classic example can be found in proverbs. “A squeaky wheel gets the grease” is an American proverb, enforcing the ideas of individualism and that asking for what you want will benefit you. </p><p>The generational clash between ask and guess culture can be frustrating and exhausting. Years ago, my brother and I were in San Diego visiting our aunt and uncle. The plan was for my grandma to come down from Los Angeles, so we could all spend time together, but our grandma had last minute minor surgery to recover from and had to stay put in LA. “So we should drive up to see her, right?” my brother and I discussed. But all of the older relatives insisted we did not, suggesting that instead we see the sights in San Diego, that we take the kids to Sea World, that the traffic would be awful and that a 2 hour drive would turn into 5 hours, that it’d be dangerous. </p><p><span>This all seemed ridiculous to us, so instead we drove the two hours, keeping our plan </span><em>secret</em><span> until we pulled up into our grandma’s driveway, so that no one could resist and thwart our plan. We had a lovely visit, and my mom later thanked us for making the drive. </span></p><p>This is guess culture — and it’s a lot of saying not really what you actually want, and it’s a lot of reading between the lines to try to figure out what people want.</p><p><span>Deciding what to eat for dinner with guess-culture people isn’t as simple as asking people what they want to eat for dinner, because they will not tell you what they </span><strong>actually</strong><span> want to want to eat for dinner. They will say “oh, whatever you want,” or “whatever is easiest.” And when you insist that you really really want to know what they want to eat for dinner, and if it’s too much work, you’ll do something else instead, the response you receive will already be a compromised version of what they want, taking into account the preferences of everyone else in the house, what the kids will eat, and the leftovers in the fridge.</span></p><p>Thoughtful? Yes. But frustrating if you actually want an honest answer of what someone wants. You may be better off listing options and gauging their response to each one.</p><p>For guess-culture people, thinking about what it is you want can feel absolutely foreign, and for me, it’s been a years-long practice to continue to tap into and understand what I want, before I then try to take others’ needs into account.</p><p>At a high level, western corporate work operates almost entirely in ask culture. But people working at these companies often operate in or were raised in guess culture, which as you might expect, is ripe for feeling misunderstood and frustrated.</p><p><span>Last week, I shared about normalizing </span><a href="https://jeanhsu.substack.com/p/do-you-keep-your-wishes-secret" rel="">sharing we want in life and at work</a><span>, so that they might actually be supported in coming true. Ask vs guess culture is another lens at looking at asking for what you want at work.</span></p><p>Being guess-culture in an ask-culture work environment looks like hoping someone will tap you to become a manager because you’re clearly the best person for the job. </p><p>It also looks like being frustrated when others loudly express enthusiasm about taking on a new project on the roadmap and are given the opportunity to lead it, when you were also interested in it and maybe dropped some hints about it being somewhat interesting.</p><p>At a certain point, guess culture is not going to work for you, and you’ll feel under-acknowledged and overlooked. If you want to get more of what you want out of your work situation, you’ll have to lean more into ask culture. </p><p>But ask culture is vulnerable, because the requests you’re making are ones that feel out of reach, and requires being ok with people saying no to you, often. It requires putting things out there that you want help with, and trusting that people will say no to you instead of helping you resentfully.</p><p>Maybe just understanding the framework is already helpful, but here are a few small ways you can start to nudge yourself into ask culture.</p><ul><li><p>Ask for help on something you’re feeling stuck on. Guess-culture people will worry that they’re interrupting someone, or someone will be annoyed if they’re in the middle of something. If it feels more comfortable, you could say, “Let me know when is good for a half-hour working session today or tomorrow.”</p></li><li><p>Want to publish something on the company blog or give a talk at an upcoming event? Try asking. If “Hey can I give a talk at the next event?” feels too uncomfortable, try “Hey I’m really interested in giving a talk at a future event. What are you looking for?” or “I’d love to give a talk about &lt;topic&gt;, what do you think?”</p></li><li><p>Get more comfortable with people saying no to you. If people are not saying no to you, you’re probably still only asking for things that you already know people will say yes to (which is guess culture). Ask for more budget, ask for an uncomfortable amount of PTO, ask for professional development budget, ask to purchase only vaguely-work-related books on your company card.</p></li><li><p><span>Ask yourself, </span><a href="https://jeanhsu.substack.com/p/if-you-could-have-your-way" rel="">“If I could have my way…”</a><span>, which is a useful hack to bypass thinking about others’ needs and honing in on exactly what you want. Use this to think about your role, your next project, your work schedule, your title, your salary and equity, your team. From that thought exercise, ask for some things you want.</span></p></li></ul><p><em>Does this framework behind ask vs guess culture provide any clarity to you around past or current frustrations? When have you experienced clashes between ask and guess culture?</em></p><p data-attrs="{&quot;url&quot;:&quot;https://jeanhsu.substack.com/p/ask-vs-guess-culture/comments&quot;,&quot;text&quot;:&quot;Leave a comment&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://jeanhsu.substack.com/p/ask-vs-guess-culture/comments" rel=""><span>Leave a comment</span></a></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft's backwards compatibility is insane (277 pts)]]></title>
            <link>https://twitter.com/mikko/status/1692503249595584526</link>
            <guid>37176666</guid>
            <pubDate>Fri, 18 Aug 2023 15:04:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/mikko/status/1692503249595584526">https://twitter.com/mikko/status/1692503249595584526</a>, See on <a href="https://news.ycombinator.com/item?id=37176666">Hacker News</a></p>
Couldn't get https://twitter.com/mikko/status/1692503249595584526: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Revealed: WHO aspartame safety panel linked to alleged Coca-Cola front group (157 pts)]]></title>
            <link>https://www.theguardian.com/business/2023/aug/17/who-panel-aspartame-diet-coke-guidelines</link>
            <guid>37176555</guid>
            <pubDate>Fri, 18 Aug 2023 14:56:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/business/2023/aug/17/who-panel-aspartame-diet-coke-guidelines">https://www.theguardian.com/business/2023/aug/17/who-panel-aspartame-diet-coke-guidelines</a>, See on <a href="https://news.ycombinator.com/item?id=37176555">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>In May, the World Health Organization issued an alarming <a href="https://www.who.int/news/item/15-05-2023-who-advises-not-to-use-non-sugar-sweeteners-for-weight-control-in-newly-released-guideline" data-link-name="in body link">report</a> that declared widely used non-sugar sweeteners like aspartame are likely ineffective for weight loss, and long term consumption may increase the risk of diabetes, cardiovascular diseases and mortality in adults.</p><p>A few months later, WHO declared aspartame, a key ingredient in Diet Coke, to be a “possible carcinogen”, then quickly issued a third report that seemed to contradict its previous findings – people could continue consuming the product at levels determined to be safe decades ago, before new science cited by WHO raised health concerns.</p><p>That contradiction stems from beverage industry corruption of the review process by consultants tied to an alleged Coca-Cola front group, the public health advocacy group US Right-To-Know said in a recent <a href="https://usrtk.org/sweeteners/coca-cola-front-group-who-review-of-aspartame/" data-link-name="in body link">report</a>.</p><p>It uncovered eight WHO panelists involved with assessing safe levels of aspartame consumption who are beverage industry consultants who currently or previously worked with the alleged Coke front group, International Life Sciences Institute (Ilsi).</p><p>Their involvement in developing intake guidelines represents “an obvious conflict of interest”, said Gary Ruskin, US Right-To-Know’s executive director. “Because of this conflict of interest, [the daily intake] conclusions about aspartame are not credible, and the public should not rely on them,” he added.</p><p>Aspartame was first approved for use in the US in the early 1980s over the objection of some researchers who warned of potential health risks. In recent years, as <a href="http://www.mpwhi.com/soffritti_2010_20896_fta.pdf" data-link-name="in body link">evidence</a> of health threats has mounted, industry has ramped up a PR <a href="https://www.nbcnews.com/healthmain/amid-falling-sales-coke-launches-campaign-defend-sweetener-6c10920355" data-link-name="in body link">campaign</a> to downplay the issues.</p><p>In the World Health Organization’s 14 July aspartame hazard and risk <a href="https://www.iarc.who.int/news-events/aspartame-hazard-and-risk-assessment-results-released/" data-link-name="in body link">assessments</a>, its cancer research arm, the International Agency for Research on Cancer (Iarc) <a href="https://www.who.int/news/item/14-07-2023-aspartame-hazard-and-risk-assessment-results-released" data-link-name="in body link">classified</a> aspartame as “<a href="https://www.thelancet.com/journals/lanonc/article/PIIS1470-2045(23)00341-8/fulltext" data-link-name="in body link">possibly carcinogenic</a>”. That same day, WHO’s Joint Expert Committee on Food Additives (Jecfa), which makes consumption recommendations, <a href="https://cdn.who.int/media/docs/default-source/food-safety/jecfa/summary-and-conclusions/jecfa96-summary-and-conclusions.pdf?sfvrsn=f7b61f6c_4&amp;download=true" data-link-name="in body link">reaffirmed</a> the acceptable daily intake of 40 mg/kg of body weight.</p><p>Ilsi describes itself as a nonprofit that conducts “science for the public good”, but it was founded in 1978 by a Coca-Cola executive who simultaneously worked for the company through 2021, US Right-To-Know found. Other Coca-Cola executives have worked with the group, and US Right-To-Know <a href="https://usrtk.org/pesticides/ilsi-is-a-food-industry-lobby-group/" data-link-name="in body link">detailed</a> tax returns that show millions in donations from Coca-Cola and other beverage industry players. Coke <a href="https://www.bloomberg.com/news/articles/2021-01-13/coca-cola-severs-longtime-ties-with-pro-sugar-industry-group#xj4y7vzkg" data-link-name="in body link">ended</a> its official membership with the group in 2021.</p><p>Over the years, Ilsi representatives have sought to shape food policy worldwide, and Ruskin, who has written multiple peer-reviewed papers on the group, characterized the aspartame controversy as a “masterpiece in how Ilsi worms its way into these regulatory processes”.</p><p>US Right To Know identified six out of 13 Jefca panel members with ties to the industry group. After it released its report, the WHO acknowledged two more of its members with industry ties.</p><p>In a statement to the Guardian, a WHO spokesperson defended the industry consultants’ inclusion in the review process.</p><p>“For the meeting on aspartame, Jefca selected the experts likely to make the best contributions to the debate,” said spokesperson Fadéla Chaib. She said WHO’s guidelines only require disclosure of conflicts of interest within the last four years.</p><p>“To our knowledge, the experts you listed by name have not participated in any Ilsi activities for at least 10 years,” she said.</p><figure data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.NewsletterSignupBlockElement"><a data-ignore="global-link-styling" href="#EmailSignup-skip-link-13">skip past newsletter promotion</a><p id="EmailSignup-skip-link-13" tabindex="0" aria-label="after newsletter promotion" role="note">after newsletter promotion</p></figure><p>But that partially contradicts a statement made by WHO just weeks before to the news outlet <a href="https://www.leparisien.fr/sciences/nouvelles-recommandations-sur-laspartame-les-liaisons-dangereuses-de-certains-experts-avec-coca-et-pepsi-19-07-2023-J3BYCFSOJJG7ZGHJEHFOVR5XFM.php" data-link-name="in body link">Le Parisien</a> in which it acknowledged two people currently working with Ilsi were involved in the process. The Guardian had also asked about those two people identified in the Parisien story but were not listed “by name” in its email.</p><p>The WHO told Le Parisien: “We regret that this interest was not declared by these two experts either in the written statement or orally at the opening of the meeting.”</p><p>WHO’s inclusion of Ilsi-tied consultants in its review process is especially alarming because WHO has in place “much higher standards” to ensure there are no conflicts of interest in its processes, Ruskin said. He noted WHO only relies on publicly available, peer-reviewed science, while excluding corporate interest studies.</p><p>Ruskin said the move also marks a change in direction for WHO, which in 2015 distanced itself from Ilsi when its executive board found the group to be a “private entity” and voted to <a href="https://usrtk.org/wp-content/uploads/2018/07/ILSI-Official-Relations-WHO.pdf" data-link-name="in body link">discontinue</a> its official relationship.</p><p>Ruskin said the damage has been done. In the “avalanche” of media coverage of WHO’s designation of aspartame as a possible carcinogen, many outlets noted WHO’s split decision, or reported that WHO <a href="https://www.theguardian.com/society/2023/jul/14/aspartame-is-safe-in-limited-amounts-say-experts-after-cancer-warning" data-link-name="in body link">found the product to be safe</a>. Those reports did not note Ilsi’s fingerprints on the safety assessment, Ruskin said.</p><p>“So much of the tone of it has been ‘There was a split decision at WHO and we shouldn’t be concerned, so go ahead and drink all you want,’” he said. “That has so poorly served the public.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why GNU su does not support the `wheel' group (2002) (133 pts)]]></title>
            <link>https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_149.html</link>
            <guid>37175754</guid>
            <pubDate>Fri, 18 Aug 2023 14:03:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_149.html">https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_149.html</a>, See on <a href="https://news.ycombinator.com/item?id=37175754">Hacker News</a></p>
<div id="readability-page-1" class="page">

<a name="SEC149"></a>
<table>
<tbody><tr><td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_148.html#SEC148"> &lt; </a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_150.html#SEC151"> &gt; </a>]</td>
<td> &nbsp; </td><td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_144.html#SEC144"> &lt;&lt; </a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_144.html#SEC144"> Up </a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_150.html#SEC151"> &gt;&gt; </a>]</td>
<td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils.html#SEC_Top">Top</a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_toc.html#SEC_Contents">Contents</a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_186.html#SEC187">Index</a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_abt.html#SEC_About"> ? </a>]</td>
</tr></tbody></table>
<hr size="1">
<h2> 22.5 <code>su</code>: Run a command with substitute user and group id </h2>
<!--docid::SEC149::-->
<p>

<code>su</code> allows one user to temporarily become another user.  It runs a
command (often an interactive shell) with the real and effective user
id, group id, and supplemental groups of a given <var>user</var>. Synopsis:
</p><p>

<table><tbody><tr><td>&nbsp;</td><td><pre>su [<var>option</var>]<small>...</small> [<var>user</var> [<var>arg</var>]<small>...</small>]
</pre></td></tr></tbody></table></p><p>

<a name="IDX1807"></a>
<a name="IDX1808"></a>
<a name="IDX1809"></a>
If no <var>user</var> is given, the default is <code>root</code>, the super-user.
The shell to use is taken from <var>user</var>'s <code>passwd</code> entry, or
`<tt>/bin/sh</tt>' if none is specified there.  If <var>user</var> has a
password, <code>su</code> prompts for the password unless run by a user with
effective user id of zero (the super-user).
</p><p>

<a name="IDX1810"></a>
<a name="IDX1811"></a>
<a name="IDX1812"></a>
<a name="IDX1813"></a>
<a name="IDX1814"></a>
By default, <code>su</code> does not change the current directory.
It sets the environment variables <code>HOME</code> and <code>SHELL</code>
from the password entry for <var>user</var>, and if <var>user</var> is not
the super-user, sets <code>USER</code> and <code>LOGNAME</code> to <var>user</var>.
By default, the shell is not a login shell.
</p><p>

Any additional <var>arg</var>s are passed as additional arguments to the
shell.
</p><p>

<a name="IDX1815"></a>
GNU <code>su</code> does not treat `<tt>/bin/sh</tt>' or any other shells specially
(e.g., by setting <code>argv[0]</code> to `<samp>-su</samp>', passing <code>-c</code> only
to certain shells, etc.).
</p><p>

<a name="IDX1816"></a>
<code>su</code> can optionally be compiled to use <code>syslog</code> to report
failed, and optionally successful, <code>su</code> attempts.  (If the system
supports <code>syslog</code>.)  However, GNU <code>su</code> does not check if the
user is a member of the <code>wheel</code> group; see below.
</p><p>

The program accepts the following options.  Also see <a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_2.html#SEC2">2. Common options</a>.
</p><dl compact="">
<dt>`<samp>-c <var>command</var></samp>'
</dt><dd></dd><dt>`<samp>--command=<var>command</var></samp>'
</dt><dd><a name="IDX1817"></a>
<a name="IDX1818"></a>
Pass <var>command</var>, a single command line to run, to the shell with
a <code>-c</code> option instead of starting an interactive shell.
</dd><dt>`<samp>-f</samp>'
</dt><dd></dd><dt>`<samp>--fast</samp>'
</dt><dd><a name="IDX1819"></a>
<a name="IDX1820"></a>
<a name="IDX1821"></a>
<a name="IDX1822"></a>
<a name="IDX1823"></a>
Pass the <code>-f</code> option to the shell.  This probably only makes sense
if the shell run is <code>csh</code> or <code>tcsh</code>, for which the <code>-f</code>
option prevents reading the startup file (`<tt>.cshrc</tt>').  With
Bourne-like shells, the <code>-f</code> option disables file name pattern
expansion (globbing), which is not likely to be useful.
</dd><dt>`<samp>-</samp>'
</dt><dd></dd><dt>`<samp>-l</samp>'
</dt><dd></dd><dt>`<samp>--login</samp>'
</dt><dd><a name="IDX1824"></a>
<a name="IDX1825"></a>
<a name="IDX1826"></a>
<a name="IDX1827"></a>
<a name="IDX1828"></a>
<a name="IDX1829"></a>
Make the shell a login shell.  This means the following.  Unset all
environment variables except <code>TERM</code>, <code>HOME</code>, and <code>SHELL</code>
(which are set as described above), and <code>USER</code> and <code>LOGNAME</code>
(which are set, even for the super-user, as described above), and set
<code>PATH</code> to a compiled-in default value.  Change to <var>user</var>'s home
directory.  Prepend `<samp>-</samp>' to the shell's name, intended to make it
read its login startup file(s).
</dd><dt>`<samp>-m</samp>'
</dt><dd></dd><dt>`<samp>-p</samp>'
</dt><dd></dd><dt>`<samp>--preserve-environment</samp>'
</dt><dd><a name="IDX1830"></a>
<a name="IDX1831"></a>
<a name="IDX1832"></a>
<a name="IDX1833"></a>
<a name="IDX1834"></a>
<a name="IDX1835"></a>
Do not change the environment variables <code>HOME</code>, <code>USER</code>,
<code>LOGNAME</code>, or <code>SHELL</code>.  Run the shell given in the environment
variable <code>SHELL</code> instead of the shell from <var>user</var>'s passwd
entry, unless the user running <code>su</code> is not the superuser and
<var>user</var>'s shell is restricted.  A <em>restricted shell</em> is one that
is not listed in the file `<tt>/etc/shells</tt>', or in a compiled-in list
if that file does not exist.  Parts of what this option does can be
overridden by <code>--login</code> and <code>--shell</code>.
</dd><dt>`<samp>-s <var>shell</var></samp>'
</dt><dd></dd><dt>`<samp>--shell=<var>shell</var></samp>'
</dt><dd><a name="IDX1836"></a>
<a name="IDX1837"></a>
Run <var>shell</var> instead of the shell from <var>user</var>'s passwd entry,
unless the user running <code>su</code> is not the superuser and <var>user</var>'s
shell is restricted (see `<samp>-m</samp>' just above).
</dd></dl>
<h2> Why GNU <code>su</code> does not support the `<samp>wheel</samp>' group </h2>
<!--docid::SEC150::-->
<p>

(This section is by Richard Stallman.)
</p><p>

<a name="IDX1841"></a>
<a name="IDX1842"></a>
Sometimes a few of the users try to hold total power over all the
rest.  For example, in 1984, a few users at the MIT AI lab decided to
seize power by changing the operator password on the Twenex system and
keeping it secret from everyone else.  (I was able to thwart this coup
and give power back to the users by patching the kernel, but I
wouldn't know how to do that in Unix.)
</p><p>

However, occasionally the rulers do tell someone.  Under the usual
<code>su</code> mechanism, once someone learns the root password who
sympathizes with the ordinary users, he or she can tell the rest.  The
"wheel group" feature would make this impossible, and thus cement the
power of the rulers.
</p><p>

I'm on the side of the masses, not that of the rulers.  If you are
used to supporting the bosses and sysadmins in whatever they do, you
might find this idea strange at first.
</p><hr size="1">
<table>
<tbody><tr><td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_148.html#SEC148"> &lt; </a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_150.html#SEC151"> &gt; </a>]</td>
<td> &nbsp; </td><td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_144.html#SEC144"> &lt;&lt; </a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_144.html#SEC144"> Up </a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_150.html#SEC151"> &gt;&gt; </a>]</td>
<td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils.html#SEC_Top">Top</a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_toc.html#SEC_Contents">Contents</a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_186.html#SEC187">Index</a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_abt.html#SEC_About"> ? </a>]</td>
</tr></tbody></table>
<br>  
<span size="-1">
This document was generated
by <i>Jeff Bailey</i> on <i>December, 28  2002</i>
using <a href="http://www.mathematik.uni-kl.de/~obachman/Texi2html"><i>texi2html</i></a>
</span>



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sargablock: Bricks from Seaweed (151 pts)]]></title>
            <link>https://fortomorrow.org/explore-solutions/sargablock</link>
            <guid>37175721</guid>
            <pubDate>Fri, 18 Aug 2023 14:00:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fortomorrow.org/explore-solutions/sargablock">https://fortomorrow.org/explore-solutions/sargablock</a>, See on <a href="https://news.ycombinator.com/item?id=37175721">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="presentation-panel" role="tabpanel" aria-labelledby="presentation-tab"><p>My journey began in 2015 when sargassum seaweed first started washing up on the shores of the Riviera Maya. Where others saw a problem, I saw an opportunity to turn it into its own sustainable solution, including placing it in service of people who need it the most.</p><p><img src="https://fortomorrowprod.blob.core.windows.net/assets/default-path/Sargablock3_10db329dbe.png" alt=""></p><p>I started collecting sargassum seaweed to use as fertilizer for my business, Blue-Green Nursery, and selling it in small amounts to my clients. Soon I obtained permits and within a year was employing about 300 families to clean the beaches for local hotels and resorts. But then it occurred to me that we could turn sargassum seaweed into construction bricks as it was already being used to make products like plates and other things. Inspired by the memory of my family’s little adobe house, I developed Sargablock, an architectural brick made from the sargassum seaweed that spoils our beaches between April and October.</p><p><img src="https://fortomorrowprod.blob.core.windows.net/assets/default-path/Sargablock1_d10b266802.png" alt=""></p><p>&nbsp;I adjusted a machine designed to make adobe bricks so that it can process a mix of 40% sargassum and 60% other organic materials for the Sargablock. The machine can turn out 1,000 blocks a day, and after four hours of baking in the sun, they are dried and ready to be used. After we built Casa Angelita, the first sargassum house named after my mother, Sargablock became one of the first seaweed projects to get off to a solid start in our state of Quintana Roo. From there, I was determined to use sargassum seaweed as a low-cost building material to build affordable housing throughout the Riviera Maya so that families can live in their own homes.</p><p><img src="https://fortomorrowprod.blob.core.windows.net/assets/default-path/15_sargassum_house_001_fc796ef165.jpeg" alt="">&nbsp;</p><p>Now we have a first artisanal factory in Mahahual, a tourist town where a dock for cruise ships operates, and we are creating jobs. A sargassum house could last 120 years, and we would like to have 10 houses finished by the end of the year, which will be donated to underprivileged families. My vision goes beyond turning a profit; I would like to see a country where local entrepreneurs create thriving, sustainable businesses that give back to their communities.</p><p>People from countries like Belize, Jamaica, Puerto Rico, the Dominican Republic, Barbados, Malaysia and the United States are contacting us for help in doing what we are doing. They’re all hit harder every day by the seaweed washing up on their beaches. It is maybe nature's way of telling us to protect our seas. We must be aware of what is happening on the beaches. The sea is very wise and is telling us something. With the contamination we have created over so many years it tells us to take care of what we have.</p><p><strong>Read more:</strong> Exposure article: Sea Change (<a href="https://undplac.exposure.co/sea-change" target="_blank">English</a> and <a href="https://undplac.exposure.co/un-cambio-para-el-oceano" target="_blank">Spanish</a>) - published on the 13th of February, 2020) by UNDP Mexico Environmental and Resilience Unit. Text by Emily Mkrtichian, Edited by Andrea Egan</p><p><em>Photography: Emily Mkrtichian for UNDP Mexico, with exception of the image of sargassum on the beach courtesy of Elena Tarassova.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mister Rogers Had a Point: Routinely Greeting Six Neighbors Maximizes Wellbeing (360 pts)]]></title>
            <link>https://www.goodnewsnetwork.org/mister-rogers-had-a-point-routinely-greeting-six-neighbors-maximizes-wellbeing-outcomes/</link>
            <guid>37175432</guid>
            <pubDate>Fri, 18 Aug 2023 13:36:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.goodnewsnetwork.org/mister-rogers-had-a-point-routinely-greeting-six-neighbors-maximizes-wellbeing-outcomes/">https://www.goodnewsnetwork.org/mister-rogers-had-a-point-routinely-greeting-six-neighbors-maximizes-wellbeing-outcomes/</a>, See on <a href="https://news.ycombinator.com/item?id=37175432">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <figure id="attachment_196771" aria-describedby="caption-attachment-196771"><img decoding="async" src="https://www.goodnewsnetwork.org/wp-content/uploads/2023/08/Mr-Fred-Rogers-CC-2.0.-Intergalactically-Speaking-retireved-via-Flickr-Copy.jpg" alt="" width="800" height="400" srcset="https://www.goodnewsnetwork.org/wp-content/uploads/2023/08/Mr-Fred-Rogers-CC-2.0.-Intergalactically-Speaking-retireved-via-Flickr-Copy.jpg 800w, https://www.goodnewsnetwork.org/wp-content/uploads/2023/08/Mr-Fred-Rogers-CC-2.0.-Intergalactically-Speaking-retireved-via-Flickr-Copy-326x163.jpg 326w, https://www.goodnewsnetwork.org/wp-content/uploads/2023/08/Mr-Fred-Rogers-CC-2.0.-Intergalactically-Speaking-retireved-via-Flickr-Copy-768x384.jpg 768w, https://www.goodnewsnetwork.org/wp-content/uploads/2023/08/Mr-Fred-Rogers-CC-2.0.-Intergalactically-Speaking-retireved-via-Flickr-Copy-696x348.jpg 696w, https://www.goodnewsnetwork.org/wp-content/uploads/2023/08/Mr-Fred-Rogers-CC-2.0.-Intergalactically-Speaking-retireved-via-Flickr-Copy-530x265.jpg 530w" sizes="(max-width: 800px) 100vw, 800px"><figcaption id="caption-attachment-196771">Mr Fred Rogers – CC 2.0. Intergalactically Speaking, retrieved via Flickr&nbsp;</figcaption></figure>
<p>It’s a beautiful day in the neighborhood, haven’t you heard? Mister Rogers said so—and now his simple advice on how to be a good person has been backed by sophisticated polling data.</p>
<p>As part of the Gallup National Health and Well-Being Index, saying hello to more than 1 neighbor was shown to correlate with greater self-perception of well-being.</p>
<p>Averaged across five dimensions that included career, communal, physical, financial, and social well-being, the increase which greeting a neighbor had led to around a 2-point increase on a scale of 0-100 up until the sixth neighbor, at which point further greetings had no measured impact.</p>
<p>Interestingly, when the well-being scores are looked at individually and not averaged together, the sixth neighbor is where the perception of well-being in life peaks for social and communal well-being, but not financial well-being.</p>
<p>No; perception of financial well-being kept on climbing and climbing, only to cease at the 11th such greeting; a profound revelation—repeated positive social interaction benefited perception of personal finance even more than personal sense of community.</p>
<p>Men were more likely to greet neighbors than women, as were people with children under the age of 18 in the household, and people with a household income of more than $120k a year.</p>
<p>Individuals aged 40 to 65+ were the most common greeters of neighbors, and 27% of the over 4,000 participants greeted 5 neighbors or more in a day.</p>
<p><strong>MORE INSPIRING POLLS: </strong><a title="Survey Reveals Americans are Retiring Earlier Than They’d Expected" href="https://www.goodnewsnetwork.org/survey-reveals-americans-are-retiring-earlier-than-theyd-expected/" rel="bookmark">Survey Reveals Americans are Retiring Earlier Than They’d Expected</a></p>
<p>“Recent Gallup research in partnership with Meta has shown that the U.S. compares favorably with other nations around the world in social interactions,” the polling company states, “with those in the U.S. more likely than those in countries such as Mexico, India, and France to interact with the people who live near them.”</p>
<p>“Notably, greeting neighbors is also linked to career wellbeing (liking what you do each day), physical wellbeing (having energy to get things done), and financial wellbeing (managing your money well),” the report continued. “The associations found among these latter three elements are likely more multifaceted in nature and could be reinforced in part through the correlations found with social and community wellbeing.”</p>
<p><strong>WATCH what used to be our daily reminder below…&nbsp;</strong></p>
 <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/FhAJnx5uwUU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> 
<p><strong>SHARE This Study (And Fred Rogers) Wisdom With Your Friends (And Neighbors)…</strong></p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Low dose radiation cancer 2x worse than predicted by LNT model (148 pts)]]></title>
            <link>https://www.bmj.com/content/382/bmj-2022-074520</link>
            <guid>37175228</guid>
            <pubDate>Fri, 18 Aug 2023 13:20:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bmj.com/content/382/bmj-2022-074520">https://www.bmj.com/content/382/bmj-2022-074520</a>, See on <a href="https://news.ycombinator.com/item?id=37175228">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Broadband monopolies push bill that would crush your ability to stand up to them (230 pts)]]></title>
            <link>https://www.techdirt.com/2023/08/18/with-hr-3557-broadband-monopolies-are-pushing-a-bill-that-would-crush-your-towns-ability-to-stand-up-to-them/</link>
            <guid>37175200</guid>
            <pubDate>Fri, 18 Aug 2023 13:17:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techdirt.com/2023/08/18/with-hr-3557-broadband-monopolies-are-pushing-a-bill-that-would-crush-your-towns-ability-to-stand-up-to-them/">https://www.techdirt.com/2023/08/18/with-hr-3557-broadband-monopolies-are-pushing-a-bill-that-would-crush-your-towns-ability-to-stand-up-to-them/</a>, See on <a href="https://news.ycombinator.com/item?id=37175200">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storywrap-418603">


<h3>from the <i>this-is-why-we-can't-have-nice-things</i> dept</h3>

<p>For thirty-plus years, giant telecom monopolies have worked tirelessly to crush all broadband competition. At the same time, they’ve lobbied state and federal governments so extensively, that the vast majority of politicians are feckless cardboard cutouts with little real interest in market or consumer health. </p>
<p>The result has been fairly obvious: Americans pay some of the highest prices in the developed world for sluggish, slow broadband with <a href="https://www.techdirt.com/2023/06/23/broadband-isp-customer-satisfaction-ranked-second-worst-above-only-gas-stations/" data-type="link" data-id="https://www.techdirt.com/2023/06/23/broadband-isp-customer-satisfaction-ranked-second-worst-above-only-gas-stations/">historically abysmal customer service</a>. </p>
<p>Telecom lobbyists love to insist that often-shitty U.S. broadband is the envy of the modern world (it isn’t). They also love to argue that the <strong>only</strong> reason U.S. broadband isn’t even <em>more awesome</em> is because of “too much government regulation,” unnecessary red tape, and “bureaucracy.”</p>
<p>In this way they get to have their cake (enjoy unchecked monopoly power free from competition or regulatory oversight) and eat it too (demonize <strong>any</strong> government effort to do <strong>anything</strong> about monopoly power as the antithesis of progress). It’s an endless cycle where your broadband monopoly gets more powerful and the government gets weaker. Often under the pretense of “deregulation” and “reform.”</p>
<p>It’s a pretty successful con. Telecom giants like Comcast and AT&amp;T have effectively lobotomized the country’s top federal telecom regulator (the FCC) for going on the <a href="https://www.techdirt.com/2023/06/02/telecom-lobbyists-have-had-the-fcc-under-their-boot-heel-for-7-straight-years-and-nobody-much-seems-to-care/" data-type="link" data-id="https://www.techdirt.com/2023/06/02/telecom-lobbyists-have-had-the-fcc-under-their-boot-heel-for-7-straight-years-and-nobody-much-seems-to-care/">better part of a decade</a>. They also have the majority of Congress under their bootheel, to the point where we can’t pass <a href="https://www.techdirt.com/2017/03/28/consumer-broadband-privacy-protections-are-dead/" data-type="link" data-id="https://www.techdirt.com/2017/03/28/consumer-broadband-privacy-protections-are-dead/">even basic consumer protection reforms</a>, or <a href="https://www.techdirt.com/2023/03/07/telecom-monopolies-win-again-gigi-sohn-forced-to-withdraw-from-fcc-nomination/" data-type="link" data-id="https://www.techdirt.com/2023/03/07/telecom-monopolies-win-again-gigi-sohn-forced-to-withdraw-from-fcc-nomination/">nominate and seat <strong>any</strong> popular reformers</a> to federal agencies. </p>
<p>Even when you see “antitrust reform” <a href="https://www.techdirt.com/2023/02/10/gop-stops-pretending-it-ever-actually-cared-about-antitrust-reform/" data-type="link" data-id="https://www.techdirt.com/2023/02/10/gop-stops-pretending-it-ever-actually-cared-about-antitrust-reform/">performatively mentioned</a> in Congress, telecom is never included. And this is all before the looming right wing Supreme Court’s <a href="https://www.politico.com/news/2023/05/01/supreme-court-chevron-doctrine-climate-change-00094670" data-type="link" data-id="https://www.politico.com/news/2023/05/01/supreme-court-chevron-doctrine-climate-change-00094670">Chevron Deference ruling</a> strips away any remaining independent authority the FCC (and every other federal regulatory agency) has left. </p>
<p>With the feds rendered useless on consumer protection and monopoly busting, telecom lobbyists have increasingly taken aim at the last vestiges of state and local government power. </p>
<p>During the Trump FCC, that involved stripping away your town’s or <a href="https://www.techdirt.com/2019/11/20/46-cities-sue-fcc-trampling-their-rights/" data-type="link" data-id="https://www.techdirt.com/2019/11/20/46-cities-sue-fcc-trampling-their-rights/">city’s local authority </a>over everything from cell tower placement to utility pole rates (again under the pretense that <em>mean ‘ole Mayors</em>, not unchecked monopolization and consolidation, is what’s holding back better broadband). </p>
<p>The Ajit Pai era net neutrality repeal tried to ban states (<a href="https://www.techdirt.com/2022/01/28/courts-again-shoot-down-telecom-lobbys-attempt-to-kill-state-level-net-neutrality-rules/" data-type="link" data-id="https://www.techdirt.com/2022/01/28/courts-again-shoot-down-telecom-lobbys-attempt-to-kill-state-level-net-neutrality-rules/">unsuccessfully</a>) from protecting broadband consumers. <a href="https://broadbandnow.com/report/municipal-broadband-roadblocks" data-type="link" data-id="https://broadbandnow.com/report/municipal-broadband-roadblocks">16 States have passed laws</a> banning your town or city from building its own broadband. House Republicans even tried to <a href="https://www.techdirt.com/2021/02/19/new-bill-tries-to-ban-community-broadband-during-pandemic/" data-type="link" data-id="https://www.techdirt.com/2021/02/19/new-bill-tries-to-ban-community-broadband-during-pandemic/">ban community-owned broadband networks</a> entirely <em>in the middle of a pandemic that painfully showcased the importance of affordable, reliable broadband</em>. </p>
<p>The goal throughout is obvious: AT&amp;T, Verizon, Comcast, Charter, CenturyLink and other telecom giants want to be free to rip you off with high prices and substandard service. They want to be free of all meaningful competition. And they want local, state, and federal governments absolutely powerless to do anything about it. If there are <strong>any</strong> rules, they want to write them to their personal benefit.</p>
<p>It’s a campaign they’ve been winning for decades.</p>
<p>Nineteen proposed new telecom laws just stumbled forth from the the House Energy and Commerce Committee. One of them, <a href="https://www.congress.gov/bill/118th-congress/house-bill/3557" data-type="link" data-id="https://www.congress.gov/bill/118th-congress/house-bill/3557">HR 3557</a> (the American Broadband Deployment Act of 2023) focus on “pre-empting” any remaining local government authority over telecom giants. This is being presented, once again, as a “<a href="https://www.nlc.org/article/2023/06/08/house-committee-advances-communications-infrastructure-preemption-bill/" data-type="link" data-id="https://westviewnews.org/2023/07/21/alert-house-bill-3557-overrides-local-governments-in-push-to-streamline-deployment-of-wireless-broadband/westview-news/">streamlining</a>” of unnecessarily bureaucratic government power. </p>
<p>In reality, HR 3557 would make it all but impossible for local governments to have much of a say in telecom infrastructure placement (regardless of environmental or historical impact), negotiate fair rates with telecom giants for things like rights of way, or utility pole usage, or have much of a any role in terms of consumer protection. <a href="https://potsandpansbyccg.com/2023/08/14/preempting-local-government/" data-type="link" data-id="https://potsandpansbyccg.com/2023/08/14/preempting-local-government/">From Doug Dawson, a widely respected industry consultant</a>:</p>
<blockquote>
<p><em>[HR 3557] “Eliminates cable franchise renewals and eliminates the ability of local governments to require rules such as an ISP having to serve the whole community, the local government requiring PEG channels, <span>or the local government requiring customer service standards</span>.”</em></p>
</blockquote>
<p>It’s worth noting that the human beings actually involved in local governments <a href="https://www.nlc.org/article/2023/06/08/house-committee-advances-communications-infrastructure-preemption-bill/" data-type="link" data-id="https://www.nlc.org/article/2023/06/08/house-committee-advances-communications-infrastructure-preemption-bill/">weren’t invited to the single hearing on this subject</a>, which speaks volumes:</p>
<blockquote>
<p><em>“When the committee&nbsp;<a href="https://energycommerce.house.gov/events/communications-and-technology-subcommittee-legislative-hearing-breaking-barriers-streamlining-permitting-to-expedite-broadband-deployment">held an initial hearing</a>&nbsp;on broadband permitting streamlining, including a draft of the American Broadband Deployment Act, no state or local government was invited to testify.”</em></p>
</blockquote>
<p>In telecom policy conversations, local governments are always framed as some kind of faceless bureaucrats hell bent on ruining everybody’s good time for no coherent reason. But a lot of the systems telecom lobbyists are trying to dismantle serve important functions:</p>
<blockquote>
<p><em>“Local permitting and cable franchising processes are intended to make sure that communications infrastructure is deployed equitably and in the public interest, that work is done safely and in a way that protects valuable public resources, including the rights of way. Local governments are the stewards of these finite public resources.”</em></p>
</blockquote>
<p>It’s no coincidence that this new lobbying push comes as local governments around the country increasingly eye building their own fiber networks with an eye on affordability and even coverage (something that’s <a href="https://www.techdirt.com/2023/05/31/community-owned-broadband-network-again-tops-list-of-most-popular-isps/" data-type="link" data-id="https://www.techdirt.com/2023/05/31/community-owned-broadband-network-again-tops-list-of-most-popular-isps/">very popular</a> among long-neglected consumers). As Dawson notes, AT&amp;T and Comcast certainly don’t want your piddly Mayor standing in the way of them doing whatever they want:</p>
<blockquote>
<p><em>“The biggest killer is that the law would give holders of franchise agreements the ability to cancel the agreement without losing any rights-of-ways included in the agreement. This would also kill local franchise fees, a major source of revenue for many governments. Perhaps the most severe provision is that franchise contract holders can eliminate any contract provisions they deem to be commercially infeasible.”</em></p>
</blockquote>
<p>To be clear: some of the telecom bills winding through Congress are actually helpful. One would renew the FCC’s authority to conduct spectrum auctions, bizarrely lapsed due to a <a href="https://www.techdirt.com/2023/03/15/congress-lets-the-fccs-spectrum-auction-authority-lapse-for-no-good-reason/" data-type="link" data-id="https://www.techdirt.com/2023/03/15/congress-lets-the-fccs-spectrum-auction-authority-lapse-for-no-good-reason/">recent bout of government incompetence</a>. Another <a href="https://communitynets.org/content/isps-large-and-small-push-tax-exempt-broadband-grants" data-type="link" data-id="https://communitynets.org/content/isps-large-and-small-push-tax-exempt-broadband-grants">would make broadband grants tax exempt</a> (a boon to telecoms big and small). </p>
<p>But as Dawson quite correctly notes, legislation like HR 3557 is seeded in there in the hopes it worms its way into a broader bill under the pretense of a broader telecom reform package. I’d be genuinely surprised if the bill itself wasn’t written by an AT&amp;T or Comcast lawyer (probably <a href="https://www.techdirt.com/2015/04/16/alec-threatens-to-sue-critics-that-point-out-it-helps-keep-broadband-uncompetitive/" data-type="link" data-id="https://www.techdirt.com/2015/04/16/alec-threatens-to-sue-critics-that-point-out-it-helps-keep-broadband-uncompetitive/">using ALEC as a proxy</a>):</p>
<blockquote>
<p><em>“This bill is going for a home run to eliminate local regulations these big companies don’t like. I’ve written recently about regulatory capture, and this is an ultimate example of changing the laws to get what the big monopoly providers want…This bill is the ultimate example of the biggest companies in telecom flexing their power and influence to bypass some of the last vestiges of regulation.”</em></p>
</blockquote>
<p>As the federal government becomes (quite intentionally) more dysfunctional, feckless, and corrupt, most meaningful telecom policy fights have shifted to the state or local level. In town after town, locals now find themselves fighting block by block against monopoly power, with less and less meaningful support from the feckless and captured federal government. Now, industry is eyeing the killing blow. </p>
<p>
Filed Under: <a href="https://www.techdirt.com/tag/american-broadband-deployment-act/" rel="tag">american broadband deployment act</a>, <a href="https://www.techdirt.com/tag/broadband/" rel="tag">broadband</a>, <a href="https://www.techdirt.com/tag/community-broadband/" rel="tag">community broadband</a>, <a href="https://www.techdirt.com/tag/corruption/" rel="tag">corruption</a>, <a href="https://www.techdirt.com/tag/fcc/" rel="tag">fcc</a>, <a href="https://www.techdirt.com/tag/hr-3557/" rel="tag">hr 3557</a>, <a href="https://www.techdirt.com/tag/local-government/" rel="tag">local government</a>, <a href="https://www.techdirt.com/tag/monopoly/" rel="tag">monopoly</a>, <a href="https://www.techdirt.com/tag/municipal-broadband/" rel="tag">municipal broadband</a>, <a href="https://www.techdirt.com/tag/pre-emption/" rel="tag">pre-emption</a>, <a href="https://www.techdirt.com/tag/regulatory-reform/" rel="tag">regulatory reform</a>, <a href="https://www.techdirt.com/tag/telecom/" rel="tag">telecom</a>
<br>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Node.js 20.6.0 will include built-in support for .env files (172 pts)]]></title>
            <link>https://twitter.com/kom_256/status/1692225622091706389</link>
            <guid>37174916</guid>
            <pubDate>Fri, 18 Aug 2023 12:47:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/kom_256/status/1692225622091706389">https://twitter.com/kom_256/status/1692225622091706389</a>, See on <a href="https://news.ycombinator.com/item?id=37174916">Hacker News</a></p>
Couldn't get https://twitter.com/kom_256/status/1692225622091706389: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[You probably don’t need to fine-tune LLMs (152 pts)]]></title>
            <link>https://www.tidepool.so/2023/08/17/why-you-probably-dont-need-to-fine-tune-an-llm/</link>
            <guid>37174850</guid>
            <pubDate>Fri, 18 Aug 2023 12:41:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tidepool.so/2023/08/17/why-you-probably-dont-need-to-fine-tune-an-llm/">https://www.tidepool.so/2023/08/17/why-you-probably-dont-need-to-fine-tune-an-llm/</a>, See on <a href="https://news.ycombinator.com/item?id=37174850">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-268">

                            
    
                        <div>
                            
<p><em>This post is targeted towards folks focused on building LLM applications (as opposed to research).</em></p>



<p>If you’re a builder, it’s important to know what’s available in your toolbox, and the right time to use a given tool. Depending on what you’re doing, there are probably ones you use more often (hammer, screwdriver), and ones that you use less often (say, a hacksaw).</p>



<p>A lot of very smart people are experimenting with LLMs right now — resulting in a pretty jam-packed toolbox, acronyms and all (fine-tuning, RLHF, RAG, chain-of-thought, etc). It’s easy to get stuck in the decision paralysis stage of “what technical approach do I use”, even if your ultimate goal is to “build an app for X”.</p>



<p>On their own, people often run into issues with base model LLMs — “the model didn’t return what I wanted” or “the model hallucinated, its answer makes no sense” or “the model doesn’t know anything about Y because it wasn’t trained on it”.</p>



<p>People sometimes turn to a fairly involved technique called fine-tuning, in hopes that it will solve all of the above. <strong>In this post, we’ll talk about why fine-tuning is probably not necessary for your app.</strong></p>



<p>Specifically, people often think of “fine-tuning” when they want one or both of the following:<br></p>



<ul>
<li><strong>Additional structure/style: </strong>They want<strong> </strong>the LLM to do a more specific task (beyond open-ended question answering) + provide answers in a desired format
<ul>
<li>This can be done with <strong>few-shot prompting</strong><strong><br></strong></li>
</ul>
</li>



<li><strong>Additional source knowledge:</strong> They want the base LLM to answer questions about things it may not have been trained on (and consequently is unaware of), and which is not publicly available on the internet (e.g. even to GPT-4).
<ul>
<li>This can be done with  <strong>retrieval-augmented generation (RAG)</strong>.</li>
</ul>
</li>
</ul>



<p><strong>A combination of these two techniques is actually sufficient for most use cases.</strong></p>



<h2>Why People <em>Think</em> Fine-tuning Might Be Helpful</h2>



<details><summary> <strong>What even is fine-tuning?</strong> 🤔</summary>
<div><p><em>Fine-tuning involves taking a pre-trained LLM (e.g. GPT-3.5, LLaMA 2) and further training it on a smaller, domain-specific dataset to make it more specialized for that particular task or data.<br></em><br><em>Out-of-the-box language models have already been pre-trained on open-source data (e.g. <a href="https://commoncrawl.org/">Common Crawl</a>) — the LLM interface you interact with already has “fixed weights” within its transformer/neural network architecture, which do not update based on the queries you feed it (aka, though there can be session history provided as context, it doesn’t “learn” on the fly).<p>When you fine-tune a model, you are essentially “unlocking” these weights and allowing them to update based on whatever new training data you feed it (e.g. a collection of legal cases, or company earnings reports, or a specific user’s tweets). These new weights should allow the model to better handle tasks related to that new domain.</p></em></p><p><em>As of Aug 2023:</em></p></div>



<ul>
<li><em>OpenAI only supports fine-tuning for its GPT-3 models (not the newer GPT-3.5 and GPT-4 models that back ChatGPT) (see <a href="https://platform.openai.com/docs/guides/fine-tuning">how-to guide</a>)</em></li>



<li><em>LLaMA 2 has similar performance to ChatGPT and is open-source, so it is typically the one that people have been using to fine-tune (while maintaining chat capabilities)</em></li>
</ul>
</details>







<p>Base LLMs have a lot of abilities (question answering, summarization, etc) that you likely want to leverage in your app, but you may find them too generic (or unaware of) your particular use case.</p>



<p>You might be drawn to fine-tuning because you believe “more training” can help your LLM application eke out better accuracy on your target task. Intuitively this makes sense — why wouldn’t it be best to adjust the model based on text from your specific domain?</p>



<p>However, the following table explains why it is sufficient, easier, and often preferable to apply other techniques to the existing base model:</p>


<figure>
<table>
<thead>
<tr>
<th>Initial Motivation for Fine-tuning &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</th>
<th>Why a Base LLM is Sufficient</th>
</tr>
</thead>
<tbody>
<tr>
<td><span>It’s cheaper than training a model from scratch, and leverages a base LLM’s existing training.</span></td>
<td>
<p><span>Yes, but comparing to “starting from scratch” fails to consider approaches that don’t require any retraining (and retraining requires access to resources like GPUs).</span></p>
<p><b>Also, the context window length — referring to the LLM Input token limit — has only been longer and longer</b><span>.</span> <span>This length can be allocated not just to your question/task description itself, but also to (1) conversation history (2) examples of output (3) additional info the LLM wasn’t trained on.</span><span><br></span><span><br></span><b>Longer context windows and possible version improvements (e.g. GPT-5)</b> <b>mean that a lot of improvements from fine-tuning will become baseline within a few months — so why invest all those time/computational resources now?</b></p>
</td>
</tr>
<tr>
<td><span>The base model doesn’t have access to private knowledge bases.</span></td>
<td>
<p><span>There is a technique called </span><b>retrieval-augmented generation (RAG)</b><span>, where documents can be stored as embeddings in vector databases, queried for based on semantic meaning, and passed into the base model prompt via the context window.</span></p>
</td>
</tr>
<tr>
<td><span>The base model doesn’t return answers in the desired style or format.</span></td>
<td>
<p><span>Actually, this can be done with a more specific prompt — and the odds of consistently formatted answers can be improved with </span><b>few-shot prompting</b><span>, an approach that provides the base model with examples (think: </span><a href="https://blog.prepscholar.com/sat-analogies-and-comparisons-why-removed-what-replaced-them"><span>SAT analogies</span></a><span>) within the context window.</span></p>
</td>
</tr>
<tr>
<td><span>Fine-tuning means that you don’t need to provide additional context in each prompt – this will save token usage per query.</span></td>
<td>
<p><span>Perhaps in the long run, but token usage is incredibly cheap and this is usually not an issue. (</span> <span>$0.06 / 1K tokens if using 32K context, AKA $1.92 for ~24,000 words)</span><span><br></span><span><br></span><span>Retraining/fine-tuning can cost hundreds of dollars, may still require some prompt engineering, and does not guarantee that your LLM will provide accurate answers.</span></p>
</td>
</tr>
<tr>
<td><span>Fine-tuning will result in more accurate, domain-specific results.</span></td>
<td>
<p><span>Fine-tuning does </span><b>not</b><span> prevent LLM from hallucinating – in fact, it may be more reliable to provide a source to the base model (e.g. from a vector database), and ask clear questions about it (and have the model indicate when the answer is not found).</span><span><br></span><span><br></span><span>Also there have been some </span><a href="https://openreview.net/forum?id=zmXJUKULDzh"><span>experiments</span></a><span> which have shown that LLM’s cross-functional abilities (summarization, classification, generation, etc) can actually degrade because fine-tuning results in overtraining.</span></p>
</td>
</tr>
</tbody>
</table>
</figure>


<p>Just to reiterate,<strong> fine-tuning (except in some rare cases) negates most of the resource-saving benefits from recent LLMs — the reasons that people are flocking to this technology in the first place.</strong> The biggest reason why NLP was hard to do before late 2022 was because you needed to collect data, label data, train models, host infra — and all that requires hiring an ML ops and eng team!</p>



<p>Now with using LLMs out of the box, the startup cost is incredibly low. There are a whole bunch of orgs that never would have done NLP if not for LLMs making the bar so low. <strong>Is it worth investing your eng time into fine-tuning when state-of-the-art is advancing so quickly?</strong> Sure, you’ll have a slight competitive advantage if your model has better accuracy/quality — but will you still think so a few months later when other companies get the same boosted functionality with GPT-5, no effort required?</p>



<p>This is why we recommend that you focus your attention on lighter-touch approaches like few-shot prompting and retrieval augmented generation (RAG).</p>



<h2>Alternative 1 (for Structure): Base LLM + Few-Shot Prompting</h2>



<p>Although most popular LLMs have been trained to respond in a Q&amp;A format, you may want them to perform a specific task (e.g. sentiment analysis, or a <a href="https://www.tidepool.so/2023/07/26/5-top-product-applications-of-llms/">range of applications</a>) or to output answers in a particular format (e.g. JSON).</p>



<p>While you can provide these instructions directly in the prompt, the LLM’s response is probabilistic, not deterministic. There isn’t a guarantee that it will answer in the way that you expect (and perhaps this is why people sometimes think fine-tuning is needed).&nbsp;</p>



<p>However, you can ensure more consistency with the following:</p>



<ul>
<li>Using basic heuristics to vet that its response falls into a domain of known/desired outputs</li>



<li>Providing examples of input/output pairs within the context window (what’s passed into the LLM, in addition to the main query) — it’s almost as if you are providing a small number of training data within the real-time query. This is <strong>few-shot prompting.</strong></li>
</ul>



<p>For example, the following prompt provides examples (taken from the <a href="https://huggingface.co/datasets/yelp_review_full"><code>yelp_review_full</code></a> dataset) so that the LLM (ChatGPT in this case) knows how to classify user reviews:</p>



<p><strong>Context for prompt:</strong></p>



<pre><code>You will be given inputs (reviews for various businesses, e.g. from Yelp) and your job will be to (1) classify the sentiment (one of [“positive”, “negative”, “neutral”, “mixed”]) and (2) try to determine the sort of place the review is for. You will output results in JSON format (with the keys “sentiment” and “business_type”).

Input: “This location never disappoints!! Food is always consistently great, and if you come at the right time, (witching hours) you may see the cook singing and dancing along with the music in the back. And it is awesome! ! Love this place!!”

Output: {“sentiment”: “positive”, “business_type”: “restaurant”}

Input: “I'm writing this review to give you a heads up before you see this Doctor. The office staff and administration are very unprofessional. I left a message with multiple people regarding my bill, and no one ever called me back. I had to hound them to get an answer about my bill. \\n\\nSecond, and most important, make sure your insurance is going to cover Dr. Goldberg's visits and blood work. He recommended to me that I get a physical, and he knew I was a student because I told him. I got the physical done. Later, I found out my health insurance doesn't pay for preventative visits. I received an $800.00 bill for the blood work. I can't pay for my bill because I'm a student and don't have any cash flow at this current time. I can't believe the Doctor wouldn't give me a heads up to make sure my insurance would cover work that wasn't necessary and was strictly preventative. The office can't do anything to help me cover the bill. In addition, the office staff said the onus is on me to make sure my insurance covers visits. Frustrating situation!”

Output: {“sentiment”: “negative”, “business_type”: “doctor’s office”}

Input: “Good beer selection. Understaffed for a light Monday night crowd, it wasn't her fault she was the only server. But it took about an hour to get our sandwiches. Mine was one of the best reubens I've ever had.”

Output: {“sentiment”: “mixed”, “business_type”: “restaurant”}

Input: “Place was alright for a one-night stay. Nothing special, room was a bit old, but fine as a pitstop on our long roadtrip.”

Output: {“sentiment”: “neutral”, “business_type”: “hotel”}

Input: &lt;INPUT&gt;

Output:</code></pre>



<p><strong>Example of raw input (inserted into the &lt;INPUT&gt; placeholder in the prompt above):</strong></p>



<pre><code>This place is absolute garbage...  Half of the tees are not available, including all the grass tees.  It is cash only, and they sell the last bucket at 8, despite having lights.  And if you finish even a minute after 8, don't plan on getting a drink.  The vending machines are sold out (of course) and they sell drinks inside, but close the drawers at 8 on the dot.  There are weeds grown all over the place.  I noticed some sort of batting cage, but it looks like those are out of order as well.  Someone should buy this place and turn it into what it should be.</code></pre>



<p><strong>ChatGPT (turbo-3.5) output:</strong></p>



<pre><code>{
“sentiment”: “negative”,
“business_type”: “golf range”
}
</code></pre>



<p>Usually the base model is sufficient to extrapolate what you want from the examples you provide (“few-shot”). However, if you need the model to perform a task using specialized knowledge that it may not have been trained on, few-shot prompting won’t be enough.</p>



<h2>Alternative 2 (for Knowledge): Base LLM + Retrieval Augmented Generation (RAG)</h2>



<p>By itself, an LLM can’t answer questions about content it hasn’t been trained on. However, it has an <a href="https://help.openai.com/en/articles/7127966-what-is-the-difference-between-the-gpt-4-models">extended context window of 32,000 tokens</a> — essentially, 24,000 words of “memory”.&nbsp; This memory can include:</p>



<ul>
<li>Previous conversation history (if any)</li>



<li>Additional info needed that the LLM hasn’t been trained on</li>



<li>Your actual query</li>
</ul>



<p>The supported context window length has been trending bigger and bigger (to the point where you really can provide <strong>pages</strong> of source material in the real-time query).</p>



<p>Say that you want to use an LLM to do smarter search over your own internal documentation. Your total number of docs &gt; 12,000 words by a long shot. However, if you have another means of figuring out which doc (and for longer docs, which “chunk” of a particular doc) likely has what you’re looking for, you can provide that “chunk” as context for your question to the LLM.</p>



<p>Luckily, vector DBs do just that — assuming that you’ve already split your docs into LLM-friendly chunks and stored these in the DB in vector form, you can subsequently:</p>



<ol>
<li>Encode your search query as a vector</li>



<li>Query the DB using to find the top X chunks most related to (1)</li>



<li>Provide each of these to the LLM along with the original search query, to see if it can determine the answer with that extra context</li>
</ol>



<p>The following diagram illustrates how you’d store your documents into a vector DB:</p>



<figure><img decoding="async" src="https://www.tidepool.so/wp-content/uploads/2023/08/seed_db-1024x761.png" alt="" width="614" height="456" srcset="https://www.tidepool.so/wp-content/uploads/2023/08/seed_db-1024x761.png 1024w, https://www.tidepool.so/wp-content/uploads/2023/08/seed_db-300x223.png 300w, https://www.tidepool.so/wp-content/uploads/2023/08/seed_db-768x571.png 768w, https://www.tidepool.so/wp-content/uploads/2023/08/seed_db-650x483.png 650w, https://www.tidepool.so/wp-content/uploads/2023/08/seed_db.png 1482w" sizes="(max-width: 614px) 100vw, 614px"></figure>



<p>This second diagram illustrates how you’d query the vector DB and provide this as context to the LLM model:</p>



<figure><img decoding="async" src="https://www.tidepool.so/wp-content/uploads/2023/08/read_db-1024x761.png" alt="" width="615" height="457" srcset="https://www.tidepool.so/wp-content/uploads/2023/08/read_db-1024x761.png 1024w, https://www.tidepool.so/wp-content/uploads/2023/08/read_db-300x223.png 300w, https://www.tidepool.so/wp-content/uploads/2023/08/read_db-768x571.png 768w, https://www.tidepool.so/wp-content/uploads/2023/08/read_db-650x483.png 650w, https://www.tidepool.so/wp-content/uploads/2023/08/read_db.png 1482w" sizes="(max-width: 615px) 100vw, 615px"></figure>



<p>Below is some pseudocode for the second part (query logic), using the vector DB <a href="https://docs.pinecone.io/docs/overview">Pinecone</a> in this case. <strong>FYI this won’t actually run,</strong> but is meant to give you a sense of what’s needed:</p>



<figure><img decoding="async" loading="lazy" width="1024" height="761" src="https://www.tidepool.so/wp-content/uploads/2023/08/Screen-Shot-2023-08-17-at-7.52.58-AM-1024x761.png" alt="" srcset="https://www.tidepool.so/wp-content/uploads/2023/08/Screen-Shot-2023-08-17-at-7.52.58-AM-1024x761.png 1024w, https://www.tidepool.so/wp-content/uploads/2023/08/Screen-Shot-2023-08-17-at-7.52.58-AM-300x223.png 300w, https://www.tidepool.so/wp-content/uploads/2023/08/Screen-Shot-2023-08-17-at-7.52.58-AM-768x570.png 768w, https://www.tidepool.so/wp-content/uploads/2023/08/Screen-Shot-2023-08-17-at-7.52.58-AM-650x483.png 650w, https://www.tidepool.so/wp-content/uploads/2023/08/Screen-Shot-2023-08-17-at-7.52.58-AM.png 1516w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<h2>When You Might Actually Fine-tune</h2>



<p>Given how far you can get with the alternatives mentioned above, there aren’t that many truly valid cases of fine-tuning. You might still consider it if:</p>



<ul>
<li>You have <strong>super stringent</strong> accuracy requirements for a certain task that justifies putting in a lot of engineering and ops resources (a heavily-resourced company like Bloomberg can do <a href="https://www.bloomberg.com/company/press/bloomberggpt-50-billion-parameter-llm-tuned-finance/">this</a>)</li>



<li>You really really care about <a href="https://twitter.com/karpathy/status/1666182244107689985">fast edge inference</a> (e.g. an LLM model running locally on your phone). In which case you’re probably not going to fine-tune an LLM, you’ll probably want to use a BERT type model because it’s lighter and more task-generalizable (<a href="https://datascience.stackexchange.com/questions/123053/why-does-everyone-use-bert-in-research-instead-of-llama-or-gpt-or-palm-etc">src</a>).</li>



<li>If few-shot and RAG combined do not get you the performance you want (e.g. a more involved style transfer task) and you really want to make it better…even then, as we’ve mentioned, the move here might be “welp let’s wait until the latest LLM version update”</li>
</ul>



<h2>Conclusion</h2>



<p>To close this out with a callback to the original toolbox metaphor, you can consider a base LLM model to be like a Swiss army knife — it is sufficient and adaptable to most use cases.</p>



<p>Given the upfront time and computational resources required for fine-tuning, we’d recommend starting with this base first (along with the supplemental techniques mentioned) and seeing how far you get with it!</p>
                        </div><!-- .entry-content -->

                                                <!-- .entry-footer -->
                        
                    </article><p><a href="https://www.tidepool.so/author/jyao/" title="View Jessica Yao’s posts" rel="author"><img src="https://secure.gravatar.com/avatar/4bba728520d0f0874ff296af346c0859?s=450&amp;d=mm&amp;r=g"></a>            <span>
                <h4> </h4>
                                <p> Software engineer and occasional technical writer. Former early employee at Aquarium! 🐬</p>
            </span>
        </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Worldcoin ignored initial order to stop iris scans in Kenya, records show (189 pts)]]></title>
            <link>https://techcrunch.com/2023/08/15/worldcoin-in-kenya/</link>
            <guid>37174758</guid>
            <pubDate>Fri, 18 Aug 2023 12:31:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2023/08/15/worldcoin-in-kenya/">https://techcrunch.com/2023/08/15/worldcoin-in-kenya/</a>, See on <a href="https://news.ycombinator.com/item?id=37174758">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary">Months before<a href="https://techcrunch.com/2023/08/02/kenya-suspends-worldcoin-scans-over-security-privacy-and-financial-concerns/?tpcc=tcplustwitter"> Kenya finally banned iris scans by Sam Altman’s crypto startup Worldcoin,</a> the Office of the Data Protection Commissioner (ODPC) had ordered its parent company, Tools for Humanity, to stop collecting personal data.</p>
<p>The ODPC had in May this year instructed the crypto startup to stop iris scans and the collection of facial recognition and other personal data in Kenya, a letter sent to Worldcoin and seen by TechCrunch shows.</p>
<p>Tools for Humanity, the company building Worldcoin, did not stop taking biometric data until early this month when Kenya’s ministry of interior and administration, a more powerful entity, suspended it following its official launch. Worldcoin’s official launch led to a spike in the number of people queuing up to have their eyeballs scanned in exchange for “free money,” drawing the attention of authorities.</p>
<p>The letter shows that ODPC had instructed Worldcoin to cease collecting data for intruding on individuals’ privacy by gathering biometric data without a well-established and compelling justification. Further, it said Worldcoin had failed to obtain valid consent from people before scanning their irises, saying its agents failed to inform its subjects about the data security and privacy measures it took, and how the data collected would be used or processed.</p>
<p>“Your client is hereby instructed to cease the collection of all facial recognition data and iris scans, from your subscribers. This cessation should be implemented without delay and should include all ongoing and future data processing activities,” said Rose Mosero, in a letter to Tools for Humanity that outlined the concerns. The letter, addressed to Ariana Issaias of Coulson Harney (Bowmans), the law firm representing the crypto startup, also restricted Worldcoin from processing the collected data further and instructed the safe storage of collected information.</p>
<p>Details of ODPC’s attempt to stop the collection of biometric data have emerged in a new petition filed before the High Court by the data protection authority.</p>
<p>The deputy data commissioner of compliance, Oscar Otieno, in an affidavit filed in court in August, said that it started the “assessment” of the respondents (Tools for Humanity and Sense Marketing Limited) in 2022. In May this year, it carried out further inquiry on their processing activities and directed that they cease processing sensitive personal data immediately.</p>
<p>“The applicant (ODPC) is aware that despite the suspension and directive to cease processing of personal data, the respondents continued to process the said personal data. It took the public directive by the cabinet ministry of interior and coordination to halt the operations of the respondents (Tools for Humanity and Sense Marketing),” said Otieno in the affidavit.</p>

<p>The ODPC sought the court’s help to have Worldcoin compelled to preserve the data it collected from Kenyans, as it finalizes (the multi-agency) investigations around security, privacy, and the legality of using “financial incentive” to obtain biometric data.</p>
<p>Since the filing of the petition, the High Court has <a href="https://www.businessdailyafrica.com/bd/economy/keep-off-kenyans-eyes-court-orders-worldcoin-as-probe-on--4335544" target="_blank" rel="noopener">barred Worldcoin from collecting data from Kenyans</a> and directed it, its agents, its representatives and its employees to preserve all information collected locally from April 19 to August 8.</p>
<p>This has emerged after Worldcoin activities were suspended in Kenya on August 2, by Kithure Kindiki, the country’s cabinet secretary for interior and national administration. Kindiki said the ban will remain in place until the authorities determine “the absence of any risks to the general public whatsoever.”</p>
<p>“Relevant security, financial service and data protection agencies have commenced inquiries and investigations to establish the authenticity and legality of the aforesaid activities, and the safety and protection of the data being harvested, and how the harvesters intend to use the data,” said Kindiki.</p>
<p>Worldcoin claims to be creating a new “human identity (World ID) and financial network” through iris scans done by “Orb,” the company’s spherical scanners to “verify your World ID,” and its own cryptocurrency “WLD.”</p>
<p>Kenya was one of the first countries where Worldcoin launched sign-ups and had been one of the biggest markets for takeup. After the global official launch at the end of July, locals who had received the tokens could sell them for USDT (the stablecoin pegged to the U.S. dollar) on crypto exchanges or to “brokers” in exchange for cash. In Kenya, that promise of “free money” quickly spread across the country, leading to an influx of people at the recruitment (Orb) stations, which drew the attention of top government officials, leading to the suspension of Worldcoin iris scans. The crypto startups said it <a href="https://techcrunch.com/2023/08/03/worldcoin-plans-to-resume-iris-scans-in-kenya/">hopes to resume activities in Kenya soon</a>.</p>
<p>Alongside the many issues that <a href="https://newsletter.mollywhite.net/p/worldcoin-a-solution-in-search-of" target="_blank" rel="noopener">skeptical peers in the technology industry</a> have been raising about the Worldcoin project and its bigger business ambitions, there are growing concerns about how those efforts to build a biometric database using the promise of free cryptocurrency have exploited economically disadvantaged people. Also, some of these issues have been there in plain sight. An MIT Technology Review <a href="https://www.technologyreview.com/2022/04/06/1048981/worldcoin-cryptocurrency-biometrics-web3/" target="_blank" rel="noopener">investigation</a> — published last year — found that it “used deceptive marketing practices, was collecting more personal data than it acknowledged, and failed to obtain meaningful informed consent.”</p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Textual: Rapid Application Development Framework for Python (224 pts)]]></title>
            <link>https://github.com/Textualize/textual</link>
            <guid>37174657</guid>
            <pubDate>Fri, 18 Aug 2023 12:22:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Textualize/textual">https://github.com/Textualize/textual</a>, See on <a href="https://news.ycombinator.com/item?id=37174657">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/Textualize/textual/main/imgs/textual.png"><img src="https://raw.githubusercontent.com/Textualize/textual/main/imgs/textual.png" alt="Textual splash image"></a></p>
<p dir="auto"><a href="https://discord.gg/Enf6Z3qhVr" rel="nofollow"><img src="https://camo.githubusercontent.com/7974ed333e6b924da47e6798cae37e22776ed404c8d84789d7c2780a641f0691/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f31303236323134303835313733343631303732" alt="Discord" data-canonical-src="https://img.shields.io/discord/1026214085173461072"></a></p>
<h2 tabindex="-1" dir="auto">Textual</h2>
<p dir="auto">Textual is a <em>Rapid Application Development</em> framework for Python.</p>
<p dir="auto">Build sophisticated user interfaces with a simple Python API. Run your apps in the terminal and (coming soon) a web browser!</p>
<details>
  <summary> 🎬 Demonstration </summary>
  <hr>
<p dir="auto">A quick run through of some Textual features.</p>
<details open="">
  <summary>
    
    <span aria-label="Video description Screen.Recording.2022-10-22.at.19.00.48.mov">Screen.Recording.2022-10-22.at.19.00.48.mov</span>
    <span></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/554369/197355913-65d3c125-493d-4c05-a590-5311f16c40ff.mov" data-canonical-src="https://user-images.githubusercontent.com/554369/197355913-65d3c125-493d-4c05-a590-5311f16c40ff.mov" controls="controls" muted="muted">

  </video>
</details>

 </details>
<h2 tabindex="-1" dir="auto">About</h2>
<p dir="auto">Textual adds interactivity to <a href="https://github.com/Textualize/rich">Rich</a> with an API inspired by modern web development.</p>
<p dir="auto">On modern terminal software (installed by default on most systems), Textual apps can use <strong>16.7 million</strong> colors with mouse support and smooth flicker-free animation. A powerful layout engine and re-usable components makes it possible to build apps that rival the desktop and web experience.</p>
<h2 tabindex="-1" dir="auto">Compatibility</h2>
<p dir="auto">Textual runs on Linux, macOS, and Windows. Textual requires Python 3.7 or above.</p>
<h2 tabindex="-1" dir="auto">Installing</h2>
<p dir="auto">Install Textual via pip:</p>

<p dir="auto">If you plan on developing Textual apps, you should also install the development tools with the following command:</p>

<p dir="auto">See the <a href="https://textual.textualize.io/getting_started/" rel="nofollow">docs</a> if you need help getting started.</p>
<h2 tabindex="-1" dir="auto">Demo</h2>
<p dir="auto">Run the following command to see a little of what Textual can do:</p>

<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/Textualize/textual/main/imgs/demo.png"><img src="https://raw.githubusercontent.com/Textualize/textual/main/imgs/demo.png" alt="Textual demo"></a></p>
<h2 tabindex="-1" dir="auto">Documentation</h2>
<p dir="auto">Head over to the <a href="http://textual.textualize.io/" rel="nofollow">Textual documentation</a> to start building!</p>
<h2 tabindex="-1" dir="auto">Join us on Discord</h2>
<p dir="auto">Join the Textual developers and community on our <a href="https://discord.gg/Enf6Z3qhVr" rel="nofollow">Discord Server</a>.</p>
<h2 tabindex="-1" dir="auto">Examples</h2>
<p dir="auto">The Textual repository comes with a number of examples you can experiment with or use as a template for your own projects.</p>
<details>
  <summary> 🎬 Code browser </summary>
  <hr>
<p dir="auto">This is the <a href="https://github.com/Textualize/textual/blob/main/examples/code_browser.py">code_browser.py</a> example which clocks in at 61 lines (<em>including</em> docstrings and blank lines).</p>
<details open="">
  <summary>
    
    <span aria-label="Video description Screen.Recording.2022-10-21.at.12.41.15.mov">Screen.Recording.2022-10-21.at.12.41.15.mov</span>
    <span></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/554369/197188237-88d3f7e4-4e5f-40b5-b996-c47b19ee2f49.mov" data-canonical-src="https://user-images.githubusercontent.com/554369/197188237-88d3f7e4-4e5f-40b5-b996-c47b19ee2f49.mov" controls="controls" muted="muted">

  </video>
</details>

 </details>
<details>
  <summary> 📷 Calculator </summary>
  <hr>
<p dir="auto">This is <a href="https://github.com/Textualize/textual/blob/main/examples/calculator.py">calculator.py</a> which demonstrates Textual grid layouts.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/Textualize/textual/main/imgs/calculator.png"><img src="https://raw.githubusercontent.com/Textualize/textual/main/imgs/calculator.png" alt="calculator screenshot"></a></p>
</details>
<details>
  <summary> 🎬 Stopwatch </summary>
  <hr>
<p dir="auto">This is the Stopwatch example from the <a href="https://textual.textualize.io/tutorial/" rel="nofollow">tutorial</a>.</p>
<details open="">
  <summary>
    
    <span aria-label="Video description Screen.Recording.2022-10-22.at.21.12.22.mov">Screen.Recording.2022-10-22.at.21.12.22.mov</span>
    <span></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/554369/197360718-0c834ef5-6285-4d37-85cf-23eed4aa56c5.mov" data-canonical-src="https://user-images.githubusercontent.com/554369/197360718-0c834ef5-6285-4d37-85cf-23eed4aa56c5.mov" controls="controls" muted="muted">

  </video>
</details>

</details>
<h2 tabindex="-1" dir="auto">Reference commands</h2>
<p dir="auto">The <code>textual</code> command has a few sub-commands to preview Textual styles.</p>
<details>
  <summary> 🎬 Easing reference </summary>
  <hr>
<p dir="auto">This is the <em>easing</em> reference which demonstrates the easing parameter on animation, with both movement and opacity. You can run it with the following command:</p>

<details open="">
  <summary>
    
    <span aria-label="Video description Screen.Recording.2022-10-17.at.11.38.13.mov">Screen.Recording.2022-10-17.at.11.38.13.mov</span>
    <span></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/554369/196157100-352852a6-2b09-4dc8-a888-55b53570aff9.mov" data-canonical-src="https://user-images.githubusercontent.com/554369/196157100-352852a6-2b09-4dc8-a888-55b53570aff9.mov" controls="controls" muted="muted">

  </video>
</details>

 </details>
<details>
  <summary> 🎬 Borders reference </summary>
  <hr>
<p dir="auto">This is the borders reference which demonstrates some of the borders styles in Textual. You can run it with the following command:</p>

<details open="">
  <summary>
    
    <span aria-label="Video description Screen.Recording.2022-10-17.at.11.44.24.mov">Screen.Recording.2022-10-17.at.11.44.24.mov</span>
    <span></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/554369/196158235-4b45fb78-053d-4fd5-b285-e09b4f1c67a8.mov" data-canonical-src="https://user-images.githubusercontent.com/554369/196158235-4b45fb78-053d-4fd5-b285-e09b4f1c67a8.mov" controls="controls" muted="muted">

  </video>
</details>

</details>
<details>
  <summary> 🎬 Colors reference </summary>
  <hr>
<p dir="auto">This is a reference for Textual's color design system.</p>

<details open="">
  <summary>
    
    <span aria-label="Video description Screen.Recording.2022-10-22.at.19.07.20.mov">Screen.Recording.2022-10-22.at.19.07.20.mov</span>
    <span></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/554369/197357417-2d407aac-8969-44d3-8250-eea45df79d57.mov" data-canonical-src="https://user-images.githubusercontent.com/554369/197357417-2d407aac-8969-44d3-8250-eea45df79d57.mov" controls="controls" muted="muted">

  </video>
</details>

</details>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Short session expiration does not help security (550 pts)]]></title>
            <link>https://www.sjoerdlangkemper.nl/2023/08/16/session-timeout/</link>
            <guid>37173339</guid>
            <pubDate>Fri, 18 Aug 2023 09:45:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sjoerdlangkemper.nl/2023/08/16/session-timeout/">https://www.sjoerdlangkemper.nl/2023/08/16/session-timeout/</a>, See on <a href="https://news.ycombinator.com/item?id=37173339">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <article>

  

  <div>
  <p>When logged into a web application, the session does not remain valid forever. Typically, the session expires after a fixed time after login, or after the user has been idle for some time. How long should these times be?</p>

<!-- Photo source: https://commons.wikimedia.org/wiki/File:Time_Expired_(237355699).jpeg -->

<h2 id="introduction">Introduction</h2>

<blockquote>
  <p>Roses are red, <br>
Violets are blue, <br>
Sessions expire, <br>
Just to spite you.</p>
</blockquote>

<p>In some web applications, sessions expire. You are logged out after a while and need to authenticate again. Current security advice is to use quite short session timeouts, such as after 15 minutes of inactivity. However, most mobile apps and big web applications such as Gmail or GitHub don’t adhere to this. You can be logged in seemingly forever without authenticating again. Are these insecure? Do Google and Microsoft know better than NIST and OWASP?</p>

<h2 id="threat-model">Threat model</h2>

<p>The threat model involves an attacker gaining unauthorized access to a user’s active session. This could happen through various means, such as stealing session cookies, exploiting session fixation vulnerabilities, or by using the same device as the victim.</p>

<p>However, would session takeover be prevented by expiring the session after 15 minutes of inactivity?</p>

<h3 id="xss">XSS</h3>

<p>If an attacker steals a session cookie with XSS or session fixation, the attacker immediately gains access to a valid session and can keep performing requests to keep the session alive. An absolute timeout would limit the amount of time the attacker has, but realistically this wouldn’t really hinder any attacker. By definition they steal a valid session token, and can use it immediately. Presumably they are going to immediately make themselves admin, or wire all your bitcoin to their account. Since this is a targeted attack, the attacker knows about session timeouts of the application and can automate their attack to strike before the session times out.</p>

<h3 id="logged-token">Logged token</h3>

<p>If an attacker sees an old session token in the logs, or on your hard drive after they steal your computer, the session timeout probably prevented session takeover. This is an argument for session timeouts, but not necessarily for short session timeouts. Also, it would be better to protect against this by securing the logs or using hard drive encryption.</p>

<h3 id="shared-computers">Shared computers</h3>

<p>Perhaps you used the shared computer in the library to access your web application, and forgot to log out. The next user of that computer could reopen the web application and take over your session.</p>

<p>Is this a thing? Are shared computers without user separation a thing? If so, these shouldn’t be used to access web applications with sensitive information at all, no matter how short the session expiry time is. The device may already be compromised, or the <a href="https://textslashplain.com/2023/05/16/how-do-random-credentials-mysteriously-appear/">browser may remember your password</a>, or sensitive information remains in the browser cache.</p>

<p>Even if internet cafes still exist, some applications are used strictly within an company from company devices. Or people use their own mobile device to access the application. For most web applications, the threat of shared public computers is not realistic.</p>

<h3 id="the-attacker-has-access-to-your-device">The attacker has access to your device</h3>

<p>You forgot to lock your computer when you went to lunch, and the attacker sat down at your desk and gained access to your machine.</p>

<p>In this case, session expiration may prevent them from gaining access to your session, if they weren’t fast enough. However, they now have access to your email, Slack, password vault, SSH agent, browser, and files. They don’t need your active session, they can just create a new one. Either by using the password vault or using the “forgot password” to mail a password reset mail.</p>

<p>One situation in which immediate access to the web application could still be prevented, if when 2FA is enabled and you took your phone or yubikey with you to lunch. But even then, the attacker could install a browser extension that sends your credentials to them the next time you log in.</p>

<h2 id="reauthentication-is-risky">Reauthentication is risky</h2>

<p>Perhaps you prefer short sessions just to be on the safe side. However, short sessions have disadvantages, both in user experience and in security. If someone needs to log in again every 15 minutes, they are going to make authenticating as easy as possible. That means keeping the password vault open, choosing an easier password, or putting the password on the clipboard every time. Reauthentication comes with its own risks. A shorter expiration time does not automatically reduce the overall risk.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Session tokens are pretty secure. The threats described above are easily fixed with other measures, such as disk encryption, locking your computer, or HttpOnly cookies.</p>

<p>Even so, if someone compromises your session, you’re screwed whether it lasts five minutes or for ever. Attacks that are prevented by short session timeouts are really rare.</p>

<p>Finally, short session timeouts come with security and user experience costs.</p>

<p>Facebook, Google, Amazon and GitHub have sessions that never expire. They think it’s an acceptable risk. I think they are right.</p>

<h2 id="read-more">Read more</h2>

<ul>
  <li><a href="https://auth0.com/blog/balance-user-experience-and-security-to-retain-customers/">Balance User Experience and Security to Retain Customers</a></li>
  <li><a href="https://chromium.googlesource.com/chromium/src/+/master/docs/security/faq.md#why-arent-physically_local-attacks-in-chromes-threat-model">Why aren‘t physically-local attacks in Chrome’s threat model?</a></li>
</ul>

  </div>

</article>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WTF is going on with R7RS Large? (180 pts)]]></title>
            <link>http://dpk.io/r7rswtf</link>
            <guid>37173231</guid>
            <pubDate>Fri, 18 Aug 2023 09:32:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://dpk.io/r7rswtf">http://dpk.io/r7rswtf</a>, See on <a href="https://news.ycombinator.com/item?id=37173231">Hacker News</a></p>
<div id="readability-page-1" class="page">

<p>The reaction to John Cowan’s <a href="https://groups.google.com/g/scheme-reports-wg2/c/xGd0_eeKmGI/m/q-xM5fbuAQAJ">resignation as chair of the R<sup>7</sup>RS Large working group</a> has been somewhat wider than I think anyone expected, making it onto general interest programming forums. Scheme may be a small island, but when it has an earthquake, it makes waves through the whole archipelago of programming languages. For the most part, these waves caused only confusion about what all this means.

</p><p>What’s happening? Is Scheme dying? Will R<sup>7</sup>RS carry on? Find out below!

</p><p>Note for the sake of disclosure of interests: I’ve explicitly invited the steering committee to consider me a candidate to replace John Cowan. This is also purely my own view on what has happened, not anyone else’s.

</p><h2>The story so far</h2>

<p>In the beginning, Scheme was created. This made a lot of people very happy and has widely been regarded as an excellent idea.

</p><p>Scheme progressed from a master’s research project through to standardized language through interest in using it in introductory computer science education (think <a href="https://mitp-content-server.mit.edu/books/content/sectbyfn/books_pres_0/6515/sicp.zip/index.html"><cite>Structure and Interpretation of Computer Programs</cite></a>), programming language research, and as a minimal dialect of Lisp for environments with limited resources. An informal group evolved the language through revisions known as R<sup><var>n</var></sup>RS (a mathematical pun with a reference to Algol hidden in it), up to R<sup>5</sup>RS in 1998, which was the first programming language to standardize a hygienic macro system.<a href="#fn-clmacro" rel="footnote" id="fnref-clmacro">*</a>&nbsp;<a href="#fn-rust" rel="footnote" id="fnref-rust">†</a>

</p><p>To further evolve the language, a process called <a href="https://srfi.schemers.org/">SRFI</a> was set up, which basically provided the equivalent of Python PEPs or whathaveyou. Some SRFIs, like <a href="https://srfi.schemers.org/srfi-1/srfi-1.html">SRFI&nbsp;1</a> (a comprehensive library of list utility procedures) were wildly popular and many implementations took them up; others found no adoption at all. So it goes.

</p><p>To move beyond R<sup>5</sup>RS, a Steering Committee was formed in 2004 which appointed editors to specify an R<sup>6</sup>RS. When they were done in 2006, the reaction from the Scheme community was&nbsp;… mixed. Compared to any previous new R<sup><var>n</var></sup>RS, the R<sup>6</sup>RS represented the biggest development from its the preceding R<sup><var>n</var>−1</sup>RS yet. Some people loved it and still love it. For others, it seemed to be the worst thing to ever happen to Scheme. Generally, their criticisms boiled down to: it’s too big for certain use cases; there are too many things implementations are required or forbidden to do, limiting implementations’ ability to create their own extensions to the language; some changes from existing Scheme practice seem gratuitous (for example, instead of adopting SRFI&nbsp;1, or even a subset of it, R<sup>6</sup>RS created its own library with much of the same functionality under incompatible names); it messes up the ‘spirit of Scheme’ (as understood by those who criticized the report). Some implementations added support for it; others refused to touch it with a barge pole.<a href="#fn-sixrejection" rel="footnote" id="fnref-sixrejection">‡</a> Even some implementations which supported it wilfully ignored its strictest requirements.

</p><p>A new Steering Committee was elected by those interested in Scheme in 2009, which recognized that Scheme was trying to serve two purposes which couldn’t be accommodated in one report. Those who liked R<sup>5</sup>RS liked it because it was small and flexible; those who liked R<sup>6</sup>RS wanted something bigger and more practical for writing real, production programs on modern computers. Thus, they planned to revise the language again and split it into two parts: small and large. New working groups were appointed, and the first one delivered <a href="https://github.com/johnwcowan/r7rs-spec/blob/errata/spec/r7rs.pdf">the report on the R<sup>7</sup>RS small language</a><sup>(PDF)</sup> in 2013. This was mostly well-received by those who appreciated the simpler nature of R<sup>5</sup>RS, and pretty much every actively maintained implementation which had rejected R<sup>6</sup>RS adopted R<sup>7</sup>RS&nbsp;small.

</p><h2>The current issues</h2>

<p>Getting the large language done has been a much slower process, in part because of its increased scope. In contrast to R<sup>6</sup>RS (which used the SRFI process to gather feedback on some, but not all of its new features), R<sup>7</sup>RS Large was going to be almost entirely SRFI-based. This meant adopting popular existing SRFIs, but also writing a whole lot of new ones. Anyone in the Scheme community was invited to vote on which ones made it into the language. To attempt to speed things up, in 2022 an <a href="https://codeberg.org/scheme/r7rs/issues">issue tracker</a> was established (until then, to-do items had been in a document maintained by the chair, John Cowan). At the same time, the community identified that there were two groups with slightly different concerns about what R<sup>7</sup>RS Large would look like: some were concerned that the small language itself didn’t impose strict enough semantics for a safe high-level programming language,<a href="#fn-bounds" rel="footnote" id="fnref-bounds">§</a> and wanted other cool new features like delimited continuations; others wanted a language with a large standard library like Python’s. For the most part, those in the former group were concerned that most progress so far had been on the standard libraries, and there was an emphasis on compatibility with the small language which seemed to undermine the idea of new core language features. So the report was split into ‘Foundations’, with stronger error checking than the small language and other new features including an advanced macro system, and ‘Batteries’, which would give us the big standard library others wanted. In theory two groups would be formed, each working on their part.

</p><p>In practice, the working group for R<sup>7</sup>RS&nbsp;Large has consisted of two groups split along very different lines:

</p><ul>
  <li>a small self-selected group (which, I hasten to add, still always welcomes new participants) of people actively contributing to discussion of individual features and proposing new SRFIs and such; plus
  </li><li>a larger community of interested Schemers who come out to vote when we ask them which of these proposals they actually want to see in the new language (likewise, anyone can join this community by sending a statement explaining their interest in Scheme to the mailing list).
</li></ul>

<p>Since these changes in 2022, the small group actively working on the language has overwhelmingly focussed on the Foundations. Those who expressed that their main interest was in the Batteries have got themselves involved in Foundations work, and (to the extent Batteries work has continued) vice versa.

</p><p>Debates on the Foundations have got bogged down a lot in questions of R<sup>6</sup>RS compatibility, even though the Foundations report at the outset was going to ensure, at least, that nothing would prevent an R<sup>6</sup>RS implementation from <em>also</em> supporting R<sup>7</sup>RS. The details of this have proven the most controversial points: should we base all our new Foundations features on R<sup>6</sup>RS? Should R<sup>6</sup>RS in its entirety become part of the Foundations, even though some of R<sup>6</sup>RS is clearly Batteries stuff, like that list library I mentioned earlier? Should R<sup>7</sup>RS implementations be <em>required</em> to support R<sup>6</sup>RS as well? Some of these arguments have undermined the settlement reached in 2022, because Batteries people don’t want the Foundations people to work on (what they perceive to be) the assumption that the Batteries will never be done. On the other side, it’s argued that simply incorporating everything from R<sup>6</sup>RS&nbsp;— possibly in revised form&nbsp;— would settle a lot of questions about the Foundations without having to debate them.

</p><p>There are also procedural disagreements: Marc Nieper-Wißkirchen, who was in theory responsible for the Foundations, is not a fan of putting things to a vote of the larger community, because he feels a report created by democratic vote won’t necessarily be a coherent whole in the end. He wants to work by unanimous consensus, i.e. ‘as long as nobody objects to something, it goes in’. Unanimous consensus of the editors was actually how reports up to and including R<sup>5</sup>RS were written (explaining the smaller size of those reports), but this stance does rule out one option for resolving these fundamental disagreements where unanimity is not possible.

</p><h2>What happens now?</h2>

<p>According to the charter for the working group for the large language, the Steering Committee now has to make a decision: either they appoint a new chair who they think can fix things; or they dissolve the working group and give up on the R<sup>7</sup>RS&nbsp;Large language.

</p><p>The number of people who could credibly become the new chair is limited; the number of people who want it even fewer. When John Cowan informed me he intended to resign (the weekend before he announced it publically), I immediately suggested a successor, whom John thought was a good choice to recommend to the Steering Committee. But, when asked, that person just as immediately turned it down. Thus John was not able to recommend any course of action in his resignation letter. The credible choices of a new chair are, in my view: someone from within that smaller active working group (one of whom, as mentioned, already said they don’t want to be chair); or someone from the Steering Committee itself. There are a few possible candidates in the wider Scheme community, but they seem even more unlikely to want to take on the task: it would be cool to have Guy Steele as chair, but I think he’s still busy with Java or something. So far I appear to be the only person mad enough to actually want to try to take on the responsibility of fixing things (though it’s possible someone else asked the Steering Committee privately to consider them a candidate, too).

</p><p>Equally, though, dissolving the working group seems premature. Everyone who is working on the language is still excited about it and wants to see it completed: work on proposals and specs has continued even while we are without a chair to present them to the voting community. It seems wrong to cancel the project in these circumstances, even if any future chair’s efforts to resolve the conflicts might be seen as ‘last ditch attempts’ before actually, finally giving up.

</p><p>Somewhat concerningly, there’s also a third possibility. All of the members of the Steering Committee have become somewhat less active in the Scheme community in recent years. Though we’ve seen occasional input from individual members, the Steering Committee as a body hasn’t taken any action since 2013 when the small language report was ratified. When the changes made in 2022 were being discussed, we got no guidance from the Committee at all. In short, we’re not sure the Steering Committee exists as a single, functioning entity any more.

</p><p>So it’s possible that the Steering Committee does <em>nothing,</em> which would amount to <i>de facto</i> ending the process, but in the least satisfying possible way. The working group would still technically exist, but without a chair. This might be the end of Scheme standardization altogether, though I hope we could at least find some source of authority to give <em>some</em> kind of future specification its seal of approval.<a href="" rel="footnote" id="fnref-vatican">‖</a>

</p><p>What I wrote in my <a href="https://groups.google.com/g/scheme-reports-wg2/c/TUbmlPY9lR0">statement requesting the Steering Committee consider me a candidate for new chair</a> is true: Scheme has an outsized importance in the world of programming languages compared to its actual use in industry, because it has so often pioneered features that later broke into the mainstream. Proper tail calls as the fundamental means of iteration is now standard in functional programming. Transformation to continuation-passing style as the basis of compilation likewise. Hygienic macros&nbsp;— now part of Rust and <a href="https://peps.python.org/pep-0638/">even being considered in Python</a>&nbsp;— were pioneered by researchers who used Scheme as their test bed. That research became part of the Scheme standard in R<sup>5</sup>RS and R<sup>6</sup>RS. In functional programming, a lot of research is being done at the moment in algebraic effects, which might revolutionize how side-effects like I/O are done in languages like Haskell&nbsp;— and all this work is based on research into delimited control operators, research which was done by Schemers. That’s research we want to put into the standard in R<sup>7</sup>RS&nbsp;Large. Above all, Scheme continues to be the first language of reference for all those who believe, in the words of the introduction to the reports, that ‘programming languages should be designed not by piling feature on top of feature, but by removing the restrictions and limitations that make additional features seem necessary’.

</p><p>It would be a damned shame if this were the end of the road for standard Scheme, but it doesn’t have to be so. I have my plan to save it, but let’s wait and see what the Steering Committee has to say.

  </p><address><a href="http://dpk.io/">Daphne Preston-Kendal</a>, 18 August 2023</address>
  
<hr>

<section id="footnotes">
  <p id="fn-clmacro">*&nbsp;Unfortunately, the standard hygienic macro system in R<sup>5</sup>RS was quite underpowered because it represented a minimum consensus about how macros should work. So the entire concept of hygienic macros in general was pooh-poohed by a lot of Common Lispers mostly due to a misconception that that was <em>all</em> that hygienic macros could do, creating a rift in the Lisp community&nbsp;– even though all Scheme implementations in practice provide a lower-level, more powerful macro system in which you can do everything you can do with unhygienic macros <em>and more</em>. <a href="#fnref-clmacro">↑</a>
  </p><p id="fn-rust">†&nbsp;Nearly twenty years later, hygienic macros&nbsp;– and program-structural macros in general&nbsp;– first made their way into a mainstream programming language when Rust took off. Hooray! <a href="#fnref-rust">↑</a>
  </p><p id="fn-sixrejection">‡&nbsp;A synopsis of all the specific reasons people voted against acception the R<sup>6</sup>RS report, together with responses from John Cowan as contributor to R<sup>7</sup>RS, can be found on the wiki page <a href="https://small.r7rs.org/wiki/SixRejection/">SixRejection</a>. Note that the responses are not any official statement of any working group, and in some cases have become obsolete because the decisions they reference have been superseded. <a href="#fnref-sixrejection">↑</a>
  </p><p id="fn-bounds">§&nbsp;Indeed, the small language doesn’t even enforce bounds checking, even though almost all implementations do it. Such are the sacrifices for a language which might be expected to work on a tiny microcontroller. <a href="#fnref-bounds">↑</a>
  </p><p id="fn-vatican">‖&nbsp;The situation reminds me somewhat of the <a href="https://en.wikipedia.org/wiki/First_Vatican_Council">First Council of the Vatican,</a> which was suspended in 1870 after the unifiers of Italy invaded Rome, but was still technically in session until it was formally closed in 1960 in preparation for a new, Second Council of the Vatican (the one your traditionalist Catholic aunt complains about all the time). Who will formally dissolve the R<sup>7</sup>RS working group eighty years from now to produce R<sup>8</sup>RS? <a href="#fnref-vatican">↑</a>
</p></section>
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Career advice for young system programmers (172 pts)]]></title>
            <link>https://glaubercosta-11125.medium.com/career-advice-for-young-system-programmers-c7443f2d3edf</link>
            <guid>37172815</guid>
            <pubDate>Fri, 18 Aug 2023 08:33:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://glaubercosta-11125.medium.com/career-advice-for-young-system-programmers-c7443f2d3edf">https://glaubercosta-11125.medium.com/career-advice-for-young-system-programmers-c7443f2d3edf</a>, See on <a href="https://news.ycombinator.com/item?id=37172815">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure></figure><div><h2 id="27e6">For some of us, the backend vs frontend discussion just doesn’t cut it. What do do, then?</h2><div><a rel="noopener follow" href="https://glaubercosta-11125.medium.com/?source=post_page-----c7443f2d3edf--------------------------------"><div aria-hidden="false"><p><img alt="Glauber Costa" src="https://miro.medium.com/v2/resize:fill:88:88/0*_vxmqP0UCaArfAxZ" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div></div><p id="5cbb">Much of the advice I see online targeted at “developers” is really bad. And the funny thing is: it is bad advice, even if it is completely true.</p><p id="f60d">That’s because in developers’ minds, it is very easy to say that a “developer” is someone who does the same things I do. But in practice, there are many kinds of developers, focusing on drastically different problems.</p><p id="08ec">Much of the advice online takes the form of “who needs this anyway?”. As a quick detour, that is one of the reasons I appreciate <a href="https://blog.turso.tech/were-proud-partners-with-theprimeagen-155d549f4b14" rel="noopener ugc nofollow" target="_blank">ThePrimeagen</a> so much: he is the rare kind of influencer that is always happy to take a step back, and invite his audience to ask “ok, let’s understand the problem and the domain first”, as opposed to “whoever is doing this or that is doing it wrong”.</p><h2 id="9b4d">It is hard online for system programmers</h2><p id="f5bd">In common discourse, the main groups of developers are “frontend” and “backend” developers. But even those are just a part of the whole. A part I was never interested in since my early days in the late 1990s.</p><p id="8699">There are other developers. The ones writing databases, operating systems, compilers, and foundational building blocks that, if they do their jobs right, end up becoming almost invisible to the average programmer.</p><h2 id="89ce">Advice for systems programmers</h2><p id="6e51">Lots of the advice that you see on Twitter (X?), is especially bad when it comes to systems level software. There’s a certain level of <em>“move fast and break things” </em>that you would never accept from your OS Kernel. The whole discussion of whether or not you should write unit tests sound frankly quite stupid from the point of view of a compiler author or some core Open Source library. I could go on.</p><p id="3ed1">My goal in this article is to give some pointers to young engineers that want to build a career doing systems programming, based on my personal experiences. People in different circumstances may have different experiences and you should listen to them too.</p><h2 id="b679">Do Open Source</h2><p id="d633">Barriers to entry in systems programming are usually higher. Writing a Kernel, a Compiler, or a Database takes years. You can write toy versions of those, but that’s still a large surface area that will take at least months to complete, and you’re now in an environment that is so unrealistic, that you’re not really learning much.</p><p id="6c9a">It is possible to get a job somewhere and learn, but that’s much harder. We live in a society that requires 5 years of experience for tools that only exist for 3 years for entry level jobs. Finding a job that will give you access to world class systems programming challenges, albeit possible, is challenging.</p><p id="4848">Thankfully, lots of cornerstone systems software have their code wide open. If you are interested in compilers, you don’t have to struggle imagining how very basic things could work in your toy compiler. You can just go and play with LLVM.</p><p id="a786">Pick a project that you like, that is industry-relevant, and is well known for its high standards. For me, in the early 2000s, that was the Linux Kernel. Today, I am the founder of Turso and libSQL, that you can contribute to.</p><p id="9e9f">But there are many other projects I can recommend, led by people that I know personally and can vouch for, like <a href="https://github.com/scylladb/scylladb" rel="noopener ugc nofollow" target="_blank">ScyllaDB</a> and its companion <a href="https://github.com/scylladb/seastar" rel="noopener ugc nofollow" target="_blank">Seastar Framework</a> (C++), <a href="https://github.com/redpanda-data/redpanda" rel="noopener ugc nofollow" target="_blank">RedPanda</a> (C++), <a href="https://github.com/tigerbeetle/tigerbeetle" rel="noopener ugc nofollow" target="_blank">TigerBeetle</a> (Zig). The list is very data oriented because that’s what I know best, but there are a variety of projects out there that will fit your interests.</p><h2 id="bc57">Do meaningful Open Source work</h2><p id="0a24">For this to work, “doing Open Source” needs to be more than a stamp. It’s easy to get your name in a project contributor’s list with trivial commits. But if your goal is to learn, at some point you have to break past this.</p><p id="22da">Getting started with a big project can be daunting, and you may feel there’s no good place to start. The good news is that when I say “do Open Source”, that doesn’t mean you need to <em>show </em>your Open Source work to anybody.</p><p id="b644">Remember I said toy projects didn’t help much? That’s because the time-to-value is huge. But a toy project <em>inside </em>an established project? That helps!</p><p id="5886">In my early days I focused more on <em>modifying </em>Linux than <em>contributing</em> to Linux. As an example, I wrote a really basic, and frankly useless, filesystem. You could write to files, but they always returned a static string.</p><p id="878f">I wanted to understand filesystems. With my toy fs, I could get started on a real kernel. I learned about the Linux VFS layer, modules, core interfaces and Linux Kernel concepts, and more importantly: the variety of unpredictable ways in which a program that is in complete control of the hardware can just <em>break</em>, and developed the experience on how to debug and fix those issues.</p><p id="3071">A couple of months later, I made my first (very small) contribution to the ext4 filesystem.</p><h2 id="1806">Does Open Source really help you get hired?</h2><p id="a6b2">The brutally honest reality is that I receive so many applications for people interested in working at our company, that your usual bullet list of technologies mean very little. Yes, interviews should help sifting candidates, but even doing an interview uses up time and resources.</p><p id="c69a">Open Source does help. But these days, getting your name listed as a contributor in an Open Source project is very easy. So your contributions have to be to be meaningful.</p><h2 id="dbe8">Hiring V</h2><p id="6e50">We recently hired someone from our Open Source community. I figured it would be helpful to share a bit of that story from our perspective, in the hopes it can help you too!</p><p id="1e8e">When we released our private beta in February 2023. <a href="https://twitter.com/iavins" rel="noopener ugc nofollow" target="_blank">Avinash</a>, from Bangalore, India (he goes by V), joined it right away. This immediately set him apart: I receive hundreds of standard job applications over a month, and I don’t have time to go through all of them. But I always have time to talk to my users.</p><figure></figure><p id="8d40">Our Discord community has lots of people who join, ask a question, and then ask for a job. I am never interested in talking to these people. But that wasn’t v’s case. He was genuinely interested in what we were doing. He kept engaging with us consistently, and giving feedback on what he’d like to see next.</p><figure></figure><p id="c12b">Not only was he giving us good feedback — that’s easy to do — he started contributing to our tooling. In fact, he quickly became the third top contributor to our Go client. Our Go client is not the most active project in the world, but his contributions were beyond trivial.</p><figure></figure><p id="def6">Then came the tipping point. We have an experimental open source initiative to replace the SQLite default storage engine with an <a href="https://github.com/penberg/tihku" rel="noopener ugc nofollow" target="_blank">MVCC implementation</a>. That’s hard stuff. V started contributing to that too. He <a href="https://github.com/penberg/tihku/issues/15" rel="noopener ugc nofollow" target="_blank">filed and fixed</a> a bug that caused the storage engine to corrupt data.</p><p id="7264">Here’s where it gets interesting: our implementation was correct, as described by the original research paper we based the implementation on. He found a bug <em>in the paper</em>, contacted the authors, who realized the mistake and updated the paper. Then he fixed the implementation.</p><figure></figure><p id="30ad">No other image on the internet could do justice to what I thought at the moment, except for the one and only, Tiffany Gomas.</p><figure></figure><p id="ef74">Companies have a hard time finding great talent. And being a good match for a company goes beyond your coding ability. Are you aligned with the vision? Do you understand what we’re trying to do? Because of that, hiring someone is always a risk, even if a person excels in a technical interview (which as I said earlier, are costly to run on their own).</p><p id="6aa5">It later became known to us that V was looking to switch jobs, and we had just opened a position for our Go backend.</p><p id="3256">The only disagreement we had internally was whether or not we should even interview this guy. My position as the CEO was: why would we even waste time interviewing him? What do we hope to learn? Just send the offer!</p><figure></figure><p id="86d1">I guess people are set in their ways. It <em>feels</em> strange to hire someone without an interview. And because companies sometimes like to give people take home assignments, Sarna, one of our engineers, brilliantly crafted a humorous take on why it was indeed, pointless to even interview V:</p><figure></figure><p id="889e">And so, we hired V.</p><h2 id="c37a">Summary</h2><p id="8d5a">If you are interested in the plumbing of our software industry — systems level programming, much of the career advice online won’t apply to you. I hope in this article I could give you some useful advice on how to stand out from the crowd.</p><p id="43db">I showed you a real example of someone who did stand out, and got hired by us. If you want to chat with V, you can join our <a href="https://discord.gg/GHNN9CNAZe" rel="noopener ugc nofollow" target="_blank">Discord channel</a> today!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pg_later: Asynchronous Queries for Postgres (171 pts)]]></title>
            <link>https://tembo.io/blog/introducing-pg-later/</link>
            <guid>37172689</guid>
            <pubDate>Fri, 18 Aug 2023 08:15:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tembo.io/blog/introducing-pg-later/">https://tembo.io/blog/introducing-pg-later/</a>, See on <a href="https://news.ycombinator.com/item?id=37172689">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__blog-post-container" itemprop="articleBody"><p>We’re working on asynchronous query execution in Postgres and have packaged the work up in an extension we’re calling <a href="https://github.com/tembo-io/pg_later" target="_blank" rel="noopener noreferrer">pg_later</a>. If you’ve used <a href="https://docs.snowflake.com/developer-guide/python-connector/python-connector-example#examples-of-asynchronous-queries" target="_blank" rel="noopener noreferrer">Snowflake’s asynchronous queries</a>, then you might already be familiar with this type of feature. Submit your queries to Postgres now, and come back later and get the query’s results.</p><p>Visit <a href="https://github.com/tembo-io/pg_later" target="_blank" rel="noopener noreferrer">pg_later</a>’s Github repository and give it a star!</p><p><img loading="lazy" alt="elephant-tasker" src="https://tembo.io/assets/images/elephant-3d5154dac74be08748e278a9ad3c7a0e.png" title="elephant-tasker" width="882" height="878"></p><h2 id="why-async-queries">Why async queries?<a href="#why-async-queries" aria-label="Direct link to Why async queries?" title="Direct link to Why async queries?">​</a></h2><p>Imagine that you’ve initiated a long-running maintenance job. You step away while it is executing, only to come back and discover it was interrupted hours ago due to your laptop shutting down. You don’t want this to happen again, so you spend some time googling or asking your favorite LLM how to run the command in the background with screen or tmux. Having asynchronous query support from the beginning would have saved you a bunch of time and headache!</p><p>Asynchronous processing is a useful development pattern in software engineering. It has advantages such as improved resource utilization, and unblocking of the main execution thread.</p><p>Some examples where async querying can be useful are:</p><ul><li>For a DBA running ad-hoc maintenance.</li><li>Developing in interactive environments such as a Jupyter notebook. Rather than submit a long-running query only to have your notebook hang idly and then crash, you can use asynchronous tasks to avoid blocking your Notebook, or simply come back later to check on your task.</li><li>Having a long-running analytical query, for example fulfilling an ad-hoc request like seeing how many new users signed up each day for the past month. You can submit that query and have it run in the background while you continue other work.</li></ul><h2 id="extending-postgres-with-async-features">Extending Postgres with async features<a href="#extending-postgres-with-async-features" aria-label="Direct link to Extending Postgres with async features" title="Direct link to Extending Postgres with async features">​</a></h2><p>At Tembo, we’ve built a similar feature for Postgres and published it as an extension called <strong>pg_later</strong>. With <strong>pg_later</strong>, you can dispatch a query to your Postgres database and, rather than waiting for the results, your program can return and retrieve the results at your convenience.</p><p>A common example is manually executing VACUUM on a table. Typically one might execute VACUUM in one session, and then use another session to check the status of the VACUUM job via <code>pg_stat_progress_vacuum</code>. pg_later gives you the power to do that in a single session. You can use it to queue up any long-running analytical or administrative task on your Postgres database.</p><h2 id="stacking-postgres-extensions">Stacking Postgres Extensions<a href="#stacking-postgres-extensions" aria-label="Direct link to Stacking Postgres Extensions" title="Direct link to Stacking Postgres Extensions">​</a></h2><p><strong>pg_later</strong> is built on top of <a href="https://tembo.io/blog/introducing-pgmq" target="_blank" rel="noopener noreferrer">PGMQ</a>, another one of Tembo's open source extensions. Once a user submits a query, <strong>pg_later</strong> seamlessly enqueues the request in a Postgres-managed message queue. This mechanism then processes the query asynchronously, ensuring no unnecessary wait times or hold-ups.</p><p>The <strong>pg_later</strong> <a href="https://www.postgresql.org/docs/current/bgworker.html" target="_blank" rel="noopener noreferrer">background worker</a> picks up the query from the queue and executes it. The results are persisted by being written to a table as <a href="https://www.postgresql.org/docs/current/functions-json.html" target="_blank" rel="noopener noreferrer">JSONB</a> and can be easily retrieved using the <strong>pg_later</strong> API. You can simply reference the unique job id given upon query submission, and retrieve the result set, or query the table directly. By default, the results are retained forever, however we are building retention policies as a feature into <strong>pg_later</strong>.</p><p><img loading="lazy" alt="diagram" src="https://tembo.io/assets/images/diagram-d6cbc048f614b1157a4ebdcdf011e537.png" title="diagram" width="2146" height="800"></p><h2 id="using-pg_later">Using pg_later<a href="#using-pg_later" aria-label="Direct link to Using pg_later" title="Direct link to Using pg_later">​</a></h2><p>To get started, check out our project’s <a href="https://github.com/tembo-io/pg_later/blob/main/README.md" target="_blank" rel="noopener noreferrer">README</a> for a guide on installing the extension.</p><p>First, you need to initialize the extension. This handles the management of PGMQ objects like a job queue and some metadata tables.</p><p>You're now set to dispatch your queries. Submit the query using pglater.exec, and be sure to take note of the <strong>job_id</strong> that is returned. In this case, it’s the first job so the <strong>job_id</strong> <strong>is 1</strong>.</p><div><pre tabindex="0"><code><span><span>select</span><span> pglater</span><span>.</span><span>exec</span><span>(</span><span></span><br></span><span><span>  </span><span>'select * from pg_available_extensions limit 2'</span><span></span><br></span><span><span></span><span>)</span><span> </span><span>as</span><span> job_id</span><span>;</span><br></span></code></pre></div><p>And whenever you're ready, your results are a query away:</p><div><pre tabindex="0"><code><span><span>select</span><span> pglater</span><span>.</span><span>fetch_results</span><span>(</span><span>1</span><span>)</span><span>;</span><br></span></code></pre></div><div><pre tabindex="0"><code><span><span>{</span><br></span><span><span>  "query": "select * from pg_available_extensions limit 2",</span><br></span><span><span>  "job_id": 1,</span><br></span><span><span>  "result": [</span><br></span><span><span>    {</span><br></span><span><span>      "name": "pg_later",</span><br></span><span><span>      "comment": "pg_later:  Run queries now and get results later",</span><br></span><span><span>      "default_version": "0.0.6",</span><br></span><span><span>      "installed_version": "0.0.6"</span><br></span><span><span>    },</span><br></span><span><span>    {</span><br></span><span><span>      "name": "pgmq",</span><br></span><span><span>      "comment": "Distributed message queues",</span><br></span><span><span>      "default_version": "0.10.1",</span><br></span><span><span>      "installed_version": "0.10.1"</span><br></span><span><span>    }</span><br></span><span><span>  ],</span><br></span><span><span>  "status": "success"</span><br></span><span><span>}</span><br></span></code></pre></div><h2 id="up-next">Up next<a href="#up-next" aria-label="Direct link to Up next" title="Direct link to Up next">​</a></h2><p><code>pg_later</code> is a new project and still under development. A few features that we are excited to build:</p><ul><li>Status and progress of in-flight queries</li><li>Security and permission models for submitted queries</li><li>Cursor support for finished jobs (fetch results row by row)</li><li>Kill a query that is in the queue or is currently in flight</li><li>Support for transactions</li><li>Configurable concurrency levels for background works to increase the throughput of jobs</li><li>Push notifications for completed and failed jobs</li><li>Retention policies for completed jobs</li></ul><p>Give us a <a href="https://github.com/tembo-io/pg_later" target="_blank" rel="noopener noreferrer">star</a> and try out pg_later by running the example in the README. If you run into issues, please create an <a href="https://github.com/tembo-io/pg_later/issues" target="_blank" rel="noopener noreferrer">issue</a>. We would greatly welcome contributions to the project as well.</p><p>Please stay tuned for a follow up post on benchmarking PGMQ vs leading open source message queues.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[London Then and Now: Aerial Shots Show City Grow over Past Two Decades (210 pts)]]></title>
            <link>https://londonist.com/london/art-and-photography/aerial-london-then-and-now</link>
            <guid>37172598</guid>
            <pubDate>Fri, 18 Aug 2023 08:01:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://londonist.com/london/art-and-photography/aerial-london-then-and-now">https://londonist.com/london/art-and-photography/aerial-london-then-and-now</a>, See on <a href="https://news.ycombinator.com/item?id=37172598">Hacker News</a></p>
<div id="readability-page-1" class="page">
  
	
	
		

		<div id="main-container">
	<article data-id="56742" itemscope="" itemtype="http://schema.org/WebPage">
		
		
		
		
		



		
		
		
		<div id="body-block" itemscope="" itemtype="http://schema.org/NewsArticle">
			<p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
				<span itemprop="name">Will Noble</span>
			</span>
			<span itemprop="headline name">London Then And Now: Aerial Shots Show City Grow Over Past Two Decades</span></p><meta itemprop="datePublished" content="2023-08-15T12:00:00+01:00">
			
				<div itemprop="articleBody"><p>London's filling out at such a pace, it's only when you stop for a second and look back on recent history that you appreciate just how much it's transformed. Aerial photographer extraordinaire Jason Hawkes has released a set of photos he took from AS355 helicopters — showing London's prodigious transformation between the 2000s and 2023. All we can say is... my, how you've grown!</p>
<figure id="aa4a133c75c622c4cd8d" data-id="227772"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-8229.jpg" alt="The Olympic stadium only half constructed" itemprop="image">
<figcaption>Queen Elizabeth Olympic Park, 2010</figcaption>
</figure>
<figure id="2461ed0966426eca54cd" data-id="227775"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-8904390_1.jpg" alt="The Olympic Park from above looking verdant" itemprop="image">
<figcaption>Queen Elizabeth Olympic Park, 2020</figcaption>
</figure>
<figure id="6de40bd0ee066ea2aa1b" data-id="227776"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-2007-2.jpg" alt="Battersea Power Station looking dishevelled and roofless" itemprop="image">
<figcaption>Battersea Power Station, 2007</figcaption>
</figure>
<figure id="357df130166a0dca8f42" data-id="227777"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-9001526.jpg" alt="The power station totally refreshed with white chimneys and green roof gardens" itemprop="image">
<figcaption>Battersea Power Station, 2023</figcaption>
</figure>
<figure id="9aa15ad54b8523a7d582" data-id="227778"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-10.jpg" alt="A panorama of the Thames with St Paul's and the Tate Modern visible" itemprop="image">
<figcaption>River Thames at Dusk, 2009</figcaption>
</figure>
<figure id="ac0a0eda97c79509a935" data-id="227779"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-9102769.jpg" alt="The same shot as above, but with a lot more skyscrapers in the City" itemprop="image">
<figcaption>River Thames at Dusk, 2023</figcaption>
</figure>
<figure id="5a8baaae506ffa99a700" data-id="227781"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-1942.jpg" alt="Aerial shot of the City, with the Gherkin dominating" itemprop="image">
<figcaption>The City, 2006</figcaption>
</figure>
<figure id="f6b2c5d7b5c6df6869f6" data-id="227780"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-9003706.jpg" alt="Same image as above, but now with a huge cluster of skyscrapers" itemprop="image">
<figcaption>The City, 2023</figcaption>
</figure>
<figure id="a927653e1a4f31ed3a34" data-id="227782"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-2007.jpg" alt="Aerial view dominated by the verdant Oval ground" itemprop="image">
<figcaption>The Oval, 2007</figcaption>
</figure>
<figure id="b61862c25926debfb2a7" data-id="227783"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-741487.jpg" alt="The same shot as above, but now much more built up" itemprop="image">
<figcaption>The Oval, 2023</figcaption>
</figure>
<figure id="fdb44a8fadfcffcc2cdb" data-id="227784"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-4888.jpg" alt="Wembley Stadium being built with lots of construction going on around it" itemprop="image">
<figcaption>Wembley Stadium, 2013</figcaption>
</figure>
<figure id="4f96fec9dfd7a476a491" data-id="227785"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-817970.jpg" alt="Wembley Stadium surrounded by high rises" itemprop="image">
<figcaption>Wembley Stadium, 2023</figcaption>
</figure>
<figure id="bcbf036f170e0c73d025" data-id="227786"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-2949.jpg" alt="The Canary Wharf buildings with a lot of construction work going on around them" itemprop="image">
<figcaption>Isle of Dogs, 2016</figcaption>
</figure>
<figure id="c440c366152e5b06d1eb" data-id="227787"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-9204214.jpg" alt="The same shot as above, but with dozens more skyscrapers" itemprop="image">
<figcaption>Isle of Dogs, 2023</figcaption>
</figure>
<figure id="00ba0b7893102c88f0fb" data-id="227788"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-6162.jpg" alt="Shot of the City looking distinctly under built - The Shard only partially constructed in the background" itemprop="image">
<figcaption>The City, 2011</figcaption>
</figure>
<figure id="a541ec20c9dfd4135507" data-id="227789"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-773702.jpg" alt="The City now looking much more built up - The Shard completed" itemprop="image">
<figcaption>The City, 2023</figcaption>
</figure>
<figure id="0fdf50107c4be4d66974" data-id="227791"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-4255.jpg" alt="Nine Elms scored with railway tracks" itemprop="image">
<figcaption>Nine Elms, 2013</figcaption>
</figure>
<figure id="bf46a8c10fe2e98ed218" data-id="227793"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-874804.jpg" alt="Nine Elms looking a lot more built up" itemprop="image">
<figcaption>Nine Elms, 2023</figcaption>
</figure>
<figure id="49bbefa483c8d1b4edd8" data-id="227794"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-0018.jpg" alt="Tower Bridge with a notably low-rise landscape around it" itemprop="image">
<figcaption>Tower Bridge, 2006</figcaption>
</figure>
<figure id="ebda01acccd253bb4425" data-id="227795"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-9102864.jpg" alt="Tower Bridge now surrounded by lots more buildings - including the Shard" itemprop="image">
<figcaption>Tower Bridge, 2023</figcaption>
</figure>
<figure id="4f13c811ce14d1c298ad" data-id="227796"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-5338.jpg" alt="The Emirates being constructed, with Highbury Stadium - still open - in the foreground" itemprop="image">
<figcaption>Highbury Stadium and The Emirates, 2005</figcaption>
</figure>
<figure id="b5085c03bac580c6873e" data-id="227797"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-9302962.jpg" alt="Highbury has now been converted into flats, with the Emirates completed" itemprop="image">
<figcaption>The old Highbury Stadium and The Emirates, 2023</figcaption>
</figure>
<figure id="180fd3d6c97eec19ccd6" data-id="227799"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-70.jpg" alt="A sweep in the Thames by the Tower of London - showing an under developed landscape" itemprop="image">
<figcaption>Tower of London, 2002</figcaption>
</figure>
<figure id="545b2077d2bec2b3fd86" data-id="227800"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-773654.jpg" alt="The same shot as above, but far more built up" itemprop="image">
<figcaption>Tower of London, 2023</figcaption>
</figure>
<figure id="fe05a74a7f03e5082d1a" data-id="227792"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-0625.jpg" alt="" itemprop="image">
<figcaption>King's Cross, 2015</figcaption>
</figure>
<figure id="8ec6cff34375121d9fc3" data-id="227802"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-775471.jpg" alt="Another aerial shot of King's Cross - this time a lot more built up" itemprop="image">
<figcaption>King's Cross, 2023</figcaption>
</figure>
<figure id="f65b456b85adc8e9117f" data-id="227803"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-10-2.jpg" alt="A sunset over the Greenwich peninsula - with the O2 by the water - but not much more development" itemprop="image">
<figcaption>North Greenwich, 2007</figcaption>
</figure>
<figure id="80a18bc29db8d8daa87d" data-id="227804"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-8903803.jpg" alt="The same shot as above - now with more development o the peninsula and across the water in the City" itemprop="image">
<figcaption>North Greenwich, 2023</figcaption>
</figure>
<p><em>All images © <a href="https://www.jasonhawkes.com/">Jason Hawkes</a></em></p>
</div>
			
			
			
				<p><em>Last Updated 17 August 2023</em></p>
		</div>
		
		
		
		

	
  	
		

	


<p><em>Continued below.</em></p>


		
		



		
		
		
		

	
    


    
	</article>
		
	<section id="side-bar">
				<h2>TRENDING</h2>
					<div data-id="51814" data-date="2023-06-23 10:45:00 +0100">
	
	
		<p><a href="https://londonist.com/london/sport/womens-world-cup-2023-screenings-where-to-watch-pubs-bars-showing-matches">
			<img alt="Where To Watch The England Vs Spain Women's World Cup Final In London" src="https://assets.londonist.com/uploads/2023/06/i300x150/womens_world_cup_boxpark_2023_screenings.jpg">
</a></p>
	
</div>
					<div data-id="52164" data-date="2022-04-25 10:30:00 +0100">
	
	
		<p><a href="https://londonist.com/london/pubs/pubs-bars-football-watch-premier-league-championship">
			<img alt="The Best Pubs And Bars For Watching Football In London" src="https://assets.londonist.com/uploads/2023/01/i300x150/london_food_and_drink_photography_-_peckham_levels_london_2022_-_nic_crilly-hargrave-35.jpg">
</a></p>
	
</div>
					<div data-id="52152" data-date="2023-07-28 10:54:00 +0100">
	
	
		<p><a href="https://londonist.com/london/beyond-london/day-trips-from-london-august">
			<img alt="13 Summery Day Trips From London: August 2023" src="https://assets.londonist.com/uploads/2023/07/i300x150/imberbus32.jpg">
</a></p>
	
</div>
					<div data-id="52145" data-date="2023-07-27 09:06:00 +0100">
	
	
		<p><a href="https://londonist.com/london/things-to-do/things-to-do-in-london-in-august">
			<img alt="40+ Summery Things To Do In London This Month: August 2023" src="https://assets.londonist.com/uploads/2023/07/i300x150/summer_sounds_kings_cross.jpg">
</a></p>
	
</div>
			</section>
</div>
  
    
    
  	


  
	
	
	
	
    
	
	
	


</div>]]></description>
        </item>
    </channel>
</rss>