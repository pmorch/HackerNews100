<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 17 Mar 2024 09:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[The return of the frame pointers (204 pts)]]></title>
            <link>https://www.brendangregg.com/blog/2024-03-17/the-return-of-the-frame-pointers.html</link>
            <guid>39731824</guid>
            <pubDate>Sun, 17 Mar 2024 03:59:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.brendangregg.com/blog/2024-03-17/the-return-of-the-frame-pointers.html">https://www.brendangregg.com/blog/2024-03-17/the-return-of-the-frame-pointers.html</a>, See on <a href="https://news.ycombinator.com/item?id=39731824">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Sometimes debuggers and profilers are obivously broken, sometimes it's subtle and hard to spot. From my <a href="https://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html#FlameGraph">flame graphs</a> page:</p>

<center><a href="https://www.brendangregg.com/FlameGraphs/cpu-bash-flamegraph.svg"><img src="https://www.brendangregg.com/blog/images/2024/cpu-bash-flamegraph-annotated.png" width="700"></a><br><span size="-1"><i>CPU flame graph (partly broken)</i></span></center>

<p>(Click for original SVG.) This is pretty common and usually goes unnoticed as the <a href="https://www.brendangregg.com/FlameGraphs/cpu-bash-flamegraph.svg">flame graph</a> looks ok at first glance. But there are 15% of samples on the left, above "[unknown]", that are in the wrong place and missing frames. The problem is that this system has a default libc that has been compiled without frame pointers, so any stack walking stops at the libc layer, producing a partial stack that's missing the application frames. These partial stacks get grouped together on the left.</p>



<p>Click here for a longer explanation.</p>



<p>Other types of profiling hit this more often. <a href="https://www.brendangregg.com/FlameGraphs/offcpuflamegraphs.html">Off-CPU flame graphs</a>, for example, can be dominated by libc read/write and mutex functions, so without frame pointers end up mostly broken. Apart from library code, maybe your application doesn't have frame pointers either, in which case everything is broken.</p>

<p><strong>I'm posting about this problem now because Fedora and Ubuntu are releasing versions that fix it</strong>, by compiling libc and more with frame pointers by default. This is great news as it not only fixes these flame graphs, but makes off-CPU flame graphs far more practical. This is also an win for continuous profilers (my employer, Intel, just <a href="https://www.intc.com/news-events/press-releases/detail/1683/intel-releases-continuous-profiler-to-increase-cpu">announced</a> one) as it makes customer adoption easier.</p>

<h2>What are frame pointers?</h2>

<p>The x86-64 ABI <a href="https://gitlab.com/x86-psABIs/x86-64-ABI">documentation</a> shows how a CPU register, %rbp, can be used as a "base pointer" to a stack frame, aka the "frame pointer." I pictured how this is used to walk stack traces in my BPF book.</p>

<center>

</center>

<p>This stack-walking technique is commonly used by external profilers and debuggers, including Linux perf and eBPF, and ultimately visualized by flame graphs. However, the x86-64 ABI has a footnote [12] to say that this register use is optional:</p>

<blockquote>"The conventional use of %rbp as a frame pointer for the stack frame may be avoided by using %rsp
(the stack pointer) to index into the stack frame. This technique saves two instructions in the prologue and
epilogue and makes one additional general-purpose register (%rbp) available."</blockquote>

<p>(Trivia: I had penciled the frame pointer function prologue and epilogue on my Netflix <a href="https://www.brendangregg.com/blog/images/2022/brendanwall.jpg">office wall</a>, lower left.)</p>

<h2>2004: Their removal</h2>

<p>In 2004 a compiler developer, Roger Sayle, changed gcc to stop generating frame pointers, <a href="https://gcc.gnu.org/legacy-ml/gcc-patches/2004-08/msg01033.html">writing</a>:</p>

<blockquote>"The simple patch below tweaks the i386 backend, such that we now
default to the equivalent of "-fomit-frame-pointer -ffixed-ebp" on
32-bit targets"</blockquote>

<p>i386 (32-bit microprocessors) only have four general purpose registers, so freeing up %ebp takes you from four to five. I'm sure this delivered large performance improvements and I wouldn't try arguing against it. Roger cited two other reasons for this change: The desire to outperform Intel's icc compiler, and the belief that it didn't break debuggers (of the time) since they supported other stack walking techniques.</p>

<h2>2005-2023: The winter of broken profilers</h2>

<p>However, the change was then applied to x86-64 (64-bit) as well, which had sixteen registers and didn't benefit so much from a seventeenth. And there are debuggers/profilers that this change did break, more so today with the rise of eBPF, which didn't exist back then (typically system profilers, not language specific ones). As my former Sun Microsystems colleague Eric Schrock (nickname Schrock) wrote in <a href="https://web.archive.org/web/20131215093042/https://blogs.oracle.com/eschrock/entry/debugging_on_amd64_part_one">November 2004</a>:</p>

<blockquote>"On i386, you at least had the advantage of increasing the number of usable registers by 20%. On amd64, adding a 17th general purpose register isn't going to open up a whole new world of compiler optimizations. You're just saving a pushl, movl, an series of operations that (for obvious reasons) is highly optimized on x86. And for leaf routines (which never establish a frame), this is a non-issue. Only in extreme circumstances does the cost (in processor time and I-cache footprint) translate to a tangible benefit - circumstances which usually resort to hand-coded assembly anyway. Given the benefit and the relative cost of losing debuggability, this hardly seems worth it."</blockquote>

<p>In Schrock's conclusion:</p>

<blockquote>"it's when people start compiling /usr/bin/ without frame pointers that it gets out of control."</blockquote>

<p>This is exactly what happened on Linux, not just /usr/bin but also /usr/lib and application code! I'm sure there are people who are too new to the industry to remember the pre-2004 days when profilers would "just work" without OS and runtime changes.</p>

<h2>2014: Java in Flames</h2>

<div><center><img src="https://www.brendangregg.com/blog/images/2024/Surge2014_CloudsToRoots_066-crop.jpg" width="350"><br>
<span size="-1"><i>Broken Java Stacks (2014)</i></span></center></div>

<p>When I joined Netflix in 2014, I found Java's lack of frame pointer support broke all application stacks (pictured in my 2014 <a href="https://www.brendangregg.com/Slides/Surge2014_CloudsToRoots/">Surge talk</a> on the right). I ended up developing a fix for the JVM c2 compiler which Oracle reworked and added as the -XX:+PreserveFramePointer option in JDK8u60 (see my <a href="http://techblog.netflix.com/2015/07/java-in-flames.html">Java in Flames</a> post for details [<a href="https://www.brendangregg.com/Articles/Netflix_Java_in_Flames.pdf">PDF</a>]).</p>

<p>While that Java change led to discovering countless performance wins in application code, libc was still breaking some portion of the samples (as pictured in the example at the top of this post) and preventing off-CPU flame graphs. I started by compiling my own libc for production use with frame pointers, and then worked with Canonical to have one prebuilt for Ubuntu. For a while I was promoting the use of Canonical's libc6-prof, which was libc6 with frame pointers.</p>

<h2>2015-2020: Overhead</h2>

<p>As part of production rollout I did many performance overhead tests, which I've described publicly before: The overhead of adding frame pointers to everything (libc and Java) was usually less than 1%, with one exception of 10%. That 10% was an unusual application that was generating stack traces over 1000 frames deep (via Groovy), so deep that it broke Linux's perf profiler and Arnaldo Carvalho de Melo (Red Hat) added the <a href="https://lkml.iu.edu/hypermail/linux/kernel/1604.2/03420.html">kernel.perf_event_max_stack</a> sysctl just for this Netflix workload. It was also a virtual machine that lacked low-level hardware profiling capabilities, so I wasn't able to do cycle analysis to confirm that the 10% was entirely frame pointer-based.</p>

<p>The actual overhead depends on your workload. Others have reported around 1% and around 2%. Microbenchmarks can be the worst, hitting 10%: This doesn't surprise me since they resolve to running a small funciton in a loop, and adding any instructions to that function can cause it to spill out of L1 cache warmth (or cache lines) causing a drop in performance. If I were analyzing such a microbenchmark, apart from observability anaylsis (cycles, instructions, PMU, PMCs, PEBS) there are also an experiment I'd like to try:</p>

<ul><span size="-1">To test the theory of I-cache spillover: Compile the microbenchmark with and without frame pointers and find the performance delta. Then flame graph the microbenchmark to understand the hot function. Then add some inline assembly to the hot function where you add enough NOPs to the start and end to mimic the frame pointer prologue and epilogue (I recommend writing them on your office wall in pencil), compile it without frame pointers, disassemble the compiled binary to confirm those NOPs weren't stripped, and now test that. If the performance delta is still large (10%) you've confirmed that it is due to cache effects, and anyone who was worked at this level in production will tell you that it's the straw that broke the camel's back. Don't blame the straw, in this case, don't blame the frame pointers. Adding <i>anything</i> will cause the same effect. Having done this before, it reminds me of CSS programming: you make a little change here and everything breaks, and you spend hours chasing your own tail.</span></ul>

<p>Someone recently told me that Python can hit 10% overhead with frame pointers. That also needs to be debugged to see what's going on. My experience is that it's the exception and not the rule. And don't forget what this is changing: It gives the compiler an extra <em>seventeenth</em> register, and adds some highly-optimized instructions to every function. It shouldn't be 10%, unless it's cache effects.</p>

<p>As I've seen frame pointers help find performance wins ranging from 5% to 500%, the typical "less than 1%" cost (or even 1% or 2% cost) is easily justified. But I'd rather the cost be zero, of course! We may get there with future technologies I'll cover later. In the meantime, frame pointers are the most practical way to find performance wins today.</p>

<p>What about Linux on devices where there is no chance of profiling or debugging, like electric toothbrushes? (I made that up, AFAIK they don't run Linux, but I may be wrong!) Sure, compile without frame pointers. The main users of this change are enterprise Linux. Back-end servers.</p>

<h2>2022: Upstreaming, first attempt</h2>

<p>Other large companies with OS and perf teams (Meta, Google) hinted strongly that they had already enabled frame pointers for everything years earlier. (Google should be no surprise because they pioneered continuous profiling.) So at this point you had Google, Meta, and Netflix running their own libc with frame pointers and able to enjoy profiling capabilities that most other companies – without dedicated OS teams – couldn't get working. Can't we just upstream this so everyone can benefit?</p>

<p>There's a bunch of difficulties when taking "works well for me" changes and trying to make them the default for everyone. Among the difficulties is that end-user companies don't have a clear return on the investment from telling their Linux vendor what they fixed, since they already fixed it. I guess the investment is quite small, we're talking about a single email, right?...Wrong! Your suggestion is now a <a href="https://pagure.io/fesco/issue/2817">116-post thread</a> where everyone is sharing different opinions and demanding this and that, as we found out the hard way. For Fedora, one person requested:</p>

<blockquote>"Meta and/or Netflix should provide infrastructure for a side repository in which the change can be tested and benchmarked and the code size measured."</blockquote>

<p>(Bear in mind that Netflix doesn't even use Fedora!)</p>

<p>Jonathan Corbet, who writes the best Linux articles, summarized this in "<a href="https://lwn.net/Articles/919940/">Fedora's tempest in a stack frame</a>" that is so detailed that I feel PTSD when reading it. It's good that the Fedora community wants to be so careful, but I'd rather spend time discussing building something <em>better</em> than frame pointers, perhaps involving ORC, LBR, eBPF, and other technologies, than so much worry about looking bad in kitchen-sink benchmarks that I wouldn't trust in the first place.</p>

<h2>2023, 2024: Frame Pointers in Fedora and Ubuntu!</h2>

<p><strong>Fedora</strong> <a href="https://pagure.io/fesco/issue/2923">revisited</a> the proposal and has accepted it this time, making it the first distro to reenable frame pointers. Thank you!</p>

<p><strong>Ubuntu</strong> has also announced <a href="https://ubuntu.com/blog/ubuntu-performance-engineering-with-frame-pointers-by-default">frame pointers by default</a> in Ubuntu 24.04 LTS. Thank you!</p>

<p>While this fixes stack walking through OS libraries, you might find your application still doesn't support stack tracing, but that's typically much easier to fix. Java, for example, has the -XX:+PreserveFramePointer option. There were ways to get Golang to support frame pointers, but that became the default years ago. Just to name a couple of languages.</p>

<h2>2034+: Beyond Frame Pointers</h2>

<p>There's more than one way to walk a stack. These could be separate blog posts, but I want to comment briefly on alternates:</p>

<ul>
<li><strong>LBR (Last Branch Record)</strong>: Intel's hardware feature that was limited to 16 or 32 frames. Most application stacks are deeper, so this can't be used to build flame graphs, but it is better than nothing. I use it as a last resort as it gives me <em>some</em> stack insights.</li>
<li><strong>BTS (Branch Trace Store)</strong>: Another Intel thing. Not so limited to stack depth, but has overhead from memory load/stores and BTS buffer overflow interrupt handling.</li>
<li><strong>AET (Archetectural Event Trace)</strong>: Another Intel thing. It's a <a href="https://www.asset-intertech.com/resources/blog/2020/03/intel-architectural-event-trace-aet-in-action/">JTAG-based tracer</a> that can trace low-level CPU, BIOS, and device events, and apparently can be used for stack traces as well. I haven't used it. (I spent years as a cloud customer where I couldn't access many HW-level things.) I hope it can be configured to output to main memory, and not just a physical debug port.</li>
<li><strong>DWARF</strong>: Binary debuginfo, has been used forever with debuggers. Doesn't exist for JIT'd runtimes like the Java JVM, and I don't really see any practical way to ever fix that. The overhead to walk DWARF is also too high, as it was designed for non-realtime use. Polar Signals did some interesting work using an eBPF walker to reduce the overhead, but...Java.</li>
<li><strong>eBPF stack walking</strong>: Mark Wielaard (Red Hat) demonstrated a Java JVM stack walker using SystemTap back at LinuxCon 2014, where an external tracer walked a runtime with no runtime support or help. Very cool. This can be done using eBPF as well. The performmance overhead could be too high, however, as it may mean a lot of user space reads of runtime internals depending on the runtime.</li>
<li><strong>ORC (oops rewind capability)</strong>: The Linux kernel's new lightweight <a href="https://lwn.net/Articles/728339/">stack unwinder</a> by Josh Poimboeuf (Red Hat) that has allowed newer kernels to remove frame pointers yet retain stack walking. You may be using ORC without realizing it; the rollout was smooth as the kernel profiler code was updated to support ORC (perf_callchain_kernel()-&gt;unwind_orc.c) at the same time as it was compiled to support ORC. Can't ORCs invade user space as well?</li>
<li><strong>SFrames (Stack Frames)</strong>: ...which is what <a href="https://lwn.net/Articles/932209/">SFrames</a> does: lightweight user stack unwinding based on ORC. There have been recent talks to explain them by <a href="https://www.youtube.com/watch?v=4XrFYpjyodo">Indu Bhagat</a> (Oracle) and <a href="https://www.youtube.com/watch?v=FKB-vudYqCw">Steven Rostedt</a> (Google). I should do a blog post just on SFrames.</li>
<li><strong>Shadow Stacks</strong>: A newer Intel and AMD security <a href="https://lwn.net/Articles/885220/">feature</a> that can be configured to push function return addresses onto a separate HW stack so that they can be double checked when the return happens. Sounds like such a HW stack could also provide a stack trace, without frame pointers.</li>
<li>(And this isn't even all of them.)</li>
</ul>

<p>Daan De Meyer (Meta) did a nice summary as well of different stack walkers on the <a href="https://fedoraproject.org/wiki/Changes/fno-omit-frame-pointer#Alternatives_to_frame_pointers">Fedora wiki</a>.</p>

<p>So what's next? Here's my guesses:</p>

<ul>
<li>2029: Ubuntu and Fedora release new versions with SFrames for OS components (including libc) and ditches frame pointers again. We'll have had five years of frame pointer-based performance wins and new innovations that make use of user space stacks, and will hit the ground running with SFrames.</li>
<li>2034: Shadow stacks have been enabled by default for security, and then are used for all stack tracing.</li>
</ul>

<h2>Conclusion</h2>

<p>I could say that times have changed and now the original 2004 reasons for omitting frame pointers are no longer valid in 2024. Those reasons were that it improved performance significantly on i386, that it didn't break the debuggers of the day (prior to eBPF), and that competing with another compiler (icc) was deemed important. Yes, times have indeed changed. But I should note that one engineer, Eric Schrock, claimed that it didn't make sense back in 2004 either when it was applied to x86-64, and I agree with him. Profiling has been broken for 20 years and we've only now just fixed it.</p>

<p><a href="https://pagure.io/fesco/issue/2923">Fedora</a> and <a href="https://ubuntu.com/blog/ubuntu-performance-engineering-with-frame-pointers-by-default">Ubuntu</a> have now returned frame pointers, which is great news. People should start running these releases in 2024 and will find that <a href="https://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html">CPU flame graphs</a> make more sense, <a href="https://www.brendangregg.com/FlameGraphs/offcpuflamegraphs.html">Off-CPU flame graphs</a> work for the first time, and other new things become possible. It's also a win for continuous profilers, as they don't need to convince their customers to make OS changes to get profiles to fully work.</p>

<h2>Thanks</h2>

<p>The online threads about this change aren't even everything, there's been many discussions, meetings, and work put into this, not just for frame pointers but other recent advances including ORC and SFrames. Special thanks to Andrii Nakryiko (Meta), Daan De Meyer (Meta), Davide Cavalca (Meta), Ian Rogers (Google), Steven Rostedt (Google), Josh Poimboeuf (Red Hat), Arjan Van De Ven (Intel), Indu Bhagat (Oracle), Mark Shuttleworth (Canonical), Jon Seager (Canonical), Oliver Smith (Canonical), and many others (see the Fedora discussions). And thanks to Schrock.</p>

<h2>Appendix: Fedora</h2>

<p>For reference, here's my writeup for the Fedora <a href="https://pagure.io/fesco/issue/2817#comment-826805">change</a>:</p>

<pre>I enabled frame pointers at Netflix, for Java and glibc, and summarized the effect in BPF
Performance Tools (page 40):

"Last time I studied the performance gain from frame pointer omission in our production
environment, it was usually less than one percent, and it was often so close to zero that it
was difficult to measure. Many microservices at Netflix are running with the frame pointer
reenabled, as the performance wins found by CPU profiling outweigh the tiny loss of
performance."

I've spent a lot of time analyzing frame pointer performance, and I did the original work to
add them to the JVM (which became -XX:+PreserveFramePoiner). I was also working with another
major Linux distro to make frame pointers the default in glibc, although I since changed jobs
and that work has stalled. I'll pick it up again, but I'd be happy to see Fedora enable it in
the meantime and be the first to do so.

We need frame pointers enabled by default because of performance. Enterprise environments are
monitored, continuously profiled, and analyzed on a regular basis, so this capability will
indeed be put to use. It enables a world of debugging and new performance tools, and once you
find a 500% perf win you have a different perspective about the &lt;1% cost. Off-CPU flame graphs
in particular need to walk the pthread functions in glibc as most blocking paths go through
them; CPU flame graphs need them as well to reconnect the floating glibc tower of
futex/pthread functions with the developers code frames.

I see the comments about benchmark results of up to 10% slowdowns. It's good to look out for
regressions, although in my experience all benchmarks are wrong or deeply misleading. You'll
need to do cycle analysis (PEBS-based) to see where the extra cycles are, and if that makes
any sense. Benchmarks can be super sensitive to degrading a single hot function (like "CPU
benchmarks" that really just hammer one function in a loop), and if extra instructions
(function prologue) bump it over a cache line or beyond L1 cache-warmth, then you can get a
noticeable hit. This will happen to the next developer who adds code anyway (assuming such a
hot function is real world) so the code change gets unfairly blamed. It will only regress in
this particular scenario, and regression is inevitable. Hence why you need the cycle analysis
("active benchmarking") to make sense of this.

There was one microservice that was an outlier and had a 10% performance loss with Java frame
pointers enabled (not glibc, I've never seen a big loss there). 10% is huge. This was before
PMCs were available in the cloud, so I could do little to debug it. Initially the microservice
ran a "flame graph canary" instance with FPs for flame graphs, but the developers eventually
just enabled FPs across the whole microservice as the gains they were finding outweighed the
10% cost. This was the only noticeable (as in, &gt;1%) production regression we saw, and it was a
microservice that was bonkers for a variety of reasons, including stack traces that were over
1000 frames deep (and that was after inlining! Over 3000 deep without. ACME added the
perf_event_max_stack sysctl just so Netflix could profile this microservice, as the prior
limit was 128). So one possibility is that the extra function prologue instructions add up if
you frequently walk 1000 frames of stack (although I still don't entirely buy it). Another
attribute was that the microservice had over 1 Gbyte of instruction text (!), and we may have
been flying close to the edge of hardware cache warmth, where adding a bit more instructions
caused a big drop. Both scenarios are debuggable with PMCs/PEBS, but we had none at the time.

So while I think we need to debug those rare 10%s, we should also bear in mind that customers
can recompile without FPs to get that performance back. (Although for that microservice, the
developers chose to eat the 10% because it was so valuable!) I think frame pointers should be
the default for enterprise OSes, and to opt out if/when necessary, and not the other way
around. It's possible that some math functions in glibc should opt out of frame pointers
(possibly fixing scimark, FWIW), but the rest (especially pthread) needs them.

In the distant future, all runtimes should come with an eBPF stack walker, and the kernel
should support hopping between FPs, ORC, LBR, and eBPF stack walking as necessary. We may
reach a point where we can turn off FPs again. Or maybe that work will never get done. Turning
on FPs now is an improvement we can do, and then we can improve it more later.

For some more background: Eric Schrock (my former colleague at Sun Microsystems) described the
then-recent gcc change in 2004 as "a dubious optimization that severely hinders debuggability"
and that "it's when people start compiling /usr/bin/* without frame pointers that it gets out
of control" I recommend reading his post: [0].

The original omit FP change was done for i386 that only had four general-purpose registers and
saw big gains freeing up a fifth, and it assumed stack walking was a solved problem thanks to
gdb(1) without considering real-time tracers, and the original change cites the need to
compete with icc [1]. We have a different circumstance today -- 18 years later -- and it's
time we updated this change.

[0] http://web.archive.org/web/20131215093042/https://blogs.oracle.com/eschrock/entry/debugging_on_amd64_part_one
[1] https://gcc.gnu.org/ml/gcc-patches/2004-08/msg01033.html
</pre>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Losing Faith on Testing (123 pts)]]></title>
            <link>https://registerspill.thorstenball.com/p/a-few-words-on-testing</link>
            <guid>39731195</guid>
            <pubDate>Sun, 17 Mar 2024 01:48:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://registerspill.thorstenball.com/p/a-few-words-on-testing">https://registerspill.thorstenball.com/p/a-few-words-on-testing</a>, See on <a href="https://news.ycombinator.com/item?id=39731195">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>First, my credentials. More than half of all the code I wrote in my life is test code. My name is attached to </span><a href="https://interpreterbook.com/" rel="">hundreds</a><span> of </span><a href="https://compilerbook.com/" rel="">pages</a><span> of TDD. In my first internship as a software developer I wrote tests and did TDD while pair programming in my first week. I’ve written unit tests, integration tests, tests for exploration, tests to stop problems from reappearing, tests to leave a message, tests using testing frameworks and BDD and no framework at all, tests in Ruby, JavaScript, C, Go, Rust, Scheme, Bash. After two drinks, I’m willing to say that I know more about testing than many others. Right now — no drinks —&nbsp;I’m willing to say that I love testing and that writing tests has brought me a lot of joy.</span></p><p>Yet I can no longer say that I’m free of doubt. To stick with the theme: I’m much more sober about testing today than I was ten years ago. Recently, in the past few months, the doubts have grown.</p><p><span>Too many flaky tests. Too much time spent getting the tests to pass after making a tiny change that I knew was correct but the tests didn’t. Too many integration tests that made people wait 20, 30, 40 minutes until they could merge their change, only to reveal — months later — that they never tested anything. Too many times have I fixed a bug and </span><em>knew</em><span> it was fixed because I tested it manually, thoroughly, and was 100% sure that I know how the code works and that this can’t happen again, but then spent hours — 10 times longer than it took me to fix the bug —&nbsp;to write a test only to prove what I knew all along, that the bug is fixed.</span></p><p>It’s not that I was ever a capital-b Believer in tests. I never believed in testing coverage as a metric, never really cared whether someone wrote their tests first or last (although I think too few people have seriously tried TDD), came to think that most discussions around functional vs.&nbsp;intergration vs. whathaveyou tests are a big misunderstanding and that people who say you should never hit the database in tests should get real and probably haven’t written enough tests yet.</p><p>Still, I’ve always thought of tests as good and untested code as bad. Whenever I merged something that didn’t have a test I felt guilty, even when deep down I knew that the test might not be worth it. Tests, I thought, are a sign of quality and the better tested something is, the higher the quality of the product.</p><p><span>Enter </span><a href="https://mitchellh.com/ghostty" rel="">Ghostty</a><span> and </span><a href="https://zed.dev/" rel="">Zed</a><span>.</span></p><p>Both are among the highest-quality software I have ever used and hacked on.</p><p>Both have less tests than I expected.</p><p><span>Both do have tests, of course. Ghostty has extensive tests for its core: the terminal state, the font rendering, the parser of escape and control sequences, and so on. Zed also a lot of tests for its foundational data structures — the rope, the SumTree, the editor, and so on — and tests for big features, and </span><a href="https://www.youtube.com/watch?v=ms8zKpS_dZE" rel="">very smart, very cool property tests for async code</a><span>.</span></p><p>But neither codebase has tests, for example, that take a long-ass time to run. No tests that click through the UI and screenshot and compare and hit the network. Zed’s complete testsuite takes 136 seconds to run 1052 tests in CI. Ghostty’s takes 38 seconds, including compilation. Many tests, but less than I thought.</p><p>In both codebases I’ve merged PRs without any tests and frequently see others do the same. And the world didn’t end and no one shed any tears and the products are still some of best I’ve ever used and the codebases contain some of the most elegant code I’ve ever read.</p><p>So now I’m writing this and it feels like a confession to say that I’m beginning to think that maybe there’s no correlation between software quality and tests. Maybe the tests are only a symptom. A symptom of something else that causes the quality.</p><p><span>Maybe the wisest thing about testing that’s ever been said and maybe the only thing that you need to know about testing — forget about the testing pyramids, and the mocks vs.&nbsp;stubs debates, and the dependency injectors, and the coverage numbers —&nbsp;is what Kent Beck said </span><a href="https://stackoverflow.com/a/153565" rel="">16 years ago in an answer on Stack Exchange</a><span>:</span></p><blockquote><p>I get paid for code that works, not for tests, so my philosophy is to test as little as possible to reach a given level of confidence […]. If I don’t typically make a kind of mistake (like setting the wrong variables in a constructor), I don’t test for it.</p></blockquote><p>Maybe that’s all you need. That and people who give a damn.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Debloat non-rooted Android devices (126 pts)]]></title>
            <link>https://github.com/0x192/universal-android-debloater</link>
            <guid>39730962</guid>
            <pubDate>Sun, 17 Mar 2024 01:09:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/0x192/universal-android-debloater">https://github.com/0x192/universal-android-debloater</a>, See on <a href="https://news.ycombinator.com/item?id=39730962">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Universal Android Debloater GUI</h2><a id="user-content-universal-android-debloater-gui" aria-label="Permalink: Universal Android Debloater GUI" href="#universal-android-debloater-gui"></a></p>
<p dir="auto"><strong>DISCLAIMER</strong>: Use at your own risk. I am not responsible for anything that
could happen to your phone.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/0x192/universal-android-debloater/blob/main/resources/screenshots/v0.5.0.png"><img src="https://github.com/0x192/universal-android-debloater/raw/main/resources/screenshots/v0.5.0.png" width="850" alt="uad_screenshot"></a></p>
<p dir="auto"><strong>This software is still in an early stage of development. Check out the issues, and feel free to contribute!</strong></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Summary</h2><a id="user-content-summary" aria-label="Permalink: Summary" href="#summary"></a></p>
<p dir="auto">This is a complete rewrite in Rust of the <a href="https://gitlab.com/W1nst0n/universal-android-debloater" rel="nofollow">UAD project</a>,
which aims to improve privacy and battery performance by removing unnecessary
and obscure system apps.
This can also contribute to improve security by reducing <a href="https://en.wikipedia.org/wiki/Attack_surface" rel="nofollow">the attack surface</a>.</p>
<p dir="auto">Packages are as well documented as possible in order to provide a better
understanding of what you can delete or not. The worst issue that could happen
is removing an essential system package needed during boot causing then an unfortunate
bootloop. After about 5 failed system boots, the phone will automatically reboot
in recovery mode, and you'll have to perform a FACTORY RESET. Make a backup first!</p>
<p dir="auto">In any case, you <strong>CANNOT</strong> brick your device with this software!
That's the main point, right?</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul>
<li> Uninstall/Disable and Restore/Enable system packages</li>
<li> Multi-user support (e.g. apps in work profiles)</li>
<li> Export/Import your selection in <code>uad_exported_selection.txt</code></li>
<li> Multi-device support: you can connect multiple phones at the same time</li>
<li> All your actions are logged, so you never forget what you've done</li>
</ul>
<p dir="auto">NB : System apps cannot truly be uninstalled without root (see the <a href="https://github.com/0x192/universal-android-debloater/wiki/FAQ">FAQ</a>)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Universal Debloat Lists</h2><a id="user-content-universal-debloat-lists" aria-label="Permalink: Universal Debloat Lists" href="#universal-debloat-lists"></a></p>
<ul>
<li> GFAM (Google/Facebook/Amazon/Microsoft)</li>
<li> AOSP</li>
<li> Manufacturers (OEM)</li>
<li> Mobile carriers</li>
<li> Qualcomm / Mediatek / Miscellaneous</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Manufacturers debloat lists</h2><a id="user-content-manufacturers-debloat-lists" aria-label="Permalink: Manufacturers debloat lists" href="#manufacturers-debloat-lists"></a></p>
<ul>
<li> Archos</li>
<li> Asus</li>
<li> Blackberry</li>
<li> Gionee</li>
<li> LG</li>
<li> Google</li>
<li> iQOO</li>
<li> Fairphone</li>
<li> HTC</li>
<li> Huawei</li>
<li> Motorola</li>
<li> Nokia</li>
<li> OnePlus</li>
<li> Oppo</li>
<li> Realme</li>
<li> Samsung</li>
<li> Sony</li>
<li> Tecno</li>
<li> TCL</li>
<li> Unihertz</li>
<li> Vivo/iQOO</li>
<li> Wiko</li>
<li> Xiaomi</li>
<li> ZTE</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Mobile carriers debloat lists</h2><a id="user-content-mobile-carriers-debloat-lists" aria-label="Permalink: Mobile carriers debloat lists" href="#mobile-carriers-debloat-lists"></a></p>
<table>
<thead>
<tr>
<th>Country</th>
<th>Carriers</th>
</tr>
</thead>
<tbody>
<tr>
<td>France</td>
<td>Orange, SFR, Free, Bouygues</td>
</tr>
<tr>
<td>USA</td>
<td>T-Mobile, Verizon, Sprint, AT&amp;T</td>
</tr>
<tr>
<td>Germany</td>
<td>Telekom</td>
</tr>
<tr>
<td>UK</td>
<td>EE</td>
</tr>
</tbody>
</table>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to use it</h2><a id="user-content-how-to-use-it" aria-label="Permalink: How to use it" href="#how-to-use-it"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Read the <a href="https://github.com/0x192/universal-android-debloater/wiki/FAQ">FAQ</a>!</strong></p>
</li>
<li>
<p dir="auto"><strong>Do a proper backup of your data! You can never be too careful!</strong></p>
</li>
<li>
<p dir="auto">Enable <em>Developer Options</em> on your smartphone.</p>
</li>
<li>
<p dir="auto">Turn on <em>USB Debugging</em> from the developer panel.</p>
</li>
<li>
<p dir="auto">From the settings, disconnect from any OEM accounts (when you delete an OEM
account package it could lock you on the lockscreen because the phone can't
associate your identity anymore)</p>
</li>
<li>
<p dir="auto">Install ADB (see the intructions by clicking on your OS below):</p>
<details>
<summary>LINUX</summary>
<ul dir="auto">
<li>Install <em>Android platform tools</em> on your PC :</li>
</ul>
<p dir="auto">Debian Base:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt install android-sdk-platform-tools"><pre>sudo apt install android-sdk-platform-tools</pre></div>
<p dir="auto">Arch-Linux Base:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo pacman -S android-tools"><pre>sudo pacman -S android-tools</pre></div>
<p dir="auto">Red Hat Base:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo yum install android-tools"><pre>sudo yum install android-tools</pre></div>
<p dir="auto">OpenSUSE Base:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo zypper install android-tools"><pre>sudo zypper install android-tools</pre></div>
</details>

<details>
<summary>MAC OS</summary>
<ul dir="auto">
<li>
<p dir="auto">Install <a href="https://brew.sh/" rel="nofollow">Homebrew</a></p>
</li>
<li>
<p dir="auto">Install <em>Android platform tools</em></p>
<div dir="auto" data-snippet-clipboard-copy-content="brew install android-platform-tools"><pre>brew install android-platform-tools</pre></div>
</li></ul></details>

</li>
</ul>
<details>
<summary>WINDOWS</summary>
<ul dir="auto">
<li>
<p dir="auto">Download <a href="https://dl.google.com/android/repository/platform-tools-latest-windows.zip" rel="nofollow">android platform tools</a>
and unzip it somewhere.</p>
</li>
<li>
<p dir="auto"><a href="https://www.architectryan.com/2018/03/17/add-to-the-path-on-windows-10/" rel="nofollow">Add the android platform tools to your PATH</a>
<strong>OR</strong> make sure to launch UAD from the same directory.</p>
</li>
<li>
<p dir="auto"><a href="https://developer.android.com/studio/run/oem-usb#Drivers" rel="nofollow">Install USB drivers for your device</a></p>
</li>
<li>
<p dir="auto">Check your device is detected:</p>

</li></ul></details>




<li>
<p dir="auto">Download the latest release of UAD GUI for your Operating System <a href="https://github.com/0x192/universal-android-debloater/releases">here</a>.
Take the <code>opengl</code> version only if the default version (with a Vulkan backend)
doesn't launch.</p>
</li>

<p dir="auto"><strong>NOTE:</strong> Chinese phones users may need to use the AOSP list for removing some stock
apps because those Chinese manufacturers (especially Xiaomi and Huawei) have been
using the name of AOSP packages for their own (modified &amp; closed-source) apps.</p>
<p dir="auto"><strong>IMPORTANT NOTE:</strong> You will have to run this software whenever your OEM pushes
an update to your phone as some <em>uninstalled</em> system apps could be reinstalled.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to contribute</h2><a id="user-content-how-to-contribute" aria-label="Permalink: How to contribute" href="#how-to-contribute"></a></p>
<p dir="auto">Hey-hey-hey! Don't go away so fast! This is a community project.
That means I need you! I'm sure you want to make this project better anyway.</p>
<p dir="auto">==&gt; <a href="https://github.com/0x192/universal-android-debloater/wiki">How to contribute</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Special thanks</h2><a id="user-content-special-thanks" aria-label="Permalink: Special thanks" href="#special-thanks"></a></p>
<ul dir="auto">
<li><a href="https://github.com/mawilms">@mawilms</a> for his LotRO plugin manager (<a href="https://github.com/mawilms/lembas">Lembas</a>)
which helped me a lot to understand how to use the <a href="https://github.com/hecrj/iced">Iced</a>
GUI library.</li>
<li><a href="https://github.com/casperstorm">@casperstorm</a> for the UI/UX inspiration.</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How web bloat impacts users with slow devices (608 pts)]]></title>
            <link>https://danluu.com/slow-device/</link>
            <guid>39729057</guid>
            <pubDate>Sat, 16 Mar 2024 20:08:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://danluu.com/slow-device/">https://danluu.com/slow-device/</a>, See on <a href="https://news.ycombinator.com/item?id=39729057">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <meta property="og:image" content="/slow-device-performance.png"> <p>In 2017, <a href="https://danluu.com/web-bloat/">we looked at how web bloat affects users with slow connections</a>. Even in the U.S., <a href="https://twitter.com/danluu/status/1116565029791260672">many users didn't have broadband speeds</a>, making much of the web difficult to use. It's still the case that many users don't have broadband speeds, both inside and outside of the U.S. and that much of the modern web isn't usable for people with slow internet, but the exponential increase in bandwidth (Neilsen suggests <abbr title="Unfortunately, I don't know of a pubic source for low-end data, say 10%-ile or 1%-ile; let me know if you have numbers on this">this is 50% per year for high-end connections</abbr>) has outpaced web bloat for typical sites, making this less of a problem than it was in 2017, although it's still a serious problem for people with poor connections.</p> <p>CPU performance for web apps hasn't scaled nearly as quickly as bandwidth so, while more of the web is becoming accessible to people with low-end connections, more of the web is becoming inaccessible to people with low-end devices even if they have high-end connections. For example, if I try browsing a "modern" Discourse-powered forum on a <code>Tecno Spark 8C</code>, it sometimes crashes the browser. Between crashes, on measuring the performance, the responsiveness is significantly worse than browsing a BBS with an <code>8 MHz 286</code> and a <code>1200 baud</code> modem. On my <code>1Gbps</code> home internet connection, the <code>2.6 MB</code> compressed payload size "necessary" to load message titles is relatively light. The over-the-wire payload size has "only" increased by <code>1000x</code>, which is dwarfed by the increase in internet speeds. But the opposite is true when it comes to CPU speeds — for web browsing and forum loading performance, the <code>8-core (2 1.6 GHz Cortex-A75 / 6 1.6 GHz Cortex-A55)</code> CPU can't handle Discourse. The CPU is something like <code>100000x</code> faster than our <code>286</code>. Perhaps a <code>1000000x</code> faster device would be sufficient.</p> <p>For anyone not familiar with the <code>Tecno Spark 8C</code>, today, a new <code>Tecno Spark 8C</code>, a quick search indicates that one can be hand for <code>USD 50-60</code> in Nigeria and perhaps <code>USD 100-110</code> in India. As <abbr title="The estimates for Nigerian median income that I looked at seem good enough, but the Indian estimate I found was a bit iffier; if you have a good source for Indian income distribution, please pass it along.">a fraction of median household income, that's substantially more than a current generation iPhone in the U.S. today.</abbr></p> <p>By worldwide standards, the <code>Tecno Spark 8C</code> isn't even close to being a low-end device, so we'll also look at performance on an <code>Itel P32</code>, which is a lower end device (though still far from the lowest-end device people are using today). Additionally, we'll look at performance with an <code>M3 Max Macbook (14-core)</code>, an <code>M1 Pro Macbook (8-core)</code>, and the <code>M3 Max</code> set to <code>10x</code> throttling in Chrome dev tools. In order to give these devices every advantage, we'll be on fairly high-speed internet (1Gbps, with a WiFi router that's benchmarked as having lower latency under load than most of its peers). We'll look at some blogging platforms and micro-blogging platforms (this blog, Substack, Medium, Ghost, Hugo, Tumblr, Mastodon, Twitter, Threads, Bluesky, Patreon), forum platforms (Discourse, Reddit, Quora, vBulletin, XenForo, phpBB, and myBB), and platforms commonly used by small businesses (Wix, Squarespace, Shopify, and WordPress again).</p> <p>In the table below, every row represents a website and every non-label column is a metric. After the website name column, we have the compressed size transferred over the wire (<code>wire</code>) and the raw, uncompressed, size (<code>raw</code>). Then we have, for each device, Largest Contentful Paint* (<code>LCP*</code>) and CPU usage on the main thread (<code>CPU</code>). Google's docs explain <code>LCP</code> as</p> <blockquote> <p>Largest Contentful Paint (LCP) measures when a user perceives that the largest content of a page is visible. The metric value for LCP represents the time duration between the user initiating the page load and the page rendering its primary content</p> </blockquote> <p><code>LCP</code> is a common optimization target because it's presented as one of the primary metrics in Google PageSpeed Insights, a "Core Web Vital" metric. There's an asterisk next to <code>LCP</code> as used in this document because, <code>LCP</code> as measured by Chrome is about painting a large fraction of the screen, as opposed to the definition above, which is about content. As sites have optimized for <code>LCP</code>, it's not uncommon to have a large paint (update) that's completely useless to the user, with the actual content of the page appearing well after the <code>LCP</code>. In cases where that happens, I've used the timestamp when useful content appears, not the <code>LCP</code> as defined by when a large but useless update occurs. The full details of the tests and why these metrics were chosen are discussed in an appendix.</p> <p>Although CPU time isn't a "Core Web Vital", it's presented here because it's a simple metric that's highly correlated with my and other users' perception of usability on slow devices. See appendix for more detailed discussion on this. One reason CPU time works as a metric is that, if a page has great numbers for all other metrics but uses a ton of CPU time, the page is not going to be usable on a slow device. If it takes 100% CPU for 30 seconds, the page will be completely unusable for 30 seconds, and if it takes 50% CPU for 60 seconds, the page will be barely usable for 60 seconds, etc. Another reason it works is that, relative to commonly used metrics, it's hard to cheat on CPU time and make optimizations that significantly move the number without impacting user experience.</p> <p>The color scheme in the table below is that, for sizes, more green = smaller / fast and more red = larger / slower. Extreme values are in black.</p>  <table> <tbody><tr> <th rowspan="2">Site</th><th colspan="2">Size</th><th colspan="2">M3 Max</th><th colspan="2">M1 Pro</th><th colspan="2">M3/10</th><th colspan="2">Tecno S8C</th><th colspan="2">Itel P32</th></tr> <tr> <th>wire</th><th>raw</th><th>LCP*</th><th>CPU</th><th>LCP*</th><th>CPU</th><th>LCP*</th><th>CPU</th><th>LCP*</th><th>CPU</th><th>LCP*</th><th>CPU</th></tr> <tr> <td>danluu.com</td><td><span color="white">6kB</span></td><td><span color="white">18kB</span></td><td><span color="white">50ms</span></td><td><span color="white">20ms</span></td><td><span color="white">50ms</span></td><td><span color="white">30ms</span></td><td><span color="white">0.2s</span></td><td><span color="white">0.3s</span></td><td>0.4s</td><td><span color="white">0.3s</span></td><td>0.5s</td><td>0.5s</td></tr> <tr> <td>HN</td><td><span color="white">11kB</span></td><td><span color="white">50kB</span></td><td><span color="white">0.1s</span></td><td><span color="white">30ms</span></td><td><span color="white">0.1s</span></td><td><span color="white">30ms</span></td><td><span color="white">0.3s</span></td><td><span color="white">0.3s</span></td><td>0.5s</td><td>0.5s</td><td>0.7s</td><td>0.6s</td></tr> <tr> <td>MyBB</td><td><span color="white">0.1MB</span></td><td><span color="white">0.3MB</span></td><td><span color="white">0.3s</span></td><td><span color="white">0.1s</span></td><td><span color="white">0.3s</span></td><td><span color="white">0.1s</span></td><td>0.6s</td><td>0.6s</td><td>0.8s</td><td>0.8s</td><td>2.1s</td><td>1.9s</td></tr> <tr> <td>phpBB</td><td>0.4MB</td><td>0.9MB</td><td><span color="white">0.3s</span></td><td><span color="white">0.1s</span></td><td>0.4s</td><td><span color="white">0.1s</span></td><td>0.7s</td><td>1.1s</td><td>1.7s</td><td>1.5s</td><td>4.1s</td><td>3.9s</td></tr> <tr> <td>WordPress</td><td>1.4MB</td><td>1.7MB</td><td><span color="white">0.2s</span></td><td><span color="white">60ms</span></td><td><span color="white">0.2s</span></td><td><span color="white">80ms</span></td><td>0.7s</td><td>0.7s</td><td>1s</td><td>1.5s</td><td>1.2s</td><td>2.5s</td></tr> <tr> <td>WordPress (old)</td><td>0.3MB</td><td>1.0MB</td><td><span color="white">80ms</span></td><td><span color="white">70ms</span></td><td><span color="white">90ms</span></td><td><span color="white">90ms</span></td><td>0.4s</td><td>0.9s</td><td>0.7s</td><td>1.7s</td><td>1.1s</td><td>1.9s</td></tr> <tr> <td>XenForo</td><td>0.3MB</td><td>1.0MB</td><td>0.4s</td><td><span color="white">0.1s</span></td><td>0.6s</td><td><span color="white">0.2s</span></td><td>1.4s</td><td>1.5s</td><td>1.5s</td><td>1.8s</td><td><span color="white">FAIL</span></td><td><span color="white">FAIL</span></td></tr> <tr> <td>Ghost</td><td>0.7MB</td><td>2.4MB</td><td><span color="white">0.1s</span></td><td><span color="white">0.2s</span></td><td><span color="white">0.2s</span></td><td><span color="white">0.2s</span></td><td>1.1s</td><td>2.2s</td><td>1s</td><td>2.4s</td><td>1.1s</td><td>3.5s</td></tr> <tr> <td>vBulletin</td><td>1.2MB</td><td>3.4MB</td><td>0.5s</td><td><span color="white">0.2s</span></td><td>0.6s</td><td><span color="white">0.3s</span></td><td>1.1s</td><td>2.9s</td><td>4.4s</td><td>4.8s</td><td>13s</td><td>16s</td></tr> <tr> <td>Squarespace</td><td>1.9MB</td><td>7.1MB</td><td><span color="white">0.1s</span></td><td>0.4s</td><td><span color="white">0.2s</span></td><td>0.4s</td><td>0.7s</td><td>3.6s</td><td>14s</td><td>5.1s</td><td>16s</td><td>19s</td></tr> <tr> <td>Mastodon</td><td>3.8MB</td><td>5.3MB</td><td><span color="white">0.2s</span></td><td><span color="white">0.3s</span></td><td><span color="white">0.2s</span></td><td>0.4s</td><td>1.8s</td><td>4.7s</td><td>2.0s</td><td>7.6s</td><td><span color="white">FAIL</span></td><td><span color="white">FAIL</span></td></tr> <tr> <td>Tumblr</td><td>3.5MB</td><td>7.1MB</td><td>0.7s</td><td>0.6s</td><td>1.1s</td><td>0.7s</td><td>1.0s</td><td>7.0s</td><td>14s</td><td>7.9s</td><td>8.7s</td><td>8.7s</td></tr> <tr> <td>Quora</td><td>0.6MB</td><td>4.9MB</td><td>0.7s</td><td>1.2s</td><td>0.8s</td><td>1.3s</td><td>2.6s</td><td>8.7s</td><td><span color="white">FAIL</span></td><td><span color="white">FAIL</span></td><td>19s</td><td><span color="white">29s</span></td></tr> <tr> <td>Bluesky</td><td>4.8MB</td><td>10MB</td><td>1.0s</td><td>0.4s</td><td>1.0s</td><td>0.5s</td><td>5.1s</td><td>6.0s</td><td>8.1s</td><td>8.3s</td><td><span color="white">FAIL</span></td><td><span color="white">FAIL</span></td></tr> <tr> <td>Wix</td><td>7.0MB</td><td>21MB</td><td>2.4s</td><td>1.1s</td><td>2.5s</td><td>1.2s</td><td>18s</td><td>11s</td><td>5.6s</td><td>10s</td><td><span color="white">FAIL</span></td><td><span color="white">FAIL</span></td></tr> <tr> <td>Substack</td><td>1.3MB</td><td>4.3MB</td><td>0.4s</td><td>0.5s</td><td>0.4s</td><td>0.5s</td><td>1.5s</td><td>4.9s</td><td>14s</td><td>14s</td><td><span color="white">FAIL</span></td><td><span color="white">FAIL</span></td></tr> <tr> <td>Threads</td><td>9.3MB</td><td>13MB</td><td>1.5s</td><td>0.5s</td><td>1.6s</td><td>0.7s</td><td>5.1s</td><td>6.1s</td><td>6.4s</td><td>16s</td><td><span color="white">28s</span></td><td><span color="white">66s</span></td></tr> <tr> <td>Twitter</td><td>4.7MB</td><td>11MB</td><td>2.6s</td><td>0.9s</td><td>2.7s</td><td>1.1s</td><td>5.6s</td><td>6.6s</td><td>12s</td><td>19s</td><td>24s</td><td><span color="white">43s</span></td></tr> <tr> <td>Shopify</td><td>3.0MB</td><td>5.5MB</td><td>0.4s</td><td><span color="white">0.2s</span></td><td>0.4s</td><td><span color="white">0.3s</span></td><td>0.7s</td><td>2.3s</td><td>10s</td><td><span color="white">26s</span></td><td><span color="white">FAIL</span></td><td><span color="white">FAIL</span></td></tr> <tr> <td>Discourse</td><td>2.6MB</td><td>10MB</td><td>1.1s</td><td>0.5s</td><td>1.5s</td><td>0.6s</td><td>6.5s</td><td>5.9s</td><td>15s</td><td><span color="white">26s</span></td><td><span color="white">FAIL</span></td><td><span color="white">FAIL</span></td></tr> <tr> <td>Patreon</td><td>4.0MB</td><td>13MB</td><td>0.6s</td><td>1.0s</td><td>1.2s</td><td>1.2s</td><td>1.2s</td><td>14s</td><td>1.7s</td><td><span color="white">31s</span></td><td>9.1s</td><td><span color="white">45s</span></td></tr> <tr> <td>Medium</td><td>1.2MB</td><td>3.3MB</td><td>1.4s</td><td>0.7s</td><td>1.4s</td><td>1s</td><td>2s</td><td>11s</td><td>2.8s</td><td><span color="white">33s</span></td><td>3.2s</td><td><span color="white">63s</span></td></tr> <tr> <td>Reddit</td><td>1.7MB</td><td>5.4MB</td><td>0.9s</td><td>0.7s</td><td>0.9s</td><td>0.9s</td><td>6.2s</td><td>12s</td><td>1.2s</td><td><span color="white">∞</span></td><td><span color="white">FAIL</span></td><td><span color="white">FAIL</span></td></tr> </tbody></table> <p>At a first glance, the table seems about in that the sites that feel slow unless you have a super fast device show up as slow in the table (as in, <code>max(LCP*,CPU))</code> is high on lower-end devices). When I polled folks about what platforms they thought would be fastest and slowest on our slow devices (<a href="https://mastodon.social/@danluu/111994437263038931">Mastodon</a>, <a href="https://twitter.com/danluu/status/1761875263359537652">Twitter</a>, <a href="https://www.threads.net/@danluu.danluu/post/C3yVpfKS-RP">Threads</a>), they generally correctly predicted that Wordpress and Ghost and Wordpress would be faster than Substack and Medium, and that Discourse would be much slower than old PHP forums like phpBB, XenForo, and vBulletin. I also pulled Google PageSpeed Insights (PSI) scores for pages (not shown) and the correlation isn't as strong with those numbers <abbr title="For the 'real world' numbers, this is also because users with slow devices can't really use some of these sites, so their devices aren't counted in the distribution and PSI doesn't normalize for this.">because</abbr> a handful of sites have managed to optimize their PSI scores without actually speeding up their pages for users.</p> <p>If you've never used a low-end device like this, the general experience is that many sites are unusable on the device and loading anything resource intensive (an app or a huge website) can cause crashes. Doing something too intense in a resource intensive app can also cause crashes. While <a href="https://www.youtube.com/watch?v=U1JMRFQWK70">reviews note</a> that <a href="https://www.youtube.com/watch?v=McawfNlydqk">you can run PUBG and other 3D games with decent performance</a> on a <code>Tecno Spark 8C</code>, this doesn't mean that the device is fast enough to read posts on modern text-centric social media platforms or modern text-centric web forums. While <code>40fps</code> is achievable in PUBG, we can easily see less than <code>0.4fps</code> when scrolling on these sites.</p> <p>We can see from the table how many of the sites are unusable if you have a slow device. All of the pages with <code>10s+ CPU</code> are a fairly bad experience even after the page loads. Scrolling is very jerky, frequently dropping to a few frames per second and sometimes well below. When we tap on any link, the delay is so long that we can't be sure if our tap actually worked. If we tap again, we can get the dreaded situation where the first tap registers and then causes the second tap to register after things have started changing, causing us to tap some random target, but if we wait, we realize that the original tap didn't actually register (or it registered, but not where we thought it did). Although MyBB doesn't service up a mobile site and is penalized by Google for not having a mobile friendly page, it's actually much more usable on these slow mobiles than all but the fastest sites because scrolling and tapping actually work.</p> <p>Another thing we can see is how much variance there is in the relative performance on different devices. For example, comparing an <code>M3/10</code> and a <code>Tecno Spark 8C</code>, for danluu.com and Ghost, an <code>M3/10</code> gives a halfway decent approximation of the <code>Tecno Spark 8C</code> (although danluu.com loads much too quickly), but the <code>Tecno Spark 8C</code> is about three times slower (<code>CPU</code>) for Medium, Substack, and Twitter, roughly four times slower for Reddit and Discourse, and over an order of magnitude faster for Shopify. For Wix, the <code>CPU</code> approximation is about accurate, but our `<code>Tecno Spark 8C</code> is more than 3 times slower on <code>LCP*</code>. It's great that Chrome lets you conveniently simulate a slower device from the convenience of your computer, but just enabling Chrome's CPU throttling (or using any combination of out-of-the-box options that are available) gives fairly different results than we get on many real devices. The full reasons for this are beyond the scope of the post; for the purposes of this post, it's sufficient to note that slow pages are often super-linearly slow as devices get slower and that slowness on one page doesn't strongly predict slowness on another page.</p> <p>If take a site-centric view instead of a device-centric view, another way to look at it is that sites like Discourse, Medium, and Reddit, don't use all that much CPU on our fast <code>M3</code> and <code>M1</code> computers, but they're among the slowest on our <code>Tecno Spark 8C</code> (Reddit's CPU is shown as <code>∞</code> because, no matter how long we wait with no interaction, Reddit uses <code>~90% CPU</code>). Discourse also sometimes crashed the browser after interacting a bit or just waiting a while. For example, one time, the browser crashed after loading Discourse, scrolling twice, and then leaving the device still for a minute or two. For consistency's sake, this wasn't marked as <code>FAIL</code> in the table since the page did load but, realistically, having a page is so resource intensive that the browser crashes is a significantly worse user experience than any of the <code>FAIL</code> cases in the table. When we looked at how <a href="https://danluu.com/web-bloat/">web bloat impacts users with slow connections</a>, we found that <abbr title="One thing to keep in mind here is that having a slow device and a slow connection have multiplicative impacts.">much of the web was unusable for people with slow connections and slow devices are no different</abbr>.</p> <p>Another pattern we can see is how the older sites are, in general, faster than the newer ones, with sites that (visually) look like they haven't been updated in a decade or two tending to be among the fastest. For example, MyBB, the least modernized and oldest looking forum is <code>3.6x / 5x faster (LCP* / CPU)</code> than Discourse on the <code>M3</code>, but on the <code>Tecno Spark 8C</code>, the difference is <code>19x / 33x</code> and, given the overall scaling, it seems safe to guess that the difference would be even larger on the Itel P32 if Discourse worked on such a cheap device.</p> <p>Another example is Wordpress (old) vs. newer, trendier, blogging platforms like Medium and Substack. Wordpress (old) is is <code>17.5x / 10x faster (LCP* / CPU)</code> than Medium and <code>5x / 7x faster (LCP* / CPU)</code> faster than Substack on our <code>M3 Max</code>, and <code>4x / 19x</code> and <code>20x / 8x</code> faster, respectively, on our <code>Tecno Spark 8C</code>. Ghost is a notable exception to this, being a modern platform (launched a year after Medium) that's competitive with older platforms (modern Wordpress is also arguably an exception, but many folks would probably still consider that to be an old platform).</p> <p>Sites that use modern techniques like partially loading the page and then dynamically loading the rest of it, such as Discourse, Reddit, and Substack, tend to be less usable than the scores in the table indicate. Although, in principle, you could build such a site in a simple way that works well with cheap devices but, in practice sites that use dynamic loading tend to be complex enough that the sites are extremely janky on low-end devices. It's generally difficult or impossible to scroll a predictable distance, which means that users will sometimes accidentally trigger more loading by scrolling too far, causing the page to lock up. Many pages actually remove the parts of the page you scrolled past as you scroll; all such pages are essentially unusable. Other basic web features, like page search, also generally stop working. Pages with this kind of dynamic loading can't rely on the simple and fast ctrl/command+F search and have to build their own search. How well this works varies (this used to work quite well in Google docs, but for the past few months or maybe a year, it takes so long to load that I have to deliberately wait after opening a doc to avoid triggering the browser's useless built in search; Discourse search has never really worked on slow devices or even not very fast but not particular slow devices).</p> <p>In principle, these modern pages that burn a ton of CPU when loading could be doing pre-work that means that later interactions on the page are faster and cheaper than on the pages that do less up-front work (this is a common argument in favor of these kinds of pages), but that's not the case for pages tested, which are slower to load initially, slower on subsequent loads, and slower after they've loaded.</p> <p>To understand why the theoretical idea that doing all this work up-front doesn't generally result in a faster experience later, this exchange between a distinguished engineer at Google and one of the founders of Discourse (and CEO at the time) is <abbr title="the founder has made similar comments elsewhere as well, so this isn't a one-off analogy for him, nor do I find it to be an unusual line of thinking in general">illustrative</abbr>, in <a href="https://danluu.com/jeff-atwood-trashes-qualcomm-engineering.png">a discussion where the founder of Discourse says that you should test mobile sites on laptops with throttled bandwidth but not throttled CPU</a>:</p> <ul> <li><b>Google</b>: *you* also don't have slow 3G. These two settings go together. Empathy needs to extend beyond iPhone XS users in a tunnel.</li> <li><b>Discourse</b>: Literally any phone of vintage iPhone 6 or greater is basically as fast as the "average" laptop. You have to understand how brutally bad Qualcomm is at their job. Look it up if you don't believe me.</li> <li><b>Google</b>: I don't need to believe you. I know. This is well known by people who care. My point was that just like not everyone has a fast connection not everyone has a fast phone. Certainly the iPhone 6 is frequently very CPU bound on real world websites. But that isn't the point.</li> <li><b>Discourse</b>: we've been trending towards infinite CPU speed for decades now (and we've been asymptotically there for ~5 years on desktop), what we are not and will never trend towards is infinite bandwidth. Optimize for the things that matter. and I have zero empathy for @qualcomm. Fuck Qualcomm, they're terrible at their jobs. I hope they go out of business and the ground their company existed on is plowed with salt so nothing can ever grow there again.</li> <li><b>Google</b>: Mobile devices are not at all bandwidth constraint in most circumstances. They are latency constraint. Even the latest iPhone is CPU constraint before it is bandwidth constraint. If you do well on 4x slow down on a MBP things are pretty alright</li> <li>...</li> <li><b>Google</b>: Are 100% of users on iOS?</li> <li><b>Discourse</b>: The influential users who spend money tend to be, I’ll tell you that ... Pointless to worry about cpu, it is effectively infinite already on iOS, and even with Qualcomm’s incompetence, will be within 4 more years on their embarrassing SoCs as well</li> </ul> <p>When someone asks the founder of Discourse, "just wondering why you hate them", he responds with a link that cites the Kraken and Octane benchmarks from <a href="https://www.anandtech.com/show/9146/the-samsung-galaxy-s6-and-s6-edge-review/5">this Anandtech review</a>, which have the Qualcomm chip at 74% and 85% of the performance of the then-current Apple chip, respectively.</p> <p>The founder and then-CEO of Discourse considers Qualcomm's mobile performance embarrassing and finds this so offensive that he thinks Qualcomm engineers should all lose their jobs for delivering <abbr title="I think it could be reasonable to cite a lower number, but I'm using the number he cited, not what I would cite">74% to 85% of the performance of Apple</abbr>. Apple has what I consider to be an all-time great performance team. Reasonable people could disagree on that, but one has to at least think of them as a world-class team. So, producing a product with <abbr title="recall that, on a Tecno Spark 8, Discourse is 33 times slower than MyBB, which isn't particularly optimized for performance">74% to 85% of an all-time-great team is considered an embarrassment worthy of losing your job</abbr>.</p> <p>There are two attitudes on display here which I see in a lot of software folks. First, that CPU speed is infinite and one shouldn't worry about CPU optimization. And second, that gigantic speedups from hardware should be expected and the only reason hardware engineers wouldn't achieve them is due to spectacular incompetence, so the slow software should be blamed on hardware engineers, not software engineers. Donald Knuth expressed a similar sentiment in</p> <blockquote> <p>I might as well flame a bit about my personal unhappiness with the current trend toward multicore architecture. To me, it looks more or less like the hardware designers have run out of ideas, and that they’re trying to pass the blame for the future demise of Moore’s Law to the software writers by giving us machines that work faster only on a few key benchmarks! I won’t be surprised at all if the whole multiithreading idea turns out to be a flop, worse than the "Itanium" approach that was supposed to be so terrific—until it turned out that the wished-for compilers were basically impossible to write. Let me put it this way: During the past 50 years, I’ve written well over a thousand programs, many of which have substantial size. I can’t think of even five of those programs that would have been enhanced noticeably by parallelism or multithreading. Surely, for example, multiple processors are no help to TeX ... I know that important applications for parallelism exist—rendering graphics, breaking codes, scanning images, simulating physical and biological processes, etc. But all these applications require dedicated code and special-purpose techniques, which will need to be changed substantially every few years. Even if I knew enough about such methods to write about them in TAOCP, my time would be largely wasted, because soon there would be little reason for anybody to read those parts ... The machine I use today has dual processors. I get to use them both only when I’m running two independent jobs at the same time; that’s nice, but it happens only a few minutes every week.</p> </blockquote> <p>In the case of Discourse, a hardware engineer is an embarrassment not deserving of a job if they can't hit 90% of the performance of an all-time-great performance team but, as a software engineer, delivering 3% the performance of a non-highly-optimized application like MyBB is no problem. In Knuth's case, hardware engineers gave programmers a 100x performance increase every decade for decades with little to no work on the part of programmers. The moment this slowed down and programmers had to adapt to take advantage of new hardware, hardware engineers were "all out of ideas", but learning a few "new" (1970s and 1980s era) ideas to take advantage of current hardware would be a waste of time. And <a href="https://www.patreon.com/posts/54329188">we've previously discussed Alan Kay's claim that hardware engineers are "unsophisticated" and "uneducated" and aren't doing "real engineering" and how we'd get a 1000x speedup if we listened to Alan Kay's "sophisticated" ideas</a>.</p> <p>It's fairly common for programmers to expect that hardware will solve all their problems, and then, when that doesn't happen, pass the issue onto the user, explaining why the programmer need't do anything to help the user. A question one might ask is how much performance improvement programmers have given us. There are cases of algorithmic improvements that result in massive speedups but, as we noted above, Discourse, the fastest growing forum software today, seems to have given us an approximately <code>1000000x</code> slowdown in performance.</p> <p>Another common attitude on display above is the idea that users who aren't wealthy don't matter. When asked if 100% of users are on iOS, the founder of Discourse says "The influential users who spend money tend to be, I’ll tell you that". We see the same attitude all over comments on <a href="https://tonsky.me/blog/js-bloat/">Tonsky's JavaScript Bloat post</a>, with people expressing <a href="https://danluu.com/cocktail-ideas/">cocktail-party sentiments</a> like "Phone apps are hundreds of megs, why are we obsessing over web apps that are a few megs? Starving children in Africa can download Android apps but not web apps? Come on and "surely no user of gitlab would be poor enough to have a slow device, let's be serious" (paraphrased for length).</p> <p>But when we look at the size of apps that are downloaded in Africa, we see that people who aren't on high-end devices use apps like Facebook Lite (a couple megs) and commonly use apps that are a single digit to low double digit number of megabytes. There are multiple reasons app makers care about their app size. One is just the total storage available on the phone; if you watch real users install apps, they often have to delete and uninstall things to put a new app on, so the smaller size is both easier to to install and has a lower chance of being uninstalled when the user is looking for more space. Another is that, if you look at data on app size and usage (I don't know of any public data on this; please pass it along if you have something public I can reference), when large apps increase the size and memory usage, they get more crashes, which drives down user retention, growth, and engagement and, conversely, when they optimize their size and memory usage, they get fewer crashes and better user retention, growth, and engagement.</p> <p>On the bit about no programmers having slow devices, I know plenty of people who are using hand-me-down devices that are old and slow. Many of them aren't even really poor; they just don't see why (for example) their kid needs a super fast device, and they don't understand how much of the modern web works poorly on slow devices. After all, the "slow" device can play 3d games and (with the right OS) compile codebases like Linux or Chromium, so why shouldn't the device be able to interact with a site like gitlab?</p> <p>Contrary to the claim from the founder of Discourse that, within years, every Android user will be on some kind of super fast Android device, it's been six years since his comment and it's going to be at least a decade before almost everyone in the world who's using a phone has a high-speed device and this could easily take two decades or more. If you look up marketshare stats for Discourse, it's extremely successful; it appears to be the fastest growing forum software in the world by a large margin. The impact of having the fastest growing forum software in the world created by an organization whose then-leader was willing to state that he doesn't really care about users who aren't "influential users who spend money", who don't have access to "infinite CPU speed", is that a lot of forums are now inaccessible to people who don't have enough wealth to buy a device with effectively infinite CPU.</p> <p>If the founder of Discourse were an anomaly, this wouldn't be too much of a problem, but he's just verbalizing the implicit assumptions a lot of programmers have, which is why we see that so many modern websites are unusable if you buy the income-adjusted equivalent of a new, current generation, iPhone in a low-income country.</p> <p><i>Thanks to Yossi Kreinen, Fabian Giesen, John O'Nolan, Joseph Scott, @acidshill, Alex Russell, Tobias Marschner, and David Turner for comments/corrections/discussion.</i></p> <h3 id="appendix-gaming-lcp">Appendix: gaming LCP</h3> <p>We noted above that we used <code>LCP*</code> and not <code>LCP</code>. This is because <code>LCP</code> basically measures when the largest change happens. When this metric was not deliberately gamed in ways that don't benefit the user, this was a great metric, but this metric has become less representative of the actual user experience as more people have gamed it. In the less blatant cases, people do small optimizations that improve <code>LCP</code> but barely improve or don't improve the actual user experience.</p> <p>In the more blatant cases, developers will deliberately flash a very large change on the page as soon as possible, generally a loading screen that has no value to the user (actually negative value because doing this increases the total amount of work done and the total time it takes to load the page) and then they carefully avoid making any change large enough that any later change would get marked as the <code>LCP</code>.</p> <p>For the same reason <a href="https://en.wikipedia.org/wiki/Volkswagen_emissions_scandal">that VW didn't publicly discuss how it was gaming its emissions numbers</a>, developers tend to shy away from discussing this kind of <code>LCP</code> optimization in public. An exception to this is Discourse, where <a href="https://meta.discourse.org/t/introducing-discourse-splash-a-visual-preloader-displayed-while-site-assets-load/232003" rel="nofollow">they publicly announced this kind of <code>LCP</code> optimization, with comments from their devs and the then-CTO (now CEO)</a>, noting that their new "Discourse Splash" feature hugely reduced <code>LCP</code> for sites after they deployed it. And then developers ask why their <code>LCP</code> is high, the standard advice from Discourse developers is to keep elements smaller than the "Discourse Splash", so that the <code>LCP</code> timestamp is computed from this useless element that's thrown up to optimize <code>LCP</code>, as opposed to having the timestamp be computed from any actual element that's relevant to the user. <a href="https://meta.discourse.org/t/theme-components-and-largest-contentful-paint-lcp/258680" rel="nofollow">Here's a typical, official, comment from Discourse</a></p> <blockquote> <p>If your banner is larger than the element we use for the "Introducing Discourse Splash - A visual preloader displayed while site assets load" you gonna have a bad time for LCP.</p> </blockquote> <p>The official response from Discourse is that you should make sure that your content doesn't trigger the <code>LCP</code> measurement and that, instead, our loading animation timestamp is what's used to compute `LCP.</p> <p>The sites with the most extreme ratio of <code>LCP</code> of useful content vs. Chrome's measured <code>LCP</code> were:</p> <ul> <li>Wix <ul> <li><code>M3</code>: <code>6</code></li> <li><code>M1</code>: <code>12</code></li> <li><code>Tecno Spark 8C</code>: <code>3</code></li> <li><code>Itel P32</code>: <code>N/A</code> <code>(FAIL)</code></li> </ul></li> <li>Discourse: <ul> <li><code>M3</code>: <code>10</code></li> <li><code>M1</code>: <code>12</code></li> <li><code>Tecno Spark 8C</code>: <code>4</code></li> <li><code>Itel P32</code>: <code>N/A</code> <code>(FAIL)</code></li> </ul></li> </ul> <p>Although we haven't discussed the gaming of other metrics, it appears that some websites also game other metrics and "optimize" them even when this has no benefit to users.</p> <h3 id="appendix-designing-for-low-performance-devices">Appendix: designing for low performance devices</h3> <p>When using slow devices or any device with low bandwidth and/or poor connectivity, the best experiences, by far, are generally the ones that load a lot of content at once into a static page. If the images have proper width and height attributes and alt text, that's very helpful. Progressive images (as in progressive jpeg) isn't particularly helpful.</p> <p>On a slow device with high bandwidth, any lightweight, static, page works well, and lightweight dynamic pages can work well if designed for performance. Heavy, dynamic, pages are doomed unless the page weight doesn't cause the page to be complex.</p> <p>With low bandwidth and/or poor connectivity, lightweight pages are fine. With heavy pages, the best experience I've had is when I trigger a page load, go do something else, and then come back when it's done (or at least the HTML and CSS are done). I can then open each link I might want to read in a new tab, and then do something else while I wait for those to load.</p> <p>A lot of the optimizations that modern websites do, such as partial loading that causes more loading when you scroll down the page, and the concomitant hijacking of search (because the browser's built in search is useless if the page isn't fully loaded) causes the interaction model that works to stop working and makes pages very painful to interact with.</p> <p>Just for example, a number of people have noted that Substack performs poorly for then because it does partial page loads. <a href="https://danluu.com/substack.mp4">Here's a video by @acidshill of what it looks like to load a Substack article and then scroll on an iPhone 8</a>, where the post has a fairly fast <code>LCP</code>, but if you want to scroll past the header, you have to wait <code>6s</code> for the next page to load, and then on scrolling again, you have to wait maybe another <code>1s</code> to <code>2s</code>:</p> <p>As an example of the opposite approach, I tried loading some fairly large plain HTML pages, such as <a href="https://danluu.com/diseconomies-scale/">https://danluu.com/diseconomies-scale/</a> (<code>0.1 MB wire</code> / <code>0.4 MB raw</code>) and <a href="https://danluu.com/threads-faq/">https://danluu.com/threads-faq/</a> (<code>0.4 MB wire</code> / <code>1.1 MB raw</code>) and these were still quite usable for me even on slow devices. <code>1.1 MB</code> seems to be larger than optimal and breaking that into a few different pages would be better on a low-end devices, but a single page with <code>1.1 MB</code> of text works much better than most modern sites on a slow device. While you can get into trouble with HTML pages that are so large that browsers can't really handle them, for pages with a normal amount of content, it generally isn't until you have <a href="https://nolanlawson.com/2023/01/17/my-talk-on-css-runtime-performance/">complex CSS payloads</a> or JS that the pages start causing problems for slow devices. Below, we test pages that are relatively simple, some of which have a fair amount of media (<code>14 MB</code> in one case) and find that these pages work ok, as long as they stay simple.</p> <h3 id="appendix-articles-on-web-performance-issues">Appendix: articles on web performance issues</h3> <ul> <li>2015: Maciej Cegłowski: <a href="https://idlewords.com/talks/website_obesity.htm">The Website Obesity Crisis</a> <ul> <li>Size: <code>1.0 MB</code> / <code>1.1 MB</code></li> <li><code>Tecno Spark 8C</code>: <code>0.9s</code> / <code>1.4s</code> <ul> <li>Scrolling a bit jerky, images take a little bit of time to appear if scrolling very quickly (jumping halfway down page from top), but delay is below what almost any user would perceive when scrolling a normal distance.</li> </ul></li> </ul></li> <li>2015: Nate Berkopec: <a href="https://www.speedshop.co/2015/11/05/page-weight-doesnt-matter.html">Page Weight Doesn't Matter</a> <ul> <li>Size: <code>80 kB</code> / <code>0.2 MB</code></li> <li><code>Tecno Spark 8C</code>: <code>0.8s</code> / <code>0.7s</code> <ul> <li>Does lazy loading, page downloads <code>650 kB</code> / <code>1.8 MB</code> if you scroll through the entire page, but scrolling is only a little jerk and the lazy loading doesn't cause delays. Probably the only page I've tried that does lazy loading in a way that makes the experience better and not worse on a slow device; I didn't test on a slow connection, where this would still make the experience worse.</li> </ul></li> <li><code>Itel P32</code>: <code>1.1s</code> / <code>1s</code> <ul> <li>Scrolling basically unusable; scroll extremely jerky and moves a random distance, often takes over <code>1s</code> for text to render when scrolling to new text; can be much worse with images that are lazy loaded. Even though this is the implementation of lazy loading I've seen in the wild, the <code>Itel P32</code> still can't handle it.</li> </ul></li> </ul></li> <li>2017: Dan Luu: <a href="https://danluu.com/web-bloat/">How web bloat impacts users with slow connections</a> <ul> <li>Size: <code>14 kB</code> / <code>57 kB</code></li> <li><code>Tecno Spark 8C</code>: <code>0.5s</code> / <code>0.3s</code> <ul> <li>Scrolling and interaction work fine.</li> </ul></li> <li><code>Itel P32</code>:<code>0.7s</code> / <code>0.5 s</code></li> </ul></li> <li>2017-2024+: Alex Russell: <a href="https://infrequently.org/series/performance-inequality/">The Performance Inequality Gap (series)</a> <ul> <li>Size: <code>82 kB</code> / <code>0.1 MB</code></li> <li><code>Tecno Spark 8C</code>: <code>0.5s</code> / <code>0.4s</code> <ul> <li>Scrolling and interaction work fine.</li> </ul></li> <li><code>Itel P32</code>: <code>0.7s</code> / <code>0.4s</code> <ul> <li>Scrolling and interaction work fine.</li> </ul></li> </ul></li> <li>2024: Nikita Prokopov (Tonsky): <a href="https://tonsky.me/blog/js-bloat/">JavaScript Bloat in 2024</a> <ul> <li>Size: <code>14 MB</code> / <code>14 MB</code></li> <li><code>Tecno Spark 8C</code>: <code>0.8s</code> / <code>1.9s</code> <ul> <li>When scrolling, it takes a while for images to show up (500ms or so) and the scrolling isn't smooth, but it's not jerky enough that it's difficult to scroll to the right place.</li> </ul></li> <li><code>Itel P32</code>: <code>2.5s</code> / <code>3s</code> <ul> <li>Scrolling isn't smooth. Scrolling accurately is a bit difficult, but can generally scroll to where you want if very careful. Generally takes a bit more than <code>1s</code> for new content to appear when you scroll a significant distance.</li> </ul></li> </ul></li> <li>2024: Dan Luu: <a href="https://danluu.com/slow-device/">This post</a> <ul> <li>Size: <code>25 kB</code> / <code>74 kB</code></li> <li><code>Tecno Spark 8C</code>: <code>0.6s</code> / <code>0.5s</code> <ul> <li>Scrolling and interaction work fine.</li> </ul></li> <li><code>Itel P32</code>: <code>1.3s</code> / <code>1.1s</code> <ul> <li>Scrolling and interaction work fine, although I had to make a change for this to be the case —&nbsp;this doc originally had an embedded video, which the <code>Itel P32</code> couldn't really handle. <ul> <li>Note that, while these numbers are worse than the numbers for "Page Weight Doesn't Matter", this page is usable after load, which that other page isn't beacuse it execute some kind of lazy loading that's too complex for this phone to handle in a reasonable timeframe.</li> </ul></li> </ul></li> </ul></li> </ul> <p>Just as an aside, something I've found funny for a long time is that I get quite a bit of hate mail about the styling on this page (and a similar volume of appreciation mail). By hate mail, I don't mean polite suggestions to change things, I mean the equivalent of road rage, but for web browsing; web rage. I know people who run sites that are complex enough that they're unusable by a significant fraction of people in the world. How come people are so incensed about the styling of this site and, proportionally, basically don't care at all that the web is unusable for so many people?</p> <p>Another funny thing here is that the people who appreciate the styling generally appreciate that the site doesn't override any kind of default styling, letting you make the width exactly what you want (by setting your window size how you want it) and it also doesn't override any kind of default styling you apply to sites. The people who are really insistent about this want everyone to have some width limit they prefer, some font they prefer, etc., but it's always framed in a way as if they don't want it, it's really for the benefit of people at large even though accommodating the preferences of the web ragers would directly oppose the preferences of people who prefer (just for example) to be able to adjust the text width by adjusting their window width.</p> <p>Until I pointed this out tens of times, this iteration would usually start with web ragers telling me that "studies show" that narrower text width is objectively better, but on reading every study that exists on the topic that I could find, I didn't find this to be the case. Moreover, on asking for citations, it's clear that people saying this generally hadn't read any studies on this at all and would sometimes hastily send me a study that they did not seem to have read. When I'd point this out, people would then change their argument to how studies can't really describe the issue (odd that they'd cite studies in the first place), although one person cited a book to me (which I read and they, apparently, had not since it also didn't support their argument) and then move to how this is what everyone wants, even though that's clearly not the case, both from the comments I've gotten as well as the data I have from when I made the change.</p> <p>Web ragers who have this line of reasoning generally can't seem to absorb the information that their preferences are not universal and will insist that they regardless of what people say they like, which I find fairly interesting. On the data, when I switched from Octopress styling (at the time, the most popular styling for programming bloggers) to the current styling, I got what appeared to be a causal an increase in traffic and engagement, so it appears that not only do people who write me appreciation mail about the styling like the styling, the overall feeling of people who don't write to me appears to be that the site is fine and apparently more appealing than standard programmer blog styling. When I've noted this, people either become further invested in the idea that their preferences are universal and people who think they have other preferences are wrong and reply with total nonsense.</p> <p>I would understand this kind of anger about how much the web has been made unusable for people who aren't, by global standards, fairly well off, but it's curious that so many people find a site that is accessible but accedes to other people's preferences to be a topic worth sending so many angry rants about, as well as one where it's worth fabricating "objective" evidence for their opinion.</p> <h3 id="appendix-empathy-for-non-rich-users">Appendix: empathy for non-rich users</h3> <p>Something I've observed over time, as programming has become more prestigious and more lucrative, is <a href="https://mastodon.social/@danluu/109901711437753852">that people have tended to come from wealthier backgrounds</a> and have less exposure to people with different income levels. An example we've discussed before, is at a well-known, prestigious, startup that has a very left-leaning employee base, where everyone got rich, on a discussion about the covid stimulus checks, in a slack discussion, a well meaning progressive employee said that it was pointless because people would just use their stimulus checks to buy stock. This person had, apparently, never talked to any middle-class (let alone poor) person about where their money goes or looked at the data on who owns equity. And that's just looking at American wealth. When we look at world-wide wealth, the general level of understanding is much lower. People seem to really underestimate the dynamic range in wealth and income across the world. From having talked to quite a few people about this, a lot of people seem to have mental buckets for "poor by American standards" (buys stock with stimulus checks) and "poor by worldwide standards" (maybe doesn't even buy stock), but the range of poverty in the world dwarfs the range of poverty in America to an extent that not many wealthy programmers seem to realize.</p> <p>Just for example, <a href="https://mastodon.social/@danluu/109537302116865694">in this discussion how lucky I was (in terms of financial opportunities) that my parents made it to America</a>, someone mentioned that it's not that big a deal because they had great financial opportunities in Poland. For one thing, with respect to the topic of the discussion, the probability that someone will end up with a high-paying programming job (senior staff eng at a high-paying tech company) or equivalent, I suspect that, when I was born, being born poor in the U.S. gives you better odds than being fairly well off in Poland, but I could believe the other case as well if presented with data. But if we're comparing Poland v. U.S. to Vietnam v. U.S., if I spend <abbr title="so, these probably aren't the optimal numbers one would use for a comparison, but I think they're good enough for this purpose">15 seconds looking up rough wealth numbers for these countries</abbr> in the year I was born, the GDP/capita ratio of U.S. : Poland was ~8:1, whereas it was ~50 : 1 for Poland : Vietnam. The difference in wealth between Poland and Vietnam was roughly the square of the difference between the U.S. and Poland, so Poland to Vietnam is roughly equivalent to Poland vs. some hypothetical country that's richer than the U.S. by the amount that the U.S. is richer than Poland. These aren't even remotely comparable, but a lot of people seem to have this mental model that there's "rich countries" and "not rich countries" and "not rich countries" are all roughly in the same bucket. GDP/capita isn't ideal, but it's easier to find than percentile income statistics; the quick search I did also turned up that annual income in Vietnam them was something like $200-$300 a year. Vietnam was also going through the tail end of a famine whose impacts are a bit difficult to determine because statistics here seem to be gamed, but if you believe the mortality rate statistics, the famine caused total overall mortality rate to jump to double the normal baseline<sup id="fnref:L"><a rel="footnote" href="#fn:L">1</a></sup>.</p> <p>Of course, at the time, the median person in a low-income country wouldn't have had a computer, let alone internet access. But, today it's fairly common for people in low-income countries to have devices. Many people either don't seem to realize this or don't understand what sorts of devices a lot of these folks use.</p>  <p>On the Discourse founder's comments on iOS vs. Android marketshare, Fabian notes</p> <blockquote> <p>In the US, according to the most recent data I could find (for 2023), iPhones have around 60% marketshare. In the EU, it's around 33%. This has knock-on effects. Not only do iOS users skew towards the wealthier end, they also skew towards the US.</p> <p>There's some secondary effects from this too. For example, in the US, iMessage is very popular for group chats etc. and infamous for interoperating very poorly with Android devices in a way that makes the experience for Android users very annoying (almost certainly intentionally so).</p> <p>In the EU, not least because Android is so much more prominent, iMessage is way less popular and anecdotally, even iPhone users among my acquaintances who would probably use iMessage in the US tend to use WhatsApp instead.</p> <p>Point being, globally speaking, recent iOS + fast Internet is even more skewed towards a particular demographic than many app devs in the US seem to be aware.</p> </blockquote> <p>And on the comment about mobile app vs. web app sizes, Fabian said:</p> <blockquote> <p>One more note from experience: apps you install when you install them, and generally have some opportunity to hold off on updates while you're on a slow or metered connection (or just don't have data at all).</p> <p>Back when I originally got my US phone, I had no US credit history and thus had to use prepaid plans. I still do because it's fine for what I actually use my phone for most of the time, but it does mean that when I travel to Germany once a year, I don't get data roaming at all. (Also, phone calls in Germany cost me $1.50 apiece, even though T-Mobile is the biggest mobile provider in Germany - though, of course, not T-Mobile US.)</p> <p>Point being, I do get access to free and fast Wi-Fi at T-Mobile hotspots (e.g. major train stations, airports etc.) and on inter-city trains that have them, but I effectively don't have any data plan when in Germany at all.</p> <p>This is completely fine with mobile phone apps that work offline and sync their data when they have a connection. But web apps are unusable while I'm not near a public Wi-Fi.</p> <p>Likewise I'm fine sending an email over a slow metered connection via the Gmail app, but I for sure wouldn't use any web-mail client that needs to download a few MBs worth of zipped JS to do anything on a metered connection.</p> <p>At least with native app downloads, I can prepare in advance and download them while I'm somewhere with good internet!</p> </blockquote> <p>Another comment from Fabian (this time paraphrased since this was from a conversation), is that people will often justify being quantitatively hugely slower because there's a qualitative reason something should be slow. One example he gave was that screens often take a long time to sync their connection and this is justified because there are operations that have to be done that take time. For a long time, these operations would often take seconds. Recently, a lot of displays sync much more quickly because Nvidia specifies how long this can take for something to be "G-Sync" certified, so display makers actually do this in a reasonable amount of time now. While it's true that there are operations that have to be done that take time, there's no fundamental reason they should take as much time as they often used to. Another example he gave was on how someone was justifying how long it took to read thousands of files because the operation required a lot of syscalls and "syscalls are slow", which is a qualitatively true statement, but if you look at the actual cost of a syscall, in the case under discussion, the cost of a syscall was many orders of magnitude from being costly enough to be a reasonable explanation for why it took so long to read thousands of files.</p> <p>On this topic, when people point out that a modern website is slow, someone will generally respond with the qualitative defense that the modern website has these great features, which the older website is lacking. And while it's true that (for example) Discourse has features that MyBB doesn't, it's hard to argue that its feature set justifies being <code>33x</code> slower.</p> <h3 id="appendix-experimental-details">Appendix: experimental details</h3> <p>With the exception of danluu.com and, arguably, HN, for each site, I tried to find the "most default" experience. For example, for WordPress, this meant a demo blog with the current default theme, twentytwentyfour. In some cases, this may not be the most likely thing someone uses today, e.g., for Shopify, I looked at the first thing that theme they give you when you browse their themes, but I didn't attempt to find theme data to see what the most commonly used theme is. For this post, I wanted to do all of the data collection and analysis as a short project, something that takes less than a day, so there were a number of shortcuts like this, which will be described below. I don't think it's wrong to use the first-presented Shopify theme in a decent fraction of users will probably use the first-presente theme, but that is, of course, less representative than grabbing whatever the most common theme is and then also testing many different sites that use that theme to see how real-world performance varies when people modify the theme for their own use. If I worked for Shopify or wanted to do competitive analysis on behalf of a competitor, I would do that, but for a one-day project on how large websites impact users on low-end devices, the performance of Shopify demonstrated here seems ok.</p> <p>For the tests on laptops, I tried to have the laptop at ~60% battery, not plugged in, and the laptop was idle for enough time to return to thermal equilibrium in a room at 20°C, so pages shouldn't be impacted by prior page loads or other prior work that was happening on the machine.</p> <p>For the mobile tests, the phones were at ~100% charge and plugged in, and also previously at 100% charge so the phones didn't have any heating effect you can get from rapidly charging. As noted above, these tests were formed with <code>1Gbps</code> WiFi. No other apps were running, the browser had no other tabs open, and the only apps that were installed on the device, so no additional background tasks should've been running other than whatever users are normally subject to by the device by default. A real user with the same device is going to see worse performance than we measured here in almost every circumstance.</p> <p>Sizes were all measured on mobile, so in cases where different assets are loaded on mobile vs. desktop, the we measured the mobile asset sizes. <code>CPU</code> was measured as CPU time on the main thread (I did also record time on other threads for sites that used other threads, but didn't use this number; if <code>CPU</code> were a metric people wanted to game, time on other threads would have to be accounted for to prevent sites from trying to offload as much work as possible to other threads, but this isn't currently an issue and time on main thread is more directly correlated to usability than sum of time across all threads, and the metric that would work for gaming is less legible with no upside for now).</p> <p>For WiFi speeds, speed tests had the following numbers:</p> <ul> <li><code>M3 Max</code> <ul> <li>Netflix (fast.com) <ul> <li>Download: <code>850 Mbps</code></li> <li>Upload: <code>840 Mbps</code></li> <li>Latency (unloaded / loaded): <code>3ms</code> / <code>8ms</code></li> </ul></li> <li>Okta <ul> <li>Download: <code>900 Mbps</code></li> <li>Upload: <code>840 Mbps</code></li> <li>Latency (unloaded / download / upload): <code>3ms</code> / <code>8ms</code> / <code>13ms</code></li> </ul></li> </ul></li> <li><code>Tecno Spark 8C</code> <ul> <li>Netflix (fast.com) <ul> <li>Download: <code>390 Mbps</code></li> <li>Upload: <code>210 Mbps</code></li> <li>Latency (unloaded / loaded): <code>2ms</code> / <code>30ms</code></li> </ul></li> <li>Okta <ul> <li>Okta web app fails, can't see results</li> </ul></li> </ul></li> <li><code>Itel P32</code> <ul> <li>Netflix <ul> <li>Download: <code>44 Mbps</code></li> <li>Upload: test fails to work (sends one chunk of data and then hangs, sending no more data)</li> <li>Latency (unloaded / loaded): <code>4ms</code> / <code>400ms</code></li> </ul></li> <li>Okta <ul> <li>Download: <code>45 Mbps</code></li> <li>Upload: test fails to work</li> <li>Latency: test fails to display latency</li> </ul></li> </ul></li> </ul> <p>One thing to note is that the <code>Itel P32</code> doesn't really have the ability to use the bandwidth that it nominally has. Looking at the top Google reviews, none of them mention this. <a href="https://www.nairaland.com/4628841/itel-p32-review-great-those" rel="nofollow">The first review reads</a></p> <blockquote> <p>Performance-wise, the phone doesn’t lag. It is powered by the latest Android 8.1 (GO Edition) ... we have 8GB+1GB ROM and RAM, to run on a power horse of 1.3GHz quad-core processor for easy multi-tasking ... I’m impressed with the features on the P32, especially because of the price. I would recommend it for those who are always on the move. And for those who take battery life in smartphones has their number one priority, then P32 is your best bet.</p> </blockquote> <p><a href="https://techjaja.com/itel-p32-review-dual-camera-smartphone-alarming-price-tag/" rel="nofollow">The second review reads</a></p> <blockquote> <p>Itel mobile is one of the leading Africa distributors ranking 3rd on a continental scale ... the light operating system acted up to our expectations with no sluggish performance on a 1GB RAM device ... fairly fast processing speeds ... the Itel P32 smartphone delivers the best performance beyond its capabilities ... at a whooping UGX 330,000 price tag, the Itel P32 is one of those amazing low-range like smartphones that deserve a mid-range flag for amazing features embedded in a single package.</p> </blockquote> <p><a href="https://pctechmag.com/2018/08/itel-p32-full-review-much-more-than-just-a-budget-entry-level-smartphone/" rel="nofollow">The third review reads</a></p> <blockquote> <p>"Much More Than Just a Budget Entry-Level Smartphone ... Our full review after 2 weeks of usage ... While switching between apps, and browsing through heavy web pages, the performance was optimal. There were few lags when multiple apps were running in the background, while playing games. However, the overall performance is average for maximum phone users, and is best for average users [screenshot of game] Even though the game was skipping some frames, and automatically dropped graphical details it was much faster if no other app was running on the phone.</p> </blockquote> <p>Notes on sites:</p> <ul> <li>Wix <ul> <li>www.wix.com/website-template/view/html/3173?originUrl=https%3A%2F%2Fwww.wix.com%2Fwebsite%2Ftemplates%2Fhtml%2Fmost-popular&amp;tpClick=view_button&amp;esi=a30e7086-28db-4e2e-ba22-9d1ecfbb1250: this was the first entry when I clicked to get a theme</li> <li><code>LCP</code> was misleading on every device</li> <li>On the <code>Tecno Spark 8C</code>, scrolling never really works. It's very jerky and this never settles down</li> <li>On the <code>Itel P32</code>, the page fails non-deterministically (different errors on different loads); it can take quite a while to error out; it was <code>23s</code> on the first run, with the CPU pegged for <code>28s</code></li> </ul></li> <li>Patreon <ul> <li>www.patreon.com/danluu: used my profile where possible</li> <li>Scrolling on Patreon and finding old posts is so painful that I maintain <a href="https://danluu.com/#pt">my own index of my Patreon posts</a> so that I can find my old posts without having to use Patreon.</li> </ul></li> <li>Threads <ul> <li>threads.net/danluu.danluu: used my profile where possible</li> <li>On the <code>Itel P32</code>, this technically doesn't load correctly and could be marked as <code>FAIL</code>, but it's close enough that I counted it. The thing that's incorrect is that profile photos have a square box around then <ul> <li>However, as with the other heavy pages, interacting with the page doesn't really work and the page is unusable, but this appears to be for the standard performance reasons and not because the page failed to render</li> </ul></li> </ul></li> <li>Twitter <ul> <li>twitter.com/danluu: used my profile where possible</li> </ul></li> <li>Discourse <ul> <li>meta.discourse.org: this is what turned up when I searched for an official forum.</li> <li>As discussed above, the <code>LCP</code> is highly gamed and basically meaningless. We linked to a post where the Discourse folks note that, on slow loads, they put a giant splash screen up at <code>2s</code> to cap the <code>LCP</code> at <code>2s</code>. Also notable is that, on loads that are faster than the 2s, the <code>LCP</code> is also highly gamed. For example, on the <code>M3 Max</code> with low-latency <code>1Gbps</code> internet, the <code>LCP</code> was reported as <code>115ms</code>, but the page loads actual content at <code>1.1s</code>. This appears to use the same fundamental trick as "Discourse Splash", in that it paints a huge change onto the screen and then carefully loads smaller elements to avoid having the actual page content detected as the <code>LCP</code>.</li> <li>On the <code>Tecno Spark 8C</code>, scrolling is unpredictable and can jump too far, triggering loading from infinite scroll, which hangs the page for <code>3s-10s</code>. Also, the entire browser sometimes crashes if you just let the browser sit on this page for a while.</li> <li>On the <code>Itel P32</code>, an error message is displayed after <code>7.5s</code></li> </ul></li> <li>Bluesky <ul> <li>bsky.app/profile/danluu.com</li> <li>Displays a blank screen on the <code>Itel P32</code></li> </ul></li> <li>Squarespace <ul> <li>cedar-fluid-demo.squarespace.com: this was the second theme that showed up when I clicked themes to get a theme; the first was one called "Bogart", but that was basically a "coming soon" single page screen with no content, so I used the second theme instead of the first one.</li> <li>A lot of errors and warnings in the console with the <code>Itel P32</code>, but the page appears to load and work, although interacting with it is fairly slow and painful</li> <li><code>LCP</code> on the <code>Tecno Spark 8C</code> was significantly before the page content actually loaded</li> </ul></li> <li>Tumblr <ul> <li>www.tumblr.com/slatestarscratchpad: used this because I know this tubmlr exists. I don't read a lot of tumblers (maybe three or four), and this one seemed like the closest thing to my blog that I know of on tumblr.</li> <li>This page fails on the <code>Itel P32</code>, but doesn't <code>FAIL</code>. The console shows that the JavaScript errors out, but the page still works fine (I tried scrolling, clicking links, etc., and these all worked), so you can actually go to the post you want and read it. The JS error appears to have made this page load much more quickly than it other would have and also made interacting with the page after it loaded fairly zippy.</li> </ul></li> <li>Shopify <ul> <li>themes.shopify.com/themes/motion/styles/classic/preview?surface_detail=listing&amp;surface_inter_position=1&amp;surface_intra_position=1&amp;surface_type=all: this was the first theme that showed up when I looked for themes</li> <li>On the first <code>M3/10</code> run, Chrome dev tools reported a nonsensical <code>697s</code> of CPU time (the run completed in a normal amount of time, well under <code>697s</code> or even <code>697/10s</code>. This run was ignored when computing results.</li> <li>On the <code>Itel P32</code>, the page load never completes and it just shows a flashing cursor-like image, which is deliberately loaded by the theme. On devices that load properly, the flashing cursor image is immediately covered up by another image, but that never happens here.</li> <li>I wondered if it wasn't fair to use this example theme because there's some stuff on the page that lets you switch theme styles, so I checked out actual uses of the theme (the page that advertises the theme lists users of the theme). I tried the first two listed real examples and they were both much slower than this demo page.</li> </ul></li> <li>Reddit <ul> <li>reddit.com</li> <li>Has an unusually low <code>LCP*</code> compared to how long it takes for the page to become usable. Although not measured in this test, I generally find the page slow and sort of unusable on Intel Macbooks which are, by historical standards, extremely fast computers (unless I use old.reddit.com)</li> </ul></li> <li>Mastodon <ul> <li>mastodon.social/@danluu: used my profile where possible</li> <li>Fails to load on <code>Itel P32</code>, just gives you a blank screen. Due to how long things generally take on the <code>Itel P32</code>, it's not obvious for a while if the page is failing or if it's just slow</li> </ul></li> <li>Quora <ul> <li>www.quora.com/Ever-felt-like-giving-up-on-your-dreams-How-did-you-come-out-of-it: I tried googling for quora + the username of a metafilter user who I've heard is now prolific on Quora. Rather than giving their profile page, Google returned this page, which appears to have nothing to do with the user I searched for. So, this isn't comparable to the social media profiles, but getting a random irrelevant Quora result from Google is how I tend to interact with Quora, so I guess this is representative of my Quora usage.</li> <li>On the <code>Itel P32</code>, the page stops executing scripts at some point and doesn't fully load. This causes it to fail to display properly. Interacting with the page doesn't really work either.</li> </ul></li> <li>Substack <ul> <li>Used thezvi.substack.com because I know Zvi has a substack and writes about similar topics.</li> </ul></li> <li>vBulletin: <ul> <li>forum.vbulletin.com: this is what turned up when I searched for an official forum.</li> </ul></li> <li>Medium <ul> <li>medium.com/swlh: I don't read anything on Medium, so I googled for programming blogs on Medium and this was the top hit. From looking at the theme, it doesn't appear to be unusually heavy or particularly customized for a Medium blog. Since it appears to be widely read and popular, it's more likely to be served from a CDN and than some of the other blogs here.</li> <li>On a run that wasn't a benchmark reference run, on the <code>Itel P32</code>, I tried scrolling starting 35s after loading the page. The delay to scroll was <code>5s-8s</code> and scrolling moved an unpredictable amount, making the page completely unusable. This wasn't marked as a <code>FAIL</code> in the table, but one could argue that this should be a <code>FAIL</code> since the page is unusable.</li> </ul></li> <li>Ghost <ul> <li>source.ghost.io because this is the current default Ghost theme and it was the first example I found</li> </ul></li> <li>Wordpress <ul> <li>2024.wordpress.net because this is the current default wordpress theme and this was the first example of it I found</li> </ul></li> <li>XenForo <ul> <li>xenforo.com/community/: this is what turned up when I searched for an official forum</li> <li>On the <code>Itel P32</code>, the layout is badly wrong and page content overlaps itself. There's no reasonable way to interact with the element you want because of this, and reading the text requires reading text that's been overprinted multiple times.</li> </ul></li> <li>Wordpress (old) <ul> <li>Used thezvi.wordpress.com because it has the same content as Zvi's substack, and happens to be on some old wordpress theme that used to be a very common choice</li> </ul></li> <li>phpBB <ul> <li>www.phpbb.com/community/index.php: this is what turned up when I searched for an official forum.</li> </ul></li> <li>MyBB <ul> <li>community.mybb.com: this is what turned up when I searched for an official forum.</li> <li>Site doesn't serve up a mobile version. In general, I find the desktop version of sites to be significantly better than the mobile version when on a slow device, so this works quite well, although they're likely penalized by Google for this.</li> </ul></li> <li>HN <ul> <li>news.ycombinator.com</li> <li>In principle, HN should be the slowest social media site or link aggregator because it's written in a custom Lisp that isn't highly optimized and the code was originally written with brevity and cleverness in mind, which generally gives you fairly poor performance. However, that's only poor relative to what you'd get if you were writing high-performance code, which is not a relevant point of comparison here.</li> </ul></li> <li>danluu.com <ul> <li>Self explanatory</li> <li>This currently uses a bit less CPU than HN, but I expect this to eventually use more CPU as the main page keeps growing. At the moment, this page has 176 links to 168 articles vs. HN's 199 links to 30 articles but, barring an untimely demise, this page should eventually have more links than HN. <ul> <li>As noted above, I find that pagination for such small pages makes the browsing experience much worse on slow devices or with bad connections, so I don't want to "optimize" this by paginating it or, even worse, doing some kind of dynamic content loading on scroll.</li> </ul></li> </ul></li> <li>Woo Commerce <ul> <li>I originally measured Woo Commerce as well but, unlike the pages and platforms tested above, I didn't find that being fast or slow on the initial load was necessarily representative of subsequent performance of other action, so this wasn't included in the table because having this in the table is sort of asking for a comparison against Shopify. In particular, while the "most default" Woo theme I could find was significantly faster than the "most default" Shopify theme on initial load on a slow device, performance was multidimensional enough that it was easy to find realistic scenarios where Shopify was faster than Woo and vice versa on a slow device, which is quite different from what I saw with newer blogging platforms like Substack and Medium compared to older platforms like Wordpress, or a modern forum like Discourse versus the older PHP-based forums. A real comparison of shopping sites that have carts, checkout flows, etc., would require a better understanding of real-world usage of these sites than I was going to get in a single day.</li> </ul></li> </ul> <p>Another kind of testing would be to try to configure pages to look as similar as possible. I'd be interested in seeing that results for that if anyone does it, and maybe I'll try doing that on another day if no one else does it.</p>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[IBM 360 in UK need a home (133 pts)]]></title>
            <link>https://www.ibm360.co.uk/</link>
            <guid>39728994</guid>
            <pubDate>Sat, 16 Mar 2024 20:00:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ibm360.co.uk/">https://www.ibm360.co.uk/</a>, See on <a href="https://news.ycombinator.com/item?id=39728994">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="primary"><main id="main" role="main">
    
<article id="post-911">
			<p><a href="https://www.ibm360.co.uk/?p=911"><img width="8870" height="3908" src="https://www.ibm360.co.uk/wp-content/uploads/2019/11/IMG_0591.jpg" alt="" decoding="async" fetchpriority="high" srcset="https://www.ibm360.co.uk/wp-content/uploads/2019/11/IMG_0591.jpg 8870w, https://www.ibm360.co.uk/wp-content/uploads/2019/11/IMG_0591-300x132.jpg 300w, https://www.ibm360.co.uk/wp-content/uploads/2019/11/IMG_0591-768x338.jpg 768w, https://www.ibm360.co.uk/wp-content/uploads/2019/11/IMG_0591-1024x451.jpg 1024w" sizes="(max-width: 8870px) 100vw, 8870px"></a>
       </p>
       	<div>
			<p>Hello! If you’re still here reading this blog, I’m impressed. We haven’t posted anything of note for some years now and frankly, thats because we haven’t done anything of note with the project. The machines have been sitting in their home, virtually untouched, for 4 years now. Chris &amp; I (Adam) are just too busy with our respective professional and personal lives to give them a second look, and we’ve come to the difficult decision that it’s time to look at letting the systems go. When we originally moved the systems to Creslow, part of our agreement was to provide PR visibility for the services offered by ecom. Whilst this initially obviously garnered some visibility, our lack of progress with the project has obviously had…</p>
<p><a href="https://www.ibm360.co.uk/?p=911">Continue Reading<span>→</span></a></p>
		</div>

</article><!-- #post-## -->

<article id="post-902">
			<p><a href="https://www.ibm360.co.uk/?p=902"><img width="2560" height="910" src="https://www.ibm360.co.uk/wp-content/uploads/2022/05/IMG_3653-scaled.jpeg" alt="" decoding="async" srcset="https://www.ibm360.co.uk/wp-content/uploads/2022/05/IMG_3653-scaled.jpeg 2560w, https://www.ibm360.co.uk/wp-content/uploads/2022/05/IMG_3653-300x107.jpeg 300w, https://www.ibm360.co.uk/wp-content/uploads/2022/05/IMG_3653-1024x364.jpeg 1024w, https://www.ibm360.co.uk/wp-content/uploads/2022/05/IMG_3653-768x273.jpeg 768w, https://www.ibm360.co.uk/wp-content/uploads/2022/05/IMG_3653-1536x546.jpeg 1536w, https://www.ibm360.co.uk/wp-content/uploads/2022/05/IMG_3653-2048x728.jpeg 2048w" sizes="(max-width: 2560px) 100vw, 2560px"></a>
       </p>
       	<div>
			<p>Dear Reader, Wow, it’s been a while since our last post here, almost 2 years! Time has totally flown by. I checked the traffic this morning and was pleasantly surprised to see that we’re still getting 2,000+ hits per month which is just incredible given that we haven’t published any updates. So, I’m guessing you probably want to know whats going on and why we’re not posting here. Let me summarily answer your most important questions below: Are all of you okay? – Yes. Is the project dead? – No. Do you have any updates for us? – Unfortunately, not really. So, the reason we haven’t been posting here is mainly because, well, nothing has changed. Chris &amp; I have both been insanely busy with…</p>
<p><a href="https://www.ibm360.co.uk/?p=902">Continue Reading<span>→</span></a></p>
		</div>

</article><!-- #post-## -->

<article id="post-891">
			<p><a href="https://www.ibm360.co.uk/?p=891"><img width="1200" height="675" src="https://www.ibm360.co.uk/wp-content/uploads/2020/05/Silicon-Glen-TX-1.jpeg" alt="" decoding="async" srcset="https://www.ibm360.co.uk/wp-content/uploads/2020/05/Silicon-Glen-TX-1.jpeg 1200w, https://www.ibm360.co.uk/wp-content/uploads/2020/05/Silicon-Glen-TX-1-300x169.jpeg 300w, https://www.ibm360.co.uk/wp-content/uploads/2020/05/Silicon-Glen-TX-1-1024x576.jpeg 1024w, https://www.ibm360.co.uk/wp-content/uploads/2020/05/Silicon-Glen-TX-1-768x432.jpeg 768w" sizes="(max-width: 1200px) 100vw, 1200px"></a>
       </p>
       	<div>
			<p>Well, what can I say. It seems as though the last update was a lifetime ago. The world has changed so much! As you might imagine we have been unable to complete any work on the IBM 360 over the last couple of months due to the lockdown restrictions in place in the UK. We are hopeful that in the coming months as the lockdown eases we will be able to return to the project and start posting regular updates again. We have received a few items off of the Amazon wishlist, and I will post the proper thanks in due course, I am currently unable to do so as they get delivered to my fathers house and due to the self isolation policies I…</p>
<p><a href="https://www.ibm360.co.uk/?p=891">Continue Reading<span>→</span></a></p>
		</div>

</article><!-- #post-## -->

<article id="post-813">
			<p><a href="https://www.ibm360.co.uk/?p=813"><img width="2560" height="1920" src="https://www.ibm360.co.uk/wp-content/uploads/2020/03/IMG_20200301_165051-scaled.jpg" alt="" decoding="async" loading="lazy" srcset="https://www.ibm360.co.uk/wp-content/uploads/2020/03/IMG_20200301_165051-scaled.jpg 2560w, https://www.ibm360.co.uk/wp-content/uploads/2020/03/IMG_20200301_165051-300x225.jpg 300w, https://www.ibm360.co.uk/wp-content/uploads/2020/03/IMG_20200301_165051-1024x768.jpg 1024w, https://www.ibm360.co.uk/wp-content/uploads/2020/03/IMG_20200301_165051-768x576.jpg 768w, https://www.ibm360.co.uk/wp-content/uploads/2020/03/IMG_20200301_165051-1536x1152.jpg 1536w, https://www.ibm360.co.uk/wp-content/uploads/2020/03/IMG_20200301_165051-2048x1536.jpg 2048w" sizes="(max-width: 2560px) 100vw, 2560px"></a>
       </p>
       	<div>
			<p>We know we’re a few months late, but we want to wish all of our readers a happy new year! And to kick off the new year (even if we are a bit late) heres our first post of 2020! We’ve all been rather busy in our personal and professional lives of late, and as such the updates haven’t been as regular as we would have liked. We’re hoping to get quite a few days of work in this year, and we promise we’ll update you all as much as possible! We all agreed to meet on Sunday the 1st of March to resume work on the machine. We didn’t really have any set goals for the day, except to continue cleaning the machine, and…</p>
<p><a href="https://www.ibm360.co.uk/?p=813">Continue Reading<span>→</span></a></p>
		</div>

</article><!-- #post-## -->

<article id="post-754">
			<p><a href="https://www.ibm360.co.uk/?p=754"><img width="2560" height="1920" src="https://www.ibm360.co.uk/wp-content/uploads/2019/12/PeterCleaning-scaled.jpg" alt="" decoding="async" loading="lazy" srcset="https://www.ibm360.co.uk/wp-content/uploads/2019/12/PeterCleaning-scaled.jpg 2560w, https://www.ibm360.co.uk/wp-content/uploads/2019/12/PeterCleaning-300x225.jpg 300w, https://www.ibm360.co.uk/wp-content/uploads/2019/12/PeterCleaning-1024x768.jpg 1024w, https://www.ibm360.co.uk/wp-content/uploads/2019/12/PeterCleaning-768x576.jpg 768w, https://www.ibm360.co.uk/wp-content/uploads/2019/12/PeterCleaning-1536x1152.jpg 1536w, https://www.ibm360.co.uk/wp-content/uploads/2019/12/PeterCleaning-2048x1536.jpg 2048w" sizes="(max-width: 2560px) 100vw, 2560px"></a>
       </p>
       	<div>
			<p>Another week, another blog post! So, there’s probably going to be a trend of things in the next few blog posts, and one of them is going to be, well, you guessed it! Cleaning! The machines were all very dirty when the arrived and it’s going to take us quite some time to clean the outsides, let alone the insides, so please bear with us whilst we post about it; hopefully you’ll find the process as satisfying as we do! Over the past week or so I’ve been conversing with a chap called Simon Van Winklen about working with us, and very kindly he’s volunteering some of his time to come and assist us with the restoration process. Simon is a retired Engineer who’s career…</p>
<p><a href="https://www.ibm360.co.uk/?p=754">Continue Reading<span>→</span></a></p>
		</div>

</article><!-- #post-## -->

<article id="post-666">
			<p><a href="https://www.ibm360.co.uk/?p=666"><img width="3024" height="4032" src="https://www.ibm360.co.uk/wp-content/uploads/2019/11/IMG_0587-1-e1574642700122.jpg" alt="" decoding="async" loading="lazy" srcset="https://www.ibm360.co.uk/wp-content/uploads/2019/11/IMG_0587-1-e1574642700122.jpg 3024w, https://www.ibm360.co.uk/wp-content/uploads/2019/11/IMG_0587-1-e1574642700122-225x300.jpg 225w, https://www.ibm360.co.uk/wp-content/uploads/2019/11/IMG_0587-1-e1574642700122-768x1024.jpg 768w" sizes="(max-width: 3024px) 100vw, 3024px"></a>
       </p>
       	<div>
			<p>So we mentioned in some of our earlier posts that we have a plethora of manuals that we found in the original building. These manuals are a rather obscure size being slightly longer than A3. This makes them very inconvenient to scan as they don’t fit on a regular scanner, and as such we needed a much bigger scanner. A0 scanners are particularly scarce, especially on the used market, and they command a high price (usually in the thousands). After browsing eBay for a while I happened upon a listing by a recycling company located fairly near to me for a Colortrac SmartLF Cx40 in unknown condition. For the price (&lt;£100) it was worth a punt! I won the bidding for just under £100 and…</p>
<p><a href="https://www.ibm360.co.uk/?p=666">Continue Reading<span>→</span></a></p>
		</div>

</article><!-- #post-## -->

<article id="post-485">
			<p><a href="https://www.ibm360.co.uk/?p=485"><img width="1600" height="1200" src="https://www.ibm360.co.uk/wp-content/uploads/2019/11/78F.jpg" alt="" decoding="async" loading="lazy" srcset="https://www.ibm360.co.uk/wp-content/uploads/2019/11/78F.jpg 1600w, https://www.ibm360.co.uk/wp-content/uploads/2019/11/78F-300x225.jpg 300w, https://www.ibm360.co.uk/wp-content/uploads/2019/11/78F-768x576.jpg 768w, https://www.ibm360.co.uk/wp-content/uploads/2019/11/78F-1024x768.jpg 1024w" sizes="(max-width: 1600px) 100vw, 1600px"></a>
       </p>
       	<div>
			<p>Well, where to start! The last few weeks have consisted primarily of anticipation, excitement, and towards the end, utter exhaustion. But finally, FINALLY, the IBM is home! It all started after our friend John Oates at the register shared our plight as detailed in our last blog post (see here). This was read by a chap in Melbourne, Australia called Kevin Silk, who then saw a LinkedIn post by a chap called Dan Apperley and left a comment: I had no idea any of this was going on until the following day when I received a call in my office just after 9am. The caller introduced himself as Dan and said he was from a company called Sunspeed. Dan told me that he had read…</p>
<p><a href="https://www.ibm360.co.uk/?p=485">Continue Reading<span>→</span></a></p>
		</div>

</article><!-- #post-## -->

<article id="post-477">
			<p><a href="https://www.ibm360.co.uk/?p=477"><img width="682" height="455" src="https://www.ibm360.co.uk/wp-content/uploads/2019/10/ezgif-6-300af96f3a06.jpg" alt="" decoding="async" loading="lazy" srcset="https://www.ibm360.co.uk/wp-content/uploads/2019/10/ezgif-6-300af96f3a06.jpg 682w, https://www.ibm360.co.uk/wp-content/uploads/2019/10/ezgif-6-300af96f3a06-300x200.jpg 300w" sizes="(max-width: 682px) 100vw, 682px"></a>
       </p>
       	<div>
			<p>So it’s been a while since we’ve posted anything here, and we apologise for that. We’ve all been rather busy with our respective personal and professional lives and it’s not left much time for us to work on getting the machine back to the UK. We’ve recently begun engaging with various transportation providers and we’re somewhat struggling to find anybody capable and willing to move the machine, but we will persevere! We’re determined to get the machine back before the October 31st Brexit deadline if at all possible, this is especially important as nobody knows what the importing costs will be post Brexit and we don’t want to end up paying thousands more in storage costs in the event of nobody knowing whats going on…</p>
<p><a href="https://www.ibm360.co.uk/?p=477">Continue Reading<span>→</span></a></p>
		</div>

</article><!-- #post-## -->

<article id="post-409">
			<p><a href="https://www.ibm360.co.uk/?p=409"><img width="768" height="382" src="https://www.ibm360.co.uk/wp-content/uploads/2019/05/header10-1.png" alt="" decoding="async" loading="lazy" srcset="https://www.ibm360.co.uk/wp-content/uploads/2019/05/header10-1.png 768w, https://www.ibm360.co.uk/wp-content/uploads/2019/05/header10-1-300x149.png 300w" sizes="(max-width: 768px) 100vw, 768px"></a>
       </p>
       	<div>
			<p>Yesterday Afternoon (21st of May 2019) I received a WhatsApp message from our German Auction house contact (come Photographer extraordinaire), Günter Hiller. It was a series of photographs, firstly of the roof of the machine room. It would seem over the weekend that the roof had leaked significantly right onto where the punched cards and the processor had been stored! It looks like we removed everything just in the absolute nick of time! Included in the photos were bits of IBM equipment, along with some engineering manuals. These had apparently been found in the room behind the room the computer was in, buried under a load of Porsche parts! This came as a surprise to everyone as they had previously been informed these rooms were…</p>
<p><a href="https://www.ibm360.co.uk/?p=409">Continue Reading<span>→</span></a></p>
		</div>

</article><!-- #post-## -->

<article id="post-185">
			<p><a href="https://www.ibm360.co.uk/?p=185"><img width="1024" height="768" src="https://www.ibm360.co.uk/wp-content/uploads/2019/05/28.jpeg" alt="" decoding="async" loading="lazy" srcset="https://www.ibm360.co.uk/wp-content/uploads/2019/05/28.jpeg 1024w, https://www.ibm360.co.uk/wp-content/uploads/2019/05/28-300x225.jpeg 300w, https://www.ibm360.co.uk/wp-content/uploads/2019/05/28-768x576.jpeg 768w" sizes="(max-width: 1024px) 100vw, 1024px"></a>
       </p>
       	<div>
			<p>So, after our last visit we were all ready to get going on the big move. We had already recovered all of the documentation back to the UK on the last visit and we had the ramp constructed which would enable reasonably smooth removal of the machine from the building. Chris and I departed for London Stansted in the late afternoon of the Wednesday (15th of May) with cabin bags containing our clothes and 2x 20kg suitcases full of things we thought we might need (e.g. ratchet straps, cloths, lights, tools etc.). We got settled into our flight and soon enough we had touched down at Nuremberg. We briefly debated taking the U-Bahn and a tram to our AirBNB but soon decided against it as…</p>
<p><a href="https://www.ibm360.co.uk/?p=185">Continue Reading<span>→</span></a></p>
		</div>

</article><!-- #post-## -->

	
</main><!-- #main --></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[20 Years in the Making, GnuCOBOL Is Ready for Industry (140 pts)]]></title>
            <link>https://thenewstack.io/20-years-in-the-making-gnucobol-is-ready-for-industry/</link>
            <guid>39728519</guid>
            <pubDate>Sat, 16 Mar 2024 19:02:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thenewstack.io/20-years-in-the-making-gnucobol-is-ready-for-industry/">https://thenewstack.io/20-years-in-the-making-gnucobol-is-ready-for-industry/</a>, See on <a href="https://news.ycombinator.com/item?id=39728519">Hacker News</a></p>
Couldn't get https://thenewstack.io/20-years-in-the-making-gnucobol-is-ready-for-industry/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Hackintosh Is Almost Dead (425 pts)]]></title>
            <link>https://aplus.rs/2024/hackintosh-almost-dead/</link>
            <guid>39728146</guid>
            <pubDate>Sat, 16 Mar 2024 18:16:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aplus.rs/2024/hackintosh-almost-dead/">https://aplus.rs/2024/hackintosh-almost-dead/</a>, See on <a href="https://news.ycombinator.com/item?id=39728146">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p><a href="https://aplus.rs/categories/hardware">hardware</a></p>

    

<p>It was a good run, though.</p>
    
    <p>
    <span>Mar 16, 2024</span>
    
    by 
        
    <span>6 minute read</span>
    
    </p>
    <p>While I knew about and even tried <a href="https://aplus.rs/2004/mac-os-xhappily-running-on-my-amd-athlon/">various very early attempts</a> to run macOS on non-Apple hardware, it wasn’t until <a href="https://aplus.rs/2020/hmac/">early 2020</a> that I’ve built my first proper one. Then I built several more which are still seeing daily use.</p>
<p>I <a href="https://aplus.rs/2020/missing-developer-mac/">explained my reasoning</a> why it was worthwhile to attempt it. The technology was mostly there thanks to a group of dedicated hackers and timing <a href="https://aplus.rs/2020/hmac-2020/">was just right</a>:</p>
<blockquote>
<p>But if ever there was a time to do it, it’s now.<br>
Apple is transitioning to their own CPUs/GPUs over the next two years. Several years from now, I see myself purchasing whatever desktop Apple Silicon-based machine is there.</p>
</blockquote>
<p>I also offered a prognosis which turned out partially true:</p>
<blockquote>
<p>Many will tell you that buying Intel-based hardware from Apple is buying obsolete models. I don’t really agree with that since it’s a given that those Intel-based Macs will be supported for 7-10 years of future macOS updates.</p>
</blockquote>
<p>It’s true that latest macOS 14 (Sonoma) still supports the latest generations of Intel Macs and it’s very likely that at least one or two major versions will still be compatible. But there’s one particular development that is de-facto killing off the Hackintosh scene.</p>
<p>In Sonoma, <strong>Apple has completely removed all traces of driver support for their oldest WiFi/Bt cards</strong>, namely various Broadcom cards that they last used in 2012/13 iMac / MacBook models. Those Mac models are not supported by macOS for few years now thus it’s not surprising the drivers are being removed. Most likely reason is that Apple is moving drivers away from <code>.kext</code> (Kernel Extensions) to <code>.dext</code> (DriverKit) thus cleaning up obsolete and unused code from macOS. They did the same with Ethernet drivers in Ventura.</p>
<p>Those particular cards were the key ingredient to many fully functional Hackintosh builds for simple reason: they worked out of the box with every single (so-called) iService Apple has: Messages, FaceTime, AirDrop, Continuity, Handoff - you name it. <em>Everything worked.</em> Despite the <a href="https://dortania.github.io/OpenCore-Legacy-Patcher/SONOMA-DROP.html#versioning">valiant efforts of OCLP crew</a> to make workarounds, those cards can work in Sonoma <em>only</em> if you <a href="https://github.com/perez987/Broadcom-wifi-back-on-macOS-Sonoma-by-OCLP">seriously downgrade</a> the macOS security.</p>
<p>There was some hope that <a href="https://github.com/OpenIntelWireless/itlwm">OpenIntelWireless</a> could replace those cards due to <a href="https://github.com/OpenIntelWireless">amazing work</a> zxystd did in the last 4 years. I mean, the WiFi speeds in macOS with Intel’s WiFi6 cards are <a href="https://forum.amd-osx.com/threads/wi-fi-speed-test-intel-ax200-vs-bcm94360ng.4756/">nothing short of spectacular</a>. But Apple’s continued cleanup and rewrite of their driver stack has pretty much killed-off any reliable support for Message and FaceTime despite iCloud sync still working great. zxystd <a href="https://github.com/OpenIntelWireless/itlwm/issues/953#issuecomment-1920759538">describes the new mountain to climb</a>:</p>
<blockquote>
<p>From Sonoma, Apple drops IO80211FamilyLegacy, I build AirportItlwmV2 on the top of IO80211Family, but using some hacks, you can simply interpret it as me implementing a set of IO80211FamilyLegacy myself. <em>This implementation may have side effects such as the iService not working etc</em>. Since IO80211Family uses skywalk API instead of original Ethernet API (Also we can foresee that the Ethernet API will also be dropped in macOS 15), without these hacks we should follow the Apple’s API and <em>rewrite the whole driver</em>, that’s what I would never do.</p>
</blockquote>
<p>In 14.4, Apple <a href="https://www.tonymacx86.com/threads/usb-problems-after-14-4-update.329275/">seem to have made changes</a> in how USB subsystem works too. This was always a tedious challenge but if minor updates can almost <a href="https://forum.amd-osx.com/threads/update-from-14-3-1-to-14-4.5013/">brick the build</a> it becomes a headache. Still…USB is a known problem with known solution thus it’s annoying but solvable.</p>
<p>WiFi with iServices is sadly not.</p>
<p>I’ve long held the opinion that it would not be CPU nor GPU changes that kill the Hacks — it would be lack of reliable WiFi drivers. And now, ~4 years later, Hackintosh hits a brick wall of no easy WiFi options available, at all.  Given how much of the macOS useful features is dependent on presence of particular WiFi chips — a decision of Apple developers I really can’t understand — I can’t really consider builds without those features to call themselves Mac.</p>
<p>I did not come to this conclusion just by reading the forums.</p>
<p>I have a rag-tag build sitting on my desk for several months now. It was supposed to be a quick proof-of-concept Sonoma build with Intel AX200 WiFi/Bt, AMD CPU and GPU, NVMe SSDs - everything that modern Mac should work with. It’s everything that my current Hackintosh is, with SIP intact, incremental updates working on their own etc — a perfect Mac.</p>
<p><img src="https://aplus.rs/images/2024/rag-tag-build.jpg" alt="My wannabe Sonoma-compatible Mac"></p>
<p>But pretty much since day one I encountered one problem after another. Things were so volatile and random that it was hard to believe, at times. Like —</p>
<ul>
<li>One day Ethernet controller (Intel I225-V) would work great, the next day it would just hard-crash the entire machine. No freaking idea why. Tried multiple ways and custom drivers to make it work but nothing was perfectly stable.</li>
<li>WiFi works fantastic, iCloud is perfect but Messages/FaceTime wouldn’t connect at all. In either Monterey, Ventura or Sonoma. That same card worked perfectly on another motherboard with Monterey and Ventura, no issues with Messages / FaceTime at all. Again — no idea why.</li>
<li>Bluetooth would work great for days but if I turn it off and restart the machine, something would become so messed up that it starts being recognised as BCM_4350C2 chip instead of Intel AX200. Only a round-trip to Windows 11 would somehow bring the chip into a state that <a href="https://github.com/OpenIntelWireless/IntelBluetoothFirmware">IntelBluetooth driver</a> can work with it.</li>
<li>Sonoma 14.3.1 works great on this build. But 14.4 update won’t install. It starts booting the installer and just reboots back almost immediately.</li>
</ul>
<p>Hence — <strong>Hackintosh is on its death bed</strong>. Some things will work for few more months or maybe even years, depending on what you use it for and wether lack of WiFi bothers you or not. But not for me. I can live without AirDrop, Continuity and Handoff but Messages and FaceTime must work. There’re also some other things Sonoma brings that are important to me thus I want to update to it. Coupled with described lack of reliability and fretting if next minor or major update would leave me dry — nah, not worth it.</p>
<p>I don’t really complain. I had a good run which helped me skip over the worst price/performance Mac lineup that I remember. There’re now plenty good choices within the current crop of M1 / M2 / M3 machines and I’ll be following eBay closely for a good used Mac mini / studio models. Or maybe even splurge on something new.</p>
<p>Lest I forget — if macOS Ventura works for you, stay on it! That’s still perfectly stable without a single issue across a variety of build options.</p>
<hr>
<p>Just to clarify one thing, to preempt someone saying Apple did this on purpose to kill off Hackintosh: <strong>they didn’t</strong>. Apple never cared about Hackintosh scene, it’s entirely irrelevant to their business. They did what they should be doing, improving the macOS codebase. It’s always a good thing to remove obsolete and deprecated code thus Apple is doing the right thing for their product.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Research shows that people who BS are more likely to fall for BS (2021) (129 pts)]]></title>
            <link>https://uwaterloo.ca/news/media/research-shows-people-who-bs-are-more-likely-fall-bs</link>
            <guid>39727529</guid>
            <pubDate>Sat, 16 Mar 2024 17:01:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://uwaterloo.ca/news/media/research-shows-people-who-bs-are-more-likely-fall-bs">https://uwaterloo.ca/news/media/research-shows-people-who-bs-are-more-likely-fall-bs</a>, See on <a href="https://news.ycombinator.com/item?id=39727529">Hacker News</a></p>
<div id="readability-page-1" class="page"><div property="content:encoded"><p><span><span>People who frequently try to impress or persuade others with misleading exaggerations and distortions are themselves more likely to be fooled by impressive-sounding misinformation, new research from the University of Waterloo shows.&nbsp;</span></span></p>

<p><span><span>The researchers found that people who frequently engage in “persuasive bullshitting” were actually quite poor at identifying it. Specifically, they had trouble distinguishing intentionally profound or scientifically accurate fact from impressive but meaningless fiction. Importantly, these frequent BSers are also much more likely to fall for fake news headlines.</span></span></p>

<p><span><span>“It probably seems intuitive to believe that you can’t bullshit a bullshitter, but our research suggests that this isn’t actually the case,” says Shane Littrell, lead author of the paper and cognitive psychology PhD candidate at Waterloo. “In fact, it appears that the biggest purveyors of persuasive bullshit are ironically some of the ones most likely to fall for it.”</span></span></p>

<p><span><span>The researchers define “bullshit” as information designed to impress, persuade, or otherwise mislead people that is often constructed without concern for the truth. They also identify two types of bullshitting— persuasive and evasive.&nbsp;“Persuasive” uses misleading exaggerations and embellishments to impress, persuade, or fit in with others, while ‘evasive’ involves giving irrelevant, evasive responses in situations where frankness might result in hurt feelings or reputational harm.&nbsp;</span></span></p>

<p><span><span>In a series of studies conducted with over 800 participants from the US and Canada, the researchers examined the relations between participants’ self-reported engagement in both types of BSing and their ratings of how profound, truthful, or accurate they found pseudo-profound and pseudo-scientific statements and fake news headlines. Participants also completed measures of cognitive ability, metacognitive insight, intellectual overconfidence, and reflective thinking.</span></span></p>

<p><span><span>“We found that the more frequently someone engages in persuasive bullshitting, the more likely they are to be duped by various types of misleading information regardless of their cognitive ability, engagement in reflective thinking, or metacognitive skills,” Littrell said. “Persuasive BSers seem to mistake superficial profoundness for actual profoundness. So, if something simply sounds profound, truthful, or accurate to them that means it really is. But evasive bullshitters were much better at making this distinction.”&nbsp;</span></span></p>

<p><span><span>The research may help shed light on the processes underlying the spread of some types of misinformation, which could have important implications for the fight against this growing problem.&nbsp;</span></span></p>

<p>The study, You can’t bullshit a bullshitter (or can you?): Bullshitting frequency predicts receptivity to various types of misleading information, authored by Littrell and Waterloo’s Faculty of Arts professors Evan Risko and Jonathan Fugelsang, appears in the <em>British Journal of Social Psychology</em>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Parsing URLs in Python (123 pts)]]></title>
            <link>https://tkte.ch/articles/2024/03/15/parsing-urls-in-python.html</link>
            <guid>39727458</guid>
            <pubDate>Sat, 16 Mar 2024 16:53:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tkte.ch/articles/2024/03/15/parsing-urls-in-python.html">https://tkte.ch/articles/2024/03/15/parsing-urls-in-python.html</a>, See on <a href="https://news.ycombinator.com/item?id=39727458">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
      <p>tl;dr - Try <a href="https://github.com/tktech/can_ada">can_ada</a> if you need to parse URLs in Python.</p>
<h2>URLs</h2>
<p>Parsing URLs <em>correctly</em> is surprisingly hard. Who even defines what a "correct"
URL is? URLs have evolved drastically since they were originally defined in
<a href="https://tools.ietf.org/html/rfc1738">1994</a>. The <a href="https://whatwg.org/">WHATWG</a> has a <a href="https://url.spec.whatwg.org/">URL specification</a> that
is comprehensive and has helped standardize the behavior of URLs across
browsers, but this specification still isn't universal and ambiguities like
"how many slashes are you allowed" can definitely get on <a href="https://daniel.haxx.se/blog/2017/01/30/one-url-standard-please">your nerves</a>:</p>
<blockquote cite="Daniel Stenberg">
    So browsers accept URLs written with thousands of forward slashes instead of
    two. That is not a good reason for the spec to say that a URL may
    legitimately contain a thousand slashes. I’m totally convinced there’s no
    critical content anywhere using such formatted URLs and no soul will be sad
    if we’d restricted the number to a single-digit. So we should. And yeah,
    then browsers should reject URLs using more.
</blockquote>

<p><em>However</em>, if you're creating something new and want to handle URLs, the
WHATWG's URL specification probably is the best place to start.</p>
<h2>URLs in Python</h2>
<p>If you're working in Python, you'd probably start with the built-in <code>urllib</code>
module. It's been around forever, but unfortunately it's not compliant with
<em>any</em> URL specification, either the much older <a href="https://tools.ietf.org/html/rfc3986">rfc3978</a>:</p>
<blockquote cite="CPython source code">
    RFC 3986 is considered the current standard and any future changes to
    urlparse module should conform with it. The urlparse module is
    currently not entirely compliant with this RFC due to defacto
    scenarios for parsing, and for backward compatibility purposes, some
    parsing quirks from older RFCs are retained. The testcases in
    test_urlparse.py provides a good indicator of parsing behavior.
</blockquote>

<p>... or the WHATWG URL Parser spec:</p>
<blockquote cite="CPython source code">
    The WHATWG URL Parser spec should also be considered. We are not compliant
    with it either due to existing user code API behavior expectations (Hyrum's
    Law). It serves as a useful guide when making changes.
</blockquote>

<p>Having existed for over 16 years, so many projects depend on the urllib module
parsing URLs in <em>exactly</em> the way it does that it's unlikely to ever change in
any significant fashion.</p>
<h2>Ada</h2>
<p>The <a href="https://github.com/ada-url/ada">Ada</a> project is a new (2024) attempt to create a URL parsing library
that adheres to the WHATWG URL specification and works <a href="https://github.com/ada-url/ada?tab=readme-ov-file#ada-is-fast">really, really fast</a>,
parsing 7 URLs for every 1 parsed by cURL. Written in C++, it now has bindings
to several other languages, including <a href="https://github.com/ada-url/ada-python">Python</a>, and has become the
URL parsing library used by Node.js as of version 18 to great success:</p>
<blockquote cite="State of Node.js Performance 2023">
    Since Node.js 18, a new URL parser dependency was added to Node.js — Ada.
    This addition bumped the Node.js performance when parsing URLs to a new
    level. Some results could reach up to an improvement of 400%.
</blockquote>

<p>The ada-python binding is perfectly functional, and the official binding for the
project. However, the ada-python bindings are built on <a href="https://cffi.readthedocs.io/en/latest/">CFFI</a>, an approach
that has the binding between C and Python written in Python itself, which loses
some of the performance benefits of using Ada in the first place when most of
the time is spent just making the function call.</p>
<h2>can_ada</h2>
<p><a href="https://lemire.me/">Daniel Lemire</a>, one of the developers behind the Ada project asked me to
<a href="https://github.com/ada-url/ada-python/pull/1#issuecomment-1550405501">take a look</a> at the ada-python bindings and out of that was born
<a href="https://github.com/tktech/can_ada">can_ada</a>, a new Python binding that uses <a href="https://pybind11.readthedocs.io/en/stable/">pybind11</a> and template magic
to generate the binding code, which is then compiled into a Python extension
module. This approach has the potential to be much faster than the ada-python
bindings, and indeed when comparing the two bindings, the new can_ada binding
is about 2x faster than the ada-python bindings which in turn is about 2x
faster than <code>urllib.parse</code>!</p>
<div><pre><span></span><code><span>---------------------------------------------------------------------------------</span>
<span>Name (time in ms)              Min                 Max                Mean       </span>
<span>---------------------------------------------------------------------------------</span>
<span>test_can_ada_parse         54</span><span>.</span><span>1304 (1</span><span>.</span><span>0)       54</span><span>.</span><span>6734 (1</span><span>.</span><span>0)       54</span><span>.</span><span>3699 (1</span><span>.</span><span>0) </span>
<span>test_ada_python_parse     107</span><span>.</span><span>5653 (1</span><span>.</span><span>99)     108</span><span>.</span><span>1666 (1</span><span>.</span><span>98)     107</span><span>.</span><span>7817 (1</span><span>.</span><span>98)</span>
<span>test_urllib_parse         251</span><span>.</span><span>5167 (4</span><span>.</span><span>65)     255</span><span>.</span><span>1327 (4</span><span>.</span><span>67)     253</span><span>.</span><span>2407 (4</span><span>.</span><span>66)</span>
<span>---------------------------------------------------------------------------------</span>
</code></pre></div>

<p>At the same time, using the pybind11 approach allows for a very succinct and
<a href="https://github.com/TkTech/can_ada/blob/main/src/binding.cpp">readable</a> binding definition, coming in at just <strong>60</strong> lines of code and
almost a 1:1 with the underlying C++ API.</p>
<p>Binary builds are available now for CPython 3.7 to 3.12, and PyPy 3.7 to 3.9 on
many OS's and architectures. Install it with pip or <a href="https://github.com/tktech/can_ada">get the source</a>:</p>


<h2>Example</h2>
<div><pre><span></span><code><span>import</span> <span>can_ada</span>
<span>urlstring</span> <span>=</span> <span>"https://www.GOoglé.com/./path/../path2/"</span>
<span>url</span> <span>=</span> <span>can_ada</span><span>.</span><span>parse</span><span>(</span><span>urlstring</span><span>)</span>
<span># prints www.xn--googl-fsa.com, the correctly parsed domain name according</span>
<span># to WHATWG</span>
<span>print</span><span>(</span><span>url</span><span>.</span><span>hostname</span><span>)</span>
<span># prints /path2/, which is the correctly parsed pathname according to WHATWG</span>
<span>print</span><span>(</span><span>url</span><span>.</span><span>pathname</span><span>)</span>
</code></pre></div>

<p>Compare this to the urllib version, which is not WHATWG compliant:</p>
<div><pre><span></span><code><span>import</span> <span>urllib.parse</span>
<span>urlstring</span> <span>=</span> <span>"https://www.GOoglé.com/./path/../path2/"</span>
<span>url</span> <span>=</span> <span>urllib</span><span>.</span><span>parse</span><span>.</span><span>urlparse</span><span>(</span><span>urlstring</span><span>)</span>
<span># prints www.googlé.com</span>
<span>print</span><span>(</span><span>url</span><span>.</span><span>hostname</span><span>)</span>
<span># prints /./path/../path2/</span>
<span>print</span><span>(</span><span>url</span><span>.</span><span>path</span><span>)</span>
</code></pre></div>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Flash Attention in ~100 lines of CUDA (209 pts)]]></title>
            <link>https://github.com/tspeterkim/flash-attention-minimal</link>
            <guid>39726781</guid>
            <pubDate>Sat, 16 Mar 2024 15:31:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/tspeterkim/flash-attention-minimal">https://github.com/tspeterkim/flash-attention-minimal</a>, See on <a href="https://news.ycombinator.com/item?id=39726781">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">flash-attention-minimal</h2><a id="user-content-flash-attention-minimal" aria-label="Permalink: flash-attention-minimal" href="#flash-attention-minimal"></a></p>
<p dir="auto">A minimal re-implementation of Flash Attention with CUDA and PyTorch.
The official <a href="https://github.com/Dao-AILab/flash-attention">implementation</a> can be quite daunting for a CUDA beginner
(like myself), so this repo tries to be small and educational.</p>
<ul dir="auto">
<li>The entire forward pass is written in ~100 lines in <code>flash.cu</code>.</li>
<li>The variable names follow the notations from the original <a href="https://arxiv.org/abs/2205.14135" rel="nofollow">paper</a>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prerequisite</h3><a id="user-content-prerequisite" aria-label="Permalink: Prerequisite" href="#prerequisite"></a></p>
<ul dir="auto">
<li>PyTorch (with CUDA)</li>
<li><code>Ninja</code> for loading in C++</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Benchmark</h3><a id="user-content-benchmark" aria-label="Permalink: Benchmark" href="#benchmark"></a></p>
<p dir="auto">Compare the wall-clock time between manual attention and minimal flash attention:</p>

<p dir="auto">Sample output on a <a href="https://aws.amazon.com/ec2/instance-types/g4/" rel="nofollow">T4</a>:</p>
<div data-snippet-clipboard-copy-content="=== profiling manual attention ===
...
Self CPU time total: 52.389ms
Self CUDA time total: 52.545ms

=== profiling minimal flash attention === 
...  
Self CPU time total: 11.452ms
Self CUDA time total: 3.908ms"><pre><code>=== profiling manual attention ===
...
Self CPU time total: 52.389ms
Self CUDA time total: 52.545ms

=== profiling minimal flash attention === 
...  
Self CPU time total: 11.452ms
Self CUDA time total: 3.908ms
</code></pre></div>
<p dir="auto">Speed-up achieved!</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">I don't have a GPU</h3><a id="user-content-i-dont-have-a-gpu" aria-label="Permalink: I don't have a GPU" href="#i-dont-have-a-gpu"></a></p>
<p dir="auto">Try out this <a href="https://colab.research.google.com/gist/tspeterkim/143bc7be7a845656817cf94c5228598e/demo-flash-attention-minimal.ipynb" rel="nofollow">online colab demo</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Caveats</h2><a id="user-content-caveats" aria-label="Permalink: Caveats" href="#caveats"></a></p>
<ul dir="auto">
<li>No backward pass! To be honest, I found it a lot more complex than the forward pass, which was enough to show the
use of shared memory to avoid large N^2 read/writes.</li>
<li>In the inner loop, I assign each thread to a row of the output matrix. This differs from the original implementation.</li>
<li>This thread-per-row simplification makes the matrix multiplications very slow. This is probably why for longer
sequences and larger block sizes, this gets slower than the manual implementation.</li>
<li>Q,K,Vs are in float32, unlike the original implementation which uses float16.</li>
<li>The block size is <a href="https://github.com/tspeterkim/flash-attention-minimal/blob/9b7ca8ef4e6afdbfeb149a9cd488c8dea9af9ad6/flash.cu#L85">fixed</a> at compile time to 32.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Todos</h2><a id="user-content-todos" aria-label="Permalink: Todos" href="#todos"></a></p>
<ul>
<li> Add backward pass</li>
<li> Speed up matmults</li>
<li> Dynamically set block size</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[macOS 14.4 causes JVM crashes (169 pts)]]></title>
            <link>https://blogs.oracle.com/java/post/java-on-macos-14-4</link>
            <guid>39726292</guid>
            <pubDate>Sat, 16 Mar 2024 14:32:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blogs.oracle.com/java/post/java-on-macos-14-4">https://blogs.oracle.com/java/post/java-on-macos-14-4</a>, See on <a href="https://news.ycombinator.com/item?id=39726292">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

            <!-- RC84v1 -->
            <section>

                 <p><span><span><span><span><span>An issue introduced by macOS 14.4, which causes Java process to terminate unexpectedly, is affecting all Java versions from Java 8 to the early access builds of JDK 22. There is no workaround available, and since there is no easy way to revert a macOS update, affected users might be unable to return to a stable configuration unless they have a complete backup of their systems prior to the OS update.</span></span></span></span></span></p>

<p><span><span><span><span><span>The issue was not present in the early access releases for macOS 14.4, so it was discovered only after Apple released the update.</span></span></span></span></span></p>

<p><span><span><span><span><span>macOS on Apple silicon processors (M1, M2, and M3) includes a feature which controls how and when dynamically generated code can be either produced (written) or executed on a per-thread basis.&nbsp;</span></span></span></span></span></p>

<p><span><span><span><span>As a normal part of the just-in-time compile and execute cycle</span></span><span><span>, processes running on macOS may access memory in protected memory regions. Prior to the macOS 14.4 update, in certain circumstances, the macOS kernel would respond to these protected memory accesses by sending a signal,&nbsp;</span></span><span><span><span><span>SIGBUS</span></span></span></span><span><span>&nbsp;or&nbsp;</span></span><span><span><span><span>SIGSEGV</span></span></span></span><span><span>, to the process. The process could then choose to handle the signal and continue execution.&nbsp; With macOS 14.4, when a thread is operating in the write mode, if a memory access to a protected memory region is attempted, macOS will send the signal&nbsp;</span></span><span><span><span><span>SIGKILL </span></span></span></span><span><span>instead. That signal cannot be handled by the process and the process is unconditionally terminated. </span></span></span></span></p>

<p><span><span><span><span><span>The Java Virtual Machine generates code dynamically and leverages the protected memory access signal mechanism both for correctness (e.g., to handle the truncation of memory mapped files) and for performance. With macOS 14.4, programs that attempt this will now terminate instead of having the opportunity to handle the signal.</span></span></span></span></span></p>

<p><span><span><span><span><span>Ahead-of-Time compiled applications created with GraalVM Native Image should not be affected, but your ability to build new images may be.</span></span></span></span></span></p>

<p><span><span><span><span><span>Oracle has notified its customers, Apple, and our partners in OpenJDK of this situation. We recommend that users of Java on ARM-based Apple devices running macOS 14 delay applying the update until this issue is resolved.</span></span></span></span></span></p>

<p><span><span><span><span><span>The issue can be tracked on&nbsp;</span></span><span><a href="https://bugs.java.com/bugdatabase/view_bug?bug_id=8327860" title="https://bugs.java.com/bugdatabase/view_bug?bug_id=8327860"><span><span>bugs.java.com with bugID JDK-8327860</span></span></a></span></span></span></span></p>


            </section>
            <!-- /RC84v1 -->

            <!-- RC84v2 -->
            <div>
                        <p><img src="https://blogs.oracle.com/content/published/api/v1.1/assets/CONT8FA109D0B313456BA3072245A6090E81/Thumbnail?cb=_cache_a30&amp;channelToken=7e01516d535048508dfbd81a6ea0d1ed&amp;format=jpg" alt="">
                        </p>
                        <div>
                                <h4>Aurelio Garcia-Ribeyro</h4>
                                <h5>Senior Director of Product Management</h5>

                                <p>Aurelio&nbsp;has been involved in the development of the JDK since JDK 7. &nbsp;He is a frequent presenter at JavaOne, Oracle Code One, and with Java User Groups and Oracle Customers. &nbsp;Aurelio's role includes making sure that Java users, within and outside of Oracle, are well informed of changes as well as to present the most relevant features and enhancements in upcoming releases. He has received a JavaOne Rock Star Award.</p>
<p>Aurelio joined Oracle in 2010 through the Sun Microsystems acquisition.&nbsp;</p>
                            </div>
                    </div>
            <!-- /RC84v2 -->


        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Y Combinator's chief startup whisperer is demoting himself (120 pts)]]></title>
            <link>https://www.wired.com/story/plaintext-y-combinator-michael-seibel-startup-whisperer/</link>
            <guid>39725678</guid>
            <pubDate>Sat, 16 Mar 2024 13:24:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/plaintext-y-combinator-michael-seibel-startup-whisperer/">https://www.wired.com/story/plaintext-y-combinator-michael-seibel-startup-whisperer/</a>, See on <a href="https://news.ycombinator.com/item?id=39725678">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>When Michael Seibel lost his position at the startup incubator Y Combinator, he didn’t find out in <a href="https://www.wired.com/story/plaintext-alphabets-layoffs-arent-very-googley/">typical tech industry fashion</a>, which might entail an email calling him to a Zoom meeting where the bad news would be delivered. He did it to himself. Today Seibel is announcing that he’s stepping down as YC’s managing director, a job that entailed running the heart of the business: selecting startup founders for the three-month program and running the boot-camp-style operation that hones the vision and execution of their ideas so they can raise money, release products, and attempt to become the next Airbnb or Stripe (both YC alumni).</p><p>Considering how important YC has been to the tech startup ecosystem, Seibel’s exit will have more resonance than your average corporate reshuffle. For one thing, the person who runs YC’s blue-chip accelerator has a significant hand in shaping the next generation of tech companies. And in recent months, YC has found itself in the crossfire of a war between tech and progressives. Whether intentional or not, Seibel, a well-liked entrepreneur and investor himself, is deftly stepping out of the line of fire.</p><p>Seibel explains the move as a more personal decision. Sometime last year he began to take stock, spurred in part by reading <a data-offer-url="https://www.amazon.com/Strength-Finding-Success-Happiness-Purpose/dp/059319148X" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.amazon.com/Strength-Finding-Success-Happiness-Purpose/dp/059319148X&quot;}" href="https://www.amazon.com/Strength-Finding-Success-Happiness-Purpose/dp/059319148X" rel="noopener" target="_blank"><em>Strength to Strength</em></a>, a book about career arcs, particularly pivots made late in life. He’s only 41, but precociousness is part of the founder mindset, and he’d been a startup CEO at 23. “I do everything early,” he says.</p><div data-testid="GenericCallout"><figure><p><span>Michael Seibel</span><span>Courtesy of Y Combinator</span></p></figure></div><p>He realized that he had been running batches for as long as the person who first imagined YC into being, <a href="https://www.wired.com/story/how-y-combinator-changed-the-world/">Paul Graham</a>. After Covid waned, YC had returned to an in-person experience, and the software that it had developed to smooth the remote Covid-era program made an IRL operation easier to manage. Now the program works by splitting each batch of new startups into four groups, none larger than <a href="https://www.newscientist.com/definition/dunbars-number/">Dunbar’s Number</a> of 150, estimated to be the maximum number of relationship’s a human brain can properly maintain. Each group has its own leader, so YC had less need for someone to oversee each cohort as a whole. And though Seibel enjoyed managing the overall program, he much preferred direct contact with company founders. So he will now become one of those four group leaders, who each mentor a quarter of the batch. It’s a particularly exciting time to do that, Seibel says, as many of the companies hinge on the AI boom.</p><p>Close observers of YC—and many in the startup ecosystem monitor the accelerator with the diligence of a behavior-tracking ad network—might wonder whether Seibel’s move might have something to do with his being passed over for the leadership of the entire operation. <a href="https://www.forbes.com/sites/alexkonrad/2024/03/08/inside-garry-tan-plan-restore-y-combinator-silicon-valley-glory/?sh=709b797b2103">Forbes has reported</a> that he was disappointed not to be tapped as CEO after the incubator’s president, Geoff Ralston, who had taken over when Sam Altman went full time leading OpenAI, left at the end of 2022. Ralston was replaced by YC’s former design guru, Garry Tan. Seibel tells me he did not feel dissed, though he would have accepted the job if offered. “If it was something that people thought was going to be the right thing, I was happy to do it. If not, I was more than happy to not,” he says. “My whole goal was to do whatever YC needed for me.”</p><p>Seibel’s self-demotion seems to be in keeping with a recent rethinking at Y Combinator: a refocusing toward a scrappy, boots-on-the-ground startup accelerator as it was under its initial leader and cofounder Graham. His successor, Altman, started a sprawling research operation that, among other things, launched OpenAI. Ralston had his own dreams, and YC started a continuity fund to enable it to make later-stage investments into maturing startups. Ralston was also enamored with scale. The Winter 2022 batch included 412 companies, each funded by the traditional seed investment from YC. <a data-offer-url="https://www.ycombinator.com/blog/ycs-500-000-standard-deal" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.ycombinator.com/blog/ycs-500-000-standard-deal&quot;}" href="https://www.ycombinator.com/blog/ycs-500-000-standard-deal" rel="noopener" target="_blank">Ralston boosted</a> that initial slug of capital from $125,000 to $500,000 per company, for a 7 percent stake. When <a href="https://www.wired.com/story/how-y-combinator-changed-the-world/">I last asked him</a> whether there was a limit to how many startups YC could accommodate in each batch, Ralston said there wasn’t. It was possible, he believed, for a batch to number “thousands” of startups.</p><p>Under Tan, who took over in January 2023, there’s been a refocus on the founders themselves. Tan says YC had become kind of an umbrella company saying yes to a lot of things. “I asked, ‘How do we focus on what made YC awesome in the first place?’” The answer was mentoring cool founders, chosen through an exacting application process. The continuity fund <a data-offer-url="https://www.theinformation.com/articles/y-combinator-to-end-late-stage-startup-continuity-fund" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.theinformation.com/articles/y-combinator-to-end-late-stage-startup-continuity-fund&quot;}" href="https://www.theinformation.com/articles/y-combinator-to-end-late-stage-startup-continuity-fund" rel="noopener" target="_blank">was discontinued</a>. YC had already <a data-offer-url="https://www.openresearchlab.org/blog/we-are-changing-our-name" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.openresearchlab.org/blog/we-are-changing-our-name&quot;}" href="https://www.openresearchlab.org/blog/we-are-changing-our-name" rel="noopener" target="_blank">separated itself</a> <a data-offer-url="https://www.ycombinator.com/blog/yc-research" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.ycombinator.com/blog/yc-research&quot;}" href="https://www.ycombinator.com/blog/yc-research" rel="noopener" target="_blank">f</a>rom Altman’s research division, which is now called Open Research. The only remaining trace of Altman’s research operation within the company now is a financial stake in OpenAI. Most notably, batch sizes have been cut almost in half. Beginning Summer 2022, they numbered in the mid 200’s, with the current batch inching up to 260. This isn’t due to demand—27,000 companies applied for those slots.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cloudflare loses 22% of its domains in Freenom .tk shutdown (292 pts)]]></title>
            <link>https://www.netcraft.com/blog/cloudflare-loses-22-of-its-domains-in-freenom-tk-shutdown/</link>
            <guid>39725303</guid>
            <pubDate>Sat, 16 Mar 2024 12:24:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.netcraft.com/blog/cloudflare-loses-22-of-its-domains-in-freenom-tk-shutdown/">https://www.netcraft.com/blog/cloudflare-loses-22-of-its-domains-in-freenom-tk-shutdown/</a>, See on <a href="https://news.ycombinator.com/item?id=39725303">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
			
<p>A staggering 12.6 million domains on TLDs controlled by <a href="https://www.freenom.com/en/index.html?lang=en">Freenom</a> (<strong>.tk</strong>, <strong>.cf</strong> and <strong>.gq</strong>) have been shut down and no longer resolve, leading to a significant reduction in the number of websites hosted by <a href="https://www.cloudflare.com/en-gb/">Cloudflare</a>.</p>



<p>The disappearance of these websites was spotted during our monthly Web Server Survey and represents a 98.7% drop from the number of Freenom domains that were resolvable last month.</p>



<figure><img fetchpriority="high" decoding="async" width="2263" height="1305" src="https://www.netcraft.com/wp-content/uploads/2024/03/freenom.svg" alt=""></figure>



<p>Nearly all <strong>.tk</strong>, <strong>.cf</strong> and <strong>.gq</strong> domains have effectively disappeared.</p>



<p>The <strong>.tk</strong>, <strong>.cf</strong> and <strong>.gq</strong> TLDs are country code top-level domains (ccTLDs) for Tokelau, Central African Republic, and Equatorial Guinea. They were officially intended to be used by entities connected with these countries, but this was very rarely the case.</p>



<p>The huge drop is likely the culmination of a series of events that started last year, when <a href="https://www.netcraft.com/blog/impact-of-freenom-halting-registrations-on-cybercrime/">Freenom was sued by Meta for ignoring abuse complaints</a>. Freenom subsequently paused new domain registrations in March 2023, and Netcraft noticed a dramatic reduction in the amount of cybercrime across two TLDs that later moved away from the provider (<strong>.ga</strong> and <strong>.ml</strong>).</p>



<p>Finally, on 12 February 2024, Freenom announced that it had decided to exit the domain name business, including the operation of registries. The same press release (which has since been removed but is archived <a href="https://web.archive.org/web/20240219215920/https://www.freenom.com/en/freenom_pressstatement_02122024_v0100.pdf">here</a>) also announced that Freenom had resolved the Meta lawsuit on confidential monetary and business terms.</p>



<h3>Cloudflare losses</h3>



<p>The affected domains represent a big loss for <a href="https://www.cloudflare.com/en-gb/">Cloudflare</a>, with <strong>.tk</strong>, <strong>.cf</strong> and <strong>.gq</strong> previously accounting for 23.1% of all domains hosted on its platform – and nearly all of these have now gone.</p>



<p>The combined amount of <strong>.tk</strong>, <strong>.cf</strong> and <strong>.gq</strong> domains hosted by Cloudflare has fallen by 99.8% since our March 2024 Web Server Survey, leading to a noticeable 22.0% drop in the total number of all domains hosted by Cloudflare.</p>



<figure><img decoding="async" width="2263" height="1305" src="https://www.netcraft.com/wp-content/uploads/2024/03/cloudflare.svg" alt="" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></figure>



<p>All <em>domains</em> hosted by Cloudflare.<br>(Note sites like <strong><em>foo</em>.example.tk</strong> and <strong><em>bar</em>.example.tk</strong> would be counted as a single domain)</p>



<p>The <strong>.tk</strong> top level domain was the most popular of those operated by Freenom. Last month it accounted for 16.2% of all domains hosted by Cloudflare, but very few of these were used by popular websites. Amongst Netcraft’s top million websites dataset, there were only 59 sites across 57 <strong>.tk</strong> domains. 36 of these still resolve, which suggests they are paid-for domains.</p>



<p>But to the vast majority who registered these domain names for free, their sudden disappearance came as a bit of a surprise. Amongst the debate in the <a href="https://community.cloudflare.com/t/dot-tk-domains/612359">Cloudflare</a> <a href="https://community.cloudflare.com/t/the-site-got-down-suddenly-with-dns-probe-finished-nxdomain-and-err-name-not-resolved-error/612747">and </a><a href="https://www.reddit.com/r/freenom/comments/1akf1cv/every_domain_is_now_down/">Reddit </a>communities, some customers reported being able to get their domain names <a href="https://www.reddit.com/r/freenom/comments/1akf1cv/comment/kphvtrf/">back up and running</a> by having their domains marked as “paid domains”. </p>



<figure><img decoding="async" width="1024" height="573" src="https://www.netcraft.com/wp-content/uploads/2024/03/image-3-1024x573.png" alt="" srcset="https://www.netcraft.com/wp-content/uploads/2024/03/image-3-1024x573.png 1024w, https://www.netcraft.com/wp-content/uploads/2024/03/image-3-300x168.png 300w, https://www.netcraft.com/wp-content/uploads/2024/03/image-3-768x430.png 768w, https://www.netcraft.com/wp-content/uploads/2024/03/image-3-1536x859.png 1536w, https://www.netcraft.com/wp-content/uploads/2024/03/image-3.png 1620w" sizes="(max-width: 1024px) 100vw, 1024px" data-src="https://www.netcraft.com/wp-content/uploads/2024/03/image-3-1024x573.png" data-srcset="https://www.netcraft.com/wp-content/uploads/2024/03/image-3-1024x573.png 1024w, https://www.netcraft.com/wp-content/uploads/2024/03/image-3-300x168.png 300w, https://www.netcraft.com/wp-content/uploads/2024/03/image-3-768x430.png 768w, https://www.netcraft.com/wp-content/uploads/2024/03/image-3-1536x859.png 1536w, https://www.netcraft.com/wp-content/uploads/2024/03/image-3.png 1620w" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></figure>



<p>The Freenom website claims new registrations are temporarily out-of-order due to “technical issues”.</p>



<h3>When did the shutdown happen?</h3>



<p>The number of SSL certificates successfully issued to <strong>.tk</strong> domains provides a good indication of when the shutdown took effect, particularly as the two largest certificate authorities – Google and Let’s Encrypt – only issue <em>domain validated</em> certificates.</p>



<p>The issuance and renewal process for a domain validated certificate involves sending an HTTP request to the website it will be issued for, and so each subject domain must be resolvable for the process to succeed.</p>



<figure><img decoding="async" width="992" height="463" src="https://www.netcraft.com/wp-content/uploads/2024/03/image-5.png" alt="" srcset="https://www.netcraft.com/wp-content/uploads/2024/03/image-5.png 992w, https://www.netcraft.com/wp-content/uploads/2024/03/image-5-300x140.png 300w, https://www.netcraft.com/wp-content/uploads/2024/03/image-5-768x358.png 768w" sizes="(max-width: 992px) 100vw, 992px" data-src="https://www.netcraft.com/wp-content/uploads/2024/03/image-5.png" data-srcset="https://www.netcraft.com/wp-content/uploads/2024/03/image-5.png 992w, https://www.netcraft.com/wp-content/uploads/2024/03/image-5-300x140.png 300w, https://www.netcraft.com/wp-content/uploads/2024/03/image-5-768x358.png 768w" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></figure>



<p>Certificates issued to <strong>.tk</strong> domains (hourly).</p>



<p>This graph shows the shutdown taking noticeable effect between 8 – 10 February 2024, crucially a few days before Freenom issued the 12 February press statement where it announced its exit from the domain name business. This lack of notice clearly caught lots of people by surprise.</p>



<p>The small trickle of issuances thereafter represents the small number of <strong>.tk</strong> domains that are still active, including those marked as “paid domains”.</p>



<p>The free and easily acquired domain names that Freenom used to provide were unsurprisingly attractive to criminals and were used to host many phishing sites, malware, and other types of cyberattacks. Consequently, one positive side effect of the shutdown is that the number of malicious URLs that we block on the affected TLDs has fallen by 86.9% since December 2023.</p>



<p>You can read more about the impacts on cybercrime in the aftermath of Freenom’s original announcement on the <a href="https://www.netcraft.com/blog/impact-of-freenom-halting-registrations-on-cybercrime/">Netcraft Blog</a>.</p>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fuzzing Ladybird with tools from Google Project Zero (430 pts)]]></title>
            <link>https://awesomekling.substack.com/p/fuzzing-ladybird-with-tools-from</link>
            <guid>39725057</guid>
            <pubDate>Sat, 16 Mar 2024 11:49:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://awesomekling.substack.com/p/fuzzing-ladybird-with-tools-from">https://awesomekling.substack.com/p/fuzzing-ladybird-with-tools-from</a>, See on <a href="https://news.ycombinator.com/item?id=39725057">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>While </span><a href="https://ladybird.dev/" rel="">Ladybird</a><span> does an okay job with well-formed web content, I thought it would be useful to throw some security research tools at it and see what kind of issues it might reveal. So today we’ll be using “</span><strong><a href="https://github.com/googleprojectzero/domato" rel="">Domato</a></strong><a href="https://github.com/googleprojectzero/domato" rel=""> 🍅</a><span>”, a DOM fuzzer from </span><a href="https://googleprojectzero.blogspot.com/" rel="">Google Project Zero</a><span>, to stress test Ladybird and fix some issues found along the way.</span></p><p>The way this works is that Domato generates randomized web pages with lots of mostly-valid but strange HTML, CSS and JavaScript. I then load these pages into a debug build of Ladybird and observe what happens.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bd74774-78da-4188-ab8b-dc3fbd2622c1_1741x1105.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bd74774-78da-4188-ab8b-dc3fbd2622c1_1741x1105.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bd74774-78da-4188-ab8b-dc3fbd2622c1_1741x1105.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bd74774-78da-4188-ab8b-dc3fbd2622c1_1741x1105.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bd74774-78da-4188-ab8b-dc3fbd2622c1_1741x1105.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bd74774-78da-4188-ab8b-dc3fbd2622c1_1741x1105.png" width="1456" height="924" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9bd74774-78da-4188-ab8b-dc3fbd2622c1_1741x1105.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:924,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:663971,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bd74774-78da-4188-ab8b-dc3fbd2622c1_1741x1105.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bd74774-78da-4188-ab8b-dc3fbd2622c1_1741x1105.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bd74774-78da-4188-ab8b-dc3fbd2622c1_1741x1105.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bd74774-78da-4188-ab8b-dc3fbd2622c1_1741x1105.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Domato generates HTML pages of roughly 500 KiB in size, filled with “interesting” JS, CSS and HTML to surprise and delight your browser engine!</figcaption></figure></div><p>The Domato README boasts a ton of bugs discovered in all major browsers, so I have no doubt it will find some in ours as well. Here we go!</p><p>Unsurprisingly, it took less than a second to find the first issue! The output produced by Domato is actually 562 KiB, but I was able to reduce it to the following:</p><pre><code>&lt;body&gt;
&lt;script&gt;
let mfrac = document.createElement("mfrac");
mfrac.appendChild(document.createElement("th"));
document.body.appendChild(mfrac);
&lt;/script&gt;</code></pre><p>I’ve compiled Ladybird with UBSAN (Undefined Behavior SANitizer) for this test, and I get the following very helpful trace output:</p><pre><code><strong>LibWeb/HTML/HTMLTableCellElement.cpp:55:36: runtime error: member call on null pointer of type 'Web::DOM::Node'
</strong><span>#0 0x7e9bfa1610dd in table_containing_cell LibWeb/HTML/HTMLTableCellElement.cpp:55:36
#1 0x7e9bfa1610dd in Web::HTML::HTMLTableCellElement::apply_presentational_hints(Web::CSS::StyleProperties&amp;) const LibWeb/HTML/HTMLTableCellElement.cpp:101:33
#2 0x7e9bf97c22d1 in Web::CSS::StyleComputer::compute_cascaded_values(Web::CSS::StyleProperties&amp;, Web::DOM::Element&amp;, AK::Optional&lt;Web::CSS::Selector::PseudoElement::Type&gt;, bool&amp;, Web::CSS::StyleComputer::ComputeStyleMode) const LibWeb/CSS/StyleComputer.cpp:1448:17
#3 0x7e9bf97e5899 in Web::CSS::StyleComputer::compute_style_impl(Web::DOM::Element&amp;, AK::Optional&lt;Web::CSS::Selector::PseudoElement::Type&gt;, Web::CSS::StyleComputer::ComputeStyleMode) const LibWeb/CSS/StyleComputer.cpp:2231:5
#4 0x7e9bf97e447e in Web::CSS::StyleComputer::compute_style(Web::DOM::Element&amp;, AK::Optional&lt;Web::CSS::Selector::PseudoElement::Type&gt;) const LibWeb/CSS/StyleComputer.cpp:2202:12
#5 0x7e9bf9a7e60c in Web::DOM::Element::recompute_style() LibWeb/DOM/Element.cpp:575:64</span></code></pre><p>It’s a good old-fashioned null pointer dereference!</p><p><span>As it turns out, we’ve implemented </span><code>&lt;th&gt;</code><span> and </span><code>&lt;td&gt;</code><span> elements with the assumption that there’s always a </span><code>&lt;table&gt;</code><span> somewhere above it in the DOM tree. We probably believed this because the following is not allowed by the HTML parser:</span></p><p><code>&lt;mfrac&gt;&lt;th&gt;</code></p><p><span>If you load the above markup in a spec-compliant browser, it will create a single </span><code>&lt;mfrac&gt;</code><span> element with nothing inside it.</span></p><p><span>However, when creating DOM nodes manually using JavaScript API, you can break some of the rules that the parser has to follow, and indeed put a </span><code>&lt;th&gt;</code><span> inside an </span><code>&lt;mfrac&gt;</code><span>!</span></p><p>Here’s the buggy function:</p><pre><code>HTMLTableElement const&amp; table_containing_cell(HTMLTableCellElement const&amp; node)
{
    auto parent_node = node.parent();
&nbsp;&nbsp;&nbsp; while (!is&lt;HTML::HTMLTableElement&gt;(parent_node))
  &nbsp;&nbsp;    parent_node = parent_node-&gt;parent();
 &nbsp;&nbsp;&nbsp;return static_cast&lt;HTML::HTMLTableElement const&amp;&gt;(*parent_node);
}</code></pre><p><span>It’s used to implement the ancient feature where </span><code>&lt;table border=3&gt;</code><span> and </span><code>&lt;table padding=5&gt;</code><span> applies CSS border and padding to each table cell, and not just the table box itself.</span></p><p><span>The fix is simply to stop assuming that </span><code>&lt;th&gt;</code><span> and </span><code>&lt;td&gt;</code><span> elements always have a containing </span><code>&lt;table&gt;</code><span> in the ancestor chain. We don’t need the </span><code>table_containing_cell</code><span> helper but can instead simply replace this:</span></p><pre><code>auto const&amp; table_element = table_containing_cell(*this);</code></pre><p>With this:</p><pre><code>auto const table_element = first_ancestor_of_type&lt;HTMLTableElement&gt;();
if (!table_element)
    return;</code></pre><p><span>And we’re done with issue #1! </span><a href="https://github.com/SerenityOS/serenity/commit/b9bacb3ff49ba675f9875434d646fb1b34d2c0df" rel="">(Fix committed here.)</a></p><p>We continue executing the fuzzer and once again, within less than 1 second, we hit a new problem. The Domato output is 472 KiB, but it whittles down to this:</p><pre><code>&lt;script&gt;
&nbsp;&nbsp;&nbsp;&nbsp;var parser = new DOMParser();
&nbsp;&nbsp;&nbsp;&nbsp;var doc = parser.parseFromString("", "text/html");
&nbsp;&nbsp;&nbsp;&nbsp;var body = doc.createElement("body");
&nbsp;&nbsp;&nbsp;&nbsp;body.onblur = null;
&lt;/script&gt;</code></pre><p>When opened in Ladybird, we fail like so:</p><pre><code>VERIFICATION FAILED: m_ptr at LibJS/Heap/GCPtr.h:174
#1&nbsp; JS::GCPtr&lt;Web::HTML::Window&gt;::operator* at LibJS/Heap/GCPtr.h:174
#2&nbsp; Web::DOM::Document::window at LibWeb/DOM/Document.h:320
#3&nbsp; Web::HTML::HTMLBodyElement::global_event_handlers_to_event_target at LibWeb/HTML/HTMLBodyElement.cpp:104
#4&nbsp; Web::HTML::GlobalEventHandlers::set_onblur at LibWeb/HTML/GlobalEventHandlers.cpp:24
#5&nbsp; Web::Bindings::HTMLElementPrototype::onblur_setter at LibWeb/Bindings/HTMLElementPrototype.cpp:1153</code></pre><p><span>The reason things go wrong here is due to the special behavior of </span><code>onfoo</code><span> event handler attributes on the </span><code>&lt;body&gt;</code><span> element. For compatibility with ancient web content, assignments to </span><code>document.body.onfoo</code><span> must forward to </span><code>window.onfoo</code><span>. However, documents created via </span><code>DOMParser</code><span> </span><em>do not have a window object!</em></p><p>It turns out we’ve misunderstood this detail in our implementation, and structured our internal object model as if every document always has a window. Oops!</p><p><span>The fix is to make </span><code>Document::window()</code><span> return a nullable value, and then handle null in a bajillion places. When assigning </span><code>document.body.onblur</code><span> in a window-less document, we now simply do nothing, same as other browsers.</span></p><p><a href="https://github.com/SerenityOS/serenity/commit/b98a2be96bf03562ec21e80b48d4b3b97ad74cd6" rel="">(Fix committed here.)</a></p><p>Modern browsers have to support SVG both inline in HTML, and also as an external image format. This adds a whole host of new edge cases and interesting interactions.</p><p>For example, SVG allows declaring gradients that reference other gradients to inherit their colors. As it turns out, we hadn’t considered the case where a gradient references itself:</p><pre><code>&lt;svg&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&lt;linearGradient id="oops" href="#oops"/&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&lt;rect fill="url(#oops)" /&gt;
&lt;/svg&gt;</code></pre><p>The SVG above would cause our implementation to loop forever as it attempted to follow the chain of referenced gradients. Oops, indeed!</p><p>The obvious fix is to stop following the chain of linked gradients if the current gradient references itself. However, that doesn’t cover reference cycles that span multiple steps like so:</p><pre><code>&lt;svg&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&lt;linearGradient id="lol" href="#lmao"/&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&lt;linearGradient id="lmao" href="#even"/&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&lt;linearGradient id="even" href="#lol"/&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&lt;rect fill="url(#lol)" /&gt;
&lt;/svg&gt;</code></pre><p>To properly handle cyclical references, we have to keep track of all the gradients we’ve visited, and stop following the chain if we encounter a gradient we’ve already visited before.</p><p>Curiously, Firefox actually complains about these kind of gradients in their developer console:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb10a9009-633b-453e-ba7f-610dcb2af2f5_842x40.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb10a9009-633b-453e-ba7f-610dcb2af2f5_842x40.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb10a9009-633b-453e-ba7f-610dcb2af2f5_842x40.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb10a9009-633b-453e-ba7f-610dcb2af2f5_842x40.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb10a9009-633b-453e-ba7f-610dcb2af2f5_842x40.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb10a9009-633b-453e-ba7f-610dcb2af2f5_842x40.png" width="842" height="40" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b10a9009-633b-453e-ba7f-610dcb2af2f5_842x40.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:40,&quot;width&quot;:842,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb10a9009-633b-453e-ba7f-610dcb2af2f5_842x40.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb10a9009-633b-453e-ba7f-610dcb2af2f5_842x40.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb10a9009-633b-453e-ba7f-610dcb2af2f5_842x40.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb10a9009-633b-453e-ba7f-610dcb2af2f5_842x40.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><a href="https://github.com/SerenityOS/serenity/commit/2e0297d7035a807d4a18c89c8b310ec06bef005d" rel="">(Fix committed here.)</a></p><p>This one is interesting:</p><pre><code>&lt;iframe&gt;&lt;/iframe&gt;
&lt;script&gt;
&nbsp;&nbsp;&nbsp;&nbsp;window.onload = function() {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;let iframe = document.querySelector("iframe")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;let iframeWindow = iframe.contentWindow;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;iframe.remove();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;iframeWindow.getSelection();
&nbsp;&nbsp;&nbsp;&nbsp;}
&lt;/script&gt;</code></pre><p>The above test would crash like this:</p><pre><code><strong>LibWeb/HTML/WindowProxy.cpp:161:136: runtime error: reference binding to null pointer of type 'const BrowsingContext'</strong><span>
#0 0x77367675cadf in Web::HTML::WindowProxy::internal_get(JS::PropertyKey const&amp;, JS::Value, JS::CacheablePropertyMetadata*) const LibWeb/HTML/WindowProxy.cpp:161:5
#1 0x773670c4fcca in JS::Bytecode::get_by_id(JS::VM&amp;, AK::DeprecatedFlyString const&amp;, JS::Value, JS::Value, JS::Bytecode::PropertyLookupCache&amp;) LibJS/Bytecode/CommonImplementations.h:105:18
#2 0x773670c182d5 in JS::Bytecode::Op::GetById::execute_impl(JS::Bytecode::Interpreter&amp;) const LibJS/Bytecode/Interpreter.cpp:1088:28
#3 0x773670bc8e1f in execute LibJS/Bytecode/Op.h:1898:9
#4 0x773670bc8e1f in JS::Bytecode::Interpreter::run_bytecode() LibJS/Bytecode/Interpreter.cpp:409:38</span></code></pre><p><span>As it turns out, this is actually a bug in the HTML spec! When an iframe is removed from the DOM, its content document is detached from its browsing context. However, when getting or setting a property on a window object, we run a spec algorithm called </span><strong>"check if an access between two browsing contexts should be reported"</strong><span> which inspects the browsing context of the “accessor” and “accessed” window. The spec incorrectly assumed that both windows have an associated browsing context at the time of property access.</span></p><p><a href="https://github.com/whatwg/html/issues/10192" rel="">I’ve opened an issue against the HTML spec</a><span>, and patched this in Ladybird in the meantime by adding a null check.</span></p><p>Finding spec bugs is actually one of my favorite things while working on Ladybird. It allows us to improve the specs for everyone by submitting a bug report or fix.</p><p><a href="https://github.com/SerenityOS/serenity/commit/ad843b6e4a4404a4897b587711f9ce97a4504ed0" rel="">(Fix committed here.)</a></p><p>When opening the next page, it just wouldn’t finish loading. It just sat there chewing 100% CPU.</p><p>Here’s the reduction:</p><pre><code>&lt;div id="one"&gt;&lt;/div&gt;&lt;div id="two"&gt;&lt;/div&gt;
&lt;script&gt;
&nbsp;&nbsp;&nbsp;&nbsp;two.before(one);
&lt;/script&gt;</code></pre><p><span>This was a mistake in the implementation of </span><code>before()</code><span>, which has to find the first preceding sibling of </span><code>&lt;div id=”two”&gt;</code><span> that isn’t one of its arguments (i.e </span><code>&lt;div id=”one”&gt;</code><span> in this case.) We had the following loop mistake in the implementation:</span></p><pre><code>while (auto previous_sibling = node-&gt;previous_sibling()) {
&nbsp; &nbsp; // check if previous_sibling is one of the arguments
}</code></pre><p><span>The problem here is that we kept fetching </span><code>node-&gt;previous_sibling</code><span>, instead of continuing to follow the sibling chain via </span><code>previous_sibling-&gt;previous_sibling</code><span>.</span></p><p>Here’s how I fixed it:</p><pre><code>for (auto sibling = node-&gt;previous_sibling(); sibling; sibling = sibling-&gt;previous_sibling()) {

&nbsp; &nbsp; // check if previous_sibling is one of the arguments

}</code></pre><p><a href="https://github.com/SerenityOS/serenity/commit/35f359c51c787153cd17c7289664ab87146241ea" rel="">(Fix committed here.)</a></p><p>This can probably go on for quite some time, so let’s call it a day. We found five real bugs, one of which was a spec bug, and were able to fix all of them.</p><p>Even though things went basically as I expected, it’s still quite interesting just how quickly we fall apart when confronted with strange and unexpected inputs. Fuzzers like Domato are an amazing resource for anyone who wants to make their software more robust.</p><p>The next step here will be to get Ladybird to the point where we can handle a sustained onslaught of fuzzed inputs. And once it’s reasonably stable, we can start running it automatically in the cloud somewhere, and hopefully shake out even more issues.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Rising Price of Power in Chips (102 pts)]]></title>
            <link>https://semiengineering.com/the-rising-price-of-power-in-chips/</link>
            <guid>39724966</guid>
            <pubDate>Sat, 16 Mar 2024 11:35:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://semiengineering.com/the-rising-price-of-power-in-chips/">https://semiengineering.com/the-rising-price-of-power-in-chips/</a>, See on <a href="https://news.ycombinator.com/item?id=39724966">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

					<p>Power is everything when it comes to processing and storing data, and much of it isn’t good.&nbsp;Power-related issues, particularly heat, dominate chip and system designs today, and those issues are widening and multiplying.</p>
<p>Transistor density has reached a point where these tiny digital switches are generating more heat than can be removed through traditional means. That may sound manageable enough, except it has created a slew of new problems that require the entire industry to solve — EDA companies, process equipment makers, fabs, packaging houses, field-level monitoring and analytics providers, materials suppliers, research groups, etc.</p>
<p>Underlying all of this activity is a continued focus on packing more transistors into a fixed area, and an associated and accelerating battle with leakage power. <a href="https://semiengineering.com/knowledge_centers/integrated-circuit/transistors/3d/finfet-3/">FinFETs</a>&nbsp;solved leaky gate issues at 16/14nm, but the problem resurfaced just two nodes later. At 3nm, a totally different transistor structure is being introduced with&nbsp;<a href="https://semiengineering.com/knowledge_centers/integrated-circuit/transistors/3d/gate-all-around-fet/">gate-all-around FETs</a>&nbsp;(a.k.a. nanosheets), which make design, metrology, inspection, and test significantly more challenging and expensive. At 2nm/18A, power delivery will begin flipping from the frontside of a chip to the backside to alleviate routing issues for even getting sufficient power to transistors, and beyond that the industry is likely to change its transistor structure once again to&nbsp;<a href="https://semiengineering.com/knowledge_centers/integrated-circuit/transistors/3d/cfet/">compound FETs</a> (CFETs). These are a lot of process and structural changes in a short time window, and each new node will include more issues that need to be tackled.</p>
<p>For example, a growing concern in high-density chips and packages is transient thermal gradients. They can move in unpredictable ways, sometimes very quickly and other times not, and they can vary based upon the workload. At 40nm, using thicker dielectrics and substrates, and more relaxed pitches, these were considered annoyances. With today’s leading-edge process technology, all of this needs to be taken much more seriously.</p>
<p>“The base leakage is lower than the previous technology, but the overall total power is higher,” said Melika Roshandell, product management director at <a href="https://semiengineering.com/entities/cadence-design-systems/">Cadence</a>. “So at the end of the day, your thermal is going to be worse because you’re packing a lot more transistors into one IC and you’re pushing the performance. You want to go to higher and higher frequencies, and to do that you’re increasing the voltage, you’re increasing the power. And now your total power is more than the previous generation, so your thermal is going to be worse. Not only that, when you go to a lower node, your area is also shrinking. That area shrinkage and increase in total power is going to be a recipe sometimes for disaster for your thermal, and you’re not going to meet your performance because your thermal is going to hit a lot faster than what you were expecting.”</p>
<p><img fetchpriority="high" decoding="async" src="https://i0.wp.com/semiengineering.com/wp-content/uploads/Screenshot-2024-03-13-at-12.23.10%E2%80%AFPM.png?resize=980%2C864&amp;ssl=1" alt="" width="980" height="864" srcset="https://i0.wp.com/semiengineering.com/wp-content/uploads/Screenshot-2024-03-13-at-12.23.10 https://semiengineering.com/the-rising-price-of-power-in-chips/PM.png?w=980&amp;ssl=1 980w, https://i0.wp.com/semiengineering.com/wp-content/uploads/Screenshot-2024-03-13-at-12.23.10 https://semiengineering.com/the-rising-price-of-power-in-chips/PM.png?resize=300%2C264&amp;ssl=1 300w, https://i0.wp.com/semiengineering.com/wp-content/uploads/Screenshot-2024-03-13-at-12.23.10 https://semiengineering.com/the-rising-price-of-power-in-chips/PM.png?resize=768%2C677&amp;ssl=1 768w, https://i0.wp.com/semiengineering.com/wp-content/uploads/Screenshot-2024-03-13-at-12.23.10 https://semiengineering.com/the-rising-price-of-power-in-chips/PM.png?resize=600%2C529&amp;ssl=1 600w" sizes="(max-width: 980px) 100vw, 980px" data-recalc-dims="1"><br>
<strong>Fig. 1: Thermal-mechanical co-simulation of 3D-IC design under operation. Source: Cadence</strong></p>
<p>Heat is becoming every hardware engineer’s shared nightmare, and it sets up some vicious cycles that are difficult to break and to model up front:</p>
<ul>
<li>Heat accelerates the breakdown of dielectric films (time-dependent dielectric breakdown, or TDDB) used to safeguard signals, and it adds mechanical stress, which can cause warping.</li>
<li>It accelerates electromigration and other aging effects, which can shrink data paths. That adds more heat due to higher resistance in circuits and an increase in energy required to drive those signals, until they are re-routed (if possible).</li>
<li>It can impact the speed at which memory operates, slowing overall performance of a system.</li>
<li>It creates noise, which impacts signal integrity. That noise can be transient, too, which makes partitioning much more difficult.</li>
</ul>
<p>All of these factors can shorten the lifespan of a chip, or even a portion of a chip. “Thermally degrading transistors can easily make or break a chip or IP,” said Pradeep Thiagarajan, principal product manager for analog and mixed-signal verification solutions at <a href="https://semiengineering.com/entities/mentor-a-siemens-business/">Siemens EDA</a>. “Fortunately, self-heating analysis of most devices can be done to assess the impact of localized heating on a design by means of a transient measurement for every MOS device, followed by loading that temperature delta data and assessing the impact of the waveforms. Now, novelty is required across the board, given increasing data transfer rate requirements. So the better every thermal interface material is modeled, the higher the chances of addressing those effects and making any appropriate design changes to avoid a short-term or long-term hardware failure. The net is that you need novel thermal solutions, but you also have to model it properly.”</p>
<p><strong>Power issues abound</strong><br>
Many chipmakers are just starting to wrestle with these issues, because most chips are not developed at the most advanced processes. But as chips increasingly become collections of chiplets, everything will have to be characterized and operate under conditions that are foreign to planar chips developed at 40nm or higher.</p>
<p>What is not always obvious is that increasing transistor density, whether in a single chip or inside an advanced package, isn’t necessarily the biggest knob to turn for boosting performance. It does, however, increase the power density, which limits the clock frequency. As a result, many of the big improvements are peripheral to the transistors themselves. Those include hardware-software co-design, faster PHYs and interconnects, new materials for insulation and electron mobility, more accurate pre-fetch with shorter recovery times for misses, sparser algorithms, and new power delivery options.</p>
<p>“The understanding of the full system stack is really important,” said Vincent Risson, senior principal CPU architect at <a href="https://semiengineering.com/entities/arm/">Arm</a>. “The computer, of course, has an important contribution to the power, but the rest of the system is also very important. That’s why we have different levels of cache, and the size of the cache is different. We have increased that over the last generation because it’s better to have something local so that the power downstream sees compute as local. And as we scale to 3D, we can imagine having 3D stacked caches, which is an opportunity to basically reduce data movement and improve efficiency.”</p>
<p>The key is to add efficiencies into every aspect of the design cycle, and not just for the hardware. While the chip industry has been talking about hardware-software co-design for the past couple decades, systems companies have prioritized that approach with their custom-designed micro-architectures, and mobile devices are looking to extend battery life significantly further for competitive reasons.</p>
<p>“There is a lot of tuning to extract more, and that’s a big focus for the CPU,” said Risson. “We are continuing to make improvements in all the pre-fetch engines, for example, to improve the accuracy of that and to reduce the downstream traffic. So we have better coverage, but we also initiate less traffic on the interconnect.”</p>
<p>That’s one piece of the puzzle, but more are required. Consider the breakdown of dielectric films over time, for example. It can be accelerated by different workloads or operating conditions, particularly inside a package filled with chiplets. “TDDB is a problem because we have so many signals and so many polygon nets running on different voltages,” said Norman Chang, fellow and chief technologist for <a href="https://semiengineering.com/entities/ansys/">Ansys’</a> Electronics, Semiconductor, and Optics business unit. “If you have a net next to a signal net with a different voltage, then the dielectric will see different voltages. As time goes on, you will see a time-dependent dielectric breakdown. This is a new problem, and we need to come up with a solution for it.”</p>
<p><strong>Inconsistencies</strong><br>
Thermal gradients are another challenge, particularly when they are transient, varying greatly from one workload to another. This problem is particularly acute in <a href="https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/2-5d-ic/">2.5D</a>, where it can cause warpage, and in <a href="https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/3d-ics/">3D-ICs</a>, which are expected to roll out sometime in the next couple years. In both cases, heat can become trapped, creating a snowball effect.</p>
<p><img decoding="async" src="https://i0.wp.com/semiengineering.com/wp-content/uploads/Ansys_Thermal-Management-in-3D-IC-fig1-Thermal-and-Warpage-3DIC-1.png?resize=549%2C480&amp;ssl=1" alt="" width="549" height="480" srcset="https://i0.wp.com/semiengineering.com/wp-content/uploads/Ansys_Thermal-Management-in-3D-IC-fig1-Thermal-and-Warpage-3DIC-1.png?w=549&amp;ssl=1 549w, https://i0.wp.com/semiengineering.com/wp-content/uploads/Ansys_Thermal-Management-in-3D-IC-fig1-Thermal-and-Warpage-3DIC-1.png?resize=300%2C262&amp;ssl=1 300w" sizes="(max-width: 549px) 100vw, 549px" data-recalc-dims="1"><br>
<strong>Fig. 2: Thermal and mechanical analysis results showing temperature gradients on 2.5D IC, including warpage at 245°C. Source: Ansys</strong></p>
<p>“If you look at the power consumption in a 3D-IC, it’s very much related to temperature,” said Chang. “When the temperature increases, the leakage power will increase, and the thermal gradient distribution is the center of the multi-physics interaction in a 3D-IC. Temperature will affect power, but it also will affect the resistance. The resistance will increase when the temperature increases, and that also will affect the dielectric constant. That will affect the signal integrity and the power integrity, and it will affect the stress. And when you are mixing digital and analog in a 3D-IC, the analog is more sensitive to stress. You have to know where is the thermal gradient, where is the thermal hotspot, because you have to move the analog components away from the hotspot. If you see a thermal cycling for the analog component, you will speed up the aging of the device, you will start seeing a transistor mismatch, and the efficiency of the analog circuit will decline rapidly compared to the digital logic.”</p>
<p>And this is just getting started. Kenneth Larsen, senior director of product management at <a href="https://semiengineering.com/entities/synopsys-inc/">Synopsys</a>, noted that getting the placement wrong for various elements in stacked die can create unexpected issues such as thermal cross-talk, which also can degrade overall performance. “We’ve gone from monolithic to chiplet-based design, which is disaggregated, and now these devices are getting closer and can influence each other. When you put one device on top of another, how does the heat escape? This is a big challenge. With 3D-ICs, the first concern is can you build systems with structural integrity. But you also have other mechanical, thermal, and power concerns — the whole shebang.”</p>
<p>In the past, the simplest approach to managing heat was to lower the voltage. That approach is starting to run out of steam, because at extremely low voltages the slightest irregularity can cause problems. “Noise is a topic for very low power technologies, like near-threshold or sub-threshold devices, as well as for high-power devices,” said Roland Jancke, design methodology head in <a href="https://semiengineering.com/entities/fraunhofer-iis-eas/">Fraunhofer IIS’</a> Engineering of Adaptive Systems Division. “It’s also a topic that is hardly understood because it typically does not appear in simulation. It appears later on in the real world, and then you have to understand it and cope with it.”</p>
<p>Cross coupling, for example, can create noise in the substrate, but that isn’t always obvious in the design phase. “We started some years ago with a substrate simulator to figure out what are the cross couplings across a substrate,” said Jancke. “You’re thinking about a single device and the neighboring devices. You don’t think about the cross coupling at the input stage, which is far away, but it’s coupled through the substrate.”</p>
<p>These types of issues can cause problems in DRAM, as well, particularly as bit cell density increases, which also makes it more susceptible to noise. “There’s definitely thermal noise,” said Onur Mutlu, professor of computer science at ETH Zurich. “Also, when you access a cell, you’re creating a noise in the structure because of the electrical interference caused by the toggling of wires, for example, or the access transistor. That activation causes noise, and this leads to some reliability issues. We call it cell-to-cell interference. The row hammer problem is just one example of that. You’re activating one row and causing disturbance in adjacent rows. RowPress is another example, where you keep one row open for a longer period of time, and this disturbs what’s happening in other rows adjacent to it. This sort of cell interference is getting more prevalent as we reduce the size of each cell and put cells closer to each other and increase density. You can get silent data corruption, and that may be what’s happening in the field.”</p>
<p>With power, there are always unexpected issues. “For whatever clock frequency you’re running at, you’d like to run it at the lowest voltage possible, because that’s where you’re going to use the least amount of energy,” said Barry Pangrle, power architect at <a href="https://semiengineering.com/entities/movellus/">Movellus</a>. “There’s a certain amount you can model, but as with any models, sometimes you have surprises. I can take a chip, run it under different conditions, and I can play around with the voltage and frequency and get an idea of where it will work under different workloads. ‘Okay, I can use these points, and if I want to be a little bit more conservative, I can always back off a bit and put in a little bit of margin.’ But people aren’t going to do that for every chip. So do you create bins and say, ‘Okay the ones that fall in this category we’ll run at this clock and this voltage?’ Then, some of the granularity will be left up to whoever is selling that chip.”</p>
<p><strong>Other issues</strong><br>
There also is a monetary aspect to power, and that spans everything from the resources required to create a complex design, to the amount of power consumed in a data center. The higher the transistor density, the more energy it takes to power up and cool down a rack of servers. And with various flavors of AI, the goal is to maximize transistor utilization, which in turn consumes more power, generating more heat and requiring more cooling.</p>
<p>“These applications are drawing huge amounts of power, and they’re exponentially rising,” said Noam Brousard, vice president of engineering solutions at <a href="https://semiengineering.com/entities/proteantecs/">proteanTecs</a>. “Efficient power consumption will eventually translate into significant savings in the data center. That’s number one. Aside from that, we also have the environmental impact. And, we want to extend the lifetime of the electronics.”</p>
<p><img decoding="async" src="https://i0.wp.com/semiengineering.com/wp-content/uploads/Screenshot-2024-03-12-at-1.06.50%E2%80%AFPM.png?resize=2368%2C1060&amp;ssl=1" alt="" width="2368" height="1060" srcset="https://i0.wp.com/semiengineering.com/wp-content/uploads/Screenshot-2024-03-12-at-1.06.50 https://semiengineering.com/the-rising-price-of-power-in-chips/PM.png?w=2368&amp;ssl=1 2368w, https://i0.wp.com/semiengineering.com/wp-content/uploads/Screenshot-2024-03-12-at-1.06.50 https://semiengineering.com/the-rising-price-of-power-in-chips/PM.png?resize=300%2C134&amp;ssl=1 300w, https://i0.wp.com/semiengineering.com/wp-content/uploads/Screenshot-2024-03-12-at-1.06.50 https://semiengineering.com/the-rising-price-of-power-in-chips/PM.png?resize=1024%2C458&amp;ssl=1 1024w, https://i0.wp.com/semiengineering.com/wp-content/uploads/Screenshot-2024-03-12-at-1.06.50 https://semiengineering.com/the-rising-price-of-power-in-chips/PM.png?resize=768%2C344&amp;ssl=1 768w, https://i0.wp.com/semiengineering.com/wp-content/uploads/Screenshot-2024-03-12-at-1.06.50 https://semiengineering.com/the-rising-price-of-power-in-chips/PM.png?resize=1536%2C688&amp;ssl=1 1536w, https://i0.wp.com/semiengineering.com/wp-content/uploads/Screenshot-2024-03-12-at-1.06.50 https://semiengineering.com/the-rising-price-of-power-in-chips/PM.png?resize=2048%2C917&amp;ssl=1 2048w, https://i0.wp.com/semiengineering.com/wp-content/uploads/Screenshot-2024-03-12-at-1.06.50 https://semiengineering.com/the-rising-price-of-power-in-chips/PM.png?resize=600%2C269&amp;ssl=1 600w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1"><br>
<strong>Fig. 3: Impact of power on chips. Source: proteanTecs</strong></p>
<p>Nor are power-related effects confined just to a chip. “With 2.5D, thermal stress is going to cause warpage, and because of that you run the risk of breaking the balls that connect the substrate to the PCB,” said Cadence’s Roshandell. “If it cracks, you get a short, and then your product is not going to work. So how you address that, and how you model it, is important. It has to happen in the earliest stages of the design where can envision it and do something about it.”</p>
<p>Things get even more complex in 3D-ICs. Once again, the emphasis is on sizing up the problems early in the design cycle, but in 3D-ICs there are additive effects. “Dynamic switching power is really tricky for 3D-ICs compared to an SoC,” said Ansys’ Chang. “We have to consider the physical architecture as early as possible, because if you have 15 chiplets in a 3D-IC, how do you partition the power among the 15 chiplets for dynamic workflow and time dimension? At a different time you may have a different workload on that chiplet, and that may create a thermal hotspot. But if the top die has a local hotspot and the bottom die also has a local hotspot, if the two local hotspots line up at a certain time, then that hotspot will become a global thermal hotspot. It may be 10 or 15 degrees hotter than the local hotspot if the other die is not switching. This caught 3D-IC circuit designers completely off guard, because when you run emulation for a chiplet in a 3D-IC, you probably cannot run an emulation for the whole 3D-IC with a realistic workflow.”</p>
<p>The problem is that there are so may dependencies that everything needs to be understood in context of something else. “There is no way you can optimize these devices independent of each other,” said Niels Faché, vice president and general manager for <a href="https://semiengineering.com/entities/keysight-technologies/">Keysight’s</a> design and simulation portfolio. “You might have an objective around thermal, such as maximum temperature, heat dissipation, but you need to understand that in the context of mechanical stress. You have to be able to model these individual physical effects. If they are very tightly coupled, you have to do it in the form of a co-simulation. We do that, for example, with an electro-thermal simulation. So when you look at the current that flows through a transistor, it’s going to have an effect on heat. Then, heat has an impact on electrical characteristics, which changes the electrical behavior, and you have to model those interactions.”</p>
<p><strong>Solutions</strong><br>
There is no single, comprehensive solution for power-related issues, but there are plenty of partial ones.</p>
<p>One approach, and probably the simplest, is to limit overdesign. “It all starts with focusing on the target use cases and defining the necessary features to address them,” said Steven Woo, <a href="https://semiengineering.com/entities/rambus-inc/">Rambus</a> fellow and distinguished inventor. “It’s tempting to add features here and there to address other potential markets and use cases, but that often leads to increased area, power, and complexities that can hurt performance for the main applications of the chip. All features must be looked at critically and judged in an almost ruthless manner to understand if they really need to be in the chip. Each new feature impacts PPA, so maintaining focus on the target markets and use cases is the first step.”</p>
<p>This can have a significant impact on the overall power consumption, particularly with AI. “In AI there are many options to consider, especially for edge devices,” Woo said. “Some options include how the chip will be powered, thermal constraints, if it needs to support training and/or inference, accuracy requirements, the environment in which the chip will be deployed, and supported number formats just to name a few. Supporting large feature sets means increasing area and power, and the added complexity of gating off features when they aren’t in use. And with data movement impacting performance and consuming large amounts of the power budget, designers need a good understanding of how much data needs to be moved to develop architectures that minimize data movement at the edge.”</p>
<p>Another approach is to run real workloads on a design. “What some customers are doing is saying, ‘Let’s run representative workloads because we don’t know what we don’t know,” said William Ruby, senior director of product management for low power solutions at Synopsys. “It’s like power coverage. ‘What do we believe is a sustained worst case? What do we believe is a good idle type of workload?’ But what they don’t know is how a new software update may change the entire activity profile. Hopefully it’s an incremental change and they’ve somehow budgeted for that, as opposed to being pessimistic and a little bit more conservative. But how do you predict what’s going to happen with a firmware update?”</p>
<p>Backside power delivery is another option, particularly at the most advanced nodes. “At some point you hit diminishing returns because you’ve got the stuff from the top layers down to the bottom, and a lot of time the stuff in the top layers is your power and ground routing,” said Movellus’ Pangrle. “If you can deliver that from the backside, and you don’t have to go through 17 metal layers up top, that’s a lot of layers you don’t have to go through. Being able to bypass that whole metal stack and come in the back door so you can be closer to the transistors and not have to worry about going through all those vias is like manufacturing magic.”</p>
<p>Using sensors inside of chips and packages to monitor changes in power-related behavior is yet another approach. “In the field there are many things that can degrade performance, so we have to bake in voltage guard-bands,” said proteanTecs’ Brousard. “We know there will be noise. We know there will be excessive workloads. We know that the chip will go through aging. All these factors force us to apply more voltage than is necessary in best-case VDD<sub>min</sub>.”</p>
<p>On top of that, copper wires can be used to conduct heat to where it can be dissipated. “You can do simple things like optimizing TSV placement with stacked die, and you may be able to use thermal vias, as well,” said Synopsys’ Larsen. “It’s very complex, but we have always dealt in exponentials in EDA. It’s things we will go and solve. But when you want to mitigate something, you add something that takes away some of the values you’re looking for, and that has to be addressed. For reliability, you may add in redundancies, which could be TSVs or hybrid bonds in the stack.”</p>
<p><strong>Conclusion</strong><br>
Power has been a problem for leading-edge chipmakers for the past couple decades. A smart phone will send out a warning that it is running too hot and shut down until it cools off, and a rack of servers may shift a load to another rack for the same reason. But chips increasingly are decomposed into various components and packaged together, and as industries such as automotive begin developing chips at 5nm and below, power issues will fan out in all directions.</p>
<p>Architecture, place-and-route, signal integrity, heat, reliability, manufacturability, and aging are all tightly coupled with power. And as the chip industry continues to combine different features in unique ways to address unique markets, the entire industry will need to learn how to work with or around power-related effects. Unlike in the past, when only the highest-volume chipmakers were concerned with power, it will be the rarer design that can ignore it.</p>
<p>—Ann Mutschler and Karen Heyman contributed to this report.</p>
<p><em><strong>Related Reading</strong><br>
<a href="https://semiengineering.com/next-gen-power-integrity-challenges/">Next-Gen Power Integrity Challenges</a><br>
Dealing with physical and electrical effects in advanced nodes and stacked die.<br>
<a href="https://semiengineering.com/backside-power-delivery-adds-new-thermal-concerns/">Backside Power Delivery Adds New Thermal Concerns</a><br>
Lack of shielding, routing issues, and new mechanical stresses could have broad impact on standard cell design.<br>
<a href="https://semiengineering.com/3d-ics-may-be-the-least-cost-option/">3D-ICs May Be The Least-Cost Option</a><br>
Advanced packaging has evolved from expensive custom solutions to those ready for more widespread adoption.</em></p>

					<br>

				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mozilla will be retiring the Mozilla Location Service (190 pts)]]></title>
            <link>https://github.com/mozilla/ichnaea/issues/2065</link>
            <guid>39724505</guid>
            <pubDate>Sat, 16 Mar 2024 09:41:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mozilla/ichnaea/issues/2065">https://github.com/mozilla/ichnaea/issues/2065</a>, See on <a href="https://news.ycombinator.com/item?id=39724505">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-turbo-body="">
      


    <div>
      <p><a href="#start-of-content">Skip to content</a>
      <span data-view-component="true">
    <span data-view-component="true"></span>
</span></p><react-partial partial-name="keyboard-shortcuts-dialog" data-ssr="false">
  
  
  
</react-partial>



      

        

            


<header role="banner" data-color-mode="light" data-light-theme="light" data-dark-theme="dark">
  

  <div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:mozilla/ichnaea" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="H2sHX5u6b_qygYwjVOs8l-20McarYcv_UIY_wLuDoXq2bz5ov88-UjOI8b5KgP8fTIUVbWFjZ-5sWKotmzvFYg" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="mozilla/ichnaea" data-current-org="mozilla" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=l4vxNKpZB1sFyChwoHBj7p%2FrQ9610rmXmNswCjxi%2Bnn%2Ba%2FMl6PgKXV%2FpPO8BO4e7zW2VSTs4QW7Vq%2FxNUx9rG5vR6YTIydStZcORNBF4jYzRv%2BoCCh5ulwul06wMGvoE60lkJOK1i9UA1Do8u3ssxzcBdGLsGrigPaKcmtweSSQoZW8kllZk9SjDzQMHJ6U0qwTGZkw1i675bpeLc%2FKF7%2FjXSUdlivF7%2B6mZh2QF%2BZ5rYPo5vU%2FUCay%2FuwxDtWhVqyzMZoIsAQMV%2Fklw4oxR0TuhxEpPavbrJMeKQlIDijn1adn4ChIgmlzTbSlvkvfJQ9veGjtBRwknySPC3HBdRAOsOwQWJqNbH4M2QYxLE6K7NojdddGJeuiI8Rwipeej7pO%2FxOKTo1G%2B6mi59L7I%2FNro%2Ff0xGQgcbcZH43yxdPpswbUTxOU7mElWNjopjpzDTCAL%2F4MesutLZvx2YCqxLXVPAXOH7FLsBwy9HnOAI6Wdps0g%2B6z1A2UJajC2rMzEMw2saS7DZernL5nEjkvoetsOT2IjIASIb60%3D--QOvBI2Rs5OTsjPh4--7l4m391dBkjIacfFlAMtLA%3D%3D&quot; />">
  
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fissues_fragments%2Fissue_layout&amp;source=header-repo&amp;source_repo=mozilla%2Fichnaea" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/mozilla/ichnaea/issues/2065&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="980001f8b17eea4ac32351041717f09fdf22bfec0edcbb5c9c01ff87ffec1a69" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/voltron/issues_fragments/issue_layout;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div>
</header>

      
    </div>

  








    


    
    <include-fragment data-base-src="https://github.com/notifications/beta/shelf"></include-fragment>






  <div itemscope="" itemtype="http://schema.org/SoftwareSourceCode" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="">
    <main id="js-repo-pjax-container">
      
  


  
      
    

    






  
  <div id="repository-container-header" data-turbo-replace="">

      

        


          <nav data-pjax="#js-repo-pjax-container" aria-label="Repository" data-view-component="true">

  
    <div data-view-component="true">      <action-menu data-select-variant="none" data-view-component="true">
  <focus-group direction="vertical" mnemonics="" retain="">
    <tool-tip id="tooltip-dfb4facb-9671-4890-a4a2-a531c8b8cb15" for="action-menu-c677db56-f8db-44d0-b6de-77ca236acd88-button" popover="manual" data-direction="s" data-type="label" data-view-component="true">Additional navigation options</tool-tip>


<anchored-position id="action-menu-c677db56-f8db-44d0-b6de-77ca236acd88-overlay" anchor="action-menu-c677db56-f8db-44d0-b6de-77ca236acd88-button" side="outside-bottom" anchor-offset="normal" popover="auto" data-view-component="true">
  </anchored-position>  </focus-group>
</action-menu></div>
</nav>

  </div>

  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance">
    <div id="repo-content-pjax-container" data-channel="eyJjIjoiaXNzdWU6MjE4NDU3MTExNzp0aW1lbGluZSIsInQiOjE3MTA2MDEyMDV9--62b4f47008ceced2ade9135ed7c1b5319de85e696fbef556228a35f86cc1c229" data-url="/mozilla/ichnaea/issues/2065/show_partial?partial=issues%2Ftitle&amp;sticky=true" data-gid="I_kwDOAJFJeM6CNejt" data-morpheus-enabled="false" data-pjax="" data-turbo-frame="">



          
<details>
  <summary data-ga-click="Issues, create new issue, view:issue_show location:issue_header style:button logged_in:false">
    
    New issue
  </summary>
  <details-dialog aria-label="Sign up for GitHub">
            <div>
  <p>
  <strong>Have a question about this project?</strong> Sign up for a free GitHub account to open an issue and contact its maintainers and the community.
  </p>

  <!-- '"` --><!-- </textarea></xmp> -->
  <p>By clicking “Sign up for GitHub”, you agree to our <a href="https://docs.github.com/terms" target="_blank">terms of service</a> and
  <a href="https://docs.github.com/privacy" target="_blank">privacy statement</a>. We’ll occasionally send you account related emails.</p>

  <p>
    Already on GitHub?
    <a data-ga-click="(Logged out) New issue modal, clicked Sign in, text:sign-in" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;new issue modal&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/mozilla/ichnaea/issues/2065&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="f045e9bba9af6b1b79364b400273949959ceac629560f1dfa77780103812fae1" href="https://github.com/login?return_to=%2Fmozilla%2Fichnaea%2Fissues%2Fnew%2Fchoose">Sign in</a>
    to your account
  </p>
</div>
  </details-dialog>
</details>
        
      </div>

</turbo-frame>


    </main>
  </div>

          




    <cookie-consent id="cookie-consent-banner" data-initial-cookie-consent-allowed="" data-cookie-consent-required="true"></cookie-consent>


  

    <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm" open="">
    <summary role="button" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal">
      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog="">
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

    

    <template id="snippet-clipboard-copy-button">
  <div class="zeroclipboard-container position-absolute right-0 top-0">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn js-clipboard-copy m-2 p-0 tooltipped-no-delay" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon m-2">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>
<template id="snippet-clipboard-copy-button-unpositioned">
  <div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>




    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AutoDev: Automated AI-driven development by Microsoft (145 pts)]]></title>
            <link>https://arxiv.org/abs/2403.08299</link>
            <guid>39724356</guid>
            <pubDate>Sat, 16 Mar 2024 09:03:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2403.08299">https://arxiv.org/abs/2403.08299</a>, See on <a href="https://news.ycombinator.com/item?id=39724356">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2403.08299">Download PDF</a>
    <a href="https://arxiv.org/html/2403.08299v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>The landscape of software development has witnessed a paradigm shift with the advent of AI-powered assistants, exemplified by GitHub Copilot. However, existing solutions are not leveraging all the potential capabilities available in an IDE such as building, testing, executing code, git operations, etc. Therefore, they are constrained by their limited capabilities, primarily focusing on suggesting code snippets and file manipulation within a chat-based interface. To fill this gap, we present AutoDev, a fully automated AI-driven software development framework, designed for autonomous planning and execution of intricate software engineering tasks. AutoDev enables users to define complex software engineering objectives, which are assigned to AutoDev's autonomous AI Agents to achieve. These AI agents can perform diverse operations on a codebase, including file editing, retrieval, build processes, execution, testing, and git operations. They also have access to files, compiler output, build and testing logs, static analysis tools, and more. This enables the AI Agents to execute tasks in a fully automated manner with a comprehensive understanding of the contextual information required. Furthermore, AutoDev establishes a secure development environment by confining all operations within Docker containers. This framework incorporates guardrails to ensure user privacy and file security, allowing users to define specific permitted or restricted commands and operations within AutoDev. In our evaluation, we tested AutoDev on the HumanEval dataset, obtaining promising results with 91.5% and 87.8% of Pass@1 for code generation and test generation respectively, demonstrating its effectiveness in automating software engineering tasks while maintaining a secure and user-controlled development environment.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Michele Tufano [<a href="https://arxiv.org/show-email/d9255482/2403.08299">view email</a>]      <br>    <strong>[v1]</strong>
        Wed, 13 Mar 2024 07:12:03 UTC (273 KB)<br>
</p></div></div>]]></description>
        </item>
    </channel>
</rss>