<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 28 Sep 2025 18:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[The AI coding trap (228 pts)]]></title>
            <link>https://chrisloy.dev/post/2025/09/28/the-ai-coding-trap</link>
            <guid>45405177</guid>
            <pubDate>Sun, 28 Sep 2025 15:43:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chrisloy.dev/post/2025/09/28/the-ai-coding-trap">https://chrisloy.dev/post/2025/09/28/the-ai-coding-trap</a>, See on <a href="https://news.ycombinator.com/item?id=45405177">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>If you ever watch someone “coding”, you might see them spending far more time staring
into space than typing on their keyboard. No, they (probably) aren’t slacking off.
Software development is fundamentally a practice of problem-solving, and so, as with
solving a tricky crossword, most of the work is done in your head.</p>
<p>In the software development lifecycle, coding is the letters filled into the crossword,
only a small amount of effort compared to all the head scratching and scribbled notes.
The real work
usually happens alongside coding, as the developer learns the domain, narrows down
requirements, maps out relevant abstractions, considers side effects, tests features
incrementally, and finally squashes bugs that survived this rigorous process. It looks
something like this:</p>
<p><img src="https://chrisloy.dev/images/2025/ai-coding-1.svg" alt="Thinking, then coding." title="Thinking, then coding."></p>
<p>But with AI-driven coding, things play out very differently.</p>
<h2>“Code first, ask questions later”</h2>
<p>AI coding agents such as <a href="https://claude.com/product/claude-code">Claude Code</a> are
making it astonishingly fast to write code in isolation. But most software lives within
complex systems, and since LLMs can't yet hold the full context
of an application in memory at once, human review, testing, and integration needs will
remain. And that is a lot harder when the code has been written without the human thinking
about it. As a result, for complex software, much of the time will be spent on post hoc
understanding of what code the AI has written.</p>
<p><img src="https://chrisloy.dev/images/2025/ai-coding-2.svg" alt="Coding, then trying to understand." title="Coding, then trying to understand."></p>
<p>This is the root of the difference between marketing copy that boasts of the paradigm
shifting speed of <strong>writing code</strong> with AI (often framed as
“<a href="https://docs.coderabbit.ai/overview/introduction">10X</a>
<a href="https://zencoder.ai/product/coding-agent">faster</a>”), and the marginal productivity gains
in <strong>delivering working software</strong> seen in the wild (usually
<a href="https://www.microsoft.com/en/customers/story/22158-allpay-github-copilot">closer to 10%</a>).</p>
<p><img src="https://chrisloy.dev/images/2025/ai-coding-3.svg" alt="alt_text" title="image_tooltip"></p>
<p>An even more dispiriting upshot of this is that, as developers, we spend an ever greater
proportion of our time merely fixing up the output of these wondrous
<a href="https://chrisloy.dev/post/2024/12/24/llms-are-scrappy-innovators">babbling machines</a>. While the LLMs get to
blast through all the fun, easy work at lightning speed, we are then left with all the
thankless tasks: testing to ensure existing functionality isn’t broken, clearing out
duplicated code, writing documentation, handling deployment and infrastructure, etc.
Very little time is actually dedicated to the thing that developers actually love doing: coding.</p>
<p>Fortunately, help is at hand. While LLMs are shaking up how software development is
performed, this issue in itself is not actually new. In fact, it is merely a stark example
of an age-old problem, which I call:</p>
<h2>The tech lead’s dilemma</h2>
<p>As engineers progress in their careers, they will eventually
step into the role of <strong>tech lead</strong>. They might be <strong>managing</strong> a team,
or they could be a <strong>principal engineer</strong>, driving technical delivery without the
people management. In either case, they are responsible for the team’s technical
delivery. They are also usually the most experienced developer in the team: either in their career,
in the specialised domain of the team, or in both.</p>
<p>Software delivery is a team effort, but one in which experience can have a highly imbalancing
effect on individual contribution
<a href="https://chrisloy.dev/post/2023/11/10/software-engineering-mechanics">velocity</a>. As such, when the tech lead’s
primary job is to maximise delivery, they will often face an internal conflict between two
ways to deliver software:</p>
<ul>
<li><strong>Fair delegation</strong> across the team, maximising learning and ownership opportunities for
junior team members, but allowing delivery to be bottlenecked by the speed of the least productive
team members.</li>
<li><strong>Mollycoddling</strong> the team, by delegating only the easy or non-critical work to juniors,
and keeping the hardest work for themselves, as the person on the team most capable of
delivering at speed.</li>
</ul>
<p>Unfortunately, while we shall see that mollycoddling is extremely harmful to long-term team
health, it is also often a very effective way to accelerate delivery. The higher bandwidth of
the tech lead is often most efficiently deployed by eating up all the hardest work:</p>
<p><img src="https://chrisloy.dev/images/2025/ai-coding-4.svg" alt="Senior engineers have higher bandwidth." title="Senior engineers have higher bandwidth."></p>
<p>As such, I have seen this pattern repeated time and again over the course of my career. And,
of course, it comes at a cost. Siloing of experience in the tech lead makes the team brittle,
it makes support harder, and it places ever greater pressure on the tech lead as a single point
of failure. What follows next is predictable: burnout, departure, and ensuing crisis as the
team struggles to survive without the one person who really knows how everything works.</p>
<p><img src="https://chrisloy.dev/images/2025/ai-coding-5.svg" alt="Mollycoddling leads to short term gains but eventual failure." title="Mollycoddling leads to short term gains but eventual failure."></p>
<p>As is usually the case, the solution lies in a third way that avoids these two extremes
and balances delivery with team growth. We might frame it as something like:</p>
<blockquote>
<p><em>Implement team practices that allow engineers to deliver working code within a framework
that minimises rework, maximises effective collaboration, and promotes personal growth and
learning.</em></p>
</blockquote>
<p>When I was CTO of Datasine, we enshrined this attitude in a simple tech team motto:</p>
<blockquote>
<p><em><strong>Learn. Deliver. Have fun.</strong></em></p>
</blockquote>
<p>Good tech leads expose their engineers to work at the limit of their capabilities,
using processes and practices that minimise delivery risk while also enabling each team
member to grow their skills, knowledge, and domain expertise. This is, in fact, the essence
of good technical leadership.</p>
<p>There are many ways to accomplish it, from strict codified frameworks such as the
<a href="http://www.extremeprogramming.org/rules.html">Extreme Programming rules</a>, through
to looser sets of principles which we might broadly refer to as “best practices”:</p>
<ul>
<li>Code reviews</li>
<li>Incremental delivery</li>
<li><a href="https://chrisloy.dev/post/2025/01/24/modular-software-design">Modular design</a></li>
<li>Test-driven development</li>
<li>Pair programming</li>
<li>Quality documentation</li>
<li>Continuous integration</li>
</ul>
<p>So, for experienced engineers today, an urgent question is: how can we translate these practices
into a world of AI-driven coding?</p>
<h2>LLMs are lightning fast junior engineers</h2>
<p>In 2025, many engineers are finding themselves for the first time in a position familiar
to every tech lead: overseeing a brilliant but unpredictable junior engineer. Harnessing
and controlling such talent, in a way that benefits effective team collaboration, is one
of the primary challenges of engineering leadership. But AI coding agents need different
management to junior engineers, because the nature of their productivity and growth is
fundamentally different.</p>
<p>As software engineers gain experience, we tend to improve our productivity in multiple
ways at the same time: writing more robust code, using better abstractions, spending less
time writing and fixing bugs, understanding more complex architectures, covering edge
cases more effectively, spotting repeated patterns earlier, etc. Engineering is a rich
and complex discipline with many avenues for specialisation, but for simplicity we might
group these dimensions into two broad themes:</p>
<ul>
<li><strong>Quality</strong>: ability to deliver more complex, more performant, more maintainable code</li>
<li><strong>Velocity</strong>: ability to develop working, bug-free code in a shorter space of time</li>
</ul>
<p>Over time, good engineers will improve in both axes.</p>
<p><img src="https://chrisloy.dev/images/2025/ai-coding-6.svg" alt="Engineers and LLMs improve in both velocity and quality." title="Engineers and LLMs improve in both velocity and quality."></p>
<p>Early LLMs were fast to write code, but time spent fixing bugs and removing hallucinations
meant they were slow to complete bug-free code. Over time, smarter LLMs and better use of
context engineering and tools have meant that modern AI coding agents are much better at
“one shot” writing of code. The current generation of commercially available agents can be
incredibly fast at producing working code for problems that would challenge some mid-level
engineers, though they cannot yet match the expertise of senior engineers:</p>
<p>So we can think of the current generation of AI coding agents as junior engineers, albeit
with two fundamental differences:</p>
<ol>
<li>LLMs deliver code much, much faster than junior engineers, constrained neither by thinking
nor writing time;</li>
<li>LLMs have no true capacity to learn, and instead only improve through more effective
<a href="https://chrisloy.dev/post/2025/08/03/context-engineering">context engineering</a> or
the arrival of new foundation models.</li>
</ol>
<p>As with junior engineering talent, there are broadly two ways that you can deploy them, depending on
whether your focus is long-term or short-term:</p>
<ul>
<li><strong>AI-driven engineering</strong>: employing best practices, foregrounding human
understanding of the code, moving slowly to make development sustainable.</li>
<li><strong>Vibe coding</strong>: throwing caution to the wind and implementing at speed, sacrificing
understanding for delivery velocity, hitting an eventual wall of unsalvageable, messy code.</li>
</ul>
<p>As might be expected, the long-term trajectories of choosing between these two approaches
follow much the same pattern as choosing between parallel delegation and mollycoddling of a
junior team:</p>
<p><img src="https://chrisloy.dev/images/2025/ai-coding-7.svg" alt="Vibe coding has the exact same failure state as mollycoddling." title="Vibe coding has the exact same failure state as mollycoddling."></p>
<p>This is why the <a href="https://chrisloy.dev/post/2025/09/07/vibe-coding-art">vibe coding</a> approach
is great for tiny projects or throwaway prototypes: applications of sufficient simplicity
can be delivered without the need for any human thinking at all. By limiting the complexity
of our projects and leaning into the capabilities of the tools, we can deliver end-to-end
working software in no time at all.</p>
<p><img src="https://chrisloy.dev/images/2025/ai-coding-8.svg" alt="Vibe coding is great as long as you don't need to think." title="Vibe coding is great as long as you don't need to think."></p>
<p>But you <em>will</em> hit a wall of complexity that AI is incapable of scaling alone.</p>
<p>Building prototypes is now easier than ever. But if we want to effectively use LLMs to
accelerate delivery of real, complex, secure, working software, and to realise more than
marginal efficiency gains, we need to write a new playbook of engineering practices tailored
to maximise collaboration between engineering teams that include both humans and LLMs.</p>
<h2>How to avoid the AI coding trap</h2>
<p>AI coding agents are dazzlingly productive, but lack in-depth knowledge of your business,
codebase, or roadmap. Left unchecked, they will happily churn
out thousands of lines of code with no heed paid to design, consistency, or maintainability.
The job of the engineer, then, is to act as a tech lead to these hotshots: to provide the structure,
standards, and processes that convert raw speed into sustainable delivery.</p>
<p>We need a new playbook for how to deliver working software efficiently, and we can look to the
past to learn how to do that. By treating LLMs as <strong>lightning-fast junior engineers</strong>, we
can lean on best practices from the software development lifecycle to build systems that scale.</p>
<p><img src="https://chrisloy.dev/images/2025/ai-coding-9.svg" alt="AI can be used at every stage of the software development lifecycle." title="AI can be used at every stage of the software development lifecycle."></p>
<p>Just as tech leads don't just write code but set practices for the team, engineers
now need to set practices for AI agents. That means bringing AI into every stage of
the lifecycle:</p>
<blockquote>
<p><strong>Specification</strong>: exploring, analysing, and refining feature specifications to cover edge cases and narrow focus.</p>
</blockquote>
<blockquote>
<p><strong>Documentation</strong>: generating and reviewing documentation up front to provide reusable guardrails and lasting evidence.</p>
</blockquote>
<blockquote>
<p><strong>Modular Design</strong>: scaffolding modular architectures to control context scope and maximise comprehension.</p>
</blockquote>
<blockquote>
<p><strong>Test-Driven Development</strong>: generating extensive test cases prior to implementation to guide implementation and prevent regression.</p>
</blockquote>
<blockquote>
<p><strong>Coding Standards</strong>: applying house styles and best practice when generating code, through context engineering.</p>
</blockquote>
<blockquote>
<p><strong>Monitoring &amp; Introspection</strong>: analysing logs and extracting insights faster than any human ever could.</p>
</blockquote>
<p>By understanding that delivering software is so much more than just writing code, we can
avoid the AI coding trap and instead hugely amplify our ability to deliver working, scalable software.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When I say "alphabetical order", I mean "alphabetical order" (150 pts)]]></title>
            <link>https://sebastiano.tronto.net/blog/2025-09-28-alphabetic-order/</link>
            <guid>45404022</guid>
            <pubDate>Sun, 28 Sep 2025 13:00:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sebastiano.tronto.net/blog/2025-09-28-alphabetic-order/">https://sebastiano.tronto.net/blog/2025-09-28-alphabetic-order/</a>, See on <a href="https://news.ycombinator.com/item?id=45404022">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>Last month I have been on a multi-day hike with my dad. Each of us took
many pictures, and when we came back we put them all in a shared folder.
We both have Android phones, and the naming scheme used for our pictures
was the same: <code>IMG_YYYYMMDD_HHmmss</code> followed maybe by some other numbers
and then a <code>.jpg</code>. Here <code>YYYY</code> stands for the year, <code>MM</code> for month and
so on, so that sorting the pictures in alphabetical order is the same as
sorting them by date.</p>
<p>Or so I thought. Strangely, when I looked at the files from my dad’s
Windows PC, they were not sorted correctly: all the pictures took
with my phone came first, followed by all the pictures took by him.
I thought this was surely some weird Microsoft bug - after using
Windows 11 at work for a while, I would not be surprised if you
told me their file explorer can’t figure out how to sort strings.</p>
<p>But then I looked at the same files in a shared Google Drive folder,
and again they were in the wrong order:</p>
<p><img src="https://sebastiano.tronto.net/blog/2025-09-28-alphabetic-order/drive.png" alt="Screenshot from Google Drive"></p>
<p>As you can see, the picture taken at 5:54 (with my dad’s phone) comes
before the one taken at 9:20 (also with my dad’s phone), but after the
one taken at 12:11 (with my phone).</p>
<p>Weird. Well, maybe Microsoft <em>and</em> Google got this wrong. But
that seems unlikely.</p>
<p>Indeed, KDE’s Dolphin file manager does the same thing:</p>
<p><img src="https://sebastiano.tronto.net/blog/2025-09-28-alphabetic-order/dolphin.png" alt="Screenshot from Dolphin"></p>
<p>I’ll spare you the screenshots, but Gnome and both the file managers
that I have on my phone also get the alphabetical order wrong.</p>
<p>At this point I thought that maybe one of the two phones is using some
weird alternative unicode character instead of the underscore <code>_</code>. Really,
I could not see any other explanation. But nope, this is not it, because
the good old <code>ls</code> sorts my files correctly:</p>
<pre><code>$ ls -l

total 218572
-rw-r--r-- 1 seba seba 1866185 Aug 28 18:51 IMG_20250820_055436307.jpg
-rw-r--r-- 1 seba seba 4749899 Aug 28 18:50 IMG_20250820_092016029_HDR.jpg
-rw-r--r-- 1 seba seba 6201609 Aug 28 18:52 IMG_20250820_092440966_HDR.jpg
-rw-r--r-- 1 seba seba 7694802 Aug 28 18:51 IMG_20250820_092832138_HDR.jpg
-rw-r--r-- 1 seba seba 1536520 Aug 20 09:57 IMG_20250820_095716_607.jpg
-rw-r--r-- 1 seba seba 1054553 Aug 20 10:38 IMG_20250820_103857_991.jpg
-rw-r--r-- 1 seba seba  965353 Aug 20 10:39 IMG_20250820_103903_811.jpg
(and so on)
</code></pre>
<p>This was consistent among the couple of Linux distros I use, as well
as my OpenBSD server. On the one hand this is good: not <em>every</em> single
piece of software fucks up something as basic as string sorting. On the
other hand, this makes it harder to debug what the fuck is going on with
all the other file managers.</p>
<p>It took me more than a month to figure this one out. Tell me, which
file do you think comes first in alphabetical order, <code>file-9.txt</code> or
<code>file-10.txt</code>?</p>
<p>Of course, the user who named those files probably wants <code>file-9.txt</code> to
come before <code>file-10.txt</code>. But <code>1</code> is smaller than <code>9</code>, so <code>file-10.txt</code>
should be first in alphabetical order. Everyone understands that, and
soon people learn to put enough leading zeros if they want their files
to stay sorted the way they like.</p>
<p>Well, apparently all these operating systems have decided that no,
users are too dumb and they cannot possibly understand what alphabetical
order means.  So when you ask them to sort your files alphabetically,
they don’t. Instead, they decide that if some piece of the file name is
a number, the real numerical value must be used.</p>
<p>I don’t know when this became the norm, to be honest I have not used a
normal graphical file manager in a long time.</p>
<p><em>I know you asked for the files to be sorted in alphabetical order,
but you don’t want <code>file-10.txt</code> to come before <code>file-9.txt</code>, do
you? No, I know you don’t. I am not even going to ask you, your
mushy human brain is too small to comprehend the intricacies of
such a question. I’ll spare you the thinking.</em></p>
<p>So it turns out that my dad’s phone wrote the milliseconds in the file
name right after the seconds, while mine added an extra underscore to
separate them from the seconds.  Which in my mind it should not have
mattered, because alphabetically they should still have been sorted
correctly to the second. But with this “modern” interpretation of the
alphabetical order, the files without the extra separator in the name had
a much higher number, so they come last.</p>
<p>Now that I know what the issue is, I can solve it by renaming the files
with a consistent scheme. I have also found a setting to fix Dolphin’s
behavior, but it was very much buried into its many configuration
options. And I would rather not have to change this setting in every
application I use, assuming they even allow it.</p>
<p>I miss the time when computers did what you told them to, instead of
trying to read your mind.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Privacy Badger is a free browser extension made by EFF to stop spying (263 pts)]]></title>
            <link>https://privacybadger.org/</link>
            <guid>45404021</guid>
            <pubDate>Sun, 28 Sep 2025 12:59:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://privacybadger.org/">https://privacybadger.org/</a>, See on <a href="https://news.ycombinator.com/item?id=45404021">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="faq-body">
    
      <h3 id="What-is-Privacy-Badger">
        <a href="#What-is-Privacy-Badger">What is Privacy Badger?</a>
      </h3>
      <p>Privacy Badger is a browser extension that stops advertisers and other third-party trackers from secretly tracking where you go and what pages you look at on the web.  If an advertiser seems to be tracking you across multiple websites without your permission, Privacy Badger automatically blocks that advertiser from loading any more content in your browser.  To the advertiser, it’s like you suddenly disappeared.</p>

    
      <h3 id="How-is-Privacy-Badger-different-from-Disconnect,-Adblock-Plus,-Ghostery,-and-other-blocking-extensions">
        <a href="#How-is-Privacy-Badger-different-from-Disconnect%2c-Adblock-Plus%2c-Ghostery%2c-and-other-blocking-extensions">How is Privacy Badger different from other blocking extensions?</a>
      </h3>
      <p>Privacy Badger was born out of our desire to be able to recommend a single extension that would:</p>
<ul>
<li>Automatically analyze and block any tracker or ad that violated the principle of user consent</li>
<li>Function well without any settings, knowledge, or configuration by the user</li>
<li>Use algorithmic methods to decide what is and isn’t tracking</li>
<li>Be produced by an organization that is unambiguously working for its users rather than for profit</li>
</ul>
<p>As a result, Privacy Badger differs from traditional ad-blocking extensions in two key ways. First, while most other blocking extensions prioritize blocking ads, Privacy Badger doesn’t block ads unless they happen to be tracking you; in fact, one of our goals is to incentivize advertisers to adopt better privacy practices.</p>
<p>Second, most other blockers rely on a human-curated list of domains or URLs to block. Privacy Badger is an algorithmic tracker blocker – we define what “tracking” looks like, and then Privacy Badger blocks or restricts domains that it observes tracking in the wild. What is and isn’t considered a tracker is entirely based on how a specific domain acts, not on human judgment.</p>
<p>Privacy Badger sends the <a href="https://globalprivacycontrol.org/">Global Privacy Control</a> signal to opt you out of data sharing and selling, and the <a href="https://www.eff.org/issues/do-not-track">Do Not Track</a> signal to tell companies not to track you. If trackers ignore these signals, Privacy Badger will learn to block them.</p>
<p>Beyond this, Privacy Badger comes with other advantages like cookie blocking, <a href="#How-does-Privacy-Badger-handle-social-media-widgets">click-to-activate placeholders</a> for potentially useful tracker widgets (video players, comments widgets, etc.), and outgoing link click tracking removal on <a href="https://www.eff.org/deeplinks/2018/05/privacy-badger-rolls-out-new-ways-fight-facebook-tracking">Facebook</a> and <a href="https://www.eff.org/deeplinks/2018/10/privacy-badger-now-fights-more-sneaky-google-tracking">Google</a>.</p>
<p>By using Privacy Badger, you support the <a href="https://www.eff.org/">Electronic Frontier Foundation</a> and help fight for a better Web for everybody.</p>

    
      <h3 id="Who-makes-Privacy-Badger">
        <a href="#Who-makes-Privacy-Badger">Who makes Privacy Badger?</a>
      </h3>
      <p>Privacy Badger was created by the <a href="https://www.eff.org/">Electronic Frontier Foundation</a>, a nonprofit organization that protects your privacy and free expression online. We make free tools like Privacy Badger, publish educational guides, testify before lawmakers about technology, and fight for the public interest in court—all thanks to support from EFF’s members. If you want a better internet and a strong democracy, <a href="https://supporters.eff.org/donate/support-privacy-badger">join the fight</a> against creepy online surveillance.</p>

    
      <h3 id="How-does-Privacy-Badger-work">
        <a href="#How-does-Privacy-Badger-work">How does Privacy Badger work?</a>
      </h3>
      <p>When you view a webpage, that page will often be made up of content from many different sources. For example, a news webpage might load the actual article from the news company, ads from an ad company, and the comments section from a different company that’s been contracted out to provide that service.</p>
<p>Privacy Badger keeps track of all of this. If the same source seems to be tracking across different websites, then Privacy Badger springs into action, telling the browser not to load any more content from that source. And when your browser stops loading content from a source, that source can no longer track you. Voila!</p>
<p>At a more technical level, Privacy Badger keeps track of the “third party” domains that embed images, scripts and advertising in the pages you visit. Privacy Badger looks for tracking techniques like uniquely identifying cookies, local storage “supercookies,” and canvas fingerprinting. If it observes the same third-party host tracking on three separate sites, Privacy Badger will automatically disallow content from that third-party tracker.</p>
<p>By default, Privacy Badger receives <a href="https://www.eff.org/deeplinks/2023/10/privacy-badger-learns-block-ever-more-trackers">periodic learning updates</a> from <a href="https://github.com/EFForg/badger-sett">Badger Sett</a>, our Badger training project. This “remote learning” automatically discovers trackers present on thousands of the most popular sites on the Web.</p>

    
      <h3 id="What-is-a-third-party-tracker">
        <a href="#What-is-a-third-party-tracker">What is a third party tracker?</a>
      </h3>
      <p>When you visit a webpage parts of the page may come from domains and servers other than the one you asked to visit. This is an essential feature of <a href="https://en.wikipedia.org/wiki/Hypertext">hypertext</a>. On the modern Web, embedded images and code often use cookies and other methods to track your browsing habits — often to display advertisements. The domains that do this are called “third party trackers”, and you can read more about how they work <a href="https://www.eff.org/wp/behind-the-one-way-mirror">here</a>.</p>

    
      <h3 id="What-do-the-red,-yellow-and-green-sliders-in-the-Privacy-Badger-menu-mean">
        <a href="#What-do-the-red%2c-yellow-and-green-sliders-in-the-Privacy-Badger-menu-mean">What do the red, yellow and green sliders in the Privacy Badger menu mean?</a>
      </h3>
      <p>Red means that content from this third party domain has been completely disallowed.</p>
<p>Yellow means that the third party domain appears to be trying to track you, but it is on Privacy Badger’s cookie-blocking “yellowlist” of third party domains that, when analyzed, seemed to be necessary for Web functionality. In that case, Privacy Badger will load content from the domain but will try to screen out third party cookies and referrers from it.</p>
<p>Green means “no action”; Privacy Badger will leave the domain alone.</p>

    
      <h3 id="Why-does-Privacy-Badger-block-ads">
        <a href="#Why-does-Privacy-Badger-block-ads">Why does Privacy Badger block ads?</a>
      </h3>
      <p>Actually, nothing in the Privacy Badger code is specifically written to block ads. Rather, it focuses on disallowing any visible or invisible “third party” scripts or images that appear to be tracking you even though you specifically denied consent by sending <a href="https://www.eff.org/issues/do-not-track">Do Not Track</a> and <a href="https://globalprivacycontrol.org/">Global Privacy Control</a> signals. It just so happens that most (but not all) of these third party trackers are advertisements. When you see an ad, the ad sees you, and can track you. Privacy Badger is here to stop that.</p>

    
      <h3 id="Why-doesn't-Privacy-Badger-block-all-ads">
        <a href="#Why-doesn%27t-Privacy-Badger-block-all-ads">Why doesn't Privacy Badger block all ads?</a>
      </h3>
      <p>Because Privacy Badger is primarily a privacy tool, not an ad blocker. Our aim is not to block ads, but to prevent non-consensual invasions of people’s privacy because we believe they are inherently objectionable. We also want to create incentives for advertising companies to do the right thing. Of course, if you really dislike ads, you can also install a traditional ad blocker.</p>

    
      <h3 id="What-is-Global-Privacy-Control">
        <a href="#What-is-Global-Privacy-Control">What is Global Privacy Control (GPC)?</a>
      </h3>
      <p><a href="https://globalprivacycontrol.org/">Global Privacy Control (GPC)</a> is a new specification that allows users to tell companies they’d like to opt out of having their data shared or sold. By default, Privacy Badger sends the GPC signal to every company you interact with alongside the Do Not Track (DNT) signal.</p>
<p>What’s the difference? Do Not Track is meant to tell companies that you don’t want to be tracked in any way (learn more about what we mean by “tracking” <a href="https://www.eff.org/pages/understanding-effs-do-not-track-policy-universal-opt-out-tracking">here</a>). Privacy Badger gives third-party companies a chance to comply with DNT by adopting <a href="https://www.eff.org/dnt-policy/">our DNT policy</a>, and blocks those that look like they’re tracking you anyway.</p>
<p>When DNT was developed, many websites simply ignored users’ requests not to be tracked. That’s why Privacy Badger has to act as an enforcer: trackers that don’t want to comply with your wishes get blocked. Today, users in many jurisdictions have the legal right to opt out of some kinds of tracking. That’s where GPC comes in.</p>
<p>GPC is meant to be a legally-binding request to all companies in places with applicable privacy laws. For example, <a href="https://theccpa.org/">the California Consumer Privacy Act</a> gives California residents the right to opt out of having their data sold. By sending the GPC signal, Privacy Badger is telling companies that you would like to exercise your rights.</p>
<p>The CCPA and other laws are <a href="https://advocacy.consumerreports.org/press_release/consumer-reports-study-finds-significant-obstacles-to-exercising-california-privacy-rights/">not perfect</a>, which is why Privacy Badger uses both approaches. It asks websites to respect your privacy, and it blocks known trackers from loading at all.</p>

    
      <h3 id="What-about-tracking-by-the-sites-I-actively-visit,-like-NYTimes.com-or-Facebook.com">
        <a href="#What-about-tracking-by-the-sites-I-actively-visit%2c-like-NYTimes.com-or-Facebook.com">What about tracking by the sites I actively visit, like NYTimes.com or Facebook.com?</a>
      </h3>
      <p>At present, Privacy Badger primarily protects you against tracking by third party sites. As far as privacy protections for “first party” sites (sites that you visit directly), Privacy Badger removes outgoing link click tracking on <a href="https://www.eff.org/deeplinks/2018/05/privacy-badger-rolls-out-new-ways-fight-facebook-tracking">Facebook</a> and <a href="https://www.eff.org/deeplinks/2018/10/privacy-badger-now-fights-more-sneaky-google-tracking">Google</a>. We plan on adding more first party privacy protections in the future.</p>
<p>We are doing things in this order because the most scandalous, intrusive and objectionable form of online tracking is that conducted by companies you’ve <a href="https://lumapartners.com/content/lumascapes/display-ad-tech-lumascape/">often never heard of</a> and have no relationship with. First and foremost, Privacy Badger is there to enforce Do Not Track against these domains by providing the technical means to restrict access to their tracking scripts and images. The right policy for whether nytimes.com, facebook.com or google.com can track you <em>when you visit that site</em> – and the technical task of preventing it – is more complicated because often tracking is interwoven with the features the site offers.</p>

    
      <h3 id="Does-Privacy-Badger-contain-a-list-of-blocked-sites">
        <a href="#Does-Privacy-Badger-contain-a-list-of-blocked-sites">Does Privacy Badger contain a list of blocked sites?</a>
      </h3>
      <p>Unlike other blocking tools, we have not made decisions about which sites to block, but rather about which behavior is objectionable. Domains will only be blocked if Privacy Badger observes the domain collecting unique identifiers after it was sent Do Not Track and Global Privacy Control signals.</p>
<p>Privacy Badger <em>does</em> contain a “<a href="https://github.com/EFForg/privacybadger/blob/master/src/data/pbconfig.json">yellowlist</a>” of some sites that are known to provide essential third party resources; those sites show up as yellow and have their cookies blocked rather than being blocked entirely. This is a compromise with practicality, and in the long term we hope to phase out the yellowlist as these third parties begin to <a href="https://www.eff.org/dnt-policy">explicitly commit to respecting Do Not Track</a>. The criteria for including a domain on the yellowlist can be <a href="https://github.com/EFForg/privacybadger/blob/master/doc/yellowlist-criteria.md">found here</a>.</p>

    
      <h3 id="How-was-the-cookie-blocking-yellowlist-created">
        <a href="#How-was-the-cookie-blocking-yellowlist-created">How was the cookie blocking yellowlist created?</a>
      </h3>
      <p>The initial list of domains that should be cookie blocked rather than blocked entirely was derived from a <a href="https://jonathanmayer.org/papers_data/bau13.pdf">research project</a> on classifying third party domains as trackers and non-trackers. We will make occasional adjustments to it as necessary. If you find domains that are under- or over-blocked, please <a href="https://github.com/EFForg/privacybadger/issues">file a bug</a> on GitHub.</p>

    
      <h3 id="Does-Privacy-Badger-prevent-fingerprinting">
        <a href="#Does-Privacy-Badger-prevent-fingerprinting">Does Privacy Badger prevent fingerprinting?</a>
      </h3>
      <p>Browser fingerprinting is an extremely subtle and problematic method of tracking, which we documented with the <a href="https://coveryourtracks.eff.org/">Cover Your Tracks project</a>. Privacy Badger can detect <a href="https://www.propublica.org/article/meet-the-online-tracking-device-that-is-virtually-impossible-to-block">canvas-based fingerprinting</a>, and will block third party domains that use it. Detection of other forms of fingerprinting and protections against first-party fingerprinting are ongoing projects. Of course, once a domain is blocked by Privacy Badger, it will no longer be able to fingerprint you.</p>

    
      <h3 id="Does-Privacy-Badger-consider-every-cookie-to-be-a-tracking-cookie">
        <a href="#Does-Privacy-Badger-consider-every-cookie-to-be-a-tracking-cookie">Does Privacy Badger consider every cookie to be a tracking cookie?</a>
      </h3>
      <p>No. Privacy Badger analyzes the cookies from each site; unique cookies that contain tracking IDs are disallowed, while “low entropy” cookies that perform other functions are allowed. For instance a cookie like LANG=fr that encodes the user’s language preference, or a cookie that preserves a very small amount of information about ads the user has been shown, would be allowed provided that individual or small groups of users’ reading habits could not be collected with them.</p>

    
      <h3 id="Will-you-be-supporting-any-other-browsers-besides-Chrome-Firefox-Opera">
        <a href="#Will-you-be-supporting-any-other-browsers-besides-Chrome-Firefox-Opera">Will you be supporting any other browsers besides Chrome, Firefox, Edge and Opera?</a>
      </h3>
      <p>We are working towards <a href="https://github.com/EFForg/privacybadger/issues/549#issuecomment-1209648999">Safari on macOS</a> support. <a href="https://github.com/EFForg/privacybadger/issues/549#issuecomment-744583479">Safari on iOS</a> seems to lack certain extension capabilities required by Privacy Badger to function properly.</p>
<p>Chrome on Android does not support extensions. To use Privacy Badger on Android, install <a href="https://play.google.com/store/apps/details?id=org.mozilla.firefox">Firefox for Android</a>.</p>
<p>Privacy Badger does not work with <a href="https://support.microsoft.com/en-us/help/4533505/what-is-microsoft-edge-legacy">Microsoft Edge Legacy</a>. Please switch to the new <a href="https://www.microsoft.com/en-us/edge">Microsoft Edge</a> browser.</p>

    
      <h3 id="Can-I-download-Privacy-Badger-directly-from-eff.org">
        <a href="#Can-I-download-Privacy-Badger-directly-from-eff.org">Can I download Privacy Badger directly from eff.org?</a>
      </h3>
      <p>If you use Google Chrome, you have to install extensions from Chrome Web Store. To install Privacy Badger in Chrome, visit <a href="https://chromewebstore.google.com/detail/privacy-badger/pkehgijcmpdhfbdbbnkijodmdjhbjlgp">Privacy Badger’s Chrome Web Store listing</a> and click the “Add to Chrome” button there.</p>
<p>Otherwise, you can use the following links to get the latest version of Privacy Badger directly from eff.org:</p>
<ul>
<li>Firefox: <a href="https://www.eff.org/files/privacy-badger-latest.xpi">https://www.eff.org/files/privacy-badger-latest.xpi</a></li>
<li>Chromium: <a href="https://www.eff.org/files/privacy_badger-chrome.crx">https://www.eff.org/files/privacy_badger-chrome.crx</a></li>
</ul>

    
      <h3 id="-I-am-an-online-advertising-tracking-company.--How-do-I-stop-Privacy-Badger-from-blocking-me">
        <a href="#-I-am-an-online-advertising-tracking-company.--How-do-I-stop-Privacy-Badger-from-blocking-me">I run a domain that uses cookies or other tracking. How do I stop Privacy Badger from blocking me?</a>
      </h3>
      <p>One way is to stop tracking users who have turned on Global Privacy Control or Do Not Track signals (i.e., stop collecting cookies, supercookies or fingerprints from them). Privacy Badger will stop learning to block that domain. The next version of Privacy Badger to ship with an updated pre-trained list will no longer include that domain in the list. Most Privacy Badger users will then update to that list.</p>
<p>You can also unblock yourself by promising to meaningfully respect the Do Not Track signal. To do so, post a <em>verbatim</em> copy of <a href="https://www.eff.org/dnt-policy">EFF’s Do Not Track policy</a> to the URL <a href="https://example.com/.well-known/dnt-policy.txt">https://example.com/.well-known/dnt-policy.txt</a>, where “example.com” is replaced by your domain. Posting EFF’s DNT policy on a domain is a promise of compliance with EFF’s DNT Policy by that domain.</p>
<p>If your domain is compliant with EFF’s DNT policy and declares this compliance, most Privacy Badgers will see this declaration the next time they encounter your domain. Also, the next version of Privacy Badger to ship with an updated pre-trained list will probably include your declaration of compliance in the list.</p>
<p>Note that the domain must support HTTPS, to protect against tampering by network attackers. The path contains “.well-known” per <a href="https://tools.ietf.org/html/rfc5785">RFC 5785</a>. Also note that you must post a copy of the policy at each compliant subdomain you control. For example, if you wish to declare compliance by both sub1.example.com and sub2.example.com, you must post EFF’s DNT policy on each domain.</p>

    
      <h3 id="Where-can-I-find-general-information-about-Privacy-Badger-that-I-can-use-for-a-piece-I'm-writing">
        <a href="#Where-can-I-find-general-information-about-Privacy-Badger-that-I-can-use-for-a-piece-I%27m-writing">Where can I find general information about Privacy Badger that I can use for a piece I'm writing?</a>
      </h3>
      <p>Glad you asked! Check out this <a href="https://privacybadger.org/files/pb_journalist_1_pager.pdf">downloadable press kit</a> that we’ve put together.</p>

    
      <h3 id="admin-deployment-and-configuration">
        <a href="#admin-deployment-and-configuration">As an administrator, how do I configure Privacy Badger on my managed devices?</a>
      </h3>
      <p>Please see our <a href="https://github.com/EFForg/privacybadger/blob/master/doc/admin-deployment.md">enterprise deployment and configuration</a> document.</p>

    
      <h3 id="What-is-the-Privacy-Badger-license--Where-is-the-Privacy-Badger-source-code">
        <a href="#What-is-the-Privacy-Badger-license--Where-is-the-Privacy-Badger-source-code">What is the Privacy Badger license? Where is the Privacy Badger source code?</a>
      </h3>
      <p>Privacy Badger’s <a href="https://github.com/EFForg/privacybadger">source code</a> is licensed under <a href="https://spdx.org/licenses/GPL-3.0-or-later.html">GPLv3+</a>. This website’s <a href="https://github.com/EFForg/privacybadger-website">source code</a> is licensed under <a href="https://spdx.org/licenses/AGPL-3.0-or-later.html">AGPLv3+</a>.</p>

    
      <h3 id="How-can-I-support-Privacy-Badger">
        <a href="#How-can-I-support-Privacy-Badger">How can I support Privacy Badger?</a>
      </h3>
      <p>Thanks for asking! Individual donations make up about half of EFF’s support, which gives us the freedom to work on user-focused projects. If you want to support the development of Privacy Badger and other projects like it, you can <a href="https://supporters.eff.org/donate/support-privacy-badger">throw us a few dollars here</a>. Thank you.</p>
<p>If you want to help directly with the project, we appreciate that as well. Please see <a href="https://github.com/EFForg/privacybadger/blob/master/CONTRIBUTING.md">Privacy Badger’s CONTRIBUTING document</a> for ways to get started.</p>

    
      <h3 id="How-does-Privacy-Badger-handle-social-media-widgets">
        <a href="#How-does-Privacy-Badger-handle-social-media-widgets">How does Privacy Badger handle social media widgets?</a>
      </h3>
      <p>Social media widgets (such as the Facebook Like button) often track your reading habits. Even if you don’t click them, the social media companies often see exactly which pages you’re seeing the widget on. When blocking social buttons and other potentially useful (video, audio, comments) widgets, <a href="https://www.eff.org/deeplinks/2024/01/privacy-badger-puts-you-control-widgets">Privacy Badger can replace them</a> with click-to-activate placeholders. You will not be tracked by these replacements unless you explicitly choose to click them.</p>

    
      <h3 id="How-do-I-uninstall-remove-Privacy-Badger">
        <a href="#How-do-I-uninstall-remove-Privacy-Badger">How do I uninstall/remove Privacy Badger?</a>
      </h3>
      <p>Firefox: See the <a href="https://support.mozilla.org/en-US/kb/disable-or-remove-add-ons#w_disabling-and-removing-extensions">Disable or remove Add-ons</a> Mozilla help page.</p>
<p>Chrome: See the <a href="https://support.google.com/chrome_webstore/answer/2664769?hl=en">Install and manage extensions</a> Chrome Web Store help page.</p>
<p>Edge: See the <a href="https://support.microsoft.com/en-us/help/4027935/microsoft-edge-add-or-remove-browser-extensions">Add or remove browser add-ons, extensions, and toolbars</a> Microsoft help page.</p>

    
      <h3 id="Is-Privacy-Badger-compatible-with-other-extensions,-including-other-adblockers">
        <a href="#Is-Privacy-Badger-compatible-with-other-extensions%2c-including-other-adblockers">Is Privacy Badger compatible with other extensions, including adblockers?</a>
      </h3>
      <p>Privacy Badger should be compatible with other extensions.</p>
<p>While there is likely to be overlap between the various manually-edited advertising/tracker lists and Privacy Badger, unlike adblockers, Privacy Badger automatically learns to block trackers based on their behavior. This means that Privacy Badger may learn to block trackers your adblocker doesn’t know about.</p>

    
      <h3 id="Is-Privacy-Badger-compatible-with-Firefox's-built-in-content-blocking">
        <a href="#Is-Privacy-Badger-compatible-with-Firefox%27s-built-in-content-blocking">Is Privacy Badger compatible with Firefox's built-in privacy protections?</a>
      </h3>
      <p>It’s fine to use Firefox’s built-in content blocking (<a href="https://blog.mozilla.org/en/products/firefox/firefox-now-available-with-enhanced-tracking-protection-by-default/">Enhanced Tracking Protection</a> or ETP) and Privacy Badger together. While there is overlap between Firefox’s tracker lists and Privacy Badger, Privacy Badger automatically learns to block trackers based on their behavior. This means that Privacy Badger’s automatically-generated and regularly updated blocklist contains trackers not found in Firefox’s human-generated lists. Additionally, <a href="https://support.mozilla.org/en-US/kb/enhanced-tracking-protection-firefox-desktop#w_what-enhanced-tracking-protection-blocks">Firefox does not fully block “tracking content”</a> in regular (non-“private”) windows by default.</p>
<p>What about Firefox’s <a href="https://blog.mozilla.org/en/products/firefox/firefox-rolls-out-total-cookie-protection-by-default-to-all-users-worldwide/">Total Cookie Protection</a> (dynamic First Party Isolation or dFPI)? Total Cookie Protection works by keeping third-party cookies isolated to the site they were set on. However, if unblocked, trackers can still use techniques like <a href="https://arxiv.org/abs/2208.12370">first-party cookie syncing</a> and <a href="https://securehomes.esat.kuleuven.be/~gacar/persistent/">browser fingerprinting</a>. They can track your IP address, or they can use some combination of these techniques. Trackers <a href="https://freedom-to-tinker.com/2020/07/14/can-the-exfiltration-of-personal-data-by-web-trackers-be-stopped/">harvest sensitive information</a>, and <a href="https://en.wikipedia.org/wiki/Malvertising">serve as vectors for malware</a>. Not to mention, unblocked trackers slow down websites and waste your bandwidth.</p>
<p>Keep in mind that Privacy Badger is <a href="#How-is-Privacy-Badger-different-from-Disconnect%2c-Adblock-Plus%2c-Ghostery%2c-and-other-blocking-extensions">not just a tracker blocker</a>.</p>

    
      <h3 id="Why-does-my-browser-connect-to-fastly.com-IP-addresses-on-startup-after-installing-Privacy-Badger">
        <a href="#Why-does-my-browser-connect-to-fastly.com-IP-addresses-on-startup-after-installing-Privacy-Badger">Why does my browser connect to fastly.com IP addresses on startup after installing Privacy Badger?</a>
      </h3>
      <p>EFF uses Fastly to host EFF’s Web resources: Fastly is EFF’s CDN. Privacy Badger pings the CDN for the following resources to ensure that the information in them is fresh even if there hasn’t been a new Privacy Badger release in a while:</p>
<ul>
<li><a href="https://www.eff.org/files/pbconfig.json">https://www.eff.org/files/pbconfig.json</a></li>
</ul>
<p>EFF does not set cookies or retain IP addresses for these queries.</p>

    
      <h3 id="Is-Privacy-Badger-spying-on-me">
        <a href="#Is-Privacy-Badger-spying-on-me">Why does Privacy Badger need access to my data for all websites?</a>
      </h3>
      <p>When you install Privacy Badger, your browser warns that Privacy Badger can “access your data for all websites” (in Firefox), or “read and change all your data on the websites you visit” (in Chrome). You are right to be alarmed. You should only install extensions made by organizations you trust.</p>
<p>Privacy Badger requires these permissions to do its job of automatically detecting and blocking trackers on all websites you visit. We are not ironically (or unironically) spying on you. For more information, see our <a href="https://github.com/EFForg/privacybadger/blob/master/doc/permissions.md">Privacy Badger extension permissions explainer</a>.</p>
<p>Note that the extension permissions warnings only cover what the extension has access to, not what the extension actually does with what it has access to (such as whether the extension secretly uploads your browsing data to its servers). Privacy Badger will never share data about your browsing unless you choose to share it (by filing a broken site report). For more information, see EFF’s <a href="https://www.eff.org/code/privacy/policy">Privacy Policy for Software</a>.</p>

    
      <h3 id="Is-Privacy-Badger-breaking-YouTube">
        <a href="#Is-Privacy-Badger-breaking-YouTube">Why aren't videos loading on YouTube? Why isn't Privacy Badger blocking ads on YouTube?</a>
      </h3>
      <p>Is YouTube not working? Try <a href="#I-found-a-bug%21-What-do-I-do-now">disabling Privacy Badger</a> on YouTube. If that resolves the issue, see if re-enabling Privacy Badger breaks YouTube again. If YouTube goes back to not working, please <a href="#I-found-a-bug%21-What-do-I-do-now">tell us</a> so we can look into what’s going on.</p>
<p>Are you surprised that ads aren’t being blocked on YouTube? Privacy Badger is primarily a privacy tool, <a href="#Why-doesn%27t-Privacy-Badger-block-all-ads">not an ad blocker</a>. When you <a href="#What-about-tracking-by-the-sites-I-actively-visit%2c-like-NYTimes.com-or-Facebook.com">visit YouTube directly</a>, Privacy Badger does not block ads on YouTube because YouTube does not use <a href="#What-is-a-third-party-tracker">“third party” trackers</a>. If you really dislike ads, you can also install a traditional ad blocker.</p>

    
      <h3 id="I-found-a-bug!-What-do-I-do-now">
        <a href="#I-found-a-bug%21-What-do-I-do-now">I need help! I found a bug! What do I do now?</a>
      </h3>
      <p>If a website isn’t working like it should, you can disable Privacy Badger just for that site, leaving Privacy Badger enabled and protecting you everywhere else. To do so, navigate to the site with the problem, click on Privacy Badger’s icon in your browser toolbar, and click the “<strong>Disable for this site</strong>” button in Privacy Badger’s popup. You can also let us know about broken sites using the “<strong>Report broken site</strong>” button.</p>
<p>To get help or to report bugs, please email <a href="mailto:extension-devs@eff.org">extension-devs@eff.org</a>. If you have a GitHub account, you can use our <a href="https://github.com/EFForg/privacybadger/issues">GitHub issue tracker</a>.</p>
<p>You can also find us on <a href="https://mastodon.social/@privacybadger">Mastodon</a> and <a href="https://bsky.app/profile/privacybadger.org">Bluesky</a>.</p>

    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Failing to Understand the Exponential, Again (103 pts)]]></title>
            <link>https://www.julian.ac/blog/2025/09/27/failing-to-understand-the-exponential-again/</link>
            <guid>45403803</guid>
            <pubDate>Sun, 28 Sep 2025 12:19:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.julian.ac/blog/2025/09/27/failing-to-understand-the-exponential-again/">https://www.julian.ac/blog/2025/09/27/failing-to-understand-the-exponential-again/</a>, See on <a href="https://news.ycombinator.com/item?id=45403803">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article_text">
    <p>The current discourse around AI progress and a <a href="https://www.wsj.com/tech/ai/ai-bubble-building-spree-55ee6128">supposed</a> “<a href="https://www.reuters.com/commentary/breakingviews/ai-investment-bubble-inflated-by-trio-dilemmas-2025-09-25/">bubble</a>” reminds me a lot of the early weeks of the Covid-19 pandemic. Long after the timing and scale of the coming global pandemic was obvious from extrapolating the exponential trends, politicians, journalists and most public commentators kept treating it as a remote possibility or a localized phenomenon.</p>
<p>Something similarly bizarre is happening with AI capabilities and further progress. People notice that while AI can now write programs, design websites, etc, it still often makes mistakes or goes in a wrong direction, and then they somehow jump to the conclusion that AI will never be able to do these tasks at human levels, or will <a href="https://www.foreignaffairs.com/united-states/cost-delusion-artificial-general-intelligence">only have a minor impact</a>. When just a few years ago, having AI do these things was complete science fiction! Or they see two consecutive model releases and don’t notice much difference in their conversations, and they conclude that AI is plateauing and scaling is over.</p>
<h3>METR</h3>
<p>Accurately evaluating AI progress is hard, and commonly requires a combination of both AI expertise and subject matter understanding. Fortunately, there are entire organizations like <a href="https://metr.org/">METR</a> whose sole purpose is to study AI capabilities! We can turn to their recent study <a href="https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/">"Measuring AI Ability to Complete Long Tasks"</a>, which measures the length of software engineering tasks models can autonomously perform:</p>
<p><img src="https://www.julian.ac/images/2025-09-27-failing-to-understand-the-exponential-metr-task-length.webp" alt="METR task length"></p>
<p>We can observe a clear exponential trend, with Sonnet 3.7 achieving the best performance by completing tasks up to an hour in length at 50% success rate.</p>
<p>However, at this point <a href="https://www.anthropic.com/news/claude-3-7-sonnet">Sonnet 3.7 is 7 months old</a>, coincidentally the same as the doubling rate claimed by METR in their study. Can we use this to verify if METR's findings hold up?</p>
<p>Yes! In fact, METR themselves keep an <a href="https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/">up-to-date plot on their study website</a>:</p>
<p><img src="https://www.julian.ac/images/2025-09-27-failing-to-understand-the-exponential-metr-task-length-v2.webp" alt="METR task length"></p>
<p>We can see the addition of recent models such as Grok 4, Opus 4.1, and GPT-5 at the top right of the graph. Not only did the prediction hold up, these recent models are actually slightly above trend, now performing tasks of more than 2 hours!</p>
<h3>GDPval</h3>
<p>A reasonable objection might be that we can't generalize from performance on software engineering tasks to the wider economy - after all, these are the tasks engineers at AI labs are bound to be most familiar with, creating some overfitting to the test set, so to speak.</p>
<p>Fortunately, we can turn to a different study, the recent <a href="https://openai.com/index/gdpval/">GDPval</a> by OpenAI - measuring model performance in 44 (!) occupations across 9 industries:</p>
<p><img src="https://www.julian.ac/images/2025-09-27-failing-to-understand-the-exponential-gdpval-category.webp" alt="GDPval categories"></p>
<p>The <a href="https://huggingface.co/datasets/openai/gdpval">evaluation tasks</a> are sourced from experienced industry professionals (avg. 14 years' experience), 30 tasks per occupation for a total of 1320 tasks. Grading is performed by blinded comparison of human and model-generated solutions, allowing for both clear preferences and ties.</p>
<p>Again we can observe a similar trend, with the latest GPT-5 already astonishingly close to human performance:</p>
<p><img src="https://www.julian.ac/images/2025-09-27-failing-to-understand-the-exponential-gdpval-progress.webp"></p><p>You might object that this plot looks like it might be levelling off, but this is probably mostly an artefact of GPT-5 being very consumer-focused. Fortunately for us, OpenAI also included other models in the evaluation<sup id="fr-benchmark-hack-1"><a href="#fn-benchmark-hack">[1]</a></sup>, and we can see that Claude Opus 4.1 (released earlier than GPT-5) performs significantly better - ahead of the trend from the previous graph, and already almost matching industry expert (!) performance:</p>
<p><img src="https://www.julian.ac/images/2025-09-27-failing-to-understand-the-exponential-gdpval-models.webp"></p><p>I want to especially commend OpenAI here for releasing an eval that shows a model from another lab outperforming their own model - this is a good sign of integrity and caring about beneficial AI outcomes!</p>
<h3>Outlook</h3>
<p>Given consistent trends of exponential performance improvements over many years and across many industries, it would be extremely surprising if these improvements suddenly stopped. Instead, even a relatively conservative extrapolation of these trends suggests that 2026 will be a pivotal year for the widespread integration of AI into the economy:</p>
<ul>
<li>Models will be able to autonomously work for full days (8 working hours) by mid-2026.</li>
<li>At least one model will match the performance of human experts across many industries before the end of 2026.</li>
<li>By the end of 2027, models will frequently outperform experts on many tasks.</li>
</ul>
<p>It may sound overly simplistic, but making predictions by extrapolating straight lines on graphs is likely to give you a better model of the future than most "experts" - even <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">better than most actual domain experts</a>!</p>
<p>For a more concrete picture of what this future would look like I recommend <a href="https://epoch.ai/blog/what-will-ai-look-like-in-2030">Epoch AI's 2030 report</a> and in particular the in-depth <a href="https://ai-2027.com/">AI 2027</a> project.</p>
<hr>
<ol><li id="fn-benchmark-hack">
<p>The underperformance of both Grok 4 and Gemini 2.5 Pro is also notable, especially given state-of-the-art claims on many benchmarks when released. Beware of <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law">Goodhart's law</a>! <a href="#fr-benchmark-hack-1">↩</a></p>
</li>

  </ol></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EPA tells some scientists to stop publishing studies (178 pts)]]></title>
            <link>https://www.washingtonpost.com/climate-environment/2025/09/20/epa-scientists-research-publications/</link>
            <guid>45403656</guid>
            <pubDate>Sun, 28 Sep 2025 11:50:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/climate-environment/2025/09/20/epa-scientists-research-publications/">https://www.washingtonpost.com/climate-environment/2025/09/20/epa-scientists-research-publications/</a>, See on <a href="https://news.ycombinator.com/item?id=45403656">Hacker News</a></p>
Couldn't get https://www.washingtonpost.com/climate-environment/2025/09/20/epa-scientists-research-publications/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Why I gave the world wide web away for free (213 pts)]]></title>
            <link>https://www.theguardian.com/technology/2025/sep/28/why-i-gave-the-world-wide-web-away-for-free</link>
            <guid>45403501</guid>
            <pubDate>Sun, 28 Sep 2025 11:17:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/technology/2025/sep/28/why-i-gave-the-world-wide-web-away-for-free">https://www.theguardian.com/technology/2025/sep/28/why-i-gave-the-world-wide-web-away-for-free</a>, See on <a href="https://news.ycombinator.com/item?id=45403501">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p><span>I</span> was 34 years old when I first had the idea for the world wide web. I took every opportunity to talk about it: pitching it in meetings, sketching it out on a whiteboard for anyone who was interested, even drawing the web in the snow with a ski pole for my friend on what was meant to be a peaceful day out.</p><p>I relentlessly petitioned bosses at the European Organization for Nuclear Research (Cern), where I worked at the time, who initially found the idea <em>“</em>a little eccentric<em>” </em>but eventually gave in and let me work on it.<em> </em>I was seized by the idea of combining two pre-existing computer technologies: the internet and hypertext, which takes an ordinary document and brings it to life by adding “links”.</p><p>I believed that giving users such a simple way to navigate the internet would unlock creativity and collaboration on a global scale. If you could put anything on it, then after a while, it would have everything on it.</p><p>But for the web to have everything on it, everyone had to be able to use it, and want to do so. This was already asking a lot. I couldn’t also ask that they pay for each search or upload they made. In order to succeed, therefore, it would have to be free. That’s why, in 1993, I convinced my Cern managers to donate the intellectual property of the world wide web, putting it into the public domain. We gave the web away to everyone.</p><p>Today, I look at my invention and I am forced to ask: is the web still free today? No, not all of it. We see a handful of large platforms harvesting users’ private data to share with commercial brokers or even repressive&nbsp;governments. We see ubiquitous algorithms that are addictive by design and damaging to our teenagers’ mental health. Trading personal data for use certainly&nbsp;does not fit with my vision for a free web.</p><p>On many platforms, we are no longer the customers, but instead have become the product. Our data, even if anonymised, is sold on to actors we never intended it to reach, who can then target us with content and&nbsp;advertising. This includes deliberately harmful content that leads to real-world violence, spreads misinformation, wreaks havoc on our psychological wellbeing and seeks to undermine social cohesion.</p><p>We have the technical capability to give that power back to the individual. <a href="https://solidproject.org/" data-link-name="in body link">Solid</a> is an open-source interoperable standard that I and my team developed at MIT more than a decade ago. Apps running on Solid don’t implicitly own your data – they have to request it from you and you choose whether to agree, or not. Rather than being in countless separate places on the internet in the hands of whomever it had been resold to, your data is in one place, controlled by you.</p><p>Sharing your information in a smart way can also liberate it. Why is your smartwatch writing your biological data to one silo in one format? Why is your credit card writing your financial data to a second silo in a different format? Why are your YouTube comments, Reddit posts, Facebook updates and tweets all stored in different places? Why is the default expectation that you aren’t supposed to be able to look at any of this stuff? You generate all this data – your actions, your choices, your body, your preferences, your decisions. You should own it. You should be empowered&nbsp;by it.</p><p>Somewhere between my original vision for web 1.0 and the rise of social media as part of web 2.0, we took the wrong path. We’re now at a new crossroads, one where we must decide if AI will be used for the betterment or to the detriment of society. How can we learn from the mistakes of the past? First of all, we must ensure policymakers do not end up playing the same decade-long game of catchup they have done over social media. The time to decide the governance model for AI was yesterday, so we must act with urgency.</p><p>In 2017, I wrote a thought experiment about an AI that works for <em>you</em>. I called it <a href="https://www.w3.org/DesignIssues/Works.html" data-link-name="in body link">Charlie</a>. Charlie works for you like your doctor or your lawyer, bound by law, regulation and codes of conduct. Why can’t the same frameworks be adopted for AI? We have learned from social media that power rests with the monopolies who control and harvest personal data. We can’t let the same thing happen with AI.</p><p>So how do we move forward? Part of the frustration with democracy in the 21st century is that governments have been too slow to meet the demands of digital citizens. The AI industry landscape is fiercely competitive, and development and governance are dictated by companies. The lesson from social media is that this will not create value for the individual.</p><p>I coded the world wide web on a single computer in a small room. But that small room didn’t belong to me, it was at Cern. Cern was created in the aftermath of the second world war by the UN and European governments who identified a historic, scientific turning point that required international collaboration. It is hard to imagine a big tech company agreeing to share the world wide web for no commercial reward like Cern allowed me to. That’s why we need a Cern-like not-for-profit body driving forward international AI research.</p><p>I gave the world wide web away for free because I thought that it would only work if it worked for everyone. Today, I believe that to be truer than ever. Regulation and global governance are technically feasible, but reliant on political willpower. If we are able to muster it, we have the chance to restore the web as a tool for collaboration, creativity and compassion across cultural borders. We can re-empower individuals, and take the web back. It’s not too late.</p><p><span data-dcr-style="bullet"></span> Tim Berners-Lee is the author of <a href="https://thisisforeveryone.timbl.com/" data-link-name="in body link">This Is for Everyone</a> <em>(Macmillan</em><em>).</em></p><h2 id="further-reading"><strong>Further reading</strong></h2><p><a href="https://guardianbookshop.com/innovators-9781471138805/?utm_source=editoriallink&amp;amp;utm_medium=merch&amp;amp;utm_campaign=article" data-link-name="in body link">The Innovators</a> by Walter Isaacson (Simon &amp; Schuster, £10.99)</p><p><a href="https://guardianbookshop.com/the-web-we-weave-9781541604124/?utm_source=editoriallink&amp;amp;utm_medium=merch&amp;amp;utm_campaign=article" data-link-name="in body link">The Web We Weave</a> by Jeff Jarvis (Basic, £25)</p><p><a href="https://guardianbookshop.com/the-history-of-the-internet-in-byte-sized-chunks-9781789295597/?utm_source=editoriallink&amp;amp;utm_medium=merch&amp;amp;utm_campaign=article" data-link-name="in body link">The History of the Internet in Byte-Sized Chunks</a> by Chris Stokel-Walker (Michael O’Mara, £12.99)</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beyond OpenMP in C++ and Rust: Taskflow, Rayon, Fork Union (103 pts)]]></title>
            <link>https://ashvardanian.com/posts/beyond-openmp-in-cpp-rust/</link>
            <guid>45402820</guid>
            <pubDate>Sun, 28 Sep 2025 08:53:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ashvardanian.com/posts/beyond-openmp-in-cpp-rust/">https://ashvardanian.com/posts/beyond-openmp-in-cpp-rust/</a>, See on <a href="https://news.ycombinator.com/item?id=45402820">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><blockquote><p>TL;DR: Most C++ and Rust thread-pool libraries leave significant performance on the table - often running 10× slower than <a href="https://en.wikipedia.org/wiki/OpenMP">OpenMP</a> on classic fork-join workloads and <a href="https://github.com/ashvardanian/ParallelReductionsBenchmark">micro-benchmarks</a>.
So I’ve drafted a minimal ~300-line library called <a href="https://github.com/ashvardanian/fork_union">Fork Union</a> that lands within 20% of OpenMP.
It does not use advanced <a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">NUMA</a> tricks; it uses only the C++ and Rust standard libraries and has no other dependencies.</p><p>Update (Sep 2025): Since the <a href="https://github.com/ashvardanian/fork_union/releases/tag/v2.0.0">v2 release</a>, Fork Union supports NUMA and Huge Pages, as well as <code>tpause</code>, <code>wfet</code>, and other “pro” features.
Check the <a href="https://github.com/ashvardanian/fork_union?tab=readme-ov-file#pro-tips">README for details</a>.</p></blockquote><p><a href="https://github.com/ashvardanian/fork_union"><img alt="Fork Union for Rust and C++" loading="lazy" src="https://ashvardanian.com/beyond-openmp-in-cpp-rust/fork_union.jpg"></a></p><p><a href="https://en.wikipedia.org/wiki/OpenMP">OpenMP</a> has been the industry workhorse for coarse-grain parallelism in C and C++ for decades.
I lean on it heavily in projects like <a href="https://github.com/unum-cloud/usearch">USearch</a>, yet I avoid it in larger systems because:</p><ul><li><strong>Fine-grain parallelism</strong> with independent subsystems doesn’t map cleanly to OpenMP’s global runtime.</li><li><strong>Portability</strong> of the C++ STL and the Rust standard library is better than OpenMP.</li><li><strong>Meta-programming</strong> with OpenMP is a pain - mixing <code>#pragma omp</code> with templates quickly becomes unmaintainable.</li></ul><p>So I went looking for ready-made thread pools in C++&nbsp;and&nbsp;Rust — only to realize <strong>most of them implement asynchronous task queues, a much heavier abstraction than OpenMP’s fork-join model</strong>.
Those extra layers introduce what I call the four horsemen of low performance:</p><ol><li><a href="#locks-and-mutexes">Locks &amp; mutexes</a> with syscalls in the hot path.</li><li><a href="#memory-allocations">Heap allocations</a> in queues, tasks, futures, and promises.</li><li><a href="#atomics-and-cas">Compare-and-swap</a> (CAS) stalls in the pessimistic path.</li><li><a href="#alignment">False sharing</a> unaligned counters thrashing cache lines.</li></ol><p>With today’s dual-socket AWS machines pushing 192 physical cores, I needed something leaner than <a href="https://github.com/taskflow/taskflow/">Taskflow</a>, <a href="https://github.com/rayon-rs/rayon/">Rayon</a>, or <a href="https://github.com/tokio-rs/tokio/">Tokio</a>.
Enter <a href="https://github.com/ashvardanian/fork_union">Fork Union</a>.</p><h2 id="benchmarks">Benchmarks</h2><p>Hardware: AWS Graviton 4 metal (single NUMA node, 96× Arm&nbsp;v9 cores,&nbsp;1 thread/core).
Workload: <a href="https://github.com/ashvardanian/ParallelReductionsBenchmark">“ParallelReductionsBenchmark”</a> - summing single-precision floats in parallel.
In this case, just one cache line (<code>float[16]</code>) per core—small enough to stress synchronization cost of the thread pool rather than arithmetic throughput of the CPU.
In other words, we are benchmarking kernels similar to:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-0-1"><a href="#hl-0-1">1</a>
</span><span id="hl-0-2"><a href="#hl-0-2">2</a>
</span><span id="hl-0-3"><a href="#hl-0-3">3</a>
</span><span id="hl-0-4"><a href="#hl-0-4">4</a>
</span><span id="hl-0-5"><a href="#hl-0-5">5</a>
</span><span id="hl-0-6"><a href="#hl-0-6">6</a>
</span><span id="hl-0-7"><a href="#hl-0-7">7</a>
</span><span id="hl-0-8"><a href="#hl-0-8">8</a>
</span><span id="hl-0-9"><a href="#hl-0-9">9</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="cpp"><span><span><span>#include</span> <span>&lt;array&gt;</span><span>
</span></span></span><span><span><span></span>
</span></span><span><span><span>float</span> <span>parallel_sum</span><span>(</span><span>std</span><span>::</span><span>array</span><span>&lt;</span><span>float</span><span>,</span> <span>96</span> <span>*</span> <span>16</span><span>&gt;</span> <span>const</span> <span>&amp;</span><span>data</span><span>)</span> <span>{</span>
</span></span><span><span>    <span>float</span> <span>result</span> <span>=</span> <span>0.0f</span><span>;</span>
</span></span><span><span><span>#pragma omp parallel for reduction(+:result) </span><span>// Not how we profile OpenMP
</span></span></span><span><span><span></span>    <span>for</span> <span>(</span><span>std</span><span>::</span><span>size_t</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>data</span><span>.</span><span>size</span><span>();</span> <span>++</span><span>i</span><span>)</span>
</span></span><span><span>        <span>result</span> <span>+=</span> <span>data</span><span>[</span><span>i</span><span>];</span>
</span></span><span><span>    <span>return</span> <span>result</span><span>;</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></td></tr></tbody></table></div><p><a href="https://github.com/google/benchmark">Google Benchmark</a> numbers for the C++ version of Fork Union, compared to OpenMP, <a href="https://github.com/taskflow/taskflow/">Taskflow</a>, and allocating 96× <code>std::thread</code> objects on-demand, are as follows:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-1-1"><a href="#hl-1-1">1</a>
</span><span id="hl-1-2"><a href="#hl-1-2">2</a>
</span><span id="hl-1-3"><a href="#hl-1-3">3</a>
</span><span id="hl-1-4"><a href="#hl-1-4">4</a>
</span><span id="hl-1-5"><a href="#hl-1-5">5</a>
</span><span id="hl-1-6"><a href="#hl-1-6">6</a>
</span><span id="hl-1-7"><a href="#hl-1-7">7</a>
</span><span id="hl-1-8"><a href="#hl-1-8">8</a>
</span><span id="hl-1-9"><a href="#hl-1-9">9</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="sh"><span><span><span>PARALLEL_REDUCTIONS_LENGTH</span><span>=</span><span>1536</span> build_release/reduce_bench
</span></span><span><span>
</span></span><span><span>-----------------------------------
</span></span><span><span>Benchmark           UserCounters...
</span></span><span><span>-----------------------------------
</span></span><span><span>std::threads        bytes/s<span>=</span>3.00106 MB/s
</span></span><span><span>tf::taskflow        bytes/s<span>=</span>76.2837 MB/s
</span></span><span><span>av::fork_union      bytes/s<span>=</span>467.714 MB/s
</span></span><span><span>openmp              bytes/s<span>=</span>585.492 MB/s
</span></span></code></pre></td></tr></tbody></table></div><blockquote><p>I’ve cleaned up the output, focusing only on the relevant rows and the reduction throughput.</p></blockquote><p><a href="https://github.com/bheisler/criterion.rs">Criterion.rs</a> numbers for the Rust version of Fork Union, compared to <a href="https://github.com/rayon-rs/rayon/">Rayon</a>, <a href="https://github.com/tokio-rs/tokio/">Tokio</a>, and Smol’s <a href="https://github.com/smol-rs/async-executor">Async Executors</a>, are as follows:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-2-1"><a href="#hl-2-1">1</a>
</span><span id="hl-2-2"><a href="#hl-2-2">2</a>
</span><span id="hl-2-3"><a href="#hl-2-3">3</a>
</span><span id="hl-2-4"><a href="#hl-2-4">4</a>
</span><span id="hl-2-5"><a href="#hl-2-5">5</a>
</span><span id="hl-2-6"><a href="#hl-2-6">6</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="sh"><span><span>$ <span>PARALLEL_REDUCTIONS_LENGTH</span><span>=</span><span>1536</span> cargo +nightly bench -- --output-format bencher
</span></span><span><span>
</span></span><span><span><span>test</span> fork_union ... bench:  5,150 ns/iter <span>(</span>+/- 402<span>)</span>
</span></span><span><span><span>test</span> rayon ... bench:      47,251 ns/iter <span>(</span>+/- 3,985<span>)</span>
</span></span><span><span><span>test</span> smol ... bench:       54,931 ns/iter <span>(</span>+/- 10<span>)</span>
</span></span><span><span><span>test</span> tokio ... bench:     240,707 ns/iter <span>(</span>+/- 921<span>)</span>
</span></span></code></pre></td></tr></tbody></table></div><p>The timing methods used in those two executables are different, but the relative observations should hold.</p><ul><li>Spawning new threads is obviously too expensive.</li><li>Most reusable thread pools are still 10x slower to sync than OpenMP.</li><li>OpenMP isn’t easy to compete with and still outperforms Fork Union by 20%.</li></ul><p>This clearly shows, how important it is to chose the right tool for the job.
Don’t pick an asynchronous task pool for a fork-join blocking workload!</p><h2 id="four-horsemen-of-performance">Four Horsemen of Performance</h2><blockquote><p>This article won’t be a deep dive into those topics.
Each deserves its own article and a proper benchmark, with some good ones already available and linked.</p></blockquote><h3 id="locks-and-mutexes">Locks and Mutexes</h3><p>Unlike the <a href="https://en.cppreference.com/w/cpp/atomic/atomic"><code>std::atomic</code></a>, the <a href="https://en.cppreference.com/w/cpp/thread/mutex"><code>std::mutex</code></a> update may result in a system call, and it can be expensive to acquire and release.
Its implementations generally have 2 executable paths:</p><ul><li>the fast path, where the mutex is not contended, where it first tries to grab the mutex via a compare-and-swap operation, and if it succeeds, it returns immediately.</li><li>the slow path, where the mutex is contended, and it has to go through the kernel to block the thread until the mutex is available.</li></ul><p>On Linux, the latter translates to a <a href="https://en.wikipedia.org/wiki/Futex">“futex” syscall</a> and an expensive <a href="https://lwn.net/Articles/940944/#:~:text=shared%20between%20at%20least%20two,system%20call">context switch</a>.
In Rust, the same applies to <a href="https://doc.rust-lang.org/std/sync/atomic/"><code>std::async::atomic</code></a> and <a href="https://doc.rust-lang.org/std/sync/struct.Mutex.html"><code>std::sync::Mutex</code></a>.
Prefer the former when possible.</p><h3 id="memory-allocations">Memory Allocations</h3><p>Most thread-pools use classes like <a href="https://en.cppreference.com/w/cpp/thread/future"><code>std::future</code></a>, <a href="https://en.cppreference.com/w/cpp/thread/packaged_task"><code>std::packaged_task</code></a>, <a href="https://en.cppreference.com/w/cpp/utility/functional/function"><code>std::function</code></a>, <a href="https://en.cppreference.com/w/cpp/container/queue"><code>std::queue</code></a>, <a href="https://en.cppreference.com/w/cpp/thread/condition_variable"><code>std::conditional_variable</code></a>.</p><blockquote><p>In Rust land, there will often be a <a href="https://doc.rust-lang.org/std/boxed/struct.Box.html"><code>std::Box</code></a>, <a href="https://doc.rust-lang.org/std/sync/struct.Arc.html"><code>std::Arc</code></a>, <a href="https://doc.rust-lang.org/std/collections/struct.VecDeque.html"><code>std::collections::VecDeque</code></a>, <a href="https://doc.rust-lang.org/std/sync/mpsc/index.html"><code>std::sync::mpsc</code></a> or even <a href="https://doc.rust-lang.org/std/sync/mpmc/index.html"><code>std::sync::mpmc</code></a>.</p></blockquote><p>Most of those, I believe, aren’t unusable in Big-Data applications, where you always operate in memory-constrained environments:</p><ul><li>Raising a <a href="https://en.cppreference.com/w/cpp/memory/new/bad_alloc"><code>std::bad_alloc</code></a> exception when there is no memory left and just hoping that someone up the call stack will catch it is not a great design idea for Systems Engineering.</li><li>The threat of having to synchronize ~200 physical CPU cores across 2-8 sockets and potentially dozens of NUMA nodes around a shared global memory allocator practically means you can’t have predictable performance.</li></ul><p>As we focus on a simpler <del>concurrency</del> parallelism model, we can avoid the complexity of allocating shared states, wrapping callbacks into some heap-allocated “tasks”, and a lot of other boilerplates.</p><p>Less work = more performance.</p><h3 id="atomics-and-cas">Atomics and CAS</h3><p>Once you get to the lowest-level primitives on concurrency, you end up with the <code>std::atomic</code> and a small set of hardware-supported atomic instructions.
Hardware implements it differently:</p><ul><li>x86 is built around the “Total Store Order” (TSO) <a href="https://en.wikipedia.org/wiki/Memory_ordering">memory consistency model</a> and provides <code>LOCK</code> variants of the <code>ADD</code> and <code>CMPXCHG</code>. These variants act as full-blown “fences” — no loads or stores can be reordered across them. This makes atomic operations on x86 straightforward but heavyweight.</li><li>Arm, on the other hand, has a “weak” memory model and provides a set of atomic instructions that are not fenced and match the C++ concurrency model. It offers <code>acquire</code>, <code>release</code>, and <code>acq_rel</code> variants of each atomic instruction — such as <code>LDADD</code>, <code>STADD</code>, and <code>CAS</code> — which allow precise control over visibility and order, especially with the introduction of <a href="https://learn.arm.com/learning-paths/servers-and-cloud-computing/lse/intro/">“Large System Extension” (LSE)</a> instructions in Armv8.1-A.</li></ul><p>A locked atomic on x86 requires the cache line in the Exclusive state in the requester’s L1 cache.
This would incur a coherence transaction (Read-for-Ownership) if another core had the line.
Both Intel and AMD handle this similarly.</p><p>It makes <a href="https://arangodb.com/2021/02/cpp-memory-model-migrating-from-x86-to-arm">Arm and Power much more suitable for lock-free programming</a> and concurrent data structures, but some observations hold for both platforms.
Most importantly, “Compare and Swap” (CAS) is costly and should be avoided at all costs.</p><p>On x86, for example, the <code>LOCK ADD</code> <a href="https://travisdowns.github.io/blog/2020/07/06/concurrency-costs">can easily take 50 CPU cycles</a>.
It is 50x slower than a regular <code>ADD</code> instruction but still easily 5-10x faster than a <code>LOCK CMPXCHG</code> instruction.
Once the contention rises, the gap naturally widens, further amplified by the increased “failure” rate of the CAS operation when the value being compared has already changed.
That’s why, for the “dynamic” mode, we resort to using an additional atomic variable rather than more typical CAS-based implementations.</p><h3 id="alignment">Alignment</h3><p>Assuming a thread pool is a heavy object anyway, nobody will care if it’s a bit larger than expected.
That allows us to over-align the internal counters to <a href="https://en.cppreference.com/w/cpp/thread/hardware_destructive_interference_size"><code>std::hardware_destructive_interference_size</code></a> or <a href="https://en.cppreference.com/w/cpp/types/max_align_t"><code>std::max_align_t</code></a> to avoid false sharing.
In that case, even on x86, where the entire cache will be exclusively owned by a single thread, in eager mode, we end up effectively “pipelining” the execution, where one thread may be incrementing the “in-flight” counter while the other is decrementing the “remaining” counter.
Others are executing the loop body in between.</p><h2 id="comparing-apis">Comparing APIs</h2><h3 id="fork-union">Fork Union</h3><p>Fork Union has a straightforward goal, so its API is equally clear.
There are only 4 core interfaces:</p><ul><li><code>for_each_thread</code> - to dispatch a callback per thread, similar to <code>#pragma omp parallel</code>.</li><li><code>for_each_static</code> - for individual evenly-sized tasks, similar to <code>#pragma omp for schedule(static)</code>.</li><li><code>for_each_slice</code> - for slices of evenly-sized tasks, similar to nested <code>#pragma omp for schedule(static)</code>.</li><li><code>for_each_dynamic</code> - for individual unevenly-sized tasks, similar to <code>#pragma omp for schedule(dynamic, 1)</code>.</li></ul><p>They all receive a C++ lambda or a Rust closure and a range of tasks to execute.
The construction of the thread pool itself is a bit trickier than typically in standard libraries, as “exceptions” and “panics” are not allowed.
So, the constructor can’t perform any real work.
In C++, the <code>try_spawn</code> method can be called to allocate all the threads:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-3-1"><a href="#hl-3-1"> 1</a>
</span><span id="hl-3-2"><a href="#hl-3-2"> 2</a>
</span><span id="hl-3-3"><a href="#hl-3-3"> 3</a>
</span><span id="hl-3-4"><a href="#hl-3-4"> 4</a>
</span><span id="hl-3-5"><a href="#hl-3-5"> 5</a>
</span><span id="hl-3-6"><a href="#hl-3-6"> 6</a>
</span><span id="hl-3-7"><a href="#hl-3-7"> 7</a>
</span><span id="hl-3-8"><a href="#hl-3-8"> 8</a>
</span><span id="hl-3-9"><a href="#hl-3-9"> 9</a>
</span><span id="hl-3-10"><a href="#hl-3-10">10</a>
</span><span id="hl-3-11"><a href="#hl-3-11">11</a>
</span><span id="hl-3-12"><a href="#hl-3-12">12</a>
</span><span id="hl-3-13"><a href="#hl-3-13">13</a>
</span><span id="hl-3-14"><a href="#hl-3-14">14</a>
</span><span id="hl-3-15"><a href="#hl-3-15">15</a>
</span><span id="hl-3-16"><a href="#hl-3-16">16</a>
</span><span id="hl-3-17"><a href="#hl-3-17">17</a>
</span><span id="hl-3-18"><a href="#hl-3-18">18</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="cpp"><span><span><span>#include</span> <span>&lt;fork_union.hpp&gt;</span><span>   </span><span>// `fork_union_t`
</span></span></span><span><span><span></span><span>#include</span> <span>&lt;cstdio&gt;</span><span>           </span><span>// `stderr`
</span></span></span><span><span><span></span><span>#include</span> <span>&lt;cstdlib&gt;</span><span>          </span><span>// `EXIT_SUCCESS`
</span></span></span><span><span><span></span>
</span></span><span><span><span>namespace</span> <span>fun</span> <span>=</span> <span>ashvardanian</span><span>::</span><span>fork_union</span><span>;</span>
</span></span><span><span>
</span></span><span><span><span>int</span> <span>main</span><span>()</span> <span>{</span>
</span></span><span><span>    <span>fun</span><span>::</span><span>fork_union_t</span> <span>pool</span><span>;</span>
</span></span><span><span>    <span>if</span> <span>(</span><span>!</span><span>pool</span><span>.</span><span>try_spawn</span><span>(</span><span>std</span><span>::</span><span>thread</span><span>::</span><span>hardware_concurrency</span><span>()))</span> <span>{</span>
</span></span><span><span>        <span>std</span><span>::</span><span>fprintf</span><span>(</span><span>stderr</span><span>,</span> <span>"Failed to fork the threads</span><span>\n</span><span>"</span><span>);</span>
</span></span><span><span>        <span>return</span> <span>EXIT_FAILURE</span><span>;</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span>
</span></span><span><span>    <span>pool</span><span>.</span><span>for_each_thread</span><span>([</span><span>&amp;</span><span>](</span><span>std</span><span>::</span><span>size_t</span> <span>thread_index</span><span>)</span> <span>noexcept</span> <span>{</span>
</span></span><span><span>        <span>std</span><span>::</span><span>printf</span><span>(</span><span>"Hello from thread # %zu (of %zu)</span><span>\n</span><span>"</span><span>,</span> <span>thread_index</span> <span>+</span> <span>1</span><span>,</span> <span>pool</span><span>.</span><span>count_threads</span><span>());</span>
</span></span><span><span>    <span>});</span>
</span></span><span><span>    <span>return</span> <span>EXIT_SUCCESS</span><span>;</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></td></tr></tbody></table></div><blockquote><p>As you may have noticed, the lambdas are forced to be <code>noexcept</code> and can’t return anything.
This is a design choice that vastly simplifies the implementation.</p></blockquote><p>In Rust, similarly, the <code>try_spawn</code> method can be used:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-4-1"><a href="#hl-4-1"> 1</a>
</span><span id="hl-4-2"><a href="#hl-4-2"> 2</a>
</span><span id="hl-4-3"><a href="#hl-4-3"> 3</a>
</span><span id="hl-4-4"><a href="#hl-4-4"> 4</a>
</span><span id="hl-4-5"><a href="#hl-4-5"> 5</a>
</span><span id="hl-4-6"><a href="#hl-4-6"> 6</a>
</span><span id="hl-4-7"><a href="#hl-4-7"> 7</a>
</span><span id="hl-4-8"><a href="#hl-4-8"> 8</a>
</span><span id="hl-4-9"><a href="#hl-4-9"> 9</a>
</span><span id="hl-4-10"><a href="#hl-4-10">10</a>
</span><span id="hl-4-11"><a href="#hl-4-11">11</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>#![feature(allocator_api)]</span><span>
</span></span></span><span><span><span></span><span>use</span><span> </span><span>std</span>::<span>error</span>::<span>Error</span><span>;</span><span>
</span></span></span><span><span><span></span><span>use</span><span> </span><span>fork_union</span>::<span>ForkUnion</span><span>;</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>fn</span> <span>main</span><span>()</span><span> </span>-&gt; <span>Result</span><span>&lt;</span><span>(),</span><span> </span><span>Box</span><span>&lt;</span><span>dyn</span><span> </span><span>Error</span><span>&gt;&gt;</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>pool</span><span> </span><span>=</span><span> </span><span>ForkUnion</span>::<span>try_spawn</span><span>(</span><span>4</span><span>)</span><span>?</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>pool</span><span>.</span><span>for_each_thread</span><span>(</span><span>|</span><span>thread_index</span><span>|</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>println!</span><span>(</span><span>"Hello from thread # </span><span>{}</span><span> (of </span><span>{}</span><span>)"</span><span>,</span><span> </span><span>thread_index</span><span> </span><span>+</span><span> </span><span>1</span><span>,</span><span> </span><span>pool</span><span>.</span><span>count_threads</span><span>());</span><span>
</span></span></span><span><span><span>    </span><span>});</span><span>
</span></span></span><span><span><span>    </span><span>Ok</span><span>(())</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p>Assuming Rust has no function overloading, there are a few alternatives:</p><ul><li><code>try_spawn</code> - to spawn a thread pool with the main allocator.</li><li><code>try_spawn_in</code> - to spawn a thread pool with a custom allocator.</li><li><code>try_named_spawn</code> - to spawn a thread pool with the main allocator and a name.</li><li><code>try_named_spawn_in</code> - to spawn a thread pool with a custom allocator and a name.</li></ul><h3 id="rayon">Rayon</h3><p>Rayon is the go-to Rust library for data parallelism.
It suffers from the same core design issues as every other thread pool I’ve looked at on GitHub, but it’s fair to say that at the high level, it provides outstanding coverage for various parallel iterators!
As such, there is <a href="https://github.com/ashvardanian/fork_union/issues/2">an open call to explore similar “Map-Reduce” and “Map-Fork-Reduce” patterns in Fork Union</a> to see if they can be implemented efficiently.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-5-1"><a href="#hl-5-1">1</a>
</span><span id="hl-5-2"><a href="#hl-5-2">2</a>
</span><span id="hl-5-3"><a href="#hl-5-3">3</a>
</span><span id="hl-5-4"><a href="#hl-5-4">4</a>
</span><span id="hl-5-5"><a href="#hl-5-5">5</a>
</span><span id="hl-5-6"><a href="#hl-5-6">6</a>
</span><span id="hl-5-7"><a href="#hl-5-7">7</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>use</span><span> </span><span>rayon</span>::<span>prelude</span>::<span>*</span><span>;</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>fn</span> <span>sum_of_squares</span><span>(</span><span>input</span>: <span>&amp;</span><span>[</span><span>i32</span><span>])</span><span> </span>-&gt; <span>i32</span> <span>{</span><span>
</span></span></span><span><span><span>    </span><span>input</span><span>.</span><span>par_iter</span><span>()</span><span> </span><span>// &lt;-- just change that!
</span></span></span><span><span><span></span><span>         </span><span>.</span><span>map</span><span>(</span><span>|&amp;</span><span>i</span><span>|</span><span> </span><span>i</span><span> </span><span>*</span><span> </span><span>i</span><span>)</span><span>
</span></span></span><span><span><span>         </span><span>.</span><span>sum</span><span>()</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p>The default <code>.par_iter()</code> API of Rayon, <a href="https://github.com/rayon-rs/rayon/blob/ae07384e3e0b238cea89f0c14891f351c65a5cee/README.md?plain=1#L26-L33">at the start of the README.md</a>, is not how I’ve used it in “Parallel Reductions Benchmark”.
To ensure that we are benchmarking the actual synchronization cost of the thread pool, I’ve gone directly to the underlying <code>rayon::ThreadPool</code> API:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-6-1"><a href="#hl-6-1"> 1</a>
</span><span id="hl-6-2"><a href="#hl-6-2"> 2</a>
</span><span id="hl-6-3"><a href="#hl-6-3"> 3</a>
</span><span id="hl-6-4"><a href="#hl-6-4"> 4</a>
</span><span id="hl-6-5"><a href="#hl-6-5"> 5</a>
</span><span id="hl-6-6"><a href="#hl-6-6"> 6</a>
</span><span id="hl-6-7"><a href="#hl-6-7"> 7</a>
</span><span id="hl-6-8"><a href="#hl-6-8"> 8</a>
</span><span id="hl-6-9"><a href="#hl-6-9"> 9</a>
</span><span id="hl-6-10"><a href="#hl-6-10">10</a>
</span><span id="hl-6-11"><a href="#hl-6-11">11</a>
</span><span id="hl-6-12"><a href="#hl-6-12">12</a>
</span><span id="hl-6-13"><a href="#hl-6-13">13</a>
</span><span id="hl-6-14"><a href="#hl-6-14">14</a>
</span><span id="hl-6-15"><a href="#hl-6-15">15</a>
</span><span id="hl-6-16"><a href="#hl-6-16">16</a>
</span><span id="hl-6-17"><a href="#hl-6-17">17</a>
</span><span id="hl-6-18"><a href="#hl-6-18">18</a>
</span><span id="hl-6-19"><a href="#hl-6-19">19</a>
</span><span id="hl-6-20"><a href="#hl-6-20">20</a>
</span><span id="hl-6-21"><a href="#hl-6-21">21</a>
</span><span id="hl-6-22"><a href="#hl-6-22">22</a>
</span><span id="hl-6-23"><a href="#hl-6-23">23</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span><span> </span><span>fn</span> <span>sum_rayon</span><span>(</span><span>pool</span>: <span>&amp;</span><span>rayon</span>::<span>ThreadPool</span><span>,</span><span> </span><span>data</span>: <span>&amp;</span><span>[</span><span>f32</span><span>],</span><span> </span><span>partial_sums</span>: <span>&amp;</span><span>mut</span><span> </span><span>[</span><span>f64</span><span>])</span><span> </span>-&gt; <span>f64</span> <span>{</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>cores</span><span> </span><span>=</span><span> </span><span>pool</span><span>.</span><span>current_num_threads</span><span>();</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>chunk_size</span><span> </span><span>=</span><span> </span><span>scalars_per_core</span><span>(</span><span>data</span><span>.</span><span>len</span><span>(),</span><span> </span><span>cores</span><span>);</span><span>       </span><span>// Defined elsewhere
</span></span></span><span><span><span></span><span>    </span><span>let</span><span> </span><span>partial_sums_ptr</span><span> </span><span>=</span><span> </span><span>partial_sums</span><span>.</span><span>as_mut_ptr</span><span>()</span><span> </span><span>as</span><span> </span><span>usize</span><span>;</span><span>  </span><span>// Pointers aren't safe to pass around
</span></span></span><span><span><span></span><span>
</span></span></span><span><span><span>    </span><span>pool</span><span>.</span><span>broadcast</span><span>(</span><span>|</span><span>context</span>: <span>rayon</span>::<span>BroadcastContext</span><span>&lt;</span><span>'_</span><span>&gt;|</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>thread_index</span><span> </span><span>=</span><span> </span><span>context</span><span>.</span><span>index</span><span>();</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>start</span><span> </span><span>=</span><span> </span><span>thread_index</span><span> </span><span>*</span><span> </span><span>chunk_size</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>if</span><span> </span><span>start</span><span> </span><span>&gt;=</span><span> </span><span>data</span><span>.</span><span>len</span><span>()</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>return</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>stop</span><span> </span><span>=</span><span> </span><span>std</span>::<span>cmp</span>::<span>min</span><span>(</span><span>start</span><span> </span><span>+</span><span> </span><span>chunk_size</span><span>,</span><span> </span><span>data</span><span>.</span><span>len</span><span>());</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>partial_sum</span><span> </span><span>=</span><span> </span><span>sum_unrolled</span><span>(</span><span>&amp;</span><span>data</span><span>[</span><span>start</span><span>..</span><span>stop</span><span>]);</span><span>
</span></span></span><span><span><span>        </span><span>unsafe</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>ptr</span>::<span>write</span><span>(</span><span> </span><span>// Cast back to a pointer:
</span></span></span><span><span><span></span><span>                </span><span>(</span><span>partial_sums_ptr</span><span> </span><span>as</span><span> </span><span>*</span><span>mut</span><span> </span><span>f64</span><span>).</span><span>add</span><span>(</span><span>thread_index</span><span>),</span><span>
</span></span></span><span><span><span>                </span><span>partial_sum</span><span>,</span><span>
</span></span></span><span><span><span>            </span><span>);</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>    </span><span>});</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>partial_sums</span><span>.</span><span>iter</span><span>().</span><span>copied</span><span>().</span><span>sum</span><span>()</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><h3 id="taskflow">Taskflow</h3><p>Taskflow is one of the most popular C++ libraries for parallelism.
It has many features, including async execution graphs on CPUs and GPUs.
<a href="https://github.com/taskflow/taskflow/blob/b3c1e5fd8e2d67eaead944a8d869f87e6b58bbbe/README.md?plain=1#L84-L106">The most common example</a> looks like this:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-7-1"><a href="#hl-7-1"> 1</a>
</span><span id="hl-7-2"><a href="#hl-7-2"> 2</a>
</span><span id="hl-7-3"><a href="#hl-7-3"> 3</a>
</span><span id="hl-7-4"><a href="#hl-7-4"> 4</a>
</span><span id="hl-7-5"><a href="#hl-7-5"> 5</a>
</span><span id="hl-7-6"><a href="#hl-7-6"> 6</a>
</span><span id="hl-7-7"><a href="#hl-7-7"> 7</a>
</span><span id="hl-7-8"><a href="#hl-7-8"> 8</a>
</span><span id="hl-7-9"><a href="#hl-7-9"> 9</a>
</span><span id="hl-7-10"><a href="#hl-7-10">10</a>
</span><span id="hl-7-11"><a href="#hl-7-11">11</a>
</span><span id="hl-7-12"><a href="#hl-7-12">12</a>
</span><span id="hl-7-13"><a href="#hl-7-13">13</a>
</span><span id="hl-7-14"><a href="#hl-7-14">14</a>
</span><span id="hl-7-15"><a href="#hl-7-15">15</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="cpp"><span><span><span>#include</span> <span>&lt;taskflow/taskflow.hpp&gt;</span><span>
</span></span></span><span><span><span></span><span>int</span> <span>main</span><span>()</span> <span>{</span>
</span></span><span><span>    <span>tf</span><span>::</span><span>Executor</span> <span>executor</span><span>;</span>
</span></span><span><span>    <span>tf</span><span>::</span><span>Taskflow</span> <span>taskflow</span><span>;</span>
</span></span><span><span>    <span>auto</span> <span>[</span><span>A</span><span>,</span> <span>B</span><span>,</span> <span>C</span><span>,</span> <span>D</span><span>]</span> <span>=</span> <span>taskflow</span><span>.</span><span>emplace</span><span>(</span> <span>// create four tasks
</span></span></span><span><span><span></span>        <span>[]</span> <span>()</span> <span>{</span> <span>std</span><span>::</span><span>cout</span> <span>&lt;&lt;</span> <span>"TaskA</span><span>\n</span><span>"</span><span>;</span> <span>},</span>
</span></span><span><span>        <span>[]</span> <span>()</span> <span>{</span> <span>std</span><span>::</span><span>cout</span> <span>&lt;&lt;</span> <span>"TaskB</span><span>\n</span><span>"</span><span>;</span> <span>},</span>
</span></span><span><span>        <span>[]</span> <span>()</span> <span>{</span> <span>std</span><span>::</span><span>cout</span> <span>&lt;&lt;</span> <span>"TaskC</span><span>\n</span><span>"</span><span>;</span> <span>},</span>
</span></span><span><span>        <span>[]</span> <span>()</span> <span>{</span> <span>std</span><span>::</span><span>cout</span> <span>&lt;&lt;</span> <span>"TaskD</span><span>\n</span><span>"</span><span>;</span> <span>}</span> 
</span></span><span><span>    <span>);</span>                                  
</span></span><span><span>    <span>A</span><span>.</span><span>precede</span><span>(</span><span>B</span><span>,</span> <span>C</span><span>);</span> <span>// A runs before B and C
</span></span></span><span><span><span></span>    <span>D</span><span>.</span><span>succeed</span><span>(</span><span>B</span><span>,</span> <span>C</span><span>);</span> <span>// D runs after  B and C
</span></span></span><span><span><span></span>    <span>executor</span><span>.</span><span>run</span><span>(</span><span>taskflow</span><span>).</span><span>wait</span><span>();</span> 
</span></span><span><span>    <span>return</span> <span>0</span><span>;</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></td></tr></tbody></table></div><p>Despite being just an example, it clearly shows how different Taskflow’s core objectives are from OpenMP and Fork Union.
It is still probably mainly used for simple static parallelism, similar to our case without complex dependencies and the <code>taskflow</code> can be reused.
Here is how “Parallel Reductions Benchmark” wraps Taskflow:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-8-1"><a href="#hl-8-1"> 1</a>
</span><span id="hl-8-2"><a href="#hl-8-2"> 2</a>
</span><span id="hl-8-3"><a href="#hl-8-3"> 3</a>
</span><span id="hl-8-4"><a href="#hl-8-4"> 4</a>
</span><span id="hl-8-5"><a href="#hl-8-5"> 5</a>
</span><span id="hl-8-6"><a href="#hl-8-6"> 6</a>
</span><span id="hl-8-7"><a href="#hl-8-7"> 7</a>
</span><span id="hl-8-8"><a href="#hl-8-8"> 8</a>
</span><span id="hl-8-9"><a href="#hl-8-9"> 9</a>
</span><span id="hl-8-10"><a href="#hl-8-10">10</a>
</span><span id="hl-8-11"><a href="#hl-8-11">11</a>
</span><span id="hl-8-12"><a href="#hl-8-12">12</a>
</span><span id="hl-8-13"><a href="#hl-8-13">13</a>
</span><span id="hl-8-14"><a href="#hl-8-14">14</a>
</span><span id="hl-8-15"><a href="#hl-8-15">15</a>
</span><span id="hl-8-16"><a href="#hl-8-16">16</a>
</span><span id="hl-8-17"><a href="#hl-8-17">17</a>
</span><span id="hl-8-18"><a href="#hl-8-18">18</a>
</span><span id="hl-8-19"><a href="#hl-8-19">19</a>
</span><span id="hl-8-20"><a href="#hl-8-20">20</a>
</span><span id="hl-8-21"><a href="#hl-8-21">21</a>
</span><span id="hl-8-22"><a href="#hl-8-22">22</a>
</span><span id="hl-8-23"><a href="#hl-8-23">23</a>
</span><span id="hl-8-24"><a href="#hl-8-24">24</a>
</span><span id="hl-8-25"><a href="#hl-8-25">25</a>
</span><span id="hl-8-26"><a href="#hl-8-26">26</a>
</span><span id="hl-8-27"><a href="#hl-8-27">27</a>
</span><span id="hl-8-28"><a href="#hl-8-28">28</a>
</span><span id="hl-8-29"><a href="#hl-8-29">29</a>
</span><span id="hl-8-30"><a href="#hl-8-30">30</a>
</span><span id="hl-8-31"><a href="#hl-8-31">31</a>
</span><span id="hl-8-32"><a href="#hl-8-32">32</a>
</span><span id="hl-8-33"><a href="#hl-8-33">33</a>
</span><span id="hl-8-34"><a href="#hl-8-34">34</a>
</span><span id="hl-8-35"><a href="#hl-8-35">35</a>
</span><span id="hl-8-36"><a href="#hl-8-36">36</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="cpp"><span><span><span>template</span> <span>&lt;</span><span>typename</span> <span>serial_at</span> <span>=</span> <span>stl_accumulate_gt</span><span>&lt;</span><span>float</span><span>&gt;&gt;</span>
</span></span><span><span><span>class</span> <span>taskflow_gt</span> <span>{</span>
</span></span><span><span>    <span>float</span> <span>const</span> <span>*</span><span>const</span> <span>begin_</span> <span>=</span> <span>nullptr</span><span>;</span>
</span></span><span><span>    <span>float</span> <span>const</span> <span>*</span><span>const</span> <span>end_</span> <span>=</span> <span>nullptr</span><span>;</span>
</span></span><span><span>    <span>std</span><span>::</span><span>size_t</span> <span>const</span> <span>cores_</span> <span>=</span> <span>0</span><span>;</span>
</span></span><span><span>
</span></span><span><span>    <span>tf</span><span>::</span><span>Executor</span> <span>executor_</span><span>;</span>
</span></span><span><span>    <span>tf</span><span>::</span><span>Taskflow</span> <span>taskflow_</span><span>;</span>
</span></span><span><span>
</span></span><span><span>    <span>struct</span> <span>alignas</span><span>(</span><span>128</span><span>)</span> <span>thread_result_t</span> <span>{</span>
</span></span><span><span>        <span>double</span> <span>partial_sum</span> <span>=</span> <span>0.0</span><span>;</span>
</span></span><span><span>    <span>};</span>
</span></span><span><span>    <span>std</span><span>::</span><span>vector</span><span>&lt;</span><span>thread_result_t</span><span>&gt;</span> <span>sums_</span><span>;</span>
</span></span><span><span>
</span></span><span><span>  <span>public</span><span>:</span>
</span></span><span><span>    <span>taskflow_gt</span><span>()</span> <span>=</span> <span>default</span><span>;</span>
</span></span><span><span>    <span>taskflow_gt</span><span>(</span><span>float</span> <span>const</span> <span>*</span><span>b</span><span>,</span> <span>float</span> <span>const</span> <span>*</span><span>e</span><span>)</span>
</span></span><span><span>        <span>:</span> <span>begin_</span> <span>{</span><span>b</span><span>},</span> <span>end_</span> <span>{</span><span>e</span><span>},</span> <span>cores_</span> <span>{</span><span>total_cores</span><span>()},</span> <span>executor_</span> <span>{</span><span>static_cast</span><span>&lt;</span><span>unsigned</span><span>&gt;</span><span>(</span><span>cores_</span><span>)},</span> <span>sums_</span> <span>{</span><span>cores_</span><span>}</span> <span>{</span>
</span></span><span><span>
</span></span><span><span>        <span>auto</span> <span>const</span> <span>input_size</span> <span>=</span> <span>static_cast</span><span>&lt;</span><span>std</span><span>::</span><span>size_t</span><span>&gt;</span><span>(</span><span>end_</span> <span>-</span> <span>begin_</span><span>);</span>
</span></span><span><span>        <span>auto</span> <span>const</span> <span>chunk_size</span> <span>=</span> <span>scalars_per_core</span><span>(</span><span>input_size</span><span>,</span> <span>cores_</span><span>);</span>
</span></span><span><span>        <span>for</span> <span>(</span><span>std</span><span>::</span><span>size_t</span> <span>thread_index</span> <span>=</span> <span>0</span><span>;</span> <span>thread_index</span> <span>&lt;</span> <span>cores_</span><span>;</span> <span>++</span><span>thread_index</span><span>)</span> <span>{</span>
</span></span><span><span>            <span>taskflow_</span><span>.</span><span>emplace</span><span>([</span><span>this</span><span>,</span> <span>input_size</span><span>,</span> <span>chunk_size</span><span>,</span> <span>thread_index</span><span>]</span> <span>{</span>
</span></span><span><span>                <span>std</span><span>::</span><span>size_t</span> <span>const</span> <span>start</span> <span>=</span> <span>std</span><span>::</span><span>min</span><span>(</span><span>thread_index</span> <span>*</span> <span>chunk_size</span><span>,</span> <span>input_size</span><span>);</span>
</span></span><span><span>                <span>std</span><span>::</span><span>size_t</span> <span>const</span> <span>stop</span> <span>=</span> <span>std</span><span>::</span><span>min</span><span>(</span><span>start</span> <span>+</span> <span>chunk_size</span><span>,</span> <span>input_size</span><span>);</span>
</span></span><span><span>                <span>sums_</span><span>[</span><span>thread_index</span><span>].</span><span>partial_sum</span> <span>=</span> <span>serial_at</span> <span>{</span><span>begin_</span> <span>+</span> <span>start</span><span>,</span> <span>begin_</span> <span>+</span> <span>stop</span><span>}();</span>
</span></span><span><span>            <span>});</span>
</span></span><span><span>        <span>}</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span>
</span></span><span><span>    <span>double</span> <span>operator</span><span>()()</span> <span>{</span>
</span></span><span><span>        <span>executor_</span><span>.</span><span>run</span><span>(</span><span>taskflow_</span><span>).</span><span>wait</span><span>();</span>
</span></span><span><span>        <span>return</span> <span>std</span><span>::</span><span>accumulate</span><span>(</span><span>sums_</span><span>.</span><span>begin</span><span>(),</span> <span>sums_</span><span>.</span><span>end</span><span>(),</span> <span>0.0</span><span>,</span>
</span></span><span><span>                               <span>[](</span><span>double</span> <span>acc</span><span>,</span> <span>thread_result_t</span> <span>const</span> <span>&amp;</span><span>x</span><span>)</span> <span>noexcept</span> <span>{</span> <span>return</span> <span>acc</span> <span>+</span> <span>x</span><span>.</span><span>partial_sum</span><span>;</span> <span>});</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span><span>};</span>
</span></span></code></pre></td></tr></tbody></table></div><p>Only the <code>operator()</code> method is timed, leaving the construction costs out of the equation.</p><h2 id="conclusions--observations">Conclusions &amp; Observations</h2><p>Fork Union shows that a lean, 300-line fork-join pool can sit within ~20% of OpenMP, while more functional pools trail by an order of magnitude.
That margin will shift as more workloads, CPUs, and compilers are tested, so treat today’s numbers as directional, not gospel.
There may still be subtle memory-ordering bugs lurking in Fork Union, but the core observations should hold: <strong>dodge mutexes, dynamic queues, likely-pessimistic CAS paths, and false sharing — regardless of language or framework</strong>.</p><p>Rust is still new territory for me.
The biggest surprise is the <a href="https://github.com/rust-lang/rust/issues/32838">missing allocator support in <code>std::collections</code></a> on the stable toolchain.
Nightly’s <code>Vec::try_reserve_in</code> helps, but until stable lands, ergonomic custom allocation remains tricky.
The machinery exists in C++, yet most projects ignore it — so the culture needs to catch up.</p><hr><p>PS: Spot dubious memory-ordering?
<a href="https://github.com/ashvardanian/fork_union/issues">Open an issue</a>.
Want to close the remaining 20% gap?
Happy forking 🤗</p><blockquote><div lang="en" dir="ltr"><p>Fork Union, arguably the most unusual parallel-processing library on GitHub, just crossed its first 100 stars — my 12th project to reach that milestone 🥳</p><p>Repository: <a href="https://t.co/Gyg43GW6d7">https://t.co/Gyg43GW6d7</a></p><p>Unlike typical thread-pools, it avoids not only mutexes but even Compare-and-Swap… <a href="https://t.co/H2H7fgaXl4">pic.twitter.com/H2H7fgaXl4</a></p></div>— Ash Vardanian (@ashvardanian) <a href="https://twitter.com/ashvardanian/status/1964634808635539904?ref_src=twsrc%5Etfw">September 7, 2025</a></blockquote></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Solar panels + cold = A potential problem (152 pts)]]></title>
            <link>https://www.linspyre.com/ecoholics/temps.html</link>
            <guid>45401051</guid>
            <pubDate>Sun, 28 Sep 2025 01:48:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.linspyre.com/ecoholics/temps.html">https://www.linspyre.com/ecoholics/temps.html</a>, See on <a href="https://news.ycombinator.com/item?id=45401051">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
        <figcaption>Photo Credit: Kevin L @ EcoFlow DELTA Facebook Group</figcaption>
    <h2>Do Not Do This...Unless You Want Magic Black Smoke</h2>
    <p>You purchased a new Delta Pro and looked up the
    <a href="https://www.linspyre.com/ecoholics/specs.html" target="_blank">specifications</a> - Delta Pro can take up to 1600w/150v/15a of solar input. Then you buy four EcoFlow 400w Rigid solar panels
    to plug them in series.</p>
    <p>You check the math</p>
    <p>37.1v VoC × 4 panels = 148.4v</p>
    <p>... just under the 150v Delta Pro solar input limit. The new solar array is right at the watt limit, just under the volt limit, and a little under the amp limit.
    You think you are good to go, right?</p><p><span>Wrong!!!</span></p>
    <p>This will likely create black magic smoke from your solar generator on the first cold and sunny day.
    This is because solar panel voltages increase as temperatures drop. We see many complaints from newbee EcoFlow customers
    who are all excited with their new toy, they proceed destroy their MPPT controller, then complain bitterly on social media about
    EcoFlow's customer support and voided warranty policy. Plugging in four 400w solar panels in series is similar to filling your gasoline powered
    car with diesel and wondering why the car manufacturer isn't replacing your new car.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bonding twelve 56K modems together to set dial-up broadband records (110 pts)]]></title>
            <link>https://www.tomshardware.com/networking/enthusiasts-bond-twelve-56k-dial-up-modems-together-to-set-dial-up-broadband-records-a-dozen-screeching-boxes-achieve-record-668-kbps-download-speeds</link>
            <guid>45400828</guid>
            <pubDate>Sun, 28 Sep 2025 00:59:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/networking/enthusiasts-bond-twelve-56k-dial-up-modems-together-to-set-dial-up-broadband-records-a-dozen-screeching-boxes-achieve-record-668-kbps-download-speeds">https://www.tomshardware.com/networking/enthusiasts-bond-twelve-56k-dial-up-modems-together-to-set-dial-up-broadband-records-a-dozen-screeching-boxes-achieve-record-668-kbps-download-speeds</a>, See on <a href="https://news.ycombinator.com/item?id=45400828">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>


<div id="article-body">
<p id="ad8b068f-18a1-4e4c-9734-78dd200b7943">The latest episode published by tech channel <a data-analytics-id="inline-link" href="https://www.youtube.com/watch?v=LZ259Jx8MQY" data-url="https://www.youtube.com/watch?v=LZ259Jx8MQY" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">The Serial Port</a> began with an interesting question: Is it possible to stream YouTube via dial-up internet? As the headline suggests, the answer is a resounding yes, with our intrepid heroes managing to establish a connection offering download speeds of 668.8 kbps. The feat was eventually achieved using an era-appropriate Windows XP PC, a Cisco VoIP unit, a couple of serial port packing PCI cards, and a dozen 56K modems bonded using Multilink PPP (MPPP) technology. This is probably a world record.</p><figure data-bordeaux-image-check="" id="8633c508-a059-4e2a-aa3c-b9a06a82b171"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/tbBDF6FdgrsgxL44yJh4Ge-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/tbBDF6FdgrsgxL44yJh4Ge-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/tbBDF6FdgrsgxL44yJh4Ge-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/tbBDF6FdgrsgxL44yJh4Ge-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/tbBDF6FdgrsgxL44yJh4Ge-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/tbBDF6FdgrsgxL44yJh4Ge-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/tbBDF6FdgrsgxL44yJh4Ge.jpg" alt="Broadband using 56K dial-up modems" srcset="https://cdn.mos.cms.futurecdn.net/tbBDF6FdgrsgxL44yJh4Ge-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/tbBDF6FdgrsgxL44yJh4Ge-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/tbBDF6FdgrsgxL44yJh4Ge-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/tbBDF6FdgrsgxL44yJh4Ge-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/tbBDF6FdgrsgxL44yJh4Ge-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/tbBDF6FdgrsgxL44yJh4Ge-320-80.jpg 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/tbBDF6FdgrsgxL44yJh4Ge.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/tbBDF6FdgrsgxL44yJh4Ge.jpg">
</picture></p></div><figcaption itemprop="caption description"><span>success! </span><span itemprop="copyrightHolder">(Image credit: &nbsp;<a href="https://www.youtube.com/watch?v=LZ259Jx8MQY" data-url="https://www.youtube.com/watch?v=LZ259Jx8MQY" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">The Serial Port</a>)</span></figcaption></figure><h2 id="when-broadband-wasn-t-so-broad-3">When broadband wasn’t so broad</h2><p id="ce08e175-c9a0-4bf7-8793-4bf7cbc2a9fb">The latest regulations from the FCC define <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/broadband-internet-prices-speed-us-comparison" data-before-rewrite-localise="https://www.tomshardware.com/news/broadband-internet-prices-speed-us-comparison">broadband </a>as 100 Mbps or higher, but in 2000, a far slower connection of 200 Kbps or higher was considered adequate to earn the designation. Back then, connectivity was slow, but by the turn of the millennium, websites and communications were simpler and had lower bandwidth. For example, downloading multimedia files like MP3s back then could tie up your phone line for 10 to 20 minutes. Thus, applications like the infamous Napster and emerging streaming video and online multimedia experiences begged for broadband.</p><p id="ce08e175-c9a0-4bf7-8793-4bf7cbc2a9fb-1">Multilink PPP technology was one possible solution to faster internet connectivity before ISDN and ADSL connectivity became widespread. As the name suggests, MPPP tech combines the bandwidth of multiple <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/apples-modems-are-three-years-behind-qualcomm-report" data-before-rewrite-localise="https://www.tomshardware.com/news/apples-modems-are-three-years-behind-qualcomm-report">modems </a>to create a single logical data pipe.</p><p>Commercial solutions like “the Diamond Multimedia Shotgun, a PCI card with two onboard modems that could be bonded together using multilink PPP,” leveraged this tech, point out the YouTubers. However, it didn’t gain traction due to the multiple lines and ISP shenanigans required.</p><p>Now, with an ISP that supports digital modems and the equipment (including a Cisco VoIP gateway) to make it happen, The Serial Port had an opportunity to see how far Multilink PPP can go. Encouragingly, the official MPPP standard doesn’t highlight any practical limits…</p><h2 id="cover-your-ears-preparing-for-screeching-modem-broadband-3">Cover your ears - Preparing for screeching modem broadband</h2><p id="ea6e6f0b-2ed5-436d-8d2b-0a80a6d4164b">Refocusing on the overarching YouTube streaming goal, our intrepid TechTubers calculated how much bandwidth would need to be squeezed out of their bonded modem array to make streaming tolerable. In brief, ~four 56K modems should be sufficient for minimum-quality desktop streaming (240p, ~200 kbps) in 2025.</p><p>The first client PC chosen was a 2001-vintage <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tag/ibm" data-auto-tag-linker="true" data-before-rewrite-localise="https://www.tomshardware.com/tag/ibm">IBM</a> desktop with Windows ME, released just ahead of the widespread availability of broadband. This setup worked with two 56K modems bonded together—a promising start.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-A78rnrL7MDW7YovDDCio4S"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div><p>With the proof that MPPP worked on this contemporaneous PC system, the TechTubers sought to pack more serial ports into the IBM. A card featuring an extra eight serial ports was found. However, driver clashes prevented further scaling…</p><p>Still optimistic about their project, the team moved up to “slightly newer hardware.” Specifically, an IBM Think Center from 2004 was chosen as a compromise, as we did not want to go too modern. <a data-analytics-id="inline-link" href="https://www.tomshardware.com/software/windows/40-years-of-windows-how-windows-xp-changed-everything" data-before-rewrite-localise="https://www.tomshardware.com/software/windows/40-years-of-windows-how-windows-xp-changed-everything">Windows XP</a> was pre-installed on this system, and it was hoped that it could do better with MPPP.</p><p>Using the newer XP PC, two identical serial expansion cards were installed. However, they didn’t work together, as they overlapped COM port addresses in the Device Manager. So the TechTubers switched to a different brand of serial expansion card for the second card. They ended up with 13 ports in total (including the one on the motherboard).</p><figure data-bordeaux-image-check="" id="939a627f-4eed-4420-ad22-7897c56e7624"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/5QyoWmdTNq6ojA22QLyQGe-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/5QyoWmdTNq6ojA22QLyQGe-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/5QyoWmdTNq6ojA22QLyQGe-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/5QyoWmdTNq6ojA22QLyQGe-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/5QyoWmdTNq6ojA22QLyQGe-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/5QyoWmdTNq6ojA22QLyQGe-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/5QyoWmdTNq6ojA22QLyQGe.jpg" alt="Broadband using 56K dial-up modems" srcset="https://cdn.mos.cms.futurecdn.net/5QyoWmdTNq6ojA22QLyQGe-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/5QyoWmdTNq6ojA22QLyQGe-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/5QyoWmdTNq6ojA22QLyQGe-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/5QyoWmdTNq6ojA22QLyQGe-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/5QyoWmdTNq6ojA22QLyQGe-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/5QyoWmdTNq6ojA22QLyQGe-320-80.jpg 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/5QyoWmdTNq6ojA22QLyQGe.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/5QyoWmdTNq6ojA22QLyQGe.jpg">
</picture><a href="https://cdn.mos.cms.futurecdn.net/5QyoWmdTNq6ojA22QLyQGe.jpg" target="_blank" data-url="https://cdn.mos.cms.futurecdn.net/5QyoWmdTNq6ojA22QLyQGe.jpg" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"></a></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: &nbsp;<a href="https://www.youtube.com/watch?v=LZ259Jx8MQY" data-url="https://www.youtube.com/watch?v=LZ259Jx8MQY" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">The Serial Port</a>)</span></figcaption></figure><h2 id="the-dusty-dozen-3">The dusty dozen</h2><p id="9ff16b99-222f-4eb5-af05-efb0eae1cf4c">Unlike with Windows ME, where each modem was dialed in turn, it was observed that XP dialed them all simultaneously! Moreover, the team successfully scaled up from two modems to 12 after several rounds of fiddling with modem DIP switches, phone line connectors, and XP’s serial port controls. As more modems were added to the system, the TechTubers laughed joyously at the sound of multiple modems dialing and negotiating.</p><p>The dozen modems connected to the Windows XP machine achieved a combined connection speed of 668.8 kbps, offering blistering download speeds. Testing confirmed that this system was able to load and stream <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/youtube-responds-to-delayed-loading-in-rival-browser-complaints" data-before-rewrite-localise="https://www.tomshardware.com/news/youtube-responds-to-delayed-loading-in-rival-browser-complaints">YouTube</a> videos, and no buffering was observed after a slight delay (likely due to the old PC's processing power).</p><p>Did The Serial Port achieve a world record? The TechTubers couldn’t find any accounts of people using more than four modems in MPPP at the same time. The video ends with a tease that they haven’t yet found a limit to MPPP…</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank" data-url="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank" data-url="https://google.com/preferences/source?q=" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
</div>



<!-- Drop in a standard article here maybe? -->



<div id="slice-container-authorBio-A78rnrL7MDW7YovDDCio4S"><p>Mark Tyson is a news editor at Tom's Hardware. He enjoys covering the full breadth of PC tech; from business and semiconductor design to products approaching the edge of reason.</p></div>
</section>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Learn to play Go (318 pts)]]></title>
            <link>https://online-go.com/learn-to-play-go</link>
            <guid>45400376</guid>
            <pubDate>Sat, 27 Sep 2025 23:50:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://online-go.com/learn-to-play-go">https://online-go.com/learn-to-play-go</a>, See on <a href="https://news.ycombinator.com/item?id=45400376">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="loading-svg-container" role="status" aria-live="polite"><p><span>Loading...</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The (economic) AI apocalypse is nigh (119 pts)]]></title>
            <link>https://pluralistic.net/2025/09/27/econopocalypse/</link>
            <guid>45399893</guid>
            <pubDate>Sat, 27 Sep 2025 22:30:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pluralistic.net/2025/09/27/econopocalypse/">https://pluralistic.net/2025/09/27/econopocalypse/</a>, See on <a href="https://news.ycombinator.com/item?id=45399893">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-11677">
	<!-- .entry-header -->

	
	
	<div>
		<p><!--
Tags:
ai, business models, economics, bubbles, ai apocalypse, ai econopocalypse, econopocalypse

Summary:
The real (economic) AI apocalypse is nigh; Hey look at this; Upcoming appearances; Recent appearances; Latest books; Upcoming books

URL:
https://pluralistic.net/2025/09/27/econopocalypse/

Title:
Pluralistic: The real (economic) AI apocalypse is nigh (27 Sep 2025) econopocalypse

Bullet:
🔋

Separator:
⠂⠄⠄⠂⠁⠁⠂⠄⠄⠂⠁⠁⠂⠄⠄⠂ ⠂⠄⠄⠂⠂⠄⠄⠂⠁⠁⠂⠄⠄⠂⠁⠁⠂⠄⠄⠂ ⠂⠄⠄⠂⠂⠄⠄⠂⠁⠁⠂⠄

Top Sources:
Today's top sources: James Boyle (https://www.thepublicdomain.org/).

--><br>
<a href="https://pluralistic.net/2025/09/27/econopocalypse/"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/craphound.com/images/27Sep2025.jpg?w=840&amp;ssl=1"></a></p>
<h2>Today's links</h2>
<ul>
<li><a href="https://pluralistic.net/2025/09/27/econopocalypse/#subprime-intelligence">The real (economic) AI apocalypse is nigh</a>: Sweating (the assets) to the oldies.
</li>
<li><a href="https://pluralistic.net/2025/09/27/econopocalypse/#linkdump">Hey look at this</a>: Delights to delectate.
</li>
<li><a href="https://pluralistic.net/2025/09/27/econopocalypse/#retro">Object permanence</a>: Dying on the job; Google audiocomplete blacklist; Lockheed Martin v. 'gathering information.'
</li>
<li><a href="https://pluralistic.net/2025/09/27/econopocalypse/#upcoming">Upcoming appearances</a>: Where to find me.
</li>
<li><a href="https://pluralistic.net/2025/09/27/econopocalypse/#recent">Recent appearances</a>: Where I've been.
</li>
<li><a href="https://pluralistic.net/2025/09/27/econopocalypse/#latest">Latest books</a>: You keep readin' em, I'll keep writin' 'em.
</li>
<li><a href="https://pluralistic.net/2025/09/27/econopocalypse/#upcoming-books">Upcoming books</a>: Like I said, I'll keep writin' 'em.
</li>
<li><a href="https://pluralistic.net/2025/09/27/econopocalypse/#bragsheet">Colophon</a>: All the rest.
</li>
</ul>

<hr>
<p><a name="subprime-intelligence"></a><br>
<img data-recalc-dims="1" decoding="async" alt="A Zimbabwean one hundred trillion dollar bill; the bill's iconography have been replaced with the glaring red eye of HAL 9000 from Stanley Kubrick's '2001: A Space Odyssey' and a stylized, engraving-style portrait of Sam Altman." src="https://i0.wp.com/craphound.com/images/ai-costs.jpg?w=840&amp;ssl=1"></p>
<h2>The real (economic) AI apocalypse is nigh (<a href="https://pluralistic.net/2025/09/27/econopocalypse/#subprime-intelligence">permalink</a>)</h2>
<p>Like you, I'm sick to the back teeth of talking about AI. Like you, I keep getting dragged into discussions of AI. Unlike you‡, I spent the summer writing a book about why I'm sick of writing about AI⹋, which Farrar, Straus and Giroux will publish in 2026.</p>
<p>‡probably</p>
<p>⹋"The Reverse Centaur's Guide to AI"</p>
<p>A week ago, I turned that book into a speech, which I delivered as the annual Nordlander Memorial Lecture at Cornell, where I'm an AD White Professor-at-Large. This was my first-ever speech about AI and I wasn't sure how it would go over, but thankfully, it went great and sparked a lively Q&amp;A. One of those questions came from a young man who said something like "So, you're saying a third of the stock market is tied up in seven AI companies that have no way to become profitable and that this is a bubble that's going to burst and take the whole economy with it?"</p>
<p>I said, "Yes, that's right."</p>
<p>He said, "OK, but what can we do about that?"</p>
<p>So I re-iterated the book's thesis: that the AI bubble is driven by monopolists who've conquered their markets and have no more growth potential, who are desperate to convince investors that they can continue to grow by moving into some other sector, e.g. "pivot to video," crypto, blockchain, NFTs, AI, and now "super-intelligence." Further: the topline growth that AI companies are selling comes from replacing most workers with AI, and re-tasking the surviving workers as AI babysitters ("humans in the loop"), which won't work. Finally: AI <em>cannot</em> do your job, but an AI salesman can <em>100%</em> convince your boss to fire you and replace you with an AI that <em>can't</em> do your job, and when the bubble bursts, the money-hemorrhaging "foundation models" will be shut off and we'll lose the AI that can't do your job, <em>and</em> you will be long gone, retrained or retired or "discouraged" and out of the labor market, and <em>no one</em> will do your job. AI is the asbestos we are shoveling into the walls of our society and our descendants will be digging it out for generations:</p>
<p><a href="https://pluralistic.net/2025/05/27/rancid-vibe-coding/#class-war">https://pluralistic.net/2025/05/27/rancid-vibe-coding/#class-war</a></p>
<p>The only thing (I said) that we can do about this is to puncture the AI bubble <em>as soon as possible</em>, to halt this before it progresses any further and to head off the accumulation of social and economic debt. To do that, we have to take aim at the <em>material basis</em> for the AI bubble (creating a growth story by claiming that defective AI can do your job).</p>
<p>"OK," the young man said, "but what can we <em>do</em> about the crash?" He was clearly very worried.</p>
<p>"I don't think there's anything we can do about that. I think it's already locked in. I mean, maybe if we had a different government, they'd fund a jobs guarantee to pull us out of it, but I don't think Trump'll do that, so –"</p>
<p>"But what can we <em>do?</em>"</p>
<p>We went through a few rounds of this, with this poor kid just repeating the same question in different tones of voice, like an acting coach demonstrating the five stages of grieving using nothing but inflection. It was an uncomfortable moment, and there was some decidedly nervous chuckling around the room as we pondered the coming AI (economic) apocalypse, and the fate of this kid graduating with mid-six-figure debts into an economy of ashes and rubble.</p>
<p>I firmly believe the (economic) AI apocalypse is coming. These companies are not profitable. They can't be profitable. They keep the lights on by soaking up hundreds of billions of dollars in other people's money and then lighting it on fire. Eventually those other people are going to want to see a return on their investment, and when they don't get it, they will halt the flow of billions of dollars. Anything that can't go on forever eventually stops.</p>
<p>This isn't like the early days of the web, or Amazon, or any of those other big winners that lost money before becoming profitable. Those were all propositions with excellent "unit economics" – they got cheaper with every successive technological generation, and the more customers they added, the more profitable they became. AI companies have – in the memorable phraseology of Ed Zitron – "dogshit unit-economics." Each generation of AI has been vastly more expensive than the previous one, and each new AI customer makes the AI companies lose <em>more</em> money:</p>
<p><a href="https://pluralistic.net/2025/06/30/accounting-gaffs/#artificial-income">https://pluralistic.net/2025/06/30/accounting-gaffs/#artificial-income</a></p>
<p>This week, no less than the <em>Wall Street Journal</em> published a lengthy, well-reported story (by Eliot Brown and Robbie Whelan) on the catastrophic finances of AI companies:</p>
<p><a href="https://www.wsj.com/tech/ai/ai-bubble-building-spree-55ee6128?st=efV1EF&amp;amp;reflink=article_email_share">https://www.wsj.com/tech/ai/ai-bubble-building-spree-55ee6128?st=efV1EF&amp;amp;reflink=article_email_share</a></p>
<p>The <em>WSJ</em> writers compare the AI bubble to other bubbles, like Worldcom's fraud-soaked fiber optic bonanza (which saw the company's CEO sent to prison, where he eventually died), and conclude that the AI bubble is <em>vastly</em> larger than any other bubble in recent history.</p>
<p>The data-center buildout has genuinely absurd finances – there are data-center companies that are collateralizing their loans by staking their giant Nvidia GPUs as collateral. This is wild: there's pretty much nothing (apart from fresh-caught fish) that loses its value faster than silicon chips. That goes <em>triple</em> for GPUs used in AI data-centers, where it's normal for tens of thousands of chips to burn out over a single, 54-day training run:</p>
<p><a href="https://techblog.comsoc.org/2024/11/25/superclusters-of-nvidia-gpu-ai-chips-combined-with-end-to-end-network-platforms-to-create-next-generation-data-centers/">https://techblog.comsoc.org/2024/11/25/superclusters-of-nvidia-gpu-ai-chips-combined-with-end-to-end-network-platforms-to-create-next-generation-data-centers/</a></p>
<p>Talk about sweating your assets!</p>
<p>That barely scratches the surface of the funny accounting in the AI bubble. Microsoft "invests" in Openai by giving the company free access to its servers. Openai reports this as a ten billion dollar investment, then redeems these "tokens" at Microsoft's data-centers. Microsoft then books this as ten billion in revenue.</p>
<p>That's par for the course in AI, where it's normal for Nvidia to "invest" tens of billions in a data-center company, which then spends that investment buying Nvidia chips. The the same chunk of money being energetically passed back and forth between these closely related companies, all of which claim it as investment, as an asset, or as revenue (or all three).</p>
<p>The <em>Journal</em> quotes David Cahn, a VC from Sequoia, who says that for AI companies to become profitable, they would have to sell us <em>$800 billion</em> worth of services <em>over the life of today's data centers and GPUs</em>. Not only is that a very large number – it's also a very short time. AI bosses themselves will tell you that these data centers and GPUs will be obsolete practically from the moment they start operating. Mark Zuckerberg says he's prepared to waste "a couple hundred billion dollars" on misspent AI investments:</p>
<p><a href="https://www.businessinsider.com/mark-zuckerberg-meta-risk-billions-miss-superintelligence-ai-bubble-2025-9">https://www.businessinsider.com/mark-zuckerberg-meta-risk-billions-miss-superintelligence-ai-bubble-2025-9</a></p>
<p>Bain &amp; Co says that the only way to make today's AI investments profitable is for the sector to bring in <em>$2 trillion</em> by 2030 (the <em>Journal</em> notes that this is more than the combined revenue of Amazon, Google, Microsoft, Apple Nvidia and Meta):</p>
<p><a href="https://www.bain.com/about/media-center/press-releases/20252/$2-trillion-in-new-revenue-needed-to-fund-ais-scaling-trend---bain--companys-6th-annual-global-technology-report/">https://www.bain.com/about/media-center/press-releases/20252/$2-trillion-in-new-revenue-needed-to-fund-ais-scaling-trend—bain–companys-6th-annual-global-technology-report/</a></p>
<p>How much money is the AI industry making? Morgan Stanley says it's $45b/year. But that $45b is based on the AI industry's own exceedingly cooked books, where <em>annual</em> revenue is actually <em>annualized</em> revenue, an accounting scam whereby a company chooses its best single revenue month and multiplies it by 12, even if that month is a wild outlier:</p>
<p><a href="https://www.wheresyoured.at/the-haters-gui/">https://www.wheresyoured.at/the-haters-gui/</a></p>
<p>Industry darlings like Coreweave (a middleman that rents out data-centers) are sitting on <em>massive</em> piles of debt, secured by short-term deals with tech companies that run out <em>long</em> before the debts can be repaid. If they can't find a bunch of new clients in a couple short years, they will default and collapse.</p>
<p>Today's AI bubble has absorbed more of the country's wealth and represents more of its economic activity than historic nation-shattering bubbles, like the 19th century UK rail bubble. A much-discussed MIT paper found that 95% of companies that had tried AI had either nothing to show for it, or experienced a loss:</p>
<p><a href="https://www.technologyreview.com/2019/01/25/1436/we-analyzed-16625-papers-to-figure-out-where-ai-is-headed-next/">https://www.technologyreview.com/2019/01/25/1436/we-analyzed-16625-papers-to-figure-out-where-ai-is-headed-next/</a></p>
<p>A less well-known U Chicago paper finds that AI has "no significant impact on workers’ earnings, recorded hours, or wages":</p>
<p><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5219933">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5219933</a></p>
<p>Anything that can't go on forever eventually stops. Trump might bail out the AI companies, but for how long? They are incinerating money faster than practically any other human endeavor in history, with precious little to show for it.</p>
<p>During my stay at Cornell, one of the people responsible for the university's AI strategy asked me what I thought the university should be doing about AI. I told them that they should be planning to absorb the productive residue that will be left behind after the bubble bursts:</p>
<p><a href="https://locusmag.com/feature/commentary-cory-doctorow-what-kind-of-bubble-is-ai/">https://locusmag.com/feature/commentary-cory-doctorow-what-kind-of-bubble-is-ai/</a></p>
<p>Plan for a future where you can buy GPUs for ten cents on the dollar, where there's a buyer's market for hiring skilled applied statisticians, and where there's a ton of extremely promising open source models that have barely been optimized and have <em>vast</em> potential for improvement.</p>
<p>There's plenty of useful things you can do with AI. But AI is (as Princeton's Arvind Narayanan and Sayash Kapoor, authors of <em>AI Snake Oil</em> put it), a <em>normal</em> technology:</p>
<p><a href="https://knightcolumbia.org/content/ai-as-normal-technology">https://knightcolumbia.org/content/ai-as-normal-technology</a></p>
<p>That doesn't mean "nothing to see here, move on." It means that AI isn't the bow-wave of "impending superintelligence." Nor is it going to deliver "humanlike intelligence."</p>
<p>It's a grab-bag of useful (sometimes very useful) tools that can sometimes make workers' lives better, when workers get to decide how and when they're used.</p>
<p>The most important thing about AI isn't its technical capabilities or limitations. The most important thing is the investor story and the ensuing mania that has teed up an economical catastrophe that will harm hundreds of millions or even billions of people. AI isn't going to wake up, become superintelligent and turn you into paperclips – but rich people with AI investor psychosis are almost certainly going to make you much, much poorer.</p>
<p>(<i>Image: <a href="https://commons.wikimedia.org/wiki/File:Sam_Altman_-_TechCrunch_Disrupt_SF_2017_(36522988343).jpg">TechCrunch</a>, <a href="https://creativecommons.org/licenses/by/2.0/deed.en">CC BY 2.0</a>; <a href="https://commons.wikimedia.org/wiki/File:HAL9000.svg">Cryteria</a>, <a href="https://creativecommons.org/licenses/by/3.0/deed.en">CC BY 3.0</a>; modified</i>)</p>
<hr>

<h2 heds="0">Hey look at this (<a href="https://pluralistic.net/2025/09/27/econopocalypse/#linkdump">permalink</a>)</h2>
<p><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/craphound.com/images/heylookatthis2.jpg?w=840&amp;ssl=1"></p>
<ul>
<li>EU ministers reach 'compromise' on digital euro roadmap <a href="https://www.reuters.com/business/finance/eu-ministers-seek-agreement-digital-euro-be-independent-visa-mastercard-2025-09-19/">https://www.reuters.com/business/finance/eu-ministers-seek-agreement-digital-euro-be-independent-visa-mastercard-2025-09-19/</a>
</li>
<li>
<p>Amazon will pay $2.5 billion to settle the FTC’s Prime lawsuit <a href="https://www.theverge.com/news/785744/amazon-ftc-prime-subscription-settlment">https://www.theverge.com/news/785744/amazon-ftc-prime-subscription-settlment</a></p>
</li>
<li>
<p>Conservative Dem Compares Ad About Her Corporate Donations to ‘Political Violence’ <a href="https://prospect.org/politics/2025-09-25-conservative-dem-ad-corporate-donations-violence-bains-california/">https://prospect.org/politics/2025-09-25-conservative-dem-ad-corporate-donations-violence-bains-california/</a></p>
</li>
<li>
<p>Monopoly Utilities Ousted America's Best Regulator <a href="https://economicpopulist.substack.com/p/monopoly-utilities-ousted-americas">https://economicpopulist.substack.com/p/monopoly-utilities-ousted-americas</a></p>
</li>
<li>
<p>WNYC offers free programs to stations affected by funding cuts <a href="https://current.org/2025/09/wnyc-offers-free-programs-to-stations-affected-by-funding-cuts/">https://current.org/2025/09/wnyc-offers-free-programs-to-stations-affected-by-funding-cuts/</a></p>
</li>
</ul>
<hr>
<p><a name="retro"></a><br>
<img data-recalc-dims="1" decoding="async" alt="A shelf of leatherbound history books with a gilt-stamped series title, 'The World's Famous Events.'" src="https://i0.wp.com/craphound.com/images/worlds-famous-events.png?w=840&amp;ssl=1"></p>
<h2 heds="0">Object permanence (<a href="https://pluralistic.net/2025/09/27/econopocalypse/#retro">permalink</a>)</h2>
<p>#20yrsago Financial Times: WIPO’s webcaster treaty is a disaster <a href="https://www.ft.com/content/441306be-2eb6-11da-9aed-00000e2511c8">https://www.ft.com/content/441306be-2eb6-11da-9aed-00000e2511c8</a></p>
<p>#15yrsago Google’s autocomplete blacklist <a href="https://www.2600.com/googleblacklist/">https://www.2600.com/googleblacklist/</a></p>
<p>#15yrsago FBI ignores DoJ report, raids activists, arrests Time Person of the Year <a href="https://www.democracynow.org/2010/9/27/fbi_raids_homes_of_anti_war">https://www.democracynow.org/2010/9/27/fbi_raids_homes_of_anti_war</a></p>
<p>#15yrsago Meta-textual analysis of mainstream science reporting <a href="https://www.theguardian.com/science/the-lay-scientist/2010/sep/24/1">https://www.theguardian.com/science/the-lay-scientist/2010/sep/24/1</a></p>
<p>#15yrsago Lockheed Martin sign prohibits sketching and “gathering information” <a href="https://www.flickr.com/photos/jef/5028187145/">https://www.flickr.com/photos/jef/5028187145/</a></p>
<p>#5yrsago Ransomware for coffee makers <a href="https://pluralistic.net/2020/09/27/junky-styling/#java-script">https://pluralistic.net/2020/09/27/junky-styling/#java-script</a></p>
<p>#5yrsago The joys of tailoring <a href="https://pluralistic.net/2020/09/27/junky-styling/#inseams">https://pluralistic.net/2020/09/27/junky-styling/#inseams</a></p>
<p>#1yrago Return to office and dying on the job <a href="https://pluralistic.net/2024/09/27/sharpen-your-blades-boys/#disciplinary-technology">https://pluralistic.net/2024/09/27/sharpen-your-blades-boys/#disciplinary-technology</a></p>
<hr>

<h2 heds="0">Upcoming appearances (<a href="https://pluralistic.net/2025/09/27/econopocalypse/#upcoming">permalink</a>)</h2>
<p><img data-recalc-dims="1" decoding="async" alt="A photo of me onstage, giving a speech, pounding the podium." src="https://i0.wp.com/craphound.com/images/appearances2.jpg?w=840&amp;ssl=1"></p>
<ul>
<li>Boston: Enshittification with Randall Munroe (Brattle Theater), Oct 7<br>
<a href="https://www.eventbrite.com/e/cory-doctorow-at-the-brattle-theatre-tickets-1591235180259?aff=oddtdtcreator">https://www.eventbrite.com/e/cory-doctorow-at-the-brattle-theatre-tickets-1591235180259?aff=oddtdtcreator</a>
</li>
<li>
<p>DC: Enshittification with Rohit Chopra (Politics and Prose), Oct 8<br>
<a href="https://politics-prose.com/cory-doctorow-10825">https://politics-prose.com/cory-doctorow-10825</a></p>
</li>
<li>
<p>NYC: Enshittification with Lina Khan (Brooklyn Public Library), Oct 9<br>
<a href="https://www.bklynlibrary.org/calendar/cory-doctorow-discusses-central-library-dweck-20251009-0700pm">https://www.bklynlibrary.org/calendar/cory-doctorow-discusses-central-library-dweck-20251009-0700pm</a></p>
</li>
<li>
<p>New Orleans: DeepSouthCon63, Oct 10-12<br>
<a href="http://www.contraflowscifi.org/">http://www.contraflowscifi.org/</a></p>
</li>
<li>
<p>New Orleans: Enshittification at Octavia Books, Oct 12<br>
<a href="https://www.octaviabooks.com/event/enshittification-cory-doctorow">https://www.octaviabooks.com/event/enshittification-cory-doctorow</a></p>
</li>
<li>
<p>Chicago: Enshittification with Anand Giridharadas (Chicago Humanities), Oct 15<br>
<a href="https://www.oldtownschool.org/concerts/2025/10-15-2025-kara-swisher-and-cory-doctorow-on-enshittification/">https://www.oldtownschool.org/concerts/2025/10-15-2025-kara-swisher-and-cory-doctorow-on-enshittification/</a></p>
</li>
<li>
<p>Los Angeles: Enshittification with David Dayen (Diesel), Oct 16<br>
<a href="https://dieselbookstore.com/event/2025-10-16/cory-doctorow-enshittification">https://dieselbookstore.com/event/2025-10-16/cory-doctorow-enshittification</a></p>
</li>
<li>
<p>San Francisco: Enshittification at Public Works with Jenny Odell (The Booksmith), Oct 20<br>
<a href="https://app.gopassage.com/events/doctorow25">https://app.gopassage.com/events/doctorow25</a></p>
</li>
<li>
<p>PDX: Enshittification at Powell's, Oct 21<br>
<a href="https://www.powells.com/events/cory-doctorow-10-21-25">https://www.powells.com/events/cory-doctorow-10-21-25</a></p>
</li>
<li>
<p>Seattle: Enshittification and the Rot Economy, with Ed Zitron (Clarion West), Oct 22<br>
<a href="https://www.clarionwest.org/event/2025-deep-dives-cory-doctorow/">https://www.clarionwest.org/event/2025-deep-dives-cory-doctorow/</a></p>
</li>
<li>
<p>Madrid: Conferencia EUROPEA 4D (Virtual), Oct 28<br>
<a href="https://4d.cat/es/conferencia/">https://4d.cat/es/conferencia/</a></p>
</li>
<li>
<p>Miami: Enshittification at Books &amp; Books, Nov 5<br>
<a href="https://www.eventbrite.com/e/an-evening-with-cory-doctorow-tickets-1504647263469">https://www.eventbrite.com/e/an-evening-with-cory-doctorow-tickets-1504647263469</a></p>
</li>
<li>
<p>Miami: Cloudfest, Nov 6<br>
<a href="https://www.cloudfest.com/usa/">https://www.cloudfest.com/usa/</a></p>
</li>
<li>
<p>Burbank: Burbank Book Festival, Nov 8<br>
<a href="https://www.burbankbookfestival.com/">https://www.burbankbookfestival.com/</a></p>
</li>
</ul>
<hr>
<p><a name="recent"></a><br>
<img data-recalc-dims="1" decoding="async" alt="A screenshot of me at my desk, doing a livecast." src="https://i0.wp.com/craphound.com/images/recentappearances2.jpg?w=840&amp;ssl=1"></p>
<h2 heds="0">Recent appearances (<a href="https://pluralistic.net/2025/09/27/econopocalypse/#recent">permalink</a>)</h2>
<ul>
<li>Enshittification (Cornell)<br>
<a href="https://ecornell.cornell.edu/keynotes/view/K091225/">https://ecornell.cornell.edu/keynotes/view/K091225/</a>
</li>
<li>
<p>Escaping Big Tech, Privacy Battles &amp; “Enshittification” (Revolution.social)<br>
<a href="https://www.youtube.com/watch?v=exvpetQRSVo">https://www.youtube.com/watch?v=exvpetQRSVo</a></p>
</li>
<li>
<p>Nerd Harder! (This Week in Tech)<br>
<a href="https://twit.tv/shows/this-week-in-tech/episodes/1047">https://twit.tv/shows/this-week-in-tech/episodes/1047</a></p>
</li>
</ul>
<hr>
<p><a name="latest"></a><br>
<img data-recalc-dims="1" decoding="async" alt="A grid of my books with Will Stahle covers.." src="https://i0.wp.com/craphound.com/images/recent.jpg?w=840&amp;ssl=1"></p>
<h2 heds="0">Latest books (<a href="https://pluralistic.net/2025/09/27/econopocalypse/#latest">permalink</a>)</h2>
<ul>
<li>"Picks and Shovels": a sequel to "Red Team Blues," about the heroic era of the PC, Tor Books (US), Head of Zeus (UK), February 2025 (<a href="https://us.macmillan.com/books/9781250865908/picksandshovels">https://us.macmillan.com/books/9781250865908/picksandshovels</a>).
</li>
<li>
<p>"The Bezzle": a sequel to "Red Team Blues," about prison-tech and other grifts, Tor Books (US), Head of Zeus (UK), February 2024 (<a href="http://the-bezzle.org/">the-bezzle.org</a>).</p>
</li>
<li>
<p>"The Lost Cause:" a solarpunk novel of hope in the climate emergency, Tor Books (US), Head of Zeus (UK), November 2023 (<a href="http://lost-cause.org/">http://lost-cause.org</a>).</p>
</li>
<li>
<p>"The Internet Con": A nonfiction book about interoperability and Big Tech (Verso) September 2023 (<a href="http://seizethemeansofcomputation.org/">http://seizethemeansofcomputation.org</a>). Signed copies at Book Soup (<a href="https://www.booksoup.com/book/9781804291245">https://www.booksoup.com/book/9781804291245</a>).</p>
</li>
<li>
<p>"Red Team Blues": "A grabby, compulsive thriller that will leave you knowing more about how the world works than you did before." Tor Books <a href="http://redteamblues.com/">http://redteamblues.com</a>.</p>
</li>
<li>
<p>"Chokepoint Capitalism: How to Beat Big Tech, Tame Big Content, and Get Artists Paid, with Rebecca Giblin", on how to unrig the markets for creative labor, Beacon Press/Scribe 2022 <a href="https://chokepointcapitalism.com/">https://chokepointcapitalism.com</a></p>
</li>
</ul>
<hr>
<p><a name="upcoming-books"></a><br>
<img data-recalc-dims="1" decoding="async" alt="A cardboard book box with the Macmillan logo." src="https://i0.wp.com/craphound.com/images/upcoming-books.jpg?w=840&amp;ssl=1"></p>
<h2 heds="0">Upcoming books (<a href="https://pluralistic.net/2025/09/27/econopocalypse/#upcoming-books">permalink</a>)</h2>
<ul>
<li>"Canny Valley": A limited edition collection of the collages I create for Pluralistic, self-published, September 2025
</li>
<li>
<p>"Enshittification: Why Everything Suddenly Got Worse and What to Do About It," Farrar, Straus, Giroux, October 7 2025<br>
<a href="https://us.macmillan.com/books/9780374619329/enshittification/">https://us.macmillan.com/books/9780374619329/enshittification/</a></p>
</li>
<li>
<p>"Unauthorized Bread": a middle-grades graphic novel adapted from my novella about refugees, toasters and DRM, FirstSecond, 2026</p>
</li>
<li>
<p>"Enshittification, Why Everything Suddenly Got Worse and What to Do About It" (the graphic novel), Firstsecond, 2026</p>
</li>
<li>
<p>"The Memex Method," Farrar, Straus, Giroux, 2026</p>
</li>
<li>
<p>"The Reverse-Centaur's Guide to AI," a short book about being a better AI critic, Farrar, Straus and Giroux, 2026</p>
</li>
</ul>
<hr>
<p><a name="bragsheet"></a><br>
<img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/craphound.com/images/colophon2.jpg?w=840&amp;ssl=1"></p>
<h2 heds="0">Colophon (<a href="https://pluralistic.net/2025/09/27/econopocalypse/#bragsheet">permalink</a>)</h2>
<p>Today's top sources: James Boyle (<a href="https://www.thepublicdomain.org/">https://www.thepublicdomain.org/</a>).</p>
<p><b>Currently writing: </b></p>
<ul>
<li>"The Reverse Centaur's Guide to AI," a short book for Farrar, Straus and Giroux about being an effective AI critic. FIRST DRAFT COMPLETE AND SUBMITTED.
</li>
<li>
<p>A Little Brother short story about DIY insulin PLANNING</p>
</li>
</ul>
<hr>
<p><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/craphound.com/images/by.svg.png?w=840&amp;ssl=1"></p>
<p>This work – excluding any serialized fiction – is licensed under a Creative Commons Attribution 4.0 license. That means you can use it any way you like, including commercially, provided that you attribute it to me, Cory Doctorow, and include a link to pluralistic.net.</p>
<p><a href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></p>
<p>Quotations and images are not included in this license; they are included either under a limitation or exception to copyright, or on the basis of a separate license. Please exercise caution.</p>
<hr>
<h2>How to get Pluralistic:</h2>
<p>Blog (no ads, tracking, or data-collection):</p>
<p><a href="http://pluralistic.net/">Pluralistic.net</a></p>
<p>Newsletter (no ads, tracking, or data-collection):</p>
<p><a href="https://pluralistic.net/plura-list">https://pluralistic.net/plura-list</a></p>
<p>Mastodon (no ads, tracking, or data-collection):</p>
<p><a href="https://mamot.fr/@pluralistic">https://mamot.fr/@pluralistic</a></p>
<p>Medium (no ads, paywalled):</p>
<p><a href="https://doctorow.medium.com/">https://doctorow.medium.com/</a></p>
<p>Twitter (mass-scale, unrestricted, third-party surveillance and advertising):</p>
<p><a href="https://twitter.com/doctorow">https://twitter.com/doctorow</a></p>
<p>Tumblr (mass-scale, unrestricted, third-party surveillance and advertising):</p>
<p><a href="https://mostlysignssomeportents.tumblr.com/tagged/pluralistic">https://mostlysignssomeportents.tumblr.com/tagged/pluralistic</a></p>
<p>"<em>When life gives you SARS, you make sarsaparilla</em>" -Joey "Accordion Guy" DeVilla</p>
<p>READ CAREFULLY: By reading this, you agree, on behalf of your employer, to release me from all obligations and waivers arising from any and all NON-NEGOTIATED agreements, licenses, terms-of-service, shrinkwrap, clickwrap, browsewrap, confidentiality, non-disclosure, non-compete and acceptable use policies ("BOGUS AGREEMENTS") that I have entered into with your employer, its partners, licensors, agents and assigns, in perpetuity, without prejudice to my ongoing rights and privileges. You further represent that you have the authority to release me from any BOGUS AGREEMENTS on behalf of your employer.</p>
<p>ISSN: 3066-764X</p>

	</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microwave weapon downs 49 drones with a single blast (194 pts)]]></title>
            <link>https://newatlas.com/military/microwave-beam-anti-drone-weapon/</link>
            <guid>45399863</guid>
            <pubDate>Sat, 27 Sep 2025 22:25:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newatlas.com/military/microwave-beam-anti-drone-weapon/">https://newatlas.com/military/microwave-beam-anti-drone-weapon/</a>, See on <a href="https://news.ycombinator.com/item?id=45399863">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In a demonstration not so much of marksmanship but more of the advantages of microwaves, an Epirus Leonidas directed energy, high-power microwave (HPM) anti-drone weapon has knocked 49 Uncrewed Aerial Vehicles (UAV) out of the air with one shot.</p><p>Two things that make drones particularly concerning is that they're small enough to appear from unexpected corners of the sky and they're cheap enough that they can be <a href="https://newatlas.com/ufo-drones-graffiti-painting/60423/" data-cms-ai="0">deployed in huge numbers</a>. In fact, they are so cheap that they pose not only a military threat, but a serious hazard to civilian aviation from individuals who are irresponsible, mischievous, or just oblivious.</p><p>This is the reason there are so many different types of <a href="https://newatlas.com/military/thor-microwave-weapon-drone-swarms/" data-cms-ai="0">anti-drone weapons</a>. Each has their advantages and disadvantages, with none providing a one-size-fits-all panacea. Instead, each needs to be fitted to a particular scenario or deployed as part of a layered defense strategy.</p><p>One countermeasure is the use of microwave weapons like Leonidas. Named after the Spartan king who held off a Persian invasion with a vastly inferior force at the Battle of Thermopylae, Leonidas is one of a family of weapons based on using long-pulse microwave beams to burn out the electronics of small drones.</p><p>The idea isn't new, but Epirus has improved on previous iterations by using Gallium Nitride (GaN) semiconductors to generate microwaves instead of fragile, power-hungry magnetron vacuum tubes. This allows for smaller, more durable, and more mobile systems that use less power. In addition, Leonidas is software driven and can tailor its waveform for optimum effect, it is safe to use around humans who may be in the field of fire, and the present system has twice the range of the 2022 version.</p><p>But the core feature is its "one-to-many" capability that gives it operational flexibility to handle a variety of scenarios. For example, it can strike against targets with precision to take out <a href="https://newatlas.com/military/valkyrie-combat-drone-launch-smaller-drone-test-flight/" data-cms-ai="0">hostile drones</a> while avoiding collateral damage, be programmed to set up no-fly zones with safety corridors to take out hostiles while allowing friendlies to pass, sustain continuous fire without overheating, and take down swarms in one go.</p><div data-align-center="">
                
                    <figure>
    
    
    
    


<p><img alt="The Leonidas microwave weapon on an armored vehicle" width="754" height="462" data-image-size="articleImage" loading="lazy" srcset="https://assets.newatlas.com/dims4/default/d6c72a3/2147483647/strip/true/crop/754x462+0+0/resize/440x270!/format/webp/quality/90/?url=https%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F0d%2F38%2F86e911324e6883ff677ccf3ab8ce%2Fscreenshot-2025-09-21-150303.jpg 440w,https://assets.newatlas.com/dims4/default/ccb09f5/2147483647/strip/true/crop/754x462+0+0/resize/725x444!/format/webp/quality/90/?url=https%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F0d%2F38%2F86e911324e6883ff677ccf3ab8ce%2Fscreenshot-2025-09-21-150303.jpg 725w,https://assets.newatlas.com/dims4/default/cb4580d/2147483647/strip/true/crop/754x462+0+0/resize/800x490!/format/webp/quality/90/?url=https%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F0d%2F38%2F86e911324e6883ff677ccf3ab8ce%2Fscreenshot-2025-09-21-150303.jpg 800w,https://assets.newatlas.com/dims4/default/0392e8b/2147483647/strip/true/crop/754x462+0+0/resize/1200x735!/format/webp/quality/90/?url=https%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F0d%2F38%2F86e911324e6883ff677ccf3ab8ce%2Fscreenshot-2025-09-21-150303.jpg 1200w,https://assets.newatlas.com/dims4/default/c31a3b3/2147483647/strip/true/crop/754x462+0+0/resize/1920x1176!/format/webp/quality/90/?url=https%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F0d%2F38%2F86e911324e6883ff677ccf3ab8ce%2Fscreenshot-2025-09-21-150303.jpg 1920w" src="https://assets.newatlas.com/dims4/default/d09a9cf/2147483647/strip/true/crop/754x462+0+0/resize/754x462!/format/webp/quality/90/?url=https%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F0d%2F38%2F86e911324e6883ff677ccf3ab8ce%2Fscreenshot-2025-09-21-150303.jpg" sizes="(min-width: 768px) 800px, 100vw">
</p>



    
    

    
        <div><figcaption itemprop="caption">The Leonidas microwave weapon on an armored vehicle</figcaption><p>Epirus</p></div>
    
</figure>

                
            </div><p>On August 26, 2025, in front of an invitation-only audience at Camp Atterbury, Indiana, Leonidas took part in a live fire exercise in which it disabled 61 drones with 100% success. This included knocking out two groups of three drones approaching <a href="https://newatlas.com/military/roadrunner-m-anti-aircraft-drone-reusable/" data-cms-ai="0">without warning</a> from opposite directions, targeting one of two drones selected by an audience member before disabling the second one, and intercepting and dropping a single drone into a predetermined safe zone.</p><p>Then came the party piece, it took on over four dozen drones at once, dropping them out of the sky simultaneously with a single pulse. That may not seem like much in words, but a video provided by the company had the lot suddenly crashing like someone had cut their strings.</p><div data-video-disable-history="" data-align-center="">
    
        <p><ps-youtubeplayer data-video-player="" data-player-id="f66f7afb1ce0d4212adc356019d091a96" data-video-id="ZrkopSw5uas" data-video-title="Epirus’ Leonidas High-Power Microwave Defeats 49-Drone Swarm">

    <iframe id="YouTubeVideoPlayer-f66f7afb1ce0d4212adc356019d091a96" role="application" title="YouTube embedded video player" allowfullscreen="" loading="lazy" src="https://www.youtube.com/embed/ZrkopSw5uas?enablejsapi=1"></iframe>
</ps-youtubeplayer>
</p>
    
    
        <p>Epirus’ Leonidas High-Power Microwave Defeats 49-Drone Swarm</p>
    
</div><p>"This is a watershed moment for Epirus," said Andy Lowery, Epirus CEO. "We believe showcasing our weaponized electromagnetic interference is the most effective way to communicate that Leonidas is the only mission-capable, counter-swarm solution for the one-to-many fight.Those who joined us witnessed this first-hand as 61 drones went up – and 61 went down."</p><p>Source: <a href="https://www.epirusinc.com/press-releases/epirus-leonidas-high-power-microwave-defeats-49-drone-swarm-100-of-drones-flown-at-live-fire-demonstration" target="_blank" data-cms-ai="0">Epirus</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We reverse-engineered Flash Attention 4 (121 pts)]]></title>
            <link>https://modal.com/blog/reverse-engineer-flash-attention-4</link>
            <guid>45399637</guid>
            <pubDate>Sat, 27 Sep 2025 21:50:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://modal.com/blog/reverse-engineer-flash-attention-4">https://modal.com/blog/reverse-engineer-flash-attention-4</a>, See on <a href="https://news.ycombinator.com/item?id=45399637">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><!----><article><!----><p>One month ago at <a rel="nofollow" href="https://hotchips.org/"><!----><!---->Hot Chips<!----></a><!---->, Tri Dao presented preliminary results on Flash Attention 4, the latest addition to the <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention"><!----><!---->Flash Attention series of CUDA kernels<!----></a><!---->. These kernels are used in the attention layers of Transformer neural networks. Along with more standard matrix multiplications, these calculations are the primary bottlenecks in contemporary generative AI workloads. Billions of dollars and gigawatts of power are being expended on GPUs to run more of these calculations faster. And Flash Attention 4 is the way to run lots of them as fast as possible. This blog post explains how it works.</p> <p>The new FA4 kernel is optimized for Nvidia’s new <a rel="nofollow" href="https://modal.com/blog/introducing-b200-h200"><!----><!---->Blackwell Streaming Multiprocessor architecture<!----></a><!----> and achieves a reported ~20% speedup over the previous state-of-the-art, the attention kernels in Nvidia’s <a rel="nofollow" href="https://modal.com/gpu-glossary/host-software/cudnn"><!----><code>cudnn</code><!----></a><!----> library.</p> <p><img src="https://modal-cdn.com/blog/images/fa4-vs-cudnn-results-slide.jpg" alt="A chart depicting the ~20% performance improvement of Flash Attention 4 over cudnn attention kernels."> <!--[!--><!--]--><!----></p> <p><code>cudnn</code> kernels are closed source, so Jensen only knows what’s going on in there.</p> <p>There’s also no official technical report on how FA4 works yet. But the source code for Flash Attention 4 was already released online <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py"><!----><!---->here<!----></a><!---->. We’ve <a rel="nofollow" href="https://github.com/sgl-project/sglang/pull/9588"><!----><!---->recently<!----></a><!----> been contributing to open source LLM inference engines, so we read the code and reverse-engineered how the kernel works, including two math tricks (faster approximate exponentials and a more efficient online softmax) that are classic Dao. This write-up contains our findings.</p> <p>Perhaps surprisingly, the architecture of FA4 is readily understandable by a general software engineering audience.</p> <p>That’s because the biggest change in FA4 isn’t the (very cool) math — it’s a massive increase in the complexity of its asynchronous “pipeline” of operations. This kind of asynchronous programming is fairly new in the world of CUDA, but <a rel="nofollow" href="https://assets.bitbashing.io/images/cubedrone-103.png"><!----><!---->pipes have been in Unix for like 40 goddamn years<!----></a><!---->. A programmer who has experience with parallel and concurrent programs, like high performance databases and web servers, will feel right at home (absent some novel <a rel="nofollow" href="https://modal.com/gpu-glossary/readme"><!----><!---->GPU technical vocabulary<!----></a><!---->).</p> <p>So we organize our write-up into two parts.</p> <p>The first section, a “quick tour”, covers the architecture of FA4 by tracing what happens as a block of inputs is turned into a block of outputs. It is written to be understandable by a practicing software engineer without any CUDA programming experience. We give brief explanations of CUDA concepts and hardware, like <a rel="nofollow" href="https://modal.com/gpu-glossary/device-software/warp"><!----><!---->warps<!----></a><!----> and <a rel="nofollow" href="https://modal.com/gpu-glossary/device-hardware/warp-scheduler"><!----><!---->warp schedulers<!----></a><!---->, but defer detailed explanation to our <a rel="nofollow" href="https://modal.com/gpu-glossary/readme"><!----><!---->GPU Glossary<!----></a><!----> (linked throughout).</p> <p>The second section, a “deep dive”, walks through each of the subcomponents in turn, explaining what each does, supported by links to the source code for particularly intrepid spelunkers.</p> <h2 id="a-quick-tour-of-flash-attention-4-the-life-of-a-tile">A quick tour of Flash Attention 4: The “Life of a Tile”</h2> <p>We start with <a rel="nofollow" href="https://float.exposed/b0x3a0a"><!----><!---->bf16<!----></a><!----> tensors of queries, keys, and values in <a rel="nofollow" href="https://modal.com/gpu-glossary/device-software/global-memory"><!----><!---->global memory<!----></a><!----> (aka <a rel="nofollow" href="https://modal.com/gpu-glossary/device-hardware/gpu-ram"><!----><!---->GPU RAM<!----></a><!---->). We’re aiming to produce a tensor of bf16 outputs, also in global memory. Outputs are values weighted by the similarity of queries to keys. Computing this weighting requires matrix multiplication, exponentiation, and normalization.</p> <p>Like the good engineers we are, we tackle this very big problem by breaking it down into smaller pieces. That’s fairly literal in this case: we take our very large input tensor and split it up into “tiles” of adjacent rows and columns, each of which contribute to the calculation of one tile of outputs.</p> <p>Specifically, one running instance of our kernel program (namely, one <a rel="nofollow" href="https://modal.com/gpu-glossary/device-software/cooperative-thread-array"><!----><!---->“cooperative thread array”<!----></a><!----> of threads) produces two tiles of the outputs tensor by reading two tiles of the queries tensor. In between, it streams all of the keys &amp; values for each query tile. Keys and values are also read in tiles. If you’re a database ‘head, you might think of it as a vectorized sequential scan for a batch of aggregation queries against a key-value store.</p> <p><img src="https://modal-cdn.com/blog/images/fa4-streaming-tiles.jpg" alt="A diagram showing a tile of queries combining with a stream of key and value tiles to produce an output tile."> <!--[!--><!--]--><!----></p> <p>By running this tile-level program many times concurrently (typically, massively in parallel), we produce the entire outputs tensor. This is a <a rel="nofollow" href="https://modal.com/gpu-glossary/device-software/kernel"><!----><!---->“single program, multiple data” execution model<!----></a><!---->, where each datum is a pair of tiles. This kind of concurrency <em>across</em> program instances is the bread-and-butter of the <a rel="nofollow" href="https://modal.com/gpu-glossary/device-software/cuda-programming-model"><!----><!---->CUDA programming model<!----></a><!----> and is transparently handled for the programmer by the <a rel="nofollow" href="https://modal.com/gpu-glossary/host-software/cuda-software-platform"><!----><!---->CUDA runtime<!----></a><!---->.</p> <p>But with the fastest contemporary kernels, like Flash Attention 3 &amp; 4 and all state-of-the-art matrix multiplications, there is also concurrency <em>within</em> our program. Each program instance sets up an asynchronous pipeline of operations that together effect the tile-level computation depicted above. We write our kernel such that all of our pipeline steps can run as concurrently as possible as we process a tile. In Flash Attention 4, we achieve this by mapping chunks of our pipeline onto 32-thread groups called <a rel="nofollow" href="https://modal.com/gpu-glossary/device-software/warp"><!----><em>warps</em><!----></a><!----> (a technique called <em>warp specialization</em>).</p> <p>We then rely on the <a rel="nofollow" href="https://modal.com/gpu-glossary/device-hardware/warp-scheduler"><!----><!---->warp schedulers<!----></a><!----> to switch between pipeline steps within program instances on each clock, swapping out when a step stalls and swapping back in when a step’s next input is ready. Think <a rel="nofollow" href="https://blog.codingconfessions.com/p/simultaneous-multithreading"><!----><!---->simultaneous multithreading<!----></a><!---->/“hyperthreading” from CPUs, but on steroids. The diagram below, from our <a rel="nofollow" href="https://modal.com/gpu-glossary/perf"><!----><!---->GPU Performance Glossary<!----></a><!---->, depicts four cycles across four parallel slots, for a total of sixteen <a rel="nofollow" href="https://modal.com/gpu-glossary/perf/issue-efficiency"><!----><!---->execution slots<!----></a><!---->, fifteen of which are filled with warps actively executing instructions thanks to this rapid warp switching. See the <a rel="nofollow" href="https://modal.com/gpu-glossary/perf/warp-execution-state"><!----><!---->associated article<!----></a><!----> for details.</p> <p><img src="https://modal-cdn.com/gpu-glossary/terminal-cycles.svg" alt="A diagram depicting sixteen execution slots. Fifteen of them are colored in, indicating that they are filled with an active warp."> <!--[!--><!--]--><!----></p> <p>This execution model is “dual” to <a rel="nofollow" href="https://ibraheem.ca/posts/too-many-web-servers/"><!----><!---->the way that an asynchronous program for CPUs works<!----></a><!----> in the following sense. In an async CPU program, a single thread implements the entire journey of a single datum (e.g. request) through a state machine (e.g. Reading, Parsing, Writing), switching between transitions as data become ready. In an async GPU program like FA4, a single warp implements a single <em>transition</em> (e.g. from queries and values to attention scores) in a similar state machine.</p> <p><img src="https://modal-cdn.com/blog/images/fa4-cpu-async-vs-gpu-async.jpg" alt="cpu-async-vs-gpu-async.drawio.png"> <!--[!--><!--]--><!----></p> <p>The pipeline is organized with a producer/consumer model and uses barriers for synchronization.</p> <p>Unlike the concurrency across program instances, the internal pipeline concurrency is all implemented manually. This leads to quite gnar code — though the control flow will look familiar to anyone who has <a rel="nofollow" href="https://os.phil-opp.com/async-await/"><!----><!---->written their own event loop<!----></a><!---->.</p> <p>So like most async code, the FA4 kernel is best understood by tracing the path of a single tile: the “life of a tile”, akin to <a rel="nofollow" href="https://groups.google.com/a/chromium.org/g/blink-dev/c/AK_rwEp61ME"><!----><!---->the “life of a pixel” in a browser’s rendering pipeline<!----></a><!---->. In particular, let’s follow the tile’s path through the <a rel="nofollow" href="https://modal.com/gpu-glossary/device-software/memory-hierarchy"><!----><!---->memory hierarchy of the GPU<!----></a><!----> as it is transformed from initial query tile to final output tile.</p> <p>At a high level, and eliding a few details about multiple buffering that increase concurrency and parallelism, that looks something like this:</p> <p><img src="https://modal-cdn.com/blog/images/fa4-life-of-a-tile.jpg" alt="fa4-life-of-tile.drawio.png"> <!--[!--><!--]--><!----></p> <p>Which vaguely resembles a <a rel="nofollow" href="https://microservices.io/patterns/microservices.html"><!----><!---->microservices diagram<!----></a><!---->. As above, so below!</p> <p>Spelled out, that’s:</p> <ul><li>A tile of queries is loaded from global memory (<code>mQ</code>) into shared memory (<code>sQ</code>) by the Load warp. <a rel="nofollow" href="https://modal.com/gpu-glossary/device-software/shared-memory"><!----><!---->Shared memory<!----></a><!----> is a “scratchpad” L1 cache managed by the programmer.</li> <li>Tiles of keys (<code>mK</code>) and values (<code>mV</code>) are streamed into shared memory (<code>sK</code>, <code>sV</code>), also by the Load warp. Note that if the working set size permits, future loads of these tiles for other query tiles will be serviced from the hardware-managed L2 cache (not pictured).</li> <li>When each tile of keys is ready, the MMA warp multiplies it with our tile of queries using a Tensor Core, producing a tile of unnormalized attention scores in Tensor Memory (<code>tS</code>). <a rel="nofollow" href="https://modal.com/gpu-glossary/device-hardware/tensor-core"><!----><!---->Tensor Cores<!----></a><!----> are single-purpose hardware for running matmuls. <a rel="nofollow" href="https://modal.com/gpu-glossary/device-hardware/tensor-memory"><!----><!---->Tensor Memory<!----></a><!----> is another programmer-managed L1 cache designed to hold and accumulate intermediates during sequences of Tensor Core operations.</li> <li>When each tile of unnormalized attention scores is ready, a Softmax warp produces normalized attention scores for that tile in Tensor Memory (<code>tP</code>) without using the Tensor Core and updates a scaling factor used for numerical stability (in shared memory, not pictured). <ul><li>⚡️ New in Flash Attention 4: this step can use CUDA Cores instead of <a rel="nofollow" href="https://modal.com/gpu-glossary/device-hardware/special-function-unit"><!----><!---->Special Function Units (SFUs)<!----></a><!----> to perform the exponential step of the normalization. SFUs are intended to provide hardware acceleration for transcendental operations like exponentials. But there are <a rel="nofollow" href="https://modal.com/gpu-glossary/device-hardware/streaming-multiprocessor"><!----><!---->far fewer SFUs than CUDA Cores<!----></a><!---->, which can lead to queueing. The basic idea, fast approximate exponentiation in software for neural networks that can tolerate a bit of accuracy loss, was proposed in <a rel="nofollow" href="https://nic.schraudolph.org/pubs/Schraudolph99.pdf"><!----><!---->a 1999 <em>Neural Computation</em> paper by Schraudolph<!----></a><!---->, but the implementation here is quite different, involving a cubic polynomial approximation (as described in detail below).</li></ul></li> <li>When each tile of normalized attention scores is ready, a Correction warp checks if the normalization scaling factor has changed and, if necessary, rescales the final output tile in Tensor Memory (<code>tO</code>). <ul><li>⚡️ New in Flash Attention 4: the choice of when to rescale became much smarter, reportedly cutting down on output rescaling operations by a factor of 10. Roughly: the scaling factor used to be a simple running maximum. Now updates are applied only when the maximum has changed enough to impact numerical stability. This seems like a good, and very portable, idea.</li></ul></li> <li>When each rescaling update finishes, the MMA warp updates the output tile in Tensor Memory (<code>tO</code>) by accumulating it with the value tile (<code>sV</code>) scaled by the attention score tile (<code>tP</code>).</li> <li>When each tile of final output values is ready, the Correction warp stores it in shared memory (<code>sO</code>), then the Epilogue warp stores it in global memory (<code>mO</code>), and we’re done with that tile.</li></ul> <p>Our high-level, tile-centric view elides a number of details, like the number of warps assigned to each pipeline step and the use of buffers to store different tiles. It also leaves out all of the details of the barrier synchronization, which is required on both sides of every producer/consumer relationship (aka where an arrow tip meets an arrow tail in the diagram). These are critical for performance.</p> <p>We go through these details in a “warp-centric” view of the kernel below, which focuses on the operations in each warp, rather than the movement of tiles, and includes links to the source code. This is necessarily more technical and goes through some GPU-specific features at higher speed, so it’s less suitable for a general software engineering audience.</p> <p>But before that, one last takeaway for those only interested in the high level.</p> <h2 id="where-does-gpu-programming-go-from-here">Where does GPU programming go from here?</h2> <p>When <a rel="nofollow" href="https://graphics.stanford.edu/papers/brookgpu/brookgpu.pdf"><!----><!---->Ian Buck<!----></a><!----> and others designed <a rel="nofollow" href="https://modal.com/gpu-glossary/host-software/cuda-c"><!----><!---->CUDA C<!----></a><!---->, they were driven by a north star: can it be used to write a single precision vector addition (<code>saxpy</code>) with respectable performance as a clean one-liner that’s easily understood by a C programmer? The core of the <a rel="nofollow" href="https://modal.com/gpu-glossary/device-software/cuda-programming-model"><!----><!---->CUDA programming model<!----></a><!----> laid down then and described in the <a rel="nofollow" href="https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/lindholm08_tesla.pdf"><!----><!---->2008 Lindholm et al. paper<!----></a><!----> still persists today.</p> <p>What’s new in the last few years (in the Hopper and Blackwell architectures) is an increasing reliance on programmer-managed asynchrony, like FA4’s multi-stage, multi-buffered pipeline. This represents a major jump in complexity from FA3’s simpler “ping-pong” pipeline (<a rel="nofollow" href="https://www.together.ai/blog/flashattention-3"><!----><!---->added to take advantage of Hopper GPUs’ async capabilities<!----></a><!---->).</p> <p>And <a rel="nofollow" href="https://bitbashing.io/async-rust.html"><!----><!---->just as in other well-designed languages<!----></a><!---->, CUDA C/C++ has struggled to accommodate the introduction of asynchrony. It is a <a rel="nofollow" href="https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/"><!----><!---->truth universally acknowledged<!----></a><!----> that <a rel="nofollow" href="https://fasterthanli.me/articles/pin-and-suffering"><!----><!---->async programming sucks absolute ass<!----></a><!---->. That’s especially true when you need to manage your own event loop, as we’re effectively doing here. And it’s made harder, not easier, by the thread-centricity and warp uniformity of the CUDA programming model and <a rel="nofollow" href="https://modal.com/gpu-glossary/device-software/parallel-thread-execution"><!----><!---->PTX machine model<!----></a><!---->.</p> <p>No wonder <a rel="nofollow" href="https://www.youtube.com/watch?v=5e1YKqsP8i8&amp;t=1059s"><!----><!---->the Triton team gave up on writing Blackwell attention<!----></a><!----> and added the new Gluon frontend at a lower level!</p> <p>Triton’s troubles notwithstanding, this kernel is a clear instance of the swing towards tile-based, warp-specialized programming. And Nvidia is betting big on a number of new languages and libraries to try to make this easier, from the <a rel="nofollow" href="https://docs.nvidia.com/cutlass/media/docs/pythonDSL/cute_dsl_general/dsl_introduction.html"><!----><!---->CuTe DSL<!----></a><!----> and <a rel="nofollow" href="https://docs.nvidia.com/cutlass/index.html"><!----><!---->CUTLASS<!----></a><!----> C++ used in this kernel to the forthcoming <a rel="nofollow" href="https://www.youtube.com/watch?v=uZTtViomW6w"><!----><!---->CuTile<!----></a><!---->. Say what you will about the chatbot hype wave, these are exciting times for high performance numerical computing!</p> <h2 id="deep-dive-for-the-gpu-enjoyers-what-does-each-warp-do-in-flash-attention-4">Deep dive for the GPU enjoyers: What does each warp do in Flash Attention 4?</h2> <p>There are five different specializations for warps in the Flash Attention 4 kernel. They are listed below, along with links to their source code.</p> <ol><li>A <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L837"><!----><!---->Load warp<!----></a><!----> to load query, key, and value tiles from global memory into shared memory</li> <li>An <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L953"><!----><!---->MMA warp<!----></a><!----> to compute unnormalized attention scores from query and key tiles and accumulate score-weighted value tiles into the output tiles</li> <li>Eight <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1143-L1145"><!----><!---->Softmax warps<!----></a><!----> to compute normalized attention scores and track running stats (max, sum)</li> <li>Four <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1413"><!----><!---->Correction warps<!----></a><!----> to watch for updates to the normalization scale and re-normalize the output tiles</li> <li>One or two <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1737"><!----><!---->Epilogue warps<!----></a><!----> to store completed output tiles from shared memory into global memory</li></ol> <p>In the above discussion, we implied that each CTA works on just two query tiles and produces just two output tiles. That’s true in some settings, but the mapping between tiles and CTAs is technically abstracted by a <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L398-L404"><!----><code>TileScheduler</code><!----></a><!---->. For the best performance, you need to use the <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/1ceaa984b2f348caea18b39a98458d33b4ea7a09/flash_attn/cute/tile_scheduler.py#L122"><!----><code>StaticPersistentTileScheduler</code><!----></a><!---->, which launches <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/1ceaa984b2f348caea18b39a98458d33b4ea7a09/flash_attn/cute/tile_scheduler.py#L162-L163"><!----><!---->at most one CTA per Streaming Multiprocessor<!----></a><!----> and then schedules tiles onto those SMs. This reduces CTA launch overhead and allows for more fine-grained concurrency (e.g. overlapping Epilogue warps for one tile with the Load and MMA warps for the next tile).</p> <p>The core work of the kernel is the same — there’s just not a clean mapping of work onto thread constructs, which makes explaining the work harder. From here, we’ll go back to speaking about the code as though each CTA handles only two tiles (which is literally true if you use the <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/1ceaa984b2f348caea18b39a98458d33b4ea7a09/flash_attn/cute/tile_scheduler.py#L56"><!----><code>SingleTileScheduler</code><!----></a><!---->).</p> <p>Also, from here we will start using some shorthand, matching the code and convention: Q for queries, K for keys, V for values, O for outputs, S for unnormalized attention scores, and P for normalized attention scores/“probabilities”.</p> <h3 id="the-load-warp-loads-two-q-tiles-and-streams-all-k-and-v-tiles">The Load warp loads two Q tiles and streams all K and V tiles.</h3> <p>The <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L837"><!----><!---->Load warp<!----></a><!----> operates on <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L841-L843"><!----><!---->pointers to Q, K, and V tensors in global memory<!----></a><!----> and writes to <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L844-L846"><!----><!---->Q, K, and V tensors in shared memory<!----></a><!---->. It supports paged keys and values (as in <a rel="nofollow" href="https://arxiv.org/abs/2309.06180"><!----><!---->Paged Attention<!----></a><!---->, not as in operating system pages) via an <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L847"><!----><!---->optional “page table” tensor<!----></a><!----> (again, <em>not</em> the page tables co-managed by the OS, the CPU, and the MMU).</p> <p>It uses the <a rel="nofollow" href="https://modal.com/gpu-glossary/device-hardware/tensor-memory-accelerator"><!----><!---->Tensor Memory Accelerator (TMA)<!----></a><!----> to reduce register pressure from multidimensional array access and <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L889-L909"><!----><!---->fire off copies asynchronously<!----></a><!---->. This also avoids very long <a rel="nofollow" href="https://modal.com/gpu-glossary/perf/warp-execution-state"><!----><!---->warp stalls<!----></a><!----> on loads that would require even more warp specialization to <a rel="nofollow" href="https://modal.com/gpu-glossary/perf/latency-hiding"><!----><!---->hide latency<!----></a><!---->.</p> <p>The Load warp <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L934-L935"><!----><!---->loads two Q tiles<!----></a><!---->. It loads all K and V blocks <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L939-L946"><!----><!---->in a loop<!----></a><!---->. It is the ”<a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L859"><!----><!---->producer<!----></a><!---->” of these tiles (in a producer/consumer setup). It can <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L170"><!----><!---->concurrently load up to three blocks each of K and V<!----></a><!---->.</p> <p>As it completes these loads, the Load warp signals their completion to the MMA warp through <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L852"><!----><!---->an array of barriers in shared memory<!----></a><!---->. All barriers (not just for Load/MMA synchronization) are <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L913"><!----><!---->referenced via their offset in this array<!----></a><!----> to support variable barrier counts with different configuration settings.</p> <h3 id="the-mma-warp-computes-unnormalized-attention-scores-and-output-values">The MMA warp computes unnormalized attention scores and output values.</h3> <p>The <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L953"><!----><!---->MMA warp<!----></a><!----> operates on pointers to Q, K, and V tensors in <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L957-L959"><!----><!---->shared memory<!----></a><!---->. For every K/V tile, it runs <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L990"><!----><!---->two matmuls to create S tiles<!----></a><!----> and <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L998"><!----><!---->two matmuls for O<!----></a><!----> (Q/K for the S tiles, P/V for the O tiles). The matmuls are <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/1ceaa984b2f348caea18b39a98458d33b4ea7a09/flash_attn/cute/blackwell_helpers.py#L327-L369"><!----><!---->emitted as inline PTX assembly<!----></a><!---->, as is necessary for CUDA C/C++ programs to use the Tensor Cores in Hopper and Blackwell. The vast majority of the FLOPS in this kernel are driven by these lines; most everything else is memory management.</p> <p>The specific PTX instruction used is <code>tcgen05.mma.cta_group::1</code>. <code>mma</code> is matrix-multiply-accumulate. <code>tcgen05</code> means <code>5</code>th generation <code>t</code>ensor <code>c</code>ore, aka Blackwell, as in <code>sm100</code>/<a rel="nofollow" href="https://modal.com/gpu-glossary/device-software/compute-capability"><!----><!---->Compute Capability 10.0<!----></a><!---->. <code>cta_group::1</code> means we run our matmul using only a single CTA, avoiding the nastiness of <a rel="nofollow" href="https://modal.com/gpu-glossary/device-hardware/texture-processing-cluster"><!----><!---->TPC<!----></a><!---->-based 2SM/2CTA matmuls <a rel="nofollow" href="https://docs.nvidia.com/cutlass/media/docs/cpp/blackwell_functionality.html"><!----><!---->available in Blackwell<!----></a><!---->. This likely introduces a small memory throughput penalty but simplifies CTA/tile scheduling. Interestingly, the <a rel="nofollow" href="https://github.com/HazyResearch/ThunderKittens/blob/2ba96ceedfb1b5c5d6e1eb4a1241a24d16049be4/kernels/attn/b200/b200.cu"><!----><!---->ThunderKittens Blackwell attention kernel<!----></a><!----> makes a different choice.</p> <p>Also on the front of scheduling/simplification: only a single <code>leader_thread</code> issues the instruction. And we’re only working from a single warp. This is an important difference from performant Hopper MMAs, which were coordinated across an entire warpgroup.</p> <p>After <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1017"><!----><!---->getting hold of a Q tile<!----></a><!----> and our first K tile, we <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1031"><!----><!---->run our first matmul<!----></a><!----> to produce our first result for S. Then we <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1045"><!----><!---->loop over the remaining K and V tiles<!----></a><!----> and update S and O. These <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L963-L964"><!----><!---->S and O tensors<!----></a><!----> live in Tensor Memory. This is <a rel="nofollow" href="https://modal.com/gpu-glossary/device-hardware/tensor-memory"><!----><!---->the “intended” use of Tensor Memory<!----></a><!---->, as a store for accumulators read from and written to by the Tensor Cores.</p> <p>Since the K and V tiles are buffered, we need to signal the Load warp every time we finish using them (eg <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1073-L1075"><!----><!---->here<!----></a><!---->, signaling that the memory containing V can be reused once it has been used to construct the second O tile). There’s some additional coordination here (around S, P, and O), which we’ll discuss as it comes in up in the other warps.</p> <h3 id="eight-softmax-warps-produce-normalized-attention-scores">Eight Softmax warps produce normalized attention scores.</h3> <p>The Softmax warps <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1401"><!----><!---->produce normalized attention scores<!----></a><!----> (P, as in “probabilities”) consumed by the MMA warps. Ignore the name and don’t try to come up with an interpretation of the attention scores as the probability distribution for a random variable; it’ll make your head hurt and give you bad intuition about Transformers. They’re <a rel="nofollow" href="https://transformer-circuits.pub/"><!----><!---->better thought of<!----></a><!----> as weights for a linear combination of vectors from V.</p> <p>The core <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1143-L1145"><!----><!---->softmax operation is implemented by two warpgroups<!----></a><!---->, aka eight warps. The two warpgroups are mapped onto the two query/output tile workstreams. Warpgroups are made up of four adjacent warps with a warp index alignment of four. Using them was critical for the fast warpgroup MMAs in Hopper GPUs, <a rel="nofollow" href="https://www.together.ai/blog/flashattention-3"><!----><!---->as in Flash Attention 3<!----></a><!---->, but we didn’t see anything in this kernel that made explicit use of them. Warpgroup alignment may lead to more even distribution of work across warp schedulers/subunits of the SM, as it did in Hopper, which had <a rel="nofollow" href="https://modal.com/gpu-glossary/device-hardware/streaming-multiprocessor-architecture"><!----><!---->four warp schedulers per SM<!----></a><!---->. To our <a rel="nofollow" href="https://en.wikipedia.org/wiki/Blackwell_(microarchitecture)#Blackwell_dies"><!----><!---->and Wikipedia’s<!----></a><!----> knowledge, this level of detail on SM100 Blackwell GPUs like B200s is not published anywhere (but it <a rel="nofollow" href="https://images.nvidia.com/aem-dam/Solutions/geforce/blackwell/nvidia-rtx-blackwell-gpu-architecture.pdf"><!----><!---->is true of SM120 RTX Blackwell GPUs<!----></a><!---->).</p> <p>We’re also not certain of the reason why some pipeline stages are assigned more warps than others and in this particular ratio. Presumably, it helps ensure balanced throughput across the different stages, but our napkin math on relative operational load, bandwidth, and latency between the matmuls and the attention operations didn’t produce a smoking gun. We speculate that it was determined by benchmarking.</p> <p>Each warp runs <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1315"><!----><!---->a single step<!----></a><!----> of the online softmax calculation at a time while <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1267-L1269"><!----><!---->looping over<!----></a><!----> the <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1384"><!----><!---->S tiles produced by the MMA warp<!----></a><!---->.</p> <p>Looking within the individual <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1315"><!----><!---->softmax step<!----></a><!---->: the unnormalized attention scores are stored in Tensor Memory, which can only be <em>directly</em> operated on by the Tensor Cores. But the Tensor Cores can only do matrix multiplication. So the Softmax warps have to <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1363"><!----><!---->copy the scores<!----></a><!----> into the registers to apply the exponentiation and then <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1398"><!----><!---->copy the result back<!----></a><!---->.</p> <p>The exponentiation is done differently than in previous versions of Flash Attention. FA3 and earlier used the GPU’s Special Function Units to perform a hardware-accelerated exponentiation. Specifically, they use the <code>exp2</code> CUDA PTX intrinsic, which is typically <a rel="nofollow" href="https://godbolt.org/z/7e5jx9qcr"><!----><!---->mapped by the (closed-source) <code>ptxas</code> compiler to the <code>MUFU.EX2</code> SASS instruction<!----></a><!---->.</p> <p>The FA4 kernel does that too, but for <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/add175637c5d54b74bc25372e49ce282d6f236fc/flash_attn/cute/flash_fwd_sm100.py#L1390-L1391"><!----><!---->smaller attention head sizes<!----></a><!----> it <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/0165c96fff7a7cd2e152aa9659f75c972a702f5d/flash_attn/cute/softmax.py#L234-L238"><!----><!---->additionally mixes in a different exponentiation algorithm on some iterations with a tunable frequency<!----></a><!---->. That implementation uses <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/add175637c5d54b74bc25372e49ce282d6f236fc/flash_attn/cute/utils.py#L501-L541"><!----><!---->this block of inline PTX<!----></a><!----> to compute <code>2 ** x</code>. The algorithm splits the exponentiation into two parts: the easy integer part (<code>2 ** floor(x)</code>) and the hard rational part (<code>2 ** (x - floor(x))</code>). It uses a cubic polynomial to approximate <code>2 ** x</code> on the unit interval (check out the approximation on Wolfram Alpha <a rel="nofollow" href="https://www.wolframalpha.com/input?i=0.07711909*r%5E3%2B0.22756439*r%5E2%2B0.69514614*r%2B1.0+compared+to+2%5Er"><!----><!---->here<!----></a><!---->).</p> <p>The cubic polynomial calculation is done, following Horner’s method for linear time polynomial evaluation, with three fused multiply-adds (<code>fma</code>):</p> <!----> <p>Note that <code>f32x2</code> means that we operate on a vector (as in <a rel="nofollow" href="https://people.eecs.berkeley.edu/~pattrsn/252S98/Lec06-vector.pdf"><!----><!---->vector lanes<!----></a><!---->) of two 32 bit values. You can read about a similar implementation for CPU vector instructions on Stack Overflow <a rel="nofollow" href="https://stackoverflow.com/questions/47025373/fastest-implementation-of-the-natural-exponential-function-using-sse"><!----><!---->here<!----></a><!---->.</p> <p>In addition to only applying this method on some iterations, it <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/0165c96fff7a7cd2e152aa9659f75c972a702f5d/flash_attn/cute/softmax.py#L234-L238"><!----><!---->stops applying it on a configurable number of the last S tiles<!----></a><!---->. Together, these suggest that the reason for applying it is to avoid a bottleneck on the SFUs (which, due to <a rel="nofollow" href="https://www.thonking.ai/p/what-shapes-do-matrix-multiplications"><!----><!---->wave quantization effects<!----></a><!---->, is less relevant for the final tiles).</p> <p>The Softmax warps also <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1282"><!----><!---->track the running statistics for rescaling and normalizing attention scores<!----></a><!----> used by the Correction warps, as discussed below.</p> <p>There’s another important change here. All softmax algorithms need to handle <a rel="nofollow" href="https://en.wikipedia.org/wiki/Softmax_function#Numerical_algorithms"><!----><!---->numerical instability caused by exponentiation of large values<!----></a><!---->. Before Flash Attention, this was usually done by finding the largest value in each row and subtracting it from the value before exponentiating. All Flash Attention kernels use a streaming or online softmax algorithm, and the largest value is not known in advance — searching through the scores to find it would defeat the purpose of using a streaming algorithm! Instead, they use a running maximum for numerical stability and update the scaling factor whenever a new maximum is encountered. This ensures continued numerical stability and avoids an extra scan, but requires a costly correction of previous values (handled by the Correction warps) every time a new maximum is observed.</p> <p>This is inefficient. We only need to update the scaling factor <em>when the new maximum changes enough to threaten numerical stability</em>, not every time a new maximum appears. That logic is implemented <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/0165c96fff7a7cd2e152aa9659f75c972a702f5d/flash_attn/cute/softmax.py#L176-L179"><!----><!---->here<!----></a><!---->. In the Hot Chips talk, Dao indicated that this reduced the number of corrections by a factor of 10.</p> <p>There is additional support for <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1153"><!----><!---->attention sinks<!----></a><!----> and storing <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1152"><!----><!---->the log-sum-exp tensor<!----></a><!----> used in the backwards pass. At time of writing in late September 2025, a backwards version of this kernel is not available, but is expected imminently.</p> <h3 id="four-correction-warps-rescale-previous-outputs-as-the-normalization-changes">Four Correction warps rescale previous outputs as the normalization changes.</h3> <p>The <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1413"><!----><!---->Correction warps<!----></a><!----> update <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1418"><!----><!---->past output results<!----></a><!----> from the MMA warps <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1476-L1477"><!----><!---->as the numerical stability scaling factor changes<!----></a><!---->. The Correction warps need to coordinate their access to the O values in Tensor Memory with the MMA warps (eg <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1533"><!----><!---->here<!----></a><!---->, indicating that those values are consumed and the memory can be reclaimed).</p> <p>Like the Softmax warps, the four Correction warps form a warpgroup. Also like the Softmax warps, they need to load from Tensor Memory to registers to apply their <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1588"><!----><!---->non-matmul rescaling operation<!----></a><!---->.</p> <p>The Correction warps are also responsible for writing the output from Tensor Memory to shared memory and applying the final scaling by the row sum. This is <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1527-L1529"><!----><!---->called the <code>correction_epilogue</code><!----></a><!---->. “Epilogue” here means the same thing as in the name of the “Epilogue” warps — an operation that occurs at the end of a sequence of operations on values stored in one memory and before the results are written to another memory. But in this case, it refers to operations on data in Tensor Memory before they are stored to shared memory, whereas the Epilogue warps take data from shared memory and store it in global memory.</p> <p>This is especially confusing because the completion of this epilogue is the signal for the Epilogue warps to start their work.</p> <p>The Correction warps have the <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1420"><!----><!---->global memory output tensor among their arguments<!----></a><!---->, but only use it in commented-out code.</p> <h3 id="the-epilogue-warps-store-complete-output-tiles-back-into-global-memory">The Epilogue Warp(s) store complete output tiles back into global memory.</h3> <p>There are either one or two <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1737"><!----><!---->Epilogue warps<!----></a><!----> depending on <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L367"><!----><!---->whether the TMA is enabled<!----></a><!---->.</p> <p>In <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1759"><!----><!---->the case that the Epilogue warps can use the TMA<!----></a><!---->, there’s only one and its work is simple. It <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1770"><!----><!---->waits on the correction loop to finish for an output tile<!----></a><!---->, then <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1772-L1773"><!----><!---->runs a TMA copy<!----></a><!---->, then <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1777"><!----><!---->signals that it has finished reading the O tensor in shared memory<!----></a><!----> and the buffer can be reused.</p> <p>If <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1778"><!----><!---->they can’t use the TMA<!----></a><!---->, their work is more complicated — they need to handle <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1780"><!----><!---->slicing<!----></a><!----> and <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1789"><!----><!---->packing<!----></a><!---->, which is <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1787"><!----><!---->pretty hard<!----></a><!---->. It also consumes <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py#L1795"><!----><!---->quite a few more registers<!----></a><!---->.</p> <h2 id="if-you-made-it-this-far-you-might-enjoy-working-at-modal">If you made it this far, you might enjoy working at Modal.</h2> <p>At Modal, we’re building the cloud infrastructure that compute-intensive workloads like giant Transformers need. Our platform is used by companies like <a href="https://modal.com/blog/suno-case-study"><!----><!---->Suno<!----></a><!---->, <a href="https://modal.com/blog/lovable-case-study"><!----><!---->Lovable<!----></a><!---->, <a href="https://modal.com/blog/ramp-case-study"><!----><!---->Ramp<!----></a><!---->, and <a href="https://modal.com/blog/substack-case-study"><!----><!---->Substack<!----></a><!---->. We’re <a href="https://modal.com/careers"><!----><!---->hiring<!----></a><!---->.</p> <p><em>The authors would like to thank Simon Mo of vLLM, Michael Goin of RedHat AI, and Kimbo Chen of SemiAnalysis for their comments on drafts of this article. We’d also like to thank Tri Dao for writing another banger of a kernel.</em></p><!----></article><!----></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Handy – Free open-source speech-to-text app written in Rust (217 pts)]]></title>
            <link>https://handy.computer/</link>
            <guid>45399106</guid>
            <pubDate>Sat, 27 Sep 2025 20:33:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://handy.computer/">https://handy.computer/</a>, See on <a href="https://news.ycombinator.com/item?id=45399106">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-j7pv25f6="" id="main-content" role="main"><div data-astro-cid-j7pv25f6=""><video width="640" height="360" controls="" playsinline="" aria-label="Handy speech-to-text demonstration video" aria-describedby="video-transcript" data-astro-cid-j7pv25f6=""><source src="https://handy.computer/handy-video.mp4" type="video/mp4" data-astro-cid-j7pv25f6=""><source src="https://handy.computer/handy-video.webm" type="video/webm" data-astro-cid-j7pv25f6=""><track kind="captions" src="/handy-video.vtt" srclang="en" label="English captions" default="" data-astro-cid-j7pv25f6=""><p data-astro-cid-j7pv25f6="">
Your browser doesn't support HTML video. <a href="https://handy.computer/handy-video.webm" data-astro-cid-j7pv25f6="">Download the video</a> instead.
</p></video><div id="video-transcript" data-astro-cid-j7pv25f6=""><h3 data-astro-cid-j7pv25f6="">Video Transcript</h3><p data-astro-cid-j7pv25f6="">
CJ: Hello, I'm CJ and I want to show you Handy. Handy is an
                    open source speech-to-text application that you can run on
                    your own computer. Simply press a keyboard shortcut, speak,
                    and release, and Handy will paste whatever you said into the
                    text field you're typing into.
</p><p data-astro-cid-j7pv25f6="">
Let's take a look at the settings menu for Handy, and it's
                    really simple. You have a push-to-talk mode that you can
                    enable, this is enabled by default so you press and hold the
                    keys or alternatively you can turn it off so the
                    transcription starts when you press the key combination and
                    it stops when you press it again. And you can also change
                    what key binding you would like to use for the
                    transcription.
</p><p data-astro-cid-j7pv25f6="">
So now it's mapped to Ctrl-Z and if I turn this off, when I
                    hit control Z, when you look up in the top corner of my Mac
                    here, this little transcription icon lights up. And when I
                    click it again, it turns off and transcribes the audio.
                    There's nothing to paste into. So it just does nothing here.
</p><p data-astro-cid-j7pv25f6="">So sit back, relax, and let Handy give you a hand.</p></div></div><div data-astro-cid-j7pv25f6=""><div data-astro-cid-j7pv25f6=""><h2 data-astro-cid-j7pv25f6="">Free</h2><p data-astro-cid-j7pv25f6="">
Accessibility tooling belongs in everyone's hands, not
                    behind a paywall.
</p></div><div data-astro-cid-j7pv25f6=""><h2 data-astro-cid-j7pv25f6="">Open Source</h2><p data-astro-cid-j7pv25f6="">
Together we can build further. Extend Handy for yourself and
                    contribute to something bigger.
</p></div><div data-astro-cid-j7pv25f6=""><h2 data-astro-cid-j7pv25f6="">Private</h2><p data-astro-cid-j7pv25f6="">
Your voice stays on your computer. Get transcriptions
                    without sending audio to the cloud.
</p></div><div data-astro-cid-j7pv25f6=""><h2 data-astro-cid-j7pv25f6="">Simple</h2><p data-astro-cid-j7pv25f6="">
One tool, one job. Transcribe what you say and put it into a
                    text box.
</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NixOS moderation team resigns over NixOS Steering Committee's interference (101 pts)]]></title>
            <link>https://discourse.nixos.org/t/a-statement-from-members-of-the-moderation-team/69828</link>
            <guid>45398891</guid>
            <pubDate>Sat, 27 Sep 2025 20:00:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://discourse.nixos.org/t/a-statement-from-members-of-the-moderation-team/69828">https://discourse.nixos.org/t/a-statement-from-members-of-the-moderation-team/69828</a>, See on <a href="https://news.ycombinator.com/item?id=45398891">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/DiscussionForumPosting">
      <meta itemprop="headline" content="A statement from members of the moderation team">
      
      <meta itemprop="datePublished" content="2025-09-27T09:22:15Z">
        <meta itemprop="articleSection" content="Announcements">
      <meta itemprop="keywords" content="">
      


          <div id="post_1">
            <div>
              


              <p><span>
                  <time datetime="2025-09-27T09:22:15Z">
                    September 27, 2025,  9:22am
                  </time>
                  <meta itemprop="dateModified" content="2025-09-27T09:22:15Z">
              <span itemprop="position">1</span>
              </span>
            </p></div>
            <div itemprop="text">
              <p><strong>We resign, effective immediately, in protest of the Steering Committee’s ongoing pattern of attempting to interfere with moderation team operation, membership and specific moderation decisions.</strong></p>
<p>This is not a statement we enjoy making, and we apologize to the community for leaving right before an election that is bound to be contentious, and likely now more so. Unfortunately, the Constitution does not provide a meaningful recourse to SC overreach, and we cannot in good faith continue operating under the current conditions, leaving us no other options.</p>
<p>The SC has involved itself in matters of moderation since its inception, but has repeatedly failed to understand the issues in the community and the requirements of moderation. We have experienced:</p>
<ul>
<li>SC members attempting to stall implementation of some moderation decisions and actively subverting others</li>
<li>SC members asserting their authority to specifically target individual community members and topics of conversation, and pressure moderation to apply additional action under threat of further interference</li>
<li>SC members demanding justification for moderation actions post-hoc, responding agressively when explanations have been misunderstood, and going silent with no acknowledgement of further clarifications</li>
<li>SC attempting to <em>unilaterally remove moderation team members with no justification</em></li>
<li>SC attempting to unilaterally appoint new members to the moderation team
<ul>
<li>intially phrased as a suggestion, with a stated goal of adding “diversity of opinion” and “tension” to the moderation team</li>
<li>apparently trying to address <em>perceptions of political bias</em> by <em>making political appointments</em></li>
<li>despite this suggestion being immediately rejected as destructive and misguided by the moderation team</li>
<li>despite the specific candidate being rejected as unsuitable by the moderation team, and agreement from SC that at least some of the reasons discussed were disqualifying</li>
<li>eventually phrased as a mandatory directive, after no further mention of the candidate in the intervening months, and after said candidate explicitly petitioning SC to install them as a moderator</li>
</ul>
</li>
</ul>
<p>The SC has also shown, in private and public conversations, their lack of understanding of basic principles of community management and open communication. They have mistaken quiet and a lack of controversy for success and peace. They have consistently become upset when there is criticism, and gone quiet on crucial issues in between. We have some fundamental conflicts in this community, which absolutely require discussion. Meanwhile, discussion with the SC has only become less effective.</p>
<p>We think that the goal of moderation should not be to avoid difficult conversations - it’s to navigate those difficult conversations in ways that remain safe and constructive. We believe we’ve made considerable progress as a community on making those conversations happen, and we believe they need to happen more for the project to grow, not be suppressed. We thank everyone for the growth that we have seen, and for their efforts to avoid personal focus in discussion, especially recently.</p>
<p><strong>We call on the SC</strong>: to join us in resigning, effective immediately, with no second terms, and allow new members to take their place based on the community vote.</p>
<p><strong>We call on the community</strong>: to demand transparency and accountability from the elected SC members, and checks and balances on their reach.</p>
<p><strong>We call on the SC candidates</strong>: to commit to implementing a Constitution reform that will require transparency and accountability from the SC, with teams like technical steering and moderation providing a counterbalance.</p>
<p>We’re not leaving the community - yet, anyway. We will be around. Measures are in place to ensure essential capabilities are maintained. We hope to see this community grow and prosper, and we believe that it is only possible through transparency, accountability and trust.</p>
<ul>
<li>0x4A6F</li>
<li>arianvp</li>
<li>K900</li>
<li>nim65s</li>
<li>uep</li>
</ul>
            </div>

            

                
          </div>
          <div id="post_2" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>Thank you for your services and commitment to this community, it was and is highly appreciated by many of us. Let’s hope we will be able to reach better days yet.</p>

            

          </div>
          <div id="post_3" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.nixos.org/u/drupol"><span itemprop="name">drupol</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-09-27T09:30:11Z">
                    September 27, 2025,  9:30am
                  </time>
                  <meta itemprop="dateModified" content="2025-09-27T09:30:11Z">
              <span itemprop="position">3</span>
              </span>
            </p>
            <p>Thanks for your work so far, hoping the future will be brighter.</p>

            

          </div>
          
          <div id="post_5" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.nixos.org/u/riotbib"><span itemprop="name">riotbib</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-09-27T10:55:35Z">
                    September 27, 2025, 10:55am
                  </time>
                  <meta itemprop="dateModified" content="2025-09-27T10:55:35Z">
              <span itemprop="position">5</span>
              </span>
            </p>
            <div itemprop="text">
              <p><strong>Thanks for your work, you all!</strong> And thank you for seeking transparency and drawing consequences. I respect your decisions.</p>
<p>Could SC comment on the accusations before the elections? Especially on the last two points about trying to remove team members and appointing their own?</p>
            </div>

            

          </div>
          <div id="post_6" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.nixos.org/u/roberth"><span itemprop="name">roberth</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-09-27T11:57:20Z">
                    September 27, 2025, 11:57am
                  </time>
                  <meta itemprop="dateModified" content="2025-09-27T11:57:20Z">
              <span itemprop="position">6</span>
              </span>
            </p>
            <div itemprop="text">
              <p>Dear 0x4A6F, arianvp, K900, nim65s, uep and community,</p>
<p>I’d like to share my personal view on your resignation, which I support.</p>
<blockquote>
<p>in protest of the Steering Committee’s ongoing pattern of attempting to interfere with moderation team operation, membership and specific moderation decisions.</p>
</blockquote>
<p>The SC has tried to work with the moderation team to understand moderation decisions and steer towards more objective moderation behavior, with the goal of making moderation fair and respectable, which feeds back into making moderation work easier.<br>
Nonetheless, we have continued to observe moderation not based on the Code of Conduct, but opinions and interpersonal tradeoffs (to put it nicely).</p>
<p>Furthermore, we have observed an unwillingness to be accountable to the Steering Committee; the only body they are directly accountable to anyway.<br>
Due to this continued pattern, we’ve had to take stronger action.</p>
<blockquote>
<p>We call on the SC: to join us in resigning, effective immediately, with no second terms, and allow new members to take their place based on the community vote.</p>
</blockquote>
<p>I have no plans to resign, nor do I believe <a href="https://discourse.nixos.org/u/ericson2314">@Ericson2314</a> will. I believe the NCA made a good decision to stagger elections and smoothen SC transitions. Furthermore I believe I can continue to represent the community.</p>
<blockquote>
<p>We call on the SC candidates: to commit to implementing a Constitution reform that will require transparency and accountability from the SC, with teams like technical steering and moderation providing a counterbalance.</p>
</blockquote>
<p>Are you asking for an elected body to be accountable to an unelected people. I don’t think this is entirely impossible, but it at least needs more thought put into it, and before taking any sort of bureaucratic approach, we should consider changing the governance culture, which is entirely within an SC’s power.</p>
<p>Looking back on the past year, I believe the lack of transparency has at first served us well in terms of reducing drama and giving some “breathing room”, but since this summer, I have felt that balance shift. To be frank, making such a change was difficult in the face of numerous ongoing issues.</p>
<p>I acknowledge that more openness is needed, and this is important for the effectiveness of the SC and the community as a whole to build a respectable reputation for the SC as part of the governance culture. This is an area in which the current SC has not been able to develop, which I agree is unfortunate. I believe it was necessary, and it should not stain the future development of the community. Also, as I have alluded to, I do not believe a constitutional change is currently required to guarantee openness, unless the next SC is somehow unable to change the governance culture to be more open.</p>
<p>This will be a turning point, for both the SC and moderation, neither of which should operate in a “damage control” mode anymore.</p>
            </div>

            

          </div>
          <div id="post_7" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.nixos.org/u/SergeK"><span itemprop="name">SergeK</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-09-27T12:07:53Z">
                    September 27, 2025, 12:07pm
                  </time>
                  <meta itemprop="dateModified" content="2025-09-27T12:07:53Z">
              <span itemprop="position">7</span>
              </span>
            </p>
            <div itemprop="text">
              





<p>IMO, you bring the fight to meydan<sup><a href="#footnote-231447-1" id="footnote-ref-231447-1">[1]</a></sup>, you let meydan learn the facts judge for themselves</p>
<hr>

<ol>
<li id="footnote-231447-1"><p><a href="https://en.wiktionary.org/wiki/%CE%B1%CE%B3%CE%BF%CF%81%CE%AC" rel="noopener nofollow ugc">αγορά - Wiktionary, the free dictionary</a><br>
<a href="https://en.wiktionary.org/wiki/maidan#Etymology_1" rel="noopener nofollow ugc">maidan - Wiktionary, the free dictionary</a> <a href="#footnote-ref-231447-1">↩︎</a></p>
</li>
</ol>
            </div>

            

          </div>
          <div id="post_8" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.nixos.org/u/joepie91"><span itemprop="name">joepie91</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-09-27T13:02:36Z">
                    September 27, 2025,  1:02pm
                  </time>
                  <meta itemprop="dateModified" content="2025-09-27T13:02:36Z">
              <span itemprop="position">8</span>
              </span>
            </p>
            <div itemprop="text">
              
<p>The project governance in whichever form has tried to chase this goal of “objectivity” for as long as I remember being involved with NixOS, and for just as long it has failed to produce the envisioned outcome of a healthy community, despite repeated changes of entire moderation teams.</p>
<p>I would suggest that the SC should take some time to think about why that is, and whether perhaps there is an expertise-based reason why moderators have not actually operated this way in practice, and whether the SC really has the requisite background to decide on the correct policy here.</p>
<p>I’ll leave my comments at that.</p>
            </div>

            

          </div>
          <div id="post_9" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.nixos.org/u/withakay"><span itemprop="name">withakay</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-09-27T13:48:52Z">
                    September 27, 2025,  1:48pm
                  </time>
                  <meta itemprop="dateModified" content="2025-09-27T13:48:52Z">
              <span itemprop="position">9</span>
              </span>
            </p>
            <div itemprop="text">
              <p>Thank you for your commitment of time and energy towards the moderation of official NixOS spaces. I believe that moderation is a fraught and difficult task; that maintaining a fair and principled approach is never easy, nor is the appearance of fairness always going to be possible; and that moderation makes demands on time, attention and availability that other aspects of governance may not. Thank you for your willingness to step into that role, and for your service.</p>
<p>Could we get clarification on some matters?</p>
<ol>
<li>The members listed as resigning are the 5 members 0x4A6F, arianvp, K900, nim65s, and uep. Based on the <a href="https://nixos.org/community/teams/moderation/">Moderation Team Page</a>, which lists 7 members, my understanding then is that lassulus and Aleksana have not resigned. Is that correct?</li>
<li>Without undue breach of privacy, is it possible to better understand:</li>
</ol>

<p>?</p>
<ol start="3">
<li>Without undue breach of privacy, is it possible to better understand</li>
</ol>

<p>?</p>
            </div>

            

          </div>
          <div id="post_10" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>I agree with everything <a href="https://discourse.nixos.org/u/roberth">@roberth</a> said. I’m not resigning either.</p>

            

          </div>
          <div id="post_11" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.nixos.org/u/Aleksanaa"><span itemprop="name">Aleksanaa</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-09-27T15:35:59Z">
                    September 27, 2025,  3:35pm
                  </time>
                  <meta itemprop="dateModified" content="2025-09-27T15:35:59Z">
              <span itemprop="position">11</span>
              </span>
            </p>
            <div itemprop="text">
              
<p>I will be withdrawing for personal reasons. I’m currently studying for a Master’s degree in another major, which is quite tight, and I have internships (and hopefully a job) coming up soon. I have also lost the interest to oversee all the NixOS community affairs.</p>
<p>Apart from that, I will try my best to use my free time to do maintenance work, but I can’t guarantee that.</p>
            </div>

            

          </div>
          <div id="post_12" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.nixos.org/u/kiara"><span itemprop="name">kiara</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-09-27T16:09:42Z">
                    September 27, 2025,  4:09pm
                  </time>
                  <meta itemprop="dateModified" content="2025-09-27T16:09:42Z">
              <span itemprop="position">12</span>
              </span>
            </p>
            <div itemprop="text">
              <p>at NixCon '24 the NCA introduced its created constitution as initial steps in a gradual process of improvement, and i believe the current situation is demonstrating opportunity for improvement.</p>
<p>in light of multiple SC members declining the opportunity to reaffirm their mandate (at a point where said mandate is credibly called into question, i.e. backed by a body’s majority resignation), I feel inclined to <a href="https://discourse.nixos.org/t/sc-member-tomberek-works-for-anduril/68971/68">reiterate</a> it’s unfortunate our governance model has no recourse yet when an elected member were to lose their electorate’s trust.</p>
<p>(if we were to revise this, a follow-up concern would be <a href="https://discourse.nixos.org/t/sc-member-tomberek-works-for-anduril/68971/76">technical implementations</a>.)</p>
            </div>

            

          </div>
          <div itemprop="comment" id="post_13" itemscope="" itemtype="http://schema.org/Comment">
              
<p>Doesn’t the board already provide counter balance to the SC? Or is it the other way around?</p>


            </div>
          <div itemprop="comment" id="post_14" itemscope="" itemtype="http://schema.org/Comment">
              <p>I agree with <a href="https://discourse.nixos.org/u/dragon_logic">@dragon_logic</a>. We already have a process, nobody is using it but many people ask for it being changed. There is an upcoming election where majority of seats is already up for election so the next SC can make thing completely different.</p>
<p>I would like to also highlight that this is the first year of the existence of the SC so maybe we need to wait a bit more time to see if the governance model really works out or not. Currently the lack of established processes probably makes it hard to start. Maybe we should lower our expectations that all problems can be solved within a year.</p>
            </div>
          <div id="post_15" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.nixos.org/u/Irene"><span itemprop="name">Irene</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-09-27T17:17:29Z">
                    September 27, 2025,  5:17pm
                  </time>
                  <meta itemprop="dateModified" content="2025-09-27T17:17:29Z">
              <span itemprop="position">15</span>
              </span>
            </p>
            <p>I’m sad that it turned out this way, though I always expected it to. Thank you for your courage in speaking out publicly, and in resigning.</p>

            

          </div>
          <div itemprop="comment" id="post_16" itemscope="" itemtype="http://schema.org/Comment">
              
<p>I think you’ve failed to understand the graveness of your mistake. Since the outset of the SC, I’ve hoped that it would work to <em>form new governance structures</em>, not form a grip on decisions made community-wide. The recent nixpkgs-core team announcement is an example of what the SC <em>should</em> be doing: delegation. The SC’s job this year was to set up rules of governance such that teams of experts could form and carry out their intended purpose reasonably autonomously. Both Robert and the resigning moderators describe the SC’s approach to moderation as very different from delegation.</p>
<p>I don’t expect the SC to be experts on many of the subjects that they govern. Robert explained that he had a different opinion of how to approach moderation than the moderation team, but frankly Robert, I don’t think you were elected to ensure every moderation action met your approval. I think your job this year was to empower the mod team, yield to their expertise, and help identify the <em>rules</em> that need changing, not the <em>decisions</em> that need changing. If there was a problem with the mod team’s rules of engagement, that could have been a broader policy conversation, rather than having the SC assume direct control of the mod team’s actions. The constitution does give the SC power to control the membership of the teams it manages, but IMO this should be seen as an extraordinary measure for use when there is zero confidence in the team.</p>
<p>The consequences of overreach here are extremely steep. <strong>When you consistently bypass delegation, you will be left with no delegates.</strong> The teams the SC manages have to be able to trust the SC. When the SC erodes that trust, the teams will vanish. This matters much more than having the teams act the way the SC wishes they would. You guys really need to realize that a mass resignation of a team is an objective and dramatic failure of the SC. It doesn’t matter that you disagreed with them; it matters that you chose the wrong methods to address your disagreement.</p>
<hr>
<p>I was approached last night by a member of the SC asking me if I wanted to be a moderator. In light of this mass resignation, I now find that disturbing.</p>
            </div>
          <div id="post_17" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.nixos.org/u/jade"><span itemprop="name">jade</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-09-27T17:38:51Z">
                    September 27, 2025,  5:38pm
                  </time>
                  <meta itemprop="dateModified" content="2025-09-27T17:38:51Z">
              <span itemprop="position">17</span>
              </span>
            </p>
            <div itemprop="text">
              <p>The rust project has a governance rule that the core team and the moderation team may disband themselves and the other team. The SC either does not understand why this rule exists or thinks themselves to have better judgement than the rust project.</p>
<p>I will defer to the mass resignation of core maintainers of the past two years due to the poor decisions of basically exactly the same people as the SC members who admitted to being the problem above as to whether they actually have better judgement than the rust project.</p>
            </div>

            

          </div>
          <div id="post_18" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.nixos.org/u/lassulus"><span itemprop="name">lassulus</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-09-27T17:42:22Z">
                    September 27, 2025,  5:42pm
                  </time>
                  <meta itemprop="dateModified" content="2025-09-27T18:31:54Z">
              <span itemprop="position">18</span>
              </span>
            </p>
            <div itemprop="text">
              <p>I’m a bit busy today (c-base is celebrating it’s 30th birthday <img src="https://discourse.nixos.org/images/emoji/twitter/tada.png?v=12" title=":tada:" alt=":tada:" loading="lazy" width="20" height="20">). I will stay a moderator until at least post election. I feel responsible for this as the longest serving moderator and part of the NCA and the Board. Please keep it civil so I don’t have a bazillion flags to resolve tomorrow <img src="https://discourse.nixos.org/images/emoji/twitter/slight_smile.png?v=12" title=":slight_smile:" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>EDIT: this means a more thorough statement will be made in the coming days</p>
            </div>

            

          </div>
          <div itemprop="comment" id="post_19" itemscope="" itemtype="http://schema.org/Comment">
              <p>Well, I’m already ending my term early and I’ve <a href="https://www.haskellforall.com/2025/09/steering-committee-retrospective.html">also publicly commented on my desire for Constitutional reforms</a> so I have no conflict of interest in that regard, but I’ll comment on the moderation team’s request for constitutional reform.  This is also me speaking in an unofficial capacity and not officially representing the Steering Committee.</p>
<p>My general disposition is: if the moderation team wants to reform the Constitution to put themselves on an equal footing to the Steering Committee team (a “counterbalance” as you put it) then they also need to reform the Constitution to make their positions elected positions instead of “self-appointed” positions (because currently only moderation team members can appoint new members).</p>
<p>More generally, it’s not clear from the resignation letter what the moderation team envisioned as the external checks or accountability on their team.  Currently, because the moderation team is unelected and self-appointed, the <em>only</em> check on the moderation team is the Steering Committee’s constitutional authority to create and manage teams.</p>
<p>However, two of the grievances of the resignation letter are related to the Steering Committee proposing to add or remove members from the team, so if the moderation team is not okay with that then they need to propose a different accountability mechanism.</p>
            </div>
          <div id="post_20" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.nixos.org/u/piegames"><span itemprop="name">piegames</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-09-27T19:20:27Z">
                    September 27, 2025,  7:20pm
                  </time>
                  <meta itemprop="dateModified" content="2025-09-27T19:20:27Z">
              <span itemprop="position">20</span>
              </span>
            </p>
            <div itemprop="text">
              

<p>qed.</p>

<p>The word “proposing” is doing a lot of work here methinks.</p>
<hr>
<p>If the Steering Committee wants to have a say in how moderation should be done, it needs to match the moderators’ skill in navigating a community this large. From what I’ve seen so far, this has very much not been the case. Especially the desire for more “friction” in the moderation team (I’d like to hear the SC’s perspective on that wording) is some pretty bad optics to me, to the point of blurring the line between serious incompetence and questionable intentions.</p>
            </div>

            

          </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[iPhone 17 chip becomes the fastest single-core CPU in the world on PassMark (123 pts)]]></title>
            <link>https://www.tomshardware.com/pc-components/cpus/apples-a19-becomes-the-fastest-single-core-cpu-in-the-world-on-passmark-beating-pc-chips-and-apples-own-m3-ultra-passively-cooled-iphone-17-chip-catapults-past-power-hungry-competitors</link>
            <guid>45398802</guid>
            <pubDate>Sat, 27 Sep 2025 19:48:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/pc-components/cpus/apples-a19-becomes-the-fastest-single-core-cpu-in-the-world-on-passmark-beating-pc-chips-and-apples-own-m3-ultra-passively-cooled-iphone-17-chip-catapults-past-power-hungry-competitors">https://www.tomshardware.com/pc-components/cpus/apples-a19-becomes-the-fastest-single-core-cpu-in-the-world-on-passmark-beating-pc-chips-and-apples-own-m3-ultra-passively-cooled-iphone-17-chip-catapults-past-power-hungry-competitors</a>, See on <a href="https://news.ycombinator.com/item?id=45398802">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-1451-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH.jpg" alt="Apple A19" srcset="https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-1451-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<figcaption>
<span>(Image credit: Apple)</span>
</figcaption>
</div>

<div id="article-body">
<p id="9598606f-6e07-4467-9c5e-8574022330ed">Apple's latest generation of iPhones is equipped with its A19 chips — the standard A19 on iPhone 17 and the A19 Pro on iPhone 17 Air and Pros — which represent the best the company has to offer, <em>literally</em>. In PassMark's single-threaded benchmark, the A19 produced <a data-analytics-id="inline-link" href="https://x.com/PassMarkInc/status/1971365505710817321" data-url="https://x.com/PassMarkInc/status/1971365505710817321" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">the best numbers of any chip</a> available, including fully-fledged desktop SKUs. It did that while consuming significantly less power and being passively cooled. At least in this hyper-specific case, Apple's A19 has become the fastest CPU available.</p><p>Both the A19 and A19 Pro benchmarked within the margin of error of each other; however, officially, it was the regular A19 that posted 5,149 points to claim the single-thread performance crown. The A19 Pro scored 5,088 points, which makes sense considering both chips share the same cores, just differing amounts of them. The A19 beats heavy hitters like Apple's own desktop-class <a data-analytics-id="inline-link" href="https://www.tomshardware.com/desktops/apple-debuts-m3-ultra-in-refreshed-mac-studio-with-up-to-512gb-memory" data-before-rewrite-localise="https://www.tomshardware.com/desktops/apple-debuts-m3-ultra-in-refreshed-mac-studio-with-up-to-512gb-memory">M3 Ultra</a> (both 28- and 32-core variants), <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/amd-ryzen-9-9950x-vs-intel-core-ultra-9-285k-faceoff-it-isnt-even-close" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/cpus/amd-ryzen-9-9950x-vs-intel-core-ultra-9-285k-faceoff-it-isnt-even-close">Intel's Core Ultra 9 285K</a>, and even the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/new-zen-5-128-core-epyc-cpu-weilds-512mb-of-l3-cache" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/cpus/new-zen-5-128-core-epyc-cpu-weilds-512mb-of-l3-cache">EPYC 4585PX</a> from AMD — all of which would be actively cooled.</p><div id="1971365505710817321"><blockquote data-lang="en"><p lang="en" dir="ltr">This is a pretty incredible single threaded benchmark result from Apple with the A19. Plus it is claimed to use only 12watts. For comparison the Ultra 9 is 125W+ and EPYC 4585PX is 170W+https://t.co/ysO73jpaVv pic.twitter.com/e9niPV5I3y<a href="https://twitter.com/cantworkitout/status/1971365505710817321" data-url="https://twitter.com/cantworkitout/status/1971365505710817321" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">September 26, 2025</a></p></blockquote></div><p id="727fbaf0-c5aa-42b1-b355-91c2dc1f4bca-0">The tweet caption lists nominal TDPs of these chips for comparison, but that's not what a single-core load would actually use. Since it's incredibly difficult to pinpoint that, PassMark itself estimated the single-threaded power consumption <a data-analytics-id="inline-link" href="https://x.com/PassMarkInc/status/1971730057862566329" data-url="https://x.com/PassMarkInc/status/1971730057862566329" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">in a reply</a>, saying the A19 is likely using 4W, the 285K is using 44W, and the EPYC is using 56W. Even if those 1/3 assumptions are wrong, the delta is so high between the three that it doesn't really matter. The A19 is miles ahead in terms of efficiency.</p><p>Where it falters, of course, is multi-threaded performance. It doesn't scale upward when you take more/all cores into account, but that's to be expected with a mobile-only chip, given that it simply has fewer cores than every other CPU on the list. Moreover, keep in mind that the A19 is inside the iPhone 17, which doesn't have a vapor chamber, so it's even more impressive for it to pull these kinds of numbers. Then again, this isn't precisely an uber-scientific test, so don't take these results at face value.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank" data-url="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank" data-url="https://google.com/preferences/source?q=" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-oES5rGgX64e8ph3uKSdwEg"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div>
</div>



<!-- Drop in a standard article here maybe? -->




<div id="slice-container-authorBio-oES5rGgX64e8ph3uKSdwEg"><p>Hassam Nasir is a die-hard hardware enthusiast with years of experience as a tech editor and writer, focusing on detailed CPU comparisons and general hardware news. When he’s not working, you’ll find him bending tubes for his ever-evolving custom water-loop gaming rig or benchmarking the latest CPUs and GPUs just for fun.  </p></div>
</section>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[2025 Nikon Small World in Motion Competition Winners (125 pts)]]></title>
            <link>https://www.nikonsmallworld.com/galleries/2025-small-world-in-motion-competition</link>
            <guid>45398731</guid>
            <pubDate>Sat, 27 Sep 2025 19:38:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nikonsmallworld.com/galleries/2025-small-world-in-motion-competition">https://www.nikonsmallworld.com/galleries/2025-small-world-in-motion-competition</a>, See on <a href="https://news.ycombinator.com/item?id=45398731">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
  <section id="galleryapp" v-touch:swipe.left="pageNext" v-touch:swipe.right="pagePrev">

            <section id="swim-winner_gallery">
    <h2>Winning Videos</h2>
        
        <hr>
    
        
              </section>
              <section id="swim-honorable-mention_gallery">
    <h2>Honorable Mentions</h2>
        
        <hr>
        
    
            
          </section>
      <section id="judges">
    <h2>Judges</h2>
  <div>
                      <div>
          <h3>Dr. Deboki Chakravarti</h3>
          <p><em>Science Communicator</em><br></p>
                    <p><img src="https://www.nikonsmallworld.com/images/bio-photos/_squareThumb2x/Deboki-Chakravarti.jpg" width="120"></p><p>Deboki Chakravarti, PhD is a science writer based out of western Massachusetts who focuses on creating educational science videos and podcasts, including <em>Journey to the Microcosmos</em>, <em>Tiny Matters</em>, <em>Scishow Tangents</em>, and <em>Crash Course Organic Chemistry</em>. From designing better bike seats to existential crises inspired by amoebas, Chakravarti’s work covers a wide range of subjects, all of which are tied together by her fascination with how science interacts with the culture around it. Chakravarti received her PhD in biomedical engineering from Boston University, where she worked on engineering T cells for cancer immunotherapy. Prior to that, she earned her bachelor’s degree in bioengineering and English from The California Institute of Technology.</p>
        </div>
                      <div>
          <h3>Jeff DelViscio</h3>
          <p><em>Chief Multimedia Editor and Executive Producer at Scientific American</em><br></p>
                    <p><img src="https://www.nikonsmallworld.com/images/bio-photos/_squareThumb2x/Jeff-DelViscio.jpg" width="120"></p><p>Jeff DelViscio is the chief multimedia editor/executive producer at <em>Scientific American</em>. He is the former director of multimedia at STAT, where he oversaw all visual, audio, and interactive journalism. Before that, he spent more than eight years at <em>The New York Time</em><em>s</em>, where he worked on five different desks across the paper. DelViscio holds dual master’s degrees from Columbia University in journalism and in earth and environmental sciences. He has worked aboard oceanographic research vessels and tracked money and politics in science from Washington, D.C. He was a Knight Science Journalism Fellow at MIT in 2018–19. DelViscio’s work has won numerous awards, including two News and Documentary Emmys.</p>
        </div>
                      <div>
          <h3><a href="https://www.nikonsmallworld.com/people/andrew-moore">Dr. Andrew Moore</a></h3>
          <p><em>Postdoctoral Scientist in the Lippincott-Schwartz Lab at the Howard Hughes Medical Institute's Janelia Research Campus</em><br><a href="https://www.nikonsmallworld.com/organizations/howard-hughes-medical-institute-hhmi"></a></p>
                    <p><img src="https://www.nikonsmallworld.com/images/bio-photos/_squareThumb2x/andrew-moore.jpg" width="120"></p><p>Andrew Moore, PhD is a postdoctoral scientist in the Lippincott-Schwartz Lab at the Howard Hughes Medical Institute’s Janelia Research Campus who specializes in cell biology with a focus on organelle-cytoskeleton interactions. He completed his graduate training in the Holzbaur Lab at the<a href="https://www.nikonsmallworld.com/organizations/university-of-pennsylvania"> University of Pennsylvania</a>, where he researched mitochondria quality control and dynamics. Currently, Moore’s work centers on understanding how cells organize and position their organelles, particularly exploring the interactions between vimentin intermediate filaments and the endoplasmic reticulum. His research combines advanced light and volume electron microscopy techniques to delve into the complexities of cell structure and function. Moore is no stranger to Nikon Small World; he has <a href="https://www.nikonsmallworld.com/people/andrew-moore">placed six photos and six videos in the competitions</a> since 2018 and he is grateful for the opportunity to experience this year’s competition from the other side of the judges’ table.</p>
        </div>
                      <div>
          <h3>Dr. Liz Roth-Johnson</h3>
          <p><em>Curator of Life Sciences at the California Science Center</em><br></p>
                    <p><img src="https://www.nikonsmallworld.com/images/bio-photos/_squareThumb2x/liz-johnson.jpg" width="120"></p><p>Liz Roth-Johnson, PhD is a scientist turned science communicator with more than a decade of experience making complex scientific ideas accessible and compelling to broad audiences. At the California Science Center, Roth-Johnson oversees the development of fun, memorable exhibit experiences that spark curiosity and inspire science learning in all ages and backgrounds. Recent projects include a Nikon Small World exhibit that explores some of the light microscopy tools and techniques scientists use to study life. Prior to her tenure at the California Science Center, Roth-Johnson created popular online food science content, reported science stories for <em>KQED Science</em>, consulted for the Autry Museum of the American West, and designed introductory biology courses for undergraduate students at UCLA. Roth-Johnson earned her PhD in molecular biology from <a href="https://www.nikonsmallworld.com/organizations/university-of-california-los-angeles">UCLA</a> and received her BA degree from <a href="https://www.nikonsmallworld.com/organizations/university-of-california-berkeley">UC Berkeley</a>, where she majored in molecular &amp; cell biology and music. She completed postdoctoral work as a Discipline-Based Education Research Fellow in the UCLA Department of Life Science Core Education.</p>
        </div>
                      <div>
          <h3>Dr. W. Gregory Sawyer</h3>
          <p><em>Chief BioEngineering Officer and Chair of the Department of BioEngineering at the Moffitt Cancer Center</em><br><a href="https://www.nikonsmallworld.com/organizations/h.-lee-moffitt-cancer-center"></a></p>
                    <p><img src="https://www.nikonsmallworld.com/images/bio-photos/_squareThumb2x/greg-sawyer.jpg" width="120"></p><p>W. Gregory Sawyer, PhD is chief bioengineering officer and chair of the Department of BioEngineering at the Moffitt Cancer Center in Tampa, Florida. Professor Sawyer has published over 200 journal papers, has over 16,000 citations, holds over 40 patents, and is most proud of his numerous PhD students who are now faculty members and scientists across the globe. He was a member of the original Mars Rover Program (NASA-JPL), a speaker at TED 8, led the first remotely space-tribology experiments on the International Space Station (ISS), developed novel biomaterials for the ocular surface, and is currently leading efforts in Cancer Engineering.</p>
        </div>
          </div>
    
      </section>
  </section>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NSPM-7 labels common beliefs as terrorism 'indicators' (116 pts)]]></title>
            <link>https://www.kenklippenstein.com/p/trumps-nspm-7-labels-common-beliefs</link>
            <guid>45398719</guid>
            <pubDate>Sat, 27 Sep 2025 19:35:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.kenklippenstein.com/p/trumps-nspm-7-labels-common-beliefs">https://www.kenklippenstein.com/p/trumps-nspm-7-labels-common-beliefs</a>, See on <a href="https://news.ycombinator.com/item?id=45398719">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!fMp7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91129dba-8e6b-4ca7-a4ac-83d34e6abfee_1200x800.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!fMp7!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91129dba-8e6b-4ca7-a4ac-83d34e6abfee_1200x800.webp 424w, https://substackcdn.com/image/fetch/$s_!fMp7!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91129dba-8e6b-4ca7-a4ac-83d34e6abfee_1200x800.webp 848w, https://substackcdn.com/image/fetch/$s_!fMp7!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91129dba-8e6b-4ca7-a4ac-83d34e6abfee_1200x800.webp 1272w, https://substackcdn.com/image/fetch/$s_!fMp7!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91129dba-8e6b-4ca7-a4ac-83d34e6abfee_1200x800.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!fMp7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91129dba-8e6b-4ca7-a4ac-83d34e6abfee_1200x800.webp" width="1200" height="800" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/91129dba-8e6b-4ca7-a4ac-83d34e6abfee_1200x800.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:800,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:67314,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://www.kenklippenstein.com/i/174704382?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91129dba-8e6b-4ca7-a4ac-83d34e6abfee_1200x800.webp&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!fMp7!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91129dba-8e6b-4ca7-a4ac-83d34e6abfee_1200x800.webp 424w, https://substackcdn.com/image/fetch/$s_!fMp7!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91129dba-8e6b-4ca7-a4ac-83d34e6abfee_1200x800.webp 848w, https://substackcdn.com/image/fetch/$s_!fMp7!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91129dba-8e6b-4ca7-a4ac-83d34e6abfee_1200x800.webp 1272w, https://substackcdn.com/image/fetch/$s_!fMp7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91129dba-8e6b-4ca7-a4ac-83d34e6abfee_1200x800.webp 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Trump displays NSPM-7 at the Oval Office on Thursday</figcaption></figure></div><p><span>With the mainstream media distracted by the made-for-TV drama of James Comey’s indictment, Trump has signed a little-noticed </span><a href="https://www.whitehouse.gov/presidential-actions/2025/09/countering-domestic-terrorism-and-organized-political-violence/" rel="">national security directive</a><span> identifying “anti-Christian” and “anti-American” views as indicators of radical left violence. Called National Security Presidential Memorandum 7, it’s being referred to as “NSPM-7” by administration insiders.</span></p><p>“This is the first time in American history that there is an all-of-government effort to dismantle left wing terrorism,” Trump’s homeland security advisor Stephen Miller said, referring to the issuance.</p><p><span>To the extent that the major media noticed the directive at all, they (even C-SPAN!) incorrectly labeled it an “executive order,” like this week’s </span><a href="https://www.kenklippenstein.com/p/breaking-trump-declares-war-on-left" rel="">designation</a><span> of “Antifa” as a domestic terrorist organization.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!mkJ5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3af4d84-03ee-40bb-a36f-62363ef39cd4_1220x988.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!mkJ5!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3af4d84-03ee-40bb-a36f-62363ef39cd4_1220x988.png 424w, https://substackcdn.com/image/fetch/$s_!mkJ5!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3af4d84-03ee-40bb-a36f-62363ef39cd4_1220x988.png 848w, https://substackcdn.com/image/fetch/$s_!mkJ5!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3af4d84-03ee-40bb-a36f-62363ef39cd4_1220x988.png 1272w, https://substackcdn.com/image/fetch/$s_!mkJ5!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3af4d84-03ee-40bb-a36f-62363ef39cd4_1220x988.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!mkJ5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3af4d84-03ee-40bb-a36f-62363ef39cd4_1220x988.png" width="1220" height="988" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f3af4d84-03ee-40bb-a36f-62363ef39cd4_1220x988.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:988,&quot;width&quot;:1220,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1379540,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.kenklippenstein.com/i/174704382?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3af4d84-03ee-40bb-a36f-62363ef39cd4_1220x988.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!mkJ5!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3af4d84-03ee-40bb-a36f-62363ef39cd4_1220x988.png 424w, https://substackcdn.com/image/fetch/$s_!mkJ5!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3af4d84-03ee-40bb-a36f-62363ef39cd4_1220x988.png 848w, https://substackcdn.com/image/fetch/$s_!mkJ5!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3af4d84-03ee-40bb-a36f-62363ef39cd4_1220x988.png 1272w, https://substackcdn.com/image/fetch/$s_!mkJ5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3af4d84-03ee-40bb-a36f-62363ef39cd4_1220x988.png 1456w" sizes="100vw"></picture></div></a><figcaption>Come on, C-SPAN</figcaption></figure></div><p>It’s hard to overstate how much different NSPM-7 is from the over 200 executive orders Trump has frantically signed since coming back into office.</p><p>An executive order publicly lays out the course of day-to-day federal government operations; whereas a national security directive is a sweeping policy decree for the defense, foreign policy, intelligence, and law enforcement apparatus. National security directives are often secret, but in this case the Trump administration chose to publish NSPM-7 — only the seventh since he’s come into office.)</p><p><span>Previous national security directives have been controversial, even politically earthshaking. In 1980, for example, President Jimmy Carter signed the Top Secret </span><a href="https://nsarchive2.gwu.edu/nukevault/ebb390/docs/7-25-80%20PD%2059.pdf?utm_source=chatgpt.com" rel="">Presidential Directive 59</a><span> (“PD-59”) directing new nuclear warfighting policies that persisted until the end of the Cold War. When revealed, PD-59 caused a public furor.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!THR3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F239e355c-5037-47ed-bfcb-31c582f4e1f0_1872x1304.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!THR3!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F239e355c-5037-47ed-bfcb-31c582f4e1f0_1872x1304.png 424w, https://substackcdn.com/image/fetch/$s_!THR3!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F239e355c-5037-47ed-bfcb-31c582f4e1f0_1872x1304.png 848w, https://substackcdn.com/image/fetch/$s_!THR3!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F239e355c-5037-47ed-bfcb-31c582f4e1f0_1872x1304.png 1272w, https://substackcdn.com/image/fetch/$s_!THR3!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F239e355c-5037-47ed-bfcb-31c582f4e1f0_1872x1304.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!THR3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F239e355c-5037-47ed-bfcb-31c582f4e1f0_1872x1304.png" width="1456" height="1014" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/239e355c-5037-47ed-bfcb-31c582f4e1f0_1872x1304.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1014,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:442736,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.kenklippenstein.com/i/174704382?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F239e355c-5037-47ed-bfcb-31c582f4e1f0_1872x1304.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!THR3!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F239e355c-5037-47ed-bfcb-31c582f4e1f0_1872x1304.png 424w, https://substackcdn.com/image/fetch/$s_!THR3!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F239e355c-5037-47ed-bfcb-31c582f4e1f0_1872x1304.png 848w, https://substackcdn.com/image/fetch/$s_!THR3!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F239e355c-5037-47ed-bfcb-31c582f4e1f0_1872x1304.png 1272w, https://substackcdn.com/image/fetch/$s_!THR3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F239e355c-5037-47ed-bfcb-31c582f4e1f0_1872x1304.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Declassified copy of PD-59 | Carter Library</figcaption></figure></div><p>Similarly, President George W. Bush signed a series of classified national security directives after 9/11, the most famous of which authorized NSA’s unlawful domestic intercepts, a directive that wasn’t publicly revealed until four years later.</p><p><span>In NSPM-7, “Countering Domestic Terrorism and Organized Political Violence,” President Trump </span><em>directs</em><span> the Justice Department, the FBI, and other national security agencies and departments to fight his version of political violence in America, retooling a network of Joint Terrorism Task Forces to focus on “leftist” political violence in America. This vast counterterrorism army, made up of federal, state, and local agents would, as Trump aide Stephen Miller said, form “the central hub of that effort.”</span></p><p>NSPM-7 directs a new national strategy to “disrupt” any individual or groups “that foment political violence,” including “before they result in violent political acts.” </p><p><span>In other words, they’re targeting pre-crime, to reference </span><em>Minority Report</em><span>.</span></p><p>The Trump administration isn’t only targeting organizations or groups but even  individuals and “entities” whom NSPM-7 says can be identified by any of the following “indica” (indicators) of violence:</p><ul><li><p>anti-Americanism,</p></li><li><p>anti-capitalism,</p></li><li><p>anti-Christianity,</p></li><li><p>support for the overthrow of the United States Government,</p></li><li><p>extremism on migration,</p></li><li><p>extremism on race,</p></li><li><p>extremism on gender</p></li><li><p>hostility towards those who hold traditional American views on family,</p></li><li><p>hostility towards those who hold traditional American views on religion, and</p></li><li><p>hostility towards those who hold traditional American views on morality.</p></li></ul><p><span>“The United States requires a national strategy to investigate and disrupt networks, entities, and organizations that foment political violence so that law enforcement can intervene in criminal conspiracies </span><em>before they result in violent political acts</em><span>,” the directive states (emphasis mine).</span></p><p>A “pre-crime” endeavor, preventing attacks before they happen, is core to the post-9/11 concept of counterterrorism itself. No longer satisfied to investigate acts of terrorism after the fact to bring terrorists to justice, the Bush administration adopted preemption. Overseas, that led to aerial assassination by drones and “special operations” kill missions. Domestically, it led to a counter-terrorism campaign whose hallmark was excessive and illegal government surveillance and the use of undercover agents and “confidential human sources” to trap (and entrap) would-be terrorists.</p><p>Now, with Donald Trump’s directive retooling the counter-terror apparatus to go after Americans at home, this means monitoring political activity, or speech, as an investigative method to discover “radicalism.” (Contrary to other national security documents all during the post-Watergate era, NSPM-7 doesn’t even mention the First Amendment or the fundamental right of Americans to organize and protest.)</p><p>The focus on speech is evident throughout NSPM-7. The directive says that political violence is the result of “organized campaigns” that often begin (with the left) dehumanizing targets in “anonymous chat foras, in-person meetings, social media, and even educational institutions.”</p><p><span>To give a sense of how broad this formulation is, Trump’s earlier designation of Antifa as a domestic terrorist group was accompanied by a White House fact sheet singling out people who “celebrated” Luigi Mangione, the alleged killer of UnitedHealthcare CEO Brian Thompson last December. As </span><a href="https://www.kenklippenstein.com/p/breaking-trump-declares-war-on-left" rel="">I wrote at the time</a><span>, this describes a lot of Americans!</span></p><p>Trump’s new national security memorandum also alludes to Mangione but adds to it even larger categories of potential targets.</p><p><span>NSPM-7 is fundamentally a law enforcement directive, and it dispenses with the complications of using the active duty military or the National Guard in pursuit of political violence. It directs the Department of Justice to focus the FBI’s </span><a href="https://www.fbi.gov/investigate/terrorism/joint-terrorism-task-forces" rel="">approximately 200</a><span> Joint Terrorism Task Forces (JTTFs) to the new mission. The FBI network of task forces comprises over 4,000 members—including FBI personnel and task force officers (or TFOs) from more than 500 state and local agencies and 50 federal agencies, including special agents, police officers, intelligence analysts and surveillance technicians. First established in New York City in 1980 to systematize FBI and NYPD cooperation, today there are task forces around the country, including at least one in each of the FBI’s 55 field offices.</span></p><p>For the Trump White House, the beauty of using an already existing network is that it bypasses Congressional oversight and scrutiny and even obscures federal activity to governors and legislatures at the state level. States, cities, and local police have already signed Memoranda of Agreements with the feds to fight terrorism and officers are already assigned as task force officers.</p><p>NSPM-7 says the JTTFs “shall investigate” potential federal crimes relating to “acts of recruiting or radicalizing persons” for the purpose of “political violence, terrorism, or conspiracy against rights; and the violent deprivation of any citizen’s rights.” It authorizes the JTTFs to investigate individuals, organizations, and funders “responsible for, sponsor, or otherwise aid and abet the principal actors engaging in the criminal conduct.”</p><p>“The Attorney General shall issue specific guidance that ensures domestic terrorism priorities include politically motivated terrorist acts such as organized doxing campaigns, swatting, rioting, looting, trespass, assault, destruction of property, threats of violence, and civil disorder,” NSPM-7 says. Civil disorder?</p><p><span>I don’t want to sound hyperbolic but the plain truth is that NSPM-7 is a declaration of war on anyone who does not support the Trump administration and its agenda. Yes, it repeats the word “violent” over and over to purport only to go after citizens who are moved to take up arms, but it also directs monitoring and intelligence collection to map and target the new “evildoers,” to borrow a Bush label </span><a href="https://georgewbush-whitehouse.archives.gov/news/releases/2001/09/20010916-2.html" rel="">he took</a><span> from the Bible just days after 9/11. </span></p><p>The partisan focus couldn’t be more obvious.</p><p>“The real problem is this: since Charlie [Kirk] was murdered — a friend of mine, assassinated — nothing’s changed on their side,” White House counter-terrorism czar Sebastian Gorka told Newsmax after NSPM-7 was signed. “Not one leader —not one left wing thought leader, member of Congress, Senator — nobody has said we distance ourselves from the violent rhetoric.” </p><p>“The left refuses to rid themselves of the justification for violence,” Gorka continued, “and as such, President Trump is taking measures to protect us from the violent rhetoric that becomes snipers and bullets.”</p><p data-attrs="{&quot;url&quot;:&quot;https://www.kenklippenstein.com/p/trumps-nspm-7-labels-common-beliefs/comments&quot;,&quot;text&quot;:&quot;Leave a comment&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.kenklippenstein.com/p/trumps-nspm-7-labels-common-beliefs/comments" rel=""><span>Leave a comment</span></a></p><p data-attrs="{&quot;url&quot;:&quot;https://www.kenklippenstein.com/p/trumps-nspm-7-labels-common-beliefs?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.kenklippenstein.com/p/trumps-nspm-7-labels-common-beliefs?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p><span>— </span><em>Edited by William M. Arkin</em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Docker Was Too Slow, So We Replaced It: Nix in Production [video] (108 pts)]]></title>
            <link>https://www.youtube.com/watch?v=iPoL03tFBtU</link>
            <guid>45398468</guid>
            <pubDate>Sat, 27 Sep 2025 18:56:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=iPoL03tFBtU">https://www.youtube.com/watch?v=iPoL03tFBtU</a>, See on <a href="https://news.ycombinator.com/item?id=45398468">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[LLM Observability in the Wild – Why OpenTelemetry Should Be the Standard (128 pts)]]></title>
            <link>https://signoz.io/blog/llm-observability-opentelemetry/</link>
            <guid>45398467</guid>
            <pubDate>Sat, 27 Sep 2025 18:56:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://signoz.io/blog/llm-observability-opentelemetry/">https://signoz.io/blog/llm-observability-opentelemetry/</a>, See on <a href="https://news.ycombinator.com/item?id=45398467">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>A few days ago I hosted a live conversation with Pranav, co-founder of Chatwoot, about issues his team was running into with LLM observability.</p><p>The short version: building, debugging, and improving AI agents in production gets messy fast. There's multiple competing standards for default libraries for LLM observability. And many such libraries like OpenInference which claim to be based on OpenTelemetry don't strictly adhere to it's conventions. This introduces problems for users who are trying to get better observability across their stack.</p><p>Here’s a write-up of what we covered and what I think it means for anyone shipping LLM features into real products. Feel free to watch the complete video</p><p><iframe src="https://www.youtube.com/embed/DPL35sYPGPU" title="YouTube Video Player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"></iframe></p><h2 id="the-problem-emerges-in-prod">The Problem Emerges in Prod</h2><p>Pranav and I go way back to our YC days in 2021, and it's always interesting to see how our paths have evolved. Chatwoot has built something really compelling - an open-source customer support platform that unifies conversations across every channel you can imagine: live chat, email, WhatsApp, social media, you name it. All in a single dashboard.</p><p>But here's where it gets interesting. They've built an AI agent called "Captain" that can work across all these channels. You build the logic once, and it can handle support queries whether they come through email, live chat, or WhatsApp. Pretty neat, right?</p><p>The problem started showing up in production in the most unexpected ways. Sometimes their AI would randomly respond in Spanish when it absolutely shouldn't. Other times, responses just weren't quite right, and they had no visibility into <em>why</em>.</p><h2 id="the-quest-for-llm-observability">The Quest for LLM Observability</h2><p>This is where Pranav's journey into LLM observability began, it mirrors what I've been seeing across many companies building LLM applications. You need to understand:</p><ul><li>What documents were retrieved for a RAG query?</li><li>Which tool calls were made?</li><li>What was the exact input and output at each step?</li><li>Why did the AI make certain decisions?</li></ul><p>Without this visibility, you're essentially flying blind in production.</p><h2 id="the-standards-problem">The Standards Problem</h2><p>Here's where things get really interesting, and frankly, frustrating. Pranav explored several solutions:</p><p><strong>OpenAI's native tracing</strong> looked promising with rich, detailed traces showing guardrails, agent flows, and tool calls. But it's tightly coupled to OpenAI's agent framework. Also, it only provides traces as an atomic unit. If you want to filter spans based on attributes or just examine specific spans directly, you can't do that.</p><div data-rmiz-content="not-found" aria-owns="rmiz-modal-" data-rmiz=""><figure><img src="https://signoz.io/img/blog/2025/09/openAI-agent-traces.webp" alt="OpenAI agent workflow traces"><figcaption><i>OpenAI agent workflow traces<!-- --> </i></figcaption></figure></div><p><strong>New Relic</strong> was easy to integrate since they already use it, and it supports OpenTelemetry. But the UI required clicking through 5-6 layers just to see relevant information. Not ideal when you're trying to debug production issues.</p><p><strong>Phoenix</strong> caught their attention because it follows the OpenInference standard, which provides much richer, AI-specific span types. You can easily filter for just LLM calls, tool calls, or agent spans. The traces are beautiful and informative.</p><div data-rmiz-content="not-found" aria-owns="rmiz-modal-" data-rmiz=""><figure><img src="https://signoz.io/img/blog/2025/09/phoenix-unknown.webp" alt="Phoenix doesn't recognize OpenTelemetry span kinds"><figcaption><i>Phoenix doesn't recognize OpenTelemetry span kinds<!-- --> </i></figcaption></figure></div><p>But here's the kicker: Chatwoot is primarily a Ruby on Rails shop, and guess what? No Ruby SDK for OpenInference. Moreover, Phoenix doesn't completely adhere to OTel semantic conventions, so if you send it telemetry data directly via OpenTelemetry, it doesn't recognize the type of spans, etc.</p><p>As shown in the example above, Phoenix doesn't shows data sent with OpenTelemetry span kinds as <code>unknown</code>.</p><h2 id="the-opentelemetry-vs-openinference-divide">The OpenTelemetry vs OpenInference Divide</h2><p>This is where the conversation got really technical and revealed a fundamental industry problem. There are essentially two standards emerging:</p><p><strong>OpenTelemetry</strong> is the industry standard. It has libraries for every language, it's production-ready, and it's widely adopted. But it was built for traditional applications, not AI workflows. It only supports basic span types: internal, server, client, producer, consumer. That's it.</p><p><strong>OpenInference</strong> was created specifically for AI applications. It has rich span types like LLM, tool, chain, embedding, agent, etc. You can easily query for "show me all the LLM calls" or "what were all the tool executions." But it's newer, has limited language support, and isn't as widely adopted.</p><p>The tragic part? OpenInference claims to be "OpenTelemetry compatible," but as Pranav discovered, that compatibility is shallow. You can send OpenTelemetry format data to Phoenix, but it doesn't recognize the AI-specific semantics and just shows everything as "unknown" spans.</p><h2 id="the-ruby-problem-makes-it-worse">The Ruby Problem Makes It Worse</h2><p>For teams using languages like Ruby that don't have direct OpenInference SDK support, this becomes even more challenging. Pranav had to choose between:</p><ol><li>Building an SDK from scratch for Ruby</li><li>Using OpenTelemetry and losing AI-specific insights</li><li>Switching to a different language stack just for AI observability (way tougher)</li></ol><p>None of these are great options.</p><h2 id="why-we-still-bias-to-opentelemetry">Why we (still) bias to OpenTelemetry</h2><p>At SigNoz we’re all-in on OpenTelemetry. One reason: OTel’s consistency enables out-of-the-box experiences across your <em>whole</em> stack. Example: we can auto-surface <a target="_blank" rel="noopener noreferrer" href="https://signoz.io/docs/external-api-monitoring/overview/">external API</a> usage and performance based on span kinds and attributes. When parts of the app send telemetry via non-OTel conventions, those views degrade.</p><p>Chatwoot lands similarly: their entire product already emits OTel. Pulling in a second telemetry standard just for LLMs fragments the picture and complicates how they go about observability. This also silos their observability into different products which makes it difficult to solves issues when they occur.</p><h2 id="takeaways-for-builders">Takeaways for builders</h2><ul><li><strong>Pick one telemetry backbone</strong> - If most of your app is OTel, prefer staying OTel-native for LLMs too, even if it means adding richer attributes until GenAI conventions catch up.</li><li><strong>LLM specific libraries</strong> - Even if you have to use LLM specific libraries like OpenInference, try to keep your usage as close to OpenTelemetry as possible so that you are aware what non-OTel attributes you are using which may break things.</li><li><strong>Follow OTel GenAI working group</strong> - There is active work happening in OTel <a target="_blank" rel="noopener noreferrer" href="https://opentelemetry.io/blog/2024/otel-generative-ai/">Gen AI working group</a>. Follow the work happening there and do share your use cases so that the standards which OpenTelemetry builds are able to cater to most common use cases.</li></ul><p>As the LLM space is still evolving rapidly, we as a community need to share our voices so that the standards are robust.</p><hr><h2 id="what-were-doing-at-signoz">What we’re doing at SigNoz</h2><p>We’re continuing to invest in OpenTelemetry-native LLM observability so teams don’t have to choose between stability and clarity. Concretely, that means:</p><ul><li><p>Clear dashboards and traces when LLM calls are modeled using OTel spans/attributes. You can find examples and dashboards in our <a target="_blank" rel="noopener noreferrer" href="https://signoz.io/docs/llm-observability/">LLM observability</a> docs. Though we have also use LLM specific libraries like OpenInference in our docs (as they are still the easiest way for ppl to get started), we have kept the dashboards as close to OTel standards as possible. We also plan to actively update this as OTel GenAI semantic conventions become more mature.</p></li><li><p>Guidance and examples for popular frameworks (LangChain, LlamaIndex, etc.) on emitting OTel-friendly telemetry.</p></li><li><p>Build features leveraging OpenTelemetry semantic conventions so that you get great out-of-box experience in SigNoz and adhere to thoughtful defaults that keep your services, DBs, queues, and LLM agents—in one coherent picture.</p></li></ul><p>If you’re wrestling with these trade-offs, we’d love to hear what’s breaking for you and what “rich semantics” you actually use day-to-day.</p><hr><h2 id="what-next">What next?</h2><p>Huge thanks to Pranav for going deep, especially from the Ruby perspective. If you’re shipping AI features and care about operability, add your voice: push for richer GenAI semantics in OpenTelemetry, and share real traces (sanitized) that show what you need to see.</p><p>If you want to compare notes or need help getting your LLM telemetry into an OTel-native view, ping me.</p><div><h3>Was this page helpful?</h3></div></article></div>]]></description>
        </item>
    </channel>
</rss>