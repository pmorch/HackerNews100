<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 05 Sep 2025 17:30:09 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[South Korea: 'many' of its nationals detained in ICE raid on GA Hyundai facility (182 pts)]]></title>
            <link>https://www.nbcnews.com/news/us-news/ice-hyundai-plant-georgia-enforcement-action-rcna229148</link>
            <guid>45139954</guid>
            <pubDate>Fri, 05 Sep 2025 15:51:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nbcnews.com/news/us-news/ice-hyundai-plant-georgia-enforcement-action-rcna229148">https://www.nbcnews.com/news/us-news/ice-hyundai-plant-georgia-enforcement-action-rcna229148</a>, See on <a href="https://news.ycombinator.com/item?id=45139954">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p id="anchor-c00f77"><a href="https://www.nbcnews.com/world/south-korea/south-korea-must-navigate-trump-risk-key-summits-japan-us-rcna226488" target="_blank">South Korea</a> said Friday that it had expressed “concern and regret” to the U.S. Embassy over an immigration raid on a Hyundai facility in <a href="https://www.nbcnews.com/news/us-news/georgia-dentist-accused-killing-wife-teenage-daughter-rcna228746" target="_blank">Georgia</a> during which it said “many” South Korean nationals had been detained.</p><p id="anchor-814048">“The economic activities of our companies investing in the U.S. and the rights and interests of our nationals must not be unfairly violated,” said Lee Jae-woong, a spokesperson for the foreign ministry of the key U.S. ally, according to the <a href="https://en.yna.co.kr/view/AEN20250905010300315?section=national/diplomacy" target="_blank">Yonhap news agency</a>.</p><p id="anchor-6fedd2">Agents from Immigration and Customs Enforcement (ICE) as well as Homeland Security Investigations and other federal agencies were involved in the operation on Thursday, which an ICE spokesperson said was conducted in connection with an investigation into “unlawful employment practices and other serious federal crimes.” </p><p id="anchor-b2e17b">Steven Schrank, special agent in charge of Homeland Security Investigations in Georgia, <a href="https://www.youtube.com/watch?v=1_oM5o-wq_g" target="_blank">told reporters on Thursday afternoon</a> that the alleged unlawful practices were taking place at the “multi-hundred acre” construction site where South Korean companies Hyundai and LG Energy Solution are jointly building a<a href="https://apnews.com/article/hyundai-georgia-lg-electric-vehicle-battery-58bdbe36e34179db41c95dc120851f77"> new battery plant </a>next to their manufacturing facility for electric vehicles.</p><p id="anchor-2115a9">The facility in the town of Ellabell, about 28 miles west of the city of Savannah, employs about 1,400 people. It is considered one of Georgia’s largest and most high-profile manufacturing sites, <a href="https://apnews.com/article/immigration-raid-hyundai-georgia-electric-vehicles-191904f42f540898035feb2a6623d98e" target="_blank">according to The Associated Press</a>.</p><p id="anchor-b0ca2d">NBC News verified a <a href="https://www.facebook.com/reel/764817449617122" target="_blank">video</a> posted on social media showing HSI agents inside the construction site at Hyundai’s facility in Ellabell. One of the agents can be heard telling workers they had a search warrant for the entire site and asked that construction “be ceased immediately.”</p><p id="anchor-662098">A worker who was there but whose name is being withheld told NBC News that agents came late Thursday morning and asked everyone on the premises whether they were U.S. citizens.</p><p id="anchor-bf3f8b">Other videos on social media show agents lining workers up. In some instances, agents can be seen asking workers questions and searching their bags.</p><p id="anchor-992f88">In a statement to NBC News, Hyundai spokesperson Michael Stewart confirmed the presence of law enforcement at the LG Energy Solution and Hyundai battery joint venture construction site in Bryan County, where Ellabell is located.</p><p id="anchor-a6023e">“We are cooperating with law enforcement and are committed to abiding by all labor and immigration regulations,” Stewart said.</p><p id="anchor-a1a37d">It remains unclear how many people have been taken into custody, but Schrank said, “We are making many arrests of undocumented individuals.”</p><p id="anchor-26daf4"><a href="https://www.wsav.com/news/breaking-heavy-federal-agency-presence-at-hyundai-facility/" target="_blank">NBC affiliate WSAV of Savannah</a> reported that hundreds of undercover law enforcement vehicles and Humvees were reportedly seen at the scene. Large buses were also seen entering the site.</p><p id="anchor-929fdc">Mary Beth Kennedy, a spokesperson for HL-GA Battery Co., LG Energy Solution and Hyundai’s joint venture, told WSAV in a statement that the company “is cooperating fully with the appropriate authorities regarding activity at our construction site. To assist their work, we have paused construction. We do not have further details at this time.”</p><p id="anchor-08b973">Schrank added that the investigation was expected to continue beyond Thursday but did not provide a timeline.</p><p id="anchor-ea30ad">The ICE spokesperson added: “This investigation is focused on ensuring accountability for those who violate the law and upholding the rule of law. Complex cases like this require strong collaboration and extensive investigative efforts.”</p><p id="anchor-20260a">South Korea, the world’s 10th-largest economy, is a major automotive and electronics manufacturer whose companies have multiple plants in the United States. In July, Seoul <a href="https://www.nbcnews.com/business/business-news/trump-hits-india-25-tariff-rcna221907" target="_blank">pledged $350 billion in U.S. investment</a> in an effort to lower President <a href="https://www.nbcnews.com/politics/donald-trump/trump-executive-order-rebranding-defense-department-war-department-rcna229217" target="_blank">Donald Trump</a>’s threatened tariffs on its products, which he ended up setting at 15%.</p><p id="anchor-e292fb">In March, Hyundai said it would <a href="https://www.cnbc.com/2025/03/24/south-korea-hyundai-us-investment.html" target="_blank">invest $21 billion in U.S. onshoring</a> from 2025 to 2028, a number it said last month had <a href="https://www.hyundai.com/worldwide/en/newsroom/detail/hyundai-motor-group-increases-u.s.-investment-to-%252426-billion-to-accelerate-growth-and-innovation-0000001003" target="_blank">increased to $26 billion</a>. </p><p id="anchor-507367">It said the initiatives involved in the investment — including a new $5.8 billion steel plant in Louisiana, expanded U.S. auto production capacity and a state-of-the-art robotics facility — were expected to create about 25,000 new direct jobs in the U.S. over the next four years.</p></div><div><div data-activity-map="expanded-byline-article-bottom"><div data-testid="byline-thumbnail"><a href="https://www.nbcnews.com/author/nicole-acevedo-ncpn384476" tabindex="-1"><picture data-testid="picture" data-flavor="focal" data-original-height="48" data-original-width="48"><source media="(min-width: 320px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_avif,q_auto:eco,dpr_2/newscms/2019_29/2934411/190618-nicole-acevedo-byline2043.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2019_29/2934411/190618-nicole-acevedo-byline2043.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2019_29/2934411/190618-nicole-acevedo-byline2043.jpg" alt="" height="48" width="48"></picture></a></div><p><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/nicole-acevedo-ncpn384476">Nicole Acevedo</a></span><span><a href="https://x.com/Nicolemarie_A" target="_blank" rel="noopener noreferrer"><span></span></a><a href="mailto:Nicole.Acevedo@nbcuni.com" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Nicole Acevedo is a national reporter for NBC News and NBC Latino.</p></div><div data-activity-map="expanded-byline-article-bottom"><p><span data-testid="byline-thumbnail"></span><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/laura-strickler-ncpn894696">Laura Strickler</a></span><span><a href="https://x.com/strickdc" target="_blank" rel="noopener noreferrer"><span></span></a><a href="mailto:Laura.Strickler@nbcuni.com" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Laura Strickler is the senior investigative producer on the national security team where she produces television stories and writes for NBCNews.com.</p></div><div data-activity-map="expanded-byline-article-bottom"><p><span data-testid="byline-thumbnail"></span><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/colin-sheeley-ncpn925916">Colin Sheeley</a></span><span></span></p><p>Colin Sheeley is a senior reporter for NBC News' Social Newsgathering team based in New York.</p></div><div data-activity-map="expanded-byline-article-bottom"><p><span data-testid="byline-thumbnail"></span><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/jennifer-jett-ncpn1278442">Jennifer Jett</a></span><span><a href="mailto:jennifer.jett@nbcuni.com" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Jennifer Jett is the Asia Digital Editor for NBC News, based in Hong Kong.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A computer upgrade has shut down BART (127 pts)]]></title>
            <link>https://www.bart.gov/news/articles/2025/news20250905</link>
            <guid>45139270</guid>
            <pubDate>Fri, 05 Sep 2025 14:52:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bart.gov/news/articles/2025/news20250905">https://www.bart.gov/news/articles/2025/news20250905</a>, See on <a href="https://news.ycombinator.com/item?id=45139270">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p><strong>Update 09/05/25, 9:15am:&nbsp;</strong></p><p>Limited East Bay service will start at approximately 9:30am. There is no service to San Francisco.</p><p>Yellow Line will service will resume from Antioch to 12th Street Oakland. Blue Line service will resume from Dublin to MacArthur. Orange line service will resume from Berryessa to Richmond. BART to Antioch service is resuming now.</p><hr><p>A computer equipment problem following network upgrade work is preventing the start of service this morning. Seek alternative means of transportation. <a href="https://planner.bart.gov/?products=364#!P|TP!H|96581">bart.gov/alternatives</a> provides options without BART service.</p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US economy added just 22,000 jobs in August, unemployment highest in 4 yrs (144 pts)]]></title>
            <link>https://www.cnn.com/2025/09/05/economy/us-jobs-report-august-final</link>
            <guid>45138703</guid>
            <pubDate>Fri, 05 Sep 2025 14:01:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnn.com/2025/09/05/economy/us-jobs-report-august-final">https://www.cnn.com/2025/09/05/economy/us-jobs-report-august-final</a>, See on <a href="https://news.ycombinator.com/item?id=45138703">Hacker News</a></p>
Couldn't get https://www.cnn.com/2025/09/05/economy/us-jobs-report-august-final: Error: Request failed with status code 451]]></description>
        </item>
        <item>
            <title><![CDATA[Development Speed Has Never Been a Bottleneck (112 pts)]]></title>
            <link>https://pawelbrodzinski.substack.com/p/development-speed-is-not-a-bottleneck</link>
            <guid>45138156</guid>
            <pubDate>Fri, 05 Sep 2025 13:13:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pawelbrodzinski.substack.com/p/development-speed-is-not-a-bottleneck">https://pawelbrodzinski.substack.com/p/development-speed-is-not-a-bottleneck</a>, See on <a href="https://news.ycombinator.com/item?id=45138156">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!ha53!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cb306df-4a8b-4ec6-bf86-8b87073a29a7_3473x1951.jpeg" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ha53!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cb306df-4a8b-4ec6-bf86-8b87073a29a7_3473x1951.jpeg 424w, https://substackcdn.com/image/fetch/$s_!ha53!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cb306df-4a8b-4ec6-bf86-8b87073a29a7_3473x1951.jpeg 848w, https://substackcdn.com/image/fetch/$s_!ha53!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cb306df-4a8b-4ec6-bf86-8b87073a29a7_3473x1951.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!ha53!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cb306df-4a8b-4ec6-bf86-8b87073a29a7_3473x1951.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!ha53!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cb306df-4a8b-4ec6-bf86-8b87073a29a7_3473x1951.jpeg" width="1456" height="818" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8cb306df-4a8b-4ec6-bf86-8b87073a29a7_3473x1951.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:818,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1237469,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://pawelbrodzinski.substack.com/i/172659340?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cb306df-4a8b-4ec6-bf86-8b87073a29a7_3473x1951.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ha53!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cb306df-4a8b-4ec6-bf86-8b87073a29a7_3473x1951.jpeg 424w, https://substackcdn.com/image/fetch/$s_!ha53!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cb306df-4a8b-4ec6-bf86-8b87073a29a7_3473x1951.jpeg 848w, https://substackcdn.com/image/fetch/$s_!ha53!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cb306df-4a8b-4ec6-bf86-8b87073a29a7_3473x1951.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!ha53!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cb306df-4a8b-4ec6-bf86-8b87073a29a7_3473x1951.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><blockquote><p><em>"You are wrong, Pawel. You can vibe code a successful product without any technical skills. Here's one example."</em></p></blockquote><p>I liked the challenge, especially since it referenced a source. What I thought would be a short comment evolved into a series of articles.</p><ul><li><p><a href="https://pawelbrodzinski.substack.com/p/vibe-coded-product-success-stories" rel="nofollow ugc noopener">Vibe-Coded Product Success Stories Ain't What They Look</a><span> covered the marketing/PR part of the story (and the holes in it).</span></p></li><li><p><a href="https://pawelbrodzinski.substack.com/p/vibe-coding-doesnt-replace-tech-skills" rel="nofollow ugc noopener">Vibe Coding Doesn't Replace Tech Skills; It Requires More of Them</a><span> focused on technical challenges triggered by vibe coding.</span></p></li></ul><p>This post is the last one (or at least I believe so at the time of writing), and I will focus on the product management side. Well, just one aspect of it.</p><p><strong>The perception that the pace of shipping features (or building in general) is the bottleneck of product development is a misconception.</strong></p><p>Ultimately, that's what vibe coding tools offer: we can build it for you with no engineering team whatsoever. In fact, the original challenge was worded along the same lines:</p><blockquote><p><em><span>"I'm working with people and I've seen others, who only used AI to create a valid tech business, scaling it up beyond a million dollars, </span><strong>before they hired any software engineer.</strong><span>"</span></em></p></blockquote><p>Let’s unpack it, then.</p><p><span>I'm a fan of vibe coding when it comes to prototyping. </span><a href="https://pawelbrodzinski.substack.com/p/role-of-vibe-coding-in-product-validation" rel="nofollow ugc noopener">It is a fabulous tool to learn whether what we ideate is desirable.</a><span> </span><strong>The first thing about prototypes, though? They are </strong><em><strong>disposable</strong></em><strong>.</strong></p><p><span>Even if we validate that we were right and our idea works (</span><a href="https://pawelbrodzinski.substack.com/p/90-of-times-validation-means-invalidation" rel="nofollow ugc noopener">which happens maybe once every ten attempts</a><span>), the prototype is still </span><em>disposable</em><span>.</span></p><p>The whole idea behind prototyping is that we trade quality for a quick and cheap outcome. It can break. It can be buggy. Sometimes it may even look ugly. The point is: it conveys the idea.</p><p>Conversely, a product has to deliver promised value sustainably over time. Awful UX? I'll move to an alternative. Bugs too annoying? I'll stop using it altogether. It breaks entirely? Why would I use it, let alone pay for it?</p><p>The quality has to be there. Otherwise, customers will go as fast as they come, and that's not a viable product strategy. Sure, we'd still love to build our product quickly and cheaply, but at some level, quality is non-negotiable. Ultimately, we need the thing to work in the long run.</p><p>How do most successful digital products take their shape? Consider any example of your choice and try to reverse-engineer their path. Do you see a clear way, going from one milestone to another, each step an inevitable consequence of all the previous ones?</p><p>Like Amazon figuring out online bookselling as the hit of the internet era, and then, inevitably, taking a shot at music, video, and other industries, while concurrently launching in non-US markets?</p><p>Expanding to include third-party sellers must have been a logical next step, right? And building the biggest cloud infrastructure, their own reading device, and video streaming business... well, by this point, we’re retrofitting the connections and we know it.</p><p><span>Amazon tried a lot of things to land with its key cash cows today. Heck, even with their foundational idea—the marketplace—they famously run thousands of experiments all the time. In fact, </span><a href="https://graphite.dev/blog/how-amazon-deploys-code" rel="nofollow ugc noopener">their whole development culture is designed around rapid experimentation</a><span>.</span></p><p><span>In other words, we never know upfront what will work in a product. </span><strong>We try stuff, see what works, stick with what does, drop what does not.</strong></p><p><a href="https://paulbuchheit.blogspot.com/2009/01/communicating-with-code.html" rel="nofollow ugc noopener">Paul Buchheit is famous for building the first GMail prototype in just a few hours. And then repeating the trick with AdSense.</a><span> All in times where all code had to be manually written, like, you know, by hand.</span></p><p>Yes, we’re talking about two of Google’s product slam dunks. Yet, if you read the story carefully, it was anything but an execution of a well-laid plan.</p><blockquote><p><em><span>"We did </span><strong>a lot of things wrong during the 2.5 years</strong><span> of pre-launch Gmail development."</span></em></p><p><em><span>"We </span><strong>re-wrote the frontend about six times and the backend three times</strong><span> by launch."</span></em></p><p><em><span>"I would just write the code, release the feature, and watch the response. Usually, </span><strong>everyone (including me) would end up hating whatever it was</strong><span> (especially my ideas), but we always learned something from the experience, and we were able to quickly move on to other ideas."</span></em></p><p>Paul Buchheit</p></blockquote><p><span>In other words, there have been numerous dead ends that they explored, invalidated, and moved on from. </span><em>There's no knowing up front.</em></p><p>The same pattern applies to products, not just product features. As an example, we can stick with Google. It is known for a swath of products in line with its mission to organize the world's information. Search, Gmail, Google Workspace, and what have you.</p><p>However, aside from the search, many of these products originated as experiments. Gmail and AdSense, mentioned above, are two notable examples.</p><p>However, for each product idea that survived the test of time, there are a dozen that did not. And for each of the latter, there are probably an order of magnitude more that weren't even released to the public.</p><p><span>I personally used and loved </span><a href="https://en.wikipedia.org/wiki/Google_Reader" rel="nofollow ugc noopener">Google Reader</a><span>, </span><a href="https://en.wikipedia.org/wiki/Google_Talk" rel="nofollow ugc noopener">Google Talk</a><span>, and </span><a href="https://en.wikipedia.org/wiki/Picasa" rel="nofollow ugc noopener">Picasa</a><span>, and the retirement of each broke my heart. There are a few more I didn't cry over, despite being an active user till their sad end.</span></p><p><span>Probably everyone remembers Google's biggest lost bets: </span><a href="https://en.wikipedia.org/wiki/Google_Buzz" rel="nofollow ugc noopener">Buzz</a><span>, </span><a href="https://en.wikipedia.org/wiki/Google%2B" rel="nofollow ugc noopener">Google+</a><span>, and </span><a href="https://en.wikipedia.org/wiki/Google_Wave" rel="nofollow ugc noopener">Google Wave</a><span>.</span></p><p><span>By the end of 2024, </span><a href="https://killedbygoogle.com/" rel="nofollow ugc noopener">the full list of things that Google retired had almost 300 entries</a><span>. It’s not a track record peppered with home runs, is it? And that’s for a company that, from the vantage point of an aspiring startup, has unlimited capabilities.</span></p><p>With all the engineering power Google has, the pace of development could have been set arbitrarily high for any of these products. While no official information is available, it's been rumored that Google had a few hundred engineers working on Google+ alone.</p><p>If the pace of development were all that counted, it would always be the incumbents who would win the product race in any niche. After all, they can pump as much engineering firepower as they want, leveraging their existing revenue streams, customer bases, and whatnot.</p><p>And we know it doesn't work this way.</p><p>Product development, in essence, is continuous discovery. First, we aim to validate the idea, then we switch to validating whether any given change brings us closer to our goals—growth, revenue, retention, or whatever that may be.</p><p><span>The problem is that validation takes time. </span><a href="https://substack.com/@leahtharin" rel="nofollow ugc noopener">Leah Tharin</a><span>, who shares her story about </span><a href="https://substack.com/home/post/p-170083802" rel="nofollow ugc noopener">working on products with tens of millions of users</a><span>, says the following.</span></p><blockquote><p><em><span>"The bottleneck of the team was </span><strong>waiting for statistical significance for most of our experiments</strong><span>, despite all the traffic we had. (...) If we changed a copy of our main website or most prominent tools, the experiments were statistically significant within hours. A more complex down the funnel change for higher value customers? </span><strong>Weeks, sometimes months.</strong><span> Ugh."</span></em></p><p>Leah Tharin</p></blockquote><p>Weeks. Sometimes months. That's before they could have learned whether the change was for the better, worse, or had no effect at all.</p><p><span>Let's make a thought experiment and assume they could reduce the cost and time of </span><em>development</em><span> by a factor of 10. Would they grow faster? Save for the simplest tweaks on the landing page, they would still need to wait to learn the outcomes.</span></p><p>And it's not like they could improve the volume of experiments by tenfold either. Sure, it's technically feasible. Except it would make a mess out of the metrics.</p><p><em>"So we're running these 27 concurrent experiments to improve retention, and the data says it's been better for a week and then got back to what we had before. What does it say about those experiments again?"</em></p><p><span>If you look at what actually matters (growth, revenue, retention), knowing the right thing to build is the most common bottleneck. </span><em>And we can't reliably know what will work from the outset.</em></p><p>But what if we do know exactly what to build? Ultimately, it's the basic pattern of project work. We define the scope, we agree on the payment, and off we go!</p><p>In such a setup, we can conveniently ignore that we might be building the wrong thing entirely. Or, with a bit of luck, it's one of the rather rare cases where we either run a direct replacement project or automate the existing business, and we have a much better initial understanding of the desirability, viability, and feasibility of the idea.</p><p>Still, it's not the development pace that makes or breaks such endeavors.</p><p>At Lunar Logic, when estimating work for a client we've never worked with before, we always go with a wide range. Not as wide as we'd like, as to be brutally honest, we'd need to go with something like "It can take less than a month, or more than a quarter." Still, the bracket is uncomfortably wide for many of our potential customers.</p><p>And that for the work with limited technical uncertainties.</p><p>Why are we all over the place? Can’t we just use 20 years of experience that we so often brag about and tell in plain English how much it will cost? No, we can’t. We don’t yet know how collaboration will look, and that factor alone will sway the actual costs more than anything directly related to the scope. </p><p><span>Poor communication creates rework. It's not unusual that </span><a href="https://brodzinski.com/2025/08/most-underestimated-factor-estimation.html" rel="nofollow ugc noopener">the quality of communication, or rather lack thereof, adds as much as an additional 100% to the effort</a><span>.</span></p><p>That compounds with the development of all unnecessary features. If you look at such a gig in hindsight, the value-adding work may stack up to just several percent of the whole effort.</p><p><span>Adding development speed would only exacerbate the problem. </span><em>The cost of rework doesn't pile up linearly.</em><span> Once you rework the rework, it's like a compound interest rate, except in reverse. Go figure what it does to the costs.</span></p><p><span>One of the many excellent observations Daniel Kahneman explains in his seminal book&nbsp;</span><a href="https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555" rel="nofollow ugc noopener">Thinking, Fast and Slow</a><span>&nbsp;is the following.</span></p><blockquote><p><em>"This is the essence of intuitive heuristics: when faced with a difficult question, we often answer an easier one instead, usually without noticing the substitution."</em></p><p>Daniel Kahneman</p></blockquote><p>We subconsciously avoid solving difficult problems by finding a similar one that's much easier to address. Then, we pretend the answer to the latter works as the answer to the former.</p><p>In that manner, we respond to the question about successful product development. We have little clue about what makes products successful. However, we certainly see that the most significant part of the effort is development. It takes a great deal of time and money to turn an idea into a product.</p><p>So we focus on the speed of development. And suddenly, we have an easy answer. We can make it faster. How? That's simple. Use AI.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!F-mQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb3462bf-76e5-4c49-add9-6c6e2da3145a_1129x587.jpeg" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!F-mQ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb3462bf-76e5-4c49-add9-6c6e2da3145a_1129x587.jpeg 424w, https://substackcdn.com/image/fetch/$s_!F-mQ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb3462bf-76e5-4c49-add9-6c6e2da3145a_1129x587.jpeg 848w, https://substackcdn.com/image/fetch/$s_!F-mQ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb3462bf-76e5-4c49-add9-6c6e2da3145a_1129x587.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!F-mQ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb3462bf-76e5-4c49-add9-6c6e2da3145a_1129x587.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!F-mQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb3462bf-76e5-4c49-add9-6c6e2da3145a_1129x587.jpeg" width="1129" height="587" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fb3462bf-76e5-4c49-add9-6c6e2da3145a_1129x587.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:587,&quot;width&quot;:1129,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:124559,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://pawelbrodzinski.substack.com/i/172659340?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb3462bf-76e5-4c49-add9-6c6e2da3145a_1129x587.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!F-mQ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb3462bf-76e5-4c49-add9-6c6e2da3145a_1129x587.jpeg 424w, https://substackcdn.com/image/fetch/$s_!F-mQ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb3462bf-76e5-4c49-add9-6c6e2da3145a_1129x587.jpeg 848w, https://substackcdn.com/image/fetch/$s_!F-mQ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb3462bf-76e5-4c49-add9-6c6e2da3145a_1129x587.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!F-mQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb3462bf-76e5-4c49-add9-6c6e2da3145a_1129x587.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Sorry to break it to you. </span><strong>Code does not equal product. What follows is that more code does not equal a better product.</strong><span> Often, it's the opposite.</span></p><p>Coding speed was never the bottleneck. Not even when we didn't have an AI shortcut.</p><p>If we stick to the context of product development, vibe coding promises us two things.</p><ul><li><p>We'll get the code fast.</p></li><li><p>We don't need to hire expensive technical expertise.</p></li></ul><p>Both parts miss the "coding speed was never the bottleneck" observation. Both respond to the simple question instead of the difficult one.</p><p>To make matters worse, the price we pay for removing ourselves from understanding the code is more rework. Yes, I know, we outsource that rework to an AI agent too, but we still need to drive it. And then all that rework stacks up. Remember compound interest rate in reverse?</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Bd83!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80e0c5c8-873b-4e21-a7e4-7676925f04dc_757x757.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Bd83!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80e0c5c8-873b-4e21-a7e4-7676925f04dc_757x757.png 424w, https://substackcdn.com/image/fetch/$s_!Bd83!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80e0c5c8-873b-4e21-a7e4-7676925f04dc_757x757.png 848w, https://substackcdn.com/image/fetch/$s_!Bd83!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80e0c5c8-873b-4e21-a7e4-7676925f04dc_757x757.png 1272w, https://substackcdn.com/image/fetch/$s_!Bd83!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80e0c5c8-873b-4e21-a7e4-7676925f04dc_757x757.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Bd83!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80e0c5c8-873b-4e21-a7e4-7676925f04dc_757x757.png" width="757" height="757" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/80e0c5c8-873b-4e21-a7e4-7676925f04dc_757x757.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:757,&quot;width&quot;:757,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:775135,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://pawelbrodzinski.substack.com/i/172659340?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80e0c5c8-873b-4e21-a7e4-7676925f04dc_757x757.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Bd83!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80e0c5c8-873b-4e21-a7e4-7676925f04dc_757x757.png 424w, https://substackcdn.com/image/fetch/$s_!Bd83!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80e0c5c8-873b-4e21-a7e4-7676925f04dc_757x757.png 848w, https://substackcdn.com/image/fetch/$s_!Bd83!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80e0c5c8-873b-4e21-a7e4-7676925f04dc_757x757.png 1272w, https://substackcdn.com/image/fetch/$s_!Bd83!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80e0c5c8-873b-4e21-a7e4-7676925f04dc_757x757.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Ultimately, using vibe coding as the main tactic to build a successful product is like solving a minor issue only to make the main problem a much bigger challenge.</p><p>The previous two parts of this informal vibe coding series:</p><div data-component-name="DigestPostEmbed"><a href="https://pawelbrodzinski.substack.com/p/vibe-coded-product-success-stories" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!LFzr!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8075028f-418a-4a26-a9a4-b465db30b27d_1920x1080.jpeg"><img src="https://substackcdn.com/image/fetch/$s_!LFzr!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8075028f-418a-4a26-a9a4-b465db30b27d_1920x1080.jpeg" sizes="100vw" alt="Vibe-Coded Product Success Stories Ain't What They Look" width="140" height="140"></picture></div></a></div><div data-component-name="DigestPostEmbed"><a href="https://pawelbrodzinski.substack.com/p/vibe-coding-doesnt-replace-tech-skills" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!D2Yp!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae64fdb1-9f66-4e2b-b041-2c556280d394_3664x2058.jpeg"><img src="https://substackcdn.com/image/fetch/$s_!D2Yp!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae64fdb1-9f66-4e2b-b041-2c556280d394_3664x2058.jpeg" sizes="100vw" alt="Vibe Coding Doesn't Replace Tech Skills; It Requires More of Them" width="140" height="140"></picture></div></a></div></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You're absolutely Right! (426 pts)]]></title>
            <link>https://absolutelyright.lol/</link>
            <guid>45137802</guid>
            <pubDate>Fri, 05 Sep 2025 12:36:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://absolutelyright.lol/">https://absolutelyright.lol/</a>, See on <a href="https://news.ycombinator.com/item?id=45137802">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <p>Claude Code said it <span id="today-inline">0</span> times today</p>
    

    <section>
      
      <p><span>
          <span></span>
          Absolutely right
        </span>
        <span>
          <span></span>
          Just right
        </span>
      </p>
    </section>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI eats jobs, then offers to help you find a new one at Walmart (206 pts)]]></title>
            <link>https://www.theregister.com/2025/09/05/openai_jobs_board/</link>
            <guid>45137658</guid>
            <pubDate>Fri, 05 Sep 2025 12:17:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/09/05/openai_jobs_board/">https://www.theregister.com/2025/09/05/openai_jobs_board/</a>, See on <a href="https://news.ycombinator.com/item?id=45137658">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>For those worried that AI is going to disrupt their jobs, OpenAI has the solution – take its certification and use a newly announced jobs board to find a new role.</p>
<p>On Thursday, Fidji Simo, OpenAI's head of applications (and <a target="_blank" href="https://www.theregister.com/2025/05/08/openai_apps_chif_instacart/">former</a> CEO of Instacart), announced the plan for workers to advertise themselves to the company's customers for new jobs. She said that while AI is going to shake up the employment market, who better to solve that problem than the people doing the shaking?</p>
<p>"AI will be disruptive. Jobs will look different, companies will have to adapt, and all of us – from shift workers to CEOs – will have to learn how to work in new ways," she said in a <a target="_blank" rel="nofollow" href="https://openai.com/index/expanding-economic-opportunity-with-ai/">blog post</a>.</p>

    

<p>"At OpenAI, we can't eliminate that disruption. But what we can do is help more people become fluent in AI and connect them with companies that need their skills, to give people more economic opportunities."</p>

        


        

<p>Simo's plan is that workers should take courses in tech literacy at its <a target="_blank" rel="nofollow" href="https://academy.openai.com/">OpenAI Academy</a> and then advertise themselves on a forthcoming jobs platform. She said the company has already signed up some big names to the scheme, although maybe the choice of Walmart as an early adopter might not encourage IT admins in their future career paths.</p>
<p>OpenAI declined to comment further on the plans.</p>

        

<p>"At Walmart, we know the future of retail won't be defined by technology alone – it will be defined by people who know how to use it," Walmart US CEO John Furner said in a canned statement.</p>
<p>"By bringing AI training directly to our associates, we're putting the most powerful technology of our time in their hands – giving them the skills to rewrite the playbook and shape the future of retail."</p>
<ul>

<li><a href="https://www.theregister.com/2025/09/03/ai_hiring_biased/">Biased bots: AI hiring managers shortlist candidates with AI resumes</a></li>

<li><a href="https://www.theregister.com/2025/04/03/openai_copyright_bypass/">OpenAI wants to bend copyright rules. Study suggests it isn't waiting for permission</a></li>

<li><a href="https://www.theregister.com/2025/07/24/white_house_wants_no_woke_ai/">White House bans 'woke' AI, but LLMs don't know the truth</a></li>

<li><a href="https://www.theregister.com/2025/08/28/microsoft_unveils_housemade_models_amid/">Microsoft unveils home-made ML models amid OpenAI negotiations</a></li>
</ul>
<p>The OpenAI Academy has had some big-name sign-ups, <a target="_blank" rel="nofollow" href="https://www.gatech.edu/news/2025/03/27/georgia-tech-leads-way-ai-literacy-openai-academy-collaboration">particularly</a> the respected computer science teachers at Georgia Tech, but Simo says that the business is pushing hard to build on a White House plan to make AI a core skill for American workers – so long as the engines they use aren't <a target="_blank" href="https://www.theregister.com/2025/07/24/white_house_wants_no_woke_ai/">too woke</a>.</p>
<p>What Simo didn't mention directly is that getting into the jobs market would bring the company into competition with Microsoft, one of its biggest backers. LinkedIn is the primary Western jobs site and OpenAI setting up a competitor might get in the way of cordial relations.</p>
<p>Microsoft had no comment on the matter, but OpenAI appears to be only scooping the AI cream, and whatever else floats to the top of the market, on its proposed employment register. There's also the question of whether or not the skills OpenAI is shilling will have any validity in the actual jobs market.</p>

        

<p>Meanwhile, CEO Sam Altman and most of the tech glitterati attended <a target="_blank" rel="nofollow" href="https://thehill.com/homenews/administration/5485218-trump-host-tech-ceos-rose-garden/">a dinner</a> hosted by First Lady Melania Trump to discuss AI last night. Elon Musk wasn't there, but <a target="_blank" rel="nofollow" href="https://x.com/elonmusk/status/1963624123998896368">insists</a> he was invited. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Ditched Docker for Podman (and You Should Too) (573 pts)]]></title>
            <link>https://codesmash.dev/why-i-ditched-docker-for-podman-and-you-should-too</link>
            <guid>45137525</guid>
            <pubDate>Fri, 05 Sep 2025 11:56:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://codesmash.dev/why-i-ditched-docker-for-podman-and-you-should-too">https://codesmash.dev/why-i-ditched-docker-for-podman-and-you-should-too</a>, See on <a href="https://news.ycombinator.com/item?id=45137525">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-content-parent"><h2 id="heading-beginnings">Beginnings</h2>
<p>I'm old enough to remember when Vagrant looked like a promised land where every development environment would look the same. Differences between language versions, as well as some unusual OS version differences, resulted in a few days of unproductive debugging of your development environment. I've had similar excitement when I started my first Docker Swarm (who uses that these days?!) - it felt revolutionary. Docker wasn't just a tool - it fundamentally changed how we thought about application development and deployment. Having a repeatable, separated environment from your local system was refreshing and looked like a superpower. It has become a must-have tool for every engineer. "<em>Just Dockerize it</em>" became my go-to solution for pretty much everything. Sure, architecture or defining a new Docker image could be a bit finicky at times, but hey, that's just how things worked. Is the persistent dockerd daemon eating upresources in the background with root privileges, just the price of doing business? I thought so.</p>
<p>If you are in this industry long enough, there is one pattern that emerges every day. Everybody begins questioning the "that's just how it's done" mentality. Along the way, the quiet Docker daemon running in the background felt less like a comfortable constant and more like a ticking bomb. More and more ways to explore this vulnerability emerged:</p>
<p><strong>2019-02-11 - CVE-2019-5736 (runC container escape):</strong> lets a process in a container overwrite the host’s runc binary → full host compromise if exploited.</p>
<p><strong>2022-03-07 - CVE-2022-0847 “Dirty Pipe” (Linux kernel):</strong> read-only file overwrite in kernel; practical container-to-host abuse scenarios documented by Docker/Sysdig. </p>
<p><strong>2022-03-07 - CVE-2022-0492 (cgroups v1 release_agent):</strong> privilege escalation / container escape via cgroups v1; mitigations via seccomp/AppArmor/SELinux. </p>
<p><strong>2024-01-31 - CVE-2024-21626 (runC “Leaky Vessels”):</strong> fd leak + process.cwd issues enabling host FS access and potential escape; fixed in runC <strong>1.1.12</strong> (Docker Engine ≥ <strong>25.0.2</strong>). </p>
<p><strong>2024-02-01 - CVE-2024-23651/23652/23653 (BuildKit, “Leaky Vessels”):</strong> build-time issues that can affect host files; fixed in BuildKit <strong>0.12.5</strong>. </p>
<p><strong>2024-09-23 - In-the-wild cryptojacking campaign:</strong> attackers targeted exposed Docker APIs and microservices. </p>
<p><strong>2024-10-01 - Docker API swarm botnet campaign:</strong> cryptojacking via exposed Docker Engine API (<a target="_blank" href="https://securitylabs.datadoghq.com/articles/threat-actors-leveraging-docker-swarm-kubernetes-mine-cryptocurrency/">details</a>).</p>
<p>I had been seeking an alternative (I assumed that someone had already questioned the status quo), and that's how I stumbled into Podman territory. It began as casual curiosity - "<em>Hey, let me check out this thing</em>" - turned into a complete overhaul of my container workflows and pulled me into using Fedora in my home lab. And honestly? I wish I'd made the switch sooner.</p>
<h2 id="heading-daemonless">Daemonless</h2>
<p>Here's the fundamental issue that kept me awake: Docker's entire architecture is built around a persistent background service - the dockerd daemon. Whenever you run a docker command, you're actually talking to this daemon, which then does the heavy lifting. Sounds about right?</p>
<p>Yes?!</p>
<p>Or rather NO, because this daemon runs with root privileges. Always. And if something goes south with a daemon - innocent bug, a crash, or worst case scenario, a security exploit - your entire container ecosystem is potentially compromised. Not just the containers, daemon, or resource that you assigned to it, but the whole host system. It was a huge relief that Podman threw this model out the window. No daemon, no processes running in the background. When you run <code>podman run my-app</code>, the container becomes a direct child of your command. And it is running under your user privileges. Simple architecture change with huge implications:</p>
<h3 id="heading-security-that-actually-makes-sense">Security that actually makes sense:</h3>
<p>Remember those late-night security advisories about Docker daemon vulnerabilities (ex., when dockerd was misconfigured to listen on TCP:2375 without TLS, attackers could spin up privileged containers remotely)? With Podman, even if someone somehow escalates privileges inside a container to root level, they're still just an unprivileged user on the actual host. It significantly reduces the surface of an attack.</p>
<h3 id="heading-no-more-single-points-of-failure">No more single points of failure:</h3>
<p>Usually Docker daemon runs just fine. But when hiccups kick in - oh boy, hold your hats, as it will take down multiple containers at once. With Podman when one container crashed, the other kept running like nothing happened. It makes so much sense, and it's built in the spirit of hermetization.</p>
<h3 id="heading-lighter-resource-footprint">Lighter resource footprint:</h3>
<p>I had been surprised when my MacBook M2 Pro started to get warmer when left unattended. After a brief investigation (with Activity Monitor), it was obvious - Docker never knows when to stop. No constantly running daemon means less memory usage. Unfortunately, running a container using Podman can be a different story (ekhm: <a href="https://blog.podman.io/2025/06/podman-and-apple-rosetta/" target="_blank">blog.podman.io/2025/06/podman-and-apple-ros..</a>) - yet the thing is getting better: <a href="https://blog.podman.io/2025/08/podman-5-6-released-rosetta-status-update/" target="_blank">blog.podman.io/2025/08/podman-5-6-released-..</a>.</p>
<h2 id="heading-where-podman-really-shines">Where Podman Really Shines</h2>
<p>Beyond the obvious daemon advantages, Podman brings some genuinely clever features that make day-to-day container work more pleasant:</p>
<p><strong>Systemd integration that doesn't suck:</strong> This one's huge if you're working on Linux servers (most of us are). Podman justgenerates proper systemd unit files. Boom, your container is a first-class citizen in the Linux service ecosystem. Boot dependencies, automatic restarts, resource limits - it all just works. I can run <code>podman generate systemd --name my-app</code> and get a clean service file. Afterwards, I can enable, start, stop, and monitor with standard systemctl commands. Say bye-bye to third-party process managers.</p>
<p><strong>Kubernetes alignment that's not just marketing:</strong> Since Red Hat (the folks behind Podman) is also a major Kubernetes contributor, the tool feels like it was designed with K8s in mind from day one. The native pod support isn't just a bolt-on feature - it's central to how Podman works. I do not need to run k3s or any local substitute for Kubernetes. Now, I can prototype multi-container applications as Podman pods locally. Then I just generate Kubernetes YAML directly from those pods with podman generate kube. My local development environment actually looks like what I'm going to deploy. This was revolutionary when I had to take over the responsibility of managing and developing a quite complex cluster.</p>
<p><strong>The Unix philosophy done right:</strong> Instead of trying to be everything to everyone, Podman focuses on running containers well and delegates specialized tasks to purpose - built tools. Need to build images with fine - grained control? That's Buildah. Want to inspect or copy images between registries? Skopeo's your friend. I can use the best tool for each job. I'm no longer stuck with whatever image-building quirks Docker decides to implement.</p>
<h2 id="heading-the-migration-that-wasnt-really-a-migration">The Migration That Wasn't Really a Migration</h2>
<p>Here's the part that surprised me most: switching from Docker to Podman was almost seamless. The Podman folks clearly understood that creating the next standard would not let them win the market, and they just adhered to the known CLI tool. I literally just aliased <code>docker=podman</code> in my shell and carried on with life. <code>podman run, podman build, podman ps</code> - they all behave exactly like their Docker counterparts. My existing Dockerfiles worked without modification. My muscle memory didn't need retraining.</p>
<p>Though there were a few places where I did hit differences that were actually improvements in disguise:</p>
<ul>
<li><p>Privileged ports in rootless mode not working? Good! That's security working as intended. A reverse proxy setup is a better architecture anyway.</p>
</li>
<li><p>Some volume permission quirks? Yes - but it's a small price, and again - if you do it right, you are limiting the scope of possible attack.</p>
</li>
<li><p>A few legacy tools that expected the Docker socket? If there is no support by now, just remember that Podman can expose a Docker-compatible API if needed.</p>
</li>
<li><p>If your Docker Compose workflow is overly complex, just convert it to Kubernetes YAML. We all use Kubernetes these days, so why even bother about this? Having the same layout for development and production is a huge bonus of doing so.</p>
</li>
</ul>
<h2 id="heading-the-real-world-difference">The Real-World Difference</h2>
<p>After six months of running Podman in production, here's what I've noticed:</p>
<p>I'm sleeping much better. Because I'm personally responsible for security, I do not have to check if every container is running in rootless mode. Something that I did not think I would benefit from is that my monitoring dashboards show cleaner resource usage patterns. Don't get me wrong - Docker isn't going anywhere. It has massive momentum, a mature ecosystem, and plenty of organizational inertia keeping it in place. But for new projects, or if you are able to make technical decisions based on merit rather than legacy, Podman represents a clear evolution in container technology. More secure by design, more aligned with Linux system management practices, and more thoughtfully architected for the way we actually deploy containers in 2025. The best way forward is to question the assumptions you didn't even realize you were making.</p>
<h2 id="heading-fastapi-migration-guide-from-docker-to-podman">FastAPI Migration Guide: From Docker to Podman</h2>
<p>Just to prove how easy transition can be, here's a practical walkthrough of migrating a FastAPI application from Docker to Podman.  </p>
<h3 id="heading-what-youll-need">What You'll Need</h3>
<p>Your existing FastAPI project with its Dockerfile and requirements.txt</p>
<p>Podman is installed on your system:</p>
<ul>
<li><p>Ubuntu/Debian: sudo apt update &amp;&amp; sudo apt install podman</p>
</li>
<li><p>Fedora/RHEL: sudo dnf install podman</p>
</li>
<li><p>macOS: Grab Podman Desktop for a GUI experience</p>
</li>
<li><p>Windows: If you are not a C# developer - stop doing this to yourself and just use Linux: <a href="https://www.youtube.com/watch?v=S_RqZG6YR5M" target="_blank">youtube.com/watch?v=S_RqZG6YR5M</a></p>
</li>
</ul>
<h3 id="heading-step-1-your-dockerfile-probably-just-works">Step 1: Your Dockerfile Probably Just Works</h3>
<p>This is the beautiful part—Podman uses the same OCI container format as Docker. Your existing Dockerfile should work without any changes. Here's a typical FastAPI setup:</p>
<pre><code>FROM python:3.10-slim-buster

WORKDIR /app

COPY requirements.txt .

RUN pip install --no-cache-dir --upgrade -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
</code></pre>
<h3 id="heading-step-2-build-your-image">Step 2: Build Your Image</h3>
<p>Instead of docker build, just run:</p>
<pre><code>podman build -t my-fastapi-app:latest .
</code></pre>
<p>That's it. Same flags, same behavior, same output. If you want to ease the transition, create an alias:</p>
<pre><code><span>alias</span> docker=podman
</code></pre>
<p>Now you can use your existing docker build commands without thinking about it.</p>
<h3 id="heading-step-3-run-your-container">Step 3: Run Your Container</h3>
<p>For development and testing:</p>
<pre><code>podman run --rm -p 8000:8000 --name my-fastapi-container my-fastapi-app:latest
</code></pre>
<p>For background services:</p>
<pre><code>podman run -d -p 8000:8000 --name my-fastapi-container my-fastapi-app:latest
</code></pre>
<p>Your app should be accessible at <a href="http://localhost:8000/" target="_blank">localhost:8000</a> just like before.</p>
<p><strong>Important note:</strong> <em>By default, Podman runs in rootless mode. This is a security win, but it means you can't bind directly to privileged ports (below 1024). For production, you'll want a reverse proxy anyway, so this pushes you toward better architecture.</em></p>
<h3 id="heading-step-4-production-deployment-with-systemd">Step 4: Production Deployment with Systemd</h3>
<p>This is where Podman really shines. Instead of wrestling with custom service management, generate a proper systemd unit file:</p>
<pre><code><span># First, make sure your container is running</span>

podman run -d -p 8000:8000 --name my-fastapi-container my-fastapi-app:latest

<span># Generate the systemd service file</span>

mkdir -p ~/.config/systemd/user/

podman generate systemd --name my-fastapi-container &gt; ~/.config/systemd/user/my-fastapi-container.service

<span># Enable and start the service</span>

systemctl --user daemon-reload

systemctl --user <span>enable</span> my-fastapi-container.service

systemctl --user start my-fastapi-container.service
</code></pre>
<p>Now your FastAPI app is managed like any other system service. It'll start on boot, restart on failure, and integrate with standard Linux logging and monitoring tools.</p>
<p>For server deployments where you want the service to persist even when you're not logged in:</p>
<p>loginctl enable-linger $(whoami)</p>
<h3 id="heading-step-5-multi-service-applications-with-pods">Step 5: Multi-Service Applications with Pods</h3>
<p>If your FastAPI app needs a database or other services, Podman's pod concept is cleaner than Docker Compose for simple setups:</p>
<pre><code><span># Create a pod that shares networking</span>
podman pod create --name my-fastapi-pod -p 8000:8000 -p 5432:5432

<span># Run your FastAPI app in the pod</span>

podman run -d --pod my-fastapi-pod --name fastapi-app my-fastapi-app:latest

<span># Run PostgreSQL in the same pod</span>

podman run -d --pod my-fastapi-pod --name postgres-db -e POSTGRES_PASSWORD=mysecretpassword postgres:13
</code></pre>
<p>Now your FastAPI app can reach PostgreSQL at localhost:5432 because they share the same network namespace.</p>
<h3 id="heading-step-6-docker-compose-compatibility">Step 6: Docker Compose Compatibility</h3>
<p>For existing Docker Compose setups, you have options:</p>
<p><strong>Option 1:</strong> Use podman-compose as a drop-in replacement:</p>
<pre><code>pip install podman-compose

podman-compose up -d
</code></pre>
<p><strong>Option 2:</strong> Convert to Kubernetes YAML for a more cloud-native approach:</p>
<pre><code><span># Install kompose first</span>

kompose convert -f docker-compose.yml -o k8s-manifest.yaml

podman play kube k8s-manifest.yaml
</code></pre>
<p>This second option is particularly nice if you're planning to deploy to Kubernetes eventually.</p>
<p><strong>Common Gotchas and Solutions</strong></p>
<p><strong>Volume permissions:</strong> If you hit permission issues with mounted volumes, remember that rootless containers run as your user. Make sure your user owns the directories you're mounting:</p>
<pre><code>chown -R $(id -un):$(id -gn) /path/to/your/data
</code></pre>
<p><strong>Legacy tooling:</strong> Some tools expect the Docker socket at /var/run/docker.sock. Podman can provide a compatible API:</p>
<pre><code>systemctl --user <span>enable</span> podman.socket

systemctl --user start podman.socket

<span>export</span> DOCKER_HOST=unix://<span>$XDG_RUNTIME_DIR</span>/podman/podman.sock
</code></pre>
<p><strong>Performance tuning:</strong> For production workloads, you might want to tune the rootless networking stack or consider running specific containers in rootful mode for maximum performance.</p>
<p>The migration process is usually much smoother than people expect. Start with a development environment, get comfortable with the workflow differences, then gradually move production workloads. The security and operational benefits make it worth the effort.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nepal moves to block Facebook, X, YouTube and others (148 pts)]]></title>
            <link>https://www.aljazeera.com/news/2025/9/4/nepal-moves-to-block-facebook-x-youtube-and-others</link>
            <guid>45137363</guid>
            <pubDate>Fri, 05 Sep 2025 11:31:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.aljazeera.com/news/2025/9/4/nepal-moves-to-block-facebook-x-youtube-and-others">https://www.aljazeera.com/news/2025/9/4/nepal-moves-to-block-facebook-x-youtube-and-others</a>, See on <a href="https://news.ycombinator.com/item?id=45137363">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><em>The restrictions come after the social media giants failed to meet state registration requirements, says government.</em></p></div><div aria-live="polite" aria-atomic="true"><p>Nepal’s government has said it will shut off access to major social media platforms, including Facebook and X, after they failed to comply with authorities’ registration requirements.</p><p>The move, announced on Thursday, is part of what the government says is an effort to curb online hate, rumours and cybercrime.</p><section><h2>Recommended Stories<!-- --> </h2><span>list of 3 items</span><ul><li><span>list 1 of 3</span><a href="https://www.aljazeera.com/news/2025/5/27/nepali-breaks-world-record-with-31st-summit-of-mount-everest">‘Everest Man’ breaks own record for climbing world’s highest mountain</a></li><li><span>list 2 of 3</span><a href="https://www.aljazeera.com/video/newsfeed/2025/7/8/dozens-missing-after-monsoon-triggers-nepal-china-floods">Dozens missing after monsoon triggers Nepal-China floods</a></li><li><span>list 3 of 3</span><a href="https://www.aljazeera.com/gallery/2025/5/5/the-last-nomads-of-nepal">Photos: The last nomads of Nepal</a></li></ul><span>end of list</span></section><p>Companies were given a deadline of Wednesday to register with the Ministry of Communications and Information Technology and provide a local contact, grievance handler and person responsible for self-regulation – or face shutdown.</p><p>“Unregistered social media platforms will be deactivated today onwards,” ministry spokesman Gajendra Kumar Thakur told AFP.</p><p>Communications and IT Minister Prithvi Subba Gurung said, “We gave them enough time to register and repeatedly requested them to comply with our request, but they ignored [this], and we had to shut their operations in Nepal.”</p><p>Meta, which owns Facebook, Instagram and WhatsApp, YouTube parent Alphabet, X, Reddit, and LinkedIn were asked to register by Wednesday’s deadline.</p><p>AFP reported that the platforms remained accessible on Thursday.</p><h2 id="directly-hits-fundamental-rights">‘Directly hits fundamental rights’</h2><p>The online restrictions follow a 2023 directive requiring social media platforms – which have millions of users in Nepal with accounts for entertainment, news and business – to register and establish a local presence.</p><p>Only five, including TikTok and Viber, have since formally registered, while two others are in the process.</p><p>Bhola Nath Dhungana, president of Digital Rights Nepal, said that the sudden closure shows the “controlling” approach of the government.</p><p>“This directly hits the fundamental rights of the public,” Dhungana said. “It is not wrong to regulate social media, but we first need to have the legal infrastructure to enforce it. A sudden closure like this is controlling.”</p><p>Nepal has restricted access to popular online platforms in the past.</p><p>Access was blocked to the Telegram messaging app in July, with the government citing a rise in online fraud and money laundering.</p><p>In August last year, Nepal lifted a nine-month ban on TikTok after the platform’s South Asia division agreed to comply with Nepali regulations.</p><p>Governments worldwide, including the United States, European Union, Brazil and Australia, are also <a href="https://www.aljazeera.com/economy/2025/7/25/meta-to-suspend-political-advertising-in-the-eu-as-transparency-law-looms">tightening oversight of social media and big tech</a>, citing concerns over misinformation, data privacy, online harm and national security. India has mandated local compliance officers and takedown mechanisms, while China maintains strict censorship and licensing controls.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Interview with Japanese Demoscener – 0b5vr (112 pts)]]></title>
            <link>https://6octaves.com/2025/09/interview-with-demoscener-0b5vr.html</link>
            <guid>45137245</guid>
            <pubDate>Fri, 05 Sep 2025 11:08:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://6octaves.com/2025/09/interview-with-demoscener-0b5vr.html">https://6octaves.com/2025/09/interview-with-demoscener-0b5vr.html</a>, See on <a href="https://news.ycombinator.com/item?id=45137245">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="mainEntityOfPage"><p><a href="https://6octaves.com/2025/09/jpdemoscener_0b5vr.html">→日本語で読む</a></p><p>Welcome to “Interviews with Demosceners”! This time, we welcome Japanese demoscener <a rel="noopener" href="https://0b5vr.com/" target="_blank">0b5vr</a>, who mainly creates 64K and 4K intros.</p><p>For many, 0b5vr is best remembered for his 64K demo “<a rel="noopener" href="https://www.pouet.net/prod.php?which=94135" target="_blank">0b5vr GLSL Techno Live Set</a>”, released at Revision 2023. In this interview, he talks about how this piece was created, as well as his recent live music performance.</p><p>He also talks about trends around the Japanese demoscene, like music production with GLSL, machine live, and generative VJ. I also took the chance to ask how he feels about sceners like me—that is, people who know nothing about programming or technology! Happy reading!</p><p>Note: If you don’t know what demoscene is, you may want to&nbsp;<a href="https://6octaves.com/demoscene">start from here</a>!</p><hr><p><strong>First of all, could you introduce yourself?</strong></p><p>I’m <a rel="noopener" href="https://0b5vr.com/" target="_blank">0b5vr</a>, and I don’t belong to any particular group. I mainly work on 64k intros and 4k intros/exegfx using WebGL. I also compete in Shader Jam and perform live coding and VJ sets at club events and similar venues.</p><p><img decoding="async" src="https://6octaves.com/wp-content/uploads/2025/09/0b5vr-324x324.webp" alt="0b5vr profile" width="224" height="224" srcset="https://6octaves.com/wp-content/uploads/2025/09/0b5vr-324x324.webp 324w, https://6octaves.com/wp-content/uploads/2025/09/0b5vr-432x432.webp 432w, https://6octaves.com/wp-content/uploads/2025/09/0b5vr-800x800.webp 800w, https://6octaves.com/wp-content/uploads/2025/09/0b5vr-768x768.webp 768w, https://6octaves.com/wp-content/uploads/2025/09/0b5vr-150x150.webp 150w, https://6octaves.com/wp-content/uploads/2025/09/0b5vr.webp 839w" sizes="(max-width: 224px) 100vw, 224px"></p><p><strong>Your demo “0b5vr GLSL Techno Live Set” had a strong impact on me. I was curious about this. It says “Live Set,” but was released in the 64K category. What is this exactly? Is this live coding?</strong></p><p>0b5vr GLSL Techno Live Set (“0mix”) is indeed a 64K intro demo. Just like any other 64K intro, this audiovisual piece is generated from a 64KB file―an HTML file, in this case.</p><p>That said, as described in the title, its format is “Live Set.” It can be somewhat tricky, because it looks like a recorded video of a live performance at an event, but it’s actually a 64K intro.</p><div><p><iframe title="0b5vr GLSL Techno Live Set - &quot;0mix&quot; (WebGL 64KB Intro)" width="1068" height="601" src="https://www.youtube.com/embed/3lOptjAeA2w?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p></div><p><strong>Hmm… I’m still not sure if I understood correctly. Could you elaborate a bit more?</strong></p><p>0mix was inspired by three different scenes: techno demos, live coding, and 64K intros.</p><p>Let me start with techno demos. There are many techno-themed demos in the history of the demoscene. If you look at the demos such as “<a rel="noopener" href="https://www.youtube.com/watch?v=wYvn653Fefw" target="_blank">Medium</a>” by Einklang.net, “<a rel="noopener" href="https://www.youtube.com/watch?v=j5HPZr7gBbY" target="_blank">X-MIX 2004: Ion Traxx</a>” by Kewlers &amp; mfx, and “<a rel="noopener" href="https://www.youtube.com/watch?v=A5PqEePIghE" target="_blank">Emix</a>” by Epoch, they use multiple tracks mixed together like a DJ set, rather than a single techno soundtrack. They also use VJ-style visuals to create an atmosphere similar to a club event. Emix has black-and-white visuals with unique textures that fit perfectly with cold, mechanical techno, and it’s one of my favorites.</p><p>Next is live coding. Live coding is a live performance where visuals and music are generated with programming in real time. On the screen, you’ll see the visuals and sound waveforms being generated alongside the code you’re writing. This highlights that the artwork is generated by code. In the demoscene, live coding sessions focus mostly on visuals in GLSL (eg, <a rel="noopener" href="https://www.youtube.com/watch?v=Rk9f4GUuXGk" target="_blank">Shader Showdown</a>, <a rel="noopener" href="https://www.youtube.com/watch?v=h_u8DkST08k" target="_blank">Shader Jam</a>). But in live coding events like Algorave and Eulerroom, music live coding is as popular as, or even more popular than, visual coding. From what I see, Tidal Cycles and Sonic Pi are the most commonly used tools in those environments. (<a rel="noopener" href="https://www.youtube.com/watch?v=wgPf24_OQZY" target="_blank">Reference video</a>)</p><p>Finally, there’s the 64K intro. It’s a category where you create visuals and audio with an executable file of just 64KB. This is the most challenging category since every element has to be procedurally generated within the intro. Most 64K creators build their own engines and tools from scratch. This category requires a broad range of knowledge and skills including modeling, animation, rendering, post-processing, music, and compression.</p><p>If I managed to merge all three inspirations and create a 64K techno demo with music generated by live coding, I knew I could present it to demosceners and other creators around the scene with confidence. I came up with the idea about a year before Revision 2023. Over the course of that year, I refined a demo engine, built a live coding environment, composed music, and created visual assets almost entirely on my own.</p><p><img loading="lazy" decoding="async" src="https://6octaves.com/wp-content/uploads/2025/09/ob5_ref1-432x432.webp" alt="0mix reference screen" width="432" height="432" srcset="https://6octaves.com/wp-content/uploads/2025/09/ob5_ref1-432x432.webp 432w, https://6octaves.com/wp-content/uploads/2025/09/ob5_ref1-800x800.webp 800w, https://6octaves.com/wp-content/uploads/2025/09/ob5_ref1-324x324.webp 324w, https://6octaves.com/wp-content/uploads/2025/09/ob5_ref1-768x768.webp 768w, https://6octaves.com/wp-content/uploads/2025/09/ob5_ref1-150x150.webp 150w, https://6octaves.com/wp-content/uploads/2025/09/ob5_ref1.webp 839w" sizes="(max-width: 432px) 100vw, 432px"><br> <span>Here’s the working environment for 0mix. The top screen shows the preview, timeline, etc., while the bottom screen is the code editor. Basically, I spend most of the time in the code editor.</span></p><p> <strong>So, you climbed the highest mountain by yourself. What was the process like?</strong></p><p>It was extremely tough and painful to spend a year working on a challenging 64K project by myself. My advice is to collaborate with others. At the very least, you should find someone you can discuss the progress with. It was indeed fun to surprise many friends at demoparties, but at the end of the day, completing the project is more important.</p><p><strong>You entered 64K compo, but it ended up being released in the PC Demo compo. Did that bother you?</strong></p><p>It’s true that 0mix was released in the PC Demo Compo at Revision 2023. That was because it was the only entry in the PC 64K intro, which wasn’t enough to hold a separate compo. So the two compos were merged. The same thing happened at Revision 2022. <a rel="noopener" href="https://demozoo.org/parties/4424/#competition_17032" target="_blank">PC 64K intro compo was incorporated into the 8K</a> intro compo because there were only two entries. Nevertheless, I’ve always pursued uncompromising quality, so I was down with it. Along with the works of other demo groups (such as Fairlight, mfx, and Still), I think I could contribute to making that compo interesting.</p><p><strong>Ah, you’re right. That felt like a never-ending compo!</strong></p><p>There were so many entries for Revision 2023, and from the chat I got the impression that many participants and viewers were exhausted after the compo. Still, it was a great compo. All of the top works featured demoscene-style visuals built with their own engines, and their narratives were also impressive. So I’m happy with my result. When there’s a big entry in the compo I’m in, I feel more accomplished because it means I helped make that compo exciting together with those great pieces.</p><p><strong>That’s right, I remember some big names rushing in at the end. Nevertheless, this demo stood out for its originality.</strong></p><p>Thank you. Revision has an award called “Crowd Favorite” where viewers can vote for their favorite demo in any category, and 0mix received first prize. 0mix is a piece that reflects what I love, so I felt happy that everyone else enjoyed it, too.</p><div id="attachment_2198"><p><img loading="lazy" decoding="async" aria-describedby="caption-attachment-2198" src="https://6octaves.com/wp-content/uploads/2025/09/ob5_ref3-324x430.webp" alt="0mix crowd favorite" width="324" height="430" srcset="https://6octaves.com/wp-content/uploads/2025/09/ob5_ref3-324x430.webp 324w, https://6octaves.com/wp-content/uploads/2025/09/ob5_ref3-432x573.webp 432w, https://6octaves.com/wp-content/uploads/2025/09/ob5_ref3.webp 632w" sizes="(max-width: 324px) 100vw, 324px"></p><p id="caption-attachment-2198">photo provided by 0b5vr</p></div><p><strong>Congratulations! It was indeed a cool demo.</strong></p><p>Oh, I have a question for you. How do you feel about the code constantly shown in 0mix? What kind of impression does it give you? <br> <span>(Interviewer’s note: I’m not from the programming field. I’m the type of person who chooses a laptop by its color.)</span></p><p><strong>Maybe it’s more like a design or typography? It says “live coding,” so I figured this code is for its visuals, but I have absolutely no idea if the code itself is cool or not. If I didn’t know what live coding is, I’d probably just look at it as part of the design, just like seeing the typography in a language I don’t understand.</strong></p><p>Ah, that’s interesting! Actually, the code displayed on the screen is not for visuals but for music. I use a programming language called GLSL, which is normally used to generate visuals. But 0mix is a live performance-themed demo where I use GLSL for music, and that’s why it’s called “GLSL Techno Live.” If you look at the code closely, you’ll see the parts for instruments, like “KICK,” “HIHAT,” and “BASS.” And by adding and subtracting these elements, I shaped the flow of music.</p><p><strong>Ohh, so that was code for music! But even after knowing this fact, my impression of this piece hasn’t really changed. I guess that shows I interpreted the code as part of the design. Is it okay if a viewer like me sees it that way? (laughs)</strong></p><p><a rel="noopener" href="https://scrapbox.io/0b5vr/Reflection_of_0mix" target="_blank">In my post about this production on Scrapbox</a>, I wrote, <em>“for viewers without coding knowledge, it feels like music-making magic. And for viewers who know programming languages and environments, it’s a hint to guess the next move.”</em> So I expected that some people would see it as part of the design.</p><p><strong>To reveal a bit more about my understanding, now I do understand that “demo is generated from an executable file” and that “a 64K piece has a 64KB file.” But I still don’t see things like “this is real-time rendering, so it’s more impressive than live-action” or “it’s great quality considering this is 64KB.” Basically, I watch demos like I watch music videos, and the only thing that matters to me is whether I find it cool or not.</strong></p><p>Ryoji Ikeda has a work that presents data including planets and genes using 5×5 pixel fonts. Of course, only experts can truly understand such data, so most of us simply enjoy the visual design that comes out of it. Even if we try to find deeper meaning in it, we probably just end up saying something like, “Wow, the world is huge.” I’ve read that Ikeda actually intended for viewers to see it that way.</p><p><strong>Oh, then I’m actually one of his intended viewers. When I first saw his installation video, I knew him as a musician, so I thought, “Wow, that’s his MV? Cool! Very futuristic!” I later realized that it wasn’t just design. It’s nice to know that creators and demosceners expected viewers like me, and personally, I feel relieved. I’d always thought they might be annoyed to hear a clueless person like me commenting on their piece. (laughs)</strong></p><p>To me, how others first got interested in a piece or in the culture is as fascinating as the motives behind its creation. So I do appreciate sceners who are not from the tech side!</p><p><strong>Thank you! That’s really nice and reassuring to hear!<br> OK, let’s go back to that music code. You wrote in your post on Scrapbox that you put a lot of time and effort into the music.</strong></p><p>Actually, I had never really made this type of techno music before, so I watched a lot of live performances of this style and tutorials on YouTube. I also bought and tried hardware for “machine live” performances, like the Elektron Syntakt and Dirtywave M8, for research.</p><p><strong>What is “machine live”?</strong></p><p>“Machine live” is a type of music performance similar to live coding. Performers use music equipment like grooveboxes and modular synths in real-time to control the sound during the performance. What you can do depends on the features of the equipment, so performers always have to be aware of limitations—something somewhat similar to the demoscene. It’s a fascinating culture. There’s even a “DAWless Live” category where you perform without using a DAW, the standard PC-based music production system. For 0mix, I drew a lot of inspiration from the philosophy and methods of machine live and applied them to GLSL live coding. (<a rel="noopener" href="https://www.youtube.com/watch?v=0hs29fAKYJE" target="_blank">Reference video</a>)</p><p><strong>I just watched the reference video you sent me. Does everyone in this scene really use that much gear?</strong></p><p>Of course not. Not everyone uses this much equipment, or equipment of this size, for live performance. Lately, it seems like the palm-sized Dirtywave M8 is trending for live sets. The Dirtywave M8 uses a tracker-style UI, and it’s fun to compose with. Plus, it fits well with the demoscene aesthetic.</p><p>I did a lot of research on machine live and live coding performances, and this gave me ideas about how to create sound and how to evolve live performance. But that only covered the technical side. When it comes to making techno, especially abstract sounds, I had to learn through trial and error and trust my feelings. Even after I learned how to make sounds on standard hardware or software, GLSL follows a completely different set of rules, and I had to be really fired up to tackle it.</p><p><strong>I heard that you did a live performance recently. What kind of event was it?</strong></p><p>I performed a live coding set at “<a rel="noopener" href="https://function-draw.com/tokyo2" target="_blank">draw(tokyo); #2</a>” in March 2025. “draw(); ” is a club event focused on audiovisuals, especially live coding and generative VJ (the so-called “<em>gene-kei</em>” performances). It takes place from time to time in VRChat and at physical venues.</p><p>At draw(tokyo); #2, I performed using Wavenerd, my custom GLSL live coding environment. For my 40-minute live set, I mainly used techno patterns created for 0mix. It was a really memorable experience, since it was my first time doing a live music performance with Wavenerd. I’d love to do more live performances in the future.</p><p><img loading="lazy" decoding="async" src="https://6octaves.com/wp-content/uploads/2025/09/ob5_ref2-432x165.webp" alt="reference screen" width="432" height="165" srcset="https://6octaves.com/wp-content/uploads/2025/09/ob5_ref2-432x165.webp 432w, https://6octaves.com/wp-content/uploads/2025/09/ob5_ref2-800x305.webp 800w, https://6octaves.com/wp-content/uploads/2025/09/ob5_ref2-324x123.webp 324w, https://6octaves.com/wp-content/uploads/2025/09/ob5_ref2-768x293.webp 768w, https://6octaves.com/wp-content/uploads/2025/09/ob5_ref2-1536x585.webp 1536w, https://6octaves.com/wp-content/uploads/2025/09/ob5_ref2.webp 1872w" sizes="(max-width: 432px) 100vw, 432px"></p><p><span>The “Wavenerd” system I used for my live coding performance at draw(tokyo); #2. Since we were chroma keying with VJ visuals, the background is blue. The performers are always lit up in blue.</span></p><p><strong>When a coder does a live music performance, aren’t you too busy typing code in front of the PC to even look at the audience’s reaction?</strong></p><p>During the performances, I rewrite parts of prewritten code, so I don’t need to constantly keep typing. But I’m busy adding and removing parts, changing parameters, and doing some DJ-style mixing, so basically I completely zone in on the screen. That said, I can still see the audience’s reactions to some extent, and I felt really happy when they reacted at the moments I expected.</p><p><strong>Do you know who the primary audience is? I guess this kind of live performance requires some knowledge to really enjoy it.</strong></p><p>I still don’t know what kind of audience it attracts. From what I saw, I got the impression that many of them are interested in musical experiences and visual production at least. But I’m not sure how many are interested in coding, or actually create things with code. How technical it should get, how strictly you stick to the technical restrictions, and how much you make the audience dance—I think performers are expected to balance these elements well. Probably, this is something <em>gene-kei</em> performers constantly have to tackle. In fact, quite a few performers change their set depending on the tone of the event.</p><p><strong>Did you have VJs for your live performance?</strong></p><p>Yes, I asked fellow demosceners, <a rel="noopener" href="https://ukonpower.dev/" target="_blank">ukonpower</a> and <a rel="noopener" href="https://x.com/Renard_VRC" target="_blank">Renard</a>, and they generated visuals that matched the techno. I just told them, “I’m going to do 0mix,” and they both knew what it meant, so everything went very smoothly. (laughs) They created visuals in my style, but their own personalities also shone through. It was really cool.</p><div><p><iframe loading="lazy" title="0b5vr x ukonpower x Renard @ draw(tokyo); #2 | GLSL Techno Live Set" width="1068" height="601" src="https://www.youtube.com/embed/Mmrl-nI8AEo?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p></div><p><strong>Oh, that’s really cool! <br> <a rel="noopener" href="https://www.pouet.net/groups.php?which=15409" target="_blank">According to your discography</a>, you also have 4K as well as 64K works. Is there a reason for that?<br> </strong></p><p>For the 4K intros I’ve released lately, I can usually create them in one or two weeks. But 64K is my soul, so I want to keep making 64K intros. The thing is, 64K requires hundreds of times more work than 4K. So, when I don’t have the time or motivation but still want to contribute to a demoparty, I just make a 4K intro.</p><p>I must say that the production environment for 4K intros is well-supported in the current demoscene. Recently I’ve been using 0x4015’s <a rel="noopener" href="https://github.com/yosshin4004/minimal_gl" target="_blank">minimalGL</a>. With this demotool, I can easily create 4K intros just by writing GLSL. That being said, I wouldn’t recommend it to everyone, because you also have to write the music in GLSL.</p><p>In 2023, I released a 4K intro called “<a rel="noopener" href="https://demozoo.org/productions/331908/" target="_blank">Architectural Shapeshifte</a>r” with Renard. For this piece, Renard was in charge of the concept and visuals, while I was in charge of the music and direction. We used minimalGL for this piece as well. It was the first time for Renard to create a 4K intro, but he was able to create it easily. We collaborated by tweaking the source code on GitHub and communicating via Discord. We exchanged ideas and suggestions on each other’s code, and it turned out to be a very efficient workflow.</p><div><p><iframe loading="lazy" title="&quot;Architectural Shapeshifter&quot; by Renard &amp; 0b5vr (Windows 4KB Intro)" width="1068" height="601" src="https://www.youtube.com/embed/j_6X7Ns0p9Y?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p></div><p>There are many coders who can write GLSL in Japan, but not many of them take on 4K. So I’d love to collaborate more using minimalGL.</p><p><strong>What’s hot in the Japanese demoscene these days? What category is popular? I noticed there was a demoparty called SESSIONS in Japan last year.</strong></p><p>It seems like a lot of people are coming into the demoscene from shader culture centered around VRChat. The people I got to know at demoparties like <a rel="noopener" href="https://sessions-party.com/" target="_blank">SESSIONS</a> were mostly active in VRChat. In particular, the event draw(); seems to have a strong influence, and many of the people who got interested in live coding or generative VJ through draw();’s audiovisual experience also developed an interest in the demoscene.</p><p><strong>Live coding and generative VJ becoming a gateway into the demoscene sounds like a new path to me.</strong></p><p>Yes, indeed. draw();’s main crew, Saina-san, purposefully aims for a crossover with demoscene culture, like SESSIONS, and this accelerates the influx. We’re really grateful for that.</p><p><strong>I’m sure a person like that is supporting the demoscene in Japan and around the world. <br> OK, let’s go back to the production. Is there anything you do in everyday life to get inspired for your creations?</strong></p><p>I check Pouet and Demozoo as much as possible to stay in the know about recent demoscene productions. If I ever stopped checking Pouet and Demozoo, I think that would be the end of me as a demoscener.</p><p>I also try to take in other cultures as well. Recently, I’ve been fascinated by the flashy audiovisual productions in pachinko and pachislot machines. They use dazzling visuals and music to stir up the spirit of gambling. These productions thoroughly pursue how to exploit the human reward system, all within machines that operate under very strict legal restrictions. In a way, I think this represents the highest peak of visual entertainment.</p><p>I also go for walks frequently. Especially walking around Tokyo late at night gives me a strong sense of urban life and social activity, and it inspires me a lot. “<a rel="noopener" href="https://www.pouet.net/prod.php?which=90435" target="_blank">Domain</a>“, a 64K intro I released at Tokyo Demo Fest 2021, was heavily inspired by night Tokyo. I find the concept of the night city very interesting, and I’d like to explore it further.</p><div><p><iframe loading="lazy" title="&quot;Domain&quot; by 0b5vr, WebGL 64KB Intro (Final)" width="1068" height="601" src="https://www.youtube.com/embed/D2COWeeEqTs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p></div><p><strong>Which areas do you usually walk around?</strong></p><p>I mainly walk around downtown. I can feel the rhythm of social activity through people’s movements, clothing, and buildings. It’s also very fun to walk around residential areas. When I imagine that this is someone’s everyday life, I can sense their presence through the scenery.</p><p><strong>Do you have anything you always keep in mind when you create, like a routine or your own personal rule?</strong></p><p>For my demo source code, I use Git for version control and share as much of the code as possible on GitHub. Basically, I publish my source code under the <a rel="noopener" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">Creative Commons BY-NC 4.0</a> license, and users can adapt and use it freely for non-profit purposes. By publishing my source code, I allow other people to refer to my production methods. In fact, I’ve often heard that people have made demos based on my code. Getting more chances to discover other demosceners’ great works is valuable for me too, so I’ll continue to publish my source code.</p><p>Also, when I do version control on Git, I try to write commit logs—comments you can add to each version—as detailed as possible. Commit logs explain which part of the code I changed, and they also serve as a kind of production journal. In addition to information like what type of change I made and for what purpose, they help me recall my state of mind or what I was thinking during the creative process.</p><p><strong>For programmers, is it a hassle to write detailed commit logs?</strong></p><p>Commit logs aren’t considered a direct contribution to a program, just like READMEs or documentation. So, engineers who want to focus on coding and dislike communicating often don’t write them at all. Usually, detailed commit logs are recommended when you work with other people on business projects. However, even for a one-off piece of code written by a single person, I think we should consider how detailed we make the commit logs, because someone else—or even yourself—may end up reading them like archaeology.</p><p><strong>Archaeology… that’s interesting. <br> Okay, let me go to the classic question: your favorite demo, a memorable demo, or a demo that changed your life… anything. Tell us about a demo, or demos that are special to you.</strong></p><p>As I mentioned, “<a rel="noopener" href="https://www.youtube.com/watch?v=A5PqEePIghE" target="_blank">Emix</a>” by Epoch is the demo I like the most. From the theme of each effect to the color grading, glitch effects, music, and direction, this piece defined what a demo should have, for me. Other pieces that helped define my standards include “<a rel="noopener" href="https://www.youtube.com/watch?v=RCh3Q08HMfs" target="_blank">cdak</a>” by Quite &amp; Orange, “<a rel="noopener" href="https://www.youtube.com/watch?app=desktop&amp;v=mTi0UppWnH8" target="_blank">Transformer 3</a>” by Limp Ninja, and “<a rel="noopener" href="https://www.youtube.com/watch?v=O3T1-nadehU" target="_blank">Clean Slate</a>” by Conspiracy. I put them together in my <a rel="noopener" href="https://www.pouet.net/lists.php?which=242" target="_blank">Pouet playlist “0b5vr’s bible”</a>, if you’re interested.</p><p><strong>Among many other forms of self-expression, why did you choose the demoscene? Or are you trapped by this culture? Tell me what’s so attractive about it.</strong></p><p>The demoscene is a creative activity free from art as a capital asset or from commercial value. We mostly create and present pieces in a format that has little value in today’s society, and we purely inspire one another’s technical curiosity and the craving for expression. Also, the demoscene ecosystem is cooperative. Anyone can access demotools, ask questions to veterans, and start creating a piece. I respect the works, workflows, and ideas of active demosceners in the community, and that’s what motivates me to create something that earns their recognition.</p><p>On the other hand, due to the methods used in the demoscene, a lot of pieces look similar, and that’s clearly a weak point of the scene. If I only keep exploring the demoscene, I can’t expand my range of expression. As a creator, I think it’s important to look at various cultures and absorb many different methods of expression. The easy exchange of fresh inspiration is one of the features of the demoscene, so I’d like to take in many forms of expression both inside and outside the scene, and keep inspiring each other.</p><p><strong>Is there anything you want to do in the future?</strong></p><p>What I want to do most is live music performance using GLSL, as I mentioned. Seemingly, this format of live music with GLSL is currently performed only by me and “<a rel="noopener" href="https://x.com/lactoice251" target="_blank">Rakuto-ice</a>” san. So I want to perform more to develop my style further, and I hope more people will enjoy it.</p><p>And of course, I want to create demos like 64K, but right now I don’t have enough motivation or ideas. To find more motivation and inspiration, I think it’s about time I formed a demogroup.</p><p><strong>Sounds like there’s much to look forward to! </strong><br> <strong>Finally, your message for demosceners and demo fans out there, please.</strong></p><p><em>For those of you who are not yet demosceners:</em> <br> I’ve seen many people who have an interest in the demoscene but also fears about the culture itself. And it’s not just Japanese people, people in other countries have reacted that way too. Please don’t be afraid of us. If you are interested in creating something with a computer and having fun at a demoparty, then you are a demoscener. Whether you already have a medium of expression or not, if you join the party, you may naturally feel inspired to think, “I want to express myself too.” Demoparties like Tokyo Demo Fest, SESSIONS, and Revision have various compos, including simple programs, illustration, photography, music, along with the demo compo. Of course, if you want to create a demo, fellow creators will help you. We demosceners hope you will have fun in this scene.</p><p><em>For those who are already demosceners (including me):</em> <br> Make 64K!</p><hr><p>Thank you very much for answering my question, <a rel="noopener" href="https://0b5vr.com/" target="_blank">0b5vr</a>!</p><p><a rel="noopener" href="https://0b5vr.com/" target="_blank">0b5vr</a>’s works can be found on <a rel="noopener" href="https://www.pouet.net/groups.php?which=15409" target="_blank">Pouet</a> and <a rel="noopener" href="https://demozoo.org/sceners/128318/" target="_blank">Demozoo</a>. Also, be sure to check his essay on the production of 0mix on <a rel="noopener" href="https://scrapbox.io/0b5vr/Reflection_of_0mix" target="_blank">Scrapbox</a>, where he goes deeper into his thoughts on the demoscene and the creative process.</p><p>Thank you very much for reading this to the end!</p><p>—————-</p><p><span lang="EN-US">In case you’re wondering what “demo” or “demoscene” is, better check out&nbsp;<a rel="noopener" href="http://6octaves.com/2012/10/moleman2-demoscene-art-of-algorithms_30.html" target="_blank">the well-made documentary called Moleman2</a>.&nbsp;&nbsp;(and the director, M.&nbsp;Szilárd Matusik’s&nbsp;interview can be read in&nbsp;<a rel="noopener" href="http://6octaves.com/2012/10/interview-with-szilard-matusik-director.html" target="_blank">here</a>.)</span>&nbsp;</p><p>#1: q from nonoil/gorakubu is&nbsp;<a rel="noopener" href="http://6octaves.com/2013/01/interview-with-japanese-demoscener-q.html" target="_blank">here</a>.&nbsp;<br> #2: Gargaj from Conspiracy,&nbsp;Ümlaüt Design is&nbsp;<a rel="noopener" href="http://6octaves.com/2013/03/interview-with-demoscener-gargaj.html" target="_blank">here</a>.<span lang="EN-US">&nbsp;<br> #3: Preacher from Brainstorm, Traction is&nbsp;<a rel="noopener" href="http://6octaves.com/2013/05/interview-with-demoscener-preacher.html" target="_blank">here</a>.</span><span lang="EN-US">&nbsp;<br> #4: Zavie from&nbsp;Ctrl-Alt-Test is&nbsp;<a rel="noopener" href="http://6octaves.com/2013/08/interview-with-demoscener-zavie-ctrl.html" target="_blank">here</a>.</span><span lang="EN-US">&nbsp;<br> #5: Smash from Fairlight is&nbsp;<a rel="noopener" href="http://6octaves.com/2013/10/interview-with-demoscener-smash.html" target="_blank">here</a>.</span><span lang="EN-US">&nbsp;<br> #6: Gloom from Excess, Dead Roman is&nbsp;<a rel="noopener" href="http://6octaves.com/2013/12/interview-with-demoscener-gloom-excess.html" target="_blank">here</a>.</span><span lang="EN-US">&nbsp;<br> #7: kioku from System K is&nbsp;<a rel="noopener" href="http://6octaves.com/2014/02/interview-with-japanese-demoscener.html" target="_blank">here</a>.</span><span lang="EN-US">&nbsp;<br> #8: kb from Farbrausch is&nbsp;<a rel="noopener" href="http://6octaves.com/2014/07/interview-with-demoscener-kb-farbrausch.html" target="_blank">here</a>.</span><span lang="EN-US">&nbsp;<br> #9: iq from RGBA is&nbsp;<a rel="noopener" href="http://6octaves.com/2014/09/interview-with-demoscener-iq-rgba.html" target="_blank">here</a>.<br> </span><span lang="EN-US">#10:&nbsp;Navis from Andromeda Software Development is&nbsp;<a rel="noopener" href="http://6octaves.com/2015/04/interview-navis-asd.html" target="_blank">here</a>.<br> </span><span lang="EN-US">#11:&nbsp;Pixtur from&nbsp;Still, LKCC&nbsp;is&nbsp;<a rel="noopener" href="http://6octaves.com/2015/06/interview-with-demoscener-pixtur-still.html" target="_blank">here</a>.<br> </span><span lang="EN-US">#12:&nbsp;Cryptic from&nbsp;Approximate&nbsp;is&nbsp;<a rel="noopener" href="http://6octaves.com/2016/01/interview-with-demoscener-cryptic.html" target="_blank">here</a>.<br> </span><span lang="EN-US">#13: 0x4015 aka Yosshin is&nbsp;<a href="https://6octaves.com/2017/06/interview-with-japanese-demoscener.html" target="_blank">here</a>.<br> </span>#14:&nbsp;Flopine from Cookie Collective is&nbsp;<a href="https://6octaves.com/2021/03/interview-with-demoscener-flopine.html" target="_blank">here</a>.<span lang="EN-US">&nbsp;<br> #15: noby from Epoch, Prismbeings is <a href="https://6octaves.com/2023/08/interview-with-demoscener-noby-epoch.html" target="_blank">here</a>.</span></p><p><span lang="EN-US">Why I’m interested in demoscene is explained in&nbsp;<a href="https://6octaves.com/2018/11/who-is-demoscener-what-ive-learned-from.html" target="_blank">this article</a>.<br> And for some of my other posts related to “demo and “demoscene” culture is&nbsp;<a href="https://6octaves.com/demoscene" target="_blank">here</a>.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I bought the cheapest EV, a used Nissan Leaf (149 pts)]]></title>
            <link>https://www.jeffgeerling.com/blog/2025/i-bought-cheapest-ev-used-nissan-leaf</link>
            <guid>45136103</guid>
            <pubDate>Fri, 05 Sep 2025 07:57:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jeffgeerling.com/blog/2025/i-bought-cheapest-ev-used-nissan-leaf">https://www.jeffgeerling.com/blog/2025/i-bought-cheapest-ev-used-nissan-leaf</a>, See on <a href="https://news.ycombinator.com/item?id=45136103">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img src="https://www.jeffgeerling.com/sites/default/files/images/jeff-buys-nissan-leaf-sv-plus.jpg" width="350" height="330" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-a463435a-cfd8-4cbb-976e-31ecec4fadb8" data-insert-attach="{&quot;id&quot;:&quot;a463435a-cfd8-4cbb-976e-31ecec4fadb8&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="Jeff Geerling with Nissan Leaf SV Plus at Dealership"></p>

<p>I bought a used 2023 Nissan Leaf in 2025, my first 'new' car in 15 years. The above photo was taken by the dealership; apparently their social media team likes to post photos of all purchasers.</p>

<p>I test drove a Tesla in 2012, and quickly realized my mistake. No gasoline-powered car (outside of supercars, maybe? Never drove one of those) could match the feel of pressing the throttle on an electric.</p>

<p>I started out with a used minivan, which I drove into the ground. Then I bought a used Olds that I drove into the ground. Then I bought a used Camry that I bought before we had kids, when I had a 16 mile commute.</p>

<p>Fast forward about 15 years, and I found myself with a very short commute, only driving a few miles a day, and a family minivan we use for nearly all the 'driving around the kids' stuff.</p>

<p>So I wanted a smaller car (get back a foot or so of garage space...) that was also more efficient.</p>

<h2>Video and GitHub EV Project</h2>

<p>If you don't like reading blog posts (why are you here?), I also posted a video going over most of this, with a little more color, on my YouTube channel:</p>

<div>
<p><iframe src="https://www.youtube.com/embed/hQQtFnLefqw" frameborder="0" allowfullscreen=""></iframe></p>
</div>

<p>Also, this blog post is also the centerpiece of my new GitHub project <a href="https://github.com/geerlingguy/electric-car">geerlingguy/electric-car</a>, where I detail all the steps on my nascent EV journey.</p>

<h2>Equipment and Add-ons</h2>

<p>Before I go further, I thought I'd mention some of the things I've added to my Leaf to make the EV experience a little nicer (some links are Amazon affiliate links. I earn for qualifying referrals):</p>

<ul>
<li><a href="https://amzn.to/4k0tHwX">Grizzl-E Level 2 Charger</a> for the garage (see <a href="https://github.com/geerlingguy/electric-car/issues/5">Issue #5</a>)</li>
<li><a href="https://amzn.to/3HIt2Tx">Lectron L1 J1772 EV charger</a> for a more portable charger, when I just need to top off the car for a few hours</li>
<li><a href="https://amzn.to/4l8f4ct">J1772 Wall mount for cable and plug</a> - I was going to 3D print one, but figured the metal product would hold up better in a garage in the midwest</li>
<li><a href="https://amzn.to/3T1PQjw">NACS to J1772</a> AC L1/L2 charging adapter</li>
<li><a href="https://a2zevshop.com/products/ccs1-to-chademo">CCS1 to CHAdeMO</a> L3 DC Fast charge adapter (see <a href="https://github.com/geerlingguy/electric-car/issues/9">Issue #9</a>)</li>
<li><a href="https://amzn.to/3SW9AVH">CarlinKit 5.0 Wireless CarPlay/Android Auto adapter</a> because the Leaf only supports wired CarPlay by default</li>
<li><a href="https://amzn.to/4n8HqEB">VIOFO A119 Mini Dashcam</a> with a <a href="https://amzn.to/44hbi9f">Dongar wiring harness adapter</a> (see <a href="https://github.com/geerlingguy/electric-car/issues/3">Issue #3</a>)</li>
</ul>

<h2>Monitoring the Battery</h2>

<p>If you're considering a used Leaf, or if you have a Leaf already, it's a good idea to keep tabs on the battery health, especially since the meter on your dash is painfully basic in how much data it provides.</p>

<p>Individual cell charge, 'State of Health' of the overall battery, and much more are available through the car's OBD-II port.</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/lelink-2-obd-ii-leaf.jpg" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-e0e6e96f-13e0-4309-b60a-30ddc5d90611" data-insert-attach="{&quot;id&quot;:&quot;e0e6e96f-13e0-4309-b60a-30ddc5d90611&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="LeLink 2 OBD-II adapter"></p>

<p>Soon after I bought my Leaf, I ordered a <a href="https://amzn.to/45GYUls">LeLink 2</a> ($35) and bought the <a href="https://leafspy.com/">LeafSpy Pro</a> App for my iPhone ($20).</p>

<p>I plugged the LeLink 2 into the OBD-II diagnostics port under the steering column, and fired up LeafSpy Pro. It gives me some helpful metrics like:</p>

<blockquote>
  <ul>
  <li>SOH: State of Health</li>
  <li>Hx: Conductance</li>
  </ul>
</blockquote>

<p>See <a href="https://github.com/geerlingguy/electric-car/issues/8">Issue #8: Document battery health</a> for all my notes monitoring my own Leaf's battery. But bottom line, my battery showed a 93.16% 'SoH' (State of Health), meaning it still has most of its capacity.</p>

<p>I've been reading up on various forums about managing the Leaf's battery, and am trying to do some things to extend the battery's life as long as possible:</p>

<ul>
<li>Limiting the number of QCs (Quick Charges / DC Fast Charge), as this heats up the uncooled Leaf battery, degrading it slightly each time, especially on hotter days</li>
<li>Keeping the charge between 50-80% when manageable</li>
<li>Charging up to 100% at least once a month, and letting it 'top off' to rebalance the pack for at least a few hours afterwards</li>
<li>Not driving like a maniac, despite having more torque in this car than I've ever had in any of my previous cars</li>
</ul>

<h2>Why buy electric?</h2>

<p>I overanalyze most things, so had been researching this purchase for about a decade now.</p>

<p>With EVs there are tradeoffs. Even in my situation, only driving a car a few miles a day, I <em>do</em> take my car on one or two regional road trips every year.</p>

<p>Having the ability to hop in at 6 am and be in Chicago or KC by late morning is nice. Having to plan a long break somewhere halfway to charge is not.</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/nissan-leaf-at-evgo-dc-fast-charger.jpg" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-69b2a922-baa6-4ac5-8200-da5c90a7ba66" data-insert-attach="{&quot;id&quot;:&quot;69b2a922-baa6-4ac5-8200-da5c90a7ba66&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="Nissan Leaf at EVGo DC fast charger"></p>

<p>But if I only take that trip once a year, I can either (a) rent a gas car that gets me there a little more quickly, and ensures I don't have to find a spot in the destination city to do a full charge before the return trip. Or (b) plan for an extra X hours total during the trip to ensure I have padding for charging.</p>

<p>Charging infrastructure's improving in the US (and in many parts of the world), but it's nowhere near as ubiquitous as gas stations.</p>

<p>Hopefully this improves over time, but for now, I plan on using the electric car for local travel, likely only going more than 100 miles or so in a day once or twice a year.</p>

<h2>Why buy Leaf?</h2>

<p>Price.</p>

<p>That's mostly it. And I drove a Nissan Sentra rental on a recent trip, and realized Nissan isn't half bad. They seem to not require an Internet connection for their cars, they offer basic lane following and adaptive cruise control, they have CarPlay/Android Auto...</p>

<p>The Leaf ticks all the little 'convenience' checkboxes, but is also not 'extravagant'.</p>

<p>And the later model years also aren't "look at me I drive an EV" ugly (though they're not amazing-looking, either).</p>

<p>But I drove a minivan, an olds, and a Camry, so obviously I'm function &gt; form when it comes to my car!</p>

<p>Because of the smaller battery (and up until 2026, a battery with no active cooling), combined with the use of a DC fast charging connector (CHAdeMO) that's going out of style in the US, used Nissan Leafs are priced <em>considerably</em> lower than competitors.</p>

<p>Well, all except maybe Teslas around a year or two older right now. But Teslas don't have native CarPlay. And I'm not a fan of how Tesla is trying to turn the car into some kind of appliance, RoboTaxi, self-driving thing, versus it being a transportation vehicle that I can do what I want with.</p>

<p>No judgement on Tesla owners, the used Tesla market was enticing at the time I bought the Leaf.</p>

<p>I also looked a lot at the Hyundai Ioniq and Kona; both were just a <em>little</em> bit too large for my liking, but they could've worked. The problem was used models in good condition were a lot more expensive than I was willing to pay.</p>

<p>So back to the Leaf: Nissan's probably not the <em>best</em> right now when it comes to EVs and features, but they're certainly the <em>cheapest</em>. And 'good enough' is fine by me.</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/nissan-leaf-ev-interior.jpeg" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-af9f7f15-eae6-4aff-ba12-43507758d0df" data-insert-attach="{&quot;id&quot;:&quot;af9f7f15-eae6-4aff-ba12-43507758d0df&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="Nissan Leaf interior driver's side"></p>

<p><em>She's got it where it counts, kid.</em></p>

<h2>Gripes about my Leaf</h2>

<p>There are a few things that baffle me about the Leaf, some that have been frustrating from the first test drive; others that are more subtle:</p>

<ul>
<li><strong>There is no 'play/pause' button.</strong> Anywhere. At least not on the steering wheel or the display area. You have to go into the music section on the entertainment display, <em>then</em> press the software play/pause button. That's dumb. I've resorted to just turning Audio on/off using the volume knob, which accomplishes the same goal but is not always ideal.</li>
<li><strong>Going into 'Neutral' is an exercise in frustration.</strong> I thought you just put your foot on the brake and move the shifter knob to the left. But you have to do it with the right timing, I think.</li>
<li><strong>There's no way to open the tailgate short of pressing the release button.</strong> At least as far as I'm aware. There's no button in the cabin or key fob to unlatch it. The manual says the other way to open it is with a screwdriver, from inside the car, pushing on the latch (lol). <a href="https://mynissanleaf.com/threads/leaf-doesnt-have-an-interior-trunk-release.5026/">I'm not alone here</a>. At least there's a button on the remote to open the charge port.</li>
</ul>

<h2>The joy of electric</h2>

<p>I don't care about engine noise. I appreciate it, though. My brother had a 1992 Forumula Firebird. And I nearly owned it after he moved away, instead of my Olds! (But I'm a boring-car person, so I think I was happier with the Olds).</p>

<p>The nice things about electric vehicles that swayed me in their favor, in descending order:</p>

<ul>
<li><strong>One pedal driving</strong> Seriously, why doesn't every EV have this mode? It makes driving one feel SO much better than any gas car, in terms of connection between driver and car movement.</li>
<li><strong>Sprightly torque</strong>: Outside of exotic tiny gas cars, you're not going to get the same zip even a cheap EV like a Leaf gives you—smash the accelerator in non-Eco mode and any passenger will giggle, every time.</li>
<li><strong>Blissful quiet</strong>: Though some cars have annoying noises (Nissan calls this VSP, or "Vehicle Sound for Pedestirians") they play at low speeds.</li>
<li><strong>Lower maintenance requirements</strong>: I hate every time I have to jack up my car and change the brakes, or take it in for oil/fluid changes. EVs (usually) require less maintenance, besides maybe tires.</li>
<li><strong>Conveniences</strong>: Like running climate control to cool down/heat up the car prior to hopping in, even while it's in the garage! Or plugging it in to charge at home, and not having to stop by a gas station.</li>
<li><strong>Long-term economics</strong>: in <em>general</em>, charging with electricity, at least here in St. Louis, is cheaper than filling up with gas, on a dollar-per-mile basis.</li>
</ul>

<h2>The pain of electric</h2>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/charge-connectors-ev-usa.jpg" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-ee2d06ba-d283-47da-9881-f9bc6cba9f11" data-insert-attach="{&quot;id&quot;:&quot;ee2d06ba-d283-47da-9881-f9bc6cba9f11&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="CHAdeMO, CCS-1, and NACS charge port diagrams"></p>

<p>All that said, I knew going into this there would be some pain. Maybe in 10 or 20 years these things will get solved, but off the top of my head:</p>

<ul>
<li><strong>Price</strong>: The Leaf (especially used, right now) is the cheapest, but it is by no means <em>cheap</em>. It takes a few years to break even with a similarly-specced gas car. But buying a gas car, you have a lot more options on the low-low end.</li>
<li><strong>Range Anxiety</strong>: Yes, it's overblown, but no, it's not non-existent. The day I <em>bought</em> my used EV, the dealership (which doesn't sell many EVs, even new) didn't have a 'Level 3' DC fast charger—and they had only charged it to about 16%. Letting it top off at L2 while I was dealing with finance, we got to 23%. I wasn't quite sure I'd make it home off the lot! Luckily I did, with 12 miles of range remaining. Road tripping or day trips require more planning when driving an EV.</li>
<li><strong>Lack of standards</strong>: For 'L3' DC Fast Charging, the Leaf has a CHAdeMO port. Teslas and many newer EVs have NACS. Then there's CCS1 and CCS2. And charging stations are run by multiple vendors with multiple apps and payment methods. It's not like gas stations, like with Shell, BP, Buckee's, etc. where you just drive up, stick the gas nozzle in your tank, and squeeze. Even adapters can be complicated and annoying, and many EV charging stations only support one or two standards—and some may only have <em>one</em> CHAdeMO plug, and that plug may have been ripped off the unit to be scrapped by a copper thief!</li>
<li><strong>Lack of standards, part 2</strong>: For L1/L2 charging, some cars use J1772, some use NACS... and then wall charging units are all over the board with supporting 6, 12, or 16 Amps for L1 (they shouldn't do 16 on a 15A circuit but it seems like some do!), or various different amperages for L2. Some of these units require apps to configure them, others have dip switches, and yet others are not configurable, and don't list their exact specs in an easy-to-find location. Usually forum posts from users who <em>buy</em> the chargers offer more information than product manufacturers' own websites!</li>
<li><strong>Being an EV</strong>: For some reason, most EVs look like... EVs. I honestly was holding out hope Tesla would just make a Corolla, but an EV version. All the cheap EVs like the Bolt, i3, Leaf, etc. just look... sorta ugly. Subjective, sure, but at least my Olds looked kinda sleek. Even if it was an Olds. EVs stand out, and that I don't enjoy. I want an EV that looks like a Camry. Just blend in and don't stand out.</li>
<li><strong>Cables and chargers</strong>: The Leaf has slightly less trunk space than my slightly-larger Camry. I didn't realize how big L1/L2 charge cables are. Even L1-only cables (which charge at a very anemic pace, like 10 miles / hour of charge) are fairly thick, bulky affairs. About 1/10 of my trunk is devoted to my charging cable. And on a road trip, I will likely carry my <a href="https://amzn.to/3T1PQjw">NACS to J1772</a> and <a href="https://a2zevshop.com/products/ccs1-to-chademo">CCS1 to CHAdeMO</a> adapters. And the latter adapter includes its <em>own</em> battery (that has to be charged) and firmware (that might need to be updated)!</li>
</ul>

<h2>Further Reading</h2>

<p>Be sure to check the <a href="https://github.com/geerlingguy/electric-car/issues">Issues</a> in my GitHub project for more of my EV adventures.</p>

<p>I don't plan on becoming an EV advocate by any means.</p>

<p>The Leaf is the perfect option for me, but I wouldn't recommend an EV for most car owners yet, especially considering the price disparity and infrastructure requirements that exclude large swaths of the population!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fil's Unbelievable Garbage Collector (532 pts)]]></title>
            <link>https://fil-c.org/fugc</link>
            <guid>45133938</guid>
            <pubDate>Fri, 05 Sep 2025 00:55:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fil-c.org/fugc">https://fil-c.org/fugc</a>, See on <a href="https://news.ycombinator.com/item?id=45133938">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
        <main>
<h2>Fil's Unbelievable Garbage Collector</h2>

<p>Fil-C uses a <em>parallel concurrent on-the-fly grey-stack Dijkstra accurate non-moving</em> garbage collector called FUGC (Fil's Unbelievable Garbage Collector). You can find the source code for the collector itself in <a href="https://github.com/pizlonator/fil-c/blob/deluge/libpas/src/libpas/fugc.c">fugc.c</a>, though be warned, that code cannot possibly work without lots of support logic in the rest of the runtime and in the compiler.</p>

<p>Let's break down FUGC's features:</p>

<ul>
<li><p>Parallel: marking and sweeping happen in multiple threads, in parallel. The more cores you have, the
faster the collector finishes.</p></li>
<li><p>Concurrent: marking and sweeping happen on some threads other than the <em>mutator</em> threads (i.e. your
program's threads). Mutator threads don't have to stop and wait for the collector. The interaction
between the collector thread and mutator threads is mostly non-blocking (locking is only used on
allocation slow paths).</p></li>
<li><p>On-the-fly: there is no global stop-the-world, but instead we use
"soft handshakes" (aka "ragged safepoints"). This means that the GC may ask threads to do some work (like scan stack), but threads do this
asynchronously, on their own time, without waiting for the collector or other threads. The only "pause"
threads experience is the callback executed in response to the soft handshake, which does work bounded
by that thread's stack height. That "pause" is usually shorter than the slowest path you might take
through a typical <code>malloc</code> implementation.</p></li>
<li><p>Grey-stack: the collector assumes it must rescan thread stacks to fixpoint. That is, GC starts with
a soft handshake to scan stack, and then marks in a loop. If this
loop runs out of work, then FUGC does another soft handshake. If that reveals more objects, then
concurrent marking resumes. This prevents us from having a <em>load barrier</em> (no instrumentation runs
when loading a pointer from the heap into a local variable). Only a <em>store barrier</em> is
necessary, and that barrier is very simple. This fixpoint converges super quickly because all newly
allocated objects during GC are pre-marked.</p></li>
<li><p>Dijkstra: storing a pointer field in an object that's in the heap or in a global variable while FUGC
is in its marking phase causes the newly pointed-to object to get marked. This is called a <em>Dijkstra
barrier</em> and it is a kind of <em>store barrier</em>. Due to the grey stack, there is no load barrier like
in the <a href="https://lamport.azurewebsites.net/pubs/garbage.pdf">classic Dijkstra collector</a>. The FUGC store
barrier uses a compare-and-swap with relaxed memory ordering on the slowest path (if the GC is running
and the object being stored was not already marked).</p></li>
<li><p>Accurate: the GC accurately (aka precisely, aka exactly) finds all pointers to objects, nothing more,
nothing less. <code>llvm::FilPizlonator</code> ensures that the runtime always knows where the root pointers are
on the stack and in globals. The Fil-C runtime has a clever API and Ruby code generator for tracking
pointers in low-level code that interacts with pizlonated code. All objects know where their outgoing
pointers are - they can only be in the <a href="https://fil-c.org/invisicaps.html">InvisiCap</a> auxiliary allocation.</p></li>
<li><p>Non-moving: the GC doesn't move objects. This makes concurrency easy to implement and avoids
a lot of synchronization between mutator and collector. However, FUGC will "move" pointers to free
objects (it will repoint the <a href="https://fil-c.org/invisicaps.html">capability</a> pointer to the free singleton so it doesn't have to mark the
freed allocation).</p></li>
</ul>

<p>This makes FUGC an <em>advancing wavefront</em> garbage collector. Advancing wavefront means that the
mutator cannot create new work for the collector by modifying the heap. Once an
object is marked, it'll stay marked for that GC cycle. It's also an <em>incremental update</em> collector, since
some objects that would have been live at the start of GC might get freed if they become free during the
collection cycle.</p>

<p>FUGC relies on <em>safepoints</em>, which comprise:</p>

<ul>
<li><p><em>Pollchecks</em> emitted by the compiler. The <code>llvm::FilPizlonator</code> compiler pass emits pollchecks often enough that only a
bounded amount of progress is possible before a pollcheck happens. The fast path of a pollcheck is
just a load-and-branch. The slow path runs a <em>pollcheck callback</em>, which does work for FUGC.</p></li>
<li><p>Soft handshakes, which request that a pollcheck callback is run on all threads and then waits for
this to happen.</p></li>
<li><p><em>Enter</em>/<em>exit</em> functionality. This is for allowing threads to block in syscalls or long-running
runtime functions without executing pollchecks. Threads that are in the <em>exited</em> state will have
pollcheck callbacks executed by the collector itself (when it does the soft handshake). The only
way for a Fil-C program to block is either by looping while entered (which means executing a
pollcheck at least once per loop iteration, often more) or by calling into the runtime and then
exiting.</p></li>
</ul>

<p>Safepointing is essential for supporting threading (Fil-C supports pthreads just fine) while avoiding
a large class of race conditions. For example, safepointing means that it's safe to load a pointer from
the heap and then use it; the GC cannot possibly delete that memory until the next pollcheck or exit.
So, the compiler and runtime just have to ensure that the pointer becomes tracked for stack scanning at
some point between when it's loaded and when the next pollcheck/exit happens, and only if the pointer is
still live at that point.</p>

<p>The safepointing functionality also supports <em>stop-the-world</em>, which is currently used to implement
<code>fork(2)</code> and for debugging FUGC (if you set the <code>FUGC_STW</code> environment variable to <code>1</code> then the
collector will stop the world and this is useful for triaging GC bugs; if the bug reproduces in STW
then it means it's not due to issues with the store barrier). The safepoint infrastructure also allows
safe signal delivery; Fil-C makes it possible to use signal handling in a practical way. Safepointing is
a common feature of virtual machines that support multiple threads and accurate garbage collection,
though usually, they are only used to stop the world rather than to request asynchronous activity from all
threads. See <a href="https://foojay.io/today/the-inner-workings-of-safepoints/">here</a> for a write-up about
how OpenJDK does it. The Fil-C implementation is in <a href="https://github.com/pizlonator/fil-c/blob/deluge/libpas/src/libpas/filc_runtime.c"><code>filc_runtime.c</code></a>.</p>

<p>Here's the basic flow of the FUGC collector loop:</p>

<ol>
<li>Wait for the GC trigger.</li>
<li>Turn on the store barrier, then soft handshake with a no-op callback.</li>
<li>Turn on black allocation (new objects get allocated marked), then soft handshake with a callback
that resets thread-local caches.</li>
<li>Mark global roots.</li>
<li>Soft handshake with a callback that requests stack scan and another reset of thread-local caches.
If all collector mark stacks are empty after this, go to step 7.</li>
<li>Tracing: for each object in the mark stack, mark its outgoing references (which may grow the mark
stack). Do this until the mark stack is empty. Then go to step 5.</li>
<li>Turn off the store barrier and prepare for sweeping, then soft handshake to reset thread-local
caches again.</li>
<li>Perform the sweep. During the sweep, objects are allocated black if they happen to be allocated out
of not-yet-swept pages, or white if they are allocated out of alraedy-swept pages.</li>
<li>Victory! Go back to step 1.</li>
</ol>

<p>If you're familiar with the literature, FUGC is sort of like the DLG (Doligez-Leroy-Gonthier) collector
(published in <a href="https://xavierleroy.org/publi/concurrent-gc.pdf">two</a>
<a href="http://moscova.inria.fr/~doligez/publications/doligez-gonthier-popl-1994.pdf">papers</a> because they
had a serious bug in the first one), except it uses the Dijkstra barrier and a grey stack, which
simplifies everything but isn't as academically pure (FUGC fixpoints, theirs doesn't). I first came
up with the grey-stack Dijkstra approach when working on
<a href="http://www.filpizlo.com/papers/pizlo-eurosys2010-fijivm.pdf">Fiji VM</a>'s CMR and
<a href="http://www.filpizlo.com/papers/pizlo-pldi2010-schism.pdf">Schism</a> garbage collectors. The main
advantage of FUGC over DLG is that it has a simpler (cheaper) store barrier and it's a slightly more
intuitive algorithm. While the fixpoint seems like a disadvantage, in practice it converges after a few
iterations.</p>

<p>Additionally, FUGC relies on a sweeping algorithm based on bitvector SIMD. This makes sweeping insanely
fast compared to marking. This is made thanks to the
<a href="https://github.com/pizlonator/fil-c/blob/deluge/libpas/src/libpas/verse_heap.h">Verse heap config</a>
that I added to
<a href="https://github.com/WebKit/WebKit/blob/main/Source/bmalloc/libpas/Documentation.md">libpas</a>. FUGC
typically spends &lt;5% of its time sweeping.</p>

<h2>Bonus Features</h2>

<p>FUGC supports a most of C-style, Java-style, and JavaScript-style memory management. Let's break down what that means.</p>

<h3>Freeing Objects</h3>

<p>If you call <code>free</code>, the runtime will flag the object as free and all subsequent accesses to the object will trap. Additionally, FUGC will not scan outgoing references from the object (since they cannot be accessed anymore).</p>

<p>Also, FUGC will redirect all capability pointers (<em>lower</em>s in <a href="https://fil-c.org/invisicaps.html">InvisiCaps</a> jargon) to free objects to point at the free singleton object instead. This allows freed object memory to really be reclaimed.</p>

<p>This means that freeing objects can be used to prevent <em>GC-induced leaks</em>. Surprisingly, a program that works fine with <code>malloc</code>/<code>free</code> (no leaks, no crashes) that gets converted to GC the naive way (<code>malloc</code> allocates from the GC and <code>free</code> is a no-op) may end up leaking due to dangling pointers that the program never accesses. Those dangling pointers will be treated as live by the GC. In FUGC, if you freed those pointers, then FUGC will really kill them.</p>

<h3>Finalizers</h3>

<p>FUGC supports finalizer queues using the <code>zgc_finq</code> API in <a href="https://github.com/pizlonator/fil-c/blob/deluge/filc/include/stdfil.h">stdfil.h</a>. This feature allows you to implement finalizers in the style of Java, except that you get to set up your own finalizer queues and choose which thread processes them.</p>

<h3>Weak References</h3>

<p>FUGC supports weak references using the <code>zweak</code> API in <a href="https://github.com/pizlonator/fil-c/blob/deluge/filc/include/stdfil.h">stdfil.h</a>. Weak references work just like the weak references in Java, except there are no reference queues. Fil-C does not support phantom or soft references.</p>

<h3>Weak Maps</h3>

<p>FUGC supports weak maps using the <code>zweak_map</code> API in <a href="https://github.com/pizlonator/fil-c/blob/deluge/filc/include/stdfil.h">stdfil.h</a>. This API works almost exactly like the JavaScript <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/WeakMap">WeakMap</a>, except that Fil-C's weak maps allow you to iterate all of their elements and get a count of elements.</p>

<h2>Conclusion</h2>

<p>FUGC allows Fil-C to give the strongest possible guarantees on misuse of <code>free</code>:</p>

<ul>
<li><p>Freeing an object and then accessing it is guaranteed to result in a trap. Unlike tag-based approaches, which will trap on use after free until until memory reclamation is forced, FUGC means you will trap even after memory is reclaimed (due to <em>lower</em> repointing to the free singleton).</p></li>
<li><p>Freeing an object twice is guaranteed to result in a trap.</p></li>
<li><p>Failing to free an object means the object gets reclaimed for you.</p></li>
</ul>
        </main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Forking Chrome to render in a terminal (2023) (146 pts)]]></title>
            <link>https://fathy.fr/carbonyl</link>
            <guid>45133935</guid>
            <pubDate>Fri, 05 Sep 2025 00:54:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fathy.fr/carbonyl">https://fathy.fr/carbonyl</a>, See on <a href="https://news.ycombinator.com/item?id=45133935">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="root"><main>
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->

<blockquote>
<p>January 27, 2023</p>
</blockquote>
<!-- -->
<p>I wrote about <a href="https://fathy.fr/html2svg">forking Chrome to turn HTML to SVG</a> two months ago, today we're going to do something similar by making it render into a terminal.</p>
<p>Let me introduce you to <a href="https://github.com/fathyb/carbonyl" target="_blank">the Carbonyl web browser</a>!</p>
<h2 id="drawing">Drawing<a aria-label="Link to this title" href="https://fathy.fr/carbonyl#drawing"></a></h2>
<div><p>There isn't much you can draw in a terminal, you're guaranteed to be able to render monospace characters in a fixed grid, and that's it. Escape sequences exist to perform actions like moving the cursor, changing the text color, or mouse tracking. Some came from the days of physical terminals like the DEC VT100, others came from the xterm project.</p><p>Assuming a popular terminal emulator, we can:</p><ul>
<li>Move the cursor</li>
<li>Write Unicode characters</li>
<li>Set a character's background and foreground color</li>
<li>Use a 6x6x6 RGB palette, or 24 bits RGB if <code>COLORTERM</code> is set the <code>truecolor</code></li>
</ul><p>One of the unicode characters we can render is the lower half block element <code>U+2584</code>: <code>▄</code>. Knowing that cells generally have an aspect ratio of 1:2, we can render perfectly square pixels by setting the background color to the top pixel color, and the foregound color to the bottom pixel color.</p></div>
<hr>
<p>Let's hook <code>html2svg</code>'s output into a Rust program:</p>
<div><p><a href="https://fathy.fr/assets/32b4d7552f48aa964ea3.png" target="_blank"><img src="https://fathy.fr/assets/32b4d7552f48aa964ea3.png" alt="The text is now correctly hidden"></a></p><div><pre><code><span>fn</span> <span>move_cursor</span><span>(</span><span>(</span>x<span>,</span> y<span>)</span><span>:</span> <span>(</span><span>usize</span><span>,</span> <span>usize</span><span>)</span><span>)</span> <span>{</span>
    <span>println!</span><span>(</span><span>"\x1b[{};{}H"</span><span>,</span> y <span>+</span> <span>1</span><span>,</span> x <span>+</span> <span>1</span><span>)</span>
<span>}</span>

<span>fn</span> <span>set_foreground</span><span>(</span><span>(</span>r<span>,</span> g<span>,</span> b<span>)</span><span>:</span> <span>(</span><span>u8</span><span>,</span> <span>u8</span><span>,</span> <span>u8</span><span>)</span><span>)</span> <span>{</span>
    <span>println!</span><span>(</span><span>"\x1b[38;2;{};{};{}m"</span><span>,</span> r<span>,</span> g<span>,</span> b<span>)</span>
<span>}</span>

<span>fn</span> <span>set_background</span><span>(</span><span>(</span>r<span>,</span> g<span>,</span> b<span>)</span><span>:</span> <span>(</span><span>u8</span><span>,</span> <span>u8</span><span>,</span> <span>u8</span><span>)</span><span>)</span> <span>{</span>
    <span>println!</span><span>(</span><span>"\x1b[48;2;{};{};{}m"</span><span>,</span> r<span>,</span> g<span>,</span> b<span>)</span>
<span>}</span>

<span>fn</span> <span>print_pixels_pair</span><span>(</span>
    top<span>:</span> <span>(</span><span>u8</span><span>,</span> <span>u8</span><span>,</span> <span>u8</span><span>)</span><span>,</span>
    bottom<span>:</span> <span>(</span><span>u8</span><span>,</span> <span>u8</span><span>,</span> <span>u8</span><span>)</span><span>,</span>
    cursor<span>:</span> <span>(</span><span>usize</span><span>,</span> <span>usize</span><span>)</span>
<span>)</span> <span>{</span>
    <span>move_cursor</span><span>(</span>cursor<span>)</span><span>;</span>
    <span>set_background</span><span>(</span>top<span>)</span><span>;</span>
    <span>set_foreground</span><span>(</span>bottom<span>)</span><span>;</span>
    <span>println!</span><span>(</span><span>"▄"</span><span>)</span><span>;</span>
<span>}</span>
</code></pre></div></div>
<hr>
<p>Not bad. To render text, we need to create a new Skia device using C++, lets call it <code>TextCaptureDevice</code>. We'll make it call a <code>draw_text</code> function written in Rust. Just like in <code>html2svg</code>, we need to convert glyph IDs into unicode characters.</p>
<div><p><a href="https://fathy.fr/assets/21f52779391c0364080d.png" target="_blank"><img src="https://fathy.fr/assets/21f52779391c0364080d.png" alt="The text is now correctly hidden"></a></p><div><pre><code><span>class</span> <span>TextCaptureDevice</span><span>:</span> <span><span>public</span> <span>SkClipStackDevice</span></span> <span>{</span>
  <span>void</span> <span>onDrawGlyphRunList</span><span>(</span>SkCanvas<span>*</span><span>,</span>
                          <span>const</span> sktext<span>::</span>GlyphRunList<span>&amp;</span> glyphRunList<span>,</span>
                          <span>const</span> SkPaint<span>&amp;</span><span>,</span>
                          <span>const</span> SkPaint<span>&amp;</span> paint<span>)</span> <span>override</span> <span>{</span>
    <span>// Get the text position</span>
    <span>auto</span> position <span>=</span> <span>localToDevice</span><span>(</span><span>)</span><span>.</span><span>mapRect</span><span>(</span>glyphRunList<span>.</span><span>origin</span><span>(</span><span>)</span><span>)</span><span>;</span>

    <span>for</span> <span>(</span><span>auto</span><span>&amp;</span> glyphRun <span>:</span> glyphRunList<span>)</span> <span>{</span>
      <span>auto</span> runSize <span>=</span> glyphRun<span>.</span><span>runSize</span><span>(</span><span>)</span><span>;</span>
      SkAutoSTArray<span>&lt;</span><span>64</span><span>,</span> SkUnichar<span>&gt;</span> <span>unichars</span><span>(</span>runSize<span>)</span><span>;</span>

      <span>// Convert glyph IDs to Unicode characters</span>
      <span>SkFontPriv</span><span>::</span><span>GlyphsToUnichars</span><span>(</span>glyphRun<span>.</span><span>font</span><span>(</span><span>)</span><span>,</span>
                                  glyphRun<span>.</span><span>glyphsIDs</span><span>(</span><span>)</span><span>.</span><span>data</span><span>(</span><span>)</span><span>,</span>
                                  runSize<span>,</span>
                                  unichars<span>.</span><span>get</span><span>(</span><span>)</span><span>)</span><span>;</span>

      <span>// Draw that text on the terminal</span>
      <span>draw_text</span><span>(</span>unichars<span>.</span><span>data</span><span>(</span><span>)</span><span>,</span> runSize<span>,</span> position<span>,</span> paint<span>.</span><span>getColor</span><span>(</span><span>)</span><span>)</span><span>;</span>
    <span>}</span>
  <span>}</span>
<span>}</span>
</code></pre></div></div>
<hr>
<p>Better! But the text is scrambled at the center. Our <code>TextCaptureDevice</code> does not account for occlusion, drawing a rectangle does not clear the text behind it.</p>
<p><a href="https://fathy.fr/assets/72599f231e123b8c1537.png" target="_blank"><img src="https://fathy.fr/assets/72599f231e123b8c1537.png" alt=""></a></p>
<p>Let's add some code to the <code>drawRect</code> and <code>drawRRect</code> methods to clear the text if we're filling with a solid color:</p>
<div><p><a href="https://fathy.fr/assets/57a89b4c81a3f81ec068.png" target="_blank"><img src="https://fathy.fr/assets/57a89b4c81a3f81ec068.png" alt="The text is now correctly hidden"></a></p><div><pre><code><span>void</span> <span>drawRRect</span><span>(</span><span>const</span> SkRRect<span>&amp;</span> rect<span>,</span> <span>const</span> SkPaint<span>&amp;</span> paint<span>)</span> <span>override</span> <span>{</span>
    <span>drawRect</span><span>(</span>rect<span>.</span><span>rect</span><span>(</span><span>)</span><span>,</span> paint<span>)</span><span>;</span>
<span>}</span>

<span>void</span> <span>drawRect</span><span>(</span><span>const</span> SkRect<span>&amp;</span> rect<span>,</span> <span>const</span> SkPaint<span>&amp;</span> paint<span>)</span> <span>override</span> <span>{</span>
    <span>if</span> <span>(</span>
        paint<span>.</span><span>getStyle</span><span>(</span><span>)</span> <span>==</span> SkPaint<span>::</span>Style<span>::</span>kFill_Style <span>&amp;&amp;</span>
        paint<span>.</span><span>getAlphaf</span><span>(</span><span>)</span> <span>==</span> <span>1.0</span>
    <span>)</span> <span>{</span>
        <span>clear_text</span><span>(</span><span>localToDevice</span><span>(</span><span>)</span><span>.</span><span>mapRect</span><span>(</span>rect<span>)</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>
<hr>
<p>The gray background behind text elements is caused by the software rasterizer rendering text in our bitmap. Let's remove it:</p>
<div><p><a href="https://fathy.fr/assets/a7af4a712a6811423325.png" target="_blank"><img src="https://fathy.fr/assets/a7af4a712a6811423325.png" alt="The text is now correctly hidden"></a></p><div><pre><code>void SkBitmapDevice::onDrawGlyphRunList(SkCanvas* canvas,
<span><span> </span><span>                                       const sktext::GlyphRunList&amp; glyphRunList,
</span><span> </span><span>                                       const SkPaint&amp; initialPaint,
</span><span> </span><span>                                       const SkPaint&amp; drawingPaint) {
</span></span><span><span>-</span><span>    SkASSERT(!glyphRunList.hasRSXForm());
</span><span>-</span><span>    LOOP_TILER( drawGlyphRunList(canvas, &amp;fGlyphPainter, glyphRunList, drawingPaint), nullptr )
</span></span>}
</code></pre></div></div>
<hr>
<p>That was the easy part, let's handle inputs!</p>
<h2 id="input">Input<a aria-label="Link to this title" href="https://fathy.fr/carbonyl#input"></a></h2>
<div><div><pre><code><span>fn</span> <span>report_mouse_move</span><span>(</span><span>(</span>x<span>,</span> y<span>)</span><span>:</span> <span>(</span><span>usize</span><span>,</span> <span>usize</span><span>)</span><span>)</span> <span>{</span>
    <span>write!</span><span>(</span><span>get_stdin</span><span>(</span><span>)</span><span>,</span> <span>"\x1b[&lt;35;{};{}M"</span><span>,</span> y <span>+</span> <span>1</span><span>,</span> x <span>+</span> <span>1</span><span>)</span>
<span>}</span>
<span>fn</span> <span>report_mouse_down</span><span>(</span><span>(</span>x<span>,</span> y<span>)</span><span>:</span> <span>(</span><span>usize</span><span>,</span> <span>usize</span><span>)</span><span>)</span> <span>{</span>
    <span>write!</span><span>(</span><span>get_stdin</span><span>(</span><span>)</span><span>,</span> <span>"\x1b[&lt;0;{};{}M"</span><span>,</span> y <span>+</span> <span>1</span><span>,</span> x <span>+</span> <span>1</span><span>)</span>
<span>}</span>
<span>fn</span> <span>report_mouse_up</span><span>(</span><span>(</span>x<span>,</span> y<span>)</span><span>:</span> <span>(</span><span>usize</span><span>,</span> <span>usize</span><span>)</span><span>)</span> <span>{</span>
    <span>write!</span><span>(</span><span>get_stdin</span><span>(</span><span>)</span><span>,</span> <span>"\x1b[&lt;0;{};{}m"</span><span>,</span> y <span>+</span> <span>1</span><span>,</span> x <span>+</span> <span>1</span><span>)</span>
<span>}</span>
</code></pre></div><p>Some sequences exist to get a terminal emulator to track and report mouse events. For example, if you print <code>\x1b[?1003h</code>, the terminal should start sending events using this format:</p></div>
<p>These are similar to the sequences we use for styling our output. The <code>\x1b[</code> prefix is called the Control Sequence Introducer.</p>
<div><div><pre><code>carbonyl<span>::</span>browser<span>-&gt;</span><span>BrowserMainThread</span><span>(</span><span>)</span><span>-&gt;</span><span>PostTask</span><span>(</span>
    FROM_HERE<span>,</span>
    base<span>::</span><span>BindOnce</span><span>(</span>
        <span>&amp;</span>HeadlessBrowserImpl<span>::</span>OnMouseDownInput<span>,</span>
        x<span>,</span>
        y
    <span>)</span>
<span>)</span><span>;</span>
</code></pre></div><p>We need to notify the browser to wrap this up, but there is a catch: we
need to block a thread to read stdin, but the browser methods should be
called from the main thread. Thankfully, messages passing is available
almost everywhere through the
<a href="https://source.chromium.org/chromium/chromium/src/+/main:base/task/task_runner.h;drc=63e1f9974bc57b0ca12d790b2a73e5ba7f5cec6e;l=60" target="_blank"><code>TaskRunner</code></a>
class.</p></div>
<div><p><figure><video src="https://fathy.fr/assets/26fa84f7239e4c1fa504.mp4" autoplay="" loop="" controls=""></video><figcaption>Google recommending me Chrome, lol</figcaption></figure></p><div><pre><code><span>for</span> <span>&amp;</span>key <span>in</span> input <span>{</span>
    sequence <span>=</span> <span>match</span> sequence <span>{</span>
        <span>Sequence</span><span>::</span><span>Char</span> <span>=&gt;</span> <span>match</span> key <span>{</span>
            <span>0x1b</span> <span>=&gt;</span> <span>Sequence</span><span>::</span><span>Escape</span><span>,</span>
            <span>0x03</span> <span>=&gt;</span> <span>emit!</span><span>(</span><span>Event</span><span>::</span><span>Exit</span><span>)</span><span>,</span>
            key <span>=&gt;</span> <span>emit!</span><span>(</span><span>Event</span><span>::</span><span>KeyPress</span> <span>{</span> key <span>}</span><span>)</span><span>,</span>
        <span>}</span><span>,</span>
        <span>Sequence</span><span>::</span><span>Escape</span> <span>=&gt;</span> <span>match</span> key <span>{</span>
            <span>b'['</span> <span>=&gt;</span> <span>Sequence</span><span>::</span><span>Control</span><span>,</span>
            <span>b'P'</span> <span>=&gt;</span> <span>Sequence</span><span>::</span><span>DeviceControl</span><span>(</span><span>DeviceControl</span><span>::</span><span>new</span><span>(</span><span>)</span><span>)</span><span>,</span>
            <span>0x1b</span> <span>=&gt;</span>
                <span>emit!</span><span>(</span><span>Event</span><span>::</span><span>KeyPress</span> <span>{</span> key<span>:</span> <span>0x1b</span> <span>}</span><span>;</span> <span>continue</span><span>)</span><span>,</span>
            key <span>=&gt;</span> <span>{</span>
                <span>emit!</span><span>(</span><span>Event</span><span>::</span><span>KeyPress</span> <span>{</span> key<span>:</span> <span>0x1b</span> <span>}</span><span>)</span><span>;</span>
                <span>emit!</span><span>(</span><span>Event</span><span>::</span><span>KeyPress</span> <span>{</span> key <span>}</span><span>)</span>
            <span>}</span>
        <span>}</span><span>,</span>
        <span>Sequence</span><span>::</span><span>Control</span> <span>=&gt;</span> <span>match</span> key <span>{</span>
            <span>b'&lt;'</span> <span>=&gt;</span> <span>Sequence</span><span>::</span><span>Mouse</span><span>(</span><span>Mouse</span><span>::</span><span>new</span><span>(</span><span>)</span><span>)</span><span>,</span>
            <span>b'A'</span> <span>=&gt;</span> <span>emit!</span><span>(</span><span>Event</span><span>::</span><span>KeyPress</span> <span>{</span> key<span>:</span> <span>0x26</span> <span>}</span><span>)</span><span>,</span>
            <span>b'B'</span> <span>=&gt;</span> <span>emit!</span><span>(</span><span>Event</span><span>::</span><span>KeyPress</span> <span>{</span> key<span>:</span> <span>0x28</span> <span>}</span><span>)</span><span>,</span>
            <span>b'C'</span> <span>=&gt;</span> <span>emit!</span><span>(</span><span>Event</span><span>::</span><span>KeyPress</span> <span>{</span> key<span>:</span> <span>0x27</span> <span>}</span><span>)</span><span>,</span>
            <span>b'D'</span> <span>=&gt;</span> <span>emit!</span><span>(</span><span>Event</span><span>::</span><span>KeyPress</span> <span>{</span> key<span>:</span> <span>0x25</span> <span>}</span><span>)</span><span>,</span>
            _ <span>=&gt;</span> <span>Sequence</span><span>::</span><span>Char</span><span>,</span>
        <span>}</span><span>,</span>
        <span>Sequence</span><span>::</span><span>Mouse</span><span>(</span><span>ref</span> <span>mut</span> mouse<span>)</span> <span>=&gt;</span> <span>parse!</span><span>(</span>mouse<span>,</span> key<span>)</span><span>,</span>
        <span>Sequence</span><span>::</span><span>DeviceControl</span><span>(</span><span>ref</span> <span>mut</span> dcs<span>)</span> <span>=&gt;</span> <span>parse!</span><span>(</span>dcs<span>,</span> key<span>)</span><span>,</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>
<h2 id="pipe">Pipe<a aria-label="Link to this title" href="https://fathy.fr/carbonyl#pipe"></a></h2>
<p>We have something that sorts of work, at the cost of a steady 400% CPU usage, and that's not counting iTerm2 which uses ~200%. We've got a few problems:</p>
<ul>
<li>We need too much resources to render at 5 FPS</li>
<li>We render every time, even when nothing changes</li>
<li>We print all characters even if they didn't change on an individual level</li>
</ul>
<p>Modern browsers employ a multi-process architecture to improve security. It separates websites into different processes, reducing the potential damage caused by vulnerabilities. The renderer process is running in an OS-level sandboxed environment that blocks certain system calls, such as file-system access. The GPU process, is also considered unprivileged and cannot reach renderer process in order to protect against vulnerabilities in GPU APIs such as WebGL. In contrast, the browser process, considered privileged, can communicate freely with any process.</p>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1347 764.1508768480842" width="2694" height="1528.3017536961684"><g stroke-linecap="round" transform="translate(730 270) rotate(0 210 60)"><path d="M30 0 M30 0 C157.04 0.46, 284.43 0.31, 390 0 M30 0 C135.95 -1.31, 242.95 -1.78, 390 0 M390 0 C411.81 2.47, 420.51 6.29, 420 30 M390 0 C413.02 -4.44, 422.08 8.32, 420 30 M420 30 C419.91 45.26, 417.11 56.14, 420 90 M420 30 C420.97 50.46, 418.25 69.52, 420 90 M420 90 C419.23 111.17, 407.26 121.67, 390 120 M420 90 C421.66 111.18, 411.88 117.61, 390 120 M390 120 C256.58 125.8, 126.15 123.73, 30 120 M390 120 C248.98 122.52, 105.46 120.63, 30 120 M30 120 C8.72 119.2, -0.04 106.2, 0 90 M30 120 C6.71 119.94, -2.21 111.74, 0 90 M0 90 C0.74 66.11, -2.59 45.15, 0 30 M0 90 C0.96 76.74, -2.03 60.13, 0 30 M0 30 C0.34 9.9, 8.66 -3.41, 30 0 M0 30 C1.19 14.25, 8.86 3.42, 30 0" stroke="white" stroke-width="1" fill="none"></path></g><g transform="translate(861.6728068622378 289.921875) rotate(0 76.5 12)"><text x="0" y="20" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="20px" fill="white" text-anchor="start" style="white-space:pre" direction="ltr">Google Chrome</text></g><g stroke-linecap="round" transform="translate(192.2109375 270.765625) rotate(0 200.34179687500003 58.0078125)"><path d="M29 0 M29 0 C161.85 -4.09, 295.82 -5.23, 371.68 0 M29 0 C140.36 1.45, 250.69 -0.6, 371.68 0 M371.68 0 C393.64 -3.86, 402.49 8.21, 400.68 29 M371.68 0 C387.95 -2.91, 398.25 10.44, 400.68 29 M400.68 29 C401.09 48.05, 396.68 69.49, 400.68 87.01 M400.68 29 C399.31 49.65, 398.33 69.96, 400.68 87.01 M400.68 87.01 C402.13 107.38, 392.65 113.93, 371.68 116.02 M400.68 87.01 C403.64 105.48, 395.2 111.87, 371.68 116.02 M371.68 116.02 C249.1 115.04, 122.52 117.22, 29 116.02 M371.68 116.02 C246.44 111.9, 122.03 110.91, 29 116.02 M29 116.02 C6.8 115.96, -1.92 107.86, 0 87.01 M29 116.02 C12.82 119.43, -1.19 108.22, 0 87.01 M0 87.01 C-1.98 72.58, 0.34 59.52, 0 29 M0 87.01 C1.28 66.6, 0.44 44.59, 0 29 M0 29 C1.04 13.37, 8.67 2.98, 29 0 M0 29 C1.71 7.26, 7.98 -2.34, 29 0" stroke="white" stroke-width="1" fill="none"></path></g><g transform="translate(848.4401120382331 348.29664913910244) rotate(0 88.5 12)"><text x="88.5" y="20" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="20px" fill="white" text-anchor="middle" style="white-space:pre" direction="ltr">Browser Process</text></g><g stroke-linecap="round" transform="translate(470 610) rotate(0 203.5 57.5)"><path d="M28.75 0 M28.75 0 C129.22 1.79, 228.12 3.78, 378.25 0 M28.75 0 C156.06 2.97, 282.25 3.75, 378.25 0 M378.25 0 C394.75 -2.53, 404.89 10.26, 407 28.75 M378.25 0 C398.86 -2.45, 404.85 10.77, 407 28.75 M407 28.75 C407.86 42.35, 406.9 61.97, 407 86.25 M407 28.75 C406.94 48.2, 407.03 65.59, 407 86.25 M407 86.25 C409.57 104.66, 401.06 111.4, 378.25 115 M407 86.25 C409.99 102.58, 396.66 119.34, 378.25 115 M378.25 115 C278.1 117.02, 172.39 117.5, 28.75 115 M378.25 115 C279.45 110.66, 181.9 111.17, 28.75 115 M28.75 115 C12.32 117.97, -1.03 107.05, 0 86.25 M28.75 115 C7.54 113.04, -0.72 102.57, 0 86.25 M0 86.25 C2.08 64.62, -2.57 50.96, 0 28.75 M0 86.25 C0.9 70.99, -0.11 53.53, 0 28.75 M0 28.75 C1.49 7.49, 8.11 -2.03, 28.75 0 M0 28.75 C-3.37 12.89, 10.35 3.27, 28.75 0" stroke="white" stroke-width="1" fill="none"></path></g><g stroke-opacity="0.5" fill-opacity="0.5" stroke-linecap="round" transform="translate(10 610) rotate(0 203.49999999999997 57.5)"><path d="M28.75 0 M28.75 0 C113.3 2.59, 200.19 1.74, 378.25 0 M28.75 0 C101.75 3.91, 172.7 3.56, 378.25 0 M378.25 0 C395.3 0.67, 408.26 7.45, 407 28.75 M378.25 0 C395.27 1.18, 406.8 5.69, 407 28.75 M407 28.75 C408.4 48.15, 408.7 66.37, 407 86.25 M407 28.75 C408.23 47.36, 408.7 67.67, 407 86.25 M407 86.25 C410.64 101.82, 400.02 112.53, 378.25 115 M407 86.25 C406.25 109.75, 397.02 116.72, 378.25 115 M378.25 115 C245.54 111.87, 116.74 112.68, 28.75 115 M378.25 115 C305.95 115.43, 235.8 115.91, 28.75 115 M28.75 115 C8.55 116.63, -1.78 103.71, 0 86.25 M28.75 115 C8.86 112.15, 0.45 105.24, 0 86.25 M0 86.25 C-3.2 66.23, -0.69 44.19, 0 28.75 M0 86.25 C-0.8 75.4, 0.44 60.52, 0 28.75 M0 28.75 C-1.47 7.55, 6.65 2.87, 28.75 0 M0 28.75 C0.77 12.85, 6.45 3.24, 28.75 0" stroke="white" stroke-width="1" fill="none"></path></g><g stroke-opacity="0.5" fill-opacity="0.5" stroke-linecap="round" transform="translate(930 610) rotate(0 203.5 57.5)"><path d="M28.75 0 M28.75 0 C124.39 -1.73, 214.4 -0.36, 378.25 0 M28.75 0 C99.03 0.84, 170.36 1.88, 378.25 0 M378.25 0 C398.67 -2.13, 405.13 10.61, 407 28.75 M378.25 0 C397.21 -3.89, 409.34 11.69, 407 28.75 M407 28.75 C408.43 38.37, 408.92 54.92, 407 86.25 M407 28.75 C408.76 47.1, 409.29 64.41, 407 86.25 M407 86.25 C409.6 102.95, 396.76 118.77, 378.25 115 M407 86.25 C406.6 107.14, 400.79 117.96, 378.25 115 M378.25 115 C290.51 113.52, 201.34 111.68, 28.75 115 M378.25 115 C274.75 117.69, 169.13 117.25, 28.75 115 M28.75 115 C7.81 113.3, -0.63 102.94, 0 86.25 M28.75 115 C10.04 114.82, 2.49 107.76, 0 86.25 M0 86.25 C0.65 73.14, -2.66 63.75, 0 28.75 M0 86.25 C1.03 62.64, -0.45 40.1, 0 28.75 M0 28.75 C-2.93 12.46, 10.25 2.84, 28.75 0 M0 28.75 C-3.13 12.82, 13.01 -1.44, 28.75 0" stroke="white" stroke-width="1" fill="none"></path></g><g transform="translate(491.5466383381638 625.914296502428) rotate(0 182 12)"><text x="0" y="20" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="20px" fill="white" text-anchor="start" style="white-space:pre" direction="ltr">Google Chrome Helper (Renderer)</text></g><g stroke-linecap="round"><g transform="translate(951.9118742957429 391.2) rotate(0 -134.21471413755773 108.71608656182883)"><path d="M1.84 -2.7 C-3.02 13.5, 12.34 76.16, -28.79 94.58 C-69.91 113.01, -205 87.39, -244.92 107.84 C-284.83 128.28, -264.76 199.36, -268.25 217.27 M-0.61 2.01 C-5.76 18.83, 9.41 79.66, -30.34 97.66 C-70.09 115.65, -199.41 89.57, -239.1 109.98 C-278.78 130.4, -263.51 202.56, -268.47 220.13" stroke="white" stroke-width="1" fill="none"></path></g><g transform="translate(951.9118742957429 391.2) rotate(0 -134.21471413755773 108.71608656182883)"><path d="M-281.53 193.38 C-276.78 197.73, -274.97 207.21, -267.8 221.39 M-277.76 189.89 C-274.65 199.49, -275.1 206.06, -268.86 220.72" stroke="white" stroke-width="1" fill="none"></path></g><g transform="translate(951.9118742957429 391.2) rotate(0 -134.21471413755773 108.71608656182883)"><path d="M-261.02 194.15 C-262.68 198.52, -267.28 207.76, -267.8 221.39 M-257.25 190.65 C-259.2 200.17, -264.7 206.55, -268.86 220.72" stroke="white" stroke-width="1" fill="none"></path></g></g><mask></mask><g stroke-linecap="round"><g transform="translate(628.274146870492 608) rotate(0 -127.43022652601434 -109.8545016936958)"><path d="M1.26 -1.02 C-4.09 -17.57, 8.42 -83.18, -30.28 -101.96 C-68.97 -120.75, -193.68 -94.27, -230.89 -113.72 C-268.1 -133.18, -249.39 -201.56, -253.53 -218.69 M-1.5 -4.04 C-6.12 -20.02, 13.78 -81.58, -24.89 -99.3 C-63.56 -117.02, -195.57 -90.76, -233.5 -110.36 C-271.43 -129.96, -249.51 -199.42, -252.47 -216.89" stroke="white" stroke-width="1" fill="none"></path></g><g transform="translate(628.274146870492 608) rotate(0 -127.43022652601434 -109.8545016936958)"><path d="M-245.53 -190.75 C-249.12 -200.66, -248.99 -207.82, -254.6 -218.76 M-243.05 -187.18 C-248.48 -197.18, -249.91 -206.41, -253.84 -216.06" stroke="white" stroke-width="1" fill="none"></path></g><g transform="translate(628.274146870492 608) rotate(0 -127.43022652601434 -109.8545016936958)"><path d="M-266.01 -192.09 C-262.13 -201.69, -254.52 -208.36, -254.6 -218.76 M-263.53 -188.51 C-262.29 -197.73, -257.04 -206.53, -253.84 -216.06" stroke="white" stroke-width="1" fill="none"></path></g></g><mask></mask><g transform="translate(731 430) rotate(0 89 19.5)"><text x="178" y="15.5" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="16px" fill="white" text-anchor="end" style="white-space:pre" direction="ltr">mouseMove(250, 250)</text><text x="178" y="35" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="16px" fill="white" text-anchor="end" style="white-space:pre" direction="ltr">mouseDown(250, 250)</text></g><g transform="translate(410 430) rotate(0 66 10)"><text x="0" y="16" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="16px" fill="white" text-anchor="start" style="white-space:pre" direction="ltr">glBindBuffer()</text></g><g transform="translate(410 450) rotate(0 61.5 10)"><text x="0" y="16" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="16px" fill="white" text-anchor="start" style="white-space:pre" direction="ltr">glDrawArray()</text></g><g stroke-linecap="round" transform="translate(310 10) rotate(0 73 70.5)"><path d="M109.02 8.55 C118.25 12.41, 127.16 20.05, 133.3 28.58 C139.44 37.1, 144.14 49.28, 145.85 59.71 C147.57 70.13, 146.85 81.24, 143.58 91.12 C140.3 101, 133.4 111.24, 126.19 118.98 C118.97 126.72, 110.17 133.7, 100.27 137.57 C90.38 141.43, 77.48 143.14, 66.8 142.17 C56.12 141.2, 44.79 136.95, 36.19 131.73 C27.58 126.52, 20.92 118.92, 15.17 110.88 C9.43 102.84, 3.55 93.68, 1.73 83.49 C-0.1 73.29, 1.67 59.83, 4.22 49.69 C6.78 39.55, 10.29 30.07, 17.07 22.66 C23.86 15.25, 35.21 9.23, 44.95 5.23 C54.69 1.24, 63.98 -2.31, 75.5 -1.3 C87.02 -0.3, 106.8 7.98, 114.07 11.27 C121.34 14.56, 119.55 17.4, 119.12 18.45 M101.44 3.91 C111.35 6.96, 121.16 15.91, 128.43 24.17 C135.7 32.44, 142.81 43.97, 145.06 53.5 C147.3 63.03, 144.07 71.6, 141.92 81.35 C139.78 91.11, 138.1 103.92, 132.18 112.05 C126.25 120.18, 115.6 124.85, 106.38 130.13 C97.15 135.41, 87.32 142.49, 76.81 143.71 C66.31 144.94, 52.98 142.23, 43.34 137.47 C33.7 132.71, 25.96 123.47, 18.98 115.14 C12 106.82, 4.79 96.73, 1.46 87.51 C-1.86 78.29, -3.17 69.26, -0.98 59.84 C1.2 50.42, 8.32 39.24, 14.58 31 C20.84 22.77, 26.97 15.73, 36.58 10.42 C46.18 5.11, 61.09 -0.33, 72.2 -0.88 C83.3 -1.42, 98.72 5.97, 103.21 7.14 C107.71 8.3, 99.87 5.28, 99.2 6.11" stroke="white" stroke-width="1" fill="none"></path></g><g transform="translate(349.5 46.5) rotate(0 33.5 32)"><text x="33.5" y="26" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="28px" fill="white" text-anchor="middle" style="white-space:pre" direction="ltr">👀</text><text x="33.5" y="58" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="28px" fill="white" text-anchor="middle" style="white-space:pre" direction="ltr">Eyes</text></g><g stroke-linecap="round" transform="translate(590 10) rotate(0 73 70.5)"><path d="M18.72 22.62 C24.58 14.83, 35.59 9.95, 45.3 6.33 C55.01 2.71, 66.73 0.32, 76.96 0.89 C87.19 1.47, 97.56 4.76, 106.7 9.78 C115.84 14.8, 125.72 22.75, 131.78 31.01 C137.84 39.28, 141.34 49.69, 143.06 59.37 C144.78 69.05, 144.31 79.51, 142.09 89.09 C139.87 98.66, 136.38 108.87, 129.75 116.83 C123.11 124.8, 112.1 133.01, 102.28 136.86 C92.47 140.7, 81.79 140.74, 70.84 139.91 C59.89 139.08, 46.13 136.55, 36.56 131.86 C26.99 127.18, 19.66 120.18, 13.44 111.79 C7.22 103.39, 1.04 91.69, -0.76 81.5 C-2.55 71.3, -6.4 63.52, 2.65 50.63 C11.7 37.73, 38.18 11.93, 53.56 4.12 C68.94 -3.69, 94.58 3.02, 94.93 3.77 M43.16 6.17 C52.19 1.77, 66.29 1.62, 76.32 1.55 C86.35 1.48, 94.15 1.38, 103.33 5.75 C112.51 10.12, 124.67 19.74, 131.37 27.76 C138.07 35.78, 141.33 43.39, 143.52 53.85 C145.72 64.32, 147.1 79.87, 144.55 90.55 C142.01 101.24, 134.97 110.86, 128.26 117.96 C121.54 125.05, 113.93 129.56, 104.25 133.14 C94.58 136.73, 81.31 139.07, 70.2 139.46 C59.1 139.84, 46.75 139.47, 37.61 135.45 C28.47 131.42, 21.75 123.42, 15.39 115.3 C9.02 107.17, 1.25 97.2, -0.57 86.7 C-2.39 76.2, 1.32 62.29, 4.46 52.29 C7.61 42.28, 12.26 34.47, 18.3 26.65 C24.35 18.83, 36.51 8.53, 40.73 5.34 C44.94 2.15, 42.63 6.3, 43.62 7.49" stroke="white" stroke-width="1" fill="none"></path></g><g transform="translate(621.5 46.5) rotate(0 41.5 32)"><text x="41.5" y="26" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="28px" fill="white" text-anchor="middle" style="white-space:pre" direction="ltr">🧠</text><text x="41.5" y="58" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="28px" fill="white" text-anchor="middle" style="white-space:pre" direction="ltr">Brain</text></g><g stroke-linecap="round" transform="translate(869.318882426343 10) rotate(0 73 70.5)"><path d="M59.78 2.04 C69.4 -0.5, 82.03 0.43, 91.94 3.1 C101.84 5.76, 111.3 11.28, 119.2 18.04 C127.1 24.81, 135.17 34.38, 139.33 43.7 C143.49 53.02, 144.63 64.13, 144.17 73.96 C143.7 83.8, 140.79 93.9, 136.53 102.7 C132.28 111.5, 126.75 120.36, 118.63 126.77 C110.5 133.18, 98.26 139.33, 87.8 141.15 C77.34 142.97, 66.49 140.63, 55.86 137.71 C45.24 134.79, 32.39 130.04, 24.06 123.6 C15.73 117.17, 10.21 108.55, 5.89 99.1 C1.57 89.65, -2.25 77.25, -1.86 66.9 C-1.48 56.55, 2.89 45.59, 8.21 37.01 C13.52 28.43, 18.15 21.66, 30.02 15.43 C41.88 9.2, 68.24 1.51, 79.37 -0.36 C90.51 -2.23, 97.32 2.99, 96.84 4.2 M32.24 14.63 C39.25 7.83, 47.57 1.06, 57.74 -0.94 C67.92 -2.94, 82.84 -0.07, 93.3 2.63 C103.77 5.33, 112.54 8.01, 120.51 15.26 C128.49 22.52, 137.05 35.93, 141.16 46.14 C145.26 56.35, 145.66 67.07, 145.15 76.52 C144.64 85.98, 143.23 94.58, 138.09 102.89 C132.94 111.2, 123.43 119.84, 114.28 126.38 C105.13 132.92, 93.33 139.79, 83.19 142.13 C73.06 144.47, 63.58 143.23, 53.45 140.42 C43.33 137.61, 30.04 132.58, 22.44 125.29 C14.84 118, 11.36 106.23, 7.86 96.69 C4.35 87.16, 1.57 78.27, 1.41 68.08 C1.25 57.9, 2 44.77, 6.89 35.59 C11.79 26.41, 26.55 16.61, 30.77 13.01 C34.99 9.42, 31.97 12.95, 32.22 14.02" stroke="white" stroke-width="1" fill="none"></path></g><g transform="translate(900.818882426343 46.5) rotate(0 41.5 32)"><text x="41.5" y="26" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="28px" fill="white" text-anchor="middle" style="white-space:pre" direction="ltr">🤌</text><text x="41.5" y="58" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="28px" fill="white" text-anchor="middle" style="white-space:pre" direction="ltr">Hands</text></g><g stroke-linecap="round"><g transform="translate(935.0486609819828 149.6799425749015) rotate(0 -0.4261813352072181 57.187294683394065)"><path d="M-0.54 0.82 C-0.47 19.98, -2.19 96.52, -1.93 115.57 M-4.28 -1.2 C-3.1 17.05, 2.76 91.23, 3.43 111.2" stroke="white" stroke-width="1" fill="none"></path></g><g transform="translate(935.0486609819828 149.6799425749015) rotate(0 -0.4261813352072181 57.187294683394065)"><path d="M-7.29 80.92 C-4.12 91.07, -2.2 97.18, 1.35 113.77 M-7.16 82.42 C-4.95 93.5, -1.45 100.1, 4.9 112.48" stroke="white" stroke-width="1" fill="none"></path></g><g transform="translate(935.0486609819828 149.6799425749015) rotate(0 -0.4261813352072181 57.187294683394065)"><path d="M13.2 79.72 C11.97 90.05, 9.48 96.41, 1.35 113.77 M13.32 81.22 C9.8 92.26, 7.54 99.2, 4.9 112.48" stroke="white" stroke-width="1" fill="none"></path></g></g><mask></mask><g transform="translate(578.8784669840671 679.8894989044489) rotate(0 82.5 12)"><text x="0" y="20" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="20px" fill="white" text-anchor="start" style="white-space:pre" direction="ltr">Render Process</text></g><g stroke-linecap="round"><g transform="translate(458.2949566502142 82.04691014625813) rotate(0 64.30948462329874 -1.149495066976673)"><path d="M1.01 0.72 C23.08 0.02, 108.81 -2.55, 130.5 -3.02 M-1.88 -1.36 C20.09 -1.81, 106.85 -0.53, 128.88 -0.18" stroke="white" stroke-width="1" fill="none"></path></g><g transform="translate(458.2949566502142 82.04691014625813) rotate(0 64.30948462329874 -1.149495066976673)"><path d="M101.58 11.3 C104.95 9.75, 112.83 8.54, 125.28 2.42 M100.37 10.42 C108.8 8.07, 113.08 2.74, 128.24 -0.58" stroke="white" stroke-width="1" fill="none"></path></g><g transform="translate(458.2949566502142 82.04691014625813) rotate(0 64.30948462329874 -1.149495066976673)"><path d="M101.88 -9.22 C105.35 -6.01, 113.16 -2.47, 125.28 2.42 M100.67 -10.1 C108.96 -7.55, 113.16 -7.99, 128.24 -0.58" stroke="white" stroke-width="1" fill="none"></path></g></g><mask></mask><g stroke-linecap="round"><g transform="translate(868.3281103955014 79.37086802206923) rotate(0 -63.97348310991066 1.1776855524617709)"><path d="M1.14 -1.46 C-20.33 -1.82, -106.46 -1.13, -127.83 -1.47 M-1.68 3.92 C-23.46 3.93, -108.12 2.81, -129.09 1.87" stroke="white" stroke-width="1" fill="none"></path></g><g transform="translate(868.3281103955014 79.37086802206923) rotate(0 -63.97348310991066 1.1776855524617709)"><path d="M-29.12 11.4 C-18.38 8.78, -11.75 4.28, -1.33 -2.11 M-25.57 10.11 C-16.92 2.86, -5.44 0.11, 1.12 -3.36" stroke="white" stroke-width="1" fill="none"></path></g><g transform="translate(868.3281103955014 79.37086802206923) rotate(0 -63.97348310991066 1.1776855524617709)"><path d="M-29.14 -9.13 C-18.28 -4.85, -11.65 -2.44, -1.33 -2.11 M-25.59 -10.41 C-16.96 -9.57, -5.48 -4.23, 1.12 -3.36" stroke="white" stroke-width="1" fill="none"></path></g></g><mask></mask><g stroke-linecap="round"><g transform="translate(382.5210329235799 270.0708617603566) rotate(0 0.5661792292823691 -60.56847381778317)"><path d="M1.8 -0.53 C1.87 -20.39, 1.71 -100.73, 1.54 -120.61 M-0.67 -3.29 C-0.85 -22.76, 0.58 -99.29, 0.69 -118.34" stroke="white" stroke-width="1" fill="none"></path></g><g transform="translate(382.5210329235799 270.0708617603566) rotate(0 0.5661792292823691 -60.56847381778317)"><path d="M9.83 -86.39 C3.91 -96.95, 1.67 -109.86, 4.46 -118.69 M9.58 -91.72 C6.19 -99.85, 3.47 -110.91, -0.74 -118.37" stroke="white" stroke-width="1" fill="none"></path></g><g transform="translate(382.5210329235799 270.0708617603566) rotate(0 0.5661792292823691 -60.56847381778317)"><path d="M-10.69 -86.65 C-9.62 -97.39, -4.87 -110.2, 4.46 -118.69 M-10.94 -91.98 C-7.4 -100.29, -3.2 -111.25, -0.74 -118.37" stroke="white" stroke-width="1" fill="none"></path></g></g><mask></mask><g transform="translate(811 190.2076543973717) rotate(0 47 19.5)"><text x="94" y="15.5" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="16px" fill="white" text-anchor="end" style="white-space:pre" direction="ltr">mechanical</text><text x="94" y="35" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="16px" fill="white" text-anchor="end" style="white-space:pre" direction="ltr">energy</text></g><g transform="translate(410 190) rotate(0 70.5 19.5)"><text x="0" y="15.5" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="16px" fill="white" text-anchor="start" style="white-space:pre" direction="ltr">electromagnetic</text><text x="0" y="35" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="16px" fill="white" text-anchor="start" style="white-space:pre" direction="ltr">energy</text></g><g transform="translate(544.2040650782787 734.1508768480842) rotate(0 127.00000000000003 10)"><text x="145" y="16" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="16px" fill="white" text-anchor="end" style="white-space:pre" direction="ltr">GitHub</text></g><g transform="translate(31 730) rotate(0 183.5 10)"><text x="380" y="16" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="16px" fill="white" text-anchor="end" style="white-space:pre" direction="ltr" opacity="0.5">superintelligent ants risks - Google Search</text></g><g stroke-opacity="0.5" fill-opacity="0.5" transform="translate(30 630) rotate(0 182 12)"><text x="0" y="20" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="20px" fill="white" text-anchor="start" style="white-space:pre" direction="ltr">Google Chrome Helper (Renderer)</text></g><g stroke-opacity="0.5" fill-opacity="0.5" transform="translate(117.33182864590333 683.9752024020208) rotate(0 82.5 12)"><text x="0" y="20" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="20px" fill="white" text-anchor="start" style="white-space:pre" direction="ltr">Render Process</text></g><g stroke-opacity="0.5" fill-opacity="0.5" transform="translate(950 630) rotate(0 182 12)"><text x="0" y="20" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="20px" fill="white" text-anchor="start" style="white-space:pre" direction="ltr">Google Chrome Helper (Renderer)</text></g><g stroke-opacity="0.5" fill-opacity="0.5" transform="translate(1037.3318286459032 683.9752024020208) rotate(0 82.5 12)"><text x="0" y="20" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="20px" fill="white" text-anchor="start" style="white-space:pre" direction="ltr">Render Process</text></g><g transform="translate(971 730) rotate(0 174 10)"><text x="308" y="16" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="16px" fill="white" text-anchor="end" style="white-space:pre" direction="ltr" opacity="0.5">diy ant farm keyboard - YouTube</text></g><g transform="translate(228.90144904599777 285.03811366027253) rotate(0 153 11.999999999999986)"><text x="0" y="20" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="20px" fill="white" text-anchor="start" style="white-space:pre" direction="ltr">Google Chrome Helper (GPU)</text></g><g transform="translate(313.4348661962879 349.77106768080404) rotate(0 65 12)"><text x="65" y="20" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="20px" fill="white" text-anchor="middle" style="white-space:pre" direction="ltr">GPU Process</text></g></svg>
<div><div><figure><p><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 460" width="1400" height="920"><g stroke-linecap="round" transform="translate(10 190) rotate(0 150 40)"><path d="M20 0 M20 0 C89.71 2.15, 153.08 1.8, 280 0 M20 0 C91.67 -1.12, 164.37 -2.05, 280 0 M280 0 C294.55 0.65, 297.5 9.13, 300 20 M280 0 C293.47 -0.14, 303.05 2.68, 300 20 M300 20 C300.76 30.79, 302.25 37.43, 300 60 M300 20 C300.79 30.63, 298.77 43.92, 300 60 M300 60 C298.92 70.22, 292.76 82.1, 280 80 M300 60 C303.65 76.88, 289.69 79.88, 280 80 M280 80 C182.81 82.88, 82.82 78.66, 20 80 M280 80 C180.38 83.55, 81.28 84.17, 20 80 M20 80 C5.9 77.15, -0.94 74.36, 0 60 M20 80 C4.43 78.08, -1.16 76.21, 0 60 M0 60 C1.98 48.47, 0.76 37.6, 0 20 M0 60 C-2.66 49.83, -1.77 39.76, 0 20 M0 20 C1.9 5.84, 9.15 0.14, 20 0 M0 20 C2.53 7.39, 10.02 -3.99, 20 0" stroke="white" stroke-width="1" fill="none"></path></g><g transform="translate(70 210) rotate(0 89 22)"><text x="89" y="18" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="19.047619047619044px" fill="white" text-anchor="middle" style="white-space:pre" direction="ltr">Renderer process</text><text x="89" y="40" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="19.047619047619044px" fill="white" text-anchor="middle" style="white-space:pre" direction="ltr">(main)</text></g><g stroke-linecap="round" transform="translate(190 10) rotate(0 160 30)"><path d="M15 0 M15 0 C84.29 0.75, 156.29 1.62, 305 0 M15 0 C124.83 1, 236.31 0.6, 305 0 M305 0 C318.64 2.81, 317.4 1.25, 320 15 M305 0 C312.23 3.8, 316.37 4.58, 320 15 M320 15 C320.01 25.12, 318.7 35.45, 320 45 M320 15 C321.48 28.32, 317.98 38.65, 320 45 M320 45 C319.81 52.11, 311.55 62.63, 305 60 M320 45 C316.75 50.86, 314.59 62.02, 305 60 M305 60 C219.22 63.56, 131.09 59.02, 15 60 M305 60 C210.32 58.49, 113.62 58.99, 15 60 M15 60 C6.58 61.62, -0.46 52.01, 0 45 M15 60 C8.78 57.52, 2.39 56.34, 0 45 M0 45 C0.8 34.47, -1.65 31.89, 0 15 M0 45 C-1.07 38.57, 1.88 31.21, 0 15 M0 15 C3.63 4.02, 4.85 0.26, 15 0 M0 15 C-2.08 1.22, 1.82 -3.35, 15 0" stroke="white" stroke-width="1" fill="none"></path></g><g transform="translate(270 30) rotate(0 84 11)"><text x="0" y="18" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="19.047619047619044px" fill="white" text-anchor="start" style="white-space:pre" direction="ltr">Browser process</text></g><g stroke-linecap="round" transform="translate(390 190) rotate(0 150 40)"><path d="M20 0 M20 0 C78.65 -3.16, 135.31 0.38, 280 0 M20 0 C78.2 0.06, 139.19 -0.16, 280 0 M280 0 C290.35 3.41, 296.63 8.51, 300 20 M280 0 C294.86 0.06, 298.98 7.04, 300 20 M300 20 C297.49 27.95, 298.25 36.75, 300 60 M300 20 C299.19 30.23, 301.28 36.94, 300 60 M300 60 C303.99 70.27, 290.64 83.66, 280 80 M300 60 C302.63 75.26, 293.61 76.5, 280 80 M280 80 C214.16 78.62, 145.92 81.64, 20 80 M280 80 C221.04 79.12, 161.29 80.89, 20 80 M20 80 C2.99 83.23, -2.06 75.01, 0 60 M20 80 C9.25 77.12, 2.15 71.41, 0 60 M0 60 C-1.6 48.58, 0.85 39.11, 0 20 M0 60 C-2.43 45.5, 1.13 31.13, 0 20 M0 20 C0.78 5.54, 2.8 -1.35, 20 0 M0 20 C-0.43 9.45, 8.06 1.77, 20 0" stroke="white" stroke-width="1" fill="none"></path></g><g transform="translate(450 210) rotate(0 89 22)"><text x="89" y="18" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="19.047619047619044px" fill="white" text-anchor="middle" style="white-space:pre" direction="ltr">Renderer process</text><text x="89" y="40" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="19.047619047619044px" fill="white" text-anchor="middle" style="white-space:pre" direction="ltr">(iframe)</text></g><g mask="url(#mask-QWNoY6lUo_I-lRB1Zocts)" stroke-linecap="round"><g transform="translate(142.84848484848476 189) rotate(0 104.1087779157383 -60.86877761483192)"><path d="M-1.66 -0.86 C3.14 -11.06, -4.55 -51.56, 27.19 -60.92 C58.94 -70.29, 158.61 -47.06, 188.84 -57.05 C219.06 -67.05, 205.72 -110.54, 208.54 -120.88 M2.64 -3.8 C7.21 -13.72, -5.91 -48.72, 24.98 -58.34 C55.86 -67.96, 157.13 -51.37, 187.94 -61.53 C218.76 -71.69, 206.17 -109.85, 209.88 -119.29" stroke="#6991de" stroke-width="1" fill="none"></path></g><g transform="translate(142.84848484848476 189) rotate(0 104.1087779157383 -60.86877761483192)"><path d="M217.85 -90.67 C214.03 -95.39, 217.32 -102.77, 207.2 -121.25 M218.23 -91.43 C214.43 -99.15, 213.33 -107.5, 209.38 -120.39" stroke="#6991de" stroke-width="1" fill="none"></path></g><g transform="translate(142.84848484848476 189) rotate(0 104.1087779157383 -60.86877761483192)"><path d="M197.34 -91.44 C198.52 -95.73, 206.8 -102.92, 207.2 -121.25 M197.72 -92.2 C200.03 -99.75, 205.03 -107.87, 209.38 -120.39" stroke="#6991de" stroke-width="1" fill="none"></path></g></g><mask id="mask-QWNoY6lUo_I-lRB1Zocts"><rect x="0" y="0" fill="#fff" width="450" height="408"></rect><rect x="164.28880584060266" y="118.43921456949435" fill="#000" width="175" height="24" opacity="1"></rect></mask><g transform="translate(194.28880584060266 118.43921456949434) rotate(0 52.6684569236204 9.692007815673726)"><text x="59" y="19" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="20px" fill="#6991de" text-anchor="middle" style="white-space:pre" direction="ltr">1. Get capture</text></g><g stroke-linecap="round" transform="translate(190 390) rotate(0 160 30)"><path d="M15 0 M15 0 C109.41 -1.4, 208.81 -1.93, 305 0 M15 0 C95.9 1.71, 173.44 1.69, 305 0 M305 0 C315.89 -1.43, 316.25 7.9, 320 15 M305 0 C311.18 3.37, 319.03 3.75, 320 15 M320 15 C321.88 18.71, 316.6 29.55, 320 45 M320 15 C320.81 21.41, 321.35 32.53, 320 45 M320 45 C323.11 57.61, 313.78 63.18, 305 60 M320 45 C322.99 51.3, 311.8 61.12, 305 60 M305 60 C246.72 61.82, 182.17 61.54, 15 60 M305 60 C211.24 63.77, 118.79 64.12, 15 60 M15 60 C1.2 59.18, 0.79 51.84, 0 45 M15 60 C6.34 61.51, 4.21 56.68, 0 45 M0 45 C1.69 39.49, 3.52 24.39, 0 15 M0 45 C-1.91 34.28, -1.82 25.72, 0 15 M0 15 C-0.23 4.81, 3.08 -1.76, 15 0 M0 15 C-1.88 4.64, 1.17 -1.91, 15 0" stroke="white" stroke-width="1" fill="none"></path></g><g transform="translate(290 410) rotate(0 61.5 11)"><text x="0" y="18" font-family="Inconsolata, Monaco, Consolas, Courier New, Courier, monospace" font-size="19.047619047619044px" fill="white" text-anchor="start" style="white-space:pre" direction="ltr">GPU process</text></g></svg></p><figcaption>doing this 60 times a second isn't very efficient</figcaption></figure></div><div><p><a href="https://source.chromium.org/chromium/chromium/src/+/main:third_party/blink/public/web/web_local_frame.h;drc=e95ee489e7a4aee5408d8bb0e13bebc61adcca0d;l=786" target="_blank"><code>CapturePaintPreview</code></a> is great for <code>html2svg</code>, but it's not designed for real-time rendering. It's using IPC calls to correctly support out-of-process iframes, making roundtrips between the browser, GPU, and renderer processes. It downloads hardware accelerated images from the GPU, explaining the surprising memory bandwidth usage. We can disable the fetching, and even disable hardware acceleration, but we still have an expensive IPC machinery holding us back.</p><p>Software rendering is still very common, it even used to be the default if you can believe it. It was fairly easy back in the single-process days, but nowadays shared memory regions are configured to efficiently render using multiple processes. If we can get our pixels into one of these memory regions, we would just have to notify our browser process using a simple IPC message.</p></div></div>
<div><pre><code><span>void</span> <span>LayeredWindowUpdater</span><span>::</span><span>OnAllocatedSharedMemory</span><span>(</span>
    <span>const</span> gfx<span>::</span>Size<span>&amp;</span> pixel_size<span>,</span>
    base<span>::</span>UnsafeSharedMemoryRegion region
<span>)</span> <span>{</span>
    <span>if</span> <span>(</span>region<span>.</span><span>IsValid</span><span>(</span><span>)</span><span>)</span>
        shm_mapping_ <span>=</span> region<span>.</span><span>Map</span><span>(</span><span>)</span><span>;</span>
<span>}</span>

<span>void</span> <span>LayeredWindowUpdater</span><span>::</span><span>Draw</span><span>(</span>
    <span>const</span> gfx<span>::</span>Rect<span>&amp;</span> damage_rect<span>,</span>
    DrawCallback draw_callback
<span>)</span> <span>{</span>
    <span>carbonyl_draw_bitmap</span><span>(</span>
        shm_mapping_<span>.</span><span><span>GetMemoryAs</span><span><span>&lt;</span><span>uint8_t</span><span>&gt;</span></span></span><span>(</span><span>)</span><span>,</span>
        shm_mapping_<span>.</span><span>size</span><span>(</span><span>)</span>
    <span>)</span><span>;</span>

    std<span>::</span><span>move</span><span>(</span>draw_callback<span>)</span><span>.</span><span>Run</span><span>(</span><span>)</span><span>;</span>
<span>}</span>
</code></pre></div>
<hr>
<p>We solved the bitmap problem, now how can we extract text data? This data lives in the renderer process, but our windowing code lives in the browser process. We need to make the renderer interact with the browser process.</p>
<h2 id="mojo">Mojo<a aria-label="Link to this title" href="https://fathy.fr/carbonyl#mojo"></a></h2>
<div><div><pre><code><span>// Our C++ bindings will be in the carbonyl::mojom namespace</span>
<span>module</span> carbonyl<span>.</span>mojom<span>;</span>

<span>// Import existing bindings to common structures</span>
<span>import</span> <span>"ui/gfx/geometry/mojom/geometry.mojom"</span><span>;</span>
<span>import</span> <span>"skia/public/mojom/skcolor.mojom"</span><span>;</span>

<span>// Define a structure to hold text to render</span>
<span>struct</span> TextData <span>{</span>
    <span>// An UTF-8 string with the contents</span>
    <span>string</span> contents<span>;</span>
    <span>// Bounds, size only defined for clearing</span>
    gfx<span>.</span>mojom<span>.</span>RectF bounds<span>;</span>
    <span>// Color of the text</span>
    skia<span>.</span>mojom<span>.</span>SkColor color<span>;</span>
<span>}</span><span>;</span>

<span>// The browser process runs this service</span>
<span>interface</span> <span>CarbonylRenderService</span> <span>{</span>
    <span>// The renderer process calls this method</span>
    <span>DrawText</span><span>(</span>array<span>&lt;</span>TextData<span>&gt;</span> data<span>)</span><span>;</span>
<span>}</span><span>;</span>
</code></pre></div><div><p>Mojo is a library for inter-process communication. It defines an IDL for serializing data which supports native handles (i.e. file descriptors, shared memory regions, callbacks), and can be used to generate C++, Java (Android), and JavaScript (DevTools) bindings. It's extensively documented, and fairly simple to use.</p><p>We'll start by making an interface <code>CarbonylRenderService</code> that runs on the browser process, with a method <code>DrawText</code> called from the renderer process.</p></div></div>
<p>This <code>.mojom</code> code generates C++ temporary files which we can then include to write the implementation code.</p>
<p>Mojo receivers such as our service are part of the native handles we can send between processes, to register the implementation we just need to add it to the <code>BrowserInterfaceBroker</code>, which will get called by the renderer through <code>BrowserInterfaceBrokerProxy</code>:</p>
<div><pre><code>map<span>-&gt;</span><span><span>Add</span><span><span>&lt;</span>carbonyl<span>::</span>mojom<span>::</span>CarbonylRenderService<span>&gt;</span></span></span><span>(</span>
    base<span>::</span><span>BindRepeating</span><span>(</span><span>&amp;</span>RenderFrameHostImpl<span>::</span>GetCarbonylRenderService<span>,</span>
                        base<span>::</span><span>Unretained</span><span>(</span>host<span>)</span><span>)</span><span>)</span><span>;</span>
</code></pre></div>
<div><pre><code><span>GetBrowserInterfaceBroker</span><span>(</span><span>)</span><span>.</span><span>GetInterface</span><span>(</span>
  std<span>::</span><span>move</span><span>(</span>carbonyl_render_service_receiver_<span>)</span>
<span>)</span><span>;</span>
</code></pre></div>
<p>Now, we need to get our text data without any expensive round-trip. Blink has a <code>GetPaintRecord()</code> method to get the latest paint data for a page, but it's not behind a public API, which we need because our code runs in the content renderer. Ideally we should hook into the compositor (<code>cc</code>), but it's way more involved. It's dirty but we can workaround this by casting to the private <code>blink::WebViewImpl</code>:</p>
<pre><code><span>auto</span><span>*</span> view <span>=</span> <span><span>static_cast</span><span><span>&lt;</span>blink<span>::</span>WebViewImpl<span>*</span><span>&gt;</span></span></span><span>(</span><span>GetWebFrame</span><span>(</span><span>)</span><span>-&gt;</span><span>View</span><span>(</span><span>)</span><span>)</span><span>;</span>

view<span>-&gt;</span><span>MainFrameImpl</span><span>(</span><span>)</span><span>-&gt;</span><span>GetFrame</span><span>(</span><span>)</span><span>-&gt;</span><span>View</span><span>(</span><span>)</span><span>-&gt;</span><span>GetPaintRecord</span><span>(</span><span>)</span><span>.</span><span>Playback</span><span>(</span><span>&amp;</span>canvas<span>)</span><span>;</span>
carbonyl_render_service_<span>-&gt;</span><span>DrawText</span><span>(</span>std<span>::</span><span>move</span><span>(</span>data<span>)</span><span>)</span><span>;</span>
</code></pre>
<p>Surprise after the first run: the text content doesn't follow the bitmap. Aaah, scrolling and animating is done on the compositor thread, which frees the main thread and makes everything smoother. Let's procastinate doing things right by adding <code>--disable-threaded-scrolling</code> <code>--disable-threaded-animation</code> to the command line arguments.</p>
<p><figure><video src="https://fathy.fr/assets/86793ae6785e6325c6db.mp4" autoplay="" loop="" controls=""></video><figcaption>Threaded compositing enabled</figcaption></figure><figure><video src="https://fathy.fr/assets/470674f7207bd5e55a41.mp4" autoplay="" loop="" controls=""></video><figcaption>Threaded compositing disabled</figcaption></figure></p>
<p>Pretty smooth, it'll be even smoother when threaded compositing is fixed! And we've fixed our biggest problem: we don't use any CPU when idling, and scrolling consumes ~15%.</p>
<h2 id="layout">Layout<a aria-label="Link to this title" href="https://fathy.fr/carbonyl#layout"></a></h2>
<div><div><pre><code><span>auto</span> font <span>=</span> state<span>.</span><span>StyleBuilder</span><span>(</span><span>)</span><span>.</span><span>GetFontDescription</span><span>(</span><span>)</span><span>;</span>

font<span>.</span><span>SetStretch</span><span>(</span><span>ExtraExpandedWidthValue</span><span>(</span><span>)</span><span>)</span><span>;</span>
font<span>.</span><span>SetKerning</span><span>(</span>FontDescription<span>::</span>kNoneKerning<span>)</span><span>;</span>
font<span>.</span><span>SetComputedSize</span><span>(</span><span>11.75</span> <span>/</span> <span>7.0</span><span>)</span><span>;</span>
font<span>.</span><span>SetGenericFamily</span><span>(</span>FontDescription<span>::</span>kMonospaceFamily<span>)</span><span>;</span>
font<span>.</span><span>SetIsAbsoluteSize</span><span>(</span><span>true</span><span>)</span><span>;</span>
state<span>.</span><span>StyleBuilder</span><span>(</span><span>)</span><span>.</span><span>SetFontDescription</span><span>(</span>font<span>)</span><span>;</span>
state<span>.</span><span>StyleBuilder</span><span>(</span><span>)</span><span>.</span><span>SetLineHeight</span><span>(</span><span>Length</span><span>::</span><span>Fixed</span><span>(</span><span>14.0</span> <span>/</span> <span>7.0</span><span>)</span><span>)</span><span>;</span>
</code></pre></div><div><p>Thing is, we can only render one font-size, but Blink doesn't know that. This causes the layout to be messed up, with text chunks overlapping or overly spaced. This is especially visible on websites with a lot of textual content and links like Wikipedia.</p><p>Another dirty - yet effective - hack we can use is forcing a monospaced font on every element. We can do that by adding some code to <a href="https://source.chromium.org/chromium/chromium/src/+/main:third_party/blink/renderer/core/css/resolver/style_resolver.cc;l=954;drc=a67b5c344bfa8645cb2f66685e010b425ac19ea1" target="_blank"><code>StyleResolver::ResolveStyle</code></a>.</p></div></div>
<hr>

<h2 id="lodpi">LoDPI<a aria-label="Link to this title" href="https://fathy.fr/carbonyl#lodpi"></a></h2>
<div><div><pre><code><span>// static</span>
<span>float</span> <span>Display</span><span>::</span><span>GetForcedDeviceScaleFactor</span><span>(</span><span>)</span> <span>{</span>
    <span>return</span> <span>1.0</span> <span>/</span> <span>7.0</span><span>;</span>
<span>}</span>

<span>// static</span>
<span>bool</span> <span>Display</span><span>::</span><span>HasForceDeviceScaleFactor</span><span>(</span><span>)</span> <span>{</span>
    <span>return</span> <span>true</span><span>;</span>
<span>}</span>
</code></pre></div><div><p>One expensive step in our rendering pipeline is downscaling: we need to resize the framebuffer from its virtual space to its physical space. What we're doing is kind of the opposite of HiDPI rendering, whose most common ratio is 2, which means 1 pixel on the web equals 4 pixels on the screen. Our ratio is <code>1 / 7</code> which means 49 pixels on the web renders to 1 block on our terminal.</p><p>The annoying thing about HiDPI is that it can make rendering ~4x slower, whereas Carbonyl LoDPI® makes rendering run ~49x faster. We just need to force our scaling into the <a href="https://source.chromium.org/chromium/chromium/src/+/main:ui/display/display.h;drc=8d3b4cc374e70dcfef858a355ac13e75d08c4f67;l=31" target="_blank"><code>Display</code></a> class.</p></div></div>
<h2 id="color">Color<a aria-label="Link to this title" href="https://fathy.fr/carbonyl#color"></a></h2>
<p>I looked for examples of RGB color conversion to <code>xterm-256</code> but the code I found was either wrong or slow because it did a nearest neighbor search. We're going to do it for every pixel so it should run fast.</p>
<p>The formula for the conversion is fairly simple, assuming color values between 0 and 1: <code>16 + r * 5 * 36 + g * 5 * 6 + b * 5</code>.</p>
<div><div><pre><code><span>pub</span> <span>fn</span> <span>to_xterm</span><span>(</span><span>&amp;</span><span>self</span><span>)</span> <span>-&gt;</span> <span>u8</span> <span>{</span>
    <span>let</span> r <span>=</span> <span>(</span><span>self</span><span>.</span>r <span>as</span> <span>f32</span> <span>-</span> <span>(</span><span>95.0</span> <span>-</span> <span>40.0</span><span>)</span><span>)</span><span>.</span><span>max</span><span>(</span><span>0.0</span><span>)</span> <span>/</span> <span>40.0</span><span>;</span>
    <span>let</span> g <span>=</span> <span>(</span><span>self</span><span>.</span>g <span>as</span> <span>f32</span> <span>-</span> <span>(</span><span>95.0</span> <span>-</span> <span>40.0</span><span>)</span><span>)</span><span>.</span><span>max</span><span>(</span><span>0.0</span><span>)</span> <span>/</span> <span>40.0</span><span>;</span>
    <span>let</span> b <span>=</span> <span>(</span><span>self</span><span>.</span>b <span>as</span> <span>f32</span> <span>-</span> <span>(</span><span>95.0</span> <span>-</span> <span>40.0</span><span>)</span><span>)</span><span>.</span><span>max</span><span>(</span><span>0.0</span><span>)</span> <span>/</span> <span>40.0</span><span>;</span>

    <span>(</span><span>16.0</span> <span>+</span>
        r<span>.</span><span>round</span><span>(</span><span>)</span> <span>*</span> <span>36.0</span> <span>+</span>
        g<span>.</span><span>round</span><span>(</span><span>)</span> <span>*</span> <span>6.0</span> <span>+</span>
        b<span>.</span><span>round</span><span>(</span><span>)</span><span>)</span> <span>as</span> <span>u8</span>
<span>}</span>
</code></pre></div><div><p>The twist that most code online gets wrong is that the 6 color levels are not linear: 0, 95, 135, 175, 215, 255; there is a 95 gap between the first and second values, and 40 for the rest.</p><p>It makes sense to limit the dark range, color differences are more visible with bright colors. For us, it means that we can convert a value between 0 and 255 using <code>max(0, color - 95 - 40) / 40</code>.</p></div></div>
<hr>
<hr>
<div><div><pre><code><span>pub</span> <span>fn</span> <span>to_xterm</span><span>(</span><span>&amp;</span><span>self</span><span>)</span> <span>-&gt;</span> <span>u8</span> <span>{</span>
    <span>if</span> <span>self</span><span>.</span><span>max_val</span><span>(</span><span>)</span> <span>-</span> <span>self</span><span>.</span><span>min_val</span><span>(</span><span>)</span> <span>&lt;</span> <span>8</span> <span>{</span>
        <span>match</span> <span>self</span><span>.</span>r <span>{</span>
            <span>0</span><span>..=</span><span>4</span> <span>=&gt;</span> <span>16</span><span>,</span>
            <span>5</span><span>..=</span><span>8</span> <span>=&gt;</span> <span>232</span><span>,</span>
            <span>238</span><span>..=</span><span>246</span> <span>=&gt;</span> <span>255</span><span>,</span>
            <span>247</span><span>..=</span><span>255</span> <span>=&gt;</span> <span>231</span><span>,</span>
            r <span>=&gt;</span> <span>232</span> <span>+</span> <span>(</span>r <span>-</span> <span>8</span><span>)</span> <span>/</span> <span>10</span><span>,</span>
        <span>}</span>
    <span>}</span> <span>else</span> <span>{</span>
        <span>let</span> scale <span>=</span> <span>5.0</span> <span>/</span> <span>200.0</span><span>;</span>

        <span>(</span><span>16.0</span>
            <span>+</span> <span>self</span>
                <span>.</span><span>cast</span><span>::</span><span>&lt;</span><span>f32</span><span>&gt;</span><span>(</span><span>)</span>
                <span>.</span><span>mul_add</span><span>(</span>scale<span>,</span> scale <span>*</span> <span>-</span><span>55.0</span><span>)</span>
                <span>.</span><span>max</span><span>(</span><span>0.0</span><span>)</span>
                <span>.</span><span>round</span><span>(</span><span>)</span>
                <span>.</span><span>dot</span><span>(</span><span>(</span><span>36.0</span><span>,</span> <span>6.0</span><span>,</span> <span>1.0</span><span>)</span><span>)</span><span>)</span> <span>as</span> <span>u8</span>
    <span>}</span>
<span>}</span>
</code></pre></div><div><p>The conversion itself can be thought of as a dot product of <code>(r, g, b)</code> and <code>(36, 6, 1)</code>. We can move the substraction to an <code>mul_add</code> call to help the compiler use a fused multiply-add instruction.</p><p>The last step is grayscale: our xterm profile offers 256 colors, there are the 216 colors from the RGB cube (<code>6 * 6 * 6</code>), the 16 configurable system colors, and 24 gray levels which go from <code>rgb(8,8,8)</code> to <code>rgb(238,238,238)</code>.</p><p>To find out if a color is on a grayscale, we can substract its minimal value to its maximum value and check if it's under a threshold, let's say 8.</p></div></div>

<p>We still have one tiny problem: how can you detect if a terminal supports true-color
or 256 colors? A quick Google search leads us to the <code>COLORTERM</code> environment variable,
which is <code>24bit</code> or <code>truecolor</code> if true-color is supported. But that won't work in
Docker or SSH, which are our primary targets.</p>
<div><div><div><pre><code><span>self</span><span>.</span>sequence <span>=</span> <span>match</span> <span>self</span><span>.</span>sequence <span>{</span>
</code></pre></div><pre><code><span>    // </span><span>^[P</span><span>1</span><span>$r0;48:2:1:13:37:42m^[\</span></code></pre><pre><code>    <span>Code</span> <span>=&gt;</span> <span>match</span> key <span>{</span>
        <span>b'0'</span> <span>|</span> <span>b'1'</span> <span>=&gt;</span> <span>Type</span><span>(</span>key<span>)</span><span>,</span>
        _ <span>=&gt;</span> <span>control_flow!</span><span>(</span><span>break</span><span>)</span><span>?</span><span>,</span>
    <span>}</span><span>,</span>
</code></pre><pre><code><span>    // </span><span>^[P1</span><span>$</span><span>r0;48:2:1:13:37:42m^[\</span></code></pre><pre><code>    <span>Type</span><span>(</span>code<span>)</span> <span>=&gt;</span> <span>match</span> key <span>{</span>
        <span>b'$'</span> <span>=&gt;</span> <span>Status</span><span>(</span><span>StatusParser</span><span>::</span><span>new</span><span>(</span>code<span>)</span><span>)</span><span>,</span>
        <span>b'+'</span> <span>=&gt;</span> <span>Resource</span><span>(</span><span>ResourceParser</span><span>::</span><span>new</span><span>(</span>code<span>)</span><span>)</span><span>,</span>
        _ <span>=&gt;</span> <span>control_flow!</span><span>(</span><span>break</span><span>)</span><span>?</span><span>,</span>
    <span>}</span><span>,</span>
</code></pre><pre><code><span>    // </span><span>^[P1$</span><span>r0;48:2:1:13:37:42m^[\</span></code></pre><pre><code>    <span>Status</span><span>(</span><span>ref</span> <span>mut</span> status<span>)</span> <span>=&gt;</span> <span>return</span> status<span>.</span><span>parse</span><span>(</span>key<span>)</span><span>,</span>
    <span>Resource</span><span>(</span><span>ref</span> <span>mut</span> resource<span>)</span> <span>=&gt;</span> <span>return</span> resource<span>.</span><span>parse</span><span>(</span>key<span>)</span><span>,</span>
<span>}</span><span>;</span>
</code></pre><div><pre><code><span>self</span><span>.</span>sequence <span>=</span> <span>match</span> <span>self</span><span>.</span>sequence <span>{</span>
</code></pre></div><pre><code><span>    // </span><span>^[P1$</span><span>r</span><span>0;48:2:1:13:37:42m^[\</span></code></pre><pre><code>    <span>Start</span> <span>=&gt;</span> <span>match</span> key <span>{</span>
        <span>b'r'</span> <span>=&gt;</span> <span>Value</span><span>,</span>
        _ <span>=&gt;</span> <span>control_flow!</span><span>(</span><span>break</span><span>)</span><span>?</span><span>,</span>
    <span>}</span><span>,</span>
</code></pre><pre><code><span>    // </span><span>^[P1$r</span><span>0;48:2:1:13:37:42m^[</span><span>\</span></code></pre><pre><code>    <span>Value</span> <span>=&gt;</span> <span>match</span> key <span>{</span>
</code></pre><pre><code><span>        // </span><span>^[P1$r0;48:2:1:13:37:42m</span><span>^[</span><span>\</span></code></pre><pre><code>        <span>0x1b</span> <span>=&gt;</span> <span>self</span><span>.</span><span>terminate</span><span>(</span><span>)</span><span>,</span>
</code></pre><pre><code><span>        // </span><span>^[P1$r0</span><span>;</span><span>48:2:1:13:37:42m^[\</span></code></pre><pre><code>        <span>b';'</span> <span>=&gt;</span> <span>self</span><span>.</span><span>push_value</span><span>(</span><span>)</span><span>,</span>
</code></pre><pre><code><span>        // </span><span>^[P1$r</span><span>0</span><span>;</span><span>48:2:1:13:37:42m</span><span>^[\</span></code></pre><pre><code>        <span>char</span> <span>=&gt;</span> <span>self</span><span>.</span><span>push_char</span><span>(</span><span>char</span><span>)</span><span>,</span>
    <span>}</span><span>,</span>
</code></pre><pre><code><span>    // </span><span>^[P1$r0;48:2:1:13:37:42m^[</span><span>\</span></code></pre><pre><code>    <span>Terminator</span> <span>=&gt;</span> <span>control_flow!</span><span>(</span><span>break</span> <span>self</span><span>.</span><span>parse_event</span><span>(</span>key<span>)</span><span>)</span><span>?</span><span>,</span>
<span>}</span><span>;</span>
</code></pre></div><div><p>A trick we can use is a DCS (Device Control Sequence) to fetch a setting value, like the current background color. If we set an RGB value and get an RGB value back, we can enable true-color.</p><p>You can try it by running the following on your terminal:</p><pre><code>$ <span>printf</span> <span>"<span title="\e">\e</span>[48;2;13;37;42m<span title="\e">\e</span>P\<span>$qm</span><span title="\e">\e</span><span title="\\">\\</span>"</span><span>;</span> <span>cat</span>
</code></pre><ul>
<li><code>\e</code>: start escape sequence<!-- -->
<ul>
<li><code>[</code>: introduce control sequence</li>
<li><code>48</code>: set foreground</li>
<li><code>2</code>: using an RGB color</li>
<li><code>13</code>: R is 13</li>
<li><code>37</code>: G is 37</li>
<li><code>42</code>: B is 42</li>
<li><code>m</code>: select graphic rendition</li>
</ul>
</li>
<li><code>\e</code>: start escape sequence<!-- -->
<ul>
<li><code>P</code>: introduce device control sequence</li>
<li><code>$</code>: enter status mode</li>
<li><code>q</code>: query current setting</li>
<li><code>m</code>: select graphic rendition</li>
</ul>
</li>
</ul><p>If the commands are supported, you should get the following output with a dark turquoise background:</p><p><code> ^[P1$r0;48:2:1:13:37:42m^[\ </code></p><p>This is what the terminal emulator sends to stdin, and what we can parse to toggle true-color on.</p></div></div>
<h2 id="title">Title<a aria-label="Link to this title" href="https://fathy.fr/carbonyl#title"></a></h2>
<div><p><figure><video src="https://fathy.fr/assets/4e858fad04b7bdef5b92.mp4" autoplay="" loop="" controls=""></video><figcaption>nice</figcaption></figure></p><div><p>A few xterm sequences allow setting the terminal window title, we could use that to display the current page title.</p><pre><code><span>fn</span> <span>set_title</span><span>(</span>title<span>:</span> <span>&amp;</span><span>str</span><span>)</span> <span>{</span>
    <span>// Set icon name and window title to string</span>
    <span>println!</span><span>(</span><span>"\x1b]0;{}\x07"</span><span>,</span> title<span>)</span><span>;</span>
    <span>// Set icon name to string</span>
    <span>println!</span><span>(</span><span>"\x1b]1;{}\x07"</span><span>,</span> title<span>)</span><span>;</span>
    <span>// Set window title to string</span>
    <span>println!</span><span>(</span><span>"\x1b]2;{}\x07"</span><span>,</span> title<span>)</span><span>;</span>
<span>}</span>
</code></pre><p>To get notified when the title changes, we can simply implement <a href="https://source.chromium.org/chromium/chromium/src/+/main:content/public/browser/web_contents_observer.h;drc=d868f781dd811d46a0a5ae88ccaf4b5afb0f0f3c;l=576" target="_blank"><code>WebContentsObserver::TitleWasSet()</code></a>:</p><pre><code><span>void</span> <span>HeadlessWebContentsImpl</span><span>::</span><span>TitleWasSet</span><span>(</span>content<span>::</span>NavigationEntry<span>*</span> entry<span>)</span> <span>{</span>
    carbonyl<span>::</span><span>Renderer</span><span>::</span><span>Main</span><span>(</span><span>)</span><span>-&gt;</span><span>SetTitle</span><span>(</span>
        base<span>::</span><span>UTF16ToUTF8</span><span>(</span>entry<span>-&gt;</span><span>GetTitleForDisplay</span><span>(</span><span>)</span><span>)</span>
    <span>)</span><span>;</span>
<span>}</span>
</code></pre></div></div>
<h2 id="final-thoughts">Final thoughts<a aria-label="Link to this title" href="https://fathy.fr/carbonyl#final-thoughts"></a></h2>
<p>That's all for today folks, <a href="https://github.com/fathyb/carbonyl" target="_blank">check out Carbonyl on GitHub</a>!</p>
<p>This was my first Rust project and I finally get the hype now. What a cool language!</p>
<h3 id="stay-tuned">Stay tuned<a aria-label="Link to this title" href="https://fathy.fr/carbonyl#stay-tuned"></a></h3>
<p>The post for next month <a href="https://fathy.fr/every-sin">will be a visual introduction to Fourier Analysis</a>. After that, we'll look into a speculative JS VM in Rust.</p>
<p>Use <a href="https://fathy.fr/rss.xml" target="_blank">the RSS feed</a> to stay tuned, you can also watch <a href="https://github.com/fathyb/fathy.fr" target="_blank">the website repo</a> for releases on GitHub.</p>
<h3 id="discuss">Discuss<a aria-label="Link to this title" href="https://fathy.fr/carbonyl#discuss"></a></h3>
<p><a href="https://news.ycombinator.com/item?id=34547259" target="_blank">Discuss on HN</a> - <a href="https://old.reddit.com/r/programming/comments/10mnt0l/forking_chrome_to_render_in_a_terminal/" target="_blank">Discuss on r/programming</a></p></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Evolving the OCaml Programming Language (2025) [pdf] (146 pts)]]></title>
            <link>https://kcsrk.info/slides/Evolution_Ashoka_2025.pdf</link>
            <guid>45133652</guid>
            <pubDate>Fri, 05 Sep 2025 00:05:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kcsrk.info/slides/Evolution_Ashoka_2025.pdf">https://kcsrk.info/slides/Evolution_Ashoka_2025.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=45133652">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[I ditched Spotify and set up my own music stack (250 pts)]]></title>
            <link>https://leshicodes.github.io/blog/spotify-migration/</link>
            <guid>45133109</guid>
            <pubDate>Thu, 04 Sep 2025 22:47:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://leshicodes.github.io/blog/spotify-migration/">https://leshicodes.github.io/blog/spotify-migration/</a>, See on <a href="https://news.ycombinator.com/item?id=45133109">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__blog-post-container">
<!-- -->
<h2 id="why-i-ditched-spotify-and-how-i-set-up-my-own-music-stack">Why I Ditched Spotify, and How I Set Up My Own Music Stack<a href="#why-i-ditched-spotify-and-how-i-set-up-my-own-music-stack" aria-label="Direct link to Why I Ditched Spotify, and How I Set Up My Own Music Stack" title="Direct link to Why I Ditched Spotify, and How I Set Up My Own Music Stack">​</a></h2>
<p>For years, I relied on Spotify like millions of others. The convenience was undeniable stream anything, anywhere, discover new music through algorithms, and share playlists with friends. But over time, several issues became impossible to ignore: <a href="https://en.wikipedia.org/wiki/Criticism_of_Spotify#Allegations_of_unfair_artist_compensation" target="_blank" rel="noopener noreferrer">artists getting paid fractions of pennies per stream</a>, <a href="https://en.wikipedia.org/wiki/Controversy_over_fake_artists_on_Spotify" target="_blank" rel="noopener noreferrer">fake Artists and ghost Tracks</a>, <a href="https://www.musicradar.com/music-tech/recording/ill-never-be-able-to-sing-that-perfectly-in-tune-but-i-dont-want-to-im-human-meet-the-real-artists-that-are-victims-of-ai-fake-tracks" target="_blank" rel="noopener noreferrer">AI music and impersonation</a>, <a href="https://www.techradar.com/audio/spotify/spotify-introduces-face-scanning-age-checks-for-uk-uses-as-some-furious-fans-threaten-to-return-to-piracy" target="_blank" rel="noopener noreferrer">creepy age verification complicity</a> and the fact that despite paying monthly, I never actually owned anything.
So I decided to take back control of my music experience. Here's how I built my own self-hosted music streaming setup that gives me everything Spotify offered and more.</p>
<div><p><span><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</p><p>There are components of this post which may be improved if you zoom in with your device. The mermaid diagram and code blocks in particular may be hard to read on smaller screens.</p></div>
<h2 id="high-level-overview">High Level Overview<a href="#high-level-overview" aria-label="Direct link to High Level Overview" title="Direct link to High Level Overview">​</a></h2>
<!-- -->
<h2 id="the-components">The Components<a href="#the-components" aria-label="Direct link to The Components" title="Direct link to The Components">​</a></h2>
<h3 id="music-player-navidrome">Music Player: Navidrome<a href="#music-player-navidrome" aria-label="Direct link to Music Player: Navidrome" title="Direct link to Music Player: Navidrome">​</a></h3>
<p>At the core of my setup is <a href="https://www.navidrome.org/docs/installation/docker/" target="_blank" rel="noopener noreferrer">Navidrome</a>, an open-source music server that handles streaming your personal music collection.</p>
<p><img src="https://leshicodes.github.io/assets/images/navidrome-62470ba14db4ff4a00eebfc17a06b528.png" alt="navidrome"></p><p>To access my music from anywhere, I expose Navidrome via a <a href="https://developers.cloudflare.com/cloudflare-one/connections/connect-networks/" target="_blank" rel="noopener noreferrer">CloudFlare Tunnel</a>, which provides secure access without exposing my home IP address or dealing with port forwarding.</p>
<p>For client apps, I use:</p>
<ul>
<li>Browser: Navidrome's built-in web player works perfectly</li>
<li>iOS: <a href="https://apps.apple.com/us/app/play-sub-music-streamer/id955329386" target="_blank" rel="noopener noreferrer">Play<!-- -->:Sub</a> connects seamlessly</li>
<li>Android: <a href="https://symfonium.app/" target="_blank" rel="noopener noreferrer">Symfonium</a> offers excellent playback quality and features</li>
<li>Desktop: <a href="https://github.com/jeffvli/feishin" target="_blank" rel="noopener noreferrer">Feishin</a> provides a native app experience with synced lyrics</li>
</ul>
<p><img src="https://leshicodes.github.io/assets/images/feishin-847514abddff0b9ca914f06eaf00ad1e.png" alt="feishin"></p><div><p><span><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Definition</p><p><strong>Scrobbling</strong>: (Internet slang) To publish one's music-listening habits to the Internet via software, in order to track when and how often certain songs are played.</p></div>
<p>Every track I play through this setup automatically scrobbles to my <a href="https://www.last.fm/user/dudelaaaa" target="_blank" rel="noopener noreferrer">Last.fm</a> account, which becomes important for music discovery later.</p>
<h3 id="music-collection-management-lidarr">Music Collection Management: Lidarr<a href="#music-collection-management-lidarr" aria-label="Direct link to Music Collection Management: Lidarr" title="Direct link to Music Collection Management: Lidarr">​</a></h3>
<p><img src="https://leshicodes.github.io/assets/images/lidarr-0e2bac8a263d84420f5dc5dc2f3e765e.png" alt="lidarr"></p><p><a href="https://github.com/Lidarr/Lidarr" target="_blank" rel="noopener noreferrer">Lidarr</a> helps manage my music collection by tracking artists and albums I own or purchase. It can monitor for new releases from favorite artists and helps organize my library.</p>
<div><p><span><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>warning</p><p>Lidarr is just a tool. Like any tool, it can be misused.
Yes, people <em>could</em> point it at less-than-legal sources. No, I'm not telling you to do that.
If you want to support artists, buy their work. If you don't, don't pretend Spotify streams are "support."</p></div>
<p><strong>Important Note</strong>: Always ensure you're obtaining music through legal channels such as:</p>
<ul>
<li>Digital purchases (Bandcamp, iTunes, Amazon, etc.)</li>
<li>Ripping CDs you've purchased</li>
<li>Free legal downloads offered by artists</li>
<li>Music available under Creative Commons licenses</li>
</ul>
<p>My setup uses <a href="https://hub.docker.com/r/linuxserver/sabnzbd" target="_blank" rel="noopener noreferrer">sabnzbd</a> integrated with Lidarr for handling downloads of content I've purchased. Both services run in Docker containers and are NOT exposed to the internet for security.</p>
<h3 id="synced-lyrics-lrcget-kasm">Synced Lyrics: lrcget-kasm<a href="#synced-lyrics-lrcget-kasm" aria-label="Direct link to Synced Lyrics: lrcget-kasm" title="Direct link to Synced Lyrics: lrcget-kasm">​</a></h3>
<p><img src="https://leshicodes.github.io/assets/images/lrcget-9e4a5896f560d23b5e5dfb8028346ce0.png" alt="lrcget"></p><p>A feature I missed from Spotify was synced lyrics. <a href="https://github.com/ShadowsDieThrice/lrcget-kasm" target="_blank" rel="noopener noreferrer">lrcget-kasm</a> fills this gap by mass-downloading LRC synced lyrics files for my music library.</p>
<p>Since lrcget is GUI-only (no CLI version yet), I'm using a containerized version of it via <a href="https://www.kasmweb.com/" target="_blank" rel="noopener noreferrer">Kasm</a> and access it through my browser. It's a bit resource-intensive, so I only run it when adding new music or updating lyrics.</p>
<p>I've opened a <a href="https://github.com/tranxuanthang/lrcget/discussions/245" target="_blank" rel="noopener noreferrer">feature request</a> for a CLI version, which would make this process more automation-friendly.</p>
<h3 id="music-discovery-lidify">Music Discovery: Lidify<a href="#music-discovery-lidify" aria-label="Direct link to Music Discovery: Lidify" title="Direct link to Music Discovery: Lidify">​</a></h3>
<p><img src="https://leshicodes.github.io/assets/images/lidify-7c9fe9c04989d7b44fe3f586bcd83968.png" alt="lidify"></p><p>One of Spotify's strongest features was music discovery. For this, I use <a href="https://github.com/TheWicklowWolf/Lidify" target="_blank" rel="noopener noreferrer">Lidify</a>, which connects to my Lidarr library and Last.fm account to generate recommendations.</p>
<p>I've also connected my Last.fm scrobbles to <a href="https://listenbrainz.org/user/leshicodes/" target="_blank" rel="noopener noreferrer">ListenBrainz</a>, which promises to build weekly discovery playlists similar to Spotify's in the future.</p>
<h2 id="the-results">The Results<a href="#the-results" aria-label="Direct link to The Results" title="Direct link to The Results">​</a></h2>
<p>After several months with this setup, I'm extremely satisfied with the results:</p>
<h3 id="how-my-solution-compares-to-spotify">How My Solution Compares to Spotify<a href="#how-my-solution-compares-to-spotify" aria-label="Direct link to How My Solution Compares to Spotify" title="Direct link to How My Solution Compares to Spotify">​</a></h3>
<table><thead><tr><th>Feature</th><th>Spotify</th><th>My Self-Hosted Stack</th></tr></thead><tbody><tr><td>Music Quality</td><td>Up to 320kbps</td><td>Unlimited (FLAC/lossless)</td></tr><tr><td>Monthly Cost</td><td>$9.99-$14.99</td><td>One-time server setup + storage</td></tr><tr><td>Artist Payment</td><td>~$0.003-0.005 per stream</td><td>Direct support via purchases</td></tr><tr><td>Music Ownership</td><td>Rental only</td><td>Full ownership forever</td></tr><tr><td>Offline Access</td><td>Limited downloads</td><td>Complete library available</td></tr><tr><td>Privacy</td><td>Data collection &amp; tracking</td><td>Complete privacy</td></tr><tr><td>Content Permanence</td><td>Can disappear anytime</td><td>Never removed unless I choose</td></tr></tbody></table>
<p>The initial setup took a weekend, but maintenance is minimal. When I want new music, Lidarr handles it automatically. If I need to manually add something, I just drop the files in the right folder and Navidrome indexes them immediately.</p>
<p>Is it for everyone? No. But if you care about music, value ownership, and have basic technical skills, building your own music streaming solution is both achievable and rewarding. The freedom from corporate streaming platforms is worth the effort.</p>
<h2 id="supporting-artists">Supporting Artists<a href="#supporting-artists" aria-label="Direct link to Supporting Artists" title="Direct link to Supporting Artists">​</a></h2>
<p>Moving away from Spotify doesn't mean abandoning artists. In fact, I now support musicians more directly by:</p>
<ul>
<li>Purchasing music directly from platforms like Bandcamp where artists receive 82-90% of sales</li>
<li>Buying physical media from official stores</li>
<li>Supporting Patreon/subscription services for favorite artists</li>
<li>Attending concerts and buying merchandise</li>
</ul>
<p>Buying a $10 album on Bandcamp puts about $8.20-$9.00 in the artist's pocket. To match that on Spotify, you're talking roughly 1.6k-3k streams of that album <strong>per listener</strong>. If the artist has a label taking a cut on Spotify, the stream counts needed go up further.</p>
<p>My self-hosted setup is about controlling my listening experience and owning what I pay for, not avoiding fair compensation to artists.</p>
<h2 id="whats-next">What's Next?<a href="#whats-next" aria-label="Direct link to What's Next?" title="Direct link to What's Next?">​</a></h2>
<p>I'm continually refining this setup. Future improvements include automating the lyrics process and exploring more discovery tools. The beauty of a self-hosted solution is that it can evolve with my needs, rather than changing at the whim of a company's business model.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Is the Fourier Transform? (394 pts)]]></title>
            <link>https://www.quantamagazine.org/what-is-the-fourier-transform-20250903/</link>
            <guid>45132810</guid>
            <pubDate>Thu, 04 Sep 2025 22:11:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/what-is-the-fourier-transform-20250903/">https://www.quantamagazine.org/what-is-the-fourier-transform-20250903/</a>, See on <a href="https://news.ycombinator.com/item?id=45132810">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-role="selectable">
    <p>As we listen to a piece of music, our ears perform a calculation. The high-pitched flutter of the flute, the middle tones of the violin, and the low hum of the double bass fill the air with pressure waves of many different frequencies. When the combined sound wave descends through the ear canal and into the spiral-shaped cochlea, hairs of different lengths resonate to the different pitches, separating the messy signal into buckets of elemental sounds.</p>
<p>It took mathematicians until the 19th century to master this same calculation.</p>
<p>In the early 1800s, the French mathematician Jean-Baptiste Joseph Fourier discovered a way to take any function and decompose it into a set of fundamental waves, or frequencies. Add these constituent frequencies back together, and you’ll get your original function. The technique, today called the Fourier transform, allowed the mathematician — previously an ardent proponent of the French revolution — to spur a mathematical revolution as well.</p>
<p>Out of the Fourier transform grew an entire field of mathematics, called harmonic analysis, which studies the components of functions. Soon enough, mathematicians began to discover deep connections between harmonic analysis and other areas of math and physics, from number theory to differential equations to quantum mechanics. You can also find the Fourier transform at work in your computer, allowing you to compress files, enhance audio signals and more.</p>
<p>“It’s hard to overestimate the influence of Fourier analysis in math,” said <a href="https://math.nyu.edu/~greengar/">Leslie Greengard</a> of New York University and the Flatiron Institute. “It touches almost every field of math and physics and chemistry and everything else.”</p>
<h2><strong>Flames of Passion </strong></h2>
<p>Fourier was born in 1768 amid the chaos of prerevolutionary France. Orphaned at 10 years old, he was educated at a convent in his hometown of Auxerre. He spent the next decade conflicted about whether to dedicate his life to religion or to math, eventually abandoning his religious training and becoming a teacher. He also promoted revolutionary efforts in France until, during the Reign of Terror in 1794, the 26-year-old was arrested and imprisoned for expressing beliefs that were considered anti-revolutionary. He was slated for the guillotine.</p>

<p>Before he could be executed, the Terror came to an end. And so, in 1795, he returned to teaching mathematics. A few years later, he was appointed as a scientific adviser to Napoleon Bonaparte and joined his army during the invasion of Egypt. It was there that Fourier, while also pursuing research into Egyptian antiquities, began the work that would lead him to develop his transform: He wanted to understand the mathematics of heat conduction. By the time he returned to France in 1801 — shortly before the French army was driven out of Egypt, the stolen Rosetta stone surrendered to the British — Fourier could think of nothing else.</p>
<p>If you heat one side of a metal rod, the heat will spread until the whole rod has the same temperature. Fourier argued that the distribution of heat through the rod could be written as a sum of simple waves. As the metal cools, these waves lose energy, causing them to smooth out and eventually disappear. The waves that oscillate more quickly — meaning they have more energy — decay first, followed eventually by the lower frequencies. It’s like a symphony that ends with each instrument fading to silence, from piccolos to tubas.</p>
<p>The proposal was radical. When Fourier presented it at a meeting of the Paris Institute in 1807, the renowned mathematician Joseph-Louis Lagrange reportedly declared the work “nothing short of impossible.”</p>
<p>What troubled his peers most were strange cases where the heat distribution might be sharply irregular — like a rod that is exactly half cold and half hot. Fourier maintained that the sudden jump in temperature could still be described mathematically: It would just require adding infinitely many simpler curves instead of a finite number. But most mathematicians at the time believed that no number of smooth curves could ever add up to a sharp corner.</p>
<p>Today, we know that Fourier was broadly right.</p>
<p>“You can represent anything as a sum of these very, very simple oscillations,” said <a href="https://www.math.princeton.edu/people/charles-fefferman">Charles Fefferman</a>, a mathematician at Princeton University. “It’s known that if you have a whole lot of tuning forks, and you set them perfectly, they can produce Beethoven’s Ninth Symphony.” The process only fails for <a href="https://www.quantamagazine.org/the-jagged-monstrous-function-that-broke-calculus-20250123/">the most bizarre functions</a>, like those that oscillate wildly no matter how much you zoom in on them.</p>
<p>So how does the Fourier transform work?</p>
<h2><strong>A Well-Trained Ear</strong></h2>
<p>Performing a Fourier transform is akin to sniffing a perfume and distinguishing its list of ingredients, or hearing a complex jazzy chord and distinguishing its constituent notes.</p>
<p>Mathematically, the Fourier transform is a function. It takes a given function — which can look complicated — as its input. It then produces as its output a set of frequencies. If you write down the simple sine and cosine waves that have these frequencies, and then add them together, you’ll get the original function.</p>
<figure>
    <p><img src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Fourier-figure-1-1.svg" alt="" decoding="async">    </p>
            <figcaption>
            <p>Samuel Velasco/<em>Quanta Magazine</em></p>
        </figcaption>
    </figure>

<p>To achieve this, the Fourier transform essentially scans all possible frequencies and determines how much each contributes to the original function. Let’s look at a simple example.</p>
<p>Consider the following function:</p>
<figure>
    <p><img src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Fourier-figure-2-1.svg" alt="" decoding="async"><img src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Fourier-figure-2-Mobile-1.svg" alt="" decoding="async">    </p>
    </figure>

<p>The Fourier transform checks how much each frequency contributes to this original function. It does so by multiplying waves together. Here’s what happens if we multiply the original by a sine wave with a frequency of 3:</p>
<figure>
    <p><img src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Fourier-figure-3-1.svg" alt="" decoding="async"><img src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Fourier-figure-3-Mobile-1.svg" alt="" decoding="async">    </p>
    </figure>

<p>There are lots of large peaks, which means the frequency 3 contributes to the original function. The average height of the peaks reveals how large the contribution is.</p>
<p>Now let’s test if the frequency 5 is present. Here’s what you get when you multiply the original function by a sine wave with the frequency 5:</p>
<figure>
    <p><img src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Fourier-figure-4-1.svg" alt="" decoding="async"><img src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Fourier-figure-4-Mobile-1.svg" alt="" decoding="async">    </p>
    </figure>

<p>There are some large peaks but also large valleys. The new graph averages out to around zero. This indicates that the frequency 5 does not contribute to the original function.</p>
<p>The Fourier transform does this for all possible frequencies, multiplying the original function by both sine and cosine waves. (In practice, it runs this comparison on the complex plane, using a combination of real and imaginary numbers.)</p>
<p>In this way, the Fourier transform can decompose a complicated-looking function into just a few numbers. This has made it a crucial tool for mathematicians: If they are stumped by a problem, they can try transforming it. Often, the problem becomes much simpler when translated into the language of frequencies.</p>
<p>If the original function has a sharp edge, like the square wave below (which is often found in digital signals), the Fourier transform will produce an infinite set of frequencies that, when added together, approximate the edge as closely as possible. This infinite set is called the Fourier series, and — despite mathematicians’ early hesitation to accept such a thing — it is now an essential tool in the analysis of functions.</p>
<figure>
    <p><img src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Fourier-figure-5-1.svg" alt="" decoding="async"><img src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Fourier-figure-5-Mobile-1.svg" alt="" decoding="async">    </p>
    </figure>

<h2><strong>Encore</strong></h2>
<p>The Fourier transform also works on higher-dimensional objects such as images. You can think of a grayscale image as a two-dimensional function that tells you how bright each pixel is. The Fourier transform decomposes this function into a set of 2D frequencies. The sine and cosine waves defined by these frequencies form striped patterns oriented in different directions. These patterns — and simple combinations of them that resemble checkerboards — can be added together to re-create any image.</p>
<p>Any 8-by-8 image, for example, can be built from some combination of the 64 building blocks below. A compression algorithm can then remove high-frequency information, which corresponds to small details, without drastically changing how the image looks to the human eye. This is how JPEGs compress complex images into much smaller amounts of data.</p>
<figure>
    <p><img src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Fourier-figure-6-1.svg" alt="" decoding="async">    </p>
    </figure>

<p>In the 1960s, the mathematicians James Cooley and John Tukey came up with an algorithm that could perform a Fourier transform much more quickly —&nbsp;aptly called the fast Fourier transform. Since then, the Fourier transform has been implemented practically every time there is a signal to process. “It’s now a part of everyday life,” Greengard said.</p>
<p>It has been used to study the tides, to detect gravitational waves, and to develop radar and magnetic resonance imaging. It allows us to reduce noise in busy audio files, and to compress and store all sorts of data. In quantum mechanics — the physics of the very small — it even provides the mathematical foundation for the uncertainty principle, which says that it’s impossible to know the precise position and momentum of a particle at the same time. You can write down a function that describes a particle’s possible positions; the Fourier transform of that function will describe the particle’s possible momenta. When your function can tell you where a particle will be located with high probability — represented by a sharp peak in the graph of the function — the Fourier transform will be very spread out. It will be impossible to determine what the particle’s momentum should be. The opposite is also true.</p>
        
        
<p>The Fourier transform has spread its roots throughout pure mathematics research, too. Harmonic analysis — which studies the Fourier transform, as well as how to reverse it to rebuild the original function — is a powerful framework for studying waves. Mathematicians have also found that harmonic analysis has deep and unexpected connections to number theory. They’ve used these connections to explore relationships among the integers, including the distribution of prime numbers, one of the greatest mysteries in mathematics.</p>
<p>“If people didn’t know about the Fourier transform, I don’t know what percent of math would then disappear,” Fefferman said. “But it would be a big percent.”</p>
<p><em>Editor’s note: The Flatiron Institute is funded by the Simons Foundation, which also funds this editorially independent magazine. Simons Foundation funding decisions have no influence on our coverage. More information about the relationship between </em>Quanta Magazine<em> and the Simons Foundation is available </em><a href="https://www.quantamagazine.org/about/"><em>here</em></a><em>. </em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[io_uring is faster than mmap (260 pts)]]></title>
            <link>https://www.bitflux.ai/blog/memory-is-slow-part2/</link>
            <guid>45132710</guid>
            <pubDate>Thu, 04 Sep 2025 22:01:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bitflux.ai/blog/memory-is-slow-part2/">https://www.bitflux.ai/blog/memory-is-slow-part2/</a>, See on <a href="https://news.ycombinator.com/item?id=45132710">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
                <h2 id="tl-dr">TL;DR</h2>
<p>Sourcing data directly from disk <em>IS</em> faster than caching in memory.  I brought receipts.
Because hardware got wider but not faster, the old methods don't get you there.  You need new tools to use what is scaling and avoid what isn't.</p>
<h2 id="introduction">Introduction</h2>
<p>In part 1 I showed how some computer performance factors are scaling exponentially while others have been stagnant for decades.  I then asserted, without proof, that sourcing data from disk can be faster than from memory.  What follows is the proof.</p>
<p>Computer Science dogma says that unused memory should be used to cache things from the filesystem because the disk is slow and memory is fast.  Given that disk bandwidth is growing exponentially and memory access latency has stagnated this isn't always true anymore.</p>
<h2 id="experimental-set-up">Experimental set up</h2>
<p>We need data and something straight forward to do with the data.  I used my free will or the illusion thereof to create a benchmark I cleverly call "counting 10s".  I write some pseudo random integers between 0 and 20 to a buffer and then count how many of the integers are 10.  I want to make sure we are doing all the counting in a single thread to simulate an Amdahl's Law situation.</p>
<p>So how fast can we expect this to run?  The upper limit would be the memory bandwidth.</p>
<p>My testing rig is a server with an old AMD EPYC 7551P 32-Core Processor on a Supermicro H11SSL-i and 96GB of DDR4 2133 MHz and a couple of 1.92TB Samsung PM983a PCIe 3.0 SSDs I pieced together from EBay parts.  Given the way this server is configured, the upper limit for memory bandwidth can be calculated as 3 channels * 2133MT/s * 8B/T / 4 numa domains = ~13GB/s for a single thread.  It's kind of an odd system but that just makes it more fun to optimize for!</p>
<p>The disks are rated at 3.1GB/s read BW each for an upper limit of 6.2GB/s.  I made a raid0 volume with 4KB stripe size, formatted the the raid as ext4 with no journaling, and made sure it fully finished initializing the metadata before running the tests.</p>
<pre data-lang="bash"><code data-lang="bash"><span>sudo</span><span> mdadm</span><span> --create</span><span> /dev/md0</span><span> --level</span><span>=0</span><span> --raid-devices</span><span>=2</span><span> --chunk</span><span>=4K /dev/nvme1n1 /dev/nvme2n1
</span><span>sudo</span><span> mkfs.ext4</span><span> -F -L</span><span> data</span><span> -O</span><span> ^has_journal</span><span> -E</span><span> lazy_itable_init=0 /dev/md0
</span><span>sudo</span><span> mount</span><span> -o</span><span> noatime /dev/md0 mnt
</span></code></pre>
<p>We'll use a 50GB dataset for most benchmarking here, because when I started this I thought the test system only had 64GB and it stuck.</p>
<h2 id="simple-loop">Simple Loop</h2>
<p>The simple and cleanest way to do this in C would look like this.</p>
<pre data-lang="c"><code data-lang="c"><span>#include </span><span>&lt;</span><span>stdio.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>stdlib.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>fcntl.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>sys/mman.h</span><span>&gt;
</span><span>
</span><span>// count_10_loop
</span><span>int </span><span>main</span><span>(</span><span>int </span><span>argc</span><span>, </span><span>char </span><span>*</span><span>argv</span><span>[]) {
</span><span>    </span><span>char</span><span>* filename = argv[</span><span>1</span><span>];
</span><span>    size_t size_bytes = </span><span>strtoull</span><span>(argv[</span><span>2</span><span>], </span><span>NULL</span><span>, </span><span>10</span><span>);
</span><span>    size_t total_ints = size_bytes / sizeof(</span><span>int</span><span>);
</span><span>    size_t count = </span><span>0</span><span>;
</span><span>
</span><span>    </span><span>int</span><span> fd = </span><span>open</span><span>(filename, O_RDONLY);
</span><span>    </span><span>int</span><span>* data = (</span><span>int</span><span>*)</span><span>mmap</span><span>(</span><span>NULL</span><span>, size_bytes, PROT_READ, MAP_SHARED, fd, </span><span>0</span><span>);
</span><span> 
</span><span>    </span><span>for </span><span>(size_t i = </span><span>0</span><span>; i &lt; total_ints; ++i) {
</span><span>        </span><span>if </span><span>(data[i] == </span><span>10</span><span>) count++;
</span><span>    }
</span><span>
</span><span>    </span><span>printf</span><span>("</span><span>Found </span><span>%ld</span><span> 10s</span><span>\n</span><span>", count);
</span><span>}
</span></code></pre>
<p>Just mmap() the file which will give us a buffer that we can read from.  Then we just loop and count the 10s.</p>
<p>Because the point is to benchmark we will integrate some timing mechanisms before we move on.</p>
<pre data-lang="c"><code data-lang="c"><span>#include </span><span>&lt;</span><span>stdio.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>stdlib.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>fcntl.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>sys/mman.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>sys/time.h</span><span>&gt;
</span><span>
</span><span>long </span><span>get_time_us</span><span>() {
</span><span>    </span><span>struct</span><span> timeval tv;
</span><span>    </span><span>gettimeofday</span><span>(&amp;tv, </span><span>NULL</span><span>);
</span><span>    </span><span>return</span><span> tv.</span><span>tv_sec </span><span>* </span><span>1000000</span><span>L </span><span>+ tv.</span><span>tv_usec</span><span>;
</span><span>}
</span><span>
</span><span>// count_10_loop
</span><span>int </span><span>main</span><span>(</span><span>int </span><span>argc</span><span>, </span><span>char </span><span>*</span><span>argv</span><span>[]) {
</span><span>    </span><span>char</span><span>* filename = argv[</span><span>1</span><span>];
</span><span>    size_t size_bytes = </span><span>strtoull</span><span>(argv[</span><span>2</span><span>], </span><span>NULL</span><span>, </span><span>10</span><span>);
</span><span>    size_t total_ints = size_bytes / sizeof(</span><span>int</span><span>);
</span><span>    size_t count = </span><span>0</span><span>;
</span><span>
</span><span>    </span><span>int</span><span> fd = </span><span>open</span><span>(filename, O_RDONLY);
</span><span>    </span><span>int</span><span>* data = (</span><span>int</span><span>*)</span><span>mmap</span><span>(</span><span>NULL</span><span>, size_bytes, PROT_READ, MAP_SHARED, fd, </span><span>0</span><span>);
</span><span> 
</span><span>    </span><span>long</span><span> start = </span><span>get_time_us</span><span>();
</span><span>    </span><span>for </span><span>(size_t i = </span><span>0</span><span>; i &lt; total_ints; ++i) {
</span><span>        </span><span>if </span><span>(data[i] == </span><span>10</span><span>) count++;
</span><span>    }
</span><span>    </span><span>long</span><span> elapsed = </span><span>get_time_us</span><span>() - start;
</span><span>
</span><span>    </span><span>printf</span><span>("</span><span>simple loop found </span><span>%ld</span><span> 10s processed at </span><span>%0.2f</span><span> GB/s</span><span>\n</span><span>", count, (</span><span>double</span><span>)(size_bytes/</span><span>1073741824</span><span>)/((</span><span>double</span><span>)elapsed/</span><span>1.0e6</span><span>));
</span><span>}
</span></code></pre>
<p>For the first run we're going to be reading from the disk. The disk/filesystem read is going to limit the performance before the memory bandwidth can.</p>
<pre><code><span>❯ sudo  ./count_10_loop ./mnt/datafile.bin 53687091200
</span><span>simple loop found 167802249 10s processed at 0.61 GB/s
</span></code></pre>
<p>As expected, it's not anywhere near memory speeds because as everyone knows, disk is slow.  We can look at the system and confirm that the first run cached the data to memory.</p>
<p><img src="https://www.bitflux.ai/pics/memory_is_slow_part2/cached.png" alt="cached"></p>
<p>Our expectation is that the second run will be faster because the data is already in memory and as everyone knows, memory is fast.</p>
<pre><code><span>❯ sudo  ./count_10_loop ./mnt/datafile.bin 53687091200
</span><span>simple loop found 167802249 10s processed at 3.71 GB/s
</span></code></pre>
<p><img src="https://www.bitflux.ai/pics/memory_is_slow_part2/performance1.png" alt="performance1"></p>
<p>It is faster, but clearly that’s slower than the memory can feed it to the processor.  What bottleneck might we be hitting?  This speed does look possibly correlated to the instructions per second limit for this generation of CPU (between 2GHz * 1.5 IPC = 3G and 3GHz boost * 1.5 IPC = 4.5G instructions per second).</p>
<p>We can use perf to see if the CPU is using vector instructions, if not then the actual compute is the bottleneck.</p>
<pre><code><span>Percent│      test     %rbp,%rbp
</span><span>       │    ↓ je       84
</span><span>       │      lea      (%rbx,%rbp,4),%rcx
</span><span>       │      mov      %rbx,%rax
</span><span>       │      xor      %ebp,%ebp
</span><span>       │      nop
</span><span>       │70:   xor      %edx,%edx
</span><span>  1.31 │      cmpl     $0xa,(%rax)
</span><span> 42.38 │      sete     %dl
</span><span> 45.72 │      add      $0x4,%rax
</span><span>  0.01 │      add      %rdx,%rbp
</span><span> 10.42 │      cmp      %rax,%rcx
</span><span>  0.16 │    ↑ jne      70
</span><span>       │84:   xor      %eax,%eax
</span><span>       │      shr      $0x14,%r12
</span><span>       │    → call     get_time_us
</span><span>       │      pxor     %xmm0,%xmm0
</span><span>       │      pxor     %xmm1,%xmm1
</span></code></pre>
<p>Confirmed. We're running non-vectorized instructions, with a single thread counting that's as fast as it can go with a 2GHz CPU.  Well crap.  We’ve hit our first non-exponential limit.  Even a brand new CPU running this machine code would probably struggle to do much better than a 50% improvement, still well below the memory bandwidth limit.</p>
<h2 id="unrolling-the-loop">Unrolling the loop</h2>
<p>Good news is this code can definitely be vectorized if we help the compiler.  Unroll the loop!</p>
<p>We're gonna make it very obvious to the compiler that it's safe to use vector instructions which could process our integers up to 8x faster.</p>
<pre data-lang="c"><code data-lang="c"><span>#include </span><span>&lt;</span><span>stdio.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>stdlib.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>fcntl.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>sys/mman.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>stdint.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>sys/time.h</span><span>&gt;
</span><span>
</span><span>long </span><span>get_time_us</span><span>() {
</span><span>    </span><span>struct</span><span> timeval tv;
</span><span>    </span><span>gettimeofday</span><span>(&amp;tv, </span><span>NULL</span><span>);
</span><span>    </span><span>return</span><span> tv.</span><span>tv_sec </span><span>* </span><span>1000000</span><span>L </span><span>+ tv.</span><span>tv_usec</span><span>;
</span><span>}
</span><span>
</span><span>// count_10_unrolled
</span><span>int </span><span>main</span><span>(</span><span>int </span><span>argc</span><span>, </span><span>char </span><span>*</span><span>argv</span><span>[]) {
</span><span>    </span><span>char</span><span>* filename = argv[</span><span>1</span><span>];
</span><span>    size_t size_bytes = </span><span>strtoull</span><span>(argv[</span><span>2</span><span>], </span><span>NULL</span><span>, </span><span>10</span><span>);
</span><span>    size_t total_ints = size_bytes / sizeof(</span><span>int</span><span>);
</span><span>    size_t count = </span><span>0</span><span>;
</span><span>
</span><span>    </span><span>int</span><span> fd = </span><span>open</span><span>(filename, O_RDONLY);
</span><span>    </span><span>void</span><span>* buffer = </span><span>mmap</span><span>(</span><span>NULL</span><span>, size_bytes, PROT_READ, MAP_SHARED, fd, </span><span>0</span><span>);
</span><span> 
</span><span>    </span><span>// Get the compiler to align the buffer
</span><span>    </span><span>const int </span><span>* </span><span>__restrict</span><span> data = (</span><span>const int </span><span>* </span><span>__restrict</span><span>)</span><span>__builtin_assume_aligned</span><span>(buffer, </span><span>4096</span><span>);
</span><span>    uint64_t c0=</span><span>0</span><span>, c1=</span><span>0</span><span>, c2=</span><span>0</span><span>, c3=</span><span>0</span><span>,
</span><span>            c4=</span><span>0</span><span>, c5=</span><span>0</span><span>, c6=</span><span>0</span><span>, c7=</span><span>0</span><span>,
</span><span>            c8=</span><span>0</span><span>, c9=</span><span>0</span><span>, c10=</span><span>0</span><span>, c11=</span><span>0</span><span>,
</span><span>            c12=</span><span>0</span><span>, c13=</span><span>0</span><span>, c14=</span><span>0</span><span>, c15=</span><span>0</span><span>;
</span><span>
</span><span>    </span><span>long</span><span> start = </span><span>get_time_us</span><span>();
</span><span>    </span><span>// Unrolling the compiler knows it can use a vector unit like AVX2 to process
</span><span>    </span><span>for </span><span>(size_t i = </span><span>0</span><span>; i &lt; total_ints; i += </span><span>16</span><span>) {
</span><span>        </span><span>// removed 'if' to get it to be branchless: each compares to 10, adds 0 or 1
</span><span>        c0  += (</span><span>unsigned</span><span>)(data[i+ </span><span>0</span><span>] == </span><span>10</span><span>);
</span><span>        c1  += (</span><span>unsigned</span><span>)(data[i+ </span><span>1</span><span>] == </span><span>10</span><span>);
</span><span>        c2  += (</span><span>unsigned</span><span>)(data[i+ </span><span>2</span><span>] == </span><span>10</span><span>);
</span><span>        c3  += (</span><span>unsigned</span><span>)(data[i+ </span><span>3</span><span>] == </span><span>10</span><span>);
</span><span>        c4  += (</span><span>unsigned</span><span>)(data[i+ </span><span>4</span><span>] == </span><span>10</span><span>);
</span><span>        c5  += (</span><span>unsigned</span><span>)(data[i+ </span><span>5</span><span>] == </span><span>10</span><span>);
</span><span>        c6  += (</span><span>unsigned</span><span>)(data[i+ </span><span>6</span><span>] == </span><span>10</span><span>);
</span><span>        c7  += (</span><span>unsigned</span><span>)(data[i+ </span><span>7</span><span>] == </span><span>10</span><span>);
</span><span>        c8  += (</span><span>unsigned</span><span>)(data[i+ </span><span>8</span><span>] == </span><span>10</span><span>);
</span><span>        c9  += (</span><span>unsigned</span><span>)(data[i+ </span><span>9</span><span>] == </span><span>10</span><span>);
</span><span>        c10 += (</span><span>unsigned</span><span>)(data[i+</span><span>10</span><span>] == </span><span>10</span><span>);
</span><span>        c11 += (</span><span>unsigned</span><span>)(data[i+</span><span>11</span><span>] == </span><span>10</span><span>);
</span><span>        c12 += (</span><span>unsigned</span><span>)(data[i+</span><span>12</span><span>] == </span><span>10</span><span>);
</span><span>        c13 += (</span><span>unsigned</span><span>)(data[i+</span><span>13</span><span>] == </span><span>10</span><span>);
</span><span>        c14 += (</span><span>unsigned</span><span>)(data[i+</span><span>14</span><span>] == </span><span>10</span><span>);
</span><span>        c15 += (</span><span>unsigned</span><span>)(data[i+</span><span>15</span><span>] == </span><span>10</span><span>);
</span><span>    }
</span><span>
</span><span>    </span><span>// pairwise reduce to help some compilers schedule better
</span><span>    uint64_t s0 = c0 + c1,   s1 = c2 + c3,   s2 = c4 + c5,   s3 = c6 + c7;
</span><span>    uint64_t s4 = c8 + c9,   s5 = c10 + c11, s6 = c12 + c13, s7 = c14 + c15;
</span><span>    uint64_t t0 = s0 + s1,   t1 = s2 + s3,   t2 = s4 + s5,   t3 = s6 + s7;
</span><span>
</span><span>    count = (t0 + t1) + (t2 + t3);
</span><span>    </span><span>long</span><span> elapsed = </span><span>get_time_us</span><span>() - start;
</span><span>
</span><span>    </span><span>printf</span><span>("</span><span>unrolled loop found </span><span>%ld</span><span> 10s processed at </span><span>%0.2f</span><span> GB/s</span><span>\n</span><span>", count, (</span><span>double</span><span>)(size_bytes/</span><span>1073741824</span><span>)/((</span><span>double</span><span>)elapsed/</span><span>1.0e6</span><span>));
</span><span>}
</span></code></pre>
<p>Check if we now have vectorized instructions with <code>perf</code>.</p>
<pre><code><span>Percent│       movq      %xmm0,%rcx
</span><span>       │       movdqa    %xmm7,%xmm14
</span><span>       │       pxor      %xmm0,%xmm0
</span><span>       │       nop
</span><span>       │ e8:   movdqa    %xmm6,%xmm4
</span><span>  0.30 │       movdqa    %xmm6,%xmm3
</span><span>  0.12 │       movdqa    %xmm6,%xmm2
</span><span>  0.35 │       add       $0x1,%rdx
</span><span>  1.54 │       pcmpeqd   (%rax),%xmm4
</span><span> 54.64 │       pcmpeqd   0x10(%rax),%xmm3
</span><span>  1.62 │       movdqa    %xmm6,%xmm1
</span><span>  0.99 │       add       $0x40,%rax
</span><span>  0.12 │       pcmpeqd   -0x20(%rax),%xmm2
</span><span>  3.03 │       pcmpeqd   -0x10(%rax),%xmm1
</span><span>  1.32 │       pand      %xmm5,%xmm4
</span><span>  1.25 │       pand      %xmm5,%xmm3
</span><span>  1.55 │       movdqa    %xmm4,%xmm15
</span><span>  0.24 │       punpckhdq %xmm0,%xmm4
</span><span>
</span></code></pre>
<p>Confirmed. We're using 128bit vector instructions, this should be up to 4x faster than the original.</p>
<blockquote>
<p>NOTE: These are 128-bit vector instructions, but I expected 256-bit.  I dug deeper here and found claims that Gen1 EPYC had unoptimized 256-bit instructions.  I forced the compiler to use 256-bit instructions and found it was actually slower.  Looks like the compiler was smart enough to know that here.</p>
</blockquote>
<p>Let's benchmark this unrolled version with the data as page cache in memory.</p>
<pre><code><span>❯ sudo  ./count_10_unrolled ./mnt/datafile.bin 53687091200
</span><span>unrolled loop found 167802249 10s processed at 5.51 GB/s
</span></code></pre>
<p><img src="https://www.bitflux.ai/pics/memory_is_slow_part2/performance2.png" alt="performance2"></p>
<p>We're still nowhere close to hitting the memory bus speed limit of 13GB/s but 50% faster than the original is a win.  There must be some other bottleneck.</p>
<h2 id="can-the-ssds-beat-that">Can the SSDs beat that?</h2>
<p>5.51GB/s?  On paper the SSDs can read at 6.2GB/s, but the first run from disk only did 0.61GB/s.  How can I meet or beat this performance sourcing the data directly from disk?</p>
<p>Consider how the default mmap() mechanism works, it is a background IO pipeline to transparently fetch the data from disk.  When you read the empty buffer from userspace it triggers a fault, the kernel handles the fault by reading the data from the filesystem, which then queues up IO from disk.  Unfortunately these legacy mechanisms just aren't set up for serious high performance IO.  Note that at 610MB/s it's faster than what a disk SATA can do.  On the other hand, it only managed 10% of our disk's potential.  Clearly we're going to have to do something else.</p>
<p>SSDs don't just automatically read data at multigigabyte speeds.  You need to put some real effort into an IO pipeline to get serious performance.</p>
<p>I made a io_uring based IO engine, a kind of userspace driver, that can hit these speeds.  The main thread will request data, the IO engine will handle the IO, then the main thread will do the counting when the data is in a buffer.  We will use a set of queues to manage the IO requests, responses, and buffers.  The IO engine will start 6 workers, target a queue depth of 8192, and have a buffer size of 16KB.</p>
<p>I wish I had tighter code here, but A) I didn’t have time to clean it up B) some of the complexity is intractable.  The IO engine code was a lot to scroll through so I moved it to github <a href="https://github.com/bitflux-ai/blog_notes/tree/main/memory_is_slow_part2/diskbased">link</a></p>
<pre data-lang="c"><code data-lang="c"><span>#include </span><span>"</span><span>io_engine.h</span><span>"
</span><span>#include </span><span>&lt;</span><span>sys/mman.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>getopt.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>stdio.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>stdlib.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>fcntl.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>sys/mman.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>stdint.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>sys/time.h</span><span>&gt;
</span><span>
</span><span>#define </span><span>DEFAULT_WORKERS </span><span>6
</span><span>#define </span><span>DEFAULT_BLOCK_SIZE </span><span>16384
</span><span>#define </span><span>DEFAULT_QUEUE_DEPTH </span><span>8192
</span><span>
</span><span>// Count the number of "10" (int format) in the buffer
</span><span>static inline </span><span>size_t </span><span>count_tens_unrolled</span><span>(</span><span>void</span><span>* </span><span>data</span><span>, size_t </span><span>size_bytes</span><span>) {
</span><span>    </span><span>const </span><span>size_t total = size_bytes / sizeof(</span><span>int</span><span>);
</span><span>    </span><span>// Get the compiler to align the buffer
</span><span>    </span><span>const int </span><span>* </span><span>__restrict</span><span> p = (</span><span>const int </span><span>* </span><span>__restrict</span><span>)</span><span>__builtin_assume_aligned</span><span>(data, </span><span>4096</span><span>);
</span><span>    uint64_t c0=</span><span>0</span><span>, c1=</span><span>0</span><span>, c2=</span><span>0</span><span>, c3=</span><span>0</span><span>,
</span><span>            c4=</span><span>0</span><span>, c5=</span><span>0</span><span>, c6=</span><span>0</span><span>, c7=</span><span>0</span><span>,
</span><span>            c8=</span><span>0</span><span>, c9=</span><span>0</span><span>, c10=</span><span>0</span><span>, c11=</span><span>0</span><span>,
</span><span>            c12=</span><span>0</span><span>, c13=</span><span>0</span><span>, c14=</span><span>0</span><span>, c15=</span><span>0</span><span>;
</span><span>
</span><span>    </span><span>// Unrolling the compiler knows it can use a vector unit like AVX2 to process
</span><span>    </span><span>for </span><span>(size_t i = </span><span>0</span><span>; i &lt; total; i += </span><span>16</span><span>) {
</span><span>        </span><span>// removed 'if' to get it to be branchless: each compares to 10, adds 0 or 1
</span><span>        c0  += (</span><span>unsigned</span><span>)(p[i+ </span><span>0</span><span>] == </span><span>10</span><span>);
</span><span>        c1  += (</span><span>unsigned</span><span>)(p[i+ </span><span>1</span><span>] == </span><span>10</span><span>);
</span><span>        c2  += (</span><span>unsigned</span><span>)(p[i+ </span><span>2</span><span>] == </span><span>10</span><span>);
</span><span>        c3  += (</span><span>unsigned</span><span>)(p[i+ </span><span>3</span><span>] == </span><span>10</span><span>);
</span><span>        c4  += (</span><span>unsigned</span><span>)(p[i+ </span><span>4</span><span>] == </span><span>10</span><span>);
</span><span>        c5  += (</span><span>unsigned</span><span>)(p[i+ </span><span>5</span><span>] == </span><span>10</span><span>);
</span><span>        c6  += (</span><span>unsigned</span><span>)(p[i+ </span><span>6</span><span>] == </span><span>10</span><span>);
</span><span>        c7  += (</span><span>unsigned</span><span>)(p[i+ </span><span>7</span><span>] == </span><span>10</span><span>);
</span><span>        c8  += (</span><span>unsigned</span><span>)(p[i+ </span><span>8</span><span>] == </span><span>10</span><span>);
</span><span>        c9  += (</span><span>unsigned</span><span>)(p[i+ </span><span>9</span><span>] == </span><span>10</span><span>);
</span><span>        c10 += (</span><span>unsigned</span><span>)(p[i+</span><span>10</span><span>] == </span><span>10</span><span>);
</span><span>        c11 += (</span><span>unsigned</span><span>)(p[i+</span><span>11</span><span>] == </span><span>10</span><span>);
</span><span>        c12 += (</span><span>unsigned</span><span>)(p[i+</span><span>12</span><span>] == </span><span>10</span><span>);
</span><span>        c13 += (</span><span>unsigned</span><span>)(p[i+</span><span>13</span><span>] == </span><span>10</span><span>);
</span><span>        c14 += (</span><span>unsigned</span><span>)(p[i+</span><span>14</span><span>] == </span><span>10</span><span>);
</span><span>        c15 += (</span><span>unsigned</span><span>)(p[i+</span><span>15</span><span>] == </span><span>10</span><span>);
</span><span>    }
</span><span>
</span><span>    </span><span>// pairwise reduce to help some compilers schedule better
</span><span>    uint64_t s0 = c0 + c1,   s1 = c2 + c3,   s2 = c4 + c5,   s3 = c6 + c7;
</span><span>    uint64_t s4 = c8 + c9,   s5 = c10 + c11, s6 = c12 + c13, s7 = c14 + c15;
</span><span>    uint64_t t0 = s0 + s1,   t1 = s2 + s3,   t2 = s4 + s5,   t3 = s6 + s7;
</span><span>
</span><span>    </span><span>return </span><span>(t0 + t1) + (t2 + t3);
</span><span>}
</span><span>
</span><span>int </span><span>main</span><span>(</span><span>int </span><span>argc</span><span>, </span><span>char </span><span>*</span><span>argv</span><span>[]) {
</span><span>    </span><span>char</span><span>* filename = argv[</span><span>1</span><span>];
</span><span>    size_t size_bytes = </span><span>strtoull</span><span>(argv[</span><span>2</span><span>], </span><span>NULL</span><span>, </span><span>10</span><span>);
</span><span>
</span><span>    </span><span>// Set up the io engine
</span><span>    ioengine_t* na = </span><span>ioengine_alloc</span><span>(filename, size_bytes, DEFAULT_QUEUE_DEPTH, DEFAULT_BLOCK_SIZE, DEFAULT_WORKERS);
</span><span>
</span><span>    </span><span>sleep</span><span>(</span><span>1</span><span>);
</span><span>
</span><span>    </span><span>// Use the background workers to read file directly
</span><span>    size_t total_blocks = na-&gt;file_size / na-&gt;block_size;
</span><span>    uint64_t uid = </span><span>1</span><span>;
</span><span>    size_t count = </span><span>0</span><span>;
</span><span>
</span><span>    </span><span>long</span><span> start = </span><span>get_time_us</span><span>();
</span><span>
</span><span>    </span><span>// Read all blocks
</span><span>    size_t blocks_queued = </span><span>0</span><span>;
</span><span>    size_t blocks_read = </span><span>0</span><span>;
</span><span>    </span><span>int</span><span> buffer_queued = </span><span>0</span><span>;
</span><span>    </span><span>while </span><span>(blocks_read &lt; total_blocks) {
</span><span>        </span><span>//// Queue IO phase //////
</span><span>        </span><span>//     Do we have more blocks to queue up?
</span><span>        </span><span>if </span><span>(buffer_queued &lt; na-&gt;num_io_buffers/</span><span>2 </span><span>&amp;&amp; blocks_queued &lt;= total_blocks) {
</span><span>            </span><span>// Calculate how many blocks on average we want our workers to queue up
</span><span>            size_t free_buffers = (size_t)(na-&gt;num_io_buffers - buffer_queued - </span><span>4</span><span>); </span><span>// hold back a few buffers
</span><span>            size_t blocks_remaining = total_blocks - blocks_queued;  </span><span>// how many blocks have we not queued
</span><span>            size_t blocks_to_queue = free_buffers &gt; blocks_remaining ? blocks_remaining : free_buffers;
</span><span>            </span><span>int</span><span> blocks_to_queue_per_worker = (</span><span>int</span><span>) (blocks_to_queue + na-&gt;num_workers - </span><span>1</span><span>) / na-&gt;num_workers;
</span><span>            </span><span>// Iterate through workers and assign work
</span><span>            </span><span>for </span><span>(</span><span>int</span><span> i = </span><span>0</span><span>; i &lt; na-&gt;num_workers; i++) {
</span><span>                worker_thread_data_t* worker = &amp;na-&gt;workers[i];
</span><span>                </span><span>// Try to queue N blocks to this worker
</span><span>                </span><span>for </span><span>(</span><span>int</span><span> j = </span><span>0</span><span>; j &lt; blocks_to_queue_per_worker; j++) {
</span><span>                    </span><span>if </span><span>(blocks_queued == total_blocks) </span><span>break</span><span>;
</span><span>                    </span><span>int</span><span> bgio_tail = worker-&gt;bgio_tail;
</span><span>                    </span><span>int</span><span> bgio_head = worker-&gt;bgio_head;
</span><span>                    </span><span>int</span><span> bgio_next = (bgio_tail + </span><span>1</span><span>) % worker-&gt;num_max_bgio;
</span><span>                    </span><span>int</span><span> next_bhead = (worker-&gt;buffer_head + </span><span>1</span><span>) % worker-&gt;num_max_bgio;
</span><span>                    </span><span>if </span><span>(bgio_next == bgio_head) </span><span>break</span><span>;  </span><span>// queue for send requests is full
</span><span>                    </span><span>if </span><span>(next_bhead == worker-&gt;buffer_tail) </span><span>break</span><span>; </span><span>// queue for recieving completed IO is full
</span><span>                    </span><span>// Queue this block with the worker.  We have to track which buffer it's going to.
</span><span>                    </span><span>int</span><span> buffer_idx = worker-&gt;buffer_start_idx + worker-&gt;buffer_head;
</span><span>                    na-&gt;buffer_state[buffer_idx] = BUFFER_PREFETCHING;
</span><span>                    worker-&gt;bgio_uids[bgio_tail] = (uid++)&lt;&lt;</span><span>16</span><span>; </span><span>// unique id helps track IOs in io_uring, we encode 4 bytes later
</span><span>                    worker-&gt;bgio_buffer_idx[bgio_tail] = buffer_idx;
</span><span>                    worker-&gt;bgio_block_idx[bgio_tail] = blocks_queued++;  </span><span>// block sized index into file
</span><span>                    worker-&gt;bgio_queued[bgio_tail] = -</span><span>1</span><span>;  </span><span>// Requested but not yet queued
</span><span>                    </span><span>int</span><span> next_tail = (bgio_tail + </span><span>1</span><span>) % worker-&gt;num_max_bgio;
</span><span>                    worker-&gt;bgio_tail = next_tail;
</span><span>                    </span><span>// Log the buffer in an ordered queue for us to read
</span><span>                    worker-&gt;complete_ring[worker-&gt;buffer_head] = buffer_idx;
</span><span>                    worker-&gt;buffer_head = next_bhead;
</span><span>                    buffer_queued++;
</span><span>                }
</span><span>                </span><span>// Tell the worker to submit IOs as a group
</span><span>                worker-&gt;bgio_submit++;
</span><span>            }
</span><span>        }
</span><span>
</span><span>        </span><span>//// Completion Phase //////
</span><span>        </span><span>//     Iterate through worker and check if they have complete IOs
</span><span>        </span><span>for </span><span>(</span><span>int</span><span> i = </span><span>0</span><span>; i &lt; na-&gt;num_workers; i++) {
</span><span>            worker_thread_data_t* worker = &amp;na-&gt;workers[i];
</span><span>            </span><span>int</span><span> current = worker-&gt;buffer_tail;
</span><span>            </span><span>// We know what IO's we're waiting on, but we have to poll
</span><span>            </span><span>//  to see if they are done.
</span><span>            </span><span>for </span><span>(</span><span>int</span><span> scan = </span><span>0</span><span>; scan &lt; worker-&gt;num_max_bgio; scan++) {
</span><span>                </span><span>// Scan until we get to the end of the list
</span><span>                </span><span>if </span><span>(current == worker-&gt;buffer_head) </span><span>break</span><span>;
</span><span>                </span><span>int</span><span> buffer_idx = worker-&gt;complete_ring[current];
</span><span>                </span><span>int</span><span> state = na-&gt;buffer_state[buffer_idx];
</span><span>                </span><span>if </span><span>(state == BUFFER_PREFETCHED) {
</span><span>                    </span><span>// This buffer is completed - Process this buffer.
</span><span>                    count += </span><span>count_tens_unrolled</span><span>(na-&gt;io_buffers[buffer_idx], na-&gt;block_size);
</span><span>                    na-&gt;buffer_state[buffer_idx] = BUFFER_UNUSED;
</span><span>                    blocks_read++;
</span><span>                    buffer_queued--;
</span><span>                }
</span><span>                current = (current + </span><span>1</span><span>) % worker-&gt;num_max_bgio;
</span><span>            }
</span><span>            </span><span>// IO's might have been completed out of order, advance the tail when we can
</span><span>            current = worker-&gt;buffer_tail;
</span><span>            </span><span>while </span><span>(current != worker-&gt;buffer_head) {
</span><span>                </span><span>int</span><span> buffer_idx = worker-&gt;complete_ring[current];
</span><span>                </span><span>int</span><span> state = na-&gt;buffer_state[buffer_idx];
</span><span>                </span><span>if </span><span>(state != BUFFER_UNUSED) </span><span>break</span><span>;
</span><span>                current = (current + </span><span>1</span><span>) % worker-&gt;num_max_bgio;
</span><span>            }
</span><span>            worker-&gt;buffer_tail = current;
</span><span>            worker-&gt;bgio_submit++;  </span><span>// probably unnecessary
</span><span>        }
</span><span>    }
</span><span>    </span><span>long</span><span> elapsed = </span><span>get_time_us</span><span>() - start;
</span><span>    </span><span>printf</span><span>("</span><span>diskbased found </span><span>%ld</span><span> 10s processed at </span><span>%0.2f</span><span> GB/s</span><span>\n</span><span>", count, (</span><span>double</span><span>)(size_bytes/</span><span>1073741824</span><span>)/((</span><span>double</span><span>)elapsed/</span><span>1.0e6</span><span>));
</span><span>
</span><span>    </span><span>// Cleanup I/O system
</span><span>    </span><span>ioengine_free</span><span>(na);
</span><span>
</span><span>    </span><span>return </span><span>0</span><span>;
</span><span>}
</span></code></pre>
<p>I hope all this extra code makes it faster.</p>
<pre><code><span>❯ sudo ./diskbased/benchmark ./mnt/datafile.bin 53687091200
</span><span>diskbased found 167802249 10s processed at 5.81 GB/s
</span></code></pre>
<p><img src="https://www.bitflux.ai/pics/memory_is_slow_part2/performance3.png" alt="performance3"></p>
<p>Boom!  Disk is faster than memory!  It takes several hundred lines of code but now we can source the data from my SSDs faster than the copy from the page cache in memory.</p>
<h2 id="so-what-s-going-on-here">So what's going on here?</h2>
<p>Of course my 6GB/s disk stripe isn’t actually faster than the memory bus, even on this weird hack of a system.  So what is happening?  Where is the bottleneck?  It's got to be the way the data is being read from the page cache in memory.</p>
<p>What if we replace the mmap() with a read() from disk into a preallocated buffer.  That way we can measure the counting with the data in-memory without any page cache related overhead mmap() can introduce.</p>
<pre data-lang="c"><code data-lang="c"><span>#include </span><span>&lt;</span><span>stdio.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>stdlib.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>sys/time.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>sys/stat.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>fcntl.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>unistd.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>stdint.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>string.h</span><span>&gt;
</span><span>
</span><span>long </span><span>get_time_us</span><span>() {
</span><span>    </span><span>struct</span><span> timeval tv;
</span><span>    </span><span>gettimeofday</span><span>(&amp;tv, </span><span>NULL</span><span>);
</span><span>    </span><span>return</span><span> tv.</span><span>tv_sec </span><span>* </span><span>1000000</span><span>L </span><span>+ tv.</span><span>tv_usec</span><span>;
</span><span>}
</span><span>
</span><span>int </span><span>main</span><span>(</span><span>int </span><span>argc</span><span>, </span><span>char </span><span>*</span><span>argv</span><span>[]) {
</span><span>    </span><span>char</span><span>* filename = argv[</span><span>1</span><span>];
</span><span>    size_t size_bytes = </span><span>strtoull</span><span>(argv[</span><span>2</span><span>], </span><span>NULL</span><span>, </span><span>10</span><span>);
</span><span>    size_t total_ints = size_bytes / sizeof(</span><span>int</span><span>);
</span><span>    size_t count = </span><span>0</span><span>;
</span><span>
</span><span>    </span><span>int</span><span> fd = </span><span>open</span><span>(filename, O_RDONLY|O_DIRECT);
</span><span>    </span><span>void </span><span>*buf;
</span><span>    </span><span>posix_memalign</span><span>(&amp;buf, </span><span>4096</span><span>, size_bytes);
</span><span>    </span><span>int </span><span>*data = buf;
</span><span>
</span><span>    size_t off = </span><span>0</span><span>;
</span><span>    </span><span>while </span><span>(off &lt; size_bytes) {
</span><span>        ssize_t n = </span><span>read</span><span>(fd, (</span><span>char</span><span>*)data + off, size_bytes - off);
</span><span>        off += (size_t)n;   </span><span>// YOLO: assume n &gt; 0 until done
</span><span>    }
</span><span>
</span><span>    </span><span>long</span><span> start = </span><span>get_time_us</span><span>();
</span><span>    </span><span>for </span><span>(size_t i = </span><span>0</span><span>; i &lt; total_ints; ++i) {
</span><span>        </span><span>if </span><span>(data[i] == </span><span>10</span><span>) count++;
</span><span>    }
</span><span>    </span><span>long</span><span> elapsed = </span><span>get_time_us</span><span>() - start;
</span><span>
</span><span>    </span><span>printf</span><span>("</span><span>simple loop </span><span>%ld</span><span> 10s processed at </span><span>%0.2f</span><span> GB/s</span><span>\n</span><span>",
</span><span>           count,
</span><span>           (</span><span>double</span><span>)(size_bytes/</span><span>1073741824</span><span>)/((</span><span>double</span><span>)elapsed/</span><span>1.0e6</span><span>));
</span><span>
</span><span>
</span><span>    </span><span>// Get the compiler to align the buffer
</span><span>    </span><span>const int </span><span>* </span><span>__restrict</span><span> p = (</span><span>const int </span><span>* </span><span>__restrict</span><span>)</span><span>__builtin_assume_aligned</span><span>((</span><span>void</span><span>*)data, </span><span>4096</span><span>);
</span><span>    uint64_t c0=</span><span>0</span><span>, c1=</span><span>0</span><span>, c2=</span><span>0</span><span>, c3=</span><span>0</span><span>,
</span><span>            c4=</span><span>0</span><span>, c5=</span><span>0</span><span>, c6=</span><span>0</span><span>, c7=</span><span>0</span><span>,
</span><span>            c8=</span><span>0</span><span>, c9=</span><span>0</span><span>, c10=</span><span>0</span><span>, c11=</span><span>0</span><span>,
</span><span>            c12=</span><span>0</span><span>, c13=</span><span>0</span><span>, c14=</span><span>0</span><span>, c15=</span><span>0</span><span>;
</span><span>
</span><span>    start = </span><span>get_time_us</span><span>();
</span><span>    </span><span>// Unrolling the compiler knows it can use a vector unit like AVX2 to process
</span><span>    </span><span>for </span><span>(size_t i = </span><span>0</span><span>; i &lt; total_ints; i += </span><span>16</span><span>) {
</span><span>        </span><span>// removed 'if' to get it to be branchless: each compares to 10, adds 0 or 1
</span><span>        c0  += (</span><span>unsigned</span><span>)(p[i+ </span><span>0</span><span>] == </span><span>10</span><span>);
</span><span>        c1  += (</span><span>unsigned</span><span>)(p[i+ </span><span>1</span><span>] == </span><span>10</span><span>);
</span><span>        c2  += (</span><span>unsigned</span><span>)(p[i+ </span><span>2</span><span>] == </span><span>10</span><span>);
</span><span>        c3  += (</span><span>unsigned</span><span>)(p[i+ </span><span>3</span><span>] == </span><span>10</span><span>);
</span><span>        c4  += (</span><span>unsigned</span><span>)(p[i+ </span><span>4</span><span>] == </span><span>10</span><span>);
</span><span>        c5  += (</span><span>unsigned</span><span>)(p[i+ </span><span>5</span><span>] == </span><span>10</span><span>);
</span><span>        c6  += (</span><span>unsigned</span><span>)(p[i+ </span><span>6</span><span>] == </span><span>10</span><span>);
</span><span>        c7  += (</span><span>unsigned</span><span>)(p[i+ </span><span>7</span><span>] == </span><span>10</span><span>);
</span><span>        c8  += (</span><span>unsigned</span><span>)(p[i+ </span><span>8</span><span>] == </span><span>10</span><span>);
</span><span>        c9  += (</span><span>unsigned</span><span>)(p[i+ </span><span>9</span><span>] == </span><span>10</span><span>);
</span><span>        c10 += (</span><span>unsigned</span><span>)(p[i+</span><span>10</span><span>] == </span><span>10</span><span>);
</span><span>        c11 += (</span><span>unsigned</span><span>)(p[i+</span><span>11</span><span>] == </span><span>10</span><span>);
</span><span>        c12 += (</span><span>unsigned</span><span>)(p[i+</span><span>12</span><span>] == </span><span>10</span><span>);
</span><span>        c13 += (</span><span>unsigned</span><span>)(p[i+</span><span>13</span><span>] == </span><span>10</span><span>);
</span><span>        c14 += (</span><span>unsigned</span><span>)(p[i+</span><span>14</span><span>] == </span><span>10</span><span>);
</span><span>        c15 += (</span><span>unsigned</span><span>)(p[i+</span><span>15</span><span>] == </span><span>10</span><span>);
</span><span>    }
</span><span>
</span><span>    </span><span>// pairwise reduce to help some compilers schedule better
</span><span>    uint64_t s0 = c0 + c1,   s1 = c2 + c3,   s2 = c4 + c5,   s3 = c6 + c7;
</span><span>    uint64_t s4 = c8 + c9,   s5 = c10 + c11, s6 = c12 + c13, s7 = c14 + c15;
</span><span>    uint64_t t0 = s0 + s1,   t1 = s2 + s3,   t2 = s4 + s5,   t3 = s6 + s7;
</span><span>
</span><span>    count = (t0 + t1) + (t2 + t3);
</span><span>    elapsed = </span><span>get_time_us</span><span>() - start;
</span><span>
</span><span>    </span><span>printf</span><span>("</span><span>unrolled loop </span><span>%ld</span><span> 10s processed at </span><span>%0.2f</span><span> GB/s</span><span>\n</span><span>",
</span><span>           count,
</span><span>           (</span><span>double</span><span>)(size_bytes/</span><span>1073741824</span><span>)/((</span><span>double</span><span>)elapsed/</span><span>1.0e6</span><span>));
</span><span>}
</span></code></pre>
<p>If we keep the dataset smaller than a numa domain and we bind this to a single numa node to prevent numa overheads we see that the theoretical memory bandwidth we projected seems to be the primary bottleneck for the unrolled loop as we hoped to see at the outset.</p>
<pre><code><span>❯  sudo numactl --cpunodebind=0   ./in_ram mnt/datafile.bin 2147483648
</span><span>simple loop 6709835 10s processed at 4.76 GB/s
</span><span>unrolled loop 6709835 10s processed at 13.04 GB/s
</span></code></pre>
<p>But this isn't useful to compare the with the other runs with the 50GB dataset.  However if we do the full 50GB dataset the performance suffers.  We have to get much of the data across numa domains which is going to be higher cost.</p>
<pre><code><span>❯ sudo ./in_ram ./mnt/datafile.bin 53687091200
</span><span>simple loop 167802249 10s processed at 3.76 GB/s
</span><span>unrolled loop 167802249 10s processed at 7.90 GB/s
</span></code></pre>
<p><img src="https://www.bitflux.ai/pics/memory_is_slow_part2/performance4.png" alt="performance4"></p>
<p>Comparing the results of "fully in-memory (50GB)" which is pre-loaded in memory before measuring against the "unrolled loop" that is only cached in memory we see 40% overhead.  That's 2.75 seconds out of 9 seconds that was spent waiting on the caching system instead of counting.  Why so much?</p>
<p><strong>mmap()</strong></p>
<p>The mmap() call presents the process with a buffer that is a blank slate even when the data is already in memory.  The buffer is populated page by page as it's accessed from the page cache.  This isn't a copy, it's just the operating system mapping the cached memory into the process.  This costs more than it might seem.  The worst case with mmap() the counting has to pause at every 4KB page boundary while the kernel processes a fault, tracks down the page of data in the page cache, then updates the page table of the process to insert the memory into the process.  Fundamentally this is a process that is limited by the memory latency, not the CPU speed or memory bandwidth.  With the potential for TLB walks and searching lists that track the page cache, we’re taking potentially dozens of CPU cache misses and several microseconds of waiting on memory for every 4KB page.</p>
<p><strong>direct IO</strong></p>
<p>Using our direct from disk approach uses pipelines and streams which avoids the kind of memory latency dominated bottleneck that mmap() has.  In our case we're limited by the bandwidth of our disks yet because of the pipelining, the larger latency of the IOs doesn't get in the critical path of the counting very much.  Allowing for higher throughput.</p>
<h2 id="scaling">Scaling</h2>
<p>Consider the implications of these experiments as we scale.  The well vetted solution to get data from memory to a process is slower than using the disk directly.  This isn't because the memory is slower than the disk.  The memory has higher bandwidth than the disk, not by an order of magnitude, but a decent margin.  But the latency of the memory is orders of magnitude lower than the disk.  Nevertheless the <em>way</em> the data in memory is accessed is the culprit.  Its a synchronous approach that assumes memory operations are cheap and low latency.  These accesses add up and it ends up waiting on memory latencies.  The disk method on the other hand is as a streaming approach built to leverage bandwidth and hide latencies.</p>
<p><strong>extending the existing rig</strong></p>
<p>If I got a few more of these disks I could push the IO bandwidth to be greater than the 13GB/s per thread memory bandwidth limit.  IO is DMA'ed to buffers that are pretty small compared to the total dataset. These buffers scale with the throughput capabilities of the CPU and the disks, not the dataset size. The buffers can be located in a single numa domain allowing us to avoid the overhead of accessing the buffers between NUMA domains.  Add more disks to this system I might be able to create a disk based solution to count at the full 13GB/s rather than be limited to the 7.90GB/s we see with the in memory example at the full 50GB dataset.  With such a system our throughput would not be affected by the dataset size, unlike the in-memory case, which has numa overhead and eventually runs out of memory to scale.</p>
<p><strong>faster than memory is possible</strong></p>
<p>On a proper modern server the CPUs will let you do IO directly to the L3 cache, bypassing memory altogether.  Because PCIe bandwidth is higher than memory bandwidth, on paper we could even get more max bandwidth than we can get from memory if we carefully pin the buffers into the CPU cache.  I haven't confirm this works in practice, however, it could be made to work and is the sort of thing that CPU designs will be forced to lean into to push performance forward.</p>
<p><strong>memory is changing too</strong></p>
<p>This isn't just about disks vs memory.  Similar techniques and principles apply to memory.  Memory bandwidth is still scaling even if the latency is not.  This means to take advantage of memory performance you have to actually treat it more like a disk and less like Random Access Memory.  To scale performance with generational updates you have to make sure to stream data from memory into the CPU caches in blocks, similar to how data is streamed from disk to memory.  If not you end up with 90s level memory throughput.  A custom mechanism to cache data in memory could easily avoid the memory latency problems seen with the default mmap() solution with much less code than the io_uring solution.</p>
<h2 id="is-this-worth-it">Is this worth it?</h2>
<p>I'm not going to say that going to the effort of implementing something like this is always worth it.  The mmap() method is sure elegant from a coding perspective, especially when compared to all the code I had to write to get the io_uring setup working.  Sometimes the simple way is the way to go.</p>
<p>Is using 6 cores of IO for 1 core of compute is always the right answer?  Probably not.  This was an extreme situation to prove a point.  In realworld situations you'll need to look at the tradeoffs and decide what's best for your use case.  Correctly understanding the strengths and weaknesses of the hardware can open up a number of possibilities where you can get a lot more performance for a lot less money.</p>
<p>The kind of overhead demonstrated with mmap() isn’t going to go away, new hardware isn't going to fix it.  At the same time disk bandwidth and the number of cores are scaling each generation.  But doing things that scale performance with new technology is going to take extra code and effort.</p>
<p>But don't just blow this stuff off.  Sure you <em>can</em> dedicate a server with 3TB of memory to serve 10K client connections. Memory in the cloud is like ~$5/GB/month, if you can afford it, then you do you.  However it is worth considering that humanity doesn't have the silicon fabs or the power plants to support this for every moron vibe coder out there making an app.  I figure either the karmic debt to the planet, or a vengeful AI demigod hungry for silicon and electricity will come for those that don't heed this warning, eventually.  Either way my conscience is clear.</p>
<h2 id="recap">Recap</h2>
<ul>
<li>Memory is slow - when you use it oldschool.</li>
<li>Disk is fast - when you are clever with it.</li>
<li>Test the dogma - compounded exponentials are flipping somethings from true to false.</li>
</ul>
<p><strong>Bad news</strong> is that this cleverness requires extra code and effort.</p>
<p><strong>Good news</strong> is we now have AI to write and test the extra code this cleverness requires.</p>
<p><strong>Better news</strong> is that, for those that are willing to learn, AI's don't do this unless you know how to ask them.</p>
<p>Lean into things that scale, avoid things that don’t.</p>
<h2 id="next-time">Next Time</h2>
<p>What will be revealed in the next episode?</p>
<ul>
<li>Is O(√n) actually faster than O(log n)?  Will the foundations of Computer Science survive this unveiling?</li>
<li>Will traditional code be consumed into the latent space of our AI overlords?</li>
<li>Is AI hiding these performance gains from me?  Is AI even capable of writing optimized code?</li>
</ul>
<hr>
<p><em>Jared Hulbert</em></p>
<blockquote>
<p>A few notes for the "um actually" haters commenting on Hacker News:</p>
<ul>
<li>This is not and does not claim to be an academic paper.</li>
<li>I do not intend to prove that NAND is a drop in replacement for DRAM.</li>
<li>Tis but a humble and hopefully fun exercise in exploring the limits and trends of modern hardware and the tradeoffs needed to maximize performance.</li>
<li>As I stated before I have no problem with your choice to ignore this and write lazy code that will perform just as fast on new hardware in 15 years as it does on todays hardware.  In fact I applaud your choice.  Jeff Bezos has an orbital yacht to build, someone has to pay for it, why not you?</li>
<li>I am not an AI.  I am a human with a computer that don't write perfect.</li>
</ul>
</blockquote>
<blockquote>
<p>source code can be found <a href="https://github.com/bitflux-ai/blog_notes">here</a>.</p>
</blockquote>

            </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What If OpenDocument Used SQLite? (241 pts)]]></title>
            <link>https://www.sqlite.org/affcase1.html</link>
            <guid>45132498</guid>
            <pubDate>Thu, 04 Sep 2025 21:36:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sqlite.org/affcase1.html">https://www.sqlite.org/affcase1.html</a>, See on <a href="https://news.ycombinator.com/item?id=45132498">Hacker News</a></p>
<div id="readability-page-1" class="page">
<div>
<p><a href="https://www.sqlite.org/index.html">
<img src="https://www.sqlite.org/images/sqlite370_banner.svg" alt="SQLite">
</a></p>
<p>
Small. Fast. Reliable.<br>Choose any three.
</p>



</div>






<h2>Introduction</h2>

<p>Suppose the
<a href="http://en.wikipedia.org/wiki/OpenDocument">OpenDocument</a> file format,
and specifically the "ODP" OpenDocument Presentation format, were
built around SQLite.  Benefits would include:
</p><ul>
<li>Smaller documents
</li><li>Faster File/Save times
</li><li>Faster startup times
</li><li>Less memory used
</li><li>Document versioning
</li><li>A better user experience
</li></ul>

<p>
Note that this is only a thought experiment.
We are not suggesting that OpenDocument be changed.
Nor is this article a criticism of the current OpenDocument
design.  The point of this essay is to suggest ways to improve
future file format designs.

</p><h2>About OpenDocument And OpenDocument Presentation</h2>

<p>
The OpenDocument file format is used for office applications:
word processors, spreadsheets, and presentations.  It was originally
designed for the OpenOffice suite but has since been incorporated into
other desktop application suites.  The OpenOffice application has been
forked and renamed a few times.  This author's primary use for OpenDocument is 
building slide presentations with either 
<a href="https://www.neooffice.org/neojava/en/index.php">NeoOffice</a> on Mac, or
<a href="http://www.libreoffice.org/">LibreOffice</a> on Linux and Windows.

</p><p>
An OpenDocument Presentation or "ODP" file is a
<a href="http://en.wikipedia.org/wiki/Zip_%28file_format%29">ZIP archive</a> containing
XML files describing presentation slides and separate image files for the
various images that are included as part of the presentation.
(OpenDocument word processor and spreadsheet files are similarly
structured but are not considered by this article.) The reader can
easily see the content of an ODP file by using the "zip -l" command.
For example, the following is the "zip -l" output from a 49-slide presentation
about SQLite from the 2014
<a href="http://southeastlinuxfest.org/">SouthEast LinuxFest</a>
conference:

</p><blockquote><pre>Archive:  self2014.odp
  Length      Date    Time    Name
---------  ---------- -----   ----
       47  2014-06-21 12:34   mimetype
        0  2014-06-21 12:34   Configurations2/statusbar/
        0  2014-06-21 12:34   Configurations2/accelerator/current.xml
        0  2014-06-21 12:34   Configurations2/floater/
        0  2014-06-21 12:34   Configurations2/popupmenu/
        0  2014-06-21 12:34   Configurations2/progressbar/
        0  2014-06-21 12:34   Configurations2/menubar/
        0  2014-06-21 12:34   Configurations2/toolbar/
        0  2014-06-21 12:34   Configurations2/images/Bitmaps/
    54702  2014-06-21 12:34   Pictures/10000000000001F40000018C595A5A3D.png
    46269  2014-06-21 12:34   Pictures/100000000000012C000000A8ED96BFD9.png
<i>... 58 other pictures omitted...</i>
    13013  2014-06-21 12:34   Pictures/10000000000000EE0000004765E03BA8.png
  1005059  2014-06-21 12:34   Pictures/10000000000004760000034223EACEFD.png
   211831  2014-06-21 12:34   content.xml
    46169  2014-06-21 12:34   styles.xml
     1001  2014-06-21 12:34   meta.xml
     9291  2014-06-21 12:34   Thumbnails/thumbnail.png
    38705  2014-06-21 12:34   Thumbnails/thumbnail.pdf
     9664  2014-06-21 12:34   settings.xml
     9704  2014-06-21 12:34   META-INF/manifest.xml
---------                     -------
 10961006                     78 files
</pre></blockquote>

<p>
The ODP ZIP archive contains four different XML files:
content.xml, styles.xml, meta.xml, and settings.xml.  Those four files
define the slide layout, text content, and styling.  This particular
presentation contains 62 images, ranging from full-screen pictures to
tiny icons, each stored as a separate file in the Pictures
folder.  The "mimetype" file contains a single line of text that says:

</p><blockquote><pre>application/vnd.oasis.opendocument.presentation
</pre></blockquote>

<p>The purpose of the other files and folders is presently 
unknown to the author but is probably not difficult to figure out.

</p><h2>Limitations Of The OpenDocument Presentation Format</h2>

<p>
The use of a ZIP archive to encapsulate XML files plus resources is an
elegant approach to an application file format.
It is clearly superior to a custom binary file format.
But using an SQLite database as the
container, instead of ZIP, would be more elegant still.

</p><p>A ZIP archive is basically a key/value database, optimized for
the case of write-once/read-many and for a relatively small number
of distinct keys (a few hundred to a few thousand) each with a large BLOB
as its value.  A ZIP archive can be viewed as a "pile-of-files"
database.  This works, but it has some shortcomings relative to an
SQLite database, as follows:

</p><ol>
<li><p><b>Incremental update is hard.</b>
</p><p>
It is difficult to update individual entries in a ZIP archive.
It is especially difficult to update individual entries in a ZIP
archive in a way that does not destroy
the entire document if the computer loses power and/or crashes
in the middle of the update.  It is not impossible to do this, but
it is sufficiently difficult that nobody actually does it.  Instead, whenever
the user selects "File/Save", the entire ZIP archive is rewritten.  
Hence, "File/Save" takes longer than it ought, especially on
older hardware.  Newer machines are faster, but it is still bothersome
that changing a single character in a 50 megabyte presentation causes one
to burn through 50 megabytes of the finite write life on the SSD.

</p></li><li><p><b>Startup is slow.</b>
</p><p>
In keeping with the pile-of-files theme, OpenDocument stores all slide 
content in a single big XML file named "content.xml".  
LibreOffice reads and parses this entire file just to display
the first slide.
LibreOffice also seems to
read all images into memory as well, which makes sense seeing as when
the user does "File/Save" it is going to have to write them all back out
again, even though none of them changed.  The net effect is that
start-up is slow.  Double-clicking an OpenDocument file brings up a
progress bar rather than the first slide.
This results in a bad user experience.
The situation grows ever more annoying as
the document size increases.

</p></li><li><p><b>More memory is required.</b>
</p><p>
Because ZIP archives are optimized for storing big chunks of content, they
encourage a style of programming where the entire document is read into
memory at startup, all editing occurs in memory, then the entire document
is written to disk during "File/Save".  OpenOffice and its descendants
embrace that pattern.

</p><p>
One might argue that it is ok, in this era of multi-gigabyte desktops, to
read the entire document into memory.
But it is not ok.
For one, the amount of memory used far exceeds the (compressed) file size
on disk.  So a 50MB presentation might take 200MB or more RAM.  
That still is not a problem if one only edits a single document at a time.  
But when working on a talk, this author will typically have 10 or 15 different 
presentations up all at the same
time (to facilitate copy/paste of slides from past presentations) and so
gigabytes of memory are required.
Add in an open web browser or two and a few other 
desktop apps, and suddenly the disk is whirling and the machine is swapping.
And even having just a single document is a problem when working
on an inexpensive Chromebook retrofitted with Ubuntu.
Using less memory is always better.
</p>

</li><li><p><b>Crash recovery is difficult.</b>
</p><p>
The descendants of OpenOffice tend to segfault more often than commercial
competitors.  Perhaps for this reason, the OpenOffice forks make
periodic backups of their in-memory documents so that users do not lose
all pending edits when the inevitable application crash does occur.
This causes frustrating pauses in the application for the few seconds
while each backup is being made.
After restarting from a crash, the user is presented with a dialog box
that walks them through the recovery process.  Managing the crash
recovery this way involves lots of extra application logic and is
generally an annoyance to the user.

</p></li><li><p><b>Content is inaccessible.</b>
</p><p>
One cannot easily view, change, or extract the content of an 
OpenDocument presentation using generic tools.
The only reasonable way to view or edit an OpenDocument document is to open
it up using an application that is specifically designed to read or write
OpenDocument (read: LibreOffice or one of its cousins).  The situation
could be worse.  One can extract and view individual images (say) from
a presentation using just the "zip" archiver tool.  But it is not reasonable
try to extract the text from a slide.  Remember that all content is stored
in a single "context.xml" file.  That file is XML, so it is a text file.
But it is not a text file that can be managed with an ordinary text
editor.  For the example presentation above, the content.xml file
consist of exactly two lines. The first line of the file is just:

</p><blockquote><pre>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
</pre></blockquote>

<p>The second line of the file contains 211792 characters of
impenetrable XML.  Yes, 211792 characters all on one line.
This file is a good stress-test for a text editor.
Thankfully, the file is not some obscure
binary format, but in terms of accessibility, it might as well be
written in Klingon.
</p></li></ol>

<h2>First Improvement:  Replace ZIP with SQLite</h2>

<p>
Let us suppose that instead of using a ZIP archive to store its files,
OpenDocument used a very simple SQLite database with the following
single-table schema:

</p><blockquote><pre>CREATE TABLE OpenDocTree(
  filename TEXT PRIMARY KEY,  -- Name of file
  filesize BIGINT,            -- Size of file after decompression
  content BLOB                -- Compressed file content
);
</pre></blockquote>

<p>
For this first experiment, nothing else about the file format is changed.
The OpenDocument is still a pile-of-files, only now each file is a row
in an SQLite database rather than an entry in a ZIP archive.
This simple change does not use the power of a relational
database.  Even so, this simple change shows some improvements.

<a name="smaller"></a>

</p><p>
Surprisingly, using SQLite in place of ZIP makes the presentation
file smaller.  Really.  One would think that a relational database file
would be larger than a ZIP archive, but at least in the case of NeoOffice
that is not so.  The following is an actual screen-scrape showing
the sizes of the same NeoOffice presentation, both in its original 
ZIP archive format as generated by NeoOffice (self2014.odp), and 
as repacked as an SQLite database using the 
<a href="https://sqlite.org/sqlar/doc/trunk/README.md">SQLAR</a> utility:

</p><blockquote><pre>-rw-r--r--  1 drh  staff  10514994 Jun  8 14:32 self2014.odp
-rw-r--r--  1 drh  staff  10464256 Jun  8 14:37 self2014.sqlar
-rw-r--r--  1 drh  staff  10416644 Jun  8 14:40 zip.odp
</pre></blockquote>

<p>
The SQLite database file ("self2014.sqlar") is about a
half percent smaller than the equivalent ODP file!  How can this be?
Apparently the ZIP archive generator logic in NeoOffice
is not as efficient as it could be, because when the same pile-of-files
is recompressed using the command-line "zip" utility, one gets a file
("zip.odp") that is smaller still, by another half percent, as seen
in the third line above.  So, a well-written ZIP archive
can be slightly smaller than the equivalent SQLite database, as one would
expect.  But the difference is slight.  The key take-away is that an
SQLite database is size-competitive with a ZIP archive.

</p><p>
The other advantage to using SQLite in place of
ZIP is that the document can now be updated incrementally, without risk
of corrupting the document if a power loss or other crash occurs in the
middle of the update.  (Remember that writes to 
<a href="https://www.sqlite.org/atomiccommit.html">SQLite databases are atomic</a>.)   True, all the
content is still kept in a single big XML file ("content.xml") which must
be completely rewritten if so much as a single character changes.  But
with SQLite, only that one file needs to change.  The other 77 files in the
repository can remain unaltered.  They do not all have to be rewritten,
which in turn makes "File/Save" run much faster and saves wear on SSDs.

</p><h2>Second Improvement:  Split content into smaller pieces</h2>

<p>
A pile-of-files encourages content to be stored in a few large chunks.
In the case of ODP, there are just four XML files that define the layout
of all slides in a presentation.  An SQLite database allows storing
information in a few large chunks, but SQLite is also adept and efficient
at storing information in numerous smaller pieces.

</p><p>
So then, instead of storing all content for all slides in a single
oversized XML file ("content.xml"), suppose there was a separate table
for storing the content of each slide separately.  The table schema
might look something like this:

</p><blockquote><pre>CREATE TABLE slide(
  pageNumber INTEGER,   -- The slide page number
  slideContent TEXT     -- Slide content as XML or JSON
);
CREATE INDEX slide_pgnum ON slide(pageNumber); -- Optional
</pre></blockquote>

<p>The content of each slide could still be stored as compressed XML.
But now each page is stored separately.  So when opening a new document,
the application could simply run:

</p><blockquote><pre>SELECT slideContent FROM slide WHERE pageNumber=1;
</pre></blockquote>

<p>This query will quickly and efficiently return the content of the first
slide, which could then be speedily parsed and displayed to the user.
Only one page needs to be read and parsed in order to render the first screen,
which means that the first screen appears much faster and
there is no longer a need for an annoying progress bar.

</p><p>If the application wanted
to keep all content in memory, it could continue reading and parsing the
other pages using a background thread after drawing the first page.  Or,
since reading from SQLite is so efficient, the application might 
instead choose to reduce its memory footprint and only keep a single
slide in memory at a time.  Or maybe it keeps the current slide and the
next slide in memory, to facilitate rapid transitions to the next slide.

</p><p>
Notice that dividing up the content into smaller pieces using an SQLite
table gives flexibility to the implementation.  The application can choose
to read all content into memory at startup.  Or it can read just a
few pages into memory and keep the rest on disk.  Or it can read just a
single page into memory at a time.  And different versions of the application
can make different choices without having to make any changes to the
file format.  Such options are not available when all content is in
a single big XML file in a ZIP archive.

</p><p>
Splitting content into smaller pieces also helps File/Save operations
to go faster.  Instead of having to write back the content of all pages
when doing a File/Save, the application only has to write back those
pages that have actually changed.

</p><p>
One minor downside of splitting content into smaller pieces is that
compression does not work as well on shorter texts and so the size of
the document might increase.  But as the bulk of the document space 
is used to store images, a small reduction in the compression efficiency 
of the text content will hardly be noticeable, and is a small price 
to pay for an improved user experience.

</p><h2>Third Improvement:  Versioning</h2>

<p>
Once one is comfortable with the concept of storing each slide separately,
it is a small step to support versioning of the presentation.  Consider
the following schema:

</p><blockquote><pre>CREATE TABLE slide(
  slideId INTEGER PRIMARY KEY,
  derivedFrom INTEGER REFERENCES slide,
  content TEXT     -- XML or JSON or whatever
);
CREATE TABLE version(
  versionId INTEGER PRIMARY KEY,
  priorVersion INTEGER REFERENCES version,
  checkinTime DATETIME,   -- When this version was saved
  comment TEXT,           -- Description of this version
  manifest TEXT           -- List of integer slideIds
);
</pre></blockquote>

<p>
In this schema, instead of each slide having a page number that determines
its order within the presentation, each slide has a unique
integer identifier that is unrelated to where it occurs in sequence.
The order of slides in the presentation is determined by a list of
slideIds, stored as a text string in the MANIFEST column of the VERSION
table.
Since multiple entries are allowed in the VERSION table, that means that
multiple presentations can be stored in the same document.

</p><p>
On startup, the application first decides which version it
wants to display.  Since the versionId will naturally increase in time
and one would normally want to see the latest version, an appropriate
query might be:

</p><blockquote><pre>SELECT manifest, versionId FROM version ORDER BY versionId DESC LIMIT 1;
</pre></blockquote>

<p>
Or perhaps the application would rather use the
most recent checkinTime:

</p><blockquote><pre>SELECT manifest, versionId, max(checkinTime) FROM version;
</pre></blockquote>

<p>
Using a single query such as the above, the application obtains a list
of the slideIds for all slides in the presentation.  The application then
queries for the content of the first slide, and parses and displays that
content, as before.

</p><p>(Aside:  Yes, that second query above that uses "max(checkinTime)"
really does work and really does return a well-defined answer in SQLite.
Such a query either returns an undefined answer or generates an error
in many other SQL database engines, but in SQLite it does what you would 
expect: it returns the manifest and versionId of the entry that has the
maximum checkinTime.)

</p><p>When the user does a "File/Save", instead of overwriting the modified
slides, the application can now make new entries in the SLIDE table for
just those slides that have been added or altered.  Then it creates a
new entry in the VERSION table containing the revised manifest.

</p><p>The VERSION table shown above has columns to record a check-in comment
(presumably supplied by the user) and the time and date at which the File/Save
action occurred.  It also records the parent version to record the history
of changes.  Perhaps the manifest could be stored as a delta from the
parent version, though typically the manifest will be small enough that
storing a delta might be more trouble than it is worth.  The SLIDE table
also contains a derivedFrom column which could be used for delta encoding
if it is determined that saving the slide content as a delta from its
previous version is a worthwhile optimization.

</p><p>So with this simple change, the ODP file now stores not just the most
recent edit to the presentation, but a history of all historic edits.  The
user would normally want to see just the most recent edition of the
presentation, but if desired, the user can now go backwards in time to 
see historical versions of the same presentation.

</p><p>Or, multiple presentations could be stored within the same document.

</p><p>With such a schema, the application would no longer need to make
periodic backups of the unsaved changes to a separate file to avoid lost
work in the event of a crash.  Instead, a special "pending" version could
be allocated and unsaved changes could be written into the pending version.
Because only changes would need to be written, not the entire document,
saving the pending changes would only involve writing a few kilobytes of
content, not multiple megabytes, and would take milliseconds instead of
seconds, and so it could be done frequently and silently in the background.
Then when a crash occurs and the user reboots, all (or almost all)
of their work is retained.  If the user decides to discard unsaved changes, 
they simply go back to the previous version.

</p><p>
There are details to fill in here.
Perhaps a screen can be provided that displays all historical changes
(perhaps with a graph) allowing the user to select which version they
want to view or edit.  Perhaps some facility can be provided to merge
forks that might occur in the version history.  And perhaps the
application should provide a means to purge old and unwanted versions.
The key point is that using an SQLite database to store the content,
rather than a ZIP archive, makes all of these features much, much easier
to implement, which increases the possibility that they will eventually
get implemented.

</p><h2>And So Forth...</h2>

<p>
In the previous sections, we have seen how moving from a key/value
store implemented as a ZIP archive to a simple SQLite database
with just three tables can add significant capabilities to an application
file format.
We could continue to enhance the schema with new tables, with indexes
added for performance, with triggers and views for programming convenience,
and constraints to enforce consistency of content even in the face of
programming errors.  Further enhancement ideas include:
</p><ul>
<li> Store an <a href="https://www.sqlite.org/undoredo.html">automated undo/redo stack</a> in a database table so that
     Undo could go back into prior edit sessions.
</li><li> Add <a href="https://www.sqlite.org/fts3.html#fts4">full text search</a> capabilities to the slide deck, or across
     multiple slide decks.
</li><li> Decompose the "settings.xml" file into an SQL table that
     is more easily viewed and edited by separate applications.
</li><li> Break out the "Presenter Notes" from each slide into a separate
     table, for easier access from third-party applications and/or scripts.
</li><li> Enhance the presentation concept beyond the simple linear sequence of
     slides to allow for side-tracks and excursions to be taken depending on
     how the audience is responding.
</li></ul>

<p>
An SQLite database has a lot of capability, which
this essay has only begun to touch upon.  But hopefully this quick glimpse
has convinced some readers that using an SQL database as an application
file format is worth a second look.

</p><p>
Some readers might resist using SQLite as an application
file format due to prior exposure to enterprise SQL databases and
the caveats and limitations of those other systems.  
For example, many enterprise database
engines advise against storing large strings or BLOBs in the database
and instead suggest that large strings and BLOBs be stored as separate
files and the filename stored in the database.  But SQLite 
is not like that.  Any column of an SQLite database can hold
a string or BLOB up to about a gigabyte in size.  And for strings and
BLOBs of 100 kilobytes or less, 
<a href="https://www.sqlite.org/intern-v-extern-blob.html">I/O performance is better</a> than using separate
files.

</p><p>
Some readers might be reluctant to consider SQLite as an application
file format because they have been inculcated with the idea that all
SQL database schemas must be factored into
<a href="https://en.wikipedia.org/wiki/Third_normal_form">Third Normal Form (3NF)</a>
and store only small primitive data types such as strings and integers.  Certainly
relational theory is important and designers should strive to understand
it.  But, as demonstrated above, it is often quite acceptable to store
complex information as XML or JSON in text fields of a database.
Do what works, not what your database professor said you ought to do.

</p><h2>Review Of The Benefits Of Using SQLite</h2>

<p>
In summary,
the claim of this essay is that using SQLite as a container for an application
file format like OpenDocument
and storing lots of smaller objects in that container
works out much better than using a ZIP archive holding a few larger objects.
To wit:

</p><ol>
<li><p>
An SQLite database file is approximately the same size, and in some cases
smaller, than a ZIP archive holding the same information.

</p></li><li><p>
The <a href="https://www.sqlite.org/atomiccommit.html">atomic update capabilities</a>
of SQLite allow small incremental changes
to be safely written into the document.  This reduces total disk I/O
and improves File/Save performance, enhancing the user experience.

</p></li><li><p>
Startup time is reduced by allowing the application to read in only the
content shown for the initial screen.  This largely eliminates the
need to show a progress bar when opening a new document.  The document
just pops up immediately, further enhancing the user experience.

</p></li><li><p>
The memory footprint of the application can be dramatically reduced by
only loading content that is relevant to the current display and keeping
the bulk of the content on disk.  The fast query capability of SQLite
make this a viable alternative to keeping all content in memory at all times.
And when applications use less memory, it makes the entire computer more
responsive, further enhancing the user experience.

</p></li><li><p>
The schema of an SQL database is able to represent information more directly
and succinctly than a key/value database such as a ZIP archive.  This makes
the document content more accessible to third-party applications and scripts
and facilitates advanced features such as built-in document versioning, and
incremental saving of work in progress for recovery after a crash.
</p></li></ol>

<p>
These are just a few of the benefits of using SQLite as an application file
format — the benefits that seem most likely to improve the user
experience for applications like OpenOffice.  Other applications might
benefit from SQLite in different ways. See the <a href="https://www.sqlite.org/appfileformat.html">Application File Format</a>
document for additional ideas.

</p><p>
Finally, let us reiterate that this essay is a thought experiment.
The OpenDocument format is well-established and already well-designed.
Nobody really believes that OpenDocument should be changed to use SQLite
as its container instead of ZIP.  Nor is this article a criticism of
OpenDocument for not choosing SQLite as its container since OpenDocument
predates SQLite.  Rather, the point of this article is to use OpenDocument
as a concrete example of how SQLite can be used to build better 
application file formats for future projects.
</p><p><small><i>This page last modified on  <a href="https://sqlite.org/docsrc/honeypot" id="mtimelink" data-href="https://sqlite.org/docsrc/finfo/pages/affcase1.in?m=26b0aa442a">2025-05-12 11:56:41</a> UTC </i></small></p>

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon RTO policy is costing it top tech talent, according to internal document (167 pts)]]></title>
            <link>https://www.businessinsider.com/amazon-rto-policy-costing-it-top-tech-talent-ai-recruiters-2025-9</link>
            <guid>45132093</guid>
            <pubDate>Thu, 04 Sep 2025 20:56:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.businessinsider.com/amazon-rto-policy-costing-it-top-tech-talent-ai-recruiters-2025-9">https://www.businessinsider.com/amazon-rto-policy-costing-it-top-tech-talent-ai-recruiters-2025-9</a>, See on <a href="https://news.ycombinator.com/item?id=45132093">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="post-body" data-component-type="post-body" data-load-strategy="exclude" data-lock-content="">
            
            
            
            <div data-component-type="post-hero" data-load-strategy="exclude">
                
                <figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
                    <div>
                      <meta itemprop="contentUrl" content="https://i.insider.com/6520574a9f7ca8b2bbdc599b?width=700">
                      <p><img src="https://i.insider.com/6520574a9f7ca8b2bbdc599b?width=700" srcset="https://i.insider.com/6520574a9f7ca8b2bbdc599b?width=400&amp;format=jpeg&amp;auto=webp 400w, https://i.insider.com/6520574a9f7ca8b2bbdc599b?width=500&amp;format=jpeg&amp;auto=webp 500w, https://i.insider.com/6520574a9f7ca8b2bbdc599b?width=700&amp;format=jpeg&amp;auto=webp 700w, https://i.insider.com/6520574a9f7ca8b2bbdc599b?width=1000&amp;format=jpeg&amp;auto=webp 1000w, https://i.insider.com/6520574a9f7ca8b2bbdc599b?width=1300&amp;format=jpeg&amp;auto=webp 1300w, https://i.insider.com/6520574a9f7ca8b2bbdc599b?width=2000&amp;format=jpeg&amp;auto=webp 2000w" sizes="(min-width: 1280px) 900px" alt="A man walks on the street near the Amazon headquarters in Seattle, featuring large glass domes." decoding="sync">
                    </p></div>
                
                  <span>
                        <span>
                          
                          <label for="caption-drawer-btn">
                            <svg role="img" xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 24 24">
                              <path fill="currentColor" fill-rule="evenodd" d="m4.56 18.5 7.486-7.72 7.394 7.626 2.56-2.64L12.046 5.5 2 15.86l2.56 2.64Z"></path>
                            </svg>        </label>
                  
                          <figcaption data-e2e-name="image-caption">
                            <span>Amazon's Seattle headquarters.</span>
                            <span>
                              <span data-e2e-name="image-source" itemprop="creditText">
                                David Ryder/Getty Images
                              </span>          </span>
                          </figcaption>
                        </span>
                  </span></figure>
            </div>
    
    
    
              
      
            
      
              
              
              
              <div data-component-type="post-summary-bullets" data-load-strategy="exclude" data-track-marfeel="post-summary-bullets">
                <ul>
                    <li>Amazon's strict return-to-office policy is limiting its recruitment efforts.</li>
                    <li>The policy requires employees to work in an office 5 days a week and relocate to hubs.</li>
                    <li>Amazon's AI reputation and pay structure also challenge its ability to attract talent.</li>
                </ul>
              </div>
      
            
            
            
            
            <section data-component-type="post-body-content" data-load-strategy="exclude" data-track-content="" data-post-type="story" data-track-marfeel="post-body-content">
            
                <p><a target="_self" href="https://www.businessinsider.com/amazon-stricter-performance-review-process-leadership-principles-2025-7" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">Amazon's</a> 5-day return-to-office policy may be restoring <a target="_self" href="https://www.businessinsider.com/inside-amazons-hardcore-culture-reset-day-1-roots-2025-9" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">discipline</a>, but it's costing the company in the war for tech talent.</p><p>An internal document and accounts from Amazon insiders show that the company's aggressive in-office work policy and requirement to move near designated <a target="_self" href="https://www.businessinsider.com/amazon-voluntary-resignation-employees-relocate-rto-2023-7" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">"hub"</a> offices are hampering recruiting efforts.</p><p>The hub strategy is listed as one of the "hotly debated topics" for Amazon's recruiters, as it is limiting the ability to hire "high-demand talent, like those with GenAI skills," according to the internal document from late last year, obtained by Business Insider.</p><p>Some Amazon recruiters told Business Insider that, starting last year, they saw an increase in candidates declining job offers specifically because of RTO. Those people were open to lower pay from other companies in exchange for the flexibility to work remotely.</p><p>One of the recruiters said the company is losing out on tech talent due to this. The people who spoke with Business Insider asked not to be identified discussing sensitive topics.</p>
                  
                <p>In addition to the strict RTO rules, Amazon has flagged its unusual pay structure and lagging AI reputation as obstacles to recruitment, BI previously <a target="_self" href="https://www.businessinsider.com/amazon-ai-talent-wars-internal-document-2025-8" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">reported</a>.</p><p>The stakes are high. Amazon is racing to stay ahead in the highly competitive generative AI space, but without attracting top talent, the company risks falling behind.</p><p>Oracle, for example, has hired away more than 600 Amazon employees in the past 2 years because Amazon's strict RTO policy has made poaching easier, Bloomberg reported recently.</p>
              
              
              
            <p>In an email to BI, Amazon's spokesperson said that the premise of this story was wrong, adding that the company continues to attract and retain some of the best people in the world."</p><p>"We are always looking for ways to optimize our recruiting strategies and looking at alternate talent rich locations," the spokesperson said. </p><p>Many firms are tightening RTO, but Amazon stands out. It demands 5 days in-office and ties compliance to <a target="_self" href="https://www.businessinsider.com/amazon-suspends-promotions-employees-not-meeting-rto-mandate-2023-11" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">promotions</a> and performance reviews. Those who refuse to relocate to "hubs" are considered by Amazon to have <a target="_self" href="https://www.businessinsider.com/amazon-voluntary-resignation-employees-relocate-rto-2023-7" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">voluntarily resigned</a>.</p><p>"We continue to believe that teams produce the best results when they're collaborating and inventing in person, and we've observed that to be true now that we've had most people back in the office each day for some time," the Amazon spokesperson said. </p><p>Wall Street is already <a target="_self" href="https://www.businessinsider.com/amazon-tumbles-ceo-andy-jassy-aws-cloud-ai-growth-concerns-2025-7" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">noticing</a>. A recent report by venture capital firm SignalFire found Amazon on the lower end of engineer retention, behind Meta, OpenAI, and Anthropic.</p><p><em>Have a tip? Contact this reporter via email at </em><a target="_blank" href="mailto:ekim@businessinsider.com" data-track-click="{&quot;click_type&quot;:&quot;other&quot;,&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;outbound_click&quot;}" rel=" nofollow"><em><u>ekim@businessinsider.com</u></em></a><em> or Signal, Telegram, or WhatsApp at 650-942-3061. Use a personal email address, a nonwork WiFi network, and a nonwork device; </em><a target="_self" rel="" href="https://www.businessinsider.com/insider-guide-to-securely-sharing-whistleblower-information-about-powerful-institutions-2021-10" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}"><em><u>here's our guide to sharing information securely</u></em></a><em>.</em></p>
            
            
            </section>
            
            
            
            
              
            
    
    
    
    
      </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Classic 8×8-pixel B&W Mac patterns (168 pts)]]></title>
            <link>https://www.pauladamsmith.com/blog/2025/09/classic-mac-patterns.html</link>
            <guid>45131538</guid>
            <pubDate>Thu, 04 Sep 2025 19:53:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pauladamsmith.com/blog/2025/09/classic-mac-patterns.html">https://www.pauladamsmith.com/blog/2025/09/classic-mac-patterns.html</a>, See on <a href="https://news.ycombinator.com/item?id=45131538">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
        <p><a href="https://paulsmith.github.io/classic-mac-patterns/"><img src="https://www.pauladamsmith.com/images/classic-mac-patterns/pattern_preview.png" alt="all 38 patterns in a 2x19 grid"></a></p>
<p><strong><a href="https://paulsmith.github.io/classic-mac-patterns/">TL;DR: I made a website for the original classic Mac patterns</a></strong></p>
<p>I was working on something and thought it would be fun to use one of the
classic Mac black-and-white patterns in the project. I'm talking about the
original 8×8-pixel ones that were in the original Control Panel for setting the
desktop background and in MacPaint as fill patterns.</p>
<p><img src="https://www.pauladamsmith.com/images/classic-mac-patterns/controlpanel2.png" alt="pic of Control Panel in System 1"></p>
<p><img src="https://www.pauladamsmith.com/images/classic-mac-patterns/macpaint2.png" alt="pic of MacPaint"></p>
<p>Screenshots via to <a href="https://aresluna.org/frame-of-preference/">Marcin's awesome interactive
history</a></p>
<p>I figured there'd must be clean, pixel-perfect GIFs or PNGs of them somewhere
on the web. And perhaps there are, but after poking around a bit, I ran out of
energy for that, but by then had a head of steam for extracting the patterns en
masse from the original source, somehow. Then I could produce whatever format I
needed for them.</p>
<p>There are 38 patterns, introduced in the original System 1.0 in the 1984 debut
of the Macintosh. They were unchanged in later versions, so I decided to get
them from a System 6 disk, since that's a little easier with access to utility
programs.</p>
<h2><span>Preparation</span></h2>
<ul>
<li>Download <a href="https://www.gryphel.com/c/minivmac/">Mini vMac</a>.</li>
<li>Acquire "<a href="https://en.wikipedia.org/wiki/Old_World_ROM">old world</a>" Mac ROMs.</li>
<li>Download a <a href="https://www.macintoshrepository.org/16994-mac-system-6-0-8-for-minivmac">System 6</a> startup disk image.</li>
<li>Download <a href="https://www.gryphel.com/c/minivmac/extras/exportfl/index.html">ExportFl</a> disk image.</li>
<li>Download <a href="https://www.gryphel.com/c/minivmac/extras/sitpack/index.html">sitPack</a> disk image.</li>
<li>Install "<a href="https://theunarchiver.com/">The Unarchiver</a>" (<code>brew install --cask the-unarchiver</code>)</li>
<li>Install the <a href="https://kagi.com/search?q=xcode+command+line+tools">Xcode command-line tools</a>.</li>
</ul>
<h2><span>Extraction process</span></h2>
<p>Start System 6 (drag the ROM onto the Mini vMac icon, then drag the System 6
disk onto the window when you see the flashing floppy disk). Mount the ExportFl
and sitPack disks by dragging their files and dropping on the classic Mac
desktop.</p>
<h3><span>In emulation</span></h3>
<p>Double-click sitPack to launch the program. Command-O to open, then navigate to
the startup disk by clicking "Drive". Scroll to find "System Folder" and
double-click on it. Scroll to the bottom, select "System" and click "Open". Save
the output file as "System.sit" in the top-level of the startup disk. Quit
sitPack back to the Finder.</p>
<p>Start the ExportFl program. Command-O or pick "Open" from the "File" menu. Find
the "System.sit" created in the last step and click "Open". A regular file save
dialog will appear on the modern Mac, pick a location and save the file.</p>
<h3><span>On the modern Mac</span></h3>
<p>Drag the "System.sit" file onto The Unarchiver, or open the file from within it.
This will produce a file called "System" (with no extension).</p>
<p>Run DeRez (part of the Xcode developer command-line tools) on the System file.
I first added <code>/Library/Developer/CommandLineTools/usr/bin</code> to my <code>$PATH</code>, then
ran:</p>
<div><pre><span></span>$<span> </span>DeRez<span> </span>-only<span> </span>PAT<span>\#</span><span> </span>System<span> </span>&gt;<span> </span>patterns.r
</pre></div>

<p>This produces a text representation of the <code>PAT#</code> resource in the System file.
It's a series of bytes that comprise 38 8×8 patterns meant for QuickDraw
commands. There's a leading big-endian unsigned 16-bit number (<code>0026</code>) to indicate the number of 8-byte patterns to follow.</p>
<pre><code>data 'PAT#' (0, purgeable) {
	$"0026 FFFF FFFF FFFF FFFF DDFF 77FF DDFF"
	$"77FF DD77 DD77 DD77 DD77 AA55 AA55 AA55"
	$"AA55 55FF 55FF 55FF 55FF AAAA AAAA AAAA"
	$"AAAA EEDD BB77 EEDD BB77 8888 8888 8888"
	$"8888 B130 031B D8C0 0C8D 8010 0220 0108"
	$"4004 FF88 8888 FF88 8888 FF80 8080 FF08"
	$"0808 8000 0000 0000 0000 8040 2000 0204"
	$"0800 8244 3944 8201 0101 F874 2247 8F17"
	$"2271 55A0 4040 550A 0404 2050 8888 8888"
	$"0502 BF00 BFBF B0B0 B0B0 0000 0000 0000"
	$"0000 8000 0800 8000 0800 8800 2200 8800"
	$"2200 8822 8822 8822 8822 AA00 AA00 AA00"
	$"AA00 FF00 FF00 FF00 FF00 1122 4488 1122"
	$"4488 FF00 0000 FF00 0000 0102 0408 1020"
	$"4080 AA00 8000 8800 8000 FF80 8080 8080"
	$"8080 081C 22C1 8001 0204 8814 2241 8800"
	$"AA00 40A0 0000 040A 0000 0384 4830 0C02"
	$"0101 8080 413E 0808 14E3 1020 54AA FF02"
	$"0408 7789 8F8F 7798 F8F8 0008 142A 552A"
	$"1408"
};
</code></pre>
<p>It would have been simple enough to
parse this text, but I had Claude quickly make a <a href="https://github.com/paulsmith/mac-desktop-patterns/blob/3e033b4f0f3aedd9de089c440bcec3387478bcef/extract.py">Python
program</a>
to do so and output them in .pbm format, which is part of the Netpbm image
format class. This is a simple image format that is text-based, a '1' or a
'0' indicating a black or white pixel in a row and column.</p>
<p>For example, this subway tile pattern <img src="https://www.pauladamsmith.com/images/classic-mac-patterns/pattern_011-64x64.png" alt="example pattern11"> is represented like
this in .pbm:</p>
<pre><code>P1
8 8
1 1 1 1 1 1 1 1
1 0 0 0 0 0 0 0
1 0 0 0 0 0 0 0
1 0 0 0 0 0 0 0
1 1 1 1 1 1 1 1
0 0 0 0 1 0 0 0
0 0 0 0 1 0 0 0
0 0 0 0 1 0 0 0
</code></pre>
<p>From here, I can generate image files for the patterns in any format and
resolution I want, using ImageMagick or similar. It's important when scaling the
patterns to use <code>-filter point</code>, so that ImageMagick doesn't try to interpolate
the pixels it needs to fill in, which would lead to blurry results.</p>
<p><a href="https://paulsmith.github.io/classic-mac-patterns/"><img src="https://www.pauladamsmith.com/images/classic-mac-patterns/pattern_preview.png" alt="all 38 patterns in a 2x19 grid"></a></p>
<h2><span>Why do all this?</span></h2>
<p>It's nostalgic, I have a fondness for these old patterns and the original B&amp;W
Mac aesthetic, it reminds me of playing games like Dark Castle and Glider,
messing around with HyperCard, and using Tex-Edit and hoarding early shareware
programs.</p>
<p>The whole point of the above is to get a copy of the System file out with the
<a href="https://en.wikipedia.org/wiki/Resource_fork">resource fork</a> intact, that's
where the desktop patterns live.</p>
<p>According to old classic Mac
<a href="https://developer.apple.com/library/archive/documentation/mac/pdf/ImagingWithQuickDraw.pdf">manuals</a>,
the patterns were QuickDraw bit-pattern resources, a simple bitmap of 8 bits
per row packed into 8 bytes (columns). It was fast for QuickDraw to copy them
over an area of the screen. For example the following pattern was used for the
default gray desktop pattern on black-and-white Mac screens.</p>
<p><img src="https://www.pauladamsmith.com/images/classic-mac-patterns/pattern_03_64x64.png" alt=""></p>
<p>I could have extracted all 38 patterns other ways: I could have screenshotted
each one, I could have looked at each one and hand-written .pbm files, both of
which would have been tedious and error-prone.</p>
<p>Ultimately, I wanted to extract the exact original data from the source (or
close enough copy thereof) and have the patterns in a format I considered
archival for this limited purpose (.pbm files are trivial to parse and
manipulate).</p>
<p>Head over to my <a href="https://paulsmith.github.io/classic-mac-patterns/">pattern site</a> to get the patterns for yourself.</p>
<p>(Credit for replica <a href="https://fontstruct.com/fontstructions/show/1744455/geneva-9-2">Geneva 9pt</a> and <a href="https://fontstruct.com/fontstructions/show/2230457/chicago-12-13-11">Chicago 12pt</a> fonts)</p>


        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLM Visualization (544 pts)]]></title>
            <link>https://bbycroft.net/llm</link>
            <guid>45130260</guid>
            <pubDate>Thu, 04 Sep 2025 18:06:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bbycroft.net/llm">https://bbycroft.net/llm</a>, See on <a href="https://news.ycombinator.com/item?id=45130260">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>LLM Visualization</p><div><p><a href="https://bbycroft.net/">Home</a></p></div></div></div>]]></description>
        </item>
    </channel>
</rss>