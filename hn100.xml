<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 29 Jan 2026 15:30:11 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[T√úV Report 2026: Tesla Model Y has the worst reliability of all 2022‚Äì2023 cars (151 pts)]]></title>
            <link>https://www.autoevolution.com/news/tuev-report-2026-tesla-model-y-has-the-worst-reliability-among-all-20222023-cars-261596.html</link>
            <guid>46809105</guid>
            <pubDate>Thu, 29 Jan 2026 12:07:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.autoevolution.com/news/tuev-report-2026-tesla-model-y-has-the-worst-reliability-among-all-20222023-cars-261596.html">https://www.autoevolution.com/news/tuev-report-2026-tesla-model-y-has-the-worst-reliability-among-all-20222023-cars-261596.html</a>, See on <a href="https://news.ycombinator.com/item?id=46809105">Hacker News</a></p>
Couldn't get https://www.autoevolution.com/news/tuev-report-2026-tesla-model-y-has-the-worst-reliability-among-all-20222023-cars-261596.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[The tech market is fundamentally fucked up and AI is just a scapegoat (256 pts)]]></title>
            <link>https://bayramovanar.substack.com/p/tech-market-is-fucked-up</link>
            <guid>46809069</guid>
            <pubDate>Thu, 29 Jan 2026 12:03:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bayramovanar.substack.com/p/tech-market-is-fucked-up">https://bayramovanar.substack.com/p/tech-market-is-fucked-up</a>, See on <a href="https://news.ycombinator.com/item?id=46809069">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>Writing about layoffs and the tech market has been on my TODO for several years. Yesterday, the news of </span><a href="https://www.aboutamazon.com/news/company-news/amazon-layoffs-corporate-jan-2026" rel="nofollow ugc noopener">16k Amazon layoffs</a><span> plus two LinkedIn posts on the same topic back-to-back encouraged me to finally write about it.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!ApMN!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccab62cb-98c6-4b9f-9e1d-4da0dc5bec91_364x600.jpeg" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ApMN!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccab62cb-98c6-4b9f-9e1d-4da0dc5bec91_364x600.jpeg 424w, https://substackcdn.com/image/fetch/$s_!ApMN!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccab62cb-98c6-4b9f-9e1d-4da0dc5bec91_364x600.jpeg 848w, https://substackcdn.com/image/fetch/$s_!ApMN!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccab62cb-98c6-4b9f-9e1d-4da0dc5bec91_364x600.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!ApMN!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccab62cb-98c6-4b9f-9e1d-4da0dc5bec91_364x600.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!ApMN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccab62cb-98c6-4b9f-9e1d-4da0dc5bec91_364x600.jpeg" width="364" height="600" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ccab62cb-98c6-4b9f-9e1d-4da0dc5bec91_364x600.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:600,&quot;width&quot;:364,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:117339,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://bayramovanar.substack.com/i/186173242?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccab62cb-98c6-4b9f-9e1d-4da0dc5bec91_364x600.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ApMN!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccab62cb-98c6-4b9f-9e1d-4da0dc5bec91_364x600.jpeg 424w, https://substackcdn.com/image/fetch/$s_!ApMN!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccab62cb-98c6-4b9f-9e1d-4da0dc5bec91_364x600.jpeg 848w, https://substackcdn.com/image/fetch/$s_!ApMN!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccab62cb-98c6-4b9f-9e1d-4da0dc5bec91_364x600.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!ApMN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccab62cb-98c6-4b9f-9e1d-4da0dc5bec91_364x600.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><strong>Disclaimer:</strong><span> I worked 5 years at Shopify. This is probably why such posts come one after another on my feed but Shopify isn‚Äôt the point here, they are just a micro piece of the whole fucked up system. </span></p><p>Tech job market is fundamentally broken and we all pointing fingers at AI. </p><p>But having spent almost 2 decades in the industry, I think the rot goes much deeper than ChatGPT. </p><p><span>Truth to be told tech market hasn‚Äôt truly ‚Äòimproved‚Äô since the 2008 financial crisis. It just mutated into something </span><strong>evil</strong><span>. </span></p><p><span>After the 2008 mortgage crisis, the economic regime significantly changed. Which was also around the time my interest in Finance began and recently </span><a href="https://bayramovanar.substack.com/p/why-i-built-bullsheet-part-1" rel="nofollow ugc noopener">I started to build my own investment tool you can read more about it here. </a></p><p>At that time time we entered an era of extensive liquidity (cheap money). When interest rates are near zero, investors demand growth above all else. </p><p><span>As a result, tech companies stopped building for </span><strong>sustainability</strong><span> and started building for </span><strong>exponential expansion</strong><span>.</span></p><p>Here is a graph shows US Fed Interest Rates by years. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!meTk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c68c210-6cda-4133-a7bf-74e5f90505da_2140x1394.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!meTk!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c68c210-6cda-4133-a7bf-74e5f90505da_2140x1394.png 424w, https://substackcdn.com/image/fetch/$s_!meTk!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c68c210-6cda-4133-a7bf-74e5f90505da_2140x1394.png 848w, https://substackcdn.com/image/fetch/$s_!meTk!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c68c210-6cda-4133-a7bf-74e5f90505da_2140x1394.png 1272w, https://substackcdn.com/image/fetch/$s_!meTk!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c68c210-6cda-4133-a7bf-74e5f90505da_2140x1394.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!meTk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c68c210-6cda-4133-a7bf-74e5f90505da_2140x1394.png" width="1456" height="948" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0c68c210-6cda-4133-a7bf-74e5f90505da_2140x1394.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:948,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:135450,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://bayramovanar.substack.com/i/186173242?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c68c210-6cda-4133-a7bf-74e5f90505da_2140x1394.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!meTk!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c68c210-6cda-4133-a7bf-74e5f90505da_2140x1394.png 424w, https://substackcdn.com/image/fetch/$s_!meTk!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c68c210-6cda-4133-a7bf-74e5f90505da_2140x1394.png 848w, https://substackcdn.com/image/fetch/$s_!meTk!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c68c210-6cda-4133-a7bf-74e5f90505da_2140x1394.png 1272w, https://substackcdn.com/image/fetch/$s_!meTk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c68c210-6cda-4133-a7bf-74e5f90505da_2140x1394.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><h6><span>Ref: </span><a href="https://www.macrotrends.net/2015/fed-funds-rate-historical-chart" rel="nofollow ugc noopener">https://www.macrotrends.net/2015/fed-funds-rate-historical-chart</a></h6><p>In traditional industries like manufacturing you don‚Äôt hire 500 factory workers unless you have a production line that needs them. You don‚Äôt over-hire based on a guess.</p><p>But in Tech, the playbook is different. Companies over-hire software engineers intentionally. To play the lottery. It is similar to having slow and steady ETF investments vs active investing. No matter how godly you are with active investing sooner or later, you will invest on a loser. Same goes for businesses.</p><p>In a factory, ‚ÄúWork in Progress‚Äù (unfinished goods) is a liability. You don‚Äôt want inventory sitting on the floor; you want it out the door. </p><p><span>In software, we convinced ourselves that ‚ÄúWork in Progress‚Äù (hiring engineers to work on projects that&nbsp;</span><strong>haven‚Äôt</strong><span>&nbsp;shipped yet) is an </span><strong>asset</strong><span>.</span></p><p><span>It is not. It is just&nbsp;</span><strong>excessive inventory.</strong></p><p>When the market turned, companies realized they were warehousing talent like unsold products. And just like unsold inventory, when the storage costs get too high, you dump it. </p><p>Layoffs have become a product feature.</p><p>Till ~2010, a layoff was a sign of failure. It meant the CEO messed up.  </p><p>In 2024, a layoff is a signal of ‚Äúdiscipline.‚Äù Companies lay off thousands, and their stock price&nbsp;jumps. </p><p>They are signaling to Wall Street that they are willing to sacrifice human capital to protect margins. </p><p>Big Tech companies (think Google, Meta, or any hyper-growth SaaS) operate on a two-tier system:</p><p><span>1. </span><strong>The Core:</strong><span> A fundamental team working on the actual revenue-generating products (the search engine, the ad network, the checkout flow).</span></p><p><span>2. </span><strong>The Bets:</strong><span> Thousands of engineers hired to build parallel products, experimental features, or simply to keep talent away from competitors and potentially build something that would move into ‚ÄúThe Core‚Äù tier. </span></p><p>The company knows that most of these side bets will fail. When the economic winds change, the ‚Äònon-core‚Äô staff becomes immediately replaceable.</p><p>It‚Äôs a vicious cycle: Hire the best people you can find to hoard talent, see what sticks, and lay off the rest when investors want to see better margins.</p><p>This dynamic creates a cruel paradox for engineers.</p><p>Most engineers (including me) spent months grinding LeetCode at least twice in their career, studying system design, and passing grueling 6-round interviews to prove they are the ‚Äútop 1%.‚Äù </p><p>Yet, once hired, they are often placed on a non-essential team where they become nothing more than a statistic on a spreadsheet.</p><p>You jump through hoops to prove you are exceptional, only to be treated as disposable.</p><p>For a long time, Europe offered a counter-balance. The pay was lower than Silicon Valley, but the trade-off was stability, stronger labor protections, and a slower, more sustainable pace of work.</p><p>That social contract is breaking.</p><div><p><span>As American tech giants expanded into Europe and as European unicorns chased the same growth-at-all-costs playbooks the incentives changed. </span><br><span>Leadership imported US-style compensation models, investor expectations, and organizational volatility, but without importing US-level pay or upside. </span></p><p><span>‚ÄùOn paper‚Äù Europe still has strong labor laws. In practice, companies learned to route around them: constant reorganizations, ‚Äústrategic refocus‚Äù layoffs, performance-managed exits.</span></p></div><p>The result is the worst of both worlds. European engineers now face US-level job insecurity with European-level compensation and limited mobility. The safety net hasn‚Äôt disappeared, but it‚Äôs being slowly hollowed out.</p><p><strong>And severances‚Ä¶ A small, one-time payment is used to justify years of below  market compensation, while offering little real protection against sudden displacement.</strong></p><p>Europe just became a lower-cost extension of Silicon Valley.</p><p>Ultimately, this comes down to how companies signal value.</p><p>Traditional businesses used to show their health through revenue, profit, and smart capital investment. Today, Tech companies use layoffs as a marketing signal to Wall Street. They cut costs not because they are going bankrupt, but to show they can be ‚Äúefficient.‚Äù</p><p>The more liquidity&nbsp;that was&nbsp;pumped into Tech, the harder this situation&nbsp;became. As long as engineers are treated as speculative assets rather than human capital, the market will remain broken regardless of how good AI gets.</p><p>The job market is not ‚Äútough‚Äù right now because of AI. It is tough because we are unwinding 14 years of financial toxicity.</p><p>The liquidity that flooded the tech sector didn‚Äôt just inflate valuations; it inflated teams, egos, and expectations. </p><p>Until the industry relearns how to build with scarcity rather than excess, the ‚Äúvicious cycle‚Äù of hire-and-dump will continue regardless of how good AI will get.</p><p><strong>So you aren‚Äôt being laid off because your performance was bad; you are being effectively ‚Äúliquidated‚Äù like a bad stock trade that you sell with a loss.</strong><span> </span></p></div></article></div><div><div id="discussion"><h4>Discussion about this post</h4></div><div><h3>Ready for more?</h3></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vitamin D and Omega-3 have a larger effect on depression than antidepressants (539 pts)]]></title>
            <link>https://blog.ncase.me/on-depression/</link>
            <guid>46808251</guid>
            <pubDate>Thu, 29 Jan 2026 10:35:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.ncase.me/on-depression/">https://blog.ncase.me/on-depression/</a>, See on <a href="https://news.ycombinator.com/item?id=46808251">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="content">

        <!-- Is Draft, Is Deprecated, Is Old -->
        
            <!-- TODO: don't show this segment at all, figure out age on build-side -->
            
        

		<p><em>(content note: scientific discussion of depression &amp; suicide)</em></p>
<p>
<b>"Too Long; Didn't Read" Summary:</b><br>
Exactly what the title says.
</p>
<p>The "standardised effect size" of antidepressants on depression, vs placebo, is around 0.4. (This is like going from an average <strong>C to a C+</strong>.)</p>
<p>In contrast, the effect size of 1500mg/day of "‚â•60% EPA" Omega-3 supplements ‚Äî which are cheaper &amp; have fewer side effects than antidepressants ‚Äî is a bit higher, around 0.6. (This is like going from a <strong>C to a B‚Äì</strong>.)</p>
<p>But, much better: the effect size of 5000mg/day of Vitamin D is around <em>1.8!</em> (This is like going from a <strong>C to an A‚Äì</strong>!) It works even for people who <em>don't</em> have a Vitamin D insufficiency, which almost half of American adults <em>do</em>.</p>
<p>Even if you're already taking Vitamin D &amp; Omega-3, you may <em>still</em> not be taking enough. The "official" recommendations <em>are all 3 to 10 times too low</em>. Both these supplements are safe, cheap, and over-the-counter, with <em>positive</em> side-effects (on Covid &amp; cognition).</p>
<p>So, unless you have specific reasons to not take Vitamin D &amp; Omega-3 ‚Äî (kidney stones, blood thinners, etc) ‚Äî please try them, for at least a month! They could save your mental health. Maybe even your life.</p>
<p><strong>Table of Contents:</strong></p>
<ul>
<li>A crash course in "effect sizes" <a href="#effect_sizes">‚Ü™</a></li>
<li>Interpreting effect sizes on depression <a href="#interpreting_depression">‚Ü™</a></li>
<li>Antidepressants <a href="#antidepressants">‚Ü™</a></li>
<li>Omega-3 <a href="#omega_3">‚Ü™</a></li>
<li>Vitamin D <a href="#vitamin_d">‚Ü™</a></li>
<li>Conclusion: <em>All this time, you lacked the Vitamin?</em> <a href="#conclusion">‚Ü™</a></li>
</ul>
<hr>

<h2>A crash course in "effect sizes"</h2>
<p>In Alicetown, the average person has 4 younger cousins.<br>
In Bobtown, the average person has 3 younger cousins.</p>
<p>Alright, not so surprising. You may not even notice a difference.</p>
<p>In Alicetown, the average person has 4 limbs.<br>
In Bobtown, the average person has 3 limbs.</p>
<p>You'd <em>definitely</em> notice.</p>
<p>It's the same absolute difference (4 vs 3) <em>and</em> relative difference (3/4). So what makes limbs more surprising than cousins? Well, partly it's more dramatic &amp; visible, but also because: <em>we expect high variation in the number of someone's younger cousins, but not their number of limbs</em>.</p>
<p>This is why scientists calculate an <strong>"effect size"</strong> or <strong>"standardized mean difference"</strong> ("mean" = average). We take the difference between two groups, then divide by the total amount of variation, <em>to account for how surprising a difference is</em>.</p>
<p>(This is a health article, not a math article, so I'll skip the formulas in this post. If you're curious, <a href="https://www.youtube.com/watch?v=tTgouKMz-eI">: check out this 4 min video</a>.)</p>
<p>Unfortunately for laypeople, the effect size is usually just reported as a number, like "+0.74" for <a href="http://www.lscp.net/persons/ramus/docs/EPR20.pdf">spacing out your studying vs cramming</a>, or "‚Äì0.776" for <a href="https://pubmed.ncbi.nlm.nih.gov/20438143/">sleep deprivation on attention</a>.</p>
<p>But what's that <em>mean?</em> How can we make these numbers <em>intuitive?</em></p>
<p>Well, a common way for data to be is <a href="https://en.wikipedia.org/wiki/Normal_distribution">a bell-shaped curve</a> (also called a "normal distribution"). And most of us are, alas, well-acquainted with the bell curve in school grades. ("grading on a curve")</p>
<p>So: school grades give us a useful way to think about standardized effect sizes! We can now convert that number <em>into an actual letter grade:</em></p>
<ul>
<li><strong>F:</strong> -2.0 below average</li>
<li><strong>D:</strong> -1.0 below average</li>
<li><strong>C:</strong> average</li>
<li><strong>B:</strong> +1.0 above average</li>
<li><strong>A:</strong> +2.0 above average</li>
</ul>
<p>(see footnote for more precise ranges.<sup><a href="#fn1" id="fnref1">[1]</a></sup> the units are in "standard deviations", or "sigmas". what's sigma? <s>sigma ba--</s> just a unit of "how far away this is from average, relative to the total variation".)</p>
<p><img src="https://blog.ncase.me/content/stuff/2026-01/depression/bell_grade.png" alt="How to convert effect sizes to letter grades" title="How to convert effect sizes to letter grades"></p>
<p>For example: spacing out your studying, relative to cramming, will on average <em>lift</em> your test scores from a C to a B‚Äì. (effect size = +0.74) And short-term sleep deprivation, relative to healthy sleep, will on average <em>tank</em> your ability to pay attention from a C to a D+. (effect size: ‚Äì0.776)</p>
<p>(Note ‚Äî when reading about effect sizes, always remember: <em>effect of what, on what, at what dose, for which group, relative to what?</em> See the Data Colada post, <a href="https://datacolada.org/104">Meaningless Means</a>.)</p>
<p>(Note 2 ‚Äî the standard way of "intuitively" describing effect sizes is Cohen's recommendations: 0.2 = small, 0.5 = medium, 0.8 = large. Personally, I prefer the "school grade letter" comparison, since it's more concrete. But hey, you do you.)</p>
<p>But it's not limited to just grades &amp; academic performance. Effect sizes can also help us understand any kind of difference between groups, in observation or in experiments!</p>
<p>For example...</p>
<hr>


<p>Let's use our school grade analogy, to interpret effect sizes on mental health:</p>
<p><strong>What's an "F in mental health"?</strong> By definition of a bell curve, ~2.3% of people are below ‚Äì2 sigma (an "F"). (See: <a href="https://homepage.divms.uiowa.edu/~mbognar/applets/normal.html">this bell curve calculator</a>.) <a href="https://www.canada.ca/en/public-health/services/publications/healthy-living/suicide-canada-key-statistics-infographic.html">In Canada</a>, ~2.6% of people had suicidal ideation in 2022, while <a href="https://www.pew.org/en/research-and-analysis/data-visualizations/2024/us-national-trends-and-disparities-in-suicidal-ideation-suicide-attempts-and-health-care-use">in the US</a>, it was ~4.9% in 2019. So, it's not too far off to say: "F in mental health = literally suicidal". (Also, reminder that ~4% is 1-in-25 people. You likely know someone, or <em>are</em> someone, who will feel suicidal this year. Please reach out to your friends &amp; loved ones!)</p>
<p><strong>What's a "D in mental health"?</strong> ~16% of people are below ‚Äì1 sigma (a "D") on a bell curve. <a href="https://pubmed.ncbi.nlm.nih.gov/12096700/">The Keyes 2002 study</a> estimated that ~14.1% of adults meet the DSM-III criteria for a major depressive episode. So, D = Depressed.</p>
<p><strong>What's an average "C in mental health"?</strong> ~68% of people are within a sigma of average (a "C") on a bell curve. Same above study found that 56.6 percent had moderate mental health. They were neither "languishing" nor "flourishing". I guess C = Could Be Worse.</p>
<p><strong>What's a "B in mental health"?</strong> ~16% of people are <em>above</em> +1 sigma (a "B") on a bell curve. Same above study found that 17.2% of adults are "flourishing". Good for them! B = Flourishing, life is good.</p>
<p><strong>What's an "A in mental health"?</strong> I don't know who these freaks are. I actually <em>could not</em> find any scientific studies on "the +2 sigma in well-being". In contrast, there's <em>lots</em> of research on suicidal ideation, the ‚Äì2 sigma in well-being. In the absence of any actual data, I'll just say: A = AWESOME</p>
<p><img src="https://blog.ncase.me/content/stuff/2026-01/depression/bell_curve_mental_health.png" alt="Bell curve of mental health, mapped to effect size / letter grade." title="Bell curve of mental health, mapped to effect size / letter grade."></p>
<p>So, if an intervention is found to have an effect size of +1.0, that's like going up a letter grade. If something's found to have an effect size of -2.0, that's like going <em>down</em> two letter grades. And so on.</p>
<p>Okay, so how do we get peoples' "mental health grades" up?</p>
<p>Let's look at antidepressants, Omega-3, and Vitamin D, in turn:</p>
<hr>

<h2>Antidepressants</h2>
<p>The good news is they work. The bad news is they don't work as well as you'd think they may work.</p>
<p><strong><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC5889788/">Cipriani et al 2018</a></strong> is a recent meta-analysis (a study collecting lots of previous studies) that investigated 21 different antidepressants. The most effective antidepressant, <em>Amitriptyline</em>, relative to placebo, had an Odds Ratio of 2.13  ‚Äî <a href="https://www.escal.site/">which converts to</a> a Cohen's d effect size of 0.417 ‚Äî which is "small-medium" according to Cohen's recommendations. Or, by our school-letter-grade comparison: <strong>the best antidepressant would take your mental health grade from an F to F+, or C to C+.</strong></p>
<p>From Figure 3 of that paper, you can see that Amitriptyline has the highest estimated effect size, while the side effects are no worse than placebo:</p>
<p><img src="https://blog.ncase.me/content/stuff/2026-01/depression/Figure3.jpg" alt="The effect sizes &amp; dropout rates of various antidepressants, vs placebo." title="The effect sizes &amp; dropout rates of various antidepressants, vs placebo."></p>
<p>Sure, "F to F+" <em>can</em> be lifesaving, but‚Ä¶ y'know‚Ä¶ that's not a lot. And again, this is the effect <em>on average.</em> Some people respond <em>much</em> better to antidepressants‚Ä¶ while some respond much worse.</p>
<hr>

<h2>Omega-3</h2>
<p>Keep getting confused on which fat is what? Me too. So, here's a crash course on various fats:</p>
<p>Fatty acids are chains of carbons &amp; hydrogens + two oxygens. They say "OOH" at one end, and "HHH" at the other end:</p>
<p><img src="https://blog.ncase.me/content/stuff/2026-01/depression/fat1.png" alt="Diagram of a fatty acid, details in main text." title="Diagram of a fatty acid, details in main text."></p>
<p>A saturated fatty acid is one where all the carbons' free spots are filled up with hydrogens. (Hence, "saturated") This makes the molecule stick straight out. This is why <em>long</em> saturated fatty acids ‚Äî like those found in butter ‚Äî tend to be solid at room temperature.</p>
<p>(Contrary to popular belief, saturated fats don't <em>literally</em> clog your arteries, like grease in plumbing pipes. What happens is <em>{ha ha I don't actually understand this}</em>. <a href="https://www.ahajournals.org/doi/10.1161/cir.0000000000000510">Something about your cholesterol levels &amp; inflammation</a>.)</p>
<p><img src="https://blog.ncase.me/content/stuff/2026-01/depression/fat2.png" alt="Diagram of saturated vs unsaturated fatty acid, details in main text." title="Diagram of saturated vs unsaturated fatty acid, details in main text."></p>
<p>In contrast, <em>unsaturated</em> fatty acids have at least one hydrogen missing. This causes them to have a double-bond "kink" in the molecule. This makes them <em>not</em> stick out, which is why unsaturated fats tend to be liquid at room temperature. <em>Mono-unsaturated</em> fatty acids (MUFAs) ‚Äî like in olive oil ‚Äî only have one kink. <em>Poly-unsaturated</em> fatty acids (PUFAs) ‚Äî like in fatty fish ‚Äî have two or more kinks. Let's be mature adults about this, please.</p>
<p>For completeness: <em>trans fats</em> are unsaturated fats whose "kink" is twisted around, causing them to go straight. That is the worst sentence I've written all month. The twisted kink is caused by the hydrogens being on opposite sides, hence "trans". (And yes, if they're on the same side it's "cis". Latin was a mistake.) The molecule being straight is why trans fats ‚Äî which margarine <em>used</em> to be full of  ‚Äî are solid at room temperature, despite being an unsaturated fat.</p>
<p><img src="https://blog.ncase.me/content/stuff/2026-01/depression/fat3.png" alt="Diagram of how a 'cis' saturated fat becomes a 'trans' fat, details in main text." title="Diagram of how a 'cis' saturated fat becomes a 'trans' fat, details in main text."></p>
<p>It's neat whenever you can trace the history of something right down to its atoms! Margarine was first invented because it's cheaper, and is spreadable straight from the fridge, unlike butter. Margarine (used to be) made by taking <em>unsaturated</em> vegetable oils, which were cheaper than animal fats, then pumping a bunch of hydrogens into it (hence, "hydrogenated oils"). If you <em>completely</em> hydrogenate an oil, it becomes a saturated fat. But they only <em>partially</em> hydrogenated those oils, leading to trans fats, which were cheaper &amp; a spreadable semi-solid at fridge temperature.</p>
<p>In the 1970s &amp; 80s, the US Food &amp; Drug Administration concluded that trans fats were <em>not</em> harmful to humans, and nutritionists <em>promoted</em> margarine over butter, because butter had "unhealthy" saturated fats. <a href="https://www.cspi.org/resource/artificial-trans-fat-timeline">But in the early 1990s</a>, scientists realized that trans fats were <em>even worse</em> for you than saturated fats. Only in the 2010's, did most Western countries start officially banning trans fats. Reminder: policy is often decades behind science.</p>
<p><em>(Hey, what do you call it when you get thiccer on HRT? Trans fat! :D)</em></p>
<p>I need to stop going on infodump tangents. Anyway, Omega-3 is any fatty acid with its first kink at the 3rd carbon from the Omega end ("HHH"), though it can have more kinks later down the chain. (And yes, Omega-6 has its first kink at the 6th carbon, and Omega-9 has its first kink at the 9th carbon. There's nothing <em>physically</em> preventing Omega-4 or Omega-5's from existing, but due to some quirk of evolution, Omega-3, -6, and -9 are the ones biological life uses most. As far as I can tell, there's no specific reason they're all multiples of 3. Probably just a coincidence. There <em>is</em> a less common Omega-7.)</p>
<p><img src="https://blog.ncase.me/content/stuff/2026-01/depression/fat4.png" alt="Diagram of Omega-3, -6, and -9, details in main text." title="Diagram of Omega-3, -6, and -9, details in main text."></p>
<p><em>Finally</em>, there's three main types of Omega-3: EPA (Eicosapentaenoic Acid), DHA (Docosahexaenoic Acid), and ALA (Alpha-Linolenic Acid). ALA is mostly found in plants like chia seeds &amp; walnuts, while EPA &amp; DHA mostly come from seafood, though there <em>are</em> algae-based vegan sources.</p>
<p><em>(Figure 1.1 from <a href="https://atrium.lib.uoguelph.ca/bitstream/10214/9940/1/Roke_Kaitlin_201609_PhD.pdf">Roke 2016</a>.‚§µ Thank you Kaitlin Samantha Roke for drawing this coz I'm too lazy to draw it myself. Note how the first double-bond "kink" for all these molecules is at the 3rd carbon from the Omega end ‚Äî hence why they're all called Omega-3's.)</em></p>
<p><img src="https://blog.ncase.me/content/stuff/2026-01/depression/fat5.png" alt="Diagram of ELA, DHA, &amp; ALA; details in main text." title="Diagram of ELA, DHA, &amp; ALA; details in main text."></p>
<p><strong>EPA &amp; DHA are the focus of this section.</strong> For bio-mechanical reasons I don't understand but I assume someone else does: EPA is the one associated with anti-inflammation, better brain health, and <em>less depression</em>... while DHA isn't. (But DHA is still needed for other stuff, like your neurons' cell walls, so don't cut them out completely!)</p>
<p>(Note: I could not find any <em>experimental</em> trials of ALA on depression, though an <em>observational</em> study in Japan (<a href="https://www.sciencedirect.com/science/article/abs/pii/S2212826313001085">Kurotani et al 2014</a>) finds a correlation between higher ALA and lower depression. But reminder, correlation is not <em>necessarily</em> causation.)</p>
<p>All the above info in a Venn (technically <a href="https://en.wikipedia.org/wiki/Euler_diagram">Euler</a>) diagram:</p>
<p><img src="https://blog.ncase.me/content/stuff/2026-01/depression/fat6.png" alt="Diagram of what I infodumped about just now." title="Diagram of what I infodumped about just now."></p>
<p>Okay, enough yap. Time for the actual data:</p>
<p><strong><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC3534764/">Sublette et al 2011</a></strong> is an older meta-analysis, but it's the only one I could find that tries to estimate the <em>actual "dose-response" curve</em>, which shows: how much effect, for how much treatment. Why is that important? Because one problem with many meta-analyses is they'll do something like: "Study 1 gave patients 1 gram of medicine and saw a +1 improvement in disease, Study 2 gave 10 grams and saw +4 improvement, Study 3 gave 100 grams and saw <em>negative</em> ‚Äì5 improvement‚Ä¶ the average of +1, +4, and ‚Äì5 is zero... therefore the medicine's effect is zero." ...As mentioned briefly earlier, this is a <a href="https://datacolada.org/104">meaningless mean</a>. That's why we want to know the response <em>at each dose</em>.</p>
<p>So, the Sublette meta-analysis gathered randomized trials studying Omega-3 on depression (vs placebo, of course) and got the following dose-response curve.‚§µ Note that the horizontal axis is <em>not</em> just amount of total Omega-3, but specifically <em>the extra amount of "unopposed" EPA, above the amount of DHA.</em> Or in other words, "EPA minus DHA":</p>
<p><img src="https://blog.ncase.me/content/stuff/2026-01/depression/Omega3.jpg" alt="Diagram of dose-response curve, of EPA minus DHA, on depression." title="Diagram of dose-response curve, of EPA minus DHA, on depression."></p>
<p>The top effect size is <strong>around +0.558, which is like going from an F to D‚Äì, or C to B‚Äì.</strong> You get this maximum effect around 1 to 2 grams of extra EPA, and <em>too much</em> EPA gets worse results. The meta-analysis finds that Omega-3 supplements that are ~60% EPA (and the rest DHA) are optimal.</p>
<p>This finding is roughly in line with later meta-analyses. <a href="https://www.nature.com/articles/s41398-019-0515-5.pdf">Liao et al 2019</a> also finds that ~1 gram of ‚â•60% EPA is best, but actually found a much higher effect size: +1.03. <a href="https://pubmed.ncbi.nlm.nih.gov/37028202/">Kelaiditis et al 2023</a> also finds 1 to 2g of ‚â•60% EPA is best, but found a lower effect size of +0.43‚Ä¶ which is still <em>as good</em> as the <em>best</em> antidepressant!</p>
<p>Either way, let's boil this down to a recommendation. You want around 1 gram of EPA a day. So if your supplements are 60% EPA, you need 1 gram √∑ 0.6 ~= 1.667 grams = 1667 milligrams. Let's round this down for convenience: <strong>get 1500 mg/day of 60%-EPA Omega-3 supplements.</strong></p>
<p>In comparison, <a href="https://www.healthline.com/nutrition/how-much-omega-3">most official health organizations recommend</a> "250‚Äì500 mg <em>combined</em> EPA and DHA each day for healthy adults." <em>That is over three times too low,</em> at least for optimal effects on depression. Which, as we calculated above, is probably around 1500 mg/day. (The official safe dose is 5000 mg/day)</p>
<p>Finally, a (small) study <em>directly</em> investigating the link between suicide &amp; Omega-3. <a href="https://psychiatryonline.org/doi/pdf/10.1176/ajp.2006.163.6.1100">Sublette et al 2006</a>: ‚ÄúLow [DHA] and low Omega-3 proportions [...] predicted risk of suicidal behavior among depressed patients over the 2-year period.‚Äù Though keep in mind this is a small study, and it's observational not experimental. Also, weird that contrary to the above studies on depression, <em>DHA</em> predicted suicide but <em>not</em> EPA. Not sure what to make of that.</p>
<p>Bonus: Omega-3 may also boost cognition? <a href="https://www.nature.com/articles/s41598-025-16129-8.pdf">Shahinfar et al 2025</a>: ‚ÄúEnhancement of global cognitive abilities was observed with increasing omega-3 dosage up to 1500 mg/day. [effect size = 1.00, like going from a grade of C to B!], followed by downward trend at higher doses.‚Äù</p>
<hr>

<h2>Vitamin D</h2>
<p><strong><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11650176/">Ghaemi et al 2024</a></strong> is a meta-analysis on Vitamin D on depression. Again, it actually estimates a dose-response curve! Below is Figure 1 + Table 2, showing the effect of Vitamin D dosage on depression vs placebo. The solid line is the average estimated effect, dashed lines are 95% confidence interval. Note the effect size is negative in this figure, because they're measuring <em>reduction</em> in depressive symptoms:</p>
<p><img src="https://blog.ncase.me/content/stuff/2026-01/depression/VitaminD.png" alt="The effect size (with uncertainty) of Vitamin D dosage on depressive symptoms." title="The effect size (with uncertainty) of Vitamin D dosage on depressive symptoms."></p>
<p><strong>The upper range of uncertainty is lowest at 5000 IU (International Units) of Vitamin D a day</strong>, with an estimated effect size of 1.82, with a 95% uncertainty range, from 0.98 to 2.66. <strong>An effect size of 1.82 is like taking your mental health from an F to a C‚Äì, or a C to an A‚Äì!</strong> And even in the <em>most pessimistic</em> case, 0.98, that's still <em>over twice as effective</em> as the top antidepressant!</p>
<p>(The paper's summary says <em>8000 IU</em> is best, with effect size 2.04, but there's much greater uncertainty there. The paper also finds that longer studies had smaller effects than shorter studies, but this does <em>not</em> necessarily mean Vitamin D's effects are short-lived. Looking at Supplementary Table 4, it seems this is partly because longer studies used <em>lower</em> average daily doses. For example, one 52-week study only gave participants 400 IU a day.)</p>
<p>This meta-analysis includes trials with participants who <em>don't</em> have Vitamin D deficiency. There's still a good effect of Vitamin D on depression for them, even if smaller! Though, you probably <em>are</em> lacking Vitamin D: <a href="https://pubmed.ncbi.nlm.nih.gov/29644951/">Liu et al 2018</a> finds that a bit under half of all adults (41.4%) have Vitamin D Insufficiency.</p>
<p>And that's according to <a href="https://www.healthline.com/nutrition/vitamin-d-dosage">the official recommendation</a>, of 400-800 IU a day‚Ä¶ which is <em>is too damn low</em>. Even the official <em>maximum safe dose</em> of Vitamin D, of 4000 IU/day, is too low. <a href="https://pubmed.ncbi.nlm.nih.gov/30611908/">McCullough et al 2019</a> gave over thousands of patients 5,000 to 10,000 IU/day, for <em>seven years</em>, and there were <em>zero</em> cases of serious side effects. This is in line with <a href="https://pubmed.ncbi.nlm.nih.gov/31746327/">Billington et al 2020</a>, a 3-year-long double-blinded randomized controlled trial, where they found "the safety profile of vitamin D supplementation is similar for doses of 400, 4000, and 10,000 IU/day." (though "<em>mild</em> hypercalcemia" increased from 3% to 9%. IMHO, that's a small cost for reducing the risk of major depression &amp; suicide.)</p>
<p>And it makes sense that 10,000 IU a day <em>should</em> be safe. Your skin, exposed to the Sun's ultraviolet rays, can synthesize up to (the equivalent of) 10,000 IU a day, before plateauing out. Source is <a href="https://www.sciencedirect.com/science/article/pii/S0002916522043763">Vieth 1999</a>: ‚ÄúBecause vitamin D is potentially toxic, intake of [1000 IU/day] has been avoided even though the weight of evidence shows that the currently accepted [limit] of [2000 IU/day] is too low by at least 5-fold.‚Äù (So why are all the official sources still so paranoid about Vitamin D? Well, unfortunately, official/governmental policy is always a few decades behind the science in <em>any</em> field. See Also: the trans fat debate, everything about educational policy.)</p>
<p>Speaking of the Sun, why take supplements instead of just getting Vitamin D from Sun exposure? Well, <a href="https://pubmed.ncbi.nlm.nih.gov/29659012/">skin cancer</a>. But also: because Sun-Skin D varies greatly depending on the season, your latitude, and your skin type. There's less ultraviolet rays from the Sun in winter/fall, and at latitudes further from the equator. And <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10861575/">the darker your skin is, the less Vitamin D your skin makes for the same amount of Sun exposure</a>. As expected from the bio-physics of skin, Black adults have <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC6075634/">the highest prevalence of Vitamin D deficiency</a> (82.1%!!), followed by Hispanic adults (62.9%). (But hey, at least Black adults have the lowest incidence of skin cancer. You win some you lose some.) The point is: speaking as someone with Southeast Asian skin, who's currently in Canada during winter... even if I stood outside naked for hours, I'd get approximately <em>zero</em> IU/day of Vitamin D from the Sun. Thus: supplements.</p>
<p>Finally, a meta-analysis <em>directly</em> measuring the effect of Vitamin D on suicide rates. <a href="https://link.springer.com/content/pdf/10.1186/s12888-025-06613-w.pdf">Yu et al 2025</a>: ‚ÄúVitamin D in patients with [suicidal behaviours] were significantly lower than in controls (standardized mean difference: ‚Äì0.69, or a 'medium' difference)‚Äù. Reminder that this paper <em>by itself</em> only measures correlation, not causation ‚Äî but combined with the above experiments of Vitamin D on depression, I think it's reasonable to guess it's partly causal.</p>
<p>To recap:</p>
<ul>
<li>Almost half of you have a Vitamin D deficiency according to the official recommendation (800 IU/day).</li>
<li>And the official recommendation is <em>way</em> too low. Even the official <em>maximum</em> safe dose (4000 IU/day) is below the optimal Vitamin D for depression (5000 IU/day) or what your body can produce from the Sun in optimal conditions (10,000 IU/day). Recent randomized controlled trials confirm that 10,000 IU/day is, indeed, mostly safe.</li>
<li>Your daily reminder than official policy is often decades behind the science.</li>
</ul>
<p>Bonus: Vitamin D supplementation was found in several randomized controlled trials <a href="https://pubmed.ncbi.nlm.nih.gov/39225947/">to reduce mortality from Covid-19</a>! It <a href="https://www.sciencedirect.com/science/article/pii/S2161831322010274">probably helps guard against influenza</a> too, though the evidence is small &amp; early.</p>
<hr>

<h2>Conclusion: <em>All this time, you lacked the Vitamin?</em></h2>
<p>Scurvy is caused by a lack of Vitamin C. It's a condition that causes your wounds to re-open up &amp; teeth to fall out. Scurvy used to kill <em>almost half(!)</em> of all sailors on major expeditions; it's estimated millions died. It can be cured by eating lemons.</p>
<p>Rickets is mostly caused by a lack of Vitamin D. It's a condition where kids' bones go all soft and deformed. During the Industrial Revolution, up to 80% of kids suffered from it. It can be prevented with cod liver oil.</p>
<p>Goiters is mostly caused by a lack of Iodine. It's a condition where the thyroid gland in your neck swells up painfully, to the size of an apple. During WWI, a <em>third</em> of adult men had goiters. It can be prevented with iodized salt.</p>
<p><img src="https://blog.ncase.me/content/stuff/2026-01/depression/The-Vitamin.png" alt="Tumblr meme: 'All this time, you lacked The Vitamin? And yet you persisted?'" title="Tumblr meme: 'All this time, you lacked The Vitamin? And yet you persisted?'"></p>
<p><a href="https://ourworldindata.org/mental-health">About 1 in 4 people are expected to have clinical depression sometime in their life</a>. Depression is the #1 source of the global "burden from disease" in the mental health category, <a href="https://ourworldindata.org/burden-of-disease#how-do-different-diseases-and-disabilities-contribute-towards-the-burden-of-disease">and <em>that</em> category is the #6 burden of disease in the world</a>, above Alzheimer's, malaria, and sexually transmitted infections.</p>
<p>(But honestly, did you need those stats? This is likely a lived experience for a lot of you reading this.)</p>
<p>The effective altruists are all, <a href="https://www.givewell.org/how-much-does-it-cost-to-save-a-life">"woah for just $3000 you can prevent a child's death from malaria"</a> ‚Äî and that's great! save them kids! ‚Äî but where's the fanfare for the accumulating evidence that, "woah with cheap daily supplements we can save millions from suicide &amp; depressed lives"?</p>
<p>Over and over again throughout history, some horrific thing that caused millions to suffer, turned out to be "yeah you were missing this one molecule lol". To be clear: not everything is gonna be <em>that</em> simple, and mental health is <em>not</em> "just" chemistry. Also, all the numbers on this page have with large error bars &amp; uncertainty, more research is needed.</p>
<p>But, as of right now, I feel I can at least confidently claim the following:</p>
<ul>
<li>Vitamin D and Omega-3 are both <em>at least on par</em> with antidepressants.</li>
<li>The evidence is much stronger for Vitamin D; it's very plausibly at least <em>twice</em> as good as antidepressants.</li>
<li>Both supplements are cheap and safe, so what's the harm of trying? (positive "expected value" for this bet)</li>
</ul>
<p>So:</p>
<p><strong>MY SPECIFIC RECOMMENDATIONS FOR YOU TO DO A.S.A.P:</strong></p>
<ul>
<li>Go to a pharmacy, buy the following supplements over-the-counter, in whatever form you like: (I like the easy-to-swallow gel capsules)</li>
<li><strong>Vitamin D</strong>
<ul>
<li>üå± By default, Vitamin D supplements are derived from‚Ä¶ (quick web search)‚Ä¶ <em>the grease in sheep's wool?</em> Huh. Also fish liver oil. Anyway, if you're vegan, make sure your bottle specifically says "vegan" or "from lichen/mushrooms". (If you're vegetarian, the sheep's-wool Vitamin D is fine, they don't kill the sheep for it.)</li>
</ul>
</li>
<li><strong>Omega-3 <em>where EPA is ~60% of the Omega-3 total.</em></strong> For example, my 500mg Omega-3 capsules have 300mg EPA, 200mg DHA.
<ul>
<li>üå± By default, Omega-3 supplements come from fish. If you're veg(etari)?an, there <em>are</em> plant-based sources of Omega-3, but look carefully: most vegan Omega-3 supplements provide <em>more DHA than EPA</em>, which the above studies suggest fully cancel out the antidepressant effect. <em>Double check the nutritional label to make sure it's ‚â•60% EPA.</em> For example, <a href="https://www.amazon.ca/Natures-Way-NutraVege-Plant-Based-Vegetarian/dp/B078T4DC2K?th=1">this one is 300mg EPA + 200mg DHA</a>. (<em>not</em> an affiliate link)</li>
</ul>
</li>
</ul>
<p><strong>Then, every day:</strong></p>
<ul>
<li><strong>Take ~5000 IU of Vitamin D</strong>
<ul>
<li>‚ö†Ô∏è be cautious if you have kidney stones, or are on medications that could interact with Vitamin D. "ask your doctor".</li>
<li>4,000 IU is the "official maximum safe dose", if you understandably don't trust a random internet blogger, even though she cited academic sources.</li>
<li>10,000 IU if you're feeling daring / have darker skin / live in less sunny climates.</li>
<li>bonus: may improve immune response to Covid &amp; influenza?</li>
</ul>
</li>
<li><strong>Take ~1500 mg of ‚â•60%-EPA Omega-3</strong>
<ul>
<li>‚ö†Ô∏è be cautious if you're on blood thinners, or other medications that could interact with Omega-3. again, "ask your doctor".</li>
<li>bonus: may improve cognition?</li>
</ul>
</li>
<li>(Don't quit your existing antidepressants if they're net-positive for you!)
<ul>
<li>you may also want to ask your doctor about <em>Amitriptyline</em>, or those other best-effect-size antidepressants.</li>
</ul>
</li>
</ul>
<p>Can you get these doses of Vitamin D &amp; Omega-3 through whole foods alone, no supplements? Probably, but it'd be expensive &amp; tedious: you'd have to eat something like 2,000 calories of farmed salmon <em>a day</em> to get 5,000 IU/day of Vitamin D. As for Omega-3, eating mostly oily fishes <em>would</em> get you &gt;1000mg of Omega-3, but they'd be <em>more DHA than EPA</em>, which the above studies suggest would cancel out the antidepressant effects.</p>
<p><strong>The effect sizes on depression:</strong></p>
<ul>
<li>The best antidepressant: <strong>+0.417</strong>
<ul>
<li>like your mental health grade going from F to F+, or C to C+</li>
</ul>
</li>
<li>1500mg of ‚â•60%-EPA Omega-3: <strong>+0.558</strong>
<ul>
<li>like your mental health grade going from F to D‚Äì, or C to B‚Äì</li>
</ul>
</li>
<li>5000 IU of Vitamin D: <strong>+1.82</strong>
<ul>
<li>like your mental health grade going from F to C‚Äì, or C to A‚Äì</li>
</ul>
</li>
</ul>
<p>For completeness &amp; comparison, here's the effect size of other things on depression:</p>
<ul>
<li>Any <a href="https://en.wikipedia.org/wiki/Dodo_bird_verdict">mainstream "bona-fide" psychotherapy</a> (CBT, Psychodynamic, Humanist, Solutions-Focused): <strong>+0.35</strong>, source: <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC5244449/">Kamenov et al 2016</a>
<ul>
<li>like going from C to C+</li>
</ul>
</li>
<li>Aerobic/Cardio Exercise: <strong>+0.79</strong>, source <a href="https://pure-oai.bham.ac.uk/ws/portalfiles/portal/54143117/Ioannis_Morres_et_al_Aerobic_exercise_for_adult_patients_Depression_and_Anxiety_2018.pdf">Ioannis et al 2018</a>
<ul>
<li>like going from C to B‚Äì</li>
<li>(dose: "45 minutes, at moderate intensity, three times/week" ‚áí ~20 min/day)</li>
</ul>
</li>
<li>Good Sleep: <strong>+1.10(???)</strong>, a <em>lot</em> of interpretation &amp; calculations, see footnote<sup><a href="#fn2" id="fnref2">[2]</a></sup>
<ul>
<li>like going from C to B</li>
<li>(dose: going from moderate insomnia to healthy sleep)</li>
</ul>
</li>
<li>Bright Light Therapy: <strong>+0.487</strong>, source <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11447633/">Menegaz de Almeida et al 2025</a>
<ul>
<li>(the above paper reports Odds Ratio of 2.42, <a href="https://www.escal.site/">which converts to</a> Cohen's d effect size of +0.487)</li>
<li>like going from C to C+</li>
<li>I went <a href="https://www.nytimes.com/wirecutter/reviews/best-light-therapy-lamp/">with Wirecutter's recommendation</a> for a UV-free 10,000 lux lamp.</li>
<li>(dose: 10,000 lux, 30 min a day)</li>
</ul>
</li>
<li>Mindfulness Meditation: <strong>+0.42</strong>, source <a href="https://www.frontiersin.org/journals/psychiatry/articles/10.3389/fpsyt.2019.00193/full">Breedvelt et al 2019</a>
<ul>
<li>like going from C to C+</li>
<li>(dose: 7 weeks, "153 min each week" ‚áí ~20 min/day)</li>
</ul>
</li>
</ul>
<p><img src="https://blog.ncase.me/content/stuff/2026-01/depression/on-depression-summary.png" alt="Diagram of all the above estimated effect sizes" title="Diagram of all the above estimated effect sizes"></p>
<p>(And remember, you can <em>stack</em> any of the above interventions to get an even larger effect! You can't just naively add up the effect sizes, but I'd be surprised if the effect of {vitamin d + omega-3 + bright lamps + cardio + good sleep + meditation} <em>combined</em> ends up being less than +2.00. Two letter grades up means going from D to B, or, theoretically, from clinically depressed to flourishing! For more papers &amp; my working research notes on "best bang for buck on depression", <a href="https://docs.google.com/document/d/1hjNxJmNjoVktrJDFPtFH1hgzKmGpZkRFQ6Vwf0s7c6w/edit?tab=t.0#heading=h.e0blw3xq3cvm">check out this Google Doc</a>.)</p>
<p>Also, remember that all the above estimates are uncertain. And in general, when scientists replicate psychology experiments more rigorously, the effect size usually shrinks by ¬Ω. But, I think the overall <em>qualitative</em> picture is still strong: there <em>exist</em> high bang-for-buck ways to reduce depression, which are <em>at least</em> on par with drugs &amp; therapy (possibly 2x to 4x better), that aren't (yet) common knowledge amongst policymakers &amp; the public. And again, they're dirt cheap with minor-to-no adverse side effects. Moderate chance of a big win, for a known tiny cost. That's a positive "expected value" bet right there.</p>
<p>I got onto this research rabbithole a few months ago while borrowing my housemate's ADHD meds, which I may or may not eventually collect into a "JOYMAXXING" informal meta-meta-analysis. (<a href="https://youtu.be/JOj97Edna7k?si=xAUt3Kp54RyfGX50&amp;t=334">: See me yap about it on video as a cartoon cat</a>.) But for this blog post, I wanted to dive deeper into Vitamin D and Omega-3, since their effect sizes are so huge, <em>and</em> they're insultingly cheap &amp; easy, compared to therapy or regular cardio.</p>
<p>Stay safe this winter, keep away the seasonal depression. Get your supplements, and reach out to your friends &amp; loved ones!</p>
<p>üíñ,<br>
~ Nicky Case</p>

<hr>
<section>
<ol>
<li id="fn1"><p>I made up these ranges by requiring the standard letter grades F,D,C,B,A, to have their centers be -2,-1,0,+1,+2. Then, I made sure all in-between grades like C+ or A‚Äì had equal intervals. Each interval is +/- ‚Öô, or ‚Öì wide:</p>
<ul>
<li>F---: -3.16 to -2.83</li>
<li>F--: -2.82 to -2.50</li>
<li>F‚Äì: -2.49 to -2.17</li>
<li><strong>F: -2.16 to -1.83</strong></li>
<li>F+: -1.82 to -1.50</li>
<li>D‚Äì: -1.49 to -1.17</li>
<li><strong>D: -1.16 to -0.83</strong></li>
<li>D+: -0.82 to -0.50</li>
<li>C‚Äì: -0.49 to -0.17</li>
<li><strong>C: -0.16 to +0.17</strong></li>
<li>C+: +0.18 to +0.50</li>
<li>B‚Äì: +0.51 to +0.83</li>
<li><strong>B: +0.84 to +1.17</strong></li>
<li>B+: +1.18 to +1.50</li>
<li>A‚Äì: +1.51 to +1.83</li>
<li><strong>A: +1.84 to +2.17</strong></li>
<li>A+: +2.18 to +2.50</li>
<li>A++: +2.51 to +2.83</li>
<li>A+++: +2.84 to +3.17</li>
</ul>
 <a href="#fnref1">‚Ü©Ô∏é</a></li>
<li id="fn2"><p><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10039857/">Lee et al 2023</a> reports the following effect sizes. Digital therapy for Insomnia ‚Üí Sleep = 0.76, and Digital therapy for Insomnia ‚Üí Depression = 0.42. Assuming the therapy <em>for insomnia specifically</em> affects depression <em>only</em> through better sleep (Digital therapy for Insomnia ‚Üí Sleep ‚Üí Depression), we can do an <a href="https://cameron.econ.ucdavis.edu/e240a/ch04iv.pdf">"Instrumental Variable"</a> estimate of the effect of Sleep ‚Üí Depression = 0.42 / 0.76 = 0.55. <strong>To be precise: this is saying, if you improve your sleep by 1 standard deviation, on average your depression improves by 0.55 standard deviations.</strong></p>
<p>So: how many standard deviations is going from "moderate insomnia" to "healthy sleep"? The standard measure is the Insomnia Severity Index (ISI), which <a href="https://www.sleepprimarycareresources.org.au/questionnaires/isi">you can take online</a>. A score of 0‚Äì7 means no insomnia, 8‚Äì14 is subclinical insomnia, 15‚Äì21 is clinical insomnia (moderate), 22‚Äì28 is clinical insomnia (severe). Let's be conservative and say we're just going from barely clinical to barely healthy: 15 to 7, or a reduction of 8 points. <a href="https://pubmed.ncbi.nlm.nih.gov/19689221/">Yang et al 2009</a> says a 6-point reduction is 1.5 standard deviations, which means 4 points is 1 standard deviation. So a reduction of 8 points is 2 standard deviations. <strong>So, if you improve your sleep from insomniac to healthy, you improve by at least 8 points, which is 2 standard deviations, so your depression should improve by 2 √ó 0.55 standard deviations, or ~1.10.</strong></p>
<p>Reminder that my estimate is <em>full</em> of assumptions upon assumptions &amp; these error bars will compound. But I'd be surprised if the true causal effect of going from insomniac to healthy sleep isn't <em>at least</em> a "large" +0.8 effect. <a href="#fnref2">‚Ü©Ô∏é</a></p>
</li>
</ol>
</section>


	</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Europe's next-generation weather satellite sends back first images (450 pts)]]></title>
            <link>https://www.esa.int/Applications/Observing_the_Earth/Meteorological_missions/meteosat_third_generation/Europe_s_next-generation_weather_satellite_sends_back_first_images</link>
            <guid>46806773</guid>
            <pubDate>Thu, 29 Jan 2026 07:07:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.esa.int/Applications/Observing_the_Earth/Meteorological_missions/meteosat_third_generation/Europe_s_next-generation_weather_satellite_sends_back_first_images">https://www.esa.int/Applications/Observing_the_Earth/Meteorological_missions/meteosat_third_generation/Europe_s_next-generation_weather_satellite_sends_back_first_images</a>, See on <a href="https://news.ycombinator.com/item?id=46806773">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

<header>
	<span>Applications</span>
	
	<p><span>27/01/2026</span>
				<span><span id="viewcount">19067</span><small> views</small></span>
										<span><span id="ezsr_total_27080629">44</span><small> likes</small></span>
				
	</p>
</header>





<p>The first images from the Meteosat Third Generation-Sounder satellite have been shared at the European Space Conference in Brussels, showing how the mission will provide data on temperature and humidity, for more accurate weather forecasting over Europe and northern Africa.</p>

<div>
				
		<p>The images from Meteosat Third Generation-Sounder (MTG-S) show a full-disc image of Earth as seen from geostationary orbit, about 36 000 km above Earth‚Äôs surface. These images were captured on 15 November 2025 by the satellite‚Äôs Infrared Sounder instrument.</p><p>In the ‚Äòtemperature‚Äô image (below), the Infrared Sounder used a long-wave infrared channel, which measured Earth‚Äôs surface temperature as well as the temperature at the top of clouds. Dark red corresponds to high temperatures, mainly on the warmer land surfaces, while blue corresponds to lower temperatures, typically on the top of clouds.</p><p>As would be expected, most of the warmest (dark red) areas in this image are on the continents of Africa and South America. In the top-centre of the image, the outline of the coast of western Africa is clearly visible in dark red, with the Cape Verde peninsula, home to Senegal‚Äôs capital Dakar, visible as among the warmest areas in this image. In the bottom-right of the image, the western coast of Namibia and South Africa are also visible in red beneath a swirl of cold cloud shown in blue, while the northeast coast of Brazil is visible in dark red on the left of the image.</p>
	</div>
			
    
							
														
																	



							
				
								
						
								
												<div>
				<figure>
					<a href="https://www.esa.int/ESA_Multimedia/Images/2026/01/Global_surface_and_cloud-top_temperatures_by_MTG-Sounder">
				<img src="https://www.esa.int/var/esa/storage/images/esa_multimedia/images/2026/01/global_surface_and_cloud-top_temperatures_by_mtg-sounder/27080438-4-eng-GB/Global_surface_and_cloud-top_temperatures_by_MTG-Sounder_article.jpg" alt="Global surface and cloud-top temperatures by MTG-Sounder ">
			</a>
				<figcaption>
							<a href="https://www.esa.int/ESA_Multimedia/Images/2026/01/Global_surface_and_cloud-top_temperatures_by_MTG-Sounder">Global surface and cloud-top temperatures by MTG-Sounder </a>
								</figcaption>
	</figure>	
		<p>The ‚Äòhumidity‚Äô image (below) was captured using the Infrared Sounder‚Äôs medium-wave infrared channel, which measures humidity in Earth‚Äôs atmosphere. Blue colours correspond to regions in the atmosphere with higher humidity, while red colours correspond to lower humidity in the atmosphere.</p><p>The outlines of landmasses are not visible in this image. The areas of least atmospheric humidity, shown in dark red, are seen approximately over the Sahara Desert and the Middle East (top of image), while a large area of ‚Äòdry‚Äô atmosphere also covers part of the South Atlantic Ocean (centre of image). Numerous patches of high humidity are seen in dark blue over the eastern part of the African continent as well as in high and low latitudes.</p>	</div>	
    
							
														
																	



							
				
								
						
								
												<div>
				<figure>
					<a href="https://www.esa.int/ESA_Multimedia/Images/2026/01/Global_air_humidity_by_MTG-Sounder">
				<img src="https://www.esa.int/var/esa/storage/images/esa_multimedia/images/2026/01/global_air_humidity_by_mtg-sounder/27080485-4-eng-GB/Global_air_humidity_by_MTG-Sounder_article.jpg" alt="Global air humidity by MTG-Sounder ">
			</a>
				<figcaption>
							<a href="https://www.esa.int/ESA_Multimedia/Images/2026/01/Global_air_humidity_by_MTG-Sounder">Global air humidity by MTG-Sounder </a>
								</figcaption>
	</figure>	
		<p>Below we see a close-up from MTG-Sounder of the European continent and part of northern Africa. Like the first image above, here we see heat from land surfaces and temperatures at the top of clouds. The heat from the African continent is seen in red in the lower part of the image, while a dark blue weather front covers Spain and Portugal. The Italian peninsula is in the centre of the image.</p>	</div>	
    
							
														
																	



							
				
								
						
								
									<div>
				<figure>
					<a href="https://www.esa.int/ESA_Multimedia/Images/2026/01/Temperatures_over_Europe_and_northern_Africa_by_MTG-Sounder">
				<img src="https://www.esa.int/var/esa/storage/images/esa_multimedia/images/2026/01/temperatures_over_europe_and_northern_africa_by_mtg-sounder/27082458-1-eng-GB/Temperatures_over_Europe_and_northern_Africa_by_MTG-Sounder_article.jpg" alt="Temperatures over Europe and northern Africa by MTG-Sounder">
			</a>
				<figcaption>
							<a href="https://www.esa.int/ESA_Multimedia/Images/2026/01/Temperatures_over_Europe_and_northern_Africa_by_MTG-Sounder">Temperatures over Europe and northern Africa by MTG-Sounder</a>
								</figcaption>
	</figure>	
		<p>And the animation (below) uses data from the MTG-Sounder satellite to track the eruption of Ethiopia's Hayli Gubbi volcano on 23 November 2025. The background imagery shows surface temperature changes while infrared channels highlight the developing ash plume. The satellite's timely observations enable tracking of the evolving ash plume over time.</p>	</div>	
    
							
														
																								



				
				
								
						
								
						<div>
				<div>
		<a href="https://www.esa.int/ESA_Multimedia/Videos/2026/01/Hayli_Gubbi_eruption_in_Ethiopia_by_MTG-Sounder">
		<div>
				

   <p><img src="https://www.esa.int/extension/pillars/design/pillars/images/play-button.svg" alt="Play"></p>
   <p><img src="https://www.esa.int/var/esa/storage/images/esa_multimedia/videos/2026/01/hayli_gubbi_eruption_in_ethiopia_by_mtg-sounder/27082378-1-eng-GB/Hayli_Gubbi_eruption_in_Ethiopia_by_MTG-Sounder_pillars.png" alt="$video.data_map.short_description.content">

 
		</p></div>
		</a>
				<p>
			Hayli Gubbi eruption in Ethiopia, by MTG-Sounder
			<br><a href="https://www.esa.int/ESA_Multimedia/Videos/2026/01/Hayli_Gubbi_eruption_in_Ethiopia_by_MTG-Sounder">Access the video</a>
		</p>
			</div>	
		<h4>Next-generation weather forecasting</h4><p>MTG is a world-class Earth observation mission developed by the European Space Agency (ESA) with European partners to address scientific and societal challenges. The mission provides game-changing data for forecasting weather and air quality over Europe.</p><p>The satellite‚Äôs geostationary position above the equator means it maintains a fixed position relative to Earth, following the same area on the planet‚Äôs surface as we rotate. This enables it to provide coverage of Europe and part of northern Africa on a 15-minute repeat cycle. It supplies new data on temperature and humidity over Europe every 30 minutes, supplying meteorologists with a complete weather picture of the region and complementing data on cloud formation and lightning from the MTG-Imager (MTG-I) satellite.</p>	</div>	
    
							
														
																	



							
				
								
						
								
												<div>
				<figure>
					<a href="https://www.esa.int/ESA_Multimedia/Images/2025/02/MTG-Sounder_satellite_over_the_equator">
				<img src="https://www.esa.int/var/esa/storage/images/esa_multimedia/images/2025/02/mtg-sounder_satellite_over_the_equator/26586287-1-eng-GB/MTG-Sounder_satellite_over_the_equator_article.jpg" alt="MTG-Sounder satellite over the equator">
			</a>
				<figcaption>
							<a href="https://www.esa.int/ESA_Multimedia/Images/2025/02/MTG-Sounder_satellite_over_the_equator">MTG-Sounder satellite over the equator</a>
								</figcaption>
	</figure>	
		<p>ESA‚Äôs Director of Earth Observation Programmes, Simonetta Cheli, said, ‚ÄúSeeing the first Infrared Sounder images from the MTG-Sounder satellite really brings this mission and its potential to life. We expect data from this mission to change the way we forecast severe storms over Europe ‚Äì and this is very exciting for communities and citizens, as well as for meteorologists and climatologists. As ever, the outstanding work done by our teams in collaboration with long-standing partners, including Eumetsat, the European Commission and dozens of European industry teams, means we now have the ability to predict extreme weather events in more accurate and timely ways than ever before.‚Äù</p><h4>A hyperspectral view over Europe</h4><p>The Infrared Sounder instrument on board MTG-S is the first European hyperspectral sounding instrument in geostationary orbit. It is designed to generate a completely new type of data product. It uses interferometric techniques, which analyse miniscule patterns in light waves, to capture data on temperature and humidity, as well as being able to measure wind and trace gases in the atmosphere. The data will eventually be used to generate three-dimensional maps of the atmosphere, helping to improve the accuracy of weather forecasting, especially for nowcasting rapidly evolving storms.</p><p>‚ÄúIt‚Äôs fantastic to see the first images from this groundbreaking mission,‚Äù said James Champion, ESA‚Äôs MTG Project Manager. ‚ÄúThis satellite has been 15 years in development and will revolutionise weather forecasting and especially nowcasting. The ability to vertically profile the full Earth‚Äôs disk with a repeat cycle of only 30 minutes for Europe is an incredible accomplishment!‚Äù</p>	</div>	
    
							
														
																	



							
				
								
						
								
												<div>
				<figure>
					<a href="https://www.esa.int/ESA_Multimedia/Images/2024/07/MTG-S_patch">
				<img src="https://www.esa.int/var/esa/storage/images/esa_multimedia/images/2024/07/mtg-s_patch/26224166-1-eng-GB/MTG-S_patch_article.png" alt=" MTG-S patch">
			</a>
				<figcaption>
							<a href="https://www.esa.int/ESA_Multimedia/Images/2024/07/MTG-S_patch"> MTG-S patch</a>
								</figcaption>
	</figure>	
		<p>‚ÄúI‚Äôm excited that we can share these first images from the Infrared Sounder, which showcase just a small selection of the 1700 infrared channels continuously acquired by the instrument as it observes Earth,‚Äù said Pieter Van den Braembussche, MTG System and Payload Manager at ESA. ‚ÄúBy combining all 1700 channels, we will soon be able to generate three dimensional maps of temperature, humidity and even trace gases in the atmosphere. This capability will offer a completely new perspective on Earth‚Äôs atmosphere, not previously available in Europe, and is expected to help forecasters predict severe storms earlier than is possible today.‚Äù</p>	</div>	
	<div>
	<h2>About MTG-Sounder</h2>
	    
						
		<p>The MTG mission currently has two satellites in orbit: MTG-I and MTG-S. The second Imager will be launched later in 2026.</p><p>MTG-S was launched on 1 July 2025. Thales Alenia Space is the prime contractor for the overall MTG mission, with OHB Systems responsible for the MTG-Sounder satellite. Mission control and data distribution are managed by Eumetsat.</p><p>The MTG-S satellite also hosts the <a href="https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-4" target="_blank">Copernicus Sentinel-4 mission</a>, which consists of an ultraviolet, visible and near-infrared (UVN) imaging spectrometer. Sentinel-4 delivered <a href="https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-4/Sentinel-4_offers_first_glimpses_of_air_pollutants" target="_blank">its first images</a> last year.</p>	</div>	
    
							
														
																								



				
				
								
						
								
						<div>
		<a href="https://www.esa.int/ESA_Multimedia/Videos/2025/07/MTG-S1_and_Copernicus_Sentinel-4_mission_highlights">
		<div>
				

   <p><img src="https://www.esa.int/extension/pillars/design/pillars/images/play-button.svg" alt="Play"></p>
   <p><img src="https://www.esa.int/var/esa/storage/images/esa_multimedia/videos/2025/07/mtg-s1_and_copernicus_sentinel-4_mission_highlights/26777490-3-eng-GB/MTG-S1_and_Copernicus_Sentinel-4_mission_highlights_pillars.png" alt="$video.data_map.short_description.content">

 
		</p></div>
		</a>
				<p>
			MTG-S1 and Copernicus Sentinel-4 mission highlights 
			<br><a href="https://www.esa.int/ESA_Multimedia/Videos/2025/07/MTG-S1_and_Copernicus_Sentinel-4_mission_highlights">Access the video</a>
		</p>
			</div>	
    
						
    
					



</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We can't send mail farther than 500 miles (2002) (508 pts)]]></title>
            <link>https://web.mit.edu/jemorris/humor/500-miles</link>
            <guid>46805665</guid>
            <pubDate>Thu, 29 Jan 2026 03:58:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://web.mit.edu/jemorris/humor/500-miles">https://web.mit.edu/jemorris/humor/500-miles</a>, See on <a href="https://news.ycombinator.com/item?id=46805665">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Maine‚Äôs ‚ÄòLobster Lady‚Äô who fished for nearly a century dies aged 105 (190 pts)]]></title>
            <link>https://www.theguardian.com/us-news/2026/jan/28/maine-lobster-lady-dies-aged-105</link>
            <guid>46804854</guid>
            <pubDate>Thu, 29 Jan 2026 02:11:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/us-news/2026/jan/28/maine-lobster-lady-dies-aged-105">https://www.theguardian.com/us-news/2026/jan/28/maine-lobster-lady-dies-aged-105</a>, See on <a href="https://news.ycombinator.com/item?id=46804854">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p><a href="https://www.theguardian.com/us-news/maine" data-link-name="in body link">Maine</a>‚Äôs governor has hailed the life of a woman who spent nearly 100 years fishing for lobsters as ‚Äúamazing‚Äù and expressed hopes that her memory inspires ‚Äúthe next century of hardworking‚Äù fishers in the state.</p><p>The subject of Governor Janet Mills‚Äô tribute, Virginia ‚ÄúGinny‚Äù Oliver, died on 21 January at age 105, according to an obituary <a href="https://www.legacy.com/us/obituaries/legacyremembers/virginia-oliver-obituary?id=60635890" data-link-name="in body link">published</a> on Monday by her family.</p><p>Some regard stories such as that of Oliver, who came to be known as her state‚Äôs ‚ÄúLobster Lady‚Äù, as evidence of the growing number of Americans who extend their working days well past the typical retirement age as the cost of living in the US has soared, wages have stagnated and many therefore have been unable to save.</p><p>Nonetheless, as recently as 2021, Oliver told the Associated Press she fell in love with trapping lobsters from the moment she started in the business at eight years old, alongside her father and older brother.</p><p>‚ÄúI like doing it ‚Äì I like being along the water,‚Äù she said when discussing her career in the largely male-dominated industry she chose. ‚ÄúAnd so I‚Äôm going to keep on doing it just as long as I can.‚Äù</p><p>Oliver would get up before dawn and use small fish colloquially known as poagies to lure lobsters from her boat, the Virginia, which was first owned by her late husband. As she established a remarkable 97-year tenure on the waters, and word of it spread, she became the subject of documentaries, major US television networks‚Äô news stories and children‚Äôs books, including one titled The Lobster Lady, her obituary recounted.</p><p><a href="https://www.theguardian.com/film/mark-hamill" data-link-name="in body link">Mark Hamill</a>, the famed actor, was among those who joined the following that Oliver developed throughout the years. Hamill, best known for his role as Luke Skywalker in the Star Wars film saga, ‚Äúcelebrated her tenacity on social media‚Äù, Oliver‚Äôs obituary noted.</p><p>The obituary also said that Oliver at one point earned an honorary invitation to join Great Britain‚Äôs Cardiff Royal Naval Association. Mills once presented Oliver with a special recognition on her birthday.</p><p>‚ÄúDespite her fame, friends and family said she remained humble and spirited,‚Äù Oliver‚Äôs obituary added. ‚ÄúHer personal aesthetic delighted her fans ‚Äì she wore lipstick and earrings every day she went out on the boat, because, as she said, ‚Äòyou never know who you are going to see.‚Äô‚Äù</p><p>Lobster evolved from working-class food to a pricey restaurant delicacy over the course of Oliver‚Äôs fishing life. Its price per pound swelled from 28 cents when she first started to $6.14 ‚Äì or 22 times more expensive.</p><p>Oliver fished for lobster until a fall at age 103, said <a href="https://www.facebook.com/barbarawalsh.author/posts/i-always-teared-up-when-i-hugged-ginny-goodbye-wondering-if-it-would-be-the-last/1658293695150785/" data-link-name="in body link">a statement</a> from her friend, author and Pulitzer prize-winning journalist Barbara Walsh.</p><p>Walsh joined Mills in paying tribute to Oliver, saying the late fisher ‚Äúbelieved in living, laughing and doing what she loved‚Äù.</p><p>‚ÄúShe was sassy and spirited, always declaring on land and at sea, ‚ÄòI‚Äôm the boss,‚Äô‚Äù Walsh‚Äôs tribute statement said. ‚ÄúSail on, sweet Ginny. May your spirit forever soar above the sea.‚Äù</p><p>Meanwhile, the <a href="https://www.theguardian.com/us-news/maine" data-link-name="in body link" data-component="auto-linked-tag">Maine</a> Lobster festival, which once designated her the grand marshal of its parade, issued a statement honoring Oliver as ‚Äúmore than a local icon‚Äù.</p><p>‚ÄúVirginia was ‚Ä¶ a living piece of Maine‚Äôs maritime history,‚Äù the festival‚Äôs statement said.</p><p>Oliver‚Äôs survivors include her children and grandchildren, according to her obituary.</p><p><em>Associated Press contributed reporting</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Render Mermaid diagrams as SVGs or ASCII art (330 pts)]]></title>
            <link>https://github.com/lukilabs/beautiful-mermaid</link>
            <guid>46804828</guid>
            <pubDate>Thu, 29 Jan 2026 02:08:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/lukilabs/beautiful-mermaid">https://github.com/lukilabs/beautiful-mermaid</a>, See on <a href="https://news.ycombinator.com/item?id=46804828">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why We Built This</h2><a id="user-content-why-we-built-this" aria-label="Permalink: Why We Built This" href="#why-we-built-this"></a></p>
<p dir="auto">Diagrams are essential for AI-assisted programming. When you're working with an AI coding assistant, being able to visualize data flows, state machines, and system architecture‚Äîdirectly in your terminal or chat interface‚Äîmakes complex concepts instantly graspable.</p>
<p dir="auto"><a href="https://mermaid.js.org/" rel="nofollow">Mermaid</a> is the de facto standard for text-based diagrams. It's brilliant. But the default renderer has problems:</p>
<ul dir="auto">
<li><strong>Aesthetics</strong> ‚Äî Might be personal preference, but wished they looked more professional</li>
<li><strong>Complex theming</strong> ‚Äî Customizing colors requires wrestling with CSS classes</li>
<li><strong>No terminal output</strong> ‚Äî Can't render to ASCII for CLI tools</li>
<li><strong>Heavy dependencies</strong> ‚Äî Pulls in a lot of code for simple diagrams</li>
</ul>
<p dir="auto">We built <code>beautiful-mermaid</code> at <a href="https://craft.do/" rel="nofollow">Craft</a> to power diagrams in <a href="https://agents.craft.do/" rel="nofollow">Craft Agents</a>. It's fast, beautiful, and works everywhere‚Äîfrom rich UIs to plain terminals.</p>
<p dir="auto">The ASCII rendering engine is based on <a href="https://github.com/AlexanderGrooff/mermaid-ascii">mermaid-ascii</a> by Alexander Grooff. We ported it from Go to TypeScript and extended it Thank you Alexander for the excellent foundation! (And inspiration that this was possible.)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><strong>5 diagram types</strong> ‚Äî Flowcharts, State, Sequence, Class, and ER diagrams</li>
<li><strong>Dual output</strong> ‚Äî SVG for rich UIs, ASCII/Unicode for terminals</li>
<li><strong>15 built-in themes</strong> ‚Äî And dead simple to add your own</li>
<li><strong>Full Shiki compatibility</strong> ‚Äî Use any VS Code theme directly</li>
<li><strong>Live theme switching</strong> ‚Äî CSS custom properties, no re-render needed</li>
<li><strong>Mono mode</strong> ‚Äî Beautiful diagrams from just 2 colors</li>
<li><strong>Zero DOM dependencies</strong> ‚Äî Pure TypeScript, works everywhere</li>
<li><strong>Ultra-fast</strong> ‚Äî Renders 100+ diagrams in under 500ms</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="npm install beautiful-mermaid
# or
bun add beautiful-mermaid
# or
pnpm add beautiful-mermaid"><pre>npm install beautiful-mermaid
<span><span>#</span> or</span>
bun add beautiful-mermaid
<span><span>#</span> or</span>
pnpm add beautiful-mermaid</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">SVG Output</h3><a id="user-content-svg-output" aria-label="Permalink: SVG Output" href="#svg-output"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import { renderMermaid } from 'beautiful-mermaid'

const svg = await renderMermaid(`
  graph TD
    A[Start] --> B{Decision}
    B -->|Yes| C[Action]
    B -->|No| D[End]
`)"><pre><span>import</span> <span>{</span> <span>renderMermaid</span> <span>}</span> <span>from</span> <span>'beautiful-mermaid'</span>

<span>const</span> <span>svg</span> <span>=</span> <span>await</span> <span>renderMermaid</span><span>(</span><span>`</span>
<span>  graph TD</span>
<span>    A[Start] --&gt; B{Decision}</span>
<span>    B --&gt;|Yes| C[Action]</span>
<span>    B --&gt;|No| D[End]</span>
<span>`</span><span>)</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">ASCII Output</h3><a id="user-content-ascii-output" aria-label="Permalink: ASCII Output" href="#ascii-output"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import { renderMermaidAscii } from 'beautiful-mermaid'

const ascii = renderMermaidAscii(`graph LR; A --> B --> C`)"><pre><span>import</span> <span>{</span> <span>renderMermaidAscii</span> <span>}</span> <span>from</span> <span>'beautiful-mermaid'</span>

<span>const</span> <span>ascii</span> <span>=</span> <span>renderMermaidAscii</span><span>(</span><span>`graph LR; A --&gt; B --&gt; C`</span><span>)</span></pre></div>
<div data-snippet-clipboard-copy-content="‚îå‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   ‚îÇ     ‚îÇ   ‚îÇ     ‚îÇ   ‚îÇ
‚îÇ A ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ B ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ C ‚îÇ
‚îÇ   ‚îÇ     ‚îÇ   ‚îÇ     ‚îÇ   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îò"><pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   ‚îÇ     ‚îÇ   ‚îÇ     ‚îÇ   ‚îÇ
‚îÇ A ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ B ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ C ‚îÇ
‚îÇ   ‚îÇ     ‚îÇ   ‚îÇ     ‚îÇ   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îò
</code></pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Theming</h2><a id="user-content-theming" aria-label="Permalink: Theming" href="#theming"></a></p>
<p dir="auto">The theming system is the heart of <code>beautiful-mermaid</code>. It's designed to be both powerful and dead simple.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Two-Color Foundation</h3><a id="user-content-the-two-color-foundation" aria-label="Permalink: The Two-Color Foundation" href="#the-two-color-foundation"></a></p>
<p dir="auto">Every diagram needs just two colors: <strong>background</strong> (<code>bg</code>) and <strong>foreground</strong> (<code>fg</code>). That's it. From these two colors, the entire diagram is derived using <code>color-mix()</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const svg = await renderMermaid(diagram, {
  bg: '#1a1b26',  // Background
  fg: '#a9b1d6',  // Foreground
})"><pre><span>const</span> <span>svg</span> <span>=</span> <span>await</span> <span>renderMermaid</span><span>(</span><span>diagram</span><span>,</span> <span>{</span>
  <span>bg</span>: <span>'#1a1b26'</span><span>,</span>  <span>// Background</span>
  <span>fg</span>: <span>'#a9b1d6'</span><span>,</span>  <span>// Foreground</span>
<span>}</span><span>)</span></pre></div>
<p dir="auto">This is <strong>Mono Mode</strong>‚Äîa coherent, beautiful diagram from just two colors. The system automatically derives:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Element</th>
<th>Derivation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Text</td>
<td><code>--fg</code> at 100%</td>
</tr>
<tr>
<td>Secondary text</td>
<td><code>--fg</code> at 60% into <code>--bg</code></td>
</tr>
<tr>
<td>Edge labels</td>
<td><code>--fg</code> at 40% into <code>--bg</code></td>
</tr>
<tr>
<td>Connectors</td>
<td><code>--fg</code> at 30% into <code>--bg</code></td>
</tr>
<tr>
<td>Arrow heads</td>
<td><code>--fg</code> at 50% into <code>--bg</code></td>
</tr>
<tr>
<td>Node fill</td>
<td><code>--fg</code> at 3% into <code>--bg</code></td>
</tr>
<tr>
<td>Node stroke</td>
<td><code>--fg</code> at 20% into <code>--bg</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Enriched Mode</h3><a id="user-content-enriched-mode" aria-label="Permalink: Enriched Mode" href="#enriched-mode"></a></p>
<p dir="auto">For richer themes, you can provide optional "enrichment" colors that override specific derivations:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const svg = await renderMermaid(diagram, {
  bg: '#1a1b26',
  fg: '#a9b1d6',
  // Optional enrichment:
  line: '#3d59a1',    // Edge/connector color
  accent: '#7aa2f7',  // Arrow heads, highlights
  muted: '#565f89',   // Secondary text, labels
  surface: '#292e42', // Node fill tint
  border: '#3d59a1',  // Node stroke
})"><pre><span>const</span> <span>svg</span> <span>=</span> <span>await</span> <span>renderMermaid</span><span>(</span><span>diagram</span><span>,</span> <span>{</span>
  <span>bg</span>: <span>'#1a1b26'</span><span>,</span>
  <span>fg</span>: <span>'#a9b1d6'</span><span>,</span>
  <span>// Optional enrichment:</span>
  <span>line</span>: <span>'#3d59a1'</span><span>,</span>    <span>// Edge/connector color</span>
  <span>accent</span>: <span>'#7aa2f7'</span><span>,</span>  <span>// Arrow heads, highlights</span>
  <span>muted</span>: <span>'#565f89'</span><span>,</span>   <span>// Secondary text, labels</span>
  <span>surface</span>: <span>'#292e42'</span><span>,</span> <span>// Node fill tint</span>
  <span>border</span>: <span>'#3d59a1'</span><span>,</span>  <span>// Node stroke</span>
<span>}</span><span>)</span></pre></div>
<p dir="auto">If an enrichment color isn't provided, it falls back to the <code>color-mix()</code> derivation. This means you can provide just the colors you care about.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">CSS Custom Properties = Live Switching</h3><a id="user-content-css-custom-properties--live-switching" aria-label="Permalink: CSS Custom Properties = Live Switching" href="#css-custom-properties--live-switching"></a></p>
<p dir="auto">All colors are CSS custom properties on the <code>&lt;svg&gt;</code> element. This means you can switch themes instantly without re-rendering:</p>
<div dir="auto" data-snippet-clipboard-copy-content="// Switch theme by updating CSS variables
svg.style.setProperty('--bg', '#282a36')
svg.style.setProperty('--fg', '#f8f8f2')
// The entire diagram updates immediately"><pre><span>// Switch theme by updating CSS variables</span>
<span>svg</span><span>.</span><span>style</span><span>.</span><span>setProperty</span><span>(</span><span>'--bg'</span><span>,</span> <span>'#282a36'</span><span>)</span>
<span>svg</span><span>.</span><span>style</span><span>.</span><span>setProperty</span><span>(</span><span>'--fg'</span><span>,</span> <span>'#f8f8f2'</span><span>)</span>
<span>// The entire diagram updates immediately</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Built-in Themes</h3><a id="user-content-built-in-themes" aria-label="Permalink: Built-in Themes" href="#built-in-themes"></a></p>
<p dir="auto">15 carefully curated themes ship out of the box:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Theme</th>
<th>Type</th>
<th>Background</th>
<th>Accent</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>zinc-light</code></td>
<td>Light</td>
<td><code>#FFFFFF</code></td>
<td>Derived</td>
</tr>
<tr>
<td><code>zinc-dark</code></td>
<td>Dark</td>
<td><code>#18181B</code></td>
<td>Derived</td>
</tr>
<tr>
<td><code>tokyo-night</code></td>
<td>Dark</td>
<td><code>#1a1b26</code></td>
<td><code>#7aa2f7</code></td>
</tr>
<tr>
<td><code>tokyo-night-storm</code></td>
<td>Dark</td>
<td><code>#24283b</code></td>
<td><code>#7aa2f7</code></td>
</tr>
<tr>
<td><code>tokyo-night-light</code></td>
<td>Light</td>
<td><code>#d5d6db</code></td>
<td><code>#34548a</code></td>
</tr>
<tr>
<td><code>catppuccin-mocha</code></td>
<td>Dark</td>
<td><code>#1e1e2e</code></td>
<td><code>#cba6f7</code></td>
</tr>
<tr>
<td><code>catppuccin-latte</code></td>
<td>Light</td>
<td><code>#eff1f5</code></td>
<td><code>#8839ef</code></td>
</tr>
<tr>
<td><code>nord</code></td>
<td>Dark</td>
<td><code>#2e3440</code></td>
<td><code>#88c0d0</code></td>
</tr>
<tr>
<td><code>nord-light</code></td>
<td>Light</td>
<td><code>#eceff4</code></td>
<td><code>#5e81ac</code></td>
</tr>
<tr>
<td><code>dracula</code></td>
<td>Dark</td>
<td><code>#282a36</code></td>
<td><code>#bd93f9</code></td>
</tr>
<tr>
<td><code>github-light</code></td>
<td>Light</td>
<td><code>#ffffff</code></td>
<td><code>#0969da</code></td>
</tr>
<tr>
<td><code>github-dark</code></td>
<td>Dark</td>
<td><code>#0d1117</code></td>
<td><code>#4493f8</code></td>
</tr>
<tr>
<td><code>solarized-light</code></td>
<td>Light</td>
<td><code>#fdf6e3</code></td>
<td><code>#268bd2</code></td>
</tr>
<tr>
<td><code>solarized-dark</code></td>
<td>Dark</td>
<td><code>#002b36</code></td>
<td><code>#268bd2</code></td>
</tr>
<tr>
<td><code>one-dark</code></td>
<td>Dark</td>
<td><code>#282c34</code></td>
<td><code>#c678dd</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<div dir="auto" data-snippet-clipboard-copy-content="import { renderMermaid, THEMES } from 'beautiful-mermaid'

const svg = await renderMermaid(diagram, THEMES['tokyo-night'])"><pre><span>import</span> <span>{</span> <span>renderMermaid</span><span>,</span> <span>THEMES</span> <span>}</span> <span>from</span> <span>'beautiful-mermaid'</span>

<span>const</span> <span>svg</span> <span>=</span> <span>await</span> <span>renderMermaid</span><span>(</span><span>diagram</span><span>,</span> <span>THEMES</span><span>[</span><span>'tokyo-night'</span><span>]</span><span>)</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Adding Your Own Theme</h3><a id="user-content-adding-your-own-theme" aria-label="Permalink: Adding Your Own Theme" href="#adding-your-own-theme"></a></p>
<p dir="auto">Creating a theme is trivial. At minimum, just provide <code>bg</code> and <code>fg</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const myTheme = {
  bg: '#0f0f0f',
  fg: '#e0e0e0',
}

const svg = await renderMermaid(diagram, myTheme)"><pre><span>const</span> <span>myTheme</span> <span>=</span> <span>{</span>
  <span>bg</span>: <span>'#0f0f0f'</span><span>,</span>
  <span>fg</span>: <span>'#e0e0e0'</span><span>,</span>
<span>}</span>

<span>const</span> <span>svg</span> <span>=</span> <span>await</span> <span>renderMermaid</span><span>(</span><span>diagram</span><span>,</span> <span>myTheme</span><span>)</span></pre></div>
<p dir="auto">Want richer colors? Add any of the optional enrichments:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const myRichTheme = {
  bg: '#0f0f0f',
  fg: '#e0e0e0',
  accent: '#ff6b6b',  // Pop of color for arrows
  muted: '#666666',   // Subdued labels
}"><pre><span>const</span> <span>myRichTheme</span> <span>=</span> <span>{</span>
  <span>bg</span>: <span>'#0f0f0f'</span><span>,</span>
  <span>fg</span>: <span>'#e0e0e0'</span><span>,</span>
  <span>accent</span>: <span>'#ff6b6b'</span><span>,</span>  <span>// Pop of color for arrows</span>
  <span>muted</span>: <span>'#666666'</span><span>,</span>   <span>// Subdued labels</span>
<span>}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Full Shiki Compatibility</h3><a id="user-content-full-shiki-compatibility" aria-label="Permalink: Full Shiki Compatibility" href="#full-shiki-compatibility"></a></p>
<p dir="auto">Use <strong>any VS Code theme</strong> directly via Shiki integration. This gives you access to hundreds of community themes:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import { getSingletonHighlighter } from 'shiki'
import { renderMermaid, fromShikiTheme } from 'beautiful-mermaid'

// Load any theme from Shiki's registry
const highlighter = await getSingletonHighlighter({
  themes: ['vitesse-dark', 'rose-pine', 'material-theme-darker']
})

// Extract diagram colors from the theme
const colors = fromShikiTheme(highlighter.getTheme('vitesse-dark'))

const svg = await renderMermaid(diagram, colors)"><pre><span>import</span> <span>{</span> <span>getSingletonHighlighter</span> <span>}</span> <span>from</span> <span>'shiki'</span>
<span>import</span> <span>{</span> <span>renderMermaid</span><span>,</span> <span>fromShikiTheme</span> <span>}</span> <span>from</span> <span>'beautiful-mermaid'</span>

<span>// Load any theme from Shiki's registry</span>
<span>const</span> <span>highlighter</span> <span>=</span> <span>await</span> <span>getSingletonHighlighter</span><span>(</span><span>{</span>
  <span>themes</span>: <span>[</span><span>'vitesse-dark'</span><span>,</span> <span>'rose-pine'</span><span>,</span> <span>'material-theme-darker'</span><span>]</span>
<span>}</span><span>)</span>

<span>// Extract diagram colors from the theme</span>
<span>const</span> <span>colors</span> <span>=</span> <span>fromShikiTheme</span><span>(</span><span>highlighter</span><span>.</span><span>getTheme</span><span>(</span><span>'vitesse-dark'</span><span>)</span><span>)</span>

<span>const</span> <span>svg</span> <span>=</span> <span>await</span> <span>renderMermaid</span><span>(</span><span>diagram</span><span>,</span> <span>colors</span><span>)</span></pre></div>
<p dir="auto">The <code>fromShikiTheme()</code> function intelligently maps VS Code editor colors to diagram roles:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Editor Color</th>
<th>Diagram Role</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>editor.background</code></td>
<td><code>bg</code></td>
</tr>
<tr>
<td><code>editor.foreground</code></td>
<td><code>fg</code></td>
</tr>
<tr>
<td><code>editorLineNumber.foreground</code></td>
<td><code>line</code></td>
</tr>
<tr>
<td><code>focusBorder</code> / keyword token</td>
<td><code>accent</code></td>
</tr>
<tr>
<td>comment token</td>
<td><code>muted</code></td>
</tr>
<tr>
<td><code>editor.selectionBackground</code></td>
<td><code>surface</code></td>
</tr>
<tr>
<td><code>editorWidget.border</code></td>
<td><code>border</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported Diagrams</h2><a id="user-content-supported-diagrams" aria-label="Permalink: Supported Diagrams" href="#supported-diagrams"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Flowcharts</h3><a id="user-content-flowcharts" aria-label="Permalink: Flowcharts" href="#flowcharts"></a></p>
<div data-snippet-clipboard-copy-content="graph TD
  A[Start] --> B{Decision}
  B -->|Yes| C[Process]
  B -->|No| D[End]
  C --> D"><pre><code>graph TD
  A[Start] --&gt; B{Decision}
  B --&gt;|Yes| C[Process]
  B --&gt;|No| D[End]
  C --&gt; D
</code></pre></div>
<p dir="auto">All directions supported: <code>TD</code> (top-down), <code>LR</code> (left-right), <code>BT</code> (bottom-top), <code>RL</code> (right-left).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">State Diagrams</h3><a id="user-content-state-diagrams" aria-label="Permalink: State Diagrams" href="#state-diagrams"></a></p>
<div data-snippet-clipboard-copy-content="stateDiagram-v2
  [*] --> Idle
  Idle --> Processing: start
  Processing --> Complete: done
  Complete --> [*]"><pre><code>stateDiagram-v2
  [*] --&gt; Idle
  Idle --&gt; Processing: start
  Processing --&gt; Complete: done
  Complete --&gt; [*]
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Sequence Diagrams</h3><a id="user-content-sequence-diagrams" aria-label="Permalink: Sequence Diagrams" href="#sequence-diagrams"></a></p>
<div data-snippet-clipboard-copy-content="sequenceDiagram
  Alice->>Bob: Hello Bob!
  Bob-->>Alice: Hi Alice!
  Alice->>Bob: How are you?
  Bob-->>Alice: Great, thanks!"><pre><code>sequenceDiagram
  Alice-&gt;&gt;Bob: Hello Bob!
  Bob--&gt;&gt;Alice: Hi Alice!
  Alice-&gt;&gt;Bob: How are you?
  Bob--&gt;&gt;Alice: Great, thanks!
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Class Diagrams</h3><a id="user-content-class-diagrams" aria-label="Permalink: Class Diagrams" href="#class-diagrams"></a></p>
<div data-snippet-clipboard-copy-content="classDiagram
  Animal <|-- Duck
  Animal <|-- Fish
  Animal: +int age
  Animal: +String gender
  Animal: +isMammal() bool
  Duck: +String beakColor
  Duck: +swim()
  Duck: +quack()"><pre><code>classDiagram
  Animal &lt;|-- Duck
  Animal &lt;|-- Fish
  Animal: +int age
  Animal: +String gender
  Animal: +isMammal() bool
  Duck: +String beakColor
  Duck: +swim()
  Duck: +quack()
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">ER Diagrams</h3><a id="user-content-er-diagrams" aria-label="Permalink: ER Diagrams" href="#er-diagrams"></a></p>
<div data-snippet-clipboard-copy-content="erDiagram
  CUSTOMER ||--o{ ORDER : places
  ORDER ||--|{ LINE_ITEM : contains
  PRODUCT ||--o{ LINE_ITEM : &quot;is in&quot;"><pre><code>erDiagram
  CUSTOMER ||--o{ ORDER : places
  ORDER ||--|{ LINE_ITEM : contains
  PRODUCT ||--o{ LINE_ITEM : "is in"
</code></pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">ASCII Output</h2><a id="user-content-ascii-output-1" aria-label="Permalink: ASCII Output" href="#ascii-output-1"></a></p>
<p dir="auto">For terminal environments, CLI tools, or anywhere you need plain text, render to ASCII or Unicode box-drawing characters:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import { renderMermaidAscii } from 'beautiful-mermaid'

// Unicode mode (default) ‚Äî prettier box drawing
const unicode = renderMermaidAscii(`graph LR; A --> B`)

// Pure ASCII mode ‚Äî maximum compatibility
const ascii = renderMermaidAscii(`graph LR; A --> B`, { useAscii: true })"><pre><span>import</span> <span>{</span> <span>renderMermaidAscii</span> <span>}</span> <span>from</span> <span>'beautiful-mermaid'</span>

<span>// Unicode mode (default) ‚Äî prettier box drawing</span>
<span>const</span> <span>unicode</span> <span>=</span> <span>renderMermaidAscii</span><span>(</span><span>`graph LR; A --&gt; B`</span><span>)</span>

<span>// Pure ASCII mode ‚Äî maximum compatibility</span>
<span>const</span> <span>ascii</span> <span>=</span> <span>renderMermaidAscii</span><span>(</span><span>`graph LR; A --&gt; B`</span><span>,</span> <span>{</span> <span>useAscii</span>: <span>true</span> <span>}</span><span>)</span></pre></div>
<p dir="auto"><strong>Unicode output:</strong></p>
<div data-snippet-clipboard-copy-content="‚îå‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   ‚îÇ     ‚îÇ   ‚îÇ
‚îÇ A ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ B ‚îÇ
‚îÇ   ‚îÇ     ‚îÇ   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îò"><pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   ‚îÇ     ‚îÇ   ‚îÇ
‚îÇ A ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ B ‚îÇ
‚îÇ   ‚îÇ     ‚îÇ   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îò
</code></pre></div>
<p dir="auto"><strong>ASCII output:</strong></p>
<div data-snippet-clipboard-copy-content="+---+     +---+
|   |     |   |
| A |---->| B |
|   |     |   |
+---+     +---+"><pre><code>+---+     +---+
|   |     |   |
| A |----&gt;| B |
|   |     |   |
+---+     +---+
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">ASCII Options</h3><a id="user-content-ascii-options" aria-label="Permalink: ASCII Options" href="#ascii-options"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="renderMermaidAscii(diagram, {
  useAscii: false,      // true = ASCII, false = Unicode (default)
  paddingX: 5,          // Horizontal spacing between nodes
  paddingY: 5,          // Vertical spacing between nodes
  boxBorderPadding: 1,  // Padding inside node boxes
})"><pre><span>renderMermaidAscii</span><span>(</span><span>diagram</span><span>,</span> <span>{</span>
  <span>useAscii</span>: <span>false</span><span>,</span>      <span>// true = ASCII, false = Unicode (default)</span>
  <span>paddingX</span>: <span>5</span><span>,</span>          <span>// Horizontal spacing between nodes</span>
  <span>paddingY</span>: <span>5</span><span>,</span>          <span>// Vertical spacing between nodes</span>
  <span>boxBorderPadding</span>: <span>1</span><span>,</span>  <span>// Padding inside node boxes</span>
<span>}</span><span>)</span></pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">API Reference</h2><a id="user-content-api-reference" aria-label="Permalink: API Reference" href="#api-reference"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>renderMermaid(text, options?): Promise&lt;string&gt;</code></h3><a id="user-content-rendermermaidtext-options-promisestring" aria-label="Permalink: renderMermaid(text, options?): Promise<string>" href="#rendermermaidtext-options-promisestring"></a></p>
<p dir="auto">Render a Mermaid diagram to SVG. Auto-detects diagram type.</p>
<p dir="auto"><strong>Parameters:</strong></p>
<ul dir="auto">
<li><code>text</code> ‚Äî Mermaid source code</li>
<li><code>options</code> ‚Äî Optional <code>RenderOptions</code> object</li>
</ul>
<p dir="auto"><strong>RenderOptions:</strong></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Option</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>bg</code></td>
<td><code>string</code></td>
<td><code>#FFFFFF</code></td>
<td>Background color</td>
</tr>
<tr>
<td><code>fg</code></td>
<td><code>string</code></td>
<td><code>#27272A</code></td>
<td>Foreground color</td>
</tr>
<tr>
<td><code>line</code></td>
<td><code>string?</code></td>
<td>‚Äî</td>
<td>Edge/connector color</td>
</tr>
<tr>
<td><code>accent</code></td>
<td><code>string?</code></td>
<td>‚Äî</td>
<td>Arrow heads, highlights</td>
</tr>
<tr>
<td><code>muted</code></td>
<td><code>string?</code></td>
<td>‚Äî</td>
<td>Secondary text, labels</td>
</tr>
<tr>
<td><code>surface</code></td>
<td><code>string?</code></td>
<td>‚Äî</td>
<td>Node fill tint</td>
</tr>
<tr>
<td><code>border</code></td>
<td><code>string?</code></td>
<td>‚Äî</td>
<td>Node stroke color</td>
</tr>
<tr>
<td><code>font</code></td>
<td><code>string</code></td>
<td><code>Inter</code></td>
<td>Font family</td>
</tr>
<tr>
<td><code>transparent</code></td>
<td><code>boolean</code></td>
<td><code>false</code></td>
<td>Render with transparent background</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>renderMermaidAscii(text, options?): string</code></h3><a id="user-content-rendermermaidasciitext-options-string" aria-label="Permalink: renderMermaidAscii(text, options?): string" href="#rendermermaidasciitext-options-string"></a></p>
<p dir="auto">Render a Mermaid diagram to ASCII/Unicode text. Synchronous.</p>
<p dir="auto"><strong>AsciiRenderOptions:</strong></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Option</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>useAscii</code></td>
<td><code>boolean</code></td>
<td><code>false</code></td>
<td>Use ASCII instead of Unicode</td>
</tr>
<tr>
<td><code>paddingX</code></td>
<td><code>number</code></td>
<td><code>5</code></td>
<td>Horizontal node spacing</td>
</tr>
<tr>
<td><code>paddingY</code></td>
<td><code>number</code></td>
<td><code>5</code></td>
<td>Vertical node spacing</td>
</tr>
<tr>
<td><code>boxBorderPadding</code></td>
<td><code>number</code></td>
<td><code>1</code></td>
<td>Inner box padding</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>fromShikiTheme(theme): DiagramColors</code></h3><a id="user-content-fromshikithemetheme-diagramcolors" aria-label="Permalink: fromShikiTheme(theme): DiagramColors" href="#fromshikithemetheme-diagramcolors"></a></p>
<p dir="auto">Extract diagram colors from a Shiki theme object.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>THEMES: Record&lt;string, DiagramColors&gt;</code></h3><a id="user-content-themes-recordstring-diagramcolors" aria-label="Permalink: THEMES: Record<string, DiagramColors>" href="#themes-recordstring-diagramcolors"></a></p>
<p dir="auto">Object containing all 15 built-in themes.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>DEFAULTS: { bg: string, fg: string }</code></h3><a id="user-content-defaults--bg-string-fg-string-" aria-label="Permalink: DEFAULTS: { bg: string, fg: string }" href="#defaults--bg-string-fg-string-"></a></p>
<p dir="auto">Default colors (<code>#FFFFFF</code> / <code>#27272A</code>).</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Attribution</h2><a id="user-content-attribution" aria-label="Permalink: Attribution" href="#attribution"></a></p>
<p dir="auto">The ASCII rendering engine is based on <a href="https://github.com/AlexanderGrooff/mermaid-ascii">mermaid-ascii</a> by Alexander Grooff. We ported it from Go to TypeScript and extended it with:</p>
<ul dir="auto">
<li>Sequence diagram support</li>
<li>Class diagram support</li>
<li>ER diagram support</li>
<li>Unicode box-drawing characters</li>
<li>Configurable spacing and padding</li>
</ul>
<p dir="auto">Thank you Alexander for the excellent foundation!</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">MIT ‚Äî see <a href="https://github.com/lukilabs/beautiful-mermaid/blob/main/LICENSE">LICENSE</a> for details.</p>
<hr>
<p dir="auto">Built with care by the team at <a href="https://craft.do/" rel="nofollow">Craft</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ross Stevens Donates $100M to Pay Every US Olympian and Paralympian $200k (172 pts)]]></title>
            <link>https://www.townandcountrymag.com/leisure/sporting/a70171886/ross-stevens-american-olympians-donation/</link>
            <guid>46803549</guid>
            <pubDate>Wed, 28 Jan 2026 23:55:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.townandcountrymag.com/leisure/sporting/a70171886/ross-stevens-american-olympians-donation/">https://www.townandcountrymag.com/leisure/sporting/a70171886/ross-stevens-american-olympians-donation/</a>, See on <a href="https://news.ycombinator.com/item?id=46803549">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-journey-body="standard-article"><p data-journey-content="true" data-node-id="0">How much <a href="https://www.townandcountrymag.com/leisure/sporting/a61804802/olympic-medalists-pay-explained/" target="_blank" data-vars-ga-outbound-link="https://www.townandcountrymag.com/leisure/sporting/a61804802/olympic-medalists-pay-explained/" data-vars-ga-ux-element="Hyperlink" data-vars-ga-call-to-action="Olympians get paid" data-node-id="0.1">Olympians get paid</a> depends entirely on the country they compete for. The International Olympic Committee (IOC) does not give out cash prizes, only a medal, but many countries do award their athletes based on whether or not they won a gold, silver, or bronze. However, if an American athlete competes (and medals) at the Olympics, the U.S. government does not pay them anything.</p><p data-journey-content="true" data-node-id="1">Billionaire financier Ross Stevens is changing that. Starting with the upcoming Milan Cortina Olympics, he will give $200,000 to every U.S. Olympic and Paralympic athlete‚Äîeven if they don‚Äôt win a medal. Per the <a href="https://www.wsj.com/sports/olympics/team-usa-milan-cortina-e131245d?st=Hhm2s6&amp;reflink=desktopwebshare_permalink" target="_blank" data-vars-ga-outbound-link="https://www.wsj.com/sports/olympics/team-usa-milan-cortina-e131245d?st=Hhm2s6&amp;reflink=desktopwebshare_permalink" data-vars-ga-ux-element="Hyperlink" data-vars-ga-call-to-action="Wall Street Journal" data-node-id="1.1"><em data-node-id="1.1.0">Wall Street Journal</em></a>, ‚ÄúHalf will come 20 years after their first qualifying Olympic appearance or at age 45, whichever comes later. Another $100,000 will be in the form of a guaranteed benefit for their families after they pass away.‚Äù</p><p data-journey-content="true" data-node-id="2">His entire donation to the U.S. Olympic &amp; Paralympic Committee (USOPC), <a href="https://www.usopc.org/news/2025/march/04/united-states-olympic-paralympic-committee-and-united-states-olympic-paralympic-foundation-announce-100-million-gift-to-support-post-games-financial-security-for-team-usa-athletes" target="_blank" data-vars-ga-outbound-link="https://www.usopc.org/news/2025/march/04/united-states-olympic-paralympic-committee-and-united-states-olympic-paralympic-foundation-announce-100-million-gift-to-support-post-games-financial-security-for-team-usa-athletes" data-vars-ga-ux-element="Hyperlink" data-vars-ga-call-to-action="announced last March" data-node-id="2.1">announced last March</a>, is $100 million‚Äîa record breaking gift to the organization. ‚ÄúI do not believe that financial insecurity should stop our nation‚Äôs elite athletes from breaking through to new frontiers of excellence,‚Äù Stevens said upon the announcement of his gift.</p><p data-journey-content="true" data-node-id="4">This won‚Äôt be the only source of income for American Olympic and Paralympic athletes; many receive money from endorsement and sponsorship deals, and the national governing body of each sport financially supports their athletes differently. However, Stevens‚Äô gift looks to their retirement.</p><p data-journey-content="true" data-node-id="6">Called the Stevens Financial Security Awards (Stevens Awards), the gift is intended to provide financial support to those who represent the U.S. on the world stage. ‚ÄúIn the heart of every Team USA athlete lies a story of dedication, sacrifice and triumph,‚Äù said USOPC Chair Gene Sykes. ‚ÄúThese extraordinary individuals have committed their lives to their sport, often at the expense of traditional career paths and financial savings. As they approach the end of their competitive journeys‚Äîoften as young as 25 or 30‚Äîmany face a daunting reality: the lack of financial savings to support them and their loved ones in their post-athletic life.‚Äù</p><p data-journey-content="true" data-node-id="7">He added, ‚ÄúBecause of Ross‚Äô extraordinary generosity and philanthropic creativity, we can create more than a financial safety net‚Äî we can build a springboard that will propel these athletes to even greater heights beyond their Olympic and Paralympic careers.‚Äù</p><div data-journey-blur="partial" data-ad-exclude="true"><p><span><img src="https://hips.hearstapps.com/rover/profile_photos/121b4598-31f9-4716-af00-a68368fb7913_1643729378.file?fill=1:1&amp;resize=120:*" alt="Headshot of Emily Burack" title="Headshot of Emily Burack" width="100%" height="100%" decoding="async" loading="lazy"></span></p><div><p>Emily Burack (she/her) is the Senior News Editor for Town &amp; Country, where she covers entertainment, celebrities, the royals, and a wide range of other topics. Before joining T&amp;C, she was the deputy managing editor at Hey Alma, a Jewish culture site. Follow her @emburack on Twitter and Instagram.</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Please Don't Say Mean Things about the AI I Just Invested a Billion Dollars In (601 pts)]]></title>
            <link>https://www.mcsweeneys.net/articles/please-dont-say-mean-things-about-the-ai-that-i-just-invested-a-billion-dollars-in</link>
            <guid>46803356</guid>
            <pubDate>Wed, 28 Jan 2026 23:36:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mcsweeneys.net/articles/please-dont-say-mean-things-about-the-ai-that-i-just-invested-a-billion-dollars-in">https://www.mcsweeneys.net/articles/please-dont-say-mean-things-about-the-ai-that-i-just-invested-a-billion-dollars-in</a>, See on <a href="https://news.ycombinator.com/item?id=46803356">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p><i>‚Äú[Nvidia <span>CEO</span>] Jensen Huang Is Begging You to Stop Being So Negative About AI‚Äù ‚Äî <a href="https://gizmodo.com/jensen-huang-is-begging-you-to-stop-being-so-negative-about-ai-2000709335">Headline from Gizmodo</a></i></p>
<p>- - </p><p>‚Äî</p>
<p>Guys, enough is enough. Bullying is a serious issue, and it‚Äôs time for me to speak out. There‚Äôs an extremely hurtful narrative going around that my product, a revolutionary new technology that exists to scam the elderly and make you distrust anything you see online, is harmful to society. This slander is totally unwarranted, and I would really appreciate it if everyone would stop being so mean about this thing I just invested a billion dollars in.</p>
<p>As someone who desperately needs this technology to work out, I can honestly say it is the most essential tool ever created in all of human history. Don‚Äôt mercilessly ridicule it just because it steals the joy out of your hobbies and creates sexually explicit images of women without their consent. Seriously, please stop! It really hurts my feelings.</p>
<p>It‚Äôs easy to throw stones if you think about the job displacement and ecological destruction caused by this pointless technology. But such black-and-white, not-wanting-billionaires-to-get-richer thinking is, quite frankly, cruel. You can‚Äôt just measure the value of something in terms of ‚Äúwhether or not it makes everything worse for everyone.‚Äù The world is much more complicated than that.</p>
<p>This technology is going to fuel innovation across industries and solve all problems of feminism and equal rights. Yes, it‚Äôs expanding the surveillance state, and yes, it‚Äôs destroying the education system, and yes, it‚Äôs being trained on copyrighted work without permission, and yes, it‚Äôs being used to create lethal autonomous weapons systems that can identify, target, and kill without human input, but‚Ä¶ I forget my point, but ultimately, I think you should embrace it.</p>
<p>Lately, I feel like I just can‚Äôt win with you guys. Please, just use my evil technology. What‚Äôs so wrong with that? Just use it. I‚Äôm begging you. I want to continue living my immoral technofascist life without any criticism.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[In 6 violent encounters, evidence contradicts immigration officials' narratives (206 pts)]]></title>
            <link>https://www.reuters.com/world/us/evidence-contradicts-trump-immigration-officials-accounts-violent-encounters-2026-01-27/</link>
            <guid>46803229</guid>
            <pubDate>Wed, 28 Jan 2026 23:25:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/world/us/evidence-contradicts-trump-immigration-officials-accounts-violent-encounters-2026-01-27/">https://www.reuters.com/world/us/evidence-contradicts-trump-immigration-officials-accounts-violent-encounters-2026-01-27/</a>, See on <a href="https://news.ycombinator.com/item?id=46803229">Hacker News</a></p>
Couldn't get https://www.reuters.com/world/us/evidence-contradicts-trump-immigration-officials-accounts-violent-encounters-2026-01-27/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[The UK paid ¬£4.1M for a bookmarks site (374 pts)]]></title>
            <link>https://mahadk.com/posts/ai-skills-hub</link>
            <guid>46803119</guid>
            <pubDate>Wed, 28 Jan 2026 23:16:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mahadk.com/posts/ai-skills-hub">https://mahadk.com/posts/ai-skills-hub</a>, See on <a href="https://news.ycombinator.com/item?id=46803119">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>  <p>The UK Government recently unveiled its <a href="https://www.gov.uk/government/news/free-ai-training-for-all-as-government-and-industry-programme-expands-to-provide-10-million-workers-with-key-ai-skills-by-2030">‚ÄòAI Skills Hub‚Äô</a>, which wants to provide 10 million workers with AI skills by 2030. The main site was delivered by PwC for the low, low price of.. <a href="https://www.find-tender.service.gov.uk/Notice/001337-2025">¬£4.1 million (~$5,657,000).</a></p>
<p><img src="https://cdn.mahadk.com/s/v3/fa4d19cdf4b054d4_image.png" alt="Image of the procurement contract"></p>
<p>It is <em>not</em> good. Like, at all - the UI is insanely bad and it‚Äôs clear that this was just a vibecoded site (to be fair, this is the <em>AI</em> Skills Hub, but c‚Äômon, where is the pride in your work? I would be ashamed to even release this as a prototype!)</p>
<p><img src="https://cdn.mahadk.com/s/v3/fa0761d06de95286_image.png" alt="Image of the AI Skills Hub"></p>
<p>PwC didn‚Äôt even write any of the course content! The only thing the Skills Hub does is link out to external pages, like Salesforce‚Äôs free Trailhead learning platform:</p>
<p><img src="https://cdn.mahadk.com/s/v3/9aed674092d49d24_image.png" alt="Screenshot of Salesforce's Trailhead learning platform"></p>
<p>Note that I‚Äôm fairly certain this course already existed before the contract was even awarded, so all the site does is.. link out to other sites?</p>
<p>PwC itself also admits that the site does not properly meet accessibility standards:</p>
<p><img src="https://cdn.mahadk.com/s/v3/f346a09eb011342a_image.png" alt="Accessibility policy"></p>
<p>Even for those without a disability, the lack of here in this regard means that the site can be very confusing and buggy as a result.</p>
<h2 id="incorrect-laws">Incorrect laws</h2>
<p>The site has a course on ‚ÄúAI and intellectual property‚Äù. One thing it mentions is <a href="https://en.wikipedia.org/wiki/Fair_use">fair use</a>:</p>
<p><img src="https://cdn.mahadk.com/s/v3/1e8f7dc501128b15_image.png" alt="Fair use"></p>
<p><a href="https://x.com/ednewtonrex/status/2016443895505203379">(credits for the photo)</a></p>
<p>Except that <em>fair use is not a thing in the UK</em> - that‚Äôs a US concept! The UK uses what‚Äôs known as ‚Äúfair dealing‚Äù, which is more restrictive than fair use, so the details here are plain wrong.</p>
<h2 id="lack-of-care-lack-of-craft">Lack of care, lack of craft</h2>
<p>The interface for this website has also not been clearly thought out - one glaring example is the process of actually enrolling in a course.</p>
<p>On the course page, the ‚ÄúEnroll Now‚Äù button is <em>tiny</em>, and if you don‚Äôt see it and try scrolling down to the bottom, you will find yourself nothing but a comment section!</p>
<p><img src="https://cdn.mahadk.com/s/v3/61dc1d957dc892f8_image.png" alt="Image of the course page"></p>
<p>Then you have other bugs too, like the ‚ÄúSkills &amp; Training Gap Analysis‚Äù - which is linked at the top of the site! - apparently being closed off to the public for no reason:</p>
<p><img src="https://cdn.mahadk.com/s/v3/f03c15b9365950ff_image.png" alt="Image of the S&amp;T Gap Analysis page"></p>
<hr>
<p>To be honest, seeing this made me angry.</p>
<p>I‚Äôm angry at the sheer <em>wastefulness</em> of the UK Government here. Our public services are collapsing - while ¬£4 million is admittedly chump change for the UK government, there are real people behind these numbers - families waiting months for NHS appointments, children in crumbling schools, vulnerable people not getting the care they need. The waste feels particularly galling when you realise that almost no one will actually use this site!</p>
<p>I‚Äôm also angry that the small webdev businesses we have here in the UK were left out of this - for less than 5% of the cost, we‚Äôd have a <em>better</em> website and help out small businesses who actually care about their work, instead of handing the project to a multinational company that made nearly $60 billion in revenue in a year and has zero qualms about ripping off the British taxpayer.</p>
<p>Do better.</p>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla ending Models S and X production (447 pts)]]></title>
            <link>https://www.cnbc.com/2026/01/28/tesla-ending-model-s-x-production.html</link>
            <guid>46802867</guid>
            <pubDate>Wed, 28 Jan 2026 22:53:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2026/01/28/tesla-ending-model-s-x-production.html">https://www.cnbc.com/2026/01/28/tesla-ending-model-s-x-production.html</a>, See on <a href="https://news.ycombinator.com/item?id=46802867">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-106844785" data-test="InlineImage"><p>A Model X is on display at a Tesla showroom on February 13, 2021 in Beijing, China.</p><p>VCG | Visual China Group | Getty Images</p></div><div><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/TSLA/">Tesla</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> CEO <a href="https://www.cnbc.com/elon-musk/">Elon Musk</a> said on Wednesday that the automaker is ending production of its Model S and X vehicles, and will use the factory in Fremont, California, to build Optimus humanoid robots. </p><p>"It's time to basically bring the Model S and X programs to an end with an honorable discharge," Musk said on the company's fourth-quarter <a href="https://www.cnbc.com/2026/01/28/tesla-tsla-2025-q4-earnings.html">earnings</a> call. "If you're interested in buying a Model S and X, now would be the time to order it."</p><p>After the original Roadster, the two models are Tesla's oldest vehicles, and in recent years the company has slashed prices as global competition for electric vehicles has soared. Tesla started selling the Model S sedan in 2012, and the <a href="https://www.cnbc.com/2015/09/30/tesla-delivers-model-x-electric-suv-to-take-on-luxury-carmakers.html">Model X SUV</a> three years later. </p><p>On Tesla's website, the Model S currently starts at about $95,000, while the Model X starts at around $100,000</p><p>Tesla's far more popular models are the 3 and Y, which accounted for 97% of the company's 1.59 million <a href="https://ir.tesla.com/press-release/tesla-fourth-quarter-2025-production-deliveries-deployments" target="_blank">deliveries</a> last year. The Model 3 now starts at about $37,000, and the Model Y is around $40,000. Tesla debuted more affordable versions of the vehicles late last year. </p><p>In its earnings announcement on Wednesday, Tesla reported its first annual revenue decline on record, with sales falling in three of the past four quarters. Musk has been trying to turn attention away from traditional EVs and toward a future of driverless cars and humanoid robots, areas where the company currently has virtually no business.</p><p>Tesla is developing Optimus with the aim of someday selling it as a bipedal, intelligent robot capable of everything from factory work to babysitting. The company said in the release that it plans to unveil the third generation of Optimus this quarter, its "first design meant for mass production."</p><p>Musk said on the call that Tesla is replacing its production line for S and X in Fremont "with a 1 million unit per year line of Optimus."</p><p>"Because it is a completely new supply chain," Musk said, "there's really nothing from the existing supply chain that exists in Optimus."</p><p>Tesla expects to boost headcount at the Fremont facility, Musk added, "and to significantly increase output."</p><p><strong>This is breaking news. Please refresh for updates.</strong></p><p><strong>WATCH:</strong> <a href="https://www.cnbc.com/video/2026/01/28/fast-money-traders-on-how-they-are-playing-tesla-after-earnings-report-boosts-stocks.html">How traders are playing Tesla</a></p></div><div id="Placeholder-ArticleBody-Video-108258644" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000402215" aria-labelledby="Placeholder-ArticleBody-Video-108258644"><p><img src="https://image.cnbcfm.com/api/v1/image/108258657-1769639965310-1769639465-43735086212-hd.jpg?v=1769639966&amp;w=750&amp;h=422&amp;vtcrop=y" alt="'Fast Money' traders on how they are playing Tesla after earnings report boosts stocks"><span></span><span></span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bf-Tree: modern read-write-optimized concurrent larger-than-memory range index (124 pts)]]></title>
            <link>https://github.com/microsoft/bf-tree</link>
            <guid>46802210</guid>
            <pubDate>Wed, 28 Jan 2026 22:05:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/microsoft/bf-tree">https://github.com/microsoft/bf-tree</a>, See on <a href="https://news.ycombinator.com/item?id=46802210">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Bf-Tree</h2><a id="user-content-bf-tree" aria-label="Permalink: Bf-Tree" href="#bf-tree"></a></p>
<p dir="auto">Bf-Tree is a modern read-write-optimized concurrent larger-than-memory range index in Rust from MSR.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Design Details</h2><a id="user-content-design-details" aria-label="Permalink: Design Details" href="#design-details"></a></p>
<p dir="auto">You can find the Bf-Tree research paper <a href="https://badrish.net/papers/bftree-vldb2024.pdf" rel="nofollow">here</a>. You can find more design docs <a href="https://github.com/microsoft/bf-tree/blob/main/doc">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">User Guide</h2><a id="user-content-user-guide" aria-label="Permalink: User Guide" href="#user-guide"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Rust</h3><a id="user-content-rust" aria-label="Permalink: Rust" href="#rust"></a></p>
<p dir="auto">Bf-Tree is written in Rust, and is available as a Rust crate. You can add Bf-Tree to your <code>Cargo.toml</code> like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="[dependencies]
bf-tree = &quot;0.1.0&quot;"><pre>[<span>dependencies</span>]
<span>bf-tree</span> = <span><span>"</span>0.1.0<span>"</span></span></pre></div>
<p dir="auto">An example use of Bf-Tree:</p>
<div dir="auto" data-snippet-clipboard-copy-content="use bf_tree::BfTree;
use bf_tree::LeafReadResult;

let mut config = bf_tree::Config::default();
config.cb_min_record_size(4);
let tree = BfTree::with_config(config, None).unwrap();
tree.insert(b&quot;key&quot;, b&quot;value&quot;);

let mut buffer = [0u8; 1024];
let read_size = tree.read(b&quot;key&quot;, &amp;mut buffer);

assert_eq!(read_size, LeafReadResult::Found(5));
assert_eq!(&amp;buffer[..5], b&quot;value&quot;);"><pre><span>use</span> bf_tree<span>::</span><span>BfTree</span><span>;</span>
<span>use</span> bf_tree<span>::</span><span>LeafReadResult</span><span>;</span>

<span>let</span> <span>mut</span> config = bf_tree<span>::</span><span>Config</span><span>::</span><span>default</span><span>(</span><span>)</span><span>;</span>
config<span>.</span><span>cb_min_record_size</span><span>(</span><span>4</span><span>)</span><span>;</span>
<span>let</span> tree = <span>BfTree</span><span>::</span><span>with_config</span><span>(</span>config<span>,</span> <span>None</span><span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>
tree<span>.</span><span>insert</span><span>(</span><span>b"key"</span><span>,</span> <span>b"value"</span><span>)</span><span>;</span>

<span>let</span> <span>mut</span> buffer = <span>[</span><span>0u8</span><span>;</span> <span>1024</span><span>]</span><span>;</span>
<span>let</span> read_size = tree<span>.</span><span>read</span><span>(</span><span>b"key"</span><span>,</span> <span>&amp;</span><span>mut</span> buffer<span>)</span><span>;</span>

<span>assert_eq</span><span>!</span><span>(</span>read_size<span>,</span> <span>LeafReadResult</span><span>::</span><span>Found</span><span>(</span><span>5</span><span>)</span><span>)</span><span>;</span>
<span>assert_eq</span><span>!</span><span>(</span><span>&amp;</span>buffer<span>[</span>..<span>5</span><span>]</span><span>,</span> <span>b"value"</span><span>)</span><span>;</span></pre></div>
<p dir="auto">PRs are accepted and preferred over feature requests. Feel free to reach out if you have any design questions.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Developer Guide</h2><a id="user-content-developer-guide" aria-label="Permalink: Developer Guide" href="#developer-guide"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building</h3><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Prerequisite</h4><a id="user-content-prerequisite" aria-label="Permalink: Prerequisite" href="#prerequisite"></a></p>
<p dir="auto">Bf-Tree supports Linux, Windows, and macOS, although only a recently version of Linux is rigorously tested. Bf-Tree is written in Rust, which you can install <a href="https://rustup.rs/" rel="nofollow">here</a>.</p>
<p dir="auto">Please install pre-commit hooks to ensure that your code is formatted and linted in the same way as the rest of the project; the coding style will be enforced in CI, these hooks act as a pre-filter.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# If on Ubuntu
sudo apt update &amp;&amp; sudo apt install pre-commit
pre-commit install"><pre><span><span>#</span> If on Ubuntu</span>
sudo apt update <span>&amp;&amp;</span> sudo apt install pre-commit
pre-commit install</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Build</h4><a id="user-content-build" aria-label="Permalink: Build" href="#build"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Testing</h3><a id="user-content-testing" aria-label="Permalink: Testing" href="#testing"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Unit Tests</h4><a id="user-content-unit-tests" aria-label="Permalink: Unit Tests" href="#unit-tests"></a></p>

<p dir="auto"><h4 tabindex="-1" dir="auto">Shuttle Tests</h4><a id="user-content-shuttle-tests" aria-label="Permalink: Shuttle Tests" href="#shuttle-tests"></a></p>
<p dir="auto">Concurrent systems are nondeterministic, and subject to exponential amount of different thread interleaving. We use <a href="https://github.com/awslabs/shuttle">shuttle</a>
to deterministically and systematically explore different thread interleaving to uncover the bugs caused by subtle multithread interactions.</p>
<div dir="auto" data-snippet-clipboard-copy-content="cargo test --features &quot;shuttle&quot; --release shuttle_bf_tree_concurrent_operations"><pre>cargo <span>test</span> --features <span><span>"</span>shuttle<span>"</span></span> --release shuttle_bf_tree_concurrent_operations</pre></div>
<p dir="auto">(Takes about 5 minute to run)</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Fuzz Tests</h4><a id="user-content-fuzz-tests" aria-label="Permalink: Fuzz Tests" href="#fuzz-tests"></a></p>
<p dir="auto">Fuzz testing is a bug finding technique that generates random inputs to the system and test for crash. Bf-Tree employs fuzzing to generate random operation sequences
(e.g., insert, read, scan) to the system and check that none of the operation sequence will crash the system or lead to inconsistent state. Check the
<a href="https://github.com/microsoft/bf-tree/blob/main/fuzz/README.md">fuzz</a> folder for more details.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Benchmarking</h3><a id="user-content-benchmarking" aria-label="Permalink: Benchmarking" href="#benchmarking"></a></p>
<p dir="auto">Check the <a href="https://github.com/microsoft/bf-tree/blob/main/benchmark/README.md">benchmark</a> folder for more details.</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd benchmark
env SHUMAI_FILTER=&quot;inmemory&quot; MIMALLOC_LARGE_OS_PAGES=1 cargo run --bin bftree --release"><pre><span>cd</span> benchmark
env SHUMAI_FILTER=<span><span>"</span>inmemory<span>"</span></span> MIMALLOC_LARGE_OS_PAGES=1 cargo run --bin bftree --release</pre></div>
<p dir="auto">More advanced benchmarking, with metrics collecting, numa-node binding, huge page, etc:</p>
<div dir="auto" data-snippet-clipboard-copy-content="env MIMALLOC_SHOW_STATS=1 MIMALLOC_LARGE_OS_PAGES=1 MIMALLOC_RESERVE_HUGE_OS_PAGES_AT=0 numactl --membind=0 --cpunodebind=0 cargo bench --features &quot;metrics-rt&quot; micro"><pre>env MIMALLOC_SHOW_STATS=1 MIMALLOC_LARGE_OS_PAGES=1 MIMALLOC_RESERVE_HUGE_OS_PAGES_AT=0 numactl --membind=0 --cpunodebind=0 cargo bench --features <span><span>"</span>metrics-rt<span>"</span></span> micro</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Code of Conduct</h3><a id="user-content-code-of-conduct" aria-label="Permalink: Code of Conduct" href="#code-of-conduct"></a></p>
<p dir="auto">This project has adopted the <a href="https://opensource.microsoft.com/codeofconduct/" rel="nofollow">Microsoft Open Source Code of Conduct</a>.
For more information see the <a href="https://opensource.microsoft.com/codeofconduct/faq/" rel="nofollow">Code of Conduct FAQ</a>
or contact <a href="mailto:opencode@microsoft.com">opencode@microsoft.com</a> with any additional questions or comments.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Contributing</h3><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Please see <a href="https://github.com/microsoft/bf-tree/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Security</h3><a id="user-content-security" aria-label="Permalink: Security" href="#security"></a></p>
<p dir="auto">See <a href="https://github.com/microsoft/bf-tree/blob/main/SECURITY.md">SECURITY.md</a> for security reporting details.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Trademarks</h3><a id="user-content-trademarks" aria-label="Permalink: Trademarks" href="#trademarks"></a></p>
<p dir="auto">This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks
or logos is subject to and must follow <a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general" rel="nofollow">Microsoft‚Äôs Trademark &amp; Brand Guidelines</a>. Use of Microsoft trademarks or logos in
modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party
trademarks or logos are subject to those third-party‚Äôs policies.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Contact</h3><a id="user-content-contact" aria-label="Permalink: Contact" href="#contact"></a></p>
<ul dir="auto">
<li><a href="mailto:bftree@microsoft.com">bftree@microsoft.com</a></li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Somebody used spoofed ADSB signals to raster the meme of JD Vance (520 pts)]]></title>
            <link>https://alecmuffett.com/article/143548</link>
            <guid>46802067</guid>
            <pubDate>Wed, 28 Jan 2026 21:50:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alecmuffett.com/article/143548">https://alecmuffett.com/article/143548</a>, See on <a href="https://news.ycombinator.com/item?id=46802067">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>This, if it is still visible: </p>



<p><a href="https://globe.adsbexchange.com/?icao=adfdf9&amp;lat=26.678&amp;lon=-80.030&amp;zoom=14.4&amp;showTrace=2026-01-28">https://globe.adsbexchange.com/?icao=adfdf9&amp;lat=26.678&amp;lon=-80.030&amp;zoom=14.4&amp;showTrace=2026-01-28</a></p>



<p>Via:</p>



<figure><div><blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">Someone is spoofing as a VC-25A / Air Force One making the meme image on ADSB. Call sign " VANCE1 " <a href="https://t.co/yZMj7bfJUU">pic.twitter.com/yZMj7bfJUU</a></p>‚Äî ??_???????????? (@SR_Planespotter) <a href="https://twitter.com/SR_Planespotter/status/2016394753772863848?ref_src=twsrc%5Etfw">January 28, 2026</a></blockquote></div></figure>



<p>Next up, age verification for ADSB? </p>




<div> <p><a href="https://alecmuffett.com/tti-cache/143548.png">‚äû</a>
</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jellyfin LLM/"AI" Development Policy (195 pts)]]></title>
            <link>https://jellyfin.org/docs/general/contributing/llm-policies/</link>
            <guid>46801976</guid>
            <pubDate>Wed, 28 Jan 2026 21:42:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jellyfin.org/docs/general/contributing/llm-policies/">https://jellyfin.org/docs/general/contributing/llm-policies/</a>, See on <a href="https://news.ycombinator.com/item?id=46801976">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header><h2>Jellyfin LLM/"AI" Development Policy</h2></header>
<p>The rise of LLMs as a useful development tool over the last year or so has been significant. The power and flexibility of tools like Claude Code and ChatGPT have given a lot of functionality both to experienced developers and new developers alike. But there are trade-offs.</p>
<p>The Jellyfin project has, from day one, had a major focus on code quality - readability, simplicity, conciseness. This is a mostly manual effort driven by a dedicated team of individuals, and is motivated by a desire to fix the code Jellyfin is based off of which, without beating a dead horse too much, was extremely fragile, spaghettified, and prone to over-engineered complexity.</p>
<p>We are seeing a precipitous rise in contributors using AI within the Jellyfin ecosystem, both in the server and for clients, as well as a rise in criticism and concern about LLMs generally. At this time we are writing this policy to address exactly what we expect and desire with respect to contributions and interactions within our community that may use LLMs. These rules apply to all of our official projects and community spaces.</p>
<h2 id="general-guidelines">General Guidelines<a href="#general-guidelines" aria-label="Direct link to General Guidelines" title="Direct link to General Guidelines" translate="no">‚Äã</a></h2>
<ol>
<li>
<p>LLM output is <strong>expressly prohibited</strong> for any direct communication, including the following:</p>
<ul>
<li>issues or comments</li>
<li>feature requests or comments</li>
<li>pull request bodies or comments</li>
<li>forum/chat/etc. posts or comments</li>
</ul>
<p>In short, if you are posting <strong>any</strong> of those things, the output must be your own words, explanation, description, etc., not a verbatim dump of an LLM's output. We expect you to understand what you're posting. Violating this rule will result in closure/deletion of the offending item(s).</p>
<p>An exception will be made for <strong>LLM-assisted translations</strong> if you are having trouble accurately conveying your intent in English. Please explicitly note this ("I have translated this from MyLanguage with an LLM") and, if possible, post in your original language as well.</p>
</li>
<li>
<p>LLM code contributions are subject to more granularity below, but the general principle is that "pure 'vibe coding' will be rejected" and "you are responsible for what you commit". We will review in that vein. If the <strong>code looks terrible</strong>, it will be <strong>rejected as such</strong>.</p>
</li>
</ol>
<h2 id="llm-code-contributions-to-official-projects">LLM Code Contributions to Official Projects<a href="#llm-code-contributions-to-official-projects" aria-label="Direct link to LLM Code Contributions to Official Projects" title="Direct link to LLM Code Contributions to Official Projects" translate="no">‚Äã</a></h2>
<p>The use of LLMs for code is controversial and open to much interpretation. These guidelines are our best effort attempt to ensure that knowledgeable developers who seek to use these tools as a legitimate aid are not overly-hindered, while also preventing an ongoing flood of slop contributions that violate our core ethos above. These apply to <strong>all official Jellyfin projects</strong>.</p>
<ol>
<li>Contributions should be <strong>concise and focused</strong>. If the PR claims to target X, and is also touching unrelated Y and Z, it will be rejected. This includes incidental changes to unrelated functionality, a hallmark of poorly-worded or too-general prompts. Similarly, a large PR must be <strong>broken into multiple small, manageable commits</strong> for review and history purposes.</li>
<li>Formatting and quality <strong>standards must be upheld</strong>. Excessive unhelpful comments, spaghetti code, spaces on empty lines, etc. will be interpreted as pure LLM output and rejected; you must <strong>clean up the mess</strong> before submitting. Also <strong>do not commit LLM metafiles</strong> (e.g. <code>.claude</code> configs) or any other editor-created non-code files.</li>
<li>You must <strong>review the output</strong> and be able to <strong>explain</strong> in the PR body - <strong>without</strong> LLM output as noted above - <strong>what is being changed and why</strong>. Your PR body (and, if applicable, commit bodies) should be providing context to other developers about why a change was made, and if your name is on it, we want <strong>your</strong> words and explanations, not an LLM's. If <strong>you can't explain</strong> what the LLM did, we are <strong>not interested</strong> in the change.</li>
<li>The changes must be <strong>tested</strong>. The code should build and run correctly, or it will be rejected. You should also <strong>explicitly test the functionality being modified</strong>.</li>
<li>You must be able and willing to <strong>handle review feedback</strong> and implement the suggested change(s) as required. What this means in practice is, if you do not know what has been changed or why (see #3), and thus can't implement suggested changes or discuss them <strong>yourself</strong>, then we are <strong>not interested</strong> in the change. Just dumping reviewer feedback into an LLM and expecting what comes out to be "good enough", is not.</li>
<li><strong>Features or refactors</strong> require <strong>an in-depth level of understanding</strong> about what is being changed and why. It is obvious to our reviewers when changes are made without the developer making them understanding what is happening. These will be rejected. And as noted in #1, the PR must <strong>contain multiple discrete commits</strong>. <em>We</em> will squash commits as deemed appropriate after review. Large changes must also follow our other development policies (discussion, review, implementation, testing process).</li>
<li>The <strong>final discretion always lies with the reviewers</strong>. If your PR is not capable of being reasonably reviewed, for any reason (over-complexity, size, squashed commits, etc.) it will be rejected, and this goes just as much for non-LLM-assisted PRs as it does for LLM-assisted PRs. You will be asked to split such a PR up into multiple PRs that each present a focused, concise set of changes instead.</li>
</ol>
<p>The golden rule is this: <strong>do not just let an LLM loose on the codebase with a vague vibe prompt and then commit the results as-is</strong>. This is lazy development, will <strong>always</strong> result in a <strong>poor-quality contribution</strong> from our perspective, and we are not at all interested in such slop. <strong>Make an effort</strong> or please do not bother. And again, you are free to use LLMs to <strong>assist</strong> you, but not as the sole source of code changes.</p>

<p>You are of course free to do whatever you wish for your own non-official projects. However, we will be enforcing the following rules for any <strong>sharing of such projects within our communities</strong>.</p>
<ol>
<li>Any primarily-LLM-developed projects should be <strong>clearly marked as such</strong>. It is up to users to decide if this is acceptable to them or not. If you used an LLM for secondary assistance (e.g. docs, formatting, etc.) in an obvious way, we would err towards disclosure as well.</li>
<li>You <strong>must</strong> respect and follow licenses. If you are basing your project off of existing code, <strong>following its license is not optional</strong>. You must <strong>credit existing contributors in full</strong> for <strong>all contributions</strong>. Do not <strong>mangle the Git history</strong>, and do not <strong>commit pending 3rd party changes as your own</strong> (i.e. by copying the code and then committing it). Doing so will result in, not just rejection, but a ban from our organization and community. We have a <strong>zero tolerance policy</strong> for code theft and bad-faith attribution attempts.</li>
<li>For members of the community, <strong>do not report</strong> LLM-generated tools, clients, etc. <strong>on that basis alone</strong>, and do not engage in anti-LLM "witch hunts". As mentioned above, this is <strong>permitted</strong> and it is your choice whether to "support" said tool/client/etc. or not.</li>
<li>We, the moderators, are not going to play "LLM police" about 3rd party projects by nitpicking to try to "find LLM contributions" that otherwise follow our rules here; this is tedious and a waste of our time and effort. What this means in practice is that rule #1 is up to the author, and rule #3 must be interpreted in that vein. If you <strong>only suspect</strong> a tool is LLM-generated and violates rule #1, then downvote/ignore it and move on. <strong>Only if</strong> we see <strong>blatant breaking of rule #1</strong> we will enforce it, but again we will not be going through code line by line playing the "was this LLM generated?" game. Rule #2 will always be enforced regardless of LLM-ness or not.</li>
</ol></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla profit tanked 46% in 2025 (104 pts)]]></title>
            <link>https://techcrunch.com/2026/01/28/tesla-earnings-profit-q4-2025/</link>
            <guid>46801851</guid>
            <pubDate>Wed, 28 Jan 2026 21:33:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2026/01/28/tesla-earnings-profit-q4-2025/">https://techcrunch.com/2026/01/28/tesla-earnings-profit-q4-2025/</a>, See on <a href="https://news.ycombinator.com/item?id=46801851">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">Tesla‚Äôs profit fell 46% in 2025 compared to the prior year, as CEO Elon Musk assumed a role in the Trump administration and federal electric vehicle subsidies were killed off by Congress, causing sales to plummet.</p>

<p>The electric vehicle company reported Wednesday that it recorded just $3.8 billion in profit across 2025, its lowest tally in years. Total revenue from car sales fell 11% year-over-year, too. Tesla already revealed that it shipped 1.63 million cars globally across 2025. That marks the second year in a row that its sales have declined, after Musk spent years promising average annual growth of 50%.</p>







<p>Investors largely expected the decline in sales in Tesla‚Äôs fourth quarter and full-year results for 2025, and the company beat Wall Street‚Äôs estimates for earnings and revenue, sending shares up in after-market trading Wednesday. It‚Äôs been largely buoyed by strength in its other industries and investments, including energy capabilities and AI, as Tesla has continued to lure investors‚Äô attention away from its stalled-out automotive business.</p>

<p>The company wrote in its shareholder letter: ‚Äú2025 marked a critical year for Tesla as we further expanded our mission and continued our transition from a hardware-centric business to a physical AI company.‚Äù </p>

<p>The company revealed in the letter that it recently invested $2 billion in Elon Musk‚Äôs artificial intelligence startup xAI, part of the latter‚Äôs recent <a href="https://x.ai/news/series-e" target="_blank" rel="noreferrer noopener nofollow">Series E funding round</a>.</p>

<p>Revenue from Tesla‚Äôs solar and energy storage businesses also grew 25% compared to 2024, and services revenue (which includes payments for Full Self-Driving software, insurance, parts, and Supercharging) grew 18%. The company was even able to grow its gross margin compared to prior quarters.</p>

<p>Long-awaited projects like the Tesla Semi (first revealed in 2017) and the Cybercab (which debuted in 2024, but has been teased for years) are supposed to enter production in the first half of this year, according to Tesla.  </p>
<div>
		
		<p>Techcrunch event</p>
		<div>
			
			<p><span>Boston, MA</span>
													<span>|</span>
													<span>June 23, 2026</span>
							</p>
			
		</div>
	</div>

<p>Tesla has a <em>lot </em>of other projects on its plate, which were detailed in the shareholder letter. The company has started pilot production at its lithium refinery in Texas. It‚Äôs developing new in-house inference chips for its autonomy and robotics programs. And it plans to reveal the third-generation version of its Optimus robot in the first quarter of this year.</p>

<p><em>This story is developing</em>&nbsp;‚Ä¶</p>


</div><div>
	
	
	
	

	
<div>
		<p>Sean O‚ÄôKane is a reporter who has spent a decade covering the rapidly-evolving business and technology of the transportation industry, including Tesla and the many startups chasing Elon Musk. Most recently, he was a reporter at Bloomberg News where he helped break stories about some of the most notorious EV SPAC flops. He previously worked at The Verge, where he also covered consumer technology, hosted many short- and long-form videos, performed product and editorial photography, and once nearly passed out in a Red Bull Air Race plane.</p>
<p>You can contact or verify outreach from Sean by emailing <a href="mailto:sean.okane@techcrunch.com">sean.okane@techcrunch.com</a> or via encrypted message at okane.01 on Signal.</p>	</div>


	
	<p>
		<a data-ctatext="View Bio" data-destinationlink="https://techcrunch.com/author/sean-okane/" data-event="button" href="https://techcrunch.com/author/sean-okane/">View Bio <svg style="width: 1em;" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"><path fill="var(--c-svg, currentColor)" d="M16.5 12 9 19.5l-1.05-1.05L14.4 12 7.95 5.55 9 4.5z"></path></svg></a>
	</p>
	
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple to Soon Take Up to 30% Cut from All Patreon Creators in iOS App (713 pts)]]></title>
            <link>https://www.macrumors.com/2026/01/28/patreon-apple-tax/</link>
            <guid>46801419</guid>
            <pubDate>Wed, 28 Jan 2026 20:59:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macrumors.com/2026/01/28/patreon-apple-tax/">https://www.macrumors.com/2026/01/28/patreon-apple-tax/</a>, See on <a href="https://news.ycombinator.com/item?id=46801419">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="maincontent"><article expanded="true"><div data-io-article-url="/2026/01/28/patreon-apple-tax/"><p>Apple has set a new deadline of November 1, 2026 for all <a href="https://apps.apple.com/us/app/patreon/id1044456188">Patreon</a> creators to switch from Patreon's legacy billing system to the App Store's in-app purchase system in the Patreon app on the iPhone and iPad, as <a href="https://techcrunch.com/2026/01/28/apple-tells-patreon-to-move-creators-to-in-app-purchase-for-subscriptions-by-november/">reported by <em>TechCrunch</em></a>.</p>
<p><img src="https://images.macrumors.com/t/juCKbtsR6vHD7VJNMPfqcM2EVg0=/400x0/article-new/2026/01/Tim-Cooking.jpg?lossy" srcset="https://images.macrumors.com/t/juCKbtsR6vHD7VJNMPfqcM2EVg0=/400x0/article-new/2026/01/Tim-Cooking.jpg?lossy 400w,https://images.macrumors.com/t/PfYdfRInOmELVWISdbgtd2cZV0A=/800x0/article-new/2026/01/Tim-Cooking.jpg?lossy 800w,https://images.macrumors.com/t/HPvkDF3sphYEAzKX3QfLArhr0no=/1600x0/article-new/2026/01/Tim-Cooking.jpg 1600w,https://images.macrumors.com/t/mA15gUqGqDeRPDXf1g4fX10o200=/2500x0/filters:no_upscale()/article-new/2026/01/Tim-Cooking.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="Tim Cooking" width="1600" height="900"><em><small>Note: This image has been edited to include a pile of cash.</small></em></p><p>Patreon is a platform where creators such as YouTubers can receive payments from fans, which can be a valuable revenue stream alongside ads and sponsorships.</p>
<p>Apple initially told Patreon that its creators must move to the App Store's in-app purchase system <a href="https://www.macrumors.com/2024/08/12/patreon-in-app-purchase-fees/">by November 2025</a>, or else Patreon would risk removal from the App Store, but the deadline was pushed back. Apple considers payments from supporters to creators on Patreon to be digital goods that it is entitled to receive a commission on.</p>
<p>Apple receives a 30% commission on in-app purchases and subscriptions, but this drops to 15% for a subscription that has been ongoing for more than a year.</p>
<p>Patreon gives creators the option to either increase their prices in the iOS app only, or absorb the fee themselves, keeping prices the same across platforms.</p>
<p>On the iPhone and iPad, Patreon users who wish to support a creator can sidestep the App Store's commission by completing their payment via Patreon's website.</p>
<p>Patreon said it is disappointed with how Apple has navigated this policy.</p>
<p>According to <em>TechCrunch</em>, only 4% of Patreon creators are still using the platform's legacy billing system, with the rest having already switched over.</p>
<p>Patreon has <a href="https://support.patreon.com/hc/en-us/articles/28801582599181-iOS-In-app-Purchases-and-Migrating-to-Subscription-Billing-FAQ">shared a FAQ</a> with more details for creators.</p>
</div></article><p><h2>Popular Stories</h2></p><div><h3><a href="https://www.macrumors.com/2026/01/26/apple-unveils-first-two-products-of-2026/">Apple Unveils First New Products of 2026</a></h3><p>Apple today introduced its first two physical products of 2026: a second-generation AirTag and the Black Unity Connection Braided Solo Loop for the Apple Watch.
Read our coverage of each announcement to learn more:Apple Unveils New AirTag With Longer Range, Louder Speaker, and More
Apple Introduces New Black Unity Apple Watch BandBoth the new AirTag and the Black Unity Connection Braided...</p></div><div><h3><a href="https://www.macrumors.com/2026/01/26/iphone-5s-software-update/">iPhone 5s Gets New Software Update 13 Years After Launch</a></h3><p>Monday January 26, 2026 3:56 pm PST by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>Alongside iOS 26.2.1, Apple today released an updated version of iOS 12 for devices that are still running that operating system update, eight years after the software was first released.
iOS 12.5.8 is available for the iPhone 5s and the iPhone 6, meaning Apple is continuing to support these devices for 13 and 12 years after launch, respectively. The iPhone 5s came out in September 2013,...</p></div><div><h3><a href="https://www.macrumors.com/2026/01/26/apple-announces-new-airtag/">Apple Unveils New AirTag With Longer Range, Louder Speaker, and More</a></h3><p>Apple today introduced the second-generation AirTag, with key features including longer range for tracking items and a louder speaker.
For those who are not familiar, the AirTag is a small accessory that you can attach to your backpack, keys, or other items. Then, you can track the location of those items in the Find My app on the iPhone, iPad, Mac, Apple Watch, and iCloud.com.
The new...</p></div><div><h3><a href="https://www.macrumors.com/2026/01/25/rumored-apple-products/">Apple to Launch These 20+ Products This Year</a></h3><p>2026 promises to be yet another busy year for Apple, with the company rumored to be planning more than 20 product announcements over the coming months.
Beyond the usual updates to iPhones, iPads, Macs, and Apple Watches, Apple is expected to release its all-new smart home hub, which was reportedly delayed until the more personalized version of Siri is ready. Other unique products rumored for ...</p></div><div><h3><a href="https://www.macrumors.com/2026/01/27/apples-next-launch-is-tomorrow/">Apple's Next Launch is Today</a></h3><p>Tuesday January 27, 2026 2:39 pm PST by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>Update: Apple Creator Studio is now available.
Apple Creator Studio launches this Wednesday, January 28. The all-in-one subscription provides access to the Final Cut Pro, Logic Pro, Pixelmator Pro, Motion, Compressor, and MainStage apps, with U.S. pricing set at $12.99 per month or $129 per year.
A subscription to Apple Creator Studio also unlocks "intelligent features" and "premium...</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Native Instruments enters into insolvency proceedings (108 pts)]]></title>
            <link>https://www.engadget.com/audio/native-instruments-enters-into-insolvency-proceedings-leaving-its-future-uncertain-183206826.html</link>
            <guid>46800645</guid>
            <pubDate>Wed, 28 Jan 2026 19:51:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.engadget.com/audio/native-instruments-enters-into-insolvency-proceedings-leaving-its-future-uncertain-183206826.html">https://www.engadget.com/audio/native-instruments-enters-into-insolvency-proceedings-leaving-its-future-uncertain-183206826.html</a>, See on <a href="https://news.ycombinator.com/item?id=46800645">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-article-body="true"><p>Music hardware and software manufacturer Native Instruments has entered into preliminary insolvency proceedings, <a data-i13n="cpos:1;pos:1" href="https://cdm.link/ni-insolvency/" rel="nofollow noopener" target="_blank" data-ylk="slk:according to a report by Create Digital Music;cpos:1;pos:1;elm:context_link;itc:0;sec:content-canvas" data-yga="{&quot;yLinkText&quot;:&quot;according to a report by Create Digital Music&quot;,&quot;yLinkPosition&quot;:&quot;1&quot;,&quot;yPosition&quot;:&quot;1&quot;,&quot;yLinkElement&quot;:&quot;context_link&quot;,&quot;yModuleName&quot;:&quot;content-canvas&quot;,&quot;yHasCommerce&quot;:false}"><ins>according to a report by </ins><em><ins>Create Digital Music</ins></em></a>. This is the company behind iconic software like Massive, Traktor and Kontakt and hardware like Maschine+. Native Instruments also owns the brands iZotope, Brainworx and Plugin Alliance.</p><p>We don't have many specifics as to what this entails and what the future of the company will look like. We do know that an administrator has been appointed to handle restructuring and, potentially, to sell off existing assets. Native Instruments employs hundreds of people and their fates also remain uncertain.</p><p>A private equity firm called Francisco Partners owns a majority stake in the company. It also owns majority stakes in entities like GoodRX and Verifone, among others. This isn't the first time Native Instruments has been forced into major restructuring. The company experienced plenty of layoffs and uncertainty <a data-i13n="cpos:2;pos:1" href="https://djtechtools.com/2019/09/03/shakeups-layoffs-whats-happening-at-native-instruments/" rel="nofollow noopener" target="_blank" data-ylk="slk:between 2019 and 2020;cpos:2;pos:1;elm:context_link;itc:0;sec:content-canvas" data-yga="{&quot;yLinkText&quot;:&quot;between 2019 and 2020&quot;,&quot;yLinkPosition&quot;:&quot;2&quot;,&quot;yPosition&quot;:&quot;1&quot;,&quot;yLinkElement&quot;:&quot;context_link&quot;,&quot;yModuleName&quot;:&quot;content-canvas&quot;,&quot;yHasCommerce&quot;:false}"><ins>between 2019 and 2020</ins></a> before being purchased by private equity.</p><p>Again, we have no idea how this will shake out. It's possible that new investors will jump on board and it goes back to business as usual. It's also possible everything will be scrapped for parts and sold to the highest bidder.</p><div><blockquote><div><p>Plugin Alliance, with entities in the US and Germany, says there's no immediate impact of the NI insolvency on them. <a href="https://t.co/u6smg1X0t4" rel="nofollow noopener" target="_blank" data-ylk="slk:https://t.co/u6smg1X0t4;elm:context_link;itc:0;sec:content-canvas" data-yga="{&quot;yLinkText&quot;:&quot;https&quot;,&quot;yLinkElement&quot;:&quot;context_link&quot;,&quot;yModuleName&quot;:&quot;content-canvas&quot;,&quot;yHasCommerce&quot;:false}">https://t.co/u6smg1X0t4</a> <a href="https://t.co/ok7BRwo3BU" rel="nofollow noopener" target="_blank" data-ylk="slk:pic.twitter.com/ok7BRwo3BU;elm:context_link;itc:0;sec:content-canvas" data-yga="{&quot;yLinkText&quot;:&quot;pic.twitter.com/ok7BRwo3BU&quot;,&quot;yLinkElement&quot;:&quot;context_link&quot;,&quot;yModuleName&quot;:&quot;content-canvas&quot;,&quot;yHasCommerce&quot;:false}">pic.twitter.com/ok7BRwo3BU</a></p><p>‚Äî cdmblogs (@cdmblogs) <a href="https://twitter.com/cdmblogs/status/2016499892064014356?ref_src=twsrc%5Etfw" rel="nofollow noopener" target="_blank" data-ylk="slk:January 28, 2026;elm:context_link;itc:0;sec:content-canvas" data-yga="{&quot;yLinkText&quot;:&quot;January 28, 2026&quot;,&quot;yLinkElement&quot;:&quot;context_link&quot;,&quot;yModuleName&quot;:&quot;content-canvas&quot;,&quot;yHasCommerce&quot;:false}">January 28, 2026</a></p></div></blockquote></div><p>We do know that subsidiary Plugin Alliance seems to be unaffected. It issued a <a data-i13n="cpos:3;pos:1" href="https://www.gearnews.com/native-instruments-insolvency/" rel="nofollow noopener" target="_blank" data-ylk="slk:statement on Facebook;cpos:3;pos:1;elm:context_link;itc:0;sec:content-canvas" data-yga="{&quot;yLinkText&quot;:&quot;statement on Facebook&quot;,&quot;yLinkPosition&quot;:&quot;3&quot;,&quot;yPosition&quot;:&quot;1&quot;,&quot;yLinkElement&quot;:&quot;context_link&quot;,&quot;yModuleName&quot;:&quot;content-canvas&quot;,&quot;yHasCommerce&quot;:false}">statement on Facebook</a> saying that it isn't involved with the proceedings and that operations will continue as normal. This means new plugins will be released, along with updates for current software.</p><p>Everything else is still up in the air. This is troubling for those who have heavily invested in the company's ecosystem of products. I'm one of them. Any hope I had for a <a data-i13n="cpos:4;pos:1" href="https://www.engadget.com/maschine-plus-native-instruments-virtual-instruments-groovebox-155725975.html" data-ylk="slk:hardware refresh of the Maschine+;cpos:4;pos:1;elm:context_link;itc:0;sec:content-canvas" data-yga="{&quot;yLinkText&quot;:&quot;hardware refresh of the Maschine+&quot;,&quot;yLinkPosition&quot;:&quot;4&quot;,&quot;yPosition&quot;:&quot;1&quot;,&quot;yLinkElement&quot;:&quot;context_link&quot;,&quot;yModuleName&quot;:&quot;content-canvas&quot;,&quot;yHasCommerce&quot;:false}"><ins>hardware refresh of the Maschine+</ins></a> just went out the window.</p><iframe height="auto" loading="lazy" src="https://www.youtube.com/embed/D4u79Z3_7xw?si=VjUk1AlbtO89Dx0e" title="youtube embed content" width="100%" data-pw="iframe"></iframe><p>If the company's robust line of software goes up for sale, Akai is likely the best bet. It has already begun incorporating <a data-i13n="cpos:5;pos:1" href="https://www.engadget.com/entertainment/music/many-akai-devices-will-soon-support-native-instruments-sound-packs-140059822.html" data-ylk="slk:Native Instruments software into MPC machines;cpos:5;pos:1;elm:context_link;itc:0;sec:content-canvas" data-yga="{&quot;yLinkText&quot;:&quot;Native Instruments software into MPC machines&quot;,&quot;yLinkPosition&quot;:&quot;5&quot;,&quot;yPosition&quot;:&quot;1&quot;,&quot;yLinkElement&quot;:&quot;context_link&quot;,&quot;yModuleName&quot;:&quot;content-canvas&quot;,&quot;yHasCommerce&quot;:false}"><ins>Native Instruments software into MPC machines</ins></a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A MitM proxy to see what your LLM tools are sending (201 pts)]]></title>
            <link>https://github.com/jmuncor/sherlock</link>
            <guid>46799898</guid>
            <pubDate>Wed, 28 Jan 2026 18:52:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/jmuncor/sherlock">https://github.com/jmuncor/sherlock</a>, See on <a href="https://news.ycombinator.com/item?id=46799898">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Sherlock</h2><a id="user-content-sherlock" aria-label="Permalink: Sherlock" href="#sherlock"></a></p>
  <p dir="auto">
    <strong>LLM API Traffic Inspector &amp; Token Usage Dashboard</strong>
  </p>
  <p dir="auto">
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f12b46fe26968f97149ee10725a248ec02c28ce205f9f5d3c40ef06a38de8ea0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e31302b2d3337373641423f6c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465"><img src="https://camo.githubusercontent.com/f12b46fe26968f97149ee10725a248ec02c28ce205f9f5d3c40ef06a38de8ea0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e31302b2d3337373641423f6c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465" alt="Python" data-canonical-src="https://img.shields.io/badge/python-3.10+-3776AB?logo=python&amp;logoColor=white"></a>
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/8bb50fd2278f18fc326bf71f6e88ca8f884f72f179d3e555e20ed30157190d0d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"><img src="https://camo.githubusercontent.com/8bb50fd2278f18fc326bf71f6e88ca8f884f72f179d3e555e20ed30157190d0d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667" alt="License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"></a>
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/09bd1562a600554ab1374f91b4008b5ab76f2b940faeb41a15b6946cffc63a62/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d6d61634f532532302537432532304c696e75782d6c69676874677265792e737667"><img src="https://camo.githubusercontent.com/09bd1562a600554ab1374f91b4008b5ab76f2b940faeb41a15b6946cffc63a62/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d6d61634f532532302537432532304c696e75782d6c69676874677265792e737667" alt="Platform" data-canonical-src="https://img.shields.io/badge/platform-macOS%20%7C%20Linux-lightgrey.svg"></a>
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3b1e543a676609e6d74d9b2079b0f9ceb22976b2d14e351901bfbb6b28d9a461/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70726f78792d6d69746d70726f78792d6f72616e67652e737667"><img src="https://camo.githubusercontent.com/3b1e543a676609e6d74d9b2079b0f9ceb22976b2d14e351901bfbb6b28d9a461/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70726f78792d6d69746d70726f78792d6f72616e67652e737667" alt="mitmproxy" data-canonical-src="https://img.shields.io/badge/proxy-mitmproxy-orange.svg"></a>
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/c55777bc9bb31c546f01368c76676f01b722560af7b40cd14f62af97550f87db/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c4c4d2d416e7468726f706963253230436c617564652d626c756576696f6c65742e737667"><img src="https://camo.githubusercontent.com/c55777bc9bb31c546f01368c76676f01b722560af7b40cd14f62af97550f87db/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c4c4d2d416e7468726f706963253230436c617564652d626c756576696f6c65742e737667" alt="Anthropic" data-canonical-src="https://img.shields.io/badge/LLM-Anthropic%20Claude-blueviolet.svg"></a>
  </p>
  <p dir="auto">
    <a href="#installation">Installation</a> ‚Ä¢
    <a href="#quick-start">Quick Start</a> ‚Ä¢
    <a href="#features">Features</a> ‚Ä¢
    <a href="#commands">Commands</a> ‚Ä¢
    <a href="#contributing">Contributing</a>
  </p>

<hr>
<p dir="auto">Sherlock is a transparent proxy that intercepts HTTPS traffic to LLM APIs and displays real-time token usage in a beautiful terminal dashboard. Track your AI costs, debug prompts, and monitor context window usage across your development session.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why Sherlock?</h2><a id="user-content-why-sherlock" aria-label="Permalink: Why Sherlock?" href="#why-sherlock"></a></p>
<ul dir="auto">
<li><strong>Track Token Usage</strong>: See exactly how many tokens each request consumes</li>
<li><strong>Monitor Context Windows</strong>: Visual fuel gauge shows cumulative usage against your limit</li>
<li><strong>Debug Prompts</strong>: Automatically saves every prompt as markdown and JSON for review</li>
<li><strong>Zero Code Changes</strong>: Works with any tool that respects proxy environment variables</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Clone the repository
git clone https://github.com/jmuncor/sherlock.git
cd sherlock

# Install in development mode
pip install -e ."><pre><span><span>#</span> Clone the repository</span>
git clone https://github.com/jmuncor/sherlock.git
<span>cd</span> sherlock

<span><span>#</span> Install in development mode</span>
pip install -e <span>.</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Requirements</h3><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>Python 3.10+</li>
<li>Node.js (for intercepting Node.js applications like Claude Code)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">1. Start Sherlock</h3><a id="user-content-1-start-sherlock" aria-label="Permalink: 1. Start Sherlock" href="#1-start-sherlock"></a></p>

<p dir="auto">On first run, Sherlock will:</p>
<ul dir="auto">
<li>Generate the mitmproxy CA certificate</li>
<li>Prompt you to install it in your system trust store</li>
<li>Ask where to save intercepted prompts</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">2. Run Your LLM Tools</h3><a id="user-content-2-run-your-llm-tools" aria-label="Permalink: 2. Run Your LLM Tools" href="#2-run-your-llm-tools"></a></p>
<p dir="auto">In a separate terminal, use Sherlock to proxy your commands:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# For Claude Code
sherlock claude

# For any command
sherlock run --node your-llm-tool"><pre><span><span>#</span> For Claude Code</span>
sherlock claude

<span><span>#</span> For any command</span>
sherlock run --node your-llm-tool</pre></div>
<p dir="auto">That's it! Watch the dashboard update in real-time as you interact with LLM APIs.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Live Terminal Dashboard</h3><a id="user-content-live-terminal-dashboard" aria-label="Permalink: Live Terminal Dashboard" href="#live-terminal-dashboard"></a></p>
<div data-snippet-clipboard-copy-content="‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üîç SHERLOCK - LLM Traffic Inspector                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Context Usage  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  42%           ‚îÇ
‚îÇ                 (84,231 / 200,000 tokens)                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Time     Provider    Model                      Tokens     ‚îÇ
‚îÇ  14:23:01 Anthropic   claude-sonnet-4-20250514   12,847     ‚îÇ
‚îÇ  14:23:45 Anthropic   claude-sonnet-4-20250514   8,234      ‚îÇ
‚îÇ  14:24:12 Anthropic   claude-sonnet-4-20250514   15,102     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Last Prompt: &quot;Can you help me refactor this function...&quot;   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"><pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üîç SHERLOCK - LLM Traffic Inspector                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Context Usage  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  42%           ‚îÇ
‚îÇ                 (84,231 / 200,000 tokens)                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Time     Provider    Model                      Tokens     ‚îÇ
‚îÇ  14:23:01 Anthropic   claude-sonnet-4-20250514   12,847     ‚îÇ
‚îÇ  14:23:45 Anthropic   claude-sonnet-4-20250514   8,234      ‚îÇ
‚îÇ  14:24:12 Anthropic   claude-sonnet-4-20250514   15,102     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Last Prompt: "Can you help me refactor this function..."   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prompt Archive</h3><a id="user-content-prompt-archive" aria-label="Permalink: Prompt Archive" href="#prompt-archive"></a></p>
<p dir="auto">Every intercepted request is saved as:</p>
<ul dir="auto">
<li><strong>Markdown</strong> - Human-readable format with metadata</li>
<li><strong>JSON</strong> - Raw API request body for debugging</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Context Fuel Gauge</h3><a id="user-content-context-fuel-gauge" aria-label="Permalink: Context Fuel Gauge" href="#context-fuel-gauge"></a></p>
<p dir="auto">Visual progress bar with color-coded warnings:</p>
<ul dir="auto">
<li>üü¢ Green: &lt; 50% usage</li>
<li>üü° Yellow: 50-80% usage</li>
<li>üî¥ Red: &gt; 80% usage</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Commands</h2><a id="user-content-commands" aria-label="Permalink: Commands" href="#commands"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Command</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>sherlock</code></td>
<td>Start the proxy and dashboard</td>
</tr>
<tr>
<td><code>sherlock start</code></td>
<td>Same as above (explicit)</td>
</tr>
<tr>
<td><code>sherlock claude</code></td>
<td>Run Claude Code with proxy configured</td>
</tr>
<tr>
<td><code>sherlock run &lt;cmd&gt;</code></td>
<td>Run any command with proxy configured</td>
</tr>
<tr>
<td><code>sherlock run --node &lt;cmd&gt;</code></td>
<td>Run Node.js app with proxy configured</td>
</tr>
<tr>
<td><code>sherlock check-certs</code></td>
<td>Verify CA certificate installation</td>
</tr>
<tr>
<td><code>sherlock install-certs</code></td>
<td>Show certificate installation instructions</td>
</tr>
<tr>
<td><code>sherlock env</code></td>
<td>Print proxy environment variables</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Options</h3><a id="user-content-options" aria-label="Permalink: Options" href="#options"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="sherlock start [OPTIONS]

Options:
  -p, --port NUM          Proxy port (default: 8080)
  -l, --limit NUM         Token limit for fuel gauge (default: 200000)
  --persist               Save token history across sessions
  --skip-cert-check       Skip certificate verification"><pre>sherlock start [OPTIONS]

Options:
  -p, --port NUM          Proxy port (default: 8080)
  -l, --limit NUM         Token limit <span>for</span> fuel gauge (default: 200000)
  --persist               Save token <span>history</span> across sessions
  --skip-cert-check       Skip certificate verification</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">How It Works</h2><a id="user-content-how-it-works" aria-label="Permalink: How It Works" href="#how-it-works"></a></p>
<div data-snippet-clipboard-copy-content="‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Your LLM Application                        ‚îÇ
‚îÇ              (with proxy environment variables)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ HTTPS
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     mitmproxy (port 8080)                        ‚îÇ
‚îÇ                   + Sherlock Interceptor                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ Parsed events
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Sherlock Dashboard                            ‚îÇ
‚îÇ              Token tracking ‚Ä¢ Request log ‚Ä¢ Prompt preview       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
                    ~/.sherlock/prompts/
                    ‚îú‚îÄ‚îÄ 2024-01-15_14-23-01_anthropic.md
                    ‚îî‚îÄ‚îÄ 2024-01-15_14-23-01_anthropic.json"><pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Your LLM Application                        ‚îÇ
‚îÇ              (with proxy environment variables)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ HTTPS
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     mitmproxy (port 8080)                        ‚îÇ
‚îÇ                   + Sherlock Interceptor                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ Parsed events
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Sherlock Dashboard                            ‚îÇ
‚îÇ              Token tracking ‚Ä¢ Request log ‚Ä¢ Prompt preview       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
                    ~/.sherlock/prompts/
                    ‚îú‚îÄ‚îÄ 2024-01-15_14-23-01_anthropic.md
                    ‚îî‚îÄ‚îÄ 2024-01-15_14-23-01_anthropic.json
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported Providers</h2><a id="user-content-supported-providers" aria-label="Permalink: Supported Providers" href="#supported-providers"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Provider</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td>Anthropic (Claude)</td>
<td>‚úÖ Supported</td>
</tr>
<tr>
<td>OpenAI</td>
<td>üîú Coming soon</td>
</tr>
<tr>
<td>Google Gemini</td>
<td>üîú Coming soon</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Certificate Setup</h3><a id="user-content-certificate-setup" aria-label="Permalink: Certificate Setup" href="#certificate-setup"></a></p>
<p dir="auto">Sherlock uses mitmproxy to intercept HTTPS traffic. On first run, it will guide you through installing the CA certificate.</p>
<p dir="auto"><strong>macOS:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo security add-trusted-cert -d -r trustRoot \
  -k /Library/Keychains/System.keychain \
  ~/.mitmproxy/mitmproxy-ca-cert.pem"><pre>sudo security add-trusted-cert -d -r trustRoot \
  -k /Library/Keychains/System.keychain \
  <span>~</span>/.mitmproxy/mitmproxy-ca-cert.pem</pre></div>
<p dir="auto"><strong>Ubuntu/Debian:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo cp ~/.mitmproxy/mitmproxy-ca-cert.pem \
  /usr/local/share/ca-certificates/mitmproxy-ca-cert.crt
sudo update-ca-certificates"><pre>sudo cp <span>~</span>/.mitmproxy/mitmproxy-ca-cert.pem \
  /usr/local/share/ca-certificates/mitmproxy-ca-cert.crt
sudo update-ca-certificates</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Environment Variables</h3><a id="user-content-environment-variables" aria-label="Permalink: Environment Variables" href="#environment-variables"></a></p>
<p dir="auto">For manual proxy configuration:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export HTTP_PROXY=&quot;http://127.0.0.1:8080&quot;
export HTTPS_PROXY=&quot;http://127.0.0.1:8080&quot;
export NODE_EXTRA_CA_CERTS=&quot;$HOME/.mitmproxy/mitmproxy-ca-cert.pem&quot;"><pre><span>export</span> HTTP_PROXY=<span><span>"</span>http://127.0.0.1:8080<span>"</span></span>
<span>export</span> HTTPS_PROXY=<span><span>"</span>http://127.0.0.1:8080<span>"</span></span>
<span>export</span> NODE_EXTRA_CA_CERTS=<span><span>"</span><span>$HOME</span>/.mitmproxy/mitmproxy-ca-cert.pem<span>"</span></span></pre></div>
<p dir="auto">Or use the helper:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions are welcome! Here's how you can help:</p>
<ol dir="auto">
<li><strong>Fork</strong> the repository</li>
<li><strong>Create</strong> a feature branch (<code>git checkout -b feature/amazing-feature</code>)</li>
<li><strong>Commit</strong> your changes (<code>git commit -m 'Add amazing feature'</code>)</li>
<li><strong>Push</strong> to the branch (<code>git push origin feature/amazing-feature</code>)</li>
<li><strong>Open</strong> a Pull Request</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Development Setup</h3><a id="user-content-development-setup" aria-label="Permalink: Development Setup" href="#development-setup"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/yourusername/sherlock.git
cd sherlock
python -m venv venv
source venv/bin/activate
pip install -e ."><pre>git clone https://github.com/yourusername/sherlock.git
<span>cd</span> sherlock
python -m venv venv
<span>source</span> venv/bin/activate
pip install -e <span>.</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Adding Provider Support</h3><a id="user-content-adding-provider-support" aria-label="Permalink: Adding Provider Support" href="#adding-provider-support"></a></p>
<p dir="auto">To add support for a new LLM provider:</p>
<ol dir="auto">
<li>Add the API host to <code>sherlock/config.py</code></li>
<li>Create a parser function in <code>sherlock/parser.py</code></li>
<li>Update the <code>parse_request()</code> function to route to your parser</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the MIT License - see the <a href="https://github.com/jmuncor/sherlock/blob/main/LICENSE">LICENSE</a> file for details.</p>
<hr>
<p dir="auto">
  <em>See what's really being sent to the LLM. Learn. Optimize. Repeat.</em>
</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LM Studio 0.4 (200 pts)]]></title>
            <link>https://lmstudio.ai/blog/0.4.0</link>
            <guid>46799477</guid>
            <pubDate>Wed, 28 Jan 2026 18:23:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lmstudio.ai/blog/0.4.0">https://lmstudio.ai/blog/0.4.0</a>, See on <a href="https://news.ycombinator.com/item?id=46799477">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Today we are thrilled to share <strong node="[object Object]">LM Studio 0.4.0</strong>, the next generation of LM Studio.</p>
<p>This release introduces parallel requests with continuous batching for high throughput serving, all-new non-GUI deployment option, new stateful REST API, and a refreshed user interface.</p>
<div><p><img src="https://lmstudio.ai/assets/blog/0.4.0/app-screen.png" alt="LM Studio 0.4.0 is here" data-caption=""></p></div>
<p><strong node="[object Object]">LM Studio 0.4.0 highlights include</strong>:</p>
<ul node="[object Object]">
<li>Deploy LM Studio's core on cloud servers, in CI, or anywhere without GUI.</li>
<li>Parallel requests to the same model with continuous batching (instead of queueing).</li>
<li>New stateful REST API endpoint: <code dir="ltr">/v1/chat</code> that allows using local MCPs.</li>
<li>Refreshed application UI with chat export, split view, developer mode, and in-app docs.</li>
</ul>
<p>Read on for more details!</p>
<hr node="[object Object]">
<h2 id="deploy-on-servers-deploy-in-ci-deploy-anywhere" node="[object Object]"><span>Deploy on servers, deploy in CI, deploy anywhere</span><a href="#deploy-on-servers-deploy-in-ci-deploy-anywhere" aria-label="Link to this section"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<div><p><img src="https://lmstudio.ai/assets/blog/0.4.0/daemon@2x.png" alt="Daemon mode" data-caption=""></p></div>
<p>Today we're introducing <code dir="ltr">llmster</code>: it's the core of the LM Studio desktop app, but packaged to be server-native, without reliance on the GUI. We've rearchitected our software to separate the GUI from the core functionality, allowing <code dir="ltr">llmster</code> to run as a standalone daemon.</p>
<p>This means <code dir="ltr">llmster</code> can be run completely independently of the app and deployed anywhere: Linux boxes, cloud servers, your GPU rig, or even Google Colabs. It can of course still be run on your local machine without the GUI, for those who prefer terminal-based workflows.</p>
<h3 id="how-to-install-llmster" node="[object Object]"><span>How to install <code dir="ltr">llmster</code></span><a href="#how-to-install-llmster" aria-label="Link to this section"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p><strong node="[object Object]">Linux / Mac</strong></p>
<pre dir="ltr"><div><pre tabindex="0"><code><span><span>curl</span><span> -fsSL</span><span> https://lmstudio.ai/install.sh </span><span>|</span><span> bash</span></span>
<span></span></code></pre></div></pre>
<p><strong node="[object Object]">Windows</strong></p>
<pre dir="ltr"><div><pre tabindex="0"><code><span><span>irm</span><span> https://lmstudio.ai/install.ps1 </span><span>|</span><span> iex</span></span>
<span></span></code></pre></div></pre>
<h3 id="using-llmster" node="[object Object]"><span>Using <code dir="ltr">llmster</code></span><a href="#using-llmster" aria-label="Link to this section"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul node="[object Object]">
<li>Start the daemon: <code dir="ltr">lms daemon up</code></li>
<li>Download a model: <code dir="ltr">lms get &lt;model&gt;</code></li>
<li>Start the local server: <code dir="ltr">lms server start</code></li>
<li>Open an interactive session: <code dir="ltr">lms chat</code></li>
<li>Update your runtime: <code dir="ltr">lms runtime update llama.cpp</code> (and <code dir="ltr">lms runtime update mlx</code> on macOS)</li>
</ul>
<div><p><img src="https://lmstudio.ai/assets/blog/0.4.0/llmster@2x.png" alt="Llmster" data-caption=""></p></div>
<hr node="[object Object]">
<h2 id="parallel-requests" node="[object Object]"><span>Parallel Requests</span><a href="#parallel-requests" aria-label="Link to this section"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>Alongside LM Studio 0.4.0, our llama.cpp engine is graduating to version 2.0.0. With it we're introducing support for concurrent inference requests to the same model.</p>
<div><p><video loop="" controls="" muted="" playsinline="" data-caption="Run parallel requests in the app with Split View" caption="Run parallel requests in the app with Split View">
  <source src="https://files.lmstudio.ai/parallel-gui.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video></p><div><p>Run parallel requests in the app with Split View</p></div></div>
<h3 id="max-concurrent-predictions-and-unified-kv-cache" node="[object Object]"><span>Max Concurrent Predictions and Unified KV Cache</span><a href="#max-concurrent-predictions-and-unified-kv-cache" aria-label="Link to this section"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>You will find 2 new load options in the model loader dialog:</p>
<ul node="[object Object]">
<li>
<p><strong node="[object Object]">Max Concurrent Predictions</strong>: sets the maximum number of concurrent requests that can be processed by the model. Requests beyond this limit will be queued.</p>
</li>
<li>
<p><strong node="[object Object]">Unified KV Cache</strong>: when enabled, preallocated resources will not be hard-partitioned per concurrent request, allowing varying request sizes per request. This is enabled by default.</p>
</li>
</ul>
<p>Parallel requests work thanks to llama.cpp's open-source continuous batching implementation, adopted in LM Studio's llm-engine. This capability has not yet made it into our MLX engine, but it is actively in the works and will land soon.</p>
<hr node="[object Object]">
<h2 id="ui-refresh" node="[object Object]"><span>UI Refresh</span><a href="#ui-refresh" aria-label="Link to this section"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>We have refreshed LM Studio's user interface from the ground up for a more consistent and pleasant experience.</p>
<div><p><img src="https://lmstudio.ai/assets/blog/0.4.0/export@2x.png" alt="Export your chats" data-caption="Export your chats" caption="Export your chats"></p></div>
<h4>Export chats to PDF, markdown, or text</h4>
<p>You can now export your chats to PDF, markdown, or plain text. Click the ‚Ä¢‚Ä¢‚Ä¢ menu on a chat and head to "Export" for all available options.</p>
<h4>Split View</h4>
<p>You can now open multiple chat sessions side by side using Split View. Click the new Split View icon in the top right corner of the chat window to open a new chat pane.</p>
<h4>Developer Mode</h4>
<p>Developer Mode is a new setting that exposes advanced options in the app. You can enable it from Settings &gt; Developer. Once enabled, it'll reveal all advanced options across the app, including in the model loader dialog and sidebars.</p>
<h4>In-app docs</h4>
<p>Head over to the Developer tab to see the new in-app documentation. It covers the new REST API, CLI commands, and advanced configuration options.</p>
<hr node="[object Object]">
<h2 id="new-cli-experience" node="[object Object]"><span>New CLI Experience</span><a href="#new-cli-experience" aria-label="Link to this section"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<div><p><video loop="" controls="" muted="" playsinline="" data-caption="New CLI experience" caption="New CLI experience">
  <source src="https://files.lmstudio.ai/lms-chat.mov" type="video/mp4">
  Your browser does not support the video tag.
</video></p></div>
<h3 id="introducing-lms-chat" node="[object Object]"><span>Introducing <code dir="ltr">lms chat</code></span><a href="#introducing-lms-chat" aria-label="Link to this section"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>With LM Studio 0.4.0, we're introducing a brand-new CLI experience centered around the <code dir="ltr">lms chat</code> command. This command opens an interactive chat session directly in your terminal, allowing you to chat with your models and download new ones.</p>
<p>Run <code dir="ltr">lms chat --help</code> to see all available options.</p>
<hr node="[object Object]">
<h2 id="new-stateful-rest-api-endpoint-v1chat" node="[object Object]"><span>New stateful REST API endpoint: <code dir="ltr">/v1/chat</code></span><a href="#new-stateful-rest-api-endpoint-v1chat" aria-label="Link to this section"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<div><p><img src="https://lmstudio.ai/assets/blog/0.4.0/tokens@2x.png" alt="Server" data-caption=""></p></div>
<h3 id="v1chat-endpoint" node="[object Object]"><span><code dir="ltr">/v1/chat</code> endpoint</span><a href="#v1chat-endpoint" aria-label="Link to this section"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p><code dir="ltr">/v1/chat</code> is a new first-party REST endpoint for chatting with local models from your apps.</p>
<p>Unlike typical "stateless" chat APIs, <code dir="ltr">/v1/chat</code> is <strong node="[object Object]">stateful</strong>: you can start a conversation, get back a <code dir="ltr">response_id</code>, and then continue it by passing <code dir="ltr">previous_response_id</code> on your next request. This keeps requests small and makes it easy to build multi-step workflows on top of LM Studio.</p>
<p>Responses also include detailed stats (tokens in/out, speed, time to first token), so you can track performance and tune load/inference settings.</p>
<p>And when you need tools, <code dir="ltr">/v1/chat</code> can also enable your locally configured MCPs - gated by permission keys.</p>
<h3 id="permission-keys" node="[object Object]"><span>Permission keys</span><a href="#permission-keys" aria-label="Link to this section"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>To allow you to control which client accesses your LM Studio server, we've introduced permission keys. You can generate and manage permission keys from the Settings &gt; Server tab in the app.</p>
<h2 id="and-a-lot-more" node="[object Object]"><span>And a lot more!</span><a href="#and-a-lot-more" aria-label="Link to this section"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>Please let us know how you like it! We'd love to hear your feedback.</p>
<p>Special thanks to the 0.4.0 beta group. Your feedback and bug reports have been invaluable &lt;3.</p>
<p>Below is the full list of release notes items.</p>
<pre dir="ltr"><div><pre tabindex="0"><code><span><span>### LM Studio 0.4.0 - Release Notes</span></span>
<span></span>
<span><span>Welcome to LM Studio 0.4.0 üëæ!</span></span>
<span><span>- We're excited to introduce the next generation of LM Studio.</span></span>
<span><span>- New features include:</span></span>
<span><span>  - `llmster`: the LM Studio Daemon for headless deployments w/o GUI on servers or cloud instances</span></span>
<span><span>  - Parallel inference requests (instead of queued) for high throughput use cases</span></span>
<span><span>  - New stateful REST API with local MCP server support - `POST /v1/chat`</span></span>
<span><span>  - A completely revamped UI experience ‚ú®</span></span>
<span></span>
<span><span>**Build 17**</span></span>
<span></span>
<span><span>- MCPs will now only be loaded when needed, instead of at app startup</span></span>
<span><span>- Fixed a bug where some fields in app settings could get reset after update</span></span>
<span></span>
<span><span>**Build 16**</span></span>
<span></span>
<span><span>- New icons and placements for Discover, My Models buttons</span></span>
<span><span>- Fixed a bug where generators wouldn't show in the top bar model picker when selected</span></span>
<span><span>- Fixed a bug which prevented additional quantizations from being downloaded for staff pick models that were already downloaded</span></span>
<span><span>- Fixed a bug where `lms import` will sometimes not work properly if llmster (daemon) is also installed</span></span>
<span><span>- Fixed a bug in `/api/v1/chat` that caused server errors when inputs were empty or `top_k` exceeded 500</span></span>
<span><span>- Fixed a bug where `lms ls` and `lms load` sometimes would fail after waking up the LM Studio service</span></span>
<span><span>- Fixed a bug where sometimes token counting would not work properly for gpt-oss models</span></span>
<span></span>
<span><span>**Build 15**</span></span>
<span></span>
<span><span>- Introduce Parallel Requests with Continuous Batching üöÄ</span></span>
<span><span>  - When loading a model, you can now select n_parallel to allow multiple requests to be processed in parallel.</span></span>
<span><span>  - When enabled, instead of queuing requests one by one, the model will process up to N requests simultaneously.</span></span>
<span><span>  - By default, parallel slots are set to 4 (with unified KV set to true, which should result in no additional memory overhead).</span></span>
<span><span>  - This is supported for LM Studio's llama.cpp engine, with MLX coming later.</span></span>
<span><span>- Introducing Split View in Chat: view two chats side by side.</span></span>
<span><span>  - Drag and drop chat tabs to either half of the window to split the view.</span></span>
<span><span>  - Close one side of the split view with the 'x' button in the top right of each pane.</span></span>
<span><span>- Introducing üîß Developer Mode: a simplification of the previous Developer/Power User/User 3 mode switch.</span></span>
<span><span>  - Developer Mode combines the previous Developer and Power User modes into a single mode with all advanced features enabled.</span></span>
<span><span>  - You can turn on Developer Mode in Settings &gt; Developer.</span></span>
<span><span>- New setting: enforce allowing only one new empty chat at a time (default: enabled)</span></span>
<span><span>  - Change in Settings &gt; Chat</span></span>
<span><span>- New üî≠ Model Search experience</span></span>
<span><span>  - Access via the üîç button on the top right or by pressing Cmd/Ctrl + Shift + M</span></span>
<span><span>  - Model format filter preferences persist between app restarts</span></span>
<span><span>  - Modal is resizable and remembers its size between app restarts</span></span>
<span><span>- Limit number of open tabs to 1 per pane. Support showing 2 side-by-side chat tabs.</span></span>
<span><span>  - Selecting a new chat replaces the current tab in that pane.</span></span>
<span><span>- Add button to create a new chat in the sidebar</span></span>
<span><span>- Pressing Cmd/Ctrl + L while the model picker is open will dismiss it</span></span>
<span><span>- On narrow window size show right hand sidebar as an ephemeral overlay</span></span>
<span><span>- Support for the LFM2 tool call format</span></span>
<span><span>- CLI now uses commit hash for versioning instead of semantic version numbers</span></span>
<span><span>- Updates to UI details in hardware settings</span></span>
<span><span>- Fixed a bug where moving large number of conversations would sometimes only move part of them</span></span>
<span><span>- Fixed a bug where `lms ls` sometimes would show incomplete list of models on startup</span></span>
<span><span>- Fixed a bug in deleting tool confirmation preferences in settings</span></span>
<span><span>- Fixed a UI bug in app onboarding</span></span>
<span><span>- Fixed a visual bug in Models Table selected row affecting the Architecture and Format columns</span></span>
<span><span>- Fixed a bug where undoing pasted content in chat input would not work as expected</span></span>
<span><span>- Fixed a bug where a leading decimal in a numeric input would parse as a 0</span></span>
<span><span>- Fixed a bug rendering multiple images in a conversation message</span></span>
<span><span>- Fixed a bug where a documentation sidebar section would sometimes get stuck in expanded state</span></span>
<span><span>- Fixed a bug where chat names would sometimes be empty</span></span>
<span><span>- Fixed a visual bug in rendering keyboard shortcuts on Windows and Linux</span></span>
<span><span>- Fixed a bug where model loader would sometimes close due to mouse move shortly after opening</span></span>
<span><span>- Fixed a bug rendering titles in preset conflict resolver dialog</span></span>
<span><span>- Fixed a bug where reloading with new load parameters would not apply next time the same model is used for a chat</span></span>
<span><span>- Fixed a bug where the model loading will get stuck if the cpu moe slider is maxed out</span></span>
<span><span>- Fixed a bug where exporting chats with very large images to PDF would fail</span></span>
<span><span>- Fixed a responsive UI overlap bug in the app header</span></span>
<span><span>- [Windows] Fixed a bug where the default embedding model will not be available after in-app update</span></span>
<span><span>- Adds download, copy, and reveal in working directory buttons to generated images in chat</span></span>
<span></span>
<span><span>**Build 14**</span></span>
<span></span>
<span><span>- (Build 14 was skipped)</span></span>
<span></span>
<span><span>**Build 13**</span></span>
<span></span>
<span><span>- App setting to control primary navigation position: 'top' or 'left'</span></span>
<span><span>- [Mac] New tray menu icon üëæ (experimental, might change)</span></span>
<span><span>- `/api/v1` endpoints and `/v1/responses` API now return better formatted errors</span></span>
<span><span>- Significantly reduce the size of the app update asset</span></span>
<span></span>
<span><span>**Build 12**</span></span>
<span></span>
<span><span>- Bugfix: New chats to be created with the same model as the previously focused chat</span></span>
<span><span>- Bring back gear button to change load parameters for currently loaded model</span></span>
<span><span>- Bring back context fullness indicator and current input token counter</span></span>
<span><span>- New in My Models: right-click on tab header to choose which columns to show/hide</span></span>
<span><span>- New in My Models: Capabilities and Format columns</span></span>
<span><span>- Fixed a flicker in model picker floating panel upon first open</span></span>
<span><span>  - P.S. you can open the model picker from anywhere in the app with Cmd/Ctrl + L</span></span>
<span><span>- Fixed focus + Enter on Eject button not working inside model picker</span></span>
<span><span>- Updated chat terminal and messages colors and style</span></span>
<span><span>- Fixed dragging and dropping chats/folders in the sidebar</span></span>
<span></span>
<span><span>**Build 11**</span></span>
<span></span>
<span><span>- ‚ú®üëæ Completely revamped UI - this is a work in progress, give us feedback!</span></span>
<span><span>- [CLI] New `lms chat` experience!</span></span>
<span><span>  - Support slash commands, thinking highlighting and pasting larger content</span></span>
<span><span>  - Slash commands available: /model, /download, /system-prompt, /help and /exit</span></span>
<span><span>- [CLI] New: `lms runtime survey` to print info about available GPUs!</span></span>
<span><span>- FunctionGemma support</span></span>
<span><span>- Added a slider to control n_cpu_moe</span></span>
<span><span>- New REST API endpoint: `api/v1/models/unload` to unload models</span></span>
<span><span>- Breaking change: in `api/v1/models/load` endpoint response, introduced in this beta, `model_instance_id` has been renamed to `instance_id`.</span></span>
<span><span>- Display live processing status for each loaded LLM on the Developer page</span></span>
<span><span>  - Prompt processing progress percentage ‚Üí token generation count</span></span>
<span><span>- Improved PDF rendering quality for tool requests and responses</span></span>
<span><span>- Significantly increased the reliability and speed of deleting multiple chats at once</span></span>
<span><span>- Updated style of chat message generation info</span></span>
<span><span>- Updated layout of Hardware settings page and other settings rows</span></span>
<span><span>- Fixed a bug where sometimes models are indexed before all files are downloaded</span></span>
<span><span>- Fixed a bug where exporting larger PDFs would sometimes fail</span></span>
<span><span>- Fixed a bug where pressing the chat clear hotkey multiple times would open multiple confirmation dialogs</span></span>
<span><span>- Fixed a bug where pressing the chat clear hotkey would sometimes duplicate the chat</span></span>
<span><span>- Fixed a bug where pressing the duplicate hotkey on the release notes would create a glitched chat tab</span></span>
<span><span>- Fixed a bug where `lms help` would not work</span></span>
<span><span>- Fixed a bug where deleting models or canceling downloads would leave behind empty folders</span></span>
<span><span>- Fixed a styling bug in the GPU section on the Hardware page</span></span>
<span><span>- [MLX] Fixed a bug where the bf16 model format was not recognized as a valid quantization</span></span>
<span></span>
<span><span>**Build 10**</span></span>
<span></span>
<span><span>- (Build 10 was skipped)</span></span>
<span></span>
<span><span>**Build 9**</span></span>
<span></span>
<span><span>- (Build 9 was skipped)</span></span>
<span></span>
<span><span>**Build 8**</span></span>
<span></span>
<span><span>- Fixed a bug where the default system prompt was still sent to the model even after the system prompt field was cleared.</span></span>
<span><span>- Fixed a bug where exported chats did not include the correct system prompt.</span></span>
<span><span>- Fixed a bug where the token count was incorrect when a default system prompt existed but the system prompt field was cleared.</span></span>
<span><span>- Fix a bug where sometimes the tool call results are not being added to the context correctly</span></span>
<span><span>- Fix chat clearing with hotkey (Cmd/Ctrl + Shift + Option/Alt + D) would clear wrong chat</span></span>
<span><span>- Fix a bug where Ctrl/Cmd + N would sometimes create two new chats</span></span>
<span><span>- Updated style for Integrations panel and select</span></span>
<span><span>- Fixed cURL copy button for embedding models displaying additional incorrect requests</span></span>
<span><span>- Fix "ghost chats" caused by moving conversations/deleting conversations</span></span>
<span></span>
<span><span>**Build 7**</span></span>
<span></span>
<span><span>- Fix jinja prompt formatting bug for some models where EOS tokens were not being included properly</span></span>
<span><span>- Bring back release notes viewer for Runtime available update</span></span>
<span><span>- Prevent tooltip from staying open when hovering tooltip content</span></span>
<span><span>- Fix a bug in deleting multiple chats at once</span></span>
<span><span>- Minor fix to overlapping labels in model loader</span></span>
<span><span>- Support for EssentialAI's rnj-1 model</span></span>
<span></span>
<span><span>**Build 6**</span></span>
<span></span>
<span><span>- Fixed a bug where Qwen3-Next user messages would not appear in formatted prompts properly</span></span>
<span></span>
<span><span>**Build 5**</span></span>
<span></span>
<span><span>- Fixed a bug where quickly deleting multiple conversations will sometimes soft-lock the app</span></span>
<span><span>- Fixed another bug that prevented the last remaining open tab from being closed</span></span>
<span></span>
<span><span>**Build 4**</span></span>
<span></span>
<span><span>- Fixed a bug where the last remaining open tab sometimes could not be closed</span></span>
<span><span>- Fixed a bug where `lms log stream` would exit immediately</span></span>
<span><span>- Fixed a bug where the server port would get printed as [object Object]</span></span>
<span><span>- Image validation checks in `v1/chat` and `v1/responses` REST API now run without model loading</span></span>
<span><span>- Fixed a bug where images without extensions were not classified correctly</span></span>
<span><span>- Fix bug in move-to-trash onboarding dialog radio selection where some parts of the label were not clickable</span></span>
<span><span>- Fix several clickable areas bugs in Settings windows buttons</span></span>
<span><span>- Fixed a bug where certain settings may get adjusted unexpectedly when using llmster (for example, the JIT model loading may become disabled)</span></span>
<span><span>- New and improved Runtime page style and structure</span></span>
<span><span>- Fixes a bug where guardrail settings were not showing up in User UI mode</span></span>
<span><span>- Fixed a bug where `lms log stream` would exit immediately</span></span>
<span></span>
<span><span>**Build 3**</span></span>
<span></span>
<span><span>- Introducing 'llmster': the LM Studio Daemon!</span></span>
<span><span>  - True headless, no GUI version of the process that powers LM Studio</span></span>
<span><span>  - Run it on servers, cloud instances, or any machine without a graphical interface</span></span>
<span><span>  - Load models on CPU/GPU and serve them, use via `lms` CLI or our APIs</span></span>
<span><span>  - To install:</span></span>
<span><span>    - Linux/Mac: `curl -fsSL https://lmstudio.ai/install.sh | bash`</span></span>
<span><span>    - Windows: `irm https://lmstudio.ai/install.ps1 | iex`</span></span>
<span><span>- Support for MistralAI Ministral models (3B, 8B, 13B)</span></span>
<span><span>- Improved `lms` output and help messages style. Run `lms --help` to explore!</span></span>
<span><span>- Get llama.cpp level logs with `lms log stream -s runtime` in the terminal</span></span>
<span><span>- `lms get` interactive mode now shows the latest model catalog options</span></span>
<span><span>- New and improved style for Downloads panel</span></span>
<span><span>- New and improved style for App Settings</span></span>
<span><span>- We're trying something out: Model search now in its own tab</span></span>
<span><span>  - still iterating on the UI for this page, please give us feedback!</span></span>
<span></span>
<span><span>**Build 2**</span></span>
<span></span>
<span><span>- Show release notes in a dedicated tab after app updates</span></span>
<span><span>- Add support to display images in exported PDFs and exported markdown files</span></span>
<span><span>- Quick Docs is now Developer Docs, with refreshed documentation and direct access from the welcome page.</span></span>
<span><span>- Allow creating permission tokens without allowed MCP permissions</span></span>
<span><span>- Fixed a bug where sometimes images created by MCPs are not showing up</span></span>
<span><span>- Fixed a bug where sometimes the plugin chips not working</span></span>
<span><span>- Fixed a bug where the "thinking" blocks will sometimes expand erroneously</span></span>
<span><span>- Fixed a bug where certain tabs would not open correctly</span></span>
<span><span>- Fixed a bug where sometimes the model list would not load</span></span>
<span><span>- Fixed a bug where in-app docs article titles would sometimes wiggle on scroll</span></span>
<span><span>- Fixed a visual bug in Preset 'resolve conflicts' modal</span></span>
<span><span>- Fixed a bug where sometimes Download button would continue showing for an already downloaded model</span></span>
<span><span>- Fixed a bug where chat sidebar buttons wouldn't be visible on narrow screens</span></span>
<span><span>- Display model indexing errors as buttons rather than hints</span></span>
<span></span>
<span><span>**Build 1**</span></span>
<span></span>
<span><span>- Welcome to the 0.4.0 Beta!</span></span>
<span></span>
<span></span></code></pre></div></pre>
<hr node="[object Object]">
<h3 id="resources" node="[object Object]"><span>Resources</span><a href="#resources" aria-label="Link to this section"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul node="[object Object]">
<li>We are hiring! Check out our <a href="https://lmstudio.ai/careers" node="[object Object]" target="_blank" rel="noopener noreferrer">careers page</a> for open roles.</li>
<li>Download LM Studio: <a href="https://lmstudio.ai/download" node="[object Object]" target="_blank" rel="noopener noreferrer">lmstudio.ai/download</a></li>
<li>Report bugs: <a href="https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues" node="[object Object]" target="_blank" rel="noopener noreferrer">lmstudio-bug-tracker</a></li>
<li>X / Twitter: <a href="https://twitter.com/lmstudio" node="[object Object]" target="_blank" rel="noopener noreferrer">@lmstudio</a></li>
<li>Discord: <a href="https://discord.gg/lmstudio" node="[object Object]" target="_blank" rel="noopener noreferrer">LM Studio Community</a></li>
</ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[That's Not How Email Works, HSBC (281 pts)]]></title>
            <link>https://danq.me/2026/01/28/hsbc-dont-understand-email/</link>
            <guid>46799304</guid>
            <pubDate>Wed, 28 Jan 2026 18:12:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://danq.me/2026/01/28/hsbc-dont-understand-email/">https://danq.me/2026/01/28/hsbc-dont-understand-email/</a>, See on <a href="https://news.ycombinator.com/item?id=46799304">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-27343" data-post-id="27343">
          
          <div>
            <p>
              I have a credit card with HSBC<sup><a id="footnote-ref-27343-1" name="footnote-ref-27343-1" href="#footnote-27343-1" title="Long, long ago I also had a current account with HSBC which I forgot to close when I switched banks‚Ä¶ 20 years ago‚Ä¶ and I possibly still owe them for the six pence the account was in debt at the time.">1</a></sup>:
              you know, <a href="https://danq.me/2004/07/19/hsbc-advertising/">the bank with virtue-signalling multiculturalism</a> in their ads.
            </p>
            <p>
              Not long ago I received a letter from them telling me that emails to me were being ‚Äúreturned undelivered‚Äù and they needed me to update the email address on my account.
            </p>
            <h2>
              ‚ÄúWhat‚Äôs happening?‚Äù
            </h2>
            <figure id="attachment_27344" aria-describedby="caption-attachment-27344">
              <a href="#lightbox-p-attachment_27344" title="Zoom in on image" aria-haspopup="dialog" role="button"><img decoding="async" fetchpriority="high" src="https://bcdn.danq.me/_q23u/2025/09/20250930_131403-640x360.jpg" alt="Posted letter from HSBC saying that emails to me have been returned undelivered, held in front of a screen showing a recent email from HSBC sitting in my Inbox." width="640" height="360" srcset="https://bcdn.danq.me/_q23u/2025/09/20250930_131403-640x360.jpg 640w, https://danq.me/_q23u/2025/09/20250930_131403-477x269.jpg 477w" sizes="100vw"></a>
              <figcaption id="caption-attachment-27344">
                I don‚Äôt know what emails are being ‚Äúreturned undelivered‚Äù to HSBC, but it isn‚Äôt any of the ones sitting, read, in my email client.
              </figcaption>
            </figure>
            <p>
              I logged into my account, per the instructions in the letter, and discovered <strong>my correct email address already right there</strong>, much to my‚Ä¶ lack of surprise<sup><a id="footnote-ref-27343-2" name="footnote-ref-27343-2" href="#footnote-27343-2" title="After all, I‚Äôd been reading their emails!">2</a></sup>.
            </p>
            <p>
              So I kicked off a live chat via their app, with an agent called Ankitha. Over the course of a drawn-out hour-long conversation, they repeatedly told to tell me <em>how</em> to update my
              email address (which was never my question). Eventually, when they understood that <em>my email address was already correct</em>, then they concluded the call, saying (emphasis mine):
            </p>
            <blockquote>
              <p>
                I can understand your frustration, but <strong>if the bank has sent the letter, you will have to update the e-mail address</strong>.
              </p>
            </blockquote>
            <p>
              This is the point at which a normal person would probably just change the email address in their online banking to a ‚Äúspare‚Äù email address.
            </p>
            <p>
              But aside from the fact that I‚Äôd rather not<sup><a id="footnote-ref-27343-3" name="footnote-ref-27343-3" href="#footnote-27343-3" title="After all, as I‚Äôll stress again: the email address HSBC have for me, and are using, is already correct.">3</a></sup>, by this point I‚Äôd caught the scent of a deeper underlying issue.
              After all, didn‚Äôt I have <a href="https://danq.me/2021/11/16/email-tracking-and-paperless-banking/">a conversation a&nbsp;<em>little</em> like this one but with a different bank</a>, about four years
              ago?
            </p>
            <figure id="attachment_27348" aria-describedby="caption-attachment-27348">
              <a href="#lightbox-p-attachment_27348" title="Zoom in on image" aria-haspopup="dialog" role="button"><img decoding="async" src="https://bcdn.danq.me/_q23u/2025/09/hsbc-say-no-you-have-to-change-your-email-address-actually-640x360.jpg" alt="Phone screen showing a live chat interface. The other party says &quot;I can understand your frustration, but if the bank has sent the letter, you will have to update the e-mail address.&quot; and then &quot;Thank you for being so understanding and patiently waiting. Thank you for contacting HSBC, if there is anything else you need please feel free to come back to us. Have a pleasant rest of the day.&quot;, before ending the conversation." width="640" height="360" srcset="https://bcdn.danq.me/_q23u/2025/09/hsbc-say-no-you-have-to-change-your-email-address-actually-640x360.jpg 640w, https://danq.me/_q23u/2025/09/hsbc-say-no-you-have-to-change-your-email-address-actually-1280x720.jpg 1280w, https://danq.me/_q23u/2025/09/hsbc-say-no-you-have-to-change-your-email-address-actually-980x551.jpg 980w, https://danq.me/_q23u/2025/09/hsbc-say-no-you-have-to-change-your-email-address-actually-477x269.jpg 477w" sizes="100vw"></a>
              <figcaption id="caption-attachment-27348">
                Perhaps I should be grateful that they didn‚Äôt say that I have to change <a href="https://danq.me/2007/03/08/q/">my name</a>, which can sometimes&nbsp; be significantly more awkward than my email
                address‚Ä¶
              </figcaption>
            </figure>
            <p>
              So I called Customer Services directly<sup><a id="footnote-ref-27343-4" name="footnote-ref-27343-4" href="#footnote-27343-4" title="In future, I‚Äôll just do this in the first instance. The benefits of live chat being able to be done ‚Äúin the background‚Äù while one gets on with some work are totally outweighed when the entire exchange takes an hour only to reach an unsatisfactory conclusion, whereas a telephone call got things sorted (well hopefully‚Ä¶) within 10 minutes.">4</a></sup>,
              who told me that <strong>if my email address is already correct then I can ignore their letter</strong>.
            </p>
            <p>
              I suggested that perhaps their letter template might need updating so it doesn‚Äôt say ‚Äúaction required‚Äù if action is <em>not</em> required. Or that perhaps what they mean to say is
              ‚Äúaction required: check your email address is correct‚Äù.
            </p>
            <figure id="attachment_27349" aria-describedby="caption-attachment-27349">
              <a href="#lightbox-p-attachment_27349" title="Zoom in on image" aria-haspopup="dialog" role="button"><img decoding="async" src="https://bcdn.danq.me/_q23u/2025/09/hsbc-letter-email-corrected1.png" alt="Edited version of the letter, now saying 'What's happening? We need to ensure that the email address we're using for you is correct' and 'Action required: Please check that you've been receiving our emails and that the address in your account is correct'." width="640" height="360" srcset="https://bcdn.danq.me/_q23u/2025/09/hsbc-letter-email-corrected1.png 640w, https://danq.me/_q23u/2025/09/hsbc-letter-email-corrected1-477x269.png 477w" sizes="100vw"></a>
              <figcaption id="caption-attachment-27349">
                Say what you mean, HSBC! I‚Äôve suggested an improvement to your letter template.
              </figcaption>
            </figure>
            <p>
              So anyway, apparently everything‚Äôs fine‚Ä¶ although I reserved final judgement until I‚Äôd seen that they were still sending me emails!
            </p>
            <h2>
              ‚ÄúAction required‚Äù
            </h2>
            <p>
              I think I can place a solid guess about what went wrong here. But it makes me feel like we‚Äôre living in the Darkest Timeline.
            </p>
            <figure id="attachment_27351" aria-describedby="caption-attachment-27351">
              <a href="#lightbox-p-attachment_27351" title="Zoom in on image" aria-haspopup="dialog" role="button"><img decoding="async" loading="lazy" src="https://bcdn.danq.me/_q23u/2025/09/The_darkest_timeline1.gif" alt="Scene from Community episode 'Remedial Chaos Theory'. Pierce lies injured on the floor, tended to by Annie and Abed, while Jeff swings a flaming blanket around his head. Troy stands in shock at the door, holding a pile of pizza boxes." width="500" height="281"></a>
              <figcaption id="caption-attachment-27351">
                You know the one I mean. Somebody rolled a ‚Äò1‚Äô, didn‚Äôt they‚Ä¶
              </figcaption>
            </figure>
            <p>
              I dissected HSBC‚Äôs latest email to me: it was of the ‚Äúyour latest statement is available‚Äù variety. Deep within the email, down at the bottom, is this code:
            </p>
            <figure>
              <div>
                <pre>&lt;<span>img</span> <span>src</span><span>=</span><span>"http://www.email1.hsbc.co.uk:8080/Tm90IHRoZSByZWFsIEhTQkMgcGF5bG9hZA=="</span>
   <span>width</span><span>=</span><span>"1"</span>
  <span>height</span><span>=</span><span>"1"</span>
     <span>alt</span><span>=</span><span>""</span>&gt;

&lt;<span>img</span> <span>src</span><span>=</span><span>"http://www.email1.hsbc.co.uk:8080/QWxzbyBub3QgcmVhbCBIU0JDIHBheWxvYWQ="</span>
   <span>width</span><span>=</span><span>"1"</span>
  <span>height</span><span>=</span><span>"1"</span>
     <span>alt</span><span>=</span><span>""</span>&gt;
</pre>
              </div>
            </figure>
            <p>
              What you‚Äôre seeing are two <em>tracking pixels</em>: tiny 1√ó1 pixel images, usually transparent or white-on-white to make them even-more invisible, used to surreptitiously track when
              somebody reads an email. When you open an email from HSBC ‚Äì potentially&nbsp;<em>every</em> time you open an email from them ‚Äì your email client connects to those web addresses to get
              the necessary images. The code at the end of each identifies the email they were contained within, which in turn can be linked back to the recipient.
            </p>
            <p>
              You know how invasive a read-receipt feels? Tracking pixels are like those‚Ä¶ but turned up to eleven. While a read-receipt only says ‚Äúthe recipient read this email‚Äù (usually only after
              the recipient gives consent for it to do so), a tracking pixel can often track <em>when</em> and&nbsp;<em>how often</em> you refer to an email<sup><a id="footnote-ref-27343-5" name="footnote-ref-27343-5" href="#footnote-27343-5" title="A tracking pixel can also collect additional personal information about you, such as your IP address at the time that you opened the email, which might disclose your location.">5</a></sup>.
            </p>
            <p>
              If I re-read a year-old email from HSBC, they‚Äôre saying that they want to know about it.
            </p>
            <p>
              But it gets worse. Because HSBC are using <code>http://</code>, rather than <code>https://</code> URLs for their tracking pixels, they‚Äôre also saying that every time you read an email
              from them, they‚Äôd like <em>everybody on the same network as you</em> to be able to know that you did so, too. If you‚Äôre at my house, on my WiFi, and you open an email from HSBC, not
              only might HSBC know about it, but <em>I</em> might know about it too.
            </p>
            <p>
              An easily-avoidable security failure there, HSBC‚Ä¶ which isn‚Äôt the kind of thing one hopes to hear about a bank!
            </p>
            <figure id="attachment_27353" aria-describedby="caption-attachment-27353">
              <a href="#lightbox-p-attachment_27353" title="Zoom in on image" aria-haspopup="dialog" role="button"><img decoding="async" loading="lazy" src="https://bcdn.danq.me/_q23u/2025/09/hsbc-tracking-pixel-highlight-zoomin-640x229.gif" alt="Zoom-in animation showing two tracking pixels at the bottom of an email, rendered visible in red and blue." width="640" height="229" srcset="https://bcdn.danq.me/_q23u/2025/09/hsbc-tracking-pixel-highlight-zoomin-640x229.gif 640w, https://danq.me/_q23u/2025/09/hsbc-tracking-pixel-highlight-zoomin-980x351.gif 980w" sizes="100vw"></a>
              <figcaption id="caption-attachment-27353">
                Tracking pixels are usually invisible, so I turned these ones visible so you can see where they hide.
              </figcaption>
            </figure>
            <p>
              But‚Ä¶ tracking pixels don‚Äôt actually work. At least, they doesn‚Äôt work <em>on me</em>. Like many privacy-conscious individuals, my devices are configured to block tracking pixels (and a
              variety of other instruments of surveillance capitalism) right out of the gate.
            </p>
            <p>
              This means that even though I <em>do</em> read most of the non-spam email that lands in my Inbox, the sender <em>doesn‚Äôt get to know that I did</em> so unless I choose to tell them.
              This is the way that email was <em>designed</em> to work, and is the only way that a sender can be confident that it <em>will</em> work.
            </p>
            <p>
              But we‚Äôre in the Darkest Timeline. <strong><a href="https://www.bbc.co.uk/news/technology-56071437">Tracking pixels have become so endemic</a> that HSBC have clearly come to the opinion
              that&nbsp;if they can‚Äôt track when I open their emails, I must not be receiving their emails</strong>. So they wrote me a letter to tell me that my emails have been ‚Äúreturned
              undelivered‚Äù (which seems to be an outright lie).
            </p>
            <p>
              Surveillance capitalism has become so ubiquitous that it‚Äôs become transparent. Transparent like the invisible spies at the bottom of your bank‚Äôs emails.
            </p>
            <figure id="attachment_27356" aria-describedby="caption-attachment-27356">
              <a href="#lightbox-p-attachment_27356" title="Zoom in on image" aria-haspopup="dialog" role="button"><img decoding="async" loading="lazy" src="https://bcdn.danq.me/_q23u/2025/09/hsbc-letter-email-corrected2.png" alt="The letter from HSBC again, but this time corrected to say 'We cannot conceive that there's anybody left who hasn't given up on trying to fight back against surveillance capitalism. Action required: turn off your privacy software so we can watch you read our emails. (We'll be letting anybody you live with read them too.)" width="640" height="360" srcset="https://bcdn.danq.me/_q23u/2025/09/hsbc-letter-email-corrected2.png 640w, https://danq.me/_q23u/2025/09/hsbc-letter-email-corrected2-477x269.png 477w" sizes="100vw"></a>
              <figcaption id="caption-attachment-27356">
                I‚Äôve changed my mind. Maybe <em>this</em> is what HSBC‚Äôs letter should have said.
              </figcaption>
            </figure>
            <p>
              So in summary, with only a little speculation:
            </p>
            <ol>
              <li>
                <strong>Surveillance capitalism became widespread</strong> enough that HSBC came to assume that tracking pixels have bulletproof reliability.
              </li>
              <li>
                <strong>HSBC started using tracking pixels</strong> them to check whether emails are being received (even though that‚Äôs not what they do when they&nbsp;<em>are</em> reliable, which
                they‚Äôre not).
                <ul>
                  <li>(Oh, and their tracking pixels are badly-implemented, if they worked they‚Äôd ‚Äúleak‚Äù data to other people on my network<sup><a id="footnote-ref-27343-6" name="footnote-ref-27343-6" href="#footnote-27343-6" title="It could be even worse still, actually! A sophisticated attacker could ‚Äúinject‚Äù images into the bottom of a HSBC email; those images could, for example, be pictures of text saying things like ‚ÄúYou need to urgently call HSBC on [attacker‚Äôs phone number].‚Äù This would allow a scammer to hijack a legitimate HSBC email by injecting their own content into the bottom of it. Seriously, HSBC, you ought to fix this.">6</a></sup>.)
                  </li>
                </ul>
              </li>
              <li>
                <strong>Eventually, HSBC assumed their tracking was bulletproof.</strong> Because HSBC couldn‚Äôt track how often, when, and where I was reading their emails‚Ä¶ they posted me a letter to
                tell me I needed to change my email address.
              </li>
            </ol>
            <h2>
              What do I think HSBC should do?
            </h2>
            <p>
              Instead of sending me a misleading letter about undelivered emails, perhaps a better approach for HSBC could be:
            </p>
            <ol>
              <li>
                <strong>At an absolute minimum</strong><strong>, stop using unencrypted connections for tracking pixels</strong>. I do not want to open a bank email on a cafe‚Äôs public WiFi and have
                <em>everybody in the cafe</em> potentially know who I bank with‚Ä¶ and that I just opened an email from them! I certainly don‚Äôt want attackers injecting content into the bottom of
                legitimate emails.
              </li>
              <li>
                <strong>Stop assuming that if somebody blocks your attempts to spy on them via your emails, it means they‚Äôre not getting your emails</strong>. It doesn‚Äôt mean that. It‚Äôs never meant
                that. There are all kinds of reasons that your tracking pixels might not work, and they‚Äôre not even all privacy-related reasons!
              </li>
              <li>
                <strong>Or, better yet: just stop trying to surveil your customers‚Äô email habits in the first place?</strong> You already sit on a wealth of personal and financial information which
                you can, and probably do, data-mine for your own benefit. Can you at least <em>try</em> to pay lip service to <a href="https://www.hsbc.com/-/files/hsbc/our-approach/risk-and-responsibility/pdfs/240715-hsbc-principles-for-the-ethical-use-of-data-and-ai.pdf">your own published principles on the
                ethical use of data</a> and, if I may quote them, ‚Äúuse only that data which is appropriate for the purpose‚Äù and ‚Äúembed privacy considerations into design and approval processes‚Äù.
              </li>
              <li>
                <strong>If you need to check that an email address is valid, do&nbsp;<em>that</em>, not an unreliable proxy for it</strong>. Instead of this letter, you could have sent an email that
                said ‚ÄúWe need to check that you‚Äôre receiving our emails. Please click this link to confirm that you are.‚Äù This not only achieves informed consent for your tracking, but it can be
                more-secure too because you can authenticate the user during the process.
              </li>
            </ol>
            <p>
              Also, to quote your own principles once more: when you make a mistake like assuming your spying is a flawless way to detect the validity of email addresses, perhaps you should ‚Äúbe
              transparent with our customers and other stakeholders about how we use their data‚Äù.
            </p>
            <p>
              Wouldn‚Äôt that be better than writing to a customer to say that their emails are being returned undelivered (when they‚Äôre not)‚Ä¶ and then having your staff tell them that having received
              such an email they have no choice but to change the email address they use (which is then disputed by your <em>other</em> staff)?
            </p>
            <p>
              <code>&lt;/rant&gt;</code>
            </p>
            
            <dialog id="lightbox-attachment_27344">
              <p id="lightbox-p-attachment_27344">
                <a href="https://bcdn.danq.me/_q23u/2025/09/20250930_131403.jpg"><img decoding="async" fetchpriority="high" src="https://bcdn.danq.me/_q23u/2025/09/20250930_131403.jpg" alt="Posted letter from HSBC saying that emails to me have been returned undelivered, held in front of a screen showing a recent email from HSBC sitting in my Inbox." width="640" height="360" sizes="100vw" loading="lazy"></a>
              </p><a href="#attachment_27344" title="Close image" role="button">√ó</a>
            </dialog>
            <dialog id="lightbox-attachment_27348">
              <p id="lightbox-p-attachment_27348">
                <a href="https://bcdn.danq.me/_q23u/2025/09/hsbc-say-no-you-have-to-change-your-email-address-actually.jpg"><img decoding="async" src="https://bcdn.danq.me/_q23u/2025/09/hsbc-say-no-you-have-to-change-your-email-address-actually.jpg" alt="Phone screen showing a live chat interface. The other party says &quot;I can understand your frustration, but if the bank has sent the letter, you will have to update the e-mail address.&quot; and then &quot;Thank you for being so understanding and patiently waiting. Thank you for contacting HSBC, if there is anything else you need please feel free to come back to us. Have a pleasant rest of the day.&quot;, before ending the conversation." width="640" height="360" sizes="100vw" loading="lazy"></a>
              </p><a href="#attachment_27348" title="Close image" role="button">√ó</a>
            </dialog>
            <dialog id="lightbox-attachment_27349">
              <p id="lightbox-p-attachment_27349">
                <a href="https://bcdn.danq.me/_q23u/2025/09/hsbc-letter-email-corrected1.png"><img decoding="async" src="https://bcdn.danq.me/_q23u/2025/09/hsbc-letter-email-corrected1.png" alt="Edited version of the letter, now saying 'What's happening? We need to ensure that the email address we're using for you is correct' and 'Action required: Please check that you've been receiving our emails and that the address in your account is correct'." width="640" height="360" sizes="100vw" loading="lazy"></a>
              </p><a href="#attachment_27349" title="Close image" role="button">√ó</a>
            </dialog>
            <dialog id="lightbox-attachment_27351">
              <p id="lightbox-p-attachment_27351">
                <a href="https://bcdn.danq.me/_q23u/2025/09/The_darkest_timeline1.gif"><img decoding="async" loading="lazy" src="https://bcdn.danq.me/_q23u/2025/09/The_darkest_timeline1.gif" alt="Scene from Community episode 'Remedial Chaos Theory'. Pierce lies injured on the floor, tended to by Annie and Abed, while Jeff swings a flaming blanket around his head. Troy stands in shock at the door, holding a pile of pizza boxes." width="500" height="281"></a>
              </p><a href="#attachment_27351" title="Close image" role="button">√ó</a>
            </dialog>
            <dialog id="lightbox-attachment_27353">
              <p id="lightbox-p-attachment_27353">
                <a href="https://bcdn.danq.me/_q23u/2025/09/hsbc-tracking-pixel-highlight-zoomin.gif"><img decoding="async" loading="lazy" src="https://bcdn.danq.me/_q23u/2025/09/hsbc-tracking-pixel-highlight-zoomin.gif" alt="Zoom-in animation showing two tracking pixels at the bottom of an email, rendered visible in red and blue." width="640" height="229" sizes="100vw"></a>
              </p><a href="#attachment_27353" title="Close image" role="button">√ó</a>
            </dialog>
            <dialog id="lightbox-attachment_27356">
              <p id="lightbox-p-attachment_27356">
                <a href="https://bcdn.danq.me/_q23u/2025/09/hsbc-letter-email-corrected2.png"><img decoding="async" loading="lazy" src="https://bcdn.danq.me/_q23u/2025/09/hsbc-letter-email-corrected2.png" alt="The letter from HSBC again, but this time corrected to say 'We cannot conceive that there's anybody left who hasn't given up on trying to fight back against surveillance capitalism. Action required: turn off your privacy software so we can watch you read our emails. (We'll be letting anybody you live with read them too.)" width="640" height="360" sizes="100vw"></a>
              </p><a href="#attachment_27356" title="Close image" role="button">√ó</a>
            </dialog>
            
          </div>
          
        </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Computer History Museum Launches Digital Portal to Its Collection (171 pts)]]></title>
            <link>https://computerhistory.org/press-releases/computer-history-museum-launches-digital-portal-to-its-vast-collection/</link>
            <guid>46798994</guid>
            <pubDate>Wed, 28 Jan 2026 17:54:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://computerhistory.org/press-releases/computer-history-museum-launches-digital-portal-to-its-vast-collection/">https://computerhistory.org/press-releases/computer-history-museum-launches-digital-portal-to-its-vast-collection/</a>, See on <a href="https://news.ycombinator.com/item?id=46798994">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><strong>Gordon and Betty Moore Foundation Funded OpenCHM to Digitize One-of-a-Kind Archive </strong></p><div>
	<p><b><span data-contrast="none">MOUNTAIN VIEW, Calif. <span lang="EN-US" xml:lang="EN-US" data-contrast="none"><span>‚Äì</span></span> </span></b><span data-contrast="none">January&nbsp;21, 2026 <span lang="EN-US" xml:lang="EN-US" data-contrast="none"><span>‚Äì</span></span> The&nbsp;</span><a href="https://computerhistory.org/"><span data-contrast="none">Computer History Museum</span></a><span data-contrast="none"> (CHM), a leader in decoding technology‚Äîits computing past, digital present, and future impact on humanity</span><span data-contrast="none">‚Äîannounced the launch of OpenCHM, a new digital portal providing global access to its unparalleled collection. </span></p>
<p><span data-contrast="none">‚ÄúOpenCHM&nbsp;is designed to inspire discovery, spark curiosity, and make the stories of the digital age more accessible to everyone, everywhere,‚Äù said CHM President and CEO Marc Etkind. ‚ÄúWe‚Äôre unlocking the collection for new audiences to explore.‚Äù</span><span data-ccp-props="{}">&nbsp;</span></p>
<p>OpenCHM is funded by the Gordon and Betty Moore Foundation and other generous donors, and this launch represents a major milestone in CHM's multi-year digitization initiative. Designed in collaboration with KeepThinking, the portal is powered by their innovative Qi collection management system.</p>
<p><b><span data-contrast="none">‚ÄúWe were excited by the prospect of CHM opening up their unique collections to broader audiences, from scholars and teachers to students and the public. The balance of the engaging, curated narratives by CHM‚Äôs own historians and field experts along with the tools and capabilities to explore one‚Äôs own interests makes the platform truly compelling. The Moore Foundation also values the OpenCHM team's </span></b><b><span data-contrast="none">commitment to thoughtful design and documentation, which we hope will inspire and enable other organizations to share their collections more openly</span></b><b><span data-contrast="none">.‚Äù‚ÄîJanet Coffey, Program Director, Science, Gordon and Betty Moore Foundation</span></b><span data-ccp-props="{&quot;134233117&quot;:false,&quot;134233118&quot;:false,&quot;201341983&quot;:0,&quot;335551550&quot;:1,&quot;335551620&quot;:1,&quot;335559685&quot;:0,&quot;335559737&quot;:0,&quot;335559738&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:279}">&nbsp;</span></p>
<p><span data-contrast="none">The OpenCHM platform expands worldwide access to CHM‚Äôs vast collection through a digital portal, and ongoing digitization regularly adds more historical materials. Along with the collection, the portal introduces new digital storytelling and discovery tools designed to bring the history of the technology revolution</span><span data-contrast="none">&nbsp;to life for both experts and general audiences. </span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="none">OpenCHM&nbsp;features include:</span><span data-ccp-props="{}">&nbsp;</span></p>
<ul>
	<li aria-setsize="-1" data-leveltext="ÔÇ∑" data-font="Symbol" data-listid="1" data-list-defn-props="{&quot;335552541&quot;:1,&quot;335559685&quot;:720,&quot;335559991&quot;:360,&quot;469769226&quot;:&quot;Symbol&quot;,&quot;469769242&quot;:[8226],&quot;469777803&quot;:&quot;left&quot;,&quot;469777804&quot;:&quot;ÔÇ∑&quot;,&quot;469777815&quot;:&quot;hybridMultilevel&quot;}" data-aria-posinset="1" data-aria-level="1"><b><span data-contrast="none">Advanced search tools</span></b><span data-contrast="none">&nbsp;with smart filters for documents, images, software, objects, and more&nbsp;allowing&nbsp;users&nbsp;to&nbsp;explore&nbsp;specific artifacts, archives, and oral histories.</span><span data-ccp-props="{}">&nbsp;</span></li>
	<li aria-setsize="-1" data-leveltext="ÔÇ∑" data-font="Symbol" data-listid="1" data-list-defn-props="{&quot;335552541&quot;:1,&quot;335559685&quot;:720,&quot;335559991&quot;:360,&quot;469769226&quot;:&quot;Symbol&quot;,&quot;469769242&quot;:[8226],&quot;469777803&quot;:&quot;left&quot;,&quot;469777804&quot;:&quot;ÔÇ∑&quot;,&quot;469777815&quot;:&quot;hybridMultilevel&quot;}" data-aria-posinset="1" data-aria-level="1"><b><span data-contrast="none">Curator&nbsp;picks&nbsp;and stories</span></b><span data-contrast="none">&nbsp;that highlight visionary pioneers and groundbreaking innovations.</span><span data-ccp-props="{}">&nbsp;</span></li>
	<li aria-setsize="-1" data-leveltext="ÔÇ∑" data-font="Symbol" data-listid="1" data-list-defn-props="{&quot;335552541&quot;:1,&quot;335559685&quot;:720,&quot;335559991&quot;:360,&quot;469769226&quot;:&quot;Symbol&quot;,&quot;469769242&quot;:[8226],&quot;469777803&quot;:&quot;left&quot;,&quot;469777804&quot;:&quot;ÔÇ∑&quot;,&quot;469777815&quot;:&quot;hybridMultilevel&quot;}" data-aria-posinset="1" data-aria-level="1"><b><span data-contrast="none">Highlights&nbsp;and discovery wall&nbsp;</span></b><span data-contrast="none">showcasing&nbsp;rare and iconic artifacts.</span><span data-ccp-props="{}">&nbsp;</span></li>
	<li aria-setsize="-1" data-leveltext="ÔÇ∑" data-font="Symbol" data-listid="1" data-list-defn-props="{&quot;335552541&quot;:1,&quot;335559685&quot;:720,&quot;335559991&quot;:360,&quot;469769226&quot;:&quot;Symbol&quot;,&quot;469769242&quot;:[8226],&quot;469777803&quot;:&quot;left&quot;,&quot;469777804&quot;:&quot;ÔÇ∑&quot;,&quot;469777815&quot;:&quot;hybridMultilevel&quot;}" data-aria-posinset="1" data-aria-level="1"><b><span data-contrast="none">My&nbsp;albums</span></b><span data-contrast="none">&nbsp;empowers&nbsp;users&nbsp;to&nbsp;create, save, and share custom folders filled with the materials they choose.</span><span data-ccp-props="{}">&nbsp;</span></li>
	<li aria-setsize="-1" data-leveltext="ÔÇ∑" data-font="Symbol" data-listid="1" data-list-defn-props="{&quot;335552541&quot;:1,&quot;335559685&quot;:720,&quot;335559991&quot;:360,&quot;469769226&quot;:&quot;Symbol&quot;,&quot;469769242&quot;:[8226],&quot;469777803&quot;:&quot;left&quot;,&quot;469777804&quot;:&quot;ÔÇ∑&quot;,&quot;469777815&quot;:&quot;hybridMultilevel&quot;}" data-aria-posinset="1" data-aria-level="1"><b><span data-contrast="none">Developer portal</span></b><span data-contrast="none">&nbsp;offering access to APIs, sample code, and registration.</span><span data-ccp-props="{}">&nbsp;</span></li>
</ul>
<p><span data-contrast="none">OpenCHM&nbsp;advances CHM‚Äôs mission to preserve and interpret the history of technology while making it&nbsp;broadly accessible&nbsp;as a public resource.</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><b><span data-contrast="none">About CHM</span></b><span data-contrast="none">‚ÄØ</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;<br>
</span><span data-contrast="none">The&nbsp;</span><a href="http://www.computerhistory.org/"><span data-contrast="none">Computer History Museum</span></a><span data-contrast="none">&nbsp;(CHM) is the leading museum decoding computing‚Äôs ongoing impact on our world. We are uniquely positioned to cull the key lessons of the past and‚Äîthrough our research, exhibits, events, and incomparable collection of computing artifacts‚Äîcreate informed digital citizens empowered to make the choices that will shape a better future.&nbsp;</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
<p><b><span data-contrast="none">Press contact:</span></b><span data-contrast="none">&nbsp;Carina Sweet,&nbsp;</span><a href="https://computerhistory.org/cdn-cgi/l/email-protection#7b18080c1e1e0f3b1814160b0e0f1e091312080f1409025514091c"><span data-contrast="none"><span data-cfemail="3457474351514074575b5944414051465c5d47405b464d1a5b4653">[email&nbsp;protected]</span></span></a><span data-contrast="none">, 650.810.1059</span></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mousefood ‚Äì Build embedded terminal UIs for microcontrollers (212 pts)]]></title>
            <link>https://github.com/ratatui/mousefood</link>
            <guid>46798402</guid>
            <pubDate>Wed, 28 Jan 2026 17:20:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ratatui/mousefood">https://github.com/ratatui/mousefood</a>, See on <a href="https://news.ycombinator.com/item?id=46798402">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ratatui/mousefood/blob/599f1026d37c8d6308a6df64a234dbefaedc0c6f/assets/logo/mousefood.svg?raw=true"><img src="https://github.com/ratatui/mousefood/raw/599f1026d37c8d6308a6df64a234dbefaedc0c6f/assets/logo/mousefood.svg?raw=true" alt="Mousefood"></a></h2><a id="" aria-label="Permalink: " href="#"></a></div>
<p dir="auto"><a href="https://crates.io/crates/mousefood" rel="nofollow"><img src="https://camo.githubusercontent.com/a1f106fdc71e9cd5156ce382ebc02eafab13171ab608078351984f64ab85f749/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f6d6f757365666f6f643f6c6f676f3d72757374267374796c653d666c61742d73717561726526636f6c6f723d656265393466" alt="Crate" data-canonical-src="https://img.shields.io/crates/v/mousefood?logo=rust&amp;style=flat-square&amp;color=ebe94f"></a>
<a href="https://docs.rs/mousefood" rel="nofollow"><img src="https://camo.githubusercontent.com/0b83b40ad07bdae4f036f3fd7014cc74ae340de9ec14bcabdab01e7eb5aa00e6/68747470733a2f2f696d672e736869656c64732e696f2f646f637372732f6d6f757365666f6f643f6c6f676f3d72757374267374796c653d666c61742d737175617265" alt="Docs" data-canonical-src="https://img.shields.io/docsrs/mousefood?logo=rust&amp;style=flat-square"></a>
<a href="https://github.com/ratatui/mousefood/blob/main/.github/workflows/ci.yml"><img src="https://camo.githubusercontent.com/6475d8db8734ced5a2cc928a062fff4c5ad84af83ae257225bc94ed6a19fca8d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f726174617475692f6d6f757365666f6f642f63692e796d6c3f7374796c653d666c61742d737175617265266c6f676f3d676974687562" alt="CI" data-canonical-src="https://img.shields.io/github/actions/workflow/status/ratatui/mousefood/ci.yml?style=flat-square&amp;logo=github"></a>
<a href="https://deps.rs/crate/mousefood" rel="nofollow"><img src="https://camo.githubusercontent.com/5430f7a1ac018a0591a40c9fac875a55af6da581f356012ec939b6fc17a3b0a1/68747470733a2f2f646570732e72732f63726174652f6d6f757365666f6f642f6c61746573742f7374617475732e7376673f7374796c653d666c61742d737175617265" alt="Deps" data-canonical-src="https://deps.rs/crate/mousefood/latest/status.svg?style=flat-square"></a></p>
<p dir="auto"><strong>Mousefood</strong> - a no-std
<a href="https://crates.io/crates/embedded-graphics" rel="nofollow">embedded-graphics</a> backend
for <a href="https://crates.io/crates/ratatui" rel="nofollow">Ratatui</a>!</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ratatui/mousefood/blob/599f1026d37c8d6308a6df64a234dbefaedc0c6f/assets/demo.jpg?raw=true"><img src="https://github.com/ratatui/mousefood/raw/599f1026d37c8d6308a6df64a234dbefaedc0c6f/assets/demo.jpg?raw=true" alt="demo"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/ratatui/mousefood/blob/599f1026d37c8d6308a6df64a234dbefaedc0c6f/assets/demo.gif?raw=true"><img src="https://github.com/ratatui/mousefood/raw/599f1026d37c8d6308a6df64a234dbefaedc0c6f/assets/demo.gif?raw=true" alt="animated demo" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quickstart</h2><a id="user-content-quickstart" aria-label="Permalink: Quickstart" href="#quickstart"></a></p>
<p dir="auto">Add mousefood as a dependency:</p>

<p dir="auto">Exemplary setup:</p>
<div dir="auto" data-snippet-clipboard-copy-content="use mousefood::embedded_graphics::{mock_display::MockDisplay, pixelcolor::Rgb888};
use mousefood::prelude::*;
use ratatui::widgets::{Block, Paragraph};
use ratatui::{Frame, Terminal};

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // replace this with your display driver
    // e.g. ILI9341, ST7735, SSD1306, etc.
    let mut display = MockDisplay::<Rgb888>::new();

    let backend = EmbeddedBackend::new(&amp;mut display, EmbeddedBackendConfig::default());
    let mut terminal = Terminal::new(backend)?;

    terminal.draw(draw)?;
    Ok(())
}

fn draw(frame: &amp;mut Frame) {
    let block = Block::bordered().title(&quot;Mousefood&quot;);
    let paragraph = Paragraph::new(&quot;Hello from Mousefood!&quot;).block(block);
    frame.render_widget(paragraph, frame.area());
}"><pre><span>use</span> mousefood<span>::</span>embedded_graphics<span>::</span><span>{</span>mock_display<span>::</span><span>MockDisplay</span><span>,</span> pixelcolor<span>::</span><span>Rgb888</span><span>}</span><span>;</span>
<span>use</span> mousefood<span>::</span>prelude<span>::</span><span>*</span><span>;</span>
<span>use</span> ratatui<span>::</span>widgets<span>::</span><span>{</span><span>Block</span><span>,</span> <span>Paragraph</span><span>}</span><span>;</span>
<span>use</span> ratatui<span>::</span><span>{</span><span>Frame</span><span>,</span> <span>Terminal</span><span>}</span><span>;</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> -&gt; <span>Result</span><span>&lt;</span><span>(</span><span>)</span><span>,</span> <span>Box</span><span>&lt;</span><span>dyn</span> std<span>::</span>error<span>::</span><span>Error</span><span>&gt;</span><span>&gt;</span> <span>{</span>
    <span>// replace this with your display driver</span>
    <span>// e.g. ILI9341, ST7735, SSD1306, etc.</span>
    <span>let</span> <span>mut</span> display = <span>MockDisplay</span><span>::</span><span>&lt;</span><span>Rgb888</span><span>&gt;</span><span>::</span><span>new</span><span>(</span><span>)</span><span>;</span>

    <span>let</span> backend = <span>EmbeddedBackend</span><span>::</span><span>new</span><span>(</span><span>&amp;</span><span>mut</span> display<span>,</span> <span>EmbeddedBackendConfig</span><span>::</span><span>default</span><span>(</span><span>)</span><span>)</span><span>;</span>
    <span>let</span> <span>mut</span> terminal = <span>Terminal</span><span>::</span><span>new</span><span>(</span>backend<span>)</span>?<span>;</span>

    terminal<span>.</span><span>draw</span><span>(</span>draw<span>)</span>?<span>;</span>
    <span>Ok</span><span>(</span><span>(</span><span>)</span><span>)</span>
<span>}</span>

<span>fn</span> <span>draw</span><span>(</span><span>frame</span><span>:</span> <span>&amp;</span><span>mut</span> <span>Frame</span><span>)</span> <span>{</span>
    <span>let</span> block = <span>Block</span><span>::</span><span>bordered</span><span>(</span><span>)</span><span>.</span><span>title</span><span>(</span><span>"Mousefood"</span><span>)</span><span>;</span>
    <span>let</span> paragraph = <span>Paragraph</span><span>::</span><span>new</span><span>(</span><span>"Hello from Mousefood!"</span><span>)</span><span>.</span><span>block</span><span>(</span>block<span>)</span><span>;</span>
    frame<span>.</span><span>render_widget</span><span>(</span>paragraph<span>,</span> frame<span>.</span><span>area</span><span>(</span><span>)</span><span>)</span><span>;</span>
<span>}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Special characters</h3><a id="user-content-special-characters" aria-label="Permalink: Special characters" href="#special-characters"></a></p>
<p dir="auto">Embedded-graphics includes bitmap fonts that have a very limited
set of characters to save space (ASCII, ISO 8859 or JIS X0201).
This makes it impossible to draw most of Ratatui's widgets,
which heavily use box-drawing glyphs, Braille,
and other special characters.</p>
<p dir="auto">Mousefood by default uses <a href="https://crates.io/crates/embedded-graphics-unicodefonts" rel="nofollow"><code>embedded-graphics-unicodefonts</code></a>,
which provides embedded-graphics fonts with a much larger set of characters.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Alternatives</h4><a id="user-content-alternatives" aria-label="Permalink: Alternatives" href="#alternatives"></a></p>
<p dir="auto">In order to save space and <a href="#performance-and-hardware-support">speed up rendering</a>,
the <code>fonts</code> feature can be disabled by turning off the default crate features.
<a href="https://crates.io/crates/ibm437" rel="nofollow"><code>ibm437</code></a> is a good alternative that includes
some drawing characters, but is not as large as embedded-graphics-unicodefonts.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Bold and italic fonts</h3><a id="user-content-bold-and-italic-fonts" aria-label="Permalink: Bold and italic fonts" href="#bold-and-italic-fonts"></a></p>
<p dir="auto">Bold and italic modifiers are supported, but this requires providing fonts
through <code>EmbeddedBackendConfig</code>.
If only regular font is provided, it serves as a fallback.
All fonts must be of the same size.</p>
<div dir="auto" data-snippet-clipboard-copy-content="use mousefood::embedded_graphics::{mock_display::MockDisplay, pixelcolor::Rgb888};
use mousefood::{EmbeddedBackend, EmbeddedBackendConfig, fonts};
use ratatui::Terminal;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    let mut display = MockDisplay::<Rgb888>::new();
    let config = EmbeddedBackendConfig {
        font_regular: fonts::MONO_6X13,
        font_bold: Some(fonts::MONO_6X13_BOLD),
        font_italic: Some(fonts::MONO_6X13_ITALIC),
        ..Default::default()
    };
    let backend = EmbeddedBackend::new(&amp;mut display, config);
    let _terminal = Terminal::new(backend)?;
    Ok(())
}"><pre><span>use</span> mousefood<span>::</span>embedded_graphics<span>::</span><span>{</span>mock_display<span>::</span><span>MockDisplay</span><span>,</span> pixelcolor<span>::</span><span>Rgb888</span><span>}</span><span>;</span>
<span>use</span> mousefood<span>::</span><span>{</span><span>EmbeddedBackend</span><span>,</span> <span>EmbeddedBackendConfig</span><span>,</span> fonts<span>}</span><span>;</span>
<span>use</span> ratatui<span>::</span><span>Terminal</span><span>;</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> -&gt; <span>Result</span><span>&lt;</span><span>(</span><span>)</span><span>,</span> <span>Box</span><span>&lt;</span><span>dyn</span> std<span>::</span>error<span>::</span><span>Error</span><span>&gt;</span><span>&gt;</span> <span>{</span>
    <span>let</span> <span>mut</span> display = <span>MockDisplay</span><span>::</span><span>&lt;</span><span>Rgb888</span><span>&gt;</span><span>::</span><span>new</span><span>(</span><span>)</span><span>;</span>
    <span>let</span> config = <span>EmbeddedBackendConfig</span> <span>{</span>
        <span>font_regular</span><span>:</span> fonts<span>::</span><span>MONO_6X13</span><span>,</span>
        <span>font_bold</span><span>:</span> <span>Some</span><span>(</span>fonts<span>::</span><span>MONO_6X13_BOLD</span><span>)</span><span>,</span>
        <span>font_italic</span><span>:</span> <span>Some</span><span>(</span>fonts<span>::</span><span>MONO_6X13_ITALIC</span><span>)</span><span>,</span>
        ..<span>Default</span><span>::</span><span>default</span><span>(</span><span>)</span>
    <span>}</span><span>;</span>
    <span>let</span> backend = <span>EmbeddedBackend</span><span>::</span><span>new</span><span>(</span><span>&amp;</span><span>mut</span> display<span>,</span> config<span>)</span><span>;</span>
    <span>let</span> _terminal = <span>Terminal</span><span>::</span><span>new</span><span>(</span>backend<span>)</span>?<span>;</span>
    <span>Ok</span><span>(</span><span>(</span><span>)</span><span>)</span>
<span>}</span></pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/ratatui/mousefood/blob/6640da9402794ea8f9370e0dc2b4bd1ebf2c6356/assets/bold_italic.png?raw=true"><img alt="Bold and Italic fonts" src="https://github.com/ratatui/mousefood/raw/6640da9402794ea8f9370e0dc2b4bd1ebf2c6356/assets/bold_italic.png?raw=true"></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Color theme</h3><a id="user-content-color-theme" aria-label="Permalink: Color theme" href="#color-theme"></a></p>
<p dir="auto">Colors can be remapped using <code>color_theme</code> on <code>EmbeddedBackendConfig</code>.
By default the ANSI palette is used.</p>
<div dir="auto" data-snippet-clipboard-copy-content="use mousefood::{ColorTheme, EmbeddedBackend, EmbeddedBackendConfig};
use mousefood::embedded_graphics::{mock_display::MockDisplay, pixelcolor::Rgb888};

fn main() -> Result<(), Box<dyn std::error::Error>> {
    let mut display = MockDisplay::<Rgb888>::new();
    let theme = ColorTheme {
        background: Rgb888::new(5, 5, 5),
        foreground: Rgb888::new(240, 240, 240),
        yellow: Rgb888::new(255, 200, 0),
        ..ColorTheme::ansi()
    };

    let config = EmbeddedBackendConfig {
        color_theme: theme,
        ..Default::default()
    };
    let backend = EmbeddedBackend::new(&amp;mut display, config);
    Ok(())
}"><pre><span>use</span> mousefood<span>::</span><span>{</span><span>ColorTheme</span><span>,</span> <span>EmbeddedBackend</span><span>,</span> <span>EmbeddedBackendConfig</span><span>}</span><span>;</span>
<span>use</span> mousefood<span>::</span>embedded_graphics<span>::</span><span>{</span>mock_display<span>::</span><span>MockDisplay</span><span>,</span> pixelcolor<span>::</span><span>Rgb888</span><span>}</span><span>;</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> -&gt; <span>Result</span><span>&lt;</span><span>(</span><span>)</span><span>,</span> <span>Box</span><span>&lt;</span><span>dyn</span> std<span>::</span>error<span>::</span><span>Error</span><span>&gt;</span><span>&gt;</span> <span>{</span>
    <span>let</span> <span>mut</span> display = <span>MockDisplay</span><span>::</span><span>&lt;</span><span>Rgb888</span><span>&gt;</span><span>::</span><span>new</span><span>(</span><span>)</span><span>;</span>
    <span>let</span> theme = <span>ColorTheme</span> <span>{</span>
        <span>background</span><span>:</span> <span>Rgb888</span><span>::</span><span>new</span><span>(</span><span>5</span><span>,</span> <span>5</span><span>,</span> <span>5</span><span>)</span><span>,</span>
        <span>foreground</span><span>:</span> <span>Rgb888</span><span>::</span><span>new</span><span>(</span><span>240</span><span>,</span> <span>240</span><span>,</span> <span>240</span><span>)</span><span>,</span>
        <span>yellow</span><span>:</span> <span>Rgb888</span><span>::</span><span>new</span><span>(</span><span>255</span><span>,</span> <span>200</span><span>,</span> <span>0</span><span>)</span><span>,</span>
        ..<span>ColorTheme</span><span>::</span><span>ansi</span><span>(</span><span>)</span>
    <span>}</span><span>;</span>

    <span>let</span> config = <span>EmbeddedBackendConfig</span> <span>{</span>
        <span>color_theme</span><span>:</span> theme<span>,</span>
        ..<span>Default</span><span>::</span><span>default</span><span>(</span><span>)</span>
    <span>}</span><span>;</span>
    <span>let</span> backend = <span>EmbeddedBackend</span><span>::</span><span>new</span><span>(</span><span>&amp;</span><span>mut</span> display<span>,</span> config<span>)</span><span>;</span>
    <span>Ok</span><span>(</span><span>(</span><span>)</span><span>)</span>
<span>}</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Built-in themes</h4><a id="user-content-built-in-themes" aria-label="Permalink: Built-in themes" href="#built-in-themes"></a></p>
<p dir="auto">Mousefood includes popular color themes that can be used directly:</p>
<ul dir="auto">
<li><code>ColorTheme::ansi()</code> - Standard ANSI colors (default)</li>
<li><code>ColorTheme::tokyo_night()</code> - Tokyo Night dark theme with blue/purple tones</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Simulator</h3><a id="user-content-simulator" aria-label="Permalink: Simulator" href="#simulator"></a></p>
<p dir="auto">Mousefood can be run in a simulator using
<a href="https://crates.io/crates/embedded-graphics-simulator" rel="nofollow">embedded-graphics-simulator</a> crate.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ratatui/mousefood/blob/66d4010deed18f755cc3148a7f682f4119b7f664/assets/simulator.png?raw=true"><img src="https://github.com/ratatui/mousefood/raw/66d4010deed18f755cc3148a7f682f4119b7f664/assets/simulator.png?raw=true" alt="Screenshot of a window running the simulator with a mousefood application"></a></p>
<p dir="auto">Run simulator example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/ratatui/mousefood.git
cd mousefood/examples/simulator
cargo run"><pre>git clone https://github.com/ratatui/mousefood.git
<span>cd</span> mousefood/examples/simulator
cargo run</pre></div>
<p dir="auto">For more details, view the <a href="https://github.com/ratatui/mousefood/blob/main/examples/simulator">simulator example</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">EPD support</h3><a id="user-content-epd-support" aria-label="Permalink: EPD support" href="#epd-support"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">WeAct Studio</h4><a id="user-content-weact-studio" aria-label="Permalink: WeAct Studio" href="#weact-studio"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ratatui/mousefood/blob/fa70cdd46567a51895caf10c44fff4104602e880/assets/epd-weact.jpg?raw=true"><img src="https://github.com/ratatui/mousefood/raw/fa70cdd46567a51895caf10c44fff4104602e880/assets/epd-weact.jpg?raw=true" alt="WeAct epd demo"></a></p>
<p dir="auto">Support for EPD (e-ink displays) produced by WeAct Studio
(<code>weact-studio-epd</code> driver) can be enabled using <code>epd-weact</code> feature.</p>
<p dir="auto">This driver requires some additional configuration.
Follow the <a href="https://docs.rs/weact-studio-epd" rel="nofollow"><code>weact-studio-epd</code></a>
crate docs and apply the same <code>flush_callback</code> pattern used in the <a href="#waveshare">Waveshare example below</a>.</p>
<details>
  <summary>Setup example</summary>
<div dir="auto" data-snippet-clipboard-copy-content="use mousefood::prelude::*;
use weact_studio_epd::graphics::Display290BlackWhite;
use weact_studio_epd::WeActStudio290BlackWhiteDriver;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Configure SPI + GPIO + delay provider for your board.
    // let (spi_interface, busy, rst, delay) = ...;

    let mut driver = WeActStudio290BlackWhiteDriver::new(spi_interface, busy, rst, delay);
    let mut display = Display290BlackWhite::new();

    driver.init()?;

    let config = EmbeddedBackendConfig {
        flush_callback: Box::new(move |d| {
            driver.full_update(d).expect(&quot;epd update failed&quot;);
        }),
        ..Default::default()
    };

    let backend = EmbeddedBackend::new(&amp;mut display, config);
    let _terminal = Terminal::new(backend)?;
    Ok(())
}"><pre><span>use</span> mousefood<span>::</span>prelude<span>::</span><span>*</span><span>;</span>
<span>use</span> weact_studio_epd<span>::</span>graphics<span>::</span><span>Display290BlackWhite</span><span>;</span>
<span>use</span> weact_studio_epd<span>::</span><span>WeActStudio290BlackWhiteDriver</span><span>;</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> -&gt; <span>Result</span><span>&lt;</span><span>(</span><span>)</span><span>,</span> <span>Box</span><span>&lt;</span><span>dyn</span> std<span>::</span>error<span>::</span><span>Error</span><span>&gt;</span><span>&gt;</span> <span>{</span>
    <span>// Configure SPI + GPIO + delay provider for your board.</span>
    <span>// let (spi_interface, busy, rst, delay) = ...;</span>

    <span>let</span> <span>mut</span> driver = <span>WeActStudio290BlackWhiteDriver</span><span>::</span><span>new</span><span>(</span>spi_interface<span>,</span> busy<span>,</span> rst<span>,</span> delay<span>)</span><span>;</span>
    <span>let</span> <span>mut</span> display = <span>Display290BlackWhite</span><span>::</span><span>new</span><span>(</span><span>)</span><span>;</span>

    driver<span>.</span><span>init</span><span>(</span><span>)</span>?<span>;</span>

    <span>let</span> config = <span>EmbeddedBackendConfig</span> <span>{</span>
        <span>flush_callback</span><span>:</span> <span>Box</span><span>::</span><span>new</span><span>(</span><span>move</span> |d| <span>{</span>
            driver<span>.</span><span>full_update</span><span>(</span>d<span>)</span><span>.</span><span>expect</span><span>(</span><span>"epd update failed"</span><span>)</span><span>;</span>
        <span>}</span><span>)</span><span>,</span>
        ..<span>Default</span><span>::</span><span>default</span><span>(</span><span>)</span>
    <span>}</span><span>;</span>

    <span>let</span> backend = <span>EmbeddedBackend</span><span>::</span><span>new</span><span>(</span><span>&amp;</span><span>mut</span> display<span>,</span> config<span>)</span><span>;</span>
    <span>let</span> _terminal = <span>Terminal</span><span>::</span><span>new</span><span>(</span>backend<span>)</span>?<span>;</span>
    <span>Ok</span><span>(</span><span>(</span><span>)</span><span>)</span>
<span>}</span></pre></div>
</details>
<p dir="auto"><h4 tabindex="-1" dir="auto">Waveshare</h4><a id="user-content-waveshare" aria-label="Permalink: Waveshare" href="#waveshare"></a></p>
<p dir="auto">Support for EPD (e-ink displays) produced by Waveshare Electronics
(<code>epd-waveshare</code> driver) can be enabled using <code>epd-waveshare</code> feature.</p>
<details>
  <summary>Setup example</summary>
<div dir="auto" data-snippet-clipboard-copy-content="use mousefood::prelude::*;
use epd_waveshare::{epd2in9_v2::*, prelude::*};

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Configure SPI + GPIO + delay provider for your board.
    // let (mut spi_device, busy, dc, rst, mut delay) = ...;

    let mut epd = Epd2in9::new(&amp;mut spi_device, busy, dc, rst, &amp;mut delay, None)?;
    let mut display = Display2in9::default();

    let config = EmbeddedBackendConfig {
        flush_callback: Box::new(move |d| {
            epd.update_and_display_frame(&amp;mut spi_device, d.buffer(), &amp;mut delay)
                .expect(&quot;epd update failed&quot;);
        }),
        ..Default::default()
    };

    let backend = EmbeddedBackend::new(&amp;mut display, config);
    let _terminal = Terminal::new(backend)?;
    Ok(())
}"><pre><span>use</span> mousefood<span>::</span>prelude<span>::</span><span>*</span><span>;</span>
<span>use</span> epd_waveshare<span>::</span><span>{</span>epd2in9_v2<span>::</span><span>*</span><span>,</span> prelude<span>::</span><span>*</span><span>}</span><span>;</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> -&gt; <span>Result</span><span>&lt;</span><span>(</span><span>)</span><span>,</span> <span>Box</span><span>&lt;</span><span>dyn</span> std<span>::</span>error<span>::</span><span>Error</span><span>&gt;</span><span>&gt;</span> <span>{</span>
    <span>// Configure SPI + GPIO + delay provider for your board.</span>
    <span>// let (mut spi_device, busy, dc, rst, mut delay) = ...;</span>

    <span>let</span> <span>mut</span> epd = <span>Epd2in9</span><span>::</span><span>new</span><span>(</span><span>&amp;</span><span>mut</span> spi_device<span>,</span> busy<span>,</span> dc<span>,</span> rst<span>,</span> <span>&amp;</span><span>mut</span> delay<span>,</span> <span>None</span><span>)</span>?<span>;</span>
    <span>let</span> <span>mut</span> display = <span>Display2in9</span><span>::</span><span>default</span><span>(</span><span>)</span><span>;</span>

    <span>let</span> config = <span>EmbeddedBackendConfig</span> <span>{</span>
        <span>flush_callback</span><span>:</span> <span>Box</span><span>::</span><span>new</span><span>(</span><span>move</span> |d| <span>{</span>
            epd<span>.</span><span>update_and_display_frame</span><span>(</span><span>&amp;</span><span>mut</span> spi_device<span>,</span> d<span>.</span><span>buffer</span><span>(</span><span>)</span><span>,</span> <span>&amp;</span><span>mut</span> delay<span>)</span>
                <span>.</span><span>expect</span><span>(</span><span>"epd update failed"</span><span>)</span><span>;</span>
        <span>}</span><span>)</span><span>,</span>
        ..<span>Default</span><span>::</span><span>default</span><span>(</span><span>)</span>
    <span>}</span><span>;</span>

    <span>let</span> backend = <span>EmbeddedBackend</span><span>::</span><span>new</span><span>(</span><span>&amp;</span><span>mut</span> display<span>,</span> config<span>)</span><span>;</span>
    <span>let</span> _terminal = <span>Terminal</span><span>::</span><span>new</span><span>(</span>backend<span>)</span>?<span>;</span>
    <span>Ok</span><span>(</span><span>(</span><span>)</span><span>)</span>
<span>}</span></pre></div>
</details>
<p dir="auto">See the full embedded example at <a href="https://github.com/ratatui/mousefood/tree/main/examples/epd-waveshare-demo"><code>examples/epd-waveshare-demo</code></a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance and hardware support</h2><a id="user-content-performance-and-hardware-support" aria-label="Permalink: Performance and hardware support" href="#performance-and-hardware-support"></a></p>
<p dir="auto">Flash memory on most embedded devices is very limited. Additionally,
to achieve high frame rate when using the <code>fonts</code> feature,
it is recommended to use <code>opt-level = 3</code>,
which can make the resulting binary even larger.</p>
<p dir="auto">Mousefood is hardware-agnostic.
Successfully tested on:</p>
<ul dir="auto">
<li>ESP32 (Xtensa)</li>
<li>ESP32-C6 (RISC-V)</li>
<li>STM32</li>
<li>RP2040</li>
<li>RP2350</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Docs</h2><a id="user-content-docs" aria-label="Permalink: Docs" href="#docs"></a></p>
<p dir="auto">Full API docs are available on <a href="https://docs.rs/mousefood" rel="nofollow">docs.rs</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">All contributions are welcome!</p>
<p dir="auto">Before opening a pull request, please read the <a href="https://github.com/ratatui/mousefood/blob/main/CONTRIBUTING.md">contributing guidelines</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Built with Mousefood</h2><a id="user-content-built-with-mousefood" aria-label="Permalink: Built with Mousefood" href="#built-with-mousefood"></a></p>
<p dir="auto">Here are some projects built using Mousefood:</p>
<ul dir="auto">
<li><a href="https://github.com/orhun/tuitar">Tuitar</a> - A portable guitar training tool.</li>
<li><a href="https://github.com/intuis/mnyaoo32">Mnyaoo32</a> - An eccentric way to consume IRC messages using ESP32.</li>
<li><a href="https://github.com/Julien-cpsn/Phone-OS">Phone-OS</a> - A modern phone OS for ESP32 CYD.</li>
</ul>
<p dir="auto">Send a pull request to add your project here!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto"><a href="https://github.com/ratatui/mousefood/blob/main/LICENSE-MIT"><img src="https://camo.githubusercontent.com/c3957e1022b48e04bf5149c28553b7f6635004b71d7d64ccab3966725659f002/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e7376673f7374796c653d666c61742d73717561726526636f6c6f723d386439376233" alt="License MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg?style=flat-square&amp;color=8d97b3"></a>
<a href="https://github.com/ratatui/mousefood/blob/main/LICENSE-APACHE"><img src="https://camo.githubusercontent.com/81843b554babacdbe84fccac5ca72d83531f10d31a9a989e062e6b1c83331d06/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e7376673f7374796c653d666c61742d73717561726526636f6c6f723d386439376233" alt="License Apache 2.0" data-canonical-src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?style=flat-square&amp;color=8d97b3"></a></p>
<p dir="auto">Mousefood is dual-licensed under
<a href="https://github.com/ratatui/mousefood/blob/main/LICENSE-APACHE">Apache 2.0</a> and <a href="https://github.com/ratatui/mousefood/blob/main/LICENSE-MIT">MIT</a> terms.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Spinning around: Please don‚Äôt ‚Äì Common problems with spin locks (132 pts)]]></title>
            <link>https://www.siliceum.com/en/blog/post/spinning-around/</link>
            <guid>46797868</guid>
            <pubDate>Wed, 28 Jan 2026 16:48:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.siliceum.com/en/blog/post/spinning-around/">https://www.siliceum.com/en/blog/post/spinning-around/</a>, See on <a href="https://news.ycombinator.com/item?id=46797868">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-vw24gqkt=""><article data-astro-cid-vw24gqkt=""><h2 id="intro">Intro</h2>
<p>This is the 3<sup>rd</sup> project in less than a year where I‚Äôve seen issues with spin-loops. I‚Äôve been dealing with spinning threads for many years now, and I won‚Äôt lie: over the years I‚Äôve been both on the offender and victim side.<br>
I‚Äôm getting tired of seeing the same issues again and again, which usually makes for a good reason to write a blog post so that, hopefully, people will read it and stop making the same mistakes others did.</p>
<p>Actually, many others have written about this, covering various issues related to spin locks <sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup> <sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup> <sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup> <sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup> <sup><a href="#user-content-fn-5" id="user-content-fnref-5" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup> <sup><a href="#user-content-fn-x" id="user-content-fnref-x" data-footnote-ref="" aria-describedby="footnote-label">6</a></sup>. But I guess there‚Äôs never enough material on those subjects. Some are about speed, others about fairness, a few about priority inversion, NUMA, and sometimes even about actually broken code.<br>
If this list hasn‚Äôt convinced you that things do spin out of control when using spin-locks, and that you should use OS primitives instead, keep reading. I‚Äôll cover what you should not do when implementing your own spin-lock. Notice I said what you should <strong>NOT</strong> do, because, <strong>again</strong>, you should <em>probably</em> not use a spin-lock at all these days.<br>
And if you do‚Ä¶ make sure you really, REALLY, <strong>REALLY</strong> know what you‚Äôre doing (<em>spoiler: it will always come back to bite you when you least expect it</em>).</p>
<p>Note this is a story about spin loops in general, not about locking algorithms for which there are many <sup><a href="#user-content-fn-5" id="user-content-fnref-5-2" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup>.</p>
<h2 id="the-broken-spin-lock">The broken spin-lock</h2>
<p>Let‚Äôs start with the basics, you want to implement your own spinlock.</p>
<blockquote>
<p>ü§™ ‚ÄúIt‚Äôs easy! You simply have a boolean, a <code>lock</code> and an <code>unlock</code> function.‚Äù</p>
</blockquote>
<p>Right‚Ä¶</p>
<p><em>For demonstration purposes, we are using <code>int</code> instead of <code>bool</code> as you might have something more complicated to do with it, such as storing metadata (for example: the thread ID). There are also quite a few pieces of code around that do not implement a spin-lock per se, but mutate some other content such as pointers.</em></p>
<pre tabindex="0" data-language="cpp"><code><span><span>class</span><span> BrokenSpinLock</span></span>
<span><span>{</span></span>
<span><span>    // Using int32_t instead of bool on purpose, don't mind it.</span></span>
<span><span>    int32_t</span><span> isLocked </span><span>=</span><span> 0</span><span>;</span></span>
<span><span>public:</span></span>
<span><span>    void</span><span> lock</span><span>()</span></span>
<span><span>    {</span></span>
<span><span>        while</span><span> (isLocked </span><span>!=</span><span> 0</span><span>)</span><span> // (1)</span></span>
<span><span>        {</span></span>
<span><span>            // Loop again until not locked anymore</span></span>
<span><span>        }</span></span>
<span><span>                            // (2)</span></span>
<span><span>        isLocked </span><span>=</span><span> 1</span><span>;</span><span>       // (3)</span></span>
<span><span>    }</span><span>                       // (4)</span></span>
<span></span>
<span><span>    void</span><span> unlock</span><span>()</span></span>
<span><span>    {</span></span>
<span><span>        isLocked </span><span>=</span><span> 0</span><span>;</span></span>
<span><span>    }</span></span>
<span><span>};</span></span></code></pre>
<p>Those who have dealt with multi-threading before will immediately spot the issue. The code is not thread-safe as, if multiple threads attempt to use this lock, we could read invalid values of <code>isLocked</code> (<em>in theory, and on a CPU where tearing could happen on its word size</em>).
Worse, even if this could not happen, a wild race-condition could appear.<br>
Consider the following example where two threads would call <code>lock</code> at the exact same time:</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>(1) ThreadA: Sees `isLocked == 0` | ThreadB: Sees `isLocked == 0` </span></span>
<span><span>(2) ThreadA: Leaves the loop      | ThreadB: Leaves the loop       </span></span>
<span><span>(3) ThreadA: Writes 1 to isLocked | ThreadB: Writes 1 to isLocked </span></span></code></pre>
<p>Now we have two threads who think they have successfully acquired the lock!</p>
<p>Some may also have heard about this shiny little thing called <code>atomic</code> variables/operations.<br>
To oversimplify: atomic operations guarantee that other threads cannot observe a partial/intermediate state of the operation and thus race-conditions can not occur (<em>on those specific operations and memory</em>).</p>
<blockquote>
<p>üí° While named after the Greek <code>atomos</code> that means ‚Äúthat which cannot be divided‚Äù, <code>atomic</code> operations might as well be as dangerous and difficult to use as nuclear energy.</p>
</blockquote>
<p>Let‚Äôs replace <code>isLocked</code> by an atomic version: <code>std::atomic&lt;int&gt;</code>. Though our code does not suffer from a race-condition on the data itself, we still do not know if the thread that sets <code>isLocked</code> to <code>1</code> is the one that now owns the lock. But we can now do an <a href="https://en.cppreference.com/w/cpp/atomic/atomic/exchange.html"><code>exchange</code></a> operation atomically, which solves our little problem!</p>
<p>Instead of first checking if the lock is locked, then writing, we actually write our value and get the previous value, in a single atomic operation! If the previous value was <code>0</code>, then it means we‚Äôre the one who actually did the locking. Otherwise we will see a <code>1</code>, meaning the lock was already held either before we tried, or because another thread‚Äôs exchange completed before ours.</p>
<pre tabindex="0" data-language="cpp"><code><span><span>void</span><span> lock</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    while</span><span> (isLocked.</span><span>exchange</span><span>(</span><span>1</span><span>) </span><span>!=</span><span> 0</span><span>) {}</span></span>
<span><span>}</span></span></code></pre>
<p>Let‚Äôs replay the scenario. Even if both threads execute the exchange simultaneously, atomicity guarantees one will finish before the other, for example Thread <strong>B</strong>‚Äôs:</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>ThreadA: `isLocked.exchange(1)` | ThreadB: `isLocked.exchange(1)` </span></span>
<span><span>ThreadA: Writes 1, sees 1       | ThreadB: Writes 1, sees 0       </span></span>
<span><span>ThreadA: Writes 1, sees 1       | ThreadB: Now owns the lock!</span></span>
<span><span>ThreadA: ...                    | ThreadB: ... </span></span>
<span><span>ThreadA: ...                    | ThreadB: `unlock()`, writes 0 </span></span>
<span><span>ThreadA: Writes 1, sees 0       | ThreadB: ... </span></span>
<span><span>ThreadA: Now owns the lock!     | ThreadB: ... </span></span></code></pre>
<p>Good, we now have a working spin-lock, but we still have a long way to go.</p>
<blockquote>
<p>üí° In the CPU lingua, a memory <strong>read</strong>/<strong>write</strong> is called a memory <strong>load</strong>/<strong>store</strong></p>
</blockquote>
<h2 id="the-spin-lock-that-burned-cpus">The spin-lock that burned CPUs</h2>
<p>You may have realized that our spin-lock will‚Ä¶ spin doing nothing, the loop is empty.</p>
<blockquote>
<p>ü§™ ‚ÄúGreat, it‚Äôll attempt to take ownership faster‚Äù</p>
</blockquote>
<p>Well, that‚Äôs only true if you want to burn your CPU. Since the CPU has no way of knowing that you are waiting and not doing any meaningful work, it might stay at a high frequency.
Modern CPUs can change the frequency of the cores to save energy, and effectively also lower the CPU core temperature. This is clearly not desirable behavior, especially on mobile/embedded devices.</p>
<p>Not convinced or do not care about the planet? (shame on you!) Then at least think about your users‚Äô power bill. Still not convinced? What if I told you this can actually be slower than doing something in the loop?<br>
Imagine that a lot of threads are attempting to lock your spin-lock. Only one can win. But worse, due to its nature you always do memory writes, which need to be synchronized between the different cores of your CPU!</p>
<p>From Intel‚Äôs Optimization Reference Manual <sup><a href="#user-content-fn-3" id="user-content-fnref-3-2" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup> <strong>11.4.2</strong>:</p>
<blockquote>
<p>On a modern microprocessor with a superscalar speculative execution engine, a loop like this results in the issue of
multiple simultaneous read requests from the spinning thread. These requests usually execute out-of-order with each
read request being allocated a buffer resource. On detection of a write by a worker thread to a load that is in progress,
the processor <strong>must guarantee</strong> no violations of memory order occur. The necessity of maintaining the order of
outstanding memory operations inevitably <strong>costs the processor a severe penalty</strong> that impacts all threads.</p>
</blockquote>
<p>And the issue will keep getting bigger with recent CPUs that have many cores and sometimes NUMA memory.</p>
<blockquote>
<p>This penalty occurs on the Intel Core Solo and Intel Core Duo processors. However, the penalty on these
processors is small compared with penalties suffered on the Intel Xeon processors. There the performance penalty for
exiting the loop is about <strong>25 times more severe</strong>.</p>
</blockquote>
<p>If you still need some convincing‚Ä¶ this is even worse if you enable SMT (hyperthreading):</p>
<blockquote>
<p>On a processor supporting Intel HT Technology, spin-wait loops can consume a significant portion of the execution
bandwidth of the processor. One logical processor executing a spin-wait loop can <strong>severely impact the performance</strong> of
the other logical processor.</p>
</blockquote>
<p>Now that I hopefully have your attention, here‚Äôs how to <del>solve</del> mitigate the issue:<br>
The best way to avoid ‚Äúbothering‚Äù your neighbours is to <del>avoid spin loops</del> tell the CPU you are waiting to be notified of a memory change/doing a spinloop!
On x86 CPUs, this is done with the <a href="https://www.felixcloutier.com/x86/pause"><code>PAUSE</code></a> instruction. It was designed exactly for this use-case!</p>
<blockquote>
<p>The penalty of exiting from a <strong>spin-wait loop can be avoided by inserting a <code>PAUSE</code> instruction in the loop</strong>. In spite of
the name, the <code>PAUSE</code> instruction <strong>improves performance</strong> by introducing a slight delay in the loop and effectively
causing the memory read requests to be issued at a rate that allows immediate detection of any store to the
synchronization variable. This <strong>prevents the occurrence of a long delay due to memory order violation</strong>.</p>
</blockquote>
<p>You can modify the code to use this instruction with compiler intrinsics:</p>
<pre tabindex="0" data-language="cpp"><code><span><span>void</span><span> cpu_pause</span><span>()</span></span>
<span><span>{</span></span>
<span><span>#if</span><span> defined</span><span>(</span><span>__i386__</span><span>) </span><span>||</span><span> defined</span><span>(</span><span>__x86_64__</span><span>) </span><span>||</span><span> defined</span><span>(</span><span>_M_IX86</span><span>) </span><span>||</span><span> defined</span><span>(</span><span>_M_X64</span><span>)</span></span>
<span><span>    _mm_pause</span><span>();</span></span>
<span><span>#elif</span><span> defined</span><span>(__arm__) </span><span>||</span><span> defined</span><span>(__aarch64__) </span><span>||</span><span> defined</span><span>(</span><span>_M_ARM</span><span>) </span><span>||</span><span> defined</span><span>(</span><span>_M_ARM64</span><span>) </span><span>||</span><span> defined</span><span>(_M_ARM64EC)</span></span>
<span><span>    __yield</span><span>();</span></span>
<span><span>#else</span></span>
<span><span>    #error</span><span> "unknown instruction set"</span></span>
<span><span>#endif</span></span>
<span><span>}</span></span>
<span></span>
<span><span>void</span><span> lock</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    while</span><span> (isLocked.</span><span>exchange</span><span>(</span><span>1</span><span>) </span><span>!=</span><span> 0</span><span>)</span></span>
<span><span>    {</span></span>
<span><span>        cpu_pause</span><span>();</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre>
<h2 id="the-spin-lock-that-didnt-wait-enough">The spin-lock that didn‚Äôt wait enough</h2>
<p>As already mentioned, the penalty of synchronizing data between CPU cores is getting more expensive as new CPUs get more cores, get multiple core complexes or NUMA architectures.
Resolving conflicts (multiple cores trying to do atomic stores) thus needs to be mitigated in some way.
A traditional approach is to use a <em>backoff</em> strategy that increases the number of <code>PAUSE</code> instructions for each attempt at locking.</p>
<p>The one you will find most (recommended by the Intel Optimization Manual, 2.7.4), is the exponential backoff:</p>
<pre tabindex="0" data-language="cpp"><code><span><span>void</span><span> lock</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    const</span><span> int</span><span> maxPauses </span><span>=</span><span> 64</span><span>;</span><span> // MAX_BACKOFF</span></span>
<span><span>    int</span><span> nbPauses </span><span>=</span><span> 1</span><span>;</span></span>
<span><span>    while</span><span> (isLocked.</span><span>exchange</span><span>(</span><span>1</span><span>) </span><span>!=</span><span> 0</span><span>)</span></span>
<span><span>    {</span></span>
<span><span>        for</span><span> (</span><span>int</span><span> i </span><span>=</span><span> 0</span><span>; i</span><span>&lt;</span><span>nbPauses; i</span><span>++</span><span>)</span></span>
<span><span>            cpu_pause</span><span>();</span></span>
<span><span>        // Multiply the number of pauses by 2 until we reach the max backoff count.</span></span>
<span><span>        nbPauses </span><span>=</span><span> nbPauses </span><span>&lt;</span><span> maxPauses </span><span>?</span><span> nbPauses </span><span>*</span><span> 2</span><span> :</span><span> nbPauses;</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre>
<p>As mentioned by Intel:</p>
<blockquote>
<p>The number of <code>PAUSE</code> instructions are increased by a factor of 2 until some <code>MAX_BACKOFF</code> is reached which is subject
to tuning.</p>
</blockquote>
<p>We also mix it with a bit of randomness by using <code>rdtsc</code>, and let‚Äôs refactor the yielding part into a structure that can be easily swapped:</p>
<pre tabindex="0" data-language="cpp"><code><span><span>struct</span><span> Yielder</span></span>
<span><span>{</span></span>
<span><span>    static</span><span> const</span><span> int</span><span> maxPauses </span><span>=</span><span> 64</span><span>;</span><span> // MAX_BACKOFF</span></span>
<span><span>    int</span><span> nbPauses </span><span>=</span><span> 1</span><span>;</span></span>
<span><span>    void</span><span> do_yield</span><span>()</span></span>
<span><span>    {</span></span>
<span><span>        // jitter is in the range of [0;nbPauses-1].</span></span>
<span><span>        // We can use bitwise AND since nbPauses is a power of 2.</span></span>
<span><span>        const</span><span> int</span><span> jitter </span><span>=</span><span> static_cast&lt;int&gt;</span><span>(</span><span>__rdtsc</span><span>() </span><span>&amp;</span><span> (nbPauses </span><span>-</span><span> 1</span><span>));</span></span>
<span><span>        // So subtracting we get a value between [1;nbPauses]</span></span>
<span><span>        const</span><span> int</span><span> nbPausesThisLoop </span><span>=</span><span> nbPauses </span><span>-</span><span> jitter;</span></span>
<span><span>        for</span><span> (</span><span>int</span><span> i </span><span>=</span><span> 0</span><span>; i </span><span>&lt;</span><span> nbPausesThisLoop; i</span><span>++</span><span>) </span></span>
<span><span>            cpu_pause</span><span>();</span></span>
<span><span>        // Multiply the number of pauses by 2 until we reach the max backoff count.</span></span>
<span><span>        nbPauses </span><span>=</span><span> nbPauses </span><span>&lt;</span><span> maxPauses </span><span>?</span><span> nbPauses </span><span>*</span><span> 2</span><span> :</span><span> nbPauses;</span></span>
<span><span>    }</span></span>
<span><span>}</span></span>
<span></span>
<span><span>void</span><span> lock</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    Yielder yielder;</span></span>
<span><span>    while</span><span> (isLocked.</span><span>exchange</span><span>(</span><span>1</span><span>) </span><span>!=</span><span> 0</span><span>)</span></span>
<span><span>    {</span></span>
<span><span>        yielder.</span><span>do_yield</span><span>();</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre>
<h2 id="the-spin-lock-that-waited-too-long">The spin-lock that waited too long</h2>
<p>Remember the comment above about <code>MAX_BACKOFF</code> being subject to tuning?
Well you‚Äôd better make sure to tune it for the exact CPU you‚Äôll be working on.<br>
Let‚Äôs have a look at the following table listing the measured<sup><a href="#user-content-fn-w" id="user-content-fnref-w" data-footnote-ref="" aria-describedby="footnote-label">7</a></sup> duration of <code>PAUSE</code> in cycles:</p>













































<div data-astro-cid-zg6ajjo3=""> <table data-astro-cid-zg6ajjo3=""> <thead><tr><th>Sandy   Bridge</th><th>Ivy Bridge</th><th>Haswell</th><th>Broadwell</th><th>Skylake</th><th>Kaby Lake</th><th>Coffee Lake</th><th>Cannon Lake</th><th>Cascade Lake</th><th>Ice Lake</th><th>Rocket Lake</th><th>Alder Lake-P</th><th>Tremont</th><th>Alder Lake-E</th><th>AMD Zen+</th><th>AMD Zen2</th><th>AMD Zen3</th><th>AMD Zen4</th></tr></thead><tbody><tr><td>11.00</td><td>10.00</td><td>9.00</td><td>9.00</td><td>140.00</td><td>140.00</td><td>152.50</td><td>157.00</td><td>40.00</td><td>138.20</td><td>138.20</td><td>160.17</td><td>176.00</td><td>61.80</td><td>3.00</td><td>65.00</td><td>65.00</td><td>65.00</td></tr></tbody> </table> </div> 
<p>And that‚Äôs where the issue lies. Depending on the architecture, you may get more than 10x changes in cycles per <code>PAUSE</code>.<br>
Old CPUs tended to have small <code>PAUSE</code> duration of <strong>~10</strong> cycles on Intel, <strong>~3</strong> on AMD, where <em>new</em> architectures have a duration of <strong>100-160</strong> cycles on Intel, and <strong>~60</strong> cycles on AMD.
And this might get worse in the future!</p>
<p>This actually is also now part of the latest Intel Optimization Reference Manual <sup><a href="#user-content-fn-3" id="user-content-fnref-3-3" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup> 2.7.4:</p>
<blockquote>
<p>The latency of the <code>PAUSE</code> instruction in prior generation microarchitectures is about 10 cycles, whereas in Skylake Client microarchitecture it has been extended to as many as 140 cycles.</p>
</blockquote>
<p>How to fix this, you ask? I‚Äôll defer to Intel‚Äôs advice again and limit the duration of the <code>PAUSE</code> loop using CPU cycles instead of a counter:</p>
<pre tabindex="0" data-language="cpp"><code><span><span>static</span><span> inline</span><span> bool</span><span> before</span><span>(</span><span>uint64_t</span><span> a</span><span>, </span><span>uint64_t</span><span> b</span><span>)</span></span>
<span><span>{</span></span>
<span><span>    return</span><span> ((</span><span>int64_t</span><span>)b </span><span>-</span><span> (</span><span>int64_t</span><span>)a) </span><span>&gt;</span><span> 0</span><span>;</span></span>
<span><span>}</span></span>
<span><span>void</span><span> pollDelay</span><span>(</span><span>uint32_t</span><span> clocks</span><span>)</span></span>
<span><span>{</span></span>
<span><span>    uint64_t</span><span> endTime </span><span>=</span><span> _rdtsc</span><span>()</span><span>+</span><span> clocks;</span></span>
<span><span>    for</span><span> (; </span><span>before</span><span>(</span><span>_rdtsc</span><span>(), endTime); )</span></span>
<span><span>        cpu_pause</span><span>();</span></span>
<span><span>}</span></span></code></pre>
<blockquote>
<p>As the <code>PAUSE</code> latency has been increased significantly, workloads that are sensitive to <code>PAUSE</code> latency will suffer some
performance loss.<br>
[‚Ä¶]<br>
Notice that in the Skylake Client microarchitecture the <code>RDTSC</code> instruction counts at the machine‚Äôs guaranteed P1
frequency independently of the current processor clock (see the INVARIANT TSC property), and therefore, when
running in Intel¬Æ Turbo-Boost-enabled mode, the delay will remain constant, but the number of instructions that
could have been executed will change.</p>
</blockquote>
<p>Let‚Äôs implement this:</p>
<pre tabindex="0" data-language="cpp"><code><span><span>struct</span><span> Yielder</span></span>
<span><span>{</span></span>
<span><span>    static</span><span> const</span><span> int</span><span> maxPauses </span><span>=</span><span> 64</span><span>;</span><span> // MAX_BACKOFF</span></span>
<span><span>    int</span><span> nbPauses </span><span>=</span><span> 1</span><span>;</span></span>
<span><span>    </span></span>
<span><span>    const</span><span> int</span><span> maxCycles </span><span>=</span><span> /*Some value*/</span><span>;</span></span>
<span><span>    </span></span>
<span><span>    void</span><span> do_yield</span><span>()</span></span>
<span><span>    {</span></span>
<span><span>        uint64_t</span><span> beginTSC </span><span>=</span><span> __rdtsc</span><span>();</span></span>
<span><span>        uint64_t</span><span> endTSC </span><span>=</span><span> beginTSC </span><span>+</span><span> maxCycles;</span><span> // Max duration of the yield</span></span>
<span><span>        // jitter is in the range of [0;nbPauses-1].</span></span>
<span><span>        // We can use bitwise AND since nbPauses is a power of 2.</span></span>
<span><span>        const</span><span> int</span><span> jitter </span><span>=</span><span> static_cast&lt;int&gt;</span><span>(beginTSC </span><span>&amp;</span><span> (nbPauses </span><span>-</span><span> 1</span><span>));</span></span>
<span><span>        // So subtracting we get a value between [1;nbPauses]</span></span>
<span><span>        const</span><span> int</span><span> nbPausesThisLoop </span><span>=</span><span> nbPauses </span><span>-</span><span> jitter;</span></span>
<span><span>        for</span><span> (</span><span>int</span><span> i </span><span>=</span><span> 0</span><span>; i </span><span>&lt;</span><span> nbPausesThisLoop </span><span>&amp;&amp;</span><span> before</span><span>(</span><span>__rdtsc</span><span>(), endTSC); i</span><span>++</span><span>) </span></span>
<span><span>            cpu_pause</span><span>();</span></span>
<span><span>        // Multiply the number of pauses by 2 until we reach the max backoff count.</span></span>
<span><span>        nbPauses </span><span>=</span><span> nbPauses </span><span>&lt;</span><span> maxPauses </span><span>?</span><span> nbPauses </span><span>*</span><span> 2</span><span> :</span><span> nbPauses;</span></span>
<span><span>    }</span></span>
<span><span>}</span></span>
<span></span>
<span><span>void</span><span> lock</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    Yielder yield;</span></span>
<span><span>    while</span><span> (isLocked.</span><span>exchange</span><span>(</span><span>1</span><span>) </span><span>!=</span><span> 0</span><span>)</span></span>
<span><span>    {</span></span>
<span><span>        yield.</span><span>do_yield</span><span>();</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre>
<p>This method has two main advantages:</p>
<ul>
<li>We define the max duration of a <code>PAUSE</code> loop in terms of <code>TSC</code> cycles, which is (on most modern CPUs) independent of the actual frequency of the core or duration of <code>PAUSE</code>.</li>
<li>If the operating system happens to preempt our thread in the middle of the loop, it will stop yielding after being rescheduled if maximum duration has been exceeded. Otherwise we could call <code>PAUSE</code> more than necessary on a thread wakeup.</li>
</ul>
<p>You‚Äôll notice that we kept the exponential backoff as a plain counter. This is to avoid having to compute the duration of a single <code>PAUSE</code> (<em>this would require getting rid of the jitter</em>).
However, we still need to choose a value for <code>maxCycles</code>. This again is purely empirical and needs tuning, but one may assume the duration of a context switch is about 3¬µs. Depending on the system and actual switch this can be more or be less. But it should be in the same order of magnitude.
We can then estimate the TSC cycles/¬µs conversion to be ~3200cycles/¬µs  for a 3.2Ghz clock. Another common frequency for the TSC is 2.5GHz.
While obviously incorrect, this is a good guesstimate for a default value on PC. At worst, you‚Äôll most likely get a 2x difference with the real value, which is way better than the x10 you could get with the varying <code>PAUSE</code> durations!</p>
<p>I did however mention this is a default value, and the best thing to do is to retrieve the real value, either from the OS or by measuring it. Sadly TSC calibration is not officially exposed by Linux/Windows, so the best way is to measure the TSC against the system high resolution clock. Ideally this should be done asynchronously (don‚Äôt do it on your application main thread at boot, please).</p>
<pre tabindex="0" data-language="cpp"><code><span><span>// Please, do this asynchronously and not during your main thread init</span></span>
<span><span>// Otherwise you will make your application boot longer for nothing!</span></span>
<span><span>// Note there are more accurate ways to do this, but we do not need a very high precision nor accuracy.</span></span>
<span><span>// You may also split this function in two and do some meaningful amount of work instead of sleeping.</span></span>
<span><span>uint64_t</span><span> MeasureCyclesPerUs</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    const</span><span> auto</span><span> clockBefore </span><span>=</span><span> std</span><span>::</span><span>chrono</span><span>::</span><span>high_resolution_clock</span><span>::</span><span>now</span><span>();</span></span>
<span><span>    const</span><span> uint64_t</span><span> cyclesBefore </span><span>=</span><span> __rdtsc</span><span>();</span></span>
<span><span>    std</span><span>::</span><span>this_thread</span><span>::</span><span>sleep_for</span><span>(</span><span>std</span><span>::</span><span>chrono</span><span>::microseconds{</span><span>10</span><span>});</span></span>
<span><span>    const</span><span> auto</span><span> clockAfter </span><span>=</span><span> std</span><span>::</span><span>chrono</span><span>::</span><span>high_resolution_clock</span><span>::</span><span>now</span><span>();</span></span>
<span><span>    const</span><span> uint64_t</span><span> cyclesAfter </span><span>=</span><span> __rdtsc</span><span>();</span></span>
<span><span>    const</span><span> auto</span><span> clockDelta </span><span>=</span><span> clockAfter </span><span>-</span><span> clockBefore;</span></span>
<span><span>    const</span><span> uint64_t</span><span> cyclesDelta </span><span>=</span><span> cyclesAfter </span><span>-</span><span> cyclesBefore;</span></span>
<span><span>    const</span><span> uint64_t</span><span> cyclesPerUs </span><span>=</span><span> (</span><span>1000</span><span> *</span><span> cyclesDelta) </span><span>/</span><span> std</span><span>::</span><span>chrono</span><span>::</span><span>nanoseconds</span><span>(clockDelta).</span><span>count</span><span>();</span></span>
<span><span>    return</span><span> cyclesPerUs;</span></span>
<span><span>}</span></span></code></pre>
<blockquote>
<p>üí° Windows actually ‚Äúexposes‚Äù this value as <a href="https://ntdoc.m417z.com/kuser_shared_data#cyclesperyield"><code>CyclesPerYield</code></a> in the kernel shared data at offset <code>0x2D6</code>. This is used internally by synchronization primitives to determine how many <code>PAUSE</code> instructions it should issue. However I wouldn‚Äôt recommend using those internals unless your code sanitizes the value.</p>
</blockquote>
<h2 id="the-spin-lock-that-used-too-many-barriers">The spin-lock that used too many barriers</h2>
<p>We only briefly touched the topic of <code>atomics</code>. All atomic operations actually take an optional parameter which is the <a href="https://en.cppreference.com/w/cpp/atomic/memory_order.html">memory order</a>.
I don‚Äôt want to spend too much time on this as entire talks are dedicated to it, and it‚Äôs not an easy topic.</p>
<p>However do know this: not providing the parameter is equivalent to using <code>std::memory_order_seq_cst</code> (sequentially consistent) which enforces the most restrictions. On some platforms this may even flush your cache via memory barriers!
Our previous example can actually be re-written using acquire/release semantics:</p>
<pre tabindex="0" data-language="cpp"><code><span><span>void</span><span> lock</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    Yielder yield;</span></span>
<span><span>    while</span><span> (isLocked.</span><span>exchange</span><span>(</span><span>1</span><span>, </span><span>std</span><span>::memory_order_acquire) </span><span>!=</span><span> 0</span><span>)</span></span>
<span><span>    {</span></span>
<span><span>        yield.</span><span>do_yield</span><span>();</span></span>
<span><span>    }</span></span>
<span><span>}</span></span>
<span></span>
<span><span>void</span><span> unlock</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    isLocked.</span><span>store</span><span>(</span><span>0</span><span>, </span><span>std</span><span>::memory_order_release);</span></span>
<span><span>}</span></span></code></pre>
<p>On my x64 machine and exponential backoff:</p>

























<div data-astro-cid-zg6ajjo3=""> <table data-astro-cid-zg6ajjo3=""> <thead><tr><th>Lock Type</th><th>Uncontended (ops/s)</th><th>Contended (ops/s)</th></tr></thead><tbody><tr><td>ExpBackoff+SeqCst</td><td>313M</td><td>55.3M</td></tr><tr><td>ExpBackoff+AcqRel</td><td>612M</td><td>58.7M</td></tr><tr><td>ExpBackoff+Acquire</td><td>652M</td><td>65.3M</td></tr></tbody> </table> </div> 
<p>You may have a look at the various assemblies generated on this compiler explorer example <a href="https://godbolt.org/z/GjEEWPsj8">https://godbolt.org/z/GjEEWPsj8</a>.</p>
<h2 id="the-spin-lock-that-saturated-the-load-ports">The spin-lock that saturated the load ports</h2>
<p>A spin-lock should be fast, otherwise you would just use your average system lock.
While we mitigated the inter-core synchronization with the jitter and exponential backoff, there are ways to reduce the cache coherency <sup><a href="#user-content-fn-f" id="user-content-fnref-f" data-footnote-ref="" aria-describedby="footnote-label">8</a></sup> traffic under contention.
This has been mentioned by many in the past <sup><a href="#user-content-fn-a" id="user-content-fnref-a" data-footnote-ref="" aria-describedby="footnote-label">9</a></sup> <sup><a href="#user-content-fn-b" id="user-content-fnref-b" data-footnote-ref="" aria-describedby="footnote-label">10</a></sup> <sup><a href="#user-content-fn-y" id="user-content-fnref-y" data-footnote-ref="" aria-describedby="footnote-label">11</a></sup> but it doesn‚Äôt hurt to remind it again. Instead of looping over a <em>Test-And-Set</em> (aka <em>Compare-And-Swap</em>) operation, prefer using both <em>Test</em> and <em>Test-And-Set</em> operations!
It also applies to our <em>Load-And-Test</em> (aka <em>Exchange</em>) operation.</p>
<p>So instead of:</p>
<pre tabindex="0" data-language="cpp"><code><span><span>void</span><span> lock</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    Yielder yield;</span></span>
<span><span>    while</span><span> (isLocked.</span><span>exchange</span><span>(</span><span>1</span><span>, </span><span>std</span><span>::memory_order_acquire) </span><span>!=</span><span> 0</span><span>)</span></span>
<span><span>    {</span></span>
<span><span>        yield.</span><span>do_yield</span><span>();</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre>
<p>Do:</p>
<pre tabindex="0" data-language="cpp"><code><span><span>void</span><span> lock</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    Yielder yield;</span></span>
<span><span>    // Actually start by an exchange, we assume the lock is not already taken</span></span>
<span><span>    // This is because the main use case of a spinlock is when there's no contention!</span></span>
<span><span>    while</span><span> (isLocked.</span><span>exchange</span><span>(</span><span>1</span><span>, </span><span>std</span><span>::memory_order_acquire) </span><span>!=</span><span> 0</span><span>)</span></span>
<span><span>    {</span></span>
<span><span>        // To avoid locking the cache line with a write access, always only read before attempting the writes</span></span>
<span><span>        do</span><span> {</span></span>
<span><span>            yield.</span><span>do_yield</span><span>();</span><span> // Yield while we fail to obtain the lock.</span></span>
<span><span>        } </span><span>while</span><span> (isLocked.</span><span>load</span><span>(</span><span>std</span><span>::memory_order_relaxed) </span><span>!=</span><span> 0</span><span>);</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre>
<h2 id="the-spin-lock-that-didnt-handle-priority-inversion">The spin-lock that didn‚Äôt handle priority inversion</h2>
<p>Priority inversion is one of the worst things that could (and will) happen with a spinlock. And it impacts most severely the platforms that need them the most! (Embedded, real-time OSes, ‚Ä¶)
Let‚Äôs have a look at the issue:</p>
<ol>
<li>A <strong>low-priority thread</strong> acquires your spinlock</li>
<li>A <strong>high-priority thread</strong> tries to acquire the lock and starts spinning</li>
<li>The OS scheduler preempts the low-priority thread to run another thread with medium/high priority (anything higher than ‚Äúlow‚Äù)</li>
<li>There are no cores left to run the <strong>low priority thread</strong> as they are all used by higher priority threads.</li>
<li>The high-priority thread burns CPU cycles spinning forever.</li>
</ol>
<blockquote>
<p>ü§™ ‚ÄúLet‚Äôs use <code>std::this_thread::yield()</code>?‚Äù</p>
</blockquote>
<p>Meh, did you test it on multiple systems? I‚Äôll play along and give it a try.</p>
<pre tabindex="0" data-language="cpp"><code><span><span>struct</span><span> Yielder</span></span>
<span><span>{</span></span>
<span><span>    void</span><span> do_yield_expo_and_jitter</span><span>()</span></span>
<span><span>    {</span></span>
<span><span>        // Same as before, exponential backoff and jitter</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    void</span><span> do_yield</span><span>()</span></span>
<span><span>    {</span></span>
<span><span>        do_yield_expo_and_jitter</span><span>();</span></span>
<span><span>        if</span><span> (nbPauses </span><span>&gt;=</span><span> maxPauses)</span></span>
<span><span>        {</span></span>
<span><span>            std</span><span>::</span><span>this_thread</span><span>::</span><span>yield</span><span>();</span><span> // Yield thread back to the OS</span></span>
<span><span>            nbPauses </span><span>=</span><span> 1</span><span>;</span></span>
<span><span>        }</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre>
<p>Now when we reach the maximum number of iterations, we make the thread yield its quantum to the operating system (<a href="https://github.com/microsoft/STL/blob/cbe2ee99caf122555a305d18f69e57c75b3fe5ec/stl/src/cthread.cpp#L86"><code>SwitchToThread</code></a> on Windows, <code>sched_yield</code> on Linux) so that another thread may be scheduled.
While in practice this may, sometimes, solve the issue as the OS is now free to schedule other threads including the <strong>low priority one</strong>, this is not mandatory!
Some implementations may end up just rescheduling the thread that just yielded since it‚Äôs of higher priority.</p>
<p>You may have also seen implementations that use <code>Sleep(0)</code> on Windows. This is better than <code>SwitchToThread</code> (which can only yield to a thread ready to run on the current core, per the <a href="https://learn.microsoft.com/en-us/windows/win32/api/processthreadsapi/nf-processthreadsapi-switchtothread?redirectedfrom=MSDN">docs</a><sup><a href="#user-content-fn-stt" id="user-content-fnref-stt" data-footnote-ref="" aria-describedby="footnote-label">12</a></sup>. Same for normal Linux schedulers<sup><a href="#user-content-fn-q" id="user-content-fnref-q" data-footnote-ref="" aria-describedby="footnote-label">13</a></sup>). However this used to<sup><a href="#user-content-fn-s" id="user-content-fnref-s" data-footnote-ref="" aria-describedby="footnote-label">14</a></sup> only yield to threads of <strong>same or higher</strong> priorities, and <strong>still does</strong> on the real-time version of the OS! For example on an embedded device, or a console.
The only way to schedule any thread on real-time kernels, be it Windows or Linux, is to sleep for a non-zero duration‚Ä¶ which we obviously would like to avoid!</p>
<p>So the <a href="https://github.com/dotnet/runtime/blob/379d100b3cc18394064a276d7610e88a2aa09b6f/src/libraries/System.Private.CoreLib/src/System/Threading/SpinWait.cs#L183-L192">solution</a> that the DotNet runtime team came up with is to start with <code>SwitchToThread</code>, then <code>Sleep(0)</code> then <code>Sleep(1)</code>!</p>
<pre tabindex="0" data-language="cpp"><code><span><span>// We prefer to call Thread.Yield first, triggering a SwitchToThread. This</span></span>
<span><span>// unfortunately doesn't consider all runnable threads on all OS SKUs. In</span></span>
<span><span>// some cases, it may only consult the runnable threads whose ideal processor</span></span>
<span><span>// is the one currently executing code. Thus we occasionally issue a call to</span></span>
<span><span>// Sleep(0), which considers all runnable threads at equal priority. Even this</span></span>
<span><span>// is insufficient since we may be spin waiting for lower priority threads to</span></span>
<span><span>// execute; we therefore must call Sleep(1) once in a while too, which considers</span></span>
<span><span>// all runnable threads, regardless of ideal processor and priority, but may</span></span>
<span><span>// remove the thread from the scheduler's queue for 10+ms, if the system is</span></span>
<span><span>// configured to use the (default) coarse-grained system timer.</span></span></code></pre>

<p>So we dealt with the priority inversion at the cost of potential sleeps.</p>
<blockquote>
<p>ü§™ ‚ÄúShip it!‚Äù</p>
</blockquote>
<p>Please god no‚Ä¶ Yes, you (<em>most likely</em>) avoid the worst case scenario (<em>the livelock</em>), but really, is it fine?</p>
<p>Let‚Äôs stop for a second here and assume we never did more than yield.</p>
<p>As you may have already guessed, a livelock is only half the story (<em>this is starting to be a recurring pattern, isn‚Äôt it?</em>).
The fact is, the issue could happen even if all your threads have the same priority! (<em>Yes, I saw you coming asking for an easy fix by removing priorities.</em>)
Consider the following scenario:</p>
<ul>
<li>4 cores machine</li>
<li>4 high priority threads: <strong>A</strong>, <strong>B</strong>, <strong>C</strong>, <strong>D</strong> (<em>your thread pool</em>)</li>
<li>4 other high priority threads: <strong>X</strong>, <strong>Y</strong>, <strong>W</strong>, <strong>Z</strong> (<em>controlled by a 3rd party, those suck. Please library writers, don‚Äôt spawn threads on your own, thank you!</em>).</li>
<li>Thread <strong>A</strong> acquires the lock</li>
<li>Threads <strong>B</strong>, <strong>C</strong>, <strong>D</strong> spin, trying to acquire it.</li>
</ul>
<p>At this point, we have the following:</p>

















<div data-astro-cid-zg6ajjo3=""> <table data-astro-cid-zg6ajjo3=""> <thead><tr><th>Core 0</th><th>Core 1</th><th>Core 2</th><th>Core 3</th></tr></thead><tbody><tr><td>Thread A</td><td>Thread B</td><td>Thread C</td><td>Thread D</td></tr></tbody> </table> </div> 
<ul>
<li>Thread <strong>X</strong> gets scheduled (<em><strong>A</strong> somehow released its quantum, still holds the lock</em>)</li>
</ul>

















<div data-astro-cid-zg6ajjo3=""> <table data-astro-cid-zg6ajjo3=""> <thead><tr><th>Core 0</th><th>Core 1</th><th>Core 2</th><th>Core 3</th></tr></thead><tbody><tr><td>Thread X</td><td>Thread B</td><td>Thread C</td><td>Thread D</td></tr></tbody> </table> </div> 
<ul>
<li>Thread <strong>B</strong> yields, <strong>Y</strong> is scheduled</li>
</ul>

















<div data-astro-cid-zg6ajjo3=""> <table data-astro-cid-zg6ajjo3=""> <thead><tr><th>Core 0</th><th>Core 1</th><th>Core 2</th><th>Core 3</th></tr></thead><tbody><tr><td>Thread X</td><td>Thread Y</td><td>Thread C</td><td>Thread D</td></tr></tbody> </table> </div> 
<ul>
<li>Thread <strong>Y</strong> yields, <strong>B</strong> is scheduled again, <strong>C</strong> and <strong>D</strong> yield to <strong>W</strong> and <strong>Z</strong></li>
</ul>

















<div data-astro-cid-zg6ajjo3=""> <table data-astro-cid-zg6ajjo3=""> <thead><tr><th>Core 0</th><th>Core 1</th><th>Core 2</th><th>Core 3</th></tr></thead><tbody><tr><td>Thread X</td><td>Thread B</td><td>Thread W</td><td>Thread Z</td></tr></tbody> </table> </div> 
<p>I could continue this for a long time. Even though thread <strong>A</strong> might get scheduled again, it might not! This depends on your scheduler‚Äôs internals. Especially since yielding may yield only to the ready threads of the current core<sup><a href="#user-content-fn-stt" id="user-content-fnref-stt-2" data-footnote-ref="" aria-describedby="footnote-label">12</a></sup> <sup><a href="#user-content-fn-q" id="user-content-fnref-q-2" data-footnote-ref="" aria-describedby="footnote-label">13</a></sup>. At the time of writing this article, this actually is a known issue with <a href="https://github.com/google/sanitizers/issues/614">Address Sanitizer</a>!</p>
<p>Oh, and even if it did get scheduled, you probably lost a lot of time switching from one thread to the other, this is your typical lock convoy<sup><a href="#user-content-fn-c" id="user-content-fnref-c" data-footnote-ref="" aria-describedby="footnote-label">15</a></sup> and is what Linus Torvalds more or less hints here<sup><a href="#user-content-fn-4" id="user-content-fnref-4-2" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup> <sup><a href="#user-content-fn-l" id="user-content-fnref-l" data-footnote-ref="" aria-describedby="footnote-label">16</a></sup> <sup><a href="#user-content-fn-q" id="user-content-fnref-q-3" data-footnote-ref="" aria-describedby="footnote-label">13</a></sup>:</p>
<blockquote>
<p>And no, adding random ‚Äú<code>sched_yield()</code>‚Äù calls while you‚Äôre spinning on the spinlock will not really help. It will easily result in scheduling storms while people are yielding to all the wrong processes.</p>
</blockquote>
<p>So no, simply using the same priority for all threads or sleeping is not fine. Let‚Äôs see what we can do about it.</p>
<h2 id="the-spin-lock-that-spoke-to-the-os">The spin-lock that spoke to the OS</h2>
<p>The real problem, when you spin in a loop, is that you expect things to go fast so that your thread may continue.<br>
But by yielding this way you defeat a lot of the kernel heuristics. It has no way to know what you actually meant, and may schedule anything (or nothing) but threads from your process. Worse, it may degrade your thread priority, move it to lower frequency cores, and you lose any kind of priority boost when waking up due to the lock being released‚Ä¶<br>
That‚Äôs clearly not what we want. If only there was a way to communicate our intent to the OS‚Ä¶</p>
<p>Well that‚Äôs exactly what Linux did when introducing the <a href="https://www.man7.org/linux/man-pages/man2/futex.2.html">futex</a> API! Since we‚Äôre waiting in a loop for a value to change, just notify the OS about it and let it handle things from there.
Windows also implements this with the <a href="https://learn.microsoft.com/en-us/windows/win32/api/synchapi/nf-synchapi-waitonaddress"><code>WaitOnAddress</code></a> API, which we‚Äôll be demonstrating here:</p>
<pre tabindex="0" data-language="cpp"><code><span><span>void</span><span> do_yield</span><span>(</span><span>int32_t*</span><span> address</span><span>, </span><span>int32_t</span><span> comparisonValue</span><span>, </span><span>uint32_t</span><span> timeoutMs</span><span>)</span></span>
<span><span>{</span></span>
<span><span>    do_yield_expo_and_jitter</span><span>();</span></span>
<span><span>    if</span><span> (nbPauses </span><span>&gt;=</span><span> maxPauses)</span></span>
<span><span>    {</span></span>
<span><span>        // The thread will stay asleep while the value at the given address doesn't change and `WakeByAddressSingle`/`WakeByAddressAll` isn't called.</span></span>
<span><span>        // We might have a spurious wakeup though, so the value needs to be checked afterward, which we already do since we spin.</span></span>
<span><span>        WaitOnAddress</span><span>(address, </span><span>&amp;</span><span>comparisonValue, </span><span>sizeof</span><span>(comparisonValue), timeoutMs);</span></span>
<span><span>        nbPauses </span><span>=</span><span> 1</span><span>;</span></span>
<span><span>    }</span></span>
<span><span>}</span></span>
<span><span>void</span><span> lock</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    Yielder yield;</span></span>
<span><span>    while</span><span> (isLocked.</span><span>exchange</span><span>(</span><span>1</span><span>, </span><span>std</span><span>::memory_order_acquire) </span><span>!=</span><span> 0</span><span>)</span></span>
<span><span>    {</span></span>
<span><span>        do</span><span> {</span></span>
<span><span>            yield.</span><span>do_yield</span><span>(</span><span>&amp;</span><span>isLocked, </span><span>1</span><span> /*while locked*/</span><span> , </span><span>1</span><span> /*ms*/</span><span>);</span></span>
<span><span>        } </span><span>while</span><span> (isLocked.</span><span>load</span><span>(</span><span>std</span><span>::memory_order_relaxed) </span><span>!=</span><span> 0</span><span>);</span></span>
<span><span>    }</span></span>
<span><span>}</span></span>
<span><span>void</span><span> unlock</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    isLocked </span><span>=</span><span> 0</span><span>;</span></span>
<span><span>    WakeByAddressSingle</span><span>(</span><span>&amp;</span><span>isLocked);</span><span> // Notify a potential thread waiting, if any.</span></span>
<span><span>}</span></span></code></pre>
<p>Windows‚Äô <code>WaitOnAddress</code> internally does a single iteration before issuing the system call, but Linux‚Äôs futex API is a direct syscall. That‚Äôs why we call <code>WaitOnAddress</code> only after spinning a bit.<br>
This lets us have a similar spinning strategy on all platforms, which ensures a more consistent behavior.</p>
<blockquote>
<p>üí° You may notice that we always end up calling <code>WakeByAddressSingle</code> even if there‚Äôs no other thread waiting. While not that slow on Windows, this is slow on Linux since it will do a syscall. To avoid that one would usually store some state such as the number of waiting (parked) threads.</p>
</blockquote>
<blockquote>
<p>ü§™ ‚ÄúWait! Wasn‚Äôt <code>std::atomic_wait</code> added to the standard recently?‚Äù</p>
</blockquote>
<p>Yes! And this is what one should have used if implementers did the right thing from the get-go (<em>and more importantly did the same thing for each implementation</em>), but this was not the case‚Ä¶<sup><a href="#user-content-fn-z" id="user-content-fnref-z" data-footnote-ref="" aria-describedby="footnote-label">17</a></sup></p>
<ul>
<li><code>libc++</code> (clang) <a href="https://github.com/llvm/llvm-project/blob/46db8d822ecdf36a714de5e1acf187736a3af5d1/libcxx/include/__atomic/atomic_sync.h#L56">used to</a> do exponential backoff with <strong>thread yields</strong> before <code>futex</code>. At least it got <a href="https://github.com/llvm/llvm-project/commit/699f19605579f25083152a9ad21e14c2751d5d66">fixed</a> in January 2025 but it still does exponential backoff.</li>
<li>MSVC STL does the <a href="https://github.com/microsoft/STL/blob/cbe2ee99caf122555a305d18f69e57c75b3fe5ec/stl/inc/atomic#L449">right thing</a>‚Ñ¢ <em>imho</em> and goes almost straight to the OS since the first implementation. Good job!</li>
<li>So <a href="https://github.com/gcc-mirror/gcc/blob/c65fdb6b03d9146ee9a1ffcfcbc689b004b2b463/libstdc%2B%2B-v3/include/bits/atomic_wait.h#L298">does</a> <code>libstdc++</code> (GCC)!</li>
</ul>
<p>So if you use it, you may get a built-in exponential backoff, or not. Both implementations actually make sense from an implementer‚Äôs point of view (<em>Do you expect <code>std::atomic_wait</code> users to use it with their own backoff strategies? Or directly as condition variables?</em>), but this difference ends up being problematic since the code behaves differently between implementations.<br>
In the end, as usual with the <code>std</code> library, you‚Äôre better off using the OS primitives directly if you want portable behaviour that you control.</p>
<blockquote>
<p>As mentioned, Windows‚Äô <code>WaitOnAddress</code> will do a single spin before doing a syscall. The duration of <code>PAUSE</code> is computed on process start by the loader in <code>LdrpInitializeProcess</code> and stored in <code>ntdll.dll!RtlpWaitOnAddressSpinCycleCount</code>.</p>
</blockquote>
<h2 id="the-spin-lock-that-was-unfair">The spin-lock that was unfair</h2>
<p>An issue with some lock algorithms is that they may be unfair: this is what happens when under contention a thread may never actually grab the ownership of the lock if other threads are faster.<br>
This time I‚Äôll simply give a warning and ask you to trust me as this article is starting to be lengthy. You may have encountered some ‚Äúticket‚Äù locks that attempt to enhance the fairness of the lock. While it may look good on paper, it‚Äôs actually not so good in practice.</p>
<p>Not only is it slower due to its complexity, but as mentioned before only the OS really knows what‚Äôs good for scheduling. And if you want to use a <code>futex</code>-like API you end up having to wake up all potential waiters instead of just the one you want. So please, rely on the OS primitives for fairness instead. (<em>Even if we didn‚Äôt have those primitives, a random+exponential backoff may perform better than a ticket lock anyway!</em>)</p>
<h2 id="the-spin-locks-that-were-falsely-sharing">The spin-locks that were falsely sharing</h2>
<p>Here comes another tidbit of CPU architecture: even if you write to different variables, they may share the same cacheline!
And this is really bad for performance when you do atomic operations on the same cacheline, even if the addresses are different.
To fix this issue, you may enforce alignment of your variables or use padding in a <code>struct</code>. False sharing is also known as destructive interference, which led to the standard‚Äôs <a href="https://en.cppreference.com/w/cpp/thread/hardware_destructive_interference_size.html"><code>std::hardware_destructive_interference_size</code></a> value!</p>
<pre tabindex="0" data-language="cpp"><code><span><span>alignas</span><span>(</span><span>std</span><span>::hardware_destructive_interference_size) MyLock lock1;</span></span>
<span><span>alignas</span><span>(</span><span>std</span><span>::hardware_destructive_interference_size) MyLock lock2;</span></span></code></pre>
<p>This is however not a silver bullet!<br>
While you will avoid false sharing, you may also fill your TLB and L1 cache faster which may lead to more cache thrashing.</p>
<p>You may even encounter cache bank conflicts. Cache bank conflicts only exist on some CPUs, but don‚Äôt trust manufacturers to avoid them. From 3.6.1.3 of the Intel Optimization Reference Manual:</p>
<blockquote>
<ul>
<li><em>‚ÄúIn the Sandy Bridge microarchitecture, the internal organization of the L1D cache may manifest [‚Ä¶]‚Äù</em></li>
<li><em>‚ÄúThe L1D cache bank conflict issue does not apply to Haswell microarchitecture.‚Äù</em></li>
<li><em>‚ÄúIn the Golden Cove microarchitecture, bank conflicts often happen when multiple loads access [‚Ä¶]‚Äù</em></li>
</ul>
<p>üí° So this was once an issue, then fixed, then it came back in another form.</p>
</blockquote>
<p>These are thankfully <strong>mitigated</strong> thanks to the random+exponential backoff, but are getting worse (<em>this pattern of ‚Äúyes, but‚Äù should really annoy you by now, that‚Äôs the whole point of this article</em>).</p>
<blockquote>
<p>Whenever possible, avoid reading the same memory location within a tight loop or using
multiple load operations.</p>
</blockquote>
<p>And the only way to really fix that is to‚Ä¶ actually park the thread by calling an OS primitive such as a futex! You should also avoid doing multiple loads per loop, as recommended <a href="#the-spin-lock-that-saturated-the-load-ports">previously</a>.</p>
<h2 id="what-about-specialized-instructions">What about specialized instructions?</h2>
<blockquote>
<p>ü§™ ‚ÄúI‚Äôve read about <code>MWAIT</code> and <code>TPAUSE</code>.‚Äù</p>
</blockquote>
<p>And you should probably have read further as those are privileged instructions! But yes they do have the same look as a futex wait/wake, which is very tempting.
And, to be fair, AMD does offer a userland alternative which is <code>monitorx</code> and <code>mwaitx</code> that we can use!</p>
<p>One advantage of <code>mwaitx</code> is that you can tell the CPU to wait for a given TSC count instead of having to loop! So it can be used to replace the <code>_mm_pause</code> loop when supported, and that‚Äôs actually what Windows‚Äô locking primitives such as <code>WaitOnAddress</code> or <code>AcquireSRWLockExclusive</code> do internally!
Not only is the ‚ÄúAPI‚Äù easier (<em>you provide a timestamp for the wakeup date</em>) but it can save power! <sup><a href="#user-content-fn-k" id="user-content-fnref-k" data-footnote-ref="" aria-describedby="footnote-label">18</a></sup> <sup><a href="#user-content-fn-p" id="user-content-fnref-p" data-footnote-ref="" aria-describedby="footnote-label">19</a></sup><br>
Just do not use it for <em>long</em> periods since you are still delaying potential work from other threads by not explicitly yielding to the OS.</p>
<blockquote>
<p>üí° <code>mwaitx</code> can spuriously wake up, but this is fine for our usage since we‚Äôll just spin and try again!</p>
</blockquote>
<h2 id="conclusion">Conclusion</h2>
<p>You‚Äôll notice I barely mentioned ARM, that‚Äôs because I do not have enough experience with this architecture to give any advice other than you should use the proper memory ordering for decent performance.</p>
<p>If you read this far, I‚Äôll say it again: in most (and pretty much all) cases you should not even need to worry about the performance of your locks. The best lock is the one you don‚Äôt use.</p>
<p>Again, from Linus: <sup><a href="#user-content-fn-4" id="user-content-fnref-4-3" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup></p>
<blockquote>
<p>Because you <strong>should never ever think that you‚Äôre clever enough</strong> to write your own locking routines.. Because the <strong>likelihood is that you aren‚Äôt</strong> (and by that ‚Äúyou‚Äù <strong>I very much include myself</strong> - we‚Äôve tweaked all the in-kernel locking over decades, and gone through the simple test-and-set to ticket locks to cacheline-efficient queuing locks, and even people who know what they are doing tend to get it wrong several times).</p>
<p>There‚Äôs a reason why you can find decades of academic papers on locking. <strong>Really. It‚Äôs hard.</strong></p>
</blockquote>
<p>But if you do, even after all those warnings, at least make sure you follow best practices and especially the pre-requisites for a spinlock to be efficient:</p>
<ul>
<li>There is low contention</li>
<li>The critical section (work done under the lock) is very small. (Consider that ‚Äúsmall‚Äù varies with the number of threads competing for the lock‚Ä¶)</li>
<li>Notify your OS about what you‚Äôre doing (<code>futex</code>, <code>WaitOnAddress</code>, ‚Ä¶)</li>
</ul>
<h2 id="bonus">Bonus!</h2>
<p>List of projects/libraries that do (<em>or did</em>) it wrong and that I happened to stumble upon:</p>
<ul>
<li><a href="https://github.com/mjansson/rpmalloc/blob/feb43aee0d4dcca9fd91b3dd54311c34c6cc6187/rpmalloc/rpmalloc.c#L284">RPMalloc</a>: the one that led to this rant, we had a dependency using it on console, and it caused livelocks. It only loops with a single CPU yield. Bad for perf, impossible (<em>read: will break</em>) to use on embedded platforms with a realtime scheduler.</li>
<li><a href="https://github.com/openbsd/src/blob/2a75873a967dc6c5bd16ad89b9dbf8713041be1b/lib/libc/thread/rthread.c#L167-L180">OpenBSD‚Äôs libc</a> goes straight to an OS thread <code>yield</code></li>
<li><a href="https://www.gnu.org/software/libc/">Glibc</a> goes straight to <code>futex</code> by default
<ul>
<li>default mutex is the <a href="https://github.com/bminor/glibc/blob/f9e61cd446d45016e20b6fe85ab87364ebdbec1b/nptl/pthread_mutex_lock.c#L87">‚Äúsimple‚Äù</a> one and only checks value once before going straight to the OS</li>
<li><a href="https://github.com/bminor/glibc/blob/f9e61cd446d45016e20b6fe85ab87364ebdbec1b/nptl/pthread_mutex_lock.c#L117"><code>PTHREAD_MUTEX_ADAPTIVE_NP</code></a> is mostly good! The number of spins is fixed by default
but can be tweaked using the tunable <a href="https://github.com/bminor/glibc/commit/6310e6be9b7c322d56a45729b3ebcd22e26dd0c2"><code>glibc.pthread.mutex_spin_count</code></a></li>
</ul>
</li>
<li><a href="https://github.com/uxlfoundation/oneTBB">Intel TBB</a>:
<ul>
<li><a href="https://github.com/uxlfoundation/oneTBB/blob/0cd32ab10a84eabf780bb699b17430deb028c0a4/include/oneapi/tbb/spin_mutex.h#L74"><code>tbb::spinlock</code></a>: simple backoff with 16 yields</li>
<li><code>tbb::mutex</code>: Pure <code>_mm_pause</code> backoff with <a href="https://github.com/uxlfoundation/oneTBB/blob/0cd32ab10a84eabf780bb699b17430deb028c0a4/include/oneapi/tbb/detail/_utils.h#L123-L133">fixed count</a>, then thread yield backoff, <a href="https://github.com/uxlfoundation/oneTBB/blob/0cd32ab10a84eabf780bb699b17430deb028c0a4/include/oneapi/tbb/detail/_waitable_atomic.h#L60-L70">then uses a futex wait</a> on linux, or plain semaphore on other platforms</li>
</ul>
</li>
<li><a href="https://github.com/WebKit/WebKit/">Webkit</a>:
<ul>
<li>OS <a href="https://github.com/WebKit/WebKit/blob/202580d81145c23c273cc4d3a82dae062c449321/Source/WTF/wtf/LockAlgorithmInlines.h#L60">Thread yield</a> (<code>SwitchToThread</code>/<code>sched_yield</code>) on CAS failure</li>
<li><a href="https://github.com/WebKit/WebKit/blob/77b3be1d7a731a6280de28e4637392c8ec4f7fb2/Source/WTF/wtf/LockAlgorithmInlines.h#L43">Hardcoded spincount</a></li>
<li>Duplicated <a href="https://github.com/WebKit/WebKit/blob/77b3be1d7a731a6280de28e4637392c8ec4f7fb2/Source/WTF/wtf/WordLock.cpp#L62">here</a></li>
<li>Same thing <a href="https://github.com/WebKit/WebKit/blob/77b3be1d7a731a6280de28e4637392c8ec4f7fb2/Source/bmalloc/bmalloc/Mutex.cpp#L57">here</a></li>
<li>Even though they have <a href="https://github.com/WebKit/WebKit/blob/a05334f6848b4433928b4a0b5120e6a765a55f69/Source/WTF/benchmarks/ToyLocks.h">benchmarks</a> (<em>both for speed and fairness</em>)!</li>
</ul>
</li>
<li><a href="https://github.com/google/sanitizers/wiki/AddressSanitizer">AddressSanitizer (ASAN)</a>
<ul>
<li>The <a href="https://github.com/google/sanitizers/issues/614">Issue</a> =&gt; can livelock with a realtime scheduler</li>
<li>The <a href="https://github.com/llvm/llvm-project/blob/23e35bd43cf18ee479e6d5df08189db4591c403c/compiler-rt/lib/sanitizer_common/sanitizer_mutex.cpp#L19-L29">Implementation</a></li>
<li>Yield is done by <a href="https://github.com/llvm/llvm-project/blob/23e35bd43cf18ee479e6d5df08189db4591c403c/compiler-rt/lib/sanitizer_common/sanitizer_linux.cpp#L595">sched_yield</a> on linux and <a href="https://github.com/llvm/llvm-project/blob/23e35bd43cf18ee479e6d5df08189db4591c403c/compiler-rt/lib/sanitizer_common/sanitizer_win.cpp#L847"><code>Sleep(0)</code></a> on Windows</li>
</ul>
</li>
<li><a href="https://github.com/dotnet/runtime">DotNet</a> runtime still uses <a href="https://github.com/dotnet/runtime/blob/9202b4c14523718f5bcbf14025d5286cbf738f70/src/libraries/System.Private.CoreLib/src/System/Threading/SpinWait.cs#L195-L210"><code>Sleep</code></a> instead of WaitOnAddress</li>
<li>And so many others‚Ä¶</li>
</ul>
<section data-footnotes="">
<ol>
<li id="user-content-fn-1">
<p><a href="https://probablydance.com/2019/12/30/measuring-mutexes-spinlocks-and-how-bad-the-linux-scheduler-really-is/">Measuring Mutexes, Spinlocks and how Bad the Linux Scheduler Really Is</a> - Malte Skarupke <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1">‚Ü©</a></p>
</li>
<li id="user-content-fn-2">
<p><a href="https://matklad.github.io/2020/01/02/spinlocks-considered-harmful.html">Spinlocks Considered Harmful</a> - matklad <a href="#user-content-fnref-2" data-footnote-backref="" aria-label="Back to reference 2">‚Ü©</a></p>
</li>
<li id="user-content-fn-3">
<p><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/intel64-and-ia32-architectures-optimization.html">Intel¬Æ 64 and IA-32 Architectures Optimization Reference Manual</a> <a href="#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 3">‚Ü©</a> <a href="#user-content-fnref-3-2" data-footnote-backref="" aria-label="Back to reference 3-2">‚Ü©<sup>2</sup></a> <a href="#user-content-fnref-3-3" data-footnote-backref="" aria-label="Back to reference 3-3">‚Ü©<sup>3</sup></a></p>
</li>
<li id="user-content-fn-4">
<p><a href="https://www.realworldtech.com/forum/?threadid=189711&amp;curpostid=189723">Linus Torvalds on spinlocks</a> (1) - Real World Technologies Forum <a href="#user-content-fnref-4" data-footnote-backref="" aria-label="Back to reference 4">‚Ü©</a> <a href="#user-content-fnref-4-2" data-footnote-backref="" aria-label="Back to reference 4-2">‚Ü©<sup>2</sup></a> <a href="#user-content-fnref-4-3" data-footnote-backref="" aria-label="Back to reference 4-3">‚Ü©<sup>3</sup></a></p>
</li>
<li id="user-content-fn-5">
<p><a href="https://hal.science/hal-02084060/file/journal.pdf">Lock‚ÄìUnlock: Is That All? A Pragmatic Analysis of Locking in Software Systems</a> - Rachid Guerraoui, Hugo Guiroux, Renaud Lachaize, Vivien Qu√©ma, Vasileios Trigonakis. ACM Transactions on Computer Systems, 2019, pp.1-149. <a href="#user-content-fnref-5" data-footnote-backref="" aria-label="Back to reference 5">‚Ü©</a> <a href="#user-content-fnref-5-2" data-footnote-backref="" aria-label="Back to reference 5-2">‚Ü©<sup>2</sup></a></p>
</li>
<li id="user-content-fn-x">
<p><a href="https://web.archive.org/web/20240925152810/https://www.intel.com/content/www/us/en/developer/articles/technical/spin-locks-considered-harmful.html">Spin Locks Considered Harmful, and How to Write Them When We Must</a> - Intel (via The Wayback Machine) <a href="#user-content-fnref-x" data-footnote-backref="" aria-label="Back to reference 6">‚Ü©</a></p>
</li>
<li id="user-content-fn-w">
<p><a href="https://uops.info/table.html?search=PAUSE&amp;cb_tp=on&amp;cb_SNB=on&amp;cb_IVB=on&amp;cb_HSW=on&amp;cb_BDW=on&amp;cb_SKL=on&amp;cb_KBL=on&amp;cb_CFL=on&amp;cb_CNL=on&amp;cb_CLX=on&amp;cb_ICL=on&amp;cb_RKL=on&amp;cb_ADLP=on&amp;cb_TRM=on&amp;cb_ADLE=on&amp;cb_ZENp=on&amp;cb_ZEN2=on&amp;cb_ZEN3=on&amp;cb_ZEN4=on&amp;cb_measurements=on&amp;cb_base=on&amp;cb_sse=on&amp;cb_others=on&amp;checkbox=on">uops.info table for PAUSE</a> <a href="#user-content-fnref-w" data-footnote-backref="" aria-label="Back to reference 7">‚Ü©</a></p>
</li>
<li id="user-content-fn-f">
<p><a href="https://fgiesen.wordpress.com/2014/07/07/cache-coherency/">Cache coherency primer</a> - Fabian Giesen <a href="#user-content-fnref-f" data-footnote-backref="" aria-label="Back to reference 8">‚Ü©</a></p>
</li>
<li id="user-content-fn-a">
<p><a href="https://gpuopen.com/download/GDC2024_AMD_Ryzen_Processor_Software_Optimization.pdf">AMD Ryzen Processor Software Optimization (GDC 2024)</a> - Ken Mitchell <a href="#user-content-fnref-a" data-footnote-backref="" aria-label="Back to reference 9">‚Ü©</a></p>
</li>
<li id="user-content-fn-b">
<p><a href="https://gpuopen.com/download/gdc_2018_sponsored_optimizing_for_ryzen.pdf">AMD RYZEN‚Ñ¢ Cpu Optimization (GDC 2018)</a> - Ken Mitchell &amp; Elliot Kim <a href="#user-content-fnref-b" data-footnote-backref="" aria-label="Back to reference 10">‚Ü©</a></p>
</li>
<li id="user-content-fn-y">
<p><a href="https://rigtorp.se/spinlock/">Correctly implementing a spinlock in C++</a> - Erik Rigtorp <a href="#user-content-fnref-y" data-footnote-backref="" aria-label="Back to reference 11">‚Ü©</a></p>
</li>
<li id="user-content-fn-stt">
<p><a href="https://learn.microsoft.com/en-us/windows/win32/api/processthreadsapi/nf-processthreadsapi-switchtothread?redirectedfrom=MSDN">SwitchToThread docs</a> - Microsoft <a href="#user-content-fnref-stt" data-footnote-backref="" aria-label="Back to reference 12">‚Ü©</a> <a href="#user-content-fnref-stt-2" data-footnote-backref="" aria-label="Back to reference 12-2">‚Ü©<sup>2</sup></a></p>
</li>
<li id="user-content-fn-q">
<p><a href="https://www.realworldtech.com/forum/?threadid=189711&amp;curpostid=189837">Linus Torvalds on spinlocks</a> (3) - Real World Technologies Forum <a href="#user-content-fnref-q" data-footnote-backref="" aria-label="Back to reference 13">‚Ü©</a> <a href="#user-content-fnref-q-2" data-footnote-backref="" aria-label="Back to reference 13-2">‚Ü©<sup>2</sup></a> <a href="#user-content-fnref-q-3" data-footnote-backref="" aria-label="Back to reference 13-3">‚Ü©<sup>3</sup></a></p>
</li>
<li id="user-content-fn-s">
<p><a href="https://learn.microsoft.com/en-us/archive/blogs/khen1234/sleeping-vs-yielding">Sleeping vs. Yielding</a> - Ken Henderson <a href="#user-content-fnref-s" data-footnote-backref="" aria-label="Back to reference 14">‚Ü©</a></p>
</li>
<li id="user-content-fn-c">
<p><a href="https://davekilian.com/lock-convoys.html">A Complete Guide to Lock Convoys</a> - Dave Kilian <a href="#user-content-fnref-c" data-footnote-backref="" aria-label="Back to reference 15">‚Ü©</a></p>
</li>
<li id="user-content-fn-l">
<p><a href="https://www.realworldtech.com/forum/?threadid=189711&amp;curpostid=189752">Linus Torvalds on spinlocks</a> (2) - Real World Technologies Forum <a href="#user-content-fnref-l" data-footnote-backref="" aria-label="Back to reference 16">‚Ü©</a></p>
</li>
<li id="user-content-fn-z">
<p><a href="https://blog.bearcats.nl/atomic-wait/">Implementing atomic wait and notify</a> - Blat Blatnik <a href="#user-content-fnref-z" data-footnote-backref="" aria-label="Back to reference 17">‚Ü©</a></p>
</li>
<li id="user-content-fn-k">
<p><a href="https://github.com/SpecialKO/SpecialK/commit/5b55f447eb4d3ab839f73255bd9f6aa5ade20aeb">SpecialK‚Äôs commit automating <code>mwaitx</code>‚Äôs usage</a> - Andon M. Coleman <a href="#user-content-fnref-k" data-footnote-backref="" aria-label="Back to reference 18">‚Ü©</a></p>
</li>
<li id="user-content-fn-p">
<p><a href="https://dl.acm.org/doi/pdf/10.1145/3676151.3719370">An Analysis of User-space Idle State Instructions on x86 Processors</a> - Malte-Christian Kuns, Hannes Tr√∂pgen, Robert Sch√∂ne. ICPE ‚Äò25: Proceedings of the 16th ACM/SPEC International Conference on Performance Engineering, 2025, pp.232-239. <a href="#user-content-fnref-p" data-footnote-backref="" aria-label="Back to reference 19">‚Ü©</a></p>
</li>
</ol>
</section></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Oban, the job processing framework from Elixir, has come to Python (231 pts)]]></title>
            <link>https://www.dimamik.com/posts/oban_py/</link>
            <guid>46797594</guid>
            <pubDate>Wed, 28 Jan 2026 16:32:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dimamik.com/posts/oban_py/">https://www.dimamik.com/posts/oban_py/</a>, See on <a href="https://news.ycombinator.com/item?id=46797594">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<article>
     
    <div><h2 id="setting-the-stage">Setting the Stage</h2>
<p>I‚Äôve used <a href="https://github.com/oban-bg/oban">Oban in Elixir</a> for almost as long as I‚Äôve been writing software in Elixir, and it has always been an essential tool for processing jobs. I always knew Oban was cool, but I never dug deeper. This article is a collection of my notes and observations on how the Python implementation of Oban works and what I‚Äôve learned while exploring its codebase. I‚Äôll also try to compare it with the Elixir version and talk about concurrency in general.</p>
<h2 id="surface-level">Surface Level</h2>
<p><a href="https://oban.pro/docs/py/index.html">Oban</a> allows you to <strong>insert and process jobs using only your database</strong>. You can insert the job to send a confirmation email in the same database transaction where you create the user. If one thing fails, everything is rolled back.</p>
<p>Additionally, like most job processing frameworks, Oban has <a href="https://oban.pro/docs/py/0.5.0/defining_queues.html">queues</a> with local and global queue limits. But unlike others, it <strong>stores your completed jobs</strong> and can <a href="https://oban.pro/docs/py/0.5.0/writing_jobs.html#recording-results">even keep their results if needed</a>. It has built-in <a href="https://oban.pro/docs/py/0.5.0/periodic_jobs.html#periodic-jobs">cron scheduling</a> and <a href="https://oban.pro/docs/py/0.5.0/managing_queues.html">many more features</a> to control how your jobs are processed.</p>
<p>Oban comes in two versions - <a href="https://oban.pro/docs/py/index.html">Open Source Oban-py</a> and <a href="https://oban.pro/docs/py_pro/adoption.html">commercial Oban-py-pro</a>.</p>
<p>OSS Oban has a few limitations, which are automatically lifted in the Pro version:</p>
<ul>
<li><strong>Single-threaded asyncio execution</strong> - concurrent but not truly parallel, so CPU-bound jobs block the event loop.</li>
<li><strong>No bulk inserts</strong> - each job is inserted individually.</li>
<li><strong>No bulk acknowledgements</strong> - each job completion is persisted individually.</li>
<li><strong>Inaccurate rescues</strong> - jobs that are long-running might get rescued even if the producer is still alive. Pro version uses smarter heartbeats to track producer liveness.</li>
</ul>
<p>In addition, Oban-py-pro comes with a few extra features you‚Äôd configure separately, like <a href="https://oban.pro/docs/py_pro/0.5.0/workflow.html">workflows</a>, <a href="https://oban.pro/docs/py_pro/0.5.0/relay.html">relay</a>, <a href="https://oban.pro/docs/py_pro/0.5.0/unique_jobs.html">unique jobs</a>, and <a href="https://oban.pro/docs/py_pro/0.5.0/smart_concurrency.html">smart concurrency</a>.</p>
<p>OSS Oban-py is a great start for your hobby project, or if you‚Äôd want to evaluate Oban philosophy itself, but for any bigger scale - I‚Äôd go with <a href="https://oban.pro/docs/py_pro/index.html">Oban Pro</a>. The pricing seems very compelling, considering the amount of work put into making the above features work.</p>
<p>I obviously can‚Äôt walk you through the Pro version features, but let‚Äôs start with the basics. How Oban Py works under the hood, from the job insertion until the job execution. Stay tuned.</p>
<h2 id="going-deeper---job-processing-path">Going Deeper - Job Processing Path</h2>
<p>Let‚Äôs get straight to it. You insert your job:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> oban <span>import</span> job
</span></span><span><span>
</span></span><span><span><span>@job</span>(queue<span>=</span><span>"default"</span>)
</span></span><span><span><span>async</span> <span>def</span> <span>send_email</span>(to: str, subject: str, body: str):
</span></span><span><span>    <span># Simple and clean, but no access to job context</span>
</span></span><span><span>    <span>await</span> smtp<span>.</span>send(to, subject, body)
</span></span><span><span>
</span></span><span><span><span>await</span> send_email<span>.</span>enqueue(<span>"<a href="https://www.dimamik.com/cdn-cgi/l/email-protection" data-cfemail="cbbeb8aeb98baeb3aaa6bba7aee5a8a4a6">[email&nbsp;protected]</a>"</span>, <span>"Hello"</span>, <span>"World"</span>)
</span></span></code></pre></div><p>After the insertion, the job lands in the <code>oban_jobs</code> database table with <code>state = 'available'</code>. Oban fires off a PostgreSQL <code>NOTIFY</code> on the <code>insert</code> channel:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># oban.py:414-419</span>
</span></span><span><span><span># Single inserts go through bulk insert path</span>
</span></span><span><span>result <span>=</span> <span>await</span> self<span>.</span>_query<span>.</span>insert_jobs(jobs)
</span></span><span><span>queues <span>=</span> {job<span>.</span>queue <span>for</span> job <span>in</span> result <span>if</span> job<span>.</span>state <span>==</span> <span>"available"</span>}
</span></span><span><span><span>await</span> self<span>.</span>_notifier<span>.</span>notify(<span>"insert"</span>, [{<span>"queue"</span>: queue} <span>for</span> queue <span>in</span> queues])
</span></span></code></pre></div><p>Every Oban node listening on that channel receives the notification. <strong>The Stager</strong> on each node gets woken up, but each <strong>Stager</strong> only cares about queues it‚Äôs actually running. Be aware that each node decides which queues it runs, so if the current node runs this queue, the producer is notified:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># _stager.py:95-99</span>
</span></span><span><span><span>async</span> <span>def</span> <span>_on_notification</span>(self, channel: str, payload: dict) <span>-&gt;</span> <span>None</span>:
</span></span><span><span>    queue <span>=</span> payload[<span>"queue"</span>]
</span></span><span><span>
</span></span><span><span>    <span>if</span> queue <span>in</span> self<span>.</span>_producers:
</span></span><span><span>        self<span>.</span>_producers[queue]<span>.</span>notify()
</span></span></code></pre></div><p>That <code>notify()</code> call sets an <code>asyncio.Event</code>, breaking <strong>the Producer</strong> out of its wait loop, so it can dispatch the jobs to the workers:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># _producer.py:244-262</span>
</span></span><span><span><span>async</span> <span>def</span> <span>_loop</span>(self) <span>-&gt;</span> <span>None</span>:
</span></span><span><span>    <span>while</span> <span>True</span>:
</span></span><span><span>        <span>try</span>:
</span></span><span><span>            <span># &lt;--- This is where the event is received ---&gt;</span>
</span></span><span><span>            <span>await</span> asyncio<span>.</span>wait_for(self<span>.</span>_notified<span>.</span>wait(), timeout<span>=</span><span>1.0</span>)
</span></span><span><span>        <span>except</span> asyncio<span>.</span>TimeoutError:
</span></span><span><span>            <span>continue</span>
</span></span><span><span>        <span>except</span> asyncio<span>.</span>CancelledError:
</span></span><span><span>            <span>break</span>
</span></span><span><span>
</span></span><span><span>        <span># &lt;--- Reset the event so it can be triggered for the next batch ---&gt;</span>
</span></span><span><span>        self<span>.</span>_notified<span>.</span>clear()
</span></span><span><span>
</span></span><span><span>        <span>try</span>:
</span></span><span><span>            <span># &lt;--- A little debounce to potentially process multiple jobs at once ---&gt;</span>
</span></span><span><span>            <span>await</span> self<span>.</span>_debounce()
</span></span><span><span>            <span># &lt;--- Dispatch (Produce) the jobs from the database to the workers ---&gt;</span>
</span></span><span><span>            <span>await</span> self<span>.</span>_produce()
</span></span><span><span>        <span>except</span> asyncio<span>.</span>CancelledError:
</span></span><span><span>            <span>break</span>
</span></span><span><span>        <span>except</span> <span>Exception</span>:
</span></span><span><span>            logger<span>.</span>exception(<span>"Error in producer for queue </span><span>%s</span><span>"</span>, self<span>.</span>_queue)
</span></span></code></pre></div><p>Before fetching the jobs, <strong>the producer</strong> persists all pre-existing job completions (acks) to the database to make sure queue limits are respected. Next, it fetches new jobs, transitioning their state to executing at the same time. A slightly more complex version of this SQL is used:</p>
<div><pre tabindex="0"><code data-lang="sql"><span><span><span>-- fetch_jobs.sql (simplified)
</span></span></span><span><span><span></span><span>WITH</span> locked_jobs <span>AS</span> (
</span></span><span><span>  <span>SELECT</span> priority, scheduled_at, id
</span></span><span><span>  <span>FROM</span>
</span></span><span><span>  oban_jobs
</span></span><span><span>  <span>WHERE</span> <span>state</span> <span>=</span> <span>'available'</span> <span>AND</span> queue <span>=</span> <span>%</span>(queue)s
</span></span><span><span>  <span>ORDER</span> <span>BY</span> priority <span>ASC</span>, scheduled_at <span>ASC</span>, id <span>ASC</span>
</span></span><span><span>  <span>LIMIT</span> <span>%</span>(demand)s
</span></span><span><span>  <span>FOR</span> <span>UPDATE</span> SKIP LOCKED
</span></span><span><span>)
</span></span><span><span><span>UPDATE</span> oban_jobs oj
</span></span><span><span><span>SET</span>
</span></span><span><span>  attempt <span>=</span> oj.attempt <span>+</span> <span>1</span>,
</span></span><span><span>  attempted_at <span>=</span> timezone(<span>'UTC'</span>, now()),
</span></span><span><span>  attempted_by <span>=</span> <span>%</span>(attempted_by)s,
</span></span><span><span>  <span>state</span> <span>=</span> <span>'executing'</span>
</span></span><span><span><span>FROM</span> locked_jobs
</span></span><span><span><span>WHERE</span> oj.id <span>=</span> locked_jobs.id
</span></span></code></pre></div><p>And this is <strong>the first really cool part</strong>.</p>
<p><strong>Segue to <em>FOR UPDATE SKIP LOCKED</em>.</strong></p>
<ul>
<li>
<p><code>FOR UPDATE</code> - Locks the selected rows so no other transaction can modify them until this transaction completes. This prevents two producers from grabbing the same job.</p>
</li>
<li>
<p><code>SKIP LOCKED</code> - If a row is already locked by another transaction, skip it instead of waiting. This is crucial for concurrency.</p>
</li>
</ul>
<p>Why this matters for job queues:
Imagine two producer instances (A and B) trying to fetch jobs simultaneously:</p>
<table>
  <thead>
      <tr>
          <th>Without SKIP LOCKED</th>
          <th>With SKIP LOCKED</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>A locks job #1</td>
          <td>A locks job #1</td>
      </tr>
      <tr>
          <td>B <strong>waits</strong> for job #1 to unlock</td>
          <td>B <strong>skips</strong> job #1, takes job #2</td>
      </tr>
      <tr>
          <td>Slow, sequential processing</td>
          <td>Fast, parallel processing</td>
      </tr>
  </tbody>
</table>
<p>Back in Python, we know that the jobs we just fetched should be processed immediately. When we fetched the job, we already transitioned its state and respected the queue demand.</p>
<p>Each job gets dispatched as an <strong>async task</strong>:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span>jobs <span>=</span> <span>await</span> self<span>.</span>_get_jobs()
</span></span><span><span><span>for</span> job <span>in</span> jobs:
</span></span><span><span>    task <span>=</span> self<span>.</span>_dispatcher<span>.</span>dispatch(self, job)
</span></span><span><span>    task<span>.</span>add_done_callback(
</span></span><span><span>        <span>lambda</span> _, job_id<span>=</span>job<span>.</span>id: self<span>.</span>_on_job_complete(job_id)
</span></span><span><span>    )
</span></span><span><span>
</span></span><span><span>    self<span>.</span>_running_jobs[job<span>.</span>id] <span>=</span> (job, task)
</span></span></code></pre></div><p><code>add_done_callback</code> ensures that independent of success or failure, we can attach a callback to handle job completion.</p>
<p><strong>The dispatcher</strong> controls how exactly the job is run. For the non-pro Oban version, it just uses <code>asyncio.create_task</code> to run the job in the event loop:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># _producer.py:69-71</span>
</span></span><span><span><span>class</span> <span>LocalDispatcher</span>:
</span></span><span><span>    <span>def</span> <span>dispatch</span>(self, producer: Producer, job: Job) <span>-&gt;</span> asyncio<span>.</span>Task:
</span></span><span><span>        <span>return</span> asyncio<span>.</span>create_task(producer<span>.</span>_execute(job))
</span></span></code></pre></div><p><a href="https://oban.pro/releases/py_pro">For pro version</a>, local asyncio dispatcher is automatically replaced with a pool of processes, so you don‚Äôt need to do anything to have true parallelism across multiple cores.</p>
<p>After the job is dispatched, <strong>the Executor</strong> takes over. It resolves your worker class from the string name, runs it, and pattern-matches the result:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># _executor.py:73-83</span>
</span></span><span><span><span>async</span> <span>def</span> <span>_process</span>(self) <span>-&gt;</span> <span>None</span>:
</span></span><span><span>  self<span>.</span>worker <span>=</span> resolve_worker(self<span>.</span>job<span>.</span>worker)()
</span></span><span><span>  self<span>.</span>result <span>=</span> <span>await</span> self<span>.</span>worker<span>.</span>process(self<span>.</span>job)
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="python"><span><span><span># _executor.py:95-133</span>
</span></span><span><span><span>match</span> result:
</span></span><span><span>    <span>case</span> <span>Exception</span>() <span>as</span> error:
</span></span><span><span>        <span># Retry or discard based on attempt count</span>
</span></span><span><span>    <span>case</span> Cancel(reason<span>=</span>reason):
</span></span><span><span>        <span># Mark cancelled</span>
</span></span><span><span>    <span>case</span> Snooze(seconds<span>=</span>seconds):
</span></span><span><span>        <span># Reschedule with decremented attempt</span>
</span></span><span><span>    <span>case</span> _:
</span></span><span><span>        <span># Completed successfully</span>
</span></span></code></pre></div><p>And that‚Äôs <strong>the second cool part</strong>! You see how similar it is to <a href="https://hexdocs.pm/elixir/pattern-matching.html">Elixir‚Äôs pattern matching</a>? I love how it‚Äôs implemented!</p>
<p>When execution finishes, the result gets queued for acknowledgement:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># _producer.py:315</span>
</span></span><span><span>self<span>.</span>_pending_acks<span>.</span>append(executor<span>.</span>action)
</span></span></code></pre></div><p>The completion callback notifies <strong>the Producer</strong> to wake up again-fetch more jobs, and batch-ack the finished ones in a single query.</p>
<p>That‚Äôs the hot path: <code>Insert ‚Üí Notify ‚Üí Fetch (with locking) ‚Üí Execute ‚Üí Ack.</code> Five hops from your code to completion. What about the background processes? What about errors and retries? What about periodic jobs, cron, and all these other pieces? Stay tuned.</p>
<h2 id="the-undercurrents---background-processes">The Undercurrents - Background Processes</h2>
<p>Oban runs several background loops that keep the system healthy.</p>
<h3 id="leader-election">Leader Election</h3>
<p>In a cluster, you don‚Äôt want every node pruning jobs or rescuing orphans. Oban elects a single leader:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># _leader.py:107-113</span>
</span></span><span><span><span>async</span> <span>def</span> <span>_election</span>(self) <span>-&gt;</span> <span>None</span>:
</span></span><span><span>    self<span>.</span>_is_leader <span>=</span> <span>await</span> self<span>.</span>_query<span>.</span>attempt_leadership(
</span></span><span><span>        self<span>.</span>_name, self<span>.</span>_node, int(self<span>.</span>_interval), self<span>.</span>_is_leader
</span></span><span><span>    )
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="sql"><span><span><span>-- Cleanup expired leaders first
</span></span></span><span><span><span></span><span>DELETE</span> <span>FROM</span>
</span></span><span><span>  oban_leaders
</span></span><span><span><span>WHERE</span>
</span></span><span><span>  expires_at <span>&lt;</span> timezone(<span>'UTC'</span>, now())
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="sql"><span><span><span>-- If current node is a leader, it re-elects itself
</span></span></span><span><span><span></span><span>INSERT</span> <span>INTO</span> oban_leaders (name, node, elected_at, expires_at)
</span></span><span><span><span>VALUES</span> (
</span></span><span><span>  <span>%</span>(name)s,
</span></span><span><span>  <span>%</span>(node)s,
</span></span><span><span>  timezone(<span>'UTC'</span>, now()),
</span></span><span><span>  timezone(<span>'UTC'</span>, now()) <span>+</span> interval <span>'%(ttl)s seconds'</span>
</span></span><span><span>)
</span></span><span><span><span>ON</span> CONFLICT (name) <span>DO</span> <span>UPDATE</span> <span>SET</span>
</span></span><span><span>  <span>-- Only update if we're the same node (i.e. current leader re-electing itself).
</span></span></span><span><span><span></span>  <span>-- Other nodes can't overwrite an active leader's lease.
</span></span></span><span><span><span></span>  expires_at <span>=</span> EXCLUDED.expires_at
</span></span><span><span><span>WHERE</span>
</span></span><span><span>  oban_leaders.node <span>=</span> EXCLUDED.node
</span></span><span><span>RETURNING node
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="sql"><span><span><span>-- Try to insert as a new leader if no leader exists
</span></span></span><span><span><span></span><span>INSERT</span> <span>INTO</span> oban_leaders (
</span></span><span><span>  name, node, elected_at, expires_at
</span></span><span><span>) <span>VALUES</span> (
</span></span><span><span>  <span>%</span>(name)s,
</span></span><span><span>  <span>%</span>(node)s,
</span></span><span><span>  timezone(<span>'UTC'</span>, now()),
</span></span><span><span>  timezone(<span>'UTC'</span>, now()) <span>+</span> interval <span>'%(ttl)s seconds'</span>
</span></span><span><span>)
</span></span><span><span><span>ON</span> CONFLICT (name) <span>DO</span> <span>NOTHING</span>
</span></span><span><span>RETURNING node
</span></span></code></pre></div><p>The leader refreshes twice as often to hold onto the role:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># _leader.py:101-105</span>
</span></span><span><span><span># Sleep for half interval if leader (to boost their refresh interval and allow them to</span>
</span></span><span><span><span># retain leadership), full interval otherwise</span>
</span></span><span><span>sleep_duration <span>=</span> self<span>.</span>_interval <span>/</span> <span>2</span> <span>if</span> self<span>.</span>_is_leader <span>else</span> self<span>.</span>_interval
</span></span></code></pre></div><p>When a node shuts down cleanly, it resigns and notifies the cluster:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># _leader.py:83-87</span>
</span></span><span><span><span>if</span> self<span>.</span>_is_leader:
</span></span><span><span>    payload <span>=</span> {<span>"action"</span>: <span>"resign"</span>, <span>"node"</span>: self<span>.</span>_node, <span>"name"</span>: self<span>.</span>_name}
</span></span><span><span>
</span></span><span><span>    <span>await</span> self<span>.</span>_notifier<span>.</span>notify(<span>"leader"</span>, payload)
</span></span><span><span>    <span>await</span> self<span>.</span>_query<span>.</span>resign_leader(self<span>.</span>_name, self<span>.</span>_node)
</span></span></code></pre></div><p>And that‚Äôs <strong>the third cool part</strong>! Leader election is delegated entirely to PostgreSQL. Oban uses <code>INSERT ... ON CONFLICT</code> with a TTL-based lease - no Raft, no consensus protocol, no external coordination service. If the leader dies, its lease expires and the next node to run the election query takes over. Simple, effective, and zero additional infrastructure.</p>
<h3 id="lifeline-rescuing-orphaned-jobs">Lifeline: Rescuing Orphaned Jobs</h3>
<p>Workers crash. Containers get killed. When that happens, jobs can get stuck executing indefinitely. <strong>The Lifeline</strong> process (leader-only) rescues them:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># _lifeline.py:73-77</span>
</span></span><span><span><span>async</span> <span>def</span> <span>_rescue</span>(self) <span>-&gt;</span> <span>None</span>:
</span></span><span><span>    <span>if</span> <span>not</span> self<span>.</span>_leader<span>.</span>is_leader:
</span></span><span><span>        <span>return</span>
</span></span><span><span>
</span></span><span><span>    <span>await</span> use_ext(<span>"lifeline.rescue"</span>, _rescue, self<span>.</span>_query, self<span>.</span>_rescue_after)
</span></span></code></pre></div><p>Oban-py rescue mechanics are purely time-based - any job in <code>executing</code> state longer than <code>rescue_after</code> (default: 5 minutes) gets moved back. Unlike the Oban Pro version, it doesn‚Äôt check whether the producer that owns the job is still alive. This means legitimately long-running jobs could be rescued and executed a second time.</p>
<p>The takeaway is that you should set <code>rescue_after</code> higher than your longest expected job duration, and design workers to be idempotent.</p>
<p>The SQL itself is straightforward - jobs stuck executing get moved back to available or discarded if they‚Äôve exhausted retries:</p>
<div><pre tabindex="0"><code data-lang="sql"><span><span><span>-- rescue_jobs.sql (simplified)
</span></span></span><span><span><span></span><span>UPDATE</span> oban_jobs
</span></span><span><span><span>SET</span>
</span></span><span><span>  <span>state</span> <span>=</span> <span>CASE</span>
</span></span><span><span>    <span>WHEN</span> attempt <span>&gt;=</span> max_attempts <span>THEN</span> <span>'discarded'</span>
</span></span><span><span>    <span>ELSE</span> <span>'available'</span>
</span></span><span><span>  <span>END</span>,
</span></span><span><span>  meta <span>=</span> <span>CASE</span>
</span></span><span><span>    <span>WHEN</span> attempt <span>&gt;=</span> max_attempts <span>THEN</span> meta
</span></span><span><span>    <span>ELSE</span> meta <span>||</span> jsonb_build_object(<span>'rescued'</span>, coalesce((meta<span>-&gt;&gt;</span><span>'rescued'</span>)::int, <span>0</span>) <span>+</span> <span>1</span>)
</span></span><span><span>  <span>END</span>
</span></span><span><span><span>WHERE</span>
</span></span><span><span>  <span>state</span> <span>=</span> <span>'executing'</span>
</span></span><span><span>  <span>AND</span> attempted_at <span>&lt;</span> timezone(<span>'UTC'</span>, now()) <span>-</span> make_interval(secs <span>=&gt;</span> <span>%</span>(rescue_after)s)
</span></span></code></pre></div><p>The rescued counter in meta lets you track how often jobs needed saving.</p>
<h3 id="pruner-cleaning-up-old-jobs">Pruner: Cleaning Up Old Jobs</h3>
<p>Without pruning, your oban_jobs table grows forever. <strong>The Pruner</strong> (also leader-only) deletes terminal jobs older than max_age (default: 1 day):</p>
<div><pre tabindex="0"><code data-lang="sql"><span><span><span>-- prune_jobs.sql
</span></span></span><span><span><span></span><span>WITH</span> jobs_to_delete <span>AS</span> (
</span></span><span><span><span>SELECT</span> id <span>FROM</span> oban_jobs
</span></span><span><span><span>WHERE</span>
</span></span><span><span>(<span>state</span> <span>=</span> <span>'completed'</span> <span>AND</span> completed_at <span>&lt;=</span> timezone(<span>'UTC'</span>, now()) <span>-</span> make_interval(secs <span>=&gt;</span> <span>%</span>(max_age)s)) <span>OR</span>
</span></span><span><span>(<span>state</span> <span>=</span> <span>'cancelled'</span> <span>AND</span> cancelled_at <span>&lt;=</span> timezone(<span>'UTC'</span>, now()) <span>-</span> make_interval(secs <span>=&gt;</span> <span>%</span>(max_age)s)) <span>OR</span>
</span></span><span><span>(<span>state</span> <span>=</span> <span>'discarded'</span> <span>AND</span> discarded_at <span>&lt;=</span> timezone(<span>'UTC'</span>, now()) <span>-</span> make_interval(secs <span>=&gt;</span> <span>%</span>(max_age)s))
</span></span><span><span><span>ORDER</span> <span>BY</span> id <span>ASC</span>
</span></span><span><span><span>LIMIT</span> <span>%</span>(<span>limit</span>)s
</span></span><span><span>)
</span></span><span><span><span>DELETE</span> <span>FROM</span> oban_jobs <span>WHERE</span> id <span>IN</span> (<span>SELECT</span> id <span>FROM</span> jobs_to_delete)
</span></span></code></pre></div><p>The LIMIT prevents long-running deletes from blocking other operations.</p>
<h3 id="retry--backoff-mechanics">Retry &amp; Backoff Mechanics</h3>
<p>When a job raises an exception, <strong>the Executor</strong> decides its fate:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># _executor.py:96-109</span>
</span></span><span><span><span>match</span> result:
</span></span><span><span>    <span>case</span> <span>Exception</span>() <span>as</span> error:
</span></span><span><span>        <span>if</span> self<span>.</span>job<span>.</span>attempt <span>&gt;=</span> self<span>.</span>job<span>.</span>max_attempts:
</span></span><span><span>            self<span>.</span>action <span>=</span> AckAction(
</span></span><span><span>                job<span>=</span>self<span>.</span>job,
</span></span><span><span>                state<span>=</span><span>"discarded"</span>,
</span></span><span><span>                error<span>=</span>self<span>.</span>_format_error(error),
</span></span><span><span>            )
</span></span><span><span>        <span>else</span>:
</span></span><span><span>            self<span>.</span>action <span>=</span> AckAction(
</span></span><span><span>                job<span>=</span>self<span>.</span>job,
</span></span><span><span>                state<span>=</span><span>"retryable"</span>,
</span></span><span><span>                error<span>=</span>self<span>.</span>_format_error(error),
</span></span><span><span>                schedule_in<span>=</span>self<span>.</span>_retry_backoff(),
</span></span><span><span>            )
</span></span></code></pre></div><p>Simple rule: under <code>max_attempts</code> - retry, otherwise - discard.</p>
<p>The default backoff uses jittery-clamped exponential growth with randomness to prevent thundering herds:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># _backoff.py:66-87</span>
</span></span><span><span><span>def</span> <span>jittery_clamped</span>(attempt: int, max_attempts: int, <span>*</span>, clamped_max: int <span>=</span> <span>20</span>) <span>-&gt;</span> int:
</span></span><span><span>    <span>if</span> max_attempts <span>&lt;=</span> clamped_max:
</span></span><span><span>        clamped_attempt <span>=</span> attempt
</span></span><span><span>    <span>else</span>:
</span></span><span><span>        clamped_attempt <span>=</span> round(attempt <span>/</span> max_attempts <span>*</span> clamped_max)
</span></span><span><span>
</span></span><span><span>    time <span>=</span> exponential(clamped_attempt, mult<span>=</span><span>1</span>, max_pow<span>=</span><span>100</span>, min_pad<span>=</span><span>15</span>)
</span></span><span><span>
</span></span><span><span>    <span>return</span> jitter(time, mode<span>=</span><span>"inc"</span>)
</span></span></code></pre></div><p>And that‚Äôs <strong>the fourth cool thing</strong>! Backoff includes jitter to prevent <a href="https://en.wikipedia.org/wiki/Thundering_herd_problem">thundering herds</a> - without it, all failed jobs from the same batch would retry at the exact same moment, spiking load all over again.</p>
<p>The formula: 15 + 2^attempt seconds, with up to 10% added jitter. Attempt 1 waits ~17s. Attempt 5 waits ~47s. Attempt 10 waits ~1039s (~17 minutes).</p>
<p>The clamping handles jobs with high <code>max_attempts</code> - if you set <code>max_attempts=100</code>, it scales the attempt number down proportionally so you don‚Äôt wait years between retries.</p>
<p>Workers can override this with custom backoff:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>@worker</span>(queue<span>=</span><span>"default"</span>)
</span></span><span><span><span>class</span> <span>MyWorker</span>:
</span></span><span><span>    <span>async</span> <span>def</span> <span>process</span>(self, job: Job):
</span></span><span><span>        <span>...</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>backoff</span>(self, job: Job) <span>-&gt;</span> int:
</span></span><span><span>        <span># Linear backoff: 60s, 120s, 180s...</span>
</span></span><span><span>        <span>return</span> job<span>.</span>attempt <span>*</span> <span>60</span>
</span></span></code></pre></div><h2 id="surfacing---takeaways">Surfacing - Takeaways</h2>
<ul>
<li><strong>PostgreSQL does the heavy lifting.</strong> <code>FOR UPDATE SKIP LOCKED</code> for concurrent job fetching, <code>LISTEN/NOTIFY</code> for real-time signaling, <code>ON CONFLICT</code> for leader election - the database isn‚Äôt just storage, it‚Äôs the coordination layer. There‚Äôs no Redis, no ZooKeeper, no external broker. One less thing to operate.</li>
<li><strong>Oban-py is concurrent, but not parallel</strong>. Async IO allows multiple jobs to be in-flight, but the event loop is single-threaded. For I/O-bound workloads, this is fine. For CPU-bound tasks, consider using the Pro version with a process pool.</li>
<li><strong>Leader election is simple and effective.</strong> No consensus protocol, no Raft - just an <code>INSERT ... ON CONFLICT</code> with a TTL. The leader refreshes at 2x the normal rate to hold the lease. If it dies, the lease expires and another node takes over. Good enough for pruning and rescuing.</li>
<li><strong>The codebase is a pleasure to read.</strong> Clear naming, consistent patterns, and well-separated concerns - exploring it felt more like reading a well-written book than understanding a library.</li>
<li><strong>OSS gets you far, Pro fills the gaps.</strong> Bulk operations, smarter rescues, and true parallelism are all Pro-only - but for what you get, Pro license feels like a great deal.</li>
</ul>
<p>Overall, <a href="https://oban.pro/docs/py/index.html">Oban.py</a> is a clean and well-structured port. If you‚Äôre coming from Elixir and miss Oban, or if you‚Äôre in Python and want a database-backed job queue that doesn‚Äôt require external infrastructure beyond PostgreSQL - it‚Äôs worth looking at.</p>


    </div>

    
</article>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon axes 16,000 jobs as it pushes AI and efficiency (631 pts)]]></title>
            <link>https://www.reuters.com/legal/litigation/amazon-cuts-16000-jobs-globally-broader-restructuring-2026-01-28/</link>
            <guid>46796745</guid>
            <pubDate>Wed, 28 Jan 2026 15:39:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/legal/litigation/amazon-cuts-16000-jobs-globally-broader-restructuring-2026-01-28/">https://www.reuters.com/legal/litigation/amazon-cuts-16000-jobs-globally-broader-restructuring-2026-01-28/</a>, See on <a href="https://news.ycombinator.com/item?id=46796745">Hacker News</a></p>
Couldn't get https://www.reuters.com/legal/litigation/amazon-cuts-16000-jobs-globally-broader-restructuring-2026-01-28/: Error: Request failed with status code 401]]></description>
        </item>
    </channel>
</rss>