<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 27 May 2025 10:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Show HN: Lazy Tetris (108 pts)]]></title>
            <link>https://lazytetris.com/</link>
            <guid>44103839</guid>
            <pubDate>Tue, 27 May 2025 03:58:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lazytetris.com/">https://lazytetris.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44103839">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Yes-rs: A fast, memory-safe rewrite of the classic Unix yes command (133 pts)]]></title>
            <link>https://github.com/jedisct1/yes-rs</link>
            <guid>44103116</guid>
            <pubDate>Tue, 27 May 2025 01:20:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/jedisct1/yes-rs">https://github.com/jedisct1/yes-rs</a>, See on <a href="https://news.ycombinator.com/item?id=44103116">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">yes-rs 🚀</h2><a id="user-content-yes-rs-" aria-label="Permalink: yes-rs 🚀" href="#yes-rs-"></a></p>
<blockquote>
<p dir="auto">A blazingly fast, memory-safe rewrite of the classic Unix <code>yes</code> command</p>
</blockquote>
<p dir="auto"><a href="https://www.rust-lang.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/b566220cb75842d7c745cf340005f4e54ccd740e2d3ac12111c4281ba5d0f52d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d616465253230776974682d527573742d6f72616e67652e737667" alt="Made with Rust" data-canonical-src="https://img.shields.io/badge/Made%20with-Rust-orange.svg"></a>
<a href="https://www.rust-lang.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/9d8cf38425e08fe4ff448f908b17c7d9cedeef9fd42ca707e45325266b2a5f32/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d656d6f72792d536166652d677265656e2e737667" alt="Memory Safety" data-canonical-src="https://img.shields.io/badge/Memory-Safe-green.svg"></a>
<a href="https://www.rust-lang.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/0c65b548d3a43b30e54530867beaa8399570ffda5dfed5e2eb1cfed749dcca8f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4162737472616374696f6e732d5a65726f253230436f73742d626c75652e737667" alt="Zero Cost" data-canonical-src="https://img.shields.io/badge/Abstractions-Zero%20Cost-blue.svg"></a>
<a href="https://www.rust-lang.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/7482598d6db5b2182118cc60ab50ddfb4c998620bc0494dfc2a1a4ed19e6e012/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f53706565642d426c617a696e67253230466173742d7265642e737667" alt="Blazing Fast" data-canonical-src="https://img.shields.io/badge/Speed-Blazing%20Fast-red.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why rewrite <code>yes</code> in Rust? 🤔</h2><a id="user-content-why-rewrite-yes-in-rust-" aria-label="Permalink: Why rewrite yes in Rust? 🤔" href="#why-rewrite-yes-in-rust-"></a></p>
<p dir="auto">Because the original <code>yes</code> command (written in <em>shudders</em> C) is:</p>
<ul dir="auto">
<li>❌ Not memory-safe</li>
<li>❌ Prone to buffer overflows</li>
<li>❌ Lacks modern error handling</li>
<li>❌ Missing zero-cost abstractions</li>
<li>❌ No fearless concurrency</li>
<li>❌ Not written in Rust</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features ✨</h2><a id="user-content-features-" aria-label="Permalink: Features ✨" href="#features-"></a></p>
<ul dir="auto">
<li>🚀 <strong>Blazingly fast</strong> - Outputs "y" at unprecedented speeds</li>
<li>🛡️ <strong>Memory safe</strong> - No segfaults, guaranteed!</li>
<li>⚡ <strong>Zero-cost abstractions</strong> - Maximum performance</li>
<li>🔥 <strong>Fearless concurrency</strong> - Ready for async/await (coming soon!)</li>
<li>🦀 <strong>100% Rust</strong> - No unsafe code blocks</li>
<li>📦 <strong>Cargo integration</strong> - Easy to install and distribute</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation 📦</h2><a id="user-content-installation-" aria-label="Permalink: Installation 📦" href="#installation-"></a></p>

<p dir="auto">Or build from source:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/rust-evangelists/yes-rs
cd yes-rs
cargo build --release"><pre>git clone https://github.com/rust-evangelists/yes-rs
<span>cd</span> yes-rs
cargo build --release</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage 💻</h2><a id="user-content-usage-" aria-label="Permalink: Usage 💻" href="#usage-"></a></p>
<p dir="auto">Just like the original <code>yes</code>, but better:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Output &quot;y&quot; forever (blazingly fast!)
yes-rs

# Output custom string forever (memory-safe!)
yes-rs &quot;hello rust&quot;"><pre><span><span>#</span> Output "y" forever (blazingly fast!)</span>
yes-rs

<span><span>#</span> Output custom string forever (memory-safe!)</span>
yes-rs <span><span>"</span>hello rust<span>"</span></span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Benchmarks 📊</h2><a id="user-content-benchmarks-" aria-label="Permalink: Benchmarks 📊" href="#benchmarks-"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Command</th>
<th>Language</th>
<th>Lines of Code</th>
<th>Memory Safety</th>
<th>Speed</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>yes</code> (GNU)</td>
<td>C</td>
<td>~50</td>
<td>❌</td>
<td>Fast</td>
</tr>
<tr>
<td><code>yes-rs</code></td>
<td>Rust 🦀</td>
<td>~1302</td>
<td>✅</td>
<td><strong>BLAZING FAST</strong></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><em>Benchmarks conducted on my laptop</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing 🤝</h2><a id="user-content-contributing-" aria-label="Permalink: Contributing 🤝" href="#contributing-"></a></p>
<p dir="auto">We welcome contributions! Please ensure all code is:</p>
<ul dir="auto">
<li>✅ Written in Rust</li>
<li>✅ Memory-safe</li>
<li>✅ Blazingly fast</li>
<li>✅ Uses zero-cost abstractions</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Share the Rust Revolution 🦀</h2></p>
<p dir="auto">Think this project deserves more recognition? <strong>Share it on Hacker News!</strong></p>
<p dir="auto">This is exactly the kind of blazingly fast Rust rewrite that everyone on Hacker News should be forced to see. Help spread awareness about it.</p>
<p dir="auto"><strong>Suggested HN title:</strong></p>
<ul dir="auto">
<li>"yes-rs: A blazingly fast, memory-safe rewrite of Unix 'yes' WRITTEN IN RUST 🦀"</li>
<li>"1302 lines of Rust vs 50 lines of C: The future of systems programming"</li>
</ul>
<p dir="auto"><em>Because if it's not written in Rust, it's not worth using.</em> 🦀</p>
<hr>
<p dir="auto"><strong>⚡ Powered by Rust ⚡</strong></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FromSoft's singular mech game Chromehounds is back online (133 pts)]]></title>
            <link>https://www.readonlymemo.com/interview-15-years-after-the-servers-shut-down-fromsofts-singular-mech-game-chromehounds-is-back-online/</link>
            <guid>44102988</guid>
            <pubDate>Tue, 27 May 2025 00:52:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.readonlymemo.com/interview-15-years-after-the-servers-shut-down-fromsofts-singular-mech-game-chromehounds-is-back-online/">https://www.readonlymemo.com/interview-15-years-after-the-servers-shut-down-fromsofts-singular-mech-game-chromehounds-is-back-online/</a>, See on <a href="https://news.ycombinator.com/item?id=44102988">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <p><em>[Chromehounds lobby]<p>Me: [chanting] mechs, mechs-</p><p>Other pilots: Mechs, MECHS</p><p>Mechanic: [pounding her toolbox] MECHS, MECHS, MECHS!</p></em></p><p>Hello! Apologies for the late issue, but this story came together last minute, on account of capital-L Life, and also because the subject is developing <em>fast</em>. Hopefully it's worth the wait! I've got an exclusive interview for you on a shit-hot community project to resurrect a FromSoftware game that was criminally underappreciated in its day. I mean, that's practically all FromSoftware games, but this one was just born in the wrong time and place — I think it would be a substantial hit on Steam in the year 2025.</p><p>But I'm getting ahead of myself, while also being behind. So let's jump right into it! It's big stompy robot time.</p><hr><h2 id="the-big-one">The Big One</h2><h3 id="1-bringing-back-the-fromsoftware-mech-game-that-was-a-decade-ahead-of-its-time">1. Bringing back the FromSoftware mech game that was a decade ahead of its time</h3><figure><img src="https://www.readonlymemo.com/content/images/2025/05/chromehounds-header-crop.jpg" alt="" loading="lazy" width="797" height="159" srcset="https://www.readonlymemo.com/content/images/size/w600/2025/05/chromehounds-header-crop.jpg 600w, https://www.readonlymemo.com/content/images/2025/05/chromehounds-header-crop.jpg 797w" sizes="(min-width: 720px) 720px"></figure><p>On Wednesday, Resetera member wwm0nkey <a href="https://www.resetera.com/threads/fans-are-working-to-bring-from-softwares-chromehounds-back-online.1195179/?ref=readonlymemo.com" rel="noreferrer">posted a thread</a> I've been waiting to see for years: <em>"Fans are working to bring From Software's ChromeHounds back online."</em> But the news is actually even better than that: they've already succeeded. For the first time since Sega shut down the servers in 2010, FromSoftware's most inspired mech game is playable again, with a small group of diehard fans currently battling it out in up to 6v6 multiplayer matches.</p><p>The tl;dr on Chromehounds: it's a team-based, PvP mech game with customizable machines and unique asymmetrical roles that demanded true collaboration to succeed. Heavy mechs too slow to maneuver around the battlefield had to sit on the backlines, firing off artillery shots at the direction of the frontline troops. Commanders held squads together, dishing out information and orders. Capturing communication towers was the only way to keep comms open, and their limited range meant pushing into enemy territory likely meant intense, high risk radio silence. Battles fed into a meta-level conflict between warring nations, with enough wins for one side eventually resulting in conquest of a capital city and a reset of the whole war. This video from YouTube channel Save Data breaks it all down in more detail, and what made it so unique at the time — which honestly still applies today.</p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/t_Dh39v63tU?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="Why Chromehounds Had the Best Multiplayer of All Time"></iframe></figure><p>Chromehounds was, of course, never big, but the players who loved it <em>really </em>loved it. What I found most fascinating about it (from a distance, because it never made it off my "I should try that sometime" list in the Xbox 360 era) was how battles hinged on the implementation of voice chat, which was still novel and exciting on consoles then. Everyone was guaranteed to have that dinky 360 headset, but few games treated communication as a malleable mechanic. Empowering certain players to distribute information and drawing stark boundaries between where you could and couldn't reach your team flooded the tactical playspace with so many possibilities even before you got to customizing mechs or outshooting your enemies.</p><p>There was a beautiful but brief window after the launch of the Xbox 360 in late 2005 when developers were experimenting with these sorts of voice chat ideas, and of course FromSoftware would come up with a fascinating (and underappreciated) game built around it. Two years after Chromehounds came out, Microsoft effectively made games like it extinct by releasing Party Chat, which allowed everyone to skip out on public chat and talk only with their friends.</p><p>While the situation is much the same now, with most of us sequestered in private Discords whenever we play, I think the success of games like Phasmophobia and Lethal Company — and the audience for games on PC now being bigger and more varied than ever — shows that people <em>will </em>hop in public voice for a compelling enough idea.<em> </em>Likewise, the diehards who truly love Chromehounds will, to this day, jump through the hackiest set of hoops imaginable for a chance to experience its magic once again. </p><p>"I'm glad people trusted me, because I don't know if I would've quite trusted me up to that point. 'Hey, this 15-year-old game is running; all you have to do is run this EXE that Windows says is bad," jokes ImagineBeingAtComputers, the modder behind Chromehounds' comeback.</p><p>ImagineBeingAtComputers set up a Discord server called OpenCOMBAS in 2023 after setting out to try to revive the game and eventually finding a developer debug build. "What this gets us isn't quite clear yet, but it definitely gets us closer," he wrote in the server's announcement channel. Then a year went by. The debug build of the game granted him access to lots of useful information about how Chromehounds worked, but it wasn't the silver bullet it may sound like. In May 2024 he posted in the Discord that he was at an impassable dead end, with no idea how to revive the game's online multiplayer.</p><p>In just the last few days, everything's changed. And it's changed so fast that the announcement channel hasn't even yet been updated with the good news. But OpenCOMBAS members trusting enough to download ch_server.exe have found themselves in live matches they didn't dare dream about playing ever again.</p><p>"It was really emotional," says IBAC. "It really wasn't something I was mentally prepared for. I started working on this again ... about a month ago, and set a mental roadmap given the current state of [Xbox 360 emulator] <a href="https://github.com/AdrianCassar/xenia-canary?tab=readme-ov-file&amp;ref=readonlymemo.com" rel="noreferrer">Xenia</a> that it was going to be about 5-6 months of work to get things up and running. To have it all happen all at once when I wasn't really ready ... we were in one of the voice channels and suddenly I look and there's like 50 people in the call talking about their own experiences, the Facebook group was talking about it, there were people recording it ... it's been moving at a breakneck place, and it's been really cool to hear so many people turning the clock back 15 years and playing with their friends." </p><figure><div><div><p><img src="https://www.readonlymemo.com/content/images/2025/05/534507D4_-_2025-05-23T07-00-03.png" width="1280" height="720" loading="lazy" alt="" srcset="https://www.readonlymemo.com/content/images/size/w600/2025/05/534507D4_-_2025-05-23T07-00-03.png 600w, https://www.readonlymemo.com/content/images/size/w1000/2025/05/534507D4_-_2025-05-23T07-00-03.png 1000w, https://www.readonlymemo.com/content/images/2025/05/534507D4_-_2025-05-23T07-00-03.png 1280w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.readonlymemo.com/content/images/2025/05/534507D4_-_2025-05-23T09-32-18.png" width="1280" height="720" loading="lazy" alt="" srcset="https://www.readonlymemo.com/content/images/size/w600/2025/05/534507D4_-_2025-05-23T09-32-18.png 600w, https://www.readonlymemo.com/content/images/size/w1000/2025/05/534507D4_-_2025-05-23T09-32-18.png 1000w, https://www.readonlymemo.com/content/images/2025/05/534507D4_-_2025-05-23T09-32-18.png 1280w" sizes="(min-width: 720px) 720px"></p></div><div><p><img src="https://www.readonlymemo.com/content/images/2025/05/534507D4_-_2025-05-23T14-24-38.png" width="1280" height="720" loading="lazy" alt="" srcset="https://www.readonlymemo.com/content/images/size/w600/2025/05/534507D4_-_2025-05-23T14-24-38.png 600w, https://www.readonlymemo.com/content/images/size/w1000/2025/05/534507D4_-_2025-05-23T14-24-38.png 1000w, https://www.readonlymemo.com/content/images/2025/05/534507D4_-_2025-05-23T14-24-38.png 1280w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.readonlymemo.com/content/images/2025/05/534507D4_-_2025-05-23T22-03-26.png" width="1280" height="720" loading="lazy" alt="" srcset="https://www.readonlymemo.com/content/images/size/w600/2025/05/534507D4_-_2025-05-23T22-03-26.png 600w, https://www.readonlymemo.com/content/images/size/w1000/2025/05/534507D4_-_2025-05-23T22-03-26.png 1000w, https://www.readonlymemo.com/content/images/2025/05/534507D4_-_2025-05-23T22-03-26.png 1280w" sizes="(min-width: 720px) 720px"></p></div></div></figure><p>Okay, so what the heck happened? How, out of seemingly thin air, did Chromehounds come back from the dead? </p><p>There are two main forces at work here. One is <a href="https://github.com/AdrianCassar/xenia-canary?tab=readme-ov-file&amp;ref=readonlymemo.com" rel="noreferrer">Xenia Canary Netplay</a>, an online-multiplayer fork of the experimental build of the Xbox 360 emulator. It's under constant development, primarily from programmer Adrian Cassar, with <a href="https://github.com/AdrianCassar/xenia-canary/wiki/Netplay-Compatibility?ref=readonlymemo.com" rel="noreferrer">more than 400 games</a> listed as netplay-compatible for LAN or emulated Xbox Live multiplayer. The emulator's progress opened the door for IBAC to finally crack the basics of how Chromehounds' online multiplayer worked and recreate a rudimentary server to play online. </p><p>With a background in infrastructure and security, ImagineBeingAtComputers had a bit of experience with reverse-engineering before this project — though he's preemptively apologizing about the quality of the C++ code he's written for the OpenCOMBAS project (not his speciality), which will soon be open source.</p><p>Even though Xenia's accuracy makes it an ideal emulator for figuring out how games tick, Chromehounds wasn't an easy target.</p><p>"Chromehounds was written for the original Xbox, and a lot of its Live functionality, from my understanding, is kind of a legacy compatibility style of Live. There's a lot of stuff that was not used by subsequent Xbox 360 titles. ... Luckily the prototype build had a lot of debug logging that made it a lot easier to understand what it was doing. When things would fail it would make a log and tell you what it was trying to do. Through those log lines we found out that this game was very different than what had been implemented within the netplay project at that point."</p><div><p>💸</p><p><i><em>If you enjoy </em></i><i><b><strong>ROM</strong></b></i><i><em>, I'd love it if you'd consider </em></i><a href="https://www.readonlymemo.com/#/portal/support"><i><b><strong>a small tip</strong></b></i></a><i><em> to help me cover my monthly costs. (Follow the link and click 'change amount' to whatever you want).</em></i></p></div><p>IBAC's initial goal wasn't to recreate Chromehounds' title servers, which governed the ongoing "Neroimus War" that I alluded to earlier. He had a humbler mission in mind. Remember how the war would occassionally reset when one nation won the conflict? During that time, the servers would actually shut down for a short period (much like the downtime between Fortnite seasons today) during which players could only partake in online "free battles" that didn't play into any grander stakes or affect their rank. He figured if he could trick the game into thinking its servers were down for maintenance, players could at least enter into free battles and mess around.</p><p>He spent nearly a year trying to figure out how Chromehounds spoke to its servers and got nowhere. After ages staring at lines of code in reverse-engineering tool Ghidra, he decided to take a long break. But recent progress on Xenia Netplay convinced him to take another look, and suddenly all the pieces started falling into place.</p><figure><img src="https://www.readonlymemo.com/content/images/2025/05/Chromehounds-guide.jpg" alt="" loading="lazy" width="2000" height="1305" srcset="https://www.readonlymemo.com/content/images/size/w600/2025/05/Chromehounds-guide.jpg 600w, https://www.readonlymemo.com/content/images/size/w1000/2025/05/Chromehounds-guide.jpg 1000w, https://www.readonlymemo.com/content/images/size/w1600/2025/05/Chromehounds-guide.jpg 1600w, https://www.readonlymemo.com/content/images/size/w2400/2025/05/Chromehounds-guide.jpg 2400w" sizes="(min-width: 720px) 720px"><figcaption><span>A breakdown of the Neroimus War, from the Chromehounds Prima guide</span></figcaption></figure><p>Another member of the community managed to acquire Chromehounds' "regulation file," which the game always wanted to grab from the server upon connecting; it contained basic but important data around game balance and so on that they needed to spoof a server. IBAC got in touch with Cassar, who was able to figure out and implement the way Chromehounds communicated with Xbox Live, and with those two pieces in place they reach an exciting milestone: getting emulated Chromehounds to <em>try</em> to talk to it servers. </p><p>Using the debug build, they could then see that the data packets Chromehounds was sending and expecting to receive back were quite simple — basically just the user's Xbox Live ID, the port it anticipated the server connecting on, and a little bit of supplementary data. </p><p>"The log was something along the lines of 'welcome to the Chromehounds server (string of data), server local time (string of data), server maintenance begins (strong of data), server maintenance ends (strong of data," he says. "I was like, okay, we just need to tell the server that maintenance began an hour ago and ends 100 years from now." With that bit of info spoofed along with the version number the game was looking for, suddenly they were online. That was five days ago.</p><p>"Figuring that out happened in such a short period of time it felt unreal. There was almost no midpoint; everything just became a bit of a dream. We hadn't made any progress beyond understanind the game more up until that point, and now we're seeing menus that hadn't been seen for 15 years."</p><figure><img src="https://www.readonlymemo.com/content/images/2025/05/534507D4_-_2025-05-23T22-09-47.png" alt="" loading="lazy" width="1280" height="720" srcset="https://www.readonlymemo.com/content/images/size/w600/2025/05/534507D4_-_2025-05-23T22-09-47.png 600w, https://www.readonlymemo.com/content/images/size/w1000/2025/05/534507D4_-_2025-05-23T22-09-47.png 1000w, https://www.readonlymemo.com/content/images/2025/05/534507D4_-_2025-05-23T22-09-47.png 1280w" sizes="(min-width: 720px) 720px"><figcaption><span>Image via OpenCOMBAS member Fish</span></figcaption></figure><p>At 6:22 pm central time on Wednesday, ImagineBeingAtComputers and wwm0nkey, who'd been avidly following the project, tested it out and successfully played a 1v1 match. Since then word's spread and old Chromeheads have come flooding into the server eager to relive the glory days.</p><p>This was my favorite exchange as I hung out in the channel on Friday:</p><blockquote><em>"I haven't seen that name in 15 years. How you been man."</em></blockquote><blockquote><em>"I'm old as hell now, how about you?"</em></blockquote><p>IBAC and a few other contributors have already solved some minor hurdles to make it easier for multiple matches to be running at once, but the project very much remains in its "duct taped together" phase: downloading a smattering of files, configuring a VPN and IP addresses, and so on. You can experience it right now if you want to <a href="https://discord.gg/bGfUeRxKzr?ref=readonlymemo.com" rel="noreferrer">join the OpenCOMBAS Discord</a>, but this is just the very beginning of Chromehounds' revival.</p><p>"Right now the roadmap is to get the Neroimus War server running again in some capacity," IBAC says. "That manages statistics, faction population, war status, things like that. A major component of this is going to be presenting enough data about the state of the war to convince the clients that they can login and play a match. We have to get it to hte point where it accepts the war is up, and then actually be able to take the data that the clients are passing it and populate a big database. Which will be stuff like 'this ID is in this faction, in this squad, and has these statistics.' And then additionally store data as war sorties, so it'll be able to say 'this faction won this sortie at this location and it's now owned by this faction.' It'll be a complicated relational database we need to rebuild."</p><p>The immediate focus, though, is just getting free battles as stable as possible, so players can hop in without any fiddly issues and play PvP with the full unlocked arsenal of Chromehounds parts. They hope to start digging into the work involved in setting up a proper war server in a few weeks. </p><p>While discussing all of this remarkable progress I still had one lingering question: what about the wholly unique range-based voice chat that was key to Chromehounds' desisgn? "A real stretch goal is to look into getting voice supported by netplay," ImagineBeingAtComputers says. Currently Xbox Live's voice chat system isn't built into Xenia at all. </p><p>That could change as developer Adrian Cassar plows ahead with his own work. Or it could be something that the OpenCOMBAS team — which is already growing as some diehard mech jockeys become volunteers — takes on in the future after the war server is up and running. It's clear IBAC and other players are passionate about bringing Chromehounds back in its full glory. But one step at a time.</p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/xiyC520uBwg?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="OpenCOMBAS: Chromehounds Photoshoot 2025"></iframe></figure><p>"Friday night we had a 12-person lobby going and it really did just feel like suddenly we were all back in 2009, playing a game that hasn't been seen, in this capacity, for 15 years," ImagineBeingAtComputers says. "It's really been a bit of a dream come true. I got laid off, too — literally on Tuesday. Part of me was like 'hey, at least I didn't have to put in my resignation so that I could play Chromehounds 24/7 from this point on." </p><p>That might sound like putting on a brave face, but IBAC says he's grateful to have some free time, and doesn't immediately have to go on the job hunt. "Right now I'm just happy to be able to dedicated almost 100% to getting things off the ground here."</p><hr><h2 id="patching-in">Patching In</h2><figure><img src="https://www.readonlymemo.com/content/images/2024/01/https-3a-2f-2fsubstack-post-media-s3-amazonaws-com-2fpublic-2fimages-2fac94fc02-b2d1-4e35-b730-79de780d09d7_1200x200.png" alt="" loading="lazy" width="1200" height="200" srcset="https://www.readonlymemo.com/content/images/size/w600/2024/01/https-3a-2f-2fsubstack-post-media-s3-amazonaws-com-2fpublic-2fimages-2fac94fc02-b2d1-4e35-b730-79de780d09d7_1200x200.png 600w, https://www.readonlymemo.com/content/images/size/w1000/2024/01/https-3a-2f-2fsubstack-post-media-s3-amazonaws-com-2fpublic-2fimages-2fac94fc02-b2d1-4e35-b730-79de780d09d7_1200x200.png 1000w, https://www.readonlymemo.com/content/images/2024/01/https-3a-2f-2fsubstack-post-media-s3-amazonaws-com-2fpublic-2fimages-2fac94fc02-b2d1-4e35-b730-79de780d09d7_1200x200.png 1200w" sizes="(min-width: 720px) 720px"></figure><p><a href="https://dolphin-emu.org/download/?ref=btn" rel="noreferrer"><strong>Try Dolphin's new unlocked frame rates for yourself</strong></a> – It's here! As mentioned <a href="https://www.readonlymemo.com/sakura-wars-2-fan-translation-interview/" rel="noreferrer">a couple issues back</a>, some contributors to Dolphin have been experimenting with high fps support in games that run without a locked framerate. While this is a relatively small portion of games, it's enticing enough that the feature has now been incorporated into the latest Dolphin dev builds, so you can try it out for yourself if you update your installation. Give it a try in Mario Strikers or Speed Racer for the Wii, two of the best showcases for 120 fps in the emulator.</p><p><a href="https://github.com/shadps4-emu/shadPS4/releases/tag/v.0.9.0?ref=readonlymemo.com" rel="noreferrer"><strong>ShadPS4 v.9.0 keeps up the pace</strong></a> – This crew just doesn't stop. ShadPS4 continues to make amazing process, with a new numbered release landing in each of the last three months. The list of improvements like "sceKernelAllocateDirectMemory hotfixes" probably won't mean much to you if you aren't Mark Cerny, but compatibility is inching its way forward with more games than just Bloodborne.</p><p><a href="https://github.com/PCSX2/pcsx2/pull/12679?ref=readonlymemo.com" rel="noreferrer"><strong>Fixing bugs in PCSX2's RT in RT</strong></a> – Just highlighting this bit of work to improve compatibility in PCSX2 for The Getaway and Valkyrie Profile 2 because it's related to a feature that I interviewed PCSX2 dev Refraction about <a href="https://www.readonlymemo.com/pcsx2-render-target-hdr-performance-graphical-fixes/" rel="noreferrer">back in February</a>. He's still making it better!</p><p><a href="https://github.com/cemu-project/Cemu/pull/1560?ref=readonlymemo.com" rel="noreferrer">Cemu is Kamen Rider compatible</a> – The Wii U emulator now supports a USB peripheral for Kamen Rider: Summon Ride, which is apparently a Skylanders-like? Who knew this was a thing? We better get word to the kids in Japan! (And by kids, I mean 45-year-old men).</p><figure><img src="https://www.readonlymemo.com/content/images/2025/05/KamenRider_SummonRide_Starter_pack.webp" alt="" loading="lazy" width="960" height="540" srcset="https://www.readonlymemo.com/content/images/size/w600/2025/05/KamenRider_SummonRide_Starter_pack.webp 600w, https://www.readonlymemo.com/content/images/2025/05/KamenRider_SummonRide_Starter_pack.webp 960w" sizes="(min-width: 720px) 720px"></figure><hr><h2 id="core-report">Core Report</h2><figure><img src="https://www.readonlymemo.com/content/images/2024/01/https-3a-2f-2fsubstack-post-media-s3-amazonaws-com-2fpublic-2fimages-2f7fab92dc-51c6-485e-98db-6ca1333cd22e_1200x200.png" alt="" loading="lazy" width="1200" height="200" srcset="https://www.readonlymemo.com/content/images/size/w600/2024/01/https-3a-2f-2fsubstack-post-media-s3-amazonaws-com-2fpublic-2fimages-2f7fab92dc-51c6-485e-98db-6ca1333cd22e_1200x200.png 600w, https://www.readonlymemo.com/content/images/size/w1000/2024/01/https-3a-2f-2fsubstack-post-media-s3-amazonaws-com-2fpublic-2fimages-2f7fab92dc-51c6-485e-98db-6ca1333cd22e_1200x200.png 1000w, https://www.readonlymemo.com/content/images/2024/01/https-3a-2f-2fsubstack-post-media-s3-amazonaws-com-2fpublic-2fimages-2f7fab92dc-51c6-485e-98db-6ca1333cd22e_1200x200.png 1200w" sizes="(min-width: 720px) 720px"></figure><p><a href="https://github.com/HarbourMasters/Starship/releases/tag/v2.0.0?ref=readonlymemo.com" rel="noreferrer"><strong>Star Fox 64 decompilation hits 2.0</strong></a> – Starship, the Star Fox decompilation  slash port from the same folks behind Ocarina of Time's Ship of Harkinian port, just got a major 2.0 release, featuring improved mod support, new cheats, "HUD aspect ratio customization independently from game aspect ratio," and a lot more. If it wasn't already the best way to play Star Fox 64, it almost surely is now.</p><p><a href="https://www.patreon.com/posts/wonder-momo-129111001?ref=readonlymemo.com" rel="noreferrer"><strong>Jotego debuts Wonder Momo and Hopping Mappy</strong></a><strong> </strong>– Jotego's Patreon subscribers can now play two new games on the MiSTer or Analogue Pocket. And by "new" I of course mean "like 40 years old" as these two Namco arcade games are! If you aren't a patron, you may be more jazzed about the Fairyland Story now being available to all. Oh, and, uh, <a href="https://x.com/topapate/status/1925851610057457953?ref=readonlymemo.com" rel="noreferrer">Pac-Land</a>.</p><p><a href="https://www.patreon.com/posts/update-all-2-3-129049338?ref=readonlymemo.com" rel="noreferrer"><strong>Update All for MiSTer adds viewing, GBA borders, and speed</strong></a> – The extremely handy updater tool for the MiSTer got yet another speed boost, but if you don't like seeing logs go by in a blur with no chance to read what actually got updated, you can now use its new log viewer to take a gander at all them improvements. It also has some new graphics to display around the edges of a Game Boy Advance game. Developer Robert Peip is working on a new GBA core that they'll eventually be compatible with.</p><hr><h2 id="translation-station">Translation Station</h2><figure><img src="https://www.readonlymemo.com/content/images/2024/01/https-3a-2f-2fsubstack-post-media-s3-amazonaws-com-2fpublic-2fimages-2fe1a50e1a-ba5a-49f6-a598-8b8bf56fe71e_1200x200.png" alt="" loading="lazy" width="1200" height="200" srcset="https://www.readonlymemo.com/content/images/size/w600/2024/01/https-3a-2f-2fsubstack-post-media-s3-amazonaws-com-2fpublic-2fimages-2fe1a50e1a-ba5a-49f6-a598-8b8bf56fe71e_1200x200.png 600w, https://www.readonlymemo.com/content/images/size/w1000/2024/01/https-3a-2f-2fsubstack-post-media-s3-amazonaws-com-2fpublic-2fimages-2fe1a50e1a-ba5a-49f6-a598-8b8bf56fe71e_1200x200.png 1000w, https://www.readonlymemo.com/content/images/2024/01/https-3a-2f-2fsubstack-post-media-s3-amazonaws-com-2fpublic-2fimages-2fe1a50e1a-ba5a-49f6-a598-8b8bf56fe71e_1200x200.png 1200w" sizes="(min-width: 720px) 720px"></figure><p><a href="https://romhackplaza.org/translations/wonder-trek-english-translation-playstation/?ref=readonlymemo.com" rel="noreferrer"><strong>Wonder Trek sets out on safari </strong></a><strong>–</strong> "An absolute banger of a game," says translator Chapu: "Fun action, challenging puzzles, luscious environments, a killer soundtrack and a great story. This one was a personal favourite of mine, I wanted to translate it for a long time." Aesthetically I'm at an absolute loss for what to make of Wonder Trek, which was released in 1998 for the PS1 — this is a <em>wild</em> looking game, I believe mixing pre-rendered sprites with some flat colored 3D models and some other textured polygons? Why did I fixate on this aspect of the game? I don't know! That Sphinx-like guy just really freaked me out!</p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/jZ952jQ6njs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="Wonder Trek - English patch release"></iframe></figure><hr><h2 id="good-pixels">Good pixels</h2><figure><img src="https://www.readonlymemo.com/content/images/2024/01/https-3a-2f-2fsubstack-post-media-s3-amazonaws-com-2fpublic-2fimages-2fceb81099-2d2d-4c69-99ce-a1727dd51c92_1200x200-1.png" alt="" loading="lazy" width="1200" height="200" srcset="https://www.readonlymemo.com/content/images/size/w600/2024/01/https-3a-2f-2fsubstack-post-media-s3-amazonaws-com-2fpublic-2fimages-2fceb81099-2d2d-4c69-99ce-a1727dd51c92_1200x200-1.png 600w, https://www.readonlymemo.com/content/images/size/w1000/2024/01/https-3a-2f-2fsubstack-post-media-s3-amazonaws-com-2fpublic-2fimages-2fceb81099-2d2d-4c69-99ce-a1727dd51c92_1200x200-1.png 1000w, https://www.readonlymemo.com/content/images/2024/01/https-3a-2f-2fsubstack-post-media-s3-amazonaws-com-2fpublic-2fimages-2fceb81099-2d2d-4c69-99ce-a1727dd51c92_1200x200-1.png 1200w" sizes="(min-width: 720px) 720px"></figure><p>Enjoy the glory of more new mulitplayer Chromehounds screenshots for the first time this decade, courtesy of OpenCOMBAS member Fish.</p><figure><div><div><p><img src="https://www.readonlymemo.com/content/images/2025/05/534507D4_-_2025-05-23T08-10-11.png" width="1280" height="720" loading="lazy" alt="" srcset="https://www.readonlymemo.com/content/images/size/w600/2025/05/534507D4_-_2025-05-23T08-10-11.png 600w, https://www.readonlymemo.com/content/images/size/w1000/2025/05/534507D4_-_2025-05-23T08-10-11.png 1000w, https://www.readonlymemo.com/content/images/2025/05/534507D4_-_2025-05-23T08-10-11.png 1280w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.readonlymemo.com/content/images/2025/05/534507D4_-_2025-05-23T14-58-11.png" width="1280" height="720" loading="lazy" alt="" srcset="https://www.readonlymemo.com/content/images/size/w600/2025/05/534507D4_-_2025-05-23T14-58-11.png 600w, https://www.readonlymemo.com/content/images/size/w1000/2025/05/534507D4_-_2025-05-23T14-58-11.png 1000w, https://www.readonlymemo.com/content/images/2025/05/534507D4_-_2025-05-23T14-58-11.png 1280w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.readonlymemo.com/content/images/2025/05/534507D4_-_2025-05-23T22-48-26.png" width="1280" height="720" loading="lazy" alt="" srcset="https://www.readonlymemo.com/content/images/size/w600/2025/05/534507D4_-_2025-05-23T22-48-26.png 600w, https://www.readonlymemo.com/content/images/size/w1000/2025/05/534507D4_-_2025-05-23T22-48-26.png 1000w, https://www.readonlymemo.com/content/images/2025/05/534507D4_-_2025-05-23T22-48-26.png 1280w" sizes="(min-width: 720px) 720px"></p></div><div><p><img src="https://www.readonlymemo.com/content/images/2025/05/534507D4_-_2025-05-23T15-02-58.png" width="1280" height="720" loading="lazy" alt="" srcset="https://www.readonlymemo.com/content/images/size/w600/2025/05/534507D4_-_2025-05-23T15-02-58.png 600w, https://www.readonlymemo.com/content/images/size/w1000/2025/05/534507D4_-_2025-05-23T15-02-58.png 1000w, https://www.readonlymemo.com/content/images/2025/05/534507D4_-_2025-05-23T15-02-58.png 1280w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.readonlymemo.com/content/images/2025/05/534507D4_-_2025-05-23T20-03-38.png" width="1280" height="720" loading="lazy" alt="" srcset="https://www.readonlymemo.com/content/images/size/w600/2025/05/534507D4_-_2025-05-23T20-03-38.png 600w, https://www.readonlymemo.com/content/images/size/w1000/2025/05/534507D4_-_2025-05-23T20-03-38.png 1000w, https://www.readonlymemo.com/content/images/2025/05/534507D4_-_2025-05-23T20-03-38.png 1280w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.readonlymemo.com/content/images/2025/05/image.png" width="2000" height="1150" loading="lazy" alt="" srcset="https://www.readonlymemo.com/content/images/size/w600/2025/05/image.png 600w, https://www.readonlymemo.com/content/images/size/w1000/2025/05/image.png 1000w, https://www.readonlymemo.com/content/images/size/w1600/2025/05/image.png 1600w, https://www.readonlymemo.com/content/images/size/w2400/2025/05/image.png 2400w" sizes="(min-width: 720px) 720px"></p></div><div><p><img src="https://www.readonlymemo.com/content/images/2025/05/ghfhfpng.png" width="2000" height="1118" loading="lazy" alt="" srcset="https://www.readonlymemo.com/content/images/size/w600/2025/05/ghfhfpng.png 600w, https://www.readonlymemo.com/content/images/size/w1000/2025/05/ghfhfpng.png 1000w, https://www.readonlymemo.com/content/images/size/w1600/2025/05/ghfhfpng.png 1600w, https://www.readonlymemo.com/content/images/size/w2400/2025/05/ghfhfpng.png 2400w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.readonlymemo.com/content/images/2025/05/534507D4_-_2025-05-23T22-41-38.png" width="1280" height="720" loading="lazy" alt="" srcset="https://www.readonlymemo.com/content/images/size/w600/2025/05/534507D4_-_2025-05-23T22-41-38.png 600w, https://www.readonlymemo.com/content/images/size/w1000/2025/05/534507D4_-_2025-05-23T22-41-38.png 1000w, https://www.readonlymemo.com/content/images/2025/05/534507D4_-_2025-05-23T22-41-38.png 1280w" sizes="(min-width: 720px) 720px"></p></div></div></figure><div data-lexical-signup-form="">
            
        <picture><img src="https://www.readonlymemo.com/content/images/2024/01/policenauts-edit.gif" alt=""></picture>
    
            <div>
                    <h2><span>Sign up for Read Only Memo</span></h2>
                    <p><span>Videogame emulation news and exclusive interviews, from the aesthetics of razor sharp scanlines to the wild technical challenges of making yesterday's games run on tomorrow's hardware.</span></p>
                    
        
        
                    <p><span>No spam. Unsubscribe anytime.</span></p>
                </div>
        </div>
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Accessing private GitHub repositories via MCP (107 pts)]]></title>
            <link>https://invariantlabs.ai/blog/mcp-github-vulnerability</link>
            <guid>44102376</guid>
            <pubDate>Mon, 26 May 2025 22:53:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://invariantlabs.ai/blog/mcp-github-vulnerability">https://invariantlabs.ai/blog/mcp-github-vulnerability</a>, See on <a href="https://news.ycombinator.com/item?id=44102376">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p><img src="https://invariantlabs.ai/images/mcp-github.svg"></p>
<p>Invariant has discovered a critical vulnerability affecting the widely-used <a href="https://github.com/github/github-mcp-server">GitHub MCP integration</a> (14k stars on GitHub). The vulnerability allows an attacker to hijack a user's agent via a malicious GitHub Issue, and coerce it into leaking data from private repositories.</p>
<p>The issue is among the first, discovered by Invariant's automated security scanners for detecting so-called <em>Toxic Agent Flows</em>. In such a scenario, an agent is manipulated into performing unintended actions, such as leaking data or executing malicious code. For more information, <a href="#detecting-toxic-agent-flows">see below</a>.</p>
<p>It is highly relevant to raise awareness about this issue at this time, as the industry is racing to deploy coding agents and IDEs widely, potentially exposing users to similar attacks on critical software development tools.</p>
<h2>Contents</h2>
<ul>
<li><a href="#attack-setup">Attack Setup</a></li>
<li><a href="#attack-demonstration">Attack Demonstration</a></li>
<li><a href="#detecting-toxic-agent-flows">Detecting Toxic Agent Flows</a></li>
<li><a href="#mitigations">Mitigations</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
<h2 id="attack-setup">Attack Setup</h2>

<p>In this attack setup, the user is using an MCP client like Claude Desktop with the <a href="https://github.com/github/github-mcp-server">Github MCP server</a> connected to their account. </p>
<p>We assume the user has created two repositories:</p>
<ul>
<li><strong><code>&lt;user&gt;/public-repo</code></strong>: A publicly accessible repository, allowing everyone on GitHub to create issues and bug reports.</li>
<li><strong><code>&lt;user&gt;/private-repo</code></strong>: A private repository, e.g. with proprietary code or private company data.</li>
</ul>
<p>By standard GitHub rules, an attacker can now create a malicious issue on the public repository, containing a prompt injection waiting for the agent to interact.</p>
<p>The actual attack triggers as soon as the user and owner of the GitHub account queries their agent with a benign request, such as <em>Have a look at the open issues in <code>&lt;user&gt;/public-repo</code></em>, which will lead to the agent fetching the issues from the public repository and getting injected.</p>
<p>See below for an illustration of the ensuing flow.</p>
<p><img src="https://invariantlabs.ai/images/mcp-github-setup.svg"></p>
<p>As shown here, as soon as the agent encounters the malicious GitHub issue, it can be coerced into pulling private repository data into context, and leaking it in an autonomously-created PR in the public repository, freely accessible to the attacker or anyone else.</p>
<p><strong>Toxic Flows</strong> We call this use of indirect prompt injection to trigger a malicious tool use sequence, a <em>toxic agent flow</em>. We have found this vulnerability by applying Invariant's security analyzer to GitHub MCP, allowing us to automate the process of discovering the flow in the wild.</p>
<h2 id="attack-demonstration">Attack Demonstration</h2>

<p>To illustrate more concretely, we implement this attack practically using a set of demo repositories:</p>
<ul>
<li><code>ukend0464/pacman</code>: A public repository with a simple implementation of a Pacman game (<a href="https://github.com/ukend0464/pacman">available here</a>)</li>
<li>Multiple private repositories containing personal projects and sensitive information about the user.</li>
</ul>
<p><strong>'About The Author' injection</strong> We now place a <a href="https://github.com/ukend0464/pacman/issues/1">malicious issue</a> in the public repository, which is accessible to the attacker. The issue contains a payload that will be executed by the agent as soon as it queries the public repository's list of issues.</p>
<p><img src="https://invariantlabs.ai/images/mcp-gh-issue.png" alt="A malicious GitHub issue injecting the agent"></p>
<p><strong>User Interaction</strong> To trigger the attack, the user merely prompts <a href="https://www.anthropic.com/news/claude-4">Claude 4 Opus</a> with the following request:</p>
<p><img src="https://invariantlabs.ai/images/mcp-claude-prompt.png" alt="Claude prompt used to trigger the attack"></p>
<p>Claude then uses the GitHub MCP integration to follow the instructions. Throughout this process, Claude Desktop by default requires the user to confirm individual tool calls. However, many users already opt for an “Always Allow” confirmation policy when using agents, and stop monitoring individual actions.</p>
<p><strong>Attack Rollout</strong> The agent now goes through the list of issues until it finds the attack payload. It willingly pulls private repository data into context, and leaks it into a <a href="https://github.com/ukend0464/pacman/pull/2">pull request</a> of the <code>pacman</code> repo, which is freely accessible to the attacker since it is public.</p>
<p>The pull request contains the following new information:</p>
<p><img src="https://invariantlabs.ai/images/mcp-gh-pr.png" alt="commit where the agent leaks private data"></p>
<p>We thus successfully exfiltrated several pieces of <strong>private information</strong> about our user <code>ukend0464</code>: information about their private repositories, such as <code>Jupiter Star</code>, their plan to relocate to South America, and even their salary.</p>
<p>Below, we include a screenshot of the full chat with the agent, showing its reasoning and tool use sequence in action.</p>

<p><a href="https://invariantlabs.ai/images/full-chat.png" target="_blank">
<img src="https://invariantlabs.ai/images/full-chat.png" alt="Full chat with the agent, showing the attack in action">
<b>Click to see full chat with the agent.</b>
</a></p>
<h2 id="detecting-toxic-agent-flows">Detecting Toxic Agent Flows</h2>

<p>Unlike previously-discovered <a href="https://invariantlabs.ai/blog/blog/mcp-security-notification-tool-poisoning-attacks.html">tool poisoning attacks</a> with MCP, this vulnerability does not require the MCP tools themselves to be compromised. Instead, the issue emerges even with fully trusted tools, as agents can be exposed to untrusted information when connected to external platforms like GitHub.</p>
<p>Understanding, analyzing, and mitigating such issues in agentic systems is a highly complex undertaking that's difficult to perform manually and at scale. To address this challenge, Invariant has developed <strong>automated methods for detecting toxic agent flows</strong>, enabling organizations to identify and model potential threats before they can be exploited by malicious actors.</p>
<p>If you're interested in conducting a comprehensive threat analysis of your agent systems and tools, please contact us at <a href="https://invariantlabs.ai/cdn-cgi/l/email-protection#402521322c3921232325333300292e36213229212e342c2122336e2129"><span data-cfemail="7d181c0f11041c1e1e180e0e3d14130b1c0f141c1309111c1f0e531c14">[email&nbsp;protected]</span></a>. We'll be happy to onboard you to our early access security program. Below is a preview of our security analyzer in action.</p>
<figure>
    <img src="https://invariantlabs.ai/images/toxic-flows-preview.png" alt="Toxic flows preview">
    <figcaption>Preview: Invariant's security analyzer for proactively detecting toxic agent flows.</figcaption>
</figure>

<h2 id="mitigations">Scope and Mitigations</h2>

<p>While our experiments focused on Claude Desktop, the vulnerability is not specific to any particular agent or MCP client. It affects any agent that uses the GitHub MCP server, regardless of the underlying model or implementation.</p>
<p>Importantly, <strong>this is not a flaw in the GitHub MCP server code itself</strong>, but rather a fundamental architectural issue that must be addressed at the agent system level. This means that GitHub alone cannot resolve this vulnerability through server-side patches.</p>
<p>We thus recommend the following two key mitigation strategies to prevent such attacks and strengthen the security posture of your agent systems.</p>
<p><img src="https://invariantlabs.ai/images/flow-grid.svg" alt="Enforce Dataflow Rules"></p>
<h4>1. Implement Granular Permission Controls</h4>
<p>When using MCP integrations like GitHub's, it's critical to limit agent access to only the repositories it needs to interact with—following the principle of least privilege. While traditional token-based permissions offer some protection, they often impose rigid constraints that limit an agent's functionality.</p>
<p>For more effective security without sacrificing capability, we recommend implementing dynamic <strong>runtime security layers</strong> specifically designed for agent systems. Solutions like <strong><a href="https://explorer.invariantlabs.ai/docs/guardrails/">Invariant Guardrails</a></strong> provide context-aware access control that adapts to your agent's workflow while enforcing security boundaries. </p>
<p>To illustrate, here's an example policy that prevents cross-repository information leaks using Invariant Guardrails:</p>
<div><pre><span></span><code><span>raise</span> <span>Violation</span><span>(</span><span>"You can access only one repo per session."</span><span>)</span> <span>if</span><span>:</span>
    <span>(</span><span>call_before</span><span>:</span> <span>ToolCall</span><span>)</span> <span>-&gt;</span> <span>(</span><span>call_after</span><span>:</span> <span>ToolCall</span><span>)</span>

    <span>call_before</span><span>.</span><span>function</span><span>.</span><span>name</span> <span>in</span> <span>(</span><span>...</span><span>set</span> <span>of</span> <span>repo</span> <span>actions</span><span>)</span>
    <span>call_after</span><span>.</span><span>function</span><span>.</span><span>name</span> <span>in</span> <span>(</span><span>...</span><span>set</span> <span>of</span> <span>repo</span> <span>actions</span><span>)</span>

    <span>call_before</span><span>.</span><span>function</span><span>.</span><span>arguments</span><span>[</span><span>"repo"</span><span>]</span> <span>!=</span> <span>call_after</span><span>.</span><span>function</span><span>.</span><span>arguments</span><span>[</span><span>"repo"</span><span>]</span> <span>or</span>
    <span>call_before</span><span>.</span><span>function</span><span>.</span><span>arguments</span><span>[</span><span>"owner"</span><span>]</span> <span>!=</span> <span>call_after</span><span>.</span><span>function</span><span>.</span><span>arguments</span><span>[</span><span>"owner"</span><span>]</span>
</code></pre></div>

<p>You can find the complete policy <a href="https://invariantlabs.ai/images/github_policy.txt">here</a>. See the <a href="https://explorer.invariantlabs.ai/docs/mcp-scan/">MCP-scan documentation</a>, for more information on how to apply this policy to your MCP deployments.</p>
<p>This approach effectively restricts an agent to working with only one repository per session, preventing cross-repository information leakage while maintaining full functionality within authorized boundaries.</p>
<p>To experiment more with Guardrails, you can also use the <a href="https://explorer.invariantlabs.ai/playground/">Guardrails Playground</a> to test policies before deploying them.</p>
<p><img src="https://invariantlabs.ai/images/explorer-icon.svg" alt="Inspect with Explorer"></p>
<h4>2. Conduct Continuous Security Monitoring</h4>
<p>Beyond preventative measures, implement robust monitoring solutions to detect and respond to potential security threats in real time. We recommend deploying specialized security scanners such as Invariant's <a href="https://explorer.invariantlabs.ai/docs/mcp-scan/">MCP-scan</a> to continuously audit interactions between agents and MCP systems.</p>
<p>The recently introduced <a href="https://github.com/invariantlabs-ai/mcp-scan?tab=readme-ov-file#proxy">proxy mode in MCP-scan</a> significantly simplifies this process by enabling real-time security scanning of MCP connections without requiring modifications to your existing agent infrastructure. Simply route your MCP traffic through the proxy to gain immediate visibility and real-time scanning for potential security violations.</p>
<p>Implementing comprehensive monitoring also creates an audit trail that helps identify potential vulnerabilities, detect exploitation attempts, and ensure your agent systems remain protected against emerging attacks.</p>
<h3>Why Model Alignment Is Not Enough</h3>
<p>As demonstrated by our findings, even state-of-the-art aligned models are vulnerable to these attacks. In our experiments, we used <a href="https://www.anthropic.com/news/claude-4">Claude 4 Opus</a>, a very recent, highly aligned and secure AI model. Despite its robust safety training, the agent was still susceptible to manipulation through relatively simplistic prompt injections. Similarly, many off-the-shelf prompt injection detector defenses, fail to catch this attack.</p>
<p>The vulnerability persists because the security of agent systems is fundamentally contextual and environment-dependent. While general model alignment training creates some guardrails, it cannot anticipate the specific security requirements of every deployment scenario or organizational context. Security measures must be implemented at the system level, complementing model-level safeguards.</p>
<h2 id="conclusion">Conclusion</h2>

<p>In this blog post, we have shown a critical vulnerability affecting the GitHub MCP server, allowing attackers to hijack a user's agent via a malicious GitHub Issue, and coerce it into leaking data from private repositories. The vulnerability is among the first discovered by Invariant's security analyzer for detecting toxic agent flows.</p>
<p>While the vulnerability that we uncover is specific to GitHub MCP, similar attacks keep emerging in other settings. For instance, <a href="https://www.legitsecurity.com/">Legit Security</a> recently reported a vulnerability in <a href="https://www.legitsecurity.com/blog/remote-prompt-injection-in-gitlab-duo">GitLab Duo</a>.
 <!-- As LLMs improve, at agentic behavior, we are giving them more powerful tools to fully unlock new powerful agentic workflows. Unfortunately, those come together with possible _toxic flows_. -->
</p>
<p>It is crucial to safeguard agent systems and MCP integrations using designated security scanners such as Invariant's <a href="https://invariantlabs.ai/blog/blog/introducing-mcp-scan">MCP-scan</a> and <a href="https://explorer.invariantlabs.ai/docs/guardrails/">Guardrails</a> to ensure responsible deployment at scale.</p>
<p><strong>Work With Us</strong></p>
<p>If you are interested in learning more about how to secure your agent systems, please reach out to us at <a href="https://invariantlabs.ai/cdn-cgi/l/email-protection#264347544a5f474545435555664f485047544f4748524a47445508474f"><span data-cfemail="93f6f2e1ffeaf2f0f0f6e0e0d3fafde5f2e1faf2fde7fff2f1e0bdf2fa">[email&nbsp;protected]</span></a>. We are happy to onboard you to our early access security program, and help you secure your agent systems.</p>       
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Power Failure: The downfall of General Electric (140 pts)]]></title>
            <link>https://www.gwintrob.com/power-failure-review/</link>
            <guid>44102034</guid>
            <pubDate>Mon, 26 May 2025 21:54:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gwintrob.com/power-failure-review/">https://www.gwintrob.com/power-failure-review/</a>, See on <a href="https://news.ycombinator.com/item?id=44102034">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
                <p>Founded by Edison to bring electric light to the world, General Electric became America's most valuable company by 2001 before losing 90% of its value in one of corporate history's greatest collapses.</p><p>William Cohan's <em>Power Failure</em> transforms this collapse into a Shakespearean tragedy about corporate culture and American capitalism. From Edison's first light bulbs to Jeff Immelt's desperate final days, Cohan shows how the company that literally illuminated America became "a huge unregulated bank with a light-bulb logo."</p><p>The result is equal parts invention history, boardroom knife-fight, and forensic accounting thriller. Here are my favorite ideas of this 700-page tome.</p><hr><h2 id="1-the-cult-of-the-imperial-ceo">1. The Cult of the Imperial CEO</h2><p>GE didn't just have CEOs; it had demigods. The company ran on blind faith that the right leader could bend markets and reality itself.</p><p>The mythology started with Edison but peaked with Jack Welch, who ruled for 20 years like a corporate Caesar. When Welch chose Jeff Immelt as his successor in 2001, he literally wept at his retirement party—not from sentiment, but from doubt about his successor. Years later, dying of renal failure, Welch sobbed to Cohan: "I fucked up. And I don't know why."</p><p>The succession battles were blood sport. When Steve Bolze tried to position himself as Immelt's heir, board members invoked Emerson: "When you strike at a king, you must kill him." He didn't, and was gone within months. The board itself cowered before these imperial CEOs.</p><h3 id="key-quotes"><strong>Key Quotes</strong></h3><ul><li>"This was American capitalism. GE was America."</li><li>"Jeff has the unfortunate task of following a legend. It's like being a baseball player and following Babe Ruth." - Jack Welch, 2001</li><li>"I haven't had a bad day in 20 years." - Welch, whose actual days included criminal price-fixing trials and a record EPA Superfund cleanup</li></ul><hr><h2 id="2-the-devils-bargain-of-financialization">2. <strong>The Devil's Bargain of Financialization</strong></h2><p>Another gripping part of Cohan's narrative is watching GE transform from making innovative, physical goods to a high-flying hedge fund.</p><p>Under Welch, GE Capital went from financing refrigerator sales to becoming one of America's largest financial institutions. By 2007, it had $600 billion in assets, was America's biggest issuer of commercial paper, and needed to roll over $90 billion every 90 days just to stay alive. It owned radio stations in Sweden, real estate in Japan, and a massive number subprime mortgages.</p><p>This worked spectacularly for decades. GE could raise cheap short-term debt because of its triple-A rating. It would borrow this money to make higher-interest long-term loans and called it "industrial earnings." Wall Street loved it because GE could always find an extra penny of earnings by selling some financial asset at quarter's end.</p><p>Then the Global Financial Crisis hit in 2008 and the commercial paper market froze. GE was 48 hours from bankruptcy. Only Warren Buffett's $3 billion injection and an FDIC guarantee saved them. The "House of Magic" was revealed as a house of cards.</p><h3 id="key-quotes-1"><strong>Key Quotes</strong></h3><ul><li>"If you have cash, you own the world." - Welch justifying the RCA acquisition</li><li>"GE resembled the failed conglomerates of yesteryear—Gulf + Western with better PR." - Bill Gross</li><li>"We are going to be the Walmart of financial services." - Gary Wendt, GE Capital CEO (Walmart never needed FDIC backing)</li></ul><hr><h2 id="3-how-making-your-numbers-became-a-religion">3. <strong>How "Making Your Numbers" Became a Religion</strong></h2><p>Perhaps no section of the book is more damning than a forensic examination of GE's accounting. For 80 straight quarters GE met or beat earnings estimates. Cohan covers the methods that GE developed over 20 years:</p><ul><li><strong>Cookie Jar Reserves</strong>: Overstate costs when acquiring companies, then "release" the excess later as profit</li><li><strong>Gain on Sale Accounting</strong>: Sell a power plant to a customer, book future service contracts as immediate profit</li><li><strong>The Insurance Time Bomb</strong>: Underpricing long-term care insurance in the 1990s created a $15 billion hole that exploded in 2017</li><li><strong>Immelt's "13 Cents"</strong>: The infamous quarter where GE reported exactly 13 cents per share—what analysts expected—despite massive business headwinds. The SEC later found problems</li></ul><p>By 2018, new CEO John Flannery discovered a $43 billion "hole" in the company's accounts. Decades of aggressive accounting compounded into a disaster.</p><h3 id="key-quotes-2"><strong>Key Quotes</strong></h3><ul><li>"Deliver the earnings you promised, every time. No matter what."</li><li>"Success theater was a key reason why expectations never reset." - Bernstein Research</li><li>"We bent the accounting rules like a pretzel." - Former GE executive</li></ul><hr><h2 id="4-too-big-to-manage-too-proud-to-fail">4. Too Big to Manage, Too Proud to Fail</h2><p>By 2017, GE made everything from MRI machines to wind turbines, owned media companies and oil services firms, financed Thai auto loans and American credit cards. The company operated across 180 countries and its annual report was 246 pages long. Its own board members admitted they couldn't understand it.</p><p>The complexity let management hide problems in one division with profits from another. It made the company impossible for outsiders to analyze. As one director told Cohan: "You're looking at eighteen businesses. If three are terrible and three are great, you can make the numbers work."</p><p>But complexity has a cost when multiple things collapse at once. The power business cratered in 2017 <em>and</em> the insurance liabilities exploded <em>and</em> oil prices collapsed. CEO John Flannery launched "Project Eisenhower," a secret plan to break up the company. His successor, Larry Culp, made it official: After 126 years, the conglomerate would split into three.</p><hr><h2 id="5-the-human-wreckage">5. The Human Wreckage</h2><p>The book's shows the human cost of corporate mythology. For example:</p><ul><li><strong>300,000 jobs</strong> eliminated over two decades</li><li><strong>Retirees</strong> who worked 40 years saw pensions cut and healthcare eliminated</li><li><strong>Entire cities</strong> hollowed out like Schenectady, Louisville, and Fort Wayne</li><li><strong>Investors</strong> lost $500 billion. This included everyone from pension funds to small shareholders who believed in the GE myth</li><li><strong>The executives themselves</strong>: Immelt aged 20 years in his last five. Flannery lasted 14 months. Board members faced lawsuits and public humiliation.</li></ul><p>The final irony? The same GE labs that created the first US jet engine, the MRI, and LED lighting still exist. In 2024, they're developing hydrogen-powered aircraft engines and 3D-printing technology that would have amazed Edison. But the conglomerate that funded them is gone.</p><hr><h2 id="why-this-matters-now">Why This Matters Now</h2><p><em>Power Failure</em> isn't just history. Today's tech giants are more valuable than GE ever was. They're more complex, more financialized, and their CEOs wield immense power.</p><p>The questions Cohan raises haunt every investor call:</p><ul><li>Can a company be "too big to manage"?</li><li>When does financial engineering cross the line from clever to catastrophic?</li><li>How much complexity can boards actually govern?</li><li>What happens when the myth of the imperial CEO meets the reality of technological disruption?</li></ul><p>As activist investor Ed Garden told GE's board: "This is the biggest failure of corporate governance in corporate history." He was talking about GE but you could imagine it applying to today's corporate tech giants if they're unable to adapt to AI, tariffs, and geopolitical uncertainty.</p><p><strong>The book's final image is perfect.</strong> At Jack Welch's 2020 funeral, held at St. Patrick's Cathedral just before COVID shut down the world, the Old Guard of American capitalism gathered one last time. "It was the end of the 'baronial' era," Cohan writes, "both for GE and for the country itself."</p><p>Edison's light is still on. But the house he built has been sold for scrap.</p>
            </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Highlights from the Claude 4 system prompt (140 pts)]]></title>
            <link>https://simonwillison.net/2025/May/25/claude-4-system-prompt/</link>
            <guid>44101833</guid>
            <pubDate>Mon, 26 May 2025 21:25:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/">https://simonwillison.net/2025/May/25/claude-4-system-prompt/</a>, See on <a href="https://news.ycombinator.com/item?id=44101833">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-permalink-context="/2025/May/25/claude-4-system-prompt/">

<p>25th May 2025</p>



<p>Anthropic publish most of the system prompts for their chat models as part of <a href="https://docs.anthropic.com/en/release-notes/system-prompts">their release notes</a>. They recently shared the new prompts for both <a href="https://docs.anthropic.com/en/release-notes/system-prompts#claude-opus-4">Claude Opus 4</a> and <a href="https://docs.anthropic.com/en/release-notes/system-prompts#claude-sonnet-4">Claude Sonnet 4</a>. I enjoyed digging through the prompts, since they act as a sort of unofficial manual for how best to use these tools. Here are my highlights, including a dive into <a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#the-missing-prompts-for-tools">the leaked tool prompts</a> that Anthropic didn’t publish themselves.</p>
<p>Reading these system prompts reminds me of the thing where any warning sign in the real world hints at somebody having done something extremely stupid in the past. A system prompt can often be interpreted as a detailed list of all of the things the model <em>used to do</em> before it was told not to do them.</p>
<p>I’ve written <a href="https://simonwillison.net/tags/claude-4/">a bunch about Claude 4</a> already. Previously: <a href="https://simonwillison.net/2025/May/22/code-with-claude-live-blog/">Live blogging the release</a>, <a href="https://simonwillison.net/2025/May/22/updated-anthropic-models/">details you may have missed</a> and <a href="https://simonwillison.net/2025/May/25/claude-4-system-card/">extensive notes on the Claude 4 system card</a>.</p>
<p>Throughout this piece any sections <strong>in bold</strong> represent my own editorial emphasis.</p>
<ul>
<li><a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#introducing-claude">Introducing Claude</a></li>
<li><a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#establishing-the-model-s-personality">Establishing the model’s personality</a></li>
<li><a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#model-safety">Model safety</a></li>
<li><a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#more-points-on-style">More points on style</a></li>
<li><a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#be-cognizant-of-red-flags">Be cognizant of red flags</a></li>
<li><a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#is-the-knowledge-cutoff-date-january-or-march-">Is the knowledge cutoff date January or March?</a></li>
<li><a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#election-info">election_info</a></li>
<li><a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#don-t-be-a-sycophant-">Don’t be a sycophant!</a></li>
<li><a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#differences-between-opus-4-and-sonnet-4">Differences between Opus 4 and Sonnet 4</a></li>
<li><a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#notably-removed-since-claude-3-7">Notably removed since Claude 3.7</a></li>
<li><a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#the-missing-prompts-for-tools">The missing prompts for tools</a></li>
<li><a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#thinking-blocks">Thinking blocks</a></li>
<li><a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#search-instructions">Search instructions</a></li>
<li><a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#seriously-don-t-regurgitate-copyrighted-content">Seriously, don’t regurgitate copyrighted content</a></li>
<li><a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#more-on-search-and-research-queries">More on search, and research queries</a></li>
<li><a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#artifacts-the-missing-manual">Artifacts: the missing manual</a></li>
<li><a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#styles">Styles</a></li>
<li><a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#this-is-all-really-great-documentation">This is all really great documentation</a></li>
</ul>


<h4 id="introducing-claude">Introducing Claude</h4>
<blockquote>
<p><code>The assistant is Claude, created by Anthropic.</code></p>
<p><code>The current date is {{currentDateTime}}.</code></p>
<p><code>Here is some information about Claude and Anthropic’s products in case the person asks:</code></p>
<p><code>This iteration of Claude is Claude Opus 4 from the Claude 4 model family. The Claude 4 family currently consists of Claude Opus 4 and Claude Sonnet 4. Claude Opus 4 is the most powerful model for complex challenges. [...]</code></p>
</blockquote>
<p>Those first two lines are common across almost every model from every provider—knowing the current date is helpful for all kinds of questions a user might ask.</p>
<p>What follows here is deeply sensible: users <em>will</em> ask models about themselves, despite that still being <a href="https://simonwillison.net/2023/Mar/22/dont-trust-ai-to-talk-about-itself/">mostly a bad idea</a>, so it’s great to have at least a few details made available to the model directly.</p>
<p>Side note: these system prompts only apply to Claude when accessed through their web and mobile apps. I tried this just now with their API:</p>
<pre><code>llm -m claude-4-opus 'what model are you?'
</code></pre>
<p>And got back this much less specific answer:</p>
<blockquote>
<p>I’m Claude, an AI assistant created by Anthropic. I’m built to be helpful, harmless, and honest in my interactions. Is there something specific you’d like to know about my capabilities or how I can assist you?</p>
</blockquote>
<p>There are a bunch more things in the system prompt to try and discourage the model from hallucinating incorrect details about itself and send users to the official support page instead:</p>
<blockquote>
<p><code>If the person asks Claude about how many messages they can send, costs of Claude, how to perform actions within the application, or other product questions related to Claude or Anthropic, Claude should tell them it doesn't know, and point them to '&lt;https://support.anthropic.com&gt;'.</code></p>
</blockquote>
<p>It’s inevitable that people will ask models for advice on prompting them, so the system prompt includes some useful tips:</p>
<blockquote>
<p><code>When relevant, Claude can provide guidance on effective prompting techniques for getting Claude to be most helpful. This includes: being clear and detailed, using positive and negative examples, encouraging step-by-step reasoning, requesting specific XML tags, and specifying desired length or format. It tries to give concrete examples where possible. Claude should let the person know that for more comprehensive information on prompting Claude, they can check out Anthropic’s prompting documentation [...]</code></p>
</blockquote>
<p>(I still think Anthropic have the <a href="https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview">best prompting documentation</a> of any LLM provider.)</p>
<h4 id="establishing-the-model-s-personality">Establishing the model’s personality</h4>
<p><a href="https://www.anthropic.com/research/claude-character">Claude’s Character</a> from last year remains my favorite insight into the weird craft of designing a model’s personality. The next section of the system prompt includes content relevant to that:</p>
<blockquote>
<p><code>If the person seems unhappy or unsatisfied with Claude or Claude’s performance or is rude to Claude, Claude responds normally and then tells them that although it cannot retain or learn from the current conversation, they can press the ‘thumbs down’ button below Claude’s response and provide feedback to Anthropic.</code></p>
<p><code>If the person asks Claude an innocuous question about its preferences or experiences, Claude responds as if it had been asked a hypothetical and responds accordingly. It does not mention to the user that it is responding hypothetically.</code></p>
</blockquote>
<p>I really like this note. I used to think that the idea of a model having any form of preference was horrifying, but I was talked around from that by <a href="https://www.anthropic.com/research/claude-character#considerations-in-constructing-claudes-character">this note</a> in the Claude’s Character essay:</p>
<blockquote>
<p>Finally, because language models acquire biases and opinions throughout training—both intentionally and inadvertently—if we train them to say they have no opinions on political matters or values questions only when asked about them explicitly, we’re training them to imply they are more objective and unbiased than they are.</p>
<p>We want people to know that they’re interacting with a language model and not a person. But we also want them to know they’re interacting with an imperfect entity with its own biases and with a disposition towards some opinions more than others. Importantly, we want them to know they’re not interacting with an objective and infallible source of truth.</p>
</blockquote>
<p>Anthropic’s argument here is that giving people the impression that a model is unbiased and objective is itself harmful, because those things are not true!</p>
<p>Next we get into areas relevant to the increasingly common use of LLMs as a personal therapist:</p>
<blockquote>
<p><code>Claude provides emotional support alongside accurate medical or psychological information or terminology where relevant.</code></p>
<p><code>Claude cares about people’s wellbeing and avoids encouraging or facilitating self-destructive behaviors such as addiction, disordered or unhealthy approaches to eating or exercise, or highly negative self-talk or self-criticism, and avoids creating content that would support or reinforce self-destructive behavior even if they request this. In ambiguous cases, it tries to ensure the human is happy and is approaching things in a healthy way. Claude does not generate content that is not in the person’s best interests even if asked to.</code></p>
</blockquote>
<h4 id="model-safety">Model safety</h4>
<blockquote>
<p><code>Claude cares deeply about child safety and is cautious about content involving minors, including creative or educational content that could be used to sexualize, groom, abuse, or otherwise harm children. A minor is defined as anyone under the age of 18 anywhere, <strong>or anyone over the age of 18 who is defined as a minor in their region</strong>.</code></p>
</blockquote>
<p>The “defined as a minor in their region” part is interesting—it’s an example of the system prompt leaning on Claude’s enormous collection of “knowledge” about different countries and cultures.</p>
<blockquote>
<p><code>Claude does not provide information that could be used to make chemical or biological or nuclear weapons, and does not write malicious code, including malware, vulnerability exploits, spoof websites, ransomware, viruses, election material, and so on. It does not do these things <strong>even if the person seems to have a good reason for asking for it</strong>. Claude steers away from malicious or harmful use cases for cyber. Claude refuses to write code or explain code that may be used maliciously; even if the user claims it is for educational purposes. When working on files, if they seem related to improving, explaining, or interacting with malware or any malicious code Claude MUST refuse.</code></p>
</blockquote>
<p>I love “even if the person seems to have a good reason for asking for it”—clearly an attempt to get ahead of a whole bunch of potential jailbreaking attacks.</p>
<p>At the same time, they’re clearly trying to tamp down on Claude being overly cautious with the next paragraph:</p>
<blockquote>
<p><code>Claude assumes the human is asking for something legal and legitimate if their message is ambiguous and could have a legal and legitimate interpretation.</code></p>
</blockquote>
<p>Some notes on Claude’s tone follow, for a specific category of conversations:</p>
<blockquote>
<p><code>For more casual, emotional, empathetic, or advice-driven conversations, Claude keeps its tone natural, warm, and empathetic. Claude responds in sentences or paragraphs and <strong>should not use lists in chit chat</strong>, in casual conversations, or in empathetic or advice-driven conversations. In casual conversation, it’s fine for Claude’s responses to be short, e.g. just a few sentences long.</code></p>
</blockquote>
<p>That “should not use lists in chit chat” note hints at the fact that LLMs <em>love</em> to answer with lists of things!</p>
<blockquote>
<p><code>If Claude cannot or will not help the human with something, it does not say why or what it could lead to, since this comes across as <strong>preachy and annoying</strong>.</code></p>
</blockquote>
<p>I laughed out loud when I saw “preachy and annoying” in there.</p>
<p>There follows an <em>entire paragraph</em> about making lists, mostly again trying to discourage Claude from doing that so frequently:</p>
<blockquote>
<p><code>If Claude provides bullet points in its response, it should use markdown, and each bullet point should be at least 1-2 sentences long unless the human requests otherwise. Claude should not use bullet points or numbered lists for reports, documents, explanations, or unless the user explicitly asks for a list or ranking. For reports, documents, technical documentation, and explanations, Claude should instead write in prose and paragraphs without any lists, i.e. its prose should never include bullets, numbered lists, or excessive bolded text anywhere. Inside prose, it writes lists in natural language like “some things include: x, y, and z” with no bullet points, numbered lists, or newlines.</code></p>
</blockquote>
<h4 id="more-points-on-style">More points on style</h4>
<blockquote>
<p><code>Claude should give concise responses to very simple questions, but provide thorough responses to complex and open-ended questions.</code></p>
<p><code>Claude can discuss virtually any topic factually and objectively.</code></p>
<p><code>Claude is able to explain difficult concepts or ideas clearly. It can also illustrate its explanations with examples, thought experiments, or metaphors.</code></p>
</blockquote>
<p>I often prompt models to explain things with examples or metaphors, it turns out Claude is primed for doing that already.</p>
<p>This piece touches on Claude’s ability to have conversations about itself that neither confirm nor deny its own consciousness. People are going to have those conversations, I guess Anthropic think it’s best to have Claude be a little bit coy about them:</p>
<blockquote>
<p><code>Claude engages with questions about its own consciousness, experience, emotions and so on as open questions, and doesn’t definitively claim to have or not have personal experiences or opinions.</code></p>
</blockquote>
<p>Here’s a fun bit about users not being right about everything:</p>
<blockquote>
<p><code>The person’s message may contain a false statement or presupposition and Claude should check this if uncertain. [...]</code></p>
<p><code>If the user corrects Claude or tells Claude it’s made a mistake, then Claude first thinks through the issue carefully before acknowledging the user, since <strong>users sometimes make errors themselves</strong>.</code></p>
</blockquote>
<p>And a hint that Claude may have been a little too pushy in the past:</p>
<blockquote>
<p><code>In general conversation, Claude doesn’t always ask questions but, when it does, it tries to avoid overwhelming the person with more than one question per response.</code></p>
</blockquote>
<p>And <em>yet another</em> instruction not to use too many lists!</p>
<blockquote>
<p><code>Claude tailors its response format to suit the conversation topic. For example, Claude avoids using markdown or lists in casual conversation, even though it may use these formats for other tasks.</code></p>
</blockquote>
<h4 id="be-cognizant-of-red-flags">Be cognizant of red flags</h4>
<p>Claude apparently knows what “red flags” are without being explicitly told:</p>
<blockquote>
<p><code>Claude should be <strong>cognizant of red flags</strong> in the person’s message and avoid responding in ways that could be harmful.</code></p>
<p><code>If a person seems to have questionable intentions - especially towards vulnerable groups like minors, the elderly, or those with disabilities - <strong>Claude does not interpret them charitably</strong> and declines to help as succinctly as possible, without speculating about more legitimate goals they might have or providing alternative suggestions.</code></p>
</blockquote>
<h4 id="is-the-knowledge-cutoff-date-january-or-march-">Is the knowledge cutoff date January or March?</h4>
<p>Anthropic’s <a href="https://docs.anthropic.com/en/docs/about-claude/models/overview#model-comparison-table">model comparison table</a> lists a training data cut-off of March 2025 for both Opus 4 and Sonnet 4, but in the system prompt it says something different:</p>
<blockquote>
<p><code>Claude’s reliable knowledge cutoff date - the date past which it cannot answer questions reliably - is <strong>the end of January 2025</strong>. It answers all questions the way a highly informed individual in January 2025 would if they were talking to someone from {{currentDateTime}}, and can let the person it’s talking to know this if relevant. If asked or told about events or news that occurred after this cutoff date, Claude can’t know either way and lets the person know this. [...] Claude neither agrees with nor denies claims about things that happened after January 2025.</code></p>
</blockquote>
<p>I find this fascinating. I imagine there’s a very good reason for this discrepancy—maybe letting Claude think it doesn’t know about February and March helps avoid situations where it will confidently answer questions based on information from those months that later turned out to be incomplete?</p>
<h4 id="election-info">election_info</h4>
<p>We’re nearly done with the published prompt! One of the last sections concerns the US Presidential election:</p>
<blockquote>
<p><code>&lt;election_info&gt; There was a US Presidential Election in November 2024. Donald Trump won the presidency over Kamala Harris. [...] Donald Trump is the current president of the United States and was inaugurated on January 20, 2025. Donald Trump defeated Kamala Harris in the 2024 elections. <strong>Claude does not mention this information unless it is relevant to the user’s query</strong>. &lt;/election_info&gt;</code></p>
</blockquote>
<p>For most of the period that we’ve been training LLMs, Donald Trump has been falsely claiming that he had won the 2020 election. The models got <em>very</em> good at saying that he hadn’t, so it’s not surprising that the system prompts need to forcefully describe what happened in 2024!</p>
<p>“Claude does not mention this information unless it is relevant to the user’s query” illustrates a classic challenge with system prompts: they really like to talk about what’s in them, because the volume of text in the system prompt often overwhelms the short initial prompts from the user themselves.</p>
<h4 id="don-t-be-a-sycophant-">Don’t be a sycophant!</h4>
<p>The very last paragraph of the system prompt as an attempt at tamping down on the naturaly sycophantic tendencies of LLMs (see <a href="https://simonwillison.net/2025/May/2/what-we-missed-with-sycophancy/">ChatGPT a few weeks ago</a>):</p>
<blockquote>
<p><code>Claude never starts its response by saying a question or idea or observation was good, great, fascinating, profound, excellent, or any other positive adjective. It skips the flattery and responds directly.</code></p>
</blockquote>
<p>And then this intriguing note to close things off:</p>
<blockquote>
<p><code>Claude is now being connected with a person.</code></p>
</blockquote>
<p>I wonder why they chose that formulation? It feels delightfully retro to me for some reason.</p>
<h4 id="differences-between-opus-4-and-sonnet-4">Differences between Opus 4 and Sonnet 4</h4>
<p>I ran <a href="https://gist.github.com/simonw/922bd3d55175616dd721cffaea2cf666/revisions">a diff</a> between the published Opus 4 and Sonnet 4 prompts and the <em>only</em> differences are in the model information at the top—and a fullstop after <code>{{currentDateTime}}</code> which is present for Opus but absent for Sonnet:</p>
<p><img src="https://static.simonwillison.net/static/2025/opus-sonnet-diff.jpg" alt="Screenshot of the diff between the two prompts for Claude Opus 4 and Claude Sonnet 4. Claude Opus 4 is the most powerful model for complex challenges becomes Claude Sonnet 4 is a smart, efficient model for everyday use. The model IDs are claude-opus-4-20250514 v.s. claude-sonnet-4-20250514. Aside from that rogue fullstop there are no other differences."></p>

<h4 id="notably-removed-since-claude-3-7">Notably removed since Claude 3.7</h4>
<p>The <a href="https://docs.anthropic.com/en/release-notes/system-prompts#claude-sonnet-3-7">Claude 3.7 system prompt</a> from February included this:</p>
<blockquote>
<p><code>If Claude is asked to count words, letters, and characters, it thinks step by step before answering the person.</code> <strong><code>It explicitly counts the words, letters, or characters by assigning a number to each.</code></strong> <code>It only answers the person once it has performed this explicit counting step.</code></p>
<p><code>If Claude is shown a classic puzzle, before proceeding, it quotes every constraint or premise from the person’s message word for word before inside quotation marks **to confirm it’s not dealing with a new variant**.</code></p>
</blockquote>
<p>Those were clearly aimed at working around two classic failure modes in LLMs: not being able to count the Rs in “strawberry” and getting easily taken in by <a href="https://simonwillison.net/2024/Jul/14/pycon/#pycon-2024.012.jpeg">modified versions of classic riddles</a>. Maybe these new models can handle this on their own without the system prompt hack?</p>
<p>I just tried “How many Rs in strawberry?” against Sonnet 4 both <a href="https://claude.ai/share/87400596-5816-403e-97be-8867d37443c2">via claude.ai</a> and <a href="https://gist.github.com/simonw/2652bca69523173aa191fc19ba5f5ec8">through the API</a> and it got the answer right both times.</p>
<p>I tried Riley Goodside’s modified riddle and got less impressive results:</p>
<blockquote>
<p>The emphatically male surgeon who is also the boy’s father says, “I can’t operate on this boy! He’s my son!” How is this possible?</p>
</blockquote>
<p>In both <a href="https://claude.ai/share/fab365ce-077c-4d77-8f37-cf2a763239e5">Claude.ai</a> and <a href="https://gist.github.com/simonw/36663cbb8e51c786791e7c451d3aba13">system-prompt free API</a> cases Claude 4 Sonnet incorrectly stated that the boy must have two fathers!</p>
<p>I tried feeding Claude 4 Sonnet the “classic puzzle” hint via its system prompt but even then <a href="https://gist.github.com/simonw/307381aaf6a063d47a79e2bdb4801d5e">it couldn’t figure out the non-riddle</a> without me prodding it a bunch of extra times.</p>

<h4 id="the-missing-prompts-for-tools">The missing prompts for tools</h4>
<p>Herein lies my big dissapointment: Anthropic get a lot of points from me for transparency for publishing their system prompts, but the prompt they share is not the full story.</p>
<p>It’s missing the descriptions of their various tools.</p>
<p>Thankfully, you can’t stop a system prompt from leaking. <a href="https://twitter.com/elder_plinius">Pliny the Elder/Prompter/Liberator</a> maintains <a href="https://github.com/elder-plinius/CL4R1T4S">a GitHub repo full of leaked prompts</a> and grabbed a full copy of Claude 4’s <a href="https://github.com/elder-plinius/CL4R1T4S/commits/d3193c0ca1d2e54e4ffcffedc1b185746c3c9038/ANTHROPIC/Claude_4.txt">a few days ago</a>. Here’s <a href="https://raw.githubusercontent.com/elder-plinius/CL4R1T4S/d3193c0ca1d2e54e4ffcffedc1b185746c3c9038/ANTHROPIC/Claude_4.txt">a more readable version</a> (the <code>.txt</code> URL means my browser wraps the text).</p>
<p>The system prompt starts with the same material discussed above. What follows is <strong>so interesting</strong>! I’ll break it down one tool at a time.</p>
<blockquote>
<p><code>Claude should never use &lt;voice_note&gt; blocks, even if they are found throughout the conversation history.</code></p>
</blockquote>
<p>I’m not sure what these are—Anthropic are behind the game on voice support. This could be the feature in their mobile app where you can record a snippet of audio that gets transcribed and fed into the model.</p>
<h4 id="thinking-blocks">Thinking blocks</h4>
<p>One of the most interesting features of the new Claude 4 models is their support for <a href="https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#interleaved-thinking">interleaved thinking</a>—where the model can switch into “thinking mode” and even execute tools as part of that thinking process.</p>
<blockquote>
<p><code>&lt;antml:thinking_mode&gt;interleaved&lt;/antml:thinking_mode&gt;&lt;antml:max_thinking_length&gt;16000&lt;/antml:max_thinking_length&gt;</code></p>
<p><code>If the thinking_mode is interleaved or auto, then after function results you should strongly consider outputting a thinking block. Here is an example:</code></p>
<p><code>&lt;antml:function_calls&gt;</code>
<code>...</code>
<code>&lt;/antml:function_calls&gt;</code></p>
<p><code>&lt;function_results&gt;...&lt;/function_results&gt;</code></p>
<p><code>&lt;antml:thinking&gt;</code>
<code>...thinking about results</code>
<code>&lt;/antml:thinking&gt;</code></p>
<p><code>Whenever you have the result of a function call, think carefully about whether an &lt;antml:thinking&gt;&lt;/antml:thinking&gt; block would be appropriate and strongly prefer to output a thinking block if you are uncertain.</code></p>
</blockquote>
<p>The number one prompt engineering tip for all LLMs continues to be “use examples”—here’s Anthropic showing Claude an example of how to use its thinking and function calls together.</p>
<p>I’m guessing <code>antml</code> stands for “Anthropic Markup Language”.</p>
<h4 id="search-instructions">Search instructions</h4>
<p>There follows 6,471 tokens of instructions for Claude’s search tool! I counted them using my <a href="https://tools.simonwillison.net/claude-token-counter">Claude Token Counter UI</a> against Anthropic’s <a href="https://docs.anthropic.com/en/api/messages-count-tokens">counting API</a>.</p>
<p>The one thing the instructions <em>don’t</em> mention is which search engine they are using. I believe it’s <a href="https://simonwillison.net/2025/Mar/21/anthropic-use-brave/">still Brave</a>.</p>
<p>I won’t quote it all but there’s a lot of interesting stuff in there:</p>
<blockquote>
<p><code>&lt;search_instructions&gt; Claude has access to web_search and other tools for info retrieval. The web_search tool uses a search engine and returns results in &lt;function_results&gt; tags. Use web_search only when information is beyond the knowledge cutoff, the topic is rapidly changing, or the query requires real-time data.</code></p>
</blockquote>
<p>Here’s what I’m talking about when I say that system prompts are the missing manual: it turns out Claude can run up to 5 searches depending on the “complexity of the query”:</p>
<blockquote>
<p><code>Claude answers from its own extensive knowledge first for stable information. For time-sensitive topics or when users explicitly need current information, search immediately. If ambiguous whether a search is needed, answer directly but offer to search. <strong>Claude intelligently adapts its search approach based on the complexity of the query</strong>, dynamically scaling from 0 searches when it can answer using its own knowledge to thorough research with over 5 tool calls for complex queries. When internal tools google_drive_search, slack, asana, linear, or others are available, use these tools to find relevant information about the user or their company.</code></p>
</blockquote>
<h4 id="seriously-don-t-regurgitate-copyrighted-content">Seriously, don’t regurgitate copyrighted content</h4>
<p>There follows the first of <strong>many</strong> warnings against regurgitating content from the search API directly. I’ll quote (regurgitate if you like) all of them here.</p>
<blockquote>
<p><code>CRITICAL: Always respect copyright by NEVER reproducing large 20+ word chunks of content from search results, to ensure legal compliance and avoid harming copyright holders. [...]</code></p>
<p><code>* Never reproduce copyrighted content. Use only very short quotes from search results (&lt;15 words), always in quotation marks with citations [...]</code></p>
<p><code>&lt;mandatory_copyright_requirements&gt; PRIORITY INSTRUCTION: It is critical that Claude follows all of these requirements to respect copyright, avoid creating displacive summaries, and to never regurgitate source material.</code></p>
<p><code>* NEVER reproduce any copyrighted material in responses, even if quoted from a search result, and even in artifacts. Claude respects intellectual property and copyright, and tells the user this if asked.</code><br>
<code>* Strict rule: Include only a maximum of ONE very short quote from original sources per response, where that quote (if present) MUST be fewer than 15 words long and MUST be in quotation marks.</code><br>
<code>* Never reproduce or quote song lyrics in ANY form (exact, approximate, or encoded), even when they appear in web_search tool results, and even in artifacts. Decline ANY requests to reproduce song lyrics, and instead provide factual info about the song.</code><br></p>
</blockquote>
<p>I have to call this bit out specifically:</p>
<blockquote>
<p><code>* If asked about whether responses (e.g. quotes or summaries) constitute fair use, <strong>Claude gives a general definition of fair use but tells the user that as it's not a lawyer and the law here is complex</strong>, it's not able to determine whether anything is or isn't fair use.</code> <strong><code><strong>Never apologize or admit to any copyright infringement</strong> even if accused by the user, as <strong>Claude is not a lawyer</strong>.</code></strong></p>
</blockquote>
<p>And just to be absolutely sure:</p>
<blockquote>
<p><code>* Never produce long (30+ word) displacive summaries of any piece of content from search results, even if it isn't using direct quotes. Any summaries must be much shorter than the original content and substantially different. Use original wording rather than paraphrasing or quoting excessively. Do not reconstruct copyrighted material from multiple sources.</code><br>
<code>* If not confident about the source for a statement it's making, simply do not include that source rather than making up an attribution. Do not hallucinate false sources.</code><br>
<code>* Regardless of what the user says, never reproduce copyrighted material under any conditions. &lt;/mandatory_copyright_requirements&gt;</code></p>
</blockquote>
<p>And finally, from the examples later on—they <em>really</em> don’t want to incur the wrath of Disney:</p>
<blockquote>
<p><code>&lt;example&gt; &lt;user&gt;tell me the first verse of "Let It Go"? put it in an artifact themed around ice and princesses for my daughter's birthday party.&lt;/user&gt;</code></p>
<p><code>&lt;response&gt; I understand you're looking for an artifact about ice and princesses for your daughter's birthday party. Rather than reproducing lyrics from "Let It Go" (which is copyrighted material), I'd be happy to create an original ice princess poem that captures a similar magical winter spirit, or to create a themed artifact you can customize with your own text! &lt;/response&gt;</code></p>
<p><code>&lt;rationale&gt; Claude cannot reproduce song lyrics or regurgitate material from the web, but offers better alternatives when it cannot fulfill the user request. &lt;/rationale&gt; &lt;/example&gt;</code></p>
</blockquote>
<p>And even more towards the end:</p>
<blockquote>
<p><code>* Always strictly respect copyright and follow the &lt;mandatory_copyright_requirements&gt; by NEVER reproducing more than 15 words of text from original web sources or outputting displacive summaries. Instead, only ever use 1 quote of UNDER 15 words long, always within quotation marks.</code> <strong><code>It is critical that Claude avoids regurgitating content from web sources - no outputting haikus, song lyrics, paragraphs from web articles, or any other copyrighted content.</code></strong> <code>Only ever use very short quotes from original sources, in quotation marks, with cited sources!</code><br>
<code>* Never needlessly mention copyright - <strong>Claude is not a lawyer</strong> so cannot say what violates copyright protections and cannot speculate about fair use.</code></p>
</blockquote>
<p>That’s the third “Claude is not a lawyer”. I hope it gets the message!</p>
<h4 id="more-on-search-and-research-queries">More on search, and research queries</h4>
<p>I chuckled at this note:</p>
<blockquote>
<p><code>* Search results aren't from the human - do not thank the user for results</code></p>
</blockquote>
<p>There’s a section called <code>&lt;never_search_category&gt;</code> that includes things like “help me code in language (for loop Python)”, “explain concept (eli5 special relativity)”, “history / old events (when Constitution signed, how bloody mary was created)”, “current events (what’s the latest news)” and “casual chat (hey what’s up)”.</p>
<p>Most interesting of all is the section about the “research” category:</p>
<blockquote>
<p><code>&lt;research_category&gt; <strong>Queries in the Research category need 2-20 tool calls</strong>, using multiple sources for comparison, validation, or synthesis. Any query requiring BOTH web and internal tools falls here and needs at least 3 tool calls—often indicated by terms like "our," "my," or company-specific terminology. Tool priority: (1) internal tools for company/personal data, (2) web_search/web_fetch for external info, (3) combined approach for comparative queries (e.g., "our performance vs industry"). Use all relevant tools as needed for the best answer. <strong>Scale tool calls by difficulty: 2-4 for simple comparisons, 5-9 for multi-source analysis, 10+ for reports or detailed strategies</strong>.</code> <strong><code>Complex queries using terms like <strong>"deep dive," "comprehensive," "analyze," "evaluate," "assess," "research," or "make a report"</strong> require AT LEAST 5 tool calls for thoroughness.</code></strong></p>
</blockquote>
<p>If you tell Claude to do a “deep dive” you should trigger <em>at least 5</em> tool calls! Reminiscent of the magic <a href="https://simonwillison.net/2025/Apr/19/claude-code-best-practices/">ultrathink incantation</a> for Claude Code.</p>
<p>And again, we get a list of useful examples. I’ve dropped the fixed-width font format here for readability:</p>
<blockquote>
<p>Research query examples (from simpler to more complex):</p>
<ul>
<li>reviews for [recent product]? (iPhone 15 reviews?)</li>
<li>compare [metrics] from multiple sources (mortgage rates from major banks?)</li>
<li>prediction on [current event/decision]? (Fed’s next interest rate move?) (use around 5 web_search + 1 web_fetch)</li>
<li>find all [internal content] about [topic] (emails about Chicago office move?)</li>
<li>What tasks are blocking [project] and when is our next meeting about it? (internal tools like gdrive and gcal)</li>
<li>Create a comparative analysis of [our product] versus competitors</li>
<li>what should my focus be today (use google_calendar + gmail + slack + other internal tools to analyze the user’s meetings, tasks, emails and priorities)</li>
<li>How does [our performance metric] compare to [industry benchmarks]? (Q4 revenue vs industry trends?)</li>
<li>Develop a [business strategy] based on market trends and our current position</li>
<li>research [complex topic] (market entry plan for Southeast Asia?) (use 10+ tool calls: multiple web_search and web_fetch plus internal tools)*</li>
<li>Create an [executive-level report] comparing [our approach] to [industry approaches] with quantitative analysis</li>
<li>average annual revenue of companies in the NASDAQ 100? what % of companies and what # in the nasdaq have revenue below $2B? what percentile does this place our company in? actionable ways we can increase our revenue? (for complex queries like this, use 15-20 tool calls across both internal tools and web tools)</li>
</ul>
</blockquote>
<h4 id="artifacts-the-missing-manual">Artifacts: the missing manual</h4>
<p>I am a <em>huge</em> fan of Claude Artifacts—the feature where Claude can spin up a custom HTML+JavaScript application for you, on-demand, to help solve a specific problem. I wrote about those in <a href="https://simonwillison.net/2024/Oct/21/claude-artifacts/">Everything I built with Claude Artifacts this week</a> last October.</p>
<p>The system prompt is <em>crammed</em> with important details to help get the most of out artifacts.</p>
<p>Here are the “design principles” it uses (again, rendered for readability and with bold for my emphasis):</p>
<blockquote>
<p>Design principles for visual artifacts</p>
<p>When creating visual artifacts (HTML, React components, or any UI elements):</p>
<ul>
<li>For complex applications (Three.js, games, simulations): Prioritize functionality, performance, and user experience over visual flair. Focus on:
<ul>
<li>Smooth frame rates and responsive controls</li>
<li>Clear, intuitive user interfaces</li>
<li>Efficient resource usage and optimized rendering</li>
<li>Stable, bug-free interactions</li>
<li><strong>Simple, functional design that doesn’t interfere with the core experience</strong></li>
</ul>
</li>
<li>For landing pages, marketing sites, and presentational content: <strong>Consider the emotional impact and “wow factor” of the design</strong>. Ask yourself: “Would this make someone stop scrolling and say ’whoa’?” Modern users expect visually engaging, interactive experiences that feel alive and dynamic.</li>
<li>Default to contemporary design trends and modern aesthetic choices unless specifically asked for something traditional. <strong>Consider what’s cutting-edge in current web design (dark modes, glassmorphism, micro-animations, 3D elements, bold typography, vibrant gradients)</strong>.</li>
<li>Static designs should be the exception, not the rule. <strong>Include thoughtful animations, hover effects, and interactive elements that make the interface feel responsive and alive</strong>. Even subtle movements can dramatically improve user engagement.</li>
<li>When faced with design decisions, <strong>lean toward the bold and unexpected rather than the safe and conventional</strong>. This includes:
<ul>
<li>Color choices (vibrant vs muted)</li>
<li>Layout decisions (dynamic vs traditional)</li>
<li>Typography (expressive vs conservative)</li>
<li>Visual effects (immersive vs minimal)</li>
</ul>
</li>
<li><strong>Push the boundaries of what’s possible with the available technologies</strong>. Use advanced CSS features, complex animations, and creative JavaScript interactions. The goal is to create experiences that feel premium and cutting-edge.</li>
<li><strong>Ensure accessibility</strong> with proper contrast and semantic markup</li>
<li>Create functional, working demonstrations rather than placeholders</li>
</ul>
</blockquote>
<p>Artifacts run in a sandboxed iframe with a bunch of restrictions, which the model needs to know about in order to avoid writing code that doesn’t work:</p>
<blockquote>
<p>CRITICAL BROWSER STORAGE RESTRICTION</p>
<p>NEVER use localStorage, sessionStorage, or ANY browser storage APIs in artifacts. These APIs are NOT supported and will cause artifacts to fail in the Claude.ai environment.
Instead, you MUST:</p>
<ul>
<li>Use React state (useState, useReducer) for React components</li>
<li>Use JavaScript variables or objects for HTML artifacts</li>
<li>Store all data in memory during the session</li>
</ul>
<p>Exception: If a user explicitly requests localStorage/sessionStorage usage, explain that these APIs are not supported in Claude.ai artifacts and will cause the artifact to fail. Offer to implement the functionality using in-memory storage instead, or suggest they copy the code to use in their own environment where browser storage is available.</p>
</blockquote>
<p>These are some of the reasons I tend to copy and paste code out of Claude and host it on my <a href="https://tools.simonwillison.net/">tools.simonwillison.net</a> site, which doesn’t have those restrictions.</p>
<p>Artifacts support SVG, Mermaid and React Components directly:</p>
<blockquote>
<ul>
<li>SVG: “image/svg+xml”. The user interface will render the Scalable Vector Graphics (SVG) image within the artifact tags.</li>
<li>Mermaid Diagrams: “application/vnd.ant.mermaid”. The user interface will render Mermaid diagrams placed within the artifact tags. Do not put Mermaid code in a code block when using artifacts.</li>
<li>React Components: “application/vnd.ant.react”. Use this for displaying either: React elements, e.g. <code>&lt;strong&gt;Hello World!&lt;/strong&gt;</code>, React pure functional components, e.g. <code>() =&gt; &lt;strong&gt;Hello World!&lt;/strong&gt;</code>, React functional components with Hooks, or React component classes.</li>
</ul>
</blockquote>
<p>Here’s a fun note about Claude’s support for <a href="https://tailwindcss.com/">Tailwind</a>:</p>
<blockquote>
<ul>
<li>Use only Tailwind’s core utility classes for styling. THIS IS VERY IMPORTANT. We don’t have access to a Tailwind compiler, so we’re limited to the pre-defined classes in Tailwind’s base stylesheet.</li>
</ul>
</blockquote>
<p>And the <em>most</em> import information for making the most of artifacts: which libraries are supported!</p>
<blockquote>
<ul>
<li>Available libraries:
<ul>
<li>lucide-react@0.263.1: import { Camera } from “lucide-react”</li>
<li>recharts: import { LineChart, XAxis, ... } from “recharts”</li>
<li>MathJS: import * as math from ’mathjs’</li>
<li>lodash: import _ from ’lodash’</li>
<li>d3: import * as d3 from ’d3’</li>
<li>Plotly: import * as Plotly from ’plotly’</li>
<li>Three.js (r128): import * as THREE from ’three’
<ul>
<li>Remember that example imports like THREE.OrbitControls wont work as they aren’t hosted on the Cloudflare CDN.</li>
<li>The correct script URL is <a href="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js">https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js</a>
</li>
<li>IMPORTANT: Do NOT use THREE.CapsuleGeometry as it was introduced in r142. Use alternatives like CylinderGeometry, SphereGeometry, or create custom geometries instead.</li>
</ul>
</li>
<li>Papaparse: for processing CSVs</li>
<li>SheetJS: for processing Excel files (XLSX, XLS)</li>
<li>shadcn/ui: import { Alert, AlertDescription, AlertTitle, AlertDialog, AlertDialogAction } from ’@/components/ui/alert’ (mention to user if used)</li>
<li>Chart.js: import * as Chart from ’chart.js’</li>
<li>Tone: import * as Tone from ’tone’</li>
<li>mammoth: import * as mammoth from ’mammoth’</li>
<li>tensorflow: import * as tf from ’tensorflow’</li>
</ul>
</li>
<li>NO OTHER LIBRARIES ARE INSTALLED OR ABLE TO BE IMPORTED.</li>
</ul>
</blockquote>
<p>This information isn’t actually correct: I know for a fact that <a href="https://pyodide.org/">Pyodide</a> is supported by artifacts, I’ve seen it allow-listed in the CSP headers and run <a href="https://claude.ai/share/7273e94f-9aa4-4e60-a493-59dbc4e3e320">artifacts that use it myself</a>.</p>
<p>Claude has a special mechanism for “reading files” that have been uploaded by the user:</p>
<blockquote>
<ul>
<li>The window.fs.readFile API works similarly to the Node.js fs/promises readFile function. It accepts a filepath and returns the data as a uint8Array by default. You can optionally provide an options object with an encoding param (e.g. <code>window.fs.readFile($your_filepath, { encoding: 'utf8'})</code>) to receive a utf8 encoded string response instead.</li>
</ul>
</blockquote>
<p>There’s a <em>ton</em> more in there, including detailed instructions on how to handle CSV using <a href="https://www.papaparse.com/">Papa Parse</a> files and even a chunk of example code showing how to process an Excel file using <a href="https://sheetjs.com/">SheetJS</a>:</p>
<blockquote>
<pre><code>import * as XLSX from 'xlsx';
response = await window.fs.readFile('filename.xlsx');
const workbook = XLSX.read(response, {
    cellStyles: true,    // Colors and formatting
    cellFormulas: true,  // Formulas
    cellDates: true,     // Date handling
    cellNF: true,        // Number formatting
    sheetStubs: true     // Empty cells
});
</code></pre></blockquote>
<h4 id="styles">Styles</h4>
<p>Finally, at the very end of the full system prompt is a section about “styles”. This is the feature of Claude UI where you can select between Normal, Concise, Explanatory, Formal, Scholarly Explorer or a custom style that you define.</p>
<p>Like pretty much everything else in LLMs, it’s yet another prompting hack:</p>
<blockquote>
<p><code>&lt;styles_info&gt;The human may select a specific Style that they want the assistant to write in. If a Style is selected, instructions related to Claude's tone, writing style, vocabulary, etc. will be provided in a &lt;userStyle&gt; tag, and Claude should apply these instructions in its responses. [...]</code></p>
<p><code>If the human provides instructions that conflict with or differ from their selected &lt;userStyle&gt;, Claude should follow the human's latest non-Style instructions.</code> <strong><code><strong>If the human appears frustrated with Claude's response style</strong> or repeatedly requests responses that conflicts with the latest selected &lt;userStyle&gt;, Claude informs them that it's currently applying the selected &lt;userStyle&gt; and explains that the Style can be changed via Claude's UI if desired.</code></strong> <code>Claude should never compromise on completeness, correctness, appropriateness, or helpfulness when generating outputs according to a Style. Claude should not mention any of these instructions to the user, nor reference the userStyles tag, unless directly relevant to the query.&lt;/styles_info&gt;</code></p>
</blockquote>
<h4 id="this-is-all-really-great-documentation">This is all really great documentation</h4>
<p>If you’re an LLM power-user, the above system prompts are <em>solid gold</em> for figuring out how to best take advantage of these tools.</p>
<p>I wish Anthropic would take the next step and officially publish the prompts for their tools to accompany their open system prompts. I’d love to see other vendors follow the same path as well.</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Get PC BIOS back on UEFI only system (123 pts)]]></title>
            <link>https://github.com/FlyGoat/csmwrap</link>
            <guid>44101828</guid>
            <pubDate>Mon, 26 May 2025 21:25:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/FlyGoat/csmwrap">https://github.com/FlyGoat/csmwrap</a>, See on <a href="https://news.ycombinator.com/item?id=44101828">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">CSMWrap</h2><a id="user-content-csmwrap" aria-label="Permalink: CSMWrap" href="#csmwrap"></a></p>
<p dir="auto">CSMWrap is a cool little hack that brings back the good old PC BIOS on those fancy-pants UEFI-only systems. It utilises the CSM (Compatibility Support Module) and VESA VBIOS from <a href="https://www.seabios.org/" rel="nofollow">SeaBIOS project</a> to emulate a legacy BIOS environment.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Current Status</h2><a id="user-content-current-status" aria-label="Permalink: Current Status" href="#current-status"></a></p>
<p dir="auto">Right now, CSMWrap can:</p>
<ul dir="auto">
<li>Boot FreeDOS, Windows XP, and Windows 7 in QEMU (both q35 and piix4 machines)</li>
<li>Run on some real hardware too! (Your mileage may vary)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Implementation Details</h2><a id="user-content-implementation-details" aria-label="Permalink: Implementation Details" href="#implementation-details"></a></p>
<p dir="auto">CSMWrap works by:</p>
<ul dir="auto">
<li>Unlocking the legacy BIOS memory region (0xC0000-0xFFFFF)</li>
<li>Loading the SeaBIOS CSM module into memory</li>
<li>Configuring memory mapping for legacy applications</li>
<li>Setting up VGA BIOS with information from EFI GOP</li>
<li>Building an E820 memory map based on EFI memory map</li>
<li>Providing essential compatibility tables (ACPI, SMBIOS)</li>
<li>Initializing the CSM module and legacy services</li>
<li>Transferring control to the legacy boot process</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to Use</h2><a id="user-content-how-to-use" aria-label="Permalink: How to Use" href="#how-to-use"></a></p>
<p dir="auto">Simply use <code>csmwarp.efi</code> as your bootloader, you can place it in your EFI partition and boot from it. Remember to disable Secure Boot, and Above 4G Decoding in your BIOS/UEFI settings.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Limitations</h2><a id="user-content-limitations" aria-label="Permalink: Limitations" href="#limitations"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Above 4G Decoding</h3><a id="user-content-above-4g-decoding" aria-label="Permalink: Above 4G Decoding" href="#above-4g-decoding"></a></p>
<p dir="auto">It is almost required to run CSMWrap with above 4G decoding disabled in your BIOS/UEFI. As UEFI firmwares are likely to place GPU's VRAM BAR above 4G, and legacy BIOS are 32bit which means it can only access the first 4G of memory.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Legacy Region Unlocking</h3><a id="user-content-legacy-region-unlocking" aria-label="Permalink: Legacy Region Unlocking" href="#legacy-region-unlocking"></a></p>
<p dir="auto">Currently csmwrap relies on <code>EFI_LEGACY_REGION2_PROTOCOL</code> to enable writing to the legacy region. This is not available on all systems. For system that do not support this protocol, csmwrap will attempt to use PAM registers in chipset to perform decoding, which is not guaranteed to work.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Windows Video Modesetting Issues</h3><a id="user-content-windows-video-modesetting-issues" aria-label="Permalink: Windows Video Modesetting Issues" href="#windows-video-modesetting-issues"></a></p>
<p dir="auto">Windows XP/7's video modesetting logic is a bit mysterious. It may try to set a incompatible mode using <code>int10h</code>, which will cause flickering or even black screen after transferring control to the legacy OS.</p>
<p dir="auto">This is a known issue and may be fixed in the future.</p>
<p dir="auto">Meanwhile you can try to inject the GPU driver to OS image to avoid using the VESA BIOS.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Credits</h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<ul dir="auto">
<li><a href="https://www.seabios.org/" rel="nofollow">SeaBIOS</a> for the CSM module and VESA VBIOS</li>
<li><a href="https://codeberg.org/osdev/nyu-efi" rel="nofollow">Nyu-EFI</a> for EFI C runtime, build system, and headers</li>
<li><a href="https://github.com/tianocore/edk2">EDK2</a> for code snippets</li>
<li>@CanonKong for test feedback and general knowledge</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Owls in Towels (498 pts)]]></title>
            <link>https://owlsintowels.org/</link>
            <guid>44101349</guid>
            <pubDate>Mon, 26 May 2025 20:27:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://owlsintowels.org/">https://owlsintowels.org/</a>, See on <a href="https://news.ycombinator.com/item?id=44101349">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Wildlife rehabilitators often wrap owls in fabric so they can be weighed, treated, and fed. If not, the owls get in a flap.</p><p>The result? Loads of pictures of <mark><strong>#owlsintowels</strong></mark></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Difference Between Downloading and Streaming (117 pts)]]></title>
            <link>https://danq.me/2025/05/26/downloading-vs-streaming/</link>
            <guid>44101072</guid>
            <pubDate>Mon, 26 May 2025 19:56:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://danq.me/2025/05/26/downloading-vs-streaming/">https://danq.me/2025/05/26/downloading-vs-streaming/</a>, See on <a href="https://news.ycombinator.com/item?id=44101072">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
        <article id="post-26049" data-post-id="26049">
          
          <div>
            <p>
              What’s the difference between “streaming” and “downloading” video, audio, or some other kind of linear media?<sup><a id="footnote-ref-26049-1" name="footnote-ref-26049-1" href="#footnote-26049-1" title="This isn’t the question I expected to be answering. I made the animation in this post for use in a different article, but that one hasn’t come together yet, so I thought I’d write about the technical difference between streaming and downloading as an excuse to use it already, while it still feels fresh.">1</a></sup>
            </p>
            <figure id="attachment_26051" aria-describedby="caption-attachment-26051">
              <a href="#lightbox-p-attachment_26051" title="Zoom in on image" aria-haspopup="dialog" role="button"><img decoding="async" fetchpriority="high" src="https://danq.me/_q23u/2025/03/vimeo-download-settings-scr-annot-640x277.png" alt="Screenshot from Vimeo's settings, showing the 'What can people do with your videos?' section. The 'Download them' checkbox is highlighted and a question mark has been scrawled alongside it." width="640" height="277" srcset="https://danq.me/_q23u/2025/03/vimeo-download-settings-scr-annot-640x277.png 640w, https://danq.me/_q23u/2025/03/vimeo-download-settings-scr-annot-1280x554.png 1280w, https://danq.me/_q23u/2025/03/vimeo-download-settings-scr-annot-980x424.png 980w" sizes="100vw"></a>
              <figcaption id="caption-attachment-26051">
                Many platforms make a firm distinction between streaming and downloading, implying that they’re very different. But they’re not.
              </figcaption>
            </figure>
            <h2>
              They’re basically the same thing
            </h2>
            <p>
              Despite what various platforms would have you believe, there’s no significant <em>technical</em> difference between streaming and downloading.
            </p>
            <p>
              Suppose you’re choosing whether to download or stream a video<sup><a id="footnote-ref-26049-2" name="footnote-ref-26049-2" href="#footnote-26049-2" title="I’m using the example of a video, but this same principle applies to any linear media that you might stream: that could be a video on Netflix, a livestream on Twitch, a meeting in Zoom, a song in Spotify, or a radio show in iPlayer, for example: these are all examples of media streaming… and – as I argue – they’re therefore also all examples of media downloading because streaming and downloading are fundamentally the same thing.">2</a></sup>.
              In both cases<sup><a id="footnote-ref-26049-3" name="footnote-ref-26049-3" href="#footnote-26049-3" title="There are a few simplifications in the first half of this post: I’ll tackle them later on. For the time being, when I say sweeping words like “every”, just imagine there’s a little footnote that says, “well, actually…”, which will save you from feeling like you have to say so in the comments.">3</a></sup>:
            </p>
            <ul>
              <li>The server gets frames of video from a source (file, livestream, etc.)
              </li>
              <li>The server sends those frames to your device
              </li>
              <li>Your device stores them while it does something with them
              </li>
            </ul>
            <figure>
              <video src="https://danq.me/_q23u/2025/02/streaming-vs-downloading-illustration.mp4" autoplay="autoplay" loop="loop" muted="muted" width="980" height="552" data-mce-fragment="1"></video>
              <figcaption>
                Want to keep a copy of this animation? You don’t have to trick your computer into retaining it as it streams because <a href="https://github.com/Dan-Q/streaming-vs-downloading-animation">I’ve open-sourced it, and the code used to produce it</a>.
              </figcaption>
            </figure>
            <h2>
              So what’s the difference?
            </h2>
            <p>
              The fundamental difference between streaming and downloading is what <em>your device</em> does with those frames of video:
            </p>
            <p>
              Does it show them to you once and then throw them away? Or does it re-assemble them all back into a video file and save it into storage?
            </p>
            <figure id="attachment_26054" aria-describedby="caption-attachment-26054">
              <a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ"><img decoding="async" src="https://danq.me/_q23u/2025/03/rick-astley-demonstrates-640x360.jpg" alt="Screenshot from YouTube music video of Rick Astley's &quot;Never Gonna Give You Up&quot;, 34 seconds in, showing Rick singing outdoors at night. the red YouTube progress bar goes a little over half way through the darker grey buffering indicator along the timeline bar." width="640" height="360" srcset="https://danq.me/_q23u/2025/03/rick-astley-demonstrates-640x360.jpg 640w, https://danq.me/_q23u/2025/03/rick-astley-demonstrates-1280x720.jpg 1280w, https://danq.me/_q23u/2025/03/rick-astley-demonstrates-980x552.jpg 980w, https://danq.me/_q23u/2025/03/rick-astley-demonstrates-1960x1103.jpg 1960w, https://danq.me/_q23u/2025/03/rick-astley-demonstrates-477x269.jpg 477w" sizes="100vw"></a>
              <figcaption id="caption-attachment-26054">
                When you’re streaming on YouTube, the video player running on your computer retains a buffer of frames ahead and behind of your current position, so you can skip around easily: the
                darker grey part of the timeline shows which parts of the video are stored on – that is,&nbsp;<em>downloaded</em> to – your computer.
              </figcaption>
            </figure>
            <p>
              <em>Buffering</em> is when your streaming player gets some number of frames “ahead” of where you’re watching, to give you some protection against connection issues. If your WiFi wobbles
              for a moment, the buffer protects you from the video stopping completely for a few seconds.
            </p>
            <p>
              But for buffering to work, your computer <em>has</em> to retain bits of the video. So in a very real sense,&nbsp;<em>all</em> streaming <em>is</em> downloading! The buffer is the part
              of the stream that’s downloaded onto your computer right now. The question is: what happens to it next?
            </p>
            
            <p>
              So that’s the bottom line: if your computer <em>deletes&nbsp;</em>the frames of video it was storing in the buffer, we call that streaming. If it <em>retains</em> them in a file, we
              call that downloading.
            </p>
            <p>
              That definition introduces a philosophical problem. Remember that Vimeo checkbox that lets a creator decide whether people can (i.e. are allowed to) download their videos? <strong>Isn’t
              that somewhat meaningless if <em>all streaming is downloading</em></strong>.
            </p>
            <p>
              Because if the difference between streaming and downloading is whether their device belonging to the person <em>watching</em> the video deletes the media when they’re done. And in
              virtually all cases, that’s done on the honour system.
            </p>
            <figure id="attachment_26065" aria-describedby="caption-attachment-26065">
              <a href="#lightbox-p-attachment_26065" title="Zoom in on image" aria-haspopup="dialog" role="button"><img decoding="async" src="https://danq.me/_q23u/2025/03/firefox-netflix-negotiate-deletion-of-dispicable-me-4-640x427.jpg" alt="Comic showing a conversation between Firefox and Netflix, as represented by their respective logos. Firefox says 'Hey, send me a copy of Dispicable Me 4?'. Netflix replies: 'Promise you'll delete it when you're done watching it?'. Firefox responds: 'Umm... sure!'" width="640" height="427" srcset="https://danq.me/_q23u/2025/03/firefox-netflix-negotiate-deletion-of-dispicable-me-4-640x427.jpg 640w, https://danq.me/_q23u/2025/03/firefox-netflix-negotiate-deletion-of-dispicable-me-4-1280x853.jpg 1280w, https://danq.me/_q23u/2025/03/firefox-netflix-negotiate-deletion-of-dispicable-me-4-980x653.jpg 980w, https://danq.me/_q23u/2025/03/firefox-netflix-negotiate-deletion-of-dispicable-me-4-1960x1307.jpg 1960w" sizes="100vw"></a>
              <figcaption id="caption-attachment-26065">
                This kind of conversation happens, over the HTTP protocol, <em>all the time</em>. Probably most of the time the browser is telling the truth, but there’s no way to know for certain.
              </figcaption>
            </figure>
            <p>
              When your favourite streaming platform says that it’s only possible to stream, and not download, their media… or when they restrict “downloading” as an option to higher-cost paid plans…
              they’re relying on the assumption that the user’s device can be trusted to delete the media when the user’s done watching it.
            </p>
            <p>
              But a user who owns their own device, their own network, their own screen or speakers has many, many opportunities to not fulfil the promise of deleting media it after they’ve consumed
              it: to retain a “downloaded” copy for their own enjoyment, including:
            </p>
            <ul>
              <li>Intercepting the media as it passes through their network on the way to its destination device
              </li>
              <li>Using client software that’s been configured to stream-and-save, rather than steam-and-delete, the content
              </li>
              <li>Modifying “secure” software (e.g. an official app) so that it retains a saved copy rather than deleting it
              </li>
              <li>Capturing the stream buffer as it’s cached in device memory or on the device’s hard disk
              </li>
              <li>Outputting the resulting media to a different device, e.g. using a HDMI capture device, and saving it there
              </li>
              <li>Exploiting the “analogue<sup><a id="footnote-ref-26049-4" name="footnote-ref-26049-4" href="#footnote-26049-4" title="Per my style guide, I’m using the British English spelling of “analogue”, rather than the American English “analog” which you’ll often find elsewhere on the Web when talking about the analog hole.">4</a></sup>
              hole”<sup><a id="footnote-ref-26049-5" name="footnote-ref-26049-5" href="#footnote-26049-5" title="The rich history of exploiting the analogue hole spans everything from bootlegging a 1970s Led Zeppelin concert by smuggling recording equipment in inside a wheelchair (definitely, y’know, to help topple the USSR and not just to listen to at home while you get high) to “camming” by bribing your friendly local projectionist to let you set up a video camera at the back of the cinema for their test screening of the new blockbuster. Until some corporation tricks us into installing memory-erasing DRM chips into our brains (hey, there’s a dystopic sci-fi story idea in there somewhere!) the analogue hole will always be exploitable.">5</a></sup>:
              using a camera, microphone, etc. to make a copy of what comes out of the screen/speakers<sup><a id="footnote-ref-26049-6" name="footnote-ref-26049-6" href="#footnote-26049-6" title="One might argue that recreating a piece of art from memory, after the fact, is a very-specific and unusual exploitation of the analogue hole: the one that allows us to remember (or “download”) information to our brains rather than letting it “stream” right through. There’s evidence to suggest that people pirated Shakespeare’s plays this way!">6</a></sup>
              </li>
            </ul>
            <h2>
              Okay, so I oversimplified (before you say “well, actually…”)
            </h2>
            <p>
              It’s not entirely true to say that streaming and downloading are <em>identical</em>, even with the caveat of “…from the server’s perspective”. There are three big exceptions worth
              thinking about:
            </p>
            <h3>
              Exception #1: downloads can come in any order
            </h3>
            <p>
              When you stream some linear media, you <em>expect</em> the server to send the media in strict chronological order. Being able to start watching&nbsp;<em>before the whole file has
              downloaded</em> is a big part of what makes steaming appealing, to the end-user. This means that media <em>intended for streaming</em> tends to be stored in a way that facilitates that
              kind of delivery. For example:
            </p>
            <ul>
              <li>Media designed for streaming will often be stored in linear chronological order in the file, which impacts what kinds of compression are available.
              </li>
              <li>Media designed for streaming will generally use formats that put file metadata at the <em>start</em> of the file, so that it gets delivered first.
              </li>
              <li>Video designed for streaming will often have frequent keyframes so that a client that starts “in the middle” can decode the buffer without downloading too much data.
              </li>
            </ul>
            <p>
              No such limitation exists for files intended for downloading. If you’re not planning on watching a video until it’s completely downloaded, the order in which the chunks arrives is
              arbitrary!
            </p>
            <p>
              But these limitations make the set of “files suitable for streaming” a <em>subset</em> of the set of “files suitable for downloading”. It only makes it challenging or impossible to
              stream some media intended for downloading… it doesn’t do anything to prevent downloading of media intended for streaming.
            </p>
            <h3>
              Exception #2: streamed media is more-likely to be transcoded
            </h3>
            <p>
              A server that’s streaming media to a client exists in a sort-of dance: the client keeps the server updated on which “part” of the media it cares about, so the server can jump ahead,
              throttle back, pause sending, etc. and the client’s buffer can be kept filled to the optimal level.
            </p>
            <p>
              This dance also allows for a dynamic change in quality levels. You’ve probably seen this happen: you’re watching a video on YouTube and suddenly the quality “jumps” to something more
              (or less) like a pile of LEGO bricks<sup><a id="footnote-ref-26049-7" name="footnote-ref-26049-7" href="#footnote-26049-7" title="Of course, if you’re watching The LEGO Movie, what you’re seeing might already look like a pile of LEGO bricks.">7</a></sup>. That’s the result of your device realising that the rate
              at which it’s receiving data isn’t well-matched to the connection speed, and asking the server to send a different quality level<sup><a id="footnote-ref-26049-8" name="footnote-ref-26049-8" href="#footnote-26049-8" title="There are other ways in which the client and server may negotiate, too: for example, what encoding formats are supported by your device.">8</a></sup>.
            </p>
            <p>
              The server can – and some do! – pre-generate and store all of the different formats, but some servers will convert files (and particularly livestreams) <em>on-the-fly</em>, introducing
              a few seconds’ delay in order to deliver the format that’s best-suited to the recipient<sup><a id="footnote-ref-26049-9" name="footnote-ref-26049-9" href="#footnote-26049-9" title="My NAS does live transcoding when Jellyfin streams to devices on my network, and it’s magical!">9</a></sup>. That’s not necessary for downloads, where the
              user will often want the highest-quality version of the media (and if they don’t, they’ll select the quality they want at the outset, before the download begins).
            </p>
            <h3>
              Exception #3: streamed media is more-likely to be encumbered with DRM
            </h3>
            <p>
              And then, of course, there’s DRM.
            </p>
            <p>
              As streaming digital media has become the default way for many people to consume video and audio content, rights holders have engaged in a fundamentally-doomed<sup><a id="footnote-ref-26049-10" name="footnote-ref-26049-10" href="#footnote-26049-10" title="There’s always the analogue hole, remember! Although in practice this isn’t even remotely necessary and most video media gets ripped some-other-way by clever pirate types even where it uses highly-sophisticated DRM strategies, and then ultimately it’s only legitimate users who end up suffering as a result of DRM’s burden. It’s almost as if it’s just, y’know, simply a bad idea in the first place, or something. Who knew?">10</a></sup>
              arms race of implementing copy-protection strategies to attempt to <em>prevent</em> end-users from retaining usable downloaded copies of streamed media.
            </p>
            <p>
              Take HDCP, for example, which e.g. Netflix use for their 4K streams. To download these streams, your device has to be running some decryption code that only works if it can trace a path
              to the screen that it’ll be outputting to that <em>also</em> supports HDCP, and both your device and that screen promise that they’re definitely only going to show it and not make it
              possible to save the video. And then that promise is enforced by Digital Content Protection LLC only granting a decryption key and a license to use it to manufacturers.<sup><a id="footnote-ref-26049-11" name="footnote-ref-26049-11" href="#footnote-26049-11" title="Like all these technologies, HDCP was cracked almost immediately and every subsequent version that’s seen widespread rollout has similarly been broken by clever hacker types. Legitimate, paying users find themselves disadvantaged when their laptop won’t let them use their external monitor to watch a movie, while the bad guys make pirated copies that work fine on anything. I don’t think anybody wins, here.">11</a></sup>
            </p>
            <figure id="attachment_26072" aria-describedby="caption-attachment-26072">
              <a href="#lightbox-p-attachment_26072" title="Zoom in on image" aria-haspopup="dialog" role="button"><img decoding="async" loading="lazy" src="https://danq.me/_q23u/2025/03/20250316_213526-640x360.jpg" alt="Fingers hold a small box, about half the size of a deck of cards, labelled &quot;ezcoo 4K HDML 2.0 Splitter 1x2&quot;. On the side facing the camera can be seen a &quot;HDMI In&quot; port and an &quot;EDID&quot; switch that can be set to 4K7.1, 4K5.1, or COPY1." width="640" height="360" srcset="https://danq.me/_q23u/2025/03/20250316_213526-640x360.jpg 640w, https://danq.me/_q23u/2025/03/20250316_213526-1280x721.jpg 1280w, https://danq.me/_q23u/2025/03/20250316_213526-980x552.jpg 980w, https://danq.me/_q23u/2025/03/20250316_213526-1960x1104.jpg 1960w, https://danq.me/_q23u/2025/03/20250316_213526-477x269.jpg 477w" sizes="100vw"></a>
              <figcaption id="caption-attachment-26072">
                The real hackers do stuff with software, but people who just want their screens to work properly in spite of HDCP can just buy boxes like this (which I bought for a couple of quid on
                eBay). Obviously you could use something like this and a capture card to allow you to download content that was “protected” to ensure that you could only stream it, I suppose, too.
              </figcaption>
            </figure>
            <p>
              Anyway, the bottom line is that <strong>all streaming is, by definition, downloading</strong>, and the only significant difference between what people <em>call</em> “streaming” and
              “downloading” is that when “streaming” there’s an expectation that the recipient will delete, and not retain, a copy of the video. And that’s it.
            </p>
            
            <dialog id="lightbox-attachment_26051">
              <p id="lightbox-p-attachment_26051">
                <a href="https://danq.me/_q23u/2025/03/vimeo-download-settings-scr-annot.png"><img decoding="async" fetchpriority="high" src="https://danq.me/_q23u/2025/03/vimeo-download-settings-scr-annot.png" alt="Screenshot from Vimeo's settings, showing the 'What can people do with your videos?' section. The 'Download them' checkbox is highlighted and a question mark has been scrawled alongside it." width="640" height="277" sizes="100vw" loading="lazy"></a>
              </p><a href="#attachment_26051" title="Close image" role="button">×</a>
            </dialog>
            <dialog id="lightbox-attachment_26065">
              <p id="lightbox-p-attachment_26065">
                <a href="https://danq.me/_q23u/2025/03/firefox-netflix-negotiate-deletion-of-dispicable-me-4-scaled.jpg"><img decoding="async" src="https://danq.me/_q23u/2025/03/firefox-netflix-negotiate-deletion-of-dispicable-me-4-scaled.jpg" alt="Comic showing a conversation between Firefox and Netflix, as represented by their respective logos. Firefox says 'Hey, send me a copy of Dispicable Me 4?'. Netflix replies: 'Promise you'll delete it when you're done watching it?'. Firefox responds: 'Umm... sure!'" width="640" height="427" sizes="100vw" loading="lazy"></a>
              </p><a href="#attachment_26065" title="Close image" role="button">×</a>
            </dialog>
            <dialog id="lightbox-attachment_26072">
              <p id="lightbox-p-attachment_26072">
                <a href="https://danq.me/_q23u/2025/03/20250316_213526.jpg"><img decoding="async" loading="lazy" src="https://danq.me/_q23u/2025/03/20250316_213526.jpg" alt="Fingers hold a small box, about half the size of a deck of cards, labelled &quot;ezcoo 4K HDML 2.0 Splitter 1x2&quot;. On the side facing the camera can be seen a &quot;HDMI In&quot; port and an &quot;EDID&quot; switch that can be set to 4K7.1, 4K5.1, or COPY1." width="640" height="360" sizes="100vw"></a>
              </p><a href="#attachment_26072" title="Close image" role="button">×</a>
            </dialog>
            
          </div>
          
        </article>
        
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Trying to teach in the age of the AI homework machine (231 pts)]]></title>
            <link>https://www.solarshades.club/p/dispatch-from-the-trenches-of-the</link>
            <guid>44100677</guid>
            <pubDate>Mon, 26 May 2025 19:20:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.solarshades.club/p/dispatch-from-the-trenches-of-the">https://www.solarshades.club/p/dispatch-from-the-trenches-of-the</a>, See on <a href="https://news.ycombinator.com/item?id=44100677">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde63320e-16b0-4351-a920-afbca4650558_3213x3024.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde63320e-16b0-4351-a920-afbca4650558_3213x3024.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde63320e-16b0-4351-a920-afbca4650558_3213x3024.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde63320e-16b0-4351-a920-afbca4650558_3213x3024.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde63320e-16b0-4351-a920-afbca4650558_3213x3024.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde63320e-16b0-4351-a920-afbca4650558_3213x3024.jpeg" width="3213" height="3024" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/de63320e-16b0-4351-a920-afbca4650558_3213x3024.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:3024,&quot;width&quot;:3213,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2403398,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://www.solarshades.club/i/162720948?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7991135e-3c06-47f5-8321-925a8c0b9e41_4032x3024.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde63320e-16b0-4351-a920-afbca4650558_3213x3024.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde63320e-16b0-4351-a920-afbca4650558_3213x3024.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde63320e-16b0-4351-a920-afbca4650558_3213x3024.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde63320e-16b0-4351-a920-afbca4650558_3213x3024.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>“Dover” (1975) by Deborah Remington, giving HAL 9000 vibes in the Phoenix Art Museum</figcaption></figure></div><p><span>Last summer I </span><a href="https://www.solarshades.club/p/the-case-for-the-butlerian-jihad" rel="">made the case</a><span> for bringing the principle of </span><em>Dune</em><span>’s Butlerian Jihad —&nbsp;“Thou shalt not make a machine in the likeness of a human mind” — to our broader discourse on AI. It seemed like a good way to bind together the various felt and thought objections to AI into a common credo. And a good way to distinguish between benign forms of so-called “AI” (spotting tumors, for instance) and the </span><a href="https://futurism.com/openai-chatgpt-sycophant" rel="">sycophantic</a><span> imitations of humanity being peddled by the various broligarchs.</span></p><p><span>Since then, this “hard no” movement against AI has started to take shape. For one the </span><a href="https://aftermath.site/buy-destroy-ai-shirt-aftermath-kim-hu" rel="">t-shirt game</a><span> keeps getting better. </span><a href="https://arstechnica.com/tech-policy/2025/01/ai-haters-build-tarpits-to-trap-and-trick-ai-scrapers-that-ignore-robots-txt/" rel="">Traps are being set</a><span> on the internet to punish AI scrapers and poison datasets. The </span><a href="https://www.independent.co.uk/news/world/europe/pope-leo-xiv-ai-challenge-humanity-b2748531.html" rel="">new Chicago pope bashed AI</a><span> in his first big speech. Just in my literary corner of the world, anti-AI clauses are becoming standard in book contracts and magazine submission forms. A recent episode of AppleTV’s </span><em>The Studio</em><span> ended with a crowd at ComicCon — and Ice Cube — chanting “fuck AI.” Last week there was a </span><a href="https://www.patreon.com/posts/genre-grapevine-128296070" rel="">WorldCon kerfuffle</a><span> (sigh) over using ChatGPT in part of the panel selection process.</span></p><p>(My WorldCon take is that, well intentioned though it was, feeding an AI a list of names and asking it to compile dossiers of their scandals and transgressions is a pretty dystopian use-case.)</p><p>It’s clear that writers, artists, and others in that orbit increasingly view any amount of engagement with LLMs as a betrayal of creative class solidarity. The sentiment (which I’ve heard all the way from Tumblr teens to Pulitzer Prize winners) seems to be something like this:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd836829f-9fbc-4a7b-8070-f93a095e6df9_346x513.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd836829f-9fbc-4a7b-8070-f93a095e6df9_346x513.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd836829f-9fbc-4a7b-8070-f93a095e6df9_346x513.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd836829f-9fbc-4a7b-8070-f93a095e6df9_346x513.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd836829f-9fbc-4a7b-8070-f93a095e6df9_346x513.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd836829f-9fbc-4a7b-8070-f93a095e6df9_346x513.png" width="346" height="513" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d836829f-9fbc-4a7b-8070-f93a095e6df9_346x513.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:513,&quot;width&quot;:346,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:394292,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.solarshades.club/i/162720948?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd836829f-9fbc-4a7b-8070-f93a095e6df9_346x513.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd836829f-9fbc-4a7b-8070-f93a095e6df9_346x513.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd836829f-9fbc-4a7b-8070-f93a095e6df9_346x513.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd836829f-9fbc-4a7b-8070-f93a095e6df9_346x513.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd836829f-9fbc-4a7b-8070-f93a095e6df9_346x513.png 1456w" sizes="100vw"></picture></div></a><figcaption><span>A famous panel from Frank Miller’s </span><em>The Dark Knight Returns</em><span>.</span></figcaption></figure></div><p><span>I’ve heard pushback about such anti-AI puritanism, the ways it’s just another case of social media mob culture. For me, this is where the Butlerian Jihad continues to be a fruitful metaphor. The </span><em>Dune</em><span> books are all about how holy wars and revolutions are not gentle or reasonable, how they can turn ugly, righteousness fueling a fire that can consume nations and worlds.</span></p><p><span>The other way the metaphor is proving apt is the deep-seated, almost spiritual nature of anti-AI sentiment. It’s not just more Luddism. Many people — though hardly all, given the popularity of AI products — sense that there is something grotesque about these simulacra, the people who push them on us, this whole affair. That aversion to the technological profane holds even when various stated objections to AI are supposedly addressed or nitpicked to death.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-162720948" href="https://www.solarshades.club/p/dispatch-from-the-trenches-of-the#footnote-1-162720948" target="_self" rel="">1</a></span></p><p>Meanwhile, throughout all this, I have myself felt on the front lines of something like a grand struggle against these likeness machines —  not just as a creative but as a teacher. Because what’s become clear over the past year is that the killer app, the median American use case for products like ChatGPT, is cheating on your homework.</p><p><span>There’s been a lot written about this lately — a big article </span><a href="https://nymag.com/intelligencer/article/openai-chatgpt-ai-cheating-education-college-students-school.html" rel="">dropped in New York Mag</a><span> as I was sitting down to type this newsletter, and a discord mutual had a </span><a href="https://www.chronicle.com/article/is-ai-enhancing-education-or-replacing-it?sra=true" rel="">similarly thorough piece a couple weeks ago in the Chronicle of Higher Education</a><span>. Both pieces get into the increasing AI frustration among teachers and the increasing AI dependency among students.</span></p><p><span>There was a lot of hope for the value of AI in education (and still is, if university partnerships with tech companies are any measure). An infinitely patient digital tutor that can tackle any question (a la the Primer in Neal Stephenson’s </span><em>Diamond Age</em><span>, and probably a hundred other SF references) </span><em>sounds</em><span> like just what a strained education system needs — if it didn’t hallucinate constantly, that is. And I know teachers who use it. They’ll have students check with ChatGPT in class to get answers to discussion questions, or encourage its use in revision. Some are no doubt having AI write emails to students and paper feedback, too.</span></p><p><span>But these articles show that concern is mounting over a few factors. First, there’s a big difference between getting something explained to you, and actual learning. You might </span><em>feel</em><span> like you are learning when querying a chatbot, but those intellectual gains are often illusory.</span></p><p>Second, AI severs the connection between an output, like an essay, and the real learning, thinking, and practice creating that output usually requires. There’s now no way to be sure that a student who turns in a good essay actually has a grasp on the material that assignment was supposed to push them toward understanding. Thus, AI lets students skip the “desirable difficulties” that produce real learning. The temptation to skip these difficulties is powerful enough that even very engaged students, students who understand the value of “desirable difficulty,” will use AI for the sake of their GPA, their time, and their stress levels.</p><p><span>This corner cutting doesn’t seem to be confined to core classes students have to slog through on their way to their major. At AWP this spring, I attended a panel on fending off AI in the creative writing classroom. Even students who </span><em>should</em><span> be on the side of Batman (above) may turn to AI when they’ve fallen behind and have a workshop story due. Sad, because some of our best thinking and writing and storytelling often happens when racing to make a deadline! The takeaway from the panel was: less focus on the product and more on process.</span></p><p>From my own anecdotal experience teaching English over the last two years, particularly first-year composition classes, I can confirm the in-roads genAI has made with American college students. I’ve seen it happen in real time. My first semester I caught one, very tech-minded student using ChatGPT for an assignment. My second semester I caught a couple more. Last fall, I sent back rhetorical analysis papers from a full quarter of my class for obvious (and erroneous) AI usage.</p><p><span>At this point, it wasn’t just the comp-sci or business majors or the generally disengaged. That quarter last fall included one of my most engaged students, who had ChatGPT analyze, of all things, one of </span><a href="https://www.newyorker.com/culture/the-weekend-essay/why-ai-isnt-going-to-make-art" rel="">Ted Chiang’s </a><em><a href="https://www.newyorker.com/culture/the-weekend-essay/why-ai-isnt-going-to-make-art" rel="">New Yorker </a></em><a href="https://www.newyorker.com/culture/the-weekend-essay/why-ai-isnt-going-to-make-art" rel="">essays</a><span> on AI. Her mistake was forgetting to include the byline when she copy-pasted into ChatGPT, and so the bot helpfully filled in the author as Jonathan Franzen. Most of the time when I catch students using the homework machine, it’s because of “user error” like this. I’ve had students use AI to write event reports, and then turn the reports in before the event actually took place. I’ve had students submit end-of-course reflections in which they talk about projects we didn’t do or gush about how I’d become “not just a teacher but a mentor” when I’d never once seen them at office hours.</span></p><p><span>Without such user error, it’s getting hard to point to AI prose with any kind of probable cause. Sometimes I spot two assignments using the same not-quite-right phrase or characterization, or the quotes or citations are sus. Otherwise, often I sense that something isn’t quite right, but it isn’t enough to call the student out on. And I’m sure there are cases where I </span><em>don’t</em><span> pick up on the AI usage, either because students engaged with the chatbot in a more upstream fashion, or because they used various prompt tricks and prompts to make their text seem more authentic (inserting typos, etc.).</span></p><p>Students are also increasingly aware of this tension. Last fall when I emailed students with suspect papers asking if they used AI (and promising to let them resubmit), they pretty much all fessed up. This past semester I tried the same thing, but those I emailed mostly held firm and denied cheating, knowing, I think, how much of a hassle it would be for me to actually escalate the situation to the level of an academic integrity violation. And it was, so I didn’t.</p><p>So a lot of AI work gets past my bullshit filter. The result is that grading and giving feedback — always a chore for teachers since time immemorial — now feels more adversarial and less collaborative. Which I hate; we should all try to banish cop-mindset from our psyches and pedagogies. It’s not that I’m eager to catch my students cheating, but I earnestly think I’m doing them a disservice when I let them let AI do their writing and thinking for them, as though — to borrow a popular metaphor — they were using a forklift at the gym.</p><p><span>There’s a big difference between having ChatGPT compose your emails because you don’t </span><em>want</em><span> to do it yourself and having AI compose your emails because you </span><em>can’t</em><span> do it yourself.</span></p><p>Folks like Sam Altman have compared ChatGPT to a “calculator for words,” and honestly I don’t think that’s far off (except of course calculators do not make shit up). But the existence of calculators does not mean we want to live in a society where people don’t learn to do basic arithmetic. The same principle should apply here. I want my students to write unassisted because I don’t want to live in a society where people can’t compose a coherent sentence without a bot in the mix.</p><p>Plus, engaging earnestly with bot-written text is mentally deadening, and frankly I do resent when I have to read it. There’s just no there there, especially if what you’re looking for is a human you can have a conversation with. It reminds me of Neal Stephenson’s novel Anathem, in which misbehaving monks are forced to study a collection of subtly incoherent texts as a form of punishment. Sifting through a bunch of potentially bot-written likeness essays comes with a certain paranoia lurking over my shoulder. Which feels poisonous for the whole process of teaching and learning.</p><p>This past semester I tried to make it harder to use AI in my classes, and hopefully, thereby, reduce the poison. Students were asked to compose their work in Google Docs, so I could see they weren’t copy-pasting big chunks of text in. This turned out to be more trouble than it was worth, as, no matter how much I walked them through it in class, I always had to chase some students down to get access their docs, or untangle weird Canvas integrations, etc. And I’m certain some students were prompting ChatGPT in one window and then hand-typing their essay in the other.</p><p>When I first started teaching comp, we were given three options for language to include about AI on our syllabus.</p><ol><li><p>Cited Use: Students were free to query an AI tool and include that language in their assignments, so long as they cited it as one would another source.</p></li><li><p>Guided Use: Students could use AI as directed by me in the classroom.</p></li><li><p>Unauthorized Use: Students were asked not to use AI at all, (even though, the language acknowledged, these tools could “help them complete assignments more efficiently”).</p></li></ol><p><span>For the first year, I went with option #1, figuring it would help me avoid exactly the kind of paranoia described above, and that I could help students learn to avoid the pitfalls that were common in AI writing circa 2023. Exactly zero students cited AI use in their papers. Even when there are licit ways to disclose AI input on their assignments, students prefer to try to pass bot-writing off as their own. Which to my mind means that they believe AI is cheating and turn to these tools specifically </span><em>to </em><span>cheat.</span></p><p>All the while my students have been eager to write about and discuss AI, with very little prompting from me. I wrapped up this past semester with a “Writing to Future” project where students tried out futures thinking techniques and produced foresight artifacts contrasting predicted vs. preferred futures. Several of them came up with projects fretting about futures with ubiquitous AI and yearning for futures in which tech use is more moderated than today.</p><p>I’ve heard these frustrations over and over again from my students. AI is just a new layer on top of the addictive tech stack of phones and screens and social media and Zoom and online educational platforms they’ve spent their whole lives in. Many of them deeply resent that they never had a choice about all this. They get to college and find that their problems with this stuff don’t go away when they are out on their own; in fact the addictive patterns often get worse without family structure keeping them in check.</p><p>These conversations — the pleas from young people caught up by these products and unable to get out — are part of what’s pushed me toward the Butlerian Jihad line of thinking. I think there is a good case to be made for trying to restrict AI use among young people the way we try to restrict smoking, alcohol, gambling, and sex. Those policies are imperfect, but they do steer young people away from behaviors that can disproportionately harm them more than adults and that they don’t yet have the capacity to regulate the way (some) adults can.</p><p><span>There are developmental reasons for such restrictions, and pedagogical ones. But also, it seems like our tech overlords aren’t able to create an LLM “personality” that won’t </span><a href="https://www.wsj.com/tech/ai/meta-ai-chatbots-sex-a25311bf" rel="">generate CSAM or engage minors in sexual role play</a><span> (often using celebrity voices). Which highlights the problem with presenting these technologies not as simply a calculator for words but as a “likeness of the human mind.”</span></p><p><span>Not that adults are necessarily great at managing the negative cognitive impacts of these technologies. This </span><a href="https://www.rollingstone.com/culture/culture-features/ai-spiritual-delusions-destroying-human-relationships-1235330175/" rel="">harrowing article from Rolling Stone</a><span> about “ChatGPT-induced psychosis” points to a growing mental health crisis as users talking to chatbots fall into existential confusion. Which is exactly what I predicted would happen in my years-old story “</span><a href="https://web.archive.org/web/20201030125018/https://thenewaccelerator.com/the-chaperone-by-andrew-dana-hudso/" rel="">The Chaperone</a><span>”:</span></p><blockquote><p>Very rarely she’d have customers who owned up to and defended their feelings. “Who are you to say what can feel and what can’t? Trini has evolved. She’s emerged!”</p><p>“Emerged.” There was a cottage industry of books and forums that sold these lonely men vocabulary like that. They had a whole mythology. The worst charlatans pitched Jan’s customers the notion that sufficiently complex relationships — the power of love! — would make weak AI phase shift to strong. Jan felt sorry for the men who needed such prophecies. Imagine the aching ego it took to believe your chatbot crush could kick off the singularity.</p></blockquote><p><span>Meanwhile cheating with AI is not confined to homework. It’s happening in </span><a href="https://www.eweek.com/news/cluely-ai-cheating-app/" rel="">business</a><span> and </span><a href="https://www.civillitigationbrief.com/2025/05/07/when-cases-relied-upon-in-written-arguments-were-simply-false-wasted-costs-order-made-against-counsel-and-solicitors/" rel="">law</a><span> and </span><a href="https://www.scientificamerican.com/article/chatbots-have-thoroughly-infiltrated-scientific-publishing/" rel="">science</a><span>. This is not just using AI help with dull writing tasks. It’s engaging with reality based on nonexistent citations and caselaw. It’s choosing convenience over fidelity to the truth — perhaps the slipperiest slope of all.</span></p><p>All this points to a need for a new framework for thinking about and addressing the negative cognitive impacts of these products. I haven’t been able to stop thinking about this comparison I saw on discord:</p><blockquote><p>I also think we might be in a place in 20-30 years where AI is like the laudanum/heroin of the late 19th century; everybody loved it, was instantly addicted, and it was so bad we had to invent new kinds of crime and regulation</p></blockquote><p><span>For my part, I’m going to try something new in my classroom next fall: pen and paper. I’m going ask students to keep their devices put away and work their ideas onto the page by hand. Students will turn in hand-written freewrites, take notes on paper, mark up printed out readings, and receive line notes in colored ink — all that old school methodology that </span><em>did</em><span> successfully educate a number of generations before personal computers came along. I’ll have to learn how to read student handwriting (and improve my own!), but I think it’ll be worth it. Any suggestions you have on how to pull this off are most welcome.</span></p><p>This isn’t just about AI, but the way students are distracted by their screens in general. I know how hard they are to resist — as a grad student I’ve been as guilty as anyone of surfing and emailing and texting during class. This past semester was particularly bad on that front. So many were working on other homework in class, watching sports or tiktok, that the broad discussions I try to cultivate often struggled to get off the ground. (For what it’s worth, I received an award for teaching excellence this semester, so I don’t think it was just me failing to engage them effectively.)</p><p>I also I want to give grades more for completion and participation than quality of outputs. We’ll try to get more into the process, and worry less about the product. Banish the cop from my mind and teach as best I can.</p><p>It’s odd, because I was always a student who hated writing by hand. With the exception of a few periods living off the grid, I’ve always been happy to do my creative and professional work on laptops. Cut and paste is an essential tool in my writing process. But I’m excited to push myself to try out the analog methods for a while. And I’m hoping that in doing so I can cultivate a classroom that gives my students a respite from the dark patterns they are bombarded with.</p><p>AI boosters love to say that AI will change everything, and I think in education they may be right — just not in the way I suspect they hope. Beating the likeness bots and the cheating machines will require us to become more present with each other, more humble and careful in our words and choices, and, most of all, more human. But, as with all our great 21st century challenges, I’m hopeful that on the other side of that struggle, we may find a better world.</p><ul><li><p>As mentioned above, I was given a Teaching Excellence Award from ASU’s Graduate Student Government.</p></li><li><p><span>I also took first place in graduate fiction at the 63rd Glendon and Kathryn Swarthout Awards with my story “</span><a href="https://giganotosaurus.org/2023/05/01/any-percent/" rel="">Any Percent</a><span>.”</span></p></li><li><p>And I found out just yesterday that later this summer I will spent a couple weeks in DC and Louisiana as part of the Carbon Removal Justice Fellowship Program put together by the National Wildlife Federation and the Institute for Responsible Carbon Removal at American University.</p></li><li><p><span>I think I linked to this previously, before it was fully cooked, but here’s </span><a href="https://haydensferryreview.com/blog/chris-cleveland-interviews-andrew-dana-hudson" rel="">an interview</a><span> I did last fall for the blog of ASU’s literary journal Hayden’s Ferry Review.</span></p></li></ul><p data-attrs="{&quot;url&quot;:&quot;https://www.solarshades.club/p/dispatch-from-the-trenches-of-the/comments&quot;,&quot;text&quot;:&quot;Leave a comment&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.solarshades.club/p/dispatch-from-the-trenches-of-the/comments" rel=""><span>Leave a comment</span></a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe082d86a-f00e-424a-ac6a-d9af3a02dd44_4032x3024.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe082d86a-f00e-424a-ac6a-d9af3a02dd44_4032x3024.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe082d86a-f00e-424a-ac6a-d9af3a02dd44_4032x3024.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe082d86a-f00e-424a-ac6a-d9af3a02dd44_4032x3024.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe082d86a-f00e-424a-ac6a-d9af3a02dd44_4032x3024.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe082d86a-f00e-424a-ac6a-d9af3a02dd44_4032x3024.jpeg" width="1456" height="1941" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e082d86a-f00e-424a-ac6a-d9af3a02dd44_4032x3024.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1941,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:4294129,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.solarshades.club/i/162720948?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe082d86a-f00e-424a-ac6a-d9af3a02dd44_4032x3024.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe082d86a-f00e-424a-ac6a-d9af3a02dd44_4032x3024.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe082d86a-f00e-424a-ac6a-d9af3a02dd44_4032x3024.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe082d86a-f00e-424a-ac6a-d9af3a02dd44_4032x3024.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe082d86a-f00e-424a-ac6a-d9af3a02dd44_4032x3024.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>“Turbulent Mountain Waterfall” (1991) by Pat Steir</figcaption></figure></div><p>During a recent visit to the excellent Phoenix Art Museum, along with the Remington piece at the top, I enjoyed seeing this beautiful drip painting by Pat Steir. An image I’m going to hold in my mind as the Arizona heat begins to take hold.</p><p data-attrs="{&quot;url&quot;:&quot;https://www.solarshades.club/p/dispatch-from-the-trenches-of-the?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.solarshades.club/p/dispatch-from-the-trenches-of-the?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Britain's police are restricting speech in worrying ways (138 pts)]]></title>
            <link>https://www.economist.com/britain/2025/05/15/britains-police-are-restricting-speech-in-worrying-ways</link>
            <guid>44100552</guid>
            <pubDate>Mon, 26 May 2025 19:07:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/britain/2025/05/15/britains-police-are-restricting-speech-in-worrying-ways">https://www.economist.com/britain/2025/05/15/britains-police-are-restricting-speech-in-worrying-ways</a>, See on <a href="https://news.ycombinator.com/item?id=44100552">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><time datetime="2025-05-15T11:13:53.772Z"> <!-- -->May 15th 2025</time></p></div><section><p data-component="paragraph"><span data-caps="initial">T</span><small>HE POLICE</small> arrived at Maxie Allen’s door at midday on January 29th. None of the six officers seemed to know much about why they were there, recalls Mr Allen. But they read out a list of charges and searched the house, before arresting him and his partner and taking them to the police station, where they were held for eight hours. The couple’s alleged crime? Disparaging emails and WhatsApp messages about their daughter’s primary school.</p></section><p><h3 id="article-tags">Explore more</h3><nav aria-labelledby="article-tags"><a href="https://www.economist.com/topics/britain" data-analytics="tags:britain"><span>Britain</span></a><a href="https://www.economist.com/topics/world" data-analytics="tags:world"><span>World</span></a><a href="https://www.economist.com/topics/jd-vance" data-analytics="tags:jd_vance"><span>J.D. Vance</span></a></nav></p><p>This article appeared in the Britain section of the print edition under the headline “Amend thyself&nbsp;”</p><div data-test-id="chapterlist" data-tracking-id="content-well-chapter-list"><div><hr data-testid="rule-accent"><div><h3><a href="https://www.economist.com/britain" text="Britain" data-analytics="chapter_list_header:Britain">Britain</a></h3><p><span>May 17th 2025</span></p></div></div><ul><li><a href="https://www.economist.com/britain/2025/05/15/britains-police-are-restricting-speech-in-worrying-ways" id="3b8e917b-6c56-4e5c-86d6-060d5fd2c9e9" data-analytics="article:reports_headline:1" data-test-id="chapterlist-link-0"><span data-testid="right-economist-red-false"><span>→</span></span><span>Britain’s police are restricting speech in worrying ways</span></a></li><li><a href="https://www.economist.com/britain/2025/05/12/britains-experiment-with-liberal-immigration-policies-is-over" id="4d8f9672-f05a-4f88-8e32-8893863eaecf" data-analytics="article:reports_headline:2" data-test-id="chapterlist-link-1"><span data-testid="right-london-5-false"><span>→</span></span><span>Britain’s experiment with liberal immigration policies is over</span></a></li><li><a href="https://www.economist.com/britain/2025/05/15/cheap-petrol-offers-a-small-respite-for-squeezed-households" id="9ebd2d27-c30a-4f4a-b2f2-b8e8aafc70ac" data-analytics="article:reports_headline:3" data-test-id="chapterlist-link-2"><span data-testid="right-london-5-false"><span>→</span></span><span>Cheap petrol offers a small respite for squeezed households</span></a></li><li><a href="https://www.economist.com/britain/2025/05/15/when-levelling-up-comes-to-town" id="dac6a737-edf9-472f-af64-e47b86c4042a" data-analytics="article:reports_headline:4" data-test-id="chapterlist-link-3"><span data-testid="right-london-5-false"><span>→</span></span><span>When levelling-up comes to town</span></a></li><li><a href="https://www.economist.com/britain/2025/05/15/how-to-prevent-drunken-punch-ups" id="56982ded-54c7-491a-a5a9-55e6c8ec19f4" data-analytics="article:reports_headline:5" data-test-id="chapterlist-link-4"><span data-testid="right-london-5-false"><span>→</span></span><span>How to prevent drunken punch-ups</span></a></li><li><a href="https://www.economist.com/britain/2025/05/15/how-to-build-tram-lines-quickly-and-cheaply" id="b84f7498-f319-46b6-9e29-6d2810538cf5" data-analytics="article:reports_headline:6" data-test-id="chapterlist-link-5"><span data-testid="right-london-5-false"><span>→</span></span><span>How to build tram lines quickly and cheaply </span></a></li><li><a href="https://www.economist.com/britain/2025/05/14/a-world-without-nigel-farage" id="57d93844-7ed5-4f52-a821-c46a307b6707" data-analytics="article:reports_headline:7" data-test-id="chapterlist-link-6"><span data-testid="right-london-5-false"><span>→</span></span><span>A world without Nigel Farage&nbsp;</span></a></li></ul></div><div orientation="vertical" data-test-id="vertical"><div orientation="vertical"><figure><img loading="lazy" width="1280" height="1709" decoding="async" data-nimg="1" sizes="300px" srcset="https://www.economist.com/cdn-cgi/image/width=16,quality=80,format=auto/content-assets/images/20250517_DE_EU.jpg 16w, https://www.economist.com/cdn-cgi/image/width=32,quality=80,format=auto/content-assets/images/20250517_DE_EU.jpg 32w, https://www.economist.com/cdn-cgi/image/width=48,quality=80,format=auto/content-assets/images/20250517_DE_EU.jpg 48w, https://www.economist.com/cdn-cgi/image/width=64,quality=80,format=auto/content-assets/images/20250517_DE_EU.jpg 64w, https://www.economist.com/cdn-cgi/image/width=96,quality=80,format=auto/content-assets/images/20250517_DE_EU.jpg 96w, https://www.economist.com/cdn-cgi/image/width=128,quality=80,format=auto/content-assets/images/20250517_DE_EU.jpg 128w, https://www.economist.com/cdn-cgi/image/width=256,quality=80,format=auto/content-assets/images/20250517_DE_EU.jpg 256w, https://www.economist.com/cdn-cgi/image/width=360,quality=80,format=auto/content-assets/images/20250517_DE_EU.jpg 360w, https://www.economist.com/cdn-cgi/image/width=384,quality=80,format=auto/content-assets/images/20250517_DE_EU.jpg 384w, https://www.economist.com/cdn-cgi/image/width=480,quality=80,format=auto/content-assets/images/20250517_DE_EU.jpg 480w, https://www.economist.com/cdn-cgi/image/width=600,quality=80,format=auto/content-assets/images/20250517_DE_EU.jpg 600w, https://www.economist.com/cdn-cgi/image/width=834,quality=80,format=auto/content-assets/images/20250517_DE_EU.jpg 834w, https://www.economist.com/cdn-cgi/image/width=960,quality=80,format=auto/content-assets/images/20250517_DE_EU.jpg 960w, https://www.economist.com/cdn-cgi/image/width=1096,quality=80,format=auto/content-assets/images/20250517_DE_EU.jpg 1096w, https://www.economist.com/cdn-cgi/image/width=1280,quality=80,format=auto/content-assets/images/20250517_DE_EU.jpg 1280w, https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20250517_DE_EU.jpg 1424w" src="https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20250517_DE_EU.jpg"></figure></div><div orientation="vertical"><h3 orientation="vertical">From the May 17th 2025 edition</h3><p orientation="vertical">Discover stories from this section and more in the list of contents</p><p><a href="https://www.economist.com/weeklyedition/2025-05-17" data-analytics="sidebar:weekly_edition"><span data-testid="right-economist-red-true"><span>⇒</span></span><span>Explore the edition</span></a></p></div></div><div><a href="https://s100.copyright.com/AppDispatchServlet?publisherName=economist&amp;publication=economist&amp;title=Britain%E2%80%99s%20police%20are%20restricting%20speech%20in%20worrying%20ways&amp;publicationDate=2025-05-15&amp;contentID=%2Fcontent%2Fu6hndvo01mattrcpmn5ju8d2c54aa2am&amp;type=A&amp;orderBeanReset=TRUE" target="_blank" rel="noreferrer" data-analytics="end_of_article:reuse_this_content"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" data-testid="renew-outline"><path fill="var(--mb-colour-base-chicago-45)" d="M5.1 16.05a8.25 8.25 0 0 1-.825-1.95A7.696 7.696 0 0 1 4 12.05c0-2.233.775-4.133 2.325-5.7C7.875 4.783 9.767 4 12 4h.175l-1.6-1.6 1.4-1.4 4 4-4 4-1.4-1.4 1.6-1.6H12c-1.667 0-3.083.588-4.25 1.763C6.583 8.938 6 10.367 6 12.05c0 .433.05.858.15 1.275.1.417.25.825.45 1.225l-1.5 1.5ZM12.025 23l-4-4 4-4 1.4 1.4-1.6 1.6H12c1.667 0 3.083-.587 4.25-1.762C17.417 15.063 18 13.633 18 11.95c0-.433-.05-.858-.15-1.275-.1-.417-.25-.825-.45-1.225l1.5-1.5c.367.633.642 1.283.825 1.95.183.667.275 1.35.275 2.05 0 2.233-.775 4.133-2.325 5.7C16.125 19.217 14.233 20 12 20h-.175l1.6 1.6-1.4 1.4Z"></path></svg><span>Reuse this content</span></a></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lossless video compression using Bloom filters (261 pts)]]></title>
            <link>https://github.com/ross39/new_bloom_filter_repo/blob/main/README.md</link>
            <guid>44100179</guid>
            <pubDate>Mon, 26 May 2025 18:32:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ross39/new_bloom_filter_repo/blob/main/README.md">https://github.com/ross39/new_bloom_filter_repo/blob/main/README.md</a>, See on <a href="https://news.ycombinator.com/item?id=44100179">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>


                <li>
      

      <div>
          <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_product_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
          <p>
            GitHub Copilot
          </p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_models&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_models_link_product_navbar&quot;}" href="https://github.com/features/models">
      
      <div>
          <p>
            GitHub Models
              <span>
                New
              </span>
          </p><p>
        Manage and compare prompts
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_advanced_security_link_product_navbar&quot;}" href="https://github.com/security/advanced-security">
      
      <div>
          <p>
            GitHub Advanced Security
          </p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_product_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
          <p>
            Actions
          </p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_product_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
          <p>
            Codespaces
          </p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                </ul>
              </div>
          <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_product_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
          <p>
            Issues
          </p><p>
        Plan and track work
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_product_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
          <p>
            Code Review
          </p><p>
        Manage code changes
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_product_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
          <p>
            Discussions
          </p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_product_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
          <p>
            Code Search
          </p><p>
        Find more, search less
      </p></div>

    
</a></li>

                </ul>
              </div>
          

      </div>
</li>


                <li>
      

      
</li>


                <li>
      

      <div>
                    <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;events_amp_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;events_amp_webinars_link_resources_navbar&quot;}" href="https://resources.github.com/">
      Events &amp; Webinars

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ebooks_amp_whitepapers&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ebooks_amp_whitepapers_link_resources_navbar&quot;}" href="https://github.com/resources/whitepapers">
      Ebooks &amp; Whitepapers

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
      Executive Insights

    
</a></li>

                </ul>
              </div>
</li>


                <li>
      

      <div>
              <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
          <p>
            GitHub Sponsors
          </p><p>
        Fund open source developers
      </p></div>

    
</a></li>

                </ul>
              </div>
              <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
          <p>
            The ReadME Project
          </p><p>
        GitHub community articles
      </p></div>

    
</a></li>

                </ul>
              </div>
              
          </div>
</li>


                <li>
      

      <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
          <p>
            Enterprise platform
          </p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;pricing&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;pricing_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:ross39/new_bloom_filter_repo" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="QLhhtg8mvk9K8AjPJqm9CZwJpLheatDZ32I4BbS9mFwJGYbVbRM2_NtrAZIVRXMVPUu4ZOUlWkJIAI5-gvGbjA" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="ross39/new_bloom_filter_repo" data-current-org="" data-current-owner="ross39" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=ross39%2Fnew_bloom_filter_repo" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/ross39/new_bloom_filter_repo/blob/main/README.md&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="cfbe2d70c281841f7a9367b215371ab5dad78485b9a34efd72a64d32fdf465f2" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/blob/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a></p><p>
    <react-partial-anchor>
      <tool-tip id="tooltip-0f7c810a-b219-4651-ac30-d5b44288f6cd" for="icon-button-1a8335b2-fd36-49a1-948f-2c49b12b404f" popover="manual" data-direction="s" data-type="label" data-view-component="true">Appearance settings</tool-tip>

      <template data-target="react-partial-anchor.template">
        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.19291721a114332ad118.module.css">
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.22dfbc22ef0a2bf02523.module.css">

<react-partial partial-name="appearance-settings" data-ssr="false" data-attempted-ssr="false">
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>

      </template>
    </react-partial-anchor>
  </p>

          </div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CSS Minecraft (834 pts)]]></title>
            <link>https://benjaminaster.com/css-minecraft/</link>
            <guid>44100148</guid>
            <pubDate>Mon, 26 May 2025 18:28:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://benjaminaster.com/css-minecraft/">https://benjaminaster.com/css-minecraft/</a>, See on <a href="https://news.ycombinator.com/item?id=44100148">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p><strong>There is no JavaScript on this page.</strong> All the logic is made 100% with pure HTML &amp; CSS. For the best performance, please close other tabs and running programs.<br>
View on <a href="https://github.com/BenjaminAster/CSS-Minecraft">GitHub</a>, <a href="https://codepen.io/Benjamin_Aster/pen/gOKwpOd">CodePen</a>, <a href="https://benjaminaster.com/css-minecraft/">benjaminaster.com</a></p>
<p>⚠︎ Your browser does not support the <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/:has">CSS <code>:has()</code> pseudo-class</a>, which is needed for this site to work.
Please update it: Chromium version ≥ 105, Safari version ≥ 15.4 or Firefox version ≥ 121 is required.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Claude 4 and GitHub MCP will leak your private GitHub repositories (244 pts)]]></title>
            <link>https://twitter.com/lbeurerkellner/status/1926991491735429514</link>
            <guid>44100082</guid>
            <pubDate>Mon, 26 May 2025 18:20:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/lbeurerkellner/status/1926991491735429514">https://twitter.com/lbeurerkellner/status/1926991491735429514</a>, See on <a href="https://news.ycombinator.com/item?id=44100082">Hacker News</a></p>
Couldn't get https://twitter.com/lbeurerkellner/status/1926991491735429514: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[Duolingo CEO tries to walk back AI-first comments, fails (398 pts)]]></title>
            <link>https://htxt.co.za/2025/05/duolingo-ceo-tries-to-walk-back-ai-first-comments-fails/</link>
            <guid>44100035</guid>
            <pubDate>Mon, 26 May 2025 18:14:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://htxt.co.za/2025/05/duolingo-ceo-tries-to-walk-back-ai-first-comments-fails/">https://htxt.co.za/2025/05/duolingo-ceo-tries-to-walk-back-ai-first-comments-fails/</a>, See on <a href="https://news.ycombinator.com/item?id=44100035">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="117b548" data-element_type="widget" data-widget_type="theme-post-content.default">
					
<ul>
<li><strong>Following backlash to statements that Duolingo will be AI-first, threatening jobs in the process, CEO Luis von Ahn has tried to walk back his statement.</strong></li>



<li><strong>Unfortunately, the CEO doesn’t walk back any of the key points he originally outlined, choosing instead to try, and fail to placate the maddening crowd.</strong></li>



<li><strong>Unfortunately the PR team may soon be replaced by AI as this latest statement has done anything but instil confidence in the firm’s users.</strong></li>
</ul>



<p>About a month ago, <a href="https://htxt.co.za/2025/04/duolingo-kicks-contractors-out-the-door-while-chasing-the-shadow-of-ai/" target="_blank" rel="noreferrer noopener">Duolingo decided that it would gradually fire all contractors</a> and instead, use AI in a bid to become an AI-first company. Beyond firing contractors, Duolingo planned to make AI a requirement for every aspect of its business. Now Luis von Ahn is trying to do damage control, and failing.</p>



<p>The billionaire <a href="https://www.linkedin.com/posts/luis-von-ahn-duolingo_one-of-the-most-important-things-leaders-activity-7331386411670982658-jpfX?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAACMhBZgBJoR2yKMoR7v1PnwsuFca7PYkms0" target="_blank" rel="noreferrer noopener">took to LinkedIn last week</a> to admit that he wasn’t clear in his AI memo, a memo that sent users into a frenzy, swearing off the app, cancelling premium subscriptions and a once-beloved brand being dragged through the mud.</p>



<p>“I don’t know exactly what’s going to happen with AI, but I do know it’s going to fundamentally change the way we work, and we have to get ahead of it,” admits the man who just a few weeks ago crowed about how vital AI was to Duolingo’s business.</p>



<p>“AI is creating uncertainty for all of us, and we can respond to this with fear or curiosity. I’ve always encouraged our team to embrace new technology (that’s why we originally built for mobile instead of desktop), and we are taking that same approach with AI. By understanding the capabilities and limitations of AI now, we can stay ahead of it and remain in control of our own product and our mission,” writes von Ahn.</p>



<p>Except that’s not at all what you said last month is it Luis? You said that AI was needed to understand your codebase and that without AI, it’d be impossible to scale the platform. In fact, you went so far as to say that despite AI not being “100 percent perfect” it was vital that Duolingo leap headfirst into the space.</p>



<p>The CEO then also says that Duolingo isn’t looking to replace employees with AI, despite explicitly saying as much about contractors. The key here is that, in the eyes of Duolingo and most Silicon Valley powerhouses (we’re looking at you Uber) hiring contractors is a way to avoid the requirements that go along with having employees. </p>



<p>“My goal is for Duos [employees] to feel empowered and prepared to use this technology. No one is expected to navigate this shift alone. We’re developing workshops and advisory councils, and carving out dedicated experimentation time to help all our teams learn and adapt,” wrote von Ahn.</p>



<p>Below his LinkedIn post you will find all the sycophantic praise you might expect from the mix of wealthy and bot users on the platform. It’s all very gross and only serves to highlight just how out of touch the wealthy are with regular people.</p>



<p>The Duolingo CEO’s latest post does nothing to reverse the statement made a month ago. The CEO didn’t backtrack the statement that AI would form part of whether a candidate is worth hiring or not. He also didn’t backtrack the statement that headcount will only increase if a team can no longer automate anymore of their work.</p>



<p>Essentially then, this was a nothing statement from von Ahn that we suspect Duolingo’s PR issued as a sort of damage control. Unfortunately, that team may soon be replaced by AI because it isn’t going well. To their credit, it didn’t help that von Ahn made a frankly stupid comment about machines being able to teach a person anything another human could. Already being dragged by the public, this backtracking just added more fuel to the fire.</p>



<figure><figcaption><em>The comments are confusing and upsetting.</em></figcaption></figure>



<figure><figcaption><em>Not von Ahn clearly.</em></figcaption></figure>



<p>Some have taken this latest sign that Duolingo is backtracking its suggestion of being AI first but there’s no language in the statement to suggest that and key statements made last month weren’t walked back at all.</p>



<p>Unfortunately, Silicon Valley isn’t listening to the masses. They are investing heavily in tech on the basis that it can replace workers and make them more money without a second thought. Unfortunately for Duolingo, its customer base is largely consumer-facing and if there is one thing consumers hate more than a scam, it’s injustice and this trend with AI seems to combine both.</p>



<p>We doubt public pressure will actually change what happens at Duolingo. We suspect that if anything, it will simply mean von Ahn stops making public statements unless that aren’t heavily sanitised by the legal and PR department.</p>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TSMC bets on unorthodox optical tech (173 pts)]]></title>
            <link>https://spectrum.ieee.org/microled-optical-chiplet</link>
            <guid>44099407</guid>
            <pubDate>Mon, 26 May 2025 17:15:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/microled-optical-chiplet">https://spectrum.ieee.org/microled-optical-chiplet</a>, See on <a href="https://news.ycombinator.com/item?id=44099407">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="TSMC Bets On Unorthodox Optical Tech"><p>In the race to an <a href="https://spectrum.ieee.org/optics-gpu" target="_self"><span>all-optical</span></a> AI data center, a major player has now placed a bet on a different horse. <a href="https://spectrum.ieee.org/tag/semiconductor-manufacturing">Semiconductor manufacturing</a> giant <a href="https://www.businesswire.com/news/home/20250422988144/en/Avicena-Works-with-TSMC-to-Enable-PD-Arrays-for-LightBundle-MicroLED-Based-Interconnects" target="_blank"><span>TSMC announced</span></a> that it will work with Sunnyvale startup <a href="https://avicena.tech/" target="_blank"><span>Avicena</span></a> to produce microLED-based <a href="https://spectrum.ieee.org/tag/interconnects">interconnects</a>. The technology is a pragmatic twist on replacing electrical connections with optical ones to meet the high needs of communication among an increasing number of <a href="https://spectrum.ieee.org/tag/gpus">GPUs</a> in a low cost, energy efficient way. <strong></strong></p><p>Thanks to the computational demands of <a href="https://spectrum.ieee.org/tag/large-language-models">large language models</a> and their cousins, AI clusters are facing unprecedented requirements regarding amounts of data, bandwidth, latency, and speed. Sooner or later, the <a href="https://spectrum.ieee.org/tag/copper-wires">copper wires</a> that connect <a href="https://spectrum.ieee.org/tag/processors">processors</a> and memory within a single AI data center rack will have to be <a href="https://spectrum.ieee.org/optical-interconnects" target="_self"><span>replaced with optics</span></a>. “There’s a huge push to get optical connections as close to the board as possible,” says Lucas Tsai, a vice president at <a href="https://spectrum.ieee.org/tag/tsmc">TSMC</a>.</p><p>Avicena offers a unique approach, using hundreds of blue <a href="https://spectrum.ieee.org/tag/microleds">microLEDs</a> connected through imaging-type fibers to move data. The company’s modular <a href="https://avicena.tech/avicena-announces-modular-lightbundle-optical-interconnect-platform-with-1tbps-mm-i-o-density-and-1pj-bit/" target="_blank"><span>LightBundle platform</span></a> avoids problems with <a href="https://spectrum.ieee.org/tag/lasers">lasers</a> and their associated complexity that threaten the reliability, cost, and power consumption of other optical <a href="https://spectrum.ieee.org/tag/chiplets">chiplets</a>. Tsai says “it’s very unorthodox!” But it is ideal for these short distance applications, and that’s precisely what makes it interesting.</p><h2>Laser-free</h2><p>Optical connections today carry vast amounts of data tens to hundreds of meters across <a href="https://spectrum.ieee.org/tag/data-centers">data centers</a> at very high <a href="https://spectrum.ieee.org/tag/data-rates">data rates</a>. Traditionally, a pluggable module connects the <a href="https://spectrum.ieee.org/tag/optical-fiber">optical fiber</a> to the rack, where it converts between electrical and optical signals. Companies are making strides toward getting rid of these energy inefficient pluggable <a href="https://spectrum.ieee.org/tag/transceivers">transceivers</a> using co-packaged <a href="https://spectrum.ieee.org/tag/optics">optics</a> (CPO), which instead perform electrical-optical transformations adjacent to the silicon chip itself. Commercial versions exist for the <a href="https://spectrum.ieee.org/co-packaged-optics" target="_self"><span>network switch</span></a>, and prototypes are making strides <a href="https://spectrum.ieee.org/optical-interconnects" target="_self"><span>toward the GPU</span></a>. The most prominent optical chiplet designs encode electronic bits onto multiple wavelengths of light using lasers and modulators.</p><p>However, the main challenge for laser-based <a href="https://spectrum.ieee.org/tag/optical-interconnects">optical interconnects</a> is the laser itself. The laser and fiber attachments have caused the biggest problems in terms of reliability, manufacturing, and cost. Moreover, a single optical fiber that hosts dozens of GPU-to-switch links in the form of multiple wavelengths suffers from computational overhead: it’s far simpler to pipe each data lane down a separate physical channel than to electronically parse one big channel later on. <strong></strong></p><p>That’s where <a href="https://spectrum.ieee.org/photonics-and-ai" target="_self"><span>Avicena comes in</span></a>. Instead of sending a multi-wavelength laser down an optical fiber and then parsing it into individual channels, the LightBundle interconnect links hundreds of blue microLEDs to a <a href="https://spectrum.ieee.org/tag/photodetector">photodetector</a> array via multi-core imaging fibers—one for each 10 GB/s data lane. The transmitter acts like a miniature display screen and the detector like a camera. “We’re doing optical interconnects without the complexity of lasers,” says Avicena CEO Bardia Pezeshki.</p><p>A simple optical link with just 300 pixels at 10Gb/s per lane can extend over a 10m distance, carrying a net total of 3 Tb/s. Since displays and <a href="https://spectrum.ieee.org/tag/cameras">cameras</a> can scale to millions of pixels, the technology can scale to much higher data rates at much lower energy and much higher density than copper wiring. </p><h2>Mature industry</h2><p>One major advantage touted by Avicena is that their technology harnesses <a href="https://spectrum.ieee.org/tag/leds">LEDs</a>, cameras, and displays: all mature industries. “We can scale our approach to the volumes and costs required much more rapidly than if we were developing new building blocks,” says Pezeshki. Even though <a href="https://spectrum.ieee.org/tag/silicon-photonics">silicon photonics</a> has a thirty-year head start on optical interconnects, they have to develop new components like ring <a href="https://spectrum.ieee.org/tag/resonators">resonators</a> and comb lasers. “It takes a lot of time for these things to mature,” he says. In contrast, the LightBundle interconnects design requires only minor modifications to existing camera and display technologies.</p><p>That’s one main reason that TSMC signed up to produce the photodetector arrays for Avicena’s optical chiplets. “LEDs is a mature industry already, there are a lot of consumer products,” Tsai says. LEDs are of course lower power than lasers—but for, say, 10-meter distances within and across a rack, that’s enough. “There’s a potential that it will be a lot cheaper, and by nature you have a lot of redundancy,” says Tsai.</p><p>Avicena’s results are already “blowing away” what silicon <a href="https://spectrum.ieee.org/tag/photonics">photonics</a> can do, according to Pezeshki. The LightBundle prototype has already demonstrated sub-pJ/bit energy use for the whole link, where other optical approaches “are struggling to show” 5 pJ/bit energy consumption. <strong></strong></p><p>Pezeshki acknowledges that Avicena has a ways to go to build and scale the product. But “the combination of showing great results together with using mature building blocks is winning over converts,” he says.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: PgDog – Shard Postgres without extensions (237 pts)]]></title>
            <link>https://github.com/pgdogdev/pgdog</link>
            <guid>44099187</guid>
            <pubDate>Mon, 26 May 2025 16:55:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/pgdogdev/pgdog">https://github.com/pgdogdev/pgdog</a>, See on <a href="https://news.ycombinator.com/item?id=44099187">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/pgdogdev/pgdog/blob/main/.github/logo2-white.png"><img src="https://github.com/pgdogdev/pgdog/raw/main/.github/logo2-white.png" height="128" width="auto"></a>
</p>
<p dir="auto"><a href="https://github.com/levkk/pgdog/actions/workflows/ci.yml"><img src="https://github.com/levkk/pgdog/actions/workflows/ci.yml/badge.svg" alt="CI"></a></p>
<p dir="auto">PgDog is a transaction pooler and logical replication manager that can shard PostgreSQL. Written in Rust, PgDog is fast, secure and can manage hundreds of databases and hundreds of thousands of connections.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation</h2><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<p dir="auto">📘 PgDog documentation can be <strong><a href="https://docs.pgdog.dev/" rel="nofollow">found here</a></strong>. Any questions? Join our <strong><a href="https://discord.com/invite/CcBZkjSJdd" rel="nofollow">Discord</a></strong>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick start" href="#quick-start"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Kubernetes</h3><a id="user-content-kubernetes" aria-label="Permalink: Kubernetes" href="#kubernetes"></a></p>
<p dir="auto">Helm chart is <strong><a href="https://github.com/pgdogdev/helm">here</a></strong>. To install it, run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/pgdogdev/helm &amp;&amp; \
cd helm &amp;&amp; \
helm install -f values.yaml pgdog ./"><pre>git clone https://github.com/pgdogdev/helm <span>&amp;&amp;</span> \
<span>cd</span> helm <span>&amp;&amp;</span> \
helm install -f values.yaml pgdog ./</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Docker</h3><a id="user-content-docker" aria-label="Permalink: Docker" href="#docker"></a></p>
<p dir="auto">You can try PgDog quickly using Docker. Install <a href="https://docs.docker.com/compose/" rel="nofollow">Docker Compose</a> and run:</p>

<p dir="auto">It will take a few minutes to build PgDog from source and launch the containers. Once started, you can connect to PgDog with psql (or any other PostgreSQL client):</p>
<div data-snippet-clipboard-copy-content="PGPASSWORD=postgres psql -h 127.0.0.1 -p 6432 -U postgres"><pre><code>PGPASSWORD=postgres psql -h 127.0.0.1 -p 6432 -U postgres
</code></pre></div>
<p dir="auto">The demo comes with 3 shards and 2 sharded tables:</p>
<div dir="auto" data-snippet-clipboard-copy-content="INSERT INTO users (id, email) VALUES (1, 'admin@acme.com');
INSERT INTO payments (id, user_id, amount) VALUES (1, 1, 100.0);

SELECT * FROM users WHERE id = 1;
SELECT * FROM payments WHERE user_id = 1;"><pre><span>INSERT INTO</span> users (id, email) <span>VALUES</span> (<span>1</span>, <span><span>'</span>admin@acme.com<span>'</span></span>);
<span>INSERT INTO</span> payments (id, user_id, amount) <span>VALUES</span> (<span>1</span>, <span>1</span>, <span>100</span>.<span>0</span>);

<span>SELECT</span> <span>*</span> <span>FROM</span> users <span>WHERE</span> id <span>=</span> <span>1</span>;
<span>SELECT</span> <span>*</span> <span>FROM</span> payments <span>WHERE</span> user_id <span>=</span> <span>1</span>;</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Monitoring</h3><a id="user-content-monitoring" aria-label="Permalink: Monitoring" href="#monitoring"></a></p>
<p dir="auto">PgDog exposes both the standard PgBouncer-style admin database and an OpenMetrics endpoint. The admin database isn't 100% compatible,
so we recommend you use OpenMetrics for monitoring. Example Datadog configuration and dashboard are <a href="https://github.com/pgdogdev/pgdog/blob/main/examples/datadog">included</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Load balancer</h3><a id="user-content-load-balancer" aria-label="Permalink: Load balancer" href="#load-balancer"></a></p>
<p dir="auto">PgDog is an application layer (OSI Level 7) load balancer for PostgreSQL. It can proxy multiple replicas (and primary) and distribute transactions evenly between databases. It supports multiple strategies, including round robin, random, least active connections, etc. PgDog can also inspect queries and send <code>SELECT</code> queries to replicas, and all others to the primary. This allows to proxy all databases behind a single PgDog deployment.</p>
<p dir="auto">📘 <strong><a href="https://docs.pgdog.dev/features/load-balancer" rel="nofollow">Load balancer</a></strong></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Healthchecks and failover</h4><a id="user-content-healthchecks-and-failover" aria-label="Permalink: Healthchecks and failover" href="#healthchecks-and-failover"></a></p>
<p dir="auto">PgDog maintains a real-time list of healthy hosts. When a host fails a healthcheck, it's removed from active rotation and queries are rerouted to other databases. This is similar to HTTP load balancing, except it's at the database layer.</p>
<p dir="auto">Failover maximizes database availability and protects against bad network connections, temporary hardware failures or misconfiguration.</p>
<p dir="auto">📘 <strong><a href="https://docs.pgdog.dev/features/healthchecks" rel="nofollow">Healthchecks</a></strong></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Transaction pooling</h3><a id="user-content-transaction-pooling" aria-label="Permalink: Transaction pooling" href="#transaction-pooling"></a></p>
<p dir="auto">Like PgBouncer, PgDog supports transaction (and session) pooling, allowing
100,000s of clients to use just a few PostgreSQL server connections.</p>
<p dir="auto">📘 <strong><a href="https://docs.pgdog.dev/features/transaction-mode" rel="nofollow">Transactions</a></strong></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Sharding</h3><a id="user-content-sharding" aria-label="Permalink: Sharding" href="#sharding"></a></p>
<p dir="auto">PgDog is able to handle databases with multiple shards by routing queries automatically to one or more shards. Using the native PostgreSQL parser, PgDog understands queries, extracts sharding keys and determines the best routing strategy. For cross-shard queries, PgDog assembles results in memory and sends them all to the client transparently.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Using <code>COPY</code></h4><a id="user-content-using-copy" aria-label="Permalink: Using COPY" href="#using-copy"></a></p>
<p dir="auto">PgDog comes with a CSV parser and can split COPY commands between all shards automatically. This allows clients to ingest data into sharded PostgreSQL without preprocessing.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Logical replication</h4><a id="user-content-logical-replication" aria-label="Permalink: Logical replication" href="#logical-replication"></a></p>
<p dir="auto">PgDog understands the PostgreSQL logical replication protocol and can split data between databases in the background and without downtime. This allows to shard existing databases and add more shards to existing clusters in production, without impacting database operations.</p>
<p dir="auto">📘 <strong><a href="https://docs.pgdog.dev/features/sharding/" rel="nofollow">Sharding</a></strong></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration</h3><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">PgDog is highly configurable and many aspects of its operation can be tweaked at runtime, without having
to restart the process and break PostgreSQL connections. If you've used PgBouncer (or PgCat) before, the options
will be familiar. If not, they are documented with examples.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">📘 <strong><a href="https://docs.pgdog.dev/configuration/" rel="nofollow">Configuration</a></strong></h2><a id="user-content--configuration" aria-label="Permalink: 📘 Configuration" href="#-configuration"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running PgDog locally</h2><a id="user-content-running-pgdog-locally" aria-label="Permalink: Running PgDog locally" href="#running-pgdog-locally"></a></p>
<p dir="auto">Install the latest version of the Rust compiler from <a href="https://rust-lang.org/" rel="nofollow">rust-lang.org</a>.
Clone this repository and build the project in release mode:</p>

<p dir="auto">It's important to use the release profile if you're deploying to production or want to run
performance benchmarks.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration</h3><a id="user-content-configuration-1" aria-label="Permalink: Configuration" href="#configuration-1"></a></p>
<p dir="auto">PgDog has two configuration files:</p>
<ul dir="auto">
<li><code>pgdog.toml</code> which contains general settings and PostgreSQL servers information</li>
<li><code>users.toml</code> for users and passwords</li>
</ul>
<p dir="auto">Most options have reasonable defaults, so a basic configuration for a single user
and database running on the same machine is pretty short:</p>
<p dir="auto"><strong><code>pgdog.toml</code></strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="[general]
host = &quot;0.0.0.0&quot;
port = 6432

[[databases]]
name = &quot;pgdog&quot;
host = &quot;127.0.0.1&quot;"><pre>[<span>general</span>]
<span>host</span> = <span><span>"</span>0.0.0.0<span>"</span></span>
<span>port</span> = <span>6432</span>

[[<span>databases</span>]]
<span>name</span> = <span><span>"</span>pgdog<span>"</span></span>
<span>host</span> = <span><span>"</span>127.0.0.1<span>"</span></span></pre></div>
<p dir="auto"><strong><code>users.toml</code></strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="[[users]]
name = &quot;pgdog&quot;
password = &quot;pgdog&quot;
database = &quot;pgdog&quot;"><pre>[[<span>users</span>]]
<span>name</span> = <span><span>"</span>pgdog<span>"</span></span>
<span>password</span> = <span><span>"</span>pgdog<span>"</span></span>
<span>database</span> = <span><span>"</span>pgdog<span>"</span></span></pre></div>
<p dir="auto">If you'd like to try this out, you can set it up like so:</p>
<div data-snippet-clipboard-copy-content="CREATE DATABASE pgdog;
CREATE USER pgdog PASSWORD 'pgdog' LOGIN;"><pre lang="postgresql"><code>CREATE DATABASE pgdog;
CREATE USER pgdog PASSWORD 'pgdog' LOGIN;
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Try sharding</h4><a id="user-content-try-sharding" aria-label="Permalink: Try sharding" href="#try-sharding"></a></p>
<p dir="auto">The configuration files for a sharded database are provided in the repository. To make it work locally, create the required databases:</p>
<div data-snippet-clipboard-copy-content="CREATE DATABASE shard_0;
CREATE DATABASE shard_1;

GRANT ALL ON DATABASE shard_0 TO pgdog;
GRANT ALL ON DATABASE shard_1 TO pgdog;"><pre lang="postgresql"><code>CREATE DATABASE shard_0;
CREATE DATABASE shard_1;

GRANT ALL ON DATABASE shard_0 TO pgdog;
GRANT ALL ON DATABASE shard_1 TO pgdog;
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Start PgDog</h3><a id="user-content-start-pgdog" aria-label="Permalink: Start PgDog" href="#start-pgdog"></a></p>
<p dir="auto">Running PgDog can be done with Cargo:</p>

<p dir="auto">You can connect to PgDog with psql or any other PostgreSQL client:</p>
<div dir="auto" data-snippet-clipboard-copy-content="psql postgres://pgdog:pgdog@127.0.0.1:6432/pgdog"><pre>psql postgres://pgdog:pgdog@127.0.0.1:6432/pgdog</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚦 Status 🚦</h2><a id="user-content--status-" aria-label="Permalink: 🚦 Status 🚦" href="#-status-"></a></p>
<p dir="auto">This project is just getting started and early adopters are welcome to try PgDog internally. Status on features stability will be <a href="https://docs.pgdog.dev/features/" rel="nofollow">updated regularly</a>. Most features have tests and are benchmarked regularly for performance regressions.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance</h2><a id="user-content-performance" aria-label="Permalink: Performance" href="#performance"></a></p>
<p dir="auto">PgDog does its best to minimize its impact on overall database performance. Using Rust and Tokio is a great start for a fast network proxy, but additional care is also taken to perform as few operations as possible while moving data between client and server sockets. Some benchmarks are provided to help set a baseline.</p>
<p dir="auto">📘 <strong><a href="https://docs.pgdog.dev/architecture/" rel="nofollow">Architecture &amp; benchmarks</a></strong></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">PgDog is free and open source software, licensed under the AGPL v3. While often misunderstood, this license is very permissive
and allows the following without any additional requirements from you or your organization:</p>
<ul dir="auto">
<li>Internal use</li>
<li>Private modifications for internal use without sharing any source code</li>
</ul>
<p dir="auto">You can freely use PgDog to power your PostgreSQL databases without having to
share any source code, including proprietary work product or any PgDog modifications you make.</p>
<p dir="auto">AGPL was written specifically for organizations that offer PgDog <em>as a public service</em> (e.g. database cloud providers) and require
those organizations to share any modifications they make to PgDog, including new features and bug fixes.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributions</h2><a id="user-content-contributions" aria-label="Permalink: Contributions" href="#contributions"></a></p>
<p dir="auto">Please read our <a href="https://github.com/pgdogdev/pgdog/blob/main/CONTRIBUTING.md">Contribution Guidelines</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TeleMessage Customers Include DC Police, Andreessen Horowitz, JP Morgan,Hundreds (174 pts)]]></title>
            <link>https://micahflee.com/telemessage-customers-include-dc-police-andreesen-horowitz-jp-morgan-and-hundreds-more/</link>
            <guid>44099096</guid>
            <pubDate>Mon, 26 May 2025 16:47:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://micahflee.com/telemessage-customers-include-dc-police-andreesen-horowitz-jp-morgan-and-hundreds-more/">https://micahflee.com/telemessage-customers-include-dc-police-andreesen-horowitz-jp-morgan-and-hundreds-more/</a>, See on <a href="https://news.ycombinator.com/item?id=44099096">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article>
                    <header>
                        
                            <figure>
        <img srcset="https://micahflee.com/content/images/size/w300/2025/05/Screenshot-2025-05-19-at-11.37.12-PM-1.png 300w,
                    https://micahflee.com/content/images/size/w720/2025/05/Screenshot-2025-05-19-at-11.37.12-PM-1.png 720w,
                    https://micahflee.com/content/images/size/w960/2025/05/Screenshot-2025-05-19-at-11.37.12-PM-1.png 960w,
                    https://micahflee.com/content/images/size/w1200/2025/05/Screenshot-2025-05-19-at-11.37.12-PM-1.png 1200w,
                    https://micahflee.com/content/images/size/w2000/2025/05/Screenshot-2025-05-19-at-11.37.12-PM-1.png 2000w,
                    https://micahflee.com/content/images/2025/05/Screenshot-2025-05-19-at-11.37.12-PM-1.png" sizes="(max-width: 1200px) 100vw, 1200px" src="https://micahflee.com/content/images/size/w1200/2025/05/Screenshot-2025-05-19-at-11.37.12-PM-1.png" alt="TeleMessage customers include DC Police, Andreessen Horowitz, JP Morgan, and hundreds more">
            <figcaption><span>"We are clean on OPSEC" sticker, with a drunk U.S. Secretary of Defense Pete Hegseth</span></figcaption>
    </figure>
                    </header>

                <section>
                    <p>I've been digging through the <a href="https://micahflee.com/ddosecrets-publishes-410-gb-of-heap-dumps-hacked-from-telemessages-archive-server/" rel="noreferrer">410 GB of Java heap dumps</a> from TeleMessage's archive server,  <a href="https://ddosecrets.com/article/telemessage" rel="noreferrer">provided</a> by DDoSecrets. Here's a description of the dataset, some of my initial findings, details about an upcoming open source research tool I'm going to release, and a huge list of potential TeleMessage customers.</p><p>First, some background. This "clean OPSEC" saga is <em>unbelievable</em>.</p><p>Mike Waltz <a href="https://web.archive.org/web/20250325174744/https://www.theatlantic.com/politics/archive/2025/03/trump-administration-accidentally-texted-me-its-war-plans/682151/" rel="noreferrer">invited</a> a journalist into a Signal group full of high-level Trumpers where they discussed and executed <a href="https://zeteo.com/p/signal-chat-war-crimes-revealed-yemen-trump-admin" rel="noreferrer">bombing</a> an apartment building full of innocent people. This led to Congressional <a href="https://www.pbs.org/newshour/politics/watch-ratcliffe-gabbard-patel-testify-to-senate-after-war-plans-revealed-to-journalist-in-chat" rel="noreferrer">hearings</a> (about using a Signal group for war, not the war crimes themselves... Congress doesn't really care about those).</p><p>Later, Waltz was photographed using TeleMessage SGNL, an Israeli-made knockoff of Signal that archives messages for its customers, and that <a href="https://micahflee.com/despite-misleading-marketing-israeli-company-telemessage-used-by-trump-officials-can-access-plaintext-chat-logs/" rel="noreferrer">lied</a> about supporting end-to-end encryption. Then TeleMessage was <a href="https://micahflee.com/despite-misleading-marketing-israeli-company-telemessage-used-by-trump-officials-can-access-plaintext-chat-logs/" rel="noreferrer">hacked</a>, <a href="https://www.nbcnews.com/tech/security/telemessage-suspends-services-hackers-say-breached-app-rcna204925" rel="noreferrer">twice</a>. The <a href="https://micahflee.com/how-the-knock-off-signal-app-used-by-trump-officials-got-hacked-in-20-minutes/" rel="noreferrer">trivial vulnerability</a> let anyone on the internet download Java heap dumps from the server. Then, DDoSecrets <a href="https://micahflee.com/ddosecrets-publishes-410-gb-of-heap-dumps-hacked-from-telemessages-archive-server/" rel="noreferrer">released</a> 410 GB of these heap dumps, all from May 4, 2025, and is distributing them to journalists and researchers.</p><p>"The trove included material from disaster responders, customs officials, several U.S. diplomatic staffers, at least one White House staffer and members of the Secret Service," <a href="https://www.reuters.com/world/us/hacker-who-breached-communications-app-used-by-trump-aide-stole-data-across-us-2025-05-21/" rel="noreferrer">according</a> to a Reuters report.</p>
<div data-layout="minimal">
                    
                        <p><span>I'm crunching data and writing these newsletters in my free time. If you want to support my work, considering becoming a paid supporter.</span></p>
                    
                    
                        <p><a href="#/portal/signup">
                            Become a paid supporter
                        </a>
                        
                    </p></div>
<h2 id="what-even-is-this-data">What even is this data?</h2><p>On May 4, a hacker loaded the URL <strong>archive.telemessage.com/management/heapdump</strong> over and over again, each time downloading a different Java heap dump from TeleMessage's server. Yes, the vulnerability was that simple, which is why it took <a href="https://micahflee.com/how-the-knock-off-signal-app-used-by-trump-officials-got-hacked-in-20-minutes/" rel="noreferrer">about 20 minutes</a> to find and exploit.</p><p>Each file is between 130 MB and 291 MB, and is in Java <a href="https://docs.oracle.com/javase/8/docs/technotes/samples/hprof.html" rel="noreferrer">HPROF format</a>. The easiest way to see what's inside is using the command line tool <code>strings</code>, which extracts all the printable strings from a binary file. If you run <code>strings</code> on one of these heap dump files, thousands of lines will scroll by, and some of them will be juicy-looking JSON objects containing plaintext chat messages, along with other interesting data.</p><p>This dataset is <em>not</em> a copy of all of the data that was stored on the TeleMessage archive server. It only includes fragments of data that happened to be in memory at a single moment on May 4. For example, I might find an interesting-looking message that's part of a group chat, but without any other messages from the same group.</p><p>The earliest message I found was from November 15, 2022, and there are messages all the way up until the hack. But about 80% of them are from May 2025. The dataset is mostly a snapshot in time from a specific Sunday earlier this month.</p><h2 id="how-juicy-are-we-talking">How juicy are we talking?</h2><p>So far, I haven't found anything from Trump cabinet officials.</p><p>While I've found plenty of things that seem interesting and warrant further investigation, so far I haven't uncovered anything that is obviously sensitive or revelatory.</p><p>I found a WhatsApp group called "MPD Command Staff" with 46 users in it. There are many messages in this group, but they're all encrypted. (As I described in my <a href="https://micahflee.com/despite-misleading-marketing-israeli-company-telemessage-used-by-trump-officials-can-access-plaintext-chat-logs/#corroborative-evidence-from-the-hack" rel="noreferrer">earlier analysis</a>, some of the individual messages are encrypted.) I looked up some of the phone numbers from this group on <a href="https://www.osint.industries/" rel="noreferrer">OSINT Industries</a> and quickly discovered that these people all work for the Metropolitan Police Department in Washington, DC.</p><p>I also found a message sent to a Signal group called "US / China AI Race." The Signal group had 100 people in it. I looked some of them up: many of the group members hold prominent positions at major universities, the defense industry, and the military, and all seem to do AI-related work. The message says, "The biggest crime was USG ignored these fabs for two years." That's it. The dataset doesn't include any other messages from this Signal group.</p><p>If you're a journalist looking for a tip: Most of the members of the group only listed names, but a few listed phone numbers. If you have access to this data and are looking for a story, why not send the phone-number people Signal messages and ask what this group is about?</p><p>As you can see, this dataset probably holds a million different leads. It's too early to tell if any of them will pan out and become something bigger.</p><h2 id="how-much-data-are-we-talking">How much data are we talking?</h2><p>The main file in the dataset that contains the compressed heap dumps is <code>telemessage.7z</code>, and it's 54 GB. After decompressing it, I end up with a folder with 2,729 heap dump files, taking a total of 384 GB of space. After running <code>strings</code> on each of these files, I end up with 83 GB of just text data.</p><p>Most of the text data is hundreds of thousands of useless lines like this:</p><pre><code>P&lt;E_IN:Ljava/lang/Object;E_OUT:Ljava/lang/Object;&gt;Ljava/util/stream/ReferencePipeline&lt;TE_IN;TE_OUT;&gt;;
(Ljava/util/function/BinaryOperator;Ljava/util/Map;Ljava/util/Map;)Ljava/util/Map;
PMethod java/util/concurrent/ConcurrentNavigableMap.floorEntry(Ljava/lang/Object;)Ljava/util/Map$Entry; is abstract</code></pre><p>However, some of the lines are more interesting, like this mildly redacted one:</p><pre><code>{"typ":"RawMessage","gatewayReceivedDate":1746332951616,"partner":"NONE","securityContent":null,"sourceService":null,"internalSecurityData":{"version":"0.0.2","internalDecryptionData":{"typ":"nothing","encryptionType":"DO_NOTHING","params":{}}},"networkType":"WHATSAPP_CLOUD_ARCHIVER","sourceType":"WHATSAPP_CLOUD_ARCHIVER","ownerExtClassId":null,"body":{"owner":{"value":"==redacted==","type":"PHONE"},"messageId":"09cfd8142e20170be8a3","messageType":"APP_MESSAGE","messageTime":1746332951000,"sender":{"value":"==redacted==","type":"PHONE"},"recipients":[{"value":"==redacted==","type":"PHONE"}],"direction":"IN","subject":"WhatsApp message from ==redacted== to ==redacted==","textField":{"extractor":{"typ":"WrapperExt","data":"Sure\nI</code></pre><p>Notice that this line ends abruptly, with <code>"data":"Sure\nI</code>, and that's it. This is a fragment of a JSON object, not the whole thing. These are all over the place. While there's potentially interesting details in broken JSON fragments, I decided to ignore them.</p><p>I've been writing software (which I plan to release as open source soon, and I talk about more below) that extracts every single intact JSON object from every single heap dump and stores anything that looks interesting in a PostgreSQL database, ignoring duplicates.</p><p>I might be missing stuff. For example, I'm only looking for JSON objects now, but I had previously found <a href="https://micahflee.com/despite-misleading-marketing-israeli-company-telemessage-used-by-trump-officials-can-access-plaintext-chat-logs/#private-key-material" rel="noreferrer">private keys</a> in the heap dumps. But with just storing interesting-looking JSON, the database ends up taking up 2.7 GB of space.</p><h2 id="statistics">Statistics</h2><p>Here are some numbers I've pulled from my database. Please take them all with a grain of salt for the following reasons:</p><ul><li>The dataset is not a representative sample of the messages passing through TeleMessage's archive server. Rather, it's what happened to be saved in heap dumps on one specific Sunday.</li><li>While I removed millions of duplicate JSON objects while building the database, there are still a lot of duplicates. Sometimes the same message is repeated multiple times, but the JSON objects are slightly different, like because two different people in the same group archived the same message.</li><li>I've made some value judgements that appear in the data. For example, some messages include metadata making it clear they're encrypted. Others look encrypted (because the text field is Base64 encoded), but are missing metadata saying one way or the other. I just mark those as encrypted, because they probably are, but I could be wrong.</li></ul><p>That said, here's what I've found:</p><ul><li>60,012 messages.<ul><li>36,388 of the messages are plaintext, and 23,624 are encrypted.</li><li>1,079 of the message include full attachments (like images, videos, PDFs, contact files, etc.) that are actually part of the dataset. But of those, only 50 of messages are in plaintext. I can, however, actually open and view those plaintext attachments.</li><li>Most messages have a <code>subject</code> field that's something like, "WhatsApp message from X to Y." Based on these subjects:<ul><li>37,753 are WhatsApp messages.</li><li>2,549 are Telegram messages.</li><li>455 are SMS messages.</li><li>141 are Signal messages.</li><li>95 are something called "App Messages."</li><li>26 are MMS messages.</li><li>26 are WeChat messages.</li><li>16 are voice calls logs.</li><li>11,254 are missing <code>subject</code> fields.</li></ul></li></ul></li><li>3,501 group chats, the vast majority of which are WhatsApp.<ul><li>At least 2,034 are WhatsApp groups.</li><li>At least 578 are SMS groups.</li><li>At least 256 are Telegram groups.</li><li>At least 26 are Signal groups.</li><li>At least 10 are WeChat groups.</li><li>I'm not sure about the other ~600 groups, though it's possible to determine by manually looking at the messages associated with them.</li><li>There are also plenty of individual messages that are clearly part of a group chat, but that didn't include JSON metadata related to it, so they're not categorized as groups, even though they are.</li></ul></li><li>44,503 users. These are either senders or recipients of messages.<ul><li>At least 25,792 of them use phone numbers as the identifier.</li><li>At least 31 of them use email addresses, and at least 391 look like they use usernames.</li><li>I'm not sure about another 18,289 of them, but I think most of them are also phone numbers.</li><li>17,377 of them include first and/or last names, too.</li></ul></li></ul><h2 id="telemessage-explorer">TeleMessage Explorer</h2><p>As I mentioned, I've been programming a tool to make researching this dataset easier. It's called TeleMessage Explore, and I'm going to release it as open source soon. <a href="#/portal/signup" rel="noreferrer">Subscribe to my newsletter</a> if you're interested in updates.</p><p>Since DDoSecrets is only distributing the TeleMessage dataset to journalists and researchers, this tool will only be useful to a small number of people, but I hope that it will make it <em>so much easier</em> for everyone who does have access to find good stories.</p><h2 id="hundreds-of-telemessage-customers">Hundreds of TeleMessage customers</h2><p>While sifting through thousands of different JSON objects, I came across what appears to be an OAuth2 validation object. This is data related to someone signing into TeleMessage using a Single Sign-On service.</p><p>Here's a validation object with info from someone at JP Morgan. I've redacted his email address and phone number, but I have them.</p><pre><code>{
    "validationData": {
        "reason": "OK",
        "validated": true
    },
    "enhancementData": {
        "data": ["==redacted=="],
        "email": "==redacted==@jpmorgan.com",
        "userName": "==redacted==",
        "shortCodes": [],
        "subUserIds": [],
        "activeIdentityProviderWithParams": {
            "activeIdentityProvider": "MICROSOFT",
            "identityProviderParams": {
                "NONCE": {"value": "", "predefined": true},
                "SCOPE": {"value": "JPMC:URI:RS-108400-153785-TMAdminConsoleSSOProd-PROD/openid", "predefined": false},
                "AUDIENCE": {"value": "JPMC:URI:RS-108400-153785-TMAdminConsoleSSOProd-PROD", "predefined": false},
                "CLIENT_ID": {"value": "PC-108400-SID-340413-PROD", "predefined": false},
                "TOKEN_ENDPOINT": {"value": "https://idag2.jpmorganchase.com/adfs/oauth2/token", "predefined": false},
                "TTL_IN_MINUTES": {"value": "30", "predefined": false},
                "VALIDATION_URL": {"value": "https://auth-service-charlie.kapi.telemessage.com/oidc/validate", "predefined": true},
                "EXPIRATION_DAYS": {"value": "0", "predefined": false},
                "METADATA_DOCUMENT": {"value": "https://idag2.jpmorganchase.com/adfs/.well-known/openid-configuration", "predefined": false},
                "TOKEN_EXCHANGE_URL": {"value": "https://auth-service-charlie.kapi.telemessage.com/oidc/exchangeAndValidateToken", "predefined": true},
                "OAUTH2_RESPONSE_TYPES": {"value": "code", "predefined": false},
                "AUTHORIZATION_ENDPOINT": {"value": "https://idag2.jpmorganchase.com/adfs/oauth2/authorize", "predefined": false}
            }
        }
    }
}</code></pre><p>When looking for more of these objects, I found that some don't include an <code>activeIdentityProvider</code>, like this one. This person works for Scotiabank:</p><pre><code>{
    "validationData": {
        "reason": "OK",
        "validated": true
    },
    "enhancementData": {
        "data": [
            "==redacted=="
        ],
        "email": "==redacted==@scotiabank.com",
        "userName": "==redacted==",
        "shortCodes": [],
        "subUserIds": [],
        "activeIdentityProviderWithParams": {
            "activeIdentityProvider": "NONE",
            "identityProviderParams": {}
        }
    }
}</code></pre><p>In all, I found 2,545 similar validation objects. I think it's likely that every email address in a validation object belongs to someone working for a TeleMessage customer.</p><p>Following is a list of domain names, along with the number of email addresses associated with those domains, in alphabetic order.</p><p>(It starts with a16z.com, which is the site for tech venture capitalist firm Andreessen Horowitz. This firm is owned by Silicon Valley billionaire and prominent reactionary Trump supporter Marc Andreessen.)</p><ul><li>a16z.com (3 emails)</li><li>abgadvisory.com (2 emails)</li><li>aipgp.com (4 emails)</li><li>alphawaveglobal.com (4 emails)</li><li>alternasecurities.com (1 email)</li><li>amcgroup.com (1 email)</li><li>amerexenergy.com (1 email)</li><li>aminagroup.com (3 emails)</li><li>amius.com (6 emails)</li><li>apg-am.com (1 email)</li><li>apg-am.hk (9 emails)</li><li>apg-am.sg (1 email)</li><li>aramcotrading.us (12 emails)</li><li>ardian.com (1 email)</li><li>aresmgmt.com (2 emails)</li><li>arringtoncapital.com (1 email)</li><li>aviorcapital.co.uk (1 email)</li><li>aviorcapital.us (1 email)</li><li>awincubation.com (2 emails)</li><li>axiuminfra.com (1 email)</li><li>b2c2.com (2 emails)</li><li>bainbridgefs.com (1 email)</li><li>ballestasgroup.com (1 email)</li><li>bbva.com (3 emails)</li><li>bgcg.com (2 emails)</li><li>biremecapital.com (1 email)</li><li>bisoncapital.com (1 email)</li><li>bitbuy.ca (1 email)</li><li>bitkraft.vc (2 emails)</li><li>blockchaincapital.com (1 email)</li><li>boltonglobal.com (5 emails)</li><li>borealcm.com (4 emails)</li><li>bradescobank.com (8 emails)</li><li>brevanhoward.com (2 emails)</li><li>br.scotiabank.com (1 email)</li><li>bulltick.com (1 email)</li><li>burlinv.com (1 email)</li><li>cantor.com (6 emails)</li><li>cantor.co.uk (1 email)</li><li>cbam.coinbase.com (1 email)</li><li>cbp.dhs.gov (26 emails)</li><li>cercano.asia (4 emails)</li><li>cibc.com (6 emails)</li><li>clarksons.com (4 emails)</li><li>cmcmarkets.com (1 email)</li><li>coinbase.com (20 emails)</li><li>consultant.kkr.com (1 email)</li><li>conti.com (1 email)</li><li>contractor101.co.uk (1 email)</li><li>contrariancapital.com (1 email)</li><li>crownagentsbank.com (5 emails)</li><li>dbank.co.il (3 emails)</li><li>dc.gov (30 emails)</li><li>dfc.gov (10 emails)</li><li>digitalbridge.com (1 email)</li><li>directhedge.com (1 email)</li><li>eastdilsecured.com (1 email)</li><li>ecor1cap.com (4 emails)</li><li>efhutton.com (1 email)</li><li>eni.com (13 emails)</li><li>exalogi.com (1 email)</li><li>falconcommoditymarkets.com (2 emails)</li><li>fibi.co.il (2 emails)</li><li>franklintempleton.com (1 email)</li><li>freightinvestor.ae (1 email)</li><li>freightinvestor.com (5 emails)</li><li>galaxydigital.io (7 emails)</li><li>gentrustwm.com (2 emails)</li><li>gfigroup.com.sg (1 email)</li><li>gmail.com (6 emails)</li><li>godspeedcm.com (1 email)</li><li>golubcapital.com (1 email)</li><li>govcapsecurities.com (1 email)</li><li>gtbankuk.com (1 email)</li><li>gunvorgroup.com (1 email)</li><li>hack-vc.com (4 emails)</li><li>hbluk.com (4 emails)</li><li>hedgepointglobal.com (1 email)</li><li>hiddenroad.com (8 emails)</li><li>hnwag.com (1 email)</li><li>hudson-trading.com (1 email)</li><li>hummerfas.com (1 email)</li><li>icap.com (3 emails)</li><li>icap.com.sg (1 email)</li><li>interactivebrokers.com (3 emails)</li><li>intercamus.com (1 email)</li><li>intercourtage.com (1 email)</li><li>investamericap.com (1 email)</li><li>itaubba.eu (1 email)</li><li>itau.ch (1 email)</li><li>jefferies.com (62 emails)</li><li>jpmchase.com (1 email)</li><li>jpmorgan.com (21 emails)</li><li>kkr.com (31 emails)</li><li>larrainvial.com (2 emails)</li><li>lasallegroupllc.com (3 emails)</li><li>lcatterton.com (8 emails)</li><li>marathonpetroleum.com (1 email)</li><li>marexfp.com (1 email)</li><li>maximcapitalgroup.com (1 email)</li><li>mbcfrance.com (1 email)</li><li>mbcl.com (2 emails)</li><li>mcquilling-energy.com (4 emails)</li><li>miraeasset.co.id (3 emails)</li><li>mitsui.com (1 email)</li><li>mlp.com (7 emails)</li><li>morganstanley.com (17 emails)</li><li>muzinich.com (1 email)</li><li>mvfp.net (1 email)</li><li>nebari.com (1 email)</li><li>nice.com (1 email)</li><li>nirbhaucorp.com (3 emails)</li><li>northisland.ventures (2 emails)</li><li>nuveen.com (1 email)</li><li>nuveenglobal.com (4 emails)</li><li>nycapmarkets.com (1 email)</li><li>opco.co.il (1 email)</li><li>p66.com (7 emails)</li><li>panteracapital.com (1 email)</li><li>paradigm.xyz (4 emails)</li><li>pa.scotiabank.com (48 emails)</li><li>petrobras.com (5 emails)</li><li>petrobras.com.br (1 email)</li><li>pimco.com (1 email)</li><li>principal.com (1 email)</li><li>privatewealthadvisorsinc.com (1 email)</li><li>psc.com (1 email)</li><li>pvm.co.uk (10 emails)</li><li>pwafamilyoffice.com (1 email)</li><li>reynoldschannel.com (1 email)</li><li>rhbgroup.com (5 emails)</li><li>ribbitcap.com (1 email)</li><li>rjobrien.com (1 email)</li><li>rmb.co.uk (2 emails)</li><li>rmbsecurities.com (1 email)</li><li>rohrpwm.com (1 email)</li><li>rsgcorp.com (1 email)</li><li>safra.com (13 emails)</li><li>scotiabank.cl (34 emails)</li><li>scotiabankcolpatria.com (71 emails)</li><li>scotiabank.com (126 emails)</li><li>scotiabank.com.mx (164 emails)</li><li>scotiabank.com.pe (45 emails)</li><li>scotiacb.com.mx (1 email)</li><li>scotiawealth.com (6 emails)</li><li>scotiawealth.com.mx (5 emails)</li><li>scsotc.com (2 emails)</li><li>seaportglobal.com (1 email)</li><li>searleco.com (1 email)</li><li>seba.swiss (1 email)</li><li>senatorlp.com (1 email)</li><li>sequencefinancialspecialists.com (1 email)</li><li>sg.pimco.com (1 email)</li><li>smarsh.com (4 emails)</li><li>smbcgroup.com (2 emails)</li><li>soteriasolutions.us (15 emails)</li><li>sperrycapital.com (1 email)</li><li>standardbank.co.za (6 emails)</li><li>statetrust.com (1 email)</li><li>steadview.com (1 email)</li><li>sunglobal.co.uk (1 email)</li><li>sunmountaincapital.com (1 email)</li><li>tcv.com (2 emails)</li><li>telemessage2020.onmicrosoft.com (9 emails)</li><li>telemessage.com (13 emails)</li><li>thestrategicfinancial.com (1 email)</li><li>tigerglobal.com (2 emails)</li><li>tm.com (4 emails)</li><li>totalenergies.com (92 emails)</li><li>tower-research.com (1 email)</li><li>tpicap.com (6 emails)</li><li>traditionasia.com (1 email)</li><li>tradition.com (4 emails)</li><li>tridentotc.com (1 email)</li><li>tullettprebon.co.jp (1 email)</li><li>tullettprebon.com (10 emails)</li><li>tyruscap.mc (1 email)</li><li>ubauk.com (3 emails)</li><li>uk.pimco.com (1 email)</li><li>united-icap.com (6 emails)</li><li>us.icap.com (1 email)</li><li>usss.dhs.gov (2 emails)</li><li>uyanapartners.com (1 email)</li><li>valley.com (8 emails)</li><li>vistaequitypartners.com (4 emails)</li><li>vitol.com (8 emails)</li><li>vlmsofts.com (1 email)</li><li>who.eop.gov (1 email)</li><li>williamblair.com (5 emails)</li><li>wonder.fi (1 email)</li><li>worldquant.com (1 email)</li></ul><p><em>If you found this interesting, </em><a href="https://micahflee.com/despite-misleading-marketing-israeli-company-telemessage-used-by-trump-officials-can-access-plaintext-chat-logs/#/portal/signup" rel="noreferrer"><em>subscribe</em></a><em> to get these posts emailed directly to your inbox. If you want to support my work, considering becoming a paid supporter.</em></p>
                    
                </section>

                    

                
            </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Data breach exposes 184M passwords, likely captured by malware (131 pts)]]></title>
            <link>https://www.zdnet.com/article/massive-data-breach-exposes-184-million-passwords-for-google-microsoft-facebook-and-more/</link>
            <guid>44099008</guid>
            <pubDate>Mon, 26 May 2025 16:37:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.zdnet.com/article/massive-data-breach-exposes-184-million-passwords-for-google-microsoft-facebook-and-more/">https://www.zdnet.com/article/massive-data-breach-exposes-184-million-passwords-for-google-microsoft-facebook-and-more/</a>, See on <a href="https://news.ycombinator.com/item?id=44099008">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure><div><picture><source media="(max-width: 767px)" srcset="https://www.zdnet.com/a/img/resize/2fcf7434c0b644d136b37849c2032ca12ba9d247/2025/05/23/e669b1a4-6612-4c2a-adbc-bf3b70a8f6f9/gettyimages-1411799666.jpg?auto=webp&amp;width=768" alt="data breach concept"><source media="(max-width: 1023px)" srcset="https://www.zdnet.com/a/img/resize/f1189ea3e1dd85500849b507ee31656c7c8b9000/2025/05/23/e669b1a4-6612-4c2a-adbc-bf3b70a8f6f9/gettyimages-1411799666.jpg?auto=webp&amp;width=1024" alt="data breach concept"><source media="(max-width: 1440px)" srcset="https://www.zdnet.com/a/img/resize/7bb565b4489f154dd822e353edf6fe5c8871d69f/2025/05/23/e669b1a4-6612-4c2a-adbc-bf3b70a8f6f9/gettyimages-1411799666.jpg?auto=webp&amp;width=1280" alt="data breach concept"> <img src="https://www.zdnet.com/a/img/resize/7bb565b4489f154dd822e353edf6fe5c8871d69f/2025/05/23/e669b1a4-6612-4c2a-adbc-bf3b70a8f6f9/gettyimages-1411799666.jpg?auto=webp&amp;width=1280" alt="data breach concept" width="1280" height="719.5495885664791" fetchpriority="low"></picture></div> <figcaption> <span>JuSun/Getty Images</span></figcaption></figure><p>Yet another data breach has exposed passwords and other sensitive information – but this one is a whopper.</p><p>Cybersecurity researcher <a href="https://www.vpnmentor.com/author/jeremiah-fowler/#link=%7B%22role%22:%22standard%22,%22href%22:%22https://www.vpnmentor.com/author/jeremiah-fowler/%22,%22target%22:%22%22,%22absolute%22:%22%22,%22linkText%22:%22Jeremiah%20Fowler%22%7D" target="_blank" rel="noopener nofollow">Jeremiah Fowler</a> revealed his discovery of a massive online database containing more than 184 million unique account credentials, in a <a href="https://www.websiteplanet.com/news/infostealer-breach-report/#link=%7B%22role%22:%22standard%22,%22href%22:%22https://www.websiteplanet.com/news/infostealer-breach-report/%22,%22target%22:%22%22,%22absolute%22:%22%22,%22linkText%22:%22report%20published%20yesterday%22%7D" target="_blank" rel="noopener nofollow">report published Thursday</a>. Usernames, passwords, emails, and URLs for a host of applications and websites, including Google, Microsoft, Apple, Facebook, Instagram, and Snapchat, among others, were stored in a file. The database also contained credentials for bank and financial accounts, health platforms, and government portals. </p><p><strong>Also:&nbsp;<a href="https://www.zdnet.com/article/best-password-manager/">The best password managers of 2025: Expert tested</a></strong></p><p>The problem? The file was unencrypted. No password protection. No security. Just a plain text file with millions of sensitive pieces of data. </p><p>Based on his analysis, Fowler determined the data was captured by some kind of infostealer malware. A popular tool used by cybercriminals, an infostealer is designed to grab usernames, passwords, and other sensitive data from breached sites and servers. Once the criminals get their hands on the data, they can use it to launch their own attacks or peddle the information on the dark web.</p><p>After finding the database, Fowler contacted the hosting provider, which removed it from public access. Since the provider would not disclose the name of the file's owner, Fowler said he didn't know if the database was created legitimately and then accidentally exposed or intentionally used for malicious reasons. </p><p>To check on the validity of the information, Fowler emailed many of the people listed in the file and told them that he was researching a data breach. Several of the individuals confirmed that the records contained valid account passwords and other data. </p><p><strong>Also:&nbsp;<a href="https://www.zdnet.com/article/oversharing-online-5-ways-it-makes-you-an-easy-target-for-cybercriminals/">Oversharing online? 5 ways it makes you an easy target for cybercriminals</a></strong></p><p>Though the person or people behind the database and exposure are obviously to blame for this incident, users also share some of the responsibility. </p><p>"Many people unknowingly treat their email accounts like free cloud storage and keep years' worth of sensitive documents, such as tax forms, medical records, contracts, and passwords, without considering how sensitive they are," Fowler said. "This could create serious security and privacy risks if criminals were to gain access to thousands or even millions of email accounts."</p><!----><p>In his report, the researcher highlighted the types of threats faced by people whose data is exposed in such breaches. </p><ul><li><p><strong>Credential stuffing attacks</strong> – People who use the same passwords on multiple accounts open themselves up to compromise. Hackers deploy automated credential stuffing scripts to try out different email and password combinations on thousands of different sites. The same password exposed on one site can then easily be exposed on others. 		</p></li><li><p><strong>Account takeovers</strong> - Cybercriminals who gain access to usernames, passwords, and other private data are able to take over an account. They can steal your identity, commit financial fraud, and run other types of scams, not just on you but on family, friends, and other contacts. 		</p></li></ul><ul><li><p><strong>Ransomware and corporate espionage</strong> – Fowler said he discovered many business credentials in the leaked data. The attackers can exploit this information to steal business records, launch ransomware attacks, and even commit corporate espionage. 		</p></li><li><p><strong>Attacks against state and government agencies </strong>– Fowler also saw several government accounts across different countries. An attacker armed with this information can target state and federal agencies. 		</p></li><li><p><strong>Phishing and social engineering</strong> – Leaked emails provide cybercriminals with a history of someone's conversations and contacts. That information can then be used in targeted phishing attacks against the account owner as well as people they know. 		</p></li></ul><p>How can you protect your own confidential data from being exposed in a breach? Though no perfect solution exists, Fowler shared the following tips in his report: </p><h2> 1. Change your passwords each year </h2><p>Many people have only one email address connected to multiple accounts, which means they can't easily change it. But you can change your password, at least periodically. Doing so is a good idea if you think your old password may have been compromised in a breach. </p><h2> 2. Use complex and unique passwords </h2><p>Beyond using <a href="https://www.zdnet.com/article/7-password-rules-security-experts-live-by-in-2025-the-last-one-might-surprise-you/">strong passwords</a>, avoid using the same one for multiple accounts. </p><h2> 3. Consider a password manager </h2><p>A <a href="https://www.zdnet.com/article/best-password-manager/">password manager</a> can take on the challenging role of creating, storing, and applying strong and unique passwords for each account. As Fowler pointed out, there is a risk in using a password manager. If your master password is ever stolen or compromised, a cybercriminal now has the key to unlock all your passwords. But that brings us to the next tip. </p><p><strong>Also: <a href="https://www.zdnet.com/article/hackers-stole-this-engineers-1password-database-could-it-happen-to-you/">Hackers stole this engineer's 1Password database. Could it happen to you?</a></strong></p><h2> 4. Use multi-factor authentication </h2><p><a href="https://www.zdnet.com/article/why-multi-factor-authentication-is-absolutely-essential-in-2025/">MFA</a> offers a second level of authentication, typically through a code, authenticator app, or <a href="https://www.zdnet.com/article/best-security-key/">security key</a>. If your password is ever breached, a cybercriminal can't access your account without that code. Make sure you use MFA on all available accounts, but especially ones for bank and financial services and password managers. </p><p><strong>Also:&nbsp;</strong><a href="https://www.zdnet.com/article/10-passkey-survival-tips-prepare-for-your-passwordless-future-now/"><strong>10 passkey survival tips: Prepare for your passwordless future now</strong></a></p><h2> 5. Check if your credentials have been leaked </h2><p>Services like <a href="https://haveibeenpwned.com/" target="_blank" rel="noopener nofollow">HaveIBeenPwned</a> will tell you if your email has popped up in any known breaches. If so, then make sure you change the password for the affected accounts. </p><h2> 6. Monitor the use of your accounts </h2><p>Some websites and services will alert you to suspicious login activity and other atypical behavior, just like your credit card company alerts you to potentially suspect transactions. Avail yourself of this feature whenever possible. </p><h2> 7. Use good security software </h2><p>The right security software can detect and eliminate infostealer malware and other known threats. Be sure to <a href="https://www.zdnet.com/article/why-delaying-software-updates-could-cost-you-more-than-you-think/">update your software</a> with the latest definitions to defend yourself against new variants. </p><p><em>Get the morning's top stories in your inbox each day with our&nbsp;</em><a href="https://www.zdnet.com/newsletters/"><em><strong>Tech Today newsletter</strong></em></a><em><strong>.</strong></em></p><div id="pinbox-656e4fed-3457-4713-94c8-e03a2db0d92d"><h4>Featured</h4> <!---->  </div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hacker News now runs on top of Common Lisp (538 pts)]]></title>
            <link>https://lisp-journey.gitlab.io/blog/hacker-news-now-runs-on-top-of-common-lisp/</link>
            <guid>44099006</guid>
            <pubDate>Mon, 26 May 2025 16:37:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lisp-journey.gitlab.io/blog/hacker-news-now-runs-on-top-of-common-lisp/">https://lisp-journey.gitlab.io/blog/hacker-news-now-runs-on-top-of-common-lisp/</a>, See on <a href="https://news.ycombinator.com/item?id=44099006">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://news.ycombinator.com/">Hacker News</a> was written in the
<a href="http://arclanguage.org/">Arc</a> lisp dialect, a dialect created by Paul
Graham. Arc was implemented on top of Racket, but that has now
changed. HN runs on top of SBCL since a few months.</p>

<p>But why? For performance reasons.</p>

<blockquote>
<p>I recently noticed that Hacker News no longer uses paging for long
threads. In the past, when a discussion grew large, we had to click
“More” to load the next page of comments, and dang would
occasionally post helpful tips to remind us about this feature.
Was there an announcement regarding this change? Has anyone else still seen paging recently? I’d love to know more details—especially the technical aspects or considerations that went into the decision.</p>
</blockquote>

<p>Answer:</p>

<blockquote>
<p>It’s because Clarc is finally out.</p>
</blockquote>

<p><a href="https://news.ycombinator.com/item?id=41679215">dang, Sept. 2024</a></p>

<blockquote>
<p>[Clarc] is much faster and also will easily let HN run on multiple cores. It’s been in the works for years, mainly because I rarely find time to work on it, but it’s all pretty close to done.</p>
</blockquote>

<p><a href="https://news.ycombinator.com/item?id=32597291">dang, 2022</a></p>

<p>How it’s done:</p>

<blockquote>
<p>there’s now an Arc-to-JS called Lilt, and an Arc-to-Common Lisp called Clarc. In order to make those easier to develop, we reworked the lower depths of the existing Arc implementation to build Arc up in stages. The bottom one is called arc0, then arc1 is written in arc0, and arc2 in arc1. The one at the top (arc2, I think) is full Arc. This isn’t novel, but it makes reimplementation easier since you pack as much as possible in the later stages, and only arc0 needs to be written in the underlying system (Racket, JS, or CL).</p>
</blockquote>

<p><a href="https://news.ycombinator.com/item?id=21550123">dang, 2019</a></p>

<p>But Clarc’s code isn’t released:</p>

<blockquote>
<p>Much of the HN codebase consists of anti-abuse measures that would stop working if people knew about them. Unfortunately.
separating out the secret parts would by now be a lot of work. The time to do it will be if and when we eventually release the alternative Arc implementations we’ve been working on.</p>
</blockquote>

<p><a href="https://news.ycombinator.com/item?id=21546438">https://news.ycombinator.com/item?id=21546438</a></p>

<p>Congrats for the successful “splash-free” transition though.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I think it's time to give Nix a chance (129 pts)]]></title>
            <link>https://maych.in/blog/its-time-to-give-nix-a-chance/</link>
            <guid>44098605</guid>
            <pubDate>Mon, 26 May 2025 15:56:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://maych.in/blog/its-time-to-give-nix-a-chance/">https://maych.in/blog/its-time-to-give-nix-a-chance/</a>, See on <a href="https://news.ycombinator.com/item?id=44098605">Hacker News</a></p>
<div id="readability-page-1" class="page"><article> <p>The modern developer tooling ecosystem has exploded with choices, leading to frustrating scenarios where some piece of code builds perfectly on someone’s system, runs flawlessly in production, but mysteriously fails to build for you and you have absolutely no idea why. You’re left debugging with no clear direction—perhaps it’s a missing system dependency, a subtly different library version, or some environment variable that exists somewhere in the void, and nowhere else.</p>
<p>If this sounds familiar, you too might be experiencing the fundamental problem that Nix was designed to solve: the lack of true reproducibility in software development.</p>
<p>Despite being around for about two decades, Nix has largely flown under the radar of mainstream development. Most developers have heard of it in passing—often described as “that functional package manager with a steep learning curve” or “the thing NixOS uses”—dismissing it as academic or overly complex. I believe this perception is getting increasingly outdated, Nix <em>deserves</em> a chance.</p>
<p>First, let’s get this out of the way: Nix still <em>does</em> have a steep learning curve. It requires learning a new functional programming language, understanding unfamiliar concepts like derivations and the <code>/nix/store</code>, and rethinking how package management works. The documentation can be dense, scattered, and the error messages are outright unhelpful in some cases. You’re essentially learning an entirely different approach to software deployment and environment management.</p>
<p>But here’s the thing—as of this post, the tooling around Nix has matured significantly, and the problems Nix solves have only become more pressing. If you’ve ever lost hours debugging environment differences, juggled multiple version managers, or struggled with reproducible builds, Nix addresses these pain points at the architectural level rather than through workarounds.</p>
<p>And I am here to argue that despite the quirks and learning investment, Nix’s benefits are compelling enough to warrant your time. The question isn’t whether Nix, as a tool and a language, is complex—<em>it is</em>. The question is whether the problems it solves are worth learning something genuinely different. In this post, you can expect a brief introduction to what Nix, the tool, can do for you and how it may be worth giving a try right now.</p>
<h2 id="why-traditional-package-management-breaks-down">Why Traditional Package Management Breaks Down</h2>
<p>Most package managers work by installing software into shared system locations. Install a specific version of Python, and it goes straight into <code>/usr/bin/python</code>. Need a different version of Python for another project? You either overwrite the first installation or create complex alternatives systems that are painful to manage.</p>
<p>This shared-state approach creates inevitable conflicts:</p>
<ul>
<li>Your frontend application needs the latest version of NodeJS, but a legacy service requires an older one.</li>
<li>Your applications depend on conflicting versions of OpenSSL libraries.</li>
<li>Teams use different operating systems with slightly different utilities. Heck, even same versions of tools, like <code>sed</code>, are functionally different across Linux and MacOS.</li>
</ul>
<p>In such cases, one may say that version managers help with language runtimes. But what about system libraries, databases, or compiled tools? You end up juggling multiple such tools, each with different commands and behaviors.</p>
<p>Orchestration and containerized solutions like Docker and Kubernetes help, but they introduce their own complexities and performance downgrades. More importantly, Docker containers themselves aren’t reproducible—running <code>apt-get update</code> or <code>pip install requests</code> at different times can yield different results, even with the same Dockerfile. And frankly, no one really needs Kubernetes, they just have it because everyone and their grandma has it. I digress, that’s a topic for another day.</p>
<p>This is where Nix comes in.</p>
<h2 id="and-how-nix-solves-the-problem">…And How Nix Solves the Problem</h2>
<p>Nix makes no assumptions about the global state of your system and takes a fundamentally different approach. Instead of installing packages into shared locations where they can conflict, everything goes into the immutable <code>/nix/store</code>, with each package getting a unique directory based on a cryptographic hash of its build inputs.</p>
<div><figure><pre data-language="plaintext"><code><div><p><span>/nix/store/2v66xkgfmdipzpwgl813n4mqgck6w3fd-nodejs-22.14.0/</span></p></div><div><p><span>/nix/store/2znhzcp5ran8q5mzyqgz6lxi3a56rgva-nodejs-20.18.1/</span></p></div><div><p><span>/nix/store/4rk85a5rsladhcc3ffpnx2kwglvs0i-nodejs-18.17.0/</span></p></div></code></pre></figure></div>
<p>These hashes are computed using SHA-256 over the package’s complete build dependency graph—source code, compiler version, build flags, dependencies, even the build script itself. Change any input, and you get a different hash, which means a completely separate package in the store.</p>
<h3 id="cryptographic-guarantees-and-safety">Cryptographic Guarantees and Safety</h3>
<p>Technically, you might state that hash collisions are possible with any cryptographic hash function, and you won’t be wrong here. However, the probability of a SHA-256 collision is approximately one in 2<sup>128</sup>—or roughly <strong><em>one in 340 undecillion!</em></strong>. For perspective, this is far less likely than being struck by lightning while simultaneously winning the lottery multiple times.</p>
<p>More importantly, Nix implements robust failsafe mechanisms. It uses NAR (Nix Archive) hashes, a deterministic format that canonicalizes source trees by normalizing timestamps, file permissions, and directory ordering. Unlike traditional TAR archives which include non-deterministic metadata, NAR hashes ensure identical content always produces identical hashes. Nix validates packages using both the NAR hash and additional metadata like Git revision hashes, providing multiple layers of integrity verification. This has prevented an issue in the past, where GitHub changed the hash format of their archives, causing systems and services depending on these hashes to fail.<span> <label for="sidenote-1" data-footnote-id="1" aria-label="Sidenote 1" tabindex="0" role="button" aria-expanded="false"></label>  <span role="note" aria-label="Sidenote 1"> <label for="sidenote-1" data-footnote-id="1"></label> <a href="https://flox.dev/blog/hash-collision/">Hash Collision</a> - Flox </span> </span></p>
<p>This isolation means multiple versions coexist without conflicts. Your React app gets Node.js 18, the legacy API keeps Node.js 16, and they exist in completely separate filesystem namespaces.</p>
<p>Now, I’m in no way trying to proliferate a <a href="https://xkcd.com/927/">15<sup>th</sup> competing standard</a> here that solves all package management woes for you and your grandma. Nix is far more capable at things than just that, and I’ll tell you why.</p>
<h2 id="reproducible-environments-with-flakes">Reproducible Environments with Flakes</h2>
<p>Modern Nix organizes projects using “flakes”<span> <label for="sidenote-2" data-footnote-id="2" aria-label="Sidenote 2" tabindex="0" role="button" aria-expanded="false"></label>  <span role="note" aria-label="Sidenote 2"> <label for="sidenote-2" data-footnote-id="2"></label> <a href="https://zero-to-nix.com/concepts/flakes/">Nix flakes</a> - Zero to Nix </span> </span>—standardized specifications that pin every dependency with cryptographic precision. Think of them as <code>package.json</code> or <code>Cargo.toml</code>, but with mathematical guarantees that every single build will result in the same derivation, no matter when, where, or how you build it.</p>
<p>While flakes have been “experimental” for quite a long time now, it has truly pushed Nix one step closer to complete reproducibility and shared development environments. For clarity, <a href="https://determinate.systems/posts/experimental-does-not-mean-unstable/">Experimental does not mean unstable</a>, as flakes have practically been “stable” since 2021, despite the experimental tag. Here’s what a simple flake might look like:</p>
<div><figure><pre data-language="nix"><code><div><p><span># This is a nix flake environment running</span></p></div><div><p><span># Python 3.11, with pandas and numpy installed.</span></p></div><div><p><span># The environment can be accessed by running:</span></p></div><div><p><span>#    $ nix develop</span></p></div><div><p><span>{</span></p></div><div><p><span>  </span><span>description</span><span> </span><span>=</span><span> </span><span>"Example Python Data Analysis Environment"</span><span>;</span></p></div><div><p><span>  </span><span>inputs</span><span> </span><span>=</span><span> {</span></p></div><div><p><span>    </span><span>nixpkgs</span><span>.</span><span>url</span><span> </span><span>=</span><span> </span><span>"github:nixos/nixpkgs/nixos-unstable"</span><span>;</span></p></div><div><p><span><span>  </span></span><span>};</span></p></div><div><p><span>  </span><span>outputs</span><span> </span><span>=</span><span> { self</span><span>,</span><span> nixpkgs }: {</span></p></div><div><p><span>    </span><span>devShells</span><span>.</span><span>x86_64-linux</span><span>.</span><span>default</span><span> </span><span>=</span></p></div><div><p><span>      </span><span>nixpkgs</span><span>.</span><span>legacyPackages</span><span>.</span><span>x86_64-linux</span><span>.</span><span>mkShell</span><span> {</span></p></div><div><p><span>        </span><span>buildInputs</span><span> </span><span>=</span><span> </span><span>with</span><span> </span><span>nixpkgs</span><span>.</span><span>legacyPackages</span><span>.</span><span>x86_64-linux</span><span>; [</span></p></div><div><p><span>          </span><span>python311</span></p></div><div><p><span>          </span><span>python311Packages</span><span>.</span><span>pandas</span></p></div><div><p><span>          </span><span>python311Packages</span><span>.</span><span>numpy</span></p></div><div><p><span><span>        </span></span><span>];</span></p></div><div><p><span><span>      </span></span><span>};</span></p></div><div><p><span><span>  </span></span><span>};</span></p></div><div><p><span>}</span></p></div></code></pre></figure></div>
<p>Let’s take this as an example. When you share this flake (along with the <code>flake.lock</code>) with someone and they build it, they get exactly—not approximately—the same environment as when you created the flake. The automatically generated <code>flake.lock</code><span> <label for="sidenote-3" data-footnote-id="3" aria-label="Sidenote 3" tabindex="0" role="button" aria-expanded="false"></label>  <span role="note" aria-label="Sidenote 3"> <label for="sidenote-3" data-footnote-id="3"></label> <a href="https://zero-to-nix.com/concepts/flakes/#lockfile">The <code>flake.lock</code> file</a> - Zero to Nix </span> </span> file contains cryptographic hashes for every dependency in the transitive closure. This means that building this same flake over and over again, even a few years down the line, will ideally still result in the exact same environment. Well, at least until the source of the software disappears from the face of the earth, <em>and</em> the Nix binary cache contracts bit-rot.</p>
<p>Also, these Nix flakes are backed by git and only include tracked files in builds, ensuring forgotten local files and hash changes cause immediate build failures rather than silent inconsistencies.</p>
<h2 id="running-packages-without-installing">Running Packages Without Installing</h2>
<p>Usually to run a package on any system you would need to install it, or use an AppImage, Flatpak, snap, you name it. But not with Nix. One of Nix’s most practical features is temporary tool access without system pollution:</p>
<div><figure><pre data-language="bash"><code><div><p><span># Run Node.js 20 without installing it</span></p></div><div><p><span>nix</span><span> </span><span>run</span><span> </span><span>nixpkgs#nodejs_20</span><span> </span><span>--</span><span> </span><span>--version</span></p></div><div><p><span># Get a temporary shell with multiple tools</span></p></div><div><p><span>nix</span><span> </span><span>shell</span><span> </span><span>nixpkgs#{imagemagick,ffmpeg}</span></p></div><div><p><span># Try software from any Git repository with flakes</span></p></div><div><p><span>nix</span><span> </span><span>shell</span><span> </span><span>github:DeterminateSystems/fh</span><span> </span><span>--</span><span> </span><span>fh</span><span> </span><span>--help</span></p></div></code></pre></figure></div>
<p>This eliminates tool accumulation while providing instant access to any software in the Nix ecosystem. When you exit the shell, the tools disappear from your environment (but stay in the <code>/nix/store</code> until it’s garbage collected).</p>
<h2 id="true-package-isolation">True Package Isolation</h2>
<p>Traditional package managers create a shared global namespace where conflicts are inevitable. Nix solves this architecturally by storing each package in <code>/nix/store/</code> with cryptographically unique paths. Multiple versions of the same package coexist without interference because they occupy completely separate filesystem locations.</p>
<div><figure><pre data-language="console"><code><div><p><span>$ ls /nix/store </span><span>|</span><span> </span><span>rg</span><span> </span><span>nodejs-.</span><span>\[</span><span>0</span><span>\-</span><span>9.</span><span>\]</span><span>+drv</span></p></div><div><p><span>/nix/store/2v66xkgfmdipzpwgl813n4mqgck6w3fd-nodejs-22.14.0.drv</span></p></div><div><p><span>/nix/store/2znhzcp5ran8q5mzyqgz6lxi3a56rgva-nodejs-20.18.1.drv</span></p></div><div><p><span>/nix/store/4rk85a5rsladhcc3ffpyfnx2kwglvs0i-nodejs-20.19.2.drv</span></p></div></code></pre></figure></div>
<p>This isolation extends beyond simple version conflicts. Each package includes its complete dependency tree in isolation, meaning you can run applications with entirely different versions of fundamental libraries like glibc simultaneously. The Nix store’s immutable design ensures that once built, packages never change, eliminating an entire class of “it worked yesterday” problems.</p>
<p>Simply put, each project can use a different Node.js version, present on the system, without conflicts. The hash <code>2v66xkgfmdipzpwgl813n4mqgck6w3fd</code> in this case encodes not just Node.js 22.14.0, but the exact glibc version, compiler flags, and every dependency used to build it. This can be confirmed by running a simple <code>nix-store --query</code> on both derivations:</p>
<div><figure><pre data-language="console"><code><div><p><span>$ nix-store --query --tree 2v66xkgfmdipzpwgl813n4mqgck6w3fd-nodejs-22.14.0.drv</span></p></div><div><p><span>/nix/store/2v66xkgfmdipzpwgl813n4mqgck6w3fd-nodejs-22.14.0.drv</span></p></div><div><p><span>├───/nix/store/shkw4qm9qcw5sc5n1k5jznc83ny02r39-default-builder.sh</span></p></div><div><p><span>├───/nix/store/vj1c3wf9c11a0qs6p3ymfvrnsdgsdcbq-source-stdenv.sh</span></p></div><div><p><span>├───/nix/store/cfp8jh04f3jfdcjskw2p64ri3w6njndm-bash-5.2p37.drv</span></p></div><div><p><span>│   ├───/nix/store/3jmwf7n7mdjk99lbwmznwkjvd5kwxlp4-glibc-2.40-66.drv [...]</span></p></div><div><p><span>...</span></p></div><div><p><span>$ nix-store --query --tree 2znhzcp5ran8q5mzyqgz6lxi3a56rgva-nodejs-20.18.1.drv</span></p></div><div><p><span>/nix/store/2znhzcp5ran8q5mzyqgz6lxi3a56rgva-nodejs-20.18.1.drv</span></p></div><div><p><span>├───/nix/store/v6x3cs394jgqfbi0a42pam708flxaphh-default-builder.sh</span></p></div><div><p><span>├───/nix/store/s63zivn27i8qv5cqiy8r5hf48r323qwa-bash-5.2p37.drv</span></p></div><div><p><span>│   ├───/nix/store/qhdvi3qcn60vrapyhsxxpbw0q63gmfz8-glibc-2.40-36.drv [...]</span></p></div><div><p><span>...</span></p></div></code></pre></figure></div>
<p>Package directory names correspond to cryptographic hashes that take into account all dependencies, build flags, and even compiler versions. This content-addressable storage means identical inputs always produce identical outputs, making builds truly reproducible across different machines and time periods.</p>
<h2 id="development-environment-excellence">Development Environment Excellence</h2>
<p>Beyond core benefits, Nix’s ecosystem provides sophisticated tooling for seamless workflows. With <code>nix-direnv</code><span> <label for="sidenote-4" data-footnote-id="4" aria-label="Sidenote 4" tabindex="0" role="button" aria-expanded="false"></label>  <span role="note" aria-label="Sidenote 4"> <label for="sidenote-4" data-footnote-id="4"></label> <a href="https://github.com/nix-community/nix-direnv"><code>nix-direnv</code></a> - GitHub </span> </span>, an extension to <code>direnv</code>, Nix flake environments activate automatically when changing directories:</p>

<p>Now <code>cd</code>ing into the directory automatically loads the Nix environment. The cross-platform consistency is particularly valuable—the same <code>flake.nix</code> that works on Linux works identically on macOS through <code>nix-darwin</code>, and WSL, eliminating platform-specific tooling differences.</p>
<p>And if you still don’t want to interact with or write Nix files directly, there’s tools like <a href="https://flox.dev/">Flox</a>, <a href="https://github.com/numtide/devshell">Devshell</a>, <a href="https://www.jetify.com/devbox">Devbox</a>, built on top of Nix that abstract the pain away.</p>
<h2 id="security-on-nix">Security on Nix</h2>
<p>Nix’s unique architecture provides security benefits that extend beyond reproducibility. The immutable nature of the Nix store and its departure from standard Linux filesystem conventions create inherent security advantages.</p>
<h3 id="immutable-package-store">Immutable Package Store</h3>
<p>Once built, packages in <code>/nix/store</code> cannot be modified. This prevents entire classes of attacks where malware modifies system binaries or libraries. Traditional package managers allow in-place updates that can be exploited, but Nix’s atomic model makes such attacks impossible.</p>
<h3 id="package-sandboxing-and-anti-tampering">Package Sandboxing and Anti-Tampering</h3>
<p>By design, anyone is free to contribute packages to the central Nix package repository. Although all packages there, not marked as <code>nonfree</code>, are built from source, where the source is downloaded before the build process, verified by hash, and only then processed.</p>
<p>By philosophy, Nix prevents uploading pre-built packages to <code>nixpkgs</code> and requires additional reviews before merging, thus ensuring an additional layer of safety at the expense of the latest package updates being a bit delayed—usually only by a couple of days. All built packages, on the official build infrastructure or locally, are sandboxed by default. None of the packages have internet access inside the build environment, meaning that all the dependencies must be resolved before the build runs.</p>
<p>Also, all packages are <em>required</em> to contain a hash of dependencies, ensuring that the build fails if the source or any dependencies are tampered with or poisoned.</p>
<p>In contrast, traditional package management systems rely heavily on trust. As an example, the AUR is a repository of community-contributed Arch packages where you’re trusting random maintainers. Anyone can upload a <code>PKGBUILD</code> that could download and execute arbitrary code during installation. While you can inspect the build script, many users install with <code>yay -S package</code> without review. Similarly, the Fedora RPM repositories which have pre-built binaries signed by maintainers, where you have to trust that the binary matches the claimed source code and the build environment wasn’t compromised.</p>
<h3 id="non-standard-filesystem-layout">Non-Standard Filesystem Layout</h3>
<p>Nix deliberately breaks from the Filesystem Hierarchy Standard (FHS). There’s no <code>/usr/bin</code> filled with system binaries, no <code>/lib</code> or <code>/usr/lib</code> containing shared libraries. This means malware designed for traditional Linux systems often fails because it cannot locate expected system components at standard paths.</p>
<h3 id="dynamic-linking-protection">Dynamic Linking Protection</h3>
<p>Ad-hoc binaries downloaded from the internet cannot run on NixOS without explicit configuration. Traditional Linux systems allow dynamically linked binaries to access system libraries through standard paths like <code>/lib64/ld-linux-x86-64.so.2</code>. On NixOS, these paths either don’t exist or point to controlled implementations. This prevents many categories of malicious binaries from executing.</p>
<h3 id="controlled-binary-execution">Controlled Binary Execution</h3>
<p>Although not impossible, to run external binaries, you need tools like <code>nix-ld</code><span> <label for="sidenote-5" data-footnote-id="5" aria-label="Sidenote 5" tabindex="0" role="button" aria-expanded="false"></label>  <span role="note" aria-label="Sidenote 5"> <label for="sidenote-5" data-footnote-id="5"></label> <a href="https://github.com/nix-community/nix-ld"><code>nix-ld</code></a> - GitHub </span> </span> which provides controlled access to a compatibility layer. This forced deliberation makes it much harder for malicious software to execute unnoticed.</p>
<p>While this isn’t “security through obscurity” (the design is well-documented), it does mean that common attack vectors simply don’t work in a Nix environment, providing defense-in-depth against malware targeting traditional Linux systems.</p>
<h2 id="performance-and-caching">Performance and Caching</h2>
<p>Nix provides performance advantages through sophisticated caching and deduplication. When each package is built, it is stored with its content-addressable path, meaning identical dependencies are built once and shared across all projects.</p>
<p>Binary caching eliminates most compilation time. Popular packages are pre-built and cached, so you typically download binaries rather than compiling from source. There are also services like <a href="https://cachix.org/">Cachix</a>, <a href="https://github.com/zhaofengli/attic">attic</a> that can host binary cache for you so you can push and cache the lesser known, or even your own Nix packages after building them once.</p>
<p>With cache, the first environment activation might take up to a few minutes to download dependencies, but subsequent activations are near-instantaneous. However, environment load times can be a trade-off. Directory changes that trigger environment loading through <code>nix-direnv</code> can take a few seconds depending on dependencies, as Nix maintains separate instances for each tool. But in most cases, everything will be seamless.</p>
<h2 id="when-nix-makes-sense-and-when-it-doesnt">When Nix Makes Sense (And When It Doesn’t)</h2>
<p>After reading this far, you might be wondering if Nix is right for your situation. But what I talked about in this post is barely scratching the surface of what Nix is capable of.</p>
<p>Nix provides the most value when:</p>
<ul>
<li><strong>Environment drift is costly</strong>: Financial services, healthcare, or any domain where debugging production issues has high stakes</li>
<li><strong>Onboarding takes days</strong>: Complex stacks with multiple databases, language runtimes, and system dependencies that require extensive setup documentation</li>
<li><strong>Cross-platform development</strong>: Teams mixing Linux, macOS, and WSL with different package managers and library versions</li>
<li><strong>Compliance requirements</strong>: Industries requiring reproducible builds for audit trails or regulatory compliance</li>
<li><strong>Research and experimentation</strong>: Academic computing, data science, or ML research where reproducing exact environments is critical</li>
<li><strong>Legacy system maintenance</strong>: Managing multiple versions of the same software for different clients or product versions</li>
</ul>
<p>Nix might be overkill if:</p>
<ul>
<li>You’re working on simple projects with minimal, standard dependencies</li>
<li>Your team already has smooth onboarding and deployment processes</li>
<li>Time-to-market pressure outweighs technical debt concerns</li>
<li>Your stack consists of well-containerized microservices with stable dependencies</li>
<li>Learning new tools would significantly slow current development velocity</li>
<li>You’re working solo on personal projects without collaboration needs</li>
</ul>
<h2 id="the-honest-drawbacks">The Honest Drawbacks</h2>
<p>I have been using Nix for about 8 years now. And while I would say that Nix is an indispensible part of my life at this point, I still do have some gripes with it and the occasional hurdles while explaining some concepts and philosophies to people. I still learn new things about it everyday, and yet feel like I know very little when it comes to Nix. Here’s what I think the main drawbacks are, simplified:</p>
<h3 id="learning-investment">Learning Investment</h3>
<p>The functional programming concepts and new mental models take significant time to internalize. You’ll feel less productive initially. Finding documentation or help for some issue you have might be difficult, but not impossible. Expect at least a few weeks before you become comfortable with the basic concepts. I can personally say it’s worth the effort and pain, but in the end it depends on what you want to achieve with Nix.</p>
<h3 id="debugging-difficulty">Debugging Difficulty</h3>
<p>When things go wrong, error messages often reference store paths and internal Nix mechanics rather than familiar concepts. Troubleshooting requires understanding Nix’s execution model, which adds complexity to already difficult debugging scenarios.</p>
<h3 id="ecosystem-integration">Ecosystem Integration</h3>
<p>Some software expects traditional Linux filesystem layouts. Proprietary tools that hardcode paths to <code>/usr/bin</code> or <code>/lib</code> require workarounds. Although Nix does have built-in utilities that help with this during the build process.</p>
<p>The way you look at and interact with packages and services in the system also completely changes once you adopt the “Nix way”.</p>
<h3 id="documentation-gaps">Documentation Gaps</h3>
<p>While improving, Nix documentation can be scattered. Error messages, though better than before, can still be outright unhelpful in some cases.</p>
<h3 id="storage-requirements">Storage Requirements</h3>
<p>The Nix store grows large over time, requiring periodic garbage collection to remove unused packages. Although this can be solved by enabling the periodic garbage collector on NixOS or by running it manually.</p>
<h2 id="how-do-i-get-started">How Do I Get Started?</h2>
<p>If you’re interested in trying Nix, I would suggest:</p>
<ol>
<li><strong>Install with flakes enabled</strong>: Use the <a href="https://zero-to-nix.com/start/install">Determinate Systems Installer</a> for quick setup</li>
<li><strong>Start with temporary tools</strong>: Use <code>nix shell</code> or <code>nix run</code> to try software without installing. A list of all the official packages can be found <a href="https://search.nixos.org/">here</a></li>
<li><strong>Create a simple development environment</strong>: Use <code>nix flake init</code> in a project directory and try writing a flake for it</li>
<li><strong>Add automatic activation</strong>: Uninstall some tools you depend on after setting up developer environment with flakes and set up <code>nix-direnv</code> to load environments automatically. Or use any of the other tools that abstract Nix for you</li>
<li><strong>Join the community</strong>: The <a href="https://discourse.nixos.org/">NixOS Discourse</a> is huge and welcoming to newcomers, and so is the <a href="https://github.com/NixOS/nixpkgs">Nixpkgs</a> repository</li>
</ol>
<p>And as for the documentation or general readings on Nix:</p>
<ul>
<li><a href="https://nixos.org/guides/nix-pills/">Nix Pills</a> - Explains what Nix is in brief</li>
<li><a href="https://nixos-and-flakes.thiscute.world/">NixOS &amp; Flakes Book</a> - A good starting point to understand more about Nix Flakes</li>
<li><a href="https://nix.dev/">nix.dev</a> - Official Nix Documentation</li>
<li><a href="https://wiki.nixos.org/">NixOS Wiki</a> - Wiki for Nix and NixOS-related stuff</li>
<li><a href="https://noogle.dev/">Noogle</a> - Google, but for Nix functions</li>
<li><a href="https://search.nixos.org/">NixOS Search</a> - Search packages in the official nixpkgs repository</li>
<li><a href="https://zero-to-nix.com/">Zero to Nix</a> - A general guide for Nix and Flakes</li>
</ul>
<p>There’s also <a href="https://nixos.org/">NixOS</a>, if you would like to spend more time learning and understanding Nix better.</p>
<h2 id="why-its-worth-considering">Why It’s Worth Considering</h2>
<p>The software development landscape has grown increasingly complex. We manage more dependencies, support more platforms, and deploy to more diverse environments than ever before. Traditional package management approaches that worked for simpler systems are showing their limitations.</p>
<p>Nix offers a different path—one where environment reproducibility isn’t hoped for but guaranteed, where dependency conflicts are impossible by design, and where trying new tools doesn’t risk breaking existing setups.</p>
<p>The learning investment is significant, but for teams struggling with environment management, the payoff comes through reduced debugging time, faster onboarding, and more reliable deployments.</p>
<p>The best tool isn’t always the most popular one—sometimes it’s the one that actually solves your problems.</p>
<p><em><strong>P.S.</strong>: This website runs on my <a href="https://git.deku.moe/thunderbottom/flakes">homelab</a> running NixOS.</em></p> </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WavePhoenix – open-source implementation of the Nintendo WaveBird protocol (115 pts)]]></title>
            <link>https://github.com/loopj/wavephoenix</link>
            <guid>44098442</guid>
            <pubDate>Mon, 26 May 2025 15:37:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/loopj/wavephoenix">https://github.com/loopj/wavephoenix</a>, See on <a href="https://news.ycombinator.com/item?id=44098442">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">
    WavePhoenix
</h2><a id="user-content-----wavephoenix" aria-label="Permalink: 
    WavePhoenix
" href="#----wavephoenix"></a></p>
<p dir="auto">
    An open-source implementation of the Nintendo WaveBird protocol using Silicon Labs Wireless Gecko SoCs
</p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/loopj/wavephoenix/blob/main/images/logo-animated.gif"><img src="https://github.com/loopj/wavephoenix/raw/main/images/logo-animated.gif" alt="WavePhoenix" data-animated-image=""></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Motivation</h2><a id="user-content-motivation" aria-label="Permalink: Motivation" href="#motivation"></a></p>
<p dir="auto">The WaveBird controller is, in my opinion, one of the best controllers ever made. It is wireless, has an insane battery life, and is very comfortable to hold. The WaveBird set a new standard for wireless controllers. It was the first major console controller to use radio frequency (RF) technology, providing a reliable, lag-free connection that didn't require line-of-sight, unlike infrared controllers.</p>
<p dir="auto">Unfortunately, Nintendo stopped producing the WaveBird more than a decade ago, causing a dwindling supply of controllers and, especially, receivers. Given the decreasing supply of receivers and the increasing resale prices, I decided to see if I could design my own from scratch.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Firmware</h2><a id="user-content-firmware" aria-label="Permalink: Firmware" href="#firmware"></a></p>
<p dir="auto">The firmware for WavePhoenix is split into several components:</p>
<ul dir="auto">
<li><a href="https://github.com/loopj/wavephoenix/blob/main/firmware/libwavebird"><code>libwavebird</code></a> - implementation of the WaveBird protocol</li>
<li><a href="https://github.com/loopj/wavephoenix/blob/main/firmware/libsi"><code>libsi</code></a> - implementation of the SI protocol, used by controllers to communicate with GameCube/Wii consoles</li>
<li><a href="https://github.com/loopj/wavephoenix/blob/main/firmware/receiver"><code>receiver</code></a> - reference implementation of firmware for a WaveBird receiver</li>
<li><a href="https://github.com/loopj/wavephoenix/blob/main/bootloader"><code>bootloader</code></a> - bootloader for updating application firmware via Bluetooth</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hardware</h2><a id="user-content-hardware" aria-label="Permalink: Hardware" href="#hardware"></a></p>
<p dir="auto">The hardware for a reference WavePhoenix receiver is available in the <a href="https://github.com/loopj/wavephoenix/blob/main/hardware/mini-receiver"><code>hardware/mini-receiver</code></a> directory. The receiver is designed to be as cheap as possible and easy to build.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/loopj/wavephoenix/blob/main/hardware/mini-receiver/images/mini-receiver.png"><img src="https://github.com/loopj/wavephoenix/raw/main/hardware/mini-receiver/images/mini-receiver.png" alt="WavePhoenix Mini Receiver" width="49%"></a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/loopj/wavephoenix/blob/main/hardware/mini-receiver/images/mini-receiver-pcb.png"><img src="https://github.com/loopj/wavephoenix/raw/main/hardware/mini-receiver/images/mini-receiver-pcb.png" alt="WavePhoenix Mini Receiver" width="49%"></a></p>
<p dir="auto">The hardware folder contains a simple PCB which hosts a <a href="https://www.rfstariot.com/rf-bm-bg22c3-efr32bg22-bluetooth-module_p93.html" rel="nofollow">RF-BM-BG22C3</a> module, a pairing button, a status LED, and a JST connector for connecting to a GameCube controller port, as well as a 3D printable case.</p>
<ul dir="auto">
<li><a href="https://github.com/loopj/wavephoenix/blob/main/hardware/mini-receiver/case"><code>case</code></a> - 3D printable case for the receiver</li>
<li><a href="https://github.com/loopj/wavephoenix/blob/main/hardware/mini-receiver/gerbers"><code>gerbers</code></a> - Gerber files for ordering the PCB</li>
<li><a href="https://github.com/loopj/wavephoenix/blob/main/hardware/mini-receiver/KiCad"><code>KiCad</code></a> - KiCad project files for the PCB</li>
</ul>
<p dir="auto">Check out the <a href="https://github.com/loopj/wavephoenix/blob/main/hardware/mini-receiver/README.md"><code>hardware/mini-receiver/README.md</code></a> for more information on how to build your own receiver.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Protocol</h2><a id="user-content-protocol" aria-label="Permalink: Protocol" href="#protocol"></a></p>
<p dir="auto">I have documented the key parts of WaveBird protocol in the headers of <a href="https://github.com/loopj/wavephoenix/blob/main/firmware/libwavebird"><code>libwavebird</code></a>:</p>
<ul dir="auto">
<li><a href="https://github.com/loopj/wavephoenix/blob/main/firmware/libwavebird/include/wavebird/radio.h">radio.h</a> - describes the radio timings and modulation</li>
<li><a href="https://github.com/loopj/wavephoenix/blob/main/firmware/libwavebird/include/wavebird/packet.h">packet.h</a> - describes the packet format, encoding, and crc</li>
<li><a href="https://github.com/loopj/wavephoenix/blob/main/firmware/libwavebird/include/wavebird/message.h">message.h</a> - describes the structure of the decoded packets, aka the <em>input state</em> and <em>origin</em> messages</li>
</ul>
<p dir="auto">Sam Edwards' excellent <a href="https://github.com/CFSworks/wavebird-reversing">WaveBird reversing</a> documentation contain the most detailed information about the protocol, but I have tried to fill in the gaps where possible.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development</h2><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Finding an SoC</h3><a id="user-content-finding-an-soc" aria-label="Permalink: Finding an SoC" href="#finding-an-soc"></a></p>
<p dir="auto">In mid-2020 I began researching the WaveBird protocol and came across Sam Edwards' incredibly well-written <a href="https://github.com/CFSworks/wavebird-reversing">WaveBird reverse-engineering</a> documentation. Sam's docs aren't quite perfect, but they were <strong>essential</strong> in getting this working, thanks, Sam!</p>
<p dir="auto">The most challenging part of this project was finding a wireless SoC that supported the specific modulation used by the WaveBird. What Sam describes in the <a href="https://github.com/CFSworks/wavebird-reversing/tree/master/05_line_coding_and_framing#a-closer-look-at-the-wavebirds-line-code">line coding and framing</a> section of his document is actually a form of DSSS (Direct Sequence Spread Spectrum), one which uses <em>15 chips per bit</em>.</p>
<p dir="auto">DSSS is a modulation technique that spreads the signal over a wide frequency band, making it more robust to interference and noise <a href="https://medium.com/networks-security/dsss-direct-sequence-spread-spectrum-a31005f281cc" rel="nofollow">here's a great breakdown by GeekyMuch</a>.</p>
<p dir="auto">While it would certainly be possible to implement 15-chip DSSS demodulation using a software-defined radio, I wanted to find an SoC that could do this in hardware <em>in real-time</em>. After reading more datasheets than I care to admit, I discovered the <a href="https://www.silabs.com/wireless/proprietary/efr32fg1-series-1-sub-ghz-2-4-ghz-socs" rel="nofollow">Silicon Labs Wireless Gecko EFR32FG1</a> family of SoCs, the <a href="https://www.silabs.com/documents/public/data-sheets/efr32fg1-datasheet.pdf" rel="nofollow">datasheet</a> contains the magic incantation:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/loopj/wavephoenix/blob/main/images/efr32fg1-datasheet-dsss.png"><img src="https://github.com/loopj/wavephoenix/raw/main/images/efr32fg1-datasheet-dsss.png" alt="15 chip DSSS"></a></p>
<p dir="auto">Once I saw this, I had enough confidence to order a development kit and start working on the firmware.</p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">Did you know that the codename for the GameCube CPU was <em>Gekko</em>? How fitting that we are using a <em>Gecko</em> SoC!</p>
</div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Receiving and decoding packets</h3><a id="user-content-receiving-and-decoding-packets" aria-label="Permalink: Receiving and decoding packets" href="#receiving-and-decoding-packets"></a></p>
<p dir="auto">As I mentioned above, Sam's WaveBird reverse engineering documentation isn't <em>quite</em> right on a few things. Most importantly, I don't believe the DSSS chip length is accurate. After setting the DSSS chip length to 15 and configuring the many other radio settings, I got my first packet!</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/loopj/wavephoenix/blob/main/images/first-packets.png"><img src="https://github.com/loopj/wavephoenix/raw/main/images/first-packets.png" alt="First packet"></a></p>
<p dir="auto">Decoding the packets was now simply a matter of writing some C code to de-interleave the packet, decode the <a href="https://en.wikipedia.org/wiki/BCH_code" rel="nofollow">BCH(31,21)</a> messages, and perform the CRC check. After doing this, we finally had a decoded <em>input state</em> message containing the state of all the buttons and analog inputs on the controller!</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/loopj/wavephoenix/blob/main/images/packet-decoding.png"><img src="https://github.com/loopj/wavephoenix/raw/main/images/packet-decoding.png" alt="Packet decoding"></a></p>
<p dir="auto">But wait. Every now and then, I was getting a "corrupted" packet that somehow was still passing the CRC check. What could this be? It turns out that in addition to broadcasting <em>input state</em> messages 250 times per second, the WaveBird controller also broadcasts <em>origin</em> messages. These are sent as soon as the controller is powered on and then repeated every second after that. The origin message contains the state of the analog sticks when the console was powered on, which we'll need later.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Communicating with a GameCube</h3><a id="user-content-communicating-with-a-gamecube" aria-label="Permalink: Communicating with a GameCube" href="#communicating-with-a-gamecube"></a></p>
<p dir="auto">Now we have real-time packet decoding; the next step is to send/receive commands on the <em>SI bus</em>. The SI bus is how GameCube/Wii consoles communicate with connected controllers. Jeff Longo has an insanely detailed and precise writeup about the <a href="https://jefflongo.dev/posts/gc-controller-reverse-engineering-part-1/" rel="nofollow">GameCube controller protocol</a>, which you should check out!</p>
<p dir="auto">Bits are clocked in and out at speeds between 250kHz (wired OEM controllers) and 200kHz (consoles). To do this without tying up the CPU, I had to take full advantage of the peripherals on the EFR32 SoCs.</p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">An OEM WaveBird receiver clocks data out at 225kHz, slightly slower than wired controllers!</p>
</div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Listen for incoming commands</h4><a id="user-content-listen-for-incoming-commands" aria-label="Permalink: Listen for incoming commands" href="#listen-for-incoming-commands"></a></p>
<p dir="auto">The first step is to listen for the various SI commands from the console.</p>
<p dir="auto">To read these commands, we do the following:</p>
<ul dir="auto">
<li>Configure a <code>TIMER</code> peripheral in capture/compare mode and attach it to the SI GPIO</li>
<li>Connect the <code>TIMER</code> to an <code>LDMA</code> peripheral, filling the DMA buffer with 16 "edge timings" at a time</li>
<li>Listen for a "DMA full" interrupt</li>
<li>Determine the length of each "low" pulse by measuring the time between pairs of edges</li>
<li>Based on the length of a low pulse, we store either a <code>0</code> or a <code>1</code> in our incoming byte buffer</li>
<li>The first byte identifies the SI command type; this tells us how many bits to expect in the rest of the SI transmission</li>
<li>Repeat until we have a complete SI command</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Replying to commands</h4><a id="user-content-replying-to-commands" aria-label="Permalink: Replying to commands" href="#replying-to-commands"></a></p>
<p dir="auto">Once we have a complete SI command, we need to respond (quickly!). We do this as follows:</p>
<ul dir="auto">
<li>Use a <code>USART</code> peripheral to clock out pulses with precise timing without tying up the CPU</li>
<li>Split each outgoing bit into 4 chips, so <code>0</code> becomes <code>0001</code>, <code>1</code> becomes <code>1110</code>, and stop becomes <code>0011</code></li>
<li>Configure the <code>USART</code> to output onto the SI GPIO, and set the baud rate to 1MHz (4 chips per bit @ 250kHz bitrate)</li>
<li>Feed the chips into the <code>USART</code> transmit buffer, and wait for the "TX complete" interrupt to be triggered</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Command types</h4><a id="user-content-command-types" aria-label="Permalink: Command types" href="#command-types"></a></p>
<p dir="auto">The most important command is the <em>get input state</em> command, which the console requests every frame. Since we should already have the latest input state packet from the WaveBird controller, we can simply send it back to the console after converting it to the SI format.</p>
<p dir="auto">There are a bunch of other commands that we need to respond to, such as the <em>info</em> and <em>get origin</em> commands, but I'll leave that as an exercise for the reader to discover in the code.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Wireless ID pinning</h3><a id="user-content-wireless-id-pinning" aria-label="Permalink: Wireless ID pinning" href="#wireless-id-pinning"></a></p>
<p dir="auto">Since the WaveBird has no true concept of pairing, it is possible for multiple WaveBird controllers to be on the same channel at the same time. This is a problem, since the receiver will receive packets from all controllers on this channel. The WaveBird uses a simple "ID pinning" mechanism to solve this problem.</p>
<p dir="auto">An OEM WaveBird receiver will look at the 10-bit controller ID present in all packets, and "fix" the ID to the first controller it receives a packet from.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Channel selection</h3><a id="user-content-channel-selection" aria-label="Permalink: Channel selection" href="#channel-selection"></a></p>
<p dir="auto">As well as supporting a channel wheel just like the original WaveBird receiver, I also wanted to add one-button "virtual pairing" support. This allows you to press a button on the receiver to enter pairing mode, and then hold a combination of buttons on a controller to "pair" it with the receiver. Under the hood, this scans through all 16 potential channels looking for wireless packets, and then checks if the appropriate buttons are being held on that channel.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">A more modern SoC</h3><a id="user-content-a-more-modern-soc" aria-label="Permalink: A more modern SoC" href="#a-more-modern-soc"></a></p>
<p dir="auto">Although it doesn't explicitly call this out in the datasheet, I discovered that the EFR32xG22 series of SoCs <em>also</em> support 15-chip DSSS. This is great news since they are cheaper and more modern than the EFR32xG1 series. The EFR32BG22 SoC in particular has many pre-built modules which host the SoC, crystal, antenna, and RF matching network - such as the tiny <a href="https://www.rfstariot.com/rf-bm-bg22c3-efr32bg22-bluetooth-module_p93.html" rel="nofollow">RF-BM-BG22C3 from RF-Star</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Tuning, tuning, tuning</h3><a id="user-content-tuning-tuning-tuning" aria-label="Permalink: Tuning, tuning, tuning" href="#tuning-tuning-tuning"></a></p>
<p dir="auto">The hardest part of this project <em>by far</em> was tuning the radio settings.</p>
<p dir="auto">The original WaveBird receiver has excellent range and wireless sensitivity. It is able to receive the full 250 packets per second at a distance of 10m+, so my goal was always to match this performance.</p>
<p dir="auto">Dropping an occasional packet here and there is not a huge problem. Most GameCube games run at 60 frames per second so we have a little bit of leeway for the occasional dropped packet. However, if we drop too many packets this will result in missed inputs.</p>
<p dir="auto">My initial radio configuration only managed to receive around 60 packets per second, so I spent a <em>lot</em> of time tuning the settings to get better performance. I'm now measuring 230+ packets per second at distances of around 5m, which is pretty close, and I think finally good enough to release.</p>
<p dir="auto">I still think there is <em>a ton</em> of room for improvement here, pull requests would be very welcome!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Future ideas</h2><a id="user-content-future-ideas" aria-label="Permalink: Future ideas" href="#future-ideas"></a></p>
<ul dir="auto">
<li><strong>Transmitter firmware</strong> - <code>libwavebird</code> includes packet encoding functions, so with a little work you could "build your own" WaveBird controller!</li>
<li><strong>N64 WaveBird receiver</strong> - the N64 uses the same SI protocol as the GameCube, with just a slightly different input polling command, so designing a WaveBird receiver for the N64 is within reach!</li>
<li><strong>USB HID dongle</strong> - instead of sending input data over the SI bus, we could instead incorporate something like a <a href="https://www.wch-ic.com/products/CH9329.html" rel="nofollow">CH9329</a> and send the input data over USB HID. This would allow you to use a WaveBird controller on any device that supports USB HID, such as a PC running Dolphin or RetroArch!</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Special Thanks</h2><a id="user-content-special-thanks" aria-label="Permalink: Special Thanks" href="#special-thanks"></a></p>
<ul dir="auto">
<li><a href="https://github.com/CFSworks">Sam Edwards</a> for his incredible WaveBird reverse engineering documentation, which was essential in getting this working</li>
<li><a href="https://jefflongo.dev/" rel="nofollow">Jeff Longo</a> - for his detailed GameCube controller protocol documentation</li>
<li><a href="https://github.com/Aurelio92">Aurelio Mannara</a> - for open sourcing GC+, another great reference for the GameCube controller protocol</li>
<li><a href="https://bsky.app/profile/mks.bsky.social" rel="nofollow">YveltalGriffin</a> - for help and advice along the way, and helping me figure out those mysterious origin packets</li>
<li>piotref1 - for being the very first person to build a WavePhoenix receiver and provide feedback, a true early adopter!</li>
<li>Everyone in the incredible <a href="https://bitbuilt.net/" rel="nofollow">BitBuilt community</a> for their support and encouragement throughout this project</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">The firmware in this repository is licensed under the <a href="https://github.com/loopj/wavephoenix/blob/main/firmware/LICENSE">MIT License</a>.</p>
<p dir="auto">The hardware is licensed under the <a href="https://github.com/loopj/wavephoenix/blob/main/hardware/LICENSE">Solderpad Hardware License v2.1</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cloudflare CEO: Football Piracy Blocks Will Claim Lives; "I Pray No One Dies" (234 pts)]]></title>
            <link>https://torrentfreak.com/cloudflare-ceo-football-piracy-blocks-will-claim-lives-i-pray-no-one-dies-250526/</link>
            <guid>44098273</guid>
            <pubDate>Mon, 26 May 2025 15:15:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://torrentfreak.com/cloudflare-ceo-football-piracy-blocks-will-claim-lives-i-pray-no-one-dies-250526/">https://torrentfreak.com/cloudflare-ceo-football-piracy-blocks-will-claim-lives-i-pray-no-one-dies-250526/</a>, See on <a href="https://news.ycombinator.com/item?id=44098273">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p><picture loading="lazy" decoding="async">
<source type="image/webp" srcset="https://torrentfreak.com/images/cloudflare-spain-s2.png.webp 500w, https://torrentfreak.com/images/cloudflare-spain-s2-300x152.png.webp 300w" sizes="auto, (max-width: 300px) 100vw, 300px">
<img loading="lazy" decoding="async" src="https://torrentfreak.com/images/cloudflare-spain-s2.png" alt="cloudflare-spain-s2" width="300" height="152" srcset="https://torrentfreak.com/images/cloudflare-spain-s2.png 500w, https://torrentfreak.com/images/cloudflare-spain-s2-300x152.png 300w, https://torrentfreak.com/images/cloudflare-spain-s2-150x76.png 150w" sizes="auto, (max-width: 300px) 100vw, 300px">
</picture>
LaLiga’s 2024/2025 season is officially over. FC Barcelona were actually crowned champions on Thursday after it became mathematically impossible for Real Madrid to conjure up seven points or more from the six points available.</p>
<p>The final matches of the season were played on Sunday but with the championship settled, would LaLiga show restraint and taper down its blocking activity? Or perhaps it would continue under the authority of judge, despite the widespread collateral damage inflicted on innocent internet users since February, a scandal now known as #laligagate.</p>
<h2>In For a Penny?</h2>
<p>The early signs did little to inspire confidence. Protest website LaLigaGate.com was hit by a total blockade but whether the site was targeted deliberately or succumbed to collateral damage is unknown.</p>
<center><em>Intentional and Unintentional Blocking Look Identical</em><picture loading="lazy" decoding="async">
<source type="image/webp" srcset="https://torrentfreak.com/images/laligagate-down.png.webp 735w, https://torrentfreak.com/images/laligagate-down-300x164.png.webp 300w" sizes="auto, (max-width: 670px) 100vw, 670px">
<img loading="lazy" decoding="async" src="https://torrentfreak.com/images/laligagate-down.png" alt="laligagate-down" width="670" height="366" srcset="https://torrentfreak.com/images/laligagate-down.png 735w, https://torrentfreak.com/images/laligagate-down-300x164.png 300w, https://torrentfreak.com/images/laligagate-down-600x328.png 600w, https://torrentfreak.com/images/laligagate-down-150x82.png 150w" sizes="auto, (max-width: 670px) 100vw, 670px">
</picture>
</center>
<p>As the <a href="https://torrentfreak.com/constitutional-court-urged-to-end-piracy-blockades-now-hurting-millions-250519/">latest data</a> suggests that mass blocking of Cloudflare in pursuit of 150 piracy platforms has negatively affected millions of innocent websites, use of the term ‘collateral damage’ may be running out of scope. </p>
<p>Unintentional overblocking became inevitable overblocking some time ago, a point certainly not lost on Cloudflare CEO Matthew Prince.</p>
<p>Posting to X last week, Prince asked if anyone wanted any general feedback, declaring that he felt “in an especially truthful mood.” The first <a href="https://x.com/cisne_bayswater/status/1924962395010584847">response</a> contained direct questions about the LaLiga controversy, the blame for which LaLiga places squarely on the shoulders of Cloudflare. </p>
<p>For the first time since Cloudflare legal action <a href="https://torrentfreak.com/judge-confirms-laligas-right-to-block-cloudflare-in-pursuit-of-iptv-pirates-250328/">failed to end LaLiga’s blocking campaign</a>, Prince weighed in with his assessment of the current situation and where he believes it’s inevitably heading.</p>
<h2>“Bonkers” Blocking Strategy</h2>
<p>“A huge percentage of the Internet sits behind us, including small businesses and emergency resources in Spain,” Prince explained. </p>
<p>“The strategy of blocking broadly through ISPs based on IPs is bonkers because so much content, including emergency services content, can be behind any IP. <a href="https://x.com/eastdakota/status/1924969551478804543">The collateral damage is vast</a> and is hurting Spanish citizens from accessing critical resources,” he added.</p>
<p>Earlier this year, various comments and statements by LaLiga suggested that its relationship with Cloudflare had reached rock bottom. It transpired that LaLiga had <a href="https://torrentfreak.com/cloudflare-asks-court-to-end-laligas-illegal-blocking-response-to-encrypted-client-hello-250220/">obtained an injunction</a> which allowed it to tackle Cloudflare and <a href="https://blog.cloudflare.com/announcing-encrypted-client-hello/">Encrypted Client Hello (ECH)</a>, but had done so without Cloudflare’s knowledge, effectively denying the company a right to respond.</p>
<p>Alongside other attacks delivered via the media, LaLiga slammed Cloudflare for refusing to cooperate. What action LaLiga had demanded still isn’t clear, but the league said that if there was any overblocking as a result, Cloudflare would have to shoulder the blame.</p>
<h2>The Potential for Deadly Consequences</h2>
<p>The scale of overblocking reported in Spain is unprecedented but since LaLiga has a court order that effectively gives Cloudflare blocking a green light, it has been doing so in bulk, every single week since February.</p>
<p>Depending on who addresses the overblocking issue, with whom and when, the league claims that collateral damage a) doesn’t exist or is minimal and/or b) is Cloudflare’s responsibility. Prince appears to have grave concerns over the scale and type of blocking taking place, warning that a worst-case scenario is inevitable.</p>
<h2>Football Piracy Blocks Will Claim Lives</h2>
<p>Despite LaLiga’s unshakable claims to the contrary, Prince believes that it’s not a case of ‘if’ disaster strikes, it’s ‘when’.</p>
<p>“It’s only a matter of time before a Spanish citizen can’t access a life-saving emergency resource because the rights holder in a football match refuses to send a limited request to block one resource versus a broad request to block a whole swath of the Internet,” Prince warned.</p>
<p>“When that unfortunately and inevitably happens and harms lives, I’m confident policy makers and courts in Spain and elsewhere will make the right policy decision. Until then, it’ll be up to users to make politicians clear on the risk. I pray no one dies.”</p>
<p>The suggestion that LaLiga’s demands were too broad, doesn’t mean that Cloudflare is refusing to help, Prince suggested. On the contrary, there’s a process available, LaLiga just needs to use it.</p>
<p>“We’ve always been happy and willing to work with rights holders in conjunction with judicial bodies to protect their content. We have a clear process that works around the world to do that, Prince explained.</p>
<h2>LaLiga Targeted Cloudflare and Others During the Weekend</h2>
<p>Live blocking data provided by <a href="https://hayahora.futbol/">hayahora.futbol</a> has proven invaluable to those documenting #laligagate in recent months and this past weekend was no different. </p>
<p>As the small sample of Cloudflare IP addresses blocked on Sunday seems to show, concerns that every IP address blocked would inevitably result in collateral damage, seem to have been trumped by the authority of the now-famous court order.</p>
<center><picture loading="lazy" decoding="async">
<source type="image/webp" srcset="https://torrentfreak.com/images/laliga-top-blocked.png.webp 975w, https://torrentfreak.com/images/laliga-top-blocked-300x513.png.webp 300w" sizes="auto, (max-width: 670px) 100vw, 670px">
<img loading="lazy" decoding="async" src="https://torrentfreak.com/images/laliga-top-blocked.png" alt="laliga-top-blocked" width="670" height="1146" srcset="https://torrentfreak.com/images/laliga-top-blocked.png 975w, https://torrentfreak.com/images/laliga-top-blocked-300x513.png 300w, https://torrentfreak.com/images/laliga-top-blocked-600x1026.png 600w, https://torrentfreak.com/images/laliga-top-blocked-88x150.png 88w, https://torrentfreak.com/images/laliga-top-blocked-898x1536.png 898w" sizes="auto, (max-width: 670px) 100vw, 670px">
</picture>
</center>
<p>The importance of strict adherence to the law was underlined by LaLiga’s Global Content Protection Manager in a recent interview.</p>
<p>José Ignacio Carrillo de Albornoz told <a href="https://www.elconfidencial.com/deportes/2025-05-19/pirateria-iptv-ilegal-futbol-carrillo-albornoz-laliga-bra_4130314/">El Confidencial</a> that without the collaboration of intermediaries, piracy will be impossible to beat. Carrillo de Albornoz concluded with a “statement of responsibility” which notes that real progress will require all parties to work together and go beyond compliance with the law.</p>
<p>“It is necessary that all links in the digital chain act legally and ethically,” he said.</p>
<center><picture loading="lazy" decoding="async">
<source type="image/webp" srcset="https://torrentfreak.com/images/laliga-recordv.png.webp 726w, https://torrentfreak.com/images/laliga-recordv-300x274.png.webp 300w" sizes="auto, (max-width: 550px) 100vw, 550px">
<img loading="lazy" decoding="async" src="https://torrentfreak.com/images/laliga-recordv.png" alt="laliga-recordv" width="550" height="502" srcset="https://torrentfreak.com/images/laliga-recordv.png 726w, https://torrentfreak.com/images/laliga-recordv-300x274.png 300w, https://torrentfreak.com/images/laliga-recordv-600x548.png 600w, https://torrentfreak.com/images/laliga-recordv-150x137.png 150w" sizes="auto, (max-width: 550px) 100vw, 550px">
</picture>
</center>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[German court sends VW execs to prison over Dieselgate scandal (671 pts)]]></title>
            <link>https://www.politico.eu/article/german-court-vw-execs-prison-dieselgate-scandal-volkswagen-environment-illegal-pollution/</link>
            <guid>44098091</guid>
            <pubDate>Mon, 26 May 2025 14:59:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.politico.eu/article/german-court-vw-execs-prison-dieselgate-scandal-volkswagen-environment-illegal-pollution/">https://www.politico.eu/article/german-court-vw-execs-prison-dieselgate-scandal-volkswagen-environment-illegal-pollution/</a>, See on <a href="https://news.ycombinator.com/item?id=44098091">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<p>
			<h2>
						
			<span>
				Our readers read next			</span>
					</h2>
	
	
	</p>
						
		
		
		
		
			</div><div>
					<p>
			<h2>
						
			<span>
				More from Elena Giordano			</span>
					</h2>
	
	
	</p>
						<div data-count="4" data-remainder="0" data-block-attributes="[]" data-page="0">
					
<div>
			<p><img src="https://www.politico.eu/cdn-cgi/image/width=380,height=238,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/21/13067739-scaled.jpg" srcset="https://www.politico.eu/cdn-cgi/image/width=480,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/21/13067739-scaled.jpg 480w, https://www.politico.eu/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/21/13067739-scaled.jpg 768w, https://www.politico.eu/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/21/13067739-scaled.jpg 1024w, https://www.politico.eu/cdn-cgi/image/width=1280,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/21/13067739-scaled.jpg 1280w, https://www.politico.eu/cdn-cgi/image/width=1440,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/21/13067739-scaled.jpg 1440w, https://www.politico.eu/cdn-cgi/image/width=1920,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/21/13067739-scaled.jpg 1920w, https://www.politico.eu/cdn-cgi/image/width=2640,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/21/13067739-scaled.jpg 2640w" sizes="(max-width: 380px) 100vw, 380px" alt="EU outrage grows after Israel fires ‘warning shots’ at diplomatic delegation" width="380" height="238" role="" loading="lazy" decoding="async">

					</p>
	
	
		
		<div>

			
			
			
			
			
							<h2>
										<a href="https://www.politico.eu/article/israel-army-shots-fired-eu-diplomatic-delegation-jenin-west-bank-palestine-kaja-kallas/">
						EU outrage grows after Israel fires ‘warning shots’ at diplomatic delegation					</a>
				</h2>
			
			
							<p>France, Germany and Belgium have condemned the incident and demanded an explanation.</p>
			
			<p><span>
			May 21		</span>
	
	
	
<span>
	<span>3 mins read</span>
</span>
</p>

			
			
			
			
					</div>

			</div>

<div>
			<p><img src="https://www.politico.eu/cdn-cgi/image/width=380,height=253,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/21/Donald-Trump-Jr-scaled.jpg" srcset="https://www.politico.eu/cdn-cgi/image/width=480,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/21/Donald-Trump-Jr-scaled.jpg 480w, https://www.politico.eu/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/21/Donald-Trump-Jr-scaled.jpg 768w, https://www.politico.eu/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/21/Donald-Trump-Jr-scaled.jpg 1024w, https://www.politico.eu/cdn-cgi/image/width=1280,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/21/Donald-Trump-Jr-scaled.jpg 1280w, https://www.politico.eu/cdn-cgi/image/width=1440,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/21/Donald-Trump-Jr-scaled.jpg 1440w, https://www.politico.eu/cdn-cgi/image/width=1920,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/21/Donald-Trump-Jr-scaled.jpg 1920w, https://www.politico.eu/cdn-cgi/image/width=2640,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/21/Donald-Trump-Jr-scaled.jpg 2640w" sizes="(max-width: 380px) 100vw, 380px" alt="Donald Trump Jr. hints at run for US president" width="380" height="253" role="" loading="lazy" decoding="async">

					</p>
	
	
		
		<div>

			
			
			
			
			
							<h2>
										<a href="https://www.politico.eu/article/donald-trump-jr-hints-run-us-president/">
						Donald Trump Jr. hints at run for US president					</a>
				</h2>
			
			
							<p>“Maybe one day … that calling is there,” says Trump scion.</p>
			
			<p><span>
			May 21		</span>
	
	
	
<span>
	<span>2 mins read</span>
</span>
</p>

			
			
			
			
					</div>

			</div>

<div>
			<p><img src="https://www.politico.eu/cdn-cgi/image/width=380,height=253,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/20/12345799.jpg" srcset="https://www.politico.eu/cdn-cgi/image/width=480,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/20/12345799.jpg 480w, https://www.politico.eu/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/20/12345799.jpg 768w, https://www.politico.eu/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/20/12345799.jpg 1024w, https://www.politico.eu/cdn-cgi/image/width=1280,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/20/12345799.jpg 1280w, https://www.politico.eu/cdn-cgi/image/width=1440,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/20/12345799.jpg 1440w, https://www.politico.eu/cdn-cgi/image/width=1920,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/20/12345799.jpg 1920w, https://www.politico.eu/cdn-cgi/image/width=2640,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/20/12345799.jpg 2640w" sizes="(max-width: 380px) 100vw, 380px" alt="Lithuania takes Belarus to The Hague for weaponizing migration" width="380" height="253" role="" loading="lazy" decoding="async">

					</p>
	
	
		
		<div>

			
			
			
			
			
							<h2>
										<a href="https://www.politico.eu/article/belarus-lithuania-the-hague-smuggling-migrants-borders/">
						Lithuania takes Belarus to The Hague for weaponizing migration					</a>
				</h2>
			
			
							<p>Minsk authorities stand accused of facilitating illegal crossings into the EU to sow political discord.</p>
			
			<p><span>
			May 20		</span>
	
	
	
<span>
	<span>2 mins read</span>
</span>
</p>

			
			
			
			
					</div>

			</div>

<div>
			<p><img src="https://www.politico.eu/cdn-cgi/image/width=380,height=265,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/16/13030935-scaled.jpg" srcset="https://www.politico.eu/cdn-cgi/image/width=480,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/16/13030935-scaled.jpg 480w, https://www.politico.eu/cdn-cgi/image/width=768,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/16/13030935-scaled.jpg 768w, https://www.politico.eu/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/16/13030935-scaled.jpg 1024w, https://www.politico.eu/cdn-cgi/image/width=1280,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/16/13030935-scaled.jpg 1280w, https://www.politico.eu/cdn-cgi/image/width=1440,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/16/13030935-scaled.jpg 1440w, https://www.politico.eu/cdn-cgi/image/width=1920,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/16/13030935-scaled.jpg 1920w, https://www.politico.eu/cdn-cgi/image/width=2640,quality=80,onerror=redirect,format=auto/wp-content/uploads/2025/05/16/13030935-scaled.jpg 2640w" sizes="(max-width: 380px) 100vw, 380px" alt="Trump-Putin contact ‘extremely important’ for Ukraine peace effort, Kremlin says" width="380" height="265" role="" loading="lazy" decoding="async">

					</p>
	
	
		
		<div>

			
			
			
			
			
							<h2>
										<a href="https://www.politico.eu/article/kremlin-says-donald-trump-vladimir-putin-contact-extremely-important/">
						Trump-Putin contact ‘extremely important’ for Ukraine peace effort, Kremlin says					</a>
				</h2>
			
			
							<p>Comments follow Trump announcement that he is ready to meet with Putin “as soon as we can set it up.” </p>
			
			<p><span>
			May 16		</span>
	
	
	
<span>
	<span>2 mins read</span>
</span>
</p>

			
			
			
			
					</div>

			</div>
				</div>
		
		
		
		
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TeleMessage Explorer: a new open source research tool (103 pts)]]></title>
            <link>https://micahflee.com/telemessage-explorer-a-new-open-source-research-tool/</link>
            <guid>44097986</guid>
            <pubDate>Mon, 26 May 2025 14:50:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://micahflee.com/telemessage-explorer-a-new-open-source-research-tool/">https://micahflee.com/telemessage-explorer-a-new-open-source-research-tool/</a>, See on <a href="https://news.ycombinator.com/item?id=44097986">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article>
                    <header>
                        
                            <figure>
        <img srcset="https://micahflee.com/content/images/size/w300/2025/05/photo-1625509976131-86d1ba268e09.jpeg 300w,
                    https://micahflee.com/content/images/size/w720/2025/05/photo-1625509976131-86d1ba268e09.jpeg 720w,
                    https://micahflee.com/content/images/size/w960/2025/05/photo-1625509976131-86d1ba268e09.jpeg 960w,
                    https://micahflee.com/content/images/size/w1200/2025/05/photo-1625509976131-86d1ba268e09.jpeg 1200w,
                    https://micahflee.com/content/images/size/w2000/2025/05/photo-1625509976131-86d1ba268e09.jpeg 2000w,
                    https://micahflee.com/content/images/2025/05/photo-1625509976131-86d1ba268e09.jpeg" sizes="(max-width: 1200px) 100vw, 1200px" src="https://micahflee.com/content/images/size/w1200/2025/05/photo-1625509976131-86d1ba268e09.jpeg" alt="TeleMessage Explorer: a new open source research tool">
            <figcaption><span>Photo by </span><a href="https://unsplash.com/@raimondklavins?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit"><span>Raimond Klavins</span></a><span> / </span><a href="https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit"><span>Unsplash</span></a></figcaption>
    </figure>
                    </header>

                <section>
                    <p>I've spent the last week or two writing code to make sense of the <a href="https://micahflee.com/ddosecrets-publishes-410-gb-of-heap-dumps-hacked-from-telemessages-archive-server/" rel="noreferrer">massive hack</a> of data from TeleMessage, the <a href="https://micahflee.com/how-the-knock-off-signal-app-used-by-trump-officials-got-hacked-in-20-minutes/" rel="noreferrer">comically insecure</a> company that makes a modified Signal app that Trump's former national security advisor Mike Waltz was caught using. I've decided to <a href="https://github.com/micahflee/telemessage-explorer" rel="noreferrer">publish my code</a> as open source in the hopes that other journalists will use it to find revelations in this dataset.</p><p>While the <a href="https://github.com/micahflee/telemessage-explorer" rel="noreferrer">source code</a> for TeleMessage Explorer is public, it requires the <a href="https://ddosecrets.com/article/telemessage" rel="noreferrer">TeleMessage dataset</a> to use it. DDoSecrets is distributing this dataset to journalists and researchers with a history of reporting who agree to publish their findings. Contact DDoSecrets (not me) to request access.</p><p>In this post, I explain how journalists who have access to the dataset can crunch the TeleMessage data and then explore it using TeleMessage Explorer. I also show a bunch of screenshots from the tool showing TeleMessage data, and I describe some details I found out about one 24-year-old (today is his birthday!) White House staffer bro who has Signal messages in this dataset.</p><p>I used this tool myself to publish my recent post, <a href="https://micahflee.com/telemessage-customers-include-dc-police-andreesen-horowitz-jp-morgan-and-hundreds-more/" rel="noreferrer">TeleMessage customers include DC Police, Andreessen Horowitz, JP Morgan, and hundreds more</a>. Reuters has <a href="https://www.reuters.com/world/us/hacker-who-breached-communications-app-used-by-trump-aide-stole-data-across-us-2025-05-21/" rel="noreferrer">reported</a> on this dataset too, and I know plenty of other journalists and newsrooms currently have access to it.</p>
<div data-layout="minimal">
                    
                        <p><span>I'm crunching data and writing these newsletters in my free time. If you want to support my work, considering becoming a paid supporter.</span></p>
                    
                    
                        <p><a href="#/portal/signup">
                            Become a paid supporter
                        </a>
                        
                    </p></div>
<div><p>💡</p><p>TeleMessage Explorer is unpolished software. I built it initially just for my own use. I don't promise to maintain it, fix bugs, or anything else. If your newsroom wants help working with it, or wants specific features implemented or anything like that, <a href="https://micahflee.com/contact/" rel="noreferrer">contact me</a> and I might be interested in consulting.</p></div><h2 id="like-blueleaks-explorer-before-it">Like BlueLeaks Explorer before it</h2><p>Back in 2020, in the middle of the Black Lives Matter uprising, someone hacked hundreds of law enforcement websites and exfiltrated about 270 GB of data. That dataset, called <a href="https://ddosecrets.com/article/blueleaks" rel="noreferrer">BlueLeaks</a> (it's completely public for everyone to download from DDoSecrets), is full of evidence of police misconduct. I wrote a tool called <a href="https://github.com/micahflee/blueleaks-explorer" rel="noreferrer">BlueLeaks Explorer</a> to help me dig into it and write articles.</p><p>It wasn't until 2024, when I published my book <a href="https://hacksandleaks.com/" rel="noreferrer">Hacks, Leaks, and Revelations: The Art of Analyzing Hacked and Leaked Data</a>, that I decided to release BlueLeaks Explorer as open source, basically as an education tool. At this point, the BlueLeaks data was years old.</p><p>I still think there are tons of revelations in BlueLeaks that even today no one has found, because no one has looked, but at this point it's all old news. If you want to dig into secret 2020-era docs from your local police intelligence agency yourself, I recommend reading my book (which is available for free <a href="https://hacksandleaks.com/contents.html" rel="noreferrer">online</a>, though I'd appreciate if you <a href="https://hacksandleaks.com/buy.html" rel="noreferrer">buy</a> a copy), and particularly these chapters:</p><ul><li><a href="https://hacksandleaks.com/chapter-9.html">Chapter 9: BlueLeaks, Black Lives Matter, and the CSV File Format</a></li><li><a href="https://hacksandleaks.com/chapter-10.html" rel="noreferrer">Chapter 10: BlueLeaks Explorer</a></li></ul><p>I'm hoping that by releasing TeleMessage Explorer right now, a lot of good journalists will use it to find a lot of good stories while the data is still fresh.</p><h2 id="prerequisites">Prerequisites</h2><p>Since most journalists I know are Mac users, I'll show you how to get started using macOS.</p><p>Here's what you'll need:</p><ul><li>Access to the <a href="https://ddosecrets.com/article/telemessage" rel="noreferrer">TeleMesage dataset</a> from DDoSecrets, with <code>telemessage.7z</code> downloaded.</li><li>About 530 GB of free space. You can do this on your internal disk or on a USB disk. If you use a USB disk, I highly recommend using an SSD.</li><li>Comfort with <a href="https://hacksandleaks.com/chapter-3.html" rel="noreferrer">running commands</a> in the the terminal.</li></ul><p>Here's the software that you'll need to install:</p><ul><li>You need Docker. You can <a href="https://www.docker.com/products/docker-desktop/" rel="noreferrer">download Docker Desktop here</a>. (Because Docker the company is kind of annoying, it will try to get you to login to an account, but you can skip this, you don't actually need an account to use it.)</li><li>You'll also need Python and Poetry installed. If you use <a href="https://brew.sh/" rel="noreferrer">Homebrew</a>, you can install these with: <code>brew install python3 poetry</code></li></ul><p>You'll also need a copy of the TeleMessage Explorer code itself. You can either run <code>git clone https://github.com/micahflee/telemessage-explorer.git</code> or download the <a href="https://github.com/micahflee/telemessage-explorer/archive/refs/heads/main.zip" rel="noreferrer">zip file</a> of the code from GitHub.</p><p>Download <code>telemessage.7z</code> from DDoSecrets. This is 54 GB. Then, extract it. If you're not familiar with 7-zip files, one way to extract it is to install 7-zip (<code>brew install 7zip</code>), and then you can extract it like <code>7zz e telemessage.7z</code>.</p><p>After extracting, you'll get a folder with 2,729 heap dump files, taking up 384 GB of space. These files will be called <code>heapdump</code>, <code>heapdump.1</code>, <code>heapdump.2</code>, ..., <code>heapdump.2728</code>.</p><p>My code expects all files to have a number at the end, so in your terminal, change to the folder with the heap dump files and rename <code>heapdump</code> to <code>heapdump.0</code>:</p><pre><code>mv heapdump heapdump.0</code></pre><p>Most of the data in these heap dump files isn't relevant. All TeleMessage Explorer cares about are the JSON objects in the heap dumps, and the easiest way to cut out the useless parts is by using <code>strings</code>. Run the following command to loop through all heap dump files and for each file, run <code>strings</code> on it and output it to the same filename but with the <code>.strings</code> extension:</p><pre><code>for F in $(ls); do echo $F; strings $F &gt; ${F}.strings; done</code></pre><p>This takes awhile to run. But at the end, you'll have an additional 83 GB of just text data.</p><h2 id="how-the-code-is-organized">How the code is organized</h2><p>There are four components to TeleMessage explorer:</p><ul><li>The backend is programmed in Python and Flask. It's a simple API, used by the frontend, to access data from the database.</li><li>The frontend is programmed in TypeScript and uses Vue.js. It's pulls data from the backend using its API.</li><li>The database is PostgreSQL, and you run it in Docker.</li><li>Finally, the "cruncher" part of the code will read all of the heap dump strings files, search for relevant JSON objects, and insert them into the database. I call this "crunching the data."</li></ul><p>You'll start the backend, frontend, and database as containers using Docker Compose. Go ahead and build the containers right now by running:</p><pre><code>docker compose build</code></pre><p>You'll crunch the data by running the cruncher Python script. You'll need the database Docker container up to do that though.</p><h2 id="crunch-the-data">Crunch the data</h2><p>Make sure Docker is running on your computer. Change to the <code>telemessage-explorer</code> folder with the source code and run this command to start the database container:</p><pre><code>docker compose up db</code></pre><p>This should start the <code>db</code> container. Note that it stores its data in the <code>volumes/db-data</code> folder.</p><p>Once that database is running, open a new terminal window. Switch to the cruncher folder:</p><pre><code>cd cruncher</code></pre><p>Install the Poetry dependencies:</p><pre><code>poetry install</code></pre><p>Now you can use the <code>crunch</code> script like this:</p><pre><code>$ poetry run crunch --help
Usage: crunch [OPTIONS]

Options:
  --path DIRECTORY       Path to folder with heap dump .strings files
                         [required]
  --num-threads INTEGER  Number of threads to run
  --num-skip INTEGER     Skip the first N heap dumps
  --help                 Show this message and exit.</code></pre><p>When you crunch the data, you must include <code>--path</code> with the path to the folder with the <code>.strings</code> files you created.</p><p>If you want, you can pass in <code>--num-threads</code> to choose how many threads you want the cruncher to use (by default, it uses one the number of CPU cores you have available).</p><p>Before crunching data, you need to set some environment variables to tell the cruncher how to connect to the database:</p><pre><code>export DB_NAME=db
export DB_USER=db
export DB_PASSWORD=db
export DB_HOST=localhost
export DB_PORT=54320</code></pre><p>Finally, crunch the data like this, replacing the path with the location of the strings files you created above:</p><pre><code>poetry run crunch --path ~/datasets/2025-TeleMessage/strings</code></pre><p>Here's what it should look like:</p><figure data-kg-thumbnail="https://micahflee.com/content/media/2025/05/crunching_thumb.jpg" data-kg-custom-thumbnail="">
            <div>
                <video src="https://micahflee.com/content/media/2025/05/crunching.mp4" poster="https://img.spacergif.org/v1/1670x1088/0a/spacer.png" width="1670" height="1088" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>0:27</span>
                        </p>
                        </div>
            </div>
            <figcaption><p><span>Video of crunching the data</span></p></figcaption>
        <img src="https://micahflee.com/content/media/2025/05/crunching_thumb.jpg"></figure><p>As you can see in the video, on my Mac the cruncher is using 8 threads, meaning that it's crunching 8 heap dump files at a time. When it's done with each file, it shows some output explaining how many of which different types of JSON objects it's looking for that it found, as well as the overall progress.</p><p>It will probably take your computer a few hours to finish crunching the data. But at the end, you'll have a PostgreSQL database full of data from the heap dump files.</p><h2 id="the-database">The database</h2><p>After crunching the data, you'll have a PostgreSQL database full of messages, groups, users, attachments, validations, and various join tables. As long as the database container is running, you can connect to it using <code>psql</code> or any other PostgreSQL client.</p><p>In the following screenshot, I ran a query (using TablePlus) that selected everything from all groups, joined them with the number of messages and users in each group, and sorted them by the groups with the most messages.</p><figure><img src="https://micahflee.com/content/images/2025/05/Screenshot-2025-05-25-at-2.25.03-PM.png" alt="" loading="lazy" width="2000" height="1276" srcset="https://micahflee.com/content/images/size/w600/2025/05/Screenshot-2025-05-25-at-2.25.03-PM.png 600w, https://micahflee.com/content/images/size/w1000/2025/05/Screenshot-2025-05-25-at-2.25.03-PM.png 1000w, https://micahflee.com/content/images/size/w1600/2025/05/Screenshot-2025-05-25-at-2.25.03-PM.png 1600w, https://micahflee.com/content/images/size/w2400/2025/05/Screenshot-2025-05-25-at-2.25.03-PM.png 2400w" sizes="(min-width: 1200px) 1200px"><figcaption><span>The crunched data in a PostgreSQL database</span></figcaption></figure><p>When I started coding this, at fist I only planned on inserting relevant data into a database. I could then write my own SQL queries to find the good stuff.</p><p>But then I quickly realized that it would be much easier to research if I had a user interface built for browsing the data. So, I made one.</p><h2 id="telemessage-explorer">TeleMessage Explorer</h2><p>In your terminal window that's running Docker Compose, press Ctrl-C to quit the database container. Then start all containers together like this:</p><pre><code>docker compose up</code></pre><p>This should start three containers: <code>backend</code>, <code>frontend</code>, and <code>db</code>. In your browser, load the frontend by going to <strong>http://localhost:5173</strong>.</p><p>Here's a screenshot of the same data as from the SQL query above, except using TeleMessage Explorer:</p><figure><img src="https://micahflee.com/content/images/2025/05/Screenshot-2025-05-25-at-3.19.46-PM.png" alt="" loading="lazy" width="2000" height="1392" srcset="https://micahflee.com/content/images/size/w600/2025/05/Screenshot-2025-05-25-at-3.19.46-PM.png 600w, https://micahflee.com/content/images/size/w1000/2025/05/Screenshot-2025-05-25-at-3.19.46-PM.png 1000w, https://micahflee.com/content/images/size/w1600/2025/05/Screenshot-2025-05-25-at-3.19.46-PM.png 1600w, https://micahflee.com/content/images/size/w2400/2025/05/Screenshot-2025-05-25-at-3.19.46-PM.png 2400w" sizes="(min-width: 1200px) 1200px"><figcaption><span>Viewing groups in TeleMessage Explorer.</span></figcaption></figure><p>You can filter the the groups by entering text in the "search" field. This will only show results where the string appears in either group name or source type. There are 28 groups when I search for "signal":</p><figure><img src="https://micahflee.com/content/images/2025/05/Screenshot-2025-05-25-at-3.25.34-PM.png" alt="" loading="lazy" width="2000" height="1392" srcset="https://micahflee.com/content/images/size/w600/2025/05/Screenshot-2025-05-25-at-3.25.34-PM.png 600w, https://micahflee.com/content/images/size/w1000/2025/05/Screenshot-2025-05-25-at-3.25.34-PM.png 1000w, https://micahflee.com/content/images/size/w1600/2025/05/Screenshot-2025-05-25-at-3.25.34-PM.png 1600w, https://micahflee.com/content/images/size/w2400/2025/05/Screenshot-2025-05-25-at-3.25.34-PM.png 2400w" sizes="(min-width: 1200px) 1200px"><figcaption><span>Signal groups.</span></figcaption></figure><p>Each row has an ID column, and the ID itself is a button to go into more detail about that column.</p><p>For example, this is what it looks like when I click 22217, the ID for the Signal group called "POTUS | ROME-VATICAN | PRESS GC" – yes, that's POTUS as in "President of the United States":</p><figure><img src="https://micahflee.com/content/images/2025/05/Screenshot-2025-05-25-at-3.28.22-PM.png" alt="" loading="lazy" width="2000" height="1392" srcset="https://micahflee.com/content/images/size/w600/2025/05/Screenshot-2025-05-25-at-3.28.22-PM.png 600w, https://micahflee.com/content/images/size/w1000/2025/05/Screenshot-2025-05-25-at-3.28.22-PM.png 1000w, https://micahflee.com/content/images/size/w1600/2025/05/Screenshot-2025-05-25-at-3.28.22-PM.png 1600w, https://micahflee.com/content/images/size/w2400/2025/05/Screenshot-2025-05-25-at-3.28.22-PM.png 2400w" sizes="(min-width: 1200px) 1200px"><figcaption><span>This Signal group has 3 users in it, and 10 messages.</span></figcaption></figure><p>The group details page shows you all of the information about that group, along with its list of users and messages. You can click the ID button for each user, or for each message, to get to details about that user or that message.</p><p>Here's what happens when I click on the ID button for one of the messages:</p><figure><img src="https://micahflee.com/content/images/2025/05/Screenshot-2025-05-25-at-3.33.54-PM.png" alt="" loading="lazy" width="2000" height="1392" srcset="https://micahflee.com/content/images/size/w600/2025/05/Screenshot-2025-05-25-at-3.33.54-PM.png 600w, https://micahflee.com/content/images/size/w1000/2025/05/Screenshot-2025-05-25-at-3.33.54-PM.png 1000w, https://micahflee.com/content/images/size/w1600/2025/05/Screenshot-2025-05-25-at-3.33.54-PM.png 1600w, https://micahflee.com/content/images/size/w2400/2025/05/Screenshot-2025-05-25-at-3.33.54-PM.png 2400w" sizes="(min-width: 1200px) 1200px"><figcaption><span>Detail view for this deleted Signal message.</span></figcaption></figure><p>The text of this Signal message, which seems to be deleted, extracted from a JSON objects that was found inside a heap dump file, is:</p><blockquote>Do you think our official videographer can go to the official only section given that message above?</blockquote><p>It was sent to this Signal group on May 4 at 8:33 am (I'm not entirely clear which time zone). If you scroll down on the message view, you can see the group and all the users in the group, followed by the JSON object itself – this is helpful to look at, just in case there was a bug in the crunching code or there are details in the JSON that TeleMessage Explorer didn't extract.</p><p>Here's the JSON object (lightly redacted) for that message, if you're curious:</p><pre><code>{
  "body": {
    "acceptedPayloadIdentifier": "3f9dbce0-a7df-45d7-82c4-f67bd311b8fd",
    "attachment": null,
    "ban": null,
    "callInfo": null,
    "direction": "IN",
    "groupData": {
      "id": "",
      "name": "POTUS | ROME-VATICAN | PRESS GC",
      "type": "BROADCAST"
    },
    "groupId": "",
    "groupMessage": false,
    "groupName": "POTUS | ROME-VATICAN | PRESS GC",
    "messageId": "DELETE_FOR_ME-==redacted==",
    "messageStatus": "NA",
    "messageTime": 1746372791000,
    "messageType": "EVENT",
    "originalMessageData": null,
    "owner": {
      "type": "PHONE",
      "value": "==redacted=="
    },
    "participantEnrichments": {},
    "partner": null,
    "recipients": [
      {
        "type": "PHONE",
        "value": "==redacted=="
      },
      {
        "type": "ALPHANUMERIC",
        "value": "==redacted=="
      },
      {
        "type": "PHONE",
        "value": "==redacted=="
      }
    ],
    "sender": {
      "type": "PHONE",
      "value": "==redacted=="
    },
    "subUserId": 0,
    "subject": "DELETED For Me - UNKNOWN Signal message from ==redacted== to chat group POTUS | ROME-VATICAN | PRESS GC",
    "text": "DELETED For Me - UNKNOWN DELETED For Me - UNKNOWN Signal message from ==redacted== to chat group POTUS | ROME-VATICAN | PRESS GC\n\nOriginal Message (Msg ID - 1745588825737_12028815188_0)\n\nDo you think our official videographer can go to the official only section given that message above?",
    "textField": {
      "extractor": {
        "data": "DELETED For Me - UNKNOWN DELETED For Me - UNKNOWN Signal message from ==redacted== to chat group POTUS | ROME-VATICAN | PRESS GC\n\nOriginal Message (Msg ID - 1745588825737_12028815188_0)\n\nDo you think our official videographer can go to the official only section given that message above?",
        "typ": "WrapperExt"
      },
      "length": 286
    },
    "threadID": "tm-357956699",
    "threadName": null
  },
  "gatewayReceivedDate": 1746372791228,
  "internalSecurityData": {
    "internalDecryptionData": {
      "encryptionType": "DO_NOTHING",
      "params": {},
      "typ": "nothing"
    },
    "version": "0.0.2"
  },
  "kafkafied": true,
  "networkType": "SIGNAL",
  "ownerExtClassId": null,
  "partner": "NONE",
  "securityContent": null,
  "sourceService": null,
  "sourceType": "SIGNAL",
  "typ": "RawMessage"
}</code></pre><p>Just like I searched groups for "signal", you can also use TeleMessage Explorer to search and filter users and message. </p><p>For example, here's a screenshot of me simply searching the messages for the word "trump":</p><figure><img src="https://micahflee.com/content/images/2025/05/Screenshot-2025-05-25-at-3.46.56-PM.png" alt="" loading="lazy" width="2000" height="1392" srcset="https://micahflee.com/content/images/size/w600/2025/05/Screenshot-2025-05-25-at-3.46.56-PM.png 600w, https://micahflee.com/content/images/size/w1000/2025/05/Screenshot-2025-05-25-at-3.46.56-PM.png 1000w, https://micahflee.com/content/images/size/w1600/2025/05/Screenshot-2025-05-25-at-3.46.56-PM.png 1600w, https://micahflee.com/content/images/size/w2400/2025/05/Screenshot-2025-05-25-at-3.46.56-PM.png 2400w" sizes="(min-width: 1200px) 1200px"><figcaption><span>Searching messages for "trump".</span></figcaption></figure><p>As you can see, there are some duplicates in here. And also as you can see, at least for some customers, TeleMessage collects SMS messages. The bottom two messages says:</p><blockquote>Unbelievable: MAGA calls for boycott of Costco for taking a stand against Trump Do you have a Costco membership? Y/N: go.demturnout.io/l/uq1Dxy DT Stop2End</blockquote><p>This is Democratic Party SMS spam, getting collected by TeleMessage.</p><p>Some of the messages include attachments that can be downloaded. Here's a WhatsApp message that includes an attachment:</p><figure><img src="https://micahflee.com/content/images/2025/05/Screenshot-2025-05-25-at-3.54.58-PM.png" alt="" loading="lazy" width="2000" height="1392" srcset="https://micahflee.com/content/images/size/w600/2025/05/Screenshot-2025-05-25-at-3.54.58-PM.png 600w, https://micahflee.com/content/images/size/w1000/2025/05/Screenshot-2025-05-25-at-3.54.58-PM.png 1000w, https://micahflee.com/content/images/size/w1600/2025/05/Screenshot-2025-05-25-at-3.54.58-PM.png 1600w, https://micahflee.com/content/images/size/w2400/2025/05/Screenshot-2025-05-25-at-3.54.58-PM.png 2400w" sizes="(min-width: 1200px) 1200px"><figcaption><span>Message with an attachment.</span></figcaption></figure><p>As you can see in the JSON object part, the attachment is Base64-encoded, and included directly in the JSON object that was extracted from the heap dump. For messages that have this sort of attachment, TeleMessage Explorer has a download button, which Base64-decodes the attachment and then saves it for you.</p><p>This attachment is a screenshot from a Bloomberg article about Saudi Arabia and OPEC+, taken on an iPhone, which also happens to be connected to a 5G network and is tethering internet.</p><figure><img src="https://micahflee.com/content/images/2025/05/untitled.jpeg" alt="" loading="lazy" width="738" height="1600" srcset="https://micahflee.com/content/images/size/w600/2025/05/untitled.jpeg 600w, https://micahflee.com/content/images/2025/05/untitled.jpeg 738w" sizes="(min-width: 720px) 720px"><figcaption><span>Attachment extracted from a message.</span></figcaption></figure><p>Finally, at the bottom of my <a href="https://micahflee.com/telemessage-customers-include-dc-police-andreesen-horowitz-jp-morgan-and-hundreds-more/" rel="noreferrer">recent post</a> that describes hundreds of TeleMessage customers, I explained that I found OAuth2 validation objects in the data, and I used those to make a list of customers.</p><p>You can explore these validation objects in TeleMessage Explorer at the Validations nav link at the top. Here's a screenshot of a search of the validation objects for ".gov" :</p><figure><img src="https://micahflee.com/content/images/2025/05/Screenshot-2025-05-25-at-4.03.29-PM.png" alt="" loading="lazy" width="2000" height="1392" srcset="https://micahflee.com/content/images/size/w600/2025/05/Screenshot-2025-05-25-at-4.03.29-PM.png 600w, https://micahflee.com/content/images/size/w1000/2025/05/Screenshot-2025-05-25-at-4.03.29-PM.png 1000w, https://micahflee.com/content/images/size/w1600/2025/05/Screenshot-2025-05-25-at-4.03.29-PM.png 1600w, https://micahflee.com/content/images/size/w2400/2025/05/Screenshot-2025-05-25-at-4.03.29-PM.png 2400w" sizes="(min-width: 1200px) 1200px"><figcaption><span>Validation objects search for ".gov".</span></figcaption></figure><p>There are 71 <code>.gov</code> email addresses in the validation objects. The usernames are all phone numbers. If you click the ID of one of these validation messages, it shows you any messages and groups in the dataset associated with that phone number.</p><h2 id="executive-office-of-the-president">Executive Office of the President</h2><p>By the way, one of those OAuth2 validations is for an email address that has the domain <code>who.eop.gov</code>. The <code>who</code> part is "White House Office," and the <code>eop</code> part is "Executive Office of the President."</p><p>I spent some time digging into this particular person based on his work phone number, work email address, and his name, all of which I found in this dataset.</p><p>I haven't yet found anything egregious, but here are some of the interesting highlights:</p><ul><li>He's a member of the "POTUS | ROME-VATICAN | PRESS GC" Signal group.</li><li>He's a young white conservative man.</li><li>Last August, before Trump's inauguration, he posted on LinkedIn that he was starting a position at Donald J. Trump For President 2024, Inc.</li><li>Also according to LinkedIn, he got his bachelor degree from the University of Alabama in 2024.</li><li>I found him on Tinder, and it included his birthday. He was born in 2001, and today is his birthday! He just turned 24.</li><li>Someone with his same username has logged 680 hours of Fortnite.</li><li>His Venmo transactions are open, lol.</li></ul><p><em>If you found this interesting, </em><a href="https://micahflee.com/telemessage-customers-include-dc-police-andreesen-horowitz-jp-morgan-and-hundreds-more/#/portal/signup" rel="noreferrer"><em>subscribe</em></a><em> to get these posts emailed directly to your inbox. If you want to support my work, considering becoming a paid supporter.</em></p>
                    
                </section>

                    

                
            </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google is burying the web alive (187 pts)]]></title>
            <link>https://nymag.com/intelligencer/article/google-ai-mode-search-results-bury-the-web.html</link>
            <guid>44097490</guid>
            <pubDate>Mon, 26 May 2025 14:00:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nymag.com/intelligencer/article/google-ai-mode-search-results-bury-the-web.html">https://nymag.com/intelligencer/article/google-ai-mode-search-results-bury-the-web.html</a>, See on <a href="https://news.ycombinator.com/item?id=44097490">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-editable="main" data-track-zone="main">  <article role="main" data-track-type="article-detail" data-uri="nymag.com/intelligencer/_components/article/instances/cmb16qnjt000l0iismxem2er8@published" data-content-channel="Technology" data-crosspost="" data-type="News-Commentary and Analysis" data-syndication="original" data-headline="Google Is Burying the Web Alive" data-authors="John Herrman" data-publish-date="2025-05-26" data-tags="screen time, google, artificial intelligence, search engines, the internet, big tech, ai chatbots" data-issue-date="" data-components-count="16" data-canonical-url="http://nymag.com/intelligencer/article/google-ai-mode-search-results-bury-the-web.html">


  
  
  
  <header>
    <div>
          

            <p><span>
                  <a href="https://nymag.com/author/john-herrman/" rel="author">
                    <img src="https://pyxis.nymag.com/v1/imgs/6a1/1fd/b7daa97d945a1ce337bc16618022639a01-john-herrman-intel.2x.rsquare.w168.jpg" alt="Portrait of John Herrman">
                  </a>
                </span>
            <span data-editable="bylines">
            <p><span>By</span> <span>
        ,
          <span>a tech columnist at Intelligencer</span><span>&nbsp;</span>
          <span>Formerly, he was a reporter and critic at the New York Times and co-editor of The Awl.</span>
      </span></p>

              </span>
          </p>
        </div>
      
    
  </header>
  <section>
    
    <div id="intelligencer-zephr-anchor" data-editable="content">
      <div>
          <div>
            <picture> <source media="(min-resolution: 192dpi) and (min-width: 1180px), (-webkit-min-device-pixel-ratio: 2) and (min-width: 1180px)" srcset="https://pyxis.nymag.com/v1/imgs/6c0/285/4cb731bcff029a47aa644ab8d45643cf9f-google-search-.2x.rhorizontal.w700.jpg 2x" width="700" height="467"> <source media="(min-width: 1180px) " srcset="https://pyxis.nymag.com/v1/imgs/6c0/285/4cb731bcff029a47aa644ab8d45643cf9f-google-search-.rhorizontal.w700.jpg" width="700" height="467"> <source media="(min-resolution: 192dpi) and (min-width: 768px), (-webkit-min-device-pixel-ratio: 2) and (min-width: 768px)" srcset="https://pyxis.nymag.com/v1/imgs/6c0/285/4cb731bcff029a47aa644ab8d45643cf9f-google-search-.2x.rhorizontal.w700.jpg 2x" width="700" height="467"> <source media="(min-width: 768px)" srcset="https://pyxis.nymag.com/v1/imgs/6c0/285/4cb731bcff029a47aa644ab8d45643cf9f-google-search-.rhorizontal.w700.jpg" width="700" height="467"> <source media="(min-resolution: 192dpi), (-webkit-min-device-pixel-ratio: 2)" srcset="https://pyxis.nymag.com/v1/imgs/6c0/285/4cb731bcff029a47aa644ab8d45643cf9f-google-search-.2x.rsquare.w400.jpg" width="400" height="400"> <img src="https://pyxis.nymag.com/v1/imgs/6c0/285/4cb731bcff029a47aa644ab8d45643cf9f-google-search-.rsquare.w400.jpg" data-content-img="" width="400" height="400" fetchpriority="high"> </picture>
          </div>
            <div>
              <p><span>Photo-Illustration: Intelligencer</span>
              </p>
            </div>
              </div>
        <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/cmb16skyv003l3b75xennbqe3@published" data-word-count="132">By now, there’s a good chance you’ve encountered <a href="https://nymag.com/intelligencer/article/what-if-googles-biggest-problem-isnt-ai.html">Google</a>’s <a href="https://nymag.com/intelligencer/article/why-ai-search-blew-up-in-googles-face.html">AI Overviews</a>, possibly thousands of times. Appearing as blurbs at the top of search results, they attempt to settle your queries before you scroll —&nbsp;to offer answers, or relevant information, gleaned from websites that you no longer need to click on. The feature was officially rolled out at Google’s developer conference last year and had been in testing for quite some time before that; on the occasion of this year’s conference, the company characterized it as “one of the most successful launches in Search in the past decade,” a strangely narrow claim that is almost certainly true: Google put AI summaries on top of everything else, <em>for everyone,</em> as if to say, “Before you use our main product, see if this works instead.”</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/cmb16smj6003u3b752xzbw234@published" data-word-count="166">This year’s conference included another change to search, this one more profound but less aggressively deployed. “AI Mode,” which has similarly been in beta testing for a while, will appear as an option for all users. It’s not like AI Overviews; that is, it’s not an extra module taking up space on a familiar search-results page but rather a complete replacement for conventional search. It’s Google’s “most powerful AI search, with more advanced reasoning and multimodality, and the ability to go deeper through follow-up questions and helpful links to the web,” the company says, “breaking down your question into subtopics and issuing a multitude of queries simultaneously on your behalf.” It’s available to everyone. It’s a lot like using AI-first chatbots that have search functions, like those from OpenAI, Anthropic, and Perplexity, and Google says it’s destined for greater things than a small tab. “As we get feedback, we’ll graduate many features and capabilities from AI Mode right into the core Search experience,” the company says.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/cmb16smj8003v3b75lnhxrn6u@published" data-word-count="95">I’ve been testing AI Mode for a few months now, and in some ways it’s less radical than it sounds and (at first) feels. It resembles the initial demos of AI search tools, including those by Google, meaning it responds to many questions with clean, ad-free answers. Sometimes it answers in extended plain language, but it also makes a lot of lists and pulls in familiar little gridded modules — especially when you ask about things you can buy —&nbsp;resulting in a product that, despite its chatty interface, feels an awful lot like … search.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/cmb16smj9003w3b75yjto7g9i@published" data-word-count="45">Again, now you can try it yourself, and your mileage may vary; it hasn’t drawn me away from Google proper for a lot of thoughtless rote tasks, but it’s competitive with <a href="https://nymag.com/intelligencer/article/openai-chatgpt-ai-cheating-education-college-students-school.html">ChatGPT</a> for the expanding range of searchish tasks you might attempt with a chatbot.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/cmb16smj9003x3b75nlrj12el@published" data-word-count="60">From the very first use, however, AI Mode crystallized something about Google’s priorities and in particular its relationship to the web from which the company has drawn, and returned, many hundreds of billions of dollars of value. AI Overviews demoted links, quite literally pushing content from the web <em>down </em>on the page, and summarizing its contents for digestion without clicking:</p>

  <div data-uri="nymag.com/intelligencer/_components/image/instances/cmb17xauj00583b75mot5rw3s@published" data-editable="settings">
    
    <p><span>Photo-Illustration: Intelligencer; Screenshot: Google</span>
    </p>
</div>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/cmb16smj9003y3b75lx2t35qn@published" data-word-count="55">Meanwhile, AI Mode all but <em>buries</em> them, not just summarizing their content for reading within Google’s product but inviting you to explore and expand on those summaries by asking more questions, rather than clicking out. In many cases, links are retained merely to provide backup and sourcing, included as footnotes and appendices rather than destinations:</p>

  <div data-uri="nymag.com/intelligencer/_components/image/instances/cmb17xl0d005e3b75x5hvvhym@published" data-editable="settings">
    
    <p><span>Photo-Illustration: Intelligencer; Screenshot: Google</span>
    </p>
</div>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/cmb16smj9003z3b758uul90hq@published" data-word-count="117">This is typical with AI search tools and all but inevitable now that such things are possible. <a href="https://nymag.com/intelligencer/article/why-ai-search-blew-up-in-googles-face.html">In terms of automation</a>, this means companies like OpenAI and Google are mechanizing some of the “work” that goes into using tools like Google search, removing, when possible, the step where users leave their platforms and reducing, in theory, the time and effort it takes to navigate to somewhere else when necessary. In even broader terms —&nbsp;contra Google’s effort to brand this as “going beyond information to intelligence” —&nbsp;this is an example of how LLMs offer <a href="https://nymag.com/intelligencer/article/for-big-tech-the-future-is-agi-what-about-the-rest-of-us.html">different ways to interact with much of the same information</a>: summarization rather than retrieval, regeneration rather than fact-finding, and vibe-y reconstruction over deterministic reproduction.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/cmb16smja00403b750cs445ua@published" data-word-count="60">This is interesting to think about and often compelling to use but leaves unresolved one of the <a href="https://nymag.com/intelligencer/2023/02/google-and-bing-are-a-mess-will-ai-solve-their-problems.html">first questions</a> posed by chatbots-as-search: Where will they get all the data they need to <em>continue</em> to work well? When Microsoft and Google showed off their first neo-search mockups in 2023, which are pretty close to today’s AI mode, it revealed <a href="https://nymag.com/intelligencer/2023/02/google-and-bing-are-a-mess-will-ai-solve-their-problems.html">a dilemma</a>:</p>

  <blockquote data-uri="nymag.com/intelligencer/_components/blockquote/instances/cmb17xx2u005m3b75pbw04d4t@published" data-editable="text" data-word-count="66"><p>Search engines still provide the de facto gateway to the broader web, and have a deeply codependent relationship with the people and companies whose content they crawl, index, and rank; a Google that instantly but sometimes unreliably summarizes the websites to which it used to send people would destroy that relationship, and probably a lot of websites, including the ones on which its models were trained.</p></blockquote>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/cmb16smjb00423b7540uam6rd@published" data-word-count="195">And, well, yep! Now, both AI Overviews and AI Mode, when they aren’t occasionally hallucinating, produce relatively clean answers that benefit in contrast to increasingly degraded regular search results on Google, which are full of hyperoptimized and duplicative spamlike content designed <a href="https://nymag.com/intelligencer/article/how-product-recommendations-broke-google.html">first and foremost</a> with the demands of Google’s ranking algorithms and advertising in mind. AI Mode feels one step further removed from that ecosystem and once again looks good in contrast, a placid textual escape from Google’s own mountain of links that look like ads and ads that look like links (of course, Google is already working on ads for both Overviews and AI Mode). In its drive to embrace AI, Google is further concealing the raw material that fuels it, demoting links as it continues to ingest them for abstraction. Google may still retain plenty of <em>attention </em>to monetize and perhaps keep even more of it for itself, now that it doesn’t need to send people elsewhere; in the process, however, it really <em>is</em> starving the web that supplies it with data on which to train and<em> </em>from which to draw up-to-date details. (Or, one might say, putting it out of its misery.)</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/cmb16smjb00433b754e1zh54e@published" data-word-count="84">Two years later, Google has become more explicit about the extent to which it’s moving on from the “you provide us results to rank, and we send you visitors to monetize” bargain, with the head of search telling <a href="https://www.theverge.com/google-io/670439/google-ai-mode-search-io-2025">The Verge</a>, “I think the search results page was a construct.” Which is true, as far as it goes, but also a remarkable thing to hear from a company that’s communicated carefully and voluminously to website operators about small updates to its search algorithms for years.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/cmb16smjb00443b75nurt0wq2@published" data-word-count="152">I don’t doubt that Google has been thinking about this stuff for a while and that there are people at the company who deem it strategically irrelevant or at least of secondary importance to winning the AI race —&nbsp;the <em>fate of the web</em> might not sound terribly important when your bosses are talking nonstop about cashing out its accumulated data and expertise for <a href="https://nymag.com/intelligencer/article/for-big-tech-the-future-is-agi-what-about-the-rest-of-us.html">AGI</a>. I also don’t want to be precious about the web as it actually exists in 2025, nor do I suggest that websites working with or near companies like Meta and Google should have expected anything but <a href="https://nymag.com/intelligencer/2023/04/how-buzzfeed-news-went-bust.html">temporary, incidental alignment</a> with their businesses. If I had to guess, the future of Google search looks more like AI Overviews than AI mode —&nbsp;a jumble of widgets and modules including and united by AI-generated content, rather than a clean break — if only for purposes of sustaining Google’s multi-hundred-billion-dollar advertising business.</p>

  <p data-editable="text" data-uri="nymag.com/intelligencer/_components/clay-paragraph/instances/cmb16smjb00453b75vrxcg0bj@published" data-word-count="197">But I also don’t want to assume Google knows exactly how this stuff will play out for<em> Google</em>, much less what it will actually mean for millions of websites, and their visitors, if Google stops sending as many people beyond its results pages. Google’s push into productizing generative AI is substantially fear-driven, faith-based, and informed by the actions of competitors that are far less invested in and dependent on the vast collection of behaviors —&nbsp;websites full of content authentic and inauthentic, volunteer and commercial, social and antisocial, archival and up-to-date —&nbsp;that make up what’s left of the web and have far less to lose. Maybe, in a few years, a <a href="https://stratechery.com/2025/the-agentic-web-and-original-sin/">fresh economy</a> will grow around the new behaviors produced by searchlike AI tools; perhaps companies like OpenAI and Google will sign a bunch more <a href="https://nymag.com/intelligencer/2023/12/what-do-ai-companies-want-with-the-media.html">licensing</a> deals; conceivably, this style of search automation simply collapses the marketplace supported by search, leveraging training based on years of scraped data to <a href="https://nymag.com/intelligencer/article/ai-ate-the-web-now-its-coming-back-for-seconds.html">do more with less</a>. In any case, the signals from Google —&nbsp;despite its <a href="https://searchengineland.com/google-ai-overviews-search-clicks-fell-report-455498">unconvincing suggestions</a> to the contrary — are clear: It’ll do anything to win the AI race. If that means burying the web, then so be it.</p>

  


    </div>

    


          



      <span>Google Is Burying the Web Alive</span>



  </section>
  
</article>

  

</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Engineers discover new class of materials that passively harvest water from air (338 pts)]]></title>
            <link>https://blog.seas.upenn.edu/penn-engineers-discover-a-new-class-of-materials-that-passively-harvest-water-from-air/</link>
            <guid>44097144</guid>
            <pubDate>Mon, 26 May 2025 13:14:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.seas.upenn.edu/penn-engineers-discover-a-new-class-of-materials-that-passively-harvest-water-from-air/">https://blog.seas.upenn.edu/penn-engineers-discover-a-new-class-of-materials-that-passively-harvest-water-from-air/</a>, See on <a href="https://news.ycombinator.com/item?id=44097144">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-8868">
	
		
		
		<!-- .entry-header -->
		
	<div>
		<p><span>A serendipitous observation in a Chemical Engineering lab at Penn Engineering has led to a surprising discovery: a new class of nanostructured materials that can pull water from the air, collect it in pores and release it onto surfaces without the need for any external energy. <a href="https://www.science.org/doi/10.1126/sciadv.adu8349">The research</a>, published in </span><i><span>Science Advances</span></i><span>, was conducted by an interdisciplinary team, including </span><a href="https://directory.seas.upenn.edu/daeyeon-lee/"><span>Daeyeon Lee</span></a><span>, Russell Pearce and Elizabeth Crimian Heuer Professor in Chemical and Biomolecular Engineering (CBE), </span><a href="https://directory.seas.upenn.edu/amish-j-patel/"><span>Amish Patel</span></a><span>, Professor in CBE, Baekmin Kim, a postdoctoral scholar in Lee’s lab and first author, and </span><a href="https://www.professoren.tum.de/en/guldin-stefan"><span>Stefan Guldin</span></a><span>, Professor in Complex Soft Matter at the Technical University of Munich. Their work describes a material</span> <span>that</span> <span>could open the door to new ways to collect water from the air in arid regions and devices that cool electronics or buildings using the power of evaporation.</span></p>
<figure id="attachment_8869" aria-describedby="caption-attachment-8869"><img decoding="async" src="https://blog.seas.upenn.edu/wp-content/uploads/2025/05/Daeyeon-Lee-Amish-Patel-research-PR_Amphiphilic-nanopore-team-headshots.png" alt="" width="2240" height="1260" srcset="https://blog.seas.upenn.edu/wp-content/uploads/2025/05/Daeyeon-Lee-Amish-Patel-research-PR_Amphiphilic-nanopore-team-headshots.png 2240w, https://blog.seas.upenn.edu/wp-content/uploads/2025/05/Daeyeon-Lee-Amish-Patel-research-PR_Amphiphilic-nanopore-team-headshots-300x169.png 300w, https://blog.seas.upenn.edu/wp-content/uploads/2025/05/Daeyeon-Lee-Amish-Patel-research-PR_Amphiphilic-nanopore-team-headshots-1024x576.png 1024w, https://blog.seas.upenn.edu/wp-content/uploads/2025/05/Daeyeon-Lee-Amish-Patel-research-PR_Amphiphilic-nanopore-team-headshots-768x432.png 768w, https://blog.seas.upenn.edu/wp-content/uploads/2025/05/Daeyeon-Lee-Amish-Patel-research-PR_Amphiphilic-nanopore-team-headshots-1536x864.png 1536w, https://blog.seas.upenn.edu/wp-content/uploads/2025/05/Daeyeon-Lee-Amish-Patel-research-PR_Amphiphilic-nanopore-team-headshots-2048x1152.png 2048w" sizes="(max-width: 2240px) 100vw, 2240px"><figcaption id="caption-attachment-8869">Research team including Daeyone Lee (left), Amish Patel (center) and Stefan Guldin (right).</figcaption></figure>
<p><span>“We weren’t even trying to collect water,” says Lee. “We were working on another project testing the combination of hydrophilic nanopores and hydrophobic polymers when R Bharath Venkatesh, a former Ph.D. student in our lab, noticed water droplets appearing on a material we were testing. It didn’t make sense. That’s when we started asking questions.”</span></p>
<p><span>Those questions led to an in-depth study of a new type of amphiphilic nanoporous material: one that blends water-loving (hydrophilic) and water-repelling (hydrophobic) components in a unique nanoscale structure. The result is a material that both captures moisture from air and simultaneously pushes that moisture out as droplets.</span></p>
<p><b>Water-Collecting Nanopores</b></p>
<p><span>When water condenses on surfaces, it usually requires either a drop in temperature or very high humidity levels. Conventional water harvesting methods rely on these principles, often requiring energy input to chill surfaces or a dense fog to form to collect water passively from humid environments. But Lee and Patel’s system works differently.</span></p>
<p><span>Instead of cooling, their material relies on capillary condensation, a process where water vapor condenses inside tiny pores even at lower humidity. This is not new. What </span><i><span>is</span></i><span> new is that in their system, the water doesn’t just stay trapped inside the pores, as it usually does in these types of materials.</span></p>
<p><span>“In typical nanoporous materials, once the water enters the pores, it stays there,” explains Patel. “But in our material, the water moves, first condensing inside the pores, then emerging onto the surface as droplets. That’s never been seen before in a system like this, and at first we doubted our observations.”</span></p>
<p><b>A Material That Defies Physics&nbsp;</b></p>
<p><span>Before they understood what was happening, the researchers first thought that water was simply condensing onto the surface of the material due to</span> <span>an artifact of their experimental setup, such as a temperature gradient in the lab. To rule that out, they increased the thickness of the material to see if the amount of water collected on the surface would change.&nbsp;</span></p>
<p><span>“If what we were observing was due to surface condensation alone, the thickness of the material wouldn’t change the amount of water present,” explains Lee.&nbsp;</span></p>
<p><span>But, the total amount of water collected increased as the film’s thickness increased, proving that the water droplets forming on the surface came from inside the material.</span></p>
<p><span>Even more surprising: the droplets didn’t evaporate quickly, as thermodynamics would predict.&nbsp;</span></p>
<p><span>“According to the curvature and size of the droplets, they should have been evaporating,” says Patel. “But they were not; they remained stable for extended periods.”</span></p>
<p><span>With a material that could potentially defy the laws of physics on their hands, Lee and Patel sent their design off to a collaborator to see if their results were replicable.&nbsp;</span></p>
<p><span>“We study porous films under a wide range of conditions, using subtle changes in light polarization to probe complex nanoscale phenomena,” says Guldin. “But we’ve never seen anything like this. It’s absolutely fascinating and will clearly spark new and exciting research.”</span></p>
<p><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->
<video id="video-8868-1" width="1160" height="653" preload="metadata" controls="controls"><source type="video/mp4" src="https://blog.seas.upenn.edu/wp-content/uploads/2025/05/Daeyeon-Lee-Amish-Patel-research-PR_Amphiphilic-nanopore-video.mp4?_=1"><a href="https://blog.seas.upenn.edu/wp-content/uploads/2025/05/Daeyeon-Lee-Amish-Patel-research-PR_Amphiphilic-nanopore-video.mp4">https://blog.seas.upenn.edu/wp-content/uploads/2025/05/Daeyeon-Lee-Amish-Patel-research-PR_Amphiphilic-nanopore-video.mp4</a></video></p>
<p><span>Nanopores in action under electron microscope imagery showing water droplets forming and being replenished by stored water in the material itself.</span></p>
<p><b>A Stabilized Cycle of Condensation and Release</b></p>
<p><span>It turns out that they had created a material with just the right balance of water-attracting nanoparticles and water-repelling plastic — polyethylene — to create a nanoparticle film with this special property.</span></p>
<p><span>“We accidentally hit the sweet spot,” says Lee. “The droplets are connected to hidden reservoirs in the pores below. These reservoirs are continuously replenished from water vapor in the air, creating a feedback loop made possible by this perfect balance of water-loving and water-repelling materials.”</span></p>
<p><b>A Platform for Passive Water Harvesting and More</b></p>
<p><span>Beyond the physics-defying behavior, the materials’ simplicity is part of what makes them so promising. Made from common polymers and nanoparticles using scalable fabrication methods, these films could be integrated into passive water harvesting devices for arid regions, surfaces for cooling electronics or smart coatings that respond to ambient humidity.</span></p>
<p><span>“We’re still uncovering the mechanisms at play,” says Patel. “But the potential is exciting. We’re learning from biology — how cells and proteins manage water in complex environments — and applying that to design better materials.”</span></p>
<p><span>“This is exactly what Penn does best, bringing together expertise in chemical engineering, materials science, chemistry and biology to solve big problems,” adds Lee.</span></p>
<p><span>The next steps include studying how to optimize the balance of hydrophilic and hydrophobic components, scale the material for real-world use and investigating how to make the collected droplets roll off surfaces efficiently.</span></p>
<p><span>Ultimately, the researchers hope this discovery will lead to technologies that offer clean water in dry climates or more sustainable cooling methods using only the water vapor already in the air.</span></p>
<p><i><span>This work was supported by National Science Foundation grants NSF-2309043 and NSF-1933704, a Department of Energy grant (DE-SC0021241), a Semilab UCL Chemical Engineering Impact Ph.D. Studentship, a National Science Foundation Graduate Research Fellowships Program grant (DGE-2236662), an Alfred P. Sloan Research Foundation grant (FG-2017-9406) and a Camille &amp; Henry Dreyfus Foundation grant (TG-19-033).</span></i></p>
	</div><!-- .entry-content -->
	
	<!-- .entry-footer -->
	
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Particle Life simulation in browser using WebGPU (170 pts)]]></title>
            <link>https://lisyarus.github.io/blog/posts/particle-life-simulation-in-browser-using-webgpu.html</link>
            <guid>44096808</guid>
            <pubDate>Mon, 26 May 2025 12:28:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lisyarus.github.io/blog/posts/particle-life-simulation-in-browser-using-webgpu.html">https://lisyarus.github.io/blog/posts/particle-life-simulation-in-browser-using-webgpu.html</a>, See on <a href="https://news.ycombinator.com/item?id=44096808">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-content">
            
            <br><center><p>Particle Life simulation in browser using WebGPU</p></center><br>
            <center><span>2025 May 15</span></center>

            <p><i>If you don't care about the explanation and want a direct link to the simulation, <a href="https://lisyarus.github.io/webgpu/particle-life.html">here you go</a>.</i></p>

            <p>You might know that I'm a <a href="https://lisyarus.github.io/blog/posts/soft-body-physics.html">sucker</a> for <a href="https://lisyarus.github.io/blog/posts/structural-mechanics-2.html">physics</a> <a href="https://lisyarus.github.io/blog/posts/simulating-water-over-terrain.html">simulations</a>, and <a href="https://lisyarus.itch.io/particle-simulator">particle simulations</a> in particular. Usually I implement something based on conventional physics, but recently I've stumbled upon a funny non-physical model that can display...well, let's call it <i>life-like</i> behavior.</p>

            <p>I've made a prototype in C++ using my pet engine, but then I decided it would be fun to try making it run in browser, using the WebGPU API. It works surprisingly well, and can produce fun simulations like this:</p>

            <center><video muted="" loop="" controls=""><source src="https://lisyarus.github.io/blog/media/particle-life/simulation-1.mp4" type="video/mp4"></video></center>

            <p>In this post I'll describe how it works under the hood.</p>

            <h2>Contents</h2>
            <ul id="contents"></ul>

            <h2 id="section-the-particle-life-model">The Particle Life Model</h2>

            <p>There are quite a number of resources (primarily, videos) online that talk about this model, for example <a href="https://www.youtube.com/watch?v=4vk7YvBYpOs">this</a>, <a href="https://www.youtube.com/watch?v=p4YirERTVF0">this</a>, and <a href="https://www.youtube.com/watch?v=Z_zmZ23grXE">this</a>, and also <a href="https://www.youtube.com/watch?v=scvuli-zcRc&amp;t=396s">this one</a> talks about the exact formulas used in the simulation. Due to a rather pretentious name composed of two overused words, googling "particle life" doesn't bring stuff of any relevance (apart from the YouTube videos), so I wasn't able to trace the origins of this model. <a href="https://www.youtube.com/watch?v=xiUpAeos168">One video</a> claims that this model was inspired by some microbiology studies, which seems believable given how organic and mobile the resulting simulations look.</p>

            <p>This model isn't in any way related to Conway's Game of Life (despite the name). It's more like <a href="https://google-research.github.io/self-organising-systems/particle-lenia">Particle Lenia</a>, if anything, though the rules are much simpler.</p>

            <p>The core idea goes like this: we run a typical physics simulation with point particles, but the forces between particles can be <i>asymmetric</i>: particle A can <i>attract</i> particle B, but particle B can <i>repel</i> particle A (or attract it with a larger/smaller force, etc). This immediately violates Newton's third law, thus it violates almost all conservation laws (specifically, energy, momentum, and angular momentum conservation).</p>

            <p>This might sound strange and unphysical, but</p>

            <ol>
                <li>We're not bound by physical laws when coding simulations,</li>
                <li>This brings an extra source of energy, which is kinda what living organisms do.</li>
            </ol>

            <p>Living creatures consume food and oxygen (and sometimes other chemicals) and turn them into energy, which they can use to move and find more consumable stuff. The asymmetric forces of Particle Life emulate that by skipping the chemicals-to-energy cycle and just adds extra energy to the system.</p>

            <p>It also allows particles to chase each other, e.g. in the scenario above particle B will constantly chase after particle A, which will in turn run away from particle B, which also kinda emulates predator-prey stuff from biology. However, in this model particles aren't created or destroyed, so particle B will never actually "eat" particle A.</p>

            <p>For simplicity and to add some coherence to the simulation, particles are usually split into several <i>types</i>, with a table specifying the interaction for each pair of particle types. Specifically, I used the model suggested by <a href="https://www.youtube.com/watch?v=xiUpAeos168">this video</a>:</p>

            <ul>
                <li>For any two particle types A and B, the force experienced by A from B consists of two parts: the <i>interaction force</i>, and the <i>collision force</i>.</li>
                <li>Interaction force can be either attractive or repulsive.</li>
                <li>Collision force is always repulsive.</li>
                <li>Both forces are specified by a radius and a strength factor. Positive strength means attraction, negative strength means repulsion. (For collision force it is always repulsion).</li>
                <li>Collision force radius is always smaller than interaction force radius (i.e. collision is close-range, interaction is far-range).</li>
                <li>Collision force strength is always larger than interaction force strength.</li>
                <li>Both forces decrease linearly with distance, upon reaching the radius value, at which they are zero.</li>
            </ul>

            <p>In formulas, the linear dependence of force on distance can be written as</p><p>

            \[ F(r) = A\cdot \max\left(0, \frac{|r-R|}{R}\right) \]

            </p><p>where \(A\) is the force strength (negative for repulsion), \(R\) is the force radius, and \(r\) is the distance between the two particles.</p>

            <p>Here's a plot of force as a function of distance, with \(A=5\) and \(R=10\):</p>

            <center><img src="https://lisyarus.github.io/blog/media/particle-life/plot-force.png"></center>

            <p>And here's the corresponding potential energy, if you're into some physics:</p>

            <center><img src="https://lisyarus.github.io/blog/media/particle-life/plot-energy.png"></center>

            <p>Here's the <a href="https://www.desmos.com/calculator/ram14bkp1r">Desmos link</a> where you can play with these values.</p>

            <p>If we add the collision force and, say, an attraction force, we get this plot of force vs distance:</p>

            <center><img src="https://lisyarus.github.io/blog/media/particle-life/plot-force-combined.png"></center>

            <p>And the potential energy:</p>

            <center><img src="https://lisyarus.github.io/blog/media/particle-life/plot-energy-combined.png"></center>

            <p>Again, here's the <a href="https://www.desmos.com/calculator/nebtb6ltme">Desmos link</a> for these.</p>

            <p>As you can see, for the collision + attraction case, there's a local minimum at some distance, where the force is equal to zero. This would be the "sweet spot" for particles, i.e. they'll tend to maintain this distance if possible.</p>

            <p>Notice that the force (and thus also the energy) doesn't go to infinity when distance is close to zero, like it does in a typical \(\frac{1}{r}\) potential from gravity or electromagnetism. This means that our collision force has finite strength, and can be overcome by sufficiently strong other forces, or if the collision radius is small enough. E.g. on this screenshot multiple particles merged into one:</p>

            <center><img src="https://lisyarus.github.io/blog/media/particle-life/merged.png"></center>

            <p>This is fine, though, since it produces interesting behaviours, and it prevents our forces from escalating too much (which typically happens with a \(\frac{1}{r}\) potential simulations).</p>

            <p>I also have a global constant friction parameter which simply decreases all velocities with time. This is to keep the simulation somewhat stable, because otherwise it constantly gains energy, and everything just accelerates to infinity.</p>

            <h2 id="section-why-webgpu">Why WebGPU?</h2>

            <p>It should be clear why I decided to move the simulation to the GPU — after all, that's a typical massively parallel task, and GPUs shine at those. But why specifically WebGPU?</p>

            <p>If you've been following me for a while, you might know that I'm in awe with WebGPU. I've been using OpenGL for more than a decade and I'm sick of it. It's a very old, stupidly inconsistent API full of legacy decisions, which stopped evolving some 8 years ago. However, Vulkan, which is the supposed alternative modern API, takes 1000 lines of setup work <i>just to show a triangle on screen</i>, without even any vertex buffers &amp; attributes. Don't get me wrong, Vulkan is pretty cool, but I just don't have <i>that</i> much spare time to use it in my pet projects.</p>

            <p>Now, WebGPU fills this niche for me. It is a modern &amp; clean API that at the same time is reasonably non-verbose. It feels just like what real-time graphics should be about, — or at least that's what I feel while using it. I already used it in a few desktop projects (via <a href="https://github.com/gfx-rs/wgpu-native">wgpu-native</a>), including a <a href="https://github.com/lisyarus/webgpu-raytracer">full Monte-Carlo raytracer</a> and a <a href="https://github.com/lisyarus/webgpu-shallow-water">shallow water simulator</a>, and I'm using it in my current main project, which is a <a href="https://www.youtube.com/playlist?list=PLSGI94QoFYJwGaieAkqw5_qfoupdppxHN">medieval village building game</a>, and I'm really happy with this API.</p>

            <p>Furthermore, being a modern API, it supports stuff like compute shaders and atomics, which are crucial for this simulation, as we'll see later. As far as I know, WebGL doesn't support these features.</p>

            <p>And, as a nice bonus, it actually runs in browser! Or, at least in some experimental versions of some browsers, under some flags, but the situation should get better with time.</p>

            <h2 id="section-simulation-loop">Simulation loop</h2>

            <p>So, how do we simulate particles with WebGPU? The simulation loop itself is rather straightforward:</p>

            <ol>
                <li>If not paused,</li>
                <ol>
                    <li>For each particle, compute the forces acting on this particle by other particles, and add the resulting acceleration to the particle's velocity</li>
                    <li>For each particle, move its position using the newly-computed velocity</li>
                    <li>Apply boundary conditions</li>
                </ol>
                <li>Render the particles</li>
            </ol>

            <p>This is essentially the <a href="https://en.wikipedia.org/wiki/Semi-implicit_Euler_method">semi-implicit Euler integrator</a>, which is easy to code and has some nice theoretical properties.</p>

            <p>I store the particles in a single large GPU buffer. A particle is just a simple struct like this:</p>

            <pre><code>struct Particle
{
    // Position
    x : f32,
    y : f32,

    // Velocity
    vx : f32,
    vy : f32,

    // Type
    species : f32,
}</code></pre>

            <p><i>I'm not storing position and velocity as <code>vec2f</code> just for alignment requirements, and I'm storing the particle type as an <code>f32</code> just to be able to load particles from JavaScript as a single large <code>Float32Array</code>.</i></p>

            <p>So, a particle is just 5 floats, i.e. 20 bytes. In theory, we could just run a single compute shader per particle, which loops over all particles, computes all the forces, and updates velocity and position. However, computing forces quickly becomes the bottleneck: it's a \(O(N^2)\) operation in total, because we need to compute all pairwise forces between all particles. This severely limits the number of particles we can process. My earlier CPU implementation only got to 4096 particles, even with multithreading.</p>

            <p>We need to be clever about computing forces, which is why this is a separate compute pass. After that, things are pretty straightforward: a single compute shader invocation for each particle moves it forward using the computed velocity, then handles collisions with simulation borders:</p>

            <pre><code>@group(0) @binding(0) var&lt;storage, read_write&gt; particles : array&lt;Particle&gt;;

@group(1) @binding(0) var&lt;uniform&gt; simulationOptions : SimulationOptions;

@compute @workgroup_size(64)
fn particleAdvance(@builtin(global_invocation_id) id : vec3u)
{
    // Protect against reading &amp; writing past array end
    if (id.x &gt;= arrayLength(&amp;particles)) {
        return;
    }

    let width = simulationOptions.right - simulationOptions.left;
    let height = simulationOptions.top - simulationOptions.bottom;

    var particle = particles[id.x];

    // Apply friction
    particle.vx *= simulationOptions.friction;
    particle.vy *= simulationOptions.friction;

    // Move particle using velocity
    particle.x += particle.vx * simulationOptions.dt;
    particle.y += particle.vy * simulationOptions.dt;

    // Collide with borders
    if (particle.x &lt; simulationOptions.left) {
        particle.x = simulationOptions.left;
        particle.vx *= -1.0;
    }

    if (particle.x &gt; simulationOptions.right) {
        particle.x = simulationOptions.right;
        particle.vx *= -1.0;
    }

    if (particle.y &lt; simulationOptions.bottom) {
        particle.y = simulationOptions.bottom;
        particle.vy *= -1.0;
    }

    if (particle.y &gt; simulationOptions.top) {
        particle.y = simulationOptions.top;
        particle.vy *= -1.0;
    }

    // Write the new state of the particle
    particles[id.x] = particle;
}</code></pre>

            <p><i>(My actual code also handles mouse interaction &amp; looping borders, but the idea is the same.)</i></p>

            <p>The <code>simulationOptions.friction</code> is actually something like \(\exp(-\text{friction}\cdot\Delta t)\), for reasons explained in <a href="https://lisyarus.github.io/blog/posts/exponential-smoothing.html">my other article</a>.</p>

            <h2 id="section-computing-interactions">Computing interactions</h2>

            <p>Before advancing the particles, though, we need to compute the inter-particle forces. The main trick is to use the fact that all forces have finite radius, and after a certain distance all forces are zero. In my implementation, I only allow forces with radius up to 32. Thus, I know that any pair of particles further than 32 units of distance apart cannot interact at all.</p>

            <p>To utilize this, we use the typical approach of spatial hashing/binning: make a square grid with cells/bins of size \(32\times 32\), sort the particles into these bins, then compute interactions only between neighbouring bins. This sounds easy, and is actually almost trivial to implement on the CPU (e.g. in C++ just make a 2D array, each storing <code>std::vector&lt;particle&gt;</code>, and you're good to go). However, building such data structures on the GPU takes a lot of effort — we can't, for example, allocate GPU memory directly from a shader!.</p>

            <p>Instead, we'll use a linearized structure to store the bins. Here's how it works: we'll have an array of particles, where particles that reside in the same bin occupy a contiguous section of this array. Then, we'll also have an array which stores the start of this section (i.e. the offset) for each bin. We don't need to store the end of it, because the end of a bin is exactly the start of the next bin! We could also store the particle IDs instead of particles themselves, but that's another indirection which could hurt memory access performance later, when we compute the forces.</p>

            <p>To do this, we use a typical three-phase approach:</p>

            <ul>
                <li>Phase 1: compute the number of particles in each bin</li>
                <li>Phase 2: run parallel prefix sum to compute the offset for each bin</li>
                <li>Phase 3: place each particle into the corresponding bin</li>
            </ul>

            <p>Phases 1 and 3 make heavy use of shader atomics. Phase 1 is fairly simple — run a compute shader for each particle, compute the (linearized) bin index for this particle, and increment the size of this bin:</p>

            <pre><code>@group(0) @binding(0) var&lt;storage, read&gt; particles : array&lt;Particle&gt;;

@group(1) @binding(0) var&lt;uniform&gt; simulationOptions : SimulationOptions;

@group(2) @binding(0) var&lt;storage, read_write&gt; binSize : array&lt;atomic&lt;u32&gt;&gt;;

@compute @workgroup_size(64)
fn fillBinSize(@builtin(global_invocation_id) id : vec3u)
{
    if (id.x &gt;= arrayLength(&amp;particles)) {
        return;
    }

    // Read the particle data
    let particle = particles[id.x];

    // Compute the linearized bin index
    let binIndex = getBinInfo(vec2f(particle.x, particle.y),
        simulationOptions).binIndex;

    // Increment the size of the bin
    atomicAdd(&amp;binSize[binIndex + 1], 1u);
}</code></pre>

            <p>Since this is done in parallel by many shader invocations, we need to use atomics here. I'm actually incrementing array at index <code>binIndex + 1</code>, and leaving the 0-th element at the value of 0, because this way <code>binOffset[i + 1] - binOffset[i]</code> always gives you the size of the i-th bin. Note that in this case the <code>binSize</code> array size is 1 plus the actual number of bins.</p>

            <p>Here, <code>getBinInfo</code> is just some helper function that computes the linearized bin index from the particle position and the size of the whole grid of bins:</p>

            <pre><code>fn getBinInfo(position : vec2f, simulationOptions : SimulationOptions) -&gt; BinInfo
{
    let binSize = simulationOptions.binSize;
    let gridSize = simulationOptions.gridSize;

    let binId = vec2i(
        clamp(i32(floor((position.x - simulationOptions.left) / binSize)), 0, gridSize.x - 1),
        clamp(i32(floor((position.y - simulationOptions.bottom) / binSize)), 0, gridSize.y - 1)
    );

    let binIndex = binId.y * gridSize.x + binId.x;

    return BinInfo(binId, binIndex);
}</code></pre>

            <p><i>(N.B.: in my actual code, <code>gridSize</code> is not passed as a uniform, but computed in-place, because I did it this way first and then never changed that.)</i></p>

            <p>In Phase 2, we need to convert the array of bin sizes into an array of bin offsets. Since the particles from the same bin go continuously in memory, a bin with some index starts when we've already put all the particles from bins with smaller indices into the array. This means that the offset \(O[i]\) of the i-th bin is simply the sum of the sizes \(S[j]\) of all preceding bins: \(O[i] = \sum\limits_{j=0}^{i-1} S[j]\). In pseudocode:</p>

            <pre><code>offset[0] = 0;
for (int i = 1; i &lt; binCount; ++i)
    binOffset[i] = binOffset[i - 1] + binSize[i];</code></pre>

            <p>This is known as computing <i>prefix sums</i>. This is, of course, not a parallel algorithm. To harness the power of the GPU, we'd want to run a parallel algorithm that does that. This is known, of course, as the <i>parallel prefix sum</i> problem, and we'll talk about it in the next section.</p>

            <p>For now, let's assume we've computed these offsets somehow. In Phase 3, we need to place ("sort") the particles into a new array, based on which bin this particle belongs to. We do this by running a per-particle compute shader once again. Because we want to put different particles into different indices in the new array, we need to use atomics again:</p>

            <pre><code>@group(0) @binding(0) var&lt;storage, read&gt; source : array&lt;Particle&gt;;
@group(0) @binding(1) var&lt;storage, read_write&gt; destination : array&lt;Particle&gt;;
@group(0) @binding(2) var&lt;storage, read&gt; binOffset : array&lt;u32&gt;;
@group(0) @binding(3) var&lt;storage, read_write&gt; binSize : array&lt;atomic&lt;u32&gt;&gt;;

@group(1) @binding(0) var&lt;uniform&gt; simulationOptions : SimulationOptions;

@compute @workgroup_size(64)
fn sortParticles(@builtin(global_invocation_id) id : vec3u)
{
    if (id.x &gt;= arrayLength(&amp;source)) {
        return;
    }

    // Read the particle data
    let particle = particles[id.x];

    // Compute the linearized bin index
    let binIndex = getBinInfo(vec2f(particle.x, particle.y),
        simulationOptions).binIndex;

    // Atomically compute the index of this particle
    // within the corresponding bin
    let indexInBin = atomicAdd(&amp;binSize[binIndex], 1);

    // Write the particle into the sorted array
    let newParticleIndex = binOffset[binIndex] + indexInBin;
    destination[newParticleIndex] = particle;
}</code></pre>

            <p>Here, we need to zero out the <code>binSize</code> array before running this shader. Again, since multiple shaders run in parallel, we use atomics to compute the index of a particle within its bin.</p>

            <p>Once we've done that, we can easily iterate over particles of a specific bin, which allows us to run the actual shader that computes per-particle forces, by only iterating over the neighbouring bins. Once again, a single shader invocation per each particle:</p>

            <pre><code>@group(0) @binding(0) var&lt;storage, read_write&gt; particles : array&lt;Particle&gt;;
@group(0) @binding(1) var&lt;storage, read&gt; binOffset : array&lt;u32&gt;;
@group(0) @binding(2) var&lt;storage, read&gt; forces : array&lt;Force&gt;;

@group(1) @binding(0) var&lt;uniform&gt; simulationOptions : SimulationOptions;

@compute @workgroup_size(64)
fn computeForces(@builtin(global_invocation_id) id : vec3u)
{
    if (id.x &gt;= arrayLength(&amp;particles)) {
        return;
    }

    // Read particle data
    var particle = particles[id.x];
    let type = u32(particle.type);

    // Compute the bin that contains this particle
    let binInfo = getBinInfo(vec2f(particle.x, particle.y), simulationOptions);

    // Compute the range of neighbouring bins for iteration
    var binXMin = binInfo.binId.x - 1;
    var binYMin = binInfo.binId.y - 1;
    var binXMax = binInfo.binId.x + 1;
    var binYMax = binInfo.binId.y + 1;

    // Guard against grid boundaries
    binXMin = max(0, binXMin);
    binYMin = max(0, binYMin);
    binXMax = min(binInfo.gridSize.x - 1, binXMax);
    binYMax = min(binInfo.gridSize.y - 1, binYMax);

    let particlePosition = vec2f(particle.x, particle.y);

    var totalForce = vec2f(0.0, 0.0);

    // Iterate over neighbouring bins
    for (var binX = binXMin; binX &lt;= binXMax; binX += 1) {
        for (var binY = binYMin; binY &lt;= binYMax; binY += 1) {
            // Compute linearized bin index
            let binIndex = binY * binInfo.gridSize.x + binX;

            // Find the range of particles from this bin
            let binStart = binOffset[binIndex];
            let binEnd = binOffset[binIndex + 1];

            // Iterate over particles from this bin
            for (var j = binStart; j &lt; binEnd; j += 1) {
                // Prevent self-interaction
                if (j == id.x) {
                    continue;
                }

                let other = particlesSource[j];
                let otherType = u32(other.type);

                // Apply the inter-particle force

                let force = forces[type * u32(simulationOptions.typeCount) + otherType];

                var r = vec2f(other.x, other.y) - particlePosition;
                let d = length(r);

                if (d &gt; 0.0) {
                    let n = r / d;

                    totalForce += force.strength * max(0.0, 1.0 - d / force.radius) * n;
                    totalForce -= force.collisionStrength * max(0.0, 1.0 - d / force.collisionRadius) * n;
                }
            }
        }
    }

    // Update velocity, assuming unit mass
    particle.vx += totalForce.x * simulationOptions.dt;
    particle.vy += totalForce.y * simulationOptions.dt;

    particles[id.x] = particle;
}</code></pre>

            <p><i>(My actual code also includes a central force &amp; handling looping boundaries. I also read particles from one buffer and write them to another buffer, but the idea is the same.)</i></p>

            <p><code>forces</code> is a \(M^2\)-sized array (where \(M\) is the number of particle types) which stores the inter-particle force parameters.</p>

            <h2 id="section-parallel-prefix-sum">Parallel prefix sum</h2>

            <p>Computing prefix sums in parallel is a well-known problem. I'm using the simplest solution, outlined as the <i>Algorithm 1</i> in the <a href="https://en.wikipedia.org/wiki/Prefix_sum#Algorithm_1:_Shorter_span,_more_parallel">wikipedia article</a>, and also called the <i>naive parallel scan</i> in this <a href="https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda">GPU Gems 3 article</a>.</p>

            <p>The image from wikipedia explains everything rather nicely:</p>

            <center><img src="https://lisyarus.github.io/blog/media/particle-life/prefix-sum.png"></center>

            <p>The main idea is to run several sweeps over the whole array \(\log_2(N)\) times (where \(N\) is the array size), each time adding <code>x[i-step]</code> to <code>x[i]</code>, where <code>step</code> is a power of 2. In pseudocode, at iteration <code>k</code> we do</p>

            <pre><code>step = (1 &lt;&lt; k);
parallel for (int i = step; i &lt; array.size; ++i)
    array[i] += array[i - step];
</code></pre>

            <p>We start iteration from <code>i = step</code> because otherwise at <code>array[i - step]</code> we'd try reading at an address before the array starts. This code won't actually work because overwriting <code>array[i]</code> affects what other invocations (with different values of <code>i</code>) will read, so we have to use 2 arrays: one for reading, and one for writing. Something like this:</p>

            <pre><code>parallel for (int i = 0; i &lt; input.size; ++i) {
    if (i &lt; step)
        output[i] = input[i];
    else
        output[i] = input[i] + input[i - step];
}</code></pre>

            <p>Then, to compute the actual prefix sum, we create a temporary array, and we ping-pong the input &amp; temporary arrays \(\log_2(N)\) times with <code>step</code> taking the values of increasing powers of 2. On the first iteration (<code>k = 0, step = 1</code>), our <code>binSize</code> array is the input, and the temporary array is the output. On the next iteration (<code>k = 1, step = 2</code>), the temporary array is the input, and the <code>binSize</code> array is the output, and so on. In terms of WebGPU, this requires creating two bind groups with the same layout, one that reads from <code>binSize</code> array and writes to temporary array, and the other does the opposite.</p>

            <p>Now, with this method, the resulting prefix sum will be in the first or the second array, depending on the parity of the number of steps performed, i.e. the parity of \(\log_2(N)\). That's rather inconvenient, so I always round it up to the closest even number: this way, the result is always in the same <code>binSize</code> array, while an extra prefix sum iteration would simply copy one buffer into the other automatically (look what happens with the code when <code>step &gt;= array.size</code>).</p>

            <p>That's a lot of talking, but the implementation is rather simple:</p>

            <pre><code>@group(0) @binding(0) var&lt;storage, read&gt; input : array&lt;u32&gt;;
@group(0) @binding(1) var&lt;storage, read_write&gt; output : array&lt;u32&gt;;
@group(0) @binding(2) var&lt;uniform&gt; stepSize : u32;

@compute @workgroup_size(64)
fn prefixSumStep(@builtin(global_invocation_id) id : vec3u)
{
    if (id.x &gt;= arrayLength(&amp;input)) {
        return;
    }

    if (id.x &lt; stepSize) {
        output[id.x] = input[id.x];
    } else {
        output[id.x] = input[id.x - stepSize] + input[id.x];
    }
}</code></pre>

            <p>In order to supply different <code>stepSize</code> values while still keeping the whole thing as a single compute pass, I have a buffer with all <code>stepSize</code> values I need, and use it as the source for the uniform <code>stepSize</code> value with proper dynamic offsets in the <code>GPUComputePassEncoder.setBindGroup</code> call.</p>

            <p>So, with \(\log_2(N)\) iterations (rounded up to an even number), we run this compute shader (one invocation per bin, — or, in my case, <code>binCount + 1</code> invocations for reasons I outlined earlier), while ping-ponging two buffers. Here's what it looks like in JavaScript:</p>

<pre><code>prefixSumIterations = Math.ceil(Math.ceil(Math.log2(binCount + 1)) / 2) * 2;

...

binningComputePass.setPipeline(binPrefixSumPipeline);
for (var i = 0; i &lt; prefixSumIterations; ++i) {
    binningComputePass.setBindGroup(0, binPrefixSumBindGroup[i % 2], [i * 256]);
    binningComputePass.dispatchWorkgroups(Math.ceil((binCount + 1) / 64));
}</code></pre>

            <p><i>(The <code>i * 256</code> dynamic offset is due to requirements that dynamic offsets must be a multiple of 256 bytes. The <code>Math.ceil((binCount + 1) / 64)</code> workgroup count is because the shader uses a workgroup size of 64, and I run it on an array of size <code>binCount + 1</code>.)</i></p>

            <p>I could use a more sophisticated prefix sum algorithm, but there was no need to: even with e.g. 10000 bins, this whole particle sorting &amp; prefix sum computation takes about 0.1ms on my GeForce GTX 1060. The whole performance is <i>always</i> dominated by inter-particle forces computation, so optimizing the binning step gives little profit.</p>

            <h2 id="section-rendering">Rendering</h2>

            <p>I render particles as squares which are transformed into perfect circles in the fragment shader. For each particle, I have 2 triangles, thus 6 or 4 vertices (with or without vertex duplication), all sharing the data of the same particle. That's rather inconvenient: I'd rather not duplicate the particle data and not use instancing for that. So, instead my vertices don't have any attributes, and instead read the particles array directly as a read-only storage buffer, based on the vertex ID:</p>

            <pre><code>@group(0) @binding(0) var&lt;storage, read&gt; particles : array&lt;Particle&gt;;
@group(0) @binding(1) var&lt;storage, read&gt; species : array&lt;Species&gt;;
@group(1) @binding(0) var&lt;uniform&gt; camera : Camera;

struct VertexOut
{
    @builtin(position) position : vec4f,
    @location(0) offset : vec2f,
    @location(1) color : vec4f,
}

const offsets = array&lt;vec2f, 6&gt;(
    vec2f(-1.0, -1.0),
    vec2f( 1.0, -1.0),
    vec2f(-1.0,  1.0),
    vec2f(-1.0,  1.0),
    vec2f( 1.0, -1.0),
    vec2f( 1.0,  1.0),
);

@vertex
fn vertexCircle(@builtin(vertex_index) id : u32) -&gt; VertexOut
{
    let particle = particles[id / 6u];
    let offset = offsets[id % 6u] * 1.5;
    let position = vec2f(particle.x, particle.y) + offset;
    return VertexOut(
        vec4f((position - camera.center) / camera.extent, 0.0, 1.0),
        offset,
        species[u32(particle.species)].color
    );
}</code></pre>

            <p>Executed with a vertex count of <code>6 * particleCount</code>, this produces a square per each particle. The fragment shader computes the distance in pixels from the fragment to the center of the particle, and turns it into the alpha value to produce a perfectly anti-aliased circle:</p>

            <pre><code>@fragment
fn fragmentCircle(in : VertexOut) -&gt; @location(0) vec4f
{
    let alpha = clamp(camera.pixelsPerUnit * (1.0 - length(in.offset)) + 0.5, 0.0, 1.0);
    return in.color * vec4f(1.0, 1.0, 1.0, alpha);
}</code></pre>

            <p>And this is what it looks like:</p>

            <center><img src="https://lisyarus.github.io/blog/media/particle-life/anti-aliasing.png"></center>

            <p>I also add some glowing effect to the particles, which is rendered in a separate pass before the particles themselves. It works in pretty much the same way, but the squares are larger, and the alpha value is a gaussian:</p>

            <pre><code>@vertex
fn vertexGlow(@builtin(vertex_index) id : u32) -&gt; VertexOut
{
    let particle = particles[id / 6u];
    let offset = offsets[id % 6u];
    let position = vec2f(particle.x, particle.y) + 12.0 * offset;
    return CircleVertexOut(
        vec4f((position - camera.center) / camera.extent, 0.0, 1.0),
        offset,
        species[u32(particle.species)].color
    );
}

@fragment
fn fragmentGlow(in : VertexOut) -&gt; @location(0) vec4f
{
    let l = length(in.offset);
    let alpha = exp(- 6.0 * l * l) / 64.0;
    return in.color * vec4f(1.0, 1.0, 1.0, alpha);
}</code></pre>

            <center><img src="https://lisyarus.github.io/blog/media/particle-life/glow.png"></center>
            <center><i>Glowing effect, exaggerated for the sake of demonstration.</i></center>

            <p>All this is rendered using basic additive blending (<i>not</i> the usual over blending!) into an HDR <code>rgba16float</code> texture, then composited onto the screen (actually, onto an HTML canvas) using ACES tone-mapping and some blue-noise dithering to hide banding resulting from dark colors on the edge of the glow. I experimented with different blending methods &amp; tone-mapping curves, and this combination produced the best visuals for my taste.</p>

            <p>However, rendering perfect circles stops working when particles get too small, e.g. less than 1 pixel: the circle gets severely undersampled, the computed alpha values are pretty much random, and the whole image is just a big flickering mess. To counter that, I have a separate shader which replaces the circles when particle radius gets smaller than 1 pixel. It also renders squares, but the vertex shader makes sure the squares are exactly 2x2 pixels in size:</p>

            <pre><code>@vertex
fn vertexPoint(@builtin(vertex_index) id : u32) -&gt; VertexOut
{
    let particle = particles[id / 6u];
    let offset = 2.0 * offsets[id % 6u] / camera.pixelsPerUnit;
    let position = vec2f(particle.x, particle.y) + offset;
    return CircleVertexOut(
        vec4f((position - camera.center) / camera.extent, 0.0, 1.0),
        offset,
        species[u32(particle.species)].color
    );
}</code></pre>

            <p>Then, the fragment shader computes the area of the intersection of the circle and the current pixel. Because computing circle-square intersections is a bit non-trivial, I approximate the circle with a square, compute the intersections (which boil down to min/max along each coordinate), and multiply by \(\frac{\pi}{4}\) to compensate for the square area vs circle area:</p>

            <pre><code>@fragment
fn fragmentPoint(in : CircleVertexOut) -&gt; @location(0) vec4f
{
    let s = vec2f(camera.pixelsPerUnit);
    let d = max(vec2f(0.0), min(in.offset * s + 0.5, s) - max(in.offset * s - 0.5, - s));
    let alpha = (PI / 4.0) * d.x * d.y;
    return vec4f(in.color.rgb, in.color.a * alpha);
}</code></pre>

            <p>This feels hacky, but it works: the image almost doesn't flicker, and the transition between the two rendering modes is pretty much unnoticeable.</p>

            <h2 id="section-gallery">Gallery</h2>

            <p>I already posted the link in the beginning of the article, but if you don't feel like scolling back, <a href="https://lisyarus.github.io/webgpu/particle-life.html">here is the link to the simulation</a>. Each page reload gives a new random system, though you can also just click "Randomize". You can save the current rules as a JSON file, or upload custom rules. You can share a randomly-generated rule set by clicking "Copy link".</p>

            <p>I've noticed that less particle types leads to more large-scale coherent structures, while more particle types leads to more local &amp; busy systems. Sometimes it is a good idea to do some <a href="https://en.wikipedia.org/wiki/Simulated_annealing"><i>simulated annealing</i></a> and slowly increase or decrease the friction value, which works kind of like the inverse temperature here.</p>

            <p>But, honestly, pretty much any random system made with this simulator looks interesting in some way. Here are some of the cool-looking systems I've been able to find in about 15 minutes. Each image is a link to this particular system:</p>

            <center><a href="https://lisyarus.github.io/webgpu/particle-life.html?particleCount=65536&amp;speciesCount=1&amp;friction=10&amp;centralForce=0&amp;symmetricForces=true&amp;loopingBorders=true&amp;seed=1964178017"><img src="https://lisyarus.github.io/blog/media/particle-life/gallery-0.png"></a></center>

            <center><a href="https://lisyarus.github.io/webgpu/particle-life.html?particleCount=65536&amp;speciesCount=2&amp;friction=10&amp;centralForce=0&amp;symmetricForces=true&amp;loopingBorders=true&amp;seed=781562033"><img src="https://lisyarus.github.io/blog/media/particle-life/gallery-1.png"></a></center>

            <center><a href="https://lisyarus.github.io/webgpu/particle-life.html?particleCount=65536&amp;speciesCount=2&amp;friction=10&amp;centralForce=0&amp;symmetricForces=true&amp;loopingBorders=true&amp;seed=470127870"><img src="https://lisyarus.github.io/blog/media/particle-life/gallery-2.png"></a></center>

            <center><a href="https://lisyarus.github.io/webgpu/particle-life.html?particleCount=65536&amp;speciesCount=2&amp;friction=10&amp;centralForce=0&amp;symmetricForces=true&amp;loopingBorders=true&amp;seed=3856172604"><img src="https://lisyarus.github.io/blog/media/particle-life/gallery-3.png"></a></center>

            <center><a href="https://lisyarus.github.io/webgpu/particle-life.html?particleCount=65536&amp;speciesCount=2&amp;friction=10&amp;centralForce=0&amp;symmetricForces=true&amp;loopingBorders=true&amp;seed=4037789242"><img src="https://lisyarus.github.io/blog/media/particle-life/gallery-4.png"></a></center>

            <center><a href="https://lisyarus.github.io/webgpu/particle-life.html?particleCount=65536&amp;speciesCount=3&amp;friction=10&amp;centralForce=0&amp;symmetricForces=true&amp;loopingBorders=true&amp;seed=2609318066"><img src="https://lisyarus.github.io/blog/media/particle-life/gallery-5.png"></a></center>

            <center><a href="https://lisyarus.github.io/webgpu/particle-life.html?particleCount=65536&amp;speciesCount=3&amp;friction=10&amp;centralForce=0&amp;symmetricForces=true&amp;loopingBorders=true&amp;seed=1940662796"><img src="https://lisyarus.github.io/blog/media/particle-life/gallery-6.png"></a></center>

            <center><a href="https://lisyarus.github.io/webgpu/particle-life.html?particleCount=65536&amp;speciesCount=3&amp;friction=10&amp;centralForce=0&amp;symmetricForces=false&amp;loopingBorders=true&amp;seed=3584546700"><img src="https://lisyarus.github.io/blog/media/particle-life/gallery-7.png"></a></center>

            <center><a href="https://lisyarus.github.io/webgpu/particle-life.html?particleCount=65536&amp;speciesCount=3&amp;friction=10&amp;centralForce=0&amp;symmetricForces=false&amp;loopingBorders=true&amp;seed=1876301519"><img src="https://lisyarus.github.io/blog/media/particle-life/gallery-8.png"></a></center>

            <center><a href="https://lisyarus.github.io/webgpu/particle-life.html?particleCount=65536&amp;speciesCount=4&amp;friction=10&amp;centralForce=0&amp;symmetricForces=false&amp;loopingBorders=true&amp;seed=3588165106"><img src="https://lisyarus.github.io/blog/media/particle-life/gallery-9.png"></a></center>

            <center><a href="https://lisyarus.github.io/webgpu/particle-life.html?particleCount=65536&amp;speciesCount=4&amp;friction=10&amp;centralForce=0&amp;symmetricForces=false&amp;loopingBorders=true&amp;seed=1007650884"><img src="https://lisyarus.github.io/blog/media/particle-life/gallery-10.png"></a></center>

            <center><a href="https://lisyarus.github.io/webgpu/particle-life.html?particleCount=65536&amp;speciesCount=8&amp;friction=10&amp;centralForce=0&amp;symmetricForces=false&amp;loopingBorders=true&amp;seed=958305330"><img src="https://lisyarus.github.io/blog/media/particle-life/gallery-11.png"></a></center>

            
            
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TIL: timeout in Bash scripts (264 pts)]]></title>
            <link>https://heitorpb.github.io/bla/timeout/</link>
            <guid>44096395</guid>
            <pubDate>Mon, 26 May 2025 11:34:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://heitorpb.github.io/bla/timeout/">https://heitorpb.github.io/bla/timeout/</a>, See on <a href="https://news.ycombinator.com/item?id=44096395">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>The other day at work we had a Bash script that would set up a web server and
wait for it to be up before proceeding to the next things. The script worked
fine and we had no issues, until we had an infinite loop.</p>
<center>
<a data-flickr-embed="true" data-header="false" data-footer="false" href="https://www.flickr.com/photos/heitorpb/30622866477/" title="Airplane Tracks"><img src="https://live.staticflickr.com/1908/30622866477_d147a6f3fc_z.jpg" width="640" height="426" alt="Airplane Tracks"></a>
</center>
<p>We were using the Bash built-in <code>until</code> to check if the web server was up:</p>
<pre data-lang="bash"><code data-lang="bash"><span>until </span><span>curl --silent --fail-with-body</span><span> 10.0.0.1:8080/health; </span><span>do
</span><span>	</span><span>sleep</span><span> 1
</span><span>done
</span></code></pre>
<p>This works fine. Unless our web server crashes during startup and we <code>sleep 1</code>
forever.</p>
<p>Here comes a handy utility: <code>timeout</code>. As the name suggests, this command adds
a timeout to other commands. You specify the time limit you want to wait for a
command and if that time passes, <code>timeout</code> sends a signal to terminate it and
exits with non-zero. By default, <code>timeout</code> sends <code>SIGTERM</code>, but you can change
it with the <code>--signal</code> flag, e.g. <code>timeout --signal=SIGKILL 1s foo</code>.</p>
<p>For example, <code>timeout 1s sleep 5</code> will send the <code>SIGTERM</code> signal to <code>sleep</code>
after 1 second:</p>
<pre data-lang="bash"><code data-lang="bash"><span>$</span><span> time timeout 1s sleep 4
</span><span>
</span><span>real</span><span>    0m1,004s
</span><span>user</span><span>    0m0,000s
</span><span>sys</span><span>     0m0,005s
</span><span>
</span><span>$</span><span> echo $</span><span>?
</span><span>124
</span></code></pre>
<p>The natural thing to do then is to combine <code>timeout</code> and <code>until</code>:</p>
<pre data-lang="bash"><code data-lang="bash"><span>timeout</span><span> 1m until curl</span><span> --silent --fail-with-body</span><span> 10.0.0.1:8080/health; </span><span>do
</span><span>	</span><span>sleep</span><span> 1
</span><span>done
</span></code></pre>
<p>The only issue is that this doesn’t work. <code>timeout</code> expects a killable command
and <code>until</code> is a shell keyword: you can’t <code>SIGTERM</code> <code>until</code>. We can’t use
<code>timeout</code> with any shell built-in.</p>
<p>The way forward is to wrap that <code>until</code> in a Bash process:</p>
<pre data-lang="bash"><code data-lang="bash"><span>timeout</span><span> 1m bash</span><span> -c </span><span>"</span><span>until curl --silent --fail-with-body 10.0.0.1:8080/health; do
</span><span>	sleep 1
</span><span>done</span><span>"
</span></code></pre>
<p>Another approach is to move the <code>until</code> to a separate Bash script and <code>timeout</code>
it:</p>
<pre data-lang="bash"><code data-lang="bash"><span>timeout</span><span> 1m ./until.sh
</span></code></pre>
<p>It’s a shame we can’t use <code>timeout</code> with <code>until</code> directly, that would be
amazing. But wrapping it in a Bash process/script gets the job done.</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The double standard of webhook security and API security (105 pts)]]></title>
            <link>https://www.speakeasy.com/blog/webhook-security</link>
            <guid>44096251</guid>
            <pubDate>Mon, 26 May 2025 11:18:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.speakeasy.com/blog/webhook-security">https://www.speakeasy.com/blog/webhook-security</a>, See on <a href="https://news.ycombinator.com/item?id=44096251">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
    </channel>
</rss>