<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 04 Jan 2026 09:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Neural Networks: Zero to Hero (232 pts)]]></title>
            <link>https://karpathy.ai/zero-to-hero.html</link>
            <guid>46485090</guid>
            <pubDate>Sun, 04 Jan 2026 05:02:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://karpathy.ai/zero-to-hero.html">https://karpathy.ai/zero-to-hero.html</a>, See on <a href="https://news.ycombinator.com/item?id=46485090">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>A course by Andrej Karpathy on building neural networks, from scratch, in code.</p><p>We start with the basics of backpropagation and build up to modern deep neural networks, like GPT. In my opinion language models are an excellent place to learn deep learning, even if your intention is to eventually go to other areas like computer vision because most of what you learn will be immediately transferable. This is why we dive into and focus on languade models.</p><p>Prerequisites: solid programming (Python), intro-level math (e.g. derivative, gaussian).</p><div>
            <h2>Syllabus</h2>

            <div>
                <p>2h25m</p>
                
                <p>This is the most step-by-step spelled-out explanation of backpropagation and training of neural networks. It only assumes basic knowledge of Python and a vague recollection of calculus from high school.</p>
            </div>

            <div>
                <p>1h57m</p>
                
                <p>We implement a bigram character-level language model, which we will further complexify in followup videos into a modern Transformer language model, like GPT. In this video, the focus is on (1) introducing torch.Tensor and its subtleties and use in efficiently evaluating neural networks and (2) the overall framework of language modeling that includes model training, sampling, and the evaluation of a loss (e.g. the negative log likelihood for classification).</p>
            </div>

            <div>
                <p>1h15m</p>
                
                <p>We implement a multilayer perceptron (MLP) character-level language model. In this video we also introduce many basics of machine learning (e.g. model training, learning rate tuning, hyperparameters, evaluation, train/dev/test splits, under/overfitting, etc.).</p>
            </div>

            <div>
                <p>1h55m</p>
                
                <p>We dive into some of the internals of MLPs with multiple layers and scrutinize the statistics of the forward pass activations, backward pass gradients, and some of the pitfalls when they are improperly scaled. We also look at the typical diagnostic tools and visualizations you'd want to use to understand the health of your deep network. We learn why training deep neural nets can be fragile and introduce the first modern innovation that made doing so much easier: Batch Normalization. Residual connections and the Adam optimizer remain notable todos for later video.</p>
            </div>

            <div>
                <p>1h55m</p>
                
                <p>We take the 2-layer MLP (with BatchNorm) from the previous video and backpropagate through it manually without using PyTorch autograd's loss.backward(): through the cross entropy loss, 2nd linear layer, tanh, batchnorm, 1st linear layer, and the embedding table. Along the way, we get a strong intuitive understanding about how gradients flow backwards through the compute graph and on the level of efficient Tensors, not just individual scalars like in micrograd. This helps build competence and intuition around how neural nets are optimized and sets you up to more confidently innovate on and debug modern neural networks.</p>
            </div>

            <div>
                <p>56m</p>
                
                <p>We take the 2-layer MLP from previous video and make it deeper with a tree-like structure, arriving at a convolutional neural network architecture similar to the WaveNet (2016) from DeepMind. In the WaveNet paper, the same hierarchical architecture is implemented more efficiently using causal dilated convolutions (not yet covered). Along the way we get a better sense of torch.nn and what it is and how it works under the hood, and what a typical deep learning development process looks like (a lot of reading of documentation, keeping track of multidimensional tensor shapes, moving between jupyter notebooks and repository code, ...).</p>
            </div>

            <div>
                <p>1h56m</p>
                
                <p>We build a Generatively Pretrained Transformer (GPT), following the paper "Attention is All You Need" and OpenAI's GPT-2 / GPT-3. We talk about connections to ChatGPT, which has taken the world by storm. We watch GitHub Copilot, itself a GPT, help us write a GPT (meta :D!) . I recommend people watch the earlier makemore videos to get comfortable with the autoregressive language modeling framework and basics of tensors and PyTorch nn, which we take for granted in this video.</p>
            </div>

            <div>
                <p>2h13m</p>
                
                <p>The Tokenizer is a necessary and pervasive component of Large Language Models (LLMs), where it translates between strings and tokens (text chunks). Tokenizers are a completely separate stage of the LLM pipeline: they have their own training sets, training algorithms (Byte Pair Encoding), and after training implement two fundamental functions: encode() from strings to tokens, and decode() back from tokens to strings. In this lecture we build from scratch the Tokenizer used in the GPT series from OpenAI. In the process, we will see that a lot of weird behaviors and problems of LLMs actually trace back to tokenization. We'll go through a number of these issues, discuss why tokenization is at fault, and why someone out there ideally finds a way to delete this stage entirely.
                </p>
            </div>

            <p>ongoing...</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Swift on Android: Full Native App Development Now Possible (200 pts)]]></title>
            <link>https://docs.swifdroid.com/app/</link>
            <guid>46483023</guid>
            <pubDate>Sat, 03 Jan 2026 23:39:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.swifdroid.com/app/">https://docs.swifdroid.com/app/</a>, See on <a href="https://news.ycombinator.com/item?id=46483023">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
                
                  


  
  



<p>You are in the most incredible place to start building Android apps in Swift!</p>
<p>This code is absolute reality now:</p>
<pre><code>ConstraintLayout {
    VStack {
        TextView("Hello from Swift!")
            .width(.matchParent)
            .height(.wrapContent)
            .textColor(.green)
            .marginBottom(16)
        MaterialButton("Tap Me")
            .onClick {
                print("Button tapped!")
            }
    }
    .centerVertical()
    .leftToParent()
    .rightToParent()
}
</code></pre>
<p><strong>You can create stunning user interfaces natively in Swift!</strong></p>
<p><strong>Droid framework</strong> is the foundation for building rich Android apps with native UI and UX.</p>
<p>It provides an extensive set of components, including AndroidX, Flexbox, and Material Design.</p>
<p>Offering a <strong>SwiftUI</strong>-like declarative syntax for everything, <strong>Droid framework</strong> simplifies the process of developing Android applications in Swift by providing a high-level API that abstracts away many complexities of the Android platform and completely hides the underlying JNI layer.</p>
<p>The application documentation is under active development. If you encounter any 404 pages or typos, please be patient ‚Äì new content is being added every day.</p>









  




                
              </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The suck is why we're here (278 pts)]]></title>
            <link>https://nik.art/the-suck-is-why-were-here/</link>
            <guid>46482877</guid>
            <pubDate>Sat, 03 Jan 2026 23:24:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nik.art/the-suck-is-why-were-here/">https://nik.art/the-suck-is-why-were-here/</a>, See on <a href="https://news.ycombinator.com/item?id=46482877">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-9814">
		<div>
		<header>

			

			<!-- .entry-meta -->
		</header><!-- .entry-header -->

		<div>
			
<p>On a catchup call, I told my friend <a href="https://nickwignall.com/" target="_blank" rel="noreferrer noopener">Nick Wignall</a> how someone had trained an AI model to write blog posts in my style. It was a pure research exercise on their part. The idea was to train the tool on my past work, then give it the headlines and opening paragraphs of my 2025 posts. Could it generate the rest of each piece in a similar fashion?</p>



<p>I only compared a handful of posts from their AI versions to their originals, but I quickly concluded the writing suffered from the same <a href="https://en.wikipedia.org/wiki/Uncanny_valley" target="_blank" rel="noreferrer noopener">uncanny valley effect</a> as many AI-generated images: It all looks fine enough at first glance, but pay attention just a little longer, and something feels off. The AI would veer off in a different direction or end up making the opposite argument. It sounded confident where I would have been doubtful and vice versa. And so on.</p>



<p>The creator wanted to know if such a model‚Äîonce it worked properly, of course‚Äîcould be useful to me. I told him even if it worked perfectly it wouldn‚Äôt. Why? Because I don‚Äôt write a daily blog to crank out a post every day. If that was the point, I‚Äôd have switched to AI long ago already. I write a daily blog to <a href="https://nik.art/when-ai-does-our-writing-who-will-do-our-thinking/">make sure I remember how to think</a>. It‚Äôs a daily practice for my brain. A creative ritual to strengthen my writing muscles. And a commitment to my readers. A promise that I‚Äôll show up for them once a day. AI can generate output, but it can‚Äôt give me any of these benefits. The output is secondary. If it happens to attract new readers, all the better. And if not? That‚Äôs fine too.</p>



<p>Nick said my story reminded him of <a href="https://www.youtube.com/watch?v=smb7hy6KufQ" target="_blank" rel="noreferrer noopener">an interview</a> with writer and Vox-founder Ezra Klein. Klein explained that, so far, AI hasn‚Äôt been all that useful to him. He uses it for light research or to structure some data, but that‚Äôs about it. Why? Because the writer doing the research is what makes the writing unique.</p>



<p>When you‚Äôre using AI as a writer, you‚Äôre ‚Äúoutsourcing the part of the work [you] need to do the most,‚Äù Klein believes. ‚ÄúHaving AI summarize a book or a paper for me is a disaster. It has no idea what I really wanted to know. It would not have made the connections I would have made.‚Äù This is why <a href="https://nik.art/the-intangible-benefits-of-reading-in-an-ai-powered-world/" target="_blank" rel="noreferrer noopener">reading actual books in full might now be more valuable than it ever has been</a>: Only if you‚Äôve seen every word will you discover insights and links an AI would never include in its average-driven summary.</p>



<p>Nick pointed out the same applies to a writer struggling when creating a piece. ‚ÄúWhen you‚Äôre stuck and sit there, thinking, trying to come up with what‚Äôs next, that‚Äôs the valuable part of writing. It‚Äôs tempting to use AI to remove that stuck-ness, but it‚Äôs basically cheating‚Äîand leads to a very different result.‚Äù AI is great at giving you a list of ideas. You‚Äôll almost always find one you can plug in and keep writing. But is it <em>the</em> idea that needs to slot into this gap? Or just a bad piece of filler that‚Äôll make for a fragile mental bridge most readers won‚Äôt dare to cross?</p>



<p>The more I think about it, the happier I am that AI is transforming the world of writing. In a way, I think it‚Äôll make it even easier to stand out‚Äîbecause the more people take shortcuts, the less quality will remain for readers to flock to, even if the overall quantity of options is much larger.</p>



<p>Whenever technology makes it feel like you can avoid the suck, it‚Äôs most likely a mirage. The path behind easy only leads to the lowest common denominator. The real artists, fighters, makers‚Äîthey stick with a truth as old as time itself: The suck is why we‚Äôre here, and only those who overcome it themselves will <a href="https://nik.art/the-rewards-grow-where-you-work-for-them/">reap all the rewards</a> of their hard labor.</p>
					</div><!-- .entry-content -->

		<!-- .entry-meta -->
	</div>

			<div>

		<!-- avatar -->
		<p><img alt="" src="https://secure.gravatar.com/avatar/e54535ff03d6b305a38c41e97f04bdff4bc72973c3976fb263251a3cb03e24fe?s=60&amp;d=mm&amp;r=r" srcset="https://secure.gravatar.com/avatar/e54535ff03d6b305a38c41e97f04bdff4bc72973c3976fb263251a3cb03e24fe?s=120&amp;d=mm&amp;r=r 2x" height="60" width="60" decoding="async">		</p>
		<!-- end avatar -->

		<!-- user bio -->
		<div>

		  <h4><a href="https://nik.art/author/linuscaldwell300/">Nik</a></h4>
		  <p>
				Niklas G√∂ke writes for dreamers, doers, and unbroken optimists. A self-taught writer with more than a decade of experience, Nik has published over 2,000 articles. His work has attracted tens of millions of readers and been featured in places like Business Insider, CNBC, Lifehacker, and many others.

Nik has self-published 2 books thus far, most recently 2-Minute Pep Talks. Outside of his day job and daily blog, Nik loves reading, video games, and pizza, which he eats plenty a slice of in Munich, Germany, where he resides.		  </p>

		</div><!-- end .author-bio-content -->

	  </div>
		
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[China DRAM Maker CXMT Targets $4.2B IPO as It Takes on Samsung, SK Hynix, Micron (181 pts)]]></title>
            <link>https://www.ic-pcb.com/chinas-leading-dram-maker-cxmt-targets-42-billion-ipo-as-it-takes-on-samsung-sk-hynix-and-micron.html</link>
            <guid>46482777</guid>
            <pubDate>Sat, 03 Jan 2026 23:14:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ic-pcb.com/chinas-leading-dram-maker-cxmt-targets-42-billion-ipo-as-it-takes-on-samsung-sk-hynix-and-micron.html">https://www.ic-pcb.com/chinas-leading-dram-maker-cxmt-targets-42-billion-ipo-as-it-takes-on-samsung-sk-hynix-and-micron.html</a>, See on <a href="https://news.ycombinator.com/item?id=46482777">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    	        <p><span>On the evening of December 30, Chinese DRAM leader ChangXin Memory Technologies Group Co., Ltd. (CXMT) formally submitted its prospectus to the Shanghai Stock Exchange, applying for a listing on the STAR Market. The offering is being sponsored by state-backed investment bank CICC and Chinese securities firm CSC Financial.</span></p><p><span>CXMT plans to raise up to CNY 29.5 billion, equivalent to approximately USD 4.22 billion, with proceeds earmarked for three core projects: upgrading mass-production DRAM wafer manufacturing lines, advancing DRAM process technologies, and funding forward-looking research and development for next-generation dynamic random-access memory.</span></p><p><span>The company said its current capacity already ranks first in China and fourth globally, but still lags the world's top three DRAM makers. With China remaining the world's largest DRAM consumption market, CXMT said the planned investments will accelerate process upgrades, reduce unit costs, enhance profitability, and better meet strong downstream demand, helping the company secure a more favorable position in the global DRAM market.</span></p><p><span>Founded in 2016, CXMT was established against the backdrop of heavy global DRAM concentration, with more than 90% of the market long dominated by Samsung Electronics, SK Hynix, and Micron Technology. CXMT said it has since broken through key DRAM technologies and achieved independent design, development, and commercial mass production, filling a long-standing gap in mainland China's DRAM industry.</span></p><p><span>Today, CXMT is China's largest and most technologically advanced integrated DRAM manufacturer, operating under an IDM model that spans R&amp;D, design, and fabrication. Its product portfolio covers DDR and LPDDR families, including DDR4, DDR5, LPDDR4X, and LPDDR5/5X, serving applications such as servers, mobile devices, PCs, smart vehicles, and AI-related systems.</span></p><p><img src="https://www.ic-pcb.com/web/uploads/image/20260102/3slhtXY513zF37159k01G76nkoFni74s.webp"></p><p><span>The company launched its first self-developed 8Gb DDR4 product in September 2019, marking what it called a "from zero to one" breakthrough for China's DRAM sector. Its latest LPDDR5X products reach speeds of up to 10,667 Mbps, a 66% increase over LPDDR5, while its domestically developed DDR5 chips deliver speeds of up to 8,000 Mbps with single-die capacity reaching 24Gb. CXMT said its overall DRAM product roadmap is now broadly on par with global leaders.</span></p><p><span>CXMT operates three 12-inch DRAM fabs in Hefei and Beijing. According to Omdia, Samsung held 40.35% of the global DRAM market by revenue in 2024, followed by SK Hynix at 33.19% and Micron at 20.73%. Based on Omdia estimates, CXMT's global DRAM market share rose to about 3.97% in the second quarter of 2025, positioning it as an emerging player among major manufacturers.</span></p><p><span>Financially, the company has reported rapid revenue growth but remains loss-making due to heavy capital and R&amp;D investment. From 2022 through the first half of 2025, CXMT generated cumulative revenue of nearly CNY 57 billion, or about USD 8.16 billion. Over the same period, cumulative net losses attributable to shareholders exceeded CNY 40.86 billion, or roughly USD 5.85 billion, reflecting large depreciation charges from capacity expansion, sustained R&amp;D spending, inventory write-downs, and sharp DRAM price volatility.</span></p><p><span><img src="https://www.ic-pcb.com/web/uploads/image/20260102/5GoHk217307v221bB0QcoKZj35P6LeC2.webp"></span></p><p><span>CXMT said it expects a turnaround in 2025, forecasting full-year revenue of CNY 55 billion to CNY 58 billion, equivalent to USD 7.87 billion to USD 8.3 billion, and net profit of CNY 2 billion to CNY 3.5 billion. The company attributed the expected rebound to surging AI-driven memory demand, tight DRAM supply, rising prices, and the release of inventory accumulated earlier in the cycle.</span></p><p><span>Customer concentration remains relatively high, with the top five customers accounting for more than 60% of revenue during the reporting periods. However, CXMT said it does not rely excessively on any single customer. End customers include major cloud, consumer electronics, and device makers such as Alibaba Cloud, ByteDance, Tencent, Lenovo, Xiaomi, OPPO, vivo, and others.</span></p><p><span>On the supply side, CXMT said its dependence on major suppliers is limited, with the top five raw material suppliers accounting for less than one-third of total procurement in each reporting period.</span></p><p><span><img src="https://www.ic-pcb.com/web/uploads/image/20251224/7mUxc7eF9X8l7g2U6p3C504wAV611M6I.webp"></span></p><p><span>R&amp;D remains central to the company's strategy. From 2022 through the first half of 2025, CXMT invested CNY 18.87 billion in R&amp;D, or about USD 2.7 billion, representing more than 33% of cumulative revenue. As of June 30, 2025, the company employed 4,653 R&amp;D personnel, accounting for over 30% of its workforce, and held 5,589 patents worldwide.</span></p><p><span>The IPO is also notable as the first STAR Market application accepted under China's new pre-review mechanism for companies engaged in critical core technology development. The system, introduced in mid-2025, aims to shorten review timelines and reduce early disclosure risks for strategically sensitive technologies. CXMT completed two rounds of pre-review inquiries in November 2025, significantly accelerating its listing process.</span></p><p><span>CXMT has a diversified shareholder base and no controlling shareholder. Major investors include state-backed Big Fund Phase II, regional government investment vehicles, and strategic partners such as Alibaba and Tencent.</span></p><p><span>The company said the listing will not only support its own capacity expansion and technology upgrades, but also drive coordinated growth across China's domestic memory ecosystem, including chip design, equipment, materials, packaging, and downstream applications. As China's flagship DRAM maker moves closer to public markets, its IPO is widely seen as a milestone for the country's ambition to build a globally competitive memory semiconductor industry.</span></p>
    	    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Total monthly number of StackOverflow questions over time (1061 pts)]]></title>
            <link>https://data.stackexchange.com/stackoverflow/query/1926661#graph</link>
            <guid>46482345</guid>
            <pubDate>Sat, 03 Jan 2026 22:23:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://data.stackexchange.com/stackoverflow/query/1926661#graph">https://data.stackexchange.com/stackoverflow/query/1926661#graph</a>, See on <a href="https://news.ycombinator.com/item?id=46482345">Hacker News</a></p>
Couldn't get https://data.stackexchange.com/stackoverflow/query/1926661#graph: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft kills official way to activate Windows 11/10 without internet (387 pts)]]></title>
            <link>https://www.neowin.net/news/report-microsoft-quietly-kills-official-way-to-activate-windows-1110-without-internet/#google_vignette</link>
            <guid>46480156</guid>
            <pubDate>Sat, 03 Jan 2026 18:53:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.neowin.net/news/report-microsoft-quietly-kills-official-way-to-activate-windows-1110-without-internet/#google_vignette">https://www.neowin.net/news/report-microsoft-quietly-kills-official-way-to-activate-windows-1110-without-internet/#google_vignette</a>, See on <a href="https://news.ycombinator.com/item?id=46480156">Hacker News</a></p>
Couldn't get https://www.neowin.net/news/report-microsoft-quietly-kills-official-way-to-activate-windows-1110-without-internet/#google_vignette: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[The C3 Programming Language (335 pts)]]></title>
            <link>https://c3-lang.org</link>
            <guid>46478647</guid>
            <pubDate>Sat, 03 Jan 2026 16:41:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://c3-lang.org">https://c3-lang.org</a>, See on <a href="https://news.ycombinator.com/item?id=46478647">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>  <header> <nav aria-label="Global">  <!-- mobile menu -->  </nav> </header>  <main> <div> <div> <h2>
The
<span>C3</span>
Programming Language
</h2> <p> <!-- Simple, fast, safe, compiled. For developing maintainable software. -->
The Ergonomic, Safe and Familiar Evolution of C
</p>   </div> <div id="code-area"> <p><code>
                  <span> <span>module</span> <span>hello_world</span>;<br> <span>import</span> <span>std</span>::<span>io</span>;<p>  <span>fn</span> <span>void</span> <span>main</span>() <br>{<br> <span> <span>io</span>::<span>printn</span>(<span>"Hello, world!"</span>); </span> <br>
                  }</p></span>
                </code> </p> </div> </div> </main> <hr> <div> <div> <div> <h3>
Full C ABI Compatibility
</h3> <p>
C3 fits right into your C/C++ application with full C ABI
            compatibility out of the box: no need for special "C compatible"
            types or functions, no limitations on what C3 features you can use
            from C.
</p> </div> <div> <h3>
Module System
</h3> <p>
A simple and straightforward module system that doesn't get in the
            way, with defaults that makes sense.
</p> </div> <div> <h3>
Operator Overloading
</h3> <p>
C3 empowers you with precise, purpose-built operator overloading ‚Äî no C++ baggage, just clean, expressive code.
            Ideal for vectors, matrices, and fixed-point math that reads exactly how it should.
</p> </div> </div> <div> <h2>
C3 is an evolution, not a revolution:
<span>the C-like for programmers who like C.</span> </h2> <div><p>
C3 is a programming language that builds on the syntax and semantics of
        the C language, with the goal of evolving it while still retaining
        familiarity for C programmers.</p><p>
Thanks to full ABI compatibility with C, it's possible to mix C and C3 in
        the same project with no effort. As a demonstration, vkQuake was compiled
        with a small portion of the code converted to C3 and compiled with the c3c
        compiler.
</p></div> <a href="https://github.com/c3lang/vkQuake">
The fork can be found here
<svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"> <path fill-rule="evenodd" clip-rule="evenodd" d="M0.975821 6.92249C0.43689 6.92249 -3.50468e-07 7.34222 -3.27835e-07 7.85999C-3.05203e-07 8.37775 0.43689 8.79749 0.975821 8.79749L12.7694 8.79748L7.60447 13.7596C7.22339 14.1257 7.22339 14.7193 7.60447 15.0854C7.98555 15.4515 8.60341 15.4515 8.98449 15.0854L15.6427 8.68862C16.1191 8.23098 16.1191 7.48899 15.6427 7.03134L8.98449 0.634573C8.60341 0.268455 7.98555 0.268456 7.60447 0.634573C7.22339 1.00069 7.22339 1.59428 7.60447 1.9604L12.7694 6.92248L0.975821 6.92249Z" fill="currentColor"></path> </svg> </a> </div> </div> <!-- Icon Blocks --> <div> <div> <!-- Icon Block --> <div> <box-icon name="command" color="currentColor" fill="currentColor" size="24px"></box-icon> <div> <h3>
Compile Time and Semantic Macros
</h3> <p>
Unlock the full power of compile-time code with macros that read like functions
              ‚Äî clearer, stronger, and miles beyond C‚Äôs preprocessor.
</p> </div> </div> <!-- End Icon Block --> <!-- Icon Block --> <div> <box-icon type="solid" name="pen" color="currentColor" fill="currentColor" size="24px"></box-icon> <div> <h3>
Gradual Contracts
</h3> <p>
C3 brings programming-by-contract to the mainstream with
              unobtrusive contracts that are used to express both runtime and
              compile-time constraints.
</p> </div> </div> <!-- End Icon Block --> <!-- Icon Block --> <div> <box-icon name="comment-error" color="currentColor" fill="currentColor" size="24px"></box-icon> <div> <h3>
Zero Overhead Errors
</h3> <p>
Error handling that combines the best parts of "Result" errors
              with the easy use of exceptions and integrates seamlessly with C.
</p> </div> </div> <!-- End Icon Block --> <div> <box-icon name="package" color="currentColor" fill="currentColor" size="24px"></box-icon> <div> <h3>
Generic modules
</h3> <p>
C3 generic modules offer superior simplicity and clarity for
              creating generic types.
</p> </div> </div> <!-- End Icon Block --> </div> <!-- End Col --> <div> <!-- Icon Block --> <div> <box-icon name="refresh" color="currentColor" fill="currentColor" size="24px"></box-icon> <div> <h3>
Runtime and compile reflection
</h3> <p>
Type introspection is available both at compile time and runtime,
              powering flexible macros and functions
</p> </div> </div> <!-- End Icon Block --> <!-- Icon Block --> <div> <box-icon name="code" color="currentColor" fill="currentColor" size="24px"></box-icon> <div> <h3>
Inline Assembly
</h3> <p>
Write asm as regular inline code without using strings or cryptic
              constraints.
</p> </div> </div> <!-- End Icon Block --> <div> <box-icon name="bug" type="solid" color="currentColor" fill="currentColor" size="24px"></box-icon> <div> <h3>
Debug with safety checks
</h3> <p>
Feel confident in your code's correctness: in debug mode the
              compiler inserts extensive runtime bounds checks and value checks,
              which together with contracts will let you catch bugs early.
</p> </div> </div> <!-- End Icon Block --> <div> <box-icon name="spreadsheet" color="currentColor" fill="currentColor" size="24px"></box-icon> <div> <h3>
Detailed stacktraces
</h3> <p>
No more anonymous "segmentation fault" errors: the C3 standard
              library enables detailed stacktraces out of the box for your debug
              builds.
</p> </div> </div> </div> <!-- End Col --> </div> <hr> <div> <h2>
Get Started
</h2>  </div>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Most Popular Blogs of Hacker News in 2025 (568 pts)]]></title>
            <link>https://refactoringenglish.com/blog/2025-hn-top-5/</link>
            <guid>46478377</guid>
            <pubDate>Sat, 03 Jan 2026 16:20:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://refactoringenglish.com/blog/2025-hn-top-5/">https://refactoringenglish.com/blog/2025-hn-top-5/</a>, See on <a href="https://news.ycombinator.com/item?id=46478377">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><article><header></header><p>With 2025 wrapped up, I can finally answer a question I‚Äôm curious about every year: who were <a href="https://refactoringenglish.com/tools/hn-popularity/?start=2025-01-01&amp;end=2025-12-31">the most popular bloggers of Hacker News</a>?</p><div><p><strong>Who counts as a blogger?</strong></p><p>I explain more in <a href="https://refactoringenglish.com/tools/hn-popularity/methodology/">my methodology page</a>, but it‚Äôs basically anyone who blogs as an individual rather than as part of a company or a team. As an example, <a href="https://jgc.org/">John Graham-Cumming</a> is the CTO of Cloudflare, so I count his <a href="https://refactoringenglish.com/tools/hn-popularity/domain/?d=jgc.org">personal blog</a> but not his posts to the Cloudflare company blog.</p></div><h2 id="1-simon-willison">#1 <a href="https://refactoringenglish.com/tools/hn-popularity/domain/?d=simonwillison.net&amp;start=2025-01-01&amp;end=2025-12-26">Simon Willison</a><a href="#1-simon-willison">üîó</a></h2><p>For the third straight year, Simon Willison was the most popular blogger on Hacker News.</p><p>At first, Simon‚Äôs position at #1 feels obvious: he wrote about AI in a year when everyone‚Äôs obsessed with AI. But there are tons of AI bloggers, and Simon is the only one who‚Äôs popular on HN, so what sets Simon apart?</p><p>First, Simon isn‚Äôt selling you anything. Simon writes about LLMs as a power user not as a sales pitch from some startup‚Äôs VP of product. He tries every AI tool he can get his hands on with no allegiance to any particular vendor. That allows him to write about how new AI tools fit into the ecosystem at large. It‚Äôs like getting restaurant recommendations from someone who eats out 20 times a week as opposed to someone who owns 20 restaurant chains.</p><p>Simon is also one of the most prolific bloggers on Hacker News. In 2025 alone, he wrote over 1,000 blog posts, though only 118 were full-length articles (‚Äúonly‚Äù).</p><p>Simon often finds ideas within walled-garden platforms (e.g., TikTok, Twitter) and simply brings them to the open web, where it‚Äôs easier for HN to discuss. Some of his most popular posts were just short quotes or links with a bit of commentary. <a href="https://news.ycombinator.com/item?id=45820872">‚ÄúI‚Äôm worried that they put co-pilot in Excel‚Äù</a> is just a quote from a video he watched on TikTok. <a href="https://news.ycombinator.com/item?id=42923870">‚ÄúA computer can never be held accountable‚Äù</a> is Simon summarizing a few tweets.</p><p>Simon has said these types of posts are easy to write yet high in value.</p><blockquote><p>Sharing interesting links with commentary is a low effort, high value way to contribute to internet life at large.</p><p>‚ÄîSimon Willison, ‚Äú<a href="https://simonwillison.net/2024/Dec/22/link-blog/">My approach to running a link blog</a>‚Äù</p></blockquote><h2 id="2-jeff-geerling">#2 <a href="https://refactoringenglish.com/tools/hn-popularity/domain/?d=jeffgeerling.com&amp;start=2025-01-01&amp;end=2025-12-26">Jeff Geerling</a><a href="#2-jeff-geerling">üîó</a></h2><p>This is Jeff‚Äôs most successful year on Hacker News, beating his <a href="https://refactoringenglish.com/tools/hn-popularity/?start=2023-01-01&amp;end=2023-12-31&amp;domain=jeffgeerling.com">#5 finish in 2023</a>.</p><p>The #2 spot was an extremely tight race this year. Jeff‚Äôs posts totaled 10,813 upvotes, edging out the #3 blogger by just 9 points (a 0.08% difference). The #4 finisher was just 100 points behind that. Past stories can still accrue upvotes, so this could still flip, but these were the rankings as of midnight on Dec. 31st.</p><p>Jeff is a popular YouTube creator with over 1M subscribers. He covers some of HN‚Äôs favorite topics, like Raspberry Pi computers, self-hosted software, and computer hardware. YouTube videos rarely succeed on Hacker News, so when Jeff publishes a new video, he often publishes an accompanying blog post. Jeff isn‚Äôt the only YouTuber who does this, but he‚Äôs one of the few who does it <em>well</em>.</p><p>I‚Äôve seen other YouTube creators try to repurpose their videos by auto-generating a transcript and calling that a blog post. Jeff approaches his blog more thoughtfully.</p><p>Jeff started out as a blogger, and he still treats his blog readers as first-class citizens. He structures his articles to fit the text medium rather than just lazily scraping dialog from his videos. You can read his post about <a href="https://www.jeffgeerling.com/blog/2025/upgrading-m4-pro-mac-minis-storage-half-price">upgrading storage on his Mac mini</a> and not even realize it‚Äôs adapted from a video.</p><h2 id="3-sean-goedecke">#3 <a href="https://refactoringenglish.com/tools/hn-popularity/domain/?d=seangoedecke.com&amp;start=2025-01-01&amp;end=2025-12-26">Sean Goedecke</a><a href="#3-sean-goedecke">üîó</a></h2><p>Sean came out of nowhere as a blogging powerhouse this year. He‚Äôd been blogging sporadically since 2020, but he hit a turning point at the end of 2024 with <a href="https://www.seangoedecke.com/how-to-ship/">‚ÄúHow I ship projects at big tech companies.‚Äù</a> It was one of HN‚Äôs top 100 posts of the year and remains <a href="https://news.ycombinator.com/item?id=42111031">Sean‚Äôs most popular post on HN</a>.</p><p>After his first success on HN, Sean went from publishing every few months to multiple times per week, becoming a regular fixture on the front page.</p><p>Sean is a Staff Software Engineer at GitHub and previously worked at Zendesk. Like Simon, Sean is extremely prolific. He wrote 140 posts this year. Of those, 47 reached the front page. I‚Äôve had a few years where a lot of my posts reached the front page, but for me that was <a href="https://refactoringenglish.com/tools/hn-popularity/domain/?d=mtlynch.io&amp;start=2020-01-01&amp;end=2020-12-31">about 10 articles</a>. I can‚Äôt imagine what it‚Äôs like to have a new article on HN almost every week.</p><p>Sean explains his strategy in <a href="https://www.seangoedecke.com/on-writing/">‚ÄúWriting a tech blog people want to read‚Äù</a>:</p><blockquote><p>the recipe for a popular post is to have a clear opinion about working in tech that many people disagree with.</p></blockquote><p>I think Sean‚Äôs insight is true, but he‚Äôs selling his own writing a bit short. To me, Sean‚Äôs greatest strength is his ability to explain big tech organizational politics to engineers.</p><p>Most junior to mid-level developers don‚Äôt care about company politics. They think of office politics as something that strong technical thinkers shouldn‚Äôt have to waste brain cells on. As a result, they can‚Äôt understand why <a href="https://www.seangoedecke.com/staff-engineer-promotions/">they can‚Äôt get promoted</a> or how <a href="https://www.seangoedecke.com/bad-code-at-big-companies/">their company‚Äôs codebase got so bad</a>. Sean‚Äôs posts explain these phenomena in a way that‚Äôs clear and intelligible to engineers.</p><p>Sean‚Äôs posts are also a good example of how much luck comes into play on Hacker News, especially for less established authors. Sean‚Äôs top three posts of the year all flopped on their first submission and didn‚Äôt succeed until their second or third try, sometimes months later. Even then, only a third of his posts reached the front page at all.</p><h2 id="4-brian-krebs">#4 <a href="https://refactoringenglish.com/tools/hn-popularity/domain/?d=krebsonsecurity.com&amp;start=2025-01-01&amp;end=2025-12-26">Brian Krebs</a><a href="#4-brian-krebs">üîó</a></h2><p>Brian Krebs is an independent investigative journalist who covers cybercrime. He‚Äôs one of HN‚Äôs <a href="https://refactoringenglish.com/tools/hn-popularity/">most popular bloggers of all time</a>, second only to <a href="https://refactoringenglish.com/tools/hn-popularity/domain/?d=paulgraham.com">Paul Graham</a>, the creator of HN. For 11 of the last 12 years, Brian has been one of HN‚Äôs top 10 bloggers.</p><p>In 2025, Brian mostly stuck to his usual beat of deeply investigated cybersecurity stories, but his <a href="https://news.ycombinator.com/item?id=43529707">second most popular story</a> of the year was a sobering post about the Trump administration‚Äôs steps to undermine free speech in the US. It immediately shot to the #1 slot and stayed there for several hours. Unfortunately, too many users flagged the post, and it was moderated off the front page, which is often the fate of political stories on HN.</p><h2 id="5-neal-agarwal">#5 <a href="https://refactoringenglish.com/tools/hn-popularity/domain/?d=neal.fun&amp;start=2025-01-01&amp;end=2025-12-26">Neal Agarwal</a><a href="#5-neal-agarwal">üîó</a></h2><p>Neal‚Äôs work isn‚Äôt what you might think of as blog posts; they‚Äôre more like interactive art. Some of his posts are games that parody the web, while others are straight-faced visual essays about topics he finds interesting.</p><p>This was Neal‚Äôs most successful year on HN. Everything he published reached the front page, with about half hitting #1, and the rest peaking at #2. <a href="https://news.ycombinator.com/item?id=42611536">Stimulation Clicker</a> was the 4th most popular post of the entire year.</p><h2 id="other-notes">Other notes<a href="#other-notes">üîó</a></h2><ul><li><a href="https://refactoringenglish.com/tools/hn-popularity/domain/?d=daringfireball.net&amp;start=2025-01-01&amp;end=2025-12-31">John Gruber</a> finished the year in #6 despite wondering aloud back in March <a href="https://news.ycombinator.com/item?id=43489058">whether Hacker News</a> had shadowbanned his blog. It was his best year on Hacker News since 2011 and his first appearance in the top 10 since 2020.</li><li><a href="https://refactoringenglish.com/tools/hn-popularity/domain/?d=skyfall.dev&amp;start=2025-01-01&amp;end=2025-12-31">Mahad Kalam</a> finished at #21 for the year with <a href="https://skyfall.dev/posts/slack">a single blog post</a>, which became <a href="https://news.ycombinator.com/item?id=45283887">the top post</a> of the year. <a href="https://refactoringenglish.com/tools/hn-popularity/domain/?d=byran.ee&amp;start=2025-01-01&amp;end=2025-12-31">Byran Huang</a> appeared right behind him, also with <a href="https://www.byran.ee/posts/creation/">a single blog post</a>, which became the #3 <a href="https://news.ycombinator.com/item?id=42797260">most upvoted post</a> of the year.</li></ul><h2 id="full-list">Full list<a href="#full-list">üîó</a></h2><ul><li><a href="https://refactoringenglish.com/tools/hn-popularity/?start=2025-01-01&amp;end=2025-12-31">The top 100 bloggers of 2025</a></li></ul></article></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[X-Clacks-Overhead (115 pts)]]></title>
            <link>https://hleb.dev/post/x-clacks-overhead/</link>
            <guid>46475437</guid>
            <pubDate>Sat, 03 Jan 2026 11:37:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hleb.dev/post/x-clacks-overhead/">https://hleb.dev/post/x-clacks-overhead/</a>, See on <a href="https://news.ycombinator.com/item?id=46475437">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p>I am a big fan of Sir Terry Pratchett. His books influenced my sense of humor and the way I look at things, far beyond just fantasy literature. Some time ago, I added a small but meaningful detail to this blog - the X-Clacks-Overhead HTTP header.</p><p><img src="https://hleb.dev/images/x-clacks-overhead/cover.jpg" alt="Post cover" title="Photo of a cherry blossom tree with many pink flowers."></p><p><a href="https://xclacksoverhead.org/home/about">This header</a> is a reference to the ‚ÄúGoing Postal‚Äù novel and the <a href="https://discworld.fandom.com/wiki/Clacks">Clacks system</a>. In short, it is a quiet way to say ‚ÄúGNU Terry Pratchett‚Äù - a signal that keeps his name moving through the network, not forgotten.</p><p>My blog is served with Cloudflare Pages. It supports custom HTTP response headers via a plain text <code>_headers</code> file placed in the root directory of your website, injecting headers according to the rules you define.</p><p>Here‚Äôs the relevant section from my <code>_headers</code> file:</p><div><pre tabindex="0"><code data-lang="text"><span><span>/*
</span></span><span><span>  X-Clacks-Overhead: "GNU Terry Pratchett"</span></span></code></pre></div>That‚Äôs it - every request for static assets and HTML gets the header.<p>If you want to check it yourself, open browser developer tools ‚Üí Network tab ‚Üí inspect the response headers, or run:</p><p>It does nothing useful. It changes nothing about performance or functionality. But sometimes small, unnecessary things are exactly what make the internet better.</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Recursive Language Models (141 pts)]]></title>
            <link>https://arxiv.org/abs/2512.24601</link>
            <guid>46475395</guid>
            <pubDate>Sat, 03 Jan 2026 11:29:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2512.24601">https://arxiv.org/abs/2512.24601</a>, See on <a href="https://news.ycombinator.com/item?id=46475395">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2512.24601">View PDF</a>
    <a href="https://arxiv.org/html/2512.24601v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Alex Zhang [<a href="https://arxiv.org/show-email/8b3b609d/2512.24601" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Wed, 31 Dec 2025 03:43:41 UTC (7,933 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Late night pizzeria nearby The Pentagon has suddenly surged in traffic (175 pts)]]></title>
            <link>https://twitter.com/PenPizzaReport/status/2007347706017251535</link>
            <guid>46474859</guid>
            <pubDate>Sat, 03 Jan 2026 10:09:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/PenPizzaReport/status/2007347706017251535">https://twitter.com/PenPizzaReport/status/2007347706017251535</a>, See on <a href="https://news.ycombinator.com/item?id=46474859">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="ScriptLoadFailure"><form action="" method="GET"><div><p><span>Something went wrong, but don‚Äôt fret ‚Äî let‚Äôs give it another shot.</span></p><p><img alt="‚ö†Ô∏è" draggable="false" src="https://abs-0.twimg.com/emoji/v2/svg/26a0.svg"><span> Some privacy related extensions may cause issues on x.com. Please disable them and try again.</span></p></div></form></div></div>]]></description>
        </item>
    </channel>
</rss>