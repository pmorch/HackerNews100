<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 21 Sep 2024 12:30:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Ultra high-resolution image of The Night Watch (111 pts)]]></title>
            <link>https://www.rijksmuseum.nl/en/stories/operation-night-watch/story/ultra-high-resolution-image-of-the-night-watch</link>
            <guid>41608648</guid>
            <pubDate>Sat, 21 Sep 2024 09:08:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.rijksmuseum.nl/en/stories/operation-night-watch/story/ultra-high-resolution-image-of-the-night-watch">https://www.rijksmuseum.nl/en/stories/operation-night-watch/story/ultra-high-resolution-image-of-the-night-watch</a>, See on <a href="https://news.ycombinator.com/item?id=41608648">Hacker News</a></p>
<div id="readability-page-1" class="page"><div v-else="">


<gtm-scroll-tracker location="article_story" category="article" scroll-element-id="openPipModal" inline-template="">
  <div>
      

<div>
      
  <p>The new high-resolution image of The Night Watch represents a major advance in the state of the art for imaging paintings, setting records for both the resolution and the total size of the image. The sampling resolution is 5 µm (0.005 mm), meaning that each pixel covers an area of the painting that is smaller than a human red blood cell. Given the large size of The Night Watch, this results in a truly enormous image: it’s 925,000 by 775,000 pixels – 717 gigapixels – with a file size of 5.6 TB!</p>

    </div>

<div>
    <h2>Grid</h2>
<p>To create this huge image, the painting was photographed in a grid with 97 rows and 87 columns with our 100-megapixel Hasselblad H6D 400 MS camera. Each of these 8,439 separate photos was captured using a sophisticated laser-guided five-axis camera positioning system that can sense the precise location of the painting so that every photo is sharp – an error of even 1/8 mm in the placement of the camera would result in a useless image.</p>

  </div>

<div>
    <h2>New technology</h2>
<p>New technology allowed the previously-released 20 µm resolution image of The Night Watch to serve as the guide for  lining up these much higher-resolution images during the process of fusing the individual captures into a single monolithic image. The technology allows each of the other types of images collected during Operation Night Watch to be precisely aligned with each other, thereby allowing all of our data to be seen in context.</p>

  </div>

<div>
    <h2>Physical state of the painting</h2>
<p>Why create such an incredibly huge image? With this resolution, we can very clearly see the precise physical state of the painting. Lead soap protrusions, tiny cracks, the shapes of individual paint pigment particles, past retouches, and the beautiful details of Rembrandt’s painting technique are all extraordinarily clear. This enables researchers to understand the painting’s condition in order to make the best plan for future conservation treatments. It helps us to better understand how Rembrandt painted, and it creates an exquisite 'snapshot’ of The Night Watch at this moment in its history.</p>

  </div>
    </div>
</gtm-scroll-tracker>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Like Makefiles (308 pts)]]></title>
            <link>https://switowski.com/blog/i-like-makefiles/</link>
            <guid>41607059</guid>
            <pubDate>Sat, 21 Sep 2024 02:37:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://switowski.com/blog/i-like-makefiles/">https://switowski.com/blog/i-like-makefiles/</a>, See on <a href="https://news.ycombinator.com/item?id=41607059">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

<div>
<p>I like makefiles. I first used a makefile more than ten years ago. Even back then, it looked like some ancient technology used by the graybeard Linux wizards. Years passed, and new build tools came and went, but I kept seeing makefiles still used here and there. I got used to them because they were part of some projects I joined. At some point, I started to like them. Today, they are often the first automation tool I use when I start a new project.</p>
<p>The reason I like makefiles is that they often follow an unwritten convention of implementing the same set of commands to get you up and running. When I find a project I know nothing about, and I see a <code>Makefile</code> file inside, chances are that I can run <code>make</code> or <code>make build</code> followed by <code>make install</code>, and I will get this project built and set up on my computer. Or at least I will get information on other steps I need to include.</p>
<p>I try to apply the same rule in my projects. If I open a folder with one of my old projects and run <code>make dev</code>, this will perform all the necessary steps to build the project and spin up a dev server. That's convenient because throughout the years, I used many different technologies, and each had different commands to build or deploy a project. I have old projects written in Jekyll, Hugo, 11ty, and all sorts of different Python web frameworks. With makefiles, when I come back to a project I haven't touched for months (or years), I don't have to remember the command to start a dev server with, let's say, Jekyll. I just run <code>make dev</code>, and this, in turn, fires up the corresponding Bundler commands. Even if I use tools like Docker or gulp in my project, I still use makefiles to orchestrate those tools. For example, I often write a <code>make build</code> command that builds all the necessary Docker images, passing additional parameters specific to a given project.</p>
<p>My makefiles are simple. I don't use conditional statements, flags or any other fancy features. Most of the tasks (they are technically called <em>targets</em>, but I always call them <em>tasks</em> in my head) consist of one or more shell commands. I could write bash scripts with a couple of functions instead, but makefiles are easier and faster to write.</p>
<p>Some common tasks that most of my personal projects<sup><a href="#fn1" id="fnref1">[1]</a></sup> contain include:</p>
<ul>
<li><code>dev</code> to start the development server</li>
<li><code>build</code> to build the project (if a build step is necessary)</li>
<li><code>deploy</code> to deploy/publish the project</li>
</ul>
<p>And that's really it. Sometimes, I include additional tasks like <code>watch</code> to automatically rerun the build task when I change any of the source files. But many of my projects can be managed with just two or three Make commands.</p>
<p>This blog that you're reading right now has a simple makefile with just one target:</p>
<pre data-language="makefile"><code><span>dev</span><span>:</span><br>	npm run dev</code></pre>
<p>And a more advanced project of mine uses the following makefile to run the dev server, watch for changes, build, encrypt and deploy the website:</p>
<pre data-language="makefile"><code><span># Run dev server</span><br><span>dev</span><span>:</span><br>	bundle exec jekyll serve --unpublished -w --config _config.yml,_config-dev.yml --livereload<p><span># Build assets</span><br><span>build</span><span>:</span><br>	npm run gulp build</p><p><span># Watch a specific folder and process assets</span><br><span>watch</span><span>:</span><br>	npm run gulp watch -- --wip</p><p><span># Build the website locally, encrypt and deploy to Netlify server</span><br><span>deploy</span><span>:</span><br>	JEKYLL_ENV<span>=</span>production bundle exec jekyll build<span>;</span> \<br>	make encrypt<span>;</span> \<br>	netlify deploy --prod</p><p><span># Encrypt the "_site" folder</span><br><span>encrypt</span><span>:</span><br>	npx staticrypt _site/*.html -r -d _site</p></code></pre>
<p><a href="https://www.gnu.org/software/make/">GNU Make</a> (the software that runs makefiles) is quite ubiquitous. If you're on Linux, you probably already have it installed. Even on my MacBook, I don't remember installing it explicitly. It must have come with some other tools that I installed in the past. Make is simple and doesn't require as many additional dependencies as some other build tools. This can be useful if you need a tool that will work in a restricted environment where installing additional packages is difficult or impossible for security reasons. Make will probably be already present in that environment. And if not, you can just take the commands from the makefile and run them manually in the shell. If gulp is not available on your server, you can't really take the JavaScript code and paste that into the terminal.</p>
<p>I'm not against other build tools. I like other build tools too. I'm excited when I find a new one that is better and faster than the one I was using before. But I will still use Make to orchestrate them because it gives me a set of familiar commands to manage all sorts of different setups with different tools.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>By "personal", I mean projects where the deployment process is much simpler than production-grade stuff. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>
</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Working in the office 5 days/week to build company culture is a myth: PwC report (139 pts)]]></title>
            <link>https://www.msn.com/en-us/money/other/working-in-the-office-5-days-a-week-to-build-company-culture-is-a-myth-pwc-report-says/ar-AA1qU17L</link>
            <guid>41606772</guid>
            <pubDate>Sat, 21 Sep 2024 01:19:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.msn.com/en-us/money/other/working-in-the-office-5-days-a-week-to-build-company-culture-is-a-myth-pwc-report-says/ar-AA1qU17L">https://www.msn.com/en-us/money/other/working-in-the-office-5-days-a-week-to-build-company-culture-is-a-myth-pwc-report-says/ar-AA1qU17L</a>, See on <a href="https://news.ycombinator.com/item?id=41606772">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Shares Full iPhone 16 and iPhone 16 Pro Repair Manuals (171 pts)]]></title>
            <link>https://www.macrumors.com/2024/09/20/iphone-16-repair-manual/</link>
            <guid>41606530</guid>
            <pubDate>Sat, 21 Sep 2024 00:17:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macrumors.com/2024/09/20/iphone-16-repair-manual/">https://www.macrumors.com/2024/09/20/iphone-16-repair-manual/</a>, See on <a href="https://news.ycombinator.com/item?id=41606530">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="maincontent"><article expanded="true"><div data-io-article-url="/2024/09/20/iphone-16-repair-manual/"><p>Following today's launch of the new <a href="https://www.macrumors.com/roundup/iphone-16/">iPhone 16</a> models, Apple has shared repair manuals for the <a href="https://support.apple.com/en-us/120652">iPhone 16</a>, the <a href="https://support.apple.com/en-us/120662">iPhone 16 Plus</a>, the <a href="https://support.apple.com/en-us/120803">iPhone 16 Pro</a>, and the <a href="https://support.apple.com/en-us/120828">iPhone 16 Pro Max</a>. The repair manuals provide technical instructions on replacing genuine Apple parts in the ‌iPhone 16‌ models, and Apple says the information is intended for "individual technicians" that have the "knowledge, experience, and tools" that are necessary to repair electronic devices.</p>
<p><img src="https://images.macrumors.com/t/DyvENV2KG5ztE-4KmOdRWXddhB0=/400x0/article-new/2024/09/apple-iphone-battery-repair.png?lossy" srcset="https://images.macrumors.com/t/DyvENV2KG5ztE-4KmOdRWXddhB0=/400x0/article-new/2024/09/apple-iphone-battery-repair.png?lossy 400w,https://images.macrumors.com/t/yEUxiBQZnvr46Va0nt6XzXTb0nk=/800x0/article-new/2024/09/apple-iphone-battery-repair.png?lossy 800w,https://images.macrumors.com/t/iUIKJQgB3N_lYIBo-1A3ddoJHkQ=/1600x0/article-new/2024/09/apple-iphone-battery-repair.png 1600w,https://images.macrumors.com/t/BOLPZc3YRFrN04Tgf5yZ0UPmptw=/2500x0/filters:no_upscale()/article-new/2024/09/apple-iphone-battery-repair.png 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="apple iphone battery repair" width="880" height="600"><br>Apple <a href="https://support.apple.com/en-us/120983">has support documents</a> on the tools that are required for various repairs, and while the ‌iPhone 16‌ tools aren't yet available on <a href="https://www.selfservicerepair.com/en-US/order">Apple's Self Service Repair website</a>, they'll likely be added soon.</p>
<p>Compared to prior <a href="https://www.macrumors.com/guide/iphone/">iPhone</a> models, the ‌iPhone 16‌ and ‌iPhone 16‌ Plus are easier to repair. Apple is using an electric battery removal process, and the steps for accessing a battery to replace it are outlined in a <a href="https://support.apple.com/en-us/120642">separate support document</a>. Per Apple's instructions, a 9-volt battery and 9-volt battery clips can be applied to the ‌iPhone 16‌ battery to remove the adhesive that holds it in place.</p>
<p>Note that the simpler electricity-based battery removal process is limited to the ‌iPhone 16‌ and ‌iPhone 16‌ Plus. For the <a href="https://www.macrumors.com/roundup/iphone-16-pro/">iPhone 16 Pro</a> and Pro Max, Apple is using standard adhesive tabs that need to be carefully pulled to release the battery.</p>
<p>Apple's instructions for all of the battery repairs include expensive equipment like an ‌iPhone‌ battery press to put a replacement battery back in place. The cost of the tools required for device repair and the cost of genuine components make self repair almost as expensive as getting a repair from an Apple retail location or an Apple Authorized Service Provider, so these manuals are really aimed at independent repair shops rather than individual consumers.</p>
<p>Apple <a href="https://www.macrumors.com/2024/09/18/iphone-16-repairability-changes/">made other changes</a> to the ‌iPhone 16‌ models this year to improve repairability, enabling on-device configuration for the <a href="https://www.macrumors.com/guide/mask-face-id/">Face ID</a> camera, allowing LiDAR Scanner repair, and offering support for swapping the TrueDepth camera modules between ‌iPhone 16‌ and ‌iPhone 16 Pro‌ models.</p>
<p>In addition to offering repair instructions, Apple's manuals provide some insight into the internal structure of the new iPhones that we often don't see until there <a href="https://www.macrumors.com/2024/09/20/iphone-16-pro-teardown-video/">are device teardowns</a>. The ‌iPhone 16 Pro‌, for example, has a metal casing for the battery, a change made for thermal reasons, and both Pro models have new casing structure that improves heat dissipation.</p>
</div></article><p><h2>Popular Stories</h2></p><div><h3><a href="https://www.macrumors.com/2024/09/17/rip-apple-id/">RIP, Apple ID</a></h3><p>Tuesday September 17, 2024 3:18 pm PDT by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>The "Apple ID" era is officially over. The transition from "Apple ID" to "Apple Account" went from a rumor to an official announcement to something that has now been fully completed. As of this week, the account.apple.com website is fully updated with Apple Account branding. "Apple ID is now Apple Account," the page says. "You can still sign in with the same email address or phone...</p></div><div><h3><a href="https://www.macrumors.com/2024/09/15/ios-18-available-tomorrow/">iOS 18 Available Now With These 8 New Features For Your iPhone</a></h3><p>Sunday September 15, 2024 10:09 am PDT by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>Following over three months of beta testing, iOS 18 was finally widely released to the public on Monday, September 16. The update is available in the Settings app under General → Software Update on the iPhone XS and newer. Below, we have highlighted eight key new features included in iOS 18, and Apple shared a complete list of new features and changes last week. Note that Apple...</p></div><div><h3><a href="https://www.macrumors.com/2024/09/19/apple-lists-3-more-states-committed-to-wallet-ids/">Apple Announces iPhone Driver's Licenses Will Come to These Additional U.S. States</a></h3><p>Thursday September 19, 2024 10:45 am PDT by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>In select U.S. states, residents can add their driver's license or state ID to the Wallet app on the iPhone and Apple Watch, providing a convenient and contactless way to display proof of identity or age at select airports and businesses, and in select apps. The list of states where the feature is available currently includes Arizona, Maryland, Colorado, Georgia, Ohio, Hawaii, and most recently...</p></div><div><h3><a href="https://www.macrumors.com/2024/09/18/apple-airpods-pro-2-firmware-7a302/">Apple Releases New AirPods Pro 2 and AirPods 4 Firmware</a></h3><p>Wednesday September 18, 2024 11:34 am PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>Apple today released a new firmware update for all AirPods Pro 2 and AirPods 4 models. The AirPods Pro 2 firmware has a build number of 7A302, up from 7A294, and the AirPods 4 firmware has a build number of 7A304. There is no word yet on what’s included in the firmware, but it comes just a week after Apple last updated the AirPods Pro 2 firmware to add iOS 18 features like support for head...</p></div><div><h3><a href="https://www.macrumors.com/2024/09/18/apple-breaks-17-year-tradition/">Apple Just Broke a Tradition It Held for 17 Years</a></h3><p>Wednesday September 18, 2024 7:40 am PDT by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>It's the end of an era. It has been confirmed that the latest iPhones do not come with Apple stickers in the box, breaking a 17-year tradition dating back to the original iPhone. Marques Brownlee shared an unboxing video that confirms the new iPhone 16, iPhone 16 Plus, iPhone 16 Pro, and iPhone 16 Pro Max do not include Apple stickers in the box, as part of Apple's goal of removing plastic...</p></div><div><h3><a href="https://www.macrumors.com/2024/09/17/apple-pulls-ipados-18-m4-ipad-pro/">Apple Pulls iPadOS 18 for M4 iPad Pro After Bricking Complaints [Updated]</a></h3><p>Tuesday September 17, 2024 11:24 am PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>Apple stopped signing the iPadOS 18 update for the M4 iPad Pro models, which means the new software is no longer available to be downloaded and installed at the current time. The update appears to have been pulled following complaints from some iPad Pro owners, who found that the update bricked their devices. There are reports on Reddit from iPad Pro users who had an interruption in the...</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CISA boss: Makers of insecure software are the real cyber villains (118 pts)]]></title>
            <link>https://www.theregister.com/2024/09/20/cisa_sloppy_vendors_cybercrime_villains/</link>
            <guid>41606493</guid>
            <pubDate>Sat, 21 Sep 2024 00:05:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2024/09/20/cisa_sloppy_vendors_cybercrime_villains/">https://www.theregister.com/2024/09/20/cisa_sloppy_vendors_cybercrime_villains/</a>, See on <a href="https://news.ycombinator.com/item?id=41606493">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>Software suppliers who ship buggy, insecure code are the true baddies in the cyber crime story, Jen Easterly, boss of the US government's Cybersecurity and Infrastructure Security Agency, has argued.</p>
<p>"The truth is: Technology vendors are the characters who are building problems" into their products, which then "open the doors for villains to attack their victims," declared Easterly during a Wednesday <a target="_blank" rel="nofollow" href="https://mwise.mandiant.com/conf24/home">keynote</a> address at Mandiant's mWise conference.</p>
<p>Easterly also implored the audience to stop "glamorizing" crime gangs with fancy poetic names. How about "Scrawny Nuisance" or "Evil Ferret," Easterly suggested.</p>

    

<p>Even calling security holes "software vulnerabilities" is too lenient, she added. This phrase "really diffuses responsibility. We should call them 'product defects,'" Easterly said. And instead of automatically blaming victims for failing to patch their products quickly enough, "why don't we ask: Why does software require so many urgent patches? The truth is: We need to demand more of technology vendors."</p>
<blockquote>

<p>Why does software require so many urgent patches? We need to demand more of vendors</p>
</blockquote>
<p>While everyone in the audience at the annual infosec conference has job security, Easterly joked, it's also the industry's role to make it more difficult for miscreants to compromise systems in the first place.</p>
<p>"Despite a multi-billion-dollar cyber security industry, we still have a multi-trillion-dollar software quality issue leading to a multi-trillion-dollar global cyber crime issue," Easterly lamented.</p>

        


        

<p>While no one would buy a car or board an airplane "entirely at your own risk," we do that every day with the software that underpins America's critical infrastructure, she added.</p>
<p>"Unfortunately we have fallen prey to the myth of techno exceptionalism," Easterly opined. "We don't have a cyber security problem – we have a software quality problem. We don't need more security products – we need more secure products."</p>
<ul>

<li><a href="https://www.theregister.com/2024/07/01/cisa_big_tech_security/">CISA director: US is 'not afraid' to shout about Big Tech's security failings</a></li>

<li><a href="https://www.theregister.com/2024/05/08/cisa_ransomware_rsac/">CISA boss: Secure code is the 'only way to make ransomware a shocking anomaly'</a></li>

<li><a href="https://www.theregister.com/2024/05/09/68_tech_firms_sign_cisas/">68 tech names sign CISA's secure-by-design pledge</a></li>

<li><a href="https://www.theregister.com/2024/05/10/dod_usb_attack/">'Four horsemen of cyber' look back on 2008 DoD IT breach that led to US Cyber Command</a></li>
</ul>
<p>This is a drum Easterly has been <a target="_blank" href="https://www.theregister.com/2023/02/28/cisa_easterly_secure_software/">beating</a> since she took the helm of the US cyber defense agency. She tends to bang it louder at industry events, such as the annual RSA Conference where she <a target="_blank" href="https://www.theregister.com/2024/05/08/cisa_ransomware_rsac/">told</a> attendees secure code "is the only way we can make ransomware and cyber attacks a shocking anomaly."</p>
<p>Naturally, if writing flawless code was super easy, it would be done without fail. Some developers are clearly careless or clueless, leading to vulnerabilities and other bugs, and sometimes skilled humans with the best intentions simply make mistakes. In any case, Easterly isn't happy with the current defect rate.</p>

        

<p>Also at RSAC, nearly 70 big names – including AWS, Microsoft, Google, Cisco, and IBM – <a target="_blank" href="https://www.theregister.com/2024/05/09/68_tech_firms_sign_cisas/">signed</a> CISA's Secure by Design pledge – a commitment to "make a good-faith effort to work towards" seven secure-software goals within a year, and be able to measurably show their progress.</p>
<p>At mWise, Easterly revealed that number has grown to nearly 200 vendors.</p>
<p>But the pledge remains voluntary, so software companies who fail to follow its guidelines – such as increasing the use of multi-factor authentication across their products and reducing default passwords – aren't going to be slapped down if they ignore it.</p>
<div>
<h2 title="Not so much when trying to convert coding veterans">Google says replacing C/C++ in firmware with Rust is easy</h2>
<p><a href="https://www.theregister.com/2024/09/06/google_rust_c_code_language/"><span>READ MORE</span></a></p></div>
<p>Easterly wants that to change. She suggested technology buyers use their procurement power to pressure software vendors, by asking suppliers if they have signed the pledge – and, hopefully, done more than just put ink to paper in terms of building <a target="_blank" rel="nofollow" href="https://www.cisa.gov/sites/default/files/2023-10/Shifting-the-Balance-of-Cybersecurity-Risk-Principles-and-Approaches-for-Secure-by-Design-Software.pdf">secure-by-design</a> [PDF] products.</p>
<p>To this end, CISA just published <a target="_blank" rel="nofollow" href="https://www.cisa.gov/resources-tools/resources/secure-demand-guide">guidance</a> that organizations buying software can use, and questions they should ask manufacturers, to better understand if they are prioritizing security in the product development life cycle.</p>

        

<p>"Use your voice, take an active role, use your purchasing power to advance secure by design, by demanding it," Easterly urged.</p>
<p>And then cross your fingers and pray that more and more vendors really do begin to take things like pre-release <a target="_blank" href="https://www.theregister.com/2024/08/01/crowdstrike_lawsuit/">software testing</a> and <a target="_blank" href="https://www.theregister.com/2024/06/15/microsoft_brad_smith_congress/">secure code</a> to heart. ®</p>                                


                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Critical Exploit in MediaTek Wi-Fi Chipsets: Zero-Click Vulnerability (154 pts)]]></title>
            <link>https://blog.sonicwall.com/en-us/2024/09/critical-exploit-in-mediatek-wi-fi-chipsets-zero-click-vulnerability-cve-2024-20017-threatens-routers-and-smartphones/</link>
            <guid>41605680</guid>
            <pubDate>Fri, 20 Sep 2024 21:28:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.sonicwall.com/en-us/2024/09/critical-exploit-in-mediatek-wi-fi-chipsets-zero-click-vulnerability-cve-2024-20017-threatens-routers-and-smartphones/">https://blog.sonicwall.com/en-us/2024/09/critical-exploit-in-mediatek-wi-fi-chipsets-zero-click-vulnerability-cve-2024-20017-threatens-routers-and-smartphones/</a>, See on <a href="https://news.ycombinator.com/item?id=41605680">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

				<main role="main" itemscope="itemscope" itemtype="https://schema.org/Blog">

                    <article itemscope="itemscope" itemtype="https://schema.org/BlogPosting" itemprop="blogPost"><p><a href="https://blog.sonicwall.com/en-us/author/securitynews/"><span><img src="https://d3ik27cqx8s5ub.cloudfront.net/blog/media/uploads/ultimatemember/3368/profile_photo-190x190.png?1726884307" width="81" height="81" alt="Security News" data-default="https://blogtest.sonicwall.com/wp-content/uploads/2018/04/user_default-avatar.png" onerror="if ( ! this.getAttribute('data-load-error') ){ this.setAttribute('data-load-error', '1');this.setAttribute('src', this.getAttribute('data-default'));}"></span></a></p><div><header><h2><em></em></h2><br><time>September 19, 2024</time></header><div itemprop="text"><h3>Overview</h3>
<p>The SonicWall Capture Labs threat research team became aware of the threat CVE-2024-20017, assessed its impact and developed mitigation measures for the vulnerability. CVE-2024-20017 is a critical zero-click vulnerability with a CVSS 3.0 score of 9.8, impacting MediaTek Wi-Fi chipsets MT7622/MT7915 and RTxxxx SoftAP driver bundles used in products from various manufacturers, including Ubiquiti, Xiaomi and Netgear. The affected versions include MediaTek SDK versions 7.4.0.1 and earlier, as well as OpenWrt 19.07 and 21.02. This translates to a large variety of vulnerable devices, including routers and smartphones. The flaw allows remote code execution without user interaction due to an out-of-bounds write issue. MediaTek has released patches to mitigate the vulnerability and users should update their devices immediately. While this vulnerability was <a href="https://corp.mediatek.com/product-security-bulletin/March-2024" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">published</a> and patched back in March, only recently did a public <a href="https://github.com/mellow-hype/cve-2024-20017/tree/main" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">PoC</a> become available making exploitation more likely.</p>
<h3>Technical Overview</h3>
<p>The vulnerability resides in wappd, a network daemon included in the MediaTek MT7622/MT7915 SDK and RTxxxx SoftAP driver bundle. This service is responsible for configuring and managing wireless interfaces and access points, particularly with Hotspot 2.0 technologies. The architecture of wappd is complex, comprising the network service itself, a set of local services that interact with the device’s wireless interfaces, and communication channels between components via Unix domain sockets.&nbsp; Ultimately, the vulnerability is a buffer overflow as a result of a length value taken directly from attacker-controlled packet data without bounds checking and placed into a memory copy.&nbsp; This buffer overflow creates an out-of-bounds write.</p>
<h3>Triggering the Vulnerability</h3>
<p>The vulnerability exists in the IAPP_RcvHandlerSSB function where an attacker controlled length value is passed to the IAPP_MEM_MOVE macro as described in hyprdude’s <a href="https://blog.coffinsec.com/0day/2024/08/30/exploiting-CVE-2024-20017-four-different-ways.html" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">blog</a> and seen in Figure 1.</p>
<div itemscope="itemscope" itemtype="https://schema.org/ImageObject"><p><a href="https://d3ik27cqx8s5ub.cloudfront.net/blog/media/uploads/Figure1-6.png" data-wpel-link="external" target="_blank" rel="external noopener noreferrer"><img src="https://d3ik27cqx8s5ub.cloudfront.net/blog/media/uploads/Figure1-6.png" alt="" title="Figure1" itemprop="thumbnailUrl"></a></p></div>
<p><em>Figure 1: Vulnerable Code sourced from hyprdude</em></p>
<p>Prior to the last line which calls IAPP_MEM_MOVE, the only bounds check done is to check that the provided length does not exceed the maximum packet length of 1600 bytes. As the size of the destination struct is only 167 bytes, this results in a stack buffer overflow of up to 1433 bytes. To trigger this vulnerability an attacker must send a packet with the expected structures prepending the attack payload.&nbsp; These structures are referred to as the RT_IAPP_HEADER and the RT_IAPP_SEND_SECURITY_BLOCK within the code.&nbsp; To bypass validation checks the length of the RT_IAPP_HEADER struct needs to be small and the RT_IAPP_HEADER.Command field must be to 50.</p>
<h3>Exploitation</h3>
<p>The publicly available exploit code achieves remote code execution by using a global address table overwrite technique via a return-oriented programming (ROP) chain. This method leverages the `system()` call to execute commands, such as sending a reverse shell back to the attacker. The reverse shell is established using Bash and the existing Netcat tool on the chipset. Figure 2 illustrates how the reverse shell command is crafted and embedded within the payload to enable this exploitation tactic.</p>
<div itemscope="itemscope" itemtype="https://schema.org/ImageObject"><p><a href="https://d3ik27cqx8s5ub.cloudfront.net/blog/media/uploads/Figure2-8.png" data-wpel-link="external" target="_blank" rel="external noopener noreferrer"><img src="https://d3ik27cqx8s5ub.cloudfront.net/blog/media/uploads/Figure2-8.png" alt="" title="Figure2" itemprop="thumbnailUrl"></a></p></div>
<p><em>Figure 2: Reverse Shell Commands</em></p>
<h3>SonicWall Protections</h3>
<p>To ensure SonicWall customers are prepared for any exploitation that may occur due to this vulnerability, the following signatures have been released:</p>
<ul>
<li>IPS: 20322 MediaTek MT7915 wlan Service OOB Write 1</li>
<li>IPS: 20323 MediaTek MT7915 wlan Service OOB Write 2</li>
</ul>
<h3>Remediation Recommendations</h3>
<p>Due to the availability of the exploit code, it is highly recommended that users upgrade to the latest version of the firmware for their respective chipset.</p>
<h3>Relevant Links</h3>
<ul>
<li><a href="https://corp.mediatek.com/product-security-bulletin/March-2024" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">https://corp.mediatek.com/product-security-bulletin/March-2024</a></li>
<li><a href="https://blog.coffinsec.com/0day/2024/08/30/exploiting-CVE-2024-20017-four-different-ways.html" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">https://blog.coffinsec.com/0day/2024/08/30/exploiting-CVE-2024-20017-four-different-ways.html</a></li>
<li><a href="https://securityonline.info/cve-2024-20017-cvss-9-8-zero-click-exploit-discovered-in-popular-wi-fi-chipsets-poc-published/" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">https://securityonline.info/cve-2024-20017-cvss-9-8-zero-click-exploit-discovered-in-popular-wi-fi-chipsets-poc-published/</a></li>
<li><a href="https://github.com/mellow-hype/cve-2024-20017/tree/main" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">https://github.com/mellow-hype/cve-2024-20017/tree/main</a></li>
</ul>

<div id="sexy-author-bio"><p><a href="https://blog.sonicwall.com/en-us/author/securitynews/" target="_top" data-wpel-link="internal"><img src="https://d3ik27cqx8s5ub.cloudfront.net/blog/media/uploads/ultimatemember/3368/profile_photo-190x190.png?1726884306" width="100" height="100" alt="Security News" data-default="https://blogtest.sonicwall.com/wp-content/uploads/2018/04/user_default-avatar.png" onerror="if ( ! this.getAttribute('data-load-error') ){ this.setAttribute('data-load-error', '1');this.setAttribute('src', this.getAttribute('data-default'));}"></a></p><p>The SonicWall Capture Labs Threat Research Team gathers, analyzes and vets cross-vector threat information from the SonicWall Capture Threat network, consisting of global devices and resources, including more than 1 million security sensors in nearly 200 countries and territories. The research team identifies, analyzes, and mitigates critical vulnerabilities and malware daily through in-depth research, which drives protection for all SonicWall customers. In addition to safeguarding networks globally, the research team supports the larger threat intelligence community by releasing weekly deep technical analyses of the most critical threats to small businesses, providing critical knowledge that defenders need to protect their networks.</p></div></div></div><span>
			<span itemscope="itemscope" itemtype="https://schema.org/ImageObject" itemprop="image">
					   <span itemprop="url">https://d3ik27cqx8s5ub.cloudfront.net/blog/media/uploads/sec-news-header-3.png</span>
					   <span itemprop="height">500</span>
					   <span itemprop="width">1200</span>
				  </span><span itemprop="publisher" itemtype="https://schema.org/Organization" itemscope="itemscope">
				<span itemprop="name">Security News</span>
				<span itemprop="logo" itemscope="" itemtype="http://schema.org/ImageObject">
				   <span itemprop="url">https://blog.sonicwall.com/wp-content/uploads/images/logo/SonicWall_Registered-Small.png</span>
				 </span>
			  </span><span itemprop="author" itemscope="itemscope" itemtype="https://schema.org/Person"><span itemprop="name">Security News</span></span><span itemprop="datePublished" datetime="2024-09-19T05:57:13-06:00">2024-09-19 05:57:13</span><span itemprop="dateModified" itemtype="https://schema.org/dateModified">2024-09-19 05:57:13</span><span itemprop="mainEntityOfPage" itemtype="https://schema.org/mainEntityOfPage"><span itemprop="name">Critical Exploit in MediaTek Wi-Fi Chipsets: Zero-Click Vulnerability (CVE-2024-20017) Threatens Routers and Smartphones</span></span></span></article>
				<!--end content-->
				</main>

				

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Qualcomm Wants to Buy Intel (165 pts)]]></title>
            <link>https://www.theverge.com/2024/9/20/24249949/intel-qualcomm-rumor-takeover-acquisition-arm-x86</link>
            <guid>41605449</guid>
            <pubDate>Fri, 20 Sep 2024 20:57:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/9/20/24249949/intel-qualcomm-rumor-takeover-acquisition-arm-x86">https://www.theverge.com/2024/9/20/24249949/intel-qualcomm-rumor-takeover-acquisition-arm-x86</a>, See on <a href="https://news.ycombinator.com/item?id=41605449">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>On Friday afternoon, <a href="https://www.wsj.com/business/deals/qualcomm-approached-intel-about-a-takeover-in-recent-days-fa114f9d?st=i5zay9xbq899g24&amp;reflink=article_copyURL_share"><em>The Wall Street Journal</em> reported</a> Intel had been approached by fellow chip giant Qualcomm about a possible takeover. While any deal is described as “far from certain,” according to the paper’s unnamed sources, it would represent a tremendous fall for a company that had been the most valuable chip company in the world, based largely on its x86 processor technology that for years had triumphed over Qualcomm’s Arm chips outside of the phone space. </p><p>It would also be a massive coup for Qualcomm, which reentered the desktop processor market this year <a href="https://www.theverge.com/2024/6/26/24186432/microsoft-windows-on-arm-qualcomm-copilot-plus-pcs-prism-emulator">as a part of Microsoft’s AI PC strategy</a> after years of dominance in mobile processors.</p><p>Intel, meanwhile, is arguably in its weakest position in years — while many of its businesses are still profitable, the company announced substantial cuts, shifts in strategy, <a href="https://www.theverge.com/2024/8/1/24210656/intel-is-laying-off-over-10000-employees-and-will-cut-10-billion-in-costs">and a 15-plus percent downsizing of its workforce this August</a> after reporting a $1.6 billion loss. </p><p>At the time, Intel CEO Pat Gelsinger said the company would stop all nonessential work and has since announced <a href="https://www.theverge.com/2024/9/16/24246599/intel-foundry-independent-spinoff">it will spin off its chipmaking business</a>, a part of the company that it had long touted as a strength over rival AMD and the many fabless chipmakers that rely on entities like Taiwan’s TSMC to produce all of their actual silicon. </p><p>Intel, too, recently had to partially rely on TSMC to produce its most cutting-edge chips as it continues to rebuild its own manufacturing efforts (<a href="https://www.theverge.com/2024/4/2/24119454/intels-chipmaking-business-7-billion-loss">the costs of which</a> are responsible for most of Intel’s recent losses). And its own 18A manufacturing process <a href="https://www.theverge.com/2024/9/4/24235682/intel-18a-chipmaking-process-broadcom-test">reportedly ran into some recent trouble</a>.</p><p>While Intel’s chief rival, AMD, also had hard times over the years and had to claw its way back, gamers helped AMD every step of the way. Aside from the Nintendo Switch, whose processors are made by Nvidia, every major game console for the last decade has featured an AMD chip — and Intel <a href="https://www.theverge.com/2024/9/16/24246234/playstation-6-rumor-sony-intel-amd">reportedly lost out on a chance to change that with the future PlayStation 6</a>. </p><div><p>Intel also recently lost some faith with PC gamers after <a href="https://www.theverge.com/24216305/intel-13th-14th-gen-raptor-lake-cpu-crash-news-updates-patches-fixes-motherboards">two generations of its flagship chips were found vulnerable to strange crashes</a>, though Intel has since agreed to extend the warranties by multiple years and issued updates that could prevent damage.</p></div><p>Many of Intel’s woes are about silicon leadership, not just manufacturing or profits — the company isn’t a big player in AI server chips yet <a href="https://www.theverge.com/2024/2/1/24058186/ai-chips-meta-microsoft-google-nvidia">as Nvidia dominates</a>, nor even necessarily <a href="https://www.theverge.com/2024/7/30/24209938/amd-q2-2024-earnings-datacenter-ai-revenue">a notable small one like AMD</a>. Even its attempts to produce its own GPUs for gamers and creators <a href="https://www.theverge.com/2023/3/21/23650611/intel-raja-koduri-gpus-amd-nvidia-apple-leave-ai-startup">have yet to impress</a>.</p><p>And while Qualcomm, AMD, and Apple are all still smaller players in laptops, Intel has now <a href="https://www.theverge.com/2024/6/3/24169115/intel-lunar-lake-architecture-platform-feature-reveal">twice overhauled how it makes flagship laptop chips</a> to combat the growing threat of their seeming battery life and integrated graphics advantages. We’re waiting to see if its <a href="https://www.theverge.com/2024/9/3/24233957/intel-lunar-lake-core-ultra-200v-launch">new Lunar Lake chips</a> succeed in October and beyond.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Discord Reduced WebSocket Traffic by 40% (154 pts)]]></title>
            <link>https://discord.com/blog/how-discord-reduced-websocket-traffic-by-40-percent</link>
            <guid>41604267</guid>
            <pubDate>Fri, 20 Sep 2024 18:09:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://discord.com/blog/how-discord-reduced-websocket-traffic-by-40-percent">https://discord.com/blog/how-discord-reduced-websocket-traffic-by-40-percent</a>, See on <a href="https://news.ycombinator.com/item?id=41604267">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="heading-1"><p>At Discord, we’re always thinking about ways to improve our services and increase performance. After all, the faster our app gets, the sooner you can return to your friends and conversations!&nbsp;&nbsp;&nbsp;</p><p>Over the last six months, we embarked on a quest to support this endeavor, working to reduce the amount of bandwidth our clients use, especially on iOS and Android, hoping that decreasing bandwidth usage would lead to a more responsive experience.</p><h2>Background</h2><p>When your client connects to Discord, it receives real-time updates about what’s happening through a service that we call the “gateway.” Since late 2017, the client’s gateway connection has been compressed using zlib, making messages anywhere from 2 to 10 times smaller.</p><p>Since then, <a href="https://facebook.github.io/zstd/">zstandard</a> (originally released in 2015) has gained enough traction to become a viable replacement for zlib. Zstandard offers higher compression ratios and shorter compression times and supports <a href="https://github.com/facebook/zstd#dictionary-compression-how-to">dictionaries</a>: a way to preemptively exchange information about compressed content, further increasing compression ratios and reducing the overall bandwidth usage.</p><p>We attempted to use zstandard in the past, but, at the time, the benefits weren’t worth the costs. Our testing in 2019 was desktop-only and used too much RAM. However, a lot can happen in five years! We wanted to give it another try, and the support for dictionaries appealed to us, especially as most of our gateway payloads are small and in a well-defined shape.</p><p>We believed the predictability of these payloads would be a perfect application of dictionaries to further reduce bandwidth usage.</p></div><div id="heading-2"><p>Armed with this knowledge, we put on our lab coats, slapped on our goggles, and started experimenting. On paper, we thought zstandard would be better than zlib but we wanted to validate this theory against our current workload.</p><p>We opted to do a “dark launch” of plain zstandard: the plan was to compress a small percentage of production traffic both with zlib and zstandard, collect a bunch of metrics, then discard the zstandard data. This allowed us to experiment with zstandard to quickly compare its results against zlib. Without this experiment, we would have to add zstandard support for our clients — desktop, iOS, and Android — which would require about a month’s lead time before we could fully determine the effects of zstandard. We didn’t know how well zstandard would perform and didn’t want to wait a whole month, but a dark launch allowed us to iterate over days as opposed to weeks.</p><p>Once we got our experiment set up and deployed onto our gateway cluster, we set up a dashboard to see how zstandard performed. We flipped the switch to start sending a teeeeeny bit of traffic through the dark launch code, and the initial results appeared to be… underwhelming. Zstandard was performing <em>worse</em> than zlib was.</p><figure><p><img src="https://cdn.prod.website-files.com/5f9072399b2640f14d6a2bf4/66eb5caa61ad96d1a395f51d_AD_4nXeCYg9XJmJM9EPtYhMpoXSrE89c5FHDpTPo-_g1CsEEAhiib-D8EUGRZlxPL9CDWw0hmSeRVdGLCJPqkwdIpt5oi80KsHQL5bxIJfOJ3dC76sFgSpj8Lh4f6lqEfsZXpdpywTPUdoEFsRo6WIPHiLyQMmt9.png" loading="lazy" alt="A compression ratio chart for Zstandard. The chart reads: “User Guild Settings Update: 12.26. Guild Emojis Update: 7.35. Ready Supplemental: 7.04. Thread Member List Update: 6.88.”"></p><figcaption>Zstandard compression ratio</figcaption></figure><figure><p><img src="https://cdn.prod.website-files.com/5f9072399b2640f14d6a2bf4/66eb5d5aa15a87d53673cd47_AD_4nXdmCN0zlF9hxaVHFEwrEkJO5kEqsRq92ig73v31W-8gg9_yRUAryJqJC3jWGCzy3vJ_F1OvvUj9AQXwaYORhAmLoij9PWsswGH9-zagbN2AxVeG7Xs6fC0LEnZpCWZ5VjiTudVionBiCAhSz21suEQaNn0.png" loading="lazy" alt="A compression ratio chart for Zlib. The chart reads: “User Guild Settings Update: 13.95. Guild Member List Update: 9.86. Auto Moderation Action: 9.56. Voice Channel Effect Send: 9.27.” "></p><figcaption>Zlib compression ratio</figcaption></figure><p>To compare the performance of these two compression algorithms, we used their “compression ratio.” The compression ratio is measured by taking the uncompressed size of the payload and dividing it by the compressed size — a larger number is better.</p><p>Looking at the images above, which measure the compression ratio for the various dispatch types (<a href="https://discord.com/developers/docs/topics/opcodes-and-status-codes#gateway-gateway-opcodes">op 0</a>), with zlib, <strong>user_guild_settings_update</strong> has a compression ratio of 13.95 while with zstandard it has a compression ratio of 12.26.&nbsp;</p><p>The graph below further illustrates that zstandard performed worse than zlib: the average size of a <strong>MESSAGE_CREATE</strong> payload compressed with zlib was around 250 bytes, while with zstandard, the same payload was over 750!</p><figure><p><img src="https://cdn.prod.website-files.com/5f9072399b2640f14d6a2bf4/66eb5d81a15a87d53673ff18_AD_4nXc_N1o4Qmj70b_Oh8Oh9xlMRZNw2hTyEkkFy8_hzYeDBJ-S99ejgt9MPkNp6kgVPoT_YX-NV-hMKMNFgSlOr1nerXGqpN_HCc3JvKoH86TMaM4II92Mw_NhbTVkCMy5g85ka153WCZ7-QHmsrEJ7j3mNqD8.png" loading="lazy" alt="A bar chart comparing compression methods to payload sizes. Zlib lands at just over 250 bytes, while zstandard reaches over 750 bytes."></p></figure><p>The same trend was observed for most other dispatches: zstandard was not outperforming zlib like we thought it would. What’s going on here?</p></div><div id="heading-3"><h2>Streaming Zstandard<br></h2><p>It turns out that one of the key differences between our zlib and zstandard implementations was that zlib was using streaming compression, while zstandard wasn’t.&nbsp;</p><p>As mentioned previously, most of our payloads are comparatively very small, only a few hundred bytes at most, which doesn’t give zstandard much historical context to work with to further optimize how it compresses future payloads. With streaming compression, the zlib stream is spun up when the connection is opened and exists until the websocket is closed. Instead of having to start fresh for every websocket message, zlib can draw on its knowledge of previously compressed data to inform its decisions on how to process fresh data. This ultimately leads to smaller payload sizes.</p><p>The question then became: “could we get zstandard to do this?” The answer to that was… “sorta.” Our gateway service is written in elixir, and while zstandard supports <a href="https://facebook.github.io/zstd/zstd_manual.html#Chapter7">streaming compression</a>, the various zstandard bindings for elixir/erlang we looked at didn’t.&nbsp;</p><p>We ultimately settled on using <a href="https://github.com/silviucpp/ezstd">ezstd</a> as it had dictionary support (more on that later).While it didn’t support streaming at the time,in the spirit of open source we forked ezstd to add support for streaming, which we later <a href="https://github.com/silviucpp/ezstd/pull/15">contributed back upstream</a>.</p><p>We then repeated the dark launch experiment, but with zstandard streaming and got the following results:</p><figure><p><img src="https://cdn.prod.website-files.com/5f9072399b2640f14d6a2bf4/66eb68683aeb8f2ee1063854_AD_4nXfRJh2JKRqLoQC5EiZuN5E8wmWEr5yAZGx07y5u_eTvqMYlzOVTTmdggzgp8-hJNWSlPklg7zjMyccyIaZkWF45AGSjvvmlhk-kzDIgaG004-6R_Ow_WnEuIbZHLvXes5AzunAPS9Jo0iA8MrZyD_vVHZ4.png" loading="lazy" alt=""></p><figcaption><strong>Message Create Compression Ratios</strong></figcaption></figure><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/5f9072399b2640f14d6a2bf4/66eb5dc8045b706867911f67_AD_4nXc5poqnDTStFq1yAQdxKYlNM76Ikyol_-1Erb2E7IQK-MozKtL7u3BFjtSlkP9-RTZNzQFZFoP1Uxpq7PB4oNDRoc3mv0cxVvk4roDqtroLtGDfstiAsyVy_i6W4TSLCHH6JMu8QvJWoYRc7Y5UrDOMQo0.png" loading="lazy" alt="A bar chart comparing payload sizes of  Zlib streaming, zstandard, and zstandard streaming. Zlib streaming payloads are over 250 bytes, while zstandard hits over 750 bytes. Zstandard streaming payloads land at under 250 bytes."></p></figure><p>As the above data illustrates, zstandard streaming increased the compression ratio from 6 to almost 10 and dropped the payload size from 270 bytes to 166.</p><p>This trend held true for most of the other dispatches: zstandard streaming significantly outperforms zlib both in time to compress and compression ratio.</p><figure><p><img src="https://cdn.prod.website-files.com/5f9072399b2640f14d6a2bf4/66eb5dfe1481af30f65a3830_AD_4nXdEg8Tp2r5wYGbJoYw-s0ezvLu8cc6JMntlFtBC1k8MmK4Nh1cQ29yItE62RM1VSVCa9N663_OGdmYBAwmJfVxMlEvX0Vjr0t8H6Kucg3LgSrjaUvyRo_Wyw3vebN1ui4-wIOSVRGte9IAKysiZxdiLFwrI.png" loading="lazy" alt="A chart comparing MESSAGE_CREATE times between zlib streaming and zstandard streaming. Zlib streaming takes just over 100 microseconds, while zstandard streaming lands at under 50 microseconds."></p></figure><p>Looking once again at <strong>MESSAGE_CREATE</strong>, the compression time per byte of data is significantly lower for zstandard streaming than zlib, with zlib taking around 100 microseconds per byte and zstandard taking 45 microseconds.</p></div><div id="heading-4"><h2>Pushing Further</h2><p>While our initial experimentation proved that zstandard streaming outperformed zlib streaming, the remaining question we had was: “How far can we push this?” Our initial experiments used the default settings for zstandard and we wanted to know how high we could push our compression ratio by playing around with the compression settings.</p><p>So how far did we get?</p><h3>Tuning</h3><p>Zstandard is highly configurable and enables us to tweak various compression parameters. We focused our efforts on three parameters that we thought would have the biggest impact on compression: chainlog, hashlog, and windowlog. These parameters offer trade-offs between compression speed, memory usage, and compression ratio. For example, increasing the value of the chainlog generally improves the compression ratio, but at the cost of increasing memory usage and compression time.</p><p>We also wanted to ensure that with the settings we decided on, the compression contexts would still fit in memory on our hosts. While it’s simple to add more hosts to soak up the extra memory usage, extra hosts cost money and at some point, provide diminishing returns on the gains.</p><p>We settled on an overall compression level of 6, a chainlog and hashlog of 16, and a windowlog of 18. These numbers are slightly above <a href="https://github.com/facebook/zstd/blob/a761013b0390892e8728fc45171f831cf23c3792/lib/compress/clevels.h#L25">the default settings that you can see here</a> and would comfortably fit in the memory of a gateway node.</p><h3>Zstandard Dictionaries</h3><p>Additionally, we wanted to investigate if we could take advantage of zstandard’s dictionary support to compress data even further. By pre-seeding zstandard with some information, it can more efficiently compress the first few kilobytes of data.&nbsp;</p><p>However, doing this adds additional complexity as both the compressor (in this case, a gateway node) and the decompressor (a Discord client) need to have the same copy of the dictionary to communicate with each other successfully.</p><p>To generate a dictionary to use, we needed data… <em>and a lot of it</em>. Zstandard has a built-in way to generate dictionaries (<em>zstd --train</em>) from a sample of data, so we just had to collect a whooole buncha samples.&nbsp;</p><p>Notably, the gateway supports two encoding methods for payloads: JSON and <a href="https://www.erlang.org/doc/apps/erts/erl_ext_dist.html">ETF</a>, and a JSON dictionary wouldn’t perform as well on ETF (and vice versa) so we had to generate two dictionaries: one for each encoding method.</p><p>Since dictionaries contain portions of the training data and we’d have to ship the dictionaries to our clients, we needed to ensure that the samples we would generate the dictionaries from were free of any personally-identifiable user data. We collected data involving 120,000 messages, split them by ETF and JSON encoding, anonymized them, and then generated our dictionaries.</p><p>Once our dictionaries were built, we could use our gathered data to quickly evaluate and iterate on its efficacy without needing to deploy our gateway cluster.</p><p>The first payload we tried compressing was “READY.” As one of the first (and largest) payloads sent to the user, READY contains most of the information about the connecting user, such as guild membership, settings, and read states (What channels should be marked as read/unread). We compressed a single READY payload of 2,517,725 bytes down to 306,745 using the default zstandard settings which established a baseline. Utilizing the dictionary we just trained, the same payload was compressed down to 306,098 bytes – a gain of around 600 bytes.</p><p>Initially, these results seemed discouraging, but we next tried compressing a smaller payload, called TYPING_START, sent to the client so it can show the “XXX is typing…” notification. In this situation, a 636 byte payload compresses down to 466 bytes without the dictionary and 187 bytes with the dictionary. We saw much better results with our dictionaries against smaller payloads simply due to how zstandard operates.&nbsp;</p><p>Most compression algorithms “learn” from data that has already been compressed, but with small payloads, there isn’t any data for it to learn from. By preemptively informing zstandard what the payload is going to look like, it can make a more informed decision on how to compress the first few kilobytes of data before its buffers have been fully populated.</p><p>Satisfied with these findings, we deployed dictionary support to our gateway cluster and started experimenting with it. Utilizing the dark launch framework, we compared zstandard to zstandard with dictionaries.</p><p>Our production testing yielded the following results:</p><figure><p><img src="https://cdn.prod.website-files.com/5f9072399b2640f14d6a2bf4/66eb5f2170b2edf1b422de56_AD_4nXfFOonNWIBnci1c-cNsmJKSC5uTEVj23McFJu-0dXGzFsoaZIyLciEpiqwJC_KSr2QP5U0nEEsba_AcriUbIcATZqRqYz7GN3qjn_UcU9QiR2TDIH_FiaANBNDnWgu3HlwgX83yHxNf3eE-Dzz1DMfWA3sd.png" loading="lazy" alt="A small chart comparing payload compression ratios of zstandard, with and without dictionaries. With dictionaries, the payload compression ratio is 6.63. Without dictionaries, the compression ratio is 6.4."></p><figcaption><strong>Ready Payload Size</strong></figcaption></figure><p>We specifically looked at the READY payload size as it’s one of the first messages sent over the websocket and would be most likely to benefit from a dictionary. As shown in the table above, the compression gains were minimal for READY, so we looked at the results for more dispatch types hoping dictionaries would give more of an edge for smaller payloads.&nbsp;</p><p>Unfortunately, the results were a bit mixed. For example, looking at the message create payload size that we’ve been comparing throughout this post, we can see that the dictionary actually made things worse.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/5f9072399b2640f14d6a2bf4/66eb5f49c09e80a03e4614c5_AD_4nXeZGwfQPGKphBAcy3Wpc2HH20FpO3wZ2Os9w7VC6adtmZTDnLH0B8MEAUPO1UoQhj9RlEdQL9czlCKW15PVC1El6hkXy52Rcx8TNzNsPJAWKvN7o7wYjwyFxj5aMyob27AOTmBjN20ZdcusTgc7FNJxt96s.png" loading="lazy" alt="A chart comparing MESSAGE_CREATE payload sizes between Zlib streaming, Zstandard, Zstandard streaming, and Zstandard Streaming Plus Dictionary. Zlib streaming’s payload is just over 250 byes and zstandard is over 750 bytes. The newest entries, Zstandard Streaming and Zstandard Streaming Plus Dictionary are both under 250 byes, but Zstandard Streaming without Dictionary has a slightly smaller file size."></p></figure><p>Ultimately, we decided not to continue with our dictionary experiments. The slightly improved compression dictionaries would provide was outweighed by the additional complexity they would add to our gateway service and clients. Data is a big driver of engineering at Discord, and the data speaks for itself: it wasn’t worth investing more effort into.</p><h3>Buffer Upgrading</h3><p>Finally, we explored increasing zstandard buffers during off-peak hours. Discord’s traffic follows a diurnal pattern, and the memory we need to handle peak demand is significantly more than what’s needed during the rest of the day.&nbsp;</p><p>On the surface, autoscaling our gateway cluster would prevent us from wasting compute resources during off-peak hours. However, due to the nature of gateway connections being long-lived, traditional autoscaling methods don’t work well for our workload. As such, we have a lot of extra memory and compute during off-peak hours. Having all this extra compute laying around raised the question: Could we take advantage of these resources to offer greater compression?</p><p>To figure this out, we built a feedback loop into the gateway cluster. This loop would run on each gateway node and monitor the memory usage by the clients connected to it. It would then determine a percentage of new connecting clients that should have their zstandard buffer upgraded. An upgraded buffer increases the windowlog, hashlog, and chainlog values by one, and since these parameters are expressed as a power of two, increasing these values by one will roughly double the amount of memory usage the buffer uses.</p><p>After deploying and letting the feedback loop run for a bit, the results weren’t as good as we had initially hoped. As illustrated by the graph below, over a 24 hour period, our gateway nodes had a relatively low upgrade ratio (Up to 30%), and was significantly less than we anticipated: around 70%.</p><figure><p><img src="https://cdn.prod.website-files.com/5f9072399b2640f14d6a2bf4/66eb5f89a5aa109682aa20c0_AD_4nXdu2egSUa4dMnBCuvHFxPFrrxdROCyeqCAs8AP95O-jHEYcKx5jUXDsiuw7bCAmXiDM9g9ewrch5e4i7TQMQi7CpaYvBVsJr_bKMTbU9KiDMOjsMC9V1wDnEK6YH-Ksox9c8oZf5e-Jdh1AtumuemXMKtI.png" loading="lazy" alt="A graph showing the upgrade ratio for each gateway host over a 24 hour period. From 00:00 to 03:00, the lines are distributed between 0 and 0.2 peaking at 00:00 and tapering off. From 03:00 to 18:00 there are no lines. From 18:00 to 23:59 the lines are distributed between 0 and 0.2 peaking around 22:00"></p></figure><p>After doing a bit of digging, we discovered that one of the primary issues that was causing the feedback loop to behave sub-optimally was memory fragmentation: the feedback loop looked at real system memory usage, but BEAM was allocating significantly more memory from the system than was needed to handle the connected clients. This caused the feedback loop to think that it had less memory to work with than was available.</p><p>To try and mitigate this, we did a little experimentation to tweak the BEAM allocator settings — more specifically, the <em>driver_alloc</em> allocator, which is responsible for (<em>shockingly</em>) driver data allocations. The bulk of the memory used by a gateway process is the zstandard streaming context, which is implemented in C using a <a href="https://www.erlang.org/doc/system/nif.html">NIF</a>. NIF memory usage is allocated by <em>driver_alloc. </em>Our hypothesis was that if we could tweak the <em>driver_alloc</em> allocator to more effectively allocate or free memory for our zstandard contexts, we’d be able to decrease fragmentation and increase upgrade ratio overall.</p><p>However, after messing around with the allocator settings for a little bit, we decided to revert the feedback loop. While we probably would have eventually found the right allocator settings to dial in, the amount of effort needed to tweak the allocators combined with the overall additional complexity that this introduced into the gateway cluster outweighed any gains that we would’ve seen if this was successful.</p><h3>Implementation and Rollout</h3><p>While the original plan was to only consider zstandard for mobile users, the bandwidth improvements were significant enough for us to ship to desktop users as well! Since zstandard ships as a C library, it was simply a matter of finding bindings in the target language —Java for Android, Objective C for iOS, and Rust for Desktop — and hooking them into each client. Implementation was straightforward for Java (<a href="https://github.com/luben/zstd-jni/">zstd-jni</a>) and Desktop (<a href="https://crates.io/crates/zstd-safe">zstd-safe</a>), as bindings already existed, however for iOS, we had to write our own bindings.</p><p>This was a risky change with the potential to render Discord completely unusable if things were to go wrong, so the rollout was gated behind an experiment. This experiment served three purposes: allow the quick rollback of these changes if things were to go wrong, validate the results we saw in the “lab,” and enable us to gauge if this change was negatively affecting any baseline metrics.</p><p>Over the course of a few months, we were able to successfully roll out zstandard to all of our users on all platforms.</p></div><div id="heading-5"><h2>Another Win: Passive Sessions V2<br></h2><p>While this next part isn’t directly related to the zstandard work, the metrics that guided us during the dark launch phase of this project revealed a surprising behavior. Looking at the actual size of dispatches that were sent to the client, passive_update_v1 stood out. This dispatch consisted of over 30% of our gateway traffic while the actual number of dispatches sent were comparatively small–around 2%.</p><figure><p><img src="https://cdn.prod.website-files.com/5f9072399b2640f14d6a2bf4/66eb5fe0f048cb0591c16080_AD_4nXf-UwN9XBhbGiKeqVukeCH4gYs4jn2JZM7jEY_vzWGncfyKL9QJiQJgnm4bXQvPpNXzD4cCObgJjyH3KoPFwDnYYfPQEQbvvZAqiSYa9qkH7ykVYJCw7IFYZHtKKDkABq07Px_J2gMmX6ejg6ucUS_IWxFU.png" loading="lazy" alt="A pie chart with multiple sections. Of over a dozen sections, the ones that are pointed out are: Passive Update V1: 35.61%, Message Create: 20.04%, Ready: 11.66%, Guild Member List Update: 8.5%, Presence Update: 2.67%."></p></figure><p>We employ passive sessions to avoid sending most messages that a server generates to clients that may not even open the server. For example, a Discord server could be very active sending thousands of messages per minute, but if a user isn’t actually reading those messages, it doesn’t make sense to send them and waste their bandwidth. Once you tab into the server, a passive session will be “upgraded” into a normal session and receive the full firehose of dispatches from that guild.</p><p>However, passive sessions still need to be periodically sent a limited amount of information, which is the purpose of PASSIVE_UPDATE_V1. Periodically, all passive sessions will receive an update with a list of channels, members, and members in voice so your client can still be kept in sync with the server.</p><p>Diving into the actual contents of one of those PASSIVE_UPDATE_V1 dispatches, we would send <em>all</em> of the channels, members, or members in voice, even if only a single element changed. Passive sessions were implemented as a means to scale Discord servers to hundreds of thousands of users and it worked well at the time. </p><p>However as we’ve <a href="https://discord.com/blog/maxjourney-pushing-discords-limits-with-a-million-plus-online-users-in-a-single-server">continued to scale</a>, sending these snapshots which consist of mostly redundant data was no longer sufficient. To fix this, we introduced a new dispatch that only sends the delta of what’s changed since the last update. This dispatch, aptly named PASSIVE_UPDATE_V2, significantly reduced the overall bandwidth from 35% of the gateway’s bandwidth to 5%, equating to a 20% reduction cluster-wide.</p><figure><p><img src="https://cdn.prod.website-files.com/5f9072399b2640f14d6a2bf4/66eb602c922d2dd61dbb8ad2_AD_4nXf8mgenY8u6qMaex3kvnEvd2xZty8fNW7sc139x4hEnnF8kQpJyKCzM3dpl5XULIRjgrBqljIb1ti3FBua6dxP3_4C2YKS8bEwYGsI6ofgRZ0xAALjFDgh-tXRxgL_JoVOzQ6StLLkCzYdtmW6UJHImxEwa.png" loading="lazy" alt="An additional pie chart with multiple sections. The sections that are pointed out are: Message Create: 28.31%, Reader: 21.86%, Guild Member List Update: 10.17%, Passive Update V2: 4.73%/4.7%. Voice State Update: 3.68%, and Guild Update: 1.01%. "></p></figure><p>‍</p><p>‍</p></div><div id="heading-6"><h2>B I G Savings</h2><p>Through the combined effects of Passive Sessions v2 and zstandard, we were able to reduce the gateway bandwidth used by our clients by almost 40%. That’s a LOT of data!&nbsp;</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/5f9072399b2640f14d6a2bf4/66eb605a048df7ca5e92d7cf_AD_4nXesiM1r_vEP3jlBiL4A0FryF6xUh8bKLxem8z-HObi-aFHRbQ2TlhJ-Rys4HScOw8_RsQhkx-w3uLE39WVR-_JXfHcoPVHFv4gmdFMwUWpT9Evhn2bPHqlQjh_r2bSRM1xez4cYHjMHw96IlH8MhDZ7VKzl.png" loading="lazy" alt="A graph of the outgoing bandwidth on the gateway cluster showing dates from January 2024, to April 2024. The line on the chart moves up and down diurnally up until April 18, which is demarcated with a red line labeled “zstandard”. The line then trends downwards until around May 30, 2024 also demarcated with a red line labeled “Passive Sessions v2.” The line levels oout after this, significantly lower than where it started."></p></figure><p>The chart shows the relative outgoing bandwidth of the gateway cluster from January 1 2024 to August 12, 2024, with the two demarcations being the zstandard rollout in April, followed by passive sessions v2 in late May.</p><p>While the passive sessions optimization was an unintended side-effect of the zstandard experimentation, it shows that with the right instrumentation and by looking at graphs with a critical eye, big savings can be achieved with a reasonable amount of effort.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Docker Desktop Alternative (375 pts)]]></title>
            <link>https://container-desktop.com/</link>
            <guid>41604262</guid>
            <pubDate>Fri, 20 Sep 2024 18:08:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://container-desktop.com/">https://container-desktop.com/</a>, See on <a href="https://news.ycombinator.com/item?id=41604262">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <div>
      <div>
        <h2>Cross-platform desktop integrated application with consistent UI</h2>
        <p>Container Desktop works on Windows, Mac and Linux providing the same graphical interface.</p>
      </div>
      <div>
        <h2>Learning tool for the powerful <span>podman</span> command line interface</h2>
        <p>Container Desktop is great for skills improvement and learning features of 'podman'.</p>
      </div>
    </div>
    <div>
      <h2>Essentials at your fingertips</h2>
      <p>
        The dashboard offers just the essential features so that the users can feel right at home.
        <br>
        See below what you can manage with Container Desktop.
      </p>
      <video controls="" poster="https://container-desktop.com/videos/demo.png" width="100%" autoplay="">
        <source src="https://container-desktop.com/videos/demo.mp4" type="video/mp4">
      </video>
    </div>

    <div>
      <p><img src="https://container-desktop.com/img/000-CrossPlatform.png">
      </p>
      <div>
        <h2>Cross Platform</h2>
        <ul>
          <li>Application looks the same everywhere, no mental mapping!</li>
          <li>Completely informs its users of where it stores logs and settings.</li>
          <li>Allows users to debug and understand what is happening behind the scenes.</li>
        </ul>
      </div>
    </div>

    <div>
      <div>
        <h2>Multiple engines</h2>
        <ul>
          <li>Fast native on <strong>Linux</strong> only</li>
          <li>Virtualized for any OS</li>
          <li>LIMA for MacOS</li>
          <li>WSL for Windows</li>
          <li>Both <strong>podman</strong> and <strong>docker</strong> engines</li>
          <li>Others are planned</li>
        </ul>
      </div>
      <p><img src="https://container-desktop.com/img/ConnectionManager.png">
      </p>
    </div>

    <div>
      <p><img src="https://container-desktop.com/img/PodmanContainers.png">
      </p>
      <div>
        <h2>Containers</h2>
        <ul>
          <li>Be informed about the origin and status of your container environment.</li>
          <li>Quickly access logs, environment variables, mounts, opened ports and monitoring stats.</li>
          <li>Perform common maintenance operations, stop, restart and remove easily.</li>
          <li>Direct access to the exposed services using your browser.</li>
          <li>Gain control of all that happens in the container using the terminal console.</li>
        </ul>
      </div>
    </div>

    <div>
      <div>
        <h2>Images</h2>
        <ul>
          <li>Be informed about the origin and status of local image store, their registry, name and tag.</li>
          <li>Immediately spawn new containers from image, customize name, port mappings and available mounts.</li>
          <li>Quickly access image build-up, check their impact and debug their setup.</li>
          <li>In-depth configuration viewer.</li>
          <li>
            Perform common maintenance operations, pull latest updates to refresh the images, push latest changes to a
            distributed image project.
          </li>
        </ul>
      </div>
      <p><img src="https://container-desktop.com/img/006-ImageActions.png">
      </p>
    </div>

    <div>
      <p><img src="https://container-desktop.com/img/ImageSecurity.png">
      </p>
      <div>
        <h2>Security</h2>
        <ul>
          <li>In-depth awareness of security checks</li>
          <li>Be informed of know vulnerabilities</li>
          <li>Helps creating and maintaining secure systems</li>
        </ul>
      </div>
    </div>

    <div>
      <div>
        <h2>Networks</h2>
        <ul>
          <li>Create and reuse networks at any moment.</li>
          <li>Know detailed setup of each network</li>
        </ul>
      </div>
      <p><img src="https://container-desktop.com/img/NetworkCreate.png">
      </p>
    </div>

    <div>
      <p><img src="https://container-desktop.com/img/PodmanPods.png">
      </p>
      <div>
        <h2>Pods</h2>
        <ul>
          <li>Full power of pods on supported engines</li>
          <li>Access logs, processes and details.</li>
          <li>Generate kube and perform common actions</li>
        </ul>
      </div>
    </div>

    <div>
      <div>
        <h2>Machines</h2>
        <ul>
          <li>Manage all available podman virtual machines, create new ones or decommission what is redundant.</li>
        </ul>
      </div>
      <p><img src="https://container-desktop.com/img/011-MachineTerminal.png">
      </p>
    </div>

    <div>
      <p><img src="https://container-desktop.com/img/013-SecretsInspect.png">
      </p>
      <div>
        <h2>Secrets</h2>
        <p>Be aware of all available secrets, define new ones or purge old from existence.</p>
      </div>
    </div>

    <div>
      <div>
        <h2>Volumes</h2>
        <p>Manage shared volumes across containers, limit repetition and also be portable.</p>
      </div>
      <p><img src="https://container-desktop.com/img/015-VolumeActions.png">
      </p>
    </div>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Inngest 1.0 – Open-source durable workflows on every platform (153 pts)]]></title>
            <link>https://www.inngest.com/</link>
            <guid>41604042</guid>
            <pubDate>Fri, 20 Sep 2024 17:33:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.inngest.com/">https://www.inngest.com/</a>, See on <a href="https://news.ycombinator.com/item?id=41604042">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><header><p>Inngest's durable functions replace queues, state management, and scheduling to enable any developer to write reliable step functions faster without touching infrastructure.</p></header><p><img alt="Screenshot of Inngest's dashboard" loading="lazy" width="1530" height="840" decoding="async" data-nimg="1" srcset="https://www.inngest.com/_next/image?url=%2Fassets%2Fhomepage%2Fhero%2F2024-08-function-dashboard.png&amp;w=1920&amp;q=75 1x, https://www.inngest.com/_next/image?url=%2Fassets%2Fhomepage%2Fhero%2F2024-08-function-dashboard.png&amp;w=3840&amp;q=75 2x" src="https://www.inngest.com/_next/image?url=%2Fassets%2Fhomepage%2Fhero%2F2024-08-function-dashboard.png&amp;w=3840&amp;q=75"></p></div><div><div><p>Powerful SDKs</p><h2>Simple APIs for reliable software</h2><p>Drop our SDK into your existing codebase to add durable execution via step functions in seconds. No queues, workers, or additional state management required.</p></div><div><div><p>Flexible enough for all use cases, powerful enough for advanced requirements.</p><div><div><h3>Run on serverless, servers, or both.</h3><p>Deploy your Inngest functions to your existing platform or infra. Inngest securely invokes your jobs wherever the code runs.</p></div><div><h3>Multi-tenant concurrency, throttle, debounce, priority, and more.</h3><p>Control exactly how your functions are run with built-in flow control. Forget about queues, workers, and customer logic.</p></div><div><h3>Batching, fan-out, and scheduling</h3><p>Essentials for any type of job or workflow creation.</p></div></div></div><div><pre><code><span>export</span><span> </span><span>const</span><span> processVideo = inngest.createFunction(
</span><span>  { </span><span>id</span><span>: </span><span>"process-video"</span><span>,
</span><span>    </span><span>concurrency</span><span>: { </span><span>limit</span><span>: </span><span>5</span><span>, </span><span>key</span><span>: </span><span>"event.data.userId"</span><span> } },
</span><span>  { </span><span>event</span><span>: </span><span>"video/uploaded"</span><span> },
</span><span>  </span><span>async</span><span> ({ event, step }) =&gt; {
</span>
<span>    </span><span>// step.run is a code-level transaction:  it retries automatically</span><span>
</span><span>    </span><span>// on failure and only runs once on success.</span><span>
</span><span>    </span><span>const</span><span> transcript = </span><span>await</span><span> step.run(</span><span>'transcribe-video'</span><span>,
</span><span>      </span><span>async</span><span> () =&gt; deepgram.transcribe(event.data.videoUrl)
</span>    )
<!-- -->
<span>    </span><span>// function state is automatically managed for fault tolerance</span><span>
</span><span>    </span><span>// across steps.</span><span>
</span><span>    </span><span>const</span><span> summary = </span><span>await</span><span> step.run(</span><span>'summarize-transcript'</span><span>,
</span><span>      </span><span>async</span><span> () =&gt; llm.createCompletion({
</span><span>        </span><span>model</span><span>: </span><span>"gpt-4o"</span><span>,
</span><span>        </span><span>prompt</span><span>: createSummaryPrompt(transcript),
</span>      })
<!-- -->    )
<!-- -->
<span>    </span><span>// easily chain a series of calls without managing infrastructure.</span><span>
</span><span>    </span><span>await</span><span> step.run(</span><span>'write-to-db'</span><span>,
</span><span>      </span><span>async</span><span> () =&gt; db.videoSummaries.upsert({
</span><span>        </span><span>videoId</span><span>: event.data.videoId,
</span>        transcript,
<!-- -->        summary,
<!-- -->      })
<!-- -->    )
<!-- -->  }
<!-- -->);</code></pre></div></div></div><div><div><p>Write functions in any language.</p></div><div><p>Functions run on your own infrastructure: serverless, servers, or edge.</p></div></div><div><blockquote><p>I wanted to find a solution that would let us just write the code, not manage the infrastructure around queues, concurrency, retries, error handling, prioritization... I don't think that developers should be even configuring and managing queues themselves in 2024.</p></blockquote></div><div><div><p>Effortless development</p><h2>Made for modern engineering teams</h2><p>We've built Inngest to enable every developer to build reliable, scalable systems with less effort, more confidence, and fewer headaches.</p></div><div><div><div><p><img src="https://www.inngest.com/assets/homepage/usp-apis-for-reliability.svg" alt="Inngest SDK APIs"></p><div><p>Use our SDK primitives to write code that automatically retries on error, runs in parallel, sleeps for days, or waits for additional input.</p><p>It's durable execution, built for <em>every developer</em>.</p></div></div><div><h3>Write reliable, fault-tolerant code with ease</h3></div></div><div><div><p><img src="https://www.inngest.com/assets/homepage/usp-flow-control.svg" alt="Flow control graphic and methods"></p><div><p>Flow Control is essential for building resilient systems with durable execution. Configure <em>how</em> and <em>when</em> you</p><p>Throttle, multi-tenant concurrency controls, prioritization, rate limiting, batching, and more in a line of code.</p></div></div><div><h3>Flow control for resilient systems</h3></div></div><div><div><p><img src="https://www.inngest.com/assets/homepage/usp-local-dev.svg" alt="Local development starts with one command"></p><div><p>Our open source Dev Server spins up an Inngest environment on your machine in a single command, with a UI to test and debug functions faster and easier then ever before.</p><p>No more battling clunky local setups for queues and workers.</p></div></div><div><h3>Local development that developers love</h3></div></div></div></div><div><blockquote><p>The DX and visibility with Inngest is really incredible. We are able to develop functions locally easier and faster that with our previous queue. Also, Inngest's tools give us the visibility to debug issues much quicker than before.</p></blockquote></div><div><div><p>Orchestration + Flow control</p><h2>Orchestration and flow control necessary for resilient systems</h2><p>Complete control over how your functions are executed without re-inventing the wheel. Inngest's built-in tools allow you to build complex workflows and run high volume jobs with fairness across your user base.</p></div><div><div><p><img src="https://www.inngest.com/assets/platform/icon-concurrency-global.svg"></p><h3>Fair, multi-tenant concurrency</h3><p>Prevent noisy neighbor issues by limiting the concurrent resources each account or user consumes. Just a couple of lines of code.</p><p><a href="https://www.inngest.com/docs/guides/concurrency?ref=homepage-orchestration">Learn more <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor"><path d="M16.1716 10.9999L10.8076 5.63589L12.2218 4.22168L20 11.9999L12.2218 19.778L10.8076 18.3638L16.1716 12.9999H4V10.9999H16.1716Z"></path></svg></a></p></div><div><p><img src="https://www.inngest.com/assets/platform/icon-workflow.svg"></p><h3>Step orchestration &amp; workflows</h3><p>Create step functions in code with orchestration and state automatically managed. Build complex pipelines of reliable business logic in minutes.</p><p><a href="https://www.inngest.com/docs/features/inngest-functions/steps-workflows?ref=homepage-orchestration">Learn more <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor"><path d="M16.1716 10.9999L10.8076 5.63589L12.2218 4.22168L20 11.9999L12.2218 19.778L10.8076 18.3638L16.1716 12.9999H4V10.9999H16.1716Z"></path></svg></a></p></div><div><p><img src="https://www.inngest.com/assets/platform/icon-batch.svg"></p><h3>Batching</h3><p>Coalesce many requests into a single function run for high-volume, low cost execution — without code, orchestration, or infra changes.</p><p><a href="https://www.inngest.com/docs/guides/batching?ref=homepage-orchestration">Learn more <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor"><path d="M16.1716 10.9999L10.8076 5.63589L12.2218 4.22168L20 11.9999L12.2218 19.778L10.8076 18.3638L16.1716 12.9999H4V10.9999H16.1716Z"></path></svg></a></p></div><div><p><img src="https://www.inngest.com/assets/platform/icon-throttle.svg"></p><h3>Throttle and rate-limiting</h3><p>Use throttling and rate-limiting to manage throughput across your functions. Handle spikes of traffic and ensure limited resources are protected, applying limits at a global or even user specific level.</p><p><a href="https://www.inngest.com/docs/guides/throttling?ref=homepage-orchestration">Throttling<!-- --> <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor"><path d="M16.1716 10.9999L10.8076 5.63589L12.2218 4.22168L20 11.9999L12.2218 19.778L10.8076 18.3638L16.1716 12.9999H4V10.9999H16.1716Z"></path></svg></a><a href="https://www.inngest.com/docs/guides/rate-limiting?ref=homepage-orchestration">Rate limiting<!-- --> <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor"><path d="M16.1716 10.9999L10.8076 5.63589L12.2218 4.22168L20 11.9999L12.2218 19.778L10.8076 18.3638L16.1716 12.9999H4V10.9999H16.1716Z"></path></svg></a></p></div><div><p><img src="https://www.inngest.com/assets/platform/icon-priority.svg"></p><h3>Dynamic prioritization</h3><p>Push paid or high-value users to the front of the queue while still ensuring fairness and quality-of-service for other users.</p><p><a href="https://www.inngest.com/docs/guides/priority?ref=homepage-orchestration">Learn more <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor"><path d="M16.1716 10.9999L10.8076 5.63589L12.2218 4.22168L20 11.9999L12.2218 19.778L10.8076 18.3638L16.1716 12.9999H4V10.9999H16.1716Z"></path></svg></a></p></div><div><p><img src="https://www.inngest.com/assets/platform/icon-debounce.svg"></p><h3>Debouncing</h3><p>Prevent wasted work and costs by debouncing functions triggered within a time window.</p><p><a href="https://www.inngest.com/docs/guides/debounce?ref=homepage-orchestration">Learn more <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor"><path d="M16.1716 10.9999L10.8076 5.63589L12.2218 4.22168L20 11.9999L12.2218 19.778L10.8076 18.3638L16.1716 12.9999H4V10.9999H16.1716Z"></path></svg></a></p></div><div><p><img src="https://www.inngest.com/assets/platform/icon-pause.svg"></p><h3>Pause for input or events</h3><p>Write functions that "wait for" additional input or events and automatically resume when matching events are received. No state management or polling required.</p><p><a href="https://www.inngest.com/docs/features/inngest-functions/steps-workflows/wait-for-event?ref=homepage-orchestration">Learn about waitForEvent<!-- --> <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor"><path d="M16.1716 10.9999L10.8076 5.63589L12.2218 4.22168L20 11.9999L12.2218 19.778L10.8076 18.3638L16.1716 12.9999H4V10.9999H16.1716Z"></path></svg></a></p></div><div><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor"><path d="M12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22ZM12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20ZM13 12H17V14H11V7H13V12Z"></path></svg><h3>Sleep, scheduling, and cron</h3><p>Sleep directly in code — in serverless functions, servers, or on the edge — and schedule functions for the future using dates or cron expressions.</p><p><a href="https://www.inngest.com/docs/features/inngest-functions/steps-workflows/sleeps?ref=homepage-orchestration">Sleeps<!-- --> <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor"><path d="M16.1716 10.9999L10.8076 5.63589L12.2218 4.22168L20 11.9999L12.2218 19.778L10.8076 18.3638L16.1716 12.9999H4V10.9999H16.1716Z"></path></svg></a><a href="https://www.inngest.com/docs/guides/delayed-functions?ref=homepage-orchestration">Scheduling<!-- --> <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor"><path d="M16.1716 10.9999L10.8076 5.63589L12.2218 4.22168L20 11.9999L12.2218 19.778L10.8076 18.3638L16.1716 12.9999H4V10.9999H16.1716Z"></path></svg></a><a href="https://www.inngest.com/docs/guides/scheduled-functions?ref=homepage-orchestration">Crons<!-- --> <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor"><path d="M16.1716 10.9999L10.8076 5.63589L12.2218 4.22168L20 11.9999L12.2218 19.778L10.8076 18.3638L16.1716 12.9999H4V10.9999H16.1716Z"></path></svg></a></p></div><div><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor"><path d="M12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22ZM12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20ZM12 10.5858L14.8284 7.75736L16.2426 9.17157L13.4142 12L16.2426 14.8284L14.8284 16.2426L12 13.4142L9.17157 16.2426L7.75736 14.8284L10.5858 12L7.75736 9.17157L9.17157 7.75736L12 10.5858Z"></path></svg><h3>Declarative cancellation</h3><p>Automatically cancel functions whenever events happen in your system — without API calls, recording job IDs, or storing state.</p><p><a href="https://www.inngest.com/docs/guides/cancel-running-functions?ref=homepage-orchestration">Learn more <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor"><path d="M16.1716 10.9999L10.8076 5.63589L12.2218 4.22168L20 11.9999L12.2218 19.778L10.8076 18.3638L16.1716 12.9999H4V10.9999H16.1716Z"></path></svg></a></p></div></div></div><div><blockquote><p>The DX and code simplicity it brings is unmatched, especially around local development. We're currently working to migrate some of our larger systems over and it’s a joy to see all the complexity it replaces, and with a much better story around partial failures and retries.</p></blockquote></div><div><div><p>Observability + Recovery</p><h2>Monitor, debug, and recover from issues</h2><p>Run in production with confidence with observability and recovery tools built into Inngest. Dig into traces to debug errors, monitor function and system health, and recovery from incidents with build-in tooling.</p></div><div><div><p><img alt="Screenshot of Debug your functions with traces" loading="lazy" width="1149" height="582" decoding="async" data-nimg="1" srcset="https://www.inngest.com/_next/image?url=%2Fassets%2Fhomepage%2Fo11y%2Ftracing.png&amp;w=1200&amp;q=75 1x, https://www.inngest.com/_next/image?url=%2Fassets%2Fhomepage%2Fo11y%2Ftracing.png&amp;w=3840&amp;q=75 2x" src="https://www.inngest.com/_next/image?url=%2Fassets%2Fhomepage%2Fo11y%2Ftracing.png&amp;w=3840&amp;q=75"></p></div><div><p><img alt="Screenshot of Full observability" loading="lazy" width="307" height="140" decoding="async" data-nimg="1" src="https://www.inngest.com/assets/homepage/o11y/metrics.svg"></p><div><h3><a href="https://www.inngest.com/docs/platform/monitor/observability-metrics?ref=homepage-observability">Full observability<!-- --> <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor"><path d="M13.1717 12.0007L8.22192 7.05093L9.63614 5.63672L16.0001 12.0007L9.63614 18.3646L8.22192 16.9504L13.1717 12.0007Z"></path></svg></a></h3><p>Rapidly diagnose system wide issues with metrics including volume, error rates, backlog and throughput.</p></div></div><div><p><img alt="Screenshot of Bulk function replay" loading="lazy" width="307" height="140" decoding="async" data-nimg="1" src="https://www.inngest.com/assets/homepage/o11y/replay.svg"></p><div><h3><a href="https://www.inngest.com/docs/platform/replay?ref=homepage-observability">Bulk function replay<!-- --> <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor"><path d="M13.1717 12.0007L8.22192 7.05093L9.63614 5.63672L16.0001 12.0007L9.63614 18.3646L8.22192 16.9504L13.1717 12.0007Z"></path></svg></a></h3><p>Leave dead letter queues in the past. Replay failed or cancelled functions in bulk to get your system back to green.</p></div></div><div><p><img alt="Screenshot of React to urgent issues" loading="lazy" width="307" height="140" decoding="async" data-nimg="1" srcset="https://www.inngest.com/_next/image?url=%2Fassets%2Fhomepage%2Fo11y%2Ffunction-actions.png&amp;w=384&amp;q=75 1x, https://www.inngest.com/_next/image?url=%2Fassets%2Fhomepage%2Fo11y%2Ffunction-actions.png&amp;w=640&amp;q=75 2x" src="https://www.inngest.com/_next/image?url=%2Fassets%2Fhomepage%2Fo11y%2Ffunction-actions.png&amp;w=640&amp;q=75"></p><div><h3><a href="https://www.inngest.com/docs/guides/pause-functions?ref=homepage-observability">React to urgent issues<!-- --> <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor"><path d="M13.1717 12.0007L8.22192 7.05093L9.63614 5.63672L16.0001 12.0007L9.63614 18.3646L8.22192 16.9504L13.1717 12.0007Z"></path></svg></a></h3><p>Quickly respond to urgent issues by pausing functions or cancelling runs in bulk. Resume and replay to recover.</p></div></div></div></div><div><blockquote><p>Configuration with Inngest is really easy. When we read our code base, we can immediately understand what it is and what it does. We are going to be gradually migrating most features to Inngest.</p></blockquote></div><div><div><p>Local Development</p><h2>Unparalleled local development workflow</h2><p>Our open source Dev Server runs on your machine for a complete local development experience, with production parity. Get instant feedback on your work and deploy to prod with full confidence.</p></div><video src="https://www.inngest.com/assets/homepage/video/2024-09-dev-server-4k.mp4" autoplay="" loop="" muted=""></video><div><div><h3>Visual debugging in <em>real-time</em></h3><p>No more parsing terminal logs. Watch your functions execute step-by-step in your browser.</p></div><div><h3>Single binary, zero dependencies</h3><p>One command to install and run. No external services, databases, or dependencies.</p></div><div><h3>Iterate faster</h3><p>Replay functions in one click for a faster feedback loop than ever before.</p></div></div></div><div><blockquote><p>Inngest changed how I build TypeScript applications. I can't stress enough how much I love using Inngest. The ease of use and developer experience is unparalleled</p></blockquote></div><div><div><p>Enterprise ready</p><h2>Security, scalability, and compliance</h2><p>We take security and reliability seriously. Our platform complies with the security standards you need to run your business.</p></div><div><div><div><h3><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor"><path d="M5 3V19H21V21H3V3H5ZM19.9393 5.93934L22.0607 8.06066L16 14.1213L13 11.121L9.06066 15.0607L6.93934 12.9393L13 6.87868L16 9.879L19.9393 5.93934Z"></path></svg>Billions of functions per month</h3><p>Our platform has been put to the test and designed to scale so your team can just focus on building a great product.</p></div><div><h3><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor"><path d="M12 1L20.2169 2.82598C20.6745 2.92766 21 3.33347 21 3.80217V13.7889C21 15.795 19.9974 17.6684 18.3282 18.7812L12 23L5.6718 18.7812C4.00261 17.6684 3 15.795 3 13.7889V3.80217C3 3.33347 3.32553 2.92766 3.78307 2.82598L12 1ZM12 3.04879L5 4.60434V13.7889C5 15.1263 5.6684 16.3752 6.7812 17.1171L12 20.5963L17.2188 17.1171C18.3316 16.3752 19 15.1263 19 13.7889V4.60434L12 3.04879ZM12 7C13.1046 7 14 7.89543 14 9C14 9.73984 13.5983 10.3858 13.0011 10.7318L13 15H11L10.9999 10.7324C10.4022 10.3866 10 9.74025 10 9C10 7.89543 10.8954 7 12 7Z"></path></svg>End-to-end encryption</h3><p>Data is encrypted in transit and at rest, so you can trust that your data is safe. Use our<!-- --> <a href="https://www.inngest.com/docs/features/middleware/encryption-middleware?ref=homepage-enterprise">encryption middleware</a> <!-- -->for additional control.</p></div><div><h3><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor"><path d="M3 6H21V18H3V6ZM2 4C1.44772 4 1 4.44772 1 5V19C1 19.5523 1.44772 20 2 20H22C22.5523 20 23 19.5523 23 19V5C23 4.44772 22.5523 4 22 4H2ZM13 8H19V10H13V8ZM18 12H13V14H18V12ZM10.5 10C10.5 11.3807 9.38071 12.5 8 12.5C6.61929 12.5 5.5 11.3807 5.5 10C5.5 8.61929 6.61929 7.5 8 7.5C9.38071 7.5 10.5 8.61929 10.5 10ZM8 13.5C6.067 13.5 4.5 15.067 4.5 17H11.5C11.5 15.067 9.933 13.5 8 13.5Z"></path></svg>SSO and SAML</h3><p>Secure your account with single sign-on and SAML.</p></div><div><h3><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor"><path d="M20 2C20.5523 2 21 2.44772 21 3V21C21 21.5523 20.5523 22 20 22H6C5.44772 22 5 21.5523 5 21V19H3V17H5V15H3V13H5V11H3V9H5V7H3V5H5V3C5 2.44772 5.44772 2 6 2H20ZM19 4H7V20H19V4ZM14 8V11H17V13H13.999L14 16H12L11.999 13H9V11H12V8H14Z"></path></svg>HIPAA BAA Available</h3><p>Our system was built with healthcare in mind to handle sensitive data.</p></div></div><p><img src="https://www.inngest.com/assets/homepage/gradient-graphics/enterprise.svg" alt="Enterprise ready"></p></div><p><a target="" href="https://www.inngest.com/contact?ref=homepage-enterprise">Chat with a solutions engineer<svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor"><path d="M13.1717 12.0007L8.22192 7.05093L9.63614 5.63672L16.0001 12.0007L9.63614 18.3646L8.22192 16.9504L13.1717 12.0007Z"></path></svg></a></p></div><div><blockquote><p>Recently we migrated a lot of our infrastructure to  reduce technical overhead and save costs. One of the key moves was going from GCP Composer (Airflow) to Inngest. This unified our backend and reduced our bill by 50x!!</p></blockquote></div><div><div><p>Developer love</p><h2>What devs are saying about Inngest</h2><p>Don't just take our word for it, this is what developers think about Inngest.</p></div><div><div><p><img alt="Image of Erik Munson" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Ferikmunson.jpg&amp;w=48&amp;q=75 1x, https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Ferikmunson.jpg&amp;w=96&amp;q=75 2x" src="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Ferikmunson.jpg&amp;w=96&amp;q=75"><span>Erik Munson<span>@<!-- -->erikmunson</span></span></p><p>I've worked on workflow systems before and am consistently amazed with the <span>@inngest</span> team's ability to do them better than anyone else. it takes me a few lines of app code to do things that would've been huge projects for an entire team in past jobs, largely due to work like this</p></div><div><p><img alt="Image of Taj English" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Flisted-b-taj-english.jpg&amp;w=48&amp;q=75 1x, https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Flisted-b-taj-english.jpg&amp;w=96&amp;q=75 2x" src="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Flisted-b-taj-english.jpg&amp;w=96&amp;q=75"><span>Taj English<span>@<!-- -->KINGiTAJ</span></span></p><p>Today @listedBApp says goodbye to Kafka and I couldn't be more excited to be replacing it with <span>@inngest</span>. Not every day do you get to replace a major infra component that improves DX, saves money and enables faster innovation :rocket:</p></div><div><p><img alt="Image of David" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fdavid-dzhng.jpg&amp;w=48&amp;q=75 1x, https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fdavid-dzhng.jpg&amp;w=96&amp;q=75 2x" src="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fdavid-dzhng.jpg&amp;w=96&amp;q=75"><span>David<span>@<!-- -->dzhng</span></span></p><p>For anyone who is building multi-step AI agents (e.g AutoGPT type systems), I highly recommend building it on top of a job queue orchestration framework like <span>@inngest</span>, the traceability these things provide out of the box is super useful, plus you get timeouts &amp; retries for free.</p></div><div><p><img alt="Image of Jesse Thomson" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fjessethomson11.jpg&amp;w=48&amp;q=75 1x, https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fjessethomson11.jpg&amp;w=96&amp;q=75 2x" src="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fjessethomson11.jpg&amp;w=96&amp;q=75"><span>Jesse Thomson<span>@<!-- -->jessethomson11</span></span></p><div><p>I think the game has changed with <span>@inngest</span>.</p><p>To me, it is the missing piece for making a JS/TS a serious backend contender.</p><p>It has queuing, rate limits, backoff, &amp; everything else you named, without needing to muck around with SQS. 10/10 recommend</p></div></div><div><p><img alt="Image of Patrick Göler von Ravensburg" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fproductlane-patrick.jpg&amp;w=48&amp;q=75 1x, https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fproductlane-patrick.jpg&amp;w=96&amp;q=75 2x" src="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fproductlane-patrick.jpg&amp;w=96&amp;q=75"><span>Patrick Göler von Ravensburg<span>@<!-- -->patrick_gvr</span></span></p><div><p>Headache prevented by <span>@inngest</span> and their concurrency feature 🤯</p><p>This function potentially runs for a long time and this allows us to not run this function again when the previous function hasn't finished based on the combination specified in 'key'.<img alt="Image of Inngest function" loading="lazy" width="400" height="124" decoding="async" data-nimg="1" srcset="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fproductlane-image.jpg&amp;w=640&amp;q=75 1x, https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fproductlane-image.jpg&amp;w=828&amp;q=75 2x" src="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fproductlane-image.jpg&amp;w=828&amp;q=75"></p></div></div><div><p><img alt="Image of Stefan Wirth" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fnafetswirth.jpg&amp;w=48&amp;q=75 1x, https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fnafetswirth.jpg&amp;w=96&amp;q=75 2x" src="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fnafetswirth.jpg&amp;w=96&amp;q=75"><span>Stefan Wirth<span>@<!-- -->NafetsWirth</span></span></p><div><p>Started playing around with <span>@inngest</span> for scheduled tasks and background jobs.</p><p>Highly recommended, super simple to set up and amazing DX.</p><p>Support also very responsive.</p><p>Great job guys!</p></div></div><div><p><img alt="Image of Guillermo Rauch" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Frauchg.jpg&amp;w=48&amp;q=75 1x, https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Frauchg.jpg&amp;w=96&amp;q=75 2x" src="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Frauchg.jpg&amp;w=96&amp;q=75"><span>Guillermo Rauch<span>@<!-- -->rauchg</span></span></p><div><p>So excited to see <span>@inngest</span> grow. It's the perfect tool to help orchestrate complex AI workloads.</p><p>Behind every successful generative AI product, there are queues and pipelines, increasingly powered by Inngest.</p></div></div><div><p><img alt="Image of James Q Quick" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fjamesqquick.jpg&amp;w=48&amp;q=75 1x, https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fjamesqquick.jpg&amp;w=96&amp;q=75 2x" src="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fjamesqquick.jpg&amp;w=96&amp;q=75"><span>James Q Quick<span>@<!-- -->jamesqquick</span></span></p><p>Tried <span>@inngest</span> for the first time today and …WOW! I'm really excited to fit this into some of my app ideas!</p></div><div><p><img alt="Image of Ray Amjad" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Frayamjad.jpg&amp;w=48&amp;q=75 1x, https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Frayamjad.jpg&amp;w=96&amp;q=75 2x" src="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Frayamjad.jpg&amp;w=96&amp;q=75"><span>Ray Amjad<span>@<!-- -->theramjad</span></span></p><p>I love this product so much! I spent 2 days setting up some background workers on Render.com and it was a total pain in the ass. I gave up and I got my background jobs set up in under 10 minutes with Inngest.</p></div><div><p><img alt="Image of Michael Roberts" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fmichaeljroberts.jpg&amp;w=48&amp;q=75 1x, https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fmichaeljroberts.jpg&amp;w=96&amp;q=75 2x" src="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fmichaeljroberts.jpg&amp;w=96&amp;q=75"><span>Michael Roberts<span>@<!-- -->codewithbhargav</span></span></p><p>Yeh so <span>@inngest</span> is perhaps one of the best SaaS platforms I have EVER used, incredible stability and crystal clear APIs. Love it already!</p></div><div><p><img alt="Image of Bhargav" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fcodewithbhargav.jpg&amp;w=48&amp;q=75 1x, https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fcodewithbhargav.jpg&amp;w=96&amp;q=75 2x" src="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fcodewithbhargav.jpg&amp;w=96&amp;q=75"><span>Bhargav<span>@<!-- -->codewithbhargav</span></span></p><p><span>@inngest</span> feels like a cheat code. Beautifully done!</p></div><div><p><img alt="Image of JB" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fjulianbenegas8.jpg&amp;w=48&amp;q=75 1x, https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fjulianbenegas8.jpg&amp;w=96&amp;q=75 2x" src="https://www.inngest.com/_next/image?url=%2Fassets%2Fcustomers%2Fsocial-proof%2Fjulianbenegas8.jpg&amp;w=96&amp;q=75"><span>JB<span>@<!-- -->julianbenegas8</span></span></p><p>ok, <span>@inngest</span> is incredible... really clear messaging, great docs, fast and well designed dashboard, great DX, etc... highly recommend.</p></div></div></div><div><div><a href="https://www.inngest.com/discord"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="4em" height="4em" fill="currentColor"><path d="M19.3034 5.33716C17.9344 4.71103 16.4805 4.2547 14.9629 4C14.7719 4.32899 14.5596 4.77471 14.411 5.12492C12.7969 4.89144 11.1944 4.89144 9.60255 5.12492C9.45397 4.77471 9.2311 4.32899 9.05068 4C7.52251 4.2547 6.06861 4.71103 4.70915 5.33716C1.96053 9.39111 1.21766 13.3495 1.5891 17.2549C3.41443 18.5815 5.17612 19.388 6.90701 19.9187C7.33151 19.3456 7.71356 18.73 8.04255 18.0827C7.41641 17.8492 6.82211 17.5627 6.24904 17.2231C6.39762 17.117 6.5462 17.0003 6.68416 16.8835C10.1438 18.4648 13.8911 18.4648 17.3082 16.8835C17.4568 17.0003 17.5948 17.117 17.7434 17.2231C17.1703 17.5627 16.576 17.8492 15.9499 18.0827C16.2789 18.73 16.6609 19.3456 17.0854 19.9187C18.8152 19.388 20.5875 18.5815 22.4033 17.2549C22.8596 12.7341 21.6806 8.80747 19.3034 5.33716ZM8.5201 14.8459C7.48007 14.8459 6.63107 13.9014 6.63107 12.7447C6.63107 11.5879 7.45884 10.6434 8.5201 10.6434C9.57071 10.6434 10.4303 11.5879 10.4091 12.7447C10.4091 13.9014 9.57071 14.8459 8.5201 14.8459ZM15.4936 14.8459C14.4535 14.8459 13.6034 13.9014 13.6034 12.7447C13.6034 11.5879 14.4323 10.6434 15.4936 10.6434C16.5442 10.6434 17.4038 11.5879 17.3825 12.7447C17.3825 13.9014 16.5548 14.8459 15.4936 14.8459Z"></path></svg></a><h4>Join our Discord community</h4><p>Join our Discord community to ask questions, share feedback, and have a direct line to shaping the future of Inngest!</p><p><a target="_blank" href="https://www.inngest.com/discord">Join the community</a></p></div><div><a href="https://github.com/inngest/inngest"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="4em" height="4em" fill="currentColor"><path d="M12.001 2C6.47598 2 2.00098 6.475 2.00098 12C2.00098 16.425 4.86348 20.1625 8.83848 21.4875C9.33848 21.575 9.52598 21.275 9.52598 21.0125C9.52598 20.775 9.51348 19.9875 9.51348 19.15C7.00098 19.6125 6.35098 18.5375 6.15098 17.975C6.03848 17.6875 5.55098 16.8 5.12598 16.5625C4.77598 16.375 4.27598 15.9125 5.11348 15.9C5.90098 15.8875 6.46348 16.625 6.65098 16.925C7.55098 18.4375 8.98848 18.0125 9.56348 17.75C9.65098 17.1 9.91348 16.6625 10.201 16.4125C7.97598 16.1625 5.65098 15.3 5.65098 11.475C5.65098 10.3875 6.03848 9.4875 6.67598 8.7875C6.57598 8.5375 6.22598 7.5125 6.77598 6.1375C6.77598 6.1375 7.61348 5.875 9.52598 7.1625C10.326 6.9375 11.176 6.825 12.026 6.825C12.876 6.825 13.726 6.9375 14.526 7.1625C16.4385 5.8625 17.276 6.1375 17.276 6.1375C17.826 7.5125 17.476 8.5375 17.376 8.7875C18.0135 9.4875 18.401 10.375 18.401 11.475C18.401 15.3125 16.0635 16.1625 13.8385 16.4125C14.201 16.725 14.5135 17.325 14.5135 18.2625C14.5135 19.6 14.501 20.675 14.501 21.0125C14.501 21.275 14.6885 21.5875 15.1885 21.4875C19.259 20.1133 21.9999 16.2963 22.001 12C22.001 6.475 17.526 2 12.001 2Z"></path></svg></a><h4>Open Source</h4><p>Inngest's core and all SDKs are open source. Explore our code bases, weigh in on RFCs, and contribute on GitHub.</p><p><a target="_blank" href="https://github.com/inngest/inngest">View project on GitHub</a></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Put this touch sensor on a robot and learn super precise tasks (307 pts)]]></title>
            <link>https://any-skin.github.io</link>
            <guid>41603865</guid>
            <pubDate>Fri, 20 Sep 2024 17:11:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://any-skin.github.io">https://any-skin.github.io</a>, See on <a href="https://news.ycombinator.com/item?id=41603865">Hacker News</a></p>
Couldn't get https://any-skin.github.io: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Federal civil rights watchdog sounds alarm over Feds use of facial recognition (145 pts)]]></title>
            <link>https://therecord.media/federal-civil-rights-watchdog-facial-recognition-technology-report</link>
            <guid>41603698</guid>
            <pubDate>Fri, 20 Sep 2024 16:51:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://therecord.media/federal-civil-rights-watchdog-facial-recognition-technology-report">https://therecord.media/federal-civil-rights-watchdog-facial-recognition-technology-report</a>, See on <a href="https://news.ycombinator.com/item?id=41603698">Hacker News</a></p>
Couldn't get https://therecord.media/federal-civil-rights-watchdog-facial-recognition-technology-report: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Visualizing Weather Forecasts Through Landscape Imagery (485 pts)]]></title>
            <link>https://github.com/lds133/weather_landscape</link>
            <guid>41603546</guid>
            <pubDate>Fri, 20 Sep 2024 16:31:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/lds133/weather_landscape">https://github.com/lds133/weather_landscape</a>, See on <a href="https://news.ycombinator.com/item?id=41603546">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Weather as Landscape</h2><a id="user-content-weather-as-landscape" aria-label="Permalink: Weather as Landscape" href="#weather-as-landscape"></a></p>
<p dir="auto">Visualizing Weather Forecasts Through Landscape Imagery</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/lds133/weather_landscape/blob/main/pic/house.png"><img src="https://github.com/lds133/weather_landscape/raw/main/pic/house.png" alt="house"></a></p>
<p dir="auto">Traditional weather stations often display sensor readings as raw numerical data. Navigating these dashboards can be overwhelming and stressful, as it requires significant effort to locate, interpret, and visualize specific parameters effectively.</p>
<p dir="auto">Viewing a landscape image feels natural to the human eye. The calming effect of observing landscape elements reduces stress and requires minimal effort, allowing for a more relaxed visual experience.</p>
<p dir="auto">The method below demonstrates how to encode weather information within a landscape image, with no or minimal reliance on numerical data.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Encoding principles</h2><a id="user-content-encoding-principles" aria-label="Permalink: Encoding principles" href="#encoding-principles"></a></p>
<p dir="auto">The landscape depicts a small house in the woods. The horizontal axis of the image represents a 24-hour timeline, starting from the current moment on the left, marked by the house, and extending to the conditions of the next day on the right. Various landscape elements distributed along the vertical axis symbolize weather events and conditions. The further an event is from the present, the farther it is positioned to the right in the image.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/lds133/weather_landscape/blob/main/pic/encode.png"><img src="https://github.com/lds133/weather_landscape/raw/main/pic/encode.png" alt="encode"></a></p>
<p dir="auto">The following information can be encoded within the landscape image:</p>
<ul dir="auto">
<li>Time markers to simplify timeline navigation:
<ul dir="auto">
<li>Sunrise and sunset times</li>
<li>Noon and midnight</li>
</ul>
</li>
<li>Weather forecast information:
<ul dir="auto">
<li>Wind direction and strength</li>
<li>Temperature fluctuations</li>
<li>Maximum and minimum temperature values</li>
<li>Cloud cover</li>
<li>Precipitation</li>
</ul>
</li>
<li>Current weather conditions:
<ul dir="auto">
<li>Temperature</li>
<li>Atmospheric pressure</li>
</ul>
</li>
<li>Non weather events:
<ul dir="auto">
<li>Birthdays</li>
<li>Holidays</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Implementation</h2><a id="user-content-implementation" aria-label="Permalink: Implementation" href="#implementation"></a></p>
<p dir="auto">The image generation code is written in <a href="https://www.python.org/" rel="nofollow">Python</a> using the <a href="https://python-pillow.org/" rel="nofollow">Pillow</a> library and is based on data from <a href="https://openweathermap.org/" rel="nofollow">OpenWeather</a>. The image is designed specifically for use on a 296x128 E-Ink display. The code tested on Python 3.9.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Event image</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/lds133/weather_landscape/blob/main/pic/sun_00.png"><img src="https://github.com/lds133/weather_landscape/raw/main/pic/sun_00.png" alt="example"></a></td>
<td>Sunrise</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/lds133/weather_landscape/blob/main/pic/moon_00.png"><img src="https://github.com/lds133/weather_landscape/raw/main/pic/moon_00.png" alt="example"></a></td>
<td>Sunset</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/lds133/weather_landscape/blob/main/pic/cloud_30.png"><img src="https://github.com/lds133/weather_landscape/raw/main/pic/cloud_30.png" alt="example"></a></td>
<td>Cloud cover</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/lds133/weather_landscape/blob/main/pic/house_00.png"><img src="https://github.com/lds133/weather_landscape/raw/main/pic/house_00.png" alt="example"></a></td>
<td>Current time position</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/lds133/weather_landscape/blob/main/pic/flower_00.png"><img src="https://github.com/lds133/weather_landscape/raw/main/pic/flower_00.png" alt="example"></a></td>
<td>Midnight</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/lds133/weather_landscape/blob/main/pic/flower_01.png"><img src="https://github.com/lds133/weather_landscape/raw/main/pic/flower_01.png" alt="example"></a></td>
<td>Midday</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/lds133/weather_landscape/blob/main/pic/palm_03.png"><img src="https://github.com/lds133/weather_landscape/raw/main/pic/palm_03.png" alt="example"></a></td>
<td>South wind</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/lds133/weather_landscape/blob/main/pic/east_03.png"><img src="https://github.com/lds133/weather_landscape/raw/main/pic/east_03.png" alt="example"></a></td>
<td>East wind</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/lds133/weather_landscape/blob/main/pic/tree_03.png"><img src="https://github.com/lds133/weather_landscape/raw/main/pic/tree_03.png" alt="example"></a></td>
<td>West wind</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/lds133/weather_landscape/blob/main/pic/pine_02.png"><img src="https://github.com/lds133/weather_landscape/raw/main/pic/pine_02.png" alt="example"></a></td>
<td>North wind</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/lds133/weather_landscape/blob/main/pic/rain.png"><img src="https://github.com/lds133/weather_landscape/raw/main/pic/rain.png" alt="example"></a></td>
<td>Rain</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>&nbsp;Landscape&nbsp;image&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/lds133/weather_landscape/blob/main/pic/weather_test.bmp"><img src="https://github.com/lds133/weather_landscape/raw/main/pic/weather_test.bmp" alt="example"></a></td>
<td>It’s around noon, with clear skies and a few clouds expected. A moderate north wind will develop overnight. Temperatures are currently rising but will begin to fall after sunset, reaching their lowest point before sunrise. During this time, the wind is expected to shift to the northeast.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/lds133/weather_landscape/blob/main/pic/test_20240903_043826.bmp"><img src="https://github.com/lds133/weather_landscape/raw/main/pic/test_20240903_043826.bmp" alt="example"></a></td>
<td>The sun is rising and it will be a hot sunny day with a light southeast breeze. The temperature will remain high even after sunset, and the wind will shift to the east, becoming stronger throughout the evening.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/lds133/weather_landscape/blob/main/pic/test_09B0B1083315.bmp"><img src="https://github.com/lds133/weather_landscape/raw/main/pic/test_09B0B1083315.bmp" alt="example"></a></td>
<td>It will be cold and rainy throughout the day and night. The south wind will shift to the northeast overnight.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running the code</h2><a id="user-content-running-the-code" aria-label="Permalink: Running the code" href="#running-the-code"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Preparing environment Linux</h4><a id="user-content-preparing-environment-linux" aria-label="Permalink: Preparing environment Linux" href="#preparing-environment-linux"></a></p>

<div data-snippet-clipboard-copy-content="source .venv/bin/activate"><pre><code>source .venv/bin/activate
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Preparing environment Windows</h4><a id="user-content-preparing-environment-windows" aria-label="Permalink: Preparing environment Windows" href="#preparing-environment-windows"></a></p>


<p dir="auto"><h4 tabindex="-1" dir="auto">Image creation test</h4><a id="user-content-image-creation-test" aria-label="Permalink: Image creation test" href="#image-creation-test"></a></p>
<p dir="auto">Update <strong>OWM_KEY</strong> variable in the weather_landscape.py file with your OpenWeather API key.</p>

<p dir="auto"><h4 tabindex="-1" dir="auto">Run server</h4><a id="user-content-run-server" aria-label="Permalink: Run server" href="#run-server"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Hardware</h2><a id="user-content-hardware" aria-label="Permalink: Hardware" href="#hardware"></a></p>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/lds133/weather_landscape/blob/main/pic/hardware.jpg"><img src="https://github.com/lds133/weather_landscape/raw/main/pic/hardware.jpg" alt="Setup"></a></p>
<p dir="auto">The hardware setup includes an <a href="https://www.adafruit.com/product/3269" rel="nofollow">ESP32 development board</a> and <a href="https://www.waveshare.com/2.9inch-e-paper-module.htm" rel="nofollow">2.9inch E-Ink display module</a>. Currently, the setup only displays an image sourced from the internet, updating every 15 minutes. It is uncertain whether the image generation code can be adapted for use with MicroPython on the ESP32 at this time.</p>
<p dir="auto"><a href="https://github.com/lds133/weather_landscape/blob/main/esp32/README.md">More information</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MemoRAG – Enhance RAG with memory-based knowledge discovery for long contexts (149 pts)]]></title>
            <link>https://github.com/qhjqhj00/MemoRAG</link>
            <guid>41602474</guid>
            <pubDate>Fri, 20 Sep 2024 14:41:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/qhjqhj00/MemoRAG">https://github.com/qhjqhj00/MemoRAG</a>, See on <a href="https://news.ycombinator.com/item?id=41602474">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto"><div dir="auto"><p>MemoRAG: Moving Towards Next-Gen RAG Via Memory-Inspired Knowledge Discovery</p></div></h2><a id="user-content-memorag-moving-towards-next-gen-rag-via-memory-inspired-knowledge-discovery" aria-label="Permalink: MemoRAG: Moving Towards Next-Gen RAG Via Memory-Inspired Knowledge Discovery" href="#memorag-moving-towards-next-gen-rag-via-memory-inspired-knowledge-discovery"></a></div>
<div dir="auto">
<p dir="auto"><strong>Empowering RAG with a memory-based data interface for all-purpose applications!</strong></p>
<p><a href="https://arxiv.org/abs/2409.05591" rel="nofollow"><img src="https://camo.githubusercontent.com/e77bc7a7bb6e94ececde002fe9566c05e16a43ce2505e3312f593c35d4478f49/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d6235323132662e7376673f6c6f676f3d6172786976" data-canonical-src="https://img.shields.io/badge/arXiv-b5212f.svg?logo=arxiv"></a>
<a href="https://huggingface.co/TommyChien/memorag-qwen2-7b-inst" rel="nofollow"><img src="https://camo.githubusercontent.com/b9cfd536e732f588fb294c166d0cf0445280e1277da0992d910d08046c71d394/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67466163652532304d6f64656c2d3237623362342e737667" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace%20Model-27b3b4.svg"></a>
<a href="https://github.com/"><img alt="License" src="https://camo.githubusercontent.com/f2d7ec897487f1f15b8aa0becf58575e8864664c20296ca5ad838c94a6aee34e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c4943454e53452d4d49542d677265656e" data-canonical-src="https://img.shields.io/badge/LICENSE-MIT-green"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/39dc05a0a44228ca43023e33f606d774bfbc34ed0aa0be2538308b2038d534c1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6d6164655f776974682d507974686f6e2d626c7565"><img alt="Static Badge" src="https://camo.githubusercontent.com/39dc05a0a44228ca43023e33f606d774bfbc34ed0aa0be2538308b2038d534c1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6d6164655f776974682d507974686f6e2d626c7565" data-canonical-src="https://img.shields.io/badge/made_with-Python-blue"></a>
</p></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto"><strong>MemoRAG</strong> is an innovative RAG framework built on top of a highly efficient, super-long memory model. Unlike standard RAG, which primarily handles queries with explicit information needs, MemoRAG leverages its memory model to achieve a global understanding of the entire database. By recalling query-specific clues from memory, MemoRAG enhances evidence retrieval, resulting in more accurate and contextually rich response generation.​</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/qhjqhj00/MemoRAG/blob/main/asset/tech_case.jpg"><img src="https://github.com/qhjqhj00/MemoRAG/raw/main/asset/tech_case.jpg"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">MemoRAG Demo</h2><a id="user-content-memorag-demo" aria-label="Permalink: MemoRAG Demo" href="#memorag-demo"></a></p>
<p dir="auto">We will provide a toy demo to demonstrate MemoRAG, you can try with the following scripts:</p>
<div dir="auto" data-snippet-clipboard-copy-content="streamlit run demo/demo.py"><pre><span>streamlit</span> <span>run</span> <span>demo</span><span>/</span><span>demo</span>.<span>py</span></pre></div>
<p dir="auto">Afterwards, you can view the demo as below:</p>
<div dir="auto">
  <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/qhjqhj00/MemoRAG/blob/main/asset/demo.gif"><img src="https://github.com/qhjqhj00/MemoRAG/raw/main/asset/demo.gif" data-animated-image=""></a>
  </p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">📃 Changelog</h2><a id="user-content-page_with_curl-changelog" aria-label="Permalink: :page_with_curl: Changelog" href="#page_with_curl-changelog"></a></p>
<p dir="auto">[13/09/24] MemoRAG adds <code>Meta-Llama-3.1-8B-Instruct</code> and <code>Llama3.1-8B-Chinese-Chat</code> as the Memory Model, see <a href="https://github.com/qhjqhj00/MemoRAG/blob/main/examples/longllm_as_memory.ipynb"><code>examples</code></a>.</p>
<p dir="auto">[10/09/24] We release MemoRAG's <a href="https://arxiv.org/pdf/2409.05591" rel="nofollow"><code>Technical Report</code></a>.</p>
<p dir="auto">[09/09/24] You can try MemoRAG on <a href="https://colab.research.google.com/drive/1fPMXKyi4AwWSBkC7Xr5vBdpPpx9gDeFX?usp=sharing" rel="nofollow"><code>Google Colab</code></a> for free.</p>
<p dir="auto">[05/09/24] A Qwen2-based memory model is available at <a href="https://huggingface.co/TommyChien/memorag-qwen2-7b-inst" rel="nofollow"><code>TommyChien/memorag-qwen2-7b-inst</code></a>.</p>
<p dir="auto">[03/09/24] A Mistral-based memory model is available at <a href="https://huggingface.co/TommyChien/memorag-mistral-7b-inst" rel="nofollow"><code>TommyChien/memorag-mistral-7b-inst</code></a>.</p>
<p dir="auto">[01/09/24] The project launched!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">✨ Features</h2><a id="user-content-sparkles-features" aria-label="Permalink: :sparkles: Features" href="#sparkles-features"></a></p>
<ul dir="auto">
<li><strong>Global Memory</strong>: Handles up to <strong>1 million tokens</strong> in a single context, providing comprehensive understanding across massive datasets.</li>
<li><strong>Optimizable &amp; Flexible</strong>: Adapts to new tasks with ease, achieving optimized performance with just a few hours of additional training.</li>
<li><strong>Contextual Clues</strong>: Generates precise clues from global memory, bridging raw input to answers and unlocking <strong>hidden insights</strong> from complex data.</li>
<li><strong>Efficient Caching</strong>: Speeds up context pre-filling by <strong>up to 30x</strong>, with support for caching chunking, indexing, and encoding.</li>
<li><strong>Context Reuse</strong>: Encodes long contexts <strong>once</strong> and supports repeated usage, boosting efficiency in tasks that require recurring data access.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔎 Roadmap</h2><a id="user-content-mag_right-roadmap" aria-label="Permalink: :mag_right: Roadmap" href="#mag_right-roadmap"></a></p>
<p dir="auto"><strong>MemoRAG</strong>  is currently under active development, with resources and prototypes continuously being published at this repository.</p>
<ul>
<li> Codes / Models / Dataset Release</li>
<li> Support OpenAI/Azure models</li>
<li> Technical Report Release</li>
<li> Support Chinese</li>
<li> Demo Codes Release</li>
<li> Training Codes for Memory model Release</li>
<li> <strong>Light-Weight Optimization</strong></li>
<li> <strong>Speed Up Inference</strong></li>
<li> <strong>Integrate Any Retrieval Methods</strong></li>
<li> <strong>Enrich the Memory Ability</strong></li>
</ul>
<p dir="auto">Note: The <strong>recent goals</strong> of MemoRAG are to achieve <strong>light-weight optimization</strong> through engineering improvements and to <strong>enhance its memory capabilities</strong>, enabling it to adapt to a wider range of applications and <strong>support longer context</strong> (e.g., more than one million tokens).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 Quick Start</h2><a id="user-content-rocket-quick-start" aria-label="Permalink: :rocket: Quick Start" href="#rocket-quick-start"></a></p>
<p dir="auto">🆓 <strong>You can directly try MemoRAG on <a href="https://colab.research.google.com/drive/1fPMXKyi4AwWSBkC7Xr5vBdpPpx9gDeFX?usp=sharing" rel="nofollow"><code>Google Colab</code></a> for free.</strong></p>
<p dir="auto">In this notebook, we run the complete MemoRAG pipeline (Memory Model + Retriever + Generation Model) on a single T4 GPU with 15GiB of memory provided by Google Colab. Despite the limited resources, MemoRAG can process half of the content from <a href="https://github.com/qhjqhj00/MemoRAG/blob/main/examples/harry_potter.txt">the example book</a> (~68K tokens) and perform all of its functions.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">To use Memorizer and MemoRAG, you need to have Python installed along with the required libraries. You can install the necessary dependencies using the following command:</p>
<p dir="auto"><strong>Install Dependencies</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install torch==2.3.1
conda install -c pytorch -c nvidia faiss-gpu=1.8.0"><pre>pip install torch==2.3.1
conda install -c pytorch -c nvidia faiss-gpu=1.8.0</pre></div>
<p dir="auto"><strong>Install from source</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# clone this repo first
cd MemoRAG
pip install -e ."><pre><span><span>#</span> clone this repo first</span>
<span>cd</span> MemoRAG
pip install -e <span>.</span></pre></div>
<p dir="auto"><strong>Install via pip</strong></p>

<p dir="auto">For <strong>Quick Start</strong>,
We provide a notebook to illustrate all functions of MemoRAG <a href="https://github.com/qhjqhj00/MemoRAG/blob/main/examples/example.ipynb">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">📓 Usage</h2><a id="user-content-notebook-usage" aria-label="Permalink: :notebook: Usage" href="#notebook-usage"></a></p>
<p dir="auto">MemoRAG is easy to use and can be initialized with HuggingFace models directly. By using the <code>MemoRAG.memorize()</code> method, the memory model builds a global memory over a long input context. Empirically, with default parameter settings, <code>TommyChien/memorag-qwen2-7b-inst</code> can handle contexts of up to 400K tokens, while <code>TommyChien/memorag-mistral-7b-inst</code> can manage contexts up to 128K tokens. By increasing the <code>beacon_ratio</code> parameter, the model’s capacity to handle longer contexts can be extended. For example, <code>TommyChien/memorag-qwen2-7b-inst</code> can process up to one million tokens with <code>beacon_ratio=16</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Basic Usage of MemoRAG</h3><a id="user-content-basic-usage-of-memorag" aria-label="Permalink: Basic Usage of MemoRAG" href="#basic-usage-of-memorag"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="from memorag import MemoRAG

# Initialize MemoRAG pipeline
pipe = MemoRAG(
    mem_model_name_or_path=&quot;TommyChien/memorag-mistral-7b-inst&quot;,
    ret_model_name_or_path=&quot;BAAI/bge-m3&quot;, 
    gen_model_name_or_path=&quot;mistralai/Mistral-7B-Instruct-v0.2&quot;, # Optional: if not specify, use memery model as the generator
    cache_dir=&quot;path_to_model_cache&quot;,  # Optional: specify local model cache directory
    access_token=&quot;hugging_face_access_token&quot;,  # Optional: Hugging Face access token
    beacon_ratio=4
)

context = open(&quot;examples/harry_potter.txt&quot;).read()
query = &quot;How many times is the Chamber of Secrets opened in the book?&quot;

# Memorize the context and save to cache
pipe.memorize(context, save_dir=&quot;cache/harry_potter/&quot;, print_stats=True)

# Generate response using the memorized context
res = pipe(context=context, query=query, task_type=&quot;memorag&quot;, max_new_tokens=256)
print(f&quot;MemoRAG generated answer: \n{res}&quot;)"><pre><span>from</span> <span>memorag</span> <span>import</span> <span>MemoRAG</span>

<span># Initialize MemoRAG pipeline</span>
<span>pipe</span> <span>=</span> <span>MemoRAG</span>(
    <span>mem_model_name_or_path</span><span>=</span><span>"TommyChien/memorag-mistral-7b-inst"</span>,
    <span>ret_model_name_or_path</span><span>=</span><span>"BAAI/bge-m3"</span>, 
    <span>gen_model_name_or_path</span><span>=</span><span>"mistralai/Mistral-7B-Instruct-v0.2"</span>, <span># Optional: if not specify, use memery model as the generator</span>
    <span>cache_dir</span><span>=</span><span>"path_to_model_cache"</span>,  <span># Optional: specify local model cache directory</span>
    <span>access_token</span><span>=</span><span>"hugging_face_access_token"</span>,  <span># Optional: Hugging Face access token</span>
    <span>beacon_ratio</span><span>=</span><span>4</span>
)

<span>context</span> <span>=</span> <span>open</span>(<span>"examples/harry_potter.txt"</span>).<span>read</span>()
<span>query</span> <span>=</span> <span>"How many times is the Chamber of Secrets opened in the book?"</span>

<span># Memorize the context and save to cache</span>
<span>pipe</span>.<span>memorize</span>(<span>context</span>, <span>save_dir</span><span>=</span><span>"cache/harry_potter/"</span>, <span>print_stats</span><span>=</span><span>True</span>)

<span># Generate response using the memorized context</span>
<span>res</span> <span>=</span> <span>pipe</span>(<span>context</span><span>=</span><span>context</span>, <span>query</span><span>=</span><span>query</span>, <span>task_type</span><span>=</span><span>"memorag"</span>, <span>max_new_tokens</span><span>=</span><span>256</span>)
<span>print</span>(<span>f"MemoRAG generated answer: <span>\n</span><span><span>{</span><span>res</span><span>}</span></span>"</span>)</pre></div>
<p dir="auto">When running the above code, <strong>the encoded key-value (KV) cache, Faiss index, and chunked passages are stored</strong> in the specified <code>save_dir</code>. Afterward, if the same context is used again, the data can be quickly loaded from the disk:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pipe.load(&quot;cache/harry_potter/&quot;, print_stats=True)"><pre><span>pipe</span>.<span>load</span>(<span>"cache/harry_potter/"</span>, <span>print_stats</span><span>=</span><span>True</span>)</pre></div>
<p dir="auto">Typically, loading cached weights is highly efficient. For example, <strong>encoding, chunking, and indexing a 200K-token context takes approximately 35 seconds</strong> using <code>TommyChien/memorag-qwen2-7b-inst</code> as the memory model, <strong>but only 1.5 seconds when loading from cached files.</strong></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using Long LLMs as Memory Model 🆕 🆕 🆕</h3><a id="user-content-using-long-llms-as-memory-model-new-new-new" aria-label="Permalink: Using Long LLMs as Memory Model :new: :new: :new:" href="#using-long-llms-as-memory-model-new-new-new"></a></p>
<p dir="auto">Recent LLMs have become effective memory models due to their expanding context windows. MemoRAG now supports leveraging these long-context LLMs as memory models, utilizing <a href="https://github.com/microsoft/MInference"><code>MInference</code></a> to optimize context prefilling. We have tested <code>Meta-Llama-3.1-8B-Instruct</code> and <code>Llama3.1-8B-Chinese-Chat</code> as memory models, both of which natively support a 128K context length. We are currently exploring additional suitable LLMs and optimizing strategies to enhance the memory mechanisms and context length further. For detailed usage instructions, please refer to the provided scripts and the <a href="https://github.com/qhjqhj00/MemoRAG/blob/main/examples/longllm_as_memory.ipynb"><code>notebook</code></a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from memorag import MemoRAG
model = MemoRAG(
    mem_model_name_or_path=&quot;shenzhi-wang/Llama3.1-8B-Chinese-Chat&quot;,    # For Chinese
    # mem_model_name_or_path=&quot;meta-llama/Meta-Llama-3.1-8B-Instruct&quot;,  # For English
    ret_model_name_or_path=&quot;BAAI/bge-m3&quot;,
    # cache_dir=&quot;path_to_model_cache&quot;,  # to specify local model cache directory (optional)
    # access_token=&quot;hugging_face_access_token&quot;  # to specify local model cache directory (optional)
    )"><pre><span>from</span> <span>memorag</span> <span>import</span> <span>MemoRAG</span>
<span>model</span> <span>=</span> <span>MemoRAG</span>(
    <span>mem_model_name_or_path</span><span>=</span><span>"shenzhi-wang/Llama3.1-8B-Chinese-Chat"</span>,    <span># For Chinese</span>
    <span># mem_model_name_or_path="meta-llama/Meta-Llama-3.1-8B-Instruct",  # For English</span>
    <span>ret_model_name_or_path</span><span>=</span><span>"BAAI/bge-m3"</span>,
    <span># cache_dir="path_to_model_cache",  # to specify local model cache directory (optional)</span>
    <span># access_token="hugging_face_access_token"  # to specify local model cache directory (optional)</span>
    )</pre></div>
<p dir="auto">Afterward, you can use MemoRAG's functions as usual.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Summarization Task</h3><a id="user-content-summarization-task" aria-label="Permalink: Summarization Task" href="#summarization-task"></a></p>
<p dir="auto">To perform summarization tasks, use the following script:</p>
<div dir="auto" data-snippet-clipboard-copy-content="res = pipe(context=context, task_type=&quot;summarize&quot;, max_new_tokens=512)
print(f&quot;MemoRAG summary of the full book:\n {res}&quot;)"><pre><span>res</span> <span>=</span> <span>pipe</span>(<span>context</span><span>=</span><span>context</span>, <span>task_type</span><span>=</span><span>"summarize"</span>, <span>max_new_tokens</span><span>=</span><span>512</span>)
<span>print</span>(<span>f"MemoRAG summary of the full book:<span>\n</span> <span><span>{</span><span>res</span><span>}</span></span>"</span>)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using APIs as Generators</h3><a id="user-content-using-apis-as-generators" aria-label="Permalink: Using APIs as Generators" href="#using-apis-as-generators"></a></p>
<p dir="auto">If you want to use APIs as a generator, refer to the script below:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from memorag import Agent, MemoRAG

# API configuration
api_dict = {
    &quot;endpoint&quot;: &quot;&quot;,
    &quot;api_version&quot;: &quot;2024-02-15-preview&quot;,
    &quot;api_key&quot;: &quot;&quot;
}
model = &quot;gpt-35-turbo-16k&quot;
source = &quot;azure&quot;

# Initialize Agent with the API
agent = Agent(model, source, api_dict)
print(agent.generate(&quot;hi!&quot;))  # Test the API

# Initialize MemoRAG pipeline with a customized generator model
pipe = MemoRAG(
    mem_model_name_or_path=&quot;TommyChien/memorag-qwen2-7b-inst&quot;,
    ret_model_name_or_path=&quot;BAAI/bge-m3&quot;,
    cache_dir=&quot;path_to_model_cache&quot;,  # Optional: specify local model cache directory
    customized_gen_model=agent,
)

# Load previously cached context
pipe.load(&quot;cache/harry_potter_qwen/&quot;, print_stats=True)

# Use the loaded context for question answering
query = &quot;How are the mutual relationships between the main characters?&quot;
context = open(&quot;harry_potter.txt&quot;).read()

res = pipe(context=context, query=query, task_type=&quot;memorag&quot;, max_new_tokens=256)
print(f&quot;MemoRAG with GPT-3.5 generated answer: \n{res}&quot;)"><pre><span>from</span> <span>memorag</span> <span>import</span> <span>Agent</span>, <span>MemoRAG</span>

<span># API configuration</span>
<span>api_dict</span> <span>=</span> {
    <span>"endpoint"</span>: <span>""</span>,
    <span>"api_version"</span>: <span>"2024-02-15-preview"</span>,
    <span>"api_key"</span>: <span>""</span>
}
<span>model</span> <span>=</span> <span>"gpt-35-turbo-16k"</span>
<span>source</span> <span>=</span> <span>"azure"</span>

<span># Initialize Agent with the API</span>
<span>agent</span> <span>=</span> <span>Agent</span>(<span>model</span>, <span>source</span>, <span>api_dict</span>)
<span>print</span>(<span>agent</span>.<span>generate</span>(<span>"hi!"</span>))  <span># Test the API</span>

<span># Initialize MemoRAG pipeline with a customized generator model</span>
<span>pipe</span> <span>=</span> <span>MemoRAG</span>(
    <span>mem_model_name_or_path</span><span>=</span><span>"TommyChien/memorag-qwen2-7b-inst"</span>,
    <span>ret_model_name_or_path</span><span>=</span><span>"BAAI/bge-m3"</span>,
    <span>cache_dir</span><span>=</span><span>"path_to_model_cache"</span>,  <span># Optional: specify local model cache directory</span>
    <span>customized_gen_model</span><span>=</span><span>agent</span>,
)

<span># Load previously cached context</span>
<span>pipe</span>.<span>load</span>(<span>"cache/harry_potter_qwen/"</span>, <span>print_stats</span><span>=</span><span>True</span>)

<span># Use the loaded context for question answering</span>
<span>query</span> <span>=</span> <span>"How are the mutual relationships between the main characters?"</span>
<span>context</span> <span>=</span> <span>open</span>(<span>"harry_potter.txt"</span>).<span>read</span>()

<span>res</span> <span>=</span> <span>pipe</span>(<span>context</span><span>=</span><span>context</span>, <span>query</span><span>=</span><span>query</span>, <span>task_type</span><span>=</span><span>"memorag"</span>, <span>max_new_tokens</span><span>=</span><span>256</span>)
<span>print</span>(<span>f"MemoRAG with GPT-3.5 generated answer: <span>\n</span><span><span>{</span><span>res</span><span>}</span></span>"</span>)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Supported APIs for Generators</h3><a id="user-content-supported-apis-for-generators" aria-label="Permalink: Supported APIs for Generators" href="#supported-apis-for-generators"></a></p>
<p dir="auto">The built-in <code>Agent</code> object supports models from both <code>openai</code> and <code>deepseek</code>. Below are the configurations for initializing these models:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Using deepseek models
model = &quot;&quot;
source = &quot;deepseek&quot;
api_dict = {
    &quot;base_url&quot;: &quot;&quot;,
    &quot;api_key&quot;: &quot;&quot;
}

# Using openai models
model = &quot;&quot;
source = &quot;openai&quot;
api_dict = {
    &quot;api_key&quot;: &quot;&quot;
}"><pre><span># Using deepseek models</span>
<span>model</span> <span>=</span> <span>""</span>
<span>source</span> <span>=</span> <span>"deepseek"</span>
<span>api_dict</span> <span>=</span> {
    <span>"base_url"</span>: <span>""</span>,
    <span>"api_key"</span>: <span>""</span>
}

<span># Using openai models</span>
<span>model</span> <span>=</span> <span>""</span>
<span>source</span> <span>=</span> <span>"openai"</span>
<span>api_dict</span> <span>=</span> {
    <span>"api_key"</span>: <span>""</span>
}</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Usage for Memory Model</h3><a id="user-content-usage-for-memory-model" aria-label="Permalink: Usage for Memory Model" href="#usage-for-memory-model"></a></p>
<p dir="auto">The Memory model can be used independently to store, recall, and interact with the context. Here’s an example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from memorag import Memory

# Initialize the Memory model
memo_model = Memory(
    &quot;TommyChien/memorag-qwen2-7b-inst&quot;,
    cache_dir=&quot;path_to_model_cache&quot;,  # Optional: specify local model cache directory
    beacon_ratio=4  # Adjust beacon ratio for handling longer contexts
)

# Load and memorize the context
context = open(&quot;harry_potter.txt&quot;).read()
memo_model.memorize(context)

# Save the memorized context to disk
memo_model.save(&quot;cache/harry_potter/memory.bin&quot;)

# Query the model for answers
query = &quot;How are the mutual relationships between the main characters?&quot;

res = memo_model.answer(query)
print(&quot;Using memory to answer the query:\n&quot;, res)

# Recall text clues for evidence retrieval
res = memo_model.recall(query)
print(&quot;Using memory to recall text clues to support evidence retrieval:\n&quot;, res)

# Rewrite the query into more specific surrogate queries
res = memo_model.rewrite(query)
print(&quot;Using memory to rewrite the input query into more specific surrogate queries:\n&quot;, res)"><pre><span>from</span> <span>memorag</span> <span>import</span> <span>Memory</span>

<span># Initialize the Memory model</span>
<span>memo_model</span> <span>=</span> <span>Memory</span>(
    <span>"TommyChien/memorag-qwen2-7b-inst"</span>,
    <span>cache_dir</span><span>=</span><span>"path_to_model_cache"</span>,  <span># Optional: specify local model cache directory</span>
    <span>beacon_ratio</span><span>=</span><span>4</span>  <span># Adjust beacon ratio for handling longer contexts</span>
)

<span># Load and memorize the context</span>
<span>context</span> <span>=</span> <span>open</span>(<span>"harry_potter.txt"</span>).<span>read</span>()
<span>memo_model</span>.<span>memorize</span>(<span>context</span>)

<span># Save the memorized context to disk</span>
<span>memo_model</span>.<span>save</span>(<span>"cache/harry_potter/memory.bin"</span>)

<span># Query the model for answers</span>
<span>query</span> <span>=</span> <span>"How are the mutual relationships between the main characters?"</span>

<span>res</span> <span>=</span> <span>memo_model</span>.<span>answer</span>(<span>query</span>)
<span>print</span>(<span>"Using memory to answer the query:<span>\n</span>"</span>, <span>res</span>)

<span># Recall text clues for evidence retrieval</span>
<span>res</span> <span>=</span> <span>memo_model</span>.<span>recall</span>(<span>query</span>)
<span>print</span>(<span>"Using memory to recall text clues to support evidence retrieval:<span>\n</span>"</span>, <span>res</span>)

<span># Rewrite the query into more specific surrogate queries</span>
<span>res</span> <span>=</span> <span>memo_model</span>.<span>rewrite</span>(<span>query</span>)
<span>print</span>(<span>"Using memory to rewrite the input query into more specific surrogate queries:<span>\n</span>"</span>, <span>res</span>)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Usage for Memory-Augmented Retrieval</h3><a id="user-content-usage-for-memory-augmented-retrieval" aria-label="Permalink: Usage for Memory-Augmented Retrieval" href="#usage-for-memory-augmented-retrieval"></a></p>
<p dir="auto">In addition to the standalone Memory Model, MemoRAG provides memory-augmented retrieval functionality. This allows for improved evidence retrieval based on recalled clues from memory.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from memorag import MemoRAG

# Initialize MemoRAG pipeline
pipe = MemoRAG(
    mem_model_name_or_path=&quot;TommyChien/memorag-qwen2-7b-inst&quot;,
    ret_model_name_or_path=&quot;BAAI/bge-m3&quot;,
    cache_dir=&quot;path_to_model_cache&quot;,  # Optional: specify local model cache directory
    access_token=&quot;hugging_face_access_token&quot;  # Optional: Hugging Face access token
)

# Load and memorize the context
test_txt = open(&quot;harry_potter.txt&quot;).read()
pipe.memorize(test_txt, save_dir=&quot;cache/harry_potter/&quot;, print_stats=True)

# Define the query
query = &quot;How are the mutual relationships between the main characters?&quot;

# Recall clues from memory
clues = pipe.mem_model.recall(query).split(&quot;\n&quot;)
clues = [q for q in clues if len(q.split()) > 3]  # Filter out short or irrelevant clues
print(&quot;Clues generated from memory:\n&quot;, clues)

# Retrieve relevant passages based on the recalled clues
retrieved_passages = pipe._retrieve(clues)
print(&quot;\n======\n&quot;.join(retrieved_passages[:3]))"><pre><span>from</span> <span>memorag</span> <span>import</span> <span>MemoRAG</span>

<span># Initialize MemoRAG pipeline</span>
<span>pipe</span> <span>=</span> <span>MemoRAG</span>(
    <span>mem_model_name_or_path</span><span>=</span><span>"TommyChien/memorag-qwen2-7b-inst"</span>,
    <span>ret_model_name_or_path</span><span>=</span><span>"BAAI/bge-m3"</span>,
    <span>cache_dir</span><span>=</span><span>"path_to_model_cache"</span>,  <span># Optional: specify local model cache directory</span>
    <span>access_token</span><span>=</span><span>"hugging_face_access_token"</span>  <span># Optional: Hugging Face access token</span>
)

<span># Load and memorize the context</span>
<span>test_txt</span> <span>=</span> <span>open</span>(<span>"harry_potter.txt"</span>).<span>read</span>()
<span>pipe</span>.<span>memorize</span>(<span>test_txt</span>, <span>save_dir</span><span>=</span><span>"cache/harry_potter/"</span>, <span>print_stats</span><span>=</span><span>True</span>)

<span># Define the query</span>
<span>query</span> <span>=</span> <span>"How are the mutual relationships between the main characters?"</span>

<span># Recall clues from memory</span>
<span>clues</span> <span>=</span> <span>pipe</span>.<span>mem_model</span>.<span>recall</span>(<span>query</span>).<span>split</span>(<span>"<span>\n</span>"</span>)
<span>clues</span> <span>=</span> [<span>q</span> <span>for</span> <span>q</span> <span>in</span> <span>clues</span> <span>if</span> <span>len</span>(<span>q</span>.<span>split</span>()) <span>&gt;</span> <span>3</span>]  <span># Filter out short or irrelevant clues</span>
<span>print</span>(<span>"Clues generated from memory:<span>\n</span>"</span>, <span>clues</span>)

<span># Retrieve relevant passages based on the recalled clues</span>
<span>retrieved_passages</span> <span>=</span> <span>pipe</span>.<span>_retrieve</span>(<span>clues</span>)
<span>print</span>(<span>"<span>\n</span>======<span>\n</span>"</span>.<span>join</span>(<span>retrieved_passages</span>[:<span>3</span>]))</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Evaluation on Benchmark</h3><a id="user-content-evaluation-on-benchmark" aria-label="Permalink: Evaluation on Benchmark" href="#evaluation-on-benchmark"></a></p>
<p dir="auto">Below are experiments results for the memory model, incorporating with three generation models.</p>
<markdown-accessiblity-table>
    We test MemoRAG on three benchmarks. The best results of each block are in bold.
    <table><thead>
        <tr>
            <th>Dataset</th>
            <th>NarrativeQA</th>
            <th>Qasper</th>
            <th>MultifieldQA</th>
            <th>Musique</th>
            <th>2Wiki</th>
            <th>HotpotQA</th>
            <th>MultiNews</th>
            <th>GovReport</th>
            <th>En.sum</th>
            <th>En.qa</th>
            <th>Fin</th>
            <th>Legal</th>
            <th>Mix</th>
        </tr>
        <tr>
            <td></td>
            <td colspan="8"><strong>LongBench</strong></td>
            <td colspan="2"><strong>InfBench</strong></td>
            <td colspan="3"><strong>UltraDomain</strong></td>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td colspan="13"><strong>Generator: Llama3-8B-Instruct-8K</strong></td>
        </tr>
        <tr>
            <td>Full</td>
            <td>21.3</td>
            <td>43.4</td>
            <td>46.6</td>
            <td>23.5</td>
            <td>38.2</td>
            <td>47.1</td>
            <td>24.6</td>
            <td>23.6</td>
            <td>13.1</td>
            <td>6.7</td>
            <td>34.2</td>
            <td>33.2</td>
            <td>42.7</td>
        </tr>
        <tr>
            <td>BGE-M3</td>
            <td>22.1</td>
            <td>44.3</td>
            <td>50.2</td>
            <td>22.2</td>
            <td>36.7</td>
            <td>48.4</td>
            <td>22.1</td>
            <td>20.1</td>
            <td>12.1</td>
            <td>15.1</td>
            <td>41.4</td>
            <td>40.6</td>
            <td>46.4</td>
        </tr>
        <tr>
            <td>Stella-v5</td>
            <td>12.3</td>
            <td>35.2</td>
            <td>44.4</td>
            <td>22.1</td>
            <td>33.3</td>
            <td>41.9</td>
            <td>22.1</td>
            <td>20.7</td>
            <td>11.7</td>
            <td>14.8</td>
            <td>41.9</td>
            <td>33.7</td>
            <td>44.9</td>
        </tr>
        <tr>
            <td>RQ-RAG</td>
            <td>20.2</td>
            <td>43.9</td>
            <td>49.1</td>
            <td>22.7</td>
            <td>36.1</td>
            <td>44.5</td>
            <td>20.6</td>
            <td>21.0</td>
            <td>12.0</td>
            <td>13.3</td>
            <td>39.5</td>
            <td>36.8</td>
            <td>44.5</td>
        </tr>
        <tr>
            <td>HyDE</td>
            <td>22.1</td>
            <td>44.3</td>
            <td>50.2</td>
            <td>22.2</td>
            <td>36.7</td>
            <td>48.4</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td><strong>19.1</strong></td>
            <td>41.4</td>
            <td>40.6</td>
            <td>46.4</td>
        </tr>
        <tr>
            <td><strong>MemoRAG</strong></td>
            <td><strong>22.8</strong></td>
            <td><strong>45.7</strong></td>
            <td><strong>50.7</strong></td>
            <td><strong>28.4</strong></td>
            <td><strong>51.4</strong></td>
            <td><strong>57.0</strong></td>
            <td><strong>27.4</strong></td>
            <td><strong>27.9</strong></td>
            <td><strong>14.1</strong></td>
            <td>16.1</td>
            <td><strong>47.8</strong></td>
            <td><strong>47.9</strong></td>
            <td><strong>55.5</strong></td>
        </tr>
        <tr>
            <td colspan="13"><strong>Generator: Phi-3-mini-128K</strong></td>
        </tr>
        <tr>
            <td>Full</td>
            <td>21.4</td>
            <td>35.0</td>
            <td>47.3</td>
            <td>19.0</td>
            <td>35.5</td>
            <td>42.1</td>
            <td>25.6</td>
            <td>23.7</td>
            <td>13.0</td>
            <td>15.2</td>
            <td>44.8</td>
            <td>40.5</td>
            <td>44.7</td>
        </tr>
        <tr>
            <td>BGE-M3</td>
            <td>20.3</td>
            <td>33.0</td>
            <td>44.3</td>
            <td>21.1</td>
            <td>35.4</td>
            <td>42.1</td>
            <td>17.7</td>
            <td>19.8</td>
            <td>9.6</td>
            <td>16.3</td>
            <td>41.7</td>
            <td>41.2</td>
            <td>43.7</td>
        </tr>
        <tr>
            <td>Stella-v5</td>
            <td>13.7</td>
            <td>32.4</td>
            <td>43.5</td>
            <td>21.0</td>
            <td>35.6</td>
            <td>40.6</td>
            <td>20.3</td>
            <td>18.2</td>
            <td>10.0</td>
            <td>19.5</td>
            <td>42.8</td>
            <td>35.1</td>
            <td>43.9</td>
        </tr>
        <tr>
            <td>RQ-RAG</td>
            <td>19.6</td>
            <td>34.1</td>
            <td>46.5</td>
            <td>21.9</td>
            <td>36.1</td>
            <td>41.7</td>
            <td>20.1</td>
            <td>18.6</td>
            <td>10.4</td>
            <td>16.1</td>
            <td>41.8</td>
            <td>40.9</td>
            <td>43.2</td>
        </tr>
        <tr>
            <td>HyDE</td>
            <td>18.7</td>
            <td>36.0</td>
            <td>47.5</td>
            <td>20.5</td>
            <td>36.8</td>
            <td>42.7</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>19.6</td>
            <td>43.1</td>
            <td>41.6</td>
            <td>44.2</td>
        </tr>
        <tr>
            <td><strong>MemoRAG</strong></td>
            <td><strong>27.5</strong></td>
            <td><strong>43.9</strong></td>
            <td><strong>52.2</strong></td>
            <td><strong>33.9</strong></td>
            <td><strong>54.1</strong></td>
            <td><strong>54.8</strong></td>
            <td><strong>32.9</strong></td>
            <td><strong>26.3</strong></td>
            <td><strong>15.7</strong></td>
            <td><strong>22.9</strong></td>
            <td><strong>51.5</strong></td>
            <td><strong>51.0</strong></td>
            <td><strong>55.6</strong></td>
        </tr>
        <tr>
            <td colspan="13"><strong>Generator: Mistral-7B-Instruct-v0.2-32K</strong></td>
        </tr>
        <tr>
            <td>Full</td>
            <td>20.8</td>
            <td>29.2</td>
            <td>46.3</td>
            <td>18.9</td>
            <td>20.6</td>
            <td>37.6</td>
            <td>23.0</td>
            <td>20.4</td>
            <td>12.4</td>
            <td>12.3</td>
            <td>36.5</td>
            <td>35.8</td>
            <td>42.1</td>
        </tr>
        <tr>
            <td>BGE-M3</td>
            <td>17.3</td>
            <td>29.5</td>
            <td>46.3</td>
            <td>18.5</td>
            <td>20.3</td>
            <td>36.2</td>
            <td>24.3</td>
            <td>26.1</td>
            <td>13.5</td>
            <td>12.2</td>
            <td>40.5</td>
            <td>42.0</td>
            <td>41.1</td>
        </tr>
        <tr>
            <td>Stella-v5</td>
            <td>13.5</td>
            <td>23.7</td>
            <td>42.1</td>
            <td>18.6</td>
            <td>22.2</td>
            <td>31.9</td>
            <td>21.1</td>
            <td>18.5</td>
            <td>13.2</td>
            <td>9.7</td>
            <td>40.9</td>
            <td>34.9</td>
            <td>42.1</td>
        </tr>
        <tr>
            <td>RQ-RAG</td>
            <td>17.1</td>
            <td>29.2</td>
            <td>47.0</td>
            <td>19.1</td>
            <td>21.5</td>
            <td>37.0</td>
            <td>22.1</td>
            <td>18.6</td>
            <td>13.1</td>
            <td>12.7</td>
            <td>44.3</td>
            <td>44.6</td>
            <td>43.4</td>
        </tr>
        <tr>
            <td>HyDE</td>
            <td>17.4</td>
            <td>29.5</td>
            <td>46.3</td>
            <td>18.5</td>
            <td>20.1</td>
            <td>36.2</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>12.2</td>
            <td>42.8</td>
            <td>35.1</td>
            <td>43.9</td>
        </tr>
        <tr>
            <td><strong>MemoRAG</strong></td>
            <td><strong>23.1</strong></td>
            <td>31.2</td>
            <td><strong>50.0</strong></td>
            <td>26.9</td>
            <td>30.3</td>
            <td>42.9</td>
            <td><strong>27.1</strong></td>
            <td><strong>31.6</strong></td>
            <td><strong>17.9</strong></td>
            <td>15.4</td>
            <td>48.0</td>
            <td>51.2</td>
            <td><strong>53.6</strong></td>
        </tr>
        <tr>
            <td><strong>MemoRAG-qwen2</strong></td>
            <td>22.2</td>
            <td><strong>32.7</strong></td>
            <td>49.6</td>
            <td><strong>31.4</strong></td>
            <td><strong>33.7</strong></td>
            <td><strong>44.4</strong></td>
            <td>27.0</td>
            <td>31.5</td>
            <td>16.8</td>
            <td><strong>17.6</strong></td>
            <td><strong>48.7</strong></td>
            <td><strong>52.3</strong></td>
            <td>48.6</td>
        </tr>
    </tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Evaluation</h3><a id="user-content-evaluation" aria-label="Permalink: Evaluation" href="#evaluation"></a></p>
<p dir="auto">To evaluate MemoRAG, use the following script:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd examples
bash longbench/eval.sh"><pre><span>cd</span> examples
bash longbench/eval.sh</pre></div>
<p dir="auto">We will update other evaluation scripts soon.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Dataset</h3><a id="user-content-dataset" aria-label="Permalink: Dataset" href="#dataset"></a></p>
<p dir="auto">UltraDomain Benchmark: <a href="https://huggingface.co/datasets/TommyChien/UltraDomain" rel="nofollow">this repo</a>.</p>
<p dir="auto">Other Evaluation Data: <a href="https://huggingface.co/datasets/TommyChien/MemoRAG-data/" rel="nofollow">this repo</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🙌 FAQs</h2><a id="user-content-raised_hands-faqs" aria-label="Permalink: :raised_hands: FAQs" href="#raised_hands-faqs"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔖 License</h2><a id="user-content-bookmark-license" aria-label="Permalink: :bookmark: License" href="#bookmark-license"></a></p>
<p dir="auto">MemoRAG is licensed under the <a href="https://github.com/qhjqhj00/MemoRAG/blob/main/LICENSE">MIT License</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">If you use MemoRAG in your research, please cite our paper:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{qian2024memorag,
      title={MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge Discovery}, 
      author={Hongjin Qian and Peitian Zhang and Zheng Liu and Kelong Mao and Zhicheng Dou},
      year={2024},
      eprint={2409.05591},
      url={https://arxiv.org/abs/2409.05591}, 
}"><pre><span>@misc</span>{<span>qian2024memorag</span>,
      <span>title</span>=<span><span>{</span>MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge Discovery<span>}</span></span>, 
      <span>author</span>=<span><span>{</span>Hongjin Qian and Peitian Zhang and Zheng Liu and Kelong Mao and Zhicheng Dou<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2024<span>}</span></span>,
      <span>eprint</span>=<span><span>{</span>2409.05591<span>}</span></span>,
      <span>url</span>=<span><span>{</span>https://arxiv.org/abs/2409.05591<span>}</span></span>, 
}</pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reactive Relational Algebra (138 pts)]]></title>
            <link>https://taylor.town/reactive-relational-algebra</link>
            <guid>41602056</guid>
            <pubDate>Fri, 20 Sep 2024 13:54:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://taylor.town/reactive-relational-algebra">https://taylor.town/reactive-relational-algebra</a>, See on <a href="https://news.ycombinator.com/item?id=41602056">Hacker News</a></p>
Couldn't get https://taylor.town/reactive-relational-algebra: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[CuPy: NumPy and SciPy for GPU (323 pts)]]></title>
            <link>https://github.com/cupy/cupy</link>
            <guid>41601730</guid>
            <pubDate>Fri, 20 Sep 2024 13:18:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/cupy/cupy">https://github.com/cupy/cupy</a>, See on <a href="https://news.ycombinator.com/item?id=41601730">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/cupy/cupy/main/docs/image/cupy_logo_1000px.png"><img src="https://raw.githubusercontent.com/cupy/cupy/main/docs/image/cupy_logo_1000px.png" width="400"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">CuPy : NumPy &amp; SciPy for GPU</h2><a id="user-content-cupy--numpy--scipy-for-gpu" aria-label="Permalink: CuPy : NumPy &amp; SciPy for GPU" href="#cupy--numpy--scipy-for-gpu"></a></p>
<p dir="auto"><a href="https://pypi.python.org/pypi/cupy" rel="nofollow"><img src="https://camo.githubusercontent.com/9f42f93fc2988966b22e8d1fe20d80081269ad0cdd9f7ec6d1733263f8e91c14/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f63757079" alt="pypi" data-canonical-src="https://img.shields.io/pypi/v/cupy"></a>
<a href="https://anaconda.org/conda-forge/cupy" rel="nofollow"><img src="https://camo.githubusercontent.com/44ea99a7b25703ede08c667128fb8017deb1f135463fa458274f93f8bec01e05/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e64612d2d666f7267652d637570792d626c7565" alt="Conda" data-canonical-src="https://img.shields.io/badge/conda--forge-cupy-blue"></a>
<a href="https://github.com/cupy/cupy"><img src="https://camo.githubusercontent.com/f000c610f4fc21ef53c9bad299fcecf7fa7b099c5dbff5a411734fdffb558dfd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f637570792f63757079" alt="GitHub license" data-canonical-src="https://img.shields.io/github/license/cupy/cupy"></a>
<a href="https://gitter.im/cupy/community" rel="nofollow"><img src="https://camo.githubusercontent.com/765ca66b2fe7f52347e995ca4de6c9d5fb07e02bfa9a9bae4c556c2e5e8e9568/68747470733a2f2f696d672e736869656c64732e696f2f6d61747269782f637570795f636f6d6d756e6974793a6769747465722e696d3f7365727665725f6671646e3d6d61747269782e6f7267" alt="Matrix" data-canonical-src="https://img.shields.io/matrix/cupy_community:gitter.im?server_fqdn=matrix.org"></a>
<a href="https://twitter.com/CuPy_Team" rel="nofollow"><img src="https://camo.githubusercontent.com/4884e27583c170dca1d21677b65e14db365e0d916af4c99fc596d42ffb2ea1eb/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f437550795f5465616d3f6c6162656c3d253430437550795f5465616d" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/follow/CuPy_Team?label=%40CuPy_Team"></a>
<a href="https://medium.com/cupy-team" rel="nofollow"><img src="https://camo.githubusercontent.com/d346eda24b15079eac1881c68671ba74545ba72e98e286e0361626a2becda086/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d656469756d2d437550792d7465616c" alt="Medium" data-canonical-src="https://img.shields.io/badge/Medium-CuPy-teal"></a></p>
<p dir="auto"><a href="https://cupy.dev/" rel="nofollow"><strong>Website</strong></a>
| <a href="https://docs.cupy.dev/en/stable/install.html" rel="nofollow"><strong>Install</strong></a>
| <a href="https://docs.cupy.dev/en/stable/user_guide/basic.html" rel="nofollow"><strong>Tutorial</strong></a>
| <a href="https://github.com/cupy/cupy/tree/main/examples"><strong>Examples</strong></a>
| <a href="https://docs.cupy.dev/en/stable/" rel="nofollow"><strong>Documentation</strong></a>
| <a href="https://docs.cupy.dev/en/stable/reference/" rel="nofollow"><strong>API Reference</strong></a>
| <a href="https://groups.google.com/forum/#!forum/cupy" rel="nofollow"><strong>Forum</strong></a></p>
<p dir="auto">CuPy is a NumPy/SciPy-compatible array library for GPU-accelerated computing with Python.
CuPy acts as a <a href="https://docs.cupy.dev/en/stable/reference/comparison.html" rel="nofollow">drop-in replacement</a> to run existing NumPy/SciPy code on NVIDIA CUDA or AMD ROCm platforms.</p>
<div dir="auto" data-snippet-clipboard-copy-content=">>> import cupy as cp
>>> x = cp.arange(6).reshape(2, 3).astype('f')
>>> x
array([[ 0.,  1.,  2.],
       [ 3.,  4.,  5.]], dtype=float32)
>>> x.sum(axis=1)
array([  3.,  12.], dtype=float32)"><pre><span>&gt;&gt;</span><span>&gt;</span> <span>import</span> <span>cupy</span> <span>as</span> <span>cp</span>
<span>&gt;&gt;</span><span>&gt;</span> <span>x</span> <span>=</span> <span>cp</span>.<span>arange</span>(<span>6</span>).<span>reshape</span>(<span>2</span>, <span>3</span>).<span>astype</span>(<span>'f'</span>)
<span>&gt;&gt;</span><span>&gt;</span> <span>x</span>
<span>array</span>([[ <span>0.</span>,  <span>1.</span>,  <span>2.</span>],
       [ <span>3.</span>,  <span>4.</span>,  <span>5.</span>]], <span>dtype</span><span>=</span><span>float32</span>)
<span>&gt;&gt;</span><span>&gt;</span> <span>x</span>.<span>sum</span>(<span>axis</span><span>=</span><span>1</span>)
<span>array</span>([  <span>3.</span>,  <span>12.</span>], <span>dtype</span><span>=</span><span>float32</span>)</pre></div>
<p dir="auto">CuPy also provides access to low-level CUDA features.
You can pass <code>ndarray</code> to existing CUDA C/C++ programs via <a href="https://docs.cupy.dev/en/stable/user_guide/kernel.html#raw-kernels" rel="nofollow">RawKernels</a>, use <a href="https://docs.cupy.dev/en/stable/reference/cuda.html" rel="nofollow">Streams</a> for performance, or even call <a href="https://docs.cupy.dev/en/stable/reference/cuda.html#runtime-api" rel="nofollow">CUDA Runtime APIs</a> directly.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Pip</h3><a id="user-content-pip" aria-label="Permalink: Pip" href="#pip"></a></p>
<p dir="auto">Binary packages (wheels) are available for Linux and Windows on <a href="https://pypi.org/org/cupy/" rel="nofollow">PyPI</a>.
Choose the right package for your platform.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Platform</th>
<th>Architecture</th>
<th>Command</th>
</tr>
</thead>
<tbody>
<tr>
<td>CUDA 11.x (11.2+)</td>
<td>x86_64 / aarch64</td>
<td><code>pip install cupy-cuda11x</code></td>
</tr>
<tr>
<td>CUDA 12.x</td>
<td>x86_64 / aarch64</td>
<td><code>pip install cupy-cuda12x</code></td>
</tr>
<tr>
<td>ROCm 4.3 (<em><a href="https://docs.cupy.dev/en/latest/install.html#using-cupy-on-amd-gpu-experimental" rel="nofollow">experimental</a></em>)</td>
<td>x86_64</td>
<td><code>pip install cupy-rocm-4-3</code></td>
</tr>
<tr>
<td>ROCm 5.0 (<em><a href="https://docs.cupy.dev/en/latest/install.html#using-cupy-on-amd-gpu-experimental" rel="nofollow">experimental</a></em>)</td>
<td>x86_64</td>
<td><code>pip install cupy-rocm-5-0</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">To install pre-releases, append <code>--pre -U -f https://pip.cupy.dev/pre</code> (e.g., <code>pip install cupy-cuda11x --pre -U -f https://pip.cupy.dev/pre</code>).</p>
</div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Conda</h3><a id="user-content-conda" aria-label="Permalink: Conda" href="#conda"></a></p>
<p dir="auto">Binary packages are also available for Linux and Windows on <a href="https://anaconda.org/conda-forge/cupy" rel="nofollow">Conda-Forge</a>.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Platform</th>
<th>Architecture</th>
<th>Command</th>
</tr>
</thead>
<tbody>
<tr>
<td>CUDA</td>
<td>x86_64 / aarch64 / ppc64le</td>
<td><code>conda install -c conda-forge cupy</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">If you need a slim installation (without also getting CUDA dependencies installed), you can do <code>conda install -c conda-forge cupy-core</code>.</p>
<p dir="auto">If you need to use a particular CUDA version (say 12.0), you can use the <code>cuda-version</code> metapackage to select the version, e.g. <code>conda install -c conda-forge cupy cuda-version=12.0</code>.</p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">If you encounter any problem with CuPy installed from <code>conda-forge</code>, please feel free to report to <a href="https://github.com/conda-forge/cupy-feedstock/issues">cupy-feedstock</a>, and we will help investigate if it is just a packaging issue in <code>conda-forge</code>'s recipe or a real issue in CuPy.</p>
</div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Docker</h3><a id="user-content-docker" aria-label="Permalink: Docker" href="#docker"></a></p>
<p dir="auto">Use <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html" rel="nofollow">NVIDIA Container Toolkit</a> to run <a href="https://hub.docker.com/r/cupy/cupy" rel="nofollow">CuPy container images</a>.</p>
<div data-snippet-clipboard-copy-content="$ docker run --gpus all -it cupy/cupy"><pre><code>$ docker run --gpus all -it cupy/cupy
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Resources</h2><a id="user-content-resources" aria-label="Permalink: Resources" href="#resources"></a></p>
<ul dir="auto">
<li><a href="https://docs.cupy.dev/en/stable/install.html" rel="nofollow">Installation Guide</a> - instructions on building from source</li>
<li><a href="https://github.com/cupy/cupy/releases">Release Notes</a></li>
<li><a href="https://github.com/cupy/cupy/wiki/Projects-using-CuPy">Projects using CuPy</a></li>
<li><a href="https://docs.cupy.dev/en/stable/contribution.html" rel="nofollow">Contribution Guide</a></li>
<li><a href="https://www.nvidia.com/en-us/on-demand/session/gtcfall21-a31149/" rel="nofollow">GPU Acceleration in Python using CuPy and Numba (GTC November 2021 Technical Session)</a></li>
<li><a href="https://github.com/awthomp/cusignal-icassp-tutorial">GPU-Acceleration of Signal Processing Workflows using CuPy and cuSignal<sup></sup></a><a href="#user-content-fn-1-0eaf4d7645e187ce6caba1b7a4030a5e" id="user-content-fnref-1-0eaf4d7645e187ce6caba1b7a4030a5e" data-footnote-ref="" aria-describedby="footnote-label">1</a> (ICASSP'21 Tutorial)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">MIT License (see <code>LICENSE</code> file).</p>
<p dir="auto">CuPy is designed based on NumPy's API and SciPy's API (see <code>docs/source/license.rst</code> file).</p>
<p dir="auto">CuPy is being developed and maintained by <a href="https://www.preferred.jp/en/" rel="nofollow">Preferred Networks</a> and <a href="https://github.com/cupy/cupy/graphs/contributors">community contributors</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Reference</h2><a id="user-content-reference" aria-label="Permalink: Reference" href="#reference"></a></p>
<p dir="auto">Ryosuke Okuta, Yuya Unno, Daisuke Nishino, Shohei Hido and Crissman Loomis.
<strong>CuPy: A NumPy-Compatible Library for NVIDIA GPU Calculations.</strong>
<em>Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Thirty-first Annual Conference on Neural Information Processing Systems (NIPS)</em>, (2017).
[<a href="http://learningsys.org/nips17/assets/papers/paper_16.pdf" rel="nofollow">PDF</a>]</p>
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{cupy_learningsys2017,
  author       = &quot;Okuta, Ryosuke and Unno, Yuya and Nishino, Daisuke and Hido, Shohei and Loomis, Crissman&quot;,
  title        = &quot;CuPy: A NumPy-Compatible Library for NVIDIA GPU Calculations&quot;,
  booktitle    = &quot;Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Thirty-first Annual Conference on Neural Information Processing Systems (NIPS)&quot;,
  year         = &quot;2017&quot;,
  url          = &quot;http://learningsys.org/nips17/assets/papers/paper_16.pdf&quot;
}"><pre><span>@inproceedings</span>{<span>cupy_learningsys2017</span>,
  <span>author</span>       = <span><span>"</span>Okuta, Ryosuke and Unno, Yuya and Nishino, Daisuke and Hido, Shohei and Loomis, Crissman<span>"</span></span>,
  <span>title</span>        = <span><span>"</span>CuPy: A NumPy-Compatible Library for NVIDIA GPU Calculations<span>"</span></span>,
  <span>booktitle</span>    = <span><span>"</span>Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Thirty-first Annual Conference on Neural Information Processing Systems (NIPS)<span>"</span></span>,
  <span>year</span>         = <span><span>"</span>2017<span>"</span></span>,
  <span>url</span>          = <span><span>"</span>http://learningsys.org/nips17/assets/papers/paper_16.pdf<span>"</span></span>
}</pre></div>
<section data-footnotes="">
<ol dir="auto">
<li id="user-content-fn-1-0eaf4d7645e187ce6caba1b7a4030a5e">
<p dir="auto">cuSignal is now part of CuPy starting v13.0.0. <a href="#user-content-fnref-1-0eaf4d7645e187ce6caba1b7a4030a5e" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
</ol>
</section>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft TypeSpec (106 pts)]]></title>
            <link>https://typespec.io/</link>
            <guid>41601728</guid>
            <pubDate>Fri, 20 Sep 2024 13:18:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://typespec.io/">https://typespec.io/</a>, See on <a href="https://news.ycombinator.com/item?id=41601728">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Three Mile Island nuclear plant restart in Microsoft AI power deal (160 pts)]]></title>
            <link>https://www.reuters.com/markets/deals/constellation-inks-power-supply-deal-with-microsoft-2024-09-20/</link>
            <guid>41601443</guid>
            <pubDate>Fri, 20 Sep 2024 12:43:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/markets/deals/constellation-inks-power-supply-deal-with-microsoft-2024-09-20/">https://www.reuters.com/markets/deals/constellation-inks-power-supply-deal-with-microsoft-2024-09-20/</a>, See on <a href="https://news.ycombinator.com/item?id=41601443">Hacker News</a></p>
Couldn't get https://www.reuters.com/markets/deals/constellation-inks-power-supply-deal-with-microsoft-2024-09-20/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Tō Reo – A Māori Spellchecker (155 pts)]]></title>
            <link>https://xn--treo-l3a.nz/</link>
            <guid>41601347</guid>
            <pubDate>Fri, 20 Sep 2024 12:30:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xn--treo-l3a.nz/">https://xn--treo-l3a.nz/</a>, See on <a href="https://news.ycombinator.com/item?id=41601347">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <main>
        

        <form id="index-form">
            
            
            
            
        </form>

        <a id="sample1" href="#">
            Generate example text
        </a>

        
    </main>

    



</div>]]></description>
        </item>
    </channel>
</rss>