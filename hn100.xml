<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 15 Oct 2024 20:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Meta's open AI hardware vision (113 pts)]]></title>
            <link>https://engineering.fb.com/2024/10/15/data-infrastructure/metas-open-ai-hardware-vision/</link>
            <guid>41851304</guid>
            <pubDate>Tue, 15 Oct 2024 18:08:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://engineering.fb.com/2024/10/15/data-infrastructure/metas-open-ai-hardware-vision/">https://engineering.fb.com/2024/10/15/data-infrastructure/metas-open-ai-hardware-vision/</a>, See on <a href="https://news.ycombinator.com/item?id=41851304">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		<ul>
<li aria-level="1"><span>At the Open Compute Project (OCP) Global Summit 2024, we’re showcasing our latest open AI hardware designs with the OCP community.</span></li>
<li aria-level="1"><span>These innovations include a new AI platform, cutting-edge open rack designs, and advanced network fabrics and components.&nbsp;</span></li>
<li aria-level="1"><span>By sharing our designs, we hope to inspire collaboration and foster innovation. If you’re passionate about building the future of AI, we invite you to engage with us and OCP to help shape the next generation of open hardware for AI.</span></li>
</ul>
<p><span>AI has been at the core of the experiences Meta has been delivering to people and businesses for years, including AI modeling innovations to optimize and improve on features like </span><a href="https://ai.meta.com/blog/facebook-feed-improvements-ai-show-more-less/" target="_blank" rel="noopener"><span>Feed</span></a><span> and our </span><a href="https://engineering.fb.com/2024/07/10/data-infrastructure/machine-learning-ml-prediction-robustness-meta/" target="_blank" rel="noopener"><span>ads system</span></a><span>. As we develop and release new, advanced AI models, we are also driven to advance our infrastructure to support our new and emerging AI workloads.</span></p>
<p><span>For example, </span><a href="https://ai.meta.com/blog/meta-llama-3-1/" target="_blank" rel="noopener"><span>Llama 3.1 405B</span></a><span>, Meta’s largest model, is a dense transformer with 405B parameters and a context window of up to 128k tokens. To train a large language model (LLM) of this magnitude, with over 15 trillion tokens, we had to make substantial optimizations to our entire training stack. This effort pushed our infrastructure to operate across more than 16,000 NVIDIA H100 GPUs, making Llama 3.1 405B the first model in the Llama series to be trained at such a massive scale.&nbsp;</span></p>
<p><span>Prior to Llama, our largest AI jobs ran on 128 NVIDIA A100 GPUs. ​But things have rapidly accelerated. ​Over the course of 2023, we rapidly scaled up our training clusters from 1K, 2K, 4K, to eventually 16K GPUs to support our AI workloads. Today, we’re training our models on two</span> <a href="https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/" target="_blank" rel="noopener"><span>24K-GPU clusters</span></a><span>.</span></p>
<p><span>We don’t expect this upward trajectory for AI clusters to slow down any time soon. In fact, we expect the amount of compute needed for AI training will grow significantly from where we are today.</span></p>
<p><span>Building AI clusters requires more than just GPUs. Networking and bandwidth play an important role in ensuring the clusters’ performance. Our systems consist of a tightly integrated HPC compute system and an isolated high-bandwidth compute network that connects all our GPUs and domain-specific accelerators. This design is necessary to meet our injection needs and address the challenges posed by our need for bisection bandwidth.</span></p>
<p><span>In the next few years, we anticipate greater injection bandwidth on the order of a terabyte per second, per accelerator, with equal normalized bisection bandwidth. This represents a growth of more than an order of magnitude compared to today’s networks!</span></p>
<p><span>To support this growth, we need a high-performance, multi-tier, non-blocking network fabric that can utilize modern congestion control to behave predictably under heavy load. This will enable us to fully leverage the power of our AI clusters and ensure they continue to perform optimally as we push the boundaries of what is possible with AI.</span></p>
<p><span>Scaling AI at this speed requires open hardware solutions. Developing new architectures, network fabrics, and system designs is the most efficient and impactful when we can build it on principles of openness. By investing in open hardware, we unlock AI’s full potential and propel ongoing innovation in the field.</span></p>
<h2><span>Introducing Catalina: Open Architecture for AI Infra</span></h2>
<figure id="attachment_21841" aria-describedby="caption-attachment-21841"><img decoding="async" src="https://engineering.fb.com/wp-content/uploads/2050/05/Catalina-Front-Back-2.png?w=683" alt="" width="456" height="683" srcset="https://engineering.fb.com/wp-content/uploads/2050/05/Catalina-Front-Back-2.png 720w, https://engineering.fb.com/wp-content/uploads/2050/05/Catalina-Front-Back-2.png?resize=611,916 611w, https://engineering.fb.com/wp-content/uploads/2050/05/Catalina-Front-Back-2.png?resize=683,1024 683w, https://engineering.fb.com/wp-content/uploads/2050/05/Catalina-Front-Back-2.png?resize=96,144 96w, https://engineering.fb.com/wp-content/uploads/2050/05/Catalina-Front-Back-2.png?resize=192,288 192w" sizes="(max-width: 992px) 100vw, 62vw"><figcaption id="caption-attachment-21841">Catalina front view (left) and rear view (right).</figcaption></figure>
<p><span>Today, we announced the upcoming release of Catalina, our new high-powered rack designed for AI workloads, to the OCP community. Catalina is based on the <a href="https://nvidianews.nvidia.com/news/nvidia-contributes-blackwell-platform-design-to-open-hardware-ecosystem-accelerating-ai-infrastructure-innovation" target="_blank" rel="noopener">NVIDIA Blackwell platform full rack-scale solution</a>, with a focus on modularity and flexibility. It is built to support the latest NVIDIA GB200 Grace Blackwell Superchip, ensuring it meets the growing demands of modern AI infrastructure.&nbsp;</span></p>
<p><span>The growing power demands of GPUs means open rack solutions need to support higher power capability. With Catalina we’re introducing the Orv3, a high-power rack (HPR) capable of supporting up to 140kW.</span></p>
<p><span>The full solution is liquid cooled and consists of a power shelf that supports a compute tray, switch tray, the Orv3 HPR, the</span> <a href="https://engineering.fb.com/2021/11/09/data-center-engineering/ocp-summit-2021/" target="_blank" rel="noopener"><span>Wedge 400</span></a><span> fabric switch, a management switch, battery backup unit, and a rack management controller.</span></p>
<p><span>We aim for Catalina’s modular design to empower others to customize the rack to meet their specific AI workloads while leveraging both existing and emerging industry standards.</span></p>
<h2><span>The Grand Teton Platform now supports AMD accelerators</span></h2>
<p><img decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/10/Grand-Teton-AMD-MI300X-Open-small.png?w=916" alt="" width="600" height="436" srcset="https://engineering.fb.com/wp-content/uploads/2024/10/Grand-Teton-AMD-MI300X-Open-small.png 1109w, https://engineering.fb.com/wp-content/uploads/2024/10/Grand-Teton-AMD-MI300X-Open-small.png?resize=916,665 916w, https://engineering.fb.com/wp-content/uploads/2024/10/Grand-Teton-AMD-MI300X-Open-small.png?resize=768,557 768w, https://engineering.fb.com/wp-content/uploads/2024/10/Grand-Teton-AMD-MI300X-Open-small.png?resize=1024,743 1024w, https://engineering.fb.com/wp-content/uploads/2024/10/Grand-Teton-AMD-MI300X-Open-small.png?resize=96,70 96w, https://engineering.fb.com/wp-content/uploads/2024/10/Grand-Teton-AMD-MI300X-Open-small.png?resize=192,139 192w" sizes="(max-width: 992px) 100vw, 62vw"></p>
<p><span>In 2022, we announced </span><a href="https://engineering.fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/" target="_blank" rel="noopener"><span>Grand Teton</span></a><span>, our next-generation AI platform (the follow-up to our Zion-EX platform). Grand Teton is designed with compute capacity to support the demands of memory-bandwidth-bound workloads, such as Meta’s <a href="https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/" target="_blank" rel="noopener">deep learning recommendation models (</a></span><span>DLRMs</span><span>), as well as compute-bound workloads like content understanding.</span></p>
<p><span>Now, we have expanded the Grand Teton platform to support the AMD Instinct MI300X and will be contributing this new version to OCP. Like its predecessors, this new version of Grand Teton</span><span>&nbsp;features a single monolithic system design with fully integrated power, control, compute, and fabric interfaces. This high level of integration simplifies system deployment, enabling rapid scaling with increased reliability for large-scale AI inference workloads.</span></p>
<p><span>In addition to supporting a range of accelerator designs, now including the AMD Instinct MI300x, Grand Teton offers significantly greater compute capacity, allowing faster convergence on a larger set of weights. This is complemented by expanded memory to store and run larger models locally, along with increased network bandwidth to scale up training cluster sizes efficiently.</span></p>
<h2><span>​Open Disaggregated Scheduled Fabric&nbsp;</span></h2>
<p><img loading="lazy" decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png?w=1024" alt="" width="1024" height="508" srcset="https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png 1871w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png?resize=916,454 916w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png?resize=768,381 768w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png?resize=1024,508 1024w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png?resize=1536,762 1536w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png?resize=96,48 96w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png?resize=192,95 192w" sizes="(max-width: 992px) 100vw, 62vw"></p>
<p><span>Developing open, vendor-agnostic networking backend is going to play an important role going forward as we continue to push the performance of our AI training clusters. Disaggregating our network allows us to work with vendors from across the industry to design systems that are innovative as well as scalable, flexible, and efficient.</span></p>
<p><span>Our new <a href="https://engineering.fb.com/2024/10/15/data-infrastructure/open-future-networking-hardware-ai-ocp-2024-meta/" target="_blank" rel="noopener">Disaggregated Scheduled Fabric (DSF)</a> for our next-generation AI clusters offers several advantages over our existing switches. By opening up our network fabric we can overcome limitations in scale, component supply options, and power density. DSF is powered by the open</span><a href="https://github.com/opencomputeproject/SAI" target="_blank" rel="noopener"><span> OCP-SAI</span></a><span> standard and</span><a href="https://engineering.fb.com/2018/09/04/data-infrastructure/research-in-brief-building-switch-software-at-scale-and-in-the-open/" target="_blank" rel="noopener"><span> FBOSS</span></a><span>, Meta’s own network operating system for controlling network switches. It also supports an open and standard Ethernet-based RoCE interface to endpoints and accelerators across several GPUS and NICS from several different vendors, including our partners at NVIDIA, Broadcom, and AMD.</span></p>
<p><span>In addition to DSF, we have also developed and built new 51T fabric switches based on Broadcom and Cisco ASICs. Finally, we are sharing our new FBNIC, a new NIC module that contains our first Meta-design network ASIC. In order to meet the growing needs of our AI&nbsp;</span></p>
<h2><span>Meta and Microsoft: Driving Open Innovation Together</span></h2>
<p><span>Meta and Microsoft have a long-standing partnership within OCP, beginning with the development of the </span><a href="https://www.opencompute.org/documents/switch-abstraction-interface-ocp-specification-v0-2-pdf" target="_blank" rel="noopener"><span>Switch Abstraction Interface (SAI)</span></a><span> for data centers in 2018. Over the years together, we’ve contributed to key initiatives such as the </span><a href="https://www.opencompute.org/blog/new-open-accelerator-infrastructure-oai-sub-project-to-launch-within-the-ocp-server-project" target="_blank" rel="noopener"><span>Open Accelerator Module (OAM)</span></a><span> standard and SSD standardization, showcasing our shared commitment to advancing open innovation.</span></p>
<p><span>Our current </span><a href="https://azure.microsoft.com/en-us/blog/accelerating-industry-wide-innovations-in-datacenter-infrastructure-and-security/" target="_blank" rel="noopener"><span>collaboration focuses on Mount Diablo</span></a><span>, a new disaggregated power rack. It’s a cutting-edge solution featuring a scalable 400 VDC unit that enhances efficiency and scalability. This innovative design allows more AI accelerators per IT rack, significantly advancing AI infrastructure. We’re excited to continue our collaboration through this contribution.</span></p>
<h2><span>The open future of AI infra</span></h2>
<p><a href="https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/" target="_blank" rel="noopener"><span>Meta is committed to open source AI</span></a><span>. We believe that open source will put the benefits and opportunities of AI into the hands of people all over the word.&nbsp;</span></p>
<p><span>AI won’t realize its full potential without collaboration. We need open software frameworks to drive model innovation, ensure portability, and promote transparency in AI development. We must also prioritize open and standardized models so we can leverage collective expertise, make AI more accessible, and work towards minimizing biases in our systems.​</span></p>
<p><span>Just as important, we also need open AI hardware systems. These systems are necessary for delivering the kind of high-performance, cost-effective, and adaptable infrastructure necessary for AI advancement.​</span></p>
<p><span>We encourage anyone who wants to help advance the future of AI hardware systems to engage with the OCP community. By addressing AI’s infrastructure needs together, we can unlock the true promise of open AI for everyone.​</span></p>

		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sqlite3 WebAssembly (212 pts)]]></title>
            <link>https://sqlite.org/wasm/doc/trunk/index.md</link>
            <guid>41851051</guid>
            <pubDate>Tue, 15 Oct 2024 17:45:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sqlite.org/wasm/doc/trunk/index.md">https://sqlite.org/wasm/doc/trunk/index.md</a>, See on <a href="https://news.ycombinator.com/item?id=41851051">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>This site is home to the documentation for the SQLite project's
WebAssembly- and JavaScript-related APIs, which enable the use of
<a href="https://sqlite.org/">sqlite3</a> in modern WASM-capable browsers. These components
were initially released for public beta with version 3.40 and will
tentatively be made API-stable with the 3.41 release, pending
community feedback.</p>

<p>Disclaimer: this site requires a modern, JavaScript-capable browser
for full functionality. This site uses client-side storage for storing
certain browsing preferences (like the bright/dark mode toggle) but
does not store any user information server-side, except for logged-in
developers. The only user-level details this site shares with any
other systems are the public SCM-related details of this site's own
developers.</p>

<h2>Site Overview</h2>
<!-- https://unicode.org/emoji/charts/full-emoji-list.html -->

<p><a href="https://sqlite.org/wasm/doc/trunk/about.md"><strong>About</strong> the sqlite3 WASM subproject</a>:</p>

<ul>
<li>📰 <a href="https://sqlite.org/wasm/doc/trunk/news.md"><strong>Project news</strong></a></li>
<li>💬 <a href="https://sqlite.org/wasm/doc/trunk/faq.md"><strong>Frequently Asked Questions</strong></a></li>
<li>🚧 <a href="https://sqlite.org/wasm/doc/trunk/todo.md"><strong>TODOs</strong></a> and (un)planned features</li>
<li>☎ <strong>Technical support</strong> is provided, and feedback gladly accepted,
via <a href="https://sqlite.org/forum/"><strong>the sqlite forum</strong></a>. Those with <a href="https://sqlite.org/prosupport.html">commercial SQLite
support contracts</a> may use their usual
support channels.</li>
</ul>

<p><strong>Making use of this project:</strong></p>

<ul>
<li><p>👣 The <strong><a href="https://sqlite.org/wasm/doc/trunk/demo-123.md">three-step HOWTO</a></strong> demonstrates how to
include and run the sqlite3 WASM module and its associated
JavaScript APIs.</p></li>
<li><p>💾 <strong><a href="https://sqlite.org/download.html">Downloads</a></strong> are available via
the main project downloads page.</p>

<ul>
<li>📸 <strong><a href="https://sqlite.org/wasm/uv/snapshot.html">Prerelease snapshots</a></strong> are
updated from time to time</li>
<li>📦 <strong><a href="https://sqlite.org/wasm/doc/trunk/npm.md">npm module</a></strong></li>
</ul></li>
<li><p>🛠 <a href="https://sqlite.org/wasm/doc/trunk/building.md"><strong>Building sqlite3 WASM</strong></a> and its associated JS
code.</p>

<ul>
<li><a href="https://sqlite.org/wasm/doc/trunk/emscripten.md">Emscripten</a> build specifics</li>
</ul></li>
<li><p>📇 <a href="https://sqlite.org/wasm/doc/trunk/api-index.md">The <strong>API index</strong></a> describes the various API
variants and how to load and access them.</p>

<ul>
<li>🧑‍🍳 <a href="https://sqlite.org/wasm/doc/trunk/cookbook.md"><strong>Cookbook</strong></a> of recipes for client-level code</li>
<li>💾 <a href="https://sqlite.org/wasm/doc/trunk/persistence.md"><strong>Persistent storage</strong> options</a></li>
<li>🤓 Using <a href="https://sqlite.org/wasm/doc/trunk/c-structs.md"><strong>C-structs</strong></a> in the JS API</li>
<li>🧱 Creating <a href="https://sqlite.org/wasm/doc/trunk/vtab.md"><strong>virtual tables</strong></a> and table-valued
functions in JS</li>
<li>💣 <a href="https://sqlite.org/wasm/doc/trunk/api-changes.md"><strong>API changes</strong></a> which <em>might</em> affect
clients</li>
<li>🔭 The <a href="https://sqlite.org/wasm/doc/trunk/module-symbols.html"><strong>module symbols app</strong></a>
generates lists of sqlite3 API symbols from the JavaScript/WASM module.</li>
</ul></li>
</ul>

<p><strong>About this documentation:</strong></p>

<ul>
<li>📜 <a href="https://sqlite.org/wasm/doc/trunk/license.md"><strong>License</strong></a></li>
<li>🚧 <a href="https://sqlite.org/wasm/doc/trunk/doc-maintenance.md"><strong>Doc and repository maintenance</strong></a></li>
</ul>



<h2>In the Wild</h2>
<p>Third-party projects known to be using this project include (in order
of their addition to this list)...</p>

<ul>
<li><a href="https://github.com/nalgeon/sqlime">SQLime</a> provides a database
browser interface.</li>
<li><a href="https://github.com/evoluhq/evolu">Evolu</a> Evolu is a local-first platform
designed for privacy, ease of use, and no vendor lock-in.</li>
<li><a href="https://github.com/mandolyte/sqlitenext">SQLiteNext</a> provides a demo of
integrating this project with next.js.</li>
<li><a href="https://github.com/overtone-app/sqlite-wasm-esm">sqlite-wasm-esm</a> demonstrates
how to use this project with the Vite toolchain.</li>
<li><a href="https://www.npmjs.com/package/sqlite-wasm-http">sqlite-wasm-http</a>
provides an SQLite VFS with read-only access to databases which are
served directly over HTTP.</li>
</ul>



<h2>Related Works</h2>
<p>(In the order in which we became aware of them...)</p>

<ul>
<li>Alon Zakai's <a href="https://github.com/sql-js/sql.js"><strong>sql.js</strong></a> is the
first known direct usage of sqlite3 in a web browser, <a href="https://github.com/kripken/sql.js/commit/cebd80648dbd369b34804c5a00b4d0bddc1cbf05">dating back to
2012</a>,
not counting WebSQL (which was a native-level feature and has long
since been removed from most browsers).</li>
<li>Roy Hashimoto's
<a href="https://github.com/rhashimoto/wa-sqlite"><strong>wa-sqlite</strong></a> is home to
the first known implementation of <a href="https://sqlite.org/wasm/doc/trunk/persistence.md">OPFS storage</a>
of sqlite3 databases.</li>
<li>James Long's
<a href="https://github.com/jlongster/absurd-sql"><strong>absurd-js</strong></a>
demonstrates storing sqlite3 databases inside IndexedDB databases.</li>
<li><a href="https://supabase.com/blog/postgres-wasm"><strong>postgres-wasm</strong></a> runs a
<a href="https://www.postgresql.org/">Postgres database server</a> in a browser.</li>
<li><a href="https://fossil.wanderinghorse.net/r/jaccwabyt"><strong>Jaccwabyt</strong></a> is a
small JS library for manipulating WASM-hosted C structs via JS code,
created specifically to support the <a href="https://sqlite.org/wasm/doc/trunk/persistence.md#vfs-opfs">OPFS
sqlite3_vfs</a> implementation in this
project. This project embeds a copy but does not expose it to client
applications.</li>
<li><a href="https://github.com/sagemathinc/cowasm"><strong>CoWasm</strong></a> is
"Collaborative WebAssembly for Servers and Browsers". <a href="https://cowasm.sh/">Their demo
app</a> includes a WASM build of the sqlite3 shell
application.</li>
<li><a href="https://sqlite.org/forum/forumpost/ca6139791c"><strong>Evan Brass's
build</strong></a> uses a <a href="https://github.com/WebAssembly/wasi-sdk">WASI
SDK</a> build, instead of Emscripten, and demonstrates some
novel features which this project's WASM build does not.</li>
</ul>

<h2>Third-party Documentation and Articles</h2>
<p>The following links reference articles and documentation published
about SQLite WASM by third parties:</p>

<!-- , listed in order of their addition here: -->

<ul>
<li><a href="https://developer.chrome.com/blog/from-web-sql-to-sqlite-wasm/">Porting WebSQL to OPFS</a>,
by the Google Chrome dev team</li>
<li>The VMWare OCTO team writes about <a href="https://wasmlabs.dev/articles/sqlite-wasi-support/">building SQLite3 for
WASI</a></li>
</ul>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The C23 edition of Modern C is now available for free (217 pts)]]></title>
            <link>https://gustedt.wordpress.com/2024/10/15/the-c23-edition-of-modern-c/</link>
            <guid>41850017</guid>
            <pubDate>Tue, 15 Oct 2024 16:06:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gustedt.wordpress.com/2024/10/15/the-c23-edition-of-modern-c/">https://gustedt.wordpress.com/2024/10/15/the-c23-edition-of-modern-c/</a>, See on <a href="https://news.ycombinator.com/item?id=41850017">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
		<p><a href="#content">
			Skip to content		</a></p><!-- .site-header -->

		<div id="content">
	<main id="main">
		
<article id="post-4215">
	<!-- .entry-header -->

	
	
	<div>
		
<p>The C23 edition of Modern C is now available for free download from</p>



<p><a href="https://hal.inria.fr/hal-02383654">https://hal.inria.fr/hal-02383654</a></p>



<div><p>This new edition has been the occasion to overhaul the presentation in many places, but its main purpose is the update to the new C standard, <a href="https://www.iso.org/standard/82075.html">C23</a>. The goal was to publish this new edition of Modern C at the same time as the new C standard goes through the procedure of ISO publication. The closest approximation of the contents of the new standard in a publically available document can be found <a href="https://www.open-std.org/JTC1/SC22/WG14/www/docs/n3220.pdf">here</a>. New releases of major compilers already implement most of the new features that it brings.</p>
<p>Among the most noticeable changes and additions that we handle are those for integers: there are new bit-precise types coined <code>_BitInt(N)</code>, new C library headers <code>(for arithmetic with overflow check) and</code> (for bit manipulation), possibilities for 128 bit types on modern architectures, and substantial improvements for enumeration types. Other new concepts in C23 include a <code>nullptr</code> constant and its underlying type, syntactic annotation with attributes, more tools for type generic programming such as type inference with <code>auto</code> and <code>typeof</code>, default initialization with <code>{}</code>, even for variable length arrays, and <code>constexpr</code> for named constants of any type. Furthermore, new material has been added, discussing compound expressions and lambdas, so-called “internationalization”, a comprehensive approach for program failure.</p>
<p>Also added has been an appendix and a temporary include header for an easy transition to C23 on existing platforms, that will allow you to start off with C23 right away.</p>
<p>Manning’s early access program (MEAP) for the new edition is still open at</p>
<p><a href="https://www.manning.com/books/modern-c-third-edition" rel="nofollow">https://www.manning.com/books/modern-c-third-edition</a></p>
<p>Unfortunately they were not yet able to tell me when their version of the C23 edition will finally be published.</p>
</div>





			
				</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article><!-- #post-4215 -->

	<nav aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>
	</main><!-- .site-main -->

	
</div><!-- .site-content -->

		<!-- .site-footer -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mothbox 4.0 (132 pts)]]></title>
            <link>https://digital-naturalism-laboratories.github.io/Mothbox/</link>
            <guid>41848804</guid>
            <pubDate>Tue, 15 Oct 2024 14:10:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://digital-naturalism-laboratories.github.io/Mothbox/">https://digital-naturalism-laboratories.github.io/Mothbox/</a>, See on <a href="https://news.ycombinator.com/item?id=41848804">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content"><main><h2 id="mothbox-40"> <a href="#mothbox-40" aria-labelledby="mothbox-40"></a> Mothbox 4.0</h2><p>The Mothbox is a low-cost, <a href="https://digital-naturalism-laboratories.github.io/Mothbox/docs/about/specs/">high-performance insect monitor</a>. It features a power efficient and lightweight design meant to help field biologists deploy it in the depths of the jungles, and its low-cost nature means you can build one to study the biodiversity at your home!</p><p>All the physical designs, electronics schematics, Pi Scripts, and insect-IDing Artificial Intelligence are <strong>provided free and open source</strong>, so you can build, share and improve on these designs yourself!</p><p>See the <a href="https://digital-naturalism-laboratories.github.io/Mothbox/docs/about/specs/">full specifications of what it can do here.</a></p><p><img src="https://github.com/user-attachments/assets/cf7da6c8-2a7d-40a8-8872-6f9987c43082" alt="PXL_20240720_054408351 MP-EDIT"></p><h2 id="why-study-insects-for-conservation"> <a href="#why-study-insects-for-conservation" aria-labelledby="why-study-insects-for-conservation"></a> Why Study Insects for Conservation?</h2><p>Insect populations can be used as an ultra high resolution sensor for changes in environments.</p><h3 id="insects-especially-moths-and-beetles-are-hyperdiverse"> <a href="#insects-especially-moths-and-beetles-are-hyperdiverse" aria-labelledby="insects-especially-moths-and-beetles-are-hyperdiverse"></a> Insects (Especially Moths and Beetles) are Hyperdiverse</h3><p>Of all life on earth (including bacteria) there are about 2 million species scientists have described and <a href="https://ourworldindata.org/how-many-species-are-there#:~:text=In%20the%20chart%2C%20we%20see,reptiles%2C%20and%20over%206%2C000%20mammals.">half of the species are insects!</a> What’s more, if you look at <strong>just Moths</strong> there are about 144,000 species, meaning <strong>about 1 in every 14 species in the world is a moth!</strong> For reference there are only about 11,000 bird species, and only 6500 species of mammals (and half are bats and rodents).</p><p>Using creatures with a super diverse gene pool allows us to make sophisticated insights into the health of different areas. Combining the activity of thousands of species of insects with other data like climate, acoustic, or soil analysis, can provide deep analysis into how environments can be repaired.</p><h3 id="offer-highly-localized-insights"> <a href="#offer-highly-localized-insights" aria-labelledby="offer-highly-localized-insights"></a> Offer Highly Localized Insights</h3><p>Because they tend to have short lives and often limited ranges, insects can provide super localized data. Moths, in particular, are great indicators of overall biodiversity because they are often host-specific as caterpillars. This means they only feed on one or a limited number of plant species. So, by monitoring moths, you are getting a proxy of the local plant community.</p><p>For comparison, think about something like a Jaguar, which is a rare find and scientifically valuable to see on a wildlife camera, but they have large ranges and long lives. The presence of a Jaguar does not necessarily tell us much about the health of the environment where it was spotted. It could simply be travelling across disturbed farmland to escape from a destroyed habitat elsewhere.</p><p>Having an insect sensor (like a Mothbox), however, that could compare the activity of thousands of different species of insects can highlight differences in environmental health in areas just a couple kilometers apart.</p><h3 id="they-are-easy-to-attract"> <a href="#they-are-easy-to-attract" aria-labelledby="they-are-easy-to-attract"></a> They are Easy to Attract</h3><p>Humans have long ago discovered that many nocturnal insects like Moths seem to be attracted to lights. We now know this is because <a href="https://www.nature.com/articles/s41467-024-44785-3">bright lights disorient their natural steering mechanism</a>. Scientists have used bright artificial lights for over a century now to take censuses of insect populations. The problem with this type of “Mothlighting” is that it can be incredibly time consuming (a scientist has to hang out all night and note which insects are visiting). Instead we have an automated device that does all this work for you!</p><h2 id="what-it-does"> <a href="#what-it-does" aria-labelledby="what-it-does"></a> What it Does</h2><p>The Mothbox stays in an ultra low-power state until your schedule tells it which nights to wake up. Then it triggers an insect lure (usually a bright UV light) and then takes ultra-high resolution photos of the visitors it attracts. <img src="https://github.com/user-attachments/assets/cb7f6a03-849b-4b2a-99b9-9580aa245816" alt="PXL_20240607_025924783 NIGHT-EDIT"></p><p>Next we created some open-source AI scripts that process these insects for you. First, a trained YOLO v8 detects where all the insects are in the images and crops them out.</p><p><img src="https://github.com/Digital-Naturalism-Laboratories/Mothbox/assets/742627/ec1a50ce-38bf-4bb3-b8b6-752ba1801050" width="48%"> <img src="https://github.com/user-attachments/assets/3d0936ce-e89c-411a-8529-a932acc6e9c8" width="48%"></p><p>Then we use a special version of BioCLIP with a user interface to help you automatically group and ID the insects to different taxonomic levels! <img src="https://github.com/user-attachments/assets/93728753-8a70-4686-b493-1e3de177627e" alt="image"></p><h2 id="build-it-yourself"> <a href="#build-it-yourself" aria-labelledby="build-it-yourself"></a> Build it Yourself!</h2><p><img src="https://github.com/user-attachments/assets/f37ec4d5-4761-4eab-b0ae-179b05b3d1cf" alt="PXL_20240810_155421647 MP"></p><p>This Mothbox documentation will provide you with documentation for how to source, build, program, and use your mothbox!</p><p><a href="https://digital-naturalism-laboratories.github.io/Mothbox/docs/building">Get started building your mothbox!</a></p><p>After following these guides, you should be able to make your own set of mothboxes to conduct your own biodiversity studies!</p><p><img src="https://github.com/Digital-Naturalism-Laboratories/Mothbox/assets/742627/2e1cacf2-35dd-48b0-83c7-29b5320fa36c" width="45%"> <img src="https://github.com/user-attachments/assets/2416c098-b080-4014-b972-3acb9e366aa6" width="45%"></p><h2 id="mothbeam"> <a href="#mothbeam" aria-labelledby="mothbeam"></a> Mothbeam</h2><p>We are also building an open-source, portable low cost light for mothlighting, the Mothbeam! Some early documentation for <a href="https://digital-naturalism-laboratories.github.io/Mothbox/docs/building/attractor/#internal-mothbeam">making your own Mothbeam is here.</a> <img src="https://github.com/Digital-Naturalism-Laboratories/Mothbox/assets/742627/fab3c9ac-f879-4768-abee-1e61d8d63172" alt="Untitled"></p><p><img src="https://github.com/user-attachments/assets/5201d9bb-20fc-43ae-8e99-69f6ac24126e" alt="PXL_20240718_190826331 MP"></p></main><hr></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I built the most over-engineered Deal With It emoji generator (472 pts)]]></title>
            <link>https://emoji.build/deal-with-it-generator/</link>
            <guid>41848150</guid>
            <pubDate>Tue, 15 Oct 2024 13:05:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://emoji.build/deal-with-it-generator/">https://emoji.build/deal-with-it-generator/</a>, See on <a href="https://news.ycombinator.com/item?id=41848150">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I 3D scanned the tunnels inside the Maya Pyramid Temples at Copan (105 pts)]]></title>
            <link>https://mused.com/guided/158/temple-26-and-excavation-tunnels-copan-ruinas/</link>
            <guid>41848099</guid>
            <pubDate>Tue, 15 Oct 2024 12:57:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mused.com/guided/158/temple-26-and-excavation-tunnels-copan-ruinas/">https://mused.com/guided/158/temple-26-and-excavation-tunnels-copan-ruinas/</a>, See on <a href="https://news.ycombinator.com/item?id=41848099">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-josh-anim-name="fadeIn" data-josh-delay="300ms"><p>Since the 1930’s, archaeologists have tunneled into the acropolis at Copan to understand the many phases of construction throughout its history. </p>
<p>With investigations now mostly complete, the tunnels measure close to 4 kilometers in cumulative length, and have uncovered early stelae, plaster facades and tombs that have taught us much about what the acropolis was like before its final phase. </p>
<p>A Harvard-led conservation plan in on-going collaboration with the Instituto Hondureño de Antropología e Historia for these tunnels created a digital 3D model of the tunnel system and addresses ways to preserve the valuable architectural heritage within, taking into account the humid climate, seasonal changes to the water table, visitor access, and risk of collapse.</p>
<p>U</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Mermaid ASCII Diagrams (122 pts)]]></title>
            <link>https://mermaid-ascii.art/</link>
            <guid>41847407</guid>
            <pubDate>Tue, 15 Oct 2024 11:30:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mermaid-ascii.art/">https://mermaid-ascii.art/</a>, See on <a href="https://news.ycombinator.com/item?id=41847407">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <form id="input" hx-post="/generate" hx-target="#result" hx-trigger="click from:.example-buttons">
                
                <div>
                        <p><label for="xPadding">xPadding:</label></p><p>3</p>
                    </div>
                <div>
                        <p><label for="yPadding">yPadding:</label></p><p>3</p>
                    </div>
                
            </form>
            
            <hr>
            <p>There are alternative forms of generating mermaid graphs in ASCII using this same Git repository.</p>
            <h2>cUrl request with data</h2>
            <div>
                <pre><code>curl https://mermaid-ascii.art/generate -d mermaid="graph LR\nABC --&gt; DEF"</code></pre>
            </div>
            <h2>Run Golang in CLI</h2>
            <div>
                <pre><code>git clone https://github.com/AlexanderGrooff/mermaid-ascii
cd mermaid-ascii
go run main.go -f test.mermaid</code></pre>
            </div>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Web Browser Engineering (587 pts)]]></title>
            <link>https://browser.engineering/index.html</link>
            <guid>41846780</guid>
            <pubDate>Tue, 15 Oct 2024 09:42:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://browser.engineering/index.html">https://browser.engineering/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=41846780">Hacker News</a></p>
<div id="readability-page-1" class="page">


<header>


<a href="https://twitter.com/browserbook">Twitter</a> ·
<a href="https://browserbook.substack.com/">Blog</a> ·
<a href="https://patreon.com/browserengineering">Patreon</a> ·
<a href="https://github.com/browserengineering/book/discussions">Discussions</a>
</header>




<nav id="toc">
<ul>
<li><a href="#introduction" id="toc-introduction">Introduction</a></li>
<li><a href="#part-1-loading-pages" id="toc-part-1-loading-pages">Part
1: Loading Pages</a></li>
<li><a href="#part-2-viewing-documents" id="toc-part-2-viewing-documents">Part 2: Viewing Documents</a></li>
<li><a href="#part-3-running-applications" id="toc-part-3-running-applications">Part 3: Running
Applications</a></li>
<li><a href="#part-4-modern-browsers" id="toc-part-4-modern-browsers">Part 4: Modern Browsers</a></li>
</ul>
</nav>

<p>Web browsers are ubiquitous, but how do they work? This book
explains, building a basic but complete web browser, from networking to
JavaScript, in a couple thousand lines of Python.</p>

<div>
<figure>
<img src="https://browser.engineering/im/cover.jpg" alt="The cover for Web Browser Engineering, from Oxford University Press">

</figure>
<section id="pre-order-web-browser-engineering">
<h2>Pre-order <em>Web Browser Engineering</em></h2>
<p><em>Web Browser Engineering</em> will be published by Oxford
University Press before the end of the year. To get it as soon as it’s
out, <a href="https://global.oup.com/academic/product/web-browser-engineering-9780198913863">pre-order
now!</a></p>
</section>
</div>
<p>Follow this book’s <a href="https://browserbook.substack.com/archive">blog</a> or <a href="https://twitter.com/browserbook">Twitter</a> for updates. You can
also talk about the book with others in our <a href="https://github.com/browserengineering/book/discussions">discussion
forum</a>.</p>
<p>If you are enjoying the book, consider supporting us on <a href="https://patreon.com/browserengineering">Patreon</a>.</p>
<p>Or just <a href="mailto:author@browser.engineering">send us an
email</a>!</p>
<section id="introduction">
<h2>Introduction</h2>
<ol type="1">
<li><a href="https://browser.engineering/preface.html">Preface</a></li>
<li><a href="https://browser.engineering/intro.html">Browsers and the Web</a></li>
<li><a href="https://browser.engineering/history.html">History of the Web</a></li>
</ol>
</section>
<h2 id="part-1-loading-pages">Part 1: Loading Pages</h2>
<ol type="1">
<li><a href="https://browser.engineering/http.html">Downloading Web Pages</a><br>
URLs and HTTP requests</li>
<li><a href="https://browser.engineering/graphics.html">Drawing to the Screen</a><br>
Creating windows and drawing to a canvas</li>
<li><a href="https://browser.engineering/text.html">Formatting Text</a><br>
Word wrapping and line spacing</li>
</ol>
<h2 id="part-2-viewing-documents">Part 2: Viewing Documents</h2>
<ol start="4" type="1">
<li><a href="https://browser.engineering/html.html">Constructing an HTML Tree</a><br>
Parsing and fixing HTML</li>
<li><a href="https://browser.engineering/layout.html">Laying Out Pages</a><br>
Inline and block layout</li>
<li><a href="https://browser.engineering/styles.html">Applying Author Styles</a><br>
Parsing and applying CSS</li>
<li><a href="https://browser.engineering/chrome.html">Handling Buttons and Links</a><br>
Hyperlinks and browser chrome</li>
</ol>
<h2 id="part-3-running-applications">Part 3: Running Applications</h2>
<ol start="8" type="1">
<li><a href="https://browser.engineering/forms.html">Sending Information to Servers</a><br>
Form submission and web servers</li>
<li><a href="https://browser.engineering/scripts.html">Running Interactive Scripts</a><br>
Changing the DOM and reacting to events</li>
<li><a href="https://browser.engineering/security.html">Keeping Data Private</a><br>
Cookies and logins, XSS and CSRF</li>
</ol>
<h2 id="part-4-modern-browsers">Part 4: Modern Browsers</h2>
<ol start="11" type="1">
<li><a href="https://browser.engineering/visual-effects.html">Adding Visual Effects</a><br>
Blending, clipping, and compositing</li>
<li><a href="https://browser.engineering/scheduling.html">Scheduling Tasks and Threads</a><br>
The event loop and the rendering pipeline</li>
<li><a href="https://browser.engineering/animations.html">Animating and Compositing</a><br>
Smooth animations using the GPU</li>
<li><a href="https://browser.engineering/accessibility.html">Making Content Accessible</a><br>
Keyboard input, zooming, and the accessibility tree</li>
<li><a href="https://browser.engineering/embeds.html">Supporting Embedded Content</a><br>
Images, iframes, and scripting</li>
<li><a href="https://browser.engineering/invalidation.html">Reusing Previous Computation</a><br>
Invalidation, editing, and correctness</li>
</ol>













</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Pumpkin – A Modern Minecraft server written in Rust (224 pts)]]></title>
            <link>https://github.com/Snowiiii/Pumpkin</link>
            <guid>41846636</guid>
            <pubDate>Tue, 15 Oct 2024 09:18:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Snowiiii/Pumpkin">https://github.com/Snowiiii/Pumpkin</a>, See on <a href="https://news.ycombinator.com/item?id=41846636">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<p dir="auto"><h2 tabindex="-1" dir="auto">Pumpkin</h2><a id="user-content-pumpkin" aria-label="Permalink: Pumpkin" href="#pumpkin"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Snowiiii/Pumpkin/actions/workflows/rust.yml/badge.svg"><img src="https://github.com/Snowiiii/Pumpkin/actions/workflows/rust.yml/badge.svg" alt="CI"></a>
<a href="https://discord.gg/wT8XjrjKkf" rel="nofollow"><img src="https://camo.githubusercontent.com/044987ec7a0aa910e2c5480a2fce200ff9fa8269a4a2533a2a5c87e06ba3a866/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f313236383539323333373434353937383139332e7376673f6c6162656c3d266c6f676f3d646973636f7264266c6f676f436f6c6f723d66666666666626636f6c6f723d373338394438266c6162656c436f6c6f723d364137454332" alt="Discord" data-canonical-src="https://img.shields.io/discord/1268592337445978193.svg?label=&amp;logo=discord&amp;logoColor=ffffff&amp;color=7389D8&amp;labelColor=6A7EC2"></a>
<a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/6cd0120cc4c5ac11d28b2c60f76033b52db98dac641de3b2644bb054b449d60c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/dc0c67e5ec43ed955dd30f906d640e079aecd82f8be64ef1d7153ca8e6afc894/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f63757272656e745f76657273696f6e2d312e32312e312d626c7565"><img src="https://camo.githubusercontent.com/dc0c67e5ec43ed955dd30f906d640e079aecd82f8be64ef1d7153ca8e6afc894/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f63757272656e745f76657273696f6e2d312e32312e312d626c7565" alt="Current version)" data-canonical-src="https://img.shields.io/badge/current_version-1.21.1-blue"></a></p>
</div>
<p dir="auto"><a href="https://snowiiii.github.io/Pumpkin/" rel="nofollow">Pumpkin</a> is a Minecraft server built entirely in Rust, offering a fast, efficient,
and customizable experience. It prioritizes performance and player enjoyment while adhering to the core mechanics of the game.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/71594357/357392192-7e2e865e-b150-4675-a2d5-b52f9900378e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjkwMDY1MDIsIm5iZiI6MTcyOTAwNjIwMiwicGF0aCI6Ii83MTU5NDM1Ny8zNTczOTIxOTItN2UyZTg2NWUtYjE1MC00Njc1LWEyZDUtYjUyZjk5MDAzNzhlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDE1VDE1MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI0Nzc3NzBlYzI3NzQwMjg0MzJlZjY2YmM4YzBmOGFkZDY0YjExZTFiYzNhMDRjNThkMzg3MGMyOWUzODlmMDQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.mLiSktRWZz3v9ppfunfxn0kkOzFN9jmIRh5VoXxUJs4"><img src="https://private-user-images.githubusercontent.com/71594357/357392192-7e2e865e-b150-4675-a2d5-b52f9900378e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjkwMDY1MDIsIm5iZiI6MTcyOTAwNjIwMiwicGF0aCI6Ii83MTU5NDM1Ny8zNTczOTIxOTItN2UyZTg2NWUtYjE1MC00Njc1LWEyZDUtYjUyZjk5MDAzNzhlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDE1VDE1MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI0Nzc3NzBlYzI3NzQwMjg0MzJlZjY2YmM4YzBmOGFkZDY0YjExZTFiYzNhMDRjNThkMzg3MGMyOWUzODlmMDQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.mLiSktRWZz3v9ppfunfxn0kkOzFN9jmIRh5VoXxUJs4" alt="image"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What Pumpkin wants to achieve</h2><a id="user-content-what-pumpkin-wants-to-achieve" aria-label="Permalink: What Pumpkin wants to achieve" href="#what-pumpkin-wants-to-achieve"></a></p>
<ul dir="auto">
<li><strong>Performance</strong>: Leveraging multi-threading for maximum speed and efficiency.</li>
<li><strong>Compatibility</strong>: Supports the latest Minecraft server version and adheres to vanilla game mechanics.</li>
<li><strong>Security</strong>: Prioritizes security by preventing known exploits.</li>
<li><strong>Flexibility</strong>: Highly configurable with the ability to disable unnecessary features.</li>
<li><strong>Extensibility</strong>: Provides a foundation for plugin development.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">What Pumpkin will not</h2><a id="user-content-what-pumpkin-will-not" aria-label="Permalink: What Pumpkin will not" href="#what-pumpkin-will-not"></a></p>
<ul dir="auto">
<li>Be a drop-in replacement for vanilla or other servers</li>
<li>Be compatible with plugins or mods for other servers</li>
<li>Function as a framework for building a server from scratch.</li>
</ul>
<div dir="auto"><p dir="auto">Important</p><p dir="auto">Pumpkin is currently under heavy development.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features (WIP)</h2><a id="user-content-features-wip" aria-label="Permalink: Features (WIP)" href="#features-wip"></a></p>
<ul>
<li> Configuration (toml)</li>
<li> Server Status/Ping</li>
<li> Login</li>
<li>Player Configuration
<ul>
<li> Registries (biome types, paintings, dimensions)</li>
<li> Server Brand</li>
<li> Server Links</li>
<li> Set Resource Pack</li>
<li> Cookies</li>
</ul>
</li>
<li>World
<ul>
<li> World Joining</li>
<li> Player Tab-list</li>
<li> World Loading</li>
<li> Entity Spawning</li>
<li> Chunk Loading</li>
<li> World Generation</li>
<li> Chunk Generation</li>
<li> World Borders</li>
<li> World Saving</li>
</ul>
</li>
<li>Player
<ul>
<li> Player Skins</li>
<li> Player Client brand</li>
<li> Player Teleport</li>
<li> Player Movement</li>
<li> Player Animation</li>
<li> Player Inventory</li>
<li> Player Combat</li>
</ul>
</li>
<li>Server
<ul>
<li> Plugins</li>
<li> Query</li>
<li> RCON</li>
<li> Inventories</li>
<li> Particles</li>
<li> Chat</li>
<li> Commands</li>
</ul>
</li>
<li>Proxy
<ul>
<li> Velocity</li>
</ul>
</li>
</ul>
<p dir="auto">Check out our <a href="https://github.com/users/Snowiiii/projects/12/views/3">Github Project</a> to see current progress</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to run</h2><a id="user-content-how-to-run" aria-label="Permalink: How to run" href="#how-to-run"></a></p>
<p dir="auto">See <a href="https://snowiiii.github.io/Pumpkin/about/quick-start.html" rel="nofollow">https://snowiiii.github.io/Pumpkin/about/quick-start.html</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributions</h2><a id="user-content-contributions" aria-label="Permalink: Contributions" href="#contributions"></a></p>
<p dir="auto">Contributions are welcome! See <a href="https://github.com/Snowiiii/Pumpkin/blob/master/CONTRIBUTING.md">CONTRIBUTING.md</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Docs</h2><a id="user-content-docs" aria-label="Permalink: Docs" href="#docs"></a></p>
<p dir="auto">The Documentation of Pumpkin can be found at <a href="https://snowiiii.github.io/Pumpkin/" rel="nofollow">https://snowiiii.github.io/Pumpkin/</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Communication</h2><a id="user-content-communication" aria-label="Permalink: Communication" href="#communication"></a></p>
<p dir="auto">Consider joining our <a href="https://discord.gg/wT8XjrjKkf" rel="nofollow">discord</a> to stay up-to-date on events, updates, and connect with other members.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Funding</h2><a id="user-content-funding" aria-label="Permalink: Funding" href="#funding"></a></p>
<p dir="auto">If you want to fund me and help the project, Check out my <a href="https://github.com/sponsors/Snowiiii">GitHub sponsors</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Thanks</h2><a id="user-content-thanks" aria-label="Permalink: Thanks" href="#thanks"></a></p>
<p dir="auto">A big thanks to <a href="https://wiki.vg/" rel="nofollow">wiki.vg</a> for providing valuable information used in the development of this project.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: MynaUI Icons – 1180 Beautifully Crafted Open Source Icons (102 pts)]]></title>
            <link>https://mynaui.com/icons</link>
            <guid>41846539</guid>
            <pubDate>Tue, 15 Oct 2024 09:03:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mynaui.com/icons">https://mynaui.com/icons</a>, See on <a href="https://news.ycombinator.com/item?id=41846539">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Not affiliated with Figma, TailwindCSS or shadcn/ui.</p><nav><a href="https://mynaui.com/legal">Legal</a><a target="_blank" rel="noreferrer noopener" title="X (Twitter)" href="https://twitter.com/praveenjuge/"><svg width="24" height="24" fill="none" stroke="currentColor" stroke-width="1.5" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg"><path d="m19 4-5.93 6.93M5 20l5.93-6.93m0 0 5.795 6.587c.19.216.483.343.794.343h1.474c.836 0 1.307-.85.793-1.435L13.07 10.93m-2.14 2.14L4.214 5.435C3.7 4.85 4.17 4 5.007 4h1.474c.31 0 .604.127.794.343l5.795 6.587"></path></svg></a><a target="_blank" rel="noreferrer noopener" title="Dribbble" href="https://dribbble.com/praveenjuge/"><svg width="24" height="24" fill="none" stroke="currentColor" stroke-width="1.5" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg"><path d="M12 21a9 9 0 1 0 0-18 9 9 0 0 0 0 18"></path><path d="M3.07 10.875c1.7.102 6.2.195 9.08-1.035s5.358-3.492 6.208-4.21"></path><path d="M8.625 3.654c1.409 1.3 4.482 4.61 5.625 7.896 1.143 3.286 1.566 7.326 1.827 8.476"></path><path d="M21 12c-1.313 0-4.936-.495-8.178.928-3.522 1.547-6.072 3.946-7.184 5.438"></path></svg></a></nav></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Drawing Graphics on Apple Vision with the Metal Rendering API (127 pts)]]></title>
            <link>https://github.com/gnikoloff/drawing-graphics-on-apple-vision-with-metal-rendering-api</link>
            <guid>41845646</guid>
            <pubDate>Tue, 15 Oct 2024 06:44:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/gnikoloff/drawing-graphics-on-apple-vision-with-metal-rendering-api">https://github.com/gnikoloff/drawing-graphics-on-apple-vision-with-metal-rendering-api</a>, See on <a href="https://news.ycombinator.com/item?id=41845646">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Drawing Graphics on Apple Vision with the Metal Rendering API</h2><a id="user-content-drawing-graphics-on-apple-vision-with-the-metal-rendering-api" aria-label="Permalink: Drawing Graphics on Apple Vision with the Metal Rendering API" href="#drawing-graphics-on-apple-vision-with-the-metal-rendering-api"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ol dir="auto">
<li><a href="#introduction">Introduction</a>
<ol dir="auto">
<li><a href="#why-write-this-article">Why Write This Article?</a></li>
<li><a href="#metal">Metal</a></li>
<li><a href="#compositor-services">Compositor Services</a></li>
</ol>
</li>
<li><a href="#creating-and-configuring-a-layerrenderer">Creating and configuring a <code>LayerRenderer</code></a>
<ol dir="auto">
<li><a href="#variable-rate-rasterization-foveation">Variable Rate Rasterization (Foveation)</a></li>
<li><a href="#organising-the-metal-textures-used-for-presenting-the-rendered-content">Organising the Metal Textures Used for Presenting the Rendered Content</a></li>
</ol>
</li>
<li><a href="#vertex-amplification">Vertex Amplification</a>
<ol dir="auto">
<li><a href="#preparing-to-render-with-support-for-vertex-amplification">Preparing to Render with Support for Vertex Amplification</a></li>
<li><a href="#enabling-vertex-amplification-for-a-render-pass">Enabling Vertex Amplification for a Render Pass</a></li>
<li><a href="#specifying-the-viewport-mappings-for-both-render-targets">Specifying the Viewport Mappings for Both Render Targets</a></li>
<li><a href="#computing-the-view-and-projection-matrices-for-each-eye">Computing the View and Projection Matrices for Each Eye</a></li>
<li><a href="#adding-vertex-amplification-to-our-shaders">Adding Vertex Amplification to our Shaders</a>
<ol dir="auto">
<li><a href="#vertex-shader">Vertex Shader</a></li>
<li><a href="#fragment-shader">Fragment Shader</a></li>
</ol>
</li>
</ol>
</li>
<li><a href="#updating-and-encoding-a-frame-of-content">Updating and Encoding a Frame of Content</a>
<ol dir="auto">
<li><a href="#rendering-on-a-separate-thread">Rendering on a Separate Thread</a></li>
<li><a href="#fetching-a-next-frame-for-drawing">Fetching a Next Frame for Drawing</a></li>
<li><a href="#getting-predicted-render-deadlines">Getting Predicted Render Deadlines</a></li>
<li><a href="#updating-our-app-state-before-rendering">Updating Our App State Before Rendering</a></li>
<li><a href="#waiting-until-optimal-rendering-time">Waiting Until Optimal Rendering Time</a></li>
<li><a href="#frame-submission-phase">Frame Submission Phase</a></li>
</ol>
</li>
<li><a href="#supporting-both-stereoscopic-and-non-vr-display-rendering">Supporting Both Stereoscopic and non-VR Display Rendering</a>
<ol dir="auto">
<li><a href="#two-rendering-paths-layerrendererframedrawable-vs-mtkview">Two Rendering Paths. <code>LayerRenderer.Frame.Drawable</code> vs <code>MTKView</code></a></li>
<li><a href="#adapting-our-vertex-shader">Adapting our Vertex Shader</a></li>
</ol>
</li>
<li><a href="#gotchas">Gotchas</a>
<ol dir="auto">
<li><a href="#cant-render-to-a-smaller-resolution-pixel-buffer-when-foveation-is-enabled">Can't Render to a Smaller Resolution Pixel Buffer when Foveation is Enabled</a></li>
<li><a href="#postprocessing">Postprocessing</a></li>
<li><a href="#true-camera-position">True Camera Position</a></li>
<li><a href="#apple-vision-simulator">Apple Vision Simulator</a></li>
</ol>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto">At the time of writing, Apple Vision Pro has been available for seven months, with numerous games released and an increasing number of developers entering this niche. When it comes to rendering, most opt for established game engines like Unity or Apple's high-level APIs like RealityKit. However, there's another option that's been available since the beginning: building your own rendering engine using the Metal API. Though challenging, this approach offers full control over the rendering pipeline, down to each byte and command submitted to the GPU on each frame.</p>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong>: visionOS 2.0 enables rendering graphics with the Metal API and compositing them in <strong>mixed</strong> mode with the user’s surroundings, captured by the device's cameras. This article focuses on developing Metal apps for fully immersive mode, though passthrough rendering will be discussed at the end. At the time of Apple Vision Pro release, visionOS 1.0 allowed for rendering with the Metal API in <strong>immersive</strong> mode only.</p>
</blockquote>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why Write This Article?</h3><a id="user-content-why-write-this-article" aria-label="Permalink: Why Write This Article?" href="#why-write-this-article"></a></p>
<p dir="auto">Mainly as a recap to myself of all I have learned. I used all of this while building <a href="https://rayquestgame.com/" rel="nofollow">RAYQUEST</a>, my first game for Apple Vision. I am not going to present any groundbreaking techniques or anything that you can not find in Apple documentation and official examples, aside from some gotchas I discovered while developing my game. In fact, I'd treat this article as an additional reading to the Apple examples. Read them first or read this article first. I will link to Apple's relevant docs and examples as much as possible as I explain the upcomming concepts.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Metal</h3><a id="user-content-metal" aria-label="Permalink: Metal" href="#metal"></a></p>
<p dir="auto">To directly quote Apple <a href="https://developer.apple.com/metal/" rel="nofollow">docs</a>:</p>
<blockquote>
<p dir="auto">Metal is a modern, tightly integrated graphics and compute API coupled with a powerful shading language that is designed and optimized for Apple platforms. Its low-overhead model gives you direct control over each task the GPU performs, enabling you to maximize the efficiency of your graphics and compute software. Metal also includes an unparalleled suite of GPU profiling and debugging tools to help you improve performance and graphics quality.</p>
</blockquote>
<p dir="auto">I will not focus too much on the intristics of Metal in this article, however will mention that the API is mature, well documented and with plenty of tutorials and examples. I personally find it <strong>very nice</strong> to work with. If you want to learn it I suggest you read <a href="https://www.kodeco.com/books/metal-by-tutorials/v4.0" rel="nofollow">this book</a>. It is more explicit than APIs such as OpenGL ES, there is more planning involved in setting up your rendering pipeline and rendering frames, but is still very approachable and more beginner friendly then, say, Vulkan or DirectX12. Furthermore, Xcode has high quality built-in Metal profiler and debugger that allows for inspecting your GPU workloads and your shader inputs, code and outputs.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Compositor Services</h3><a id="user-content-compositor-services" aria-label="Permalink: Compositor Services" href="#compositor-services"></a></p>
<p dir="auto">Compositor Services is a visionOS-specific API that bridges your SwiftUI code with your Metal rendering engine. It enables you to encode and submit drawing commands directly to the Apple Vision displays, which include separate screens for the left and right eye.</p>
<p dir="auto">At the app’s initialization, Compositor Services automatically creates and configures a <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer" rel="nofollow"><code>LayerRenderer</code></a> object to manage rendering on Apple Vision during the app lifecycle. This configuration includes texture layouts, pixel formats, foveation settings, and other rendering options. If no custom configuration is provided, Compositor Services defaults to its standard settings. Additionally, the <code>LayerRenderer</code> supplies timing information to optimize the rendering loop and ensure efficient frame delivery.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Creating and configuring a <code>LayerRenderer</code></h2><a id="user-content-creating-and-configuring-a-layerrenderer" aria-label="Permalink: Creating and configuring a LayerRenderer" href="#creating-and-configuring-a-layerrenderer"></a></p>
<p dir="auto">In our scene creation code, we need to pass a type that adopts the <a href="https://developer.apple.com/documentation/compositorservices/compositorlayerconfiguration" rel="nofollow"><code>CompositorLayerConfiguration</code></a> protocol as a parameter to our scene content. The system will then use that configuration to create a <code>LayerRenderer</code> that will hold information such as the pixel formats of the final color and depth buffers, how the textures used to present the rendered content to Apple Vision displays are organised, whether foveation is enabled and so on. More on all these fancy terms a bit later. Here is some boilerplate code:</p>
<div dir="auto" data-snippet-clipboard-copy-content="struct ContentStageConfiguration: CompositorLayerConfiguration {
  func makeConfiguration(capabilities: LayerRenderer.Capabilities, configuration: inout LayerRenderer.Configuration) {
      // Specify the formats for both the color and depth output textures that Apple Vision will create for us.
      configuration.depthFormat = .depth32Float
      configuration.colorFormat = .bgra8Unorm_srgb

      // TODO: we will adjust the rest of the configuration further down in the article.
  }
}

@main
struct MyApp: App {
  var body: some Scene {
    WindowGroup {
      ContentView()
    }
    ImmersiveSpace(id: &quot;ImmersiveSpace&quot;) {
      CompositorLayer(configuration: ContentStageConfiguration()) { layerRenderer in
         // layerRenderer is what we will use for rendering, frame timing and other presentation info in our engine
      }
    }
  }
}"><pre><span>struct</span> <span>ContentStageConfiguration</span><span>:</span> <span>CompositorLayerConfiguration</span> <span>{</span>
  <span>func</span> makeConfiguration<span>(</span>capabilities<span>:</span> <span>LayerRenderer</span><span>.</span><span>Capabilities</span><span>,</span> configuration<span>:</span> <span>inout</span> <span>LayerRenderer</span><span>.</span><span>Configuration</span><span>)</span> <span>{</span>
      <span>// Specify the formats for both the color and depth output textures that Apple Vision will create for us.</span>
      configuration<span>.</span>depthFormat <span>=</span> <span>.</span>depth32Float
      configuration<span>.</span>colorFormat <span>=</span> <span>.</span>bgra8Unorm_srgb

      <span>// TODO: we will adjust the rest of the configuration further down in the article.</span>
  <span>}</span>
<span>}</span>

<span>@<span>main</span></span>
<span>struct</span> <span>MyApp</span><span>:</span> <span>App</span> <span>{</span>
  <span>var</span> <span><span>body</span></span><span>:</span> <span>some</span> <span>Scene</span> <span>{</span>
    <span>WindowGroup</span> <span>{</span>
      <span>ContentView</span><span>(</span><span>)</span>
    <span>}</span>
    <span>ImmersiveSpace</span><span>(</span>id<span>:</span> <span>"</span><span>ImmersiveSpace</span><span>"</span><span>)</span> <span>{</span>
      <span>CompositorLayer</span><span>(</span>configuration<span>:</span> <span>ContentStageConfiguration</span><span>(</span><span>)</span><span>)</span> <span>{</span> layerRenderer <span>in</span>
         <span>// layerRenderer is what we will use for rendering, frame timing and other presentation info in our engine</span>
      <span>}</span>
    <span>}</span>
  <span>}</span>
<span>}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Variable Rate Rasterization (Foveation)</h3><a id="user-content-variable-rate-rasterization-foveation" aria-label="Permalink: Variable Rate Rasterization (Foveation)" href="#variable-rate-rasterization-foveation"></a></p>
<p dir="auto">Next thing we need to set up is whether to enable support for <strong>foveation</strong> in <code>LayerRenderer</code>. Foveation allows us to render at a higher resolution the content our eyes gaze directly at and render at a lower resolution everything else. That is very beneficial in VR as it allows for improved performance.</p>
<p dir="auto">Apple Vision does eye-tracking and foveation automatically for us (in fact, it is not possible for developers to access the user's gaze <strong>at all</strong> due to security concerns). We need to setup our <code>LayerRenderer</code> to support it and we will get it "for free" during rendering. When we render to the <code>LayerRenderer</code> textures, Apple Vision will automatically adjust the resolution to be higher at the regions of the textures we directly gaze at. Here is the previous code that configures the <code>LayerRenderer</code>, updated with support for foveation:</p>
<div dir="auto" data-snippet-clipboard-copy-content="func makeConfiguration(capabilities: LayerRenderer.Capabilities, configuration: inout LayerRenderer.Configuration) {
   // ...

   // Enable foveation
   let foveationEnabled = capabilities.supportsFoveation
   configuration.isFoveationEnabled = foveationEnabled
}"><pre><span>func</span> makeConfiguration<span>(</span>capabilities<span>:</span> <span>LayerRenderer</span><span>.</span><span>Capabilities</span><span>,</span> configuration<span>:</span> <span>inout</span> <span>LayerRenderer</span><span>.</span><span>Configuration</span><span>)</span> <span>{</span>
   <span>// ...</span>

   <span>// Enable foveation</span>
   <span>let</span> <span>foveationEnabled</span> <span>=</span> capabilities<span>.</span>supportsFoveation
   configuration<span>.</span>isFoveationEnabled <span>=</span> foveationEnabled
<span>}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Organising the Metal Textures Used for Presenting the Rendered Content</h3><a id="user-content-organising-the-metal-textures-used-for-presenting-the-rendered-content" aria-label="Permalink: Organising the Metal Textures Used for Presenting the Rendered Content" href="#organising-the-metal-textures-used-for-presenting-the-rendered-content"></a></p>
<p dir="auto">We established we need to render our content as two views to both Apple Vision left and right displays. We have three options when it comes to the organization of the textures' layout we use for drawing:</p>
<ol dir="auto">
<li><a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/layout/dedicated" rel="nofollow"><code>LayerRenderer.Layout.dedicated</code></a> - A layout that assigns a separate texture to each rendered view. Two eyes - two textures.</li>
<li><a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/layout/shared" rel="nofollow"><code>LayerRenderer.Layout.shared</code></a> - A layout that uses a single texture to store the content for all rendered views. One texture big enough for both eyes.</li>
<li><a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/layout/layered" rel="nofollow"><code>LayerRenderer.Layout.layered</code></a> - A layout that specifies each view’s content as a slice of a single 3D texture with two slices.</li>
</ol>
<p dir="auto">Which one should you use? Apple official examples use <code>.layered</code>. Ideally <code>.shared</code> or <code>.layered</code>, as having one texture to manage results in fewer things to keep track of, less commands to submit and less GPU context switches. Some important to Apple Vision rendering techniques such as vertex amplification do not work with <code>.dedicated</code>, which expects a separate render pass to draw content for each eye texture, so it is best avoided.</p>
<p dir="auto">Let's update the configuration code once more:</p>
<div dir="auto" data-snippet-clipboard-copy-content="func makeConfiguration(capabilities: LayerRenderer.Capabilities, configuration: inout LayerRenderer.Configuration) {
   // ...

   // Set the LayerRenderer's texture layout configuration
   let options: LayerRenderer.Capabilities.SupportedLayoutsOptions = foveationEnabled ? [.foveationEnabled] : []
   let supportedLayouts = capabilities.supportedLayouts(options: options)

   configuration.layout = supportedLayouts.contains(.layered) ? .layered : .shared
}"><pre><span>func</span> makeConfiguration<span>(</span>capabilities<span>:</span> <span>LayerRenderer</span><span>.</span><span>Capabilities</span><span>,</span> configuration<span>:</span> <span>inout</span> <span>LayerRenderer</span><span>.</span><span>Configuration</span><span>)</span> <span>{</span>
   <span>// ...</span>

   <span>// Set the LayerRenderer's texture layout configuration</span>
   <span>let</span> <span>options</span><span>:</span> <span>LayerRenderer</span><span>.</span><span>Capabilities</span><span>.</span><span>SupportedLayoutsOptions</span> <span>=</span> foveationEnabled ? <span>[</span><span>.</span>foveationEnabled<span>]</span> <span>:</span> <span>[</span><span>]</span>
   <span>let</span> <span>supportedLayouts</span> <span>=</span> capabilities<span>.</span><span>supportedLayouts</span><span>(</span>options<span>:</span> options<span>)</span>

   configuration<span>.</span>layout <span>=</span> supportedLayouts<span>.</span><span>contains</span><span>(</span><span>.</span>layered<span>)</span> ? <span>.</span>layered <span>:</span> <span>.</span>shared
<span>}</span></pre></div>
<p dir="auto">That takes care of the basic configuration for <code>LayerRenderer</code> for rendering our content. We set up our textures' pixel formats, whether to enable foveation and the texture layout to use for rendering. Let's move on to rendering our content.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Vertex Amplification</h3><a id="user-content-vertex-amplification" aria-label="Permalink: Vertex Amplification" href="#vertex-amplification"></a></p>
<p dir="auto">Imagine we have a triangle we want rendered on Apple Vision. A triangle consists of 3 vertices. If we were to render it to a "normal" non-VR display we would submit 3 vertices to the GPU and let it draw them for us. On Apple Vision we have two displays. How do we go about it? A naive way would be to submit two drawing commands:</p>
<ol dir="auto">
<li>Issue draw command <strong>A</strong> to render 3 vertices to the left eye display.</li>
<li>Issue draw command <strong>B</strong> to render the same 3 vertices again, this time for the right eye display.</li>
</ol>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> Rendering everything twice and issuing double the amount of commands is required if you have chosen <code>.dedicated</code> texture layout when setting up your <code>LayerRenderer</code>.</p>
</blockquote>
<p dir="auto">This is not optimal as it doubles the commands needed to be submited to the GPU for rendering. A 3 vertices triangle is fine, but for more complex scenes with even moderate amounts of geometry it becomes unwieldly very fast. Thankfully, Metal allows us to submit the 3 vertices once for both displays via a technique called <strong>Vertex Amplification</strong>.</p>
<p dir="auto">Taken from this great <a href="https://developer.apple.com/documentation/metal/render_passes/improving_rendering_performance_with_vertex_amplification" rel="nofollow">article</a> on vertex amplification from Apple:</p>
<blockquote>
<p dir="auto">With vertex amplification, you can encode drawing commands that process the same vertex multiple times, one per render target.</p>
</blockquote>
<p dir="auto">That is very useful to us because one "render target" from the quote above translates directly to one display on Apple Vision. Two displays for the left and right eyes - two render targets to which we can submit the same 3 vertices once, letting the Metal API "amplify" them for us, for free, with hardware acceleration, and render them to both displays <strong>at the same time</strong>. Vertex Amplification is not used only for rendering to both displays on Apple Vision and has its benefits in general graphics techniques such as Cascaded Shadowmaps, where we submit one vertex and render it to multiple "cascades", represented as texture slices, for more adaptive and better looking realtime shadows.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Preparing to Render with Support for Vertex Amplification</h4><a id="user-content-preparing-to-render-with-support-for-vertex-amplification" aria-label="Permalink: Preparing to Render with Support for Vertex Amplification" href="#preparing-to-render-with-support-for-vertex-amplification"></a></p>
<p dir="auto">But back to vertex amplification as means for efficient rendering to both Apple Vision displays. Say we want to render the aforementioned 3 vertices triangle on Apple Vision. In order to render anything, on any Apple device, be it with a non-VR display or two displays set-up, we need to create a <a href="https://developer.apple.com/documentation/metal/mtlrenderpipelinedescriptor" rel="nofollow"><code>MTLRenderPipelineDescriptor</code></a> that will hold all of the state needed to render an object in a single render pass. Stuff like the vertex and fragment shaders to use, the color and depth pixel formats to use when rendering, the sample count if we use MSAA and so on. In the case of Apple Vision, we need to explicitly set the <code>maxVertexAmplificationCount</code> property when creating our <code>MTLRenderPipelineDescriptor</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let pipelineStateDescriptor = MTLRenderPipelineDescriptor()
pipelineStateDescriptor.vertexFunction = vertexFunction
pipelineStateDescriptor.fragmentFunction = fragmentFunction
pipelineStateDescriptor.maxVertexAmplificationCount = 2
// ..."><pre><span>let</span> <span>pipelineStateDescriptor</span> <span>=</span> <span>MTLRenderPipelineDescriptor</span><span>(</span><span>)</span>
pipelineStateDescriptor<span>.</span>vertexFunction <span>=</span> vertexFunction
pipelineStateDescriptor<span>.</span>fragmentFunction <span>=</span> fragmentFunction
pipelineStateDescriptor<span>.</span>maxVertexAmplificationCount <span>=</span> <span>2</span>
<span>// ...</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Enabling Vertex Amplification for a Render Pass</h4><a id="user-content-enabling-vertex-amplification-for-a-render-pass" aria-label="Permalink: Enabling Vertex Amplification for a Render Pass" href="#enabling-vertex-amplification-for-a-render-pass"></a></p>
<p dir="auto">We now have a <code>MTLRenderPipelineDescriptor</code> that represents a graphics pipeline configuration with vertex amplification enabled. We can use it to create a render pipeline, represented by <a href="https://developer.apple.com/documentation/metal/mtlrenderpipelinestate" rel="nofollow"><code>MTLRenderPipelineState</code></a>. Once this render pipeline has been created, the call to render this pipeline needs to be encoded into list of per-frame commands to be submitted to the GPU. What are examples of such commands? Imagine we are building a game with two objects and on each frame we do the following operations:</p>
<ol dir="auto">
<li>Set the clear color before rendering.</li>
<li>Set the viewport size.</li>
<li>Set the render target we are rendering to.</li>
<li>Clear the contents of the render target with the clear color set in step 1.</li>
<li>Set <code>MTLRenderPipelineState</code> for object A as active</li>
<li>Render object A.</li>
<li>Set <code>MTLRenderPipelineState</code> for object B as active</li>
<li>Render object B.</li>
<li>Submit all of the above commands to the GPU</li>
<li>Write the resulting pixel values to some pixel attachment</li>
</ol>
<p dir="auto">All of these rendering commands represent a single <strong>render pass</strong> that happens on each frame while our game is running. This render pass is configured via <a href="https://developer.apple.com/documentation/metal/mtlrenderpassdescriptor" rel="nofollow"><code>MTLRenderPassDescriptor</code></a>. We need to configure the render pass to use foveation and output to two render targets simultaneously.</p>
<ol dir="auto">
<li>Enable foveation by supplying a <a href="https://developer.apple.com/documentation/metal/mtlrasterizationratemap" rel="nofollow"><code>rasterizationRateMap</code></a> property to our <code>MTLRenderPassDescriptor</code>. This property, represented by <a href="https://developer.apple.com/documentation/metal/mtlrasterizationratemap" rel="nofollow"><code>MTLRasterizationRateMap</code></a> is created for us behind the scenes by the Compositor Services. We don't have a direct say in its creation. Instead, we need to query it. On each frame, <code>LayerRenderer</code> will supply us with a <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/frame" rel="nofollow"><code>LayerRenderer.Frame</code></a> object. Among other things, <code>LayerRenderer.Frame</code> holds a <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/drawable" rel="nofollow"><code>LayerRenderer.Drawable</code></a>. More on these objects later. For now, we need to know that this <code>LayerRenderer.Drawable</code> object holds not only the textures for both eyes we will render our content into, but also an array of <code>MTLRasterizationRateMap</code>s that hold the foveation settings for each display.</li>
<li>Set the amount of render targets we will render to by setting the <a href="https://developer.apple.com/documentation/metal/mtlrenderpassdescriptor/1437975-rendertargetarraylength" rel="nofollow"><code>renderTargetArrayLength</code></a> property. Since we are dealing with two displays, we set it to 2.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="// Get the current frame from Compositor Services
guard let frame = layerRenderer.queryNextFrame() else {
   return
}

// Get the current frame drawable
guard let drawable = frame.queryDrawable() else {
   return
}

let renderPassDescriptor = MTLRenderPassDescriptor()
// ...

// both eyes ultimately have the same foveation settings. Let's use the left eye MTLRasterizationRateMap for both eyes
renderPassDescriptor.rasterizationRateMap = drawable.rasterizationRateMaps.first
renderPassDescriptor.renderTargetArrayLength = 2"><pre><span>// Get the current frame from Compositor Services</span>
guard <span>let</span> frame <span>=</span> layerRenderer<span>.</span><span>queryNextFrame</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
   <span>return</span>
<span>}</span>

<span>// Get the current frame drawable</span>
guard <span>let</span> drawable <span>=</span> frame<span>.</span><span>queryDrawable</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
   <span>return</span>
<span>}</span>

<span>let</span> <span>renderPassDescriptor</span> <span>=</span> <span>MTLRenderPassDescriptor</span><span>(</span><span>)</span>
<span>// ...</span>

<span>// both eyes ultimately have the same foveation settings. Let's use the left eye MTLRasterizationRateMap for both eyes</span>
renderPassDescriptor<span>.</span>rasterizationRateMap <span>=</span> drawable<span>.</span>rasterizationRateMaps<span>.</span>first
renderPassDescriptor<span>.</span>renderTargetArrayLength <span>=</span> <span>2</span></pre></div>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> Turning on foveation prevents rendering to a pixel buffer with smaller resolution than the device display. Certain graphics techniques allow for rendering to a lower resolution pixel buffer and upscaling it before presenting it or using it as an input to another effect. That is a performance optimisation. Apple for example has the <a href="https://developer.apple.com/documentation/metalfx" rel="nofollow">MetalFX</a> upscaler that allows us to render to a smaller pixel buffer and upscale it back to native resolution. That is not possible when rendering on visionOS with foveation enabled due to the <code>rasterizationRateMaps</code> property. That property is set internally by Compositor Services when a new <code>LayerRenderer</code> is created based on whether we turned on the <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/configuration-swift.struct/isfoveationenabled" rel="nofollow"><code>isFoveationEnabled</code></a> property in our layer configuration. We don't have a say in the direct creation of the <code>rasterizationRateMaps</code> property. We can not use smaller viewport sizes when rendering to our <code>LayerRenderer</code> textures that have predefined rasterization rate maps because the viewport dimensions will not match. We can not change the dimensions of the predefined rasterization rate maps.</p>
<p dir="auto">With foveation disabled you <strong>can</strong> render to a pixel buffer smaller in resolution than the device display. You can render at, say, 75% of the native resolution and use MetalFX to upscale it to 100%. This approach works on Apple Vision.</p>
</blockquote>
<p dir="auto">Once created with the definition above, the render pass represented by a <a href="https://developer.apple.com/documentation/metal/mtlrendercommandencoder" rel="nofollow"><code>MTLRenderCommandEncoder</code></a>. We use this <code>MTLRenderCommandEncoder</code> to encode our <strong>rendering</strong> commands from the steps above into a <a href="https://developer.apple.com/documentation/metal/mtlcommandbuffer" rel="nofollow"><code>MTLCommandBuffer</code></a> which is submitted to the GPU for execution. For a given frame, after these commands have been issued and submitted by the CPU to the GPU for encoding, the GPU will execute each command in correct order, produce the final pixel values for the specific frame, and write them to the final texture to be presented to the user.</p>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> A game can and often does have multiple render passes per frame. Imagine you are building a first person racing game. The main render pass would draw the interior of your car, your opponents' cars, the world, the trees and so on. A second render pass will draw all of the HUD and UI on top. A third render pass might be used for drawing shadows. A fourth render pass might render the objects in your rearview mirror and so on. All of these render passes need to be encoded and submitted to the GPU on each new frame for drawing.</p>
</blockquote>
<p dir="auto">It is important to note that the commands to be encoded in a <code>MTLCommandBuffer</code> and submitted to the GPU are not only limited to rendering. We can submit "compute" commands to the GPU for general-purpose non-rendering work such as fast number crunching via the <a href="https://developer.apple.com/documentation/metal/mtlcomputecommandencoder" rel="nofollow"><code>MTLComputeCommandEncoder</code></a> (modern techniques for ML, physics, simulations, etc are all done on the GPU nowadays). Apple Vision internal libraries for example use Metal for all the finger tracking, ARKit environment recognition and tracking and so on. However, let's focus only on the rendering commands for now.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Specifying the Viewport Mappings for Both Render Targets</h4><a id="user-content-specifying-the-viewport-mappings-for-both-render-targets" aria-label="Permalink: Specifying the Viewport Mappings for Both Render Targets" href="#specifying-the-viewport-mappings-for-both-render-targets"></a></p>
<p dir="auto">We already created a render pass with vertex amplification enabled. We need to instruct Metal on the correct viewport offsets and sizes for each render target before we render. We need to:</p>
<ol start="2" dir="auto">
<li>Specify the view mappings that hold per-output offsets to a specific render target and viewport.</li>
<li>Specify the viewport sizes for each render target.</li>
</ol>
<p dir="auto">The viewport sizes and view mappings into each render target depend on our textures' layout we specified when creating the <code>LayerRenderer</code> configuration used in Compositor Services earlier in the article. We should <strong>never</strong> hardcode these values ourselves. Instead, we can query this info from the current frame <code>LayerRenderer.Drawable</code>. It provides the information and textures we need to draw into for a given frame of content. We will explore these objects in more detail later on, but the important piece of information is that the <code>LayerRenderer.Drawable</code> we just queried will give us the correct viewport sizes and view mappings for each render target we will draw to.</p>
<div dir="auto" data-snippet-clipboard-copy-content="// Get the current frame from Compositor Services
guard let frame = layerRenderer.queryNextFrame() else {
   return
}

// Get the current frame drawable
guard let drawable = frame.queryDrawable() else {
   return
}

// Creates a MTLRenderCommandEncoder
guard let renderEncoder = commandBuffer.makeRenderCommandEncoder(descriptor: renderPassDescriptor) else {
  return
}

// Query the current frame drawable view offset mappings for each render target
var viewMappings = (0 ..< 2).map {
   MTLVertexAmplificationViewMapping(
     viewportArrayIndexOffset: UInt32($0),
     renderTargetArrayIndexOffset: UInt32($0)
   )
}
// Set the number of amplifications and the correct view offset mappings for each render target
renderEncoder.setVertexAmplificationCount(2, viewMappings: &amp;viewMappings)

let viewports = drawable.views.map { $0.textureMap.viewport }
renderEncoder.setViewports(viewports)

// Encode our rendering commands into the MTLRenderCommandEncoder

// Submit the MTLRenderCommandEncoder to the GPU for execution"><pre><span>// Get the current frame from Compositor Services</span>
guard <span>let</span> frame <span>=</span> layerRenderer<span>.</span><span>queryNextFrame</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
   <span>return</span>
<span>}</span>

<span>// Get the current frame drawable</span>
guard <span>let</span> drawable <span>=</span> frame<span>.</span><span>queryDrawable</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
   <span>return</span>
<span>}</span>

<span>// Creates a MTLRenderCommandEncoder</span>
guard <span>let</span> renderEncoder <span>=</span> commandBuffer<span>.</span><span>makeRenderCommandEncoder</span><span>(</span>descriptor<span>:</span> renderPassDescriptor<span>)</span> <span>else</span> <span>{</span>
  <span>return</span>
<span>}</span>

<span>// Query the current frame drawable view offset mappings for each render target</span>
<span>var</span> <span>viewMappings</span> <span>=</span> <span>(</span><span>0</span> <span>..&lt;</span> <span>2</span><span>)</span><span>.</span><span>map</span> <span>{</span>
   <span>MTLVertexAmplificationViewMapping</span><span>(</span>
     viewportArrayIndexOffset<span>:</span> <span>UInt32</span><span>(</span>$0<span>)</span><span>,</span>
     renderTargetArrayIndexOffset<span>:</span> <span>UInt32</span><span>(</span>$0<span>)</span>
   <span>)</span>
<span>}</span>
<span>// Set the number of amplifications and the correct view offset mappings for each render target</span>
renderEncoder<span>.</span><span>setVertexAmplificationCount</span><span>(</span><span>2</span><span>,</span> viewMappings<span>:</span> <span>&amp;</span>viewMappings<span>)</span>

<span>let</span> <span>viewports</span> <span>=</span> drawable<span>.</span>views<span>.</span><span>map</span> <span>{</span> $0<span>.</span>textureMap<span>.</span>viewport <span>}</span>
renderEncoder<span>.</span><span>setViewports</span><span>(</span>viewports<span>)</span>

<span>// Encode our rendering commands into the MTLRenderCommandEncoder</span>

<span>// Submit the MTLRenderCommandEncoder to the GPU for execution</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Computing the View and Projection Matrices for Each Eye</h4><a id="user-content-computing-the-view-and-projection-matrices-for-each-eye" aria-label="Permalink: Computing the View and Projection Matrices for Each Eye" href="#computing-the-view-and-projection-matrices-for-each-eye"></a></p>
<p dir="auto">Okay, we created our <code>LayerRenderer</code> that holds the textures we will render to, enabled foveation, and have vertex amplification enabled. Next we need to compute the correct view and projection matrices <strong>for each eye</strong> to use during rendering. If you have done computer graphics work or used a game engine like Unity, you know that usually we create a virtual camera that sits somewhere in our 3D world, is oriented towards a specific direction, has a specific field of view, a certain aspect ratio, a near and a far plane and other attributes. We use the view and projection matrix of the camera to transform a vertex's 3D position in our game world to clip space, which in turn is is further transformed by the GPU to finally end up in device screen space coordinates.</p>
<p dir="auto">When rendering to a non-VR screen, it is up to us, as programmers, to construct this virtual camera and decide what values all of these properties will have. Since our rendered objects' positions are ultimately presented on a 2D screen that we look at from some distance, these properties do not have to be "physically based" to match our eyes and field of view. We can go crazy with really small range of field of view, use a portrait aspect ratio, some weird projection ("fish eye") and so on for rendering. Point being is that, in non-VR rendering, we are given leeway on how to construct the camera we use depending on the effect and look we are trying to achieve.</p>
<p dir="auto">When rendering on Apple Vision we can not set these camera properties or augment them manually in any way as it might cause sickness. Changing the default camera properties will result in things looking "weird", and not matching our eyes (remember the initial eye setup you had to do when you bought your Apple Vision?). I can't imagine Apple being okay with publishing apps that augment the default camera projections as they might break the immersion, feel "off" and make the product look crappy.</p>
<p dir="auto">My point is that we have to use the projection and view matrices given to us by Apple Vision. We are trying to simulate a world in immersive mode or mix our content with the real environment in mixed mode. It should feel natural to the user, as if she is not even wearing a device. We should not downscale the field of view, change the aspect ratio or mess with any other settings.</p>
<p dir="auto">So on each frame, we need to query 2 view matrices representing each eye's position and orientation in the physical world. Similarly, we need to query 2 perspective projection matrices that encode the <strong>correct</strong> aspect, field of view, near and far planes for each eye from the current frame <code>LayerRenderer.Drawable</code>. Each eye's "view" is represented by <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/drawable/view" rel="nofollow"><code>LayerRenderer.Drawable.View </code></a>. Compositor Services provides a distinct view for each eye, i.e. each display. It is up to us to obtain these 4 matrices on each frame from both the left and right eye <code>LayerRenderer.Drawable.View</code> and use them to render our content to both of the displays. These 4 matrices are:</p>
<ol dir="auto">
<li>Left eye view matrix</li>
<li>Left eye projection matrix</li>
<li>Right eye view matrix</li>
<li>Right eye projection matrix</li>
</ol>
<p dir="auto"><h5 tabindex="-1" dir="auto">View Matrices</h5><a id="user-content-view-matrices" aria-label="Permalink: View Matrices" href="#view-matrices"></a></p>
<p dir="auto">These matrices represent each eye's position and orientation <strong>with regards to the world coordinate space</strong>. As you move around your room the view matrices will change. Shorter people will get different view matrices then tall people. You sitting on a couch and looking to the left will produce different view matrices than you standing up and looking to the right.</p>
<p dir="auto">Obtaining the view matrices for both eyes is a 3 step process:</p>
<ol dir="auto">
<li>Obtain Apple Vision view transform <strong>pose</strong> matrix that indicates the device position and orientation in the world coordinate system.</li>
</ol>
<p dir="auto">This is global and not tied to a specific eye. It has nothing do to with Compositor Services or the current frame's <code>LayerRenderer.Drawable</code>. Instead, to obtain it, we need to use ARKit and more specifically the visionOS-specific <a href="https://developer.apple.com/documentation/arkit/worldtrackingprovider" rel="nofollow"><code>WorldTrackingProvider</code></a>, which is a source of live data about the device pose and anchors in a person’s surroundings. Here is some code:</p>
<div dir="auto" data-snippet-clipboard-copy-content="// During app initialization
let worldTracking = WorldTrackingProvider()
let arSession = ARKitSession()

// During app update loop
Task {
  do {
    let dataProviders: [DataProvider] = [worldTracking]
    try await arSession.run(dataProviders)
  } catch {
    fatalError(&quot;Failed to run ARSession&quot;)
  }
}

// During app render loop
let deviceAnchor = worldTracking.queryDeviceAnchor(atTimestamp: time)

// Query Apple Vision world position and orientation anchor. If not available for some reason, fallback to an identity matrix
let simdDeviceAnchor = deviceAnchor?.originFromAnchorTransform ?? float4x4.identity"><pre><span>// During app initialization</span>
<span>let</span> <span>worldTracking</span> <span>=</span> <span>WorldTrackingProvider</span><span>(</span><span>)</span>
<span>let</span> <span>arSession</span> <span>=</span> <span>ARKitSession</span><span>(</span><span>)</span>

<span>// During app update loop</span>
<span>Task</span> <span>{</span>
  <span>do</span> <span>{</span>
    <span>let</span> <span>dataProviders</span><span>:</span> <span>[</span><span>DataProvider</span><span>]</span> <span>=</span> <span>[</span>worldTracking<span>]</span>
    <span><span>try</span></span> <span>await</span> arSession<span>.</span><span>run</span><span>(</span>dataProviders<span>)</span>
  <span>}</span> <span>catch</span> <span>{</span>
    <span>fatalError</span><span>(</span><span>"</span><span>Failed to run ARSession</span><span>"</span><span>)</span>
  <span>}</span>
<span>}</span>

<span>// During app render loop</span>
<span>let</span> <span>deviceAnchor</span> <span>=</span> worldTracking<span>.</span><span>queryDeviceAnchor</span><span>(</span>atTimestamp<span>:</span> time<span>)</span>

<span>// Query Apple Vision world position and orientation anchor. If not available for some reason, fallback to an identity matrix</span>
<span>let</span> <span>simdDeviceAnchor</span> <span>=</span> deviceAnchor<span><span>?</span></span><span>.</span>originFromAnchorTransform <span>??</span> float4x4<span>.</span>identity</pre></div>
<p dir="auto"><code>simdDeviceAnchor</code> now holds Apple Vision head transform pose matrix.</p>
<ol start="2" dir="auto">
<li>Obtain the eyes' local transformation matrix</li>
</ol>
<p dir="auto">These matrices specify the position and orientation of the left and right eyes <strong>releative</strong> to the device's pose. Just like any eye-specific information, we need to query it from the current frame's <code>LayerRenderer.Drawable</code>. Here is how we obtain the left and right eyes local view matrices:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let leftViewLocalMatrix = drawable.views[0].transform
let rightViewLocalMatrix = drawable.views[1].transform"><pre><span>let</span> <span>leftViewLocalMatrix</span> <span>=</span> drawable<span>.</span><span>views</span><span>[</span><span>0</span><span>]</span><span>.</span>transform
<span>let</span> <span>rightViewLocalMatrix</span> <span>=</span> drawable<span>.</span><span>views</span><span>[</span><span>1</span><span>]</span><span>.</span>transform</pre></div>
<ol start="3" dir="auto">
<li>Multiply the device pose matrix by each eye local transformation matrix to obtain each eye view transform matrix in the world coordinate space.</li>
</ol>
<p dir="auto">To get the final world transformation matrix for each eye we multiply the matrix from step 1. by both eyes' matrices from step 2:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let leftViewWorldMatrix = (deviceAnchorMatrix * leftEyeLocalMatrix.transform).inverse
let rightViewWorldMatrix = (deviceAnchorMatrix * rightEyeLocalMatrix.transform).inverse"><pre><span>let</span> <span>leftViewWorldMatrix</span> <span>=</span> <span>(</span>deviceAnchorMatrix <span>*</span> leftEyeLocalMatrix<span>.</span>transform<span>)</span><span>.</span>inverse
<span>let</span> <span>rightViewWorldMatrix</span> <span>=</span> <span>(</span>deviceAnchorMatrix <span>*</span> rightEyeLocalMatrix<span>.</span>transform<span>)</span><span>.</span>inverse</pre></div>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> Pay special attention to the <code>.inverse</code> part in the end! That is because Apple Vision expects us to use a reverse-Z projection. This is especially important for passthrough rendering with Metal on visionOS 2.0.</p>
</blockquote>
<p dir="auto">Hopefully this image illustrates the concept:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/gnikoloff/drawing-graphics-on-apple-vision-with-metal-rendering-api/blob/main/vision-pro-matrices.png"><img src="https://github.com/gnikoloff/drawing-graphics-on-apple-vision-with-metal-rendering-api/raw/main/vision-pro-matrices.png" alt="Apple Vision eye matrices illustrated"></a></p>
<p dir="auto">To recap so far, let's refer to the 4 matrices needed to render our content on Apple Vision displays. We already computed the first two, the eyes world view transformation matrices, so let's cross them out from our to-do list:</p>
<ol dir="auto">
<li><del>Left eye view matrix</del></li>
<li><del>Right eye view matrix</del></li>
<li>Left eye projection matrix</li>
<li>Right eye projection matrix</li>
</ol>
<p dir="auto">Two more projection matrices to go.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Left and Right Eyes Projection Matrices</h5><a id="user-content-left-and-right-eyes-projection-matrices" aria-label="Permalink: Left and Right Eyes Projection Matrices" href="#left-and-right-eyes-projection-matrices"></a></p>
<p dir="auto">These two matrices encode the perspective projections for each eye. Just like any eye-specific information, they very much rely on Compositor Services and the current <code>LayerRenderer.Frame</code>. How do we go about computing them?</p>
<p dir="auto">Each <code>LayerRenderer.Drawable.View</code> for both eyes gives us a property called <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/drawable/view/4082271-tangents" rel="nofollow"><code>.tangents</code></a>. It represents the values for the angles you use to determine the planes of the viewing frustum. We can use these angles to construct the volume between the near and far clipping planes that contains the scene’s visible content. We will use these tangent values to build the perspective projection matrix for each eye.</p>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> The <code>.tangents</code> property is in fact deprecated on visionOS 2.0 and should not be used in new code. To obtain correct projection matrices for a given eye, one should use the new Compositor Services' <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/drawable/computeprojection(convention:viewindex:)" rel="nofollow"><code>.computeProjection</code></a> method. I will still cover doing it via the <code>.tangents</code> property for historical reasons.</p>
</blockquote>
<p dir="auto">Let's obtain the tangent property for both eyes:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let leftViewTangents = drawable.views[0].tangents
let rightViewTangents = drawable.views[0].tangents"><pre><span>let</span> <span>leftViewTangents</span> <span>=</span> drawable<span>.</span><span>views</span><span>[</span><span>0</span><span>]</span><span>.</span>tangents
<span>let</span> <span>rightViewTangents</span> <span>=</span> drawable<span>.</span><span>views</span><span>[</span><span>0</span><span>]</span><span>.</span>tangents</pre></div>
<p dir="auto">We will also need to get the near and far planes to use in our projections. They are the same for both eyes. We can query them from the current frame's <code>LayerRenderer.Drawable</code> like so:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let farPlane = drawable.depthRange.x
let nearPlane = drawable.depthRange.y"><pre><span>let</span> <span>farPlane</span> <span>=</span> drawable<span>.</span>depthRange<span>.</span>x
<span>let</span> <span>nearPlane</span> <span>=</span> drawable<span>.</span>depthRange<span>.</span>y</pre></div>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> Notice that the far plane is encoded in the <code>.x</code> property, while the near plane is in the <code>.y</code> range. That is, and I can not stress it enough, because Apple expects us to use reverse-Z projection matrices.</p>
</blockquote>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> At the time of writing this article, at least on visionOS 1.0, the far plane (<code>depthRange.x</code>) in the reverse Z projection is actually positioned at negative infinity. I am not sure if this is the case in visionOS 2.0. Not sure why Apple decided to do this. Leaving it as-is at infiity will break certain techniques (for example subdividing the viewing frustum volume into subparts for Cascaded Shadowmaps). In RAYQUEST I actually artifically overwrite and cap this value at something like -500 before constructing my projection matrices. Remember what I said about never overwriting the default projection matrix attributes Apple Vision gives you? Well, I did it only in this case. It works well for immersive space rendering. I can imagine however that overwriting any of these values is a big no-no for passthrough rendering on visionOS 2.0 (which has a different way of constructing projection matrices for each eye alltogether via the <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/drawable/computeprojection(convention:viewindex:)" rel="nofollow"><code>.computeProjection</code></a>).</p>
</blockquote>
<p dir="auto">Now that we have both <code>tangents</code> for each eye, we will utilise Apple's <a href="https://developer.apple.com/documentation/spatial" rel="nofollow">Spatial</a> API. It will allow us to create and manipulate 3D mathematical primitives. What we are interested in particular is the <a href="https://developer.apple.com/documentation/spatial/projectivetransform3d" rel="nofollow"><code>ProjectiveTransform3D</code></a> that will allow us to obtain a perspective matrix for each eye given the tangents we queried earlier. Here is how it looks in code:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let leftViewProjectionMatrix = ProjectiveTransform3D(
  leftTangent: Double(leftViewTangents[0]),
  rightTangent: Double(leftViewTangents[1]),
  topTangent: Double(leftViewTangents[2]),
  bottomTangent: Double(leftViewTangents[3]),
  nearZ: depthRange.x,
  farZ: depthRange.y,
  reverseZ: true
)

let rightViewProjectionMatrix = ProjectiveTransform3D(
  leftTangent: Double(rightViewTangents[0]),
  rightTangent: Double(rightViewTangents[1]),
  topTangent: Double(rightViewTangents[2]),
  bottomTangent: Double(rightViewTangents[3]),
  nearZ: depthRange.x,
  farZ: depthRange.y,
  reverseZ: true
)"><pre><span>let</span> <span>leftViewProjectionMatrix</span> <span>=</span> <span>ProjectiveTransform3D</span><span>(</span>
  leftTangent<span>:</span> <span>Double</span><span>(</span><span>leftViewTangents</span><span>[</span><span>0</span><span>]</span><span>)</span><span>,</span>
  rightTangent<span>:</span> <span>Double</span><span>(</span><span>leftViewTangents</span><span>[</span><span>1</span><span>]</span><span>)</span><span>,</span>
  topTangent<span>:</span> <span>Double</span><span>(</span><span>leftViewTangents</span><span>[</span><span>2</span><span>]</span><span>)</span><span>,</span>
  bottomTangent<span>:</span> <span>Double</span><span>(</span><span>leftViewTangents</span><span>[</span><span>3</span><span>]</span><span>)</span><span>,</span>
  nearZ<span>:</span> depthRange<span>.</span>x<span>,</span>
  farZ<span>:</span> depthRange<span>.</span>y<span>,</span>
  reverseZ<span>:</span> true
<span>)</span>

<span>let</span> <span>rightViewProjectionMatrix</span> <span>=</span> <span>ProjectiveTransform3D</span><span>(</span>
  leftTangent<span>:</span> <span>Double</span><span>(</span><span>rightViewTangents</span><span>[</span><span>0</span><span>]</span><span>)</span><span>,</span>
  rightTangent<span>:</span> <span>Double</span><span>(</span><span>rightViewTangents</span><span>[</span><span>1</span><span>]</span><span>)</span><span>,</span>
  topTangent<span>:</span> <span>Double</span><span>(</span><span>rightViewTangents</span><span>[</span><span>2</span><span>]</span><span>)</span><span>,</span>
  bottomTangent<span>:</span> <span>Double</span><span>(</span><span>rightViewTangents</span><span>[</span><span>3</span><span>]</span><span>)</span><span>,</span>
  nearZ<span>:</span> depthRange<span>.</span>x<span>,</span>
  farZ<span>:</span> depthRange<span>.</span>y<span>,</span>
  reverseZ<span>:</span> true
<span>)</span></pre></div>
<p dir="auto">And that's it! We have obtained all 4 matrices needed to render our content. The global view and projection matrix for the left and the right eyes.</p>
<p dir="auto">Armed with these 4 matrices we can now move on to writing our shaders for stereoscoping rendering.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Adding Vertex Amplification to our Shaders</h4><a id="user-content-adding-vertex-amplification-to-our-shaders" aria-label="Permalink: Adding Vertex Amplification to our Shaders" href="#adding-vertex-amplification-to-our-shaders"></a></p>
<p dir="auto">Usually when rendering objects we need to supply a pair of shaders: the vertex and fragment shader. Let's focus on the vertex shader first.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Vertex Shader</h5><a id="user-content-vertex-shader" aria-label="Permalink: Vertex Shader" href="#vertex-shader"></a></p>
<p dir="auto">If you have done traditional, non-VR non-stereoscoping rendering, you know that you construct a virtual camera, position and orient it in the world and supply it to the vertex shader which in turn multiplies each vertex with the camera view and projection matrices to turn it from local space to clip space. If you made it this far in this article, I assume you have seen this in your shader language of choice:</p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef struct {
   matrix_float4x4 projectionMatrix;
   matrix_float4x4 viewMatrix;
   // ...
} CameraEyeUniforms;

typedef struct {
   float4 vertexPosition [[attribute(0)]];
} VertexIn;

typedef struct {
  float4 position [[position]];
  float2 texCoord [[shared]];
  float3 normal [[shared]];
} VertexOut;

vertex VertexOut myVertexShader(
   Vertex in [[stage_in]],
   constant CameraEyeUniforms &amp;camera [[buffer(0)]]
) {
   VertexOut out = {
      .position = camera.projectionMatrix * camera.viewMatrix * in.vertexPosition,
      .texCoord = /* compute UV */,
      .normal = /* compute normal */
   };
   return out;
}

fragment float4 myFragShader() {
   return float4(1, 0, 0, 1);
}"><pre><span>typedef</span> <span>struct</span> {
   matrix_float4x4 projectionMatrix;
   matrix_float4x4 viewMatrix;
   <span><span>//</span> ...</span>
} CameraEyeUniforms;

<span>typedef</span> <span>struct</span> {
   float4 vertexPosition [[attribute(<span>0</span>)]];
} VertexIn;

<span>typedef</span> <span>struct</span> {
  float4 position [[position]];
  float2 texCoord [[shared]];
  float3 <span>normal</span> [[shared]];
} VertexOut;

vertex VertexOut <span>myVertexShader</span>(
   Vertex in [[stage_in]],
   constant CameraEyeUniforms &amp;camera [[buffer(<span>0</span>)]]
) {
   VertexOut out = {
      .<span>position</span> = camera.<span>projectionMatrix</span> * camera.<span>viewMatrix</span> * in.<span>vertexPosition</span>,
      .<span>texCoord</span> = <span><span>/*</span> compute UV <span>*/</span></span>,
      .<span>normal</span> = <span><span>/*</span> compute normal <span>*/</span></span>
   };
   <span>return</span> out;
}

fragment float4 <span>myFragShader</span>() {
   <span>return</span> <span>float4</span>(<span>1</span>, <span>0</span>, <span>0</span>, <span>1</span>);
}</pre></div>
<p dir="auto">This vertex shader expects a single pair of matrices - the view matrix and projection matrix.</p>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> Take a look at the <code>VertexOut</code> definition. <code>texCoord</code> and <code>normal</code> are marked as <code>shared</code>, while <code>position</code> is not. That's because the position values will change depending on the current vertex amplification index. Both eyes have a different pair of matrices to transform each vertex with. The output vertex for the left eye render target will have different final positions than the output vertex for the right eye.
I hope this makes clear why <code>texCoord</code> and <code>normal</code> are <code>shared</code>. The values are <strong>not</strong> view or projection dependent. Their values will always be uniforms across different render targets, regardless of with which eye are we rendering them. For more info check out this <a href="https://developer.apple.com/documentation/metal/render_passes/improving_rendering_performance_with_vertex_amplification" rel="nofollow">article</a>.</p>
</blockquote>
<p dir="auto">Remember we have two displays and two eye views on Apple Vision. Each view holds it's own respective view and projection matrices. We need a vertex shader that will accept 4 matrices - a view and projection matrices for each eye.</p>
<p dir="auto">Let's introduce a new struct:</p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef struct {
   CameraEyeUniforms camUniforms[2];
   // ...
} CameraBothEyesUniforms;"><pre><span>typedef</span> <span>struct</span> {
   CameraEyeUniforms camUniforms[<span>2</span>];
   <span><span>//</span> ...</span>
} CameraBothEyesUniforms;</pre></div>
<p dir="auto">We treat the original <code>CameraUniforms</code> as a single eye and combine both eyes in <code>camUniforms</code>. With that out of the way, we need to instruct the vertex shader which pair of matrices to use exactly. How do we do that? Well, we get a special <code>amplification_id</code> property as input to our shaders. It allows us to query the index of which vertex amplification are we currently executing. We have two amplifications for both eyes, so now we can easily query our <code>camUniforms</code> array! Here is the revised vertex shader:</p>
<div dir="auto" data-snippet-clipboard-copy-content="vertex VertexOut myVertexShader(
   ushort ampId [[amplification_id]],
   Vertex in [[stage_in]],
   constant CameraBothEyesUniforms &amp;bothEyesCameras [[buffer(0)]]
) {
   constant CameraEyeUniforms &amp;camera = bothEyesCameras.camUniforms[ampId];
   VertexOut out = {
      .position = camera.projectionMatrix * camera.viewMatrix * in.vertexPosition;
   };
   return out;
}"><pre>vertex VertexOut <span>myVertexShader</span>(
   <span>ushort</span> ampId [[amplification_id]],
   Vertex in [[stage_in]],
   constant CameraBothEyesUniforms &amp;bothEyesCameras [[buffer(<span>0</span>)]]
) {
   constant CameraEyeUniforms &amp;camera = bothEyesCameras.<span>camUniforms</span>[ampId];
   VertexOut out = {
      .<span>position</span> = camera.<span>projectionMatrix</span> * camera.<span>viewMatrix</span> * in.<span>vertexPosition</span>;
   };
   <span>return</span> out;
}</pre></div>
<p dir="auto">And that's it! Our output textures and render commands have been setup correctly, we have obtained all required matrices and compiled our vertex shader with support for vertex amplification.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Fragment Shader</h5><a id="user-content-fragment-shader" aria-label="Permalink: Fragment Shader" href="#fragment-shader"></a></p>
<p dir="auto">I will skip on the fragment shader code for brevity sake. However will mention a few techniques that require us to know the camera positions and / or matrices in our fragment shader code:</p>
<ol dir="auto">
<li>Lighting, as we need to shade pixels based on the viewing angle between the pixel and the camera</li>
<li>Planar reflections</li>
<li>Many post-processing effects</li>
<li>Non-rendering techniques such as Screen Space Particle Collisions</li>
</ol>
<p dir="auto">In all these cases, we need the two pair of view + projection matrices for each eye. Fragment shaders also get the <code>amplification_id</code> property as input, so we can query the correct matrices in exactly the same way as did in the vertex shader above.</p>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> Compute shaders <strong>do not</strong> get an <code>[[amplification_id]]</code> property. That makes porting and running view-dependent compute shaders harder when using two texture views in stereoscoping rendering. When adopting well established algorithms you may need to rethink them to account for two eyes and two textures.</p>
</blockquote>
<p dir="auto">All that's left to do is...</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Updating and Encoding a Frame of Content</h2><a id="user-content-updating-and-encoding-a-frame-of-content" aria-label="Permalink: Updating and Encoding a Frame of Content" href="#updating-and-encoding-a-frame-of-content"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Rendering on a Separate Thread</h3><a id="user-content-rendering-on-a-separate-thread" aria-label="Permalink: Rendering on a Separate Thread" href="#rendering-on-a-separate-thread"></a></p>
<p dir="auto">Rendering on a separate thread is recommended in general but especially important on Apple Vision. That is because, during rendering, we will pause the render thread to wait until the optimal rendering time provided to us by Compositor Services. We want the main thread to be able to continue to run, process user inputs, network and so on in the meantime.</p>
<p dir="auto">How do we go about this? Here is some code:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@main
struct MyApp: App {
  var body: some Scene {
    WindowGroup {
      ContentView()
    }
    ImmersiveSpace(id: &quot;ImmersiveSpace&quot;) {
      CompositorLayer(configuration: ContentStageConfiguration()) { layerRenderer in
         let engine = GameEngine(layerRenderer)
         engine.startRenderLoop()
      }
    }
  }
}

class GameEngine {
   private var layerRenderer: LayerRenderer

   public init(_ layerRenderer: LayerRenderer) {
      self.layerRenderer = layerRenderer

       layerRenderer.onSpatialEvent = { eventCollection in
         // process spatial events
       }
   }

   public func startRenderLoop() {
      let renderThread = Thread {
         self.renderLoop()
       }
       renderThread.name = &quot;Render Thread&quot;
       renderThread.start()
   }

   private func renderLoop() {
      while true {
        if layerRenderer.state == .invalidated {
          print(&quot;Layer is invalidated&quot;)
          return
        } else if layerRenderer.state == .paused {
          layerRenderer.waitUntilRunning()
          continue
        } else {
          autoreleasepool {
            // render next frame here
            onRender()
          }
        }
      }
   }

   private func onRender() {
      // ...
   }
}"><pre><span>@<span>main</span></span>
<span>struct</span> <span>MyApp</span><span>:</span> <span>App</span> <span>{</span>
  <span>var</span> <span><span>body</span></span><span>:</span> <span>some</span> <span>Scene</span> <span>{</span>
    <span>WindowGroup</span> <span>{</span>
      <span>ContentView</span><span>(</span><span>)</span>
    <span>}</span>
    <span>ImmersiveSpace</span><span>(</span>id<span>:</span> <span>"</span><span>ImmersiveSpace</span><span>"</span><span>)</span> <span>{</span>
      <span>CompositorLayer</span><span>(</span>configuration<span>:</span> <span>ContentStageConfiguration</span><span>(</span><span>)</span><span>)</span> <span>{</span> layerRenderer <span>in</span>
         <span>let</span> <span>engine</span> <span>=</span> <span>GameEngine</span><span>(</span>layerRenderer<span>)</span>
         engine<span>.</span><span>startRenderLoop</span><span>(</span><span>)</span>
      <span>}</span>
    <span>}</span>
  <span>}</span>
<span>}</span>

<span>class</span> <span>GameEngine</span> <span>{</span>
   <span>private</span> <span>var</span> <span><span>layerRenderer</span></span><span>:</span> <span>LayerRenderer</span>

   <span>public</span> <span>init</span><span>(</span>_ layerRenderer<span>:</span> <span>LayerRenderer</span><span>)</span> <span>{</span>
      <span>self</span><span>.</span>layerRenderer <span>=</span> layerRenderer

       layerRenderer<span>.</span>onSpatialEvent <span>=</span> <span>{</span> eventCollection <span>in</span>
         <span>// process spatial events</span>
       <span>}</span>
   <span>}</span>

   <span>public</span> <span>func</span> startRenderLoop<span>(</span><span>)</span> <span>{</span>
      <span>let</span> <span>renderThread</span> <span>=</span> <span>Thread</span> <span>{</span>
         <span>self</span><span>.</span><span>renderLoop</span><span>(</span><span>)</span>
       <span>}</span>
       renderThread<span>.</span>name <span>=</span> <span>"</span><span>Render Thread</span><span>"</span>
       renderThread<span>.</span><span>start</span><span>(</span><span>)</span>
   <span>}</span>

   <span>private</span> <span>func</span> renderLoop<span>(</span><span>)</span> <span>{</span>
      while true <span>{</span>
        if layerRenderer<span>.</span>state <span>==</span> <span>.</span>invalidated <span>{</span>
          <span>print</span><span>(</span><span>"</span><span>Layer is invalidated</span><span>"</span><span>)</span>
          <span>return</span>
        <span>}</span> <span>else</span> if layerRenderer<span>.</span>state <span>==</span> <span>.</span>paused <span>{</span>
          layerRenderer<span>.</span><span>waitUntilRunning</span><span>(</span><span>)</span>
          continue
        <span>}</span> <span>else</span> <span>{</span>
          <span>autoreleasepool</span> <span>{</span>
            <span>// render next frame here</span>
            <span>onRender</span><span>(</span><span>)</span>
          <span>}</span>
        <span>}</span>
      <span>}</span>
   <span>}</span>

   <span>private</span> <span>func</span> onRender<span>(</span><span>)</span> <span>{</span>
      <span>// ...</span>
   <span>}</span>
<span>}</span></pre></div>
<p dir="auto">We start a separate thread that on each frame checks the <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/state-swift.enum" rel="nofollow"><code>LayerRenderer.State</code></a> property. Depending on this property value, it may skip the current frame, quit the render loop entirely or draw to the current frame. The main thread is unaffected and continues running other code and waits for spatial events.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Fetching a Next Frame for Drawing</h3><a id="user-content-fetching-a-next-frame-for-drawing" aria-label="Permalink: Fetching a Next Frame for Drawing" href="#fetching-a-next-frame-for-drawing"></a></p>
<p dir="auto">Remember all the code we wrote earlier that used <code>LayerRenderer.Frame</code>? We obtained the current <code>LayerRenderer.Drawable</code> from it and queried it for the current frame view and projection matrices, view mappings and so on. This <code>LayerRenderer.Frame</code> is obviously different across frames and we have to constantly query it before using it and encoding draw commands to the GPU. Let's expand upon the <code>onRender</code> method from the previous code snippet and query the next frame for drawing:</p>
<div dir="auto" data-snippet-clipboard-copy-content="class GameEngine {
   // ...
   private func onRender() {
      guard let frame = layerRenderer.queryNextFrame() else {
         print(&quot;Could not fetch current render loop frame&quot;)
         return
       }
   }
}"><pre><span>class</span> <span>GameEngine</span> <span>{</span>
   <span>// ...</span>
   <span>private</span> <span>func</span> onRender<span>(</span><span>)</span> <span>{</span>
      guard <span>let</span> frame <span>=</span> layerRenderer<span>.</span><span>queryNextFrame</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
         <span>print</span><span>(</span><span>"</span><span>Could not fetch current render loop frame</span><span>"</span><span>)</span>
         <span>return</span>
       <span>}</span>
   <span>}</span>
<span>}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Getting Predicted Render Deadlines</h3><a id="user-content-getting-predicted-render-deadlines" aria-label="Permalink: Getting Predicted Render Deadlines" href="#getting-predicted-render-deadlines"></a></p>
<p dir="auto">We need to block our render thread until the optimal rendering time to start the submission phase given to us by Compositor Services. First we will query this optimal rendering time and use it later. Let's expand our <code>onRender</code> method:</p>
<div dir="auto" data-snippet-clipboard-copy-content="private func onRender() {
   guard let frame = layerRenderer.queryNextFrame() else {
      print(&quot;Could not fetch current render loop frame&quot;)
      return
   }
   guard let timing = frame.predictTiming() else {
      return
   }
}"><pre><span>private</span> <span>func</span> onRender<span>(</span><span>)</span> <span>{</span>
   guard <span>let</span> frame <span>=</span> layerRenderer<span>.</span><span>queryNextFrame</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
      <span>print</span><span>(</span><span>"</span><span>Could not fetch current render loop frame</span><span>"</span><span>)</span>
      <span>return</span>
   <span>}</span>
   guard <span>let</span> timing <span>=</span> frame<span>.</span><span>predictTiming</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
      <span>return</span>
   <span>}</span>
<span>}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Updating Our App State Before Rendering</h3><a id="user-content-updating-our-app-state-before-rendering" aria-label="Permalink: Updating Our App State Before Rendering" href="#updating-our-app-state-before-rendering"></a></p>
<p dir="auto">Before doing any rendering, we need to update our app state. What do I mean by this? We usually have to process actions such as:</p>
<ol dir="auto">
<li>User input</li>
<li>Animations</li>
<li>Frustum Culling</li>
<li>Physics</li>
<li>Enemy AI</li>
<li>Audio</li>
</ol>
<p dir="auto">These tasks can be done on the CPU or on the GPU via compute shaders, it does not matter. What does matter is that we need to process and run all of them <strong>before rendering</strong>, because they will dictate what and how exactly do we render. As an example, if you find out that two enemy tanks are colliding during this update phase, you may want to color them differently during rendering. If the user is pointing at a button, you may want to change the appearance of the scene. Apple calls this the <strong>update phase</strong> in their docs btw.</p>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> All of the examples above refer to non-rendering work. However we <strong>can</strong> do rendering during the update phase!
The important distinction that Apple makes is whether we rely on the device anchor information during rendering. It is important to do only rendering work that <strong>does not</strong> depend on the device anchor in the update phase. Stuff like shadow map generation would be a good candidate for this. We render our content from the point of view of the sun, so the device anchor is irrelevant to us during shadowmap rendering.
Remember, it still may be the case that we have to wait until optimal rendering time and skip the current frame. We do not have reliable device anchor information <strong>yet</strong> during the update phase.</p>
</blockquote>
<p dir="auto">We mark the start of the update phase by calling Compositor Service's <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/frame/startupdate()" rel="nofollow"><code>startUpdate</code></a> method. Unsurprisingly, after we are done with the update phase we call <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/frame/endupdate()" rel="nofollow"><code>endUpdate</code></a> that notifies Compositor Services that you have finished updating the app-specific content you need to render the frame. Here is our updated render method:</p>
<div dir="auto" data-snippet-clipboard-copy-content="private func onRender() {
   guard let frame = layerRenderer.queryNextFrame() else {
      print(&quot;Could not fetch current render loop frame&quot;)
      return
   }
   guard let timing = frame.predictTiming() else {
      return
   }

   frame.startUpdate()

   // do your game's physics, animation updates, user input, raycasting and non-device anchor related rendering work here

   frame.endUpdate()
}"><pre><span>private</span> <span>func</span> onRender<span>(</span><span>)</span> <span>{</span>
   guard <span>let</span> frame <span>=</span> layerRenderer<span>.</span><span>queryNextFrame</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
      <span>print</span><span>(</span><span>"</span><span>Could not fetch current render loop frame</span><span>"</span><span>)</span>
      <span>return</span>
   <span>}</span>
   guard <span>let</span> timing <span>=</span> frame<span>.</span><span>predictTiming</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
      <span>return</span>
   <span>}</span>

   frame<span>.</span><span>startUpdate</span><span>(</span><span>)</span>

   <span>// do your game's physics, animation updates, user input, raycasting and non-device anchor related rendering work here</span>

   frame<span>.</span><span>endUpdate</span><span>(</span><span>)</span>
<span>}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Waiting Until Optimal Rendering Time</h3><a id="user-content-waiting-until-optimal-rendering-time" aria-label="Permalink: Waiting Until Optimal Rendering Time" href="#waiting-until-optimal-rendering-time"></a></p>
<p dir="auto">We already queried and know the optimal rendering time given to us by Compositor Services. After wrapping up our update phase, we need to block our render thread until the optimal time to start the submission phase of our frame. To block the thread we can use Compositor Service's <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/clock" rel="nofollow"><code>LayerRenderer.Clock</code></a> and more specifically its <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/clock/wait(until:tolerance:)" rel="nofollow"><code>.wait()</code></a> method:</p>
<div dir="auto" data-snippet-clipboard-copy-content="private func onRender() {
   guard let frame = layerRenderer.queryNextFrame() else {
      print(&quot;Could not fetch current render loop frame&quot;)
      return
   }
   guard let timing = frame.predictTiming() else {
      return
   }

   frame.startUpdate()

   // do your game's physics, animation updates, user input, raycasting and non-device anchor related rendering work here

   frame.endUpdate()

   // block the render thread until optimal input time
   LayerRenderer.Clock().wait(until: timing.optimalInputTime)
}"><pre><span>private</span> <span>func</span> onRender<span>(</span><span>)</span> <span>{</span>
   guard <span>let</span> frame <span>=</span> layerRenderer<span>.</span><span>queryNextFrame</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
      <span>print</span><span>(</span><span>"</span><span>Could not fetch current render loop frame</span><span>"</span><span>)</span>
      <span>return</span>
   <span>}</span>
   guard <span>let</span> timing <span>=</span> frame<span>.</span><span>predictTiming</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
      <span>return</span>
   <span>}</span>

   frame<span>.</span><span>startUpdate</span><span>(</span><span>)</span>

   <span>// do your game's physics, animation updates, user input, raycasting and non-device anchor related rendering work here</span>

   frame<span>.</span><span>endUpdate</span><span>(</span><span>)</span>

   <span>// block the render thread until optimal input time</span>
   <span>LayerRenderer</span><span>.</span><span>Clock</span><span>(</span><span>)</span><span>.</span><span>wait</span><span>(</span>until<span>:</span> timing<span>.</span>optimalInputTime<span>)</span>
<span>}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Frame Submission Phase</h3><a id="user-content-frame-submission-phase" aria-label="Permalink: Frame Submission Phase" href="#frame-submission-phase"></a></p>
<p dir="auto">We have ended our update phase and waited until the optimal rendering time. It is time to start the <strong>submission phase</strong>. <strong>That</strong> is the right time to query the device anchor information, compute the correct view and projection matrices and submit any view-related drawing commands with Metal (basically all the steps we did in the "Vertex Amplification" chapter of this article).</p>
<p dir="auto">Once we have submitted all of our drawing and compute commands to the GPU, we end the frame submission. The GPU will take all of the submited commands and execute them for us.</p>
<div dir="auto" data-snippet-clipboard-copy-content="private func onRender() {
   guard let frame = layerRenderer.queryNextFrame() else {
      print(&quot;Could not fetch current render loop frame&quot;)
      return
   }
   guard let timing = frame.predictTiming() else {
      return
   }

   frame.startUpdate()

   // do your game's physics, animation updates, user input, raycasting and non-device anchor related rendering work here

   frame.endUpdate()

   LayerRenderer.Clock().wait(until: timing.optimalInputTime)

   frame.startSubmission()

   // we already covered this code, query device anchor position and orientation in physical world
   let deviceAnchor = worldTracking.queryDeviceAnchor(atTimestamp: time)
   let simdDeviceAnchor = deviceAnchor?.originFromAnchorTransform ?? float4x4.identity

   // submit all of your rendering and compute related Metal commands here

   // mark the frame as submitted and hand it to the GPU
   frame.endSubmission()
}"><pre><span>private</span> <span>func</span> onRender<span>(</span><span>)</span> <span>{</span>
   guard <span>let</span> frame <span>=</span> layerRenderer<span>.</span><span>queryNextFrame</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
      <span>print</span><span>(</span><span>"</span><span>Could not fetch current render loop frame</span><span>"</span><span>)</span>
      <span>return</span>
   <span>}</span>
   guard <span>let</span> timing <span>=</span> frame<span>.</span><span>predictTiming</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
      <span>return</span>
   <span>}</span>

   frame<span>.</span><span>startUpdate</span><span>(</span><span>)</span>

   <span>// do your game's physics, animation updates, user input, raycasting and non-device anchor related rendering work here</span>

   frame<span>.</span><span>endUpdate</span><span>(</span><span>)</span>

   <span>LayerRenderer</span><span>.</span><span>Clock</span><span>(</span><span>)</span><span>.</span><span>wait</span><span>(</span>until<span>:</span> timing<span>.</span>optimalInputTime<span>)</span>

   frame<span>.</span><span>startSubmission</span><span>(</span><span>)</span>

   <span>// we already covered this code, query device anchor position and orientation in physical world</span>
   <span>let</span> <span>deviceAnchor</span> <span>=</span> worldTracking<span>.</span><span>queryDeviceAnchor</span><span>(</span>atTimestamp<span>:</span> time<span>)</span>
   <span>let</span> <span>simdDeviceAnchor</span> <span>=</span> deviceAnchor<span><span>?</span></span><span>.</span>originFromAnchorTransform <span>??</span> float4x4<span>.</span>identity

   <span>// submit all of your rendering and compute related Metal commands here</span>

   <span>// mark the frame as submitted and hand it to the GPU</span>
   frame<span>.</span><span>endSubmission</span><span>(</span><span>)</span>
<span>}</span></pre></div>
<p dir="auto">And that's it! To recap: we have to use a dedicated render thread for drawing and Compositor Services' methods to control its execution. We are presented with two phases: update and submit. We update our app state in the update phase and issue draw commands with Metal in the submit phase.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supporting Both Stereoscopic and non-VR Display Rendering</h2><a id="user-content-supporting-both-stereoscopic-and-non-vr-display-rendering" aria-label="Permalink: Supporting Both Stereoscopic and non-VR Display Rendering" href="#supporting-both-stereoscopic-and-non-vr-display-rendering"></a></p>
<p dir="auto">As you can see, Apple Vision requires us to <strong>always</strong> think in terms of two eyes and two render targets. Our rendering code, matrices and shaders were built around this concept. So we have to write a renderer that supports "traditional" non-VR and stereoscoping rendering simultaneously. Doing so however requires some careful planning and inevitably some preprocessor directives in your codebase.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Two Rendering Paths. <code>LayerRenderer.Frame.Drawable</code> vs <code>MTKView</code></h3><a id="user-content-two-rendering-paths-layerrendererframedrawable-vs-mtkview" aria-label="Permalink: Two Rendering Paths. LayerRenderer.Frame.Drawable vs MTKView" href="#two-rendering-paths-layerrendererframedrawable-vs-mtkview"></a></p>
<p dir="auto">On Apple Vision, you configure a <code>LayerRenderer</code> at init time and the system gives you <code>LayerRenderer.Frame.Drawable</code> on each frame to draw to. On macOS / iOS / iPadOS and so on, you create a <code>MTKView</code> and a <code>MTKViewDelegate</code> that allows you to hook into the system resizing and drawing updates. In both cases you present your rendered content to the user by drawing to the texture provided by the system for you. How would this look in code? How about this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="open class Renderer {
   #if os(visionOS)
      public var currentDrawable: LayerRenderer.Drawable
   #else
      public var currentDrawable: MTKView
   #endif

   private func renderFrame() {
      // prepare frame, run animations, collect user input, etc

      #if os(visionOS)
         // prepare a two sets of view and projection matrices for both eyes
         // render to both render targets simultaneously 
      #else
         // prepare a view and projection matrix for a single virtual camera
         // render to single render target
      #endif

      // submit your rendering commands to the GPU for rendering
   }
}"><pre><span>open</span> <span>class</span> <span>Renderer</span> <span>{</span>
   <span>#if os(visionOS)</span>
      <span>public</span> <span>var</span> <span><span>currentDrawable</span></span><span>:</span> <span>LayerRenderer</span><span>.</span><span>Drawable</span>
   <span>#else</span>
      <span>public</span> <span>var</span> <span><span>currentDrawable</span></span><span>:</span> <span>MTKView</span>
   <span>#endif</span>

   <span>private</span> <span>func</span> renderFrame<span>(</span><span>)</span> <span>{</span>
      <span>// prepare frame, run animations, collect user input, etc</span>

      <span>#if os(visionOS)</span>
         <span>// prepare a two sets of view and projection matrices for both eyes</span>
         <span>// render to both render targets simultaneously </span>
      <span>#else</span>
         <span>// prepare a view and projection matrix for a single virtual camera</span>
         <span>// render to single render target</span>
      <span>#endif</span>

      <span>// submit your rendering commands to the GPU for rendering</span>
   <span>}</span>
<span>}</span></pre></div>
<p dir="auto">By using preprocessor directives in Swift, we can build our project for different targets. This way we can have two render paths for stereoscoping (Apple Vision) and normal 2D rendering (all other Apple hardware).</p>
<p dir="auto">It should be noted that the 2D render path will omit all of the vertex amplification commands we prepared earlier on the CPU to be submitted to the GPU for drawing. Stuff like <code>renderEncoder.setVertexAmplificationCount(2, viewMappings: &amp;viewMappings)</code> and <code>renderEncoder.setViewports(viewports)</code> is no longer needed.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Adapting our Vertex Shader</h3><a id="user-content-adapting-our-vertex-shader" aria-label="Permalink: Adapting our Vertex Shader" href="#adapting-our-vertex-shader"></a></p>
<p dir="auto">The vertex shader we wrote earlier needs some rewriting to support non-Vertex Amplified rendering. That can be done easily with <a href="https://developer.apple.com/documentation/metal/using_function_specialization_to_build_pipeline_variants" rel="nofollow">Metal function constants</a>. Function constants allow us to compile one shader binary and then conditionally enable / disable things in it when using it to build render or compute pipelines. Take a look:</p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef struct {
   matrix_float4x4 projectionMatrix;
   matrix_float4x4 viewMatrix;
} CameraUniforms;

typedef struct {
   CameraEyeUniforms camUniforms[2];
} CameraBothEyesUniforms;

typedef struct {
  float4 position [[position]];
} VertexOut;

constant bool isAmplifiedRendering [[function_constant(0)]];
constant bool isNonAmplifiedRendering = !isAmplifiedRendering;

vertex VertexOut myVertexShader(
   ushort ampId                                    [[amplification_id]],
   constant CameraUniforms &amp;camera                 [[buffer(0), function_constant(isNonAmplifiedRendering]],
   constant CameraBothEyesUniforms &amp;cameraBothEyes [[buffer(1), function_constant(isAmplifiedRendering)]]
) {
   if (isAmplifiedRendering) {
      constant CameraEyeUniforms &amp;camera = bothEyesCameras.uniforms[ampId];
      out.position = camera.projectionMatrix * camera.viewMatrix * vertexPosition;
   } else {
      out.position = camera.projectionMatrix * camera.viewMatrix * vertexPosition;
   }
   return out;
}

fragment float4 myFragShader() {
   return float4(1, 0, 0, 1);
}"><pre><span>typedef</span> <span>struct</span> {
   matrix_float4x4 projectionMatrix;
   matrix_float4x4 viewMatrix;
} CameraUniforms;

<span>typedef</span> <span>struct</span> {
   CameraEyeUniforms camUniforms[<span>2</span>];
} CameraBothEyesUniforms;

<span>typedef</span> <span>struct</span> {
  float4 position [[position]];
} VertexOut;

constant <span>bool</span> isAmplifiedRendering [[function_constant(<span>0</span>)]];
constant <span>bool</span> isNonAmplifiedRendering = !isAmplifiedRendering;

vertex VertexOut <span>myVertexShader</span>(
   <span>ushort</span> ampId                                    [[amplification_id]],
   constant CameraUniforms &amp;camera                 [[buffer(<span>0</span>), function_constant(isNonAmplifiedRendering]],
   constant CameraBothEyesUniforms &amp;cameraBothEyes [[buffer(<span>1</span>), function_constant(isAmplifiedRendering)]]
) {
   <span>if</span> (isAmplifiedRendering) {
      constant CameraEyeUniforms &amp;camera = bothEyesCameras.<span>uniforms</span>[ampId];
      out.<span>position</span> = camera.<span>projectionMatrix</span> * camera.<span>viewMatrix</span> * vertexPosition;
   } <span>else</span> {
      out.<span>position</span> = camera.<span>projectionMatrix</span> * camera.<span>viewMatrix</span> * vertexPosition;
   }
   <span>return</span> out;
}

fragment float4 <span>myFragShader</span>() {
   <span>return</span> <span>float4</span>(<span>1</span>, <span>0</span>, <span>0</span>, <span>1</span>);
}</pre></div>
<p dir="auto">Our updated shader supports both flat 2D and stereoscoping rendering. All we need to set the <code>isAmplifiedRendering</code> function constant when creating a <code>MTLRenderPipelineState</code> and supply the correct matrices to it.</p>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> It is important to note that even when rendering on Apple Vision you may need to render to a flat 2D texture. One example would be drawing shadows, where you put a virtual camera where the sun should be, render to a depth buffer and then project these depth values when rendering to the main displays to determine if a pixel is in shadow or not. Rendering from the Sun point of view in this case does not require multiple render targets or vertex amplification. With our updated vertex shader, we can now support both.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Gotchas</h2><a id="user-content-gotchas" aria-label="Permalink: Gotchas" href="#gotchas"></a></p>
<p dir="auto">I have hinted at some of these throughout the article, but let's recap them and write them down together.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Can't Render to a Smaller Resolution Pixel Buffer when Foveation is Enabled</h3><a id="user-content-cant-render-to-a-smaller-resolution-pixel-buffer-when-foveation-is-enabled" aria-label="Permalink: Can't Render to a Smaller Resolution Pixel Buffer when Foveation is Enabled" href="#cant-render-to-a-smaller-resolution-pixel-buffer-when-foveation-is-enabled"></a></p>
<p dir="auto">Turning on foveation prevents rendering to a pixel buffer with smaller resolution than the device display. Certain graphics techniques allow for rendering to a lower resolution pixel buffer and upscaling it before presenting it or using it as an input to another effect. That is a performance optimisation. Apple for example has the <a href="https://developer.apple.com/documentation/metalfx" rel="nofollow">MetalFX</a> upscaler that allows us to render to a smaller pixel buffer and upscale it back to native resolution. That is not possible when rendering on visionOS with foveation enabled due to the <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/drawable/rasterizationratemaps" rel="nofollow"><code>rasterizationRateMaps</code></a> property. That property is set internally by Compositor Services when a new <code>LayerRenderer</code> is created based on whether we turned on the <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/configuration-swift.struct/isfoveationenabled" rel="nofollow"><code>isFoveationEnabled</code></a> property in our layer configuration. We don't have a say in the direct creation of the <code>rasterizationRateMaps</code> property. We can not use smaller viewport sizes sizes when rendering to our <code>LayerRenderer</code> textures that have predefined rasterization rate maps because the viewport dimensions will not match. We can not change the dimensions of the predefined rasterization rate maps.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Postprocessing</h3><a id="user-content-postprocessing" aria-label="Permalink: Postprocessing" href="#postprocessing"></a></p>
<p dir="auto">Many Apple official examples use compute shaders to postprocess the final scene texture. Implementing sepia, vignette and other graphics techniques happen at the postprocessing stage.</p>
<p dir="auto">Using compute shaders to write to the textures provided by Compositor Services' <code>LayerRenderer</code> is not allowed. That is because these textures do not have the <a href="https://developer.apple.com/documentation/metal/mtltextureusage/1515854-shaderwrite" rel="nofollow"><code>MTLTextureUsage.shaderWrite</code></a> flag enabled. We can not enable this flag post factum ourselves, because they were internally created by Compositor Services. So for postprocessing we are left with spawning fullscreen quads for each display and using fragment shader to implement our postprocessing effects. That is allowed because the textures provided by Compositor Services <strong>do</strong> have the <a href="https://developer.apple.com/documentation/metal/mtltextureusage/1515701-rendertarget" rel="nofollow"><code>MTLTextureUsage.renderTarget</code></a> flag enabled. That is the case with the textures provided by <code>MTKView</code> on other Apple hardware btw.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">True Camera Position</h3><a id="user-content-true-camera-position" aria-label="Permalink: True Camera Position" href="#true-camera-position"></a></p>
<p dir="auto">Remember when we computed the view matrices for both eyes earlier?</p>
<div dir="auto" data-snippet-clipboard-copy-content="let leftViewWorldMatrix = (deviceAnchorMatrix * leftEyeLocalMatrix.transform).inverse
let rightViewWorldMatrix = (deviceAnchorMatrix * rightEyeLocalMatrix.transform).inverse"><pre><span>let</span> <span>leftViewWorldMatrix</span> <span>=</span> <span>(</span>deviceAnchorMatrix <span>*</span> leftEyeLocalMatrix<span>.</span>transform<span>)</span><span>.</span>inverse
<span>let</span> <span>rightViewWorldMatrix</span> <span>=</span> <span>(</span>deviceAnchorMatrix <span>*</span> rightEyeLocalMatrix<span>.</span>transform<span>)</span><span>.</span>inverse</pre></div>
<p dir="auto">These are 4x4 matrices and we can easily extract the translation out of them to obtain each eye's world position. Something like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="extension SIMD4 {
  public var xyz: SIMD3<Scalar> {
    get {
      self[SIMD3(0, 1, 2)]
    }
    set {
      self.x = newValue.x
      self.y = newValue.y
      self.z = newValue.z
    }
  }
}

// SIMD3<Float> vectors representing the XYZ position of each eye
let leftEyeWorldPosition = leftViewWorldMatrix.columns.3.xyz
let rightEyeWorldPosition = rightViewWorldMatrix.columns.3.xyz"><pre><span>extension</span> <span>SIMD4</span> <span>{</span>
  <span>public</span> <span>var</span> <span><span>xyz</span></span><span>:</span> <span>SIMD3</span><span>&lt;</span><span>Scalar</span><span>&gt;</span> <span>{</span>
    <span>get</span> <span>{</span>
      <span>self</span><span>[</span><span>SIMD3</span><span>(</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>2</span><span>)</span><span>]</span>
    <span>}</span>
    <span>set</span> <span>{</span>
      <span>self</span><span>.</span>x <span>=</span> newValue<span>.</span>x
      <span>self</span><span>.</span>y <span>=</span> newValue<span>.</span>y
      <span>self</span><span>.</span>z <span>=</span> newValue<span>.</span>z
    <span>}</span>
  <span>}</span>
<span>}</span>

<span>// SIMD3&lt;Float&gt; vectors representing the XYZ position of each eye</span>
<span>let</span> <span>leftEyeWorldPosition</span> <span>=</span> leftViewWorldMatrix<span>.</span>columns<span>.</span><span>3</span><span>.</span>xyz
<span>let</span> <span>rightEyeWorldPosition</span> <span>=</span> rightViewWorldMatrix<span>.</span>columns<span>.</span><span>3</span><span>.</span>xyz</pre></div>
<p dir="auto">So what is the true camera position? We might need it in our shaders, to implement certain effects, etc.</p>
<p dir="auto">Since the difference between them is small, we can just pick the left eye and use its position as the unified camera world position.</p>
<div dir="auto" data-snippet-clipboard-copy-content="let cameraWorldPosition = leftEyeWorldPosition"><pre><span>let</span> <span>cameraWorldPosition</span> <span>=</span> leftEyeWorldPosition</pre></div>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong>: Why do we have to pick the left eye and not the right one? Xcode simulator uses the left eye to render, while the right one is ignored.</p>
</blockquote>
<p dir="auto">Or we can take their average:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let cameraWorldPosition = (leftEyeWorldPosition + rightEyeWorldPosition) * 0.5"><pre><span>let</span> <span>cameraWorldPosition</span> <span>=</span> <span>(</span>leftEyeWorldPosition <span>+</span> rightEyeWorldPosition<span>)</span> <span>*</span> 0.5</pre></div>
<p dir="auto">I use this approach in my code.</p>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> Using this approach breaks in Xcode's Apple Vision simulator! The simulator renders the scene for just the left eye. You will need to use the <code>#if targetEnvironment(simulator)</code> preprocessor directive to use only the <code>leftEyeWorldPosition</code> when running your code in the simulator.</p>
</blockquote>
<p dir="auto"><h3 tabindex="-1" dir="auto">Apple Vision Simulator</h3><a id="user-content-apple-vision-simulator" aria-label="Permalink: Apple Vision Simulator" href="#apple-vision-simulator"></a></p>
<p dir="auto">First of all, the Simulator renders your scene only for the left eye. It simply ignores the right eye. All of your vertex amplification code will work just fine, but the second vertex amplification will be ignored.</p>
<p dir="auto">Secondly, it also lacks some features (which is the case when simulating other Apple hardware as well). MSAA for example is not allowed so you will need to use the <code>#if targetEnvironment(simulator)</code> directive and implement two code paths for with MSAA and without.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Retreat to Muskworld (134 pts)]]></title>
            <link>https://niedermeyer.io/2024/10/11/the-retreat-to-muskworld/</link>
            <guid>41845596</guid>
            <pubDate>Tue, 15 Oct 2024 06:34:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://niedermeyer.io/2024/10/11/the-retreat-to-muskworld/">https://niedermeyer.io/2024/10/11/the-retreat-to-muskworld/</a>, See on <a href="https://news.ycombinator.com/item?id=41845596">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<figure><img loading="lazy" data-attachment-id="277" data-permalink="https://niedermeyer.io/2024/10/11/the-retreat-to-muskworld/screenshot-2024-10-11-174447/" data-orig-file="https://niedermeyer.io/wp-content/uploads/2024/10/screenshot-2024-10-11-174447.png" data-orig-size="847,644" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot 2024-10-11 174447" data-image-description="" data-image-caption="" data-medium-file="https://niedermeyer.io/wp-content/uploads/2024/10/screenshot-2024-10-11-174447.png?w=300" data-large-file="https://niedermeyer.io/wp-content/uploads/2024/10/screenshot-2024-10-11-174447.png?w=847" tabindex="0" role="button" width="847" height="644" src="https://niedermeyer.io/wp-content/uploads/2024/10/screenshot-2024-10-11-174447.png?w=847" alt="A Tesla Cybercab driving past a small town movie set, during Tesla's We, Robot event"><figcaption><em>A totally real robotaxi, driving in an extremely normal town</em></figcaption></figure>



<p>Almost eight years ago, Elon Musk announced that every Tesla made from that moment forward would be capable of Level 5 autonomous driving with nothing more than a software update. It was a pivotal moment in Tesla’s history, committing the company to not just succeed as an electric automaker, but solve one of the most ambitious AI and robotics challenges possible. To create confidence in that staggering aspiration, Tesla released a video of a Model X driving around Palo Alto autonomously to the Rolling Stones’ “Paint it Black,” claiming that the driver behind the wheel was only there “for legal purposes.”</p>



<p>Eight long and hype-filled years later, Tesla is still looking for ways to build confidence in its ability to deliver a “general solution to self-driving” through hype and spectacle, even as companies like Waymo deliver <a href="https://techcrunch.com/2024/08/20/waymo-is-now-giving-100000-robotaxi-rides-week/">the reality of 100,000 driverless taxi rides per week</a>. Rather than meeting the competitive challenge from Waymo with real driverless rides on real public streets, Tesla’s latest ploy for credibility sees the firm retreating ever deeper into fantasy, building what can only be described as a temporary theme park on a movie studio lot for its first ever “driverless” demonstration.</p>



<p>This contrast is instructive. The “Paint It Black” video of eight years ago was no more “real” or “fake” than yesterday’s “We, Robot” demonstration, but at least it had the pretense of reality: it depicted a real car on real roads. Tesla’s latest spectacle likely cost orders of magnitude more to produce, but it didn’t even purport to show any actual real-world capability. The entire thing was pure fantasy, in a contained fantasy world, built on a movie theater lot that exists for the sole purpose of producing such spectacles.</p>



<p>This trajectory, from simulating future capability on public roads to creating a fantasy world for fantasy cars to show off fantasy capabilities, should worry Tesla’s supporters. We can already see Musk retreating into a misinformation-fueled fantasy world every day on Twitter, and the jarring divisiveness of the Cybertruck suggests that his runaway ego is already making Tesla’s products less palatable. If Musk’s retreat into a self-soothing fantasy bubble is also making his hype game less effective, and the 8% drop in Tesla’s stock price suggests that it is, his most important skill set is on the line.</p>



<p>Of course, with Wall Street analysts almost universally declaring themselves “underwhelmed,” it has to be asked: what else could they have possibly expected? Did they really believe that now, after eight years of empty hype, fake statistics, and blown deadlines, Tesla would actually start providing credible evidence to back the litany of bullshit? Having made no effort to explain his chronic inability to meet (let alone stop making) his self-imposed deadlines, and facing no real consequences for nearly a decade of what is either unprecedented public delusion or deception, why on earth would Elon Musk make a serious play for credibility now?</p>



<p>Having drawn a poor hand eight years ago, <a href="https://niedermeyer.io/2024/04/22/no-more-rebuys-mr-musk/">Elon Musk is playing poker the only way he knows how: going all-in on every hand</a>. This strategy has created a confidence game of unprecedented proportions in our financial markets, as every gambler in the casino wants to put a chip on the guy going all-in and winning every time, and only the $13 billion of hung debt for the Twitter deal suggests his hot streak might ever end. All Musk has to do to keep the music playing is to project confidence, which is infinitely easier to do in a studio lot setpiece than out on public roads.</p>



<p>For everyone not locked into this financial-cognitive nightmare, it’s hard to imagine anyone seriously believing that a night of delusional Disney Adult cringe might actually inflate Tesla’s stock beyond the current ~$680 billion valuation. Given that Tesla’s “Full Self-Driving” is already the subject of a multi-year <a href="https://apnews.com/article/tesla-investigations-justice-department-musk-self-driving-29a68864f75c9fabbd04f7a87d169444">federal investigation</a> into <a href="https://www.reuters.com/business/autos-transportation/tesla-autopilot-probe-us-prosecutors-focus-securities-wire-fraud-2024-05-08/">securities and wire fraud</a>, We, Robot’s blatant fantasy-mongering is downright shocking. If anything, showing a low-speed, closed-course theme park ride in order to build confidence around Tesla’s progress toward actual real-world driverless capability is almost too childish to call a fraud. </p>



<p>Ultimately, Musk’s increasingly-degenerate gambling run is slouching toward one last big coinflip: the 2024 presidential election. With <a href="https://www.nytimes.com/2024/10/11/us/politics/elon-musk-donald-trump-pennsylvania.html">Musk going “all-in” on Donald Trump</a>, and <a href="https://nypost.com/2024/10/07/us-news/elon-musk-suggests-hell-be-thrown-in-prison-if-harris-beats-trump-if-he-loses-im-fed/">musing that he will end up in a prison cell if Kamala Harris is elected</a>, it’s clear that his main political issue is his freedom to keep rolling over his endless confidence game without legal consequences. If Trump wins and delivers Musk the impunity he craves, the line between amusement park fantasy and $700 billion self-driving juggernaut will all but disappear, and we will all find ourselves living in Muskworld’s house of mirrors.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Superstitious Users and the FreeBSD Logo (108 pts)]]></title>
            <link>https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006642.html</link>
            <guid>41845427</guid>
            <pubDate>Tue, 15 Oct 2024 06:10:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006642.html">https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006642.html</a>, See on <a href="https://news.ycombinator.com/item?id=41845427">Hacker News</a></p>
<div id="readability-page-1" class="page">
   
<!--htdig_noindex-->
    <b>Brett Glass</b> 
    <a href="mailto:freebsd-chat%40freebsd.org?Subject=Superstitious%20users%20and%20the%20FreeBSD%20logo&amp;In-Reply-To=" title="Superstitious users and the FreeBSD logo">brett at lariat.net
       </a><br>
    <i>Wed Nov 30 02:30:04 UTC 2011</i>
    <ul>
        <li>Previous message: <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006641.html">Query about about freebsd-chat digestt, Vol 398, Issue 1
</a></li>
        <li>Next message: <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006643.html">Superstitious users and the FreeBSD logo
</a></li>
         <li> <b>Messages sorted by:</b> 
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/date.html#6642">[ date ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/thread.html#6642">[ thread ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/subject.html#6642">[ subject ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/author.html#6642">[ author ]</a>
         </li>
       </ul>
    <hr>  
<!--/htdig_noindex-->
<!--beginarticle-->
<pre>Everyone:

I just got a call from the owner of a hotel for which we provide 
hotspot service. She says that a guest spotted the "Powered by 
FreeBSD" logo at the bottom of the login page, and was offended; 
the guest was convinced that either we or the hotel management 
"worshipped the Devil" and refused to stay at the hotel unless the 
logo was removed. The owner could make no headway by explaining 
that the besneakered mascot was a cartoon character and was a 
daemon, not the Devil. And she feared upsetting the guest even more 
if she said that large portions of the same software are inside 
every Mac and iPad. The hotel stands to lose more than $1000 if the 
guest, who had originally planned to stay for a long period, moves out.

One of our tech support people also got a call directly from the 
hotel guest, who claimed that having the logo on the page 
constituted "abuse." The guest also claimed to be "losing money" 
because she wouldn't use the hotspot if there was a "devil" on the 
splash page. He didn't even realize what she was talking about at 
first.... He couldn't imagine why on Earth this person was calling 
him and going on about devils.

Attempts at misguided religious censorship notwithstanding, I don't 
want to see one of my  ISP's customers lose business. And I'd like 
to keep a FreeBSD logo on our hotspot page. Is there artwork that 
doesn't include horned creatures that might offend the ignorant or 
superstitious?

--Brett Glass

</pre>


<!--endarticle-->
<!--htdig_noindex-->
    <hr>
    <ul>
        <!--threads-->
	<li>Previous message: <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006641.html">Query about about freebsd-chat digestt, Vol 398, Issue 1
</a></li>
	<li>Next message: <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006643.html">Superstitious users and the FreeBSD logo
</a></li>
         <li> <b>Messages sorted by:</b> 
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/date.html#6642">[ date ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/thread.html#6642">[ thread ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/subject.html#6642">[ subject ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/author.html#6642">[ author ]</a>
         </li>
       </ul>

<hr>
<a href="http://lists.freebsd.org/mailman/listinfo/freebsd-chat">More information about the freebsd-chat
mailing list</a><br>
<!--/htdig_noindex-->

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cheating alleged after men's world conker champion found with steel chestnut (394 pts)]]></title>
            <link>https://www.theguardian.com/sport/2024/oct/14/cheating-alleged-after-mens-world-conker-champion-found-with-steel-chestnut</link>
            <guid>41844545</guid>
            <pubDate>Tue, 15 Oct 2024 03:12:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/sport/2024/oct/14/cheating-alleged-after-mens-world-conker-champion-found-with-steel-chestnut">https://www.theguardian.com/sport/2024/oct/14/cheating-alleged-after-mens-world-conker-champion-found-with-steel-chestnut</a>, See on <a href="https://news.ycombinator.com/item?id=41844545">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>The World Conker Championships is investigating cheating allegations after the men’s winner was found to have a steel chestnut in his pocket.</p><p>David Jakins won the annual title in Southwick, <a href="https://www.theguardian.com/uk-news/northamptonshire" data-link-name="in body link" data-component="auto-linked-tag">Northamptonshire</a>, on Sunday for the first time after competing since 1977.</p><p>But the 82-year-old was found to have a metal replica in his pocket when he was searched by organisers after his victory.</p><p>The retired engineer has denied using the metal variety in the tournament.</p><p>Jakins was responsible for drilling and inserting strings into other competitors’ chestnuts as the competition’s top judge, known as the “King Conker”.</p><p>Alastair Johnson-Ferguson, who lost in the men’s final against Jakins, said he suspected “foul play”, the Telegraph reported.</p><p>The 23-year-old said: “My conker disintegrated in one hit, and that just doesn’t happen … I’m suspicious of foul play and have expressed my surprise to organisers.”</p><p>Kelci Banschbach, 34, from Indianapolis, defeated the men’s champion in the grand final to become the first American to win the competition. More than 200 people took part.</p><p>Jakins said: “I was found with the steel conker in my pocket, but I only carry [it] around with me for humour value and I did not use it during the event.</p><p>“Yes, I did help prepare the conkers before the tournament. But this isn’t cheating or a fix, and I didn’t mark the strings.”</p><p>St John Burkett, a spokesperson for the World Conker Championships, said the cheating claims were being investigated.</p><p>“Allegations of foul play have been received that somehow King Conker swapped his real conker for the metal one later found in his pocket.</p><p>“Players select conkers from a sack before each round.</p><p>“There are also suggestions that King Conker had marked the strings of harder nuts. We can confirm he was involved in drilling and lacing the nuts before the event.</p><p>“We are investigating.”</p><p>More than 2,000 conkers had been prepared prior to the event.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zamba2-7B (275 pts)]]></title>
            <link>https://www.zyphra.com/post/zamba2-7b</link>
            <guid>41842975</guid>
            <pubDate>Mon, 14 Oct 2024 22:45:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.zyphra.com/post/zamba2-7b">https://www.zyphra.com/post/zamba2-7b</a>, See on <a href="https://news.ycombinator.com/item?id=41842975">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-animation="default" data-collapse="tiny" data-duration="400" data-easing="ease" data-easing2="ease" role="banner"><p><a href="https://www.zyphra.com/"><img width="123" sizes="(max-width: 479px) 123px, (max-width: 767px) 22vw, 123px" alt="" src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch.png" loading="lazy" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch.png 1505w"></a></p></div><div><p><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash.jpg" loading="lazy" width="726" height="Auto" alt="" sizes="(max-width: 479px) 90vw, (max-width: 1279px) 80vw, 90vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-500.jpg 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-800.jpg 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-1080.jpg 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-1600.jpg 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-2000.jpg 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-2600.jpg 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-3200.jpg 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash.jpg 5760w"></p><p><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670700157cd6aceff5b383ed_zamba27b.png" loading="lazy" width="648" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670700157cd6aceff5b383ed_zamba27b-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670700157cd6aceff5b383ed_zamba27b-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670700157cd6aceff5b383ed_zamba27b.png 1470w"></p></div><div><p>October 14, 2024</p><p>PALO ALTO, CALIFORNIA</p><p>Zyphra is excited to release Zamba2-7B, a state-of-the-art small language model. At the 7B scale, we outperform the leading models of Mistral, Google’s Gemma and Meta’s Llama3 series in both quality and performance. We believe Zamba2-7B is the leading model for running on-device and on consumer GPUs as well as for many enterprise applications which require a powerful but compact and efficient model for natural-language tasks.</p><p>Authors</p><p>Zyphra Team</p><p>Collaborators</p><p>Daniel A Roberts (Sequoia Capital &amp; MIT), Andrey Gromov (Meta FAIR), Kushal Tirumala (Meta FAIR) and Hassan Shapourian (Cisco)</p></div><div id="introduction"><div><div id="zamba1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="zamba2"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="Quality vs. Inference Speed" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="zamba3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div></section></div><div><div id="zyda1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="zyda2"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="zyda3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div></section><section id="zyda4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section></div><div><section id="small1"><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><div><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div></section><section id="small2"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="small3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section><section id="small4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a></section><section id="small5"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><div><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div></section></div><div><div id="rag1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="rag2"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="rag3"><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section><section id="rag4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a></section><div id="rag5"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></div><div><div id="tree1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="tree2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div></section><section id="tree3"><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" width="674" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" width="586" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section></div><div><div id="layer1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="layer2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" width="619" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" width="658" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" width="680" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div></section><div id="layer3"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></div><div><section id="edge1"><div><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" width="455" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><div id="edge2"><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><section id="edge3"><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section><div id="edge4"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><div id="edge5"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></div><div><div id="hop1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="hop2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="hop3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" width="581" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div></section><section id="hop4"><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section><div id="hop5"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><section id="hop6"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" width="367" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section><section id="hop7"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" width="479" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a></section><section id="hop8"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" width="458" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w"></a></section><section id="hop9"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png" loading="lazy" width="360" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png 1389w"></a></section></div><div><div target="_blank" id="cook1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="cook2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="cook3"><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a></section><div id="cook4"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><div id="cook5"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div><section id="cook7"><p>What is Annealing?</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a></section><section id="cook8"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a></section><section id="hop9"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png 1389w"></a></section></div><div><div target="_blank" id="mini1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="mini2"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="mini3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a></section><section id="mini4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a></section><section id="mini5"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png" loading="lazy" width="384" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div></section></div><div><section id="noc1"><div target="_blank"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" width="429" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="noc2"><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" width="267" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" width="319" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a><div><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section><section id="noc3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" width="400" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a></section></div><div><div target="_blank" id="longrag1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="longrag2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><div target="_blank" id="longrag3"><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><section id="longrag4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><div target="_blank"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section><section id="longrag5"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div target="_blank"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section></div><div><div target="_blank" id="zamba2_1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="zamba2_2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" width="570" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" width="570" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div target="_blank"><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" width="570" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" width="319" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a></section><section id="zamba2_3"><div target="_blank"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" width="571" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png" loading="lazy" width="570" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png" loading="lazy" width="571" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png 1389w"></a><div target="_blank"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section></div><div><div target="_blank" id="zyda2_1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="zyda2_2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="zyda2_3"><div target="_blank"><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div target="_blank"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div target="_blank"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section><section id="zyda2_4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a></section><section id="zyda2_5"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png 1389w"></a></section><section id="zyda2_7"><h3>Analysis of Global Duplicates</h3><p>We present histograms depicting distribution of cluster sizes in all the datasets (see Fig. 7-11). Please, note that all the figures are in log-log scale. We see a significant drop in the number of clusters starting from the size of around 100. This drop is present both in DCLM and FineWeb-Edu2 (see Fig. 8 and 9 respectively), and most likely is explained by a combination of&nbsp; the deduplication strategy and quality when creating both datasets: DCLM deduplication was done individually within 10 shards, while FineWeb-Edu2 was deduplicated within every Common Crawl snapshot. We find that large clusters usually contain low quality material (repeated advertisements, license agreements templates, etc), so it’s not surprising that such documents were removed. Notably, DCLM still contained one cluster with the size close to 1 million documents, containing low quality documents seemingly coming from the advertisements (see Appendix).We find both Zyda-1and Dolma-CC contain a small amount of duplicates, which is expected, since both datasets were deduplicated globally by their authors. Remaining duplicates are likely false negatives from the initial deduplication procedure. Note, that distribution of duplicates clusters sizes of these two datasets (Fig. 10 and 11) don’t contain any sharp drops, but rather hyper exponentially decreases with cluster size.&nbsp;</p><a href="#"><img src="https://cdn.prod.website-files.com/img/placeholder-thumb.svg" loading="lazy" alt=""></a><h5><em>Figure 7: Distribution of cluster sizes of duplicates in global dataset (log-log scale).</em></h5><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9.png" loading="lazy" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9.png 1600w" alt=""></a><h5><em>Figure 8: Distribution of cluster sizes of duplicates in DCLM (log-log scale).</em></h5><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10.png" loading="lazy" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10.png 1600w" alt=""></a><h5><em>Figure 9: Distribution of cluster sizes of duplicates in FineWeb-Edu2 (log-log scale).</em></h5><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11.png" loading="lazy" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11.png 1600w" alt=""></a><h5><em>Figure 10: Distribution of cluster sizes of duplicates in Zyda-1 (log-log scale).</em></h5><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12.png" loading="lazy" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12.png 1600w" alt=""></a><h5><em>Figure 11: Distribution of cluster sizes of duplicates in Dolma-CC (log-log scale).</em></h5><h3>Largest cluster in DCLM</h3><p>Below is an example of the document from the largest cluster (~1M documents) of duplicates in DCLM (quality score 0.482627):<br>‍<em>Is safe? Is scam?<br>Is safe for your PC?<br>Is safe or is it scam?<br>Domain is SafeSafe score: 1</em>‍<br>‍<em>The higher the number, the more dangerous the website.Any number higher than 1 means DANGER.</em>‍<br>‍<em>Positive votes:<br>Negative votes:<br>Vote Up Vote Down review</em>‍<br>‍<em>Have you had bad experience with Warn us, please!</em></p><h3>Examples of varying quality score in DCLM in a cluster</h3><p>Below one will find a few documents with different quality scores from DCLM coming from the same duplicates cluster. Quality score varies from ~0.2 to ~0.04.</p></section></div></div>
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Physics of Magic Windows (2021) (172 pts)]]></title>
            <link>https://mattferraro.dev/posts/caustics-engineering</link>
            <guid>41842775</guid>
            <pubDate>Mon, 14 Oct 2024 22:25:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mattferraro.dev/posts/caustics-engineering">https://mattferraro.dev/posts/caustics-engineering</a>, See on <a href="https://news.ycombinator.com/item?id=41842775">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><p><time datetime="2021-08-18">August 18, 2021</time></p><p>I recently made a physical object that defies all intuition. It's a square of acrylic, smooth on both sides, totally transparent. A tiny window.</p><div><p><img alt="Clear Acrylic" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fwindow_clear.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fwindow_clear.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fwindow_clear.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy"></p></div><p>But it has the magic property that if you shine a flashlight on it, it forms an image:</p><div><p><img alt="2D Image of Cat" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2F2D_image.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2F2D_image.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2F2D_image.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy"></p></div><p>And if you take it out in the sun, it produces this 3D hologram:</p><video width="100%" height="auto" autoplay="" muted="" controls="" loop=""><source src="https://mattferraro.dev/images/caustics-engineering/3dcat.mp4" type="video/mp4"></video><p>This post describes the math that went into making the object, and how you can create your own.</p><h2 id="but-first-how-is-this-even-possible">But first: How is this even possible?</h2><p>Let's focus on the 2D image before talking about the hologram.</p><p>The physical phenomenon we're looking at is called a <em>caustic</em>.</p><div><p><img alt="Example Caustic" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fglass_caustic_pd.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fglass_caustic_pd.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fglass_caustic_pd.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy"></p></div><p>Caustics are the bright patches of light we see when illuminating a transparent object. All the photons that don't pass directly through the object are what form the object's shadow. All those photons still have to go somewhere; they contribute to the caustic pattern.</p><p>The most interesting aspect of caustics is that they arise from even the tiniest of variations in surface flatness. Even the gentlest waves on the surface of a pool form powerful lenses that cast intense caustics on the floor below.</p><div><p><img alt="Water Caustics" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fwater_caustics_cc.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fwater_caustics_cc.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fwater_caustics_cc.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy"></p></div><p>The reason my acrylic square can form an image is because I've distributed just the right amount of concavity and convexity into the surface so that the refracted light forms a caustic image.</p><p>To gain some intuition for how it is done, consider a traditional convex lens:</p><div><p><img alt="Parabolic Lens" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ftraditional_lens_2.svg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ftraditional_lens_2.svg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ftraditional_lens_2.svg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>This lens forms the simplest possible caustic. If all the incoming light is from a single, very distant light source like the Sun, this lens focuses all of its incoming light into a single point. The caustic image from this lens is dark everywhere with one very bright spot in the center.</p><p>Zooming in on one small section of the lens we notice a few properties:</p><ol><li>The overall thickness of the lens does not have a direct impact on the outgoing ray angle. We could add material to the left side of this lens and nothing would change. The first transition, from air to glass, can be entirely ignored.</li><li>The angle between the incoming light rays and the glass-air boundary has a strong effect on the refracted ray angle.</li><li>Whether two rays converge or diverge is controlled by how <em>curved</em> the lens is where the glass meets the air</li></ol><p>In other words, the height of the glass <span>h(x)</span> is not on its own important. But the slope of the glass, <span>\frac{\mathrm{d}h}{\mathrm{d}x}</span>, gives us the outgoing ray angle via Snell's law. Where rays converge the image is brighter than the light source. Where rays diverge the image is darker. Therefore the brightness of the image (at that point, where the rays fall) is related to <span>\frac{\mathrm{d}^2h}{\mathrm{d}x^2}</span>.</p><p>The thickness of my acrylic slab varies across the entire <span>xy</span> plane, so I'll call it <span>h(x,y)</span> and we'll think of it as a <strong>heightmap</strong>.</p><p>By controlling <span>\nabla h = (\frac{\partial h}{\partial x}, \frac{\partial h}{\partial y}</span>), and <span>\nabla ^2 h = (\frac{\partial ^2 h}{\partial x^2} + \frac{\partial ^2 h}{\partial y^2})</span>, we can steer all of our incoming light to the correct locations in the image, while contributing the right brightness to make it recognizable. By making some simplifying assumptions we can guarantee that the resulting heightmap will be smooth and continuous.</p><p>For the Magic Window shown above, the total height variation over the <span>10cm \times 10cm</span> surface is about <span>2.0mm</span>.</p><div><p><img alt="Slight Refraction" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fslight_refraction.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fslight_refraction.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fslight_refraction.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>See how the slight variations in surface height distort the straight line of the floor moulding? Our Magic Window works like any other lens—by bending light.</p><h2 id="table-of-contents">Table of Contents</h2><ul><li><a href="#formulating-the-problem">Formulating the Problem</a></li><li><a href="#steps-to-a-solution">Steps to a Solution</a></li><li><a href="#morphing-the-cells">Morphing the Cells</a><ul><li><a href="#computing-the-loss">Computing the Loss</a></li><li><a href="#stepping-to-reduce-loss">Stepping to Reduce Loss</a></li></ul></li><li><a href="#snells-law-and-normal-vectors">Snell's Law and Normal Vectors</a></li><li><a href="#finding-the-heightmap">Finding the Heightmap</a></li><li><a href="#manufacturing">Manufacturing</a></li><li><a href="#acknowledgements">Acknowledgements</a></li><li><a href="#my-code">My Code</a></li><li><a href="#licensing">Licensing</a></li><li><a href="#contact-me">Contact me</a></li><li><a href="#one-last-thing">One Last Thing</a></li></ul><h2 id="formulating-the-problem">Formulating the Problem</h2><p>We want to find a heightmap <span>h(x,y)</span> whose caustic image has brightness <span>b(u,v)</span>, equal to some input image. To achieve this we can imagine a grid of cells, akin to pixels, on the surface of the acrylic lens. Here each "pixel" on the lens corresponds to a pixel in the image. Image pixels and their corresponding lens-space "pixels" are labeled with shared <span>(i, j)</span> coordinates.</p><div><p><img alt="Diagram" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fdiagram.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fdiagram.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fdiagram.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Remember that <span>(i, j)</span> are integers labeling the column and row of the pixel, whereas <span>(x, y)</span> and <span>(u, v)</span> are real numbers measured in something like meters or inches.</p><h2 id="steps-to-a-solution">Steps to a Solution</h2><p><strong>Step 1:</strong> We morph the cells on the lens, making them bigger or smaller, so that the area of lens cell <span>(i, j)</span> is proportional to the brightness of image cell <span>(i, j)</span>. The resulting lens grid is no longer square—lots of warping and skew have to be introduced to maintain continuity. This step is by far the hardest part and must be solved iteratively.</p><p><strong>Step 2:</strong> For each cell <span>(i, j)</span> we need to find the angle from the lens cell to image cell and use Snell's law to find the required surface normal. This step is straightforward geometry.</p><p><strong>Step 3:</strong> Integrate all the surface normals to find a continuous heightmap <span>h(x,y)</span>. We're back to iterative methods here, but if we apply certain contraints to how we solve step 1, this step is actually fast and easy.</p><h2 id="morphing-the-cells">Morphing the Cells</h2><p>For an image with <span>n \times n</span> pixels, the lens grid will need <span>(n+1) \times (n+1)</span> points, so that each cell in the lens grid is defined by four points. Technically we should adopt yet another coordinate system to label the <em>points</em> in the lens grid since they are distinct from the <em>cells</em> in the lens grid, but I think it's easier to just reuse <span>(i, j)</span> and we can say that for grid cell <span>(i, j)</span>, the point in the upper left is defined as grid point <span>(i, j)</span>.</p><div><p><img alt="Diagram 2" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fdiagram2.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fdiagram2.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fdiagram2.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>This leaves us with one row and one column of extra grid points along the bottom and right edges, but that will be trivial to deal with when it comes up.</p><p>Each <em>point</em> in the lens grid <span>(i,j)</span> has an <span>(x, y)</span> coordinate. A point's <span>(i, j)</span> coordinates never change but the <span>(x, y)</span> coordinates will change as we morph the cells more and more.</p><h2 id="computing-the-loss">Computing the Loss</h2><p>Given the <span>(x, y)</span> locations of all the lens grid points, simple geometry lets us calculate the area of each lens grid cell. Of course at first every cell has the same area, but that will change as soon as we start morphing things.</p><p>The condition we want is that every lens grid <em>cell</em> <span>(i, j)</span> has an <em>area</em> which scales with the <em>brightness</em> of image pixel <span>b(i, j)</span>.</p><p>Area and brightness are not compatible units so it is helpful to normalize cell area by the full window area, and pixel brightness by total image brightness, so that each is measured in a unitless "percentage".</p><p>\tag{1.0}
\frac{A_{ij}}{\Sigma A} = \frac{b_{ij}}{\Sigma b}</p><p>Intuitively, this means:</p><blockquote><p>If a single pixel contributes <span>x\%</span> of the brightness of the entire image, the corresponding window cell should take up <span>x\%</span> of the area of the entire window.</p></blockquote><p>Equation <span>(1.0)</span> is the goal, but it will not be not be true until after we've morphed the window grid. Until we've done that, we need to compute a loss function which tells us how badly we're missing our target. Something like:</p><p>\tag{1.1}
L = \frac{b_{ij}}{\Sigma b} - \frac{A_{ij}}{\Sigma A}</p><p>In code:</p><pre><code><span># In Julia-flavored psuedocode</span>
img <span>=</span> read_image<span>(</span><span>"cat.png"</span><span>)</span>
brightness <span>=</span> convert_to_grayscale<span>(</span>img<span>)</span>
total_brightness <span>=</span> sum<span>(</span>brightness<span>)</span>
brightness <span>=</span> brightness <span>.</span><span>/</span> total_brightness

w <span>=</span> <span>.1</span> <span># meters</span>
h <span>=</span> <span>.1</span> <span># meters</span>
area_total <span>=</span> w <span>*</span> h
loss <span>=</span> compute_pixel_area<span>(</span>grid<span>)</span> <span>.</span><span>/</span> area_total <span>-</span> brightness
</code></pre><div><p><img alt="Image and Loss Function" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fimage_and_loss.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fimage_and_loss.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fimage_and_loss.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Where I've colorized the loss function so that red areas indicate regions where our grid cells need to grow and blue regions indicate where our grid cells need to shrink.</p><p>This image is the loss function <span>L</span> and I'll refer to it a lot. </p><h2 id="stepping-to-reduce-loss">Stepping to Reduce Loss</h2><p>The loss image can be thought of as a scalar field <span>L(x, y)</span>. The gradient of a scalar field yields a vector field, which we could call <span>\nabla L(x,y)</span>. We can step each grid point slowly in the direction of the gradient field, and in doing so the cells that are too small will get bigger and the cells that are too big will get smaller. Our loss will shrink, and we'll create our image!</p><p>The first thing to do is compute <span>\nabla L</span> and look at the vector field:</p><div><p><img alt="Gradient of L as a vector field" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fgrad_L.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fgrad_L.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fgrad_L.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Crap.</p><p><span>\nabla L</span> is a very poorly behaved vector field. It is noisy, discontinuous, and in many places equal to zero.</p><p>Almost everywhere, neighboring points need to step in drastically different directions. This creates a situation where improving one cell's loss will necessarily worsen its neighbor's losses, which means that in practice this method can never converge. It's a dead end.</p><hr><p>Instead let's draw an analogy to Computational Fluid Dynamics. We need to dilate certain cells and shrink others according to a brightness function. This is similar to modeling compressible air flow where each cell has pressure defined as a pressure function.</p><p>If every cell in a 2D grid has some initial pressure, how does the system relax over time? The regions with high pressure expand and the regions of low pressure contract, with regions of middling pressure getting shoved around in a sort of global tug-of-war. Clearly, our problem is analogous.</p><p>So, how is this problem solved in CFD simulations? A standard approach is to define a <strong>Velocity Potential</strong> called <span>\Phi</span> (read: <em>phi</em>). The Velocity Potential <span>\Phi</span> is a scalar field defined at each cell. Its units are <span>meters^2 / second</span> which at first glance is not very easy to interpret. But the reason <span>\Phi</span> is convenient is that its spatial derivatives are measured in <span>meters/second</span>. In other words, the gradient of <span>\Phi</span> gives a vector whose units are velocity:</p><p>\tag{1.2}
\nabla \Phi = \left( \frac{\partial{\Phi}}{\partial{x}}, \frac{\partial{\Phi}}{\partial{y}} \right) = \vec{v}</p><div><p><img alt="Phi" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_phi.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_phi.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_phi.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Here is an example <span>\Phi</span>. It is just some scalar field best viewed as a heightmap.</p><div><p><img alt="Gradient of Phi" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_grad_phi.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_grad_phi.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_grad_phi.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>And here is the gradient of that same <span>\Phi</span>. These vectors are velocity vectors that point uphill. If we were performing Computational Fluid Dynamics, these vectors would indicate how fluid might flow from regions of high pressure to regions of low pressure. </p><p>Notice how well behaved this vector field is! There is gentle variation across the field but any two neighbors are very similar to each other. None of the arrows pierce the boundary.</p><p>In our case we don't have fluid pressure, we have light pressure. Regions in our image which are too bright have high light pressure, which is quantified in our loss function <span>L</span>.</p><p>If we can somehow use <span>L</span> to find a <span>\Phi</span> that describes our light pressure distribution, all we need to do is calculate <span>\vec{v} = \nabla \Phi</span> and we'll be able to morph all of our lens grid points according to <span>\vec{v}</span> to decrease our loss!</p><p>So how do we find a suitable <span>\Phi</span>? Well, the property we know about each cell is its loss, which encodes how much that cell needs to grow or shrink. </p><blockquote><p>This property, how much a cell grows or shrinks over time as it moves with a velocity field, is called the <strong>divergence</strong> of that field.</p></blockquote><p>Divergence is written as <span>\nabla \cdot</span>, so in our case, we know that we need to find a velocity field <span>\vec{v}</span> whose divergence equals the loss:</p><p>\tag{1.3}
\nabla \cdot \vec{v} = L(x, y)</p><p>Unfortunately there is no "inverse divergence" operator so we cannot easily invert this equation to find <span>\vec{v}</span> directly. But we <em>can</em> plug equation <span>(1.2)</span> in to equation <span>(1.3)</span> to yield:</p><p>\tag{1.4}
\nabla \cdot \nabla \Phi = L(x, y)</p><p>Which we read as <em>The divergence of the gradient of the potential field <span>\Phi</span> equals the loss</em>.</p><p>This equation comes up surprisingly frequently in many branches of physics and math. It is usually written in a more convenient shorthand:</p><p>\tag{1.5}
\nabla ^2 \Phi = L</p><p>Which you may recognize as <a href="https://mattferraro.dev/posts/poissons-equation">Poisson's Equation</a>!</p><p>This is fantastic news because Poisson's equation is <a href="https://mattferraro.dev/posts/poissons-equation#how-do-i-solve-it">extremely easy</a> to solve! If you aren't familiar with it, just think of this step like inverting a big matrix, or numerically integrating an ODE, or finding the square root of a real number. It's an intricate, tedious task that would be painful to do with a paper and pencil, but it's the kind of thing computers are <em>really</em> good at.</p><p>Now that we've written down the problem as Poisson's Equation, it is as good as solved. We can use any off the shelf solver, plug in our known <span>L(x, y)</span> using Neumann boundary conditions and boom, and out pops <span>\Phi(x,y)</span> as if by magic.</p><div><p><img alt="Phi" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fclearly_cat_phi.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fclearly_cat_phi.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fclearly_cat_phi.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Can you figure out why the cat appears so clearly in this 3D rendering of <span>\Phi</span>? What controls the brightness of each pixel in a render like this?</p><p>We plug <span>\Phi</span> in to Equation <span>(1.2)</span> to find <span>\vec{v}</span> and we take a look at the vector field:</p><div><p><img alt="Gradient of Phi" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_grad_phi.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_grad_phi.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_grad_phi.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Disappointingly, it does not look like a kitty to me.</p><p>And technically we need to march our points in the direction of <em>negative</em> <span>\nabla L</span> if we want to <em>decrease</em> <span>L</span>. Here's <span>-\nabla L</span>:</p><div><p><img alt="Negative Gradient of Phi" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fnegative_grad_phi.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fnegative_grad_phi.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fnegative_grad_phi.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>But the good news is that this vector field is smooth and well-behaved. We simply march the grid points along this vector field and we'll get exactly what we need. </p><p>If you squint you can almost see how the bright background will expand and the cat's dark fur will shrink.</p><div><p><img alt="Image and Vector Field" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fside_by_side.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fside_by_side.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fside_by_side.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>We step all the lens grid points forward some small amount in the direction of <span>-\vec{v}</span>. After morphing the grid a tiny amount we recompute the loss function <span>L</span>, find a new <span>\Phi</span> and new <span>-\vec{v}</span>, and take another small step.</p><pre><code><span># In Julia-flavored psuedocode</span>
image <span>=</span> read_image<span>(</span><span>"cat.png"</span><span>)</span>
gray <span>=</span> convert_to_grayscale<span>(</span>image<span>)</span>
grid <span>=</span> create_initial_grid<span>(</span>gray<span>.</span>size <span>+</span> <span>1</span><span>)</span>

L <span>=</span> compute_loss<span>(</span>gray<span>,</span> grid<span>)</span>

<span>while</span> max<span>(</span>L<span>)</span> <span>&gt;</span> <span>0.01</span>
    ϕ <span>=</span> poisson_solver<span>(</span>L<span>,</span> <span>"neumann"</span><span>,</span> <span>0</span><span>)</span>
    v <span>=</span> compute_gradient<span>(</span>ϕ<span>)</span>
    grid <span>=</span> step_grid<span>(</span>grid<span>,</span> <span>-</span>v<span>)</span>
    L <span>=</span> compute_loss<span>(</span>gray<span>,</span> grid<span>)</span>
<span>end</span>
</code></pre><p>After three or four iterations the loss gets very small and we've got our morphed cells!</p><div><p><img alt="Grid After Warping" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fside_by_side_warped.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fside_by_side_warped.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fside_by_side_warped.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Look at how this cat's chin ballooned out but her nose and forehead shrunk. Her left ear is noticably longer and thinner because the bright background had to grow to take up more light. Her pupils went from oblong to sharp.</p><p>Note that image on the right is just a screenshot of Fusion360's default mesh rendering with the wireframe turned on:</p><div><p><img alt="Screenshot of Fusion360" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2FFusion360.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2FFusion360.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2FFusion360.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>The reason it is darker in some areas is because the mesh is more tightly packed in those areas. Let's zoom in on the eye:</p><div><p><img alt="Zoom in on the Eye" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fzoom_eye.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fzoom_eye.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fzoom_eye.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Look at how detailed that is! We've managed to capture even the bright reflections in her eyes. Zooming in further to just the pupil:</p><div><p><img alt="Zoom in on the Pupil" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fzoom_pupil.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fzoom_pupil.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fzoom_pupil.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>We can see the fine structure of the grid cells. Our formulation of the problem is only concerned with cells as quadralaterals. The triangles you see are just an artifact of converting our quadralateral grid into a triangle mesh more suitable for other software to deal with.</p><p>So again, in summary:</p><div><p><img alt="Overall Flow" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Foverall_flow.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Foverall_flow.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Foverall_flow.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>If we follow these steps we will successfully morph our grid points. Now we've got to do some geometry!</p><h2 id="snells-law-and-normal-vectors">Snell's Law and Normal Vectors</h2><p>Snell's law tells us how light bends when passing from one material to another. </p><div><p><img alt="Snell's Law" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fsnells_law_2.svg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fsnells_law_2.svg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fsnells_law_2.svg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>\tag{2.0}
\frac{\sin(\theta_2)}{\sin(\theta_1)} = \frac{n_1}{n_2}</p><p>Where <span>n_1 = 1.49</span> is the <a href="https://en.wikipedia.org/wiki/Refractive_index">Refractive Index</a> of acrylic and <span>n_2 = 1</span> is the refractive index of air. If we know <span>\theta_2</span>, Snell's Law gives us <span>\theta_1</span>.</p><p>Snell's law is not some arbitrary axiom of physics. It is a direct consequence of Fermat's <a href="https://en.wikipedia.org/wiki/Fermat%27s_principle">Principle of Least Time</a>, which is a fascinating and critical link between ray optics and wave optics. But that's a topic for another day.</p><p>In our case, each lens cell <span>(i, j)</span> has migrated to position <span>(x, y)</span>, and it needs to send its light to the image plane at <span>(u, v)</span>, which sits some distance away <span>d</span>.</p><p>We start by defining a 3D normal vector <span>\vec{N}(x, y)</span> which everywhere points normal to our heightmap <span>h(x, y)</span>.</p><div><p><img alt="Example Surface Normals" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fsurface_normals.svg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fsurface_normals.svg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fsurface_normals.svg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Normal vectors always point perpendicular to the surface they start on. They generally encode meaning in their direction, not their length, so we're free to scale them to any length that is convenient for our purposes. Very often people choose to make their Normal vectors of length <span>1</span>.</p><p>But if we normalize <span>\vec{N}</span> so that its <span>z</span> coordinate is <span>-1</span>, we can write it:</p><p>\tag{2.1}
\vec{N} = (\frac{\partial{h}}{\partial{x}}, \frac{\partial{h}}{\partial{y}}, -1)</p><p>If you consider just the <span>x</span> and <span>y</span> components, we recognize that</p><p>\tag{2.2}
\vec{N}_{xy} = \nabla h</p><p>Which is a property often used in computer graphics applications, as well as geospatial applications involving <a href="https://en.wikipedia.org/wiki/Digital_elevation_model">Digital Elevation Models</a>.</p><p>Using Snell's Law, a small angle approximation, and a lot of tedious geometry, we find the <span>x</span> and <span>y</span> components of the normal vector <span>\vec{N}</span>:</p><p>\tag{2.3}
N_x(i, j) = \tan \frac{\tan^{-1} \left( \frac{u - x} {d} \right)} {(n_1 - n_2)}</p><p>\tag{2.4}
N_y(i, j) = \tan \frac{\tan^{-1} \left( \frac{v - y} {d} \right)} {(n_1 - n_2)}</p><p>There is nothing interesting about this derivation so I've skipped it here.</p><h2 id="finding-the-heightmap">Finding the Heightmap</h2><p>At this point we have our morphed grid cells and we've found all our surface normals. All we have to do is find a heightmap <span>h(x,y)</span> that has the required surface normals.</p><p>Unfortunately, this is not a problem that is solvable in the general case.</p><p>We could try to integrate the normals manually, starting at one corner and working our way down the grid, but this method does not usually result in a physically realizable object. </p><p>If the integral of the normals running left to right pulls your surface up, but the integral of the normals running top to bottom pulls your surface down, there is just no solution that results in a solid, unbroken surface.</p><p>A much better approach is to reach back to equation <span>(2.2)</span>, repeated here:</p><p>\tag{2.2}
\vec{N}_{xy} = \nabla h</p><p>And to take the divergence of both sides:</p><p>\tag{2.5}
\nabla \cdot \vec{N}_{xy} = \nabla \cdot \nabla h</p><p>Do you recognize the form of this equation? Adopting shorthand and swapping sides:</p><p>\tag{2.6}
\nabla ^2 h = \nabla \cdot \vec{N}_{xy}</p><p>We arrive at yet another instance of <a href="https://mattferraro.dev/posts/poissons-equation">Poisson's Equation</a>! We found <span>\vec{N}_{xy}</span> in the previous section, and calculating the divergence of a known vector field is easy:</p><p>\tag{2.7}
\nabla \cdot \vec{N}_{xy} = \left( \frac{\partial}{\partial{x}}, \frac{\partial}{\partial{y}} \right) \cdot (\vec{N}_x, \vec{N}_y) = \frac{\partial{\vec{N}_x}}{\partial{x}} + \frac{\partial{\vec{N}_y}}{\partial{y}}</p><p>In code it looks like:</p><pre><code>δx <span>=</span> <span>(</span>Nx<span>[</span>i<span>+</span><span>1</span><span>,</span> j<span>]</span> <span>-</span> Nx<span>[</span>i<span>,</span> j<span>]</span><span>)</span>
δy <span>=</span> <span>(</span>Ny<span>[</span>i<span>,</span> j<span>+</span><span>1</span><span>]</span> <span>-</span> Ny<span>[</span>i<span>,</span> j<span>]</span><span>)</span>
divergence<span>[</span>i<span>,</span> j<span>]</span> <span>=</span> δx <span>+</span> δy
</code></pre><p>All that's left is to plug our known <span>\nabla \cdot \vec{N}_{xy}</span> in to a Poisson solver with Neumann boundary conditions and out pops <span>h(x, y)</span>, ready to use!</p><p>Well, there's one thing left to improve. By modifying the height of each point we've actually changed the distance from each lens point to the image, so the lens-image distance is no longer a constant <span>d</span> it is actually a function <span>D(x,y)</span>. With our heightmap in hand we can easily calculate:</p><p>\tag{2.8}
D(x,y) = d - h(x,y)</p><p>And repeat the process by calculating new normals using <span>D(x,y)</span> instead of <span>d</span>, which lets us create a new heightmap.</p><p>We can loop this process and measure changes to ensure convergence, but in practice just 2 or 3 iterations is all you need:</p><pre><code><span># In Julia-flavored psuedocode</span>
d <span>=</span> <span>.2</span> <span># meters</span>
D <span>=</span> d <span>.</span><span>*</span> array_of_ones<span>(</span>n<span>,</span> n<span>)</span>

<span>for</span> i <span>in</span> <span>1</span><span>:</span><span>3</span>
    Nx<span>,</span> Ny <span>=</span> compute_normals<span>(</span>grid<span>,</span> D<span>)</span>
    divergence <span>=</span> compute_divergence<span>(</span>Nx<span>,</span> Ny<span>)</span>
    h <span>=</span> poisson_solver<span>(</span>divergence<span>,</span> <span>"neumann"</span><span>,</span> <span>0</span><span>)</span>
    D <span>=</span> copy<span>(</span>h<span>)</span>
<span>end</span>
</code></pre><p>The resulting heightmap can be converted to a solid object by adopting a triangular grid and closing off the back surface.</p><div><p><img alt="Final Object" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinal_object_1.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinal_object_1.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinal_object_1.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Note that the image looks mirrored when looking at it head on. That's because the heightmap forms the <em>back</em> surface of the Magic Window. The front surface is factory flat.</p><div><p><img alt="Final Object" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinal_object_2.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinal_object_2.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinal_object_2.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>The height differences are subtle but certainly enough to get the job done.</p><div><p><img alt="Finished Product" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinished_product.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinished_product.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinished_product.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><h2 id="manufacturing">Manufacturing</h2><p>The process of manufacturing our Magic Window is identical to carving any other 2.5D object.</p><p>We bring our object into Fusion360 or any other CAM software. We set up a roughing toolpath left to right, and a finishing toolpath top to bottom just like you find in most tutorials.</p><p>Any old CNC router or mill will work. I designed and built my own router last year. If you want to do the same I recommend you start <a href="https://mattferraro.dev/posts/cnc-router">here</a>.</p><p>I used a <span>\frac{1}{4}</span> inch diameter, ball-nosed, carbide bit for both roughing and finishing passes, which took 10 minutes and 90 minutes respectively.</p><div><p><img alt="On the Router" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fon_the_router.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fon_the_router.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fon_the_router.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>After carving the surface finish is rough and transluscent. We need to wet sand using <span>200, 400, 600, 1000</span> and <span>1500</span> grit sandpapers, then finish with a soft rag and some automotive polish. Sanding and polishing takes about half an hour for a <span>10 cm \times 10 cm</span> Magic Window.</p><div><p><img alt="After Sanding" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fafter_sanding.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fafter_sanding.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fafter_sanding.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><h2 id="acknowledgements">Acknowledgements</h2><p>All of the math for this post came from <a href="http://nishitalab.org/user/egaku/tog14/yue-continuous-caustics-lens.pdf">Poisson-Based Continuous Surface Generation for Goal-Based Caustics</a>, a phenomenal 2014 paper by Yue et al. If you continue this work in some way, please cite them.</p><h2 id="my-code">My Code</h2><p>My source code is available <a href="https://github.com/MattFerraro/causticsEngineering">here</a>. I am a novice at programming in Julia so if you have suggestions for how to improve this code, please reach out or make a pull request!</p><p><strong>Caveats</strong>: There are a lot of issues with my code. I confuse <span>x</span> and <span>y</span> in several places. I have extra negative signs that I inserted that make the code work but I don't know why. My units and notation are inconsistent throughout. The original paper suggests a better way of calculating loss but I didn't implement it because the naive way was easier, yet I rolled my own mesh utilities and Poisson solver because I enjoyed the challenge.</p><p>In short: To me this code is a fun side project. If you want to build a business off of this code you should probably hire someone who knows how to program professionally in Julia.</p><h2 id="licensing">Licensing</h2><p>I've posted all my code under the MIT license. Please feel free to use this code for anything you want, including hobbyist, educational, and commercial uses. I only ask that if you make something, please show me!</p><p>Except where otherwise attributed, all images in this blog post and the blog post itself are my own work that I license as <a href="https://creativecommons.org/licenses/by/2.0/">CC-BY</a>.</p><p>The cat in this post is named Mitski and she approves of you using her image as the new standard reference image for image processing papers. It's time to let Lenna <a href="https://en.wikipedia.org/wiki/Lenna#Criticism">retire</a>.</p><p>If you use my code to make your own Magic Windows, I'd love to see them! I'm on Twitter at <a href="https://twitter.com/mferraro89">@mferraro89</a>. Email me at <a href="mailto:mattferraro.dev@gmail.com">mattferraro.dev@gmail.com</a> and I will gladly help if you get stuck!</p><h2 id="one-last-thing">One Last Thing</h2><p>I know what you're thinking. <em>What about the hologram?!</em></p><p>Does the math above imply that a hologram will always be created, or is this one cat hologram just an incredible coincidence?</p><p>Well you see, I've discovered a truly marvelous proof of this, which this website's margin is unfortunately too narrow to contain :)</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Routine dental X-rays are not backed by evidence–experts want it to stop (308 pts)]]></title>
            <link>https://arstechnica.com/health/2024/10/do-you-really-need-those-routine-dental-x-rays-probably-not/</link>
            <guid>41842294</guid>
            <pubDate>Mon, 14 Oct 2024 21:37:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/health/2024/10/do-you-really-need-those-routine-dental-x-rays-probably-not/">https://arstechnica.com/health/2024/10/do-you-really-need-those-routine-dental-x-rays-probably-not/</a>, See on <a href="https://news.ycombinator.com/item?id=41842294">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
            <article data-id="2056298">
  
  <header>
  <div>
    <div>
      

      

      <p>
        The actual recommendations might surprise you—along with the state of modern dentistry.
      </p>

      
    </div>

          <div>
        <p><img width="1000" height="1000" src="https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-907343114-1000x1000.jpg" alt="" loading="eager" decoding="async" fetchpriority="high" srcset="https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-907343114-1000x1000.jpg 1000w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-907343114-150x150.jpg 150w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-907343114-500x500.jpg 500w" sizes="(max-width: 1000px) 100vw, 1000px">
        </p>
        <div>
    
    <p>
      An expert looking at a dental X-ray and saying "look at that unnecessary X-ray," probably.

              <span>
          Credit:

                      <a href="https://www.gettyimages.com/detail/photo/dentist-showing-a-patient-her-x-ray-royalty-free-image/907343114?phrase=dental+x+rays&amp;adppopup=true">
          
          Getty | MilanEXPO

                      </a>
                  </span>
          </p>
  </div>
      </div>
      </div>
</header>

  

  
      
    
    <div>
                      
                      
          
<p>Has your dentist ever told you that it's recommended to get routine dental X-rays every year? My (former) dentist's office did this year—in writing, even. And they claimed that the recommendation came from the American Dental Association.</p>
<p>It's a common refrain from dentists, but it's false. The American Dental Association <em>does not</em> recommend annual routine X-rays. And this is not new; it's been that way for well over a decade.</p>
<p><a href="https://www.ada.org/-/media/project/ada-organization/ada/ada-org/files/resources/library/oral-health-topics/dental_radiographic_examinations_2012.pdf?rev=fd33893f4d634cbaab92733c2313c354&amp;hash=45F728CEF900B5B654539635A9147AA9">The association's guidelines from 2012</a> recommended that adults who don't have an increased risk of dental caries (myself included) need only bitewing X-rays of the back teeth every two to three years. Even people with a higher risk of caries can go as long as 18 months between bitewings. The guidelines also note that X-rays should not be preemptively used to look for problems: "Radiographic screening for the purpose of detecting disease before clinical examination should not be performed," the guidelines read. In other words, dentists are supposed to examine your teeth <em>before</em> they take any X-rays.</p>
<p>But, of course, the 2012 guidelines are outdated—the latest ones go further. In <a href="https://jada.ada.org/article/S0002-8177(23)00734-1/fulltext">updated guidance published in April</a>, the ADA doesn't recommend any specific time window for X-rays at all. Rather, it emphasizes that patient exposure to X-rays should be minimized, and any X-rays should be clinically justified.</p>
<p>There's a good chance you're surprised. Dentistry's overuse of X-rays is a problem dentists do not appear eager to discuss—and would likely prefer to skirt. My former dentist declined to comment for this article, for example. And other dentists have been doing that for years. Nevertheless, the problem is well-established. A New York Times article from 2016, titled "<a href="https://www.nytimes.com/2016/07/26/upshot/you-probably-dont-need-dental-x-rays-every-year.html">You Probably Don’t Need Dental X-Rays Every Year</a>," quoted a dental expert noting the exact problem:</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>"Many patients of all ages receive bitewing X-rays far more frequently than necessary or recommended. And adults in good dental health can go a decade between full-mouth X-rays."</p>
<h2>Data is lacking</h2>
<p>The problem has bubbled up again in a series of commentary pieces published in JAMA Internal Medicine today. The pieces were all sparked by a viewpoint that Ars reported on in May, in which three dental and health experts highlighted that <a href="https://arstechnica.com/science/2024/05/do-you-need-a-dentist-visit-every-6-months-that-filling-the-data-is-weak/">many routine aspects of dentistry, including biannual cleanings, are not evidence-based</a> and that the industry is rife with overdiagnosis and overtreatment. That viewpoint, titled "<a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2818193">Too Much Dentistry</a>," also appeared in JAMA Internal Medicine.</p>
<p>The new pieces take a more specific aim at dental radiography. But, as in the May viewpoint, experts also blasted dentistry more generally for being out of step with modern medicine in its lack of data to support its practices—practices that continue amid financial incentives to overtreat and little oversight to stop it, they note.</p>
<p>In a piece titled "<a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2825067">Too Much Dental Radiography</a>," Sheila Feit, a retired medical expert based in New York, pointed out that using X-rays for dental screenings is not backed by evidence. "Data are lacking about outcomes," she wrote. If anything, the weak data we have makes it look ineffective. For instance, <a href="https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD014545/full">a 2021 systemic review</a> of 77 studies that included data on a total of 15,518 tooth sites or surfaces found that using X-rays to detect early tooth decay led to a high degree of false-negative results. In other words, it led to missed cases.</p>
<p>Feit called for gold-standard randomized clinical trials to evaluate the risks and benefits of X-ray screenings for patients, particularly adults at low risk of caries. "Financial aspects of dental radiography also deserve further study," Feit added. Overall, Feit called the May viewpoint "a timely call for evidence to support or refute common clinical dental practices."</p>

          
                  </div>
                    
        
          
    
    <div>
          
          

<h2>Dentistry without oversight</h2>
<p><a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2825065">In a response</a> published simultaneously in JAMA Internal Medicine, oral medicine expert Yehuda Zadik championed Feit's point, calling it "an essential discussion about the necessity and risks of routine dental radiography, emphasizing once again the need for evidence-based dental care."</p>
<p>Zadik, a professor of dental medicine at The Hebrew University of Jerusalem, noted that the overuse of radiography in dentistry is a global problem, one aided by dentistry's unique delivery:</p>
<p>"Dentistry is among the few remaining health care professions where clinical examination, diagnostic testing including radiographs, diagnosis, treatment planning, and treatment are all performed in place, often by the same care practitioner" Zadik wrote. "This model of care delivery prevents external oversight of the entire process."</p>
<p>While routine X-rays continue at short intervals, Zadik notes that current data "favor the reduction of patient exposure to diagnostic radiation in dentistry," while advancements in dentistry dictate that X-rays should be used at "longer intervals and based on clinical suspicion."</p>
<p>Though the digital dental X-rays often used today provide smaller doses of radiation than the film X-rays used in the past, radiation's harms are cumulative. Zadik emphasizes that with the primary tenet of medicine being "First, do no harm," any unnecessary X-ray is an unnecessary harm. Further, other technology can sometimes be used instead of radiography, including <a href="https://en.wikipedia.org/wiki/Electronic_apex_locator">electronic apex locators</a> for root canal procedures.</p>
<p>"Just as it is now unimaginable that, in the past, <a href="https://wi101.wisc.edu/the-rise-and-fall-of-shoe-fitting-fluroscopes/">shoe fittings for children were conducted using X-rays</a>, in the future it will be equally astonishing to learn that the fit of dental crowns was assessed using radiographic imaging," Zadik wrote.</p>

          
                  </div>
                    
        
          
    
    <div>
        
        
        
        <div>
          
          
<h2>X-rays do more harm than good in children</h2>
<p>Feit's commentary also prompted a reply from the three authors of the original May viewpoint: Paulo Nadanovsky, Ana Paula Pires dos Santos, and David Nunan. The three followed up on Feit's point that data is weak on whether X-rays are useful for detecting early decay, specifically white spot lesions. The experts raise the damning point that even if dental X-rays were shown to be good at doing that, there's still no evidence that that's good for patients.</p>
<p>"[T]here is no evidence that detecting white spot lesions, with or without radiographs, benefits patients," the researchers wrote. "Most of these lesions do not progress into dentine cavities," and there's no evidence that early treatments make a difference in the long run.</p>
<p>To bolster the point, the three note that data from children suggest that X-ray screening does more harm than good. <a href="https://bmcoralhealth.biomedcentral.com/articles/10.1186/s12903-021-01528-w">In a randomized clinical trial published in 2021</a>, 216 preschool children were split into two groups: one that received only a visual-tactile dental exam, while the others received both a visual-tactile exam and X-rays. The study found that adding X-rays caused more harm than benefit because the X-rays led to false positives and overdiagnosis of cavitated caries needing restorative treatment. The authors of the trial concluded that "visual inspection should be conducted alone in regular clinical practice."</p>
<p>Like Zadik, the three researchers note that screenings for decay and cavities are not the only questionable use of X-rays in dental practice. Other common dental and orthodontic treatments involving radiography—practices often used in children and teens—might also be unnecessary harms. They raise the argument against the <a href="https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD003879.pub5/full">preventive removal of wisdom teeth</a>, which is also <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1963310/">not backed by evidence</a>.</p>
<p>Like Feit, the three researchers reiterate the call for well-designed trials to back up or refute common dental practices.</p>


          
                  </div>

                  
          <div>
  <div>
          <p><a href="https://arstechnica.com/author/beth/"><img src="https://arstechnica.com/wp-content/uploads/2016/05/b.mole-45957.jpg" alt="Photo of Beth Mole"></a></p>
  </div>

  <div>
    

    <p>
      Beth is Ars Technica’s Senior Health Reporter. Beth has a Ph.D. in microbiology from the University of North Carolina at Chapel Hill and attended the Science Communication program at the University of California, Santa Cruz. She specializes in covering infectious diseases, public health, and microbes.
    </p>
  </div>
</div>


  


  


  
              </div>
  </article>


<div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/10/starship_flight5_catch1-768x432.jpg" alt="Listing image for first story in Most Read: SpaceX catches returning rocket in mid-air, turning a fanciful idea into reality" decoding="async" loading="lazy">
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>


  

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLMs can't perform "genuine logical reasoning," Apple researchers suggest (105 pts)]]></title>
            <link>https://arstechnica.com/ai/2024/10/llms-cant-perform-genuine-logical-reasoning-apple-researchers-suggest/</link>
            <guid>41842194</guid>
            <pubDate>Mon, 14 Oct 2024 21:25:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/ai/2024/10/llms-cant-perform-genuine-logical-reasoning-apple-researchers-suggest/">https://arstechnica.com/ai/2024/10/llms-cant-perform-genuine-logical-reasoning-apple-researchers-suggest/</a>, See on <a href="https://news.ycombinator.com/item?id=41842194">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="app">
    <p><a href="#main">
  Skip to content
</a></p>



<main id="main">
            <article data-id="2056408">
  
  <header>
  <div>
    <div>
      

      

      <p>
        Irrelevant red herrings lead to "catastrophic" failure of logical inference.
      </p>

      
    </div>

          <div>
        <p><img width="1000" height="1000" src="https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-1622583937-1000x1000-1728939580.jpg" alt="" loading="eager" decoding="async" fetchpriority="high" srcset="https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-1622583937-1000x1000-1728939580.jpg 1000w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-1622583937-150x150-1728939579.jpg 150w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-1622583937-500x500-1728939580.jpg 500w" sizes="(max-width: 1000px) 100vw, 1000px">
        </p>
        <div>
    
    <p>
      What is going on inside that anthropomorphized digital brain?

              <span>
          Credit:

          
          Getty Images

                  </span>
          </p>
  </div>
      </div>
      </div>
</header>

  

  
      
    
    <div>
                      
                      
          
<p>For a while now, companies like OpenAI and Google have been <a href="https://arstechnica.com/information-technology/2024/09/openais-new-reasoning-ai-models-are-here-o1-preview-and-o1-mini/">touting advanced "reasoning" capabilities</a> as <a href="https://arstechnica.com/information-technology/2024/07/google-ai-earns-silver-medal-equivalent-at-international-mathematical-olympiad/">the next big step</a> in their latest artificial intelligence models. Now, though, a new study from six Apple engineers shows that the mathematical "reasoning" displayed by advanced large language models can be extremely brittle and unreliable in the face of seemingly trivial changes to common benchmark problems.</p>
<p>The fragility highlighted in these new results helps support previous research suggesting that LLMs use of probabilistic pattern matching is missing the formal understanding of underlying concepts needed for truly reliable mathematical reasoning capabilities. "Current LLMs are not capable of genuine logical reasoning," the researchers hypothesize based on these results. "Instead, they attempt to replicate the reasoning steps observed in their training data."</p>
<h2>Mix it up</h2>
<p>In "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models"—currently available <a href="https://arxiv.org/pdf/2410.05229">as a pre-print paper</a>—the six Apple researchers start with <a href="https://huggingface.co/datasets/openai/gsm8k">GSM8K's standardized set of over 8,000 grade-school level mathematical word problems</a>, which is <a href="https://klu.ai/glossary/GSM8K-eval">often used as a benchmark</a> for modern LLMs' complex reasoning capabilities. They then take the novel approach of modifying a portion of that testing set to dynamically replace certain names and numbers with new values—so a question about Sophie getting 31 building blocks for her nephew in GSM8K could become a question about Bill getting 19 building blocks for his brother in the new GSM-Symbolic evaluation.</p>
<p>This approach helps avoid any potential "data contamination" that can result from the static GSM8K questions being fed directly into an AI model's training data. At the same time, these incidental changes don't alter the actual difficulty of the inherent mathematical reasoning at all, meaning models should theoretically perform just as well when tested on GSM-Symbolic as GSM8K.</p>
<figure>
    <div>
              <p><a data-pswp-width="1440" data-pswp-height="629" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-300x131.png 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-640x279.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-768x335.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-1536x670.png 1536w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-980x428.png 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-1440x629.png 1440w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1.png 1748w" data-cropped="true" href="https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-1440x629.png" target="_blank">
                <img decoding="async" width="1748" height="763" src="https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1.png" alt="" srcset="https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1.png 1748w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-300x131.png 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-640x279.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-768x335.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-1536x670.png 1536w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-980x428.png 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study1-1440x629.png 1440w" sizes="(max-width: 1748px) 100vw, 1748px">
              </a></p><div id="caption-2056424"><p>
                Simply changing specific names and numbers found in GSM8K tests led to significant decreases in performance in many models.
                                  </p>
                              </div>
            </div>
                  <figcaption>
          <div>
    
    <p>
      Simply changing specific names and numbers found in GSM8K tests led to significant decreases in performance in many models.

              <span>
          Credit:

                      <a href="https://arxiv.org/pdf/2410.05229">
          
          Apple Research

                      </a>
                  </span>
          </p>
  </div>
        </figcaption>
            </figure>

<p>Instead, when the researchers tested more than 20 state-of-the-art LLMs on GSM-Symbolic, they found average accuracy reduced across the board compared to GSM8K, with performance drops between 0.3 percent and 9.2 percent, depending on the model. The results also showed high variance across 50 separate runs of GSM-Symbolic with different names and values. Gaps of up to 15 percent accuracy between the best and worst runs were common within a single model and, for some reason, changing the numbers tended to result in worse accuracy than changing the names.</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>This kind of variance—both within different GSM-Symbolic runs and compared to GSM8K results—is more than a little surprising since, as the researchers point out, "the overall reasoning steps needed to solve a question remain the same." The fact that such small changes lead to such variable results suggests to the researchers that these models are not doing any "formal" reasoning but are instead "attempt[ing] to perform a kind of in-distribution pattern-matching, aligning given questions and solution steps with similar ones seen in the training data."</p>

<h2>Don’t get distracted</h2>
<p>Still, the overall variance shown for the GSM-Symbolic tests was often relatively small in the grand scheme of things. OpenAI's ChatGPT-4o, for instance, dropped from 95.2 percent accuracy on GSM8K to a still-impressive 94.9 percent on GSM-Symbolic. That's a pretty high success rate using either benchmark, regardless of whether or not the model itself is using "formal" reasoning behind the scenes (though total accuracy for many models dropped precipitously when the researchers added just one or two additional logical steps to the problems).</p>
<figure>
    <div>
              <p><a data-pswp-width="1440" data-pswp-height="745" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-300x155.png 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-640x331.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-768x398.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-1536x795.png 1536w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-980x507.png 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-1440x745.png 1440w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3.png 1762w" data-cropped="true" href="https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-1440x745.png" target="_blank">
                <img decoding="async" width="1762" height="912" src="https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3.png" alt="" srcset="https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3.png 1762w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-300x155.png 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-640x331.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-768x398.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-1536x795.png 1536w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-980x507.png 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study3-1440x745.png 1440w" sizes="(max-width: 1762px) 100vw, 1762px">
              </a></p><div id="caption-2056422"><p>
                An example showing how some models get mislead by irrelevant information added to the GSM8K benchmark suite.
                                  </p>
                              </div>
            </div>
                  <figcaption>
          <div>
    
    <p>
      An example showing how some models get mislead by irrelevant information added to the GSM8K benchmark suite.

              <span>
          Credit:

                      <a href="https://arxiv.org/pdf/2410.05229">
          
          Apple Research

                      </a>
                  </span>
          </p>
  </div>
        </figcaption>
            </figure>

<p>The tested LLMs fared much worse, though, when the Apple researchers modified the GSM-Symbolic benchmark by adding "seemingly relevant but ultimately inconsequential statements" to the questions. For this "GSM-NoOp" benchmark set (short for "no operation"), a question about how many kiwis someone picks across multiple days might be modified to include the incidental detail that "five of them [the kiwis] were a bit smaller than average."</p>
<p>Adding in these red herrings led to what the researchers termed "catastrophic performance drops" in accuracy compared to GSM8K, ranging from 17.5 percent to a whopping 65.7 percent, depending on the model tested. These massive drops in accuracy highlight the inherent limits in using simple "pattern matching" to "convert statements to operations without truly understanding their meaning," the researchers write.</p>
<figure>
    <div>
              <p><a data-pswp-width="696" data-pswp-height="953" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study2-300x411.png 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study2-640x876.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study2.png 696w" data-cropped="true" href="https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study2.png" target="_blank">
                <img decoding="async" width="696" height="953" src="https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study2.png" alt="" srcset="https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study2.png 696w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study2-300x411.png 300w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/gsm-study2-640x876.png 640w" sizes="(max-width: 696px) 100vw, 696px">
              </a></p><div id="caption-2056423"><p>
                Introducing irrelevant information to the prompts often led to "catastrophic" failure for most "reasoning" LLMs
                                  </p>
                              </div>
            </div>
                  <figcaption>
          <div>
    
    <p>
      Introducing irrelevant information to the prompts often led to "catastrophic" failure for most "reasoning" LLMs

              <span>
          Credit:

                      <a href="https://arxiv.org/pdf/2410.05229">
          
          Apple Research

                      </a>
                  </span>
          </p>
  </div>
        </figcaption>
            </figure>

<p>In the example with the smaller kiwis, for instance, most models try to subtract the smaller fruits from the final total because, the researchers surmise, "their training datasets included similar examples that required conversion to subtraction operations." This is the kind of "critical flaw" that the researchers say "suggests deeper issues in [the models'] reasoning processes" that can't be helped with fine-tuning or other refinements.</p>

          
                  </div>
                    
        
          
    
    <div>
        
        
        
        <div>
          
          
<h2>The illusion of understanding</h2>
<p>The results of this new GSM-Symbolic paper aren't completely new in the world of AI research. <a href="https://www.semanticscholar.org/paper/Can-large-language-models-reason-and-plan-Kambhampati/f531d1a681ed12fd582767133318d0728316a0ae">Other</a> recent <a href="https://arxiv.org/abs/2206.10498">papers</a> have similarly suggested that LLMs don't actually perform formal reasoning and instead mimic it with probabilistic pattern-matching of the closest similar data seen in their vast training sets.</p>
<p>Still, the new research highlights just how fragile this kind of mimicry can be when the prompt in question pushes it in a direction that doesn't precisely match any training data. It also highlights the inherent limitations in trying to perform high-level reasoning without any underlying model of the logic or world behind it. As Ars' Benj Edwards put it <a href="https://arstechnica.com/information-technology/2024/07/we-made-a-cat-drink-a-beer-with-runways-ai-video-generator-and-it-sprouted-hands/">in a July story about AI video generation</a>:</p>
<blockquote><p>One of the reasons OpenAI's GPT-4 turned heads in text synthesis is that the model finally reached a size where it was large enough to have absorbed enough information (in training data) to give the impression that it might be able to genuinely understand and model the world when, in reality, a key aspect of its success is that it "knows" far more than most humans and can impress us by combining those existing concepts in novel ways. With enough training data and computation, the AI industry will likely reach what you might call "the illusion of understanding" with AI video synthesis eventually...</p></blockquote>
<p>We're likely seeing a similar "illusion of understanding" with AI's latest "reasoning" models, and seeing how that illusion can break when the model runs in to unexpected situations.</p>
<p>AI expert Gary Marcus, in <a href="https://garymarcus.substack.com/p/llms-dont-do-formal-reasoning-and">his analysis</a> of the new GSM-Symbolic paper, argues that the next big leap in AI capability will only come when these neural networks can integrate true "symbol manipulation, in which some knowledge is represented truly abstractly in terms of variables and operations over those variables, much as we see in algebra and traditional computer programming..." Until then, we're going to get the kind of brittle "reasoning" that can lead AI models to fail mathematical tests in ways that calculators never do.</p>


          
                  </div>

                  
          <div>
  <div>
          <p><a href="https://arstechnica.com/author/kyle-orland/"><img src="https://arstechnica.com/wp-content/uploads/2016/05/k.orland-13.jpg" alt="Photo of Kyle Orland"></a></p>
  </div>

  <div>
    

    <p>
      Kyle Orland has been the Senior Gaming Editor at Ars Technica since 2012, writing primarily about the business, tech, and culture behind video games. He has journalism and computer science degrees from University of Maryland. He once <a href="https://bossfightbooks.com/collections/books/products/minesweeper-by-kyle-orland">wrote a whole book about <em>Minesweeper</em></a>.
    </p>
  </div>
</div>


  


  



  
              </div>
  </article>


<div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-907343114-768x432.jpg" alt="Listing image for first story in Most Read: Routine dental X-rays are not backed by evidence—experts want it to stop" decoding="async" loading="lazy">
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>


  

  </main>







  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla Optimus Bots Were Remotely Operated at Cybercab Event (215 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2024-10-14/tesla-s-optimus-robots-were-remotely-operated-at-cybercab-event</link>
            <guid>41842060</guid>
            <pubDate>Mon, 14 Oct 2024 21:10:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2024-10-14/tesla-s-optimus-robots-were-remotely-operated-at-cybercab-event">https://www.bloomberg.com/news/articles/2024-10-14/tesla-s-optimus-robots-were-remotely-operated-at-cybercab-event</a>, See on <a href="https://news.ycombinator.com/item?id=41842060">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
    </channel>
</rss>