<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 20 Jun 2024 19:30:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Small claims court became Meta's customer service hotline (113 pts)]]></title>
            <link>https://www.engadget.com/how-small-claims-court-became-metas-customer-service-hotline-160224479.html</link>
            <guid>40741197</guid>
            <pubDate>Thu, 20 Jun 2024 17:37:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.engadget.com/how-small-claims-court-became-metas-customer-service-hotline-160224479.html">https://www.engadget.com/how-small-claims-court-became-metas-customer-service-hotline-160224479.html</a>, See on <a href="https://news.ycombinator.com/item?id=40741197">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Last month, Ray Palena boarded a plane from New Jersey to California to appear in court. He found himself engaged in a legal dispute against one of the largest corporations in the world, and improbably, the venue for their David-versus-Goliath showdown would be San Mateo's small claims court.</p><p>Over the course of eight months and an estimated $700 (mostly in travel expenses), he was able to claw back what all other methods had failed to render: his personal Facebook account.</p><p>Those may be extraordinary lengths to regain a digital profile with no relation to its owner's livelihood, Palena is one of a growing number of frustrated users of Meta's services who, unable to get help from an actual human through normal channels of recourse, are using the court system instead. And in many cases, it's working.</p><p>Engadget spoke with five individuals who have sued Meta in small claims court over the last two years in four different states. In three cases, the plaintiffs were able to restore access to at least one lost account. One person was also able to win financial damages and another reached a cash settlement. Two cases were dismissed. In every case, the plaintiffs were at least able to get the attention of Meta’s legal team, which appears to have something of a playbook for handling these claims.</p><h2 id="why-small-claims"><strong>Why small claims?</strong></h2><p>At the heart of these cases is the fact that Meta lacks the necessary volume of human customer service workers to assist those who lose their accounts. The company’s official help pages steer users who have been hacked toward confusing automated tools that often lead users to dead-end links or emails that don’t work if your account information has been changed. (The company recently launched a $14.99-per-month program, Meta Verified, which grants access to human customer support. Its track record as a means of recovering hacked accounts after the fact has been spotty at best, according to anecdotal descriptions.)</p><p>Hundreds of thousands of people also turn to their state Attorney General’s office as some state AGs have made requests on users’ behalf — on Reddit, this is known as the “AG method.” But attorneys general across the country have been so inundated with these requests they formally asked Meta to fix their customer service, too. “We refuse to operate as the customer service representatives of your company,” a coalition of 41 state AGs wrote in <a data-i13n="elm:context_link;elmt:doNotAffiliate;cpos:1;pos:1" href="https://www.engadget.com/41-state-attorneys-general-tell-meta-to-fix-their-customer-support-for-hacking-victims-184709904.html" data-ylk="slk:a letter;elm:context_link;elmt:doNotAffiliate;cpos:1;pos:1;itc:0;sec:content-canvas">a letter</a> to the company earlier this year.</p><p>Facebook and Instagram users have long sought creative and sometimes extreme measures to get hacked accounts back due to Meta’s lack of customer support features. Some users have resorted to <a data-i13n="elm:context_link;elmt:doNotAffiliate;cpos:2;pos:1" href="https://www.vice.com/en/article/59vnvk/hacked-instagram-influencers-get-accounts-back-white-hat-hackers" rel="nofollow noopener" target="_blank" data-ylk="slk:hiring;elm:context_link;elmt:doNotAffiliate;cpos:2;pos:1;itc:0;sec:content-canvas">hiring</a> their own hackers or <a data-i13n="elm:context_link;elmt:doNotAffiliate;cpos:3;pos:1" href="https://www.npr.org/2021/08/02/1023801277/your-facebook-account-was-hacked-getting-help-may-take-weeks-or-299" rel="nofollow noopener" target="_blank" data-ylk="slk:buying;elm:context_link;elmt:doNotAffiliate;cpos:3;pos:1;itc:0;sec:content-canvas">buying</a> an Oculus headset since Meta has dedicated support staff for the device (users on Reddit report this “method” no longer works). The small claims approach has become a popular topic on Reddit forums where frustrated Meta users trade advice on various “methods” for getting an account back. People Clerk, a site that helps people write demand letters and other paperwork required for small claims court, published a <a data-i13n="elm:context_link;elmt:doNotAffiliate;cpos:4;pos:1" href="https://www.peopleclerk.com/post/how-to-sue-facebook" rel="nofollow noopener" target="_blank" data-ylk="slk:help article;elm:context_link;elmt:doNotAffiliate;cpos:4;pos:1;itc:0;sec:content-canvas">help article</a> called “How to Sue facebook,” in March.</p><p>It’s difficult to estimate just how many small claims cases are being brought by Facebook and Instagram users, but they may be on the rise. Patrick Forrest, the chief legal officer for Justice Direct, the legal services startup that owns People Clerk, says the company has seen a “significant increase” in cases against Meta over the last couple years.</p><p>One of the advantages of small claims court is that it’s much more accessible to people without deep pockets and legal training. Filing fees are typically under $100 and many courthouses have resources to help people complete the necessary paperwork for a case. “There's no discovery, there are no depositions, there's no pre-trial,” says Bruce Zucker, a law professor at California State University, Northridge. “You get a court date and it's going to be about a five or 10 minute hearing, and you have a judge who's probably also tried to call customer service and gotten nowhere.”</p><h2 id="the-stakes"><strong>The stakes</strong></h2><p>“Facebook and Instagram and WhatsApp [have] become crucial marketplaces where people conduct their business, where people are earning a living," Forrest said. “And if you are locked out of that account, business or personal, it can lead to severe financial damages, and it can disrupt your ability to sustain your livelihood.”</p><p>One such person whose finances were enmeshed with Meta's products is Valerie Garza, the owner of a massage business. She successfully sued the company in a San Diego small claims court in 2022 after a hack which cost her access to personal Facebook and Instagram accounts, as well as those associated with her business. She was able to document thousands of dollars in resulting losses.</p><p>A Meta legal representative contacted Garza a few weeks before her small claims court hearing, requesting she drop the case. She declined, and when Meta didn’t show up to her hearing, she won by default. "When we went through all of the loss of revenues," Garza told Engadget, "[the judge] kind of had to give it to me.”</p><p>But that wasn’t the end of Garza’s legal dispute with Meta. After the first hearing, the company filed a motion asking the judge to set aside the verdict, citing its own failure to appear at the hearing. Meta also tried to argue that its terms of service set a maximum of $100 liability. Another hearing was scheduled and a lawyer again contacted Garza offering to help get her account back.</p><p>“He seemed to actually kind of just want to get things turned back on, and that was still my goal, at this point,” Garza said. It was then she discovered that her business’ Instagram was being used to advertise sex work.</p><p>She began collecting screenshots of the activity on the account, which violated Instagram’s terms of service, as well as fraudulent charges for Facebook ads bought by whoever hacked her account. Once again, Meta didn’t show up to the hearing and a judge ordered the company to pay her the $7,268.65 in damages she had requested.</p><p>“I thought they were going to show up this time because they sent their exhibits, they didn't ask for a postponement or anything,” she says. “My guess is they didn't want to go on record and have a transcript showing how completely grossly negligent they are in their business and how very little they care about the safety or financial security of their paying advertisers.”</p><p>In July of 2023, Garza indicated in court documents that Meta had paid in full. In all, the process took more than a year, three court appearances and countless hours of work. But Garza says it was worth it. “I just can't stand letting somebody take advantage and walking away,” she says.</p><p>Even for individuals whose work doesn't depend on Meta's platforms, a hacked account can result in real harm.</p><p>Palena, who flew cross-country to challenge Meta in court, had no financial stake in his Facebook account, which he claimed nearly 20 years ago when the social network was still limited to college students. But whoever hacked him had changed the associated email address and phone number, and began using his page to run scam listings on Facebook Marketplace.</p><p>“I was more concerned about the damage it could do to me and my name if something did happen, if someone actually was scammed,” he tells Engadget. In his court filing, he asked for $10,000 in damages, the maximum allowed in California small claims court. He wrote that Meta had violated its own terms of service by allowing a hacked account to stay up, damaging his reputation. “I didn't really care that much about financial compensation,” Palena says “I really just wanted the account back because the person who hacked the account was still using it. They were using my profile with my name and my profile image."</p><p>A couple weeks later, a legal rep from Meta reached out to him and asked him for information about his account. They exchanged a few emails over several weeks, but his account was still inaccessible. The same day he boarded a plane to San Mateo, the Meta representative emailed him again and asked if he would be willing to drop the case since “the access team is close to getting your account secure and activated again.” He replied that he intended to be in court the next day as he was still unable to get into his account.</p><p>Less than half an hour before his hearing was scheduled to start, he received the email he had spent months waiting for: a password reset link to get back into his account. Palena still attended the hearing, though Meta did not. According to court records reviewed by Engadget, Palena told the judge the case had been “tentatively resolved,” though he hasn’t officially dropped the case yet.</p><h2 id="the-hurdles-of-small-claims"><strong>The hurdles of small claims</strong></h2><p>While filing a small claims court case is comparatively simple, it can still be a minefield, even to figure out something as seemingly straightforward as which court to file to. Forrest notes that Facebook’s <a data-i13n="elm:context_link;elmt:doNotAffiliate;cpos:5;pos:1" href="https://web.archive.org/web/20240617191705/https://www.facebook.com/legal/terms?paipv=0&amp;eav=AfZS6qv1h_Yae53lI1YzSLPqunlR5E-qm81YGS-4ha4JAWI3_ZwvsJwVSr94z6FVhhI&amp;_rdr" rel="nofollow noopener" target="_blank" data-ylk="slk:terms of service;elm:context_link;elmt:doNotAffiliate;cpos:5;pos:1;itc:0;sec:content-canvas">terms of service</a> stipulates that legal cases must be brought in San Mateo County, home of Meta’s headquarters. But, confusingly, the terms of service for Meta accounts <a data-i13n="elm:context_link;elmt:doNotAffiliate;cpos:6;pos:1" href="https://webcache.googleusercontent.com/search?q=cache%3Ahttps%3A%2F%2Fwww.meta.com%2Flegal%2Fsupplemental-terms-of-service%2F&amp;rlz=1C5GCEA_enUS1076US1076&amp;oq=cache%3Ahttps%3A%2F%2Fwww.meta.com%2Flegal%2Fsupplemental-terms-of-service%2F&amp;gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIGCAEQRRg60gEIMjk2MGowajeoAgCwAgA&amp;sourceid=chrome&amp;ie=UTF-8" rel="nofollow noopener" target="_blank" data-ylk="slk:states;elm:context_link;elmt:doNotAffiliate;cpos:6;pos:1;itc:0;sec:content-canvas">states</a> that cases <em>other than small claims court</em> must be filed in San Mateo. In spite of the apparent contradiction, some people (like Garza) have had success suing Meta outside of San Mateo.</p><p>Each jurisdiction also has different rules for maximum allowable compensation in small claims, what sorts of relief those courts are able to grant and even whether or not parties are allowed to have a lawyer present. The low barrier to entry means many first-time plaintiffs are navigating the legal system for the first time without help, and making rookie mistakes along the way.</p><p>Shaun Freeman had spent years building up two Instagram accounts, which he describes as similar to TMZ but with “a little more character.” The pages, which had hundreds of thousands of followers, had also been a significant source of income to Freeman, who has also worked in the entertainment industry and uses the stage name Young Platinum.</p><p>He says his pages had been suspended or disabled in the past, but he was able to get them back through Meta’s appeals process, and once through a complaint to the California Attorney General’s office. But in 2023 he again lost access to both accounts. He says one was disabled and one is inaccessible due to what seems like a technical glitch.</p><p>He tried to file appeals and even asked a friend of a friend who worked at Meta to look into what had happened, but was unsuccessful. Apparently out of other options, he filed a small claims case in Nevada in February. A hearing was scheduled for May, but Freeman had trouble figuring out the legal mechanics. “It took me months and months to figure out how to get them served,” Freeman says. He was eventually able to hire a process server and got the necessary signature 10 days before his hearing. But it may have been too late. Court records show the case was dismissed for failure to serve.</p><p>Even without operator error, Meta seems content to create hardship for would-be litigants over matters much smaller than the company's more headline-grabbing antitrust and child safety disputes. Based on correspondence reviewed by Engadget, the company maintains a separate "small claims docket" email address to contact would-be litigants.</p><p>Ron Gaul, who lives in North Dakota, filed a small claims suit after Meta disabled his account following a wave of what he describes as targeted harassment. The case was eventually dismissed after Meta’s lawyers had the case moved to district court, which is permissible for a small claims case under North Dakota law.</p><p>Gaul says he couldn’t keep up with the motions filed by Meta’s lawyers, whom he had hoped to avoid by filing in small claims court. “I went to small claims because I couldn't have a lawyer,” he tells Engadget.</p><p>Ryan, an Arizona real estate agent who asked to be identified by his first name only, decided to sue Meta in small claims with his partner after their Facebook accounts were disabled in the fall of 2022. They were both admins of several large Facebook Groups and he says their accounts were disabled over a supposed copyright violation.</p><p>Before a scheduled hearing, the company reached out. “They started basically trying to bully us,” says Ryan, who asked to be identified by his first name only. “They started saying that they have a terms of service [and] they can do whatever they want, they could delete people for any reason.” Much like Gaul, Ryan expected small claims would level the playing field. But according to emails and court records reviewed by Engadget, Meta often deploys its own legal resources as well as outside law firms to respond to these sorts of claims and engage with small claims litigants outside of court. "They put people that still have legal training against these people that are, you know, representing themselves,” he said.</p><p>In the end, Meta’s legal team was able to help Ryan get his account back and he agreed to drop himself from the small claims case. But two months later his partner had still not gotten back into hers. Meta eventually told her that her account had been permanently deleted and was no longer able to be restored. Meta eventually offered $3,500 — the maximum amount for a small claims case in Arizona. He says they wanted more, but Meta refused, and they felt like they were out of options. Ryan claims they had already lost tens of thousands of dollars in potential sales that they normally sourced from Facebook. “We were prepared to go further, but no lawyer would really take it on without a $15,000 retainer and it wasn't worth it.”</p><p>While it may seem surprising that Meta would give these small claims cases so much attention, Zucker, the Cal State Northridge professor, says that big companies have their own reasons for wanting to avoid court. “I don’t think places like Google or Meta want to have a bunch of judgments against them … because then that becomes a public record and starts floating around,” he says. “So they do take these things seriously.”</p><p>Without responding to specific questions about the substance of this story, Meta instead sent Engadget the following statement:</p><div data-src=""><blockquote><p>"We know that losing and recovering access to your online accounts can be a frustrating experience. We invest heavily in designing account security systems to help prevent account compromise in the first place, and in educating our users, including by regularly sharing new security features and tips for how people can stay safe and vigilant against potential targeting by hackers. But we also know that bad actors, including scammers, target people across the internet and constantly adapt to evade detection by social media platforms like ours, email and telecom providers, banks and others. To detect malicious activity and help protect people who may have gotten compromised via email phishing, malware or other means, we also constantly improve our detection, enforcement and support systems, in addition to providing channels where people can report account access issues to us, working with law enforcement and taking legal action against malicious groups."</p></blockquote></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a puzzle game that gently introduces my favorite math mysteries (223 pts)]]></title>
            <link>https://www.rahulilango.com/coloring/</link>
            <guid>40740021</guid>
            <pubDate>Thu, 20 Jun 2024 15:45:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.rahulilango.com/coloring/">https://www.rahulilango.com/coloring/</a>, See on <a href="https://news.ycombinator.com/item?id=40740021">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <p>
        Color the map of the British Isles so that no two physically bordering regions have the same color
        
    </p>
    

    <!-- sa uk pi af us wus nsa  -->
    <p>(Click to color)</p>
    <interactive-canvas mode="color" coloring-mode="four" preset-map="uk"></interactive-canvas>
    
    


    
    
    <!-- 100% privacy-first analytics -->
    
    


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Atkinson Dithering (125 pts)]]></title>
            <link>https://beyondloom.com/blog/dither.html</link>
            <guid>40739710</guid>
            <pubDate>Thu, 20 Jun 2024 15:18:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://beyondloom.com/blog/dither.html">https://beyondloom.com/blog/dither.html</a>, See on <a href="https://news.ycombinator.com/item?id=40739710">Hacker News</a></p>
<div id="readability-page-1" class="page"><h2 id="atkinsondithering">Atkinson Dithering</h2>

<p>When the <a href="https://beyondloom.com/blog/thinkc.html">Macintosh</a> was released in 1984, it featured a square-pixeled black-and-white display at a crisp 72 dots per inch. The 512x342 resolution might seem less than impressive today, but for the time it was a pleasantly high-resolution consumer-grade computer. Among other things, the monospaced <a href="https://en.wikipedia.org/wiki/Monaco_(typeface)">Monaco 9pt</a> bitmap font featured characters that were 6 pixels wide, allowing the Macintosh to render a standard 80-column terminal with ample room for a vertical scrollbar and other niceties.</p>

<p>Beyond <a href="https://en.wikipedia.org/wiki/Susan_Kare">Susan Kare’s</a> brilliant type design and pixel-art icons, a cornerstone of the visual vocabulary of the Macintosh is the use of dithered images. Dithering algorithms use a small palette (in our case, black and white) to represent a larger one (simulated grayscale). Here we will examine two popular techniques: Floyd-Steinberg dithering, and a variation common on the Macintosh, Atkinson dithering.</p>

<h2 id="floyd-steinberg">Floyd-Steinberg</h2>

<p>The central idea of Floyd-Steinberg dithering is <em>error diffusion</em>. When determining the 1-bit color to assign to a given pixel in a grayscale image, quantizing will result in some error- a pixel either <em>darker</em> or <em>lighter</em> than the true grayscale value. By “pushing” this rounding error to neighboring pixels, we can ensure that <em>on average</em> the distribution of black and white in the output image resembles the input. Your eye closes the gap, and reconstructs some of the lost information.</p>

<p>Floyd-Steinberg dithering scans input pixels in a single pass, top-to-bottom and left-to-right. Error left over from each pixel is distributed between neighboring pixels which have not been converted yet, in the following proportions:</p>

<figure>
	<img src="https://beyondloom.com/blog/dither-figures/floyd.png">
	<figcaption>Floyd-Steinberg error diffusion</figcaption>
</figure>

<p>Suppose we have the <em>[0.0,1.0]</em> grayscale values of an image in an array <code>pixels</code> and the width of the image <code>w</code>, and we wish to generate an array of <em>{0,1}</em> output pixels. If we are permitted to mutate <code>pixels</code> in-place, a JavaScript implementation of the algorithm might look something like the following:</p>

<pre><code>function floyd(pixels,w) {
	const m=[[1,7],[w-1,3],[w,5],[w+1,1]]
	for (let i=0; i&lt;pixels.length; i++) {
		const x=pixels[i], col=x&gt;.5, err=(x-col)/16
		m.forEach(([x,y]) =&gt; i+x&lt;pixels.length &amp;&amp; (pixels[i+x]+=err*y))
		pixels[i]=col
	}
	return pixels
}
</code></pre>

<p>Given a <em>mask</em> (<code>m</code>) of relative offsets to neighbor pixels and the proportion of error they should get, we map over our pixels in order. The <em>color</em> at each pixel (<code>col</code>) is a simple quantization of the grayscale value (<code>x&gt;.5</code>), the <em>error</em> (<code>err</code>) is the difference between this quantized value and the original, and we simply add a fraction of that error to neighbor pixels before returning <code>col</code>.</p>

<figure>
	<div>
		<div>
			<p><img src="https://beyondloom.com/blog/dither-figures/david.png"></p><figcaption>Original</figcaption>
		</div>
		<div>
			<p><img src="https://beyondloom.com/blog/dither-figures/david-floyd.png"></p><figcaption>Floyd-Steinberg dithering</figcaption>
		</div>
	</div>
</figure>

<p>Note that in this implementation error diffused from the right edge of the image will bleed into the left edge of the following row. The inherent noisyness of dithering makes this aesthetically irrelevant, and doing so simplifies the implementation. Everyone wins!</p>

<p>If we wanted to avoid modifying <code>pixels</code> in-place, we could observe that the accumulated error we care about would fit in a fixed-size circular buffer (<code>e</code>) proportional to the width of our image. The result is slightly more complex, but in some ways conceptually cleaner, since our main loop is now a <code>map()</code>:</p>

<pre><code>function floyd2(pixels,w) {
	const e=Array(w+1).fill(0), m=[[0,7],[w-2,3],[w-1,5],[w,1]]
	return pixels.map(x =&gt; {
		const pix=x+(e.push(0),e.shift()), col=pix&gt;.5, err=(pix-col)/16
		m.forEach(([x,y]) =&gt; e[x]+=err*y)
		return col
	})
}
</code></pre>

<p>(<code>Array.shift()</code> and <code>Array.push()</code> are used here for conciseness; use your imagination for a more efficient low-level implementation.)</p>

<h2 id="atkinsonsalgorithm">Atkinson’s Algorithm</h2>

<p>Apple’s <a href="https://en.wikipedia.org/wiki/Bill_Atkinson">Bill Atkinson</a> developed a variation of the above technique which produces results that are subjectively nicer-looking. The basics work the same, but error is spread in a broader pattern, and only 3/4ths of the error is preserved:</p>

<figure>
	<img src="https://beyondloom.com/blog/dither-figures/atkinson.png">
	<figcaption>Atkinson error diffusion</figcaption>
</figure>

<p>Modifying <code>floyd2()</code> above, we can simplify <code>m</code>, as all the error is propagated in an even proportion. The sliding error window <code>e</code> now needs to be roughly twice as large to account for the broader spread pattern:</p>

<pre><code>function atkinson(pixels,w) {
	const e=Array(2*w).fill(0), m=[0,1,w-2,w-1,w,2*w-1]
	return pixels.map(x =&gt; {
		const pix=x+(e.push(0),e.shift()), col=pix&gt;.5, err=(pix-col)/8
		m.forEach(x =&gt; e[x]+=err)
		return col
	})
}
</code></pre>

<figure>
	<div>
		<div>
			<p><img src="https://beyondloom.com/blog/dither-figures/mandrill.png"></p><figcaption>Original</figcaption>
		</div>
		<div>
			<p><img src="https://beyondloom.com/blog/dither-figures/mandrill-floyd.png"></p><figcaption>Floyd-Steinberg dithering</figcaption>
		</div>
		<div>
			<p><img src="https://beyondloom.com/blog/dither-figures/mandrill-atkinson.png"></p><figcaption>Atkinson dithering</figcaption>
		</div>
	</div>
</figure>

<p>Compared side-by-side with Floyd-Steinberg dithering, Atkinson’s algorithm seems to produce richer contrast, at the cost of some detail in very light or dark areas of the image.</p>

<h2 id="ditheringinike">Dithering in iKe</h2>

<p>Readers of this blog might be curious to know what Atkinson dithering would look like in <a href="https://github.com/JohnEarnest/ok/tree/gh-pages/ike">iKe</a>.</p>

<p>We can leverage iKe’s capabilities for loading remote images via the special <code>/i</code> operative. By specifying the built-in palette <code>gray</code>, the input image will be converted into a matrix of <em>[0,255]</em> grayscale values. To resize the iKe graphics window to match the image, we initialize the magic variables <code>w</code> and <code>h</code> based on the dimensions of this matrix. Our running error window is stored in a global (<code>e</code>) initialized with two rows worth of zeroes (<code>&amp;2*w</code>). The definition of our mask <code>m</code> is virtually identical to the JavaScript version above.</p>

<p>Most of the work happens in <code>d</code>, which is meant to dither one pixel. We find the value of the current pixel, incorporating accumulated error (<code>p:x+*e</code>), quantize the output color (<code>c:p&gt;.5</code>), shift out the consumed error value and shift in a zero (<code>e::1_e,0</code>), and then <em>amend</em> <code>e</code> by adding the error at the current pixel at every index in our mask <code>m</code> (<code>@[`e;m;+;(p-c)%8]</code>).</p>

<p>To dither the entire image, we explicitly apply <code>d</code> to each pixel of the matrix, remembering to first rescale the input <em>[0,255]</em> palette indices into floating point <em>[0.0,1.0]</em> values. For an animated application of this code, it would be necessary to reset <code>e</code> after each frame.</p>

<pre><code>/i pixels;gray;https://i.imgur.com/Lcq4Xi4.png
w: #*pixels
h: #pixels
e: &amp;2*w
m: (0;1;w-2;w-1;w;2*w-1)
d: {p:x+*e; c:p&gt;.5; e::1_e,0; @[`e;m;+;(p-c)%8]; c}
,(;;d''pixels%255)
</code></pre>

<figure>
<img src="https://beyondloom.com/blog/dither-figures/bill.png" alt="Thanks, Bill.">
<figcaption>Thanks, Bill.</figcaption>
</figure>

<p><a href="https://beyondloom.com/blog/index.html">back</a></p>
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Could AI be a dot com sized bubble? (103 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=40739431</link>
            <guid>40739431</guid>
            <pubDate>Thu, 20 Jun 2024 14:54:23 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=40739431">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="40739431">
      <td><span></span></td>      <td><center><a id="up_40739431" href="https://news.ycombinator.com/vote?id=40739431&amp;how=up&amp;goto=item%3Fid%3D40739431"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=40739431">Ask HN: Could AI be a dot com sized bubble?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_40739431">103 points</span> by <a href="https://news.ycombinator.com/user?id=jameslk">jameslk</a> <span title="2024-06-20T14:54:23"><a href="https://news.ycombinator.com/item?id=40739431">4 hours ago</a></span> <span id="unv_40739431"></span> | <a href="https://news.ycombinator.com/hide?id=40739431&amp;goto=item%3Fid%3D40739431">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Could%20AI%20be%20a%20dot%20com%20sized%20bubble%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=40739431&amp;auth=11d20f8e338d8c8c26f2ae6471155cb537c45ff0">favorite</a> | <a href="https://news.ycombinator.com/item?id=40739431">91&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>AI hype has landed in the laps of retail investors and in general anyone passively investing as NVIDIA and Microsoft shares have risen and become part of major ETFs in the expectation that the AI-driven demand for their products will continue unabated. Other tech stocks seem to be seeing similar treatment around AI hype.</p><p>I do expect the current trajectory of generative models will eventually be incredibly important just like the internet was and is, but it seems there’s a lot of high expectations of how useful it can be in the near future with fuzzy ideas around business models like in the dot com era.</p><p>If these near future expectations don’t pan out, could companies slow down their R&amp;D expenditures which are floating NVIDIA, Microsoft, et al and lead to a sizable stock market correction?</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Claude 3.5 Sonnet (440 pts)]]></title>
            <link>https://www.anthropic.com/news/claude-3-5-sonnet</link>
            <guid>40738916</guid>
            <pubDate>Thu, 20 Jun 2024 14:03:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/news/claude-3-5-sonnet">https://www.anthropic.com/news/claude-3-5-sonnet</a>, See on <a href="https://news.ycombinator.com/item?id=40738916">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure><img alt="Claude head illustration" loading="eager" width="2880" height="1620" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4e78f69ef8d4186fb5691714abe36224483d91b0-2880x1620.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4e78f69ef8d4186fb5691714abe36224483d91b0-2880x1620.png&amp;w=3840&amp;q=75"></figure><p>Today, we’re launching Claude 3.5 Sonnet—our first release in the forthcoming Claude 3.5 model family. Claude 3.5 Sonnet raises the industry bar for intelligence, outperforming competitor models and Claude 3 Opus on a wide range of evaluations, with the speed and cost of our mid-tier model, Claude 3 Sonnet.</p><p>Claude 3.5 Sonnet is now available for free on Claude.ai and the Claude iOS app, while Claude Pro and Team plan subscribers can access it with significantly higher rate limits. It is also available via the Anthropic API, Amazon Bedrock, and Google Cloud’s Vertex AI. The model costs $3 per million input tokens and $15 per million output tokens, with a 200K token context window.</p><figure><img alt="Claude model family" loading="eager" width="2200" height="1174" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F1f044104447e9db6b22db3a06e45d114f50f274e-2200x1174.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F1f044104447e9db6b22db3a06e45d114f50f274e-2200x1174.png&amp;w=3840&amp;q=75"></figure><h3>Frontier intelligence at 2x the speed</h3><p>Claude 3.5 Sonnet sets new industry benchmarks for graduate-level reasoning (GPQA), undergraduate-level knowledge (MMLU), and coding proficiency (HumanEval). It shows marked improvement in grasping nuance, humor, and complex instructions, and is exceptional at writing high-quality content with a natural, relatable tone.</p><p>Claude 3.5 Sonnet operates at twice the speed of Claude 3 Opus. This performance boost, combined with cost-effective pricing, makes Claude 3.5 Sonnet ideal for complex tasks such as context-sensitive customer support and orchestrating multi-step workflows.</p><p>In an <a href="https://cdn.sanity.io/files/4zrzovbb/website/fed9cc193a14b84131812372d8d5857f8f304c52.pdf">internal agentic coding evaluation</a>, Claude 3.5 Sonnet solved 64% of problems, outperforming Claude 3 Opus which solved 38%. Our evaluation tests the model’s ability to fix a bug or add functionality to an open source codebase, given a natural language description of the desired improvement. When instructed and <a href="https://www.anthropic.com/news/tool-use-ga">provided with the relevant tools</a>, Claude 3.5 Sonnet can independently write, edit, and execute code with sophisticated reasoning and troubleshooting capabilities. It handles code translations with ease, making it particularly effective for updating legacy applications and migrating codebases.</p><figure><img alt="Claude 3.5 Sonnet benchmarks" loading="lazy" width="2200" height="1894" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fcf2c754458e9102b7334731fb18a965bfeb7ad08-2200x1894.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fcf2c754458e9102b7334731fb18a965bfeb7ad08-2200x1894.png&amp;w=3840&amp;q=75"></figure><h3>State-of-the-art vision</h3><p>Claude 3.5 Sonnet is our strongest vision model yet, surpassing Claude 3 Opus on standard vision benchmarks. These step-change improvements are most noticeable for tasks that require visual reasoning, like interpreting charts and graphs. Claude 3.5 Sonnet can also accurately transcribe text from imperfect images—a core capability for retail, logistics, and financial services, where AI may glean more insights from an image, graphic or illustration than from text alone.</p><!--$!--><template data-dgst="NEXT_DYNAMIC_NO_SSR_CODE"></template><!--/$--><figure><img alt="Claude 3.5 Sonnet vision evals" loading="lazy" width="2200" height="1110" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fcaff3d60763b27b59fe33e4ae984530f0dba4ddb-2200x1110.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fcaff3d60763b27b59fe33e4ae984530f0dba4ddb-2200x1110.png&amp;w=3840&amp;q=75"></figure><h3>Artifacts—a new way to use Claude</h3><p>Today, we’re also introducing Artifacts on Claude.ai, a new feature that expands how users can interact with Claude. When a user asks Claude to generate content like code snippets, text documents, or website designs, these Artifacts appear in a dedicated window alongside their conversation. This creates a dynamic workspace where they can see, edit, and build upon Claude’s creations in real-time, seamlessly integrating AI-generated content into their projects and workflows.</p><p>This preview feature marks Claude’s evolution from a conversational AI to a collaborative work environment. It’s just the beginning of a broader vision for Claude.ai, which will soon expand to support team collaboration. In the near future, teams—and eventually entire organizations—will be able to securely centralize their knowledge, documents, and ongoing work in one shared space, with Claude serving as an on-demand teammate.</p><!--$!--><template data-dgst="NEXT_DYNAMIC_NO_SSR_CODE"></template><!--/$--><h3>Commitment to safety and privacy</h3><p>Our models are subjected to rigorous testing and have been trained to reduce misuse. Despite Claude 3.5 Sonnet’s leap in intelligence, our red teaming assessments have concluded that Claude 3.5 Sonnet remains at <a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy">ASL-2</a>. More details can be found in the <a href="https://cdn.sanity.io/files/4zrzovbb/website/fed9cc193a14b84131812372d8d5857f8f304c52.pdf">model card addendum</a>.</p><p>As part of our commitment to safety and transparency, we’ve engaged with external experts to test and refine the safety mechanisms within this latest model. We recently provided Claude 3.5 Sonnet to the UK’s Artificial Intelligence Safety Institute (UK AISI) for pre-deployment safety evaluation. The UK AISI completed tests of 3.5 Sonnet and shared their results with the US AI Safety Institute (US AISI) as part of a Memorandum of Understanding, made possible by the partnership between the US and UK AISIs <a href="https://www.commerce.gov/news/press-releases/2024/04/us-and-uk-announce-partnership-science-ai-safety">announced earlier this year</a>.</p><p>We have integrated policy feedback from outside subject matter experts to ensure that our evaluations are robust and take into account new trends in abuse. This engagement has helped our teams scale up our ability to evaluate 3.5 Sonnet against various types of misuse. For example, we used feedback from child safety experts at <a href="https://www.thorn.org/">Thorn</a> to update our classifiers and fine-tune our models.</p><p>One of the core constitutional principles that guides our AI model development is privacy. We do not train our generative models on user-submitted data unless a user gives us explicit permission to do so. To date we have not used any customer or user-submitted data to train our generative models.</p><h3>Coming soon</h3><p>Our aim is to substantially improve the tradeoff curve between intelligence, speed, and cost every few months. To complete the Claude 3.5 model family, we’ll be releasing Claude 3.5 Haiku and Claude 3.5 Opus later this year.</p><p>In addition to working on our next-generation model family, we are developing new modalities and features to support more use cases for businesses, including integrations with enterprise applications. Our team is also exploring features like Memory, which will enable Claude to remember a user’s preferences and interaction history as specified, making their experience even more personalized and efficient.</p><p>We’re constantly working to improve Claude and love hearing from our users. You can submit feedback on Claude 3.5 Sonnet directly in-product to inform our development roadmap and help our teams to improve your experience. As always, we look forward to seeing what you build, create, and discover with Claude.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wc2: Investigates optimizing 'wc', the Unix word count program (120 pts)]]></title>
            <link>https://github.com/robertdavidgraham/wc2</link>
            <guid>40738833</guid>
            <pubDate>Thu, 20 Jun 2024 13:54:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/robertdavidgraham/wc2">https://github.com/robertdavidgraham/wc2</a>, See on <a href="https://news.ycombinator.com/item?id=40738833">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">wc2 - asynchronous state machine parsing</h2><a id="user-content-wc2---asynchronous-state-machine-parsing" aria-label="Permalink: wc2 - asynchronous state machine parsing" href="#wc2---asynchronous-state-machine-parsing"></a></p>
<p dir="auto">There have been multiple articles lately implementing the
classic <code>wc</code> program in various programming <em>languages</em>, to
"prove" their favorite language can be "just as fast" as C.</p>
<p dir="auto">This project does something different.
Instead of a different <em>language</em> it uses a different <em>algorithm</em>.
The new algorithm is significantly faster -- implementing in a
slow language like JavaScript is still faster than the original
<code>wc</code> program written in C.</p>
<p dir="auto">The algorithm is known as an "asynchronous state-machine parser".
It's a technique for <em>parsing</em> that you don't learn in college.
It's more <em>efficient</em>, but more importantly, it's more <em>scalable</em>.
That's why your browser uses a state-machine to parse GIFs,
and most web servers use state-machiens to parse incoming HTTP requests.</p>
<p dir="auto">This projects contains three versions:</p>
<ul dir="auto">
<li><code>wc2o.c</code> is a simplified 25 line version highlighting the idea</li>
<li><code>wc2.c</code> is the full version in C, supporting Unicode</li>
<li><code>wc2.js</code> is the version in JavaScript</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">The basic algorithm</h2><a id="user-content-the-basic-algorithm" aria-label="Permalink: The basic algorithm" href="#the-basic-algorithm"></a></p>
<p dir="auto">The algorithm reads input and passes each byte one at a time
to a state-machine. It looks something like:</p>
<div dir="auto" data-snippet-clipboard-copy-content="    length = fread(buf, 1, sizeof(buf), fp);
    for (i=0; i<length; i++) {
        c = buf[i];
        state = table[state][c];
        counts[state]++;
    }"><pre>    <span>length</span> <span>=</span> <span>fread</span>(<span>buf</span>, <span>1</span>, <span>sizeof</span>(<span>buf</span>), <span>fp</span>);
    <span>for</span> (<span>i</span><span>=</span><span>0</span>; <span>i</span><span>&lt;</span><span>length</span>; <span>i</span><span>++</span>) {
        <span>c</span> <span>=</span> <span>buf</span>[<span>i</span>];
        <span>state</span> <span>=</span> <span>table</span>[<span>state</span>][<span>c</span>];
        <span>counts</span>[<span>state</span>]<span>++</span>;
    }</pre></div>
<p dir="auto">No, you aren't suppose to be able to see how the word-count works
by looking at this code. The complexity happens elsewhere, setting
up the state-machine.</p>
<p dir="auto">The state-machine table is the difference between the simple version
(<code>wc2o.c</code>) and complex version (<code>wc2.c</code>) of the program. The algorithm
is the same, the one shown above, the difference is in how they setup
the table. The simple program creates a table for ASCII, the complex
program creates a much larger table supporting UTF-8.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How <code>wc</code> works</h2><a id="user-content-how-wc-works" aria-label="Permalink: How wc works" href="#how-wc-works"></a></p>
<p dir="auto">The <code>wc</code> word-count program counts the number of words in a file. A "word"
is some non-space characters separate by space.</p>
<p dir="auto">Those who re-implement <code>wc</code> simplify the problem by only doing ASCII instead
of the full UTF-8 Unicode. This is cheating, because much of the speed of
<code>wc</code> comes from its need to handle character-sets like UTF-8.
The real programs spend most of their time
in functions like <code>mbrtowc()</code> to parse multi-byte characters and
<code>iswspace()</code> to test if they are spaces -- which re-implementations
of <code>wc</code> skip.</p>
<p dir="auto">For this reason, we've implemented a full UTF-8 version in this project, to
prove that it works without cheating. Now the real <code>wc</code> works with a lot
more character-sets, and we don't do that. But by implementing UTF-8, we've
shown that it's possible, and that the speed for any character-set is the same.</p>
<p dir="auto">Another simplification is how invalid input is handled. The original <code>wc</code> program
largley ignores errors, but it's still an important factor in making sure you
are doing things correctly.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Benchmark input files</h2><a id="user-content-benchmark-input-files" aria-label="Permalink: Benchmark input files" href="#benchmark-input-files"></a></p>
<p dir="auto">This project uses a number of large input files for benchmarking.
The traditional <code>wc</code> program has wildly different performance depending
upon input, such as whether the file is full of illegal characters, or
whether UTF-8 is being handled. The first test file is downloaded
from the Internet as "real-world data", while the others are generated
using a program built with this project (<code>wctool</code>).</p>
<ul dir="auto">
<li><code>pocorgtfo18.pdf</code> a large 92-million byte PDF file that contains binary/illegal characters</li>
<li><code>ascii.txt</code> a file the same size containing random words, ASCII-only</li>
<li><code>utf8.txt</code> a file containing random UTF-8 sequences of 1, 2, 3, and 4 bytes</li>
<li><code>word.txt</code> a file containing 92-million 'x' characters</li>
<li><code>space.txt</code> a file containing 92-million ' ' (space) characters</li>
</ul>
<p dir="auto">Before benchmarking the old <code>wc</code>, set the character-set to UTF-8. It's
probably already set to this on new systems, but do this to make sure:</p>
<div data-snippet-clipboard-copy-content="$ export LC_CTYPE=en_US.UTF-8"><pre><code>$ export LC_CTYPE=en_US.UTF-8
</code></pre></div>
<p dir="auto">When running <code>wc</code>, the <code>-lwc</code> is the default for counting words in ASCII text.
To convert it into UTF-8 "multi-byte" mode, change <code>c</code> o <code>m</code>, as in <code>-lwm</code>.</p>
<p dir="auto">The numbers are reported come from the Unix <code>time</code> command, the number of seconds for
<code>user</code> time. In other words, <code>elapsed</code> time or <code>system</code> time aren't reported.</p>
<p dir="auto">The following table shows benchmarking a 2019 x86 MacBook Air of the old
<code>wc</code> program. As you can see, it has a wide variety of speeds depending
on input.</p>
<p dir="auto">The <code>wc</code> program included with macOS and Linux are completely different.
Therefore, the following table shows them benchmarked against each other
on the same hardware.</p>
<table>
<thead>
<tr>
<th>Command</th>
<th>Input File</th>
<th>macOS</th>
<th>Linux</th>
</tr>
</thead>
<tbody>
<tr>
<td>wc -lwc</td>
<td>pocorgtfo18.pdf</td>
<td>0.709</td>
<td>5.591</td>
</tr>
<tr>
<td>wc -lwm</td>
<td>pocorgtfo18.pdf</td>
<td>0.693</td>
<td>5.419</td>
</tr>
<tr>
<td>wc -lwc</td>
<td>ascii.txt</td>
<td>0.296</td>
<td>2.509</td>
</tr>
<tr>
<td>wc -lwm</td>
<td>utf8.txt</td>
<td>0.532</td>
<td>1.840</td>
</tr>
<tr>
<td>wc -lwc</td>
<td>space.txt</td>
<td>0.296</td>
<td>0.284</td>
</tr>
<tr>
<td>wc -lwm</td>
<td>space.txt</td>
<td>0.295</td>
<td>0.298</td>
</tr>
<tr>
<td>wc -lwc</td>
<td>word.txt</td>
<td>0.302</td>
<td>1.268</td>
</tr>
<tr>
<td>wc -lwm</td>
<td>word.txt</td>
<td>0.294</td>
<td>1.337</td>
</tr>
</tbody>
</table>
<p dir="auto">These results tell us:</p>
<ul dir="auto">
<li>Illegal characters (in <code>pocorgtfo18.pdf</code>) slow things down a lot,
twice as slow on macOS, 10x slower on Linux.</li>
<li>Text that randomly switches between spaces and words is much slower
than text containing all the same character.</li>
<li>On Linux, the code path that reads all spaces is significantly faster.</li>
<li>The macOS program is in general much faster than the Linux version.</li>
<li>Processing Unicode (the file <code>utf8.txt</code> with the <code>-m</code> option) is slower
than processing ASCII (the file <code>ascii.txt</code> with the <code>-c</code> option).</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Our benchmarks</h2><a id="user-content-our-benchmarks" aria-label="Permalink: Our benchmarks" href="#our-benchmarks"></a></p>
<p dir="auto">The time for our algorithm, in C and JavaScript, are the following.
The state-machine parser is immune to input type, all the input files
show the same results.</p>
<table>
<thead>
<tr>
<th>Program</th>
<th>Input File</th>
<th>macOS</th>
<th>Linux</th>
</tr>
</thead>
<tbody>
<tr>
<td>wc2.c</td>
<td>(all)</td>
<td>0.206</td>
<td>0.278</td>
</tr>
<tr>
<td>wc2.js</td>
<td>(all)</td>
<td>0.281</td>
<td>0.488</td>
</tr>
</tbody>
</table>
<p dir="auto">These results tell us:</p>
<ul dir="auto">
<li>This state machine approach always results in the same speed, regardless
of input.</li>
<li>This state machine approach is faster than the built-in programs.</li>
<li>Even written in JavaScript, the state machine approach is competitive in speed.</li>
<li>The difference in macOS and Linux speed is actually the difference in <code>clang</code> and <code>gcc</code>
speed. The LLVM <code>clang</code> compiler is doing better optimizations for x86 processors here.</li>
<li>I don't know why Node.js behaves differently on macOS and Linux, it's probably just
due to different versions.</li>
<li>A JIT (like NodeJS) works well with simple compute algorithms. This tells
us little about it's relative performance in larger programs. All languages
that have a JIT should compile this sort of algorithm to roughly the same
speed.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Asynchronous scalability</h2><a id="user-content-asynchronous-scalability" aria-label="Permalink: Asynchronous scalability" href="#asynchronous-scalability"></a></p>
<p dir="auto">The algorithm is <em>faster</em>, but more importantly, it's more <em>scalable</em>.</p>
<p dir="auto">Such scalability isn't usefull for <code>wc</code>, but is incredibly important for network
programs. Consider an HTTP web-server. The traditional way that the Apache web-server
worked was by reading the entire header in and buffering it, before then parsing
the header. This need to buffer the entire header caused an enormous scalability
problem. In contrast, asynchronous web-servers like Nginx use a state-machine
parser. They parse the bytes as they arrive, and discard them.</p>
<p dir="auto">This is analogous to NFA and DFA regular-expressions. If you use the NFA
approach, you need to buffer the entire chunk of data, so that the regex
can backtrack. Using the DFA approach, input can be provided as a stream,
one byte at a time, without needing buffering. DFAs are more scalable than NFAs.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">State machine parsers</h2><a id="user-content-state-machine-parsers" aria-label="Permalink: State machine parsers" href="#state-machine-parsers"></a></p>
<p dir="auto">This project contains a minimalistic <code>wc2o.c</code> program to highlight
the algorithm, without all the fuss of building UTF-8 tables, supporting
only ASCII.</p>
<div dir="auto" data-snippet-clipboard-copy-content="#include <stdio.h>
int main(void)
{
    static const unsigned char table[4][4] = {
        {2,0,1,0,}, {2,0,1,0,}, {3,0,1,0,},  {3,0,1,0,}
    };
    static const unsigned char column[256] = {
        0,0,0,0,0,0,0,0,0,1,2,1,1,1,0,0,0,
        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,
    };
    unsigned long counts[4] = {0,0,0,0};
    int state = 0;
    int c;

    while ((c = getchar()) != EOF) {
        state = table[state][column[c]];
        counts[state]++;
    }

    printf(&quot;%lu %lu %lu\n&quot;, counts[1], counts[2], 
                counts[0] + counts[1] + counts[2] + counts[3]);
    return 0;
}"><pre><span>#include</span> <span>&lt;stdio.h&gt;</span>
<span>int</span> <span>main</span>(<span>void</span>)
{
    <span>static</span> <span>const</span> <span>unsigned <span>char</span></span> <span>table</span>[<span>4</span>][<span>4</span>] <span>=</span> {
        {<span>2</span>,<span>0</span>,<span>1</span>,<span>0</span>,}, {<span>2</span>,<span>0</span>,<span>1</span>,<span>0</span>,}, {<span>3</span>,<span>0</span>,<span>1</span>,<span>0</span>,},  {<span>3</span>,<span>0</span>,<span>1</span>,<span>0</span>,}
    };
    <span>static</span> <span>const</span> <span>unsigned <span>char</span></span> <span>column</span>[<span>256</span>] <span>=</span> {
        <span>0</span>,<span>0</span>,<span>0</span>,<span>0</span>,<span>0</span>,<span>0</span>,<span>0</span>,<span>0</span>,<span>0</span>,<span>1</span>,<span>2</span>,<span>1</span>,<span>1</span>,<span>1</span>,<span>0</span>,<span>0</span>,<span>0</span>,
        <span>0</span>,<span>0</span>,<span>0</span>,<span>0</span>,<span>0</span>,<span>0</span>,<span>0</span>,<span>0</span>,<span>0</span>,<span>0</span>,<span>0</span>,<span>0</span>,<span>0</span>,<span>0</span>,<span>0</span>,<span>1</span>,<span>0</span>,
    };
    <span>unsigned long</span> <span>counts</span>[<span>4</span>] <span>=</span> {<span>0</span>,<span>0</span>,<span>0</span>,<span>0</span>};
    <span>int</span> <span>state</span> <span>=</span> <span>0</span>;
    <span>int</span> <span>c</span>;

    <span>while</span> ((<span>c</span> <span>=</span> <span>getchar</span>()) <span>!=</span> <span>EOF</span>) {
        <span>state</span> <span>=</span> <span>table</span>[<span>state</span>][<span>column</span>[<span>c</span>]];
        <span>counts</span>[<span>state</span>]<span>++</span>;
    }

    <span>printf</span>(<span>"%lu %lu %lu\n"</span>, <span>counts</span>[<span>1</span>], <span>counts</span>[<span>2</span>], 
                <span>counts</span>[<span>0</span>] <span>+</span> <span>counts</span>[<span>1</span>] <span>+</span> <span>counts</span>[<span>2</span>] <span>+</span> <span>counts</span>[<span>3</span>]);
    <span>return</span> <span>0</span>;
}</pre></div>
<p dir="auto">The key part that does all the word counting is in the two lines inside:</p>
<div dir="auto" data-snippet-clipboard-copy-content="    while ((c = getchar()) != EOF) {
        state = table[state][column[c]];
        counts[state]++;
    }"><pre>    <span>while</span> ((<span>c</span> <span>=</span> <span>getchar</span>()) <span>!=</span> <span>EOF</span>) {
        <span>state</span> <span>=</span> <span>table</span>[<span>state</span>][<span>column</span>[<span>c</span>]];
        <span>counts</span>[<span>state</span>]<span>++</span>;
    }</pre></div>
<p dir="auto">This is only defined for ASCII, so you can see the state-machine on a
single-line in the code (<code>table</code>).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Additional tools</h2><a id="user-content-additional-tools" aria-label="Permalink: Additional tools" href="#additional-tools"></a></p>
<p dir="auto">This project includes additional tools:</p>
<ul dir="auto">
<li><code>wctool</code> to generate large test files</li>
<li><code>wcdiff</code> to find difference between two implementatins of <code>wc</code></li>
<li><code>wcstream</code> to fragment input files (demonstrates a bug in macOS's <code>wc</code>)</li>
</ul>
<p dir="auto">The program <code>wc2.c</code> has the same logic, the difference being that it
generates a larger state-machine for parsing UTF-8.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Pointer arithmetic</h2><a id="user-content-pointer-arithmetic" aria-label="Permalink: Pointer arithmetic" href="#pointer-arithmetic"></a></p>
<p dir="auto">C has a peculiar idiom called "pointer arithmetic", where pointers can
be incremented. Looping through a buffer is done with an expression like
<code>*buf++</code> instead of <code>buf[i++]</code>. Many programmers think pointer-arithmetic
is faster.</p>
<p dir="auto">To test this, the <code>wc2.c</code> program has an option <code>-P</code> that makes this
small change, to test the difference in speed.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NPM and NodeJS should do more to make ES Modules easy to use (160 pts)]]></title>
            <link>https://borischerny.com/javascript,/typescript/2024/06/19/ES-Modules-Are-A-Mess.html</link>
            <guid>40737508</guid>
            <pubDate>Thu, 20 Jun 2024 11:40:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://borischerny.com/javascript,/typescript/2024/06/19/ES-Modules-Are-A-Mess.html">https://borischerny.com/javascript,/typescript/2024/06/19/ES-Modules-Are-A-Mess.html</a>, See on <a href="https://news.ycombinator.com/item?id=40737508">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <section role="heading">
      <header role="heading">
        <h2>
          <a href="https://borischerny.com/">Boris Cherny's Blog</a>
        </h2>
      </header>
      
    </section>
    <section role="article">
<h3>June 19, 2024</h3>

<p>Coming back to JavaScript and TypeScript after a few years neck deep in Python and Hack, I kept hitting a number of new, cryptic errors when running NodeJS code in my dev environment:</p>

<div><pre><code><span># when I ran ESM TypeScript code the wrong way:</span>
Error <span>[</span>ERR_REQUIRE_ESM]: Must use import to load ES Module

<span># when I imported an ESModule from a CommonJS .js file:</span>
Error <span>[</span>ERR_REQUIRE_ESM]: require<span>()</span> of ES Module .../lodash.js from .../index.cjs not supported

<span># when I imported an ESModule from a .ts file:</span>
error TS1479: The current file is a CommonJS module whose imports will produce <span>'require'</span> calls

<span># when I used ES6 import syntax in a .js file:</span>
SyntaxError: Cannot use import statement outside a module
</code></pre></div>

<p>These errors are all related to importing, typechecking, and loading modules. The JavaScript ecosystem moves fast, and things changing over the last few years was not a surprise. However, it was surprising to see so many errors related to such a core piece of the language!</p>

<h2 id="how-we-got-here">How we got here</h2>

<p>Modules in JavaScript and TypeScript have changed significantly over time:</p>

<ul>
  <li>Years ago, there was no module system for JavaScript and TypeScript. A number of solutions sprang up around ways to declare and load modules: IIFEs, <a href="https://github.com/getify/LABjs">LabJS</a>, <a href="https://github.com/amdjs/amdjs-api/blob/master/AMD.md">AMD</a>, <a href="https://requirejs.org/">require.js</a>, <a href="https://www.typescriptlang.org/docs/handbook/namespaces.html">TypeScript namespaces</a>, and more. Tooling support and interop were hit or miss.</li>
  <li>CommonJS emerged as a standard-by-convention for modules, across browser, server, JavaScript, and TypeScript.</li>
  <li>When ES6 came out, folks started switching over to <code>import</code> and <code>export</code> syntax (from CommonJS’s <code>require</code> and <code>module.exports</code>), using tools like Babel and TypeScript to compile code down to CommonJS.</li>
  <li>CommonJS can be challenging to statically analyze, and uses an <a href="https://www.youtube.com/watch?v=W5CXzo4TZVU">inefficient</a>, synchronous module loading algorithm at runtime. ES Modules were <a href="https://tc39.es/ecma262/#sec-modules">introduced</a> as the way to use <code>import</code> and <code>export</code>, while at the same time improving code load times at runtime.</li>
  <li>ES Modules introduced significant complexity for NodeJS in particular: instead of reusing the <em>.js</em> and <em>.ts</em> file extensions, ES Modules in NodeJS require either using <em>.mjs</em>, or setting <code>type=module</code> in your <em>package.json</em>. Interoperating these modules with an ecosystem-ful of CommonJS remains painful.</li>
</ul>

<h2 id="current-state-by-the-numbers">Current state, by the numbers</h2>

<p>I was curious – since ES Modules (<code>import</code>/<code>export</code>) were introduced in <a href="https://262.ecma-international.org/6.0/#sec-modules">2015</a>, and NodeJS has supported <code>type=module/commonjs</code>, <em>.mjs</em>, and <em>.cjs</em>, with the goal of replacing <em>.js</em>, since <a href="https://nodejs.org/api/packages.html#type">2019</a>, to what degree have these new conventions been adopted?</p>

<p>I answered this with data, using two approaches:</p>

<ol>
  <li>Looking at the most starred JavaScript and TypeScript repos on Github</li>
  <li>Looking at the most downloaded packages on NPM</li>
</ol>

<p>The <a href="https://github.com/bcherny/es-module-stats">results</a> are not rosy. After 5+ years, adoption of ES Modules remains weak:</p>

<ol>
  <li>Between 9-27% of JavaScript/TypeScript projects declare themselves to be ES Modules via the <code>type</code> (and lesser-used <code>exports</code>) fields in their <em>package.json</em>s.</li>
  <li>Less than 6% of JavaScript/TypeScript files declare that they are ES Modules via the <em>.mjs</em>, <em>.cjs</em>, <em>.mts</em>, etc. file extensions.</li>
</ol>

<p>Note that these ranges come from the two approaches I used to estimate the numbers. Head <a href="https://github.com/bcherny/es-module-stats">here</a> for more detailed data and code.</p>

<h2 id="how-do-we-fix-it">How do we fix it?</h2>

<p>This helps explain why it’s so painful to interoperate ES Modules and CommonJS across both NodeJS and TypeScript: enough libraries use ES Modules that for many projects you need to either use ES Modules, or figure out how to interoperate ES Modules with your CommonJS code. At the same time, enough code still uses CommonJS that you often need to figure out how to include that legacy code in your otherwise-ES Module project.</p>

<p>The benefits of ES Modules are significant. Rolling everything back to CommonJS is not the way forward. Is there more we can do to simplify the ecosystem, and push harder on adoption? Some ideas:</p>

<ol>
  <li>We should kill <em>.mjs</em>, <em>.cjs</em>, <em>.mts</em>, etc. The vast majority of projects use <code>type=module</code> in their <em>package.json</em>, rather than file extensions. It would simplify things considerably if we drop support for these new file extensions and stick to <em>.js</em>, <em>.jsx</em>, <em>.ts</em>, and <em>.tsx</em>.</li>
  <li>We should make <code>type=module</code> the <a href="https://github.com/npm/cli/issues/7594">default</a> for new <em>package.json</em> files for the <code>npm init</code>, <code>yarn init</code>, and <code>pnpm init</code> commands. Package managers’ <code>publish</code> commands should warn when <code>type</code> is not set to <code>module</code>.</li>
  <li>We should upgrade the most common libraries used by the community to ES Modules, either manually or through automated pull requests (this feels like something that can be semi-automated).</li>
  <li>The NPM registry can require an explicit <code>module</code> field on new packages, making it clear when a package intentionally uses CommonJS (eg. because it targets legacy NodeJS versions).</li>
  <li>NodeJS can officially drop support for <code>require</code> and <code>module.exports</code> in a future version, creating a bit more pressure to migrate.</li>
</ol>

<p>I’d love to hear others’ thoughts. Have you also felt the pain of interoperating ES Modules and CommonJS?</p>

<hr>

<p>Discuss this post on <a href="https://news.ycombinator.com/item?id=40737508">HackerNews</a> or on <a href="https://www.threads.net/@boris_cherny/post/C8aDJuGI5HM">Threads</a>.</p>

</section>
  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Curl is inside 22,734 Steam games (132 pts)]]></title>
            <link>https://daniel.haxx.se/blog/2024/06/20/inside-22734-steam-games/</link>
            <guid>40737349</guid>
            <pubDate>Thu, 20 Jun 2024 11:18:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daniel.haxx.se/blog/2024/06/20/inside-22734-steam-games/">https://daniel.haxx.se/blog/2024/06/20/inside-22734-steam-games/</a>, See on <a href="https://news.ycombinator.com/item?id=40737349">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">

	<div id="primary" role="main">
			
<article id="post-25020">
	
		<p><img width="672" height="372" src="https://daniel.haxx.se/blog/wp-content/uploads/2016/09/GTA-end-credits-libcurl-672x372.jpg" alt="" decoding="async" srcset="https://daniel.haxx.se/blog/wp-content/uploads/2016/09/GTA-end-credits-libcurl-672x372.jpg 672w, https://daniel.haxx.se/blog/wp-content/uploads/2016/09/GTA-end-credits-libcurl-200x110.jpg 200w, https://daniel.haxx.se/blog/wp-content/uploads/2016/09/GTA-end-credits-libcurl-1038x576.jpg 1038w" sizes="(max-width: 672px) 100vw, 672px">		</p>

		
	<!-- .entry-header -->

		<div>
		
<p>About a year ago I blogged about <a href="https://daniel.haxx.se/blog/2023/06/09/games-curl-too/" data-type="post" data-id="22459">games that use curl</a>. In that post I listed a bunch of well-known titles I knew use curl and there was a list of 136 additional games giving credit to curl.</p>



<p>Kind of amazing that over <em>one hundred games</em> decided to use curl!</p>



<p>At the time, lots of people told me that number was probably way low and while I kind of had that feeling as well it was just a feeling and nothing else. We cannot be <em>absolutely certain</em> unless there is data or evidence to actually back it up.</p>



<p>The speculation could stop this week when someone provided me with a link to a database of Steam titles (<a href="https://en.wikipedia.org/wiki/Steam_(service)">Steam</a>, as in the video game service). <a href="https://steamdb.info/">SteamDB</a> is a third-party site that among other things extracts data and figures out which “SDKs” are used by Steam games: <a href="https://steamdb.info/tech/SDK/cURL/">Their list of game titles on Steam using curl</a>.</p>



<p>Since that list is capped at 10,000 titles, I had to filter it and add up the number of titles based on release year. Out of the 91,559 titles they currently list in their database, <strong>22,734</strong> are identified to be using curl: <strong>24.8%</strong>.</p>



<p>Not too shabby for a hobby.</p>


<div>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/daniel.haxx.se\/blog\/wp-content\/uploads\/2024\/04\/curl-is-just-2000.jpg&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-full&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-24622&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2000,&quot;targetHeight&quot;:1500,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img decoding="async" width="2000" height="1500" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://daniel.haxx.se/blog/wp-content/uploads/2024/04/curl-is-just-2000.jpg" alt=""></figure></div>	</div><!-- .entry-content -->
	
	</article><!-- #post-25020 -->
		<nav>
		<h2>
			Post navigation		</h2>
		<!-- .nav-links -->
		</nav><!-- .navigation -->
		
<!-- #comments -->
		</div><!-- #primary -->

<!-- #content-sidebar -->
<div id="secondary">
		<h2>tech, open source and networking</h2>
	
	
		<!-- #primary-sidebar -->
	</div><!-- #secondary -->

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Delusion of Advanced Plastic Recycling (167 pts)]]></title>
            <link>https://www.propublica.org/article/delusion-advanced-chemical-plastic-recycling-pyrolysis</link>
            <guid>40737308</guid>
            <pubDate>Thu, 20 Jun 2024 11:12:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.propublica.org/article/delusion-advanced-chemical-plastic-recycling-pyrolysis">https://www.propublica.org/article/delusion-advanced-chemical-plastic-recycling-pyrolysis</a>, See on <a href="https://news.ycombinator.com/item?id=40737308">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-pp-location="article body">

        
        
        




                    
<p data-pp-blocktype="copy" data-pp-id="2.0"><span>Last year,</span> I became obsessed with a plastic cup. </p>

<p data-pp-blocktype="copy" data-pp-id="2.1">It was a small container that held diced fruit, the type thrown into lunch boxes. And it was the first product I’d seen born of what’s being touted as a cure for a crisis.</p>
        
    
                        


   
            
            
            
            

   
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="4.0">Plastic doesn’t break down in nature. If you turned all of what’s been made into cling wrap, it would <a href="https://www.sciencedaily.com/releases/2016/01/160127083854.htm">cover every inch of the globe</a>. It’s piling up, <a href="https://www.epa.gov/plastics/impacts-plastic-pollution">leaching into our water</a> and <a href="https://www.sciencenews.org/article/microplastics-nanoplastics-heart-attacks-strokes-health">poisoning our bodies</a>.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="5.0">Scientists say the key to fixing this is to make less of it; the world churns out 430 million metric tons each year.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="6.0">But businesses that rely on plastic production, like fossil fuel and chemical companies, have worked since the <a href="https://grist.org/accountability/petrochemical-companies-have-known-for-40-years-that-plastics-recycling-wouldnt-work/">1980s to spin the pollution as a failure of waste management</a> — one that can be solved with recycling.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="7.0"><a href="https://www.npr.org/2020/03/31/822597631/plastic-wars-three-takeaways-from-the-fight-over-the-future-of-plastics">Industry leaders knew then</a> what we know now: Traditional recycling <a href="https://theintercept.com/2019/07/20/plastics-industry-plastic-recycling/">would </a><a href="https://theintercept.com/2019/07/20/plastics-industry-plastic-recycling/">barely put a dent in the trash heap</a>. It’s hard to transform flimsy candy wrappers into sandwich bags, or to make containers that once held motor oil clean enough for milk.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="8.0">Now, the industry is heralding nothing short of a miracle: an “advanced”type of recycling known as pyrolysis — “pyro” means fire and “lysis” means separation. It uses heat to break plastic all the way down to its molecular building blocks.</p>
        
    
                    
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="10.0">While old-school, “mechanical” recycling yields plastic that’s degraded or contaminated, this type of “chemical” recycling promises plastic that behaves like it’s new, and could usher in what the industry casts as a green revolution: Not only would it save hard-to-recycle plastics like frozen food wrappers from the dumpster, but it would turn them into new products that can replace the old ones and be chemically recycled again and again.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="11.0">So when three companies used ExxonMobil’s pyrolysis-based technology to successfully conjure up that fruit cup, <a href="https://www.packworld.com/supplier-news/news/22871469/printpack-printpack-exxonmobil-pacific-coast-producers-bring-circularity-to-fruit-cups">they announced it to the world</a>.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="12.0">“This is a significant milestone,” said Printpack, which turned the plastic into cups. The fruit supplier Pacific Coast Producers called it “the most important initiative a consumer-packaged goods company can pursue.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="13.0">“ExxonMobil is supporting the circularity of plastics,” the August 2023 news release said, citing <a href="https://grist.org/accountability/circular-economy-plastics-recycling-reuse-waste-conference-seattle/">a </a><a href="https://grist.org/accountability/circular-economy-plastics-recycling-reuse-waste-conference-seattle/">buzzword</a> that implies an infinite loop of using, recycling and reusing.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="14.0">They were so proud, I hoped they would tell me all about how they made the cup, how many of them existed and where I could buy one.</p>

<p data-pp-blocktype="copy" data-pp-id="14.1">So began my long — and, well, circular — pursuit of the truth at a time when it really matters.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="15.0">This year, nearly all of the world’s countries are hammering out a <a href="https://www.propublica.org/article/plastics-waste-united-nations-international-conference-treaty-ottawa">United Nations treaty to deal with the plastic crisis</a>. As they consider limiting production, <a href="https://www.ehn.org/chemical-recycling-of-plastics-2667297273.html">the industry is making a hard push to shift the conversation</a> to the wonders of chemical recycling. It’s also buying ads during cable news shows as U.S. states consider laws to limit plastic packaging and lobbying federal agencies to loosen the very definition of what it means to recycle.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="16.0">It’s been selling governments on chemical recycling, with quite a bit of success. American and <a href="https://www.euwid-recycling.com/news/business/lyondellbasell-to-receive-eur40m-in-eu-funding-for-chemical-recycling-plant-080124/">European regulators</a> <a href="https://theintercept.com/2023/10/31/plastics-pollution-advanced-recycling/">have spent tens of </a><a href="https://theintercept.com/2023/10/31/plastics-pollution-advanced-recycling/">millions</a><a href="https://theintercept.com/2023/10/31/plastics-pollution-advanced-recycling/"> subsidizing pyrolysis facilities</a>. <a href="https://www.americanchemistry.com/chemistry-in-america/news-trends/press-release/2024/with-wyoming-half-the-country-open-to-advanced-recycling">Half of all U.S. states</a> have <a href="https://e360.yale.edu/features/advanced-plastics-recycling-pyrolysis">eased air pollution rules</a> for the process, which has been found to <a href="https://www.nrdc.org/sites/default/files/chemical-recycling-greenwashing-incineration-ib.pdf">release carcinogens like benzene and dioxins</a> and <a href="https://pubs.acs.org/doi/10.1021/acssuschemeng.2c05497#">give off more greenhouse gases than making plastic from crude oil</a>.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="17.0">Given the high stakes of this moment, I set out to understand exactly what the world is getting out of this recycling technology. For months, I tracked press releases, interviewed experts, tried to buy plastic made via pyrolysis and learned more than I ever wanted to know about the science of recycled molecules.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="18.0">Under all the math and engineering, I found an inconvenient truth: Not much is being recycled at all, nor is pyrolysis capable of curbing the plastic crisis.  </p>

<p data-pp-blocktype="copy" data-pp-id="18.1">Not now. Maybe not ever.</p>
        
    
                    

<figure data-pp-id="20" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="300" height="200" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/item-10.png?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=80&amp;w=800&amp;s=7c0cf6520a1b45b5c6b466ab9f20c825" srcset="https://img.assets-d.propublica.org/v5/images/item-10.png?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=267&amp;q=80&amp;w=400&amp;s=c2a95b75734aaec3d82ebab09ca6e07a 400w, https://img.assets-d.propublica.org/v5/images/item-10.png?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=80&amp;w=800&amp;s=7c0cf6520a1b45b5c6b466ab9f20c825 800w, https://img.assets-d.propublica.org/v5/images/item-10.png?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=800&amp;q=80&amp;w=1200&amp;s=35a4dfd7efe9417e228127c272472461 1200w, https://img.assets-d.propublica.org/v5/images/item-10.png?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=867&amp;q=80&amp;w=1300&amp;s=292b2d8c2964a3aea440141efbaf99da 1300w, https://img.assets-d.propublica.org/v5/images/item-10.png?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=967&amp;q=80&amp;w=1450&amp;s=fbb209578b9489431171ea59aa2b5947 1450w, https://img.assets-d.propublica.org/v5/images/item-10.png?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1067&amp;q=80&amp;w=1600&amp;s=0651de84e0db97e13899d7e6ac61577d 1600w, https://img.assets-d.propublica.org/v5/images/item-10.png?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1333&amp;q=80&amp;w=2000&amp;s=4d3b38b00a27b9a622eeebf3dd17ed4e 2000w">

            
    
<figcaption>
    
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="21.0"><span>Let’s take</span> a closer look at that Printpack press release, which uses convoluted terms to describe the recycled plastic in that fruit cup: </p>

<p data-pp-blocktype="copy" data-pp-id="21.1">“30% ISCC PLUS certified-circular”</p>

<p data-pp-blocktype="copy" data-pp-id="21.2">“mass balance free attribution”</p>

<p data-pp-blocktype="copy" data-pp-id="21.3">It’s easy to conclude the cup was made with 30% recycled plastic — until you break down the numerical sleight of hand that props up that number. </p>

<p data-pp-blocktype="copy" data-pp-id="21.4">It took interviews with a dozen academics, consultants, environmentalists and engineers to help me do just that. </p>

<p data-pp-blocktype="copy" data-pp-id="21.5">Stick with me as I unravel it all.</p>
        
            
    
    
    
                            

<figure data-pp-id="1" data-pp-blocktype="embed">

    


                        <div id="lesson-one">
    <div>
         <p>
         <h3><span>Lesson 1</span></h3>
         </p>
          <h2>Most of the old plastic that goes <span>into</span> pyrolysis doesn’t actually become new&nbsp;plastic.
</h2>
      </div>
         <p><img src="https://static.propublica.org/projects/graphics/2024-plastics/images/item-1.png">
       <img src="https://static.propublica.org/projects/graphics/2024-plastics/images/item-2.png">
        <img src="https://static.propublica.org/projects/graphics/2024-plastics/images/item-3.png">
         <img src="https://static.propublica.org/projects/graphics/2024-plastics/images/item-5.png">
     
      
    </p>
</div>
            
    
<figcaption>
    
    
    
    </figcaption>


</figure>

            
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="24.0"><span>In traditional recycling,</span> plastic is turned into tiny pellets or flakes,  which you can melt again and mold back into recycled plastic products.</p>

<p data-pp-blocktype="copy" data-pp-id="24.1">Even in a real-life scenario, where bottles have labels and a little bit of juice left in them, most of the plastic products that go into the process find new life.</p>
        
            
    
    
    
                            

<figure data-pp-id="1" data-pp-blocktype="embed">

    


                        <!-- Generated by ai2html v0.115.1 - 2024-06-13 12:31 -->
<!-- ai file: plastic-recyclingrate-mechanical-ai2html.ai -->




<!-- End ai2html - 2024-06-13 12:31 -->
            
    
<figcaption>
    
    
    
    </figcaption>


</figure>

            
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="26.0">The numbers are much lower for pyrolysis.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="27.0">It’s “very, very, very, very difficult” to break down plastic that way, said Steve Jenkins, vice president of chemicals consulting at Wood Mackenzie, an energy and resources analytics firm. “The laws of nature and the laws of physics are trying to stop you.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="28.0">Waste is heated until it turns into oil. Part of that oil is composed of a liquid called naphtha, which is essential for making plastic. </p>

<p data-pp-blocktype="copy" data-pp-id="28.1">There are two ingredients in the naphtha that recyclers want to isolate: propylene and ethylene — gases that can be turned into solid plastics.</p>

<p data-pp-blocktype="copy" data-pp-id="28.2">To split the naphtha into different chemicals, it’s fed into a machine called a steam cracker. Less than half of what it spits out becomes propylene and ethylene.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="29.0">This means that if a pyrolysis operator started with 100 pounds of plastic waste, it can expect to end up with 15-20 pounds of reusable plastic. Experts told me the process can yield less if the plastic used is dirty or more if the technology is particularly advanced.</p>
        
            
    
    
    
                            

<figure data-pp-id="1" data-pp-blocktype="embed">

    


                        <!-- Generated by ai2html v0.115.1 - 2024-06-13 12:31 -->
<!-- ai file: plastic-recyclingrate-chemical-ai2html.ai -->




<!-- End ai2html - 2024-06-13 12:31 -->
            
    
<figcaption>
    
    
    
    </figcaption>


</figure>

            
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="31.0">I reached out to several companies to ask how much new plastic their processes actually yield, and none provided numbers. The American Chemistry Council, the nation’s largest plastic lobby, told me that because so many factors impact a company’s yield, it’s impossible to estimate that number for the entire industry.</p>
        
            
    
    
    
                            

<figure data-pp-id="1" data-pp-blocktype="embed">

    


                        <div id="lesson-two">

    <div>
         <p>
         <h3><span>Lesson 2</span></h3>
         </p>
          <h2> The plastic that comes <span>out of</span> pyrolysis contains very little recycled material. 

</h2>
      </div>
       <p><img src="https://static.propublica.org/projects/graphics/2024-plastics/images/item-7.png">

         <img src="https://static.propublica.org/projects/graphics/2024-plastics/images/item-5.png">
         <img src="https://static.propublica.org/projects/graphics/2024-plastics/images/item-2.png">
         <img src="https://static.propublica.org/projects/graphics/2024-plastics/images/item-6.png">




   </p>


</div>


            
    
<figcaption>
    
    
    
    </figcaption>


</figure>

            
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="34.0"><span>With mechanical recycling,</span> it’s hard to make plastic that’s 100% recycled; it’s expensive to do, and the process degrades plastic. Recycled pellets are often combined with new pellets to make stuff that’s 25% or 50% recycled, for example.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="35.0">But far less recycled plastic winds up in products made through pyrolysis. </p>

<p data-pp-blocktype="copy" data-pp-id="35.1">That’s because the naphtha created using recycled plastic is contaminated. Manufacturers add all kinds of chemicals to make products bend or keep them from degrading in the sun.</p>
        
            
    
    
    
                            

<figure data-pp-id="1" data-pp-blocktype="embed">

    


                        

    <img alt="" width="1460" height="2285" loading="lazy" js-autosizes="" src="https://static.propublica.org/projects/graphics/2024-plastics/graphics/plastic-steamcracker-mobile-FINAL.png">
            
    
<figcaption>
    
    
    
    </figcaption>


</figure>

            
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="37.0">Recyclers can overpower them by heavily diluting the recycled naphtha. With what, you ask? Nonrecycled naphtha made from ordinary crude oil!</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="38.0">This is the quiet — and convenient — part of the industry’s revolutionary pyrolysis method: It relies heavily on extracting fossil fuels. At least 90% of the naphtha used in pyrolysis is fossil fuel naphtha. Only then can it be poured into the steam cracker to separate the chemicals that make plastic.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="39.0">So at the end of the day, nothing that comes out of pyrolysis physically contains more than 10% recycled material (though experts and studies have shown that, in practice, it’s more like 5% or 2%).</p>
        
            
    
    
    
                            

<figure data-pp-id="1" data-pp-blocktype="embed">

    


                        

    <img alt="" width="1460" height="2285" loading="lazy" js-autosizes="" src="https://static.propublica.org/projects/graphics/2024-plastics/graphics/plastic-steamcracker-mobile-FINAL.png">
            
    
<figcaption>
    
    
    
    </figcaption>


</figure>

            
        
            
    
    
    
                            

<figure data-pp-id="1" data-pp-blocktype="embed">

    


                        <div id="lesson-three">

    <div>
         <p>
         <h3><span>Lesson 3</span></h3>
         </p>
          <h2>The industry uses mathematical acrobatics to make pyrolysis look like a&nbsp;success. 
</h2>
      </div>

        <p><img src="https://static.propublica.org/projects/graphics/2024-plastics/images/item-3.png">


              <img src="https://static.propublica.org/projects/graphics/2024-plastics/images/item-7.png">

     </p>


</div>


            
    
<figcaption>
    
    
    
    </figcaption>


</figure>

            
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="43.0"><span>Ten percent</span> doesn’t look very impressive. Some consumers are willing to pay <a href="https://theconversation.com/climate-explained-are-consumers-willing-to-pay-more-for-climate-friendly-products-146757">a premium for sustainability</a>, so companies use a form of accounting called mass balance to inflate the recycled-ness of their products. It’s not unlike offset schemes I’ve uncovered that <a href="https://features.propublica.org/brazil-carbon-offsets/inconvenient-truth-carbon-credits-dont-work-deforestation-redd-acre-cambodia/">absolve refineries of their carbon emissions</a> and <a href="https://www.propublica.org/article/biodiversity-offsets-guinea-world-bank-group-chimpanzees-outbreak">enable mining companies to kill chimpanzees</a>. Industry-affiliated groups like the International Sustainability and Carbon Certification write the rules. (ISCC didn’t respond to requests for comment.)</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="44.0">To see how this works, let’s take a look at what might happen to a batch of recycled naphtha. Let’s say the steam cracker splits the batch into 100 pounds of assorted ingredients.</p>
        
            
    
    
    
                            

<figure data-pp-id="1" data-pp-blocktype="embed">

    


                        

  

      <section id="scrolly-1">
        <div>

                <!-- persistent labels -->
                  <p>
                    = 1 pound
                  </p>
                  <p id="recycled-dot-label">
                    = 1 "recycled" pound
                  </p>
                  <p>
                    Propylene
                  </p>
                  <p>
                    Ethylene
                  </p>
                  <p>
                    Other chemicals, including fuel
                  </p>


                  
                  
                <!-- close all-content below -->
                </div>
          

        <div>
          <p>You'll get some colorless gasses that are used to make plastic: 13 pounds of propylene and 30 pounds of ethylene. You'll also wind up with 57 pounds of other chemicals.</p>
          <p>Propylene makes sturdy material such as butter tubs <img src="https://static.propublica.org/projects/graphics/2024-plastics/explainer/inline-butter.png">; ethylene makes flexible plastics like yogurt pouches <span><img src="https://static.propublica.org/projects/graphics/2024-plastics/explainer/inline-yogurt.png">.</span> Many of the other chemicals <img src="https://static.propublica.org/projects/graphics/2024-plastics/explainer/inline-chemicals.png"> aren't used to make plastic — some get used to make rubber and paint or are used as fuel. </p>
          <p>All of these outputs are technically 10% recycled, since they were made from 10% recycled naphtha. (I’m using this optimistic hypothetical to make the math easy.) </p>
          <p>But companies can do a number shuffle to assign all of the recycled value from the butter tubs <img src="https://static.propublica.org/projects/graphics/2024-plastics/explainer/inline-butter.png"> to the yogurt pouches <span><img src="https://static.propublica.org/projects/graphics/2024-plastics/explainer/inline-yogurt.png">.</span>  </p>
          <p>That way they can market the yogurt pouches <img src="https://static.propublica.org/projects/graphics/2024-plastics/explainer/inline-yogurt.png"> as 14% recycled (or “circular”), even though nothing has physically changed about the makeup of the pouches.  </p>
          <p>What's more, through a method called free attribution, companies can assign the recycled value from other chemicals <img src="https://static.propublica.org/projects/graphics/2024-plastics/explainer/inline-chemicals.png"> (even if they would never be turned into plastic) to the yogurt pouches <span><img src="https://static.propublica.org/projects/graphics/2024-plastics/explainer/inline-yogurt.png">.</span></p>
          <p>Now, the yogurt pouches <img src="https://static.propublica.org/projects/graphics/2024-plastics/explainer/inline-yogurt.png"> can be sold as 33% recycled. </p>
 
        </div>
      </section>

  
    
            
    
<figcaption>
    
    
    
    </figcaption>


</figure>

            
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="46.0">There are many flavors of this kind of accounting. Another version of free attribution would allow the company to take that entire 30-pound batch of “33% recycled” pouches and split them even further:</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="47.0">A third of them, 10 pounds, could be labeled 100% recycled — shifting the value of the full batch onto them — so long as the remaining 20 pounds aren’t labeled as recycled at all.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="48.0">As long as you avoid double counting, Jenkins told me, you can attribute the full value of recycled naphtha to the products that will make the most money. Companies need that financial incentive to recoup the costs of pyrolysis, he said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="49.0">But it’s hard to argue that this type of marketing is transparent. Consumers aren’t going to parse through the caveats of a 33% recycled claim or understand how the green technology they’re being sold perpetuates the fossil fuel industry. I posed the critiques to the industry, including environmentalists’ accusations that mass balance is just a fancy way of greenwashing.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="50.0">The American Chemistry Council told me it’s impossible to know whether a particular ethylene molecule comes from pyrolysis naphtha or fossil fuel naphtha; the compounds produced are “fungible” and can be used for multiple products, like making rubber, solvents and paints that would reduce the amount of new fossil fuels needed. Its statement called mass balance a “well-known methodology” that’s been used by other industries including fair trade coffee, chocolate and renewable energy.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="51.0">Legislation in the European Union already forbids free attribution, and leaders are debating whether to allow other forms of mass balance. U.S. regulation is far behind that, but as the Federal Trade Commission revises its general guidelines for green marketing, the industry is arguing that mass balance is crucial to the future of advanced recycling. “The science of advanced recycling simply does not support any other approach because the ability to track individual molecules does not readily exist,” said a <a href="https://www.documentcloud.org/documents/24688789-2023-04-24-exxonmobil-ftc-comments#document/p5/a2562175">comment from ExxonMobil</a>.</p>
        
    
                    

<figure data-pp-id="53" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="300" height="200" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/item-4.png?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=80&amp;w=800&amp;s=4295b6950dd8b4d6cbca0138e6620862" srcset="https://img.assets-d.propublica.org/v5/images/item-4.png?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=267&amp;q=80&amp;w=400&amp;s=0967e518b1a2cf3cc2f1ad07c56bf7ff 400w, https://img.assets-d.propublica.org/v5/images/item-4.png?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=80&amp;w=800&amp;s=4295b6950dd8b4d6cbca0138e6620862 800w, https://img.assets-d.propublica.org/v5/images/item-4.png?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=800&amp;q=80&amp;w=1200&amp;s=d06685d5ac9a42c7b8f8e122aaf2dc61 1200w, https://img.assets-d.propublica.org/v5/images/item-4.png?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=867&amp;q=80&amp;w=1300&amp;s=d6a7708b4b0eabe988f74f0041f3f0f2 1300w, https://img.assets-d.propublica.org/v5/images/item-4.png?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=967&amp;q=80&amp;w=1450&amp;s=40944522eff6cdd5c523b4d3ccb5d794 1450w, https://img.assets-d.propublica.org/v5/images/item-4.png?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1067&amp;q=80&amp;w=1600&amp;s=ca693587302aff1f58ee145d106798c1 1600w, https://img.assets-d.propublica.org/v5/images/item-4.png?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1333&amp;q=80&amp;w=2000&amp;s=f381d567cd668920d90865bf81773611 2000w">

            
    
<figcaption>
    
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="54.0"><span>If you think</span> navigating the ins and outs of pyrolysis is hard, try getting your hands on actual plastic made through it.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="55.0">It’s not as easy as going to the grocery store. Those water bottles you might see with 100% recycled claims are almost certainly made through traditional recycling. The biggest giveaway is that the labels don’t contain the asterisks or fine print typical of products made through pyrolysis, like “mass balance,” “circular” or “certified.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="56.0">When I asked about the fruit cup, ExxonMobil directed me to its partners. Printpack didn’t respond to my inquiries. Pacific Coast Producers told me it was “engaged in a small pilot pack of plastic bowls that contain post-consumer content with materials certified” by third parties, and that it “has made no label claims regarding these cups and is evaluating their use.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="57.0">I pressed the American Chemistry Council for other examples.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="58.0">“Chemical recycling is a proven technology that is already manufacturing products, conserving natural resources, and offering the potential to dramatically improve recycling rates,” said Matthew Kastner, a media relations director. His colleague added that much of the plastic made via pyrolysis is “being used for food- and medical-grade packaging, oftentimes not branded.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="59.0">They provided links to products including a Chevron Phillips Chemical announcement about bringing recycled plastic food wrapping <a href="https://www.waste360.com/waste-producers/chevron-phillips-chemical-and-charter-next-generation-to-bring-circular-polyethylene-to-retail-stores">to retail stores</a>.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="60.0">“For competitive reasons,” a Chevron spokesperson declined to discuss brand names, the product’s availability or the amount produced.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="61.0">In another case, a grocery store chain sold <a href="https://www.exxonmobilchemical.com/en/resources/library/library-detail/107131/circularity_demo_press_release_en">chicken wrapped in plastic</a> made by ExxonMobil’s pyrolysis process. The producers told me they were part of a small project that’s now discontinued.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="62.0">In the end, I ran down half a dozen claims about products that came out of pyrolysis; each either existed in limited quantities or had its recycled-ness obscured with mass balance caveats. </p>

<p data-pp-blocktype="copy" data-pp-id="62.1">Then this April, nearly eight months after I’d begun my pursuit, I could barely contain myself when I got my hands on an actual product.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="63.0">I was at a United Nations treaty negotiation in Ottawa, Ontario, and an industry group had set up a nearby showcase. On display was a case of Heinz baked beans, packaged in “39% recycled plastic*.” (The asterisk took me down an online rabbit hole about certification and circularity. Heinz didn’t respond to my questions.)</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="64.0">This, too, was part of an old trial. The beans were expired.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="65.0">Pyrolysis is a “fairy tale,” I heard from Neil Tangri, the science and policy director at the environmental justice network Global Alliance for Incinerator Alternatives. He said he’s been hearing pyrolysis claims since the ’90s but has yet to see proof it works as promised.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="66.0">“If anyone has cracked the code for a large-scale, efficient and profitable way to turn plastic into plastic,” he said, “every reporter in the world” would get a tour.</p>

<p data-pp-blocktype="copy" data-pp-id="66.1">If I did get a tour, I wondered, would I even see all of that stubborn, dirty plastic they were supposedly recycling?</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="67.0">The industry’s marketing implied we could soon toss sandwich bags and string cheese wrappers into curbside recycling bins, where they would be diverted to pyrolysis plants. But I grew skeptical as I watched a webinar for ExxonMobil’s pyrolysis-based technology, the kind used to make the fruit cup. The company showed photos of plastic packaging and oil field equipment as examples of its starting material but then mentioned something that made me sit up straight: It was using pre-consumer plastic to “give consistency” to the waste stream.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="68.0">Chemical plants need consistency, so it’s easier to use plastic that hasn’t been gunked up by consumer use, Jenkins explained.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="69.0">But plastic waste that had never been touched by consumers, such as industrial scrap found at the edges of factory molds, could easily be recycled the old-fashioned way. Didn’t that negate the need for this more polluting, less efficient process?</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="70.0">I asked ExxonMobil how much post-consumer plastic it was actually using. Catie Tuley, a media relations adviser, said it depends on what’s available. “At the end of the day, advanced recycling allows us to divert plastic waste from landfills and give new life to plastic waste.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="71.0">I posed the same question to several other operators. A company in Europe told me it uses “mixed post-consumer, flexible plastic waste” and does not recycle pre-consumer waste.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="72.0">But this spring at an environmental journalism conference, an American Chemistry Council executive confirmed the industry’s preference for clean plastic as he talked about an Atlanta-based company and its pyrolysis process. My colleague Sharon Lerner asked whether it was sourcing curbside-recycled plastic for pyrolysis.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="73.0">If Nexus Circular had a “magic wand,” it would, he acknowledged, but right now that kind of waste “isn’t good enough.” He added, “It’s got tomatoes in it.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="74.0">(Nexus later confirmed that most of the plastic it used was pre-consumer and about a third was post-consumer, including motor oil containers sourced from car repair shops and bags dropped off at special recycling centers.)</p>
        
    
                                  
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="76.0">Clean, well-sorted plastic is a valuable commodity. If the chemical recycling industry grows, experts told me, those companies could end up competing with the far more efficient traditional recycling.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="77.0">To spur that growth, the American Chemistry Council is lobbying for mandates that would require more recycled plastic in packaging; it wants to make sure that chemically recycled plastic counts. “This would create market-driven demand signals,” Kastner told me, and ease the way for large-scale investment in new chemical recycling plants.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="78.0">I asked Jenkins, the energy industry analyst, to play out this scenario on a larger scale. </p>

<p data-pp-blocktype="copy" data-pp-id="78.1">Were all of these projects adding up? Could the industry conceivably make enough propylene and ethylene through pyrolysis to replace much of our demand for new plastic? </p>

<p data-pp-blocktype="copy" data-pp-id="78.2">He looked three years into the future, using his company’s latest figures on global pyrolysis investment, and gave an optimistic assessment. </p>

<p data-pp-blocktype="copy" data-pp-id="78.3">At best, the world could replace 0.2% of new plastic churned out in a year with products made through pyrolysis.</p>
        
    
                    <div data-pp-location="bottom-note">
                                    

                                    
        <div data-pp-location="bottom-note">
                        <h2><strong>About the Math</strong></h2>
<p>Our article is focused on pyrolysis because it’s the most popular form of chemical recycling. Other types of chemical recycling technologies have their own strengths and weaknesses.</p>
<p>There are different variations of pyrolysis, and steam crackers produce a range of ethylene and propylene yields. Companies are secretive about their operations. To estimate the efficiencies of pyrolysis and mass balance, I read dozens of peer-reviewed studies, reports, industry presentations, advertisements and news stories. I also fact checked with a dozen experts who have different opinions on pyrolysis, mass balance and recycling. Some of them, including Jenkins and Anthony Schiavo, senior director at Lux Research, provided estimates of overall yields for companies trying to make plastic. All of that information coalesced around a 15% to 20% yield for conventional pyrolysis processes and 25% to 30% for more advanced technologies. We are showcasing the conventional process because it’s the most common scenario.</p>
<p>We took steps to simplify the math and jargon. For instance, we skipped over the fact that a small amount of the naphtha fed into the steam cracker is consumed as fuel. And we called the fraction of pyrolysis oil that’s suitable for a steam cracker “pyrolysis naphtha”; it is technically a naphtha-like product.</p>
<p>These processes may improve over time as new technologies are developed. But there are hard limits and tradeoffs associated with the nature of steam cracking, the contamination in the feedstock, the type of feedstock used and financial and energy costs.</p>

        </div>

    


                                    
        <div data-pp-location="bottom-note">
                        <p>Graphics and development by <a href="https://www.propublica.org/people/lucas-waldron">Lucas Waldron</a>. Design and development by <a href="https://www.propublica.org/people/Anna-Donlan">Anna Donlan</a>. <a href="https://www.propublica.org/people/Mollie-Simon">Mollie Simon</a> and <a href="https://www.propublica.org/people/Gabriel-Sandoval">Gabriel Sandoval</a> contributed research.</p>

        </div>

    
            </div><!-- end .article-body__bottom-notes -->
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tetris as Font (440 pts)]]></title>
            <link>https://erikdemaine.org/fonts/tetris/</link>
            <guid>40737294</guid>
            <pubDate>Thu, 20 Jun 2024 11:11:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://erikdemaine.org/fonts/tetris/">https://erikdemaine.org/fonts/tetris/</a>, See on <a href="https://news.ycombinator.com/item?id=40737294">Hacker News</a></p>
<div id="readability-page-1" class="page"><table id="data"><tbody><tr><td><label for="text">Enter text to render: &nbsp;</label><p><label for="rot">Obscure in URL</label></p></td><td><label for="anim">Animate</label><p><label for="rotate">Include rotations</label><br><label for="speed">Speed:</label>&nbsp;</p><label for="puzzle">Puzzle font</label><br><label for="black">Black pieces (dissection puzzle)</label></td><td><label for="grid">Grid</label><br><label for="center">Rotation center</label><br><label for="floor">Bottom floor</label><br></td></tr></tbody></table><p>GIF width:<a id="download"></a></p><hr><p><b><a href="https://en.wikipedia.org/wiki/Tetris">Tetris</a></b> is
<a href="https://en.wikipedia.org/wiki/List_of_best-selling_video_games">among the best-selling</a> (and perhaps best-known) video games ever.
We grew up playing the
<a href="https://en.wikipedia.org/wiki/Game_Boy">Game Boy</a> and
<a href="https://en.wikipedia.org/wiki/Spectrum_HoloByte">Spectrum HoloByte</a> PC editions.
Erik is even a <a href="http://erikdemaine.org/images/tetris_award_large.jpg">Tetris Master</a>.
Nowadays you can play
<a href="https://tetris.com/play-tetris/">in your browser</a>
or <a href="https://en.wikipedia.org/wiki/Tetris_99">on a Switch</a>
or <a href="https://en.wikipedia.org/wiki/Tetris_Effect">on PS4/PC/VR</a>.
</p><p><b>Font design.</b>  Each letter and digit in this typeface is made up of
exactly one of each of the Tetris pieces:
<span id="pieceI"></span> (I),
<span id="pieceJ"></span> (J),
<span id="pieceL"></span> (L),
<span id="pieceO"></span> (O),
<span id="pieceS"></span> (S),
<span id="pieceT"></span> (T), and
<span id="pieceZ"></span> (Z).
Furthermore, the letter is designed so that it can actually be constructed
by stacking these pieces one at a time and be supported by previous pieces,
as in Tetris.
These designs were found by hand, aided by the
<a href="http://burrtools.sourceforge.net/">BurrTools</a> software
which enabled searching for whether the Tetris pieces could fit inside
a candidate outline for a letter.
The piece colors roughly follow
<a href="https://en.wikipedia.org/wiki/Tetris#Game_pieces">The Tetris Company's standard colors</a>,
or you can switch to black pieces.
The initial rotations follow the standard
<a href="https://tetris.fandom.com/wiki/SRS">Super Rotation System</a>.
</p><p><b>Puzzles.</b>
In the <b>puzzle font</b>, the letters are at the correct rotations and
horizontal positions, and their vertical position represents their drop
sequence.  Drop the pieces in your head (or via <b>animate</b>) to figure
out what letter is encoded.
•
Even without puzzle font turned on, in the <b>animated</b> font, you can
try to guess what the letter is before all the pieces have arrived.
•
One final set of puzzles: In the unanimated unpuzzle <b>black-pieces</b> font,
try to figure out how one of each Tetris piece perfectly packs that shape.
(This is the task that BurrTools is very good at.)
</p><p><b>Related mathematics.</b>
(Perfect-information)
<a href="http://erikdemaine.org/papers/Tetris_IJCGA/">Tetris is NP-complete</a>,
meaning that it's computationally intractable to figure out whether you can
survive, or clear the board, given an initial board configuration and a
sequence of <i>n</i> pieces to come.
<a href="http://erikdemaine.org/papers/TotalTetris_JIP/">Similar results
hold</a> for <i>k</i>-tris played with
<a href="https://en.wikipedia.org/wiki/Polyomino"><i>k</i>-ominoes</a>
instead of tetrominoes.  Most recently, we
<a href="http://erikdemaine.org/papers/ThinTetris_JIP/">analyzed the
complexity of Tetris with few rows or columns</a>; this font appears
in that paper.
</p><p><b>Acknowledgments.</b>  This font was inspired by a collaboration with
Alex Streif and Kate Jones of
<a href="http://www.gamepuzzles.com/">Kadon Enterprises</a>
during <a href="https://bridgesmathart.org/bridges-2017/">BRIDGES 2017</a>,
where we started designing a font using just 5 pieces:
the “<a href="https://en.wikipedia.org/wiki/Tetromino">free
tetrominoes</a>” where S is the same piece as Z and J is the same
piece as L.
Relatedly, Kate Jones designed <a href="https://erikdemaine.org/fonts/tetris/kadon_fonts.jpg">other polyomino
fonts included in some Kadon manuals</a>.
By contrast, this typeface aims closer to the rules of Tetris,
where reflection matters and the pieces must stack and be supported.
</p><p>Check out <a href="http://erikdemaine.org/fonts/">other mathematical and
puzzle fonts</a>. • Feedback or not working?
<a href="mailto:edemaine+fonts@mit.edu">Email Erik</a>. •
<a href="https://github.com/edemaine/font-tetris">Source code on GitHub</a>.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EU Council has withdrawn the vote on Chat Control (794 pts)]]></title>
            <link>https://stackdiary.com/eu-council-has-withdrawn-the-vote-on-chat-control/</link>
            <guid>40736771</guid>
            <pubDate>Thu, 20 Jun 2024 09:45:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stackdiary.com/eu-council-has-withdrawn-the-vote-on-chat-control/">https://stackdiary.com/eu-council-has-withdrawn-the-vote-on-chat-control/</a>, See on <a href="https://news.ycombinator.com/item?id=40736771">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				
<p>The EU Council and its participants have decided to withdraw the vote on the contentious Chat Control plan proposed by Belgium, the current EU President.</p>



<p>According to <a href="https://netzpolitik.org/2024/etappensieg-belgien-scheitert-mit-abstimmung-zur-chatkontrolle/" target="_blank" rel="noreferrer noopener">Netzpolitik</a> (<em>German</em>), “The EU Council did not make a decision on chat control today, as the agenda item was removed due to the lack of a majority, confirmed by Council and member state spokespersons”.</p>



<p>Belgium’s draft law, which was supposed to be adopted as the Council’s negotiating position, was instead postponed indefinitely. Although the Committee of Permanent Representatives meets weekly, Belgium cannot currently present a proposal that would gain a majority. In July, the Council Presidency will transfer from Belgium to Hungary, which has stated its intention to advance negotiations on chat control as part of its work program.</p>



<p>At the start of 2022, the European Commission proposed <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=COM%3A2022%3A209%3AFIN&amp;qid=1652451192472" target="_blank" rel="noreferrer noopener">monitoring all chat messages</a> and other forms of digital communication among citizens. This initiative includes client-side scanning for end-to-end encrypted services, meaning all messages would be checked irrespective of suspicion.</p>



<p>The plan targets the detection of both known and unknown abusive material and grooming activities. Experts have cautioned that such measures are prone to generating numerous false positives, particularly when identifying unknown content, leading to innocent citizens being misidentified as senders of abusive material.</p>



<p>European legislation is formed through a trialogue process involving negotiations between the European Commission, the European Parliament, and the Council of Ministers. Initially, the European Parliament rejected the European Commission’s proposal and <a href="https://oeil.secure.europarl.europa.eu/oeil/popups/ficheprocedure.do?reference=2022/0155(COD)&amp;l=en" target="_blank" rel="noreferrer noopener">introduced its own</a>, which, while still critical, excluded end-to-end encrypted services. However, Belgium’s new proposal <a href="https://www.euronews.com/next/2024/05/21/diplomats-mull-online-child-abuse-with-no-deal-in-sight" target="_blank" rel="noreferrer noopener">reintroduced client-side scanning</a> for these services, stipulating that users must consent to chat controls; otherwise, they would lose the ability to send photos, videos, and URLs. </p>



<p>This method, termed “upload moderation” by Belgium, has been criticized by opponents as merely a rebranding of the original concept.</p>



<h2>Signal and other apps threaten to leave the EU if the proposal is enacted as law</h2>



<p>Meredith Whittaker, president of the chat app Signal, <a href="https://x.com/mer__edith/status/1796508893822238881" target="_blank" rel="noreferrer noopener">has been vocal</a> against these plans. She argues that implementing such measures within end-to-end encrypted communications fundamentally undermines encryption and introduces significant vulnerabilities in the digital infrastructure. </p>



<p>Whittaker emphasizes that these vulnerabilities have far-reaching global implications, not just within Europe. She has repeatedly highlighted the issue, stating, “There is no way to implement such proposals without fundamentally undermining encryption and introducing dangerous vulnerabilities.”</p>



<p>On June 17, Whittaker published <a href="https://signal.org/blog/pdfs/upload-moderation.pdf" target="_blank" rel="noreferrer noopener">an official position</a> condemning the EU’s proposed “upload moderation” as a rebranding of client-side scanning that fundamentally undermines end-to-end encryption. </p>



<p>She emphasized that despite attempts to mask the dangers through marketing, these measures expose encrypted communications to mass surveillance, creating vulnerabilities exploitable by hackers and hostile nations. Whittaker urged a cessation of such rhetorical games, reiterating that any form of mandated mass scanning compromises encryption, thereby threatening global security and privacy at a critically unstable geopolitical moment.</p>



<p>The privacy messenger Threema published <a href="https://threema.ch/en/blog/posts/stop-chat-control" target="_blank" rel="noreferrer noopener">a blog post</a> saying the EU’s proposed Chat Control bill represents a dangerous mass surveillance initiative that would undermine data security, violate privacy rights, and negatively impact professionals and minors.</p>



<p>Patrick Breyer, <a href="https://stackdiary.com/patrick-breyer-and-pirate-party-lose-eu-parliament-seats/">the outgoing MEP</a> from the Pirate Party, raised concerns, noting that proponents of chat control have leveraged the period following the European elections, when attention is lower and the European Parliament is in transition, to advance their agenda. Breyer has called on European citizens <a href="https://www.patrick-breyer.de/en/council-to-greenlight-chat-control-take-action-now/" target="_blank" rel="noreferrer noopener">to take action</a> and urge their politicians to oppose the measures.</p>



<p>Edward Snowden, the NSA whistleblower, <a href="https://x.com/Snowden/status/1803127597158760735" target="_blank" rel="noreferrer noopener">criticized the proposal</a>, stating, “EU apparatchiks are trying to legislate a terrible mass surveillance measure, despite universal public opposition (no sane person wants this), by inventing a new word for it – upload moderation – and hoping no one finds out what it is until it’s too late.”</p>



<h2>What happens next?</h2>



<p>With the EU Council withdrawing the vote on the Chat Control proposal today, the legislative process faces new uncertainty. The proposal will return to the drawing board, as the European Commission[<a href="#footnote">1</a>] and the European Parliament continue to deliberate on the best way forward.</p>



<p>The discussions will resume after the summer, once the new Parliament is seated and Hungary assumes the Council presidency from Belgium in July. Hungary has already committed to developing a comprehensive legislative framework to prevent and combat online child sexual abuse and revising the directive against the sexual exploitation of children.</p>



<p>The forthcoming negotiations are anticipated to be highly contentious, especially since the European Parliament has firmly opposed any measures that would circumvent end-to-end encryption. The Member States and the Parliament have until April 2026 to agree. This deadline is crucial, as <a href="https://www.europarl.europa.eu/news/en/press-room/20240408IPR20311/child-sexual-abuse-online-current-rules-extended-until-april-2026" target="_blank" rel="noreferrer noopener">an existing exemption</a> allowing social networks to self-moderate content will expire, potentially eliminating current safeguards against sharing sensitive images.</p>



<p>In the meantime, privacy advocates and digital rights organizations will likely continue to voice their concerns, urging EU citizens to remain vigilant and engaged in the debate over digital privacy and surveillance. The next steps will involve intense negotiations and potential revisions to address the complex issues at stake.</p>



<p id="footnote"><em>[footnote #1]: On June 20, at the European Data Protection Supervisor (EDPS) 20th anniversary summit, EU Commissioner for Justice Vera Jourová&nbsp;<a href="https://x.com/ellajakubowska1/status/1803709746429538685" target="_blank" rel="noreferrer noopener">stated</a>&nbsp;that the European Commission’s proposal for the Child Sexual Abuse Regulation (CSAR) would break encryption. This marks the first time the European Commission has publicly acknowledged that the CSAR proposal would compromise encryption, a significant departure from the stance maintained over the past three years by Home Affairs Commissioner Ylva Johansson, who consistently claimed that the proposal would not affect encryption.</em></p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bun is much faster than Node.js 22 at decoding Base64 but both rely on same lib (109 pts)]]></title>
            <link>https://twitter.com/lemire/status/1803598132334436415</link>
            <guid>40736685</guid>
            <pubDate>Thu, 20 Jun 2024 09:31:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/lemire/status/1803598132334436415">https://twitter.com/lemire/status/1803598132334436415</a>, See on <a href="https://news.ycombinator.com/item?id=40736685">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Free software hijacked Philip Hazel's life (226 pts)]]></title>
            <link>https://lwn.net/SubscriberLink/978463/608c876c1153fd31/</link>
            <guid>40736577</guid>
            <pubDate>Thu, 20 Jun 2024 09:14:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/SubscriberLink/978463/608c876c1153fd31/">https://lwn.net/SubscriberLink/978463/608c876c1153fd31/</a>, See on <a href="https://news.ycombinator.com/item?id=40736577">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>Philip Hazel was 51 when he began the <a href="https://www.exim.org/">Exim</a> message transfer agent (MTA)
project in 1995, which
led to the <a href="https://github.com/PCRE2Project/pcre2">Perl-Compatible Regular
Expressions</a> (PCRE) project in 1998. At 80,
he's maintained PCRE, and its successor PCRE2, for more than 27
years. For those doing the math, that's a year longer than LWN has
been in publication. Exim maintenance was handed off around the time
of his retirement in 2007. Now, he is ready to hand off PCRE2 as well,
if a successor can be found.</p>

<h4>Punch cards to flat screens</h4>

<p>Hazel's tenure as a free-software developer is exceptional, if not
record-breaking in its length. Linus Torvalds began
working on Linux in 1991 as a college student and is still leading its development 33 years
later with no signs of slowing. However, as Hazel <a href="https://drive.google.com/file/d/10TAOEgIL--CmzqOl0fkdP-cagjSLBM4J/view?usp=sharing">wrote</a>
in his technical memoir <em>From Punched Cards To Flat Screens</em>,
he began contributing to free software "<q>nearer the end than the
start</q>" of his career.</p>

<blockquote>
	<b>Like what you are reading?</b>
    		<a href="https://lwn.net/Promo/slink-trial-terse/claim">Try LWN for free</a> for 1 month,
    		no credit card required.
</blockquote>


<p>At the start of his career, Hazel was introduced to computers as an
undergraduate when the University of Cape Town (UCT) received its
first computer: an <a href="https://en.wikipedia.org/wiki/ICT_1301">International Computers
and Tabulators (ICT) 1301</a>, with a dazzling capacity to read as
many as 600 punched cards per minute. He attended introductory
programming lectures, read the Manchester <a href="https://en.wikipedia.org/wiki/Autocode">Autocode</a> programming
manual from cover to cover, and began writing test programs encoded onto paper punch cards:</p>

<blockquote>
If you were lucky and your program worked, the output appeared on the
printer; more often that not (at least at the start) all you got was
an error message, which meant that you had to fix your program, return
to the back of the queue, and hope to get another run before the end
of the session. The more attempts it took to get a program to work,
the more useful scrap paper one accumulated.
</blockquote>

<p>He later moved on to the University of Cambridge as a PhD
student. Cambridge was blessed with four computers when Hazel arrived
in 1967: a <a href="https://en.wikipedia.org/wiki/Titan_(1963_computer)">Titan</a>,
<a href="https://en.wikipedia.org/wiki/IBM_1130">IBM 1130</a>, <a href="https://en.wikipedia.org/wiki/IBM_System/360">IBM 360</a>, and
<a href="https://en.wikipedia.org/wiki/PDP-7">PDP-7</a>. Because Hazel
knew the Manchester Autocode language, which is similar to Titan Autocode, he
was given an account on the Titan "<q>and that was the start of the
slippery slope down which I have been sliding ever since</q>".</p>

<p>That slope, as well-detailed in Hazel's memoir (and mercilessly
abbreviated here), ultimately led to his joining the Cambridge
Computing Service as a software developer in 1971. During his tenure
at Cambridge, and at home, Hazel had the opportunity to work with a
number of interesting systems including the PDP-11, Ultrix,
Sinclair&nbsp;ZX-81, and BBC&nbsp;Micro. In 1990 the Computing Service
decided to set up a cluster of Unix systems for the use of staff and
graduate students, and he began working with Sun boxes
running SunOS. This coincided with the transition from the <a href="https://en.wikipedia.org/wiki/X.25">X.25</a> wide-area network
standard to the Internet Protocol (IP).</p>

<p>Hazel was responsible for the email service, and found it difficult
to configure <a href="https://www.proofpoint.com/us/products/email-protection/open-source-email-solution">Sendmail</a>
to choose between IP and X.25 to
deliver email. A colleague suggested that he try the free-software <a href="https://en.wikipedia.org/wiki/Smail">Smail</a> MTA instead. Hazel
took him up on the suggestion and added two features to Smail to
manage the DNS lookups and decide whether to send mail via SMTP or
X.25. While managing Smail for Cambridge, he submitted additional
features to cope with messages that had bad senders:</p>

<blockquote>
If such a message could not be delivered, the bad sender meant that an error report could
also not be delivered, and the postmaster (me) had to intervene to sort things
out. By rejecting messages whose sender address could not be verified, I pushed
the onus of dealing with this problem out to the sending MTA. This kind of
checking is now standard MTA practice (along with many other checks for
spam and viruses).
</blockquote>

<p>This was a small start that led Hazel to thinking about more
checking and verification that would be needed for MTAs. He
considered, but decided against, trying to modify Smail. It was
written in pre-standard C and carried "<q>a lot of <a href="https://en.wikipedia.org/wiki/UUCP">UUCP</a> baggage</q>". Hazel
wanted to write an MTA for modern (at the time) operating systems with
standard C compilers and runtimes, and permanent connections to a
TCP/IP network. He began working on the Experimental Internet Mailer
(Exim) in March 1995. By November it was "<q>just about able to send
and receive emails</q>".</p>

<h4>Exim</h4>

<p>Hazel had informed a colleague, Piete Brooks, that he was working
on Exim. Brooks wanted to try it out, but Hazel demurred because he
had not written any documentation. (This may be the first known case
of a programmer refusing to distribute undocumented code...) Brooks
response was "<q>I don't want the documentation, I want the
code.</q>" Even so, Hazel insisted on writing a first cut of the Exim
manual before packaging up the code and sending it off to meet its fate.</p>

<p>Brooks put the code into service right away, and started
telling others about it. Hazel put Exim releases on a public FTP site,
and gave a talk on Exim at a "Campus Mail Day" in Aberdeen, which led
to more usage. A request from Richard Stallman persuaded him to switch
from a homegrown license for Exim to the GPL. It began being ported to
other operating systems, and eventually found its way to Linux in 1997
or 1998, "<q>which by then had become an operating system that could be
used in production environments</q>". Over time, Exim would become
(and still is) a popular MTA used all over the planet. Its adoption
far exceeded Hazel's expectations:</p>

<blockquote>
<p>I never expected commercial sites to get involved, nor for it to
become the default MTA in any operating systems. In short, I did not
foresee that it would grow into the fully-fledged open source
development project that it has.</p>
</blockquote>

<h4>PCRE</h4>

<p>Regular expressions play a rather large part in managing mail, and
Hazel wanted to have more flexible regular expressions for Exim than were
present in Smail. He chose a regular-expression library
written by Henry Spencer for early versions of Exim, but found it
limiting compared to Perl's extended pattern-matching features. He
thought about lifting Perl's implementation, but it was too integrated
to be easily adapted to Exim. "<q>In the end, I solved the problem the
way programmers generally solve their problems: by writing something
myself.</q>" That, of course, became PCRE.</p>

<p>Hazel bundled PCRE with Exim and also released it as a standalone
library. Like Exim, it filled a need that he did not even realize
existed. PCRE was adopted by Apache HTTPD, and Hazel was
"<q>particularly gratified</q>" to find that it had been included with
the <a href="https://www.postfix.org/">Postfix</a> MTA. After a while, PCRE looked beyond Perl
to include features such as recursion inside a regular expression, and
named subpatterns (borrowed from Python), as well as other ideas taken
from other regular-expression implementations.</p>

<p>In 2014, Hazel decided that PCRE's API was "<q>not capable of much
further extension</q>" and began working on a new, incompatible,
version of the code. The first PCRE2 release (starting at version
10.00 to avoid confusion with then-current PCRE 8.x releases) was <a href="https://lists.exim.org/lurker/message/20150105.162835.0666407a.en.html">announced</a>
in January 2015. Hazel continued supporting PCRE until its <a href="https://lists.exim.org/lurker/message/20210615.162400.c16ff8a3.en.html">final release</a> in 2021. PCRE2 <a href="https://groups.google.com/g/pcre2-dev/c/OUI1FYTcm_E">moved to GitHub</a> in 2022. The most recent release of PCRE2, <a href="https://github.com/PCRE2Project/pcre2/releases/tag/pcre2-10.44">10.44</a>,
came out on June 7, 2024.</p>

<p>Hazel wrote that PCRE may now be even
more important than Exim was "<q>because of its widespread use in many
different applications and operating systems</q>". Indeed, just
looking to see the installed software that depends on the PCRE2
library on Fedora&nbsp;40 turns up use by Git, Grep, MariaDB, nmap,
SELinux's command-line tools, Sway, Wget&nbsp;2, and quite a few others.</p>

<p>In his memoir, Hazel had written that it felt right to pass
maintenance of Exim on to others after so long. "<q>More than a decade on one
project is long enough.</q>" In his 2017 update to the memoir, he
noted that the sentence had come back to bite him: He was still
working on PCRE 19 years after he started to write it. Little did he
suspect he would still be maintaining it in 2024. He would like to <a href="https://github.com/PCRE2Project/pcre2/issues/426">hand it
off</a> while he can help with the transition.</p>

<h4>Passing the torch</h4>

<p>I emailed Hazel with a few questions about his thoughts on
long-term free-software maintenance, changes to the industry, and what
he might do once PCRE was handed off. Hazel wrote that he does not
have any post-PCRE plans. All of <a href="http://quercite.dx.am/">his projects</a>, with the exception of the <a href="https://github.com/PhilipHazel/b2pf">base to presentation
form</a> (B2PF) library for converting Unicode characters for
printing, were started while he was still working. "Since I retired I
haven't felt a lack of anything enough to spur me into writing something
new". He added that he would continue to maintain his non-PCRE2
projects "if necessary" and would help with PCRE2 if needed.</p>

<p>When asked how new generations of tools, and developers, had
changed his work habits, Hazel replied that he has changed
little. "I am sufficiently old that I am well stuck in my ways, and
haven't changed how I do things since the 90s" with the exception
of moving development to GitHub. He admitted that he was not good at
anticipating or keeping up with trends in development. For example, he
said he was not aware of Unicode when he began PCRE development, so it
was written to support ASCII and with a limit of 64K for compiled
patterns. Others had to persuade him to extend PCRE with Unicode
support, and the option for larger patterns. Contributors,
he said, have not changed. The world has, though:</p>

<blockquote>
When I was born, there were no digital computers, though their
predecessors, for example the Colossus at Bletchley Park, were around. The
world has gone from nothing to where we are now in just one lifetime.
Incredible! What particularly amazes me more than the CPU power is the
amount of storage that I carry in my pocket.
</blockquote>

<p>Hazel did offer some advice for those starting new software
projects, though he noted he was not the first to make this point:</p>

<blockquote>
It's worth remembering that the effort needed to maintain a piece of
successful software over its lifetime far outweighs the effort of
writing it in the first place. In the case of PCRE there have been
several major re-designs and re-writes of the internals as
well as continual extensions to the user-visible features.
</blockquote>

<p>He also suggested that developers think about how software would be
tested as it is designed: "Effort put into building test harnesses is
never wasted."</p>

<p>I asked Hazel, given the recent <a href="https://lwn.net/Articles/967866/">XZ backdoor</a> attempt, how
he intended to vet any prospective PCRE2 maintainers. He replied that it
was a good question "to which I have no answer. I will have to see
who (if anyone) makes an offer". To date, he said he had received
"no communications whatsoever" about taking over the
project. Perhaps, once the word gets out more widely, a qualified
maintainer will step forward to take PCRE2 into the future.</p><br clear="all">
               <br clear="all">
               <hr><p>
           (<a href="https://lwn.net/Login/?target=/Articles/978463/">Log in</a> to post comments)
           </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lessons Learned from Scaling to Multi-Terabyte Datasets (115 pts)]]></title>
            <link>https://v2thegreat.com/2024/06/19/lessons-learned-from-scaling-to-multi-terabyte-datasets/</link>
            <guid>40735859</guid>
            <pubDate>Thu, 20 Jun 2024 07:20:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://v2thegreat.com/2024/06/19/lessons-learned-from-scaling-to-multi-terabyte-datasets/">https://v2thegreat.com/2024/06/19/lessons-learned-from-scaling-to-multi-terabyte-datasets/</a>, See on <a href="https://news.ycombinator.com/item?id=40735859">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p>This post is meant to guide you through some of the lessons I’ve learned while working with multi-terabyte datasets. The lessons shared are focused on what someone may face as the size of their dataset scales up and some of the things I’ve done to overcome them. I hope you’re waiting for something to finish running while reading this!</p>



<p>Remember, this is not a rigid guide. It’s about introducing concepts and explaining why you should start applying them. Numerous other tools can surpass the ones I’ve used, and I strongly encourage you to take the initiative and explore them independently. Your active exploration is key to your professional growth.</p>



<p>I’ve divided this post into two sections: scaling on single machines and multi-machine scaling. The goal is to maximize your available resources and reach your goals as quickly as possible.</p>



<p>Lastly, I want to emphasize that no optimization or scaling can compensate for a flawed algorithm. Before scaling up, it’s crucial to evaluate your algorithm. This should always be your first step, providing a confident guide for your work.</p>



<h2>Scaling on a Single Machine</h2>



<h3>Joblib</h3>



<p>Compute is the first bottleneck that comes to mind when scaling. Scaling up computations can be done in several different practical ways. If you’re a data scientist or a machine learning engineer, you might already be familiar with Joblib, a library used to run code in parallel (among other things). It is often used in other libraries, such as scikit-learn or XGBoost.</p>



<p>The process of parallelizing something using Joblib is simple, as follows (modified for clarity from the Joblib docs):</p>


<div><pre title="">&gt;&gt;&gt; from joblib import Parallel, delayed
&gt;&gt;&gt; from math import sqrt

&gt;&gt;&gt; parallel_mapper = Parallel(n_jobs=-1)
&gt;&gt;&gt; delayed_func = delayed(sqrt)
&gt;&gt;&gt; jobs = [
    delayed_func(x**2)
    for x in range(10)
]
&gt;&gt;&gt; parallel_mapper(jobs)
[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
</pre></div>


<p>Joblib is a great way to scale up your parallel workloads. It’s used in scikit-learn and other tools, proving reliable for many workloads. This isn’t even considering its other excellent features regarding memoization or Fast Compressed Persistence. Joblib is helpful for just making a function parallelizable across all your CPU cores.</p>



<h3>GNU Parallel</h3>



<p>GNU Parallel is a powerful tool for preprocessing or extracting data in the CLI. It differs from Joblib as it can be used outside a script and is versatile. You can even run other Python scripts in parallel. One of the most common use cases is decompressing many files simultaneously. Here’s how I would do it:</p>


<div><pre title="">&gt; ls
random_0.zip&amp;nbsp; random_2.zip&amp;nbsp; random_4.zip&amp;nbsp; random_6.zip&amp;nbsp; random_8.zip
random_1.zip&amp;nbsp; random_3.zip&amp;nbsp; random_5.zip&amp;nbsp; random_7.zip&amp;nbsp; random_9.zip
...

&gt; mkdir output
&gt; ls | parallel --eta --bar "unzip -q {} -d output/"
100% 10:0=0s random_9.zip

&gt; ls output/
random_0.txt&amp;nbsp; random_2.txt&amp;nbsp; random_4.txt&amp;nbsp; random_6.txt&amp;nbsp; random_8.txt
random_1.txt&amp;nbsp; random_3.txt&amp;nbsp; random_5.txt&amp;nbsp; random_7.txt&amp;nbsp; random_9.txt
...
</pre></div>


<p>These commands are pretty straightforward if you have used a Linux terminal before. The main part to focus on is piping the file names to <code>parallel</code> so that <code>unzip</code> can decompress them.</p>



<p>For any task, once you have a bash command set to run on a single file, you can parallelize it by modifying your command slightly. By default, parallel uses all available CPU cores and can execute commands on multiple machines using ssh, meaning that it could be used as an ad-hock computing cluster.</p>



<p>Another use case is downloading a large number of files. With <code>wget</code> and <code>parallel</code> and a list of files to download, writing a quick one-liner to download all the files in parallel is easy. Other tools, such as <code>axel</code> and <code>aria2c</code>, can do this just as well, but I’ve found this to work better when I need to download many smaller files.</p>



<p><strong>A quick note:</strong> While you can use this to download many files, be aware that this can cause strain on servers by creating multiple connections, leading to network congestion and reduced performance for other users or even being seen as a DOS attack. This increased server load can be particularly problematic for smaller websites or servers with limited bandwidth. Famously, aria2c <a href="https://github.com/aria2/aria2/issues/1039">has rejected proposals</a> to increase the maximum number of connections from 16, even though computers have gotten faster, and network bandwidth has increased dramatically. Given their position, I agree with their decisions and ask you to act responsibly when downloading.</p>



<p>Another point I’d like to mention is that while you can get things working quicker with Parallel, it may be challenging to manage <code>bash</code> commands, especially for a beginner where the rest of the team might be more Python/traditional programming language focused. Due to this, I generally recommend keeping Parallel for one-off tasks rather than writing complex ETL pipelines in <code>bash</code>. Maintainable code is second to only no code at all.</p>



<h2>Scaling to Multiple Machines</h2>



<h3>When to Start Using Multiple Machines</h3>



<p>One key identifier for when it makes sense to switch to using multiple machines (think Spark or, my favourite, Dask) is when computing is taking too long for your use cases. This could be experiments, data processing, or whatever. The worst timeframe I’ve estimated is some jobs taking months or a year to finish computing if I were to stick to a single instance, even on AWS’s <a href="https://instances.vantage.sh/aws/ec2/u-24tb1.112xlarge">u-24tb1.112xlarge</a> (a beast of a machine). I’m against the waste of any kind, and the better you can utilize the resources available, the better, in my opinion.</p>



<p>By switching to multiple smaller machines, you leverage several performance benefits over a more prominent instance. Depending on your scaling solution, horizontal scaling allows for almost linear scaling across your CPU, memory, and networking with the number of instances you use.</p>



<p>Most reasonably large EC2 instances offer up to 10 GBit internet speeds, which can help alleviate IO bottlenecks, especially if you’re rapidly streaming data to or from S3. If your workload requires data coming in at 50 Gbit/s, you’ve got the option to either use a m7i.48xlarge instance, which costs <code><em>$9.6768</em></code> hourly and runs at 50 GBit, or four m7i.8xlarge instances, which costs <code>$1.6128</code> hourly per instance or <code>$6.4512 hourly</code> for the same network bandwidth.</p>



<p>I selected networking speeds and cost as the two metrics to focus on here, but if you’re looking to maximize your memory and CPU usage, we can compare the previously mentioned u-24tb1.112xlarge. For the exact cost, you can rent out 135 m7i.8xlarge instances. That gives you 4320 CPUs (10x the instance), 17.28TB of RAM, and 1687.5 Gigabit internet speed (~17x the instance)! While RAM is less, I’ve used a general-purpose instance here to scale, not a memory-optimized one. Using the memory-optimized equivalent, we can get 34.56 TB of RAM, with all the other benefits of using multiple machines (redundancy, finer control for the instance size, etc).</p>



<p>Moreover, with the correct backend, I can scale to as many instances as my use case, orchestration tool, or accounting department will allow. This level of scalability is a crucial advantage, enabling you to meet the demands of your workload without being limited by the capabilities of a single instance.</p>



<p>As with everything, there are benefits to your different approaches. It’s your job to evaluate the pros and cons of each solution and determine what works best for your use case. Minimizing cost while maximizing performance is a good exercise in building intuition for these tasks.</p>



<p>However, given these incredible benefits, I only recommend using multiple instances once you’ve understood the bottlenecks you face. I have seen teams start to scale and over-engineer their approach to computing before understanding their use case. I may have even been a part of those teams before learning my lesson. In some instances, <a href="https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html">well-written cli tools could process data faster than an entire spark cluster</a>.</p>



<h2>Different Computing Models</h2>



<h3>For Embarrassingly Parallel Workloads</h3>



<p>Embarrassingly Parallel Workloads are generally the easiest to scale compared to other types of workloads. We’ve already touched on how to scale up computing using Joblib or Parallel, but what about scaling to multiple machines? There are quite a few tools to scale up computation. I would recommend using AWS Batch or AWS Lambda for embarrassingly parallel workloads that are one-off. Batch is scalable, and with spot pricing, you can get most of your tasks done at a fraction of the cost of using on-demand instances in a fraction of the time it would take to run them in parallel on a single machine. There are other tools you can use (GCP’s Cloud Run, for example), but I can only recommend AWS Batch for longer-running tasks since that’s what I’ve used in the past.</p>



<p>Since setting up the cluster can be time-consuming and is out of the scope of this post, I’ve included a link <a href="https://medium.com/@jackthecloudguy/scale-out-your-genomics-analysis-with-aws-batch-923c41a3c99">here</a> incase you’re interested in exploring this yourself.</p>



<p>One caveat worth mentioning is that the general throughput of your job will be limited by your read and write speeds more so than the compute speed. If you’re reading from/writing to a database, then the database is likely to be a bottleneck (or even crash). S3 is a viable option for reading and writing, given it’s designed to scale better, but it still has its limits. 3,500 writes and 5,500 reads per second per partitioned prefix. S3 is designed to be invisible when scaling to the user, so you may have little control over how it adapts to the increased throughput.</p>



<p>Once the data is in S3 (or whatever service you use), you can transfer it wherever needed.</p>



<p>This setup is quite tedious but scales well for one-off tasks. With a few iterations, you can reduce the setup time to a few minutes, depending on how well you’ve automated the process and your team’s needs. Generally, I’ve found that the setup time is worth it for the computing and engineering time saved, but you can understand my hesitation in using this for every task.</p>



<h3>Analytical Workloads</h3>



<p>Analytical workloads are a bit more challenging to scale. This is because you’re generally working with a single dataset and trying to do a lot of operations on that dataset. You may also have an element of interactivity, such as things running in a Jupyter Notebook. My go-to tool for scaling up analytical workloads is Dask, with an alternative being Spark. Dask and Spark are open-source tools that allow you to scale up your workloads to multiple machines, with their pros and cons. Both these tools can also be used locally, and their implementations of DataFrames (Dask DataFrame and Spark Dataframe) can be used to scale up existing workloads.</p>



<p>Dask is much easier to set up and install. I can get Dask running locally in a few minutes with a single command (<code>pip install "dask[complete]"</code> by the way). On the other hand, Spark requires a bit more setup, and I’ve found that running on my local machine is a bit more challenging. Dask also comes with the benefit that any data scientist using Pandas or Numpy can get used to it quickly while knowing Spark is an entirely different skill set. Dask is also better integrated with several PyData tools, meaning you can take advantage of them immediately. However, given all this, Spark and the Spark ecosystem are much more mature by comparison, and it’s likely that your team already has invested time into getting a Spark cluster up and running. I run into the occasional bug or performance issue with Dask, while Spark is known to be much more stable due to its maturity. Dask is also not suited for longer-running computations.</p>



<p>Given this, my general recommendation is:</p>



<ul>
<li>If you’re a small team or startup with no infrastructure for big data or distributed computing. In that case, I recommend at least experimenting with Dask, regardless of the team’s experience with Spark. In the time you take to get Spark running locally, you could’ve validated your use case with Dask, and your team will be able to leverage other tools in the PyData space.</li>



<li>If you’re already part of a larger organization that uses Spark or some other significant data infrastructure. In that case, it makes sense to stick with it unless you have a compelling reason not to. I recommend watching <a href="https://www.youtube.com/watch?v=obKZzFRNTxo">Eric Dill’s talk on Is Spark Still Relevant?</a> for why larger organizations prefer to use Spark over more modern tools. It is five years old, so some talking points may be outdated. That said, you should still try Dask since <a href="https://docs.dask.org/en/latest/spark.html#reasons-to-choose-both">you can use both</a>.</li>
</ul>



<h2>Conclusion</h2>



<p>In conclusion, managing and scaling multi-terabyte datasets requires a deep understanding of both your data and the tools at your disposal. By leveraging Joblib and GNU Parallel for single-machine scaling, you can maximize the efficiency of your computational resources. When scaling beyond a single machine is necessary, AWS Batch, Dask, and Spark provide robust solutions for various workloads, from embarrassingly parallel tasks to complex analytical operations.</p>



<p>The key takeaway is to start by optimizing your algorithms before scaling, ensuring you’re not merely amplifying inefficiencies. Actively exploring and adapting new tools can significantly enhance your performance and cost-effectiveness. Successful scaling is as much about strategic planning and resource management as raw computational power. Embrace the learning curve; you’ll be well-equipped to handle even the largest datasets confidently and skillfully.</p>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Oldest white wine in the world found in a first-century tomb in Spain (182 pts)]]></title>
            <link>https://doi.org/10.1016/j.jasrep.2024.104636</link>
            <guid>40735743</guid>
            <pubDate>Thu, 20 Jun 2024 06:58:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://doi.org/10.1016/j.jasrep.2024.104636">https://doi.org/10.1016/j.jasrep.2024.104636</a>, See on <a href="https://news.ycombinator.com/item?id=40735743">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Protecting Children's Safety Requires End-to-End Encryption (108 pts)]]></title>
            <link>https://simplex.chat/blog/20240601-protecting-children-safety-requires-e2e-encryption.html</link>
            <guid>40735627</guid>
            <pubDate>Thu, 20 Jun 2024 06:37:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simplex.chat/blog/20240601-protecting-children-safety-requires-e2e-encryption.html">https://simplex.chat/blog/20240601-protecting-children-safety-requires-e2e-encryption.html</a>, See on <a href="https://news.ycombinator.com/item?id=40735627">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article">
<p>As lawmakers grapple with the serious issue of child exploitation online, some proposed solutions would fuel the very problem they aim to solve. Despite expert warnings, the Belgian Presidency persists in pushing for the implementation of client-side scanning on encrypted messaging services, rebranding the effort as "upload moderation". Their <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=COM%3A2022%3A209%3AFIN&amp;qid=1652451192472">latest proposal</a> mandates that providers of private communication services obtain user consent for AI-based scanning of their private chats. If users do not consent, they will be prohibited from sharing images, videos, and URLs.</p>
<p>Privacy critics have long pushed for measures like centralized scanning of private photos and messaging data, arguing it could detect illicit content. However, invasive monitoring of private communications would create detrimental risks that far outweigh any perceived benefits.</p>
<h2 id="why-were-taking-action" tabindex="-1">Why we’re taking action</h2>
<p>SimpleX Chat signed a <a href="https://www.globalencryption.org/2024/05/joint-statement-on-the-dangers-of-the-may-2024-council-of-the-eu-compromise-proposal-on-eu-csam/">joint statement</a> about the dangers of the EU compromise proposal on EU CSAM because maintaining end-to-end encryption is crucial for protecting privacy and security for everyone, including and especially children.</p>
<p>We urge the Ministers in the Council of the EU to stand firm against any scanning proposals that undermine end-to-end encryption, which would enable mass surveillance and misuse by bad actors, whether framed as client-side scanning, upload moderation, or any other terminology. Compromising this basic principle opens the door to devastating privacy violations. We also urge any organizations or individuals reading this to write to their representatives and voice their concerns. European Digital Rights has <a href="https://edri.org/our-work/be-scanned-or-get-banned/">outlined these issues</a> in greater detail for anyone seeking more information.</p>
<h2 id="why-compromising-privacy-endangers-children" tabindex="-1">Why compromising privacy endangers children</h2>
<p>The core issue is that compromising encryption and privacy makes innocent people vulnerable to malicious hackers and criminals seeking to exploit users data. Centralized scanning systems become a tempting target, potentially exposing millions of private family photos when breached. This would easily open up avenues for blackmail, abuse, and victimization of children. A case in point is the recent <a href="https://techcrunch.com/2024/01/17/unredacted-meta-documents-reveal-historical-reluctance-to-protect-children-new-mexico-lawsuit/">criminal charges</a> against Meta in New Mexico, which highlights how the tech giant's algorithms enabled child exploitation by encouraging connections between minors and sexual predators. Privacy-eroding initiatives like client-side scanning would play into the hands of malicious actors by making more sensitive information accessible and weaponized in the same way that it has been on Meta platforms.</p>
<h2 id="what-should-be-done" tabindex="-1">What should be done</h2>
<p>Rather than undermining privacy, to achieve child safety online users should be empowered with high standards for encryption and data control. For example, adopting a model where children (and users in general) cannot be discovered or approached on networks unless they or their parents permit it, similar to the SimpleX network privacy model. Intelligent multi-device synchronization could enable this oversight without compromising end-to-end encryption overall. It’s always possible to protect children without opening everyone, especially children themselves, to greater vulnerabilities due to such proposals.</p>
<p>However, some recent legislative efforts have bizarrely moved in the opposite direction by seeking to limit parental access. The chilling truth is that the least private platforms have been major enablers of child exploitation. Eroding privacy protections on other services will only aid criminals further, not protect children. Preserving strong encryption and user privacy must be the foundation for any credible effort to combat online child exploitation. Initiatives trading privacy for supposed safety are not just technically flawed, but would achieve the exact opposite of their stated intent. We must avoid being gaslighted by narratives that defy logic, and instead provide users with the highest possible standards for privacy protections as a core principle.</p>
<p>Protecting end-to-end encryption without carving out backdoors or vulnerabilities should be non-negotiable for children's and everyone’s safety. It is critical to redirect the discourse to focus on taking genuine privacy further by protecting against <a href="https://simplex.chat/blog/20240416-dangers-of-metadata-in-messengers.html">metadata hoarding</a> and other means by which people’s data can be abused or subjected to surveillance.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OSRD: Open-Source Railway Designer (173 pts)]]></title>
            <link>https://osrd.fr/en/</link>
            <guid>40733705</guid>
            <pubDate>Thu, 20 Jun 2024 00:21:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://osrd.fr/en/">https://osrd.fr/en/</a>, See on <a href="https://news.ycombinator.com/item?id=40733705">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main role="main"><div id="td-cover-block-0"><p><img src="https://osrd.fr/logos/logo_white.svg"></p><h2>Open Source Railway Designer</h2></div><div><p>An open source web application for railway infrastructure design, capacity analysis, timetabling and more!</p></div><div><div><h4>Railway design</h4><p>Design railway infrastructure and timetables</p></div><div><h4>Capacity Analysis</h4><p>Detect conflicts and visualize capacity</p></div><div><h4>Short-term planning</h4><p>Automatically add new trains to an existing timetable</p></div></div><div><div><h4>Open source development</h4><p>Anyone can use, develop and distribute OSRD</p><p><a href="https://osrd.fr/en/about/opensource/">Read more …</a></p></div><div><h4>Open governance</h4><p>OSRD is publicly designed, decisions are taken collectively</p><p><a href="https://osrd.fr/en/about/governance/">Read more …</a></p></div><div><h4>Designed for interoperability</h4><p>Make custom infrastructure formats and signaling systems work together</p></div></div><section><video controls="">
<source src="https://osrd.fr/en/video.en.webm" type="video/webm"></video></section><section><h2>Sponsors</h2><p><img src="https://osrd.fr/sponsors/france-dot.svg" width="180px" height="180px" alt="Ministère chargé des Transports">
<img src="https://osrd.fr/sponsors/european-union.svg" width="180px" height="180px" alt="European Union">
<img src="https://osrd.fr/sponsors/sncf-reseau.svg" width="180px" height="180px" alt="SNCF Réseau"></p></section></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nobody knows what's going on (206 pts)]]></title>
            <link>https://www.raptitude.com/2024/06/nobody-knows-whats-going-on/</link>
            <guid>40733615</guid>
            <pubDate>Thu, 20 Jun 2024 00:06:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.raptitude.com/2024/06/nobody-knows-whats-going-on/">https://www.raptitude.com/2024/06/nobody-knows-whats-going-on/</a>, See on <a href="https://news.ycombinator.com/item?id=40733615">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img src="https://www.raptitude.com/wp-content/uploads/2024/06/artem-beliaikin-v6kii3H5CcU-unsplash.jpg" width="600" height="400" alt="Post image for Nobody Knows What’s Going On"></p><p>A major online publication once reported in a profile on me that I had retired at 33. A few old friends and acquaintances reached out to congratulate me on my financial independence.</p>



<p>I think it was an honest mistake on the part of the reporter. I told her I had quit my job to write full time, and I guess she thought that meant I must have millions of dollars.</p>



<p>To be clear, I was not then, and am not now financially independent. The 100 or so people that actually know me could discern that just by seeing my kitchen. Yet perhaps 20,000 people read somewhere that I am. That means potentially 200 times more people are wrong than right on this question, because of an inference made by a reporter.</p>



<p>This scenario, in which there’s much more wrongness going around than rightness, is probably the norm. People make bad inferences like that all day long. These wrong ideas replicate themselves whenever the person tells someone else what they know, which the internet makes easier than ever.</p>



<p>Consider the possibility that <em>most</em> of the information being passed around, on whatever topic, is bad information, even where there’s no intentional deception. As George Orwell said, “The most fundamental mistake of man is that he thinks he knows what’s going on. Nobody knows what’s going on.”</p>



<p>Technology may have made this state of affairs inevitable. Today, the vast majority of a person’s worldview is assembled from second-hand sources, not from their own experience. Second-hand knowledge, from “reliable” sources or not, usually functions as hearsay – if it <em>seems</em> true, it is immediately incorporated into one’s worldview, usually without any attempt to substantiate it. Most of what you “know” is just something you heard somewhere.</p>


<div>
<figure><a href="https://www.raptitude.com/wp-content/uploads/2024/06/titanic-onion.jpg"><img fetchpriority="high" decoding="async" width="300" height="297" src="https://www.raptitude.com/wp-content/uploads/2024/06/titanic-onion-300x297.jpg" alt="" srcset="https://www.raptitude.com/wp-content/uploads/2024/06/titanic-onion-300x297.jpg 300w, https://www.raptitude.com/wp-content/uploads/2024/06/titanic-onion-150x150.jpg 150w, https://www.raptitude.com/wp-content/uploads/2024/06/titanic-onion-194x192.jpg 194w, https://www.raptitude.com/wp-content/uploads/2024/06/titanic-onion.jpg 400w" sizes="(max-width: 300px) 100vw, 300px"></a><figcaption><em>Standard editorial approach</em></figcaption></figure></div>


<p>When people go on to share what they “know”, there’s usually no penalty for being wrong, but there are rewards for convincing people you’re right: attention, money, adoration, public rhetorical victories over others, and many other things humans enjoy.</p>



<h3>The Two Kinds of Knowing</h3>



<p>First-hand knowledge is a whole different thing from the second-hand kind. When you experience an event with your senses, you’re not just accepting a verbal claim, such as “There’s fighting in the streets of Kabul” — the truth is actually happening to you. The experienced sailor <em>knows</em> that looming cloud formation means trouble. The soldier knows the attack on his unit’s position was repelled. The dog owner knows exactly how long she can leave Rocco alone at home before he relieves himself on the floor. The friend sitting on my fifteen-year-old couch knows I’m not independently wealthy. Experience imprints reality right into your neurons; it doesn’t just add another thought to the abstract space in your brain where you keep your axioms and factoids.</p>


<div>
<figure><a href="https://www.raptitude.com/wp-content/uploads/2024/06/jamie-street-s9Tf1eBDFqw-unsplash600.jpg"><img decoding="async" width="300" height="225" src="https://www.raptitude.com/wp-content/uploads/2024/06/jamie-street-s9Tf1eBDFqw-unsplash600-300x225.jpg" alt="" srcset="https://www.raptitude.com/wp-content/uploads/2024/06/jamie-street-s9Tf1eBDFqw-unsplash600-300x225.jpg 300w, https://www.raptitude.com/wp-content/uploads/2024/06/jamie-street-s9Tf1eBDFqw-unsplash600-256x192.jpg 256w, https://www.raptitude.com/wp-content/uploads/2024/06/jamie-street-s9Tf1eBDFqw-unsplash600.jpg 600w" sizes="(max-width: 300px) 100vw, 300px"></a><figcaption><em>Good for about six hours</em></figcaption></figure></div>


<p>Only a tiny percentage of what a given person “knows” is in this first-hand, embodied form. The rest is made of impressions gathered from anecdotes, newspapers, books, schoolteachers, blogs, and things our older siblings told us when we were little.</p>



<p>If you ever read an article on a subject with which you have a lot of first-hand experience, you’ll notice that they <em>always</em> get major things wrong – basic facts, dates, names of people and organizations, the stated intentions of involved parties, the reasons a thing is happening – things even a novice in the space would know better about.</p>



<p>It makes perfect sense, if you think about it, that reporting is so reliably unreliable. Why do we expect reporters to learn about a suddenly newsworthy situation, gather information about it under deadline, then confidently explain the subject to the rest of the nation after having known about it for all of a week? People form their entire worldviews out of this stuff.</p>


<div>
<figure><a href="https://www.raptitude.com/wp-content/uploads/2024/06/owlsnot-as-they-seem500.jpg"><img decoding="async" width="300" height="278" src="https://www.raptitude.com/wp-content/uploads/2024/06/owlsnot-as-they-seem500-300x278.jpg" alt="" srcset="https://www.raptitude.com/wp-content/uploads/2024/06/owlsnot-as-they-seem500-300x278.jpg 300w, https://www.raptitude.com/wp-content/uploads/2024/06/owlsnot-as-they-seem500-207x192.jpg 207w, https://www.raptitude.com/wp-content/uploads/2024/06/owlsnot-as-they-seem500.jpg 500w" sizes="(max-width: 300px) 100vw, 300px"></a><figcaption><em>Me forming my worldview</em></figcaption></figure></div>


<p>What doesn’t make sense is that we immediately become credulous again as soon as the subject matter changes back to a topic on which we <em>don’t </em>have first-hand experience. You know they don’t know what they hell they’re talking about on Subject A, but hey what’s this about Subject B? In 2002, author Michael Crichton named this the “Gell-Mann Amnesia effect”:</p>



<blockquote>
<p>Briefly stated, the Gell-Mann Amnesia effect is as follows. You open the newspaper to an article on some subject you know well. In [Murray Gell-Mann’s] case, physics. In mine, show business. You read the article and see the journalist has absolutely no understanding of either the facts or the issues. Often, the article is so wrong it actually presents the story backward—reversing cause and effect. I call these the “wet streets cause rain” stories. Paper’s full of them.</p>



<p>In any case, you read with exasperation or amusement the multiple errors in a story, and then turn the page to national or international affairs, and read as if the rest of the newspaper was somehow more accurate about Palestine than the baloney you just read. You turn the page, and forget what you know.</p>
</blockquote>



<p>Crichton clarifies in the <a href="https://www.docdroid.net/4wgVecr/why-speculate-michael-crichton-pdf#page=2">full speech</a> that by “media” he’s not only talking about newspapers, but books, television, and internet too, and of course anybody’s recounting of what these sources say. Well, that accounts for just about 100% of our second-hand knowledge.</p>


<div>
<figure><a href="https://www.raptitude.com/wp-content/uploads/2024/06/gaurav-bagdi-K-fbBy5HNSg-unsplash600cropped.jpg"><img loading="lazy" decoding="async" width="282" height="300" src="https://www.raptitude.com/wp-content/uploads/2024/06/gaurav-bagdi-K-fbBy5HNSg-unsplash600cropped-282x300.jpg" alt="" srcset="https://www.raptitude.com/wp-content/uploads/2024/06/gaurav-bagdi-K-fbBy5HNSg-unsplash600cropped-282x300.jpg 282w, https://www.raptitude.com/wp-content/uploads/2024/06/gaurav-bagdi-K-fbBy5HNSg-unsplash600cropped-181x192.jpg 181w, https://www.raptitude.com/wp-content/uploads/2024/06/gaurav-bagdi-K-fbBy5HNSg-unsplash600cropped.jpg 600w" sizes="(max-width: 282px) 100vw, 282px"></a><figcaption>“<em>The situation is developing”</em></figcaption></figure></div>


<p>People do know things though. We have airplanes and phones and spaceships. Clearly somebody knows something. Human beings <em>can</em> be reliable sources of knowledge, but only about small slivers of the whole of what’s going on. They know things because they deal with their sliver every day, and they’re personally invested in how well they know their sliver, which gives them constant feedback on the quality of their beliefs.</p>



<p>Plumbing knowledge, for example, is constantly tested by whether the place floods after you’ve advanced your theory about what pipe connects to what. You need to get it right because it costs you something when you get it wrong.</p>



<p>The mechanic has seen a thousand check-engine lights and knows how each of them was resolved. The English professor has seen a thousand essays and can tell you what’s wrong with yours. The night club bouncer has dealt with a thousand drunk patrons and knows which guests will be trouble even before they do.</p>


<div>
<figure><a href="https://www.raptitude.com/wp-content/uploads/2024/06/rivage-4CNNH2KEjhc-unsplash600.jpg"><img loading="lazy" decoding="async" width="300" height="225" src="https://www.raptitude.com/wp-content/uploads/2024/06/rivage-4CNNH2KEjhc-unsplash600-300x225.jpg" alt="" srcset="https://www.raptitude.com/wp-content/uploads/2024/06/rivage-4CNNH2KEjhc-unsplash600-300x225.jpg 300w, https://www.raptitude.com/wp-content/uploads/2024/06/rivage-4CNNH2KEjhc-unsplash600-256x192.jpg 256w, https://www.raptitude.com/wp-content/uploads/2024/06/rivage-4CNNH2KEjhc-unsplash600.jpg 600w" sizes="(max-width: 300px) 100vw, 300px"></a><figcaption><em>Somebody’s sliver, thankfully</em></figcaption></figure></div>


<p>There are ways to carefully gather, scrutinize, and compare high-quality second-hand sources, and maybe learn something reliable, but this is extremely difficult for the groupish, emotional creature we are. It is <a href="https://www.raptitude.com/2023/05/how-to-think-about-politics-without-going-insane/">viscerally unpleasant</a> (not to mention time-consuming) to honestly question beliefs you feel positively towards, or honestly entertain ones you don’t, and ultimately you’re just determining what “feels right” anyway.</p>



<p>Aside from our own respective slivers of reliable knowledge, we mostly carry a lot of untested beliefs — teetering piles of them, accumulated over years, from random people assuring us “this is how it is.” Most of these beliefs are bunk, but we don’t know which ones. &nbsp;&nbsp;</p>



<h3>Beliefs are Mostly Mind-Candy</h3>



<p>Humans love beliefs, not because they’re reliable pointers to what’s true, but because they often feel good in some way, or have social rewards. Expressing and sharing beliefs can get us attention and social status, make us feel competent, sell our goods and services, and motivate people to do things for us, and they can just feel satisfying to say aloud. A convincing belief is simply one that feels good to the ears, or the mind.</p>


<div>
<figure><a href="https://www.raptitude.com/wp-content/uploads/2024/06/ryan-brooklyn-e5Ja8Z_WEvA-unsplash600.jpg"><img loading="lazy" decoding="async" width="300" height="200" src="https://www.raptitude.com/wp-content/uploads/2024/06/ryan-brooklyn-e5Ja8Z_WEvA-unsplash600-300x200.jpg" alt="" srcset="https://www.raptitude.com/wp-content/uploads/2024/06/ryan-brooklyn-e5Ja8Z_WEvA-unsplash600-300x200.jpg 300w, https://www.raptitude.com/wp-content/uploads/2024/06/ryan-brooklyn-e5Ja8Z_WEvA-unsplash600-288x192.jpg 288w, https://www.raptitude.com/wp-content/uploads/2024/06/ryan-brooklyn-e5Ja8Z_WEvA-unsplash600.jpg 600w" sizes="(max-width: 300px) 100vw, 300px"></a><figcaption><em>Beliefs of mine, awaiting testing</em></figcaption></figure></div>


<p>Theory feels good. Pithiness and analogy feel good. A tight sentence feels good. Neat and snappy stories about what’s “true” are like candy to the sense-craving part of the human brain.</p>



<p>Notice how many smart people believe things like, “You can’t reason yourself out of a belief you didn’t reason yourself into.” This is a belief nobody would arrive at through reason. It doesn’t stand up to even a minute’s logical scrutiny. You certainly didn’t reason yourself into a belief that North-Pole-dwelling elves made your childhood toys, but you probably reasoned yourself out of it. Both beliefs are just mind-candy, only for different audiences.</p>



<p>In short, human beings are bad at gathering information, inferring the right things from it, and responsibly passing it on to others. It is incredible what we’ve achieved in spite of this — almost entirely by carefully combining and testing our respective reliable slivers — but as a species we remain supremely untalented at knowing what’s true outside the range of our senses.</p>


<div>
<figure><a href="https://www.raptitude.com/wp-content/uploads/2024/06/Kids_in_the_Hall_Brain_Candy_poster.jpg"><img loading="lazy" decoding="async" width="257" height="388" src="https://www.raptitude.com/wp-content/uploads/2024/06/Kids_in_the_Hall_Brain_Candy_poster.jpg" alt="" srcset="https://www.raptitude.com/wp-content/uploads/2024/06/Kids_in_the_Hall_Brain_Candy_poster.jpg 257w, https://www.raptitude.com/wp-content/uploads/2024/06/Kids_in_the_Hall_Brain_Candy_poster-199x300.jpg 199w, https://www.raptitude.com/wp-content/uploads/2024/06/Kids_in_the_Hall_Brain_Candy_poster-127x192.jpg 127w" sizes="(max-width: 257px) 100vw, 257px"></a><figcaption><em>A relevant documentary (1996)</em></figcaption></figure></div>


<p>Much of the problem is that we want so badly to be believed, to be seen as someone who knows stuff. In the rest of Crichton’s speech, he explains why he named the Gell-Mann Amnesia effect after Murray Gell-Mann: because he’s famous, and he’s a physicist. People believe things named after physicists because we know they’re smart. And Crichton is a medical doctor, so you should listen to the guy.</p>



<p>Also, none of you will be able to confirm this, but George Orwell did not say the line in the intro of this article. I just said that he did so you would take my own assertions more seriously. And it’s too late — you’ve already tasted the candy. I mean, Orwell could have said something like that. He might as well have said it. He probably did, basically! I will sleep soundly anyway. There are few penalties for bullshit, and many rewards. Because nobody knows what’s going on.</p>



<p>***</p>



<p><em>Images by <a href="https://unsplash.com/@belart84">Artem Beliaikin</a>, <a href="https://www.theonion.com/">The Onion</a>, <a href="https://unsplash.com/@jamie452">Jamie Street</a>, <a href="https://unsplash.com/@sigmund">Rivage</a>, <a href="https://unsplash.com/@rbrooklyn">Ryan Brooklyn</a>, <a href="https://unsplash.com/@dfyngrvty">Gaurav Bagdi</a></em></p>

            

        				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hypermedia Systems (230 pts)]]></title>
            <link>https://hypermedia.systems/</link>
            <guid>40733160</guid>
            <pubDate>Wed, 19 Jun 2024 22:39:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hypermedia.systems/">https://hypermedia.systems/</a>, See on <a href="https://news.ycombinator.com/item?id=40733160">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <ul id="blurb" role="list" data-cols="1 3" data-cols@s="1" data-rows="1 3">
        <li>The revolutionary ideas that empowered the Web</li>
        <li>A simpler approach to building applications on the Web and beyond with <a href="https://htmx.org/">htmx</a> and <a href="https://hyperview.org/">Hyperview</a></li>
        <li>Enhancing web applications without using SPA frameworks</li>
        <li></li>
        </ul>

        <ul data-cols="4 5" data-cols@s="1" role="list">
        <li><strong><a href="https://hypermedia.systems/book/contents">
            <span>
                Read online
                <small>(Free, forever)</small>
            </span>
        </a></strong>
        </li>
        <li><strong><a href="https://www.amazon.com/dp/B0C9S88QV6/ref=sr_1_1">
            <span>
                Hard copy
                <small>(Buy on Amazon)</small>
            </span>
        </a></strong>
        </li>
        <li><strong><a href="https://www.amazon.com/Hypermedia-Systems-Carson-Gross-ebook/dp/B0CC315VJK/ref=tmm_kin_swatch_0">
            <span>
                Ebook
                <small>(Buy on Amazon)</small>
            </span>
        </a></strong>
        </li>
        </ul>

        <p data-cols="4 5" data-cols@s="1">
            For web developers frustrated with the complexity of modern practice,<span></span>
            those looking to brush up on web fundamentals,<span></span>
            web development shops looking to bring their apps to mobile,<span></span>
            and any workaday programmer looking for an introduction to hypermedia and REST.
        </p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unique3D: Image-to-3D Generation from a Single Image (128 pts)]]></title>
            <link>https://github.com/AiuniAI/Unique3D</link>
            <guid>40732490</guid>
            <pubDate>Wed, 19 Jun 2024 21:20:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/AiuniAI/Unique3D">https://github.com/AiuniAI/Unique3D</a>, See on <a href="https://news.ycombinator.com/item?id=40732490">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><strong><a href="https://github.com/AiuniAI/Unique3D/blob/main/README_zh.md">中文版本</a></strong></p>
<p dir="auto"><strong><a href="https://github.com/AiuniAI/Unique3D/blob/main/README_jp.md">日本語版</a></strong></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Unique3D</h2><a id="user-content-unique3d" aria-label="Permalink: Unique3D" href="#unique3d"></a></p>
<p dir="auto">Official implementation of Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image.</p>
<p dir="auto"><a href="https://scholar.google.com/citations?user=VTU0gysAAAAJ&amp;hl=zh-CN&amp;oi=ao" rel="nofollow">Kailu Wu</a>, <a href="https://liuff19.github.io/" rel="nofollow">Fangfu Liu</a>, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, <a href="https://duanyueqi.github.io/" rel="nofollow">Yueqi Duan</a>, <a href="https://group.iiis.tsinghua.edu.cn/~maks/" rel="nofollow">Kaisheng Ma</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://arxiv.org/abs/2405.20343" rel="nofollow">Paper</a> | <a href="https://wukailu.github.io/Unique3D/" rel="nofollow">Project page</a> | <a href="https://huggingface.co/spaces/Wuvin/Unique3D" rel="nofollow">Huggingface Demo</a> | <a href="https://u45213-bcf9-ef67553e.westx.seetacloud.com:8443/" rel="nofollow">Gradio Demo</a> | <a href="https://www.aiuni.ai/" rel="nofollow">Online Demo</a></h2><a id="user-content-paper--project-page--huggingface-demo--gradio-demo--online-demo" aria-label="Permalink: Paper | Project page | Huggingface Demo | Gradio Demo | Online Demo" href="#paper--project-page--huggingface-demo--gradio-demo--online-demo"></a></p>
<ul dir="auto">
<li>Demo inference speed: Gradio Demo &gt; Huggingface Demo &gt; Huggingface Demo2 &gt; Online Demo</li>
</ul>
<p dir="auto"><strong>If the Gradio Demo unfortunately hangs or is very crowded, you can use the Online Demo <a href="https://www.aiuni.ai/" rel="nofollow">aiuni.ai</a>, which is free to try (get the registration invitation code Join Discord: <a href="https://discord.gg/aiuni" rel="nofollow">https://discord.gg/aiuni</a>). However, the Online Demo is slightly different from the Gradio Demo, in that the inference speed is slower, and the generation results is less stable, but the quality of the material is better.</strong></p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/AiuniAI/Unique3D/blob/main/assets/teaser_safe.jpg"><img src="https://github.com/AiuniAI/Unique3D/raw/main/assets/teaser_safe.jpg"></a>
</p>
<p dir="auto">High-fidelity and diverse textured meshes generated by Unique3D from single-view wild images in 30 seconds.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">More features</h2><a id="user-content-more-features" aria-label="Permalink: More features" href="#more-features"></a></p>
<p dir="auto">The repo is still being under construction, thanks for your patience.</p>
<ul>
<li> Upload weights.</li>
<li> Local gradio demo.</li>
<li> Detailed tutorial.</li>
<li> Huggingface demo.</li>
<li> Detailed local demo.</li>
<li> Comfyui support.</li>
<li> Windows support.</li>
<li> Docker support.</li>
<li> More stable reconstruction with normal.</li>
<li> Training code release.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Preparation for inference</h2><a id="user-content-preparation-for-inference" aria-label="Permalink: Preparation for inference" href="#preparation-for-inference"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Linux System Setup.</h3><a id="user-content-linux-system-setup" aria-label="Permalink: Linux System Setup." href="#linux-system-setup"></a></p>
<p dir="auto">Adapted for Ubuntu 22.04.4 LTS and CUDA 12.1.</p>
<div data-snippet-clipboard-copy-content="conda create -n unique3d python=3.11
conda activate unique3d

pip install ninja
pip install diffusers==0.27.2

pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu121/torch2.3.1/index.html

pip install -r requirements.txt"><pre lang="angular2html"><code>conda create -n unique3d python=3.11
conda activate unique3d

pip install ninja
pip install diffusers==0.27.2

pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu121/torch2.3.1/index.html

pip install -r requirements.txt
</code></pre></div>
<p dir="auto"><a href="https://github.com/oak-barry">oak-barry</a> provide another setup script for torch210+cu121 at <a href="https://github.com/oak-barry/Unique3D">here</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Windows Setup.</h3><a id="user-content-windows-setup" aria-label="Permalink: Windows Setup." href="#windows-setup"></a></p>
<ul dir="auto">
<li>Thank you very much <code>jtydhr88</code> for the windows installation method! See <a href="https://github.com/AiuniAI/Unique3D/issues/15" data-hovercard-type="issue" data-hovercard-url="/AiuniAI/Unique3D/issues/15/hovercard">issues/15</a>.</li>
</ul>
<p dir="auto">According to <a href="https://github.com/AiuniAI/Unique3D/issues/15" data-hovercard-type="issue" data-hovercard-url="/AiuniAI/Unique3D/issues/15/hovercard">issues/15</a>, implemented a bat script to run the commands, so you can:</p>
<ol dir="auto">
<li>Might still require Visual Studio Build Tools, you can find it from <a href="https://visualstudio.microsoft.com/downloads/?q=build+tools" rel="nofollow">Visual Studio Build Tools</a>.</li>
<li>Create conda env and activate it
<ol dir="auto">
<li><code>conda create -n unique3d-py311 python=3.11</code></li>
<li><code>conda activate unique3d-py311</code></li>
</ol>
</li>
<li>download <a href="https://huggingface.co/madbuda/triton-windows-builds/resolve/main/triton-2.1.0-cp311-cp311-win_amd64.whl" rel="nofollow">triton whl</a> for py311, and put it into this project.</li>
<li>run <strong>install_windows_win_py311_cu121.bat</strong></li>
<li>answer y while asking you uninstall onnxruntime and onnxruntime-gpu</li>
<li>create the output folder <strong>tmp\gradio</strong> under the driver root, such as F:\tmp\gradio for me.</li>
<li>python app/gradio_local.py --port 7860</li>
</ol>
<p dir="auto">More details prefer to <a href="https://github.com/AiuniAI/Unique3D/issues/15" data-hovercard-type="issue" data-hovercard-url="/AiuniAI/Unique3D/issues/15/hovercard">issues/15</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Interactive inference: run your local gradio demo.</h3><a id="user-content-interactive-inference-run-your-local-gradio-demo" aria-label="Permalink: Interactive inference: run your local gradio demo." href="#interactive-inference-run-your-local-gradio-demo"></a></p>
<ol dir="auto">
<li>Download the weights from <a href="https://huggingface.co/spaces/Wuvin/Unique3D/tree/main/ckpt" rel="nofollow">huggingface spaces</a> or <a href="https://cloud.tsinghua.edu.cn/d/319762ec478d46c8bdf7/" rel="nofollow">Tsinghua Cloud Drive</a>, and extract it to <code>ckpt/*</code>.</li>
</ol>
<div data-snippet-clipboard-copy-content="Unique3D
    ├──ckpt
        ├── controlnet-tile/
        ├── image2normal/
        ├── img2mvimg/
        ├── realesrgan-x4.onnx
        └── v1-inference.yaml"><pre><code>Unique3D
    ├──ckpt
        ├── controlnet-tile/
        ├── image2normal/
        ├── img2mvimg/
        ├── realesrgan-x4.onnx
        └── v1-inference.yaml
</code></pre></div>
<ol start="2" dir="auto">
<li>Run the interactive inference locally.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python app/gradio_local.py --port 7860"><pre>python app/gradio_local.py --port 7860</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">ComfyUI Support</h2><a id="user-content-comfyui-support" aria-label="Permalink: ComfyUI Support" href="#comfyui-support"></a></p>
<p dir="auto">Thanks for the <a href="https://github.com/jtydhr88/ComfyUI-Unique3D">ComfyUI-Unique3D</a> implementation from <a href="https://github.com/jtydhr88">jtydhr88</a>!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tips to get better results</h2><a id="user-content-tips-to-get-better-results" aria-label="Permalink: Tips to get better results" href="#tips-to-get-better-results"></a></p>
<ol dir="auto">
<li>Unique3D is sensitive to the facing direction of input images. Due to the distribution of the training data, orthographic front-facing images with a rest pose always lead to good reconstructions.</li>
<li>Images with occlusions will cause worse reconstructions, since four views cannot cover the complete object. Images with fewer occlusions lead to better results.</li>
<li>Pass an image with as high a resolution as possible to the input when resolution is a factor.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgement</h2><a id="user-content-acknowledgement" aria-label="Permalink: Acknowledgement" href="#acknowledgement"></a></p>
<p dir="auto">We have intensively borrowed code from the following repositories. Many thanks to the authors for sharing their code.</p>
<ul dir="auto">
<li><a href="https://github.com/CompVis/stable-diffusion">Stable Diffusion</a></li>
<li><a href="https://github.com/xxlong0/Wonder3D">Wonder3d</a></li>
<li><a href="https://github.com/SUDO-AI-3D/zero123plus">Zero123Plus</a></li>
<li><a href="https://github.com/Profactor/continuous-remeshing">Continues Remeshing</a></li>
<li><a href="https://github.com/YertleTurtleGit/depth-from-normals">Depth from Normals</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Collaborations</h2><a id="user-content-collaborations" aria-label="Permalink: Collaborations" href="#collaborations"></a></p>
<p dir="auto">Our mission is to create a 4D generative model with 3D concepts. This is just our first step, and the road ahead is still long, but we are confident. We warmly invite you to join the discussion and explore potential collaborations in any capacity. <span><strong>If you're interested in connecting or partnering with us, please don't hesitate to reach out via email (<a href="mailto:wkl22@mails.tsinghua.edu.cn">wkl22@mails.tsinghua.edu.cn</a>)</strong></span>.</p>
<ul dir="auto">
<li>Follow us on twitter for the latest updates: <a href="https://x.com/aiuni_ai" rel="nofollow">https://x.com/aiuni_ai</a></li>
<li>Join AIGC 3D/4D generation community on discord: <a href="https://discord.gg/aiuni" rel="nofollow">https://discord.gg/aiuni</a></li>
<li>Research collaboration, please contact: <a href="mailto:ai@aiuni.ai">ai@aiuni.ai</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">If you found Unique3D helpful, please cite our report:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{wu2024unique3d,
      title={Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image}, 
      author={Kailu Wu and Fangfu Liu and Zhihan Cai and Runjie Yan and Hanyang Wang and Yating Hu and Yueqi Duan and Kaisheng Ma},
      year={2024},
      eprint={2405.20343},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}"><pre><span>@misc</span>{<span>wu2024unique3d</span>,
      <span>title</span>=<span><span>{</span>Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image<span>}</span></span>, 
      <span>author</span>=<span><span>{</span>Kailu Wu and Fangfu Liu and Zhihan Cai and Runjie Yan and Hanyang Wang and Yating Hu and Yueqi Duan and Kaisheng Ma<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2024<span>}</span></span>,
      <span>eprint</span>=<span><span>{</span>2405.20343<span>}</span></span>,
      <span>archivePrefix</span>=<span><span>{</span>arXiv<span>}</span></span>,
      <span>primaryClass</span>=<span><span>{</span>cs.CV<span>}</span></span>
}</pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[X debut 40 years ago (1984) (308 pts)]]></title>
            <link>https://www.talisman.org/x-debut.shtml</link>
            <guid>40731922</guid>
            <pubDate>Wed, 19 Jun 2024 20:09:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.talisman.org/x-debut.shtml">https://www.talisman.org/x-debut.shtml</a>, See on <a href="https://news.ycombinator.com/item?id=40731922">Hacker News</a></p>
Couldn't get https://www.talisman.org/x-debut.shtml: Error: connect ENETUNREACH 2001:470:b8ba:1::99:443]]></description>
        </item>
    </channel>
</rss>