(ignoring known css parsing error)
<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 29 Sep 2025 22:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[California governor signs AI transparency bill into law (110 pts)]]></title>
            <link>https://www.gov.ca.gov/2025/09/29/governor-newsom-signs-sb-53-advancing-californias-world-leading-artificial-intelligence-industry/</link>
            <guid>45418428</guid>
            <pubDate>Mon, 29 Sep 2025 20:33:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gov.ca.gov/2025/09/29/governor-newsom-signs-sb-53-advancing-californias-world-leading-artificial-intelligence-industry/">https://www.gov.ca.gov/2025/09/29/governor-newsom-signs-sb-53-advancing-californias-world-leading-artificial-intelligence-industry/</a>, See on <a href="https://news.ycombinator.com/item?id=45418428">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				
				
				
				
				<p><strong>What you need to know:</strong> Governor Newsom today signed legislation further establishing California as a world leader in safe, secure, and trustworthy artificial intelligence, creating a new law that helps the state both boost innovation and protect public safety.</p>
			</div><div>
				
				
				
				
				<p><strong>SACRAMENTO — </strong>Governor Newsom today signed into law Senate Bill 53, the Transparency in Frontier Artificial Intelligence Act (TFAIA), authored by Senator Scott Wiener (D-San Francisco) – legislation carefully designed to enhance online safety by installing commonsense guardrails on the development of frontier artificial intelligence models, helping build public trust while also continuing to spur innovation in these new technologies. The new law builds on recommendations from <a href="https://www.gov.ca.gov/2025/03/18/california-strengthens-its-position-as-the-global-ai-leader-with-new-working-report-issued-by-experts-and-academics/" data-cke-saved-href="https://www.gov.ca.gov/2025/03/18/california-strengthens-its-position-as-the-global-ai-leader-with-new-working-report-issued-by-experts-and-academics/">California’s first-in-the-nation report</a>, called for by Governor Newsom and published earlier this year — and helps advance California’s position as a <a href="https://www.gov.ca.gov/2025/09/18/californication-of-ai-golden-state-is-1-in-ai-and-the-birthplace-of-modern-tech-so-yeah-be-quiet-ted-cruz/" data-cke-saved-href="https://www.gov.ca.gov/2025/09/18/californication-of-ai-golden-state-is-1-in-ai-and-the-birthplace-of-modern-tech-so-yeah-be-quiet-ted-cruz/">national leader in responsible and ethical AI</a>, the world’s fourth-largest economy, the birthplace of new technology, and the top pipeline for tech talent.</p>
			</div><div><h4>“California has proven that we can establish regulations to protect our communities while also ensuring that the growing AI industry continues to thrive. This legislation strikes that balance. AI is the new frontier in innovation, and California is not only here for it – but stands strong as a national leader by enacting the first-in-the-nation frontier AI safety legislation that builds public trust as this emerging technology rapidly evolves.”</h4>
<p><strong>Governor Gavin Newsom</strong></p></div><div>
				
				
				
				
				<p>California works closely to foster tech leadership and create an environment where industry and talent thrive. The state is balancing its work to advance AI with commonsense laws to protect the public, embracing the technology to make our lives easier and make government more efficient, effective, and transparent. California’s leadership in the AI industry is helping to guide the world in the responsible implementation and use of this emerging technology.&nbsp;</p>
			</div><div><h4>“With a technology as transformative as AI, we have a responsibility to support that innovation while putting in place commonsense guardrails to understand and reduce risk. With this law, California is stepping up, once again, as a global leader on both technology innovation and safety. I’m grateful to the Governor for his leadership in convening the Joint California AI Policy Working Group, working with us to refine the legislation, and now signing it into law. His Administration’s partnership helped this groundbreaking legislation promote innovation and establish guardrails for trust, fairness, and accountability in the most remarkable new technology in many years.”</h4>
<p><strong>Senator Scott Wiener</strong></p></div><div>
				
				
				
				
				<p>Earlier this year, a group of world-leading AI academics and experts — <a href="https://www.gov.ca.gov/2024/09/29/governor-newsom-announces-new-initiatives-to-advance-safe-and-responsible-ai-protect-californians/" data-cke-saved-href="https://www.gov.ca.gov/2024/09/29/governor-newsom-announces-new-initiatives-to-advance-safe-and-responsible-ai-protect-californians/">convened at the request of Governor Newsom</a> — released a <a href="https://www.gov.ca.gov/2025/03/18/california-strengthens-its-position-as-the-global-ai-leader-with-new-working-report-issued-by-experts-and-academics/" data-cke-saved-href="https://www.gov.ca.gov/2025/03/18/california-strengthens-its-position-as-the-global-ai-leader-with-new-working-report-issued-by-experts-and-academics/">first-in-the-nation report </a>on sensible AI guardrails, based on an empirical, science-based analysis of the capabilities and attendant risks of frontier models. The report included recommendations on ensuring evidence-based policymaking, balancing the need for transparency with considerations such as security risks, and determining the appropriate level of regulation in this fast-evolving field. SB 53 is responsive to the recommendations in the report — and will help ensure California’s position as an AI leader. This legislation is particularly important given the failure of the federal government to enact comprehensive, sensible AI policy. SB 53 fills this gap and presents a model for the nation to follow.</p>
			</div><div><h4>“Last year Governor Newsom called upon us to study how California should properly approach frontier artificial intelligence development. The Transparency in Frontier Artificial Intelligence Act (TFAIA) moves us towards the transparency and ‘trust but verify’ policy principles outlined in our report.&nbsp; As artificial intelligence continues its long journey of development, more frontier breakthroughs will occur. AI policy should continue emphasizing thoughtful scientific review and keeping America at the forefront of technology.”</h4>
<p><strong>Mariano-Florentino (Tino) Cuéllar</strong><br>Former California Supreme Court Justice and former member of National Academy of Sciences Committee on the Social and Ethical Implications of Computing Research</p>
<p><strong>Dr. Fei-Fei Li</strong><br>Co-Director, Stanford Institute for Human-Centered Artificial Intelligence</p>
<p><strong>Jennifer Tour Chayes</strong><br>Dean of the College of Computing, Data Science, and Society at UC Berkeley</p></div><div><h2><strong>California’s AI dominance&nbsp;</strong></h2>
<p>California continues to dominate the AI sector. In addition to being the birthplace of AI, the state is home to <a href="https://www.forbes.com/lists/ai50/" data-cke-saved-href="https://www.forbes.com/lists/ai50/">32 of the 50 top AI companies</a> worldwide. California leads U.S. demand for AI talent. In 2024, 15.7% of all U.S. AI job postings were in California — #1 by state, well ahead of Texas (8.8% and New York (5.8%), per the 2025 Stanford AI Index. In 2024, <a href="https://pitchbook.com/news/articles/as-sf-bay-area-eats-up-52-of-all-ai-and-ml-vc-dollars-foreign-vcs-warn-of-waste" data-cke-saved-href="https://pitchbook.com/news/articles/as-sf-bay-area-eats-up-52-of-all-ai-and-ml-vc-dollars-foreign-vcs-warn-of-waste">more than half</a> of global VC funding for AI and machine learning startups went to companies in the Bay Area. California is also home to three of the four companies that have passed the $3 trillion valuation mark. Each of these California-based companies — Google, Apple, and Nvidia — are tech companies involved in AI and have created hundreds of thousands of jobs.</p>
<h2>What the law does:</h2>
<p>SB 53 establishes new requirements for frontier AI developers creating stronger:</p>
<p><strong>✅ Transparency: </strong>Requires large frontier developers to publicly publish a framework on its website describing how the company has incorporated national standards, international standards, and industry-consensus best practices into its frontier AI framework.</p>
<p><strong>✅ Innovation: </strong>Establishes a new consortium within the Government Operations Agency to develop a framework for creating a public computing cluster. The consortium, called CalCompute, will advance the development and deployment of artificial intelligence that is safe, ethical, equitable, and sustainable by fostering research and innovation.</p>
<p><strong>✅ Safety: </strong>Creates a new mechanism for frontier AI companies and the public to report potential critical safety incidents to California’s Office of Emergency Services.</p>
<p><strong>✅ Accountability: </strong>Protects whistleblowers who disclose significant health and safety risks posed by frontier models, and creates a civil penalty for noncompliance, enforceable by the Attorney General’s office.</p>
<p><strong>✅ Responsiveness:</strong> Directs the California Department of Technology to annually recommend appropriate updates to the law based on multistakeholder input, technological developments, and international standards.&nbsp;</p>
<p><a href="https://www.gov.ca.gov/wp-content/uploads/2025/09/SB-53-Signing-Message.pdf" data-cke-saved-href="https://www.gov.ca.gov/wp-content/uploads/2025/09/SB-53-Signing-Message.pdf">A signing message can be found here.</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FCC Accidentally Leaked iPhone Schematics (195 pts)]]></title>
            <link>https://www.engadget.com/big-tech/fcc-accidentally-leaked-iphone-schematics-potentially-giving-rivals-a-peek-at-company-secrets-154551807.html</link>
            <guid>45416231</guid>
            <pubDate>Mon, 29 Sep 2025 17:12:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.engadget.com/big-tech/fcc-accidentally-leaked-iphone-schematics-potentially-giving-rivals-a-peek-at-company-secrets-154551807.html">https://www.engadget.com/big-tech/fcc-accidentally-leaked-iphone-schematics-potentially-giving-rivals-a-peek-at-company-secrets-154551807.html</a>, See on <a href="https://news.ycombinator.com/item?id=45416231">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-article-body="true"><p>The Federal Communications Commission (FCC) <a data-i13n="cpos:1;pos:1" href="https://fccid.io/BCG-E8726A/Schematics/A3212-A3408-A3409-A3410-System-Electrical-Schematics-V1-0-8024509" rel="nofollow noopener" target="_blank" data-ylk="slk:recently published a 163-page PDF;cpos:1;pos:1;elm:context_link;itc:0;sec:content-canvas"><ins>recently published a 163-page PDF</ins></a> showing the electrical schematics for the iPhone 16e, despite Apple specifically requesting them to be confidential. This was most likely a mistake on the part of the FCC, <a data-i13n="cpos:2;pos:1" href="https://appleinsider.com/articles/25/09/29/fcc-mistakenly-leaks-confidential-iphone-16e-schematics?utm_source=feedly" rel="nofollow noopener" target="_blank" data-ylk="slk:according to a report by AppleInsider;cpos:2;pos:1;elm:context_link;itc:0;sec:content-canvas"><ins>according to a report by </ins><em><ins>AppleInsider</ins></em></a>.</p><p>The agency also distributed a cover letter from Apple alongside the schematics, which is dated September 16, 2024. This letter verifies the company's request for privacy, indicating that the documents contain "confidential and proprietary trade secrets." The cover letter asks for the documents to be withheld from public view "indefinitely." Apple even suggested that a release of the files could give competitors an "unfair advantage."</p><p>To that end, the documents feature full schematics of the iPhone 16e. These include block diagrams, electrical schematic diagrams, antenna locations and more. Competitors could simply buy a handset and open it up to get to this information, as the <a data-i13n="cpos:3;pos:1" href="https://www.engadget.com/mobile/smartphones/iphone-16e-review-whats-your-acceptable-compromise-020016288.html" data-ylk="slk:iPhone 16e came out back in February;cpos:3;pos:1;elm:context_link;itc:0;sec:content-canvas"><ins>iPhone 16e came out back in February</ins></a>, but this leak would eliminate any guesswork. However, Apple is an extremely litigious company when it comes to <a data-i13n="cpos:4;pos:1" href="https://www.engadget.com/big-tech/apple-wins-250-in-masimo-smartwatch-patent-case-150020340.html" data-ylk="slk:stuff like patent infringement;cpos:4;pos:1;elm:context_link;itc:0;sec:content-canvas"><ins>stuff like patent infringement</ins></a>.</p><p>The FCC hasn't addressed how this leak happened or what it intends to do about it. <em>AppleInsider's</em> reporting suggested that this probably happened due to an incorrect setting in a database. This was likely not an intentional act against Apple, which tracks given that the company has been especially supportive of the Trump administration. CEO Tim Cook even <a data-i13n="cpos:5;pos:1" href="https://www.engadget.com/mobile/engadget-podcast-apple-bows-to-the-trump-regime-again-113025360.html" data-ylk="slk:brought the president a gold trophy;cpos:5;pos:1;elm:context_link;itc:0;sec:content-canvas"><ins>brought the president a gold trophy</ins></a> for being such a good and important boy.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Claude Code 2.0 (414 pts)]]></title>
            <link>https://www.npmjs.com/package/@anthropic-ai/claude-code</link>
            <guid>45416228</guid>
            <pubDate>Mon, 29 Sep 2025 17:12:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.npmjs.com/package/@anthropic-ai/claude-code">https://www.npmjs.com/package/@anthropic-ai/claude-code</a>, See on <a href="https://news.ycombinator.com/item?id=45416228">Hacker News</a></p>
Couldn't get https://www.npmjs.com/package/@anthropic-ai/claude-code: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Buy It in ChatGPT: Instant Checkout and the Agentic Commerce Protocol (118 pts)]]></title>
            <link>https://openai.com/index/buy-it-in-chatgpt/</link>
            <guid>45416080</guid>
            <pubDate>Mon, 29 Sep 2025 17:00:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/buy-it-in-chatgpt/">https://openai.com/index/buy-it-in-chatgpt/</a>, See on <a href="https://news.ycombinator.com/item?id=45416080">Hacker News</a></p>
Couldn't get https://openai.com/index/buy-it-in-chatgpt/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Claude Sonnet 4.5 (855 pts)]]></title>
            <link>https://www.anthropic.com/news/claude-sonnet-4-5</link>
            <guid>45415962</guid>
            <pubDate>Mon, 29 Sep 2025 16:52:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/news/claude-sonnet-4-5">https://www.anthropic.com/news/claude-sonnet-4-5</a>, See on <a href="https://news.ycombinator.com/item?id=45415962">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><p>Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It’s the best model at using computers. And it shows substantial gains in reasoning and math.</p><p>Code is everywhere. It runs every application, spreadsheet, and software tool you use. Being able to use those tools and reason through hard problems is how modern work gets done.</p><p>Claude Sonnet 4.5 makes this possible. We're releasing it along with a set of major upgrades to our products. In <a href="https://anthropic.com/news/enabling-claude-code-to-work-more-autonomously" target="_blank" rel="noopener noreferrer">Claude Code</a>, we've added checkpoints—one of our most requested features—that save your progress and allow you to roll back instantly to a previous state. We've refreshed the terminal interface and shipped a <a href="https://marketplace.visualstudio.com/items?itemName=anthropic.claude-code" target="_blank" rel="noopener noreferrer">native VS Code extension</a>. We've added a new <a href="https://anthropic.com/news/context-management" target="_blank" rel="noopener noreferrer">context editing feature and memory tool</a> to the Claude API that lets agents run even longer and handle even greater complexity. In the Claude <a href="https://claude.ai/redirect/website.v1.f79c597c-8da8-4257-a3ef-0233ea5a1dd0/download" target="_blank" rel="noopener noreferrer">apps</a>, we've brought code execution and <a href="https://www.anthropic.com/news/create-files" target="_blank" rel="noopener noreferrer">file creation</a> (spreadsheets, slides, and documents) directly into the conversation. And we've made the <a href="https://www.anthropic.com/news/claude-for-chrome" target="_blank" rel="noopener noreferrer">Claude for Chrome</a> extension available to Max users who joined the waitlist last month.</p><p>We're also giving developers the building blocks we use ourselves to make Claude Code. We're calling this the <a href="https://anthropic.com/engineering/building-agents-with-the-claude-agent-sdk" target="_blank" rel="noopener noreferrer">Claude Agent SDK</a>. The infrastructure that powers our frontier products—and allows them to reach their full potential—is now yours to build with.</p><p>This is the <a href="https://www.anthropic.com/claude-sonnet-4-5-system-card" target="_blank" rel="noopener noreferrer">most aligned frontier model</a> we’ve ever released, showing large improvements across several areas of alignment compared to previous Claude models.</p><p>Claude Sonnet 4.5 is available everywhere today. If you’re a developer, simply use <code>claude-sonnet-4-5</code> via <a href="https://docs.claude.com/en/docs/about-claude/models/overview" target="_blank" rel="noopener noreferrer">the Claude API</a>. Pricing remains the same as Claude Sonnet 4, at $3/$15 per million tokens.</p><h2 id="frontier-intelligence">Frontier intelligence</h2><p>Claude Sonnet 4.5 is state-of-the-art on the SWE-bench Verified evaluation, which measures real-world software coding abilities. Practically speaking, we’ve observed it maintaining focus for more than 30 hours on complex, multi-step tasks.</p><div><figure><img alt="Chart showing frontier model performance on SWE-bench Verified with Claude Sonnet 4.5 leading" loading="lazy" width="3840" height="2160" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F6421e7049ff8b2c4591497ec92dc4157b2ac1b30-3840x2160.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F6421e7049ff8b2c4591497ec92dc4157b2ac1b30-3840x2160.png&amp;w=3840&amp;q=75"></figure></div><p>Claude Sonnet 4.5 represents a significant leap forward on computer use. On OSWorld, a benchmark that tests AI models on real-world computer tasks, Sonnet 4.5 now leads at 61.4%. Just four months ago, Sonnet 4 held the lead at 42.2%. Our <a href="https://www.anthropic.com/news/claude-for-chrome" target="_blank" rel="noopener noreferrer">Claude for Chrome</a> extension puts these upgraded capabilities to use. In the demo below, we show Claude working directly in a browser, navigating sites, filling spreadsheets, and completing tasks.</p><p>The model also shows improved capabilities on a broad range of evaluations including reasoning and math:</p><div><figure><img alt="Benchmark table comparing frontier models across popular public evals" loading="lazy" width="2600" height="2288" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F2825bb34cb9b9aa1ef347f816c590b5bbdae2b4d-2600x2288.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F2825bb34cb9b9aa1ef347f816c590b5bbdae2b4d-2600x2288.png&amp;w=3840&amp;q=75"><figcaption>Claude Sonnet 4.5 is our most powerful model to date. See footnotes for methodology.</figcaption></figure></div><p>Experts in finance, law, medicine, and STEM found Sonnet 4.5 shows dramatically better domain-specific knowledge and reasoning compared to older models, including Opus 4.1.</p><p>The model’s capabilities are also reflected in the experiences of early customers:</p><div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/464cf83cd04ad624fee1730a71914b18e89cdf9b-150x48.svg"></p><p><span>“</span></p><blockquote><p><strong>We're seeing state-of-the-art coding performance from Claude Sonnet 4.5</strong>, with significant improvements on longer horizon tasks. It reinforces why many developers using Cursor choose Claude for solving their most complex problems.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/7715b118c5eb0ff2a85f1f7914bce8c634ecacbd-150x48.svg"></p><p><span>“</span></p><blockquote><p><strong>Claude Sonnet 4.5 amplifies GitHub Copilot's core strengths</strong>. Our initial evals show significant improvements in multi-step reasoning and code comprehension—enabling Copilot's agentic experiences to handle complex, codebase-spanning tasks better.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/daef759120b29e4db8ba4a5664d7574750964ab9-150x48.svg"></p><p><span>“</span></p><blockquote><p><strong>Claude Sonnet 4.5 is excellent at software development tasks</strong>, learning our codebase patterns to deliver precise implementations. It handles everything from debugging to architecture with deep contextual understanding, transforming our development velocity.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/eb96f772e9ae5e340de41e6b07f3c6d50b3fff22-150x48.svg"></p><p><span>“</span></p><blockquote><p>Claude Sonnet 4.5 <strong>reduced average vulnerability intake time for our Hai security agents by 44% while improving accuracy by 25%</strong>, helping us reduce risk for businesses with confidence.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/8cbf56e184dd5174705a0f55cb91b0af545982ff-150x48.svg"></p><p><span>“</span></p><blockquote><p><strong>Claude Sonnet 4.5 is state of the art on the most complex litigation tasks.</strong> For example, analyzing full briefing cycles and conducting research to synthesize excellent first drafts of an opinion for judges, or interrogating entire litigation records to create detailed summary judgment analysis.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/431e098a503851789fa4508b88a0418853f513eb-150x48.svg"></p><p><span>“</span></p><blockquote><p>Claude Sonnet 4.5's edit capabilities are exceptional —<strong> we went from 9% error rate on Sonnet 4 to 0% on our internal code editing benchmark</strong>. Higher tool success at lower cost is a major leap for agentic coding. Claude Sonnet 4.5 balances creativity and control perfectly.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/66e0000e396aea64ea31ed3fea7b2b20ac329312-150x48.svg"></p><p><span>“</span></p><blockquote><p>Claude Sonnet 4.5 delivers impressive gains on our most complex, long-context tasks—from engineering in our codebase to in-product features and research. <strong>It's noticeably more intelligent and a big leap forward</strong>, helping us push what 240M+ users can design with Canva.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/cdec0ff1244295571db38838e90f61c47681d63d-150x48.svg"></p><p><span>“</span></p><blockquote><p><strong>Claude Sonnet 4.5 has noticeably improved Figma Make in early testing</strong>, making it easier to prompt and iterate. Teams can explore and validate their ideas with more functional prototypes and smoother interactions, while still getting the design quality Figma is known for.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/094b76abf3e64453c224e12ae388b8008b02660e-150x48.svg"></p><p><span>“</span></p><blockquote><p><strong>Sonnet 4.5 represents a new generation of coding models</strong>. It's surprisingly efficient at maximizing actions per context window through parallel tool execution, for example running multiple bash commands at once. </p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/6e418ccebe0a1d6fd13f21094852b080a0c93ae5-150x48.svg"></p><p><span>“</span></p><blockquote><p>For Devin, Claude Sonnet 4.5 increased planning performance by 18% and end-to-end eval scores by 12%—<strong>the biggest jump we've seen since the release of Claude Sonnet 3.6</strong>. It excels at testing its own code, enabling Devin to run longer, handle harder tasks, and deliver production-ready code.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/5a7dfab326b449aedc0d11053f9d42f48951ae7e-150x48.svg"></p><p><span>“</span></p><blockquote><p><strong>Claude Sonnet 4.5 shows strong promise for red teaming</strong>, generating creative attack scenarios that accelerate how we study attacker tradecraft. These insights strengthen our defenses across endpoints, identity, cloud, data, SaaS, and AI workloads.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/b0b6b40b55f3aa73e8a32ce81f9bb927134fd3da-150x48.svg"></p><p><span>“</span></p><blockquote><p>Claude Sonnet 4.5 resets our expectations—<strong>it handles 30+ hours of autonomous coding</strong>, freeing our engineers to tackle months of complex architectural work in dramatically less time while maintaining coherence across massive codebases.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/4fcce1a2389ddafa9f3302c51960e1ff4bfbd3d7-150x48.svg"></p><p><span>“</span></p><blockquote><p>For complex financial analysis—risk, structured products, portfolio screening—Claude Sonnet 4.5 with thinking <strong>delivers investment-grade insights that require less human review</strong>. When depth matters more than speed, it's a meaningful step forward for institutional finance.</p></blockquote></div></div><h2 id="our-most-aligned-model-yet">Our most aligned model yet</h2><p>As well as being our most capable model, Claude Sonnet 4.5 is our most aligned frontier model yet. Claude’s improved capabilities and our extensive safety training have allowed us to substantially improve the model’s behavior, reducing concerning behaviors like sycophancy, deception, power-seeking, and the tendency to encourage delusional thinking. For the model’s agentic and computer use capabilities, we’ve also made considerable progress on defending against prompt injection attacks, one of the most serious risks for users of these capabilities.</p><p>You can read a detailed set of safety and alignment evaluations, which for the first time includes tests using techniques from mechanistic interpretability, in the Claude Sonnet 4.5 <a href="https://www.anthropic.com/claude-sonnet-4-5-system-card" target="_blank" rel="noopener noreferrer">system card</a>.</p><div><figure><img loading="lazy" width="3840" height="2160" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F33efc283321feeff94dd80973dbcd38409806cf5-3840x2160.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F33efc283321feeff94dd80973dbcd38409806cf5-3840x2160.png&amp;w=3840&amp;q=75"><figcaption>Overall misaligned behavior scores from an automated behavioral auditor (lower is better). Misaligned behaviors include (but are not limited to) deception, sycophancy, power-seeking, encouragement of delusions, and compliance with harmful system prompts. More details can be found in the Claude Sonnet 4.5 <a href="https://www.anthropic.com/claude-sonnet-4-5-system-card">system card</a>.</figcaption></figure></div><p>Claude Sonnet 4.5 is being released under our AI Safety Level 3 (ASL-3) protections, as per <a href="https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy" target="_blank" rel="noopener noreferrer">our framework</a> that matches model capabilities with appropriate safeguards. These safeguards include filters called classifiers that aim to detect potentially dangerous inputs and outputs—in particular those related to chemical, biological, radiological, and nuclear (CBRN) weapons.</p><p>These classifiers might sometimes inadvertently flag normal content. We’ve made it easy for users to continue any interrupted conversations with Sonnet 4, a model that poses a lower CBRN risk. We've already made significant progress in reducing these false positives, reducing them by a factor of ten since <a href="https://www.anthropic.com/news/constitutional-classifiers" target="_blank" rel="noopener noreferrer">we originally described them</a>, and a factor of two since Claude Opus 4 was released in May. We’re continuing to make progress in making the classifiers more discerning<sup>1</sup>.</p><h2 id="the-claude-agent-sdk">The Claude Agent SDK</h2><p>We've spent more than six months shipping updates to Claude Code, so we know what it takes to <a href="https://www.youtube.com/watch?v=DAQJvGjlgVM" target="_blank" rel="noopener noreferrer">build</a> and <a href="https://www.youtube.com/watch?v=vLIDHi-1PVU" target="_blank" rel="noopener noreferrer">design</a> AI agents. We've solved hard problems: how agents should manage memory across long-running tasks, how to handle permission systems that balance autonomy with user control, and how to coordinate subagents working toward a shared goal.</p><p>Now we’re making all of this available to you. The <a href="https://anthropic.com/engineering/building-agents-with-the-claude-agent-sdk" target="_blank" rel="noopener noreferrer">Claude Agent SDK</a> is the same infrastructure that powers Claude Code, but it shows impressive benefits for a very wide variety of tasks, not just coding. As of today, you can use it to build your own agents.</p><p>We built Claude Code because the tool we wanted didn’t exist yet. The Agent SDK gives you the same foundation to build something just as capable for whatever problem you're solving.</p><h2 id="bonus-research-preview">Bonus research preview</h2><p>We’re releasing a temporary research preview alongside Claude Sonnet 4.5, called "<a href="https://claude.ai/redirect/website.v1.f79c597c-8da8-4257-a3ef-0233ea5a1dd0/imagine" target="_blank" rel="noopener noreferrer">Imagine with Claude</a>".</p><p>In this experiment, Claude generates software on the fly. No functionality is predetermined; no code is prewritten. What you see is Claude creating in real time, responding and adapting to your requests as you interact.</p><p>It's a fun demonstration showing what Claude Sonnet 4.5 can do—a way to see what's possible when you combine a capable model with the right infrastructure.</p><p>"Imagine with Claude" is available to Max subscribers for the next five days. We encourage you to try it out on <a href="https://claude.ai/redirect/website.v1.f79c597c-8da8-4257-a3ef-0233ea5a1dd0/imagine" target="_blank" rel="noopener noreferrer">claude.ai/imagine</a>.</p><h2 id="further-information">Further information</h2><p>We recommend upgrading to Claude Sonnet 4.5 for all uses. Whether you’re using Claude through our apps, our API, or Claude Code, Sonnet 4.5 is a drop-in replacement that provides much improved performance for the same price. Claude Code updates are available to all users. <a href="https://claude.com/platform/api" target="_blank" rel="noopener noreferrer">Claude Developer Platform</a> updates, including the Claude Agent SDK, are available to all developers. Code execution and file creation are available on all paid plans in the Claude apps.</p><p>For complete technical details and evaluation results, see our <a href="https://www.anthropic.com/claude-sonnet-4-5-system-card" target="_blank" rel="noopener noreferrer">system card</a>, <a href="https://www.anthropic.com/claude/sonnet" target="_blank" rel="noopener noreferrer">model page</a>, and <a href="https://docs.claude.com/en/docs/about-claude/models/overview" target="_blank" rel="noopener noreferrer">documentation</a>. For more information, explore our <a href="https://www.anthropic.com/engineering/building-agents-with-the-claude-agent-sdk" target="_blank" rel="noopener noreferrer">engineering</a> <a href="https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents" target="_blank" rel="noopener noreferrer">posts</a> and research post on <a href="https://red.anthropic.com/2025/ai-for-cyber-defenders" target="_blank" rel="noopener noreferrer">cybersecurity</a>.</p></div></article></div><div><h4>Footnotes</h4><div><p><em>1<strong>: </strong>Customers in the cybersecurity and biological research industries can work with their account teams to join our allowlist in the meantime.</em></p><p><strong>Methodology</strong></p></div><ul><li><strong>SWE-bench Verified</strong>: All Claude results were reported using a simple scaffold with two tools—bash and file editing via string replacements. We report 77.2%, which was averaged over 10 trials, no test-time compute, and 200K thinking budget on the full 500-problem SWE-bench Verified dataset.<ul><li>The score reported uses a minor prompt addition: "You should use tools as much as possible, ideally more than 100 times. You should also implement your own tests first before attempting the problem."</li><li>A 1M context configuration achieves 78.2%, but we report the 200K result as our primary score as the 1M configuration was implicated in our recent <a href="https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues">inference issues</a>.</li><li>For our "high compute" numbers we adopt additional complexity and parallel test-time compute as follows:<ul><li>We sample multiple parallel attempts.</li><li>We discard patches that break the visible regression tests in the repository, similar to the rejection sampling approach adopted by <a href="https://arxiv.org/abs/2407.01489">Agentless</a> (Xia et al. 2024); note no hidden test information is used.</li><li>We then use an internal scoring model to select the best candidate from the remaining attempts.</li><li>This results in a score of 82.0% for Sonnet 4.5.</li></ul></li></ul></li><li><strong>Terminal-Bench</strong>: All scores reported use the default agent framework (Terminus 2), with XML parser, averaging multiple runs during different days to smooth the eval sensitivity to inference infrastructure.</li><li><strong>τ2-bench: </strong>Scores were achieved using extended thinking with tool use and a prompt addendum to the Airline and Telecom Agent Policy instructing Claude to better target its known failure modes when using the vanilla prompt. A prompt addendum was also added to the Telecom User prompt to avoid failure modes from the user ending the interaction incorrectly.</li><li><strong>AIME</strong>: Sonnet 4.5 score reported using sampling at temperature 1.0. The model used 64K reasoning tokens for the Python configuration.</li><li><strong>OSWorld: </strong>All scores reported use the official OSWorld-Verified framework with 100 max steps, averaged across 4 runs.</li><li><strong>MMMLU</strong>: All scores reported are the average of 5 runs over 14 non-English languages with extended thinking (up to 128K).</li><li><strong>Finance Agent</strong>: All scores reported were run and published by <a href="https://vals.ai/">Vals AI</a> on their public leaderboard. All Claude model results reported are with extended thinking (up to 64K) and Sonnet 4.5 is reported with interleaved thinking on.</li><li>All OpenAI scores reported from their <a href="https://openai.com/index/introducing-gpt-5/">GPT-5 post</a>, <a href="https://openai.com/index/introducing-gpt-5-for-developers/">GPT-5 for developers post</a>, <a href="https://cdn.openai.com/gpt-5-system-card.pdf">GPT-5 system card</a> (SWE-bench Verified reported using n=500), <a href="https://www.tbench.ai/">Terminal Bench leaderboard</a> (using Terminus 2), and public <a href="http://vals.ai/">Vals AI</a> leaderboard. All Gemini scores reported from their <a href="https://deepmind.google/models/gemini/pro/">model web page</a>, <a href="https://www.tbench.ai/">Terminal Bench leaderboard</a> (using Terminus 1), and public <a href="https://vals.ai/">Vals AI</a> leaderboard.</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Every single torrent is on this website (101 pts)]]></title>
            <link>https://infohash.lol/</link>
            <guid>45415539</guid>
            <pubDate>Mon, 29 Sep 2025 16:14:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://infohash.lol/">https://infohash.lol/</a>, See on <a href="https://news.ycombinator.com/item?id=45415539">Hacker News</a></p>
Couldn't get https://infohash.lol/: Error: getaddrinfo ENOTFOUND infohash.lol]]></description>
        </item>
        <item>
            <title><![CDATA[Write the Damn Code (172 pts)]]></title>
            <link>https://antonz.org/write-code/</link>
            <guid>45415232</guid>
            <pubDate>Mon, 29 Sep 2025 15:45:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://antonz.org/write-code/">https://antonz.org/write-code/</a>, See on <a href="https://news.ycombinator.com/item?id=45415232">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article data-namespace=""><div><header></header><p>Here's some popular programming advice these days:</p><blockquote><p>Learn to decompose problems into smaller chunks, be specific about what you want, pick the right AI model for the task, and <strong>iterate on your prompts</strong>.</p></blockquote><p>Don't do this.</p><p>I mean, "learn to decompose the problem" — sure. "Iterate on your prompts" — not so much. Write the actual code instead:</p><ul><li>Ask AI for an initial version and then refactor it to match your expectations.</li><li>Write the initial version yourself and ask AI to review and improve it.</li><li>Write the critical parts and ask AI to do the rest.</li><li>Write an outline of the code and ask AI to fill the missing parts.</li></ul><p>You probably see the pattern now. Get involved with the code, don't leave it all to AI.</p><p>If, given the prompt, AI does the job perfectly on first or second iteration — fine. Otherwise, stop refining the prompt. Go write some code, then get back to the AI. You'll get much better results.</p><p>Don't get me wrong: this is not anti-AI advice. Use it, by all means. Use it a lot if you want to. But don't fall into the trap of endless back-and-forth prompt refinement, trying to get the perfect result from AI by "programming in English". It's an imprecise, slow and terribly painful way to get things done.</p><p>Get your hands dirty. Write the code. It's what you are good at.</p><p>You are a software engineer. Don't become a prompt refiner.</p><p><a href="https://antonz.org/subscribe/">★&nbsp;Subscribe</a> to keep up with new posts.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Loadmo.re: design inspiration for unconventional web (240 pts)]]></title>
            <link>https://loadmo.re</link>
            <guid>45415207</guid>
            <pubDate>Mon, 29 Sep 2025 15:42:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://loadmo.re">https://loadmo.re</a>, See on <a href="https://news.ycombinator.com/item?id=45415207">Hacker News</a></p>
Couldn't get https://loadmo.re: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Electronic Arts Goes Private for $52.5B in Largest LBO (105 pts)]]></title>
            <link>https://www.wsj.com/business/deals/electronic-arts-to-go-private-in-55-billion-deal-a4a4479c</link>
            <guid>45413767</guid>
            <pubDate>Mon, 29 Sep 2025 13:47:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/business/deals/electronic-arts-to-go-private-in-55-billion-deal-a4a4479c">https://www.wsj.com/business/deals/electronic-arts-to-go-private-in-55-billion-deal-a4a4479c</a>, See on <a href="https://news.ycombinator.com/item?id=45413767">Hacker News</a></p>
Couldn't get https://www.wsj.com/business/deals/electronic-arts-to-go-private-in-55-billion-deal-a4a4479c: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Why friction is necessary for growth (144 pts)]]></title>
            <link>https://jameelur.com/blog/overcoming-friction-leads-to-growth</link>
            <guid>45413654</guid>
            <pubDate>Mon, 29 Sep 2025 13:39:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jameelur.com/blog/overcoming-friction-leads-to-growth">https://jameelur.com/blog/overcoming-friction-leads-to-growth</a>, See on <a href="https://news.ycombinator.com/item?id=45413654">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-mdx-content="true"><p><img alt="" loading="lazy" width="1200" height="627" decoding="async" data-nimg="1" srcset="https://jameelur.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ffriction-growth.dcafe3c3.png&amp;w=1200&amp;q=75 1x, https://jameelur.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ffriction-growth.dcafe3c3.png&amp;w=3840&amp;q=75 2x" src="https://jameelur.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ffriction-growth.dcafe3c3.png&amp;w=3840&amp;q=75"></p><p>The title of this article says it all. Overcoming friction leads to growth. Comfort leads to stagnation.</p>
<p>ChatGPT and by extension “AI” is likely the biggest “revolution” of my generation. It is likely also going to be the biggest killer of creativity in my generation. I always thought the creativity killer was going to be access to infinite entertainment. I think I was wrong.</p>
<p>I’ve come to believe that with the rise of convenience and comfort, it becomes harder for us to reach our potential. Technology and Capitalism is taking us towards an extreme.</p>
<p>A certain level of convenience can lead to efficiency gains. Automation is important for a reason. Too much convenience though, that's a killer. When friction was inherent in the system, applying ourselves led to growth as we overcame that friction. We simply didn’t have an alternative that was viable. And this principle applies to everything.</p>
<p>When I was a child in Sri Lanka, I ended up memorizing the landline numbers of all my close relatives. To this day I remember them. The moment I got a phone where my contacts could be saved, I stopped remembering numbers. It may seem like a small thing but it illustrates the principle. The ease of access to information has geared us towards efficiently looking up information instead of remembering it. I won't argue the utility of having hundreds of numbers saved on your phone, I simply want to make a point. Overcoming friction leads to growth.</p>
<p>Let's take another activity where creativity is important, writing. When it's easier to prompt ChatGPT to write your college essay, you'll never apply yourself. Afterall, when everyone is doing it, why not you? As everyone uses ChatGPT, the expectation of high quality writing will increase, making it harder for people to be vulnerable. You can’t become a master without making mistakes and learning from it.</p>
<p>Humans are creatures of comfort. Just like so many things in this world, we follow the path of least resistance. With access to technology being ubiquitous, and ChatGPT being so widely available, to choose not to use it is very hard. You need to deliberately prioritize your growth and choose to go against the current. You need to deliberately introduce friction to the process.</p>
<p>That said, total abstinence is not the solution. ChatGPT is here to stay. Just like most advancements in technology are. As a child of the 21st Century, you’ll need to learn to utilize this new tool in a manner that aids you, not hinders you. More importantly, not hinder the <strong>future</strong> you.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta-analysis of 2.2M people: Loneliness increases mortality risk by 32% (326 pts)]]></title>
            <link>https://lightcapai.medium.com/the-loneliness-epidemic-threatens-physical-health-like-smoking-e063220dde8b</link>
            <guid>45413481</guid>
            <pubDate>Mon, 29 Sep 2025 13:25:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lightcapai.medium.com/the-loneliness-epidemic-threatens-physical-health-like-smoking-e063220dde8b">https://lightcapai.medium.com/the-loneliness-epidemic-threatens-physical-health-like-smoking-e063220dde8b</a>, See on <a href="https://news.ycombinator.com/item?id=45413481">Hacker News</a></p>
Couldn't get https://lightcapai.medium.com/the-loneliness-epidemic-threatens-physical-health-like-smoking-e063220dde8b: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Larry Ellison – 'citizens will be on their best behavior' amid nonstop recording (184 pts)]]></title>
            <link>https://fortune.com/2025/09/28/larry-ellison-ai-surveillance-oracle-tiktok-deal-social-media/</link>
            <guid>45413090</guid>
            <pubDate>Mon, 29 Sep 2025 12:51:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fortune.com/2025/09/28/larry-ellison-ai-surveillance-oracle-tiktok-deal-social-media/">https://fortune.com/2025/09/28/larry-ellison-ai-surveillance-oracle-tiktok-deal-social-media/</a>, See on <a href="https://news.ycombinator.com/item?id=45413090">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-cy="article-content" id="article-content"><p>A year ago, <a href="https://fortune.com/company/oracle/" target="_blank" aria-label="Go to https://fortune.com/company/oracle/">Oracle</a> cofounder and Chairman Larry Ellison described a future where everyone, including law enforcement, <a href="https://fortune.com/2024/09/17/oracle-larry-ellison-surveillance-state-police-ai/" target="_blank" rel="noreferrer noopener" aria-label="Go to https://fortune.com/2024/09/17/oracle-larry-ellison-surveillance-state-police-ai/" data-type="link" data-id="https://fortune.com/2024/09/17/oracle-larry-ellison-surveillance-state-police-ai/">will face regular surveillance</a> as daily life is documented seemingly nonstop.</p><div>



<p>At Oracle’s&nbsp;<a href="https://www.oracle.com/events/financial-analyst-meeting-2024/" target="_blank" rel="noreferrer noopener" aria-label="Go to https://www.oracle.com/events/financial-analyst-meeting-2024/">financial analyst meeting</a>&nbsp;last September, he predicted artificial intelligence will help process the vast amounts of footage recorded by cameras placed on everything from car dashboards and front doors to security systems and cops.



</p><p>“We’re going to have supervision,” Ellison said. “Every police officer is going to be supervised at all times, and if there’s a problem, AI will report that problem and report it to the appropriate person. Citizens will be on their best behavior because we are constantly recording and reporting everything that’s going on.”



</p><p>Those comments have gained fresh relevance now that his company has emerged as a major player in the AI industry and is poised to play a critical role in the deal for TikTok’s U.S. operations. TikTok’s video-sharing platform is among the most popular social media properties in the country.



</p><p>Oracle didn’t immediately respond to a request for comment.</p><p>The company has been an AI infrastructure provider and stunned Wall Street earlier this month by reaching a <a href="https://www.wsj.com/business/openai-oracle-sign-300-billion-computing-deal-among-biggest-in-history-ff27c8fe?st=vUBGuq&amp;reflink=desktopwebshare_permalink" target="_blank" rel="noreferrer noopener" aria-label="Go to https://www.wsj.com/business/openai-oracle-sign-300-billion-computing-deal-among-biggest-in-history-ff27c8fe?st=vUBGuq&amp;reflink=desktopwebshare_permalink" data-type="link" data-id="https://www.wsj.com/business/openai-oracle-sign-300-billion-computing-deal-among-biggest-in-history-ff27c8fe?st=vUBGuq&amp;reflink=desktopwebshare_permalink">$300 billion deal with OpenAI</a>, which will purchase computing power over about five years in one of the largest cloud contracts ever signed. 



</p><p>And earlier this week, OpenAI signed deals with SoftBank and Oracle&nbsp;for new data centers as part of the massive Stargate Project.



</p><p>In its most recent quarterly earnings call with analysts, management also offered&nbsp;revenue projections that cited $455 billion in contracts, up 359% from a year earlier.&nbsp;CEO Safra Catz&nbsp;revealed that Oracle landed deals with three different customers during the quarter.



</p><p>Meanwhile, <a href="https://fortune.com/how-we-test-supplements/" target="_blank" rel="noreferrer noopener" aria-label="Go to https://fortune.com/how-we-test-supplements/" data-type="page" data-id="4319596">Oracle is expected to be among the companies</a> that will buy TikTok’s U.S. business from Chinese parent company ByteDance. In addition, Oracle&nbsp;will spearhead U.S. oversight of the algorithm and security.



</p><p>President Donald Trump on Thursday afternoon signed an executive order clearing the way for a deal to put TikTok in U.S. hands. The <a href="https://fortune.com/2025/09/25/trumps-billionaire-backers-tiktok-deal-murdochs/" target="_blank" rel="noreferrer noopener" aria-label="Go to https://fortune.com/2025/09/25/trumps-billionaire-backers-tiktok-deal-murdochs/" data-type="link" data-id="https://fortune.com/2025/09/25/trumps-billionaire-backers-tiktok-deal-murdochs/">U.S. ownership structure is still being finalized</a>, but Trump said&nbsp;Oracle&nbsp;and Ellison would play a “big” role in managing the app, while conservative media mogul Rupert Murdoch and computer billionaire Michael Dell would sit on the board. Trump hinted that three more “blue-chip” backers are also part of the ownership group.</p><p>Vice President JD Vance said that the algorithm will be “under the control of American investors,” adding that more details would be forthcoming.&nbsp;



</p><p><a href="https://finance.yahoo.com/news/tiktok-algorithm-secured-oracle-trump-100000512.html" target="_blank" rel="noreferrer noopener" aria-label="Go to https://finance.yahoo.com/news/tiktok-algorithm-secured-oracle-trump-100000512.html">Reports earlier this week</a>&nbsp;said&nbsp;Oracle&nbsp;will re-create TikTok’s algorithm and provide a new U.S. version while also ensuring security for users’ data.



</p><p>“This deal will allow for the U.S. to control the app’s algorithm,” Vance said. “It’s actually going to be American-operated all the way.”
</p></div><p><strong>Fortune Global Forum</strong> returns Oct. 26–27, 2025 in Riyadh. CEOs and global leaders will gather for a dynamic, invitation-only event shaping the future of business. <a href="https://conferences.fortune.com/event/global-forum-2025/summary?utm_source=fortunecom&amp;utm_medium=plealink" target="_self" aria-label="Go to https://conferences.fortune.com/event/global-forum-2025/summary?utm_source=fortunecom&amp;utm_medium=plealink">Apply for an invitation.</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EA Announces Agreement to be Acquired by PIF, Silver Lake, and Affinity Partners (243 pts)]]></title>
            <link>https://ir.ea.com/press-releases/press-release-details/2025/EA-Announces-Agreement-to-be-Acquired-by-PIF-Silver-Lake-and-Affinity-Partners-for-55-Billion/default.aspx</link>
            <guid>45413083</guid>
            <pubDate>Mon, 29 Sep 2025 12:50:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ir.ea.com/press-releases/press-release-details/2025/EA-Announces-Agreement-to-be-Acquired-by-PIF-Silver-Lake-and-Affinity-Partners-for-55-Billion/default.aspx">https://ir.ea.com/press-releases/press-release-details/2025/EA-Announces-Agreement-to-be-Acquired-by-PIF-Silver-Lake-and-Affinity-Partners-for-55-Billion/default.aspx</a>, See on <a href="https://news.ycombinator.com/item?id=45413083">Hacker News</a></p>
Couldn't get https://ir.ea.com/press-releases/press-release-details/2025/EA-Announces-Agreement-to-be-Acquired-by-PIF-Silver-Lake-and-Affinity-Partners-for-55-Billion/default.aspx: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[What if I don't want videos of my hobby time available to the world? (600 pts)]]></title>
            <link>https://neilzone.co.uk/2025/09/what-if-i-dont-want-videos-of-my-hobby-time-available-to-the-entire-world/</link>
            <guid>45412419</guid>
            <pubDate>Mon, 29 Sep 2025 11:28:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://neilzone.co.uk/2025/09/what-if-i-dont-want-videos-of-my-hobby-time-available-to-the-entire-world/">https://neilzone.co.uk/2025/09/what-if-i-dont-want-videos-of-my-hobby-time-available-to-the-entire-world/</a>, See on <a href="https://news.ycombinator.com/item?id=45412419">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<article>
    <p>I am very much enjoying my newly-resurrected hobby of Airsoft.</p>
<p>Running around in the woods, firing small plastic pellets at other people, in pursuit of a contrived-to-be-fun mission, turns out to be, well, fun.</p>
<p>I have also had to accept that, for some other players, part of that fun comes from making videos of their game days, and uploading them to YouTube.</p>
<p>They often have quite impressive setups, with multiple cameras - head, rear-facing from barrel of weapon, and scope cam - and clearly put time, money, and effort into doing this.</p>
<p>Great! Just like someone taking photos on their holidays, or when out and about, I can see the fun in it.</p>
<p>It is the “non-consensually publishing it online for the world to see” aspect which bugs me a bit.</p>
<p>In the handful of games that I have played, no-one has ever asked about consent of other participants.</p>
<p>There has been no “put on this purple lanyard if you don’t want to be included in the public version of the video” rule, which I’ve seen work pretty well at conferences I have attended (even if it is opt-out rather than consent).</p>
<p>I could, I suppose, ask each person that I see with a camera “would you mind not including me in anything you upload, please?”. And, since everyone with whom I’ve spoken at games, so far anyway, has been perfectly pleasant and friendly, I’d be hopeful that they would at least consider my request. I have not done this.</p>
<p>The impression I get is that this is just seen as part and parcel of the hobby: by running around in the woods of northern Newbury on a Sunday morning, I need to accept that I may well appear on YouTube, for the world to see.</p>
<p>I don’t love it, but it is not a big enough deal for me to make a fuss.</p>
<h2 id="other-notes">Other notes</h2>
<p>I occasionally see people saying “well, if you don’t want to be in photos published online, don’t be in public spaces”.</p>
<p>This is nonsense, for a number of reasons. Clearly, one should be able to exist in society, including going outside one’s own home, without needing to accept this kind of thing.</p>
<p>In any case, here, the issue is somewhat different, since it is a private site, where people engage in private activity (a hobby).</p>
<p>But then I’ve seen the same at (private) conferences, with people saying “Of course I’m free to take photos of identifiable individuals without their consent and publish them online”.</p>
<p>Publishing someone’s photo online, without their consent, without another strong justification, just because they happen to be in view of one’s camera lens, feels wrong to me.</p>
<p>This isn’t about what is legal (although, in some cases, claims of legality may be poorly conceived), but around my own perceptions of a private life, and a dislike for the fact that, just because one <em>can</em> publish such things, that one <em>should</em>.</p>



  <div>
	<hr>   
	<h2>You may also like:</h2>
	
    
</div>


</article>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek-v3.2-Exp (284 pts)]]></title>
            <link>https://github.com/deepseek-ai/DeepSeek-V3.2-Exp</link>
            <guid>45412098</guid>
            <pubDate>Mon, 29 Sep 2025 10:26:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp">https://github.com/deepseek-ai/DeepSeek-V3.2-Exp</a>, See on <a href="https://news.ycombinator.com/item?id=45412098">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">DeepSeek-V3.2-Exp</h2><a id="user-content-deepseek-v32-exp" aria-label="Permalink: DeepSeek-V3.2-Exp" href="#deepseek-v32-exp"></a></p>



<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true"><img src="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3"></a>
</p>
<hr>
<p><a href="https://www.deepseek.com/" rel="nofollow">
    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/badge.svg?raw=true">
  </a>
  <a href="https://chat.deepseek.com/" rel="nofollow">
    <img alt="Chat" src="https://camo.githubusercontent.com/00ba04aacbe45f97b5ebcc3d1b9c0f546e0ce3981265e97a110994184ef67fc8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa496253230436861742d446565705365656b25323056332d3533366166353f636f6c6f723d353336616635266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&amp;logoColor=white">
  </a>
  <a href="https://huggingface.co/deepseek-ai" rel="nofollow">
    <img alt="Hugging Face" src="https://camo.githubusercontent.com/5e3115539d4583e22d65cb89eb1759e767cb9e1d70772923292fcfc80a654be4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d446565705365656b25323041492d6666633130373f636f6c6f723d666663313037266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;logoColor=white">
  </a>
</p>
<p><a href="https://discord.gg/Tc7c45Zzu5" rel="nofollow">
    <img alt="Discord" src="https://camo.githubusercontent.com/e227481a149714ed5187e4fd0b60b9f736099c2dd2083e6c091e29f1446cbb1a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d446565705365656b25323041492d3732383964613f6c6f676f3d646973636f7264266c6f676f436f6c6f723d776869746526636f6c6f723d373238396461" data-canonical-src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;logoColor=white&amp;color=7289da">
  </a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true">
    <img alt="Wechat" src="https://camo.githubusercontent.com/562efc618da65f0a69bc804395005b8124f5c2ed2eb73441c4e359185cc01467/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5765436861742d446565705365656b25323041492d627269676874677265656e3f6c6f676f3d776563686174266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;logoColor=white">
  </a>
  <a href="https://twitter.com/deepseek_ai" rel="nofollow">
    <img alt="Twitter Follow" src="https://camo.githubusercontent.com/8272710ecd020c821b4f62c1c455efb89e0db4eb179c5f5f971c3c1f69452c54/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f547769747465722d646565707365656b5f61692d77686974653f6c6f676f3d78266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;logoColor=white">
  </a>
</p>
<p><a href="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/LICENSE">
    <img alt="License" src="https://camo.githubusercontent.com/1af067540f64107f8fe7715d12150fb910e21f0d2c6aa0c319087c7510c8b934/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d6635646535333f26636f6c6f723d663564653533" data-canonical-src="https://img.shields.io/badge/License-MIT-f5de53?&amp;color=f5de53">
  </a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto">We are excited to announce the official release of DeepSeek-V3.2-Exp, an experimental version of our model. As an intermediate step toward our next-generation architecture, V3.2-Exp builds upon V3.1-Terminus by introducing DeepSeek Sparse Attention—a sparse attention mechanism designed to explore and validate optimizations for training and inference efficiency in long-context scenarios.</p>
<p dir="auto">This experimental release represents our ongoing research into more efficient transformer architectures, particularly focusing on improving computational efficiency when processing extended text sequences.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/cost.jpg"><img src="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/raw/main/cost.jpg"></a>
</p>
<ul dir="auto">
<li>
<p dir="auto">DeepSeek Sparse Attention (DSA) achieves fine-grained sparse attention for the first time, delivering substantial improvements in long-context training and inference efficiency while maintaining virtually identical model output quality.</p>
</li>
<li>
<p dir="auto">To rigorously evaluate the impact of introducing sparse attention, we deliberately aligned the training configurations of DeepSeek-V3.2-Exp with V3.1-Terminus. Across public benchmarks in various domains, DeepSeek-V3.2-Exp demonstrates performance on par with V3.1-Terminus.</p>
</li>
</ul>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Benchmark</th>
<th>DeepSeek-V3.1-Terminus</th>
<th>DeepSeek-V3.2-Exp</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Reasoning Mode w/o Tool Use</strong></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MMLU-Pro</td>
<td>85.0</td>
<td>85.0</td>
</tr>
<tr>
<td>GPQA-Diamond</td>
<td>80.7</td>
<td>79.9</td>
</tr>
<tr>
<td>Humanity's Last Exam</td>
<td>21.7</td>
<td>19.8</td>
</tr>
<tr>
<td>LiveCodeBench</td>
<td>74.9</td>
<td>74.1</td>
</tr>
<tr>
<td>AIME 2025</td>
<td>88.4</td>
<td>89.3</td>
</tr>
<tr>
<td>HMMT 2025</td>
<td>86.1</td>
<td>83.6</td>
</tr>
<tr>
<td>Codeforces</td>
<td>2046</td>
<td>2121</td>
</tr>
<tr>
<td>Aider-Polyglot</td>
<td>76.1</td>
<td>74.5</td>
</tr>
<tr>
<td><strong>Agentic Tool Use</strong></td>
<td></td>
<td></td>
</tr>
<tr>
<td>BrowseComp</td>
<td>38.5</td>
<td>40.1</td>
</tr>
<tr>
<td>BrowseComp-zh</td>
<td>45.0</td>
<td>47.9</td>
</tr>
<tr>
<td>SimpleQA</td>
<td>96.8</td>
<td>97.1</td>
</tr>
<tr>
<td>SWE Verified</td>
<td>68.4</td>
<td>67.8</td>
</tr>
<tr>
<td>SWE-bench Multilingual</td>
<td>57.8</td>
<td>57.9</td>
</tr>
<tr>
<td>Terminal-bench</td>
<td>36.7</td>
<td>37.7</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Open-Source Kernels</h2><a id="user-content-open-source-kernels" aria-label="Permalink: Open-Source Kernels" href="#open-source-kernels"></a></p>
<p dir="auto">For TileLang kernels with <strong>better readability and research-purpose design</strong>, please refer to <a href="https://github.com/tile-ai/tilelang/tree/main/examples/deepseek-v32">TileLang</a>.</p>
<p dir="auto">For <strong>high-performance CUDA kernels</strong>, indexer logit kernels (including paged versions) are available in <a href="https://github.com/deepseek-ai/DeepGEMM/pull/200" data-hovercard-type="pull_request" data-hovercard-url="/deepseek-ai/DeepGEMM/pull/200/hovercard">DeepGEMM</a>. Sparse attention kernels are released in <a href="https://github.com/deepseek-ai/FlashMLA/pull/98" data-hovercard-type="pull_request" data-hovercard-url="/deepseek-ai/FlashMLA/pull/98/hovercard">FlashMLA</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to Run Locally</h2><a id="user-content-how-to-run-locally" aria-label="Permalink: How to Run Locally" href="#how-to-run-locally"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">HuggingFace</h3><a id="user-content-huggingface" aria-label="Permalink: HuggingFace" href="#huggingface"></a></p>
<p dir="auto">We provide an updated inference demo code in the <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/tree/main/inference" rel="nofollow">inference</a> folder to help the community quickly get started with our model and understand its architectural details.</p>
<p dir="auto">First convert huggingface model weights to the the format required by our inference demo. Set <code>MP</code> to match your available GPU count:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd inference
export EXPERTS=256
python convert.py --hf-ckpt-path ${HF_CKPT_PATH} --save-path ${SAVE_PATH} --n-experts ${EXPERTS} --model-parallel ${MP}"><pre><span>cd</span> inference
<span>export</span> EXPERTS=256
python convert.py --hf-ckpt-path <span>${HF_CKPT_PATH}</span> --save-path <span>${SAVE_PATH}</span> --n-experts <span>${EXPERTS}</span> --model-parallel <span>${MP}</span></pre></div>
<p dir="auto">Launch the interactive chat interface and start exploring DeepSeek's capabilities:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export CONFIG=config_671B_v3.2.json
torchrun --nproc-per-node ${MP} generate.py --ckpt-path ${SAVE_PATH} --config ${CONFIG} --interactive"><pre><span>export</span> CONFIG=config_671B_v3.2.json
torchrun --nproc-per-node <span>${MP}</span> generate.py --ckpt-path <span>${SAVE_PATH}</span> --config <span>${CONFIG}</span> --interactive</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">SGLang</h3><a id="user-content-sglang" aria-label="Permalink: SGLang" href="#sglang"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Installation with Docker</h4><a id="user-content-installation-with-docker" aria-label="Permalink: Installation with Docker" href="#installation-with-docker"></a></p>
<div data-snippet-clipboard-copy-content="# H200
docker pull lmsysorg/sglang:dsv32

# MI350
docker pull lmsysorg/sglang:dsv32-rocm

# NPUs
docker pull lmsysorg/sglang:dsv32-a2
docker pull lmsysorg/sglang:dsv32-a3"><pre><code># H200
docker pull lmsysorg/sglang:dsv32

# MI350
docker pull lmsysorg/sglang:dsv32-rocm

# NPUs
docker pull lmsysorg/sglang:dsv32-a2
docker pull lmsysorg/sglang:dsv32-a3
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Launch Command</h4><a id="user-content-launch-command" aria-label="Permalink: Launch Command" href="#launch-command"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m sglang.launch_server --model deepseek-ai/DeepSeek-V3.2-Exp --tp 8 --dp 8 --page-size 64"><pre>python -m sglang.launch_server --model deepseek-ai/DeepSeek-V3.2-Exp --tp 8 --dp 8 --page-size 64</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">vLLM</h3><a id="user-content-vllm" aria-label="Permalink: vLLM" href="#vllm"></a></p>
<p dir="auto">vLLM provides day-0 support of DeepSeek-V3.2-Exp. See the <a href="https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-V3_2-Exp.html" rel="nofollow">recipes</a> for up-to-date details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This repository and the model weights are licensed under the <a href="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/LICENSE">MIT License</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<div data-snippet-clipboard-copy-content="@misc{deepseekai2024deepseekv32,
      title={DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention}, 
      author={DeepSeek-AI},
      year={2025},
}"><pre><code>@misc{deepseekai2024deepseekv32,
      title={DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention}, 
      author={DeepSeek-AI},
      year={2025},
}
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contact</h2><a id="user-content-contact" aria-label="Permalink: Contact" href="#contact"></a></p>
<p dir="auto">If you have any questions, please raise an issue or contact us at <a href="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/service@deepseek.com">service@deepseek.com</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Optimizing a 6502 image decoder, from 70 minutes to 1 minute (158 pts)]]></title>
            <link>https://www.colino.net/wordpress/en/archives/2025/09/28/optimizing-a-6502-image-decoder-from-70-minutes-to-1-minute/</link>
            <guid>45412022</guid>
            <pubDate>Mon, 29 Sep 2025 10:11:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.colino.net/wordpress/en/archives/2025/09/28/optimizing-a-6502-image-decoder-from-70-minutes-to-1-minute/">https://www.colino.net/wordpress/en/archives/2025/09/28/optimizing-a-6502-image-decoder-from-70-minutes-to-1-minute/</a>, See on <a href="https://news.ycombinator.com/item?id=45412022">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<p><span><i></i>2025/09/28</span>
						<span> - </span>
						<span><i></i>About 8 minutes read</span>
					</p>
					
				</div><div>	
						
<p>When I set out to write a program that would allow me to do basic digital photography on the Apple II, I decided I would do it with the Quicktake cameras. It seemed the obvious choice as they were Apple cameras, and their interface to the computer is via serial port.</p>



<p>The scope creeped a bit after managing to decode Quicktake 100 photos. I wanted it to be able to decode Quicktake 150 and Quicktake 200 pictures too. This threw me into image processing much more than I initially wanted to. This article explains the process of how I got the Quicktake 150 decoder to a reasonabl-ish speed on a 6502 at 1MHz.</p>



<p>The Quicktake 150 format is proprietary and undocumented. Free software decoders exist though, in the dcraw project. This was my source for the initial Apple II decoder. Sadly, it is written in C, is extremely non-documented, and is extremely hard to read and understand (to me). The compression is based on Huffman coding, with variable-length codes (which means bit-shifting), and the image construction involves a lot of 16-bits math. None of this is good on a 6502.</p>



<p>But first I had to rework the original algorithm to work with bands of 20 pixels, for memory reasons. Once I had a functional decoder, it ran perfectly, but it took… seventy minutes to decode a single picture.</p>



<figure><a href="https://www.colino.net/wordpress/wp-content/uploads/QT150_24.png"><img fetchpriority="high" decoding="async" width="640" height="480" src="https://www.colino.net/wordpress/wp-content/uploads/QT150_24.png" alt="" srcset="https://www.colino.net/wordpress/wp-content/uploads/QT150_24.png 640w, https://www.colino.net/wordpress/wp-content/uploads/QT150_24-400x300.png 400w, https://www.colino.net/wordpress/wp-content/uploads/QT150_24-150x113.png 150w" sizes="(max-width: 640px) 100vw, 640px"></a><figcaption>A Quicktake 150 picture, fully decoded by dcraw</figcaption></figure>



<figure><a href="https://www.colino.net/wordpress/wp-content/uploads/TEST150.DGHR_.png"><img decoding="async" width="560" height="384" src="https://www.colino.net/wordpress/wp-content/uploads/TEST150.DGHR_.png" alt="" srcset="https://www.colino.net/wordpress/wp-content/uploads/TEST150.DGHR_.png 560w, https://www.colino.net/wordpress/wp-content/uploads/TEST150.DGHR_-400x274.png 400w, https://www.colino.net/wordpress/wp-content/uploads/TEST150.DGHR_-150x103.png 150w" sizes="(max-width: 560px) 100vw, 560px"></a><figcaption>The same picture, dithered for display on a monochrome Apple II</figcaption></figure>



<p>Of course, I didn’t get there that fast. My first implementation was released two years ago, in November 2023. Getting where I’m now took, I think, five or six deep dives with each time, one or two weeks worth of late evenings and full week-ends dedicated to progressing, wading through hundreds or thousands of debug printf()s, gdb’ing, variables and offsets comparisons, etc.</p>



<p>Follow me through the algorithmic iterations that allowed me to get that decoding time to under one minute. My implementation is now full assembly, but the commits I will link to here are to the general decoding algorithm, that is easier to read in C. </p>



<p>I have noticed that hand-optimizing assembler yields good results, but usually optimizing the algorithm itself leads to much more impressive speed gains. Doing <strong>too many things</strong> faster is not as good as doing <strong>the minimum</strong> faster. And that Quicktake 150 decoder sure did useless things, especially in my case where I don’t care about color and end up with a 256×192 image!</p>



<p>I have made a specific repository to track these algorithmic changes. It started <a href="https://github.com/colinleroy/qtkn_decoder/commit/ceef883b4528a01658f446f9fde233d8846b98e0#diff-538608780127f20219d177da9582dffbd84063d74807cd8980d62ee09f7689a3" data-type="link" data-id="https://github.com/colinleroy/qtkn_decoder/commit/ceef883b4528a01658f446f9fde233d8846b98e0#diff-538608780127f20219d177da9582dffbd84063d74807cd8980d62ee09f7689a3">here</a> (already a little bit deobfuscated <a href="https://github.com/ncruces/dcraw/blob/8fe4d3825595816ea7d6880d9253f9edd143cacb/dcraw.c#L2198">compared to dcraw</a>), at 301 millions x86_64 instructions.</p>



<p><strong>Dropping color</strong></p>



<p>I didn’t have to decode color at all, as I was going to drop it, anyway. <a href="https://github.com/colinleroy/qtkn_decoder/commit/615ecd7406a65dd43da07443cb5c3f93a8d5fa95#diff-538608780127f20219d177da9582dffbd84063d74807cd8980d62ee09f7689a3R217">I added a flag to only decode the green pixels out of the Bayer matrix, and drop the rest</a>. 264M instructions.</p>



<p><strong>Understanding the buffers</strong></p>



<p>I then set out to understand the use of the various temporary buffers: the more buffers, the more intermediary steps, the more copy and looping. I wanted to drop as much of them as possible. <a href="https://github.com/colinleroy/qtkn_decoder/commit/fd6cd2c44d1cc961f8827ec6b5958521543e6d8d#diff-538608780127f20219d177da9582dffbd84063d74807cd8980d62ee09f7689a3L244">The first step towards it was unrolling some little imbricated loops that worked on y [1,2], x [col+1,col].</a> 238M instructions.</p>



<p>I figured I still had extra processing I didn’t need, <a href="https://github.com/colinleroy/qtkn_decoder/commit/73935481468c2a1d20a4640352caed4e47308b54#diff-538608780127f20219d177da9582dffbd84063d74807cd8980d62ee09f7689a3">removed it, dropped a buffer</a> (and dropped the #ifdef COLOR conditional to make things clearer). 193M instructions.</p>



<figure><a href="https://www.colino.net/wordpress/wp-content/uploads/image-green.png"><img decoding="async" width="640" height="480" src="https://www.colino.net/wordpress/wp-content/uploads/image-green.png" alt="" srcset="https://www.colino.net/wordpress/wp-content/uploads/image-green.png 640w, https://www.colino.net/wordpress/wp-content/uploads/image-green-400x300.png 400w, https://www.colino.net/wordpress/wp-content/uploads/image-green-150x113.png 150w" sizes="(max-width: 640px) 100vw, 640px"></a><figcaption>Our image looks like this now</figcaption></figure>



<p>At that point, my implementation still outputted green pixels only in a Bayer matrix to a 640×480 buffer, and then interpolated them. It was useless, so <a href="https://github.com/colinleroy/qtkn_decoder/commit/75a45d8fdf6943cc26a56c3f53c21f57e2877bbf#diff-538608780127f20219d177da9582dffbd84063d74807cd8980d62ee09f7689a3L263" data-type="link" data-id="https://github.com/colinleroy/qtkn_decoder/commit/75a45d8fdf6943cc26a56c3f53c21f57e2877bbf#diff-3564ad20c962b9eec56839b1adac221e589cda13b85547ac3d409396e9b6a28c">I dropped that entirely</a>. 29M instructions.</p>



<figure><a href="https://www.colino.net/wordpress/wp-content/uploads/out-grayscale-large.png"><img loading="lazy" decoding="async" width="640" height="480" src="https://www.colino.net/wordpress/wp-content/uploads/out-grayscale-large.png" alt="" srcset="https://www.colino.net/wordpress/wp-content/uploads/out-grayscale-large.png 640w, https://www.colino.net/wordpress/wp-content/uploads/out-grayscale-large-400x300.png 400w, https://www.colino.net/wordpress/wp-content/uploads/out-grayscale-large-150x113.png 150w" sizes="auto, (max-width: 640px) 100vw, 640px"></a><figcaption>Without interpolating, we can clearly see we only have half the pixels.</figcaption></figure>



<p>I still had half the pixels black in the destination buffer, so I dropped them earlier rather than later, by <a href="https://github.com/colinleroy/qtkn_decoder/commit/fdad1e8efb45e7597659b6c7d08e0a7714094d59#diff-538608780127f20219d177da9582dffbd84063d74807cd8980d62ee09f7689a3L130" data-type="link" data-id="https://github.com/colinleroy/qtkn_decoder/commit/fdad1e8efb45e7597659b6c7d08e0a7714094d59">outputting a 320×240 images with only the relevant pixels</a>. 25M instructions.</p>



<figure><a href="https://www.colino.net/wordpress/wp-content/uploads/image-119.png"><img loading="lazy" decoding="async" width="640" height="480" src="https://www.colino.net/wordpress/wp-content/uploads/image-119.png" alt="" srcset="https://www.colino.net/wordpress/wp-content/uploads/image-119.png 640w, https://www.colino.net/wordpress/wp-content/uploads/image-119-400x300.png 400w, https://www.colino.net/wordpress/wp-content/uploads/image-119-150x113.png 150w" sizes="auto, (max-width: 640px) 100vw, 640px"></a><figcaption>The 8-bits grayscale buffer at that point</figcaption></figure>



<p>At this point I was able to figure out that out of the three buf_m[3], only <a href="https://github.com/colinleroy/qtkn_decoder/commit/b3ac5d70edf0e836aa4b784fc06093d2606f7747">buf_m[1] was used to construct the picture</a>, that <a href="https://github.com/colinleroy/qtkn_decoder/commit/8b6a1c65011645c810f1a9fbb7d9eb56fe3e702e">buf_m[2] was only used to feed back into buf_m[0] at the start of a row</a>, that I could <a href="https://github.com/colinleroy/qtkn_decoder/commit/b44aac77d966c292f0762165cdf5b0818a8a0f91">construct the image from the buf_m[1] values on the fly instead of doing an extra loop on it</a>, and that <a href="https://github.com/colinleroy/qtkn_decoder/commit/4142412bda1388b6fadb5d343de07605c1e93ae8">I could entirely drop it too</a>. This allowed me to <a href="https://github.com/colinleroy/qtkn_decoder/commit/5161e9e33e5c3f064b0a3c0e5d29e6e7ece4280f" data-type="link" data-id="https://github.com/colinleroy/qtkn_decoder/commit/5161e9e33e5c3f064b0a3c0e5d29e6e7ece4280f">rename the last remaining buffer for more clarity</a>. 22M instructions. </p>



<p><strong>Optimizing divisions</strong></p>



<p>That was about it for the buffers. The rework of the code, at that point, made clear that every final pixel value was computed by dividing the 16-bits values computed from the image data by a given factor, and that this factor changes at most once every two rows. The result of that division was then clamped to [0-255]. This allowed me to <a href="https://github.com/colinleroy/qtkn_decoder/commit/865ca287f15bfccbca909b464b5939d8c4368f5a">precompute a division table every two rows, storing the final result, pre-clamped, in a simple array.</a> This also came with a bit of non-visible precision loss. On an x86_64, still 22M instructions, but on 6502, this was a huge gain, transforming 153600 divisions into less than 2000.</p>



<p>I verified the precision loss was acceptable using a small ad-hoc tool displaying my output buffers and comparing the reference decoding to the approximated one. Pixel values differ by at most 1.</p>



<figure><a href="https://www.colino.net/wordpress/wp-content/uploads/image-122.png"><img loading="lazy" decoding="async" width="1541" height="386" src="https://www.colino.net/wordpress/wp-content/uploads/image-122.png" alt="" srcset="https://www.colino.net/wordpress/wp-content/uploads/image-122.png 1541w, https://www.colino.net/wordpress/wp-content/uploads/image-122-400x100.png 400w, https://www.colino.net/wordpress/wp-content/uploads/image-122-150x38.png 150w, https://www.colino.net/wordpress/wp-content/uploads/image-122-768x192.png 768w, https://www.colino.net/wordpress/wp-content/uploads/image-122-1536x385.png 1536w, https://www.colino.net/wordpress/wp-content/uploads/image-122-1280x321.png 1280w" sizes="auto, (max-width: 1541px) 100vw, 1541px"></a><figcaption>Left: the normal output; middle: the comparison; right: the output with approximated divisions</figcaption></figure>



<p><strong>Output index</strong></p>



<p>So far we set the output buffer using the usual <strong>buffer[y*WIDTH+x]</strong> access method, which is really slow on a processor with no multiplication support. <a href="https://github.com/colinleroy/qtkn_decoder/commit/7112b2b96e3d249f4a65532035dff9a8708bc914">I changed that to a much simpler line-by-line indexing</a>. (Even on x86_64, the change is notable: 20M instructions).</p>



<p><strong>Huffman decoding</strong></p>



<p>The algorithm initialized full tables so that it was possible to get a Huffman code by just looking at the bitbuffer: for code <strong>10001</strong>, for example, all codes from <strong>10001000</strong> to <strong>10001111</strong> were matched to the correct value, then the bitbuffer shifted &lt;&lt;5. This seems good at first, but not on 6502, as this requires a 16-bits bitbuffer to make sure we always have a full byte to look at. <a href="https://github.com/colinleroy/qtkn_decoder/commit/b0179e034a4396b8b6d09d489608bf0080e7b8d6">I reworked that to get bits one at a time</a>. This made the x86_64 implementation slower, but the 6502 one 20 seconds faster, spending 9 seconds shifting bits instead of 29. It also allowed me to pack the tables more tight, freeing up some memory for the cache.</p>



<p><strong>Assembly</strong></p>



<p><a href="https://github.com/colinleroy/qtkn_decoder/blob/b0179e034a4396b8b6d09d489608bf0080e7b8d6/qtkn-decoder.c">This algorithm</a> still performs very poorly when compiled by cc65, but is far easier to manually translate into optimized 6502 assembly. There are also a lot of ad-hoc optimisations, for example:</p>



<ul>
<li>The division factor for final pixel values for a pair of rows is 48 more than 50% of the time, on any image I tested. So the 6502 implementation has two divisions lookup tables, one for 48 that is never recomputed, one for another factor, recomputed if needed at the start of a pair of rows.</li>



<li>The row initialization multiplies all 320 next_line values by a factor, which is 255 about 66% of the time. In this case, instead of multiplying a = a*255, the assembly version does (a*256)-a, which is (a&lt;&lt;8)-a, which is much faster.</li>



<li>There is a whole lot of &lt;&lt;4 going on in the main loop, which is lookup-table based in the assembly implementation. &lt;&lt;4 is larger than 8 bits, so there are two tables needed, but it still is worth the memory usage.</li>



<li>Half the Huffman codes read are discarded (they are used for blue and red pixels), so “discarder” functions are used in that case, only shifting the bitbuffer without fetching the value.</li>



<li>Buffers accesses (to next_line and output buffer) are patched in self-modifying code rather than using zero-page pointers, which require to keep track and patch about 54 labels on each page cross. This is ugly as hell, but this requires about 50k cycles per image, but spares 9M cycles overall.</li>
</ul>



<p><strong>The final code</strong></p>



<p>I have pointed to commits to my “test” repository so far, but if you’re interested in the actual 6502 implementation, you can find it in my repository: <a href="https://github.com/colinleroy/a2tools/blob/master/src/quicktake/qtkn_platform.s">the decoder</a>, and <a href="https://github.com/colinleroy/a2tools/blob/master/src/quicktake/qtk_bithuff.s">the bitbuffer</a>.</p>



<p><strong>Questions remain</strong></p>



<p>There still are things I don’t understand in dcraw’s decoder, that my simplifications didn’t uncover. The main thing I wonder is, how did Dave Coffin, dcraw’s author, implement this decoder first? It seems so full of “magic” numbers and arithmetic operations that I have no idea how one would look at pictures at the bit level, and figure out <em>anything</em> about the format. Did he reverse-engineer Apple’s binary? Did he have documentation? Is it in fact a common kind of encoding I have no idea about?</p>



<p>I would love to see documentation of this format, maybe I would understand more and be able to progress further.</p>



<p><strong>Bonus: first and current implementation video</strong></p>



<figure><video controls="" src="https://www.colino.net/wordpress/wp-content/uploads/qt150-decoding-web.mp4"></video><figcaption>The current implementation</figcaption></figure>



<figure><video controls="" src="https://www.colino.net/wordpress/wp-content/uploads/qt150-original.mp4"></video><figcaption>The first implementation (are you patient?)</figcaption></figure>
																
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google appears to have deleted its political ad archive for the EU (244 pts)]]></title>
            <link>https://www.thebriefing.ie/google-just-erased-7-years-of-our-political-history/</link>
            <guid>45411332</guid>
            <pubDate>Mon, 29 Sep 2025 07:57:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thebriefing.ie/google-just-erased-7-years-of-our-political-history/">https://www.thebriefing.ie/google-just-erased-7-years-of-our-political-history/</a>, See on <a href="https://news.ycombinator.com/item?id=45411332">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    <article>

        <header>

            


                <figure>
        <img srcset="https://www.thebriefing.ie/content/images/size/w320/2025/09/Briefing-1-1.png 320w,
                    https://www.thebriefing.ie/content/images/size/w600/2025/09/Briefing-1-1.png 600w,
                    https://www.thebriefing.ie/content/images/size/w960/2025/09/Briefing-1-1.png 960w,
                    https://www.thebriefing.ie/content/images/size/w1200/2025/09/Briefing-1-1.png 1200w,
                    https://www.thebriefing.ie/content/images/size/w2000/2025/09/Briefing-1-1.png 2000w" sizes="(max-width: 1200px) 100vw, 1120px" src="https://www.thebriefing.ie/content/images/size/w1200/2025/09/Briefing-1-1.png" alt="Google just erased 7 years of our political history">
    </figure>

        </header>

        <section>
            <p>Google appears to have deleted its political ad archive for the EU; so the last 7 years of ads, of political spending, of messaging, of targeting - on YouTube, on Search and for display ads - for countless elections across 27 countries - is all gone. </p><p>We had been told that Google would try to stop people placing political ads, a "ban" that was to come into effect this week. I did not read anywhere that this would mean the erasure of this archive of our political history. </p><p>When you go to the Google Ad Archive (which you can here: <a href="https://adstransparency.google.com/?region=IE&amp;ref=thebriefing.ie">https://adstransparency.google.com/</a>) until last week you could search all political ads shown in your country by a date range of your choosing going back to 2018. You could browse all the ads in that range, or search for keywords, candidates, parties. You could view each ad - watch the video, see the images - who had been targeted, how much had been spent etc. </p><p>Now when you try to click on "political ads" you get re-directed to a page asking you to select from a small number of countries - the US, of course, UK, India, Australia, Brazil, Israel - but not one EU country (see below): </p><figure><img src="https://www.thebriefing.ie/content/images/2025/09/Screenshot-2025-09-28-at-10.38.47.png" alt="" loading="lazy" width="585" height="819"><figcaption><span>The page you are diverted too if you try to find political ads on Google. </span></figcaption></figure><p>The political ad archive - now deleted? - allowed people like me (and many others) to understand what happened in elections, like this longer piece I was able to write during the European &amp; Local elections last year on the use of YouTube by a far right party, Sinn Féin's big push on search result ads, and the growth of attacks ads in Ireland: </p><figure><a href="https://www.thebriefing.ie/campaign-round-up-google-youtube-ad-spend/"><div><p>Campaign round up: Irish Freedom Party dominating YouTube ad spend</p><p>The Irish Freedom Party spent €4,100 in the past 3 weeks on YouTube ads, the biggest spenders on YouTube ads this election.</p><p><img src="https://www.thebriefing.ie/content/images/icon/tb-logos-61.png" alt=""><span>Liz Carolan</span></p></div><p><img src="https://www.thebriefing.ie/content/images/thumbnail/Website-1920x1080-7.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Now you need the specific name of an advertiser, and when I looked for, for example, "Sinn Fein", it (a) only gave me the option of searching for their website, and (b) showed zero results. This is despite Sinn Fein spending upwards of <a href="https://www.thebriefing.ie/ad-spend-roundup-and-last-minute-shenanigans/" rel="noreferrer">€10k a day during some of the elections last year</a>. </p><figure><img src="https://www.thebriefing.ie/content/images/2025/09/Screenshot-2025-09-28-at-11.17.51.png" alt="" loading="lazy" width="850" height="496" srcset="https://www.thebriefing.ie/content/images/size/w600/2025/09/Screenshot-2025-09-28-at-11.17.51.png 600w, https://www.thebriefing.ie/content/images/2025/09/Screenshot-2025-09-28-at-11.17.51.png 850w" sizes="(min-width: 720px) 720px"><figcaption><span>Google no longer share any data on Irish political party and candidate spending on political ads on their platform; as shown here, a search for "Sinn Fein" returns "no ads found" </span></figcaption></figure><p>Some good things have been written about the impact that an ad "ban" might have on campaigning, especially when the algorithm still dominates all major social platforms (see this <a href="https://www.liberties.eu/en/stories/meta-google-advertising-bans/45513?ref=thebriefing.ie" rel="noreferrer">great piece</a> by the Civil Liberties Union for Europe), </p><p>But the ad archives were introduced 7 years ago for a reason - in no small part because of the chaos of the Brexit and Trump 2016 votes, and our own advocacy here in Ireland about interference in the 2018 8th amendment referendum. </p><p>They were introduced to allow for scrutiny of campaigns, and also to provide a historical record so we could go back and look at what had been promised, and what had been spent, and to see if this lined up with what happened later. </p><p>This erasure of our political past feels dangerous, for scrutiny, for accountability, for shared memory, for enforcement of our rules - for our democracy. </p><h2 id="icymi">ICYMI </h2><figure><a href="https://www.thebriefing.ie/campaigning-likes-it-1999-when-it-feels-like-its-1938/"><div><p>Campaigning likes it 1999 (when it feels like its 1938)</p><p>How do you run a political campaign without digital ads? I asked two of our best campaign gurus what we might expect.</p><p><img src="https://www.thebriefing.ie/content/images/icon/tb-logos-62.png" alt=""><span>The Briefing</span><span>Liz Carolan</span></p></div><p><img src="https://www.thebriefing.ie/content/images/thumbnail/Briefing-11.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://www.thebriefing.ie/musks-profoundly-ignorant-and-dangerous-intervention-in-aras25/"><div><p>Musk’s profoundly ignorant and dangerous intervention in #Aras25</p><p>I really, really hope this isn’t the start of a whole thing. Elon Musk has tweeted about the race for the Aras in the most profoundly ignorant and stupidly dangerous way possible. The tweet is screenshot below; in it Musk sub-tweets another tweet claiming Simon Harris “ordered his Party to</p><p><img src="https://www.thebriefing.ie/content/images/icon/tb-logos-63.png" alt=""><span>The Briefing</span><span>Liz Carolan</span></p></div><p><img src="https://www.thebriefing.ie/content/images/thumbnail/thebriefing_elon-2.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://www.thebriefing.ie/the-first-aras25-ads-are-out/"><div><p>The first #Aras25 ads are out</p><p>What the first political ads tell us about how the campaign might look</p><p><img src="https://www.thebriefing.ie/content/images/icon/tb-logos-64.png" alt=""><span>The Briefing</span><span>Liz Carolan</span></p></div><p><img src="https://www.thebriefing.ie/content/images/thumbnail/Briefing-1-1.png" alt="" onerror="this.style.display = 'none'"></p></a></figure>
        </section>

    </article>

        

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What is "good taste" in software engineering? (296 pts)]]></title>
            <link>https://www.seangoedecke.com/taste/</link>
            <guid>45410940</guid>
            <pubDate>Mon, 29 Sep 2025 06:41:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.seangoedecke.com/taste/">https://www.seangoedecke.com/taste/</a>, See on <a href="https://news.ycombinator.com/item?id=45410940">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p>Technical taste is different from technical skill. You can be technically strong but have bad taste, or technically weak with good taste. Like taste in general, technical taste sometimes runs ahead of your ability: just like you can tell good food from bad without being able to cook, you can know what kind of software you like before you’ve got the ability to build it. You can develop technical ability by study and repetition, but good taste is developed in a more mysterious way.</p>
<p>Here are some indicators of software taste:</p>
<ul>
<li>What kind of code “looks good” to you? What kind of code “looks ugly”?</li>
<li>Which design decisions you feel really good about, and which ones are just fine?</li>
<li>Which software problems really bother you, to the point where you’re worrying about them outside of work? Which problems can you just brush off?</li>
</ul>
<p>I think taste is <strong>the ability to adopt the set of engineering values that fit your current project</strong>.</p>
<h3>Why taste is different from skill</h3>
<p>Aren’t the indicators above just a part of skill? For instance, doesn’t code look good <em>if it’s good code</em>? I don’t think so.</p>
<p>Let’s take an example. Personally, I feel like code that uses map and filter looks nicer than using a for loop. It’s tempting to think that this is a case of me being straightforwardly correct about a point of engineering. For instance, map and filter typically involve pure functions, which are easier to reason about, and they avoid an entire class of off-by-one iterator bugs. It feels to me like this isn’t a matter of taste, but a case where I’m right and other engineers are wrong.</p>
<p>But of course it’s more complicated than that. Languages like Golang don’t contain map and filter at all, for principled reasons. Iterating with a for loop is easier to reason about from a performance perspective, and is more straightforward to extend to other iteration strategies (like taking two items at a time). I don’t care about these reasons as much as I care about the reasons in favour of map and filter - that’s why I don’t write a lot of for loops - but it would be far too arrogant for me to say that engineers who prefer for loops are simply less skilled. In many cases, they have technical capabilites that I don’t have. They just care about different things.</p>
<p>In other words, our disagreement comes down to a difference in <em>values</em>. I wrote about this point in <a href="https://www.seangoedecke.com/confidence"><em>I don’t know how to build software and you don’t either</em></a>. Even if the big technical debates do have definite answers, no working software engineer is ever in a position to know what those answers are, because you can only fit so much experience into one career. We are all at least partly relying on our own personal experience: on our particular set of engineering values.</p>
<h3>What engineering taste actually is</h3>
<p>Almost every decision in software engineering is a tradeoff. You’re rarely picking between two options where one is strictly better. Instead, each option has its own benefits and downsides. Often you have to make hard tradeoffs between engineering <em>values</em>: past a certain point, you cannot easily increase performance without harming readability, for instance<sup id="fnref-1"><a href="#fn-1">1</a></sup>.</p>
<p>Really understanding this point is (in my view) the biggest indicator of maturity in software engineering. Immature engineers are rigid about their decisions. They think it’s always better to do X or Y. Mature engineers are usually willing to consider both sides of a decision, because they know that both sides come with different benefits. The trick is not deciding if technology X is better than Y, but whether the benefits of X outweigh Y <em>in this particular case</em>.</p>
<p>In other words, <strong>immature engineers are too inflexible about their taste</strong>. They know what they like, but they mistake that liking for a principled engineering position. What defines a particular engineer’s taste?</p>
<p>In my view, <strong>your engineering taste is composed of the set of engineering values you find most important</strong>. For instance:</p>
<p><strong>Resiliency</strong>. If an infrastructure component fails (a service dies, a network connection becomes unavailable), does the system remain functional? Can it recover without human intervention?</p>
<p><strong>Speed</strong>. How fast is the software, compared to the theoretical limit? Is work being done in the hot path that isn’t strictly necessary?</p>
<p><strong>Readability</strong>. Is the software easy to take in at a glance and to onboard new engineers to? Are functions relatively short and named well? Is the system well-documented?</p>
<p><strong>Correctness</strong>. Is it possible to represent an invalid state in the system? How locked-down is the system with tests, types, and asserts? Do the tests use techniques like fuzzing? In the extreme case, has the program been proven correct by formal methods like <a href="https://en.wikipedia.org/wiki/Alloy_(specification_language)">Alloy</a>?</p>
<p><strong>Flexibility</strong>. Can the system be trivially extended? How easy is it to make a change? If I need to change something, how many different parts of the program do I need to touch in order to do so?</p>
<p><strong>Portability</strong>. Is the system tied down to a particular operational environment (say, Microsoft Windows, or AWS)? If the system needs to be redeployed elsewhere, can that happen without a lot of engineering work?</p>
<p><strong>Scalability</strong>. If traffic goes up 10x, will the system fall over? What about 100x? Does the system have to be over-provisioned or can it scale automatically? What bottlenecks will require engineering intervention?</p>
<p><strong>Development speed</strong>. If I need to extend the system, how fast can it be done? Can most engineers work on it, or does it require a domain expert?</p>
<p>There are many other engineering values: elegance, modern-ness, use of open source, monetary cost of keeping the system running, and so on. All of these are important, but <strong>no engineer cares equally about all of these things.</strong> Your taste is determined by which of these values you rank highest. For instance, if you value speed and correctness more than development speed, you are likely to prefer Rust over Python. If you value scalability over portability, you are likely to argue for a heavy investment in your host’s (e.g. AWS) particular quirks and tooling. If you value resiliency over speed, you are likely to want to split your traffic between different regions. And so on<sup id="fnref-2"><a href="#fn-2">2</a></sup>.</p>
<p>It’s possible to break these values down in a more fine-grained way. Two engineers who both deeply care about readability could disagree because one values short functions and the other values short call-stacks. Two engineers who both care about correctness could disagree because one values exhaustive test suites and the other values formal methods. But the principle is the same - there are lots of possible engineering values to care about, and because they are often in tension, each engineer is forced to take some more seriously than others.</p>
<h3>How to identify bad taste</h3>
<p>I’ve said that all of these values are important. Despite that, it’s possible to have <em>bad</em> taste. In the context of software engineering, bad taste means that <strong>your preferred values are not a good fit for the project you’re working on</strong>.</p>
<p>Most of us have worked with engineers like this. They come onto your project evangelizing about something - formal methods, rewriting in Golang, Ruby meta-programming, cross-region deployment, or whatever - because it’s worked well for them in the past. Whether it’s a good fit for your project or not, they’re going to argue for it, because it’s what they like. Before you know it, you’re making sure your internal metrics dashboard has five nines of reliability, at the cost of making it impossible for any junior engineer to understand.</p>
<p>In other words, most bad taste comes from <strong>inflexibility</strong>. I will always distrust engineers who justify decisions by saying “it’s best practice”. No engineering decision is “best practice” in all contexts! You have to make the right decision for the specific problem you’re facing.</p>
<p>One interesting consequence of this is that engineers with bad taste are like broken compasses. If you’re in the right spot, a broken compass will still point north. It’s only when you start moving around that the broken compass will steer you wrong. Likewise, many engineers with bad taste can be quite effective in the particular niche where their preferences line up with what the project needs. But when they’re moved between projects or jobs, or when the nature of the project changes, the wheels immediately come off. No job stays the same for long, particularly <a href="https://www.seangoedecke.com/good-times-are-over">in these troubled post-2021 times</a>.</p>
<h3>How to identify good taste</h3>
<p>Good taste is a lot more elusive than technical ability. That’s because, unlike technical ability, good taste is the ability to select the right set of engineering values <strong>for the particular technical problem you’re facing</strong>. It’s thus much harder to identify if someone has good taste: you can’t test it with toy problems, or by asking about technical facts. You need there to be a real problem, with all of its messy real-world context.</p>
<p>You can tell you have good taste if the projects you’re working on succeed. If you’re not meaningfully contributing to the design of a project (maybe you’re just doing ticket-work), you can tell you have good taste if the projects where you agree with the design decisions succeed, and the projects where you disagree are rocky. Importantly, you need a set of different kinds of projects. If it’s just the one project, or the same kind of project over again, you might just be a good fit for that. Even if you go through many different kinds of projects, that’s no guarantee that you have good taste in domains you’re less familiar with<sup id="fnref-3"><a href="#fn-3">3</a></sup>.</p>
<p>How do you develop good taste? It’s hard to say, but I’d recommend working on a variety of things, paying close attention to which projects (or which parts of the project) are easy and which parts are hard. You should focus on <strong>flexibility</strong>: try not to acquire strong universal opinions about the right way to write software. What good taste I have I acquired pretty slowly. Still, I don’t see why you couldn’t acquire it fast. I’m sure there are prodigies with taste beyond their experience in programming, just as there are prodigies in other domains.</p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zero ASIC releases Wildebeest, the highest performance FPGA synthesis tool (179 pts)]]></title>
            <link>https://www.zeroasic.com/blog/wildebeest-launch</link>
            <guid>45410155</guid>
            <pubDate>Mon, 29 Sep 2025 03:45:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.zeroasic.com/blog/wildebeest-launch">https://www.zeroasic.com/blog/wildebeest-launch</a>, See on <a href="https://news.ycombinator.com/item?id=45410155">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <h3 id="zero-asic-releases-wildebeest-the-worlds-highest-performance-fpga-synthesis-tool">Zero ASIC releases Wildebeest, the world’s highest performance FPGA synthesis tool.</h3>
<p>Cambridge, MA – September 17, 2025 – Zero ASIC, a U.S. semiconductor startup on a mission to democratize silicon, today announced the release of Wildebeest<sup>TM</sup>, the world’s highest performance FPGA synthesis tool.</p>
<h3 id="background">Background</h3>
<p>The software world has largely moved away from proprietary, vendor-locked compilers in favor of open source alternatives such as LLVM<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup> and GCC. Early on, these open source compilers lagged behind in performance, but over time, through the collective effort of the community, they caught up and even surpassed their proprietary counterparts.</p>
<p>In hardware, a similar transformation has been unfolding. Thanks to the pioneering work of Alan Mishchenko (ABC<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup>), Claire Xenia Wolf (Yosys<sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup>), and the broader open source EDA community, FPGA developers have had access to a full-featured Verilog RTL synthesis toolchain for years. Recently, SystemVerilog support has since been added through Mike Popoloski’s excellent Slang parser<sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup>. Thanks to strong community involvement, the Yosys project now supports FPGA synthesis for a number of commercial and academic FPGA architectures.</p>
<p>Unfortunately, funding for open source FPGA synthesis has been minimal, and as a result a large QoR gap between open source and proprietary synthesis remains. Industrial users, who care obsessively about performance, have thus been stuck between a rock and a hard place: “Freedom or Performance”.</p>













































<table><thead><tr><th>Attribute</th><th>Vendor Tool</th><th>Yosys</th></tr></thead><tbody><tr><td>FPGA Support</td><td>Yes</td><td>No</td></tr><tr><td>Lock-in</td><td>Yes</td><td>No</td></tr><tr><td>Open Source</td><td>No</td><td>Yes</td></tr><tr><td>Free</td><td>Yes/No</td><td>Yes</td></tr><tr><td>Binary Size</td><td>Large</td><td>Small</td></tr><tr><td>QoR</td><td>Great</td><td>Good</td></tr><tr><td>Robustness</td><td>Great</td><td>Good</td></tr></tbody></table>
<h3 id="wildebeest-intro">Wildebeest Intro</h3>
<p>Wildebeest introduces a number of critical optimization techniques to open source. Some of these techniques are standard practice in commercial compilers, but this is the first time they have been demonstrated in an open source FPGA synthesis tool.</p>
<p>The most important Wildebeest strategy is the use of circuit size as a primary feature for selecting the synthesis algorithms. Existing single script solutions don’t work well because they either fail to converge for large circuits or sacrifice performance for robustness. Using a carefully selected set of size appropriate optimization scripts, Wildebeest achieves robustness and high performance for a wide range of benchmark (up to 1M LUT designs).</p>
<p>Another important aspect of the Wildebeest approach is the effective use of the most advanced <code>abc9</code> commands for speculative synthesis and logic depth minimization. ABC is an incredibly powerful logic synthesis library, but making effective use of all commands is a non-trivial task that requires deep expertise in logic synthesis, the ABC architecture, and Yosys, and software development.</p>
<p>Logic optimization is only as good as the benchmark data that grounds the algorithms used. Wildebeest adopted an industrial approach to development from day one, developing an internal suite of 150+ carefully selected benchmarks and automated profiling utilities. The open source <a href="https://github.com/zeroasiccorp/logikbench">LogikBench</a> benchmarks suite was created to enable independent evaluation and benchmarking.</p>
<p>Logic synthesis has been around for over 50 years. During this time, basic synthesis algorithms and approaches have been openly published by the synthesis R&amp;D community, but many of the “outer loop” tricks of the trade have been kept hidden by practitioners within proprietary tools. The lead Wildebeest developer, Dr. Thierry Besson is an industry insider with 30 years of experience in developing state of the art commercial logic synthesis solutions. Dr. Besson has previously contributed the fastest/smallest results on a number of the EPFL logic synthesis benchmarks and with Wildebeest he is releasing many of these techniques into the wild.<sup><a href="#user-content-fn-5" id="user-content-fnref-5" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup></p>
<h3 id="benchmark-results">Benchmark Results</h3>
<p>The table below shows how Wildebeest compares against both open-source and proprietary synthesis tools for the <a href="https://raw.githubusercontent.com/zeroasiccorp/logikbench/refs/heads/main/logikbench/blocks/picorv32/rtl/picorv32.v">picorv32 CPU design</a>. To run Wildebeest across a broader set of benchmarks, see the <a href="https://github.com/zeroasiccorp/logikbench">LogikBench</a> project.</p>













































































<table><thead><tr><th>Device</th><th>Arch</th><th>Tool</th><th>Synthesis Command</th><th>LUTs</th><th>Logic Depth</th></tr></thead><tbody><tr><td>z1060</td><td>LUT6</td><td>wildebeest</td><td>synth_fpga</td><td>2312</td><td>40</td></tr><tr><td>z1060</td><td>LUT6</td><td>wildebeest</td><td>synth_fpga -opt delay</td><td>2677</td><td>6</td></tr><tr><td>Vendor-1</td><td>LUT6</td><td>vendor</td><td>(proprietary)</td><td>2870</td><td>7</td></tr><tr><td>Vendor-2</td><td>LUT6</td><td>vendor</td><td>(proprietary)</td><td>2947</td><td>8</td></tr><tr><td>xc7</td><td>LUT6</td><td>yosys (0.56)</td><td>synth_xilinx -nocarry</td><td>3072</td><td>17</td></tr><tr><td>z1010</td><td>LUT4</td><td>wildebeest</td><td>synth_fpga</td><td>3593</td><td>39</td></tr><tr><td>z1010</td><td>LUT4</td><td>wildebeest</td><td>synth_fpga -opt delay</td><td>4112</td><td>8</td></tr><tr><td>ice40</td><td>LUT4</td><td>yosys (0.56)</td><td>synth_ice40 -dsp -nocarry</td><td>4378</td><td>33</td></tr></tbody></table>
<p>The results show that Wildebeest QoR exceeds both proprietary and open source FPGA synthesis solutions.</p>
<h3 id="future-work">Future Work</h3>
<p>This initial Wildebeest release is only the beginning of the journey. The development team has a pipeline of optimization techniques in development with QoR that is expected to exceed current proprietary tools by a wide margin.</p>
<p>The long term goal of the Wildebeest project is to help bring forth an era of “LLVM for synthesis” by working with the community to develop high performance open source FPGA tools, robust standard IRs and file formats, and broad hardware vendor adoption.</p>
<h3 id="demo">Demo</h3>
<p>To try out the <code>Wildebeest</code>, follow these <a href="https://github.com/zeroasiccorp/wildebeest?tab=readme-ov-file#prerequisites">installation</a> instructions, download the <a href="https://raw.githubusercontent.com/zeroasiccorp/logikbench/refs/heads/main/logikbench/blocks/picorv32/rtl/picorv32.v">picorv32 CPU example</a>, launch <code>yosys</code>, and run the command sequence below.</p>
<pre tabindex="0" data-language="bash"><code><span><span>plugin</span><span> -i</span><span> wildebeest</span></span>
<span><span>read_verilog</span><span> picorv32.v</span></span>
<span><span>hierarchy</span><span> -check</span><span> -top</span><span> picorv32</span></span>
<span><span>synth_fpga</span><span> -partname</span><span> z1010</span></span></code></pre>
<h3 id="availability">Availability</h3>
<p>The Wildebeest source code was officially released on September 17, 2025 and can be downloaded via github:</p>
<p><a href="https://github.com/zeroasiccorp/wildebeest">https://github.com/zeroasiccorp/wildebeest</a></p>
<h3 id="about-zero-asic">About Zero ASIC</h3>
<p>Zero ASIC is a semiconductor startup based in Cambridge, Massachusetts. The company mission is to democratize access to silicon through chiplets and design automation. Zero ASIC is building the world’s first composable chiplet platform, enabling billions of unique silicon systems to be assembled in hours from a catalog of off-the-shelf chiplets.</p>
<h3 id="references">References</h3>
<section data-footnotes="">
<ol>
<li id="user-content-fn-1">
<p>C. Lattner, V. Adve, “LLVM: A Compilation Framework for Lifelong Program Analysis &amp; Transformation”, Proc. International Symposium on Code Generation and Optimization  2004 <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="user-content-fn-2">
<p>R. Brayton and A. Mishchenko, “ABC: An academic industrial-strength verification tool”, Proc. CAV 2010 <a href="#user-content-fnref-2" data-footnote-backref="" aria-label="Back to reference 2">↩</a></p>
</li>
<li id="user-content-fn-3">
<p>C. Wolf, Johann Glaser., “Yosys - A Free Verilog Synthesis Suite”, Proc. Austrochip 2013 <a href="#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 3">↩</a></p>
</li>
<li id="user-content-fn-4">
<p>M. Popoloski, “Slang: a SystemVerilog Compiler”, <a href="https://github.com/MikePopoloski/slang">https://github.com/MikePopoloski/slang</a> <a href="#user-content-fnref-4" data-footnote-backref="" aria-label="Back to reference 4">↩</a></p>
</li>
<li id="user-content-fn-5">
<p>EPFL Benchmark Suite Best Results, <a href="https://github.com/lsils/benchmarks/tree/master/best_results">https://github.com/lsils/benchmarks/tree/master/best_results</a> <a href="#user-content-fnref-5" data-footnote-backref="" aria-label="Back to reference 5">↩</a></p>
</li>
</ol>
</section> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[F-Droid and Google's Developer Registration Decree (1329 pts)]]></title>
            <link>https://f-droid.org/2025/09/29/google-developer-registration-decree.html</link>
            <guid>45409794</guid>
            <pubDate>Mon, 29 Sep 2025 02:10:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://f-droid.org/2025/09/29/google-developer-registration-decree.html">https://f-droid.org/2025/09/29/google-developer-registration-decree.html</a>, See on <a href="https://news.ycombinator.com/item?id=45409794">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>For the past 15 years<sup id="fnref:fifteen" role="doc-noteref"><a href="#fn:fifteen" rel="footnote">1</a></sup>, F-Droid has provided a safe and secure
haven for Android users around the world to find and install free and open
source apps. When contrasted with the commercial app stores — of which the
Google Play store is the most prominent — the differences are stark: they
are hotbeds of spyware and scams, blatantly promoting apps that prey on
their users through attempts to monetize their attention and mine their
intimate information through any means necessary, including trickery and
dark patterns.[^spyware1]</p>

<p><a href="https://f-droid.org/en/2025/09/04/twif.html">https://f-droid.org/2025/09/04/twif.html</a>
[^spyware1]: “Spyware maker caught distributing malicious Android apps for
years”:
<a href="https://techcrunch.com/2025/02/13/spyware-maker-caught-distributing-malicious-android-apps-for-years">https://techcrunch.com/2025/02/13/spyware-maker-caught-distributing-malicious-android-apps-for-years</a></p>

<p>F-Droid is different. It distributes apps that have been validated to work
<em>for</em> the user’s interests, rather than for the interests of the app’s
distributors. The way F-Droid works is simple: when a developer creates an
app and hosts the source code publicly somewhere, the F-Droid team reviews
it, inspecting it to ensure that it is completely open source and contains
no undocumented <em>anti-features</em> such as advertisements or
trackers<sup id="fnref:antifeatures" role="doc-noteref"><a href="#fn:antifeatures" rel="footnote">2</a></sup>. Once it passes inspection, the F-Droid build
service compiles and packages the app to make it ready for distribution. The
package is then signed either with F-Droid’s cryptographic key, or, if the
build is reproducible[^reproducible], enables distribution using the
original developer’s private key. In this way, users can trust that any app
distributed through F-Droid is the one that was built from the specified
source code and has not been tampered with.</p>

<p><a href="https://f-droid.org/en/docs/Anti-Features/">https://f-droid.org/docs/Anti-Features/</a>
[^reproducible]: F-Droid Reproducible Builds Introduction:
<a href="https://f-droid.org/en/docs/Reproducible_Builds/">https://f-droid.org/docs/Reproducible_Builds/</a></p>

<p>Do you want a weather app that doesn’t transmit your every movement to a
shadowy data broker<sup id="fnref:weather-apps" role="doc-noteref"><a href="#fn:weather-apps" rel="footnote">3</a></sup>? Or a scheduling assistant that doesn’t
siphon your intimate details into an advertisement
network[^surveillance-ads]? F-Droid has your back. Just as sunlight is the
best disinfectant against corruption, open source is the best defense
against software acting against the interests of the user.</p>

<p><a href="https://www.howtogeek.com/884233/your-weather-app-is-spying-on-you-heres-what-to-do/#why-are-weather-apps-such-a-privacy-nightmare">https://www.howtogeek.com/884233/your-weather-app-is-spying-on-you-heres-what-to-do/#why-are-weather-apps-such-a-privacy-nightmare</a>
[^surveillance-ads]: “Online Behavioral Ads Fuel the Surveillance
Industry—Here’s How”:
<a href="https://www.eff.org/deeplinks/2025/01/online-behavioral-ads-fuel-surveillance-industry-heres-how">https://www.eff.org/deeplinks/2025/01/online-behavioral-ads-fuel-surveillance-industry-heres-how</a></p>

<h3 id="googles-move-to-break-free-app-distribution">Google’s move to break free app distribution</h3>

<p>The future of this elegant and proven system was put in jeopardy last month,
when Google unilaterally decreed<sup id="fnref:regann" role="doc-noteref"><a href="#fn:regann" rel="footnote">4</a></sup> that Android developers everywhere
in the world are going to be required to register centrally with Google. In
addition to demanding payment of a registration fee and agreement to their
(non-negotiable and ever-changing) terms and conditions, Google will also
require the uploading of personally identifying documents[^regid], including
government ID, by the authors of the software, as well as enumerating all
the unique “application identifiers” for every app that is to be distributed
by the registered developer.[^regappid]</p>

<p>require all apps to be registered by verified developers in order to be
installed by users on certified Android devices.”
<a href="https://android-developers.googleblog.com/2025/08/elevating-android-security.html">https://android-developers.googleblog.com/2025/08/elevating-android-security.html</a>
[^regid]: Android developer verification: “You will need to provide and
verify your personal details, like your legal name, address, email address,
and phone number. You may also need to upload official government ID.”:
<a href="https://developer.android.com/developer-verification#verify-your-identity">https://developer.android.com/developer-verification#verify-your-identity</a>
[^regappid]: Android developer verification: “You’ll need to prove you own
your apps by providing your app package name and app signing keys.”:
<a href="https://developer.android.com/developer-verification#register-your-apps">https://developer.android.com/developer-verification#register-your-apps</a></p>

<p>The F-Droid project cannot require that developers register their apps
through Google, but at the same time, we cannot “take over” the application
identifiers for the open-source apps we distribute, as that would
effectively seize exclusive distribution rights to those applications.</p>

<p>If it were to be put into effect, the developer registration decree will end
the F-Droid project and other free/open-source app distribution sources as
we know them today, and the world will be deprived of the safety and
security of the catalog of thousands of apps that can be trusted and
verified by any and all. F-Droid’s myriad users<sup id="fnref:howmanyusers" role="doc-noteref"><a href="#fn:howmanyusers" rel="footnote">5</a></sup> will be left
adrift, with no means to install — or even update their existing installed —
applications.</p>

<p>because we don’t track users or have any registration. “No user accounts, by
design”:
<a href="https://f-droid.org/en/2022/02/28/no-user-accounts-by-design.html">https://f-droid.org/2022/02/28/no-user-accounts-by-design.html</a></p>

<h3 id="the-security-canard">The Security Canard</h3>

<p>While directly installing — or “sideloading”<sup id="fnref:sideloading" role="doc-noteref"><a href="#fn:sideloading" rel="footnote">6</a></sup> — software can be
construed as carrying some inherent risk, it is false to claim that
centralized app stores are the only safe option for software
distribution. Google Play itself has repeatedly hosted
malware[^playmal1][^playmal2], proving that corporate gatekeeping doesn’t
guarantee user protection. By contrast, F-Droid offers a trustworthy and
transparent alternative approach to security: every app is free and open
source, the code can be audited by anyone, the build process and logs are
public, and reproducible builds ensure that what is published matches the
source code exactly. This transparency and accountability provides a
<em>stronger</em> basis for trust than closed platforms, while still giving users
freedom to choose. Restricting direct app installation not only undermines
that choice, it also erodes the diversity and resilience of the open-source
ecosystem by consolidating control in the hands of a few corporate players.</p>

<p>came up with; it means “installing software without our permission,” which
we used to just call “installing software” (because you don’t need a
manufacturer’s permission to install software on your computer).’ —
Pluralistic: <em>Darth Android</em>:
<a href="https://pluralistic.net/2025/09/01/fulu/">https://pluralistic.net/2025/09/01/fulu/</a>
[^playmal1]: “224 malicious apps removed from the Google Play Store after ad
fraud campaign discovered”:
<a href="https://www.malwarebytes.com/blog/news/2025/09/224-malicious-apps-removed-from-the-google-play-store-after-ad-fraud-campaign-discovered">https://www.malwarebytes.com/blog/news/2025/09/224-malicious-apps-removed-from-the-google-play-store-after-ad-fraud-campaign-discovered</a>
[^playmal2]: “Malware-ridden apps made it into Google’s Play Store, scored
19 million downloads”:
<a href="https://www.theregister.com/2025/08/26/apps_android_malware/">https://www.theregister.com/2025/08/26/apps_android_malware/</a></p>

<p>Furthermore, Google’s framing that they need to mandate developer
registration in order to defend against malware is disingenuous because they
<em>already</em> have a remediation mechanism for malware they identify on a
device: the Play Protect service<sup id="fnref:playprotect" role="doc-noteref"><a href="#fn:playprotect" rel="footnote">7</a></sup> that is enabled on all
Android Certified devices already scans and disables apps that have been
identified as malware, regardless of their provenience. Any perceived risks
associated with direct app installation can be mitigated through user
education, open-source transparency, and existing security measures without
imposing exclusionary registration requirements.</p>

<p>harmful behavior”:
<a href="https://support.google.com/googleplay/answer/2812853">https://support.google.com/googleplay/answer/2812853</a></p>

<p>We do not believe that developer registration is motivated by security. We
believe it is about consolidating power and tightening control over a
formerly open ecosystem.</p>

<h3 id="the-right-to-run">The Right to Run</h3>

<p>If you own a computer, you should have the right to run whatever programs
you want on it. This is just as true with the apps on your Android/iPhone
mobile device as it is with the applications on your Linux/Mac/Windows
desktop or server. Forcing software creators into a centralized registration
scheme in order to publish and distribute their works is as egregious as
forcing writers and artists to register with a central authority in order to
be able to distribute their creative works. It is an offense to the core
principles of free speech and thought that are central to the workings of
democratic societies around the world.</p>

<p>By tying application identifiers to personal ID checks and fees, Google is
building a choke point that restricts competition and limits user
freedom. It must find a solution which preserves user rights, freedom of
choice, and a healthy, competitive ecosystem.</p>

<h3 id="what-do-we-propose">What do we propose?</h3>

<p>Regulatory and competition authorities should look carefully at Google’s
proposed activities, and ensure that policies designed to improve security
are not abused to consolidate monopoly control. We urge regulators to
safeguard the ability of alternative app stores and open-source projects to
operate freely, and to protect developers who cannot or will not comply with
exclusionary registration schemes and demands for personal information.</p>

<p>If you are a developer or user who values digital freedom, you can
help. Write to your Member of Parliament<sup id="fnref:mp" role="doc-noteref"><a href="#fn:mp" rel="footnote">8</a></sup>,
Congressperson[^congressperson] or other representative, sign petitions in
defense of sideloading, and contact the European Commission’s Digital
Markets Act (DMA) team<sup id="fnref:dmacontact" role="doc-noteref"><a href="#fn:dmacontact" rel="footnote">9</a></sup> to express why preserving open
distribution matters. By making your voice heard, you help defend not only
F-Droid, but the principle that software should remain a commons, accessible
and free from unnecessary corporate gatekeeping.</p>

<p><a href="https://www.europarl.europa.eu/meps/en/home">https://www.europarl.europa.eu/meps/en/home</a>
[^congressperson]: Find Your Representative
<a href="https://www.house.gov/representatives/find-your-representative">https://www.house.gov/representatives/find-your-representative</a></p>

<p><a href="https://digital-markets-act.ec.europa.eu/contact-dma-team_en">https://digital-markets-act.ec.europa.eu/contact-dma-team_en</a></p>


  </div>

</article>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Primer on FedEx's Distribution Network (2024) (134 pts)]]></title>
            <link>https://ontheseams.substack.com/p/a-brief-primer-on-fedexs-distribution</link>
            <guid>45409552</guid>
            <pubDate>Mon, 29 Sep 2025 01:18:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ontheseams.substack.com/p/a-brief-primer-on-fedexs-distribution">https://ontheseams.substack.com/p/a-brief-primer-on-fedexs-distribution</a>, See on <a href="https://news.ycombinator.com/item?id=45409552">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><em>*Updated July 1, 2025</em></p><p>FedEx was founded in 1971 as an express delivery service. An early motto captures its basic promise: “When it absolutely, positively has to be there overnight.”</p><p><span>The Federal Aviation Act of 1977, which was intensely advocated for by FedEx founding president Fred Smith, allowed FedEx </span><a href="https://www-sciencedirect-com.ezproxy1.lib.asu.edu/science/article/pii/S0966692312001299" rel="">to bring larger aircraft into its fleet</a><span>. Larger aircraft in turn allowed FedEx to concentrate most of its sorting operations in one hub, Memphis, where the company built its superhub in 1979. Today Memphis is the largest cargo airport in the country, and second in the world, all thanks to FedEx’s massive operation.</span></p><p>While FedEx was founded as an air service, and UPS as a ground service, both FedEx and UPS have air and ground services today, though FedEx still has an edge in the air game, with 741 aircraft it operates as opposed to UPS’s 506. It is the largest cargo airline in the world, serving more than 650 airports around the world and shipping 16 million packages daily.</p><p><span>As with all of our primers, this one has an associated </span><a href="https://www.google.com/maps/d/edit?mid=1CSzopj5FzanEmqLVh9ZbAqALsL28laU&amp;usp=sharing" rel="">map of FedEx’s Distribution Network</a><span>, which </span><em>On the Seams</em><span> will do its best to keep updated. (Once again, if you have an update to this map, email us at ontheseams.newsletter@gmail.com.)</span></p><p><span>FedEx </span><a href="https://d3.harvard.edu/platform-rctom/submission/fedex-the-worlds-largest-continuous-flow-process/" rel="">traditionally generates</a><span> between 55-60% of its revenue from Express, about 25% from Ground, 10% from freight, and the remainder from other services. </span></p><p><span>Speed is the name of the game for Express, so everything in this operation is about getting packages on and off planes as quickly as possible. At many airports (102 by my count), they have Air Freight Centers (AFCs); these are also listed publicly. Employment levels at these AFCs range widely, from 27 in Grand Junction, CO to 643 at JFK. They also have a whole bunch of Ship Centers that are either on airport grounds or just a few blocks away, and I’ve listed these on the map as well. (Note, only the Ship Centers </span><em>close to airports</em><span> are on the map. FedEx has many other Ship Centers, the spoke nodes of the Express system, and you can search for them on </span><a href="https://local.fedex.com/en-us/wa/seattle" rel="">FedEx’s site</a><span>.)</span></p><p>Express operates by a hub-and-spoke logic, which the company pioneered, though the spokes typically have their own mini-spokes as well. For instance, a package coming from Grand Forks, North Dakota will first go to Fargo, North Dakota (KFAR), be processed at its Air Freight Center there, and then go on to FedEx’s world hub in Memphis (KMEM), where it’s sorted to its destination.&nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!ajZN!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b934bcc-876b-4999-aafa-62df958898cb_1540x926.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ajZN!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b934bcc-876b-4999-aafa-62df958898cb_1540x926.png 424w, https://substackcdn.com/image/fetch/$s_!ajZN!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b934bcc-876b-4999-aafa-62df958898cb_1540x926.png 848w, https://substackcdn.com/image/fetch/$s_!ajZN!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b934bcc-876b-4999-aafa-62df958898cb_1540x926.png 1272w, https://substackcdn.com/image/fetch/$s_!ajZN!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b934bcc-876b-4999-aafa-62df958898cb_1540x926.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!ajZN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b934bcc-876b-4999-aafa-62df958898cb_1540x926.png" width="500" height="300.4807692307692" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2b934bcc-876b-4999-aafa-62df958898cb_1540x926.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:875,&quot;width&quot;:1456,&quot;resizeWidth&quot;:500,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ajZN!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b934bcc-876b-4999-aafa-62df958898cb_1540x926.png 424w, https://substackcdn.com/image/fetch/$s_!ajZN!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b934bcc-876b-4999-aafa-62df958898cb_1540x926.png 848w, https://substackcdn.com/image/fetch/$s_!ajZN!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b934bcc-876b-4999-aafa-62df958898cb_1540x926.png 1272w, https://substackcdn.com/image/fetch/$s_!ajZN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b934bcc-876b-4999-aafa-62df958898cb_1540x926.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>FedEx Air Hubs (yellow) and Airports served (orange)</figcaption></figure></div><p>By far the largest Express sort facilities are in Memphis and Indianapolis (KIND), the latter being FedEx’s national hub. KIND has about 180 flights in and out every day, KMEM about 250. Around 5500 people are employed at the facilities around KIND, and more than 20,000 at the facilities around KMEM. Both facilities can handle more than 2 million packages a day; a package can be in and out of a sort facility like this within 15 minutes. In addition to these superhubs, there are smaller regional hubs in Anchorage (PANC), Fort Worth (KAFW), Greensboro (KGSO), Miami (KMIA), Newark (KEWR), Oakland (KOAK), and Ontario (KONT).&nbsp;</p><p>Sortation is a science all its own, and we will devote a separate primer to various sort logics, operations, technologies, etc. But the basic goal of these sort hubs is all the same: get inbound packages organized on conveyor belts so that they can be scanned and kicked to slides/chutes with other packages bound for the same area.&nbsp;</p><div id="youtube2-SNFE_1Rnsu8" data-attrs="{&quot;videoId&quot;:&quot;SNFE_1Rnsu8&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/SNFE_1Rnsu8?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p><span>FedEx Express pickup and delivery drivers are employees of the company. Unlike with Ground, which will send a driver out for the day to do pickups/deliveries, Express drivers typically have to work around </span><a href="https://www.reddit.com/r/FedEx/comments/hawj7d/delivery_routes/" rel="">time-committed packages</a><span>, meeting one or a few loop deadlines for the day, doing on-call pickups, and making a certain number of required delivery attempts. </span></p><p>FedEx Ground was founded in 1985 as Roadway Package System (RPS). It was bought in 1998 by FedEx and rebranded as Ground in 2000.&nbsp;</p><p>Like Express, Ground operates by a hub-and-spoke structure. Let’s say a package is dropped off at a FedEx store in Cloquet, Minnesota, and it’s going to Chandler, Arizona by Ground. It will first get picked up and sent to the local Ground Facility in Superior, Wisconsin (ZDUL), and then sent to the regional Ground Hub in St. Paul (STPL). The Ground Facilities are in green, and the Ground Hubs are in purple. Below is depicted the spoke structure around the St. Paul hub; highlighted are the hub itself and the Superior facility.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!UdK8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c66c0bd-7640-4dca-87bf-63ea676b2ef1_722x824.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!UdK8!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c66c0bd-7640-4dca-87bf-63ea676b2ef1_722x824.png 424w, https://substackcdn.com/image/fetch/$s_!UdK8!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c66c0bd-7640-4dca-87bf-63ea676b2ef1_722x824.png 848w, https://substackcdn.com/image/fetch/$s_!UdK8!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c66c0bd-7640-4dca-87bf-63ea676b2ef1_722x824.png 1272w, https://substackcdn.com/image/fetch/$s_!UdK8!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c66c0bd-7640-4dca-87bf-63ea676b2ef1_722x824.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!UdK8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c66c0bd-7640-4dca-87bf-63ea676b2ef1_722x824.png" width="356" height="406.29362880886424" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4c66c0bd-7640-4dca-87bf-63ea676b2ef1_722x824.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:824,&quot;width&quot;:722,&quot;resizeWidth&quot;:356,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!UdK8!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c66c0bd-7640-4dca-87bf-63ea676b2ef1_722x824.png 424w, https://substackcdn.com/image/fetch/$s_!UdK8!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c66c0bd-7640-4dca-87bf-63ea676b2ef1_722x824.png 848w, https://substackcdn.com/image/fetch/$s_!UdK8!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c66c0bd-7640-4dca-87bf-63ea676b2ef1_722x824.png 1272w, https://substackcdn.com/image/fetch/$s_!UdK8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c66c0bd-7640-4dca-87bf-63ea676b2ef1_722x824.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>FedEx Ground Hub and Spoke Structure around St. Paul Hub (purple)</figcaption></figure></div><p>From STPL, the package goes on a linehaul run to another Ground Hub, in this case the one in Phoenix (PHOE). The hub structure allows FedEx to consolidate shipments for long-distance travel, but in this case in full truckloads rather than on full planes. Then from the PHOE hub it goes to the Chandler Ground Facility (ZCHD), and then out on a delivery truck.</p><div id="youtube2-Xwv3ZOMKlAA" data-attrs="{&quot;videoId&quot;:&quot;Xwv3ZOMKlAA&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/Xwv3ZOMKlAA?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p>Once again, you have the hub-and-spoke structure around the Phoenix hub, but in this case the package goes out toward a spoke facility rather than into a hub from it.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!ZNV-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e918049-bb2b-4c07-94bb-3125ef44280e_966x698.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ZNV-!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e918049-bb2b-4c07-94bb-3125ef44280e_966x698.png 424w, https://substackcdn.com/image/fetch/$s_!ZNV-!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e918049-bb2b-4c07-94bb-3125ef44280e_966x698.png 848w, https://substackcdn.com/image/fetch/$s_!ZNV-!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e918049-bb2b-4c07-94bb-3125ef44280e_966x698.png 1272w, https://substackcdn.com/image/fetch/$s_!ZNV-!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e918049-bb2b-4c07-94bb-3125ef44280e_966x698.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!ZNV-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e918049-bb2b-4c07-94bb-3125ef44280e_966x698.png" width="410" height="296.2525879917184" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4e918049-bb2b-4c07-94bb-3125ef44280e_966x698.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:698,&quot;width&quot;:966,&quot;resizeWidth&quot;:410,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ZNV-!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e918049-bb2b-4c07-94bb-3125ef44280e_966x698.png 424w, https://substackcdn.com/image/fetch/$s_!ZNV-!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e918049-bb2b-4c07-94bb-3125ef44280e_966x698.png 848w, https://substackcdn.com/image/fetch/$s_!ZNV-!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e918049-bb2b-4c07-94bb-3125ef44280e_966x698.png 1272w, https://substackcdn.com/image/fetch/$s_!ZNV-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e918049-bb2b-4c07-94bb-3125ef44280e_966x698.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>FedEx Ground Hub and Spoke Structure around Phoenix Hub (purple)&nbsp;</figcaption></figure></div><p><span>Also represented here in brown are the Regional Sort Facilities, which can process </span><a href="https://www.rev.com/blog/transcripts/fedex-corp-fdx-earnings-call-transcript-q2-2021" rel="">12-15,000 packages/hour</a><span> and are something like inbound facilities for large retailers—entry points that are typically situated close to Ground Hubs. They can also help with sortation during peak season, and so also serve as spillover sortation centers.</span></p><p>Each Ground Hub employs 1,200 people on average. Employment at Ground facilities varies widely, but there are about 200 employees working at each on average. Average employment at a Regional Sort Facility is 575. This is all according to 2024 OSHA ITA data.</p><p><span>Unlike with Express, where drivers are employees of FedEx, Ground P&amp;D drivers work for third-party Independent Service Providers (ISPs), which contract with FedEx to pickup/deliver packages in a specific geographic area. ISP contractors, of which there are roughly </span><a href="https://www.deliveryroutesforsale.com/resources/fedex-isp/" rel="">5900 in the US</a><span>, are required to cover at least 5 routes or 500 daily deliveries. The linehaul drivers are also independent contractors but work for what FedEx calls Transportation Service Providers (TSPs). Both ISPs and TSPs are considered CSPs (Contracted Service Providers).</span></p><p><span>FedEx Freight is its less-than truckload (LTL) business; it handles shipments weighing over 150 lbs. It operates about </span><a href="https://www.truckingdive.com/news/fedex-freight-opens-new-facility-phoenix-market-ltl-dock-door/629458/" rel="">30,000 vehicles out of roughly 400 Service Centers</a><span>, which are included on the map. It also appears to operate by a hub-and-spoke system, though I have not been able to determine the hubs thus far (and will update this primer if I can).</span></p><p><span>Freight has a much higher operating margin than Express or Ground (</span><a href="https://www.freightwaves.com/news/fedex-launches-long-awaited-operational-integration" rel="">20-22% as opposed to 8-9% and 11-12% respectively</a><span>), and the company is in the process of spinning it off into its own company in order to </span><a href="https://www.freightwaves.com/news/fedex-explores-divestment-of-freight-business" rel="">“unlock sustainable shareholder value.”</a><span> At the same time that it is organizationally separating Freight into its own unit, however, it’s also integrating the Ground and Freight networks as part of their much anticipated Network 2.0 initiative. A large number of Freight Service Centers </span><a href="https://finance.yahoo.com/news/fedex-freight-close-7-centers-154905491.html" rel="">have been closed</a><span> in the past two years.</span></p><p>FedEx also has a relatively small Supply Chain segment that provides different logistical services, including warehousing, order fulfillment, freight forwarding, transportation management, and other specialized services. It claims to have 30+ fulfillment centers, 130+ distribution centers, and more than 40 million square feet of warehouse space. </p><p>Again, Supply Chain is a negligible part of FedEx’s overall operation, but they do have some pretty big facilities in that segment. For instance, they operate a distribution center for Target in York, PA and one for T-Mobile in Coppell, TX that each employ about 1,200 people. </p><p><span>In 2022, FedEx announced its intention to create </span><a href="https://www.freightwaves.com/news/fedex-launches-long-awaited-operational-integration" rel="">Network 2.0</a><span>, which will unsilo its previously independent operations. As Satish Jindel explains </span><a href="https://www.freightwaves.com/news/fedex-and-ups-must-adapt-to-changing-parcel-landscape" rel="">here</a><span>, both FedEx and UPS’s Ground operations have become so fast that they’ve unintentionally created competitors for their Express services. The average time that a package spends in the FedEx Ground system is a </span><a href="https://www.youtube.com/watch?v=SDEF0EgQgaI" rel="">mere 1.9 days</a><span>. FedEx is telling its contractors that it is merely piloting network integration at present, but that they should begin to see significant overhaul to the network in 2026 and 2027. FedEx is planning to shrink its distribution network from roughly 1200 facilities to about 1000.</span></p><p>In more remote locations, like Yakima, Kennewick, and Lewiston (pictured below), FedEx will often have flights going in and out of the regional airport and also have a Ground Facility and Freight Service Center. An integrated network will likely not see this same kind of redundant clustering.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!r2H4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdba52f29-e605-476a-b729-6a6b62027a76_1580x354.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!r2H4!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdba52f29-e605-476a-b729-6a6b62027a76_1580x354.png 424w, https://substackcdn.com/image/fetch/$s_!r2H4!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdba52f29-e605-476a-b729-6a6b62027a76_1580x354.png 848w, https://substackcdn.com/image/fetch/$s_!r2H4!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdba52f29-e605-476a-b729-6a6b62027a76_1580x354.png 1272w, https://substackcdn.com/image/fetch/$s_!r2H4!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdba52f29-e605-476a-b729-6a6b62027a76_1580x354.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!r2H4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdba52f29-e605-476a-b729-6a6b62027a76_1580x354.png" width="560" height="125.38461538461539" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/dba52f29-e605-476a-b729-6a6b62027a76_1580x354.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:326,&quot;width&quot;:1456,&quot;resizeWidth&quot;:560,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!r2H4!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdba52f29-e605-476a-b729-6a6b62027a76_1580x354.png 424w, https://substackcdn.com/image/fetch/$s_!r2H4!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdba52f29-e605-476a-b729-6a6b62027a76_1580x354.png 848w, https://substackcdn.com/image/fetch/$s_!r2H4!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdba52f29-e605-476a-b729-6a6b62027a76_1580x354.png 1272w, https://substackcdn.com/image/fetch/$s_!r2H4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdba52f29-e605-476a-b729-6a6b62027a76_1580x354.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Clustered FedEx facilities in Yakima, Kennewick, and Lewiston</figcaption></figure></div></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Autism may be the price of human intelligence, linked to human brain evolution (178 pts)]]></title>
            <link>https://academic.oup.com/mbe/article/42/9/msaf189/8245036?login=false</link>
            <guid>45408994</guid>
            <pubDate>Sun, 28 Sep 2025 23:32:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://academic.oup.com/mbe/article/42/9/msaf189/8245036?login=false">https://academic.oup.com/mbe/article/42/9/msaf189/8245036?login=false</a>, See on <a href="https://news.ycombinator.com/item?id=45408994">Hacker News</a></p>
Couldn't get https://academic.oup.com/mbe/article/42/9/msaf189/8245036?login=false: Error: Request failed with status code 403]]></description>
        </item>
    </channel>
</rss>