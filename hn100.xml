<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 22 Oct 2024 08:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[LTESniffer: An Open-Source LTE Downlink/Uplink Eavesdropper (152 pts)]]></title>
            <link>https://github.com/SysSec-KAIST/LTESniffer</link>
            <guid>41910084</guid>
            <pubDate>Tue, 22 Oct 2024 00:42:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/SysSec-KAIST/LTESniffer">https://github.com/SysSec-KAIST/LTESniffer</a>, See on <a href="https://news.ycombinator.com/item?id=41910084">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">LTESniffer - An Open-source LTE Downlink/Uplink Eavesdropper</h2><a id="user-content-ltesniffer---an-open-source-lte-downlinkuplink-eavesdropper" aria-label="Permalink: LTESniffer - An Open-source LTE Downlink/Uplink Eavesdropper" href="#ltesniffer---an-open-source-lte-downlinkuplink-eavesdropper"></a></p>
<p dir="auto"><strong>LTESniffer</strong> is An Open-source LTE Downlink/Uplink Eavesdropper</p>
<p dir="auto">It first decodes the Physical Downlink Control Channel (PDCCH) to obtain the Downlink Control Informations (DCIs) and Radio Network Temporary Identifiers (RNTIs) of all active users. Using decoded DCIs and RNTIs, LTESniffer further decodes the Physical Downlink Shared Channel (PDSCH) and Physical Uplink Shared Channel (PUSCH) to retrieve uplink and downlink data traffic.</p>
<p dir="auto">LTESniffer supports an API with three functions for security applications and research. Many LTE security research assumes
a passive sniffer that can capture privacy-related packets on the air. However, non of the current open-source sniffers satisfy their requirements as they cannot decode protocol packets in PDSCH and PUSCH. We developed a proof-of-concept security API that supports three tasks that were proposed by previous works: 1) Identity mapping, 2) IMSI collecting, and 3) Capability profiling.</p>
<p dir="auto">Please refer to our <a href="https://syssec.kaist.ac.kr/pub/2023/wisec2023_tuan.pdf" rel="nofollow">paper</a> for more details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">LTESniffer in layman's terms</h2><a id="user-content-ltesniffer-in-laymans-terms" aria-label="Permalink: LTESniffer in layman's terms" href="#ltesniffer-in-laymans-terms"></a></p>
<p dir="auto">LTESniffer is a tool that can capture the LTE wireless messages that are sent between a cell tower and smartphones connected to it. LTESniffer supports capturing the messages in both directions, from the tower to the smartphones, and from the smartphones back to the cell tower.</p>
<p dir="auto">LTESniffer <strong>CANNOT DECRYPT</strong> encrypted messages between the cell tower and smartphones. It can be used for analyzing unencrypted parts of the communication between the cell tower and smartphones. For example, for encrypted messages, it can allow the user to analyze unencrypted parts, such as headers in MAC and physical layers. However, those messages sent in plaintext can be completely analyzable. For example, the broadcast messages sent by the cell tower, or the messages at the beginning of the connection are completely visible.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Ethical Consideration</h2><a id="user-content-ethical-consideration" aria-label="Permalink: Ethical Consideration" href="#ethical-consideration"></a></p>
<p dir="auto">The main purpose of LTESniffer is to support security and analysis research on the cellular network. Due to the collection of uplink-downlink user data, any use of LTESniffer must follow the local regulations on sniffing the LTE traffic. We are not responsible for any illegal purposes such as intentionally collecting user privacy-related information.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">New Update v2.1.0</h3><a id="user-content-new-update-v210" aria-label="Permalink: New Update v2.1.0" href="#new-update-v210"></a></p>
<ul dir="auto">
<li>Supports recording IQ raw data of subframes to file. Please refer to <code>LTESniffer-record-subframe</code> branch and its <a href="https://github.com/SysSec-KAIST/LTESniffer/tree/LTESniffer-record-subframe">README</a> for more details.</li>
<li>Supports offline decoding using recorded files (<a href="https://github.com/SysSec-KAIST/LTESniffer/tree/LTESniffer-record-subframe">README</a>).</li>
<li>Enable API in the downlink mode (only apply for identity collecting and mapping API)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">New Update v2.0.0</h3><a id="user-content-new-update-v200" aria-label="Permalink: New Update v2.0.0" href="#new-update-v200"></a></p>
<ul dir="auto">
<li>Supports two USRP B-series for uplink sniffing mode. Please refer to <code>LTESniffer-multi-usrp</code> branch and its <a href="https://github.com/SysSec-KAIST/LTESniffer/tree/LTESniffer-multi-usrp">README</a> for more details.</li>
<li>Fixed some bugs.</li>
</ul>
<p dir="auto">LTESniffer is implemented on top of <a href="https://github.com/falkenber9/falcon">FALCON</a> with the help of <a href="https://github.com/srsran/srsRAN_4G">srsRAN</a> library. LTESniffer supports:</p>
<ul dir="auto">
<li>Real-time decoding LTE uplink-downlink control-data channels: PDCCH, PDSCH, PUSCH</li>
<li>LTE Advanced and LTE Advanced Pro, up to 256QAM in both uplink and downlink</li>
<li>DCI formats: 0, 1A, 1, 1B, 1C, 2, 2A, 2B</li>
<li>Transmission modes: 1, 2, 3, 4</li>
<li>FDD only</li>
<li>Maximum 20 MHz base station.</li>
<li>Automatically detect maximum UL/DL modulation schemes of smartphones (64QAM/256QAM on DL and 16QAM/64QAM/256QAM on UL)</li>
<li>Automatically detect physical layer configuration per UE.</li>
<li>LTE Security API: RNTI-TMSI mapping, IMSI collecting, UECapability Profiling.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hardware and Software Requirement</h2><a id="user-content-hardware-and-software-requirement" aria-label="Permalink: Hardware and Software Requirement" href="#hardware-and-software-requirement"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">OS Requirement</h3><a id="user-content-os-requirement" aria-label="Permalink: OS Requirement" href="#os-requirement"></a></p>
<p dir="auto">Currently, LTESniffer works stably on Ubuntu 18.04/20.04/22.04.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hardware Requirement</h3><a id="user-content-hardware-requirement" aria-label="Permalink: Hardware Requirement" href="#hardware-requirement"></a></p>
<p dir="auto">Achieving real-time decoding of LTE traffic requires a high-performance CPU with multiple physical cores, especially during peak hours when the base station has many active users. LTESniffer successfully achieved real-time decoding when deployed on an Intel i7-9700K PC, decoding traffic from a base station with 150 active users.</p>
<p dir="auto"><strong>The following hardware is recommended</strong></p>
<ul dir="auto">
<li>Intel i7 CPU with at least 8 physical cores</li>
<li>At least 16Gb RAM</li>
<li>256 Gb SSD storage</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">SDR</h3><a id="user-content-sdr" aria-label="Permalink: SDR" href="#sdr"></a></p>
<p dir="auto">LTESniffer requires different SDR for its uplink and downlink sniffing modes.</p>
<p dir="auto">To sniff only downlink traffic from the base station, LTESniffer is compatible with most SDRs that are supported by the srsRAN library (for example, USRP or BladeRF). The SDR should be connected to the PC via a USB 3.0 port. Also, it should be equipped with GPSDO and two RX antennas to decode downlink messages in transmission modes 3 and 4.</p>
<p dir="auto">On the other hand, to sniff uplink traffic from smartphones to base stations, LTESniffer needs to listen to two different frequencies (Uplink and Downlink) concurrently. To solve this problem, LTESniffer supports two options:</p>
<ul dir="auto">
<li>Using a single USRP X310. USRP X310 has two Local Oscillators (LOs) for 2 RX channels, which can turn each RX channel to a distinct Uplink/Downlink frequency. To use this option, please refer to the <code>main</code> branch of LTESniffer.</li>
<li>Using 2 USRP B-Series. LTESniffer utilizes 2 USRP B-series (B210/B200) for uplink and downlink separately. It achieves synchronization between 2 USRPs by using GPSDO for clock source and time reference. To use this option, please refer to the <code>LTESniffer-multi-usrp</code> branch of LTESniffer and its <a href="https://github.com/SysSec-KAIST/LTESniffer/tree/LTESniffer-multi-usrp">README</a>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><strong>Important note: To avoid unexpected errors, please follow the following steps on Ubuntu 18.04/20.04/22.04.</strong></p>
<p dir="auto"><strong>Dependencies</strong></p>
<ul dir="auto">
<li><strong>Important dependency</strong>: <a href="https://github.com/EttusResearch/uhd">UHD</a> library version &gt;= 4.0 must be installed in advance (recommend building from source). The following steps can be used on Ubuntu 18.04. Refer to UHD Manual for full installation guidance.</li>
</ul>
<p dir="auto">UHD dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt update
sudo apt-get install autoconf automake build-essential ccache cmake cpufrequtils doxygen ethtool \
g++ git inetutils-tools libboost-all-dev libncurses5 libncurses5-dev libusb-1.0-0 libusb-1.0-0-dev \
libusb-dev python3-dev python3-mako python3-numpy python3-requests python3-scipy python3-setuptools \
python3-ruamel.yaml"><pre>sudo apt update
sudo apt-get install autoconf automake build-essential ccache cmake cpufrequtils doxygen ethtool \
g++ git inetutils-tools libboost-all-dev libncurses5 libncurses5-dev libusb-1.0-0 libusb-1.0-0-dev \
libusb-dev python3-dev python3-mako python3-numpy python3-requests python3-scipy python3-setuptools \
python3-ruamel.yaml</pre></div>
<p dir="auto">Clone and build UHD from source (make sure that the current branch is higher than 4.0)</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/EttusResearch/uhd.git
cd <uhd-repo-path>/host
mkdir build
cd build
cmake ../
make -j 4
make test
sudo make install
sudo ldconfig"><pre>git clone https://github.com/EttusResearch/uhd.git
<span>cd</span> <span>&lt;</span>uhd-repo-path<span>&gt;</span>/host
mkdir build
<span>cd</span> build
cmake ../
make -j 4
make <span>test</span>
sudo make install
sudo ldconfig</pre></div>
<p dir="auto">Download firmwares for USRPs:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo uhd_images_downloader"><pre>sudo uhd_images_downloader</pre></div>
<p dir="auto">We use a <a href="https://www.ettus.com/all-products/10gige-kit/" rel="nofollow">10Gb card</a> to connect USRP X310 to PC, refer to UHD Manual <a href="https://files.ettus.com/manual/page_usrp_x3x0.html" rel="nofollow">[1]</a>, <a href="https://files.ettus.com/manual/page_usrp_x3x0_config.html" rel="nofollow">[2]</a> to configure USRP X310 and 10Gb card interface. For USRP B210, it should be connected to PC via a USB 3.0 port.</p>
<p dir="auto">Test the connection and firmware (for USRP X310 only):</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo sysctl -w net.core.rmem_max=33554432
sudo sysctl -w net.core.wmem_max=33554432
sudo ifconfig <10Gb card interface> mtu 9000
sudo uhd_usrp_probe"><pre>sudo sysctl -w net.core.rmem_max=33554432
sudo sysctl -w net.core.wmem_max=33554432
sudo ifconfig <span>&lt;</span>10Gb card interface<span>&gt;</span> mtu 9000
sudo uhd_usrp_probe</pre></div>
<ul dir="auto">
<li>srsRAN dependencies:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev"><pre>sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev</pre></div>
<ul dir="auto">
<li>LTESniffer dependencies:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev"><pre>sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev</pre></div>
<p dir="auto"><strong>Build LTESniffer from source:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/SysSec-KAIST/LTESniffer.git
cd LTESniffer
mkdir build
cd build
cmake ../
make -j 4 (use 4 threads)"><pre>git clone https://github.com/SysSec-KAIST/LTESniffer.git
<span>cd</span> LTESniffer
mkdir build
<span>cd</span> build
cmake ../
make -j 4 (use 4 threads)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">LTESniffer has 3 main functions:</p>
<ul dir="auto">
<li>Sniffing LTE downlink traffic from the base station</li>
<li>Sniffing LTE uplink traffic from smartphones</li>
<li>Security API</li>
</ul>
<p dir="auto">After building from source, <code>LTESniffer</code> is located in <code>&lt;build-dir&gt;/src/LTESniffer</code></p>
<p dir="auto">Note that before using LTESniffer on the commercial, one should have to check the local regulations on sniffing LTE traffic, as we explained in the <strong>Ethical Consideration</strong>.</p>
<p dir="auto">To figure out the base station and Uplink-Downlink band the test smartphone is connected to, install <a href="https://play.google.com/store/apps/details?id=make.more.r2d2.cellular_z&amp;hl=en&amp;gl=US&amp;pli=1" rel="nofollow">Cellular-Z</a> app on the test smartphone (the app only supports Android). It will show the cell ID and Uplink-Downlink band/frequency to which the test smartphone is connected. Make sure that LTESniffer also connects to the same cell and frequency.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">General downlink sniffing</h3><a id="user-content-general-downlink-sniffing" aria-label="Permalink: General downlink sniffing" href="#general-downlink-sniffing"></a></p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/SysSec-KAIST/LTESniffer/blob/main/png/dl_mode_png.png"><img src="https://github.com/SysSec-KAIST/LTESniffer/raw/main/png/dl_mode_png.png" alt="LTESniffer Downlink Mode"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo ./<build-dir>/src/LTESniffer -A 2 -W <number of threads> -f <DL Freq> -C -m 0
example: sudo ./src/LTESniffer -A 2 -W 4 -f 1840e6 -C -m 0
-A: number of antennas
-W: number of threads
-f: downlink frequency
-C: turn on cell search
-m: sniffer mode, 0 for downlink sniffing and 1 for uplink sniffing"><pre>sudo ./<span>&lt;</span>build-dir<span>&gt;</span>/src/LTESniffer -A 2 -W <span>&lt;</span>number of threads<span>&gt;</span> -f <span>&lt;</span>DL Freq<span>&gt;</span> -C -m 0
example: sudo ./src/LTESniffer -A 2 -W 4 -f 1840e6 -C -m 0
-A: number of antennas
-W: number of threads
-f: downlink frequency
-C: turn on cell search
-m: sniffer mode, 0 <span>for</span> downlink sniffing and 1 <span>for</span> uplink sniffing</pre></div>
<p dir="auto">Note: to run <code>LTESniffer</code> with USRP B210 in the downlink mode, add option <code>-a "num_recv_frames=512" </code> to the command line.
This option extends the receiving buffer for USRP B210 to achieve better synchronization.</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo ./<build-dir>/src/LTESniffer -A 2 -W <number of threads> -f <DL Freq> -C -m 0 -a &quot;num_recv_frames=512&quot;
example: sudo ./src/LTESniffer -A 2 -W 4 -f 1840e6 -C -m 0 -a &quot;num_recv_frames=512&quot;"><pre>sudo ./<span>&lt;</span>build-dir<span>&gt;</span>/src/LTESniffer -A 2 -W <span>&lt;</span>number of threads<span>&gt;</span> -f <span>&lt;</span>DL Freq<span>&gt;</span> -C -m 0 -a <span><span>"</span>num_recv_frames=512<span>"</span></span>
example: sudo ./src/LTESniffer -A 2 -W 4 -f 1840e6 -C -m 0 -a <span><span>"</span>num_recv_frames=512<span>"</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">General uplink sniffing</h3><a id="user-content-general-uplink-sniffing" aria-label="Permalink: General uplink sniffing" href="#general-uplink-sniffing"></a></p>
<p dir="auto">Note: In the uplink sniffing mode, the test smartphones should be located nearby the sniffer, because the uplink signal power from UE is significantly weaker compared to the downlink signal from the base station.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/SysSec-KAIST/LTESniffer/blob/main/png/ul_mode_png.png"><img src="https://github.com/SysSec-KAIST/LTESniffer/raw/main/png/ul_mode_png.png" alt="LTESniffer Uplink Mode"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo ./<build-dir>/src/LTESniffer -A 2 -W <number of threads> -f <DL Freq> -u <UL Freq> -C -m 1
example: sudo ./src/LTESniffer -A 2 -W 4 -f 1840e6 -u 1745e6 -C -m 1
-u: uplink frequency"><pre>sudo ./<span>&lt;</span>build-dir<span>&gt;</span>/src/LTESniffer -A 2 -W <span>&lt;</span>number of threads<span>&gt;</span> -f <span>&lt;</span>DL Freq<span>&gt;</span> -u <span>&lt;</span>UL Freq<span>&gt;</span> -C -m 1
example: sudo ./src/LTESniffer -A 2 -W 4 -f 1840e6 -u 1745e6 -C -m 1
-u: uplink frequency</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Security API</h3><a id="user-content-security-api" aria-label="Permalink: Security API" href="#security-api"></a></p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/SysSec-KAIST/LTESniffer/blob/main/png/api_png.png"><img src="https://github.com/SysSec-KAIST/LTESniffer/raw/main/png/api_png.png" alt="LTESniffer API Mode"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo ./<build-dir>/src/LTESniffer -A 2 -W <number of threads> -f <DL Freq> -u <UL Freq> -C -m 1 -z 3
example: sudo ./src/LTESniffer -A 2 -W 4 -f 1840e6 -u 1745e6 -C -m 1 -z 3
-z: 3 for turnning on 3 functions of sniffer, which are identity mapping, IMSI collecting, and UECapability profiling.
    2 for UECapability profiling
    1 for IMSI collecting
    0 for identity mapping"><pre>sudo ./<span>&lt;</span>build-dir<span>&gt;</span>/src/LTESniffer -A 2 -W <span>&lt;</span>number of threads<span>&gt;</span> -f <span>&lt;</span>DL Freq<span>&gt;</span> -u <span>&lt;</span>UL Freq<span>&gt;</span> -C -m 1 -z 3
example: sudo ./src/LTESniffer -A 2 -W 4 -f 1840e6 -u 1745e6 -C -m 1 -z 3
-z: 3 <span>for</span> turnning on 3 functions of sniffer, which are identity mapping, IMSI collecting, and UECapability profiling.
    2 <span>for</span> UECapability profiling
    1 <span>for</span> IMSI collecting
    0 <span>for</span> identity mapping</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Specify a base station</h3><a id="user-content-specify-a-base-station" aria-label="Permalink: Specify a base station" href="#specify-a-base-station"></a></p>
<p dir="auto">LTESniffer can sniff on a specific base station by using options <code>-I &lt;Phycial Cell ID (PCI)&gt; -p &lt;number of Physical Resource Block (PRB)&gt;</code>. In this case, LTESniffer does not do the cell search but connects directly to the specified cell.</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo ./<build-dir>/src/LTESniffer -A 2 -W <number of threads> -f <DL Freq> -I <PCI> -p <PRB> -m 0
sudo ./<build-dir>/src/LTESniffer -A 2 -W <number of threads> -f <DL Freq> -u <UL Freq> -I <PCI> -p <PRB> -m 1
example: sudo ./src/LTESniffer -A 2 -W 4 -f 1840e6 -u 1745e6 -I 379 -p 100 -m 1"><pre>sudo ./<span>&lt;</span>build-dir<span>&gt;</span>/src/LTESniffer -A 2 -W <span>&lt;</span>number of threads<span>&gt;</span> -f <span>&lt;</span>DL Freq<span>&gt;</span> -I <span>&lt;</span>PCI<span>&gt;</span> -p <span>&lt;</span>PRB<span>&gt;</span> -m 0
sudo ./<span>&lt;</span>build-dir<span>&gt;</span>/src/LTESniffer -A 2 -W <span>&lt;</span>number of threads<span>&gt;</span> -f <span>&lt;</span>DL Freq<span>&gt;</span> -u <span>&lt;</span>UL Freq<span>&gt;</span> -I <span>&lt;</span>PCI<span>&gt;</span> -p <span>&lt;</span>PRB<span>&gt;</span> -m 1
example: sudo ./src/LTESniffer -A 2 -W 4 -f 1840e6 -u 1745e6 -I 379 -p 100 -m 1</pre></div>
<p dir="auto">The debug mode can be enabled by using option <code>-d</code>. In this case, the debug messages will be printed on the terminal.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Output of LTESniffer</h3><a id="user-content-output-of-ltesniffer" aria-label="Permalink: Output of LTESniffer" href="#output-of-ltesniffer"></a></p>
<p dir="auto">LTESniffer provides pcap files in the output. The pcap file can be opened by WireShark for further analysis and packet trace.
The name of downlink pcap file: <code>sniffer_dl_mode.pcap</code>, uplink pcap file: <code>sniffer_ul_mode.pcap</code>, and API pcap file: <code>api_collector.pcap</code>.
The pcap files are located in the same directory <code>LTESniffer</code> has been executed.
To enable the WireShark to analyze the decoded packets correctly, please refer to the WireShark configuration guide <a href="https://github.com/SysSec-KAIST/LTESniffer/blob/main/pcap_file_example/README.md">here</a>. There are also some examples of pcap files in the link.<br>
<strong>Note:</strong> The uplink pcap file contains both uplink and downlink messages. On the WireShark, use this filter to monitor only uplink messages: <code>mac-lte.direction == 0</code>; or this filter to monitor only downlink messages: <code>mac-lte.direction == 1</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Application Note</h2><a id="user-content-application-note" aria-label="Permalink: Application Note" href="#application-note"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Distance for uplink sniffing</h3><a id="user-content-distance-for-uplink-sniffing" aria-label="Permalink: Distance for uplink sniffing" href="#distance-for-uplink-sniffing"></a></p>
<p dir="auto">The effective range for sniffing uplink is limited in LTESniffer due to the capability of the RF front-end of the hardware (i.e. SDR). The uplink signal power from UE is significantly weaker compared to the downlink signal because UE is a handheld device that optimizes battery usage, while the eNB uses sufficient power to cover a large area. To successfully capture the uplink traffic, LTESniffer can increase the strength of the signal power by i) being physically close to the UE, or ii) improving the signal reception capability with specialized hardware, such as a directional antenna, dedicated RF front-end, and signal amplifier.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The information displayed on the terminal</h3><a id="user-content-the-information-displayed-on-the-terminal" aria-label="Permalink: The information displayed on the terminal" href="#the-information-displayed-on-the-terminal"></a></p>
<p dir="auto"><strong>Downlink Sniffing Mode</strong></p>
<p dir="auto"><code>Processed 1000/1000 subframes</code>: Number of subframes was processed by LTESniffer last 1 second. There are 1000 LTE subframes per second by design. <br>
<code>RNTI</code>: Radio Network Temporary Identifier of UEs. <br>
<code>Table</code>: The maximum modulation scheme that is used by smartphones in downlink. LTESniffer supports up to 256QAM in the downlink. Refer to our <a href="https://syssec.kaist.ac.kr/pub/2023/wisec2023_tuan.pdf" rel="nofollow">paper</a> for more details. <br>
<code>Active</code>: Number of detected messages of RNTIs. <br>
<code>Success</code>: Number of successfully decoded messages over number of detected messages (<code>Active</code>). <br>
<code>New TX, ReTX, HARQ, Normal</code>: Statistic of new messages and retransmitted messages. This function is in development. <br>
<code>W_MIMO, W_pinfor, Other</code>: Number of messages with wrong radio configuration, only for debugging.</p>
<p dir="auto"><strong>Uplink Sniffing Mode</strong></p>
<p dir="auto"><code>Max Mod</code>: The maximum modulation scheme that is used by smartphones in uplink. It can be 16/64/256QAM depending on the support of smartphones and the configuration of the network. Refer to our <a href="https://syssec.kaist.ac.kr/pub/2023/wisec2023_tuan.pdf" rel="nofollow">paper</a> for more details. <br>
<code>SNR</code>: Signal-to-noise ratio (dB). Low SNR means the uplink signal quality from the smartphone is bad. One possible reason is the smartphone is far from the sniffer. <br>
<code>DL-UL_delay</code>: The average of time delay between downlink signal from the base station and uplink signal from the smartphone. <br>
<code>Other Info</code>: Information only for debugging.</p>
<p dir="auto"><strong>API Mode</strong></p>
<p dir="auto"><code>Detected Identity</code>: The name of detected identity. <br>
<code>Value</code>: The value of detected identity. <br>
<code>From Message</code>: The name of the message that contains the detected identity.</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Credits</h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<p dir="auto">We sincerely appreciate the <a href="https://github.com/falkenber9/falcon">FALCON</a> and <a href="https://github.com/srsran/srsRAN_4G">SRS team</a> for making their great softwares available.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributor</h2><a id="user-content-contributor" aria-label="Permalink: Contributor" href="#contributor"></a></p>
<p dir="auto">Special thanks to all the contributors who helped us to fix bugs and improve LTESniffer</p>
<ol dir="auto">
<li><a href="https://github.com/cellular777">@cellular777</a></li>
<li><a href="https://www.youtube.com/@cemaxecuter7783" rel="nofollow">Cemaxecuter</a></li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">BibTex</h2><a id="user-content-bibtex" aria-label="Permalink: BibTex" href="#bibtex"></a></p>
<p dir="auto">Please refer to our <a href="https://syssec.kaist.ac.kr/pub/2023/wisec2023_tuan.pdf" rel="nofollow">paper</a> for more details.</p>
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{hoang:ltesniffer,
  title = {{LTESniffer: An Open-source LTE Downlink/Uplink Eavesdropper}},
  author = {Hoang, Dinh Tuan and Park, CheolJun and Son, Mincheol and Oh, Taekkyung and Bae, Sangwook and Ahn, Junho and Oh, BeomSeok and Kim, Yongdae},
  booktitle = {16th ACM Conference on Security and Privacy in Wireless and Mobile Networks (WiSec '23)},
  year = {2023}
}"><pre><span>@inproceedings</span>{<span>hoang:ltesniffer</span>,
  <span>title</span> = <span><span>{</span>{LTESniffer: An Open-source LTE Downlink/Uplink Eavesdropper}<span>}</span></span>,
  <span>author</span> = <span><span>{</span>Hoang, Dinh Tuan and Park, CheolJun and Son, Mincheol and Oh, Taekkyung and Bae, Sangwook and Ahn, Junho and Oh, BeomSeok and Kim, Yongdae<span>}</span></span>,
  <span>booktitle</span> = <span><span>{</span>16th ACM Conference on Security and Privacy in Wireless and Mobile Networks (WiSec '23)<span>}</span></span>,
  <span>year</span> = <span><span>{</span>2023<span>}</span></span>
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>

<p dir="auto"><strong>Q:</strong> Is it mandatory to use GPSDO with the USRP in order to run LTESniffer? <br>
<strong>A:</strong> GPSDO is useful for more stable synchronization. However, for downlink sniffing mode, LTESniffer still can synchronize with the LTE signal to decode the packets without GPSDO. For uplink sniffing mode, GPSDO is only required when using 2 USRP B-series, as it is the time and clock reference sources for synchrozation between uplink and downlink channels. Another uplink SDR option, using a single USRP X310, does not require GPSDO.</p>
<p dir="auto"><strong>Q:</strong> For downlink traffic, can I use a cheaper SDR? <br>
<strong>A:</strong> Technically, any SDRs supported by srsRAN library such as Blade RF can be used to run LTESniffer in the downlink sniffing mode. However, we only tested the downlink sniffing function of LTESniffer with USRP B210 and X310.</p>
<p dir="auto"><strong>Q:</strong> Is it illegal to use LTESniffer to sniff the LTE traffic? <br>
<strong>A:</strong> You should have to check the local regulations on sniffing (unencrypted) LTE traffic. Another way to test LTESniffer is setting up a personal LTE network by using <a href="https://github.com/srsran/srsRAN_4G">srsRAN</a> - an open-source LTE implementation in a Faraday cage.</p>
<p dir="auto"><strong>Q:</strong> Can LTESniffer be used to view the content of messages between two users? <br>
<strong>A:</strong> One can see only the "unencrypted" part of the messages. Note that the air traffic between the base station and users is mostly encrypted.</p>
<p dir="auto"><strong>Q:</strong> Is there any device identity exposed in plaintext in the LTE network? <br>
<strong>A:</strong> Yes, literature shows that there are multiple identities exposed, such as TMSI, GUTI, IMSI, and RNTI. Please refer to the academic literature for more details. e.g. <a href="https://syssec.kaist.ac.kr/pub/2022/sec22summer_bae.pdf" rel="nofollow">Watching the Watchers: Practical Video Identification Attack in LTE Networks</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple's AirPods Pro hearing health features (167 pts)]]></title>
            <link>https://www.theverge.com/24275178/apple-airpods-pro-hearing-aid-test-protection-preview</link>
            <guid>41909967</guid>
            <pubDate>Tue, 22 Oct 2024 00:25:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/24275178/apple-airpods-pro-hearing-aid-test-protection-preview">https://www.theverge.com/24275178/apple-airpods-pro-hearing-aid-test-protection-preview</a>, See on <a href="https://news.ycombinator.com/item?id=41909967">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><p>Apple announced a trio of major new hearing health features for the AirPods Pro 2 in September, including clinical-grade hearing aid functionality, a hearing test, and more robust hearing protection. All three will roll out next week with the release of iOS 18.1, and they <a href="https://www.theverge.com/2024/9/12/24241960/apple-airpods-pro-2-otc-hearing-aid-health-industry">could mark a watershed moment</a> for hearing health awareness. Apple is about to instantly turn the world’s most popular earbuds into an over-the-counter hearing aid.</p><p>That also means we’re about to enter an era where we’ll need to get comfortable with people wearing earbuds at all times. There’s a perception that leaving your earbuds in while talking with other people is rude. Transparency mode in many of today’s earbuds sounds totally natural and lifelike, yet I still constantly remove my buds to show someone they’ve got my undivided attention. That way of thinking has to change when popular earbuds start pulling double duty as hearing aids. It’s a powerful way to reduce the stigma that’s all too common with hearing aids, but this shift will take time.</p><p>Over the last several days, I’ve been able to preview Apple’s hearing health features. At times, the experience has been emotionally intense. I’m someone who grew up with a Discman and iPod basically attached to my hip, and I’ve been to countless concerts over the decades. I also haven’t seen an audiologist since 2018 or so. That’s anything but unusual; Apple says 80 percent of adults in the US haven’t had their hearing checked in at least five years. Putting a test right on your iPhone is a great way to improve that trend.</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Hands-on photos of Apple’s AirPods Pro hearing health features." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/376x251/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/384x256/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/415x277/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/480x320/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/540x360/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/640x427/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/750x500/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/828x552/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1080x720/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1200x800/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1440x960/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1920x1280/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2048x1365/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2400x1600/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2400x1600/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><p><figcaption><em>The AirPods Pro 2 are transforming into something much more than wireless earbuds.</em></figcaption></p></div><p><h3>Prevention: hearing protection</h3></p><p>Did you know there are people who’ve already been <a href="https://www.tomsguide.com/features/i-used-airpods-pro-instead-of-ear-plugs-at-a-concert-heres-what-happened">replacing earplugs with the AirPods Pro</a> at concerts? Until this fall, Apple had never endorsed such a use case or advertised its earbuds as hearing protection devices. The company knew people were doing it but kept quiet on the subject. </p><p>That’s now changed. With iOS 18.1 and the soon-to-be-released AirPods firmware update, the AirPods Pro 2 will offer hearing protection at all times across noise cancellation, transparency, and adaptive audio modes. There’s no “concert mode” or a specific setting to toggle. You can think of this as an expansion of the loud sound reduction option that was already in place. Hearing protection is on by default, and Apple says “an all-new multiband high dynamic range algorithm” helps to preserve the natural sound of concerts and other live events.</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="A chart that shows the effectiveness of Apple’s hearing protection in the AirPods Pro 2." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/376x360/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/384x368/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/415x398/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/480x460/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/540x517/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/640x613/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/750x719/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/828x793/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/1080x1035/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/1200x1150/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/1440x1380/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/1920x1839/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/2048x1962/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/2400x2299/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/2400x2299/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>This chart shows the effectiveness of Apple’s hearing protection across the different sound modes of the AirPods Pro 2.</em></figcaption> <p><cite>Image: Apple</cite></p></div></div><p>Which listening mode you’ll use for concerts comes down to personal preference. I’ve found adaptive audio works well since it lets you customize whether you prefer more noise cancellation or more passthrough. But even in full transparency mode, some level of hearing protection is active. The more noise cancellation that’s applied, the longer you can remain in relatively loud environments. </p><p>There are limits to what the AirPods Pro 2 can handle; Apple’s hearing protection isn’t cut out for extremely loud, sudden noises like gunfire, fireworks, or a jackhammer. Sustained noises over 110dBA are also too much for the earbuds. Some clubs and concerts can definitely exceed that threshold, so be aware.</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="A photo of a person wearing Apple’s AirPods Pro 2." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/376x251/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/384x256/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/415x277/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/480x320/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/540x360/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/640x427/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/750x500/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/828x552/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1080x720/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1200x800/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1440x960/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1920x1280/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2048x1365/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2400x1600/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2400x1600/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><p><figcaption><em>The AirPods Pro aren’t as discreet as some hearing aids, but their popularity and ubiquity could help reduce stigma.</em></figcaption></p></div><p><h3>Awareness: the Apple hearing test</h3></p><p>You’ll need a quiet space when taking Apple’s hearing test. Before getting started, your iPhone will do a quick analysis of ear tip fit and environmental noise to ensure you’re good to go. All of these hearing health features are calibrated for Apple’s stock silicone tips, so if you’re using aftermarket third-party tips (including foam), there’s no guarantee you’ll get the optimal experience. Once the test begins, you just tap the screen whenever you hear any of the three-beep tone sequences. </p><p>There are a few key things to know about Apple’s hearing test. For one, it’s designed so that you can’t predict or game it. The test can play any frequency at any time, so no two are the same. Apple tests your left ear first, and here’s something I wish I’d known going in: it’s <em>completely normal</em> to hear nothing at all for several seconds at a time. It was in those moments, when five, six, or even 10 seconds would pass without an obvious tone sequence, where I’d start feeling pretty anxious.&nbsp;</p><p>My best advice is to avoid wondering if you <em>should</em> be hearing something at a given moment and instead just focus on the tones as they come. Some can be incredibly faint. There are visual cues that let you know the test is still moving along even during silence — the most obvious one being a large circle that animates onscreen throughout the process. (You’ll also notice a progress dial for each ear that fills as you take it.) </p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="A screenshot of Apple hearing test results." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/376x205/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/384x209/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/415x226/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/480x262/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/540x294/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/640x349/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/750x409/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/828x451/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/1080x589/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/1200x654/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/1440x785/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/1920x1046/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/2048x1116/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/2400x1308/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/2400x1308/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>My left ear has a bit more hearing loss than my right.</em></figcaption> <p><cite>Screenshot: Chris Welch / The Verge</cite></p></div></div><p>I took the test twice several days apart, and my results for both ears fall under “little to no hearing loss.” Having recently turned 40, I’ll take that. The ranges are as follows:</p><div><ul><li>Little to no loss: up to 25dBHL</li><li>Mild loss: 26–40dBHL</li><li>Moderate loss: 41–60dBHL</li><li>Severe loss: 61–80dBHL</li><li>Profound loss: above 80dBHL</li></ul></div><p>I also learned that my left ear has definitely lost a bit more over the years than my right, which is something I’ve never noticed in daily life. The slight difference between my two tests is exactly the margin that Apple expects for people who take it multiple times. Results are stored in the Health app, where you can export individual tests (or all of them) as a PDF. Here’s how one of mine charts out: </p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="A screenshot of Apple hearing test chart data." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/376x363/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/384x371/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/415x401/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/480x464/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/540x522/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/640x619/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/750x725/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/828x800/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/1080x1044/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/1200x1160/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/1440x1392/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/1920x1856/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/2048x1980/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/2400x2320/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/2400x2320/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>Hearing test results can easily be accessed from the Health app.</em></figcaption> <p><cite>Screenshot: Chris Welch / The Verge</cite></p></div></div><p>You can also import charts from tests you’ve taken with a hearing professional. This data is what’s used to configure the hearing aid feature. </p><p>Apple’s hearing test only takes about five minutes, but it felt like a <em>long</em> five minutes for me and everyone else I’ve let try it. Again, that’s probably because it’s been too long since many of us have done this. The second time wasn’t nearly as stressful. I can definitely see the hearing test having a viral moment on TikTok and other social media, which seems like a great thing for awareness all around. Hearing loss is incredibly common: 1.5 billion people around the world are living with some extent of it, according to the World Health Organization.</p><p><h3>Assistance: AirPods as hearing aids</h3></p><p>For those 18 years and older with mild to moderate hearing loss, the AirPods Pro 2 can now serve as a clinical-grade hearing aid. Once enabled, you can also toggle on a “Media Assist” setting that uses your hearing test results to optimize the sound of music, phone calls, and video content. </p><p>Within the settings menu, you can use sliders to fine-tune the hearing aid feature’s amplification, tone, and balance. These options are also accessible via Control Center on an iPhone, iPad, or Mac. Much like you can slide a finger on the AirPods Pro 2 stem to adjust volume, you can use that same gesture to control amplification when the hearing aid mode is active. You can only use the hearing aid feature when in transparency mode. Apple’s instructions for the hearing aid feature advise that it takes time — in some cases, weeks — for customers to get fully accustomed to the sound. </p><p>Jerry Saltz, the art critic at <em>New York </em>Magazine (and someone with diagnosed hearing loss), took Apple’s hearing test at our Manhattan office and was impressed in his brief time trying out the hearing aid feature. But no one has had access to it long enough for an in-depth assessment. I’m sure there’ll be detailed comparisons between the AirPods Pro 2 and existing OTC devices in the near future once iOS 18.1 is widely available.</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="A photo showing the hearing aid setting on an iPhone in a crowded bar." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/376x251/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/384x256/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/415x277/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/480x320/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/540x360/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/640x427/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/750x500/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/828x552/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1080x720/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1200x800/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1440x960/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1920x1280/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2048x1365/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2400x1600/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2400x1600/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><p><figcaption><em>You can make hearing aid adjustments from the iOS settings menu.</em></figcaption></p></div><p>Being able to use Apple’s $250 earbuds as a hearing aid is a huge deal for those who can benefit from this capability. That’s substantially less expensive than over-the-counter hearing aids from <a href="https://www.jabraenhance.com/product">Jabra</a>, <a href="https://hearing.electronics.sony.com/lp2_hearingaids">Sony</a>, and other brands. But the AirPods won’t be right for everyone. People with more severe hearing loss will still need to seek out other solutions (including those pricier products). And the main tradeoff with the AirPods Pro 2 is battery life: they can last for around six hours with the hearing aid engaged, which doesn’t match what you’ll get from many OTC and prescription hearing aids. </p><p>But this is a big milestone — and it seems inevitable that Samsung, Google, and other tech heavyweights will follow Apple’s lead fairly quickly. I’m all for that, even if it feels strange that hearing aid functionality has become the latest aspect of ecosystem lock-in. We spent a long time bemoaning the loss of the headphone jack. With advancements like this, and earbuds helping to improve so many people’s quality of life, we’re finally starting to see a worthwhile payoff. </p><p><em>Photography by Chris Welch / The Verge</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Math is still catching up to the genius of Ramanujan (391 pts)]]></title>
            <link>https://www.quantamagazine.org/srinivasa-ramanujan-was-a-genius-math-is-still-catching-up-20241021/</link>
            <guid>41909564</guid>
            <pubDate>Mon, 21 Oct 2024 23:19:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/srinivasa-ramanujan-was-a-genius-math-is-still-catching-up-20241021/">https://www.quantamagazine.org/srinivasa-ramanujan-was-a-genius-math-is-still-catching-up-20241021/</a>, See on <a href="https://news.ycombinator.com/item?id=41909564">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-role="selectable">
    <p><span>O</span>ne afternoon in January 2011, <a href="https://webusers.imj-prg.fr/~hussein.mourtada/">Hussein Mourtada</a> leapt onto his desk and started dancing. He wasn’t alone: Some of the graduate students who shared his Paris office were there, too. But he didn’t care. The mathematician realized that he could finally confirm a sneaking suspicion he’d first had while writing his doctoral dissertation, which he’d finished a few months earlier. He’d been studying special points, called singularities, where curves cross themselves or come to sharp turns. Now he had unexpectedly found what he’d been looking for, a way to prove that these singularities had a surprisingly deep underlying structure. Hidden within that structure were mysterious mathematical statements first written down a century earlier by a young Indian mathematician named Srinivasa Ramanujan. They had come to him in a dream.</p>
<p>Ramanujan brings life to the myth of the self-taught genius. He grew up poor and uneducated and did much of his research while isolated in southern India, barely able to afford food. In 1912, when he was 24, he began to send a series of letters to prominent mathematicians. These were mostly ignored, but one recipient, the English mathematician G.H. Hardy, corresponded with Ramanujan for a year and eventually persuaded him to come to England, smoothing the way with the colonial bureaucracies.</p>
<p>It became apparent to Hardy and his colleagues that Ramanujan could sense mathematical truths — could access entire worlds — that others simply could not. (Hardy, a mathematical giant in his own right, is said to have quipped that his greatest contribution to mathematics was the discovery of Ramanujan.) Before Ramanujan died in 1920 at the age of 32, he came up with thousands of elegant and surprising results, often without proof. He was fond of saying that his equations had been bestowed on him by the gods.</p>
<p>More than 100 years later, mathematicians are still trying to catch up to Ramanujan’s divine genius, as his visions appear again and again in disparate corners of the world of mathematics.</p>

<p>Ramanujan is perhaps most famous for coming up with partition identities, equations about the different ways you can break a whole number up into smaller parts (such as 7 = 5 + 1 + 1). In the 1980s, mathematicians began to find deep and surprising connections between these equations and other areas of mathematics: in statistical mechanics and the study of phase transitions, in knot theory and string theory, in number theory and representation theory and the study of symmetries.</p>
<p>Most recently, they’ve appeared in Mourtada’s work on curves and surfaces that are defined by algebraic equations, an area of study called algebraic geometry. Mourtada and his collaborators have spent more than a decade trying to better understand that link, and to exploit it to uncover rafts of brand-new identities that resemble those Ramanujan wrote down.</p>
<p>“It turned out that these kinds of results have basically occurred in almost every branch of mathematics. That’s an amazing thing,” said <a href="https://people.smp.uq.edu.au/OleWarnaar/">Ole Warnaar</a> of the University of Queensland in Australia. “It’s not just a happy coincidence. I don’t want to sound religious, but the mathematical god is trying to tell us something.”</p>
<h2><strong>New Worlds </strong></h2>
<p>Ramanujan’s mathematical prowess was obvious to those who knew him. Without formal training, he excelled; by the time he was in high school he had devoured advanced, though often outdated, textbooks, and was doing independent research on different kinds of numerical properties and patterns.</p>
<p>In 1904, he was granted a full scholarship to the Government Arts College in Kumbakonam, the small city where he had grown up, in what is now the Indian state of Tamil Nadu. But he ignored all subjects besides math and lost his scholarship within a year. He later enrolled in another university, this time in Madras (now Chennai), the provincial capital some 250 kilometers north. Again he flunked out.</p>
<figure>
    <p><img width="1300" height="1041" src="https://www.quantamagazine.org/wp-content/uploads/2024/10/Ramanujan-Missing-Boy_cr-The-Hindu.webp" alt="" decoding="async" loading="lazy" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/10/Ramanujan-Missing-Boy_cr-The-Hindu.webp 1300w, https://www.quantamagazine.org/wp-content/uploads/2024/10/Ramanujan-Missing-Boy_cr-The-Hindu-520x416.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2024/10/Ramanujan-Missing-Boy_cr-The-Hindu-768x615.webp 768w" sizes="(max-width: 1300px) 100vw, 1300px">    </p>
            <figcaption>
                            <p>After failing out of college, Ramanujan ran away from home, prompting his mother to post a missing-person notice in <em>The Hindu</em>.</p>
            <p><em>The Hindu</em></p>
        </figcaption>
    </figure>

<p>He continued his research on his own for years, often while in poor health. During that time, he tutored students in math to support himself. Eventually he secured a job as a clerk at the Madras Port Trust in 1912. He pursued mathematics on the side and published some of his results in Indian journals.</p>
<p>Hoping to get some of his work into more prestigious and widely read publications, Ramanujan wrote letters to several British mathematicians, enclosing pages of findings for their review. “I have not trodden through the conventional regular course which is followed in a university course,” he wrote, “but I am striking out a new path for myself.” Among the recipients was Hardy, an expert in number theory and analysis at the University of Cambridge.</p>
<figure>
    <p><img width="1400" height="2106" src="https://www.quantamagazine.org/wp-content/uploads/2024/10/FirstLetterLastPage-Courtesy_KenOno.webp" alt="" decoding="async" loading="lazy" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/10/FirstLetterLastPage-Courtesy_KenOno.webp 1400w, https://www.quantamagazine.org/wp-content/uploads/2024/10/FirstLetterLastPage-Courtesy_KenOno-1143x1720.webp 1143w, https://www.quantamagazine.org/wp-content/uploads/2024/10/FirstLetterLastPage-Courtesy_KenOno-346x520.webp 346w, https://www.quantamagazine.org/wp-content/uploads/2024/10/FirstLetterLastPage-Courtesy_KenOno-768x1155.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2024/10/FirstLetterLastPage-Courtesy_KenOno-1021x1536.webp 1021w, https://www.quantamagazine.org/wp-content/uploads/2024/10/FirstLetterLastPage-Courtesy_KenOno-1361x2048.webp 1361w" sizes="(max-width: 1400px) 100vw, 1400px">    </p>
            <figcaption>
                            <p>Ramanujan’s first letter to G.H. Hardy included formulas (5), (6) and (7), strange nested fractions that Hardy said “defeated me completely; I had never seen anything in the least like them before.”</p>
            <p>Courtesy of Ken Ono</p>
        </figcaption>
    </figure>

<p>Hardy was shocked at what he saw. Ramanujan had identified and then solved a number of continued fractions — expressions that can be written as infinite nests of fractions within fractions, such as:</p>
<figure>
    <p><img src="https://www.quantamagazine.org/wp-content/uploads/2024/10/Ramanujan_Equation.svg" alt="" decoding="async" loading="lazy">    </p>
    </figure>

<p>They “defeated me completely; I had never seen anything in the least like them before,” Hardy later wrote. “They must be true because, if they were not true, no one would have had the imagination to invent them.” The formulas, unproved, were so striking that they inspired Hardy to offer Ramanujan a fellowship at Cambridge. In 1914, Ramanujan arrived in England, and for the next five years he studied and collaborated with Hardy.</p>

<p>One of Ramanujan’s first tasks was to prove a general statement about his continued fractions. To do so, he needed to prove two other statements. But he couldn’t. Neither could Hardy, nor could any of the colleagues he reached out to.</p>
<p>It turned out that they didn’t need to. The statements had been proved 20 years earlier by a little-known English mathematician named L.J. Rogers. Rogers wrote poorly, and at the time the proofs were published no one paid any attention. (Rogers was content to do his research in relative obscurity, play piano, garden and apply his spare time to a variety of other pursuits.) Ramanujan uncovered this work in 1917, and the pair of statements later became known as the Rogers-Ramanujan identities.</p>
<p>Amid Ramanujan’s prodigious output, these statements stand out. They have carried through the decades and across nearly all of mathematics. They are the seeds that mathematicians continue to sow, growing brilliant new gardens seemingly wherever they fall.</p>
<p>Ramanujan fell ill and returned to India in 1919, where he died the next year. It would fall to others to explore the world his identities had revealed.</p>
<h2><strong>The Music of the Game</strong></h2>
<p>Hussein Mourtada grew up in the 1980s in Lebanon, in a small city called Baalbek. As a teenager, he didn’t like studying and preferred to play: soccer, billiards, basketball. Math, too. “It looked like a game,” he said. “And I liked playing.”</p>
<p>As an undergraduate at the Lebanese University in Beirut, he studied both law and mathematics, with an eye to a legal career. But he soon found that while he enjoyed the philosophical aspects of law, he did not enjoy it in practice. He turned his attention to math, where he was particularly drawn to the community. As a child, his teachers and classmates were what excited him about going to school, even though he often fell asleep during class. As a budding mathematician, “I had the impression that these are beautiful people,” he said. “They are honest. You need to be honest with yourself to be a mathematician. Otherwise, it doesn’t work.”</p>
<figure>
    <p><img width="1300" height="866" src="https://www.quantamagazine.org/wp-content/uploads/2024/10/HusseinMourtada_crBasmaJaffal.webp" alt="" decoding="async" loading="lazy" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/10/HusseinMourtada_crBasmaJaffal.webp 1300w, https://www.quantamagazine.org/wp-content/uploads/2024/10/HusseinMourtada_crBasmaJaffal-520x346.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2024/10/HusseinMourtada_crBasmaJaffal-768x512.webp 768w" sizes="(max-width: 1300px) 100vw, 1300px">    </p>
            <figcaption>
                            <p>Hussein Mourtada has been bringing Ramanujan’s work into the 21st century.</p>
            <p>Basma Jaffal</p>
        </figcaption>
    </figure>

<p>He moved to France for his doctorate and started to focus on algebraic geometry — the study of algebraic varieties, or shapes cut out by polynomial equations. These are equations that can be written as sums of variables raised to whole-number powers. A line, for instance, is cut out by the equation <em>x</em> + <em>y</em> = 0, a circle by <em>x</em><sup>2</sup> + <em>y</em><sup>2</sup> = 1, a figure eight by <em>x</em><sup>4</sup> = <em>x</em><sup>2</sup> − <em>y</em><sup>2</sup>. While the line and circle are completely smooth, the figure eight has a point where it intersects itself — a singularity.</p>
<p>It’s easy to spot singularities when you’re dealing with shapes that you can draw on a sheet of paper. But higher-dimensional algebraic varieties are far more complicated and impossible to visualize. Algebraic geometers are in the business of understanding their singularities, too.</p>
<p>They’ve developed all sorts of tools to do this. One dates back to the mathematician John Nash, who in the 1960s started studying related objects called arc spaces. Nash would take a point, or singularity, and define infinitely many short trajectories — little arcs — that passed through it. By looking at all these short trajectories together, he could test how smooth his variety was at that point. “If you want to see if it’s smooth, you want to pet it,” said <a href="https://www.lix.polytechnique.fr/Labo/Gleb.POGUDIN/">Gleb Pogudin</a> of the École Polytechnique in France.</p>
<p>In practical terms, an arc space provides an infinite collection of polynomial equations. “This is really the thing that Mourtada is expert in: understanding the meaning of those equations,” said <a href="https://webusers.imj-prg.fr/~bernard.teissier/">Bernard Teissier</a>, a colleague of Mourtada’s at the Institute of Mathematics of Jussieu in Paris. “Because these equations can be very complicated. But they have a certain music to them. There is a lot of structure which governs the nature of these equations, and he’s just the person, I think, who best listens to this music and understands what it means.”</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A near impossible literacy test Louisiana used to suppress the black vote (119 pts)]]></title>
            <link>https://www.openculture.com/2024/10/take-the-near-impossible-literacy-test-louisiana-used-to-suppress-the-black-vote.html</link>
            <guid>41908701</guid>
            <pubDate>Mon, 21 Oct 2024 21:29:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openculture.com/2024/10/take-the-near-impossible-literacy-test-louisiana-used-to-suppress-the-black-vote.html">https://www.openculture.com/2024/10/take-the-near-impossible-literacy-test-louisiana-used-to-suppress-the-black-vote.html</a>, See on <a href="https://news.ycombinator.com/item?id=41908701">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p><img loading="lazy" fetchpriority="high" decoding="async" src="https://cdn8.openculture.com/2024/10/20191430/cf10083a-fc0b-4a26-b321-1f63373f6c22.avif" alt="" width="920" height="1334" srcset="https://cdn8.openculture.com/2024/10/20191430/cf10083a-fc0b-4a26-b321-1f63373f6c22.avif 920w, https://cdn8.openculture.com/2024/10/20191430/cf10083a-fc0b-4a26-b321-1f63373f6c22-248x360.avif 248w, https://cdn8.openculture.com/2024/10/20191430/cf10083a-fc0b-4a26-b321-1f63373f6c22-706x1024.avif 706w, https://cdn8.openculture.com/2024/10/20191430/cf10083a-fc0b-4a26-b321-1f63373f6c22-166x240.avif 166w, https://cdn8.openculture.com/2024/10/20191430/cf10083a-fc0b-4a26-b321-1f63373f6c22-768x1114.avif 768w" sizes="(max-width: 920px) 100vw, 920px" data-old-src="https://www.openculture.com/wp-content/plugins/native-lazyload/assets/images/placeholder.svg"></p>
<p>In William Faulkner’s 1938 nov­el <a href="https://amzn.to/3NwnMlg"><em>The Unvan­quished</em></a>, the implaca­ble Colonel Sar­toris takes dras­tic action to stop the elec­tion of a black Repub­li­can can­di­date to office after the Civ­il War, destroy­ing the bal­lots of black vot­ers and shoot­ing two North­ern car­pet­bag­gers. While such dra­mat­ic means of vot­er sup­pres­sion occurred often enough in the Recon­struc­tion South, tac­tics of elec­toral exclu­sion refined over time, such that by the mid-twen­ti­eth cen­tu­ry the Jim Crow South relied large­ly on near­ly impos­si­ble-to-pass lit­er­a­cy tests to impede free and fair elec­tions.</p>
<p>These tests, <a href="http://www.slate.com/blogs/the_vault/2013/06/28/voting_rights_and_the_supreme_court_the_impossible_literacy_test_louisiana.html">writes Rebec­ca Onion at <em>Slate</em></a>, were “sup­pos­ed­ly applic­a­ble to both white and black prospec­tive vot­ers who couldn’t prove a cer­tain lev­el of edu­ca­tion” (typ­i­cal­ly up to the fifth grade). Yet they were “in actu­al­i­ty dis­pro­por­tion­ate­ly admin­is­tered to black vot­ers.”</p>


<p>Addi­tion­al­ly, many of the tests were rigged so that reg­is­trars could give poten­tial vot­ers an easy or a dif­fi­cult ver­sion, and could score them dif­fer­ent­ly as well. For exam­ple, the <a href="http://www.crmvet.org/info/lithome.htm">Vet­er­ans of the Civ­il Rights Move­ment</a> describes a test admin­is­tered in Alaba­ma that is so entire­ly sub­jec­tive that it mea­sures the registrar’s shrewd­ness and cun­ning more than any­thing else.</p>
<p><img loading="lazy" decoding="async" src="https://cdn8.openculture.com/2024/10/20191516/ae77f4f1-7de5-429f-a21b-7b5d3649a7cd.avif" alt="" width="920" height="1259" srcset="https://cdn8.openculture.com/2024/10/20191516/ae77f4f1-7de5-429f-a21b-7b5d3649a7cd.avif 920w, https://cdn8.openculture.com/2024/10/20191516/ae77f4f1-7de5-429f-a21b-7b5d3649a7cd-263x360.avif 263w, https://cdn8.openculture.com/2024/10/20191516/ae77f4f1-7de5-429f-a21b-7b5d3649a7cd-748x1024.avif 748w, https://cdn8.openculture.com/2024/10/20191516/ae77f4f1-7de5-429f-a21b-7b5d3649a7cd-175x240.avif 175w, https://cdn8.openculture.com/2024/10/20191516/ae77f4f1-7de5-429f-a21b-7b5d3649a7cd-768x1051.avif 768w" sizes="(max-width: 920px) 100vw, 920px" data-old-src="https://www.openculture.com/wp-content/plugins/native-lazyload/assets/images/placeholder.svg"></p>
<p>The test here from Louisiana con­sists of ques­tions so ambigu­ous that no one, what­ev­er their lev­el of edu­ca­tion, can divine a “right” or “wrong” answer to most of them. And yet, as the instruc­tions state, “one wrong answer denotes fail­ure of the test,” an impos­si­ble stan­dard for even a legit­i­mate exam. Even worse, vot­ers had only ten min­utes to com­plete the three-page, 30-ques­tion doc­u­ment. The Louisiana test dates from 1964, the year before the pas­sage of the <a href="https://www.archives.gov/milestone-documents/voting-rights-act">Vot­ing Rights Act</a>, which effec­tive­ly put an end to these bla­tant­ly dis­crim­i­na­to­ry prac­tices.</p>
<p>Learn more about the his­to­ry of Jim Crow vot­er sup­pres­sion at Rebec­ca Onion’s orig­i­nal post <a href="http://www.slate.com/blogs/the_vault/2013/06/28/voting_rights_and_the_supreme_court_the_impossible_literacy_test_louisiana.html">here</a> and an update <a href="http://www.slate.com/blogs/the_vault/2013/07/03/louisiana_literacy_test_update_the_hunt_for_the_original_document.html">here</a>. And <a href="https://www.openculture.com/2014/11/harvard-students-fail-the-literacy-test.html">here you can watch video of Har­vard stu­dents try­ing to take the test</a>.</p>
<p><img loading="lazy" decoding="async" src="https://cdn8.openculture.com/2024/10/20191550/021b9032-9783-42e3-889a-caec9a10384e.avif" alt="" width="920" height="1256" srcset="https://cdn8.openculture.com/2024/10/20191550/021b9032-9783-42e3-889a-caec9a10384e.avif 920w, https://cdn8.openculture.com/2024/10/20191550/021b9032-9783-42e3-889a-caec9a10384e-264x360.avif 264w, https://cdn8.openculture.com/2024/10/20191550/021b9032-9783-42e3-889a-caec9a10384e-750x1024.avif 750w, https://cdn8.openculture.com/2024/10/20191550/021b9032-9783-42e3-889a-caec9a10384e-176x240.avif 176w, https://cdn8.openculture.com/2024/10/20191550/021b9032-9783-42e3-889a-caec9a10384e-768x1048.avif 768w" sizes="(max-width: 920px) 100vw, 920px" data-old-src="https://www.openculture.com/wp-content/plugins/native-lazyload/assets/images/placeholder.svg"></p>
<p>Note: Note: An ear­li­er ver­sion of this post appeared on our site in 2014.</p>
<p><strong>Relat­ed Con­tent:</strong></p>
<p><a title="Permanent Link to Watch Harvard Students Fail the Literacy Test Louisiana Used to Suppress the Black Vote in 1964" href="https://www.openculture.com/2014/11/harvard-students-fail-the-literacy-test.html" rel="bookmark">Watch Har­vard Stu­dents Fail the Lit­er­a­cy Test Louisiana Used to Sup­press the Black Vote in 1964</a></p>
<p><a title="Permanent Link to Philosopher Richard Rorty Chillingly Predicts the Results of the 2016 Election … Back in 1998" href="https://www.openculture.com/2016/11/philosopher-richard-rorty-chillingly-predicts-the-results-of-the-2016-election.html" rel="bookmark">Philoso­pher Richard Rorty Chill­ing­ly Pre­dicts the Results of the 2016 Elec­tion … Back in 1998</a></p>
<p><em><a href="http://about.me/jonesjoshua">Josh Jones</a>&nbsp;is a writer and musi­cian based in Durham, NC. Fol­low him at&nbsp;<a href="https://twitter.com/jdmagness">@jdmagness</a></em></p>
<br>		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Thought experiments that fray the fabric of space-time (103 pts)]]></title>
            <link>https://www.quantamagazine.org/the-thought-experiments-that-fray-the-fabric-of-space-time-20240925/</link>
            <guid>41908541</guid>
            <pubDate>Mon, 21 Oct 2024 21:08:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/the-thought-experiments-that-fray-the-fabric-of-space-time-20240925/">https://www.quantamagazine.org/the-thought-experiments-that-fray-the-fabric-of-space-time-20240925/</a>, See on <a href="https://news.ycombinator.com/item?id=41908541">Hacker News</a></p>
<div id="readability-page-1" class="page">
                    <header>
                
                
                
            
            </header>
        <main>
    
</main>
    
    



<div data-function="toggle" data-name="reset-password">
        <p>Change your password</p>
        <p>Enter your new password</p>
    </div>


















<!-- Google Tag Manager (noscript) -->

<!-- End Google Tag Manager (noscript) -->
			<div id="cookie-notice" role="banner"><p><span id="cn-notice-text">We care about your data, and we'd like to use cookies to give you a smooth browsing experience. Please agree and read more about our <a href="https://www.quantamagazine.org/privacy-policy">privacy policy</a>.</span></p>
				</div>
        



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scalene: A high-performance, high-precision CPU, GPU, memory profiler for Python (112 pts)]]></title>
            <link>https://github.com/plasma-umass/scalene</link>
            <guid>41908536</guid>
            <pubDate>Mon, 21 Oct 2024 21:07:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/plasma-umass/scalene">https://github.com/plasma-umass/scalene</a>, See on <a href="https://news.ycombinator.com/item?id=41908536">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/plasma-umass/scalene/raw/master/docs/scalene-icon-white.png"><img src="https://github.com/plasma-umass/scalene/raw/master/docs/scalene-icon-white.png" alt="scalene"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Scalene: a Python CPU+GPU+memory profiler with AI-powered optimization proposals</h2><a id="user-content-scalene-a-python-cpugpumemory-profiler-with-ai-powered-optimization-proposals" aria-label="Permalink: Scalene: a Python CPU+GPU+memory profiler with AI-powered optimization proposals" href="#scalene-a-python-cpugpumemory-profiler-with-ai-powered-optimization-proposals"></a></p>
<p dir="auto">by <a href="https://emeryberger.com/" rel="nofollow">Emery Berger</a>, <a href="https://samstern.me/" rel="nofollow">Sam Stern</a>, and <a href="https://github.com/jaltmayerpizzorno">Juan Altmayer Pizzorno</a>.</p>
<p dir="auto"><a href="https://join.slack.com/t/scaleneprofil-jge3234/shared_invite/zt-110vzrdck-xJh5d4gHnp5vKXIjYD3Uwg" rel="nofollow"><img src="https://github.com/plasma-umass/scalene/raw/master/docs/images/slack-logo.png" alt="Scalene community Slack"></a><a href="https://join.slack.com/t/scaleneprofil-jge3234/shared_invite/zt-110vzrdck-xJh5d4gHnp5vKXIjYD3Uwg" rel="nofollow">Scalene community Slack</a></p>
<p dir="auto"><a href="https://pypi.org/project/scalene/" rel="nofollow"><img src="https://camo.githubusercontent.com/999e3fc8d7e5ec0d4b465371620a1cec5dbb7f7fc2591ef1e3e8f7214bea61b0/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f7363616c656e652e737667" alt="PyPI Latest Release" data-canonical-src="https://img.shields.io/pypi/v/scalene.svg"></a><a href="https://anaconda.org/conda-forge/scalene" rel="nofollow"><img src="https://camo.githubusercontent.com/60d006b383bad7470fc7ca8368cf8097b3de4044938a4714a788975dc43041ab/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f762f636f6e64612d666f7267652f7363616c656e65" alt="Anaconda-Server Badge" data-canonical-src="https://img.shields.io/conda/v/conda-forge/scalene"></a> <a href="https://pepy.tech/project/scalene" rel="nofollow"><img src="https://camo.githubusercontent.com/ecdab4442ec9a2ab4cab1de36bebfb0a5c7bc00f288807723ca673522b4b4dc2/68747470733a2f2f7374617469632e706570792e746563682f62616467652f7363616c656e65" alt="Downloads" data-canonical-src="https://static.pepy.tech/badge/scalene"></a><a href="https://anaconda.org/conda-forge/scalene" rel="nofollow"><img src="https://camo.githubusercontent.com/bc3427e8e678f6da977b954640137ecd09fbaee32bc2b1ae169bec0c48bfd304/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f642f636f6e64612d666f7267652f7363616c656e653f6c6f676f3d636f6e6461" alt="Anaconda downloads" data-canonical-src="https://img.shields.io/conda/d/conda-forge/scalene?logo=conda"></a> <a href="https://pepy.tech/project/scalene" rel="nofollow"><img src="https://camo.githubusercontent.com/c80e0ee2e6b083f8ec0b720007a5fcca7c7e33a5702f8a91eb8cca4afb65ced7/68747470733a2f2f7374617469632e706570792e746563682f62616467652f7363616c656e652f6d6f6e7468" alt="Downloads" data-canonical-src="https://static.pepy.tech/badge/scalene/month"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/577e8524ded7f9bd5610662b971236965fbbebbf1dbcf24dd9b695733fea4809/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f7363616c656e652e7376673f7374796c653d666c61742d737175617265"><img src="https://camo.githubusercontent.com/577e8524ded7f9bd5610662b971236965fbbebbf1dbcf24dd9b695733fea4809/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f7363616c656e652e7376673f7374796c653d666c61742d737175617265" alt="Python versions" data-canonical-src="https://img.shields.io/pypi/pyversions/scalene.svg?style=flat-square"></a><a href="https://marketplace.visualstudio.com/items?itemName=EmeryBerger.scalene" rel="nofollow"><img src="https://camo.githubusercontent.com/f5a0f652c19744c3680c9e621707576452f929653a982cf216b8ad46962c9ae2/68747470733a2f2f696d672e736869656c64732e696f2f76697375616c2d73747564696f2d6d61726b6574706c6163652f762f656d6572796265726765722e7363616c656e653f6c6f676f3d76697375616c73747564696f636f6465" alt="Visual Studio Code Extension version" data-canonical-src="https://img.shields.io/visual-studio-marketplace/v/emeryberger.scalene?logo=visualstudiocode"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9e2d96d0196d22379d49dcc59fc3d72685bcf46c4957b9d3eddab0986a132bac/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f706c61736d612d756d6173732f7363616c656e65"><img src="https://camo.githubusercontent.com/9e2d96d0196d22379d49dcc59fc3d72685bcf46c4957b9d3eddab0986a132bac/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f706c61736d612d756d6173732f7363616c656e65" alt="License" data-canonical-src="https://img.shields.io/github/license/plasma-umass/scalene"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/plasma-umass/scalene/raw/master/docs/Ozsvald-tweet.png"><img src="https://github.com/plasma-umass/scalene/raw/master/docs/Ozsvald-tweet.png" alt="Ozsvald tweet"></a></p>
<p dir="auto">(tweet from Ian Ozsvald, author of <a href="https://smile.amazon.com/High-Performance-Python-Performant-Programming/dp/1492055026/ref=sr_1_1?crid=texbooks" rel="nofollow"><em>High Performance Python</em></a>)</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/plasma-umass/scalene/raw/master/docs/semantic-scholar-success.png"><img src="https://github.com/plasma-umass/scalene/raw/master/docs/semantic-scholar-success.png" alt="Semantic Scholar success story"></a></p>
<p dir="auto"><em><strong>Scalene web-based user interface:</strong></em> <a href="http://plasma-umass.org/scalene-gui/" rel="nofollow">http://plasma-umass.org/scalene-gui/</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">About Scalene</h2><a id="user-content-about-scalene" aria-label="Permalink: About Scalene" href="#about-scalene"></a></p>
<p dir="auto">Scalene is a high-performance CPU, GPU <em>and</em> memory profiler for
Python that does a number of things that other Python profilers do not
and cannot do.  It runs orders of magnitude faster than many other
profilers while delivering far more detailed information. It is also
the first profiler ever to incorporate AI-powered proposed
optimizations.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">AI-powered optimization suggestions</h3><a id="user-content-ai-powered-optimization-suggestions" aria-label="Permalink: AI-powered optimization suggestions" href="#ai-powered-optimization-suggestions"></a></p>
<blockquote>
<p dir="auto"><strong>Note</strong></p>
<p dir="auto">To enable AI-powered optimization suggestions, you need to enter an <a href="https://openai.com/api/" rel="nofollow">OpenAI key</a> in the box under "Advanced options". <em>Your account will need to have a positive balance for this to work</em> (check your balance at <a href="https://platform.openai.com/account/usage" rel="nofollow">https://platform.openai.com/account/usage</a>).</p>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/1612723/211639253-ec926b38-3efe-4a20-8514-e10dde94ec01.png"><img width="487" alt="Scalene advanced options" src="https://user-images.githubusercontent.com/1612723/211639253-ec926b38-3efe-4a20-8514-e10dde94ec01.png"></a>
</blockquote>
<p dir="auto">Once you've entered your OpenAI key (see above), click on the lightning bolt (⚡) beside any line or the explosion (💥) for an entire region of code to generate a proposed optimization. Click on a proposed optimization to copy it to the clipboard.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/1612723/211639968-37cf793f-3290-43d1-9282-79e579558388.png"><img width="571" alt="example proposed optimization" src="https://user-images.githubusercontent.com/1612723/211639968-37cf793f-3290-43d1-9282-79e579558388.png"></a></p>
<p dir="auto">You can click as many times as you like on the lightning bolt or explosion, and it will generate different suggested optimizations. Your mileage may vary, but in some cases, the suggestions are quite impressive (e.g., order-of-magnitude improvements).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Quick Start</h3><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Installing Scalene:</h4><a id="user-content-installing-scalene" aria-label="Permalink: Installing Scalene:" href="#installing-scalene"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 -m pip install -U scalene"><pre><span>python3 -m pip install -U scalene</span></pre></div>
<p dir="auto">or</p>
<div dir="auto" data-snippet-clipboard-copy-content="conda install -c conda-forge scalene"><pre><span>conda install -c conda-forge scalene</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Using Scalene:</h4><a id="user-content-using-scalene" aria-label="Permalink: Using Scalene:" href="#using-scalene"></a></p>
<p dir="auto">After installing Scalene, you can use Scalene at the command line, or as a Visual Studio Code extension.</p>
<details>
  <summary>
    Using the Scalene VS Code Extension:
  </summary>
<p dir="auto">First, install <a href="https://marketplace.visualstudio.com/items?itemName=EmeryBerger.scalene" rel="nofollow">the Scalene extension from the VS Code Marketplace</a> or by searching for it within VS Code by typing Command-Shift-X (Mac) or Ctrl-Shift-X (Windows). Once that's installed, click Command-Shift-P or Ctrl-Shift-P to open the <a href="https://code.visualstudio.com/docs/getstarted/userinterface" rel="nofollow">Command Palette</a>. Then select <b>"Scalene: AI-powered profiling..."</b> (you can start typing Scalene and it will pop up if it's installed). Run that and, assuming your code runs for at least a second, a Scalene profile will appear in a webview.</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/1612723/269439965-7e78e3d2-e649-4f02-86fd-0da2a259a1a4.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1ODI1MDIsIm5iZiI6MTcyOTU4MjIwMiwicGF0aCI6Ii8xNjEyNzIzLzI2OTQzOTk2NS03ZTc4ZTNkMi1lNjQ5LTRmMDItODZmZC0wZGEyYTI1OWExYTQucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTAyMiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDEwMjJUMDczMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OWQ3MzdkNjJkMWY2ZDE4OGNmYzU5ZTRjYTZiZTBmZjY5NDFiOWIyODUyOWY2YTJiMDNiYjY2ZTM3NjkwMWQxMSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.HGFPesaVBb9b5fzF6hdUf9_6YE36aFVjhdGPZ4Rszb4"><img width="734" alt="Screenshot 2023-09-20 at 7 09 06 PM" src="https://private-user-images.githubusercontent.com/1612723/269439965-7e78e3d2-e649-4f02-86fd-0da2a259a1a4.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1ODI1MDIsIm5iZiI6MTcyOTU4MjIwMiwicGF0aCI6Ii8xNjEyNzIzLzI2OTQzOTk2NS03ZTc4ZTNkMi1lNjQ5LTRmMDItODZmZC0wZGEyYTI1OWExYTQucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTAyMiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDEwMjJUMDczMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OWQ3MzdkNjJkMWY2ZDE4OGNmYzU5ZTRjYTZiZTBmZjY5NDFiOWIyODUyOWY2YTJiMDNiYjY2ZTM3NjkwMWQxMSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.HGFPesaVBb9b5fzF6hdUf9_6YE36aFVjhdGPZ4Rszb4"></a>
</details>
<details>
<summary>
Commonly used command-line options:
</summary>
<div dir="auto" data-snippet-clipboard-copy-content="scalene your_prog.py                             # full profile (outputs to web interface)
python3 -m scalene your_prog.py                  # equivalent alternative

scalene --cli your_prog.py                       # use the command-line only (no web interface)

scalene --cpu your_prog.py                       # only profile CPU
scalene --cpu --gpu your_prog.py                 # only profile CPU and GPU
scalene --cpu --gpu --memory your_prog.py        # profile everything (same as no options)

scalene --reduced-profile your_prog.py           # only profile lines with significant usage
scalene --profile-interval 5.0 your_prog.py      # output a new profile every five seconds

scalene (Scalene options) --- your_prog.py (...) # use --- to tell Scalene to ignore options after that point
scalene --help                                   # lists all options"><pre><span>scalene your_prog.py                             # full profile (outputs to web interface)</span>
<span>python3 -m scalene your_prog.py                  # equivalent alternative</span>

<span>scalene --cli your_prog.py                       # use the command-line only (no web interface)</span>

<span>scalene --cpu your_prog.py                       # only profile CPU</span>
<span>scalene --cpu --gpu your_prog.py                 # only profile CPU and GPU</span>
<span>scalene --cpu --gpu --memory your_prog.py        # profile everything (same as no options)</span>

<span>scalene --reduced-profile your_prog.py           # only profile lines with significant usage</span>
<span>scalene --profile-interval 5.0 your_prog.py      # output a new profile every five seconds</span>

<span>scalene (Scalene options) --- your_prog.py (...) # use --- to tell Scalene to ignore options after that point</span>
<span>scalene --help                                   # lists all options</span></pre></div>
</details>
<details>
<summary>
Using Scalene programmatically in your code:
</summary>
<p dir="auto">Invoke using <code>scalene</code> as above and then:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from scalene import scalene_profiler

# Turn profiling on
scalene_profiler.start()

# your code

# Turn profiling off
scalene_profiler.stop()"><pre><span>from</span> <span>scalene</span> <span>import</span> <span>scalene_profiler</span>

<span># Turn profiling on</span>
<span>scalene_profiler</span>.<span>start</span>()

<span># your code</span>

<span># Turn profiling off</span>
<span>scalene_profiler</span>.<span>stop</span>()</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="from scalene.scalene_profiler import enable_profiling

with enable_profiling():
    # do something"><pre><span>from</span> <span>scalene</span>.<span>scalene_profiler</span> <span>import</span> <span>enable_profiling</span>

<span>with</span> <span>enable_profiling</span>():
    <span># do something</span></pre></div>
</details>
<details>
<summary>
Using Scalene to profile only specific functions via <code>@profile</code>:
</summary>
<p dir="auto">Just preface any functions you want to profile with the <code>@profile</code> decorator and run it with Scalene:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# do not import profile!

@profile
def slow_function():
    import time
    time.sleep(3)"><pre><span># do not import profile!</span>

<span>@<span>profile</span></span>
<span>def</span> <span>slow_function</span>():
    <span>import</span> <span>time</span>
    <span>time</span>.<span>sleep</span>(<span>3</span>)</pre></div>
</details>
<p dir="auto"><h4 tabindex="-1" dir="auto">Web-based GUI</h4><a id="user-content-web-based-gui" aria-label="Permalink: Web-based GUI" href="#web-based-gui"></a></p>
<p dir="auto">Scalene has both a CLI and a web-based GUI <a href="http://plasma-umass.org/scalene-gui/" rel="nofollow">(demo here)</a>.</p>
<p dir="auto">By default, once Scalene has profiled your program, it will open a
tab in a web browser with an interactive user interface (all processing is done
locally). Hover over bars to see breakdowns of CPU and memory
consumption, and click on underlined column headers to sort the
columns. The generated file <code>profile.html</code> is self-contained and can be saved for later use.</p>
<p dir="auto"><a href="https://raw.githubusercontent.com/plasma-umass/scalene/master/docs/scalene-gui-example-full.png" rel="nofollow"><img src="https://raw.githubusercontent.com/plasma-umass/scalene/master/docs/scalene-gui-example.png" alt="Scalene web GUI"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Scalene Overview</h2><a id="user-content-scalene-overview" aria-label="Permalink: Scalene Overview" href="#scalene-overview"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Scalene talk (PyCon US 2021)</h3><a id="user-content-scalene-talk-pycon-us-2021" aria-label="Permalink: Scalene talk (PyCon US 2021)" href="#scalene-talk-pycon-us-2021"></a></p>
<p dir="auto"><a href="https://youtu.be/5iEf-_7mM1k" rel="nofollow">This talk</a> presented at PyCon 2021 walks through Scalene's advantages and how to use it to debug the performance of an application (and provides some technical details on its internals). We highly recommend watching this video!</p>
<p dir="auto"><a href="https://youtu.be/5iEf-_7mM1k" title="Scalene presentation at PyCon 2021" rel="nofollow"><img src="https://raw.githubusercontent.com/plasma-umass/scalene/master/docs/images/scalene-video-img.png" alt="Scalene presentation at PyCon 2021"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Fast and Accurate</h3><a id="user-content-fast-and-accurate" aria-label="Permalink: Fast and Accurate" href="#fast-and-accurate"></a></p>
<ul dir="auto">
<li>
<p dir="auto">Scalene is <strong><em>fast</em></strong>. It uses sampling instead of instrumentation or relying on Python's tracing facilities. Its overhead is typically no more than 10-20% (and often less).</p>
</li>
<li>
<p dir="auto">Scalene is <strong>accurate</strong>. We tested CPU profiler accuracy and found that Scalene is among the most accurate profilers, correctly measuring time taken.</p>
</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/plasma-umass/scalene/raw/master/docs/cpu-accuracy-comparison.png"><img src="https://github.com/plasma-umass/scalene/raw/master/docs/cpu-accuracy-comparison.png" alt="Profiler accuracy"></a></p>
<ul dir="auto">
<li>Scalene performs profiling <strong><em>at the line level</em></strong> <em>and</em> <strong><em>per function</em></strong>, pointing to the functions and the specific lines of code responsible for the execution time in your program.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">CPU profiling</h3><a id="user-content-cpu-profiling" aria-label="Permalink: CPU profiling" href="#cpu-profiling"></a></p>
<ul dir="auto">
<li>Scalene <strong>separates out time spent in Python from time in native code</strong> (including libraries). Most Python programmers aren't going to optimize the performance of native code (which is usually either in the Python implementation or external libraries), so this helps developers focus their optimization efforts on the code they can actually improve.</li>
<li>Scalene <strong>highlights hotspots</strong> (code accounting for significant percentages of CPU time or memory allocation) in red, making them even easier to spot.</li>
<li>Scalene also separates out <strong>system time</strong>, making it easy to find I/O bottlenecks.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">GPU profiling</h3><a id="user-content-gpu-profiling" aria-label="Permalink: GPU profiling" href="#gpu-profiling"></a></p>
<ul dir="auto">
<li>Scalene reports <strong>GPU time</strong> (currently limited to NVIDIA-based systems).</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Memory profiling</h3><a id="user-content-memory-profiling" aria-label="Permalink: Memory profiling" href="#memory-profiling"></a></p>
<ul dir="auto">
<li>Scalene <strong>profiles memory usage</strong>. In addition to tracking CPU usage, Scalene also points to the specific lines of code responsible for memory growth. It accomplishes this via an included specialized memory allocator.</li>
<li>Scalene separates out the percentage of <strong>memory consumed by Python code vs. native code</strong>.</li>
<li>Scalene produces <strong><em>per-line</em> memory profiles</strong>.</li>
<li>Scalene <strong>identifies lines with likely memory leaks</strong>.</li>
<li>Scalene <strong>profiles <em>copying volume</em></strong>, making it easy to spot inadvertent copying, especially due to crossing Python/library boundaries (e.g., accidentally converting <code>numpy</code> arrays into Python arrays, and vice versa).</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Other features</h3><a id="user-content-other-features" aria-label="Permalink: Other features" href="#other-features"></a></p>
<ul dir="auto">
<li>Scalene can produce <strong>reduced profiles</strong> (via <code>--reduced-profile</code>) that only report lines that consume more than 1% of CPU or perform at least 100 allocations.</li>
<li>Scalene supports <code>@profile</code> decorators to profile only specific functions.</li>
<li>When Scalene is profiling a program launched in the background (via <code>&amp;</code>), you can <strong>suspend and resume profiling</strong>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Comparison to Other Profilers</h2><a id="user-content-comparison-to-other-profilers" aria-label="Permalink: Comparison to Other Profilers" href="#comparison-to-other-profilers"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance and Features</h2><a id="user-content-performance-and-features" aria-label="Permalink: Performance and Features" href="#performance-and-features"></a></p>
<p dir="auto">Below is a table comparing the <strong>performance and features</strong> of various profilers to Scalene.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/plasma-umass/scalene/master/docs/images/profiler-comparison.png"><img src="https://raw.githubusercontent.com/plasma-umass/scalene/master/docs/images/profiler-comparison.png" alt="Performance and feature comparison"></a></p>
<ul dir="auto">
<li><strong>Slowdown</strong>: the slowdown when running a benchmark from the Pyperformance suite. Green means less than 2x overhead. Scalene's overhead is just a 35% slowdown.</li>
</ul>
<p dir="auto">Scalene has all of the following features, many of which only Scalene supports:</p>
<ul dir="auto">
<li><strong>Lines or functions</strong>: does the profiler report information only for entire functions, or for every line -- Scalene does both.</li>
<li><strong>Unmodified Code</strong>: works on unmodified code.</li>
<li><strong>Threads</strong>: supports Python threads.</li>
<li><strong>Multiprocessing</strong>: supports use of the <code>multiprocessing</code> library -- <em>Scalene only</em></li>
<li><strong>Python vs. C time</strong>: breaks out time spent in Python vs. native code (e.g., libraries) -- <em>Scalene only</em></li>
<li><strong>System time</strong>: breaks out system time (e.g., sleeping or performing I/O) -- <em>Scalene only</em></li>
<li><strong>Profiles memory</strong>: reports memory consumption per line / function</li>
<li><strong>GPU</strong>: reports time spent on an NVIDIA GPU (if present) -- <em>Scalene only</em></li>
<li><strong>Memory trends</strong>: reports memory use over time per line / function -- <em>Scalene only</em></li>
<li><strong>Copy volume</strong>: reports megabytes being copied per second -- <em>Scalene only</em></li>
<li><strong>Detects leaks</strong>: automatically pinpoints lines responsible for likely memory leaks -- <em>Scalene only</em></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Output</h2><a id="user-content-output" aria-label="Permalink: Output" href="#output"></a></p>
<p dir="auto">If you include the <code>--cli</code> option, Scalene prints annotated source code for the program being profiled
(as text, JSON (<code>--json</code>), or HTML (<code>--html</code>)) and any modules it
uses in the same directory or subdirectories (you can optionally have
it <code>--profile-all</code> and only include files with at least a
<code>--cpu-percent-threshold</code> of time).  Here is a snippet from
<code>pystone.py</code>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/plasma-umass/scalene/master/docs/images/sample-profile-pystone.png"><img src="https://raw.githubusercontent.com/plasma-umass/scalene/master/docs/images/sample-profile-pystone.png" alt="Example profile"></a></p>
<ul dir="auto">
<li><strong>Memory usage at the top</strong>: Visualized by "sparklines", memory consumption over the runtime of the profiled code.</li>
<li><strong>"Time Python"</strong>: How much time was spent in Python code.</li>
<li><strong>"native"</strong>: How much time was spent in non-Python code (e.g., libraries written in C/C++).</li>
<li><strong>"system"</strong>: How much time was spent in the system (e.g., I/O).</li>
<li><strong>"GPU"</strong>: (not shown here) How much time spent on the GPU, if your system has an NVIDIA GPU installed.</li>
<li><strong>"Memory Python"</strong>: How much of the memory allocation happened on the Python side of the code, as opposed to in non-Python code (e.g., libraries written in C/C++).</li>
<li><strong>"net"</strong>: Positive net memory numbers indicate total memory allocation in megabytes; negative net memory numbers indicate memory reclamation.</li>
<li><strong>"timeline / %"</strong>: Visualized by "sparklines", memory consumption generated by this line over the program runtime, and the percentages of total memory activity this line represents.</li>
<li><strong>"Copy (MB/s)"</strong>: The amount of megabytes being copied per second (see "About Scalene").</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Scalene</h2><a id="user-content-scalene" aria-label="Permalink: Scalene" href="#scalene"></a></p>
<p dir="auto">The following command runs Scalene on a provided example program.</p>

<details>
 <summary>
  Click to see all Scalene's options (available by running with <code>--help</code>)
 </summary>
<div dir="auto" data-snippet-clipboard-copy-content="    % scalene --help
     usage: scalene [-h] [--outfile OUTFILE] [--html] [--reduced-profile]
                    [--profile-interval PROFILE_INTERVAL] [--cpu-only]
                    [--profile-all] [--profile-only PROFILE_ONLY]
                    [--use-virtual-time]
                    [--cpu-percent-threshold CPU_PERCENT_THRESHOLD]
                    [--cpu-sampling-rate CPU_SAMPLING_RATE]
                    [--malloc-threshold MALLOC_THRESHOLD]
     
     Scalene: a high-precision CPU and memory profiler.
     https://github.com/plasma-umass/scalene
     
     command-line:
        % scalene [options] yourprogram.py
     or
        % python3 -m scalene [options] yourprogram.py
     
     in Jupyter, line mode:
        %scrun [options] statement
     
     in Jupyter, cell mode:
        %%scalene [options]
        code...
        code...
     
     optional arguments:
       -h, --help            show this help message and exit
       --outfile OUTFILE     file to hold profiler output (default: stdout)
       --html                output as HTML (default: text)
       --reduced-profile     generate a reduced profile, with non-zero lines only (default: False)
       --profile-interval PROFILE_INTERVAL
                             output profiles every so many seconds (default: inf)
       --cpu-only            only profile CPU time (default: profile CPU, memory, and copying)
       --profile-all         profile all executed code, not just the target program (default: only the target program)
       --profile-only PROFILE_ONLY
                             profile only code in filenames that contain the given strings, separated by commas (default: no restrictions)
       --use-virtual-time    measure only CPU time, not time spent in I/O or blocking (default: False)
       --cpu-percent-threshold CPU_PERCENT_THRESHOLD
                             only report profiles with at least this percent of CPU time (default: 1%)
       --cpu-sampling-rate CPU_SAMPLING_RATE
                             CPU sampling rate (default: every 0.01s)
       --malloc-threshold MALLOC_THRESHOLD
                             only report profiles with at least this many allocations (default: 100)
     
     When running Scalene in the background, you can suspend/resume profiling
     for the process ID that Scalene reports. For example:
     
        % python3 -m scalene [options] yourprogram.py &amp;
      Scalene now profiling process 12345
        to suspend profiling: python3 -m scalene.profile --off --pid 12345
        to resume profiling:  python3 -m scalene.profile --on  --pid 12345"><pre><span>    % scalene --help</span>
<span>     usage: scalene [-h] [--outfile OUTFILE] [--html] [--reduced-profile]</span>
<span>                    [--profile-interval PROFILE_INTERVAL] [--cpu-only]</span>
<span>                    [--profile-all] [--profile-only PROFILE_ONLY]</span>
<span>                    [--use-virtual-time]</span>
<span>                    [--cpu-percent-threshold CPU_PERCENT_THRESHOLD]</span>
<span>                    [--cpu-sampling-rate CPU_SAMPLING_RATE]</span>
<span>                    [--malloc-threshold MALLOC_THRESHOLD]</span>
<span>     </span>
<span>     Scalene: a high-precision CPU and memory profiler.</span>
<span>     https://github.com/plasma-umass/scalene</span>
<span>     </span>
<span>     command-line:</span>
<span>        % scalene [options] yourprogram.py</span>
<span>     or</span>
<span>        % python3 -m scalene [options] yourprogram.py</span>
<span>     </span>
<span>     in Jupyter, line mode:</span>
<span>        %scrun [options] statement</span>
<span>     </span>
<span>     in Jupyter, cell mode:</span>
<span>        %%scalene [options]</span>
<span>        code...</span>
<span>        code...</span>
<span>     </span>
<span>     optional arguments:</span>
<span>       -h, --help            show this help message and exit</span>
<span>       --outfile OUTFILE     file to hold profiler output (default: stdout)</span>
<span>       --html                output as HTML (default: text)</span>
<span>       --reduced-profile     generate a reduced profile, with non-zero lines only (default: False)</span>
<span>       --profile-interval PROFILE_INTERVAL</span>
<span>                             output profiles every so many seconds (default: inf)</span>
<span>       --cpu-only            only profile CPU time (default: profile CPU, memory, and copying)</span>
<span>       --profile-all         profile all executed code, not just the target program (default: only the target program)</span>
<span>       --profile-only PROFILE_ONLY</span>
<span>                             profile only code in filenames that contain the given strings, separated by commas (default: no restrictions)</span>
<span>       --use-virtual-time    measure only CPU time, not time spent in I/O or blocking (default: False)</span>
<span>       --cpu-percent-threshold CPU_PERCENT_THRESHOLD</span>
<span>                             only report profiles with at least this percent of CPU time (default: 1%)</span>
<span>       --cpu-sampling-rate CPU_SAMPLING_RATE</span>
<span>                             CPU sampling rate (default: every 0.01s)</span>
<span>       --malloc-threshold MALLOC_THRESHOLD</span>
<span>                             only report profiles with at least this many allocations (default: 100)</span>
<span>     </span>
<span>     When running Scalene in the background, you can suspend/resume profiling</span>
<span>     for the process ID that Scalene reports. For example:</span>
<span>     </span>
<span>        % python3 -m scalene [options] yourprogram.py &amp;</span>
<span>      Scalene now profiling process 12345</span>
<span>        to suspend profiling: python3 -m scalene.profile --off --pid 12345</span>
<span>        to resume profiling:  python3 -m scalene.profile --on  --pid 12345</span></pre></div>
</details>
<p dir="auto"><h3 tabindex="-1" dir="auto">Scalene with Jupyter</h3><a id="user-content-scalene-with-jupyter" aria-label="Permalink: Scalene with Jupyter" href="#scalene-with-jupyter"></a></p>
<details>
<summary>
Instructions for installing and using Scalene with Jupyter notebooks
</summary>
<p dir="auto"><a href="https://nbviewer.jupyter.org/github/plasma-umass/scalene/blob/master/docs/scalene-demo.ipynb" rel="nofollow">This notebook</a> illustrates the use of Scalene in Jupyter.</p>
<p dir="auto">Installation:</p>
<div dir="auto" data-snippet-clipboard-copy-content="!pip install scalene
%load_ext scalene"><pre><span>!pip install scalene</span>
<span>%load_ext scalene</span></pre></div>
<p dir="auto">Line mode:</p>
<div dir="auto" data-snippet-clipboard-copy-content="%scrun [options] statement"><pre><span>%scrun [options] statement</span></pre></div>
<p dir="auto">Cell mode:</p>
<div dir="auto" data-snippet-clipboard-copy-content="%%scalene [options]
code...
code..."><pre><span>%%scalene [options]</span>
<span>code...</span>
<span>code...</span></pre></div>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<details open="">
<summary>Using <code>pip</code> (Mac OS X, Linux, Windows, and WSL2)</summary>
<p dir="auto">Scalene is distributed as a <code>pip</code> package and works on Mac OS X, Linux (including Ubuntu in <a href="https://docs.microsoft.com/en-us/windows/wsl/wsl2-index" rel="nofollow">Windows WSL2</a>) and (with limitations) Windows platforms.</p>
<blockquote>
<p dir="auto"><strong>Note</strong></p>
<p dir="auto">The Windows version currently only supports CPU and GPU profiling, but not memory or copy profiling.</p>
</blockquote>
<p dir="auto">You can install it as follows:</p>

<p dir="auto">or</p>
<div dir="auto" data-snippet-clipboard-copy-content="  % python3 -m pip install -U scalene"><pre><span>  % python3 -m pip install -U scalene</span></pre></div>
<p dir="auto">You may need to install some packages first.</p>
<p dir="auto">See <a href="https://stackoverflow.com/a/19344978/4954434" rel="nofollow">https://stackoverflow.com/a/19344978/4954434</a> for full instructions for all Linux flavors.</p>
<p dir="auto">For Ubuntu/Debian:</p>
<div dir="auto" data-snippet-clipboard-copy-content="  % sudo apt install git python3-all-dev"><pre><span>  % sudo apt install git python3-all-dev</span></pre></div>
</details>
<details>
<summary>Using <code>conda</code> (Mac OS X, Linux, Windows, and WSL2)</summary>
<div dir="auto" data-snippet-clipboard-copy-content="  % conda install -c conda-forge scalene"><pre><span>  % conda install -c conda-forge scalene</span></pre></div>
<p dir="auto">Scalene is distributed as a <code>conda</code> package and works on Mac OS X, Linux (including Ubuntu in <a href="https://docs.microsoft.com/en-us/windows/wsl/wsl2-index" rel="nofollow">Windows WSL2</a>) and (with limitations) Windows platforms.</p>
<blockquote>
<p dir="auto"><strong>Note</strong></p>
<p dir="auto">The Windows version currently only supports CPU and GPU profiling, but not memory or copy profiling.</p>
</blockquote>
</details>
<details>
<summary>On ArchLinux</summary>
<p dir="auto">You can install Scalene on Arch Linux via the <a href="https://aur.archlinux.org/packages/python-scalene-git/" rel="nofollow">AUR
package</a>. Use your favorite AUR helper, or
manually download the <code>PKGBUILD</code> and run <code>makepkg -cirs</code> to build. Note that this will place
<code>libscalene.so</code> in <code>/usr/lib</code>; modify the below usage instructions accordingly.</p>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Frequently Asked Questions</h2><a id="user-content-frequently-asked-questions" aria-label="Permalink: Frequently Asked Questions" href="#frequently-asked-questions"></a></p>
<details>
<summary>
Can I use Scalene with PyTest?
</summary>
<p dir="auto"><strong>A:</strong> Yes! You can run it as follows (for example):</p>
<p dir="auto"><code>python3 -m scalene --- -m pytest your_test.py</code></p>
</details>
<details>
<summary>
Is there any way to get shorter profiles or do more targeted profiling?
</summary>
<p dir="auto"><strong>A:</strong> Yes! There are several options:</p>
<ol dir="auto">
<li>Use <code>--reduced-profile</code> to include only lines and files with memory/CPU/GPU activity.</li>
<li>Use <code>--profile-only</code> to include only filenames containing specific strings (as in, <code>--profile-only foo,bar,baz</code>).</li>
<li>Decorate functions of interest with <code>@profile</code> to have Scalene report <em>only</em> those functions.</li>
<li>Turn profiling on and off programmatically by importing Scalene profiler (<code>from scalene import scalene_profiler</code>) and then turning profiling on and off via <code>scalene_profiler.start()</code> and <code>scalene_profiler.stop()</code>. By default, Scalene runs with profiling on, so to delay profiling until desired, use the <code>--off</code> command-line option (<code>python3 -m scalene --off yourprogram.py</code>).</li>
</ol>
</details>
<details>
<summary>
How do I run Scalene in PyCharm?
</summary>
<p dir="auto"><strong>A:</strong>  In PyCharm, you can run Scalene at the command line by opening the terminal at the bottom of the IDE and running a Scalene command (e.g., <code>python -m scalene &lt;your program&gt;</code>). Use the options <code>--cli</code>, <code>--html</code>, and <code>--outfile &lt;your output.html&gt;</code> to generate an HTML file that you can then view in the IDE.</p>
</details>
<details>
<summary>
How do I use Scalene with Django?
</summary>
<p dir="auto"><strong>A:</strong> Pass in the <code>--noreload</code> option (see <a data-error-text="Failed to load title" data-id="874855968" data-permission-text="Title is private" data-url="https://github.com/plasma-umass/scalene/issues/178" data-hovercard-type="issue" data-hovercard-url="/plasma-umass/scalene/issues/178/hovercard" href="https://github.com/plasma-umass/scalene/issues/178">#178</a>).</p>
</details>
<details>
<summary>
Does Scalene work with gevent/Greenlets?
</summary>
<p dir="auto"><strong>A:</strong> Yes! Put the following code in the beginning of your program, or modify the call to <code>monkey.patch_all</code> as below:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from gevent import monkey
monkey.patch_all(thread=False)"><pre><span>from</span> <span>gevent</span> <span>import</span> <span>monkey</span>
<span>monkey</span>.<span>patch_all</span>(<span>thread</span><span>=</span><span>False</span>)</pre></div>
</details>
<details>
<summary>
How do I use Scalene with PyTorch on the Mac?
</summary>
<p dir="auto"><strong>A:</strong> Scalene works with PyTorch version 1.5.1 on Mac OS X. There's a bug in newer versions of PyTorch (<a data-error-text="Failed to load title" data-id="870369259" data-permission-text="Title is private" data-url="https://github.com/pytorch/pytorch/issues/57185" data-hovercard-type="issue" data-hovercard-url="/pytorch/pytorch/issues/57185/hovercard" href="https://github.com/pytorch/pytorch/issues/57185">pytorch/pytorch#57185</a>) that interferes with Scalene (discussion here: <a data-error-text="Failed to load title" data-id="780743681" data-permission-text="Title is private" data-url="https://github.com/plasma-umass/scalene/issues/110" data-hovercard-type="issue" data-hovercard-url="/plasma-umass/scalene/issues/110/hovercard" href="https://github.com/plasma-umass/scalene/issues/110">#110</a>), but only on Macs.</p>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Technical Information</h2><a id="user-content-technical-information" aria-label="Permalink: Technical Information" href="#technical-information"></a></p>
<p dir="auto">For details about how Scalene works, please see the following paper, which won the Jay Lepreau Best Paper Award at <a href="https://www.usenix.org/conference/osdi23/presentation/berger" rel="nofollow">OSDI 2023</a>: <a href="https://arxiv.org/pdf/2212.07597" rel="nofollow">Triangulating Python Performance Issues with Scalene</a>. (Note that this paper does not include information about the AI-driven proposed optimizations.)</p>
<details>
<summary>
To cite Scalene in an academic paper, please use the following:
</summary>
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{288540,
author = {Emery D. Berger and Sam Stern and Juan Altmayer Pizzorno},
title = {Triangulating Python Performance Issues with {S}calene},
booktitle = {{17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)}},
year = {2023},
isbn = {978-1-939133-34-2},
address = {Boston, MA},
pages = {51--64},
url = {https://www.usenix.org/conference/osdi23/presentation/berger},
publisher = {USENIX Association},
month = jul
}"><pre>@inproceedings{288540,
author = {Emery D. Berger and Sam Stern and Juan Altmayer Pizzorno},
title = {Triangulating Python Performance Issues with {S}calene},
booktitle = {{17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)}},
year = {2023},
isbn = {978-1-939133-34-2},
address = {Boston, MA},
pages = {51--64},
url = {https://www.usenix.org/conference/osdi23/presentation/berger},
publisher = {USENIX Association},
month = jul
}</pre></div>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Success Stories</h2><a id="user-content-success-stories" aria-label="Permalink: Success Stories" href="#success-stories"></a></p>
<p dir="auto">If you use Scalene to successfully debug a performance problem, please <a href="https://github.com/plasma-umass/scalene/issues/58" data-hovercard-type="issue" data-hovercard-url="/plasma-umass/scalene/issues/58/hovercard">add a comment to this issue</a>!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<p dir="auto">Logo created by <a href="https://www.linkedin.com/in/sophia-berger/" rel="nofollow">Sophia Berger</a>.</p>
<p dir="auto">This material is based upon work supported by the National Science
Foundation under Grant No. 1955610. Any opinions, findings, and
conclusions or recommendations expressed in this material are those of
the author(s) and do not necessarily reflect the views of the National
Science Foundation.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sam's Club CTO to Exit Due to Walmart Relocation Policy (107 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2024-10-21/senior-walmart-wmt-executive-to-leave-company-due-to-relocation-policy</link>
            <guid>41908400</guid>
            <pubDate>Mon, 21 Oct 2024 20:50:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2024-10-21/senior-walmart-wmt-executive-to-leave-company-due-to-relocation-policy">https://www.bloomberg.com/news/articles/2024-10-21/senior-walmart-wmt-executive-to-leave-company-due-to-relocation-policy</a>, See on <a href="https://news.ycombinator.com/item?id=41908400">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[T-Mobile, AT&T oppose unlocking rule, claim locked phones are good for users (194 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2024/10/t-mobile-att-oppose-unlocking-rule-claim-locked-phones-are-good-for-users/</link>
            <guid>41908231</guid>
            <pubDate>Mon, 21 Oct 2024 20:31:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2024/10/t-mobile-att-oppose-unlocking-rule-claim-locked-phones-are-good-for-users/">https://arstechnica.com/tech-policy/2024/10/t-mobile-att-oppose-unlocking-rule-claim-locked-phones-are-good-for-users/</a>, See on <a href="https://news.ycombinator.com/item?id=41908231">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
            <article data-id="2057562">
  
  <header>
  <div>
    <div>
      

      

      <p>
        Carriers fight plan to require unlocking of phones 60 days after activation.
      </p>

      
    </div>

          <div>
        <p><img width="1000" height="1000" src="https://cdn.arstechnica.net/wp-content/uploads/2024/10/phone-locking-1000x1000-1729540365.jpg" alt="A smartphone wrapped in a metal chain and padlock" loading="eager" decoding="async" fetchpriority="high" srcset="https://cdn.arstechnica.net/wp-content/uploads/2024/10/phone-locking-1000x1000-1729540365.jpg 1000w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/phone-locking-150x150-1729540364.jpg 150w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/phone-locking-500x500-1729540365.jpg 500w" sizes="(max-width: 1000px) 100vw, 1000px">
        </p>
        
      </div>
      </div>
</header>

  

  
      
    
    <div>
                      
                      
          
<p>T-Mobile and AT&amp;T say US regulators should drop a plan to require unlocking of phones within 60 days of activation, claiming that locking phones to a carrier's network makes it possible to provide cheaper handsets to consumers. "If the Commission mandates a uniform unlocking policy, it is consumers—not providers—who stand to lose the most," T-Mobile alleged in an <a href="https://www.fcc.gov/ecfs/document/1017178290200/1">October 17 filing</a> with the Federal Communications Commission.</p>
<p>The proposed rule has support from consumer advocacy groups who say it will give users more choice and lower their costs. T-Mobile has been criticized for locking phones for up to a year, which makes it impossible to use a phone on a rival's network. T-Mobile claims that with a 60-day unlocking rule, "consumers risk losing access to the benefits of free or heavily subsidized handsets because the proposal would force providers to reduce the line-up of their most compelling handset offers."</p>
<p>If the proposed rule is enacted, "T-Mobile estimates that its prepaid customers, for example, would see subsidies reduced by 40 percent to 70 percent for both its lower and higher-end devices, such as the Moto G, Samsung A15, and iPhone 12," the carrier said. "A handset unlocking mandate would also leave providers little choice but to limit their handset offers to lower cost and often lesser performing handsets."</p>
<p>T-Mobile and other carriers are responding to a call for public comments that began after the FCC approved a <a href="https://docs.fcc.gov/public/attachments/FCC-24-77A1.pdf">Notice of Proposed Rulemaking</a> (NPRM) in a 5–0 vote. The FCC is proposing "to require all mobile wireless service providers to unlock handsets 60 days after a consumer's handset is activated with the provider, unless within the 60-day period the service provider determines the handset was purchased through fraud."</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>When the FCC proposed the 60-day unlocking rule in July 2024, the agency <a href="https://arstechnica.com/tech-policy/2024/07/fcc-blasts-t-mobiles-365-day-phone-locking-proposes-60-day-unlock-rule/">criticized T-Mobile</a> for locking prepaid phones for a year. The NPRM pointed out that "T-Mobile recently increased its locking period for one of its brands, Metro by T-Mobile, from 180 days to 365 days."</p>
<p>T-Mobile's <a href="https://www.t-mobile.com/responsibility/consumer-info/policies/sim-unlock-policy">policy</a> says the carrier will only unlock mobile devices on prepaid plans if "at least 365 days... have passed since the device was activated on the T-Mobile network."</p>
<p>"You bought your phone, you should be able to take it to any provider you want," FCC Chairwoman Jessica Rosenworcel said when the FCC proposed the rule. "Some providers already operate this way. Others do not. In fact, some have recently increased the time their customers must wait until they can unlock their device by as much as 100 percent."</p>
<h2>T-Mobile locking policy more onerous</h2>
<p>T-Mobile executives, who also argue that the FCC lacks authority to impose the proposed rule, met with FCC officials last week to express their concerns.</p>
<p>"T-Mobile is passionate about winning customers for life, and explained how its handset unlocking policies greatly benefit our customers," the carrier said in its post-meeting filing. "Our policies allow us to deliver access to high-speed mobile broadband on a nationwide 5G network <em>via handsets that are free or heavily discounted</em> off the manufacturer's suggested retail price. T-Mobile's unlocking policies are transparent, and there is absolutely no evidence of consumer harm stemming from these policies. T-Mobile's current unlocking policies also help T-Mobile combat handset theft and fraud by sophisticated, international criminal organizations."</p>
<p>For postpaid users, T-Mobile says it allows unlocking of fully paid-off phones that have been active for at least 40 days. But given the 365-day lock on prepaid users, T-Mobile's overall policy is more onerous than those of other carriers. T-Mobile has also <a href="https://arstechnica.com/tech-policy/2024/06/t-mobile-users-enraged-as-un-carrier-breaks-promise-to-never-raise-prices/">faced angry customers</a> because of a recent decision to raise prices on plans that were <a href="https://arstechnica.com/tech-policy/2024/06/t-mobile-users-thought-they-had-a-lifetime-price-lock-guess-what-happened-next/">advertised as having a lifetime price lock</a>.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>AT&amp;T <a href="https://www.att.com/support/article/wireless/KM1262649/">enables unlocking</a> of paid-off phones after 60 days for postpaid users and after six months for prepaid users. AT&amp;T lodged similar complaints as T-Mobile, saying in an October 7 <a href="https://www.fcc.gov/ecfs/document/1008217166272/1">filing</a> that the FCC's proposed rules would "mak[e] handsets less affordable for consumers, especially those in low-income households," and "exacerbate handset arbitrage, fraud, and trafficking. "</p>
<p>AT&amp;T told the FCC that "requiring providers to unlock handsets before they are paid-off would ultimately harm consumers by creating upward pressure on handset prices and disincentives to finance handsets on flexible terms." If the FCC implements any rules, it should maintain "existing contractual arrangements between customers and providers, ensure that providers have at least 180 days to detect fraud before unlocking a device, and include at least a 24-month period for providers to implement any new rules," AT&amp;T said.</p>
<p>Verizon, which already faces unlocking rules because of <a href="https://arstechnica.com/information-technology/2015/02/wireless-net-neutrality-puts-verizon-and-rivals-on-equal-footing/">requirements imposed on spectrum licenses</a> it owns, automatically <a href="https://www.verizon.com/about/consumer-safety/device-unlocking-policy">unlocks</a> phones after 60 days for prepaid and postpaid users. Among the three major carriers, Verizon is the most amenable to the FCC's new rules.</p>

<h2>Consumer groups: Make Verizon rules industry-wide</h2>
<p>An <a href="https://www.fcc.gov/ecfs/document/1018046516995/1">October 18 filing</a> supporting a strict unlocking rule was submitted by numerous consumer advocacy groups including Public Knowledge, New America's Open Technology Institute, Consumer Reports, the National Consumers League, the National Consumer Law Center, and the National Digital Inclusion Alliance.</p>
<p>"Wireless users are subject to unnecessary restrictions in the form of locked devices, which tie them to their service providers even when better options may be available. Handset locking practices limit consumer freedom and lessen competition by creating an artificial technological barrier to switching providers," the groups said.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>The groups cited the Verizon rules as a model and urged the FCC to require "that device unlocking is truly automatic—that is, unlocked after the requisite time period without any additional actions of the consumer." Carriers should not be allowed to lock phones for longer than 60 days even when a phone is on a financing plan with outstanding payments, the groups' letter said:</p>
<blockquote><p>Providers should be required to transition out of selling devices without this [automatic unlocking] capability and the industry-wide rule should be the same as the one protecting Verizon customers today: after the expiration of the initial period, the handset must automatically unlock regardless of whether: (1) the customer asks for the handset to be unlocked or (2) the handset is fully paid off. Removing this barrier to switching will make the standard simple for consumers and encourage providers to compete more vigorously on mobile service price, quality, and innovation.</p></blockquote>
<p>In an October 2 <a href="https://www.fcc.gov/ecfs/document/10022636123363/1">filing</a>, Verizon said it supports "a uniform approach to handset unlocking that allows all wireless providers to lock wireless handsets for a reasonable period of time to limit fraud and to enable device subsidies, followed by automatic unlocking absent evidence of fraud."</p>
<p>Verizon said 60 days should be the minimum for postpaid devices so that carriers have time to detect fraud and theft, and that "a longer, 180-day locking period for prepaid is necessary to enable wireless providers to continue offering subsidies that make phones affordable for prepaid customers." Regardless of what time frame the FCC chooses, Verizon said "a uniform unlocking policy that applies to all providers... will benefit both consumers and competition."</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<h2>FCC considers impact on phone subsidies</h2>
<p>While the FCC is likely to impose an unlocking rule, one question is whether it will apply when a carrier has provided a discounted phone. The FCC's NPRM asked the public for "comment on the impact of a 60-day unlocking requirement in connection with service providers' incentives to offer discounted handsets for postpaid and prepaid service plans."</p>
<p>The FCC acknowledged Verizon's argument "that providers may rely on handset locking to sustain their ability to offer handset subsidies and that such subsidies may be particularly important in prepaid environments." But the FCC noted that public interest groups "argue that locked handsets tied to prepaid plans can disadvantage low-income customers most of all since they may not have the resources to switch service providers or purchase new handsets."</p>
<p>The public interest groups also note that unlocked handsets "facilitate a robust secondary market for used devices, providing consumers with more affordable options," the NPRM said.</p>
<p>The FCC says it can impose phone-unlocking rules using its legal authority under Title III of the Communications Act "to protect the public interest through spectrum licensing and regulations to require mobile wireless service providers to provide handset unlocking." The FCC said it <a href="https://arstechnica.com/information-technology/2015/02/wireless-net-neutrality-puts-verizon-and-rivals-on-equal-footing/">previously relied</a> on the same Title III authority when it imposed the unlocking rules on 700 MHz C Block spectrum licenses purchased by Verizon.</p>
<p>T-Mobile told the FCC in a <a href="https://www.fcc.gov/ecfs/document/10923041590680/1">filing</a> last month that "none of the litany of Title III provisions cited in the NPRM support the expansive authority asserted here to regulate consumer handsets (rather than telecommunications services)." T-Mobile also said that "the Commission's legal vulnerabilities on this score are only magnified in light of recent Supreme Court precedent."</p>
<p>The Supreme Court recently <a href="https://arstechnica.com/tech-policy/2024/06/scotus-kills-chevron-deference-giving-courts-more-power-to-block-federal-rules/">overturned</a> the 40-year-old <em>Chevron</em> precedent that gave agencies like the FCC judicial deference when interpreting ambiguous laws. The end of <em>Chevron</em> makes it harder for agencies to issue regulations without explicit authorization from Congress. This is a potential problem for the FCC in its fight to revive net neutrality rules, which are currently <a href="https://arstechnica.com/tech-policy/2024/08/fcc-suffers-major-setback-in-attempt-to-defend-net-neutrality-rules/">blocked</a> by a court order pending the outcome of litigation.</p>


          
                  </div>

                  
          <div>
  <div>
          <p><a href="https://arstechnica.com/author/jon-brodkin/"><img src="https://arstechnica.com/wp-content/uploads/2016/05/j.brodkin-11_2.jpg" alt="Photo of Jon Brodkin"></a></p>
  </div>

  <div>
    

    <p>
      Jon is a Senior IT Reporter for Ars Technica. He covers the telecom industry, Federal Communications Commission rulemakings, broadband consumer affairs, court cases, and government regulation of the tech industry.
    </p>
  </div>
</div>


  


  


  
              </div>
  </article>


<div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/10/2025-VW-ID.-Buzz-1-768x432.jpg" alt="Listing image for first story in Most Read: The 2025 VW ID Buzz electric bus delivers on the hype" decoding="async" loading="lazy">
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>


  

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[First images from Euclid are in (544 pts)]]></title>
            <link>https://dlmultimedia.esa.int/download/public/videos/2024/10/023/orig-2410_023_AR_EN.mp4</link>
            <guid>41908075</guid>
            <pubDate>Mon, 21 Oct 2024 20:15:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dlmultimedia.esa.int/download/public/videos/2024/10/023/orig-2410_023_AR_EN.mp4">https://dlmultimedia.esa.int/download/public/videos/2024/10/023/orig-2410_023_AR_EN.mp4</a>, See on <a href="https://news.ycombinator.com/item?id=41908075">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Data Formulator – AI-powered data visualization from Microsoft Research (108 pts)]]></title>
            <link>https://github.com/microsoft/data-formulator</link>
            <guid>41907719</guid>
            <pubDate>Mon, 21 Oct 2024 19:42:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/microsoft/data-formulator">https://github.com/microsoft/data-formulator</a>, See on <a href="https://news.ycombinator.com/item?id=41907719">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/data-formulator/blob/main/public/favicon.ico"><img src="https://github.com/microsoft/data-formulator/raw/main/public/favicon.ico" alt="Data Formulator icon" width="28"></a> <b>Data Formulator: Create Rich Visualizations with AI</b>
</h2><a id="user-content------data-formulator-create-rich-visualizations-with-ai" aria-label="Permalink: Data Formulator: Create Rich Visualizations with AI" href="#-----data-formulator-create-rich-visualizations-with-ai"></a></div>
<p dir="auto"><a href="https://arxiv.org/abs/2408.16119" rel="nofollow"><img src="https://camo.githubusercontent.com/14d37bb78cfe9f732aa995eed2e8478e8af2ca184600e69d886b267a42045ccb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50617065722d61725869763a323430382e31363131392d6233316231622e737667" alt="arxiv" data-canonical-src="https://img.shields.io/badge/Paper-arXiv:2408.16119-b31b1b.svg"></a> 
<a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/6cd0120cc4c5ac11d28b2c60f76033b52db98dac641de3b2644bb054b449d60c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"></a> 
<a href="https://youtu.be/3ndlwt0Wi3c" rel="nofollow"><img src="https://camo.githubusercontent.com/5a9427530b8083884be8c5591fd00ca34a8d9ccf2bd68fa168a57987de68803e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f596f75547562652d77686974653f6c6f676f3d796f7574756265266c6f676f436f6c6f723d253233464630303030" alt="YouTube" data-canonical-src="https://img.shields.io/badge/YouTube-white?logo=youtube&amp;logoColor=%23FF0000"></a> 
<a href="https://github.com/microsoft/data-formulator/actions/workflows/python-build.yml"><img src="https://github.com/microsoft/data-formulator/actions/workflows/python-build.yml/badge.svg" alt="build"></a></p>
<p dir="auto">Transform data and create rich visualizations iteratively with AI 🪄. Try Data Formulator now in GitHub Codespaces!</p>
<p dir="auto"><a href="https://codespaces.new/microsoft/data-formulator?quickstart=1" rel="nofollow"><img src="https://github.com/codespaces/badge.svg" alt="Open in GitHub Codespaces"></a></p>
<kbd>
  <a href="https://codespaces.new/microsoft/data-formulator?quickstart=1" title="open Data Formulator in GitHub Codespaces" rel="nofollow"><img src="https://github.com/microsoft/data-formulator/raw/main/public/data-formulator-screenshot.png"></a>
</kbd>
<p dir="auto"><h2 tabindex="-1" dir="auto">News 🔥🔥🔥</h2><a id="user-content-news-" aria-label="Permalink: News 🔥🔥🔥" href="#news-"></a></p>
<ul dir="auto">
<li>
<p dir="auto">[10-11-2024] Data Formulator python package released!</p>
<ul dir="auto">
<li>You can now install Data Formulator using Python and run it locally, easily. <a href="#get-started">[check it out]</a>.</li>
<li>Our Codespace configuration is also updated for fast start up ⚡️. <a href="https://codespaces.new/microsoft/data-formulator?quickstart=1" rel="nofollow">[try it now!]</a></li>
<li>New exprimental feature: load an image or a messy text, and ask AI parsing and cleaning it for you(!). <a href="https://github.com/microsoft/data-formulator/pull/31#issuecomment-2403652717" data-hovercard-type="pull_request" data-hovercard-url="/microsoft/data-formulator/pull/31/hovercard">[demo]</a></li>
</ul>
</li>
<li>
<p dir="auto">[10-01-2024] Initial release of Data Formulator, check out our <a href="https://www.microsoft.com/en-us/research/blog/data-formulator-exploring-how-ai-can-help-analysts-create-rich-data-visualizations/" rel="nofollow">[blog]</a> and <a href="https://youtu.be/3ndlwt0Wi3c" rel="nofollow">[video]</a>!</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto"><strong>Data Formulator</strong> is an application from Microsoft Research that uses large language models to transform data, expediting the practice of data visualization.</p>
<p dir="auto">Data Formulator is an AI-powered tool for analysts to iteratively create rich visualizations. Unlike most chat-based AI tools where users need to describe everything in natural language, Data Formulator combines <em>user interface interactions (UI)</em> and <em>natural language (NL) inputs</em> for easier interaction. This blended approach makes it easier for users to describe their chart designs while delegating data transformation to AI.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Get Started</h2><a id="user-content-get-started" aria-label="Permalink: Get Started" href="#get-started"></a></p>
<p dir="auto">Play with Data Formulator with one of the following options:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Option 1: Install via Python PIP</strong></p>
<p dir="auto">Use Python PIP for an easy setup experience, running locally (recommend: install it in a virtual environment).</p>
<div dir="auto" data-snippet-clipboard-copy-content="# install data_formulator
pip install data_formulator

# start data_formulator
data_formulator 

# alternatively, you can run data formualtor with this command
python -m data_formulator"><pre><span><span>#</span> install data_formulator</span>
pip install data_formulator

<span><span>#</span> start data_formulator</span>
data_formulator 

<span><span>#</span> alternatively, you can run data formualtor with this command</span>
python -m data_formulator</pre></div>
<p dir="auto">Data Formulator will be automatically opened in the browser at <a href="http://localhost:5000/" rel="nofollow">http://localhost:5000</a>.</p>
</li>
<li>
<p dir="auto"><strong>Option 2: Codespaces (5 minutes)</strong></p>
<p dir="auto">You can also run Data Formualtor in codespace, we have everything pre-configured. For more details, see <a href="https://github.com/microsoft/data-formulator/blob/main/CODESPACES.md">CODESPACES.md</a>.</p>
<p dir="auto"><a href="https://codespaces.new/microsoft/data-formulator?quickstart=1" rel="nofollow"><img src="https://github.com/codespaces/badge.svg" alt="Open in GitHub Codespaces"></a></p>
</li>
<li>
<p dir="auto"><strong>Option 3: Working in the developer mode</strong></p>
<p dir="auto">You can build Data Formulator locally if you prefer full control over your development environment and the ability to customize the setup to your specific needs. For detailed instructions, refer to <a href="https://github.com/microsoft/data-formulator/blob/main/DEVELOPMENT.md">DEVELOPMENT.md</a>.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Using Data Formulator</h2><a id="user-content-using-data-formulator" aria-label="Permalink: Using Data Formulator" href="#using-data-formulator"></a></p>
<p dir="auto">Once you’ve completed the setup using either option, follow these steps to start using Data Formulator:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The basics of data visualization</h3><a id="user-content-the-basics-of-data-visualization" aria-label="Permalink: The basics of data visualization" href="#the-basics-of-data-visualization"></a></p>
<ul dir="auto">
<li>Provide OpenAI keys and select a model (GPT-4o suggested) and choose a dataset.</li>
<li>Choose a chart type, and then drag-and-drop data fields to chart properties (x, y, color, ...) to specify visual encodings.</li>
</ul>
<details open="">
  <summary>
    
    <span aria-label="Video description renewable.mp4">renewable.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/93549116/362473677-0fbea012-1d2d-46c3-a923-b1fc5eb5e5b8.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1ODI1MDMsIm5iZiI6MTcyOTU4MjIwMywicGF0aCI6Ii85MzU0OTExNi8zNjI0NzM2NzctMGZiZWEwMTItMWQyZC00NmMzLWE5MjMtYjFmYzVlYjVlNWI4Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDIyVDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUzODM2NmVjNmUwZWNkZjI0YjFiNWU1NzQyODJmZjUxOGQ3YmU2YmQwNGJkMmNkZWU5ZTUxZmFlODkyOTkxNjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.LazVdekJNdzcdrX46rQwPoHJJg_bHUGPuJGBNOFx7Jc" data-canonical-src="https://private-user-images.githubusercontent.com/93549116/362473677-0fbea012-1d2d-46c3-a923-b1fc5eb5e5b8.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1ODI1MDMsIm5iZiI6MTcyOTU4MjIwMywicGF0aCI6Ii85MzU0OTExNi8zNjI0NzM2NzctMGZiZWEwMTItMWQyZC00NmMzLWE5MjMtYjFmYzVlYjVlNWI4Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDIyVDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUzODM2NmVjNmUwZWNkZjI0YjFiNWU1NzQyODJmZjUxOGQ3YmU2YmQwNGJkMmNkZWU5ZTUxZmFlODkyOTkxNjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.LazVdekJNdzcdrX46rQwPoHJJg_bHUGPuJGBNOFx7Jc" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h3 tabindex="-1" dir="auto">Create visualization beyond the initial dataset (powered by 🤖)</h3><a id="user-content-create-visualization-beyond-the-initial-dataset-powered-by-" aria-label="Permalink: Create visualization beyond the initial dataset (powered by 🤖)" href="#create-visualization-beyond-the-initial-dataset-powered-by-"></a></p>
<ul dir="auto">
<li>You can type names of <strong>fields that do not exist in current data</strong> in the encoding shelf:
<ul dir="auto">
<li>this tells Data Formulator that you want to create visualizions that require computation or transformation from existing data,</li>
<li>you can optionally provide a natural language prompt to explain your intent to clarify your intent (not necessary when field names are self-explanatory).</li>
</ul>
</li>
<li>Click the <strong>Formulate</strong> button.
<ul dir="auto">
<li>Data Formulator will transform data and instantiate the visualization based on the encoding and prompt.</li>
</ul>
</li>
<li>Inspect the data, chart and code.</li>
<li>To create a new chart based on existing ones, follow up in natural language:
<ul dir="auto">
<li>provide a follow up prompt (e.g., <em>``show only top 5!''</em>),</li>
<li>you may also update visual encodings for the new chart.</li>
</ul>
</li>
</ul>
<details open="">
  <summary>
    
    <span aria-label="Video description renewable-pct.mp4">renewable-pct.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/93549116/362473692-160c69d2-f42d-435c-9ff3-b1229b5bddba.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1ODI1MDMsIm5iZiI6MTcyOTU4MjIwMywicGF0aCI6Ii85MzU0OTExNi8zNjI0NzM2OTItMTYwYzY5ZDItZjQyZC00MzVjLTlmZjMtYjEyMjliNWJkZGJhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDIyVDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWRjYTY5MDY0YThhZTFiMWFmZTdjODVlYjgwOTc4NWVmYjA2YjY1NzJiNWZjN2ZiZDJjZTZhZjNkMzcwZGQ0YWImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.HcvvNh0w679raPdJSJveVEJJrJbF_nZFcPk42A5sjjk" data-canonical-src="https://private-user-images.githubusercontent.com/93549116/362473692-160c69d2-f42d-435c-9ff3-b1229b5bddba.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1ODI1MDMsIm5iZiI6MTcyOTU4MjIwMywicGF0aCI6Ii85MzU0OTExNi8zNjI0NzM2OTItMTYwYzY5ZDItZjQyZC00MzVjLTlmZjMtYjEyMjliNWJkZGJhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDIyVDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWRjYTY5MDY0YThhZTFiMWFmZTdjODVlYjgwOTc4NWVmYjA2YjY1NzJiNWZjN2ZiZDJjZTZhZjNkMzcwZGQ0YWImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.HcvvNh0w679raPdJSJveVEJJrJbF_nZFcPk42A5sjjk" controls="controls" muted="muted">

  </video>
</details>

<details open="">
  <summary>
    
    <span aria-label="Video description renewable-rank.mp4">renewable-rank.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/93549116/362473693-c93b3e84-8ca8-49ae-80ea-f91ceef34acb.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1ODI1MDMsIm5iZiI6MTcyOTU4MjIwMywicGF0aCI6Ii85MzU0OTExNi8zNjI0NzM2OTMtYzkzYjNlODQtOGNhOC00OWFlLTgwZWEtZjkxY2VlZjM0YWNiLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDIyVDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc4MjNmYzNlYTRkNGMyODFhN2JiMzU0ZTg4NWZiODkyODJhNjJjNzI4MzNkOTMyODcxYjQ5YmFkZjMxNjc4OTYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.h5DVujuxv-F8aPyEiUyMThp9xl88-j-9h6LylXwytUo" data-canonical-src="https://private-user-images.githubusercontent.com/93549116/362473693-c93b3e84-8ca8-49ae-80ea-f91ceef34acb.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1ODI1MDMsIm5iZiI6MTcyOTU4MjIwMywicGF0aCI6Ii85MzU0OTExNi8zNjI0NzM2OTMtYzkzYjNlODQtOGNhOC00OWFlLTgwZWEtZjkxY2VlZjM0YWNiLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDIyVDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc4MjNmYzNlYTRkNGMyODFhN2JiMzU0ZTg4NWZiODkyODJhNjJjNzI4MzNkOTMyODcxYjQ5YmFkZjMxNjc4OTYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.h5DVujuxv-F8aPyEiUyMThp9xl88-j-9h6LylXwytUo" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Repeat this process as needed to explore and understand your data. Your explorations are trackable in the <strong>Data Threads</strong> panel.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Developers' Guide</h2><a id="user-content-developers-guide" aria-label="Permalink: Developers' Guide" href="#developers-guide"></a></p>
<p dir="auto">Follow the <a href="https://github.com/microsoft/data-formulator/blob/main/DEVELOPMENT.md">developers' instructions</a> to build your new data analysis tools on top of Data Formulator.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Research Papers</h2><a id="user-content-research-papers" aria-label="Permalink: Research Papers" href="#research-papers"></a></p>
<ul dir="auto">
<li><a href="https://arxiv.org/abs/2408.16119" rel="nofollow">Data Formulator 2: Iteratively Creating Rich Visualizations with AI</a></li>
</ul>
<div data-snippet-clipboard-copy-content="@article{wang2024dataformulator2iteratively,
      title={Data Formulator 2: Iteratively Creating Rich Visualizations with AI}, 
      author={Chenglong Wang and Bongshin Lee and Steven Drucker and Dan Marshall and Jianfeng Gao},
      year={2024},
      booktitle={ArXiv preprint arXiv:2408.16119},
}"><pre><code>@article{wang2024dataformulator2iteratively,
      title={Data Formulator 2: Iteratively Creating Rich Visualizations with AI}, 
      author={Chenglong Wang and Bongshin Lee and Steven Drucker and Dan Marshall and Jianfeng Gao},
      year={2024},
      booktitle={ArXiv preprint arXiv:2408.16119},
}
</code></pre></div>
<ul dir="auto">
<li><a href="https://arxiv.org/abs/2309.10094" rel="nofollow">Data Formulator: AI-powered Concept-driven Visualization Authoring</a></li>
</ul>
<div data-snippet-clipboard-copy-content="@article{wang2023data,
  title={Data Formulator: AI-powered Concept-driven Visualization Authoring},
  author={Wang, Chenglong and Thompson, John and Lee, Bongshin},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  year={2023},
  publisher={IEEE}
}"><pre><code>@article{wang2023data,
  title={Data Formulator: AI-powered Concept-driven Visualization Authoring},
  author={Wang, Chenglong and Thompson, John and Lee, Bongshin},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  year={2023},
  publisher={IEEE}
}
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">This project welcomes contributions and suggestions. Most contributions require you to
agree to a Contributor License Agreement (CLA) declaring that you have the right to,
and actually do, grant us the rights to use your contribution. For details, visit
<a href="https://cla.microsoft.com/" rel="nofollow">https://cla.microsoft.com</a>.</p>
<p dir="auto">When you submit a pull request, a CLA-bot will automatically determine whether you need
to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the
instructions provided by the bot. You will only need to do this once across all repositories using our CLA.</p>
<p dir="auto">This project has adopted the <a href="https://opensource.microsoft.com/codeofconduct/" rel="nofollow">Microsoft Open Source Code of Conduct</a>.
For more information see the <a href="https://opensource.microsoft.com/codeofconduct/faq/" rel="nofollow">Code of Conduct FAQ</a>
or contact <a href="mailto:opencode@microsoft.com">opencode@microsoft.com</a> with any additional questions or comments.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Trademarks</h2><a id="user-content-trademarks" aria-label="Permalink: Trademarks" href="#trademarks"></a></p>
<p dir="auto">This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
<a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general" rel="nofollow">Microsoft's Trademark &amp; Brand Guidelines</a>.
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Software Engineering Body of Knowledge (SWEBOK) v4.0 is out [pdf] (166 pts)]]></title>
            <link>https://ieeecs-media.computer.org/media/education/swebok/swebok-v4.pdf</link>
            <guid>41907412</guid>
            <pubDate>Mon, 21 Oct 2024 19:16:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ieeecs-media.computer.org/media/education/swebok/swebok-v4.pdf">https://ieeecs-media.computer.org/media/education/swebok/swebok-v4.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=41907412">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Overengineering a way to know if people are in my university's CS lab (150 pts)]]></title>
            <link>https://www.amoses.dev/blog/upl-people-counter/</link>
            <guid>41907360</guid>
            <pubDate>Mon, 21 Oct 2024 19:12:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.amoses.dev/blog/upl-people-counter/">https://www.amoses.dev/blog/upl-people-counter/</a>, See on <a href="https://news.ycombinator.com/item?id=41907360">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-w6n32adp=""> <h2 id="a-history-of-upls-cameras"><a href="#a-history-of-upls-cameras">#</a>A History of UPL’s Cameras</h2>
<p>For almost as long as <a href="https://www.upl.cs.wisc.edu/">the Undergraduate Projects Lab</a> at the University of Wisconsin has existed, there’s been a camera of some sort peering at the room. There’s evidence of a system existing even as far back as the 1990s, with a <a href="https://web.archive.org/web/19981202092257/http://www.upl.cs.wisc.edu/cgi-bin/uplcam.html">prehistoric revision of the site</a> mentioning that an old iteration was:</p>
<blockquote>
<p>…a $15 video camera attached to the wall with duct tape, connected to a VCR, connected to a video spigot in a Mac IIcx, running Timed Video Grabber (TVG), and FTPd. Dax, an HP workstation ran a script that would try to FTP the latest image every 60 seconds. Because the clocks would drift, occasionally, the file accesses would collide, and the whole scheme would break.</p>
</blockquote>
<p>Just <em>reading</em> that makes me stare at the camera that now sits perched on top of the arcade cabinet with wonder. What used to be several thousand dollars of equipment is now achievable (with immeasurably better quality) with a $50 webcam plugged into a Raspberry Pi.</p>
<p><img src="https://www.amoses.dev/images/upl-pc/old_upl.png" alt="A grainy image featuring an interior view of the UPL, a triangular-shaped undergraduate lab at UW-Madison.">
    <img src="https://www.amoses.dev/images/upl-pc/new_upl.jpeg" alt="An image featuring the interior of the UPL, a lab at UW-Madison. Students sit at laptops.">
</p>
<p><i>Taken ~25 years apart.</i></p>
<p>I could—and probably will—write an entire other blog post about the intricate history of the UPL, mentioning how <a href="https://web.archive.org/web/20001003051528/http://www.upl.cs.wisc.edu/uplcam/spincam.html">older versions of the website</a> allowed for users to control the tilt and pan of the camera using four stepper motors attached to the camera.</p>
<p>However, the focus of this article is about the two <em>latest</em> iterations of cameras in the UPL.</p>
<h2 id="is-the-upl-open-right-now"><a href="#is-the-upl-open-right-now">#</a>“Is the UPL open right now?”</h2>
<p>I’m sure that any UPL member can testify the horror of arriving to the lab to see a closed door. If you live anywhere off campus, it’s heartbreaking to see your arduous trek to the CS building result in failure.</p>
<p>There’s no doubt that as far back as IRC, members of the UPL messaged each other asking if the lab was open. With the advent of mobile phones, it’s gotten easier to bother your friends—who may not even be in the room!</p>
<p>Well, myself, in collaboration with other UPL members, decided to fix this issue in, perhaps, the most CS-student-esque way possible: an automated system to identify the occupancy of the lab.</p>
<h2 id="people-counting"><a href="#people-counting">#</a>People counting</h2>
<p>The first iteration of the people counting system (as built by <a href="https://github.com/mdberkey">Michael Berkey</a>) utilized a Logitech C920 camera mounted on a vantage point that had a clear view of the room. A Discord bot was set on a 15 minute loop (using discord.py.ext’s <code>@tasks.loop(minutes=15)</code>) to call a YOLOv7 model set to class 0 (detecting people). The bot called the webcam to take an image, then ran it through the model for inference. It returned the number of people in the room (and annotated the image with bounding boxes of where it believed the people to be, for debug purposes).</p>
<p><img src="https://www.amoses.dev/images/upl-pc/camera-over.png" alt="The front side of a C920 webcam.">
    <img src="https://www.amoses.dev/images/upl-pc/camera-peek.png" alt="The back side of a C920 webcam.">
</p>
<p><i>…don’t mind the tape.</i></p>
<p>It then set the name of a channel to the results (either <code>1-person-in-upl</code> or <code>X-people-in-upl</code>), which others could check.</p>
<p><img src="https://www.amoses.dev/images/upl-pc/8-people-in-upl.png" alt="A channel in the UPL Discord reads '8 people in UPL'.">
</p>
<p><i>An example of what the Discord looked like on a day with a semi-busy UPL.</i></p>
<h2 id="switching-to-door-sensing"><a href="#switching-to-door-sensing">#</a>Switching to door sensing</h2>
<p>This worked perfectly for a while — people would check the Discord channel name and see the estimated count of the number of people in the room. If it said “zero people”, they could infer that the UPL wasn’t open.</p>
<p>However, this solution started presenting issues. For one, having people in the room didn’t necessarily indicate that the UPL was open. There could be a meeting, or a separate gathering where the doors were closed and people weren’t allowed inside. This was confusing to people who might have seen “8 people inside the UPL”, only to arrive at the building to see coords having a meeting.</p>
<p>There was also the issue of the model sometimes interpreting the chair in the corner as a person<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup>:</p>
<p><img src="https://www.amoses.dev/images/upl-pc/upl_empty.png" alt="An image of the empty lab. An annotation on a brown armchair reads 'Person 0', despite no human on the chair.">
</p>
<p><i>Either the model is too sensitive, or the UPL has a friendly ghost!</i></p>
<p>It was around this time that I stumbled upon the homepage of <a href="https://miters.mit.edu/">MITERS</a>, a makerspace at MIT. On their website, they broadcast whether the door to the space is open using a reed switch attached to a Raspberry Pi. Reed switches are small, physical components that are able to detect a magnetic field. If you put one on a doorframe, and then attach a tiny magnet to the door itself, you have an effective way of detecting whether a door is open or closed! I was able to find <a href="https://andrewbirkel.com/projects/MITERS_Door.html">a writeup</a> by a former member of the space on their implementation, but I can’t guarantee that it’s accurate to how it’s set up there currently.</p>
<p>I considered using similar components for a door status checker for the UPL — it wouldn’t have been too much effort to buy WiFi enabled ESP32 modules and off-the-shelf door-mountable reed switches. Then, I would have the chips simply send a POST request with their status every time the door was opened or closed.</p>
<p>I decided against this approach for a few reasons:</p>
<ul>
<li>
<p>The UPL doesn’t really have the equipment to maintain such a system. I don’t know how to solder, and mounting breadboards to the walls doesn’t seem like the most future-proof or aesthetically pleasing solution.</p>
</li>
<li>
<p>If the system were to spontaneously break after I left, it would be difficult to find somebody to fix it. The UPL is mainly a software oriented lab!</p>
</li>
<li>
<p>The WiFi ran by the university (UWNet) requires you to log in with a <a href="https://en.wikipedia.org/wiki/Captive_portal">captive portal</a> to register your device to connect to the network. Without intervention, it will occasionally require you to sign back in to renew your ability to connect<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup>. While there <em>are</em> some ways to emulate the specific requests a typical browser would use to authenticate with your NetID, it would be a ton of recurring effort (and the login it used would have to be changed as people graduated)!</p>
</li>
</ul>
<p>So, I decided that the sensors themselves would have to act autonomously and simply relay their state to a device elsewhere in the room. Luckily, the Raspberry Pi that ran the code for the people counter was easily repurposed. I installed <a href="https://www.home-assistant.io/">Home Assistant</a>, an open source platform for interfacing with various network connected devices.</p>
<p>There are plenty of devices that track the status of doors, made by companies like Ring and ADT for home security. However, they usually require proprietary hubs to check their status, and don’t offer easily accessible APIs to interface with the device. Luckily, there was a better solution!</p>
<h2 id="zigbee"><a href="#zigbee">#</a>Zigbee!</h2>
<p>Enter Zigbee. It’s a low-rate mesh wireless protocol that allows for smart devices to communicate over a personal area network. A benefit of this is that you’re able to use one hub to communicate with a variety of devices, even those made by different manufacturers. Instead of searching for a particular brand for the door contact sensors, I would just have to find ones that supported the Zigbee protocol. Then I would be able to view their status through Home Assistant’s dashboard.</p>
<p>It’s important to note that Zigbee radios operate independently from WiFi or Bluetooth antennas. If you want to interface with Zigbee devices, you’ll have to pick up a special receiver that can support the protocol. For this project, I picked up <a href="https://www.amazon.com/SONOFF-Universal-Assistant-Zigbee2MQTT-Wireless/dp/B0B6P22YJC/">this one</a> made by SONOFF. Home Assistant’s Zigbee integration is called <a href="https://www.home-assistant.io/integrations/zha/">Zigbee Home Automation</a>, and it supports a variety of Zigbee coordinators (the USB dongles that allow for connections). When you use this integration, Home Assistant automatically creates a Zigbee network that the devices can join.</p>
<p>I decided to use <a href="https://www.amazon.com/Aqara-MCCGQ11LM-Window-Sensor-White/dp/B07D37VDM3/">these Aqara door and window sensors</a> for this project. They had the best reviews out of all of the Zigbee door sensors I looked at, and have a battery life of two years (with an easily replaceable CR1632 cell).</p>
<p>Once the coordinator and sensors arrived, I created a Home Assistant login and installed the ZHA integration. Pairing simply required holding the “reset” button on the sensors until Home Assistant recognized them and added the corresponding entities in the dashboard.</p>

<h2 id="using-the-door-statuses"><a href="#using-the-door-statuses">#</a>Using the door statuses</h2>
<p>Once this was all configured, I had the live statuses of the doors through the Home Assistant dashboard! I’m not going to lie, it was really fun opening and closing the doors repeatedly and seeing the dashboard change in real-time (even if passerby in the CS building probably thought I was crazy).</p>
<p>
    <video src="https://www.amoses.dev/images/upl-pc/doors.mp4" controls="">
    Your browser does not support the video tag.
</video>
</p>
<p><i>It’s so satisfying to watch this happen in real-time.<sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup></i></p>
<p>An important thing to note here is that UWNet provides total access point isolation. None of the devices on the network can see any of the others (for good reason, as it would be a huge security vulnerability for any devices with open ports). If this wasn’t a limitation, I would just have the website directly query the rpi.</p>
<p>My first intuition was to use Home Assistant’s <a href="https://www.home-assistant.io/integrations/rest_command/">RESTful Command</a> integration to send a POST request to my webserver whenever the status of the doors changed. These require you to setup each command ahead of time, in HA’s <code>configuration.yml</code>:</p>
<div><figure><pre data-language="yaml"><code><div><div><p>1</p></div><p><span>rest_command</span><span>:</span></p></div><div><div><p>2</p></div><p><span>  </span><span>door1_opened</span><span>:</span></p></div><div><div><p>3</p></div><p><span>    </span><span>url</span><span>: </span><span>"https://doors.amoses.dev/door1/open"</span></p></div><div><div><p>4</p></div><p><span>    </span><span>method</span><span>: </span><span>POST</span></p></div><div><div><p>5</p></div><p><span>    </span><span>headers</span><span>:</span></p></div><div><div><p>6</p></div><p><span>      </span><span>content-type</span><span>: </span><span>"application/json"</span></p></div><div><div><p>7</p></div><p><span>    </span><span>payload</span><span>: </span><span>'{"door": "door1", "state": "open"}'</span></p></div><div><div><p>8</p></div><p><span>    </span><span>content_type</span><span>: </span><span>"application/json; charset=utf-8"</span></p></div><div><p>9</p></div><div><div><p>10</p></div><p><span>  </span><span>door1_closed</span><span>:</span></p></div><div><div><p>11</p></div><p><span>    </span><span>url</span><span>: </span><span>"https://doors.amoses.dev/door1/close"</span></p></div><div><div><p>12</p></div><p><span>    </span><span>method</span><span>: </span><span>POST</span></p></div><div><div><p>13</p></div><p><span>    </span><span>headers</span><span>:</span></p></div><div><div><p>14</p></div><p><span>      </span><span>content-type</span><span>: </span><span>"application/json"</span></p></div><div><div><p>15</p></div><p><span>    </span><span>payload</span><span>: </span><span>'{"door": "door1", "state": "closed"}'</span></p></div><div><div><p>16</p></div><p><span>    </span><span>content_type</span><span>: </span><span>"application/json; charset=utf-8"</span></p></div><div><p>17</p></div><div><div><p>18</p></div><p><span>  </span><span>door2_opened</span><span>:</span></p></div><div><div><p>19</p></div><p><span>    </span><span>url</span><span>: </span><span>"https://doors.amoses.dev/door2/open"</span></p></div><div><div><p>20</p></div><p><span>    </span><span>method</span><span>: </span><span>POST</span></p></div><div><div><p>21</p></div><p><span>    </span><span>headers</span><span>:</span></p></div><div><div><p>22</p></div><p><span>      </span><span>content-type</span><span>: </span><span>"application/json"</span></p></div><div><div><p>23</p></div><p><span>    </span><span>payload</span><span>: </span><span>'{"door": "door2", "state": "open"}'</span></p></div><div><div><p>24</p></div><p><span>    </span><span>content_type</span><span>: </span><span>"application/json; charset=utf-8"</span></p></div><div><p>25</p></div><div><div><p>26</p></div><p><span>  </span><span>door2_closed</span><span>:</span></p></div><div><div><p>27</p></div><p><span>    </span><span>url</span><span>: </span><span>"https://doors.amoses.dev/door2/close"</span></p></div><div><div><p>28</p></div><p><span>    </span><span>method</span><span>: </span><span>POST</span></p></div><div><div><p>29</p></div><p><span>    </span><span>headers</span><span>:</span></p></div><div><div><p>30</p></div><p><span>      </span><span>content-type</span><span>: </span><span>"application/json"</span></p></div><div><div><p>31</p></div><p><span>    </span><span>payload</span><span>: </span><span>'{"door": "door2", "state": "closed"}'</span></p></div><div><div><p>32</p></div><p><span>    </span><span>content_type</span><span>: </span><span>"application/json; charset=utf-8"</span></p></div></code></pre></figure></div>
<p>…but I very quickly realized that this solution wasn’t the best. For one, when I published <a href="https://github.com/UW-UPL/people-counter-v2/blob/main/home-assistant/configuration.yaml">the source code</a> onto GitHub, some very funny students decided that they would manually simulate the POST requests and change the status of the doors to be inaccurate. That’s what I get for leaving the endpoint unsecured!<sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup></p>
<p>I eventually learned that Home Assistant provides a <a href="https://developers.home-assistant.io/docs/api/rest/">RESTful API</a> directly alongside the web dashboard. If I set that up, I would be able to query the instance for the states of the connected devices.<sup><a href="#user-content-fn-5" id="user-content-fnref-5" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup> All it took was appending an <code>/api/</code> route to the HA URL. I could just use that!</p>
<p>The API has all of its routes authenticated with a bearer token (to most likely mirror the permissions of the frontend, which requires a user login before showing any data). Given that I wanted to display the door status on the UPL’s page, I realized the potential danger in shipping the bearer token with the site. Any crafty user could take it and access any other route on Home Assistant’s API. Given the level of information and control available on HA instances, this could be disastrous.</p>
<p>I made a quick webserver using Express that proxies the request with the bearer token and only serves the relevant door information. Because it displays this separately, the user has no way of seeing or manipulating anything beyond this.</p>
<div><figure><pre data-language="js"><code><details><summary><div><div><p>7 collapsed lines</p></div></div></summary><div><div><p>1</p></div><p><span>const</span><span> </span><span>express</span><span> = </span><span>require</span><span>(</span><span>"express"</span><span>);</span></p></div><div><div><p>2</p></div><p><span>const</span><span> </span><span>axios</span><span> = </span><span>require</span><span>(</span><span>"axios"</span><span>);</span></p></div><div><div><p>3</p></div><p><span>const</span><span> </span><span>cors</span><span> = </span><span>require</span><span>(</span><span>"cors"</span><span>);</span></p></div><div><p>4</p></div><div><div><p>5</p></div><p><span>const</span><span> </span><span>app</span><span> = </span><span>express</span><span>();</span></p></div><div><div><p>6</p></div><p><span>const</span><span> </span><span>PORT</span><span> = </span><span>3500</span><span>;</span></p></div><div><p>7</p></div></details><div><div><p>8</p></div><p><span>const</span><span> </span><span>apiUrl</span><span> = </span><span>"https://HOMEASSISTANT-URL-HERE/api/states"</span><span>;</span></p></div><div><div><p>9</p></div><p><span>const</span><span> </span><span>token</span><span> = </span><span>"Bearer TOKEN-GOES-HERE"</span><span>;</span></p></div><div><p>10</p></div><div><div><p>11</p></div><p><span>app</span><span>.</span><span>use</span><span>(</span><span>cors</span><span>());</span></p></div><div><p>12</p></div><div><div><p>13</p></div><p><span>app</span><span>.</span><span>get</span><span>(</span><span>"/door-status"</span><span>, </span><span>async</span><span> (</span><span>req</span><span>, </span><span>res</span><span>) </span><span>=&gt;</span><span> {</span></p></div><div><div><p>14</p></div><p><span>  </span><span>try</span><span> {</span></p></div><div><div><p>15</p></div><p><span>    </span><span>const</span><span> </span><span>response</span><span> = </span><span>await</span><span> </span><span>axios</span><span>.</span><span>get</span><span>(</span><span>apiUrl</span><span>, {</span></p></div><div><div><p>16</p></div><p><span>      </span><span>headers:</span><span> {</span></p></div><div><div><p>17</p></div><p><span>        </span><span>Authorization:</span><span> </span><span>token</span><span>,</span></p></div><div><div><p>18</p></div><p><span><span>      </span></span><span>},</span></p></div><div><div><p>19</p></div><p><span><span>    </span></span><span>});</span></p></div><div><p>20</p></div><div><div><p>21</p></div><p><span>    </span><span>const</span><span> </span><span>data</span><span> = </span><span>response</span><span>.</span><span>data</span><span>;</span></p></div><div><p>22</p></div><div><div><p>23</p></div><p><span>    </span><span>// grab the items with the appropriate HA entity ids</span></p></div><div><div><p>24</p></div><p><span>    </span><span>const</span><span> </span><span>doors</span><span> = </span><span>data</span><span>.</span><span>filter</span><span>(</span></p></div><div><div><p>25</p></div><p><span><span>      </span></span><span>(</span><span>item</span><span>) </span><span>=&gt;</span></p></div><div><div><p>26</p></div><p><span>        </span><span>item</span><span>.</span><span>entity_id</span><span> === </span><span>"binary_sensor.back"</span><span> ||</span></p></div><div><div><p>27</p></div><p><span>        </span><span>item</span><span>.</span><span>entity_id</span><span> === </span><span>"binary_sensor.front"</span></p></div><div><div><p>28</p></div><p><span><span>    </span></span><span>);</span></p></div><div><p>29</p></div><div><div><p>30</p></div><p><span>    </span><span>// extract status and last updated information</span></p></div><div><div><p>31</p></div><p><span>    </span><span>const</span><span> </span><span>doorStatus</span><span> = </span><span>doors</span><span>.</span><span>map</span><span>((</span><span>door</span><span>) </span><span>=&gt;</span><span> ({</span></p></div><div><div><p>32</p></div><p><span>      </span><span>door:</span><span> </span><span>door</span><span>.</span><span>attributes</span><span>.</span><span>friendly_name</span><span>,</span></p></div><div><div><p>33</p></div><p><span>      </span><span>status:</span><span> </span><span>door</span><span>.</span><span>state</span><span>,</span></p></div><div><div><p>34</p></div><p><span>      </span><span>last_updated:</span><span> </span><span>door</span><span>.</span><span>last_updated</span><span>,</span></p></div><div><div><p>35</p></div><p><span><span>    </span></span><span>}));</span></p></div><div><p>36</p></div><div><div><p>37</p></div><p><span>    </span><span>// send the filtered data as a json response</span></p></div><div><div><p>38</p></div><p><span>    </span><span>res</span><span>.</span><span>json</span><span>(</span><span>doorStatus</span><span>);</span></p></div><div><div><p>39</p></div><p><span><span>  </span></span><span>} </span><span>catch</span><span> (</span><span>error</span><span>) {</span></p></div><div><div><p>40</p></div><p><span>    </span><span>res</span><span>.</span><span>status</span><span>(</span><span>500</span><span>).</span><span>send</span><span>(</span><span>"Error fetching data"</span><span>);</span></p></div><div><div><p>41</p></div><p><span><span>  </span></span><span>}</span></p></div><div><div><p>42</p></div><p><span>});</span></p></div><details><summary><div><div><p>11 collapsed lines</p></div></div></summary><div><p>43</p></div><div><div><p>44</p></div><p><span>// :P</span></p></div><div><div><p>45</p></div><p><span>app</span><span>.</span><span>get</span><span>(</span><span>"/"</span><span>, </span><span>async</span><span> (</span><span>req</span><span>, </span><span>res</span><span>) </span><span>=&gt;</span><span> {</span></p></div><div><div><p>46</p></div><p><span>  </span><span>res</span></p></div><div><div><p>47</p></div><p><span><span>    </span></span><span>.</span><span>status</span><span>(</span><span>200</span><span>)</span></p></div><div><div><p>48</p></div><p><span><span>    </span></span><span>.</span><span>send</span><span>(</span><span>"&lt;html&gt;&lt;body&gt;&lt;b&gt;wow upl door status endpoint 443&lt;/b&gt;&lt;/body&gt;&lt;/html&gt;"</span><span>);</span></p></div><div><div><p>49</p></div><p><span>});</span></p></div><div><p>50</p></div><div><div><p>51</p></div><p><span>app</span><span>.</span><span>listen</span><span>(</span><span>PORT</span><span>, () </span><span>=&gt;</span><span> {</span></p></div><div><div><p>52</p></div><p><span>  </span><span>console</span><span>.</span><span>log</span><span>(</span><span>`Server is running on port </span><span>${</span><span>PORT</span><span>}</span><span>`</span><span>);</span></p></div><div><div><p>53</p></div><p><span>});</span></p></div></details></code></pre></figure></div>
<p>Now, the server will query Home Assistant’s API on your behalf (with the proper bearer token). It’ll return a JSON object of the door statuses and their last change, like so:</p>
<div><figure><pre data-language="json"><code><div><p><span>[</span></p></div><div><p><span><span>  </span></span><span>{</span></p></div><div><p><span>    </span><span>"door"</span><span>: </span><span>"back"</span><span>,</span></p></div><div><p><span>    </span><span>"status"</span><span>: </span><span>"on"</span><span>,</span></p></div><div><p><span>    </span><span>"last_updated"</span><span>: </span><span>"2024-10-12T20:01:54.353657+00:00"</span></p></div><div><p><span><span>  </span></span><span>},</span></p></div><div><p><span><span>  </span></span><span>{</span></p></div><div><p><span>    </span><span>"door"</span><span>: </span><span>"front"</span><span>,</span></p></div><div><p><span>    </span><span>"status"</span><span>: </span><span>"on"</span><span>,</span></p></div><div><p><span>    </span><span>"last_updated"</span><span>: </span><span>"2024-10-12T20:02:10.132178+00:00"</span></p></div><div><p><span><span>  </span></span><span>}</span></p></div><div><p><span>]</span></p></div></code></pre></figure></div>
<p>…and the Discord bot/UPL website can use that to let people know what the status is.</p>
<p><img src="https://www.amoses.dev/images/upl-pc/open_door.png" alt="The UPL website reads 'the doors are open!' with an icon of an open door.">
</p>
<p><i>The UPL website uses a header component which fetches the door status every 15 seconds.</i></p>
<p><img src="https://www.amoses.dev/images/upl-pc/upl_discord.png" alt="A channel in the UPL Discord reads 'UPL doors open'.">
</p>
<p><i>The Discord channel name is an easy way to see the status without opening the site.</i></p>
<h2 id="conclusion"><a href="#conclusion">#</a>Conclusion</h2>
<p>I’m pretty happy with how this project turned out. It’s been really fun developing something that I actually use every day, and I find it pretty special that every time I check if the UPL’s open or not, I’m doing it via something that I made myself.</p>
<br>
<hr>
<br>
<section data-footnotes="">
<ol>
<li id="user-content-fn-1">
<p>I’m sure that you could apply various transformations to the image to mask out that area from detection. But people occasionally sit in it! <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="user-content-fn-2">
<p>If you’ve ever lived in the UW dorms, you’ll know all too well what I’m talking about. Every device without browser access needs to have its MAC address whitelisted by the network system. This authorization expires in six months, so you’ll lose internet access and have to renew. <a href="#user-content-fnref-2" data-footnote-backref="" aria-label="Back to reference 2">↩</a></p>
</li>
<li id="user-content-fn-3">
<p>The UPL has a front and back entrance, hence the wording being “doors” instead of “door”. <a href="#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 3">↩</a></p>
</li>
<li id="user-content-fn-4">
<p>Before you try, these endpoints aren’t in use anymore. :P <a href="#user-content-fnref-4" data-footnote-backref="" aria-label="Back to reference 4">↩</a></p>
</li>
<li id="user-content-fn-5">
<p>Keen eyed readers might be asking “what about the AP isolation issue that you just mentioned?!”. Well, I found a fantastic addon for Home Assistant that allows you to access your dashboard (and the API, by extension) when not on the LAN of the pi. It uses Cloudflare tunnels, and you can find <a href="https://github.com/brenner-tobias/addon-cloudflared">its GitHub repository here.</a> <a href="#user-content-fnref-5" data-footnote-backref="" aria-label="Back to reference 5">↩</a></p>
</li>
</ol>
</section> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft is introducing hidden APIs to VS Code only enabled for Copilot (348 pts)]]></title>
            <link>https://old.reddit.com/r/ChatGPTCoding/comments/1g8xrub/microsoft_is_introducing_hidden_apis_to_vs_code/</link>
            <guid>41907350</guid>
            <pubDate>Mon, 21 Oct 2024 19:11:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/ChatGPTCoding/comments/1g8xrub/microsoft_is_introducing_hidden_apis_to_vs_code/">https://old.reddit.com/r/ChatGPTCoding/comments/1g8xrub/microsoft_is_introducing_hidden_apis_to_vs_code/</a>, See on <a href="https://news.ycombinator.com/item?id=41907350">Hacker News</a></p>
Couldn't get https://old.reddit.com/r/ChatGPTCoding/comments/1g8xrub/microsoft_is_introducing_hidden_apis_to_vs_code/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Please do not write below the line (313 pts)]]></title>
            <link>http://www.bbctvlicence.com/Please%20do%20not%20write%20below%20the%20line.htm</link>
            <guid>41907071</guid>
            <pubDate>Mon, 21 Oct 2024 18:42:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://www.bbctvlicence.com/Please%20do%20not%20write%20below%20the%20line.htm">http://www.bbctvlicence.com/Please%20do%20not%20write%20below%20the%20line.htm</a>, See on <a href="https://news.ycombinator.com/item?id=41907071">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <span face="Bell MT"><b><span color="#000000" size="+2" face="Verdana">Please do not write below this line</span></b></span><p><span face="Bell MT"><b><span color="#000000" size="+1" face="Verdana">I have been vexed for some time by the request at the bottom of
						  each letter that I am not to write below the line. </span></b></span></p><img src="http://www.bbctvlicence.com/Please%20do%20not%20write%20below%20this%20line.jpg" width="538" height="25"> 
						<p><span face="Bell MT"><b><span color="#000000" size="+1" face="Verdana">I emailed TVL/BBC on 5 November 2006 to find out why:
						  </span></b></span></p> 
						<div> 
								<p><span size="-1" color="#000000" face="Arial">I've had a letter from TV Licencing and I'm interested in the
										statement at the bottom of the page. It says: 'Please do not write below this
										line'. I would like to know why the letter requests this. The line referred to
										is about half an inch from the bottom edge of the letter. </span><br><span size="-1" color="#000000" face="Arial"></span><br><span size="-1" color="#000000" face="Arial">What will happen if I write there? How would you know? I am not
										asked to return the letter, so why the request?</span></p><span face="Bell MT"><b><span color="#000000" face="Arial">Seven weeks later, on 27 December 2006, I receive
								this from one Kelly Wright: </span></b></span><p><span size="-1" color="#000000" face="Arial">Thank you for contacting us. Unfortunately I am unable to deal
										with your request, as you have not provided your address and licence number. If
										you have moved address I will need both your new and old address. Once I have
										this information I will action your request and send you the appropriate form
										or confirmation.</span></p><br><span face="Bell MT"><b><span color="#000000" face="Arial">I reply: </span></b></span> <p><span size="-1" color="#000000" face="Arial"><span face="Arial"><span size="-1" color="#000000">Dear Ms Wright,
										</span></span><br><span face="Arial"><span size="-1" color="#000000">I do not
										have a licence. The letter was sent to me as part of TVL's routine mail-out. It
										was not solicited by me. Copies of these letters are commonly reproduced on the
										internet [example]. </span></span></span><span size="-1" color="#000000" face="Arial">You will see that these letters say "Please do not
										write below this line". So did the one sent to me. Please explain to me why I
										am not allowed to write below the line. </span></p><br><span face="Bell MT"><b><span color="#000000" face="Arial">Evidently, my question was too taxing for Ms
								Wright, as the next response came from Ruairi Mcclean, on 3 January
								2007:</span></b></span><p><span size="-1" color="#000000" face="Arial">The reason you would receive such letters is because we would have
										no record of a TV licence at your address. </span><br><span size="-1" color="#000000" face="Arial"></span><br><span size="-1" color="#000000" face="Arial">The reason you cannot write below the line is because the letters
										go through a OCR machine, and anything below the line is rejected.</span></p><br><span face="Bell MT"><b><span color="#000000" face="Arial">OCR is an abbreviation for "optical character
								recognition", software that scans documents for editing on a computer.
								</span></b></span><br><span face="Bell MT"><b><span color="#000000" face="Arial"></span></b></span><br><span face="Bell MT"><b><span color="#000000" face="Arial">This explanation surprised me. I did not
								understand why, having sent me a letter, TVL wanted it back to scan and edit;
								why not scan it before sending it to me? I responded on 27 January
								2007:</span></b></span><p><span size="-1" color="#000000" face="Arial"><span face="Arial"><span size="-1" color="#000000">Dear Mr Mcclean
										</span></span><br><span face="Arial"><span size="-1" color="#000000">Thank you
										for your reply. You say that I cannot write below the line because the letter
										will go through a OCR machine, and anything below the line will be rejected.
										</span></span><br><span face="Arial"><span size="-1" color="#000000"></span></span><br><span face="Arial"><span size="-1" color="#000000">I have two further questions:
										</span></span><br></span><br><span size="-1" color="#000000" face="Arial"><span face="Arial"><span size="-1" color="#000000">i) I take it from your reply that
										a TV officer is planning to collect the letter back off me in order to scan it.
										Please tell me what purpose this serves. </span></span><br></span><br><span size="-1" color="#000000" face="Arial"><span face="Arial"><span size="-1" color="#000000">ii) Anything I write above the line would also be
										rejected by the OCR. Why am I allowed to write above the line, if I am not
										allowed to write in the narrow strip beneath it? </span></span></span></p><br><span face="Bell MT"><b><span color="#000000" face="Arial">On 30 January 2007, I received this reply from Cas
								Scott:</span></b></span> <p><span size="-1" color="#000000" face="Arial">A
										Licensing officer may call at your property not to collect the letters but to
										check that you are not watching a TV. </span><span size="-1" color="#000000" face="Arial">You may write above the line but as we advised you
										previously anything written below the line when they go through the OCR machine
										they will be rejected. </span><span size="-1" color="#000000" face="Arial">If you would like to confirm your address I can up date our
										records to advise no Television is being watched. </span></p><br> 
								<p><span size="-1" color="#000000" face="Arial">Dear Ms Scott</span><br><span size="-1" color="#000000" face="Arial">Thank you for confirming that I may write above the line.
										</span><br><span size="-1" color="#000000" face="Arial"></span><br><span size="-1" color="#000000" face="Arial">Please explain why, having sent me the
										letter, you want it back for scanning. Also, please explain how I am to get the
										letter to you.</span></p><br><span face="Bell MT"><b><span color="#000000" face="Arial">6 February 2007: I have a reply from Gary
								Bessell.</span></b></span> <p><span size="-1" color="#000000" face="Arial">Dear Ms Scott </span><br><span size="-1" color="#000000" face="Arial">Without your address we are unable to amend our records to show
										that you are not using TV equipment. </span><span size="-1" color="#000000" face="Arial">To return an enquiry letter to TV licensing,
										simply return it. </span></p><br><span face="Bell MT"><b><span color="#000000" face="Arial">I don't seem to be getting a straight
								answer.</span></b></span><p><span size="-1" color="#000000" face="Arial">Dear Mr Bessell</span><br><span size="-1" color="#000000" face="Arial">Thank you for your reply. The purpose of my query is not to ask
										why you want my address.</span><span size="-1" color="#000000" face="Arial">The information that I am seeking is why you want the letter back
										for scanning. There was nothing on the letter that said I had to return it.
										</span><span size="-1" color="#000000" face="Arial">Please note that I
										am not Miss Scott.</span></p><br><span face="Bell MT"><b><span color="#000000" face="Arial">9 February 2007: a reply from Karen
								Mcallister.</span></b></span><p><span size="-1" color="#000000" face="Arial">The information about returning the letter was not on the letter
										itself but on the envelope. </span><br><span size="-1" color="#000000" face="Arial"></span><br><span size="-1" color="#000000" face="Arial">The only
										reason we ask you to return the letter is to help us update our records,
										however if you could provide us with your address we can update our records
										without you returning the letter. </span></p><br><span face="Bell MT"><b><span color="#000000" face="Arial">Having kept all my TVL/BBC envelopes, I examined
								them to see whether any displayed an instruction that I was to return the
								letter. None did. There was a return address but only was for undelivered
								letters. I resist the temptation to pursue this point.
								</span></b></span><p><span size="-1" color="#000000" face="Arial">Dear Ms Mcallister</span><br><span size="-1" color="#000000" face="Arial">Thank you for your reply. </span><span size="-1" color="#000000" face="Arial">What I still do not understand is why you would
										wish to OCR my letter in order to update the records. </span><span size="-1" color="#000000" face="Arial">Obviously, the number below the line must be very
										important. Please could you explain its purpose. </span></p><br><span face="Bell MT"><b><span color="#000000" face="Arial">A reply from Carl
								Graves:</span></b></span><p><span size="-1" color="#000000" face="Arial">I
										apologise that it has not been made clear to you. An OCR stands for a Optical
										Character System. This machine enables us to deal/process with large volumes of
										information in a relatively short space of time. The OCR machine reads the
										information below the line and updates the corresponding records on our
										computer system many times faster than if manually processed.
										</span><span size="-1" color="#000000" face="Arial">If the information
										below the line is obscured in anyway the OCR machine will be unable to read the
										information effectively. The number below the line is a unique number that
										relates to the specific property that the letter has been sent to. Once this
										number is read by the OCR machine it will automatically update the computer
										records that relate to that letter/property/licence/application.
										</span><span size="-1" color="#000000" face="Arial">I hope the above
										information is helpful. </span></p><span face="Bell MT"><b><span color="#000000" face="Arial">I am still not satisfied. If I send a letter back
								to TVL/BBC, and they scan the number at the bottom, it will generate the same
								information as they have already got; so, what's the point?
								</span></b></span><span face="Bell MT"><b><span color="#000000" face="Arial">Let's compare TVL/BBC's operation to another company. Below is a
								scan of a pre-paid return postcard from a company called Santander, in which I
								am a shareholder, which I am invited to return. </span></b></span><p><img src="http://www.bbctvlicence.com/Please%20do%20not%20write%20-%20Santander.jpg" width="553" height="421"></p><span face="Bell MT"><b><span color="#000000" face="Arial">Santander requests that I do not write below the line but, unlike
								TVL/BBC, there is an explanation which is on the right hand side. It says that
								they will retrieve my personal data (which I have authorised them to hold,
								unlike TVL/BBC) by scanning the barcode and number. This makes perfect
								sense.</span></b></span><span face="Bell MT"><b><span color="#000000" face="Arial">TVL/BBC letters, however, do not ask for its letters to be
								returned, and Cas Scott has said that the letters are not sought by TVL/BBC
								agents who make street visits. Even if they did collect the letters, the number
								at the bottom would duplicate the information that they already hold. So, the
								original question - why we not permitted to write below the line - remains a
								mystery.</span></b></span><br></div> <br> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. border surveillance towers have always been broken (125 pts)]]></title>
            <link>https://www.eff.org/deeplinks/2024/10/us-border-surveillance-towers-have-always-been-broken</link>
            <guid>41906283</guid>
            <pubDate>Mon, 21 Oct 2024 17:26:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/deeplinks/2024/10/us-border-surveillance-towers-have-always-been-broken">https://www.eff.org/deeplinks/2024/10/us-border-surveillance-towers-have-always-been-broken</a>, See on <a href="https://news.ycombinator.com/item?id=41906283">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article role="article">
  
  
  <div><p><span>A new </span><a href="https://www.nbcnews.com/investigations/30-cameras-border-patrols-main-surveillance-system-are-broken-memo-say-rcna175281"><span>bombshell scoop</span></a><span> from NBC News revealed an internal U.S. Border Patrol memo claiming that 30 percent of camera towers that compose the agency's "Remote Video Surveillance System" (RVSS) program are broken. According to the report, the memo describes "several technical problems" affecting approximately 150 towers.</span></p>
<p><span>Except, this isn't a bombshell. What should actually be shocking is that Congressional leaders are acting shocked, like those who recently sent a </span><a href="https://www.foxnews.com/us/whistleblowers-border-patrol-surveillance-cameras-out-service-gop-demands-answers-dhs"><span>letter</span></a><span> about the towers to Department of Homeland Security (DHS) Secretary Alejandro Mayorkas. These revelations simply reiterate what people who have been watching border technology have known for decades: Surveillance at the U.S.-Mexico border is a wasteful endeavor that is ill-equipped to respond to an ill-defined problem. <br></span></p>
<p><span>Yet, after years of bipartisan recognition that these programs were straight-up boondoggles, there seems to be a competition among political leaders to throw the most money at programs that continue to fail. <br></span></p>
<p><span>Official oversight reports about the failures, repeated breakages, and general ineffectiveness of these camera towers have been public since at least the mid-2000s. So why haven't border security agencies confronted the problem in the last 25 years? One reason is that these cameras are largely political theater; the technology dazzles publicly, then fizzles quietly. Meanwhile, communities that should be thriving at the border are treated like a laboratory for tech companies looking to cash in on often exaggerated—if not fabricated—homeland security threats.</span></p>
<h3><strong>The Acronym Game</strong></h3>
<div><p><a href="https://www.eff.org/deeplinks/2023/03/cbp-expanding-its-surveillance-tower-program-us-mexico-border-and-were-mapping-it"><img src="https://www.eff.org/files/2024/10/18/eff_map_10-18-2024.png" width="1125" height="729" alt="A map of the US-Mexico border with multicolored dots representing surveillance towers." title="EFF is mapping surveillance at the US-Mexico border"></a></p><p>EFF is mapping surveillance at the U.S.-Mexico border</p></div><p><em><i><span>.</span></i></em></p><p><span></span><span>In fact, the history of camera towers at the border is an ugly cycle. First, Border Patrol introduces a surveillance program with a catchy name and big promises. Then a few years later, oversight bodies, including Congress, conclude it's an abject mess. But rather than abandon the program once and for all, border security officials come up with a new name, slap on a fresh coat of paint, and continue on. A few years later, history repeats. </span></p>
<p><span>In the early 2000s, there was the Integrated Surveillance Intelligence System (ISIS), with the installation of RVSS towers in places like Calexico, California and Nogales, Arizona, which was later became the America's Shield Initiative (ASI). After those failures, there was Project 28 (P-28), the first stage of the Secure Border Initiative (SBInet). When that program was canceled, there were various new programs like the Arizona Border Surveillance Technology Plan, which became the Southwest Border Technology Plan. Border Patrol introduced the Integrated Fixed Tower (IFT) program and the RVSS Update program, then the Automated Surveillance Tower (AST) program. And now we've got a whole slew of new acronyms, including the Integrated Surveillance Tower (IST) program and the Consolidated Towers and Surveillance Equipment (CTSE) program. <br></span></p>
<p><span>Feeling overwhelmed by acronyms? Welcome to the shell game of border surveillance. Here's what happens whenever oversight bodies take a closer look. </span></p>
<h3><b>ISIS and ASI</b></h3>
<div><p><img src="https://www.eff.org/files/2024/10/18/calexico_rvss.jpg" width="1920" height="1346" alt="A surveillance tower over a home." title=""></p><p>An RVSS from the early 2000s in Calexico, California.</p></div>
<p><span>Let's start with the Integrated Surveillance Intelligence System (ISIS), a program comprised of towers, sensors and databases originally launched in 1997 by the Immigration and Naturalization Service. A few years later, INS was reorganized into the U.S. Department of Homeland Security (DHS), and ISIS became part of the newly formed Customs &amp; Border Protection (CBP). <br></span></p>
<p><span>It was only a matter of years before the DHS Inspector General </span><a href="https://www.documentcloud.org/documents/23314612-oig_06-15_dec05"><span>concluded that ISIS</span></a><span> was a flop:</span><span> "ISIS remote surveillance technology yielded few apprehensions as a percentage of detection, resulted in needless investigations of legitimate activity, and consumed valuable staff time to perform video analysis or investigate sensor alerts." </span></p>
<p><span>During </span><a href="https://www.documentcloud.org/documents/24483802-chrg-109shrg99863"><span>Senate hearings</span></a><span>, Sen. Judd Gregg (R-NH), complained about a "</span><a href="https://www.documentcloud.org/documents/24483802-chrg-109shrg99863#document/p125"><span>total breakdown in the camera structures</span></a><span>," and that the U.S. government "</span><a href="https://www.documentcloud.org/documents/24483802-chrg-109shrg99863#document/p141"><span>bought cameras that didn't work.</span></a><span>" </span></p>
<p><span>Around 2004, ISIS was folded into the new America's Shield Initiative (ASI), which officials claimed would fix those problems. CBP Commissioner Robert Bonner even <a href="https://www.documentcloud.org/documents/24483802-chrg-109shrg99863#document/p30">promoted</a> ASI as a "critical part of CBP’s strategy to build smarter borders." Yet, less than a year later, Bonner stepped down, and the Government Accountability Office (GAO) found the ASI had <a href="https://www.documentcloud.org/documents/25242401-gao-06-295#document/p2">numerous unresolved issues</a> necessitating a total reevaluation. CBP disputed none of the findings and <a href="https://www.documentcloud.org/documents/25242401-gao-06-295#document/p6">explained</a> it was dismantling ASI in order to move onto something new that would solve everything: the Secure Border Initiative (SBI).<br></span></p>
<p><a href="https://www.documentcloud.org/documents/23825159-chrg-110hhrg44137#document/p10"><span>Reflecting on the ISIS/ASI programs in 2008</span></a><span>, Rep. Mike Rogers (R-MI) said, "What we found was a camera and sensor system that was plagued by mismanagement, operational problems, and financial waste. At that time, we put the Department on notice that mistakes of the past should not be repeated in SBInet."&nbsp;</span></p>
<p><span>You can guess what happened next.&nbsp;</span></p>
<h3><b>P-28 and SBInet</b></h3>
<p><span>The next iteration was called Project 28, which then evolved into the Secure Border Initiative's SBInet, starting in the Arizona desert.&nbsp;</span></p>
<p><span>In 2010, the DHS Chief Information Officer summarized its </span><a href="https://s3.documentcloud.org/documents/24487999/itpa-cbp-sbinet2010.pdf"><span>comprehensive review</span></a><span>: "'Project 28,' the initial prototype for the SBInet system, did not perform as planned. Project 28 was not scalable to meet the mission requirements for a national comment [sic] and control system, and experienced significant technical difficulties."&nbsp;</span></p>
<div><p><img src="https://www.eff.org/files/2024/10/18/sbinet.jpg" width="1000" height="750" alt="A convoluted graphic illustrating how SBInet surveillance towers fit into the border security plan." title=""></p><p>A DHS graphic illustrating the SBInet concept</p></div>
<p><span>Meanwhile, bipartisan consensus had emerged about the failure of the program, due to the technical problems as well as contracting irregularities and cost overruns.</span></p>
<p><span>As Rep. Christopher Carney (D-PA) said in his </span><a href="https://www.documentcloud.org/documents/23825159-chrg-110hhrg44137#document/p9"><span>prepared statement</span></a><span> during Congressional hearings:</span></p>
<blockquote><p><span> P–28 and the larger SBInet program are supposed to be a model of how the Federal Government is leveraging technology to secure our borders, but Project 28, in my mind, has achieved a dubious distinction as a trifecta of bad Government contracting: Poor contract management; poor contractor performance; and a poor final product.</span></p>
</blockquote>
<p><span>Rep. Rogers' remarks were even more cutting: "You know the history of ISIS and what a disaster that was, and we had hoped to take the lessons from that and do better on this and, apparently, we haven’t done much better. "</span></p>
<p><span>Perhaps most damning of all was yet another GAO </span><a href="https://www.documentcloud.org/documents/24488007-ad1179135"><span>report</span></a><span> that found, "SBInet defects have been found, with the number of new defects identified generally increasing faster than the number being fixed—a trend that is not indicative of a system that is maturing."&nbsp;</span></p>
<p><span>In January 2011, DHS Secretary Janet Napolitano </span><a href="https://www.politico.com/story/2011/01/dhs-cancels-virtual-border-fence-047625"><span>canceled</span></a><span> the $3-billion program.</span></p>
<h3><b>IFTs, RVSSs, and ASTs<br></b></h3>
<p><span>Following the termination of SBInet, the </span><i><span>Christian Science Monitor</span></i><span> ran the naive headline, "</span><a href="https://www.csmonitor.com/USA/2011/0115/US-cancels-virtual-fence-along-Mexican-border.-What-s-Plan-B"><span>US cancels 'virtual fence' along Mexican border. What's Plan B</span></a><span>?" Three years later, the newspaper answered its own question with another question, "'</span><a href="https://www.csmonitor.com/USA/2014/0319/Virtual-border-fence-idea-revived.-Another-billion-dollar-boondoggle"><span>Virtual' border fence idea revived. Another 'billion dollar boondoggle'</span></a><span>?" <br></span></p>
<p><span>Boeing was the main contractor blamed for SBINet's failure, but Border Patrol ultimately awarded one of the biggest new contracts to Elbit Systems, which had been one of Boeing's subcontractors on SBInet. Elbit began installing IFTs (again, that stands for "Integrated Fixed Towers") in many of the exact same places slated for SBInet. In some cases, the equipment was simply swapped on an existing SBInet tower.<br></span></p>
<p><span>Meanwhile, another contractor, General Dynamics Information Technology, began installing new RVSS towers and upgrading old ones as part of the RVSS-U program. Border Patrol also started installing hundreds of "Autonomous Surveillance Towers" (ASTs) by yet another vendor, Anduril Industries, embracing the new buzz of artificial intelligence. <br></span></p>
<div><p><img src="https://www.eff.org/files/2024/10/18/ast_and_rvss.jpg" width="1313" height="941" alt="Two surveillance towers and a Border Patrol vehicle along the Rio Grande" title=""></p><p>An Autonomous Surveillance Tower and an RVSS tower along the Rio Grande.</p></div>
<p><span>In 2017, the GAO </span><a href="https://www.documentcloud.org/documents/24733271-gao-18-119#document/p2"><span>complained</span></a><span> the Border Patrol's poor data quality made the agency "limited in its ability to determine the mission benefits of its surveillance technologies." In one case, Border Patrol stations in the Rio Grande Valley claimed IFTs assisted in 500 cases in just six months. The problem with that assertion was there are no IFTs in Texas or, in fact, anywhere outside Arizona. <br></span></p>
<p><span>A few years later, the DHS Inspector General issued yet another </span><a href="https://www.documentcloud.org/documents/23706411-oig-21-21-feb21"><span>report</span></a><span> indicating not much had improved: <br></span></p>
<blockquote><p><span>CBP faced additional challenges that reduced the effectiveness of its existing technology. Border Patrol officials stated they had inadequate personnel to fully leverage surveillance technology or maintain current information technology systems and infrastructure on site. Further, we identified security vulnerabilities on some CBP servers and workstations not in compliance due to disagreement about the timeline for implementing DHS configuration management requirements.</span></p>
<p><span>CBP is not well-equipped to assess its technology effectiveness to respond to these deficiencies. CBP has been aware of this challenge since at least 2017 but lacks a standard process and accurate data to overcome it. <br></span></p>
<p><span>Overall, these deficiencies have limited CBP’s ability to detect and prevent the illegal entry of noncitizens who may pose threats to national security.</span></p>
</blockquote>
<p><span>Around that same time, the RAND Corporation published a </span><a href="https://www.documentcloud.org/documents/24711700-rand_rr4348#document/p60/a2562275"><span>study</span></a><span> funded by DHS that found "strong evidence" the IFT program was having no impact on apprehension levels at the border, and only "weak" and "inconclusive" evidence that the RVSS towers were having any effect on apprehensions.</span></p>
<p><span>And yet, border authorities and their supporters in Congress are continuing to promote unproven, AI-driven technologies as the latest remedy for years of failures, including the ones voiced in the memo obtained by NBC News. These systems involve cameras controlled by algorithms that automatically identify and track objects or people of interest. But in an age when algorithmic errors and bias are being identified nearly everyday in every sector including law enforcement, it is unclear how this technology has earned the trust of the government. <br></span></p>
<h3><b>History Keeps Repeating <br></b></h3>
<p><span>That brings us today, with reportedly 150 or more towers out of service. So why does Washington keep supporting surveillance at the border? Why are they proposing record-level funding for a system that seems irreparable? Why have they abandoned their duty to scrutinize federal programs? <br></span></p>
<p><span>Well, one reason may be that treating problems at the border as humanitarian crises or pursuing foreign policy or immigration reform measures isn't as politically useful as promoting a phantom "invasion" that requires a military-style response. Another reason may be that </span><a href="https://www.eff.org/deeplinks/2024/06/hundreds-tech-companies-want-cash-border-security-funding-heres-who-they-are-and"><span>tech companies and military contractors</span></a><span> wield immense amounts of influence and stand to make millions, if not billions, profiting off border surveillance. The price is paid by taxpayers, but also in the civil liberties of border communities and the human rights of asylum seekers and migrants.</span></p>
<p><span>But perhaps the biggest reason this history keeps repeating itself is that no one is ever really held accountable for wasting potentially billions of dollars on high-tech snake oil.</span></p>

</div>

          </article>
    </div><div>
          <h2>Join EFF Lists</h2>
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An amateur historian has discovered a long-lost short story by Bram Stoker (262 pts)]]></title>
            <link>https://www.bbc.com/news/articles/c4g9119l64qo</link>
            <guid>41905664</guid>
            <pubDate>Mon, 21 Oct 2024 16:14:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/c4g9119l64qo">https://www.bbc.com/news/articles/c4g9119l64qo</a>, See on <a href="https://news.ycombinator.com/item?id=41905664">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="text-block"><p>An amateur historian has discovered a long-lost short story by Bram Stoker, published just seven years before his legendary gothic novel Dracula.<!-- --></p><p>Brian Cleary stumbled upon the 134-year-old ghostly tale while browsing the archives of the National Library of Ireland.<!-- --></p><p>Gibbet Hill was originally published in a Dublin newspaper in 1890 - when the Irishman started working on Dracula - but has been undocumented ever since.<!-- --></p><p>Stoker biographer Paul Murray says the story sheds light on his development as an author and was a significant “station on his route to publishing Dracula”.<!-- --></p></div><div data-component="text-block"><p>The ghostly story tells the tale of a sailor murdered by three criminals whose bodies were strung up on a hanging gallows as a warning to passing travellers.<!-- --></p><p>It is set in Gibbet Hill in Surrey, a location also referenced in Charles Dickens’ 1839 novel Nicholas Nickleby.<!-- --></p><p>Mr Cleary made the discovery after taking time off work following a sudden onset of hearing loss in 2021 - during which period he would pass the time at the national library in <!-- --><a target="_self" href="https://www.bbc.co.uk/news/uk-northern-ireland-51053870">Stoker's native Dublin.<!-- --></a></p><p>In October 2023, the Stoker fan came across an unfamiliar title in an 1890 Christmas supplement of the Daily Express Dublin Edition.<!-- --></p><p>Mr Clearly told the AFP news agency: "I read the words Gibbet Hill and I knew that wasn't a Bram Stoker story that I had ever heard of in any of the biographies or bibliographies."<!-- --></p><p>“And I was just astounded, flabbergasted.<!-- --></p><p>"I sat looking at the screen wondering, am I the only living person who had read it?”<!-- --></p><p>He said of the moment he made the discovery: “What on earth do I do with it?”<!-- --></p><p>The library's director Audrey Whitty said Mr Cleary called her and said: "I’ve found something extraordinary in your newspaper archives - you won’t believe it."<!-- --></p><p>She added that his "astonishing amateur detective work" was a testament to the library's archives.<!-- --></p><p>"There are truly world-important discoveries waiting to be found", she said.<!-- --></p></div><div data-component="text-block"><p>After his initial sleuthing, Mr Cleary contacted biographer Paul Murray - who confirmed there had been no trace of the story for over a century.<!-- --></p><p>He said 1890 was when he was a young writer and made his first notes for Dracula.<!-- --></p><p>"It's a classic Stoker story, the struggle between good and evil, evil which crops up in exotic and unexplained ways," he added.<!-- --></p><p>Gibbet Hill is being published alongside artwork by the Irish artist Paul McKinley by the Rotunda Foundation - the fundraising arm of Dublin's Rotunda Hospital for which Mr Cleary worked.<!-- --></p><p>All proceeds will go to the newly formed Charlotte Stoker Fund - named after Bram Stoker’s mother who was a hearing loss campaigner - to fund research on infant hearing loss.<!-- --></p><p>The discovery is also being highlighted in the city's Bram Stoker festival later this month.<!-- --></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Brain endurance training improves older adults' cognitive, physical performance (105 pts)]]></title>
            <link>https://www.sciencedirect.com/science/article/pii/S1469029224001687</link>
            <guid>41905477</guid>
            <pubDate>Mon, 21 Oct 2024 15:56:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sciencedirect.com/science/article/pii/S1469029224001687">https://www.sciencedirect.com/science/article/pii/S1469029224001687</a>, See on <a href="https://news.ycombinator.com/item?id=41905477">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="root" data-aa-name="root"><header id="gh-cnt"></header><div id="mathjax-container" role="main"><div role="region" aria-label="Download options and search"><ul aria-label="PDF Options"><li><a target="_blank" aria-label="View PDF. Opens in a new window."><svg focusable="false" viewBox="0 0 35 32" height="20"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span><span><span>View&nbsp;<strong>PDF</strong></span></span></span></a></li><li></li></ul></div><div><article lang="en"><div id="publication"><p><a href="https://www.sciencedirect.com/journal/psychology-of-sport-and-exercise" title="Go to Psychology of Sport and Exercise on ScienceDirect"><span><span><img src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/f8a1503dd2b0e0c47a3179274653c0f24b12eee8/image/elsevier-non-solus.png" alt="Elsevier"></span></span></a></p><p><a href="https://www.sciencedirect.com/journal/psychology-of-sport-and-exercise/vol/76/suppl/C"><span><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1469029224X00059-cov150h.gif" alt="Psychology of Sport and Exercise"></span></span></a></p></div><div><p><span>Under a Creative Commons </span><a href="http://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noreferrer noopener"><span><span>license</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></p><p><span></span>open access</p></div><div id="abstracts"><div id="abs0015" lang="en"><h2>Highlights</h2><div id="abssec0030"><ul><li><span>•</span><span><p>Cognitive and physical performance were improved by 6-weeks of training compared to control.</p></span></li><li><span>•</span><span><p>BET improved performance on some cognitive and exercise tasks more than standard exercise training, especially when fatigued.</p></span></li><li><span>•</span><span><p>BET can improve cognitive and performance performance in older adults.</p></span></li><li><span>•</span><span><p>Combined training interventions can tackle aging-related declines in performance.</p></span></li></ul></div></div><div id="abs0010" lang="en"><h2>Abstract</h2><div id="abssec0010"><h3 id="sectitle0015">Objectives</h3><p>Cognitive and physical performance is impaired by aging and fatigue. Cognitive and exercise training may mitigate such impairments. Accordingly, we investigated the effect of Brain Endurance Training (BET) – combined cognitive and exercise training – on cognitive and physical performance when fresh and fatigued in older adults.</p></div><div id="abssec0015"><h3 id="sectitle0020">Design</h3><p>Twenty-four healthy sedentary women (65–78 years) were randomly allocated to one of three training groups: BET, exercise training, and control (no training). The BET and exercise training groups completed the same physical training protocol comprising three 45-min exercise sessions (20-min resistance exercise plus 25-min endurance exercise) per week for eight weeks. The BET group completed a 20-min cognitive task prior to exercise tasks. Cognitive (tasks: psychomotor vigilance, Stroop) and physical (tests: walk, chair-stand, arm curl) performance was tested when fresh and fatigued (before and after a 30-min cognitive task) at weeks 0 (pre-test), 4 (mid-test), 8 (post-test), and 12 (follow-up test).</p></div><div id="abssec0020"><h3 id="sectitle0025">Results</h3><p>Cognitive and physical and performance was generally superior when fresh and fatigued at mid-test and post-test for both BET and exercise training groups compared to the control group. The BET group outperformed the exercise group when fatigued at mid-test and post-test both cognitively (always) and physically (sometimes). The pre-to-post changes in cognitive performance when fresh and fatigued averaged 3.7&nbsp;% and 7.8&nbsp;% for BET, 3.6&nbsp;% and 4.5&nbsp;% for exercise, and −0.4&nbsp;% and 0.3&nbsp;% for control groups. The corresponding changes in physical performance averaged 16.5&nbsp;% and 29.9&nbsp;% for BET, 13.8&nbsp;% and 22.4&nbsp;% for exercise, and 10.8&nbsp;% and 7.1&nbsp;% for control groups.</p></div><div id="abssec0025"><h3 id="sectitle0030">Conclusion</h3><p>These findings show that BET can improve cognitive and physical performance in older adults.</p></div></div></div><ul id="issue-navigation"><li></li><li></li></ul><div id="kwrds0010"><h2>Keywords</h2><p><span>Cognitive training</span></p><p><span>Fatigue</span></p><p><span>Healthy aging</span></p><p><span>Older adults</span></p><p><span>Physical training</span></p></div><section id="da0010"><h2 id="sectitle0045">Data availability</h2><p>Data will be made available on request.</p></section><section aria-label="Cited by" id="section-cited-by"><header id="citing-articles-header"><h2>Cited by (0)</h2></header></section><p><span>© 2024 The Authors. Published by Elsevier Ltd.</span></p></article></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Can SpaceX land a rocket with 1/2 cm accuracy? (129 pts)]]></title>
            <link>https://theshamblog.com/can-spacex-land-a-rocket-with-1-2-cm-accuracy/</link>
            <guid>41905215</guid>
            <pubDate>Mon, 21 Oct 2024 15:27:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theshamblog.com/can-spacex-land-a-rocket-with-1-2-cm-accuracy/">https://theshamblog.com/can-spacex-land-a-rocket-with-1-2-cm-accuracy/</a>, See on <a href="https://news.ycombinator.com/item?id=41905215">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
	
<p><strong>No. But they don’t need to.</strong></p>



<p>In preparation for the <a href="https://www.youtube.com/watch?v=2-PSODNOwJY">5th test flight of Starship</a>, SpaceX announced that they would try to catch the booster using “Mechazilla’s chopsticks.” Later during pre-launch discussions, SpaceX VP Bill Gerstenmaier claimed that they were confident of success since they had landed the booster in the ocean during Flight 4 with “<a href="https://x.com/jeff_foust/status/1844183958873047215" data-type="link" data-id="https://x.com/jeff_foust/status/1844183958873047215">half a centimeter accuracy</a>.” And then last Sunday they went for the landing and nailed it!</p>



<p>But did they really nail it to within half a centimeter? That number sounds too good to be true, and it sparked quite a bit of skepticism from industry observers. What can SpaceX <em>really</em> expect for their landing accuracy?</p>



<p>My background: I am an aerospace engineer who has designed guidance, navigation, &amp; control (GNC) systems for successful <a href="https://www.youtube.com/watch?v=_eCvxPieA84" data-type="link" data-id="https://www.youtube.com/watch?v=_eCvxPieA84">orbital launch vehicles</a>, worked with RTK precision GPS systems on the ground, and implemented GNSS systems on satellite constellations. Standard disclaimer that I am only speaking for myself, and am only using public non-ITAR information. I don’t know what SpaceX is using to estimate the position and orientation on their rockets or their exact control schemes, and I might be missing some information that they’ve made public, but I am well versed in what goes into the design space and can hopefully make some good guesses. Please comment if I’m missing or misinterpreting anything. Also this post isn’t meant as a “gotcha” against Bill – take that as an excuse to dive into the cool engineering behind catching the rocket!</p>



<p>So buckle in for a deep dive and let’s look at what goes into landing a rocket!</p>



<figure><p><iframe title="CLOSEUP! SpaceX Super Heavy Booster Catch" width="1200" height="675" src="https://www.youtube.com/embed/XGC31lmdS6s?start=6&amp;feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p></figure>



<h5><strong>Main Points:</strong></h5>



<ul>
<li>Half a centimeter landing accuracy is not possible, and Bill likely misspoke or was talking about control error.</li>



<li>SpaceX Super Heavy booster landing margins are so wide that you could land one with your smartphone electronics.</li>



<li>Falcon 9 is likely harder to land.</li>



<li>Catching the booster is an absolutely tremendous achievement that the team should be incredibly proud of!</li>
</ul>



<h2>Controlling Position</h2>



<h5><strong>How Accurately Can SpaceX Measure Position?</strong></h5>



<p>The position of a rocket can be measured two primary ways. First, using GNSS (GPS) to get an absolute position. Second, by using an inertial measurement unit (IMU) that includes an accelerometer to estimate the distance from a known reference position (the launch pad). These sensors are both necessary and sufficient for rocket flight, so I’ll focus on them.</p>



<p>The Super Heavy booster lands back in the chopsticks 7 minutes after launch. If we use a <a data-type="link" data-id="https://aerospace.honeywell.com/us/en/products-and-services/product/hardware-and-systems/sensors/inertial-measurement-units" href="https://aerospace.honeywell.com/us/en/products-and-services/product/hardware-and-systems/sensors/inertial-measurement-units">nice expensive IMU</a> that has around 0.01 milli-g’s of accelerometer bias, that double integrates up to 8.6 meters of error. So flying by dead reckoning isn’t going to cut it. (This is a very quick-n’-dirty calculation – a real error propagation analysis would find a larger number due to velocity and attitude errors, so think of this as a lower bound).</p>



<p>We need to bring in GPS to get a better absolute position. Let’s look at the datasheet for <a href="https://novatel.com/products/receivers/gnss-gps-receiver-boards">high-end GNSS chips </a>to get a sense of what’s feasible. Civilian GPS is the L1 band at 250 cm accuracy (looking at the 95% confidence sphere), and military GPS adds the L2 band to 240 cm accuracy, so note that even if SpaceX is using the military band that doesn’t do much on its own in an open-air environment. You could use SBAS (Satellite based augmentation systems, over the US it’s the <a href="https://en.wikipedia.org/wiki/Wide_Area_Augmentation_System" data-type="link" data-id="https://en.wikipedia.org/wiki/Wide_Area_Augmentation_System">WAAS system</a> which airplanes use for landing at airports) which improves accuracy to 120 cm and is available through just the GPS satellite link. Going further than that requires communication between the booster and the ground. At the most precise, an <a href="https://en.wikipedia.org/wiki/Real-time_kinematic_positioning" data-type="link" data-id="https://en.wikipedia.org/wiki/Real-time_kinematic_positioning">RTK positioning system</a> could lower position accuracy all the way down to 2.5 cm (+1cm per km of distance). If SpaceX put a receiver on the launch tower or the ocean buoys, then the landing position could be incredibly accurate. But even the most advance positioning tech won’t guarantee it down to 0.5 cm. And RTK does rely on being able to acquire and maintain a link between the booster and ground for this precision.</p>


<div>
<figure><a href="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-7.png?ssl=1"><img data-recalc-dims="1" fetchpriority="high" decoding="async" width="940" height="367" src="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-7.png?resize=940%2C367&amp;ssl=1" alt="" srcset="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-7.png?resize=940%2C367&amp;ssl=1 940w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-7.png?resize=580%2C226&amp;ssl=1 580w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-7.png?resize=768%2C300&amp;ssl=1 768w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-7.png?w=1453&amp;ssl=1 1453w" sizes="(max-width: 940px) 100vw, 940px"></a></figure></div>


<p><a data-type="link" data-id="https://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/" href="https://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/">Sensor fusion with the accelerometer (aka Kalman filtering)</a> will help fill any gaps in GNSS signals, provide higher rate estimates, and allow for identification &amp; rejection of GNSS errors, but it won’t appreciably improve the absolute position error.</p>



<p>Furthermore, this is just the position of the GPS receiver on the rocket. How does that translate to the position of the landing pins? If there is angular pitch/yaw motion of the rocket, there will be a dynamic offset between the GPS antenna and the landing pins (though this can be calculated and compensated for). There will be manufacturing tolerances which may stack up to less than 5mm, but that would be incredibly tight for something this size. The booster itself will also change dimensions as it is cooled by propellant and heated by reentry. The coefficient of thermal expansion of steel is about 12 μm/m-degC, so for a 71 m booster with liquid oxygen at -183 degC and reentry temperature of (let’s say) 50 degC, that’s a 20 cm change in length. Cut that in half since the LOX tank is half the booster, and you still get 10 cm of elongation. It’s not clear to me whether the booster avionics are near the pins (in which case the local thermal deformation would be minimal) or the engines (in which case it definitely matters), but this shows that thermal effects alone could dwarf half a centimeter accuracy. Additionally, for the Flight 4 landing at sea which quoted the 0.5 cm number, movement of the buoys would be more than this.</p>



<p>SpaceX has said that they use radar altimeters to measure distance to the ground on Falcon 9 landings, which helps constrain the vertical axis error. That would help here as well, though the Super Heavy booster is coming down on an irregular pad rather than a flat landing zone, so a radar return signal would be harder to interpret reliably.</p>



<p>Could you use other real-time distance measurements like laser rangefinding or visual processing? I don’t think so – the surface of the vehicle is too irregular to get a reliable fix point, especially while it is moving, and these are vulnerable to smoke/fog/gas/ambient lighting. Technologies like Ultra Wideband are vulnerable to multipath reflections and attenuation by the booster’s steel walls, and aren’t more accurate than RTK anyway. It is possible that SpaceX has found a proprietary technique for more accurate localization, but at this point we are speculating. And I would point to the SpaceX design philosophy that “the best part is no part” if that extra accuracy isn’t needed. As we’ll see below, it’s not.</p>



<p>So 0.5 cm position accuracy is not possible with applicable technologies. I think the most likely scenario is that Bill misspoke and meant to say “half a meter” or “centimeters level accuracy” and conflated the two.</p>


<div>
<figure><a href="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-10.png?ssl=1"><img data-recalc-dims="1" decoding="async" width="940" height="948" src="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-10.png?resize=940%2C948&amp;ssl=1" alt="" srcset="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-10.png?resize=940%2C948&amp;ssl=1 940w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-10.png?resize=580%2C585&amp;ssl=1 580w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-10.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-10.png?resize=768%2C774&amp;ssl=1 768w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-10.png?w=1192&amp;ssl=1 1192w" sizes="(max-width: 940px) 100vw, 940px"></a><figcaption>Just how small half a centimeter is compared to the landing pins, <a href="https://x.com/mcrs987/status/1844189329889071144" data-type="link" data-id="https://x.com/mcrs987/status/1844189329889071144">from twitter</a></figcaption></figure></div>


<h5><strong>How Accurately Can SpaceX Control Position?</strong></h5>



<p>The algorithms here can get arbitrarily precise. I think &lt; 10 cm accuracy is achievable, and 0.5 cm is impressive but not unbelievable. But this is only control of the vehicle relative to where it <em>thinks</em> it is. I think it’s also possible that this metric is what Bill was talking about, though it’s not the ultimate number that matters for landing.</p>



<p>In preparation for this landing attempt, SpaceX undoubtedly performed extensive <a href="https://github.com/scottshambaugh/monaco/blob/main/README.md" data-type="link" data-id="https://theshamblog.com/introducing-the-monaco-monte-carlo-python-library/">Monte Carlo analyses</a>, simulating the flight of the booster millions of times with different variations in vehicle properties, engine performance, environmental effects such as wind, contingency and off-nominal scenarios such as engine failures, timing errors and signal lag, etc. This would result in realistic landing accuracy numbers. Any estimate of how accurately SpaceX can position the rocket <em>must </em>be downstream of a full analysis like this that incorporates the dynamics of the landing event with all sources of uncertainty and error, and is well outside the scope of this post.</p>



<p>But we can bound the estimates by looking at Falcon 9 landings. Reddit user FortisVeritas <a href="https://www.reddit.com/r/spacex/comments/b3zr0i/chart_with_the_approximate_falcon_9_landing/" data-type="link" data-id="https://www.reddit.com/r/spacex/comments/b3zr0i/chart_with_the_approximate_falcon_9_landing/">collected the locations of Falcon 9 landings</a> and made the plot below. Looking at just the green landing locations on land (in order to remove the extra error from landing on a moving droneship), they tend to land in approximately a 5-10 meter wide area (the large yellow circle on the landing zone is ~20 meters wide).</p>


<div>
<figure><a href="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-9.png?ssl=1"><img data-recalc-dims="1" decoding="async" width="940" height="741" src="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-9.png?resize=940%2C741&amp;ssl=1" alt="" srcset="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-9.png?resize=940%2C741&amp;ssl=1 940w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-9.png?resize=580%2C457&amp;ssl=1 580w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-9.png?resize=768%2C606&amp;ssl=1 768w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-9.png?w=1107&amp;ssl=1 1107w" sizes="(max-width: 940px) 100vw, 940px"></a></figure></div>


<p>However, Falcon 9 has several disadvantages relative to the Super Heavy booster:</p>



<ul>
<li>It does not have separate landing propellant tanks, so propellant slosh will disturb its trajectory. The Super Heavy booster has <a data-type="link" data-id="https://ringwatchers.com/article/booster-prop-distribution" href="https://ringwatchers.com/article/booster-prop-distribution">dedicated central header tanks</a> for landing propellant, so there should be minimal propellant slosh to disturb the vehicle attitude.</li>



<li>It lands with a single engine which cannot throttle low enough to hover the vehicle, and as such must perform a “hoverslam” maneuver to bring the vehicle to a stop right on the ground. While the Super Heavy booster must perform most of a hoverslam maneuver to slow down just before coming in to the tower, it can hover for the final fine positioning.</li>



<li>Because it lands with a single engine, roll control is minimal close to touchdown when the airspeed is low and the grid fins can impart minimal torque, and is limited to its weaker cold-gas thrusters. The Super Heavy booster can control roll with its 3 engines all the way to the ground.</li>



<li>Falcon 9 has no engine-out capability for landing. SpaceX has not confirmed it for the Super Heavy booster, but I believe one engine out is likely possible (more on this later).</li>



<li>It is smaller with a lower moment of inertia. Rockets get more stable and easier to control the larger they are, much like it’s easier to balance a broom on your finger than a pencil.</li>



<li>It is smaller, and so thanks to the <a href="https://en.wikipedia.org/wiki/Square%E2%80%93cube_law#:~:text=The%20square%E2%80%93cube%20law%20can,the%20cube%20of%20the%20multiplier." data-type="link" data-id="https://en.wikipedia.org/wiki/Square%E2%80%93cube_law#:~:text=The%20square%E2%80%93cube%20law%20can,the%20cube%20of%20the%20multiplier.">cubed-square law</a> has a higher area:mass ratio. This means that it will be more affected by wind gusts that might blow it off course.</li>
</ul>



<p>All this to say that the Super Heavy booster should be easier to control precisely than Falcon 9, and its landings more accurate than a ±2.5 meter range.</p>



<h5><strong>How Accurate Does SpaceX Need Position To Be?</strong></h5>



<p>Pulling from <a href="https://www.youtube.com/watch?v=ub6HdADut50" data-type="link" data-id="https://www.youtube.com/watch?v=ub6HdADut50">Ryan Hansen’s excellent video</a> where he modeled and simulated the Super Heavy catch system, the below arc is the feasible catch zone for the booster. This area is 22 m at its narrowest point, and needs to fit a 9 m booster in it, so this allows for ± 6.5m of side-to-side error. The catch arm area is 18 m long, so call that ± 9 m of front-to-back accuracy required.</p>


<div>
<figure><a href="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-8.png?ssl=1"><img data-recalc-dims="1" loading="lazy" decoding="async" width="940" height="526" src="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-8.png?resize=940%2C526&amp;ssl=1" alt="" srcset="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-8.png?resize=940%2C526&amp;ssl=1 940w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-8.png?resize=580%2C324&amp;ssl=1 580w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-8.png?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-8.png?w=1253&amp;ssl=1 1253w" sizes="(max-width: 940px) 100vw, 940px"></a></figure></div>


<p>This side-to-side error range does assume that the catch arms can still close around the booster if it is off-center – can the tower can adjust the arm positions in real time so it doesn’t knock into one side of the rocket first and push it over? In the video below it certainly looks like this is the case. The left arm moves first and we see them come in at different speeds to keep centered around the booster in real time. They start moving before the rocket is between the arms, so I don’t expect this is being controlled by tower sensors such as radars mounted on the arms.</p>



<figure><p><iframe title="TOWER CAM! SpaceX Super Heavy Booster Catch" width="1200" height="675" src="https://www.youtube.com/embed/JlcrNakUGVs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p></figure>



<p>Vertical distance is likely to be the most constrained. If the engines shut off several meters above the arms, the booster would hit with quite a bit of force and in the worst case may bounce back off. There are pistons on the catch rails that could allow for damping out the impact from a dropped booster, but you still want to avoid this. The way to mitigate this is to center the booster in the right spot and then drop down slowly at a speed which you know is manageable for a successful catch. This is what we see SpaceX do. The limiter here becomes fuel – dropping at a steady speed burns as much as hovering. If you are dropping at 1 m/s, can you do that for 5 seconds? It’s not clear how much fuel margin SpaceX has here. The engines turns off immediately after contact, but that may have been because contact was detected, and SpaceX had more fuel margin to go. With a lot of uncertainty here, let’s ballpark a 5 meter tall box for a range of ±2.5m.</p>


<div>
<figure><a href="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-3.png?ssl=1"><img data-recalc-dims="1" loading="lazy" decoding="async" width="940" height="527" src="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-3.png?resize=940%2C527&amp;ssl=1" alt="" srcset="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-3.png?resize=940%2C527&amp;ssl=1 940w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-3.png?resize=580%2C325&amp;ssl=1 580w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-3.png?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-3.png?resize=1536%2C861&amp;ssl=1 1536w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-3.png?resize=2048%2C1148&amp;ssl=1 2048w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-3.png?resize=800%2C450&amp;ssl=1 800w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-3.png?w=2400&amp;ssl=1 2400w" sizes="(max-width: 940px) 100vw, 940px"></a></figure></div>


<h5><strong>Position Summary</strong></h5>



<p>With standard consumer grade GPS, SpaceX can localize its rocket to within 2.5 meters. Using SBAS which would require no extra complexity, that shrinks to 1.2 meters. To get more accurate than that, they would need to have a communications link between the rocket and a base station on the pad. Using RTK for this they could get as accurate as 2.5 cm, but this adds complexity to the system. On top of this will be several centimeters of hardware offsets due to manufacturing tolerances and thermal effect. On the other hand, the allowable error box is roughly ± 6.5, 9 and 2.5 (?) meters large, for safety margins of 2-8x. This is a little low for a test flight – if I were SpaceX I would try to use RTK or DGPS to make the margins larger, but would also feel good that the simpler system could work as backup. This may also suggest that radar is still being used for vertical accuracy.</p>



<p>Going off of Falcon 9 landing history, that rocket consistently lands within a ±2.5 – 5 meter area. However I would expect the Super Heavy booster to be easier to control for a number of reasons.</p>



<p>One reason that the landing looks so precise is simply that everything is so big, and GPS accuracy doesn’t change with scale. We aren’t surprised when a consumer drone flies back to us autonomously using GPS and lands on the grass at our feet, and this rocket is using the same underlying technology. It’s just that 1 meter error looks really small when your booster is 71 meters tall.</p>



<h2>Controlling Orientation / Attitude</h2>



<h5><strong>How Accurately Can SpaceX Measure Attitude?</strong></h5>



<p>Without a device that measures absolute attitude such as a star tracker, the angular orientation of a rocket will be reliant on measuring angular velocity with its onboard gryoscopes, which is then integrated up into angular position relative to the launch orientation. </p>



<p>The Super Heavy booster landing back in the chopsticks 7 minutes after launch is short enough that you don’t need a very good gyro at all to stay pointed well. Without digging too deep into the details of Allan Deviance, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10490716/">the MEMS gryo</a> in an iPhone XR has an angular bias of 27 deg/hr, and s Pixel 7 Pro is 10x more accurate at 1.9 deg/hr. Over 7 minutes, that is an error of 3.2 deg and 0.2 deg respectively. And that’s just using the cheap gyro in your phone! SpaceX is probably using <a href="https://aerospace.honeywell.com/us/en/products-and-services/product/hardware-and-systems/sensors/inertial-measurement-units">a nicer “tactical grade” MEMS gyro</a> with 10x more stability at ~0.25 deg/hr (0.03 deg over 7 minutes), or all the way to “navigation grade” ring laser gyros, which can be 100x more accurate that that. This analysis is very rough, and linear error accumulation ignores other sources of error such as scale factors, bias instability, temperature sensitivity, and G-loading sensitivity. But it also ignores fancier sensor fusion techniques you can use by combining multiple sensors, as well as calibration techniques. So it’s a good order-of-magnitude look.</p>



<p>Perhaps surprisingly, I expect that this error will be dominated by the measurement error of the initial orientation. Ensuring that the rocket is perfectly vertical can be a surprisingly tricky problem. You can measure exterior markers using laser range finding, and you can measure the local gravity vector using accelerometer measurements. But how do you account for mechanical strain under propellant load and thermal contraction which might change the orientation of the IMU during fill operations, and throughout flight? How do you account for mechanical machining tolerances where surfaces are not manufactured perfectly flat and change the static mounting orientation of the IMU relative to other components? Will that thermal deformation be symmetric, or tend to tilt your IMU one way or another? These effects and more can be measured and analyzed, but never perfectly compensated for, especially with very little flight history. I do still expect that the error here will be well under 1 deg.</p>



<figure><p><iframe title="ONBOARD CAM! SpaceX Super Heavy Booster Catch" width="1200" height="675" src="https://www.youtube.com/embed/ExV6PHRM8eI?start=15&amp;feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p></figure>



<h5><strong>How Accurately Can SpaceX Control Attitude?</strong></h5>



<p>This can also get arbitrarily precise. Based on eyeballing their Falcon 9 landings, I would guess that SpaceX can definitely control landing attitude to well under 1 degree of error in each of its pitch/yaw/roll axes. The Super Heavy trajectory divert just before landing from a safe impact location to the chopsticks is a much more dynamic maneuver than Falcon 9’s landing, but for much the same reasons as its better position accuracy I would expect better attitude control on the Super Heavy booster.</p>



<p>But again, a full accounting here must be downstream of flight simulations which take into account dynamics and uncertainties.</p>



<h5>How Accurate Does SpaceX Need Attitude To Be?</h5>



<p><strong>Roll</strong>: Pulling from <a href="https://www.youtube.com/watch?v=ub6HdADut50" data-type="link" data-id="https://www.youtube.com/watch?v=ub6HdADut50">Ryan Hansen’s video</a> again, he estimates that there is ±9 deg of baseline roll tolerance to safely catch the pins on the catch rails, and this expands to ±15 deg with moderate compression of the foam padding that hugs the booster core.</p>


<div>
<figure><a href="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/Screenshot-2024-10-13-151637-1.png?ssl=1"><img data-recalc-dims="1" loading="lazy" decoding="async" width="940" height="529" src="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/Screenshot-2024-10-13-151637-1.png?resize=940%2C529&amp;ssl=1" alt="" srcset="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/Screenshot-2024-10-13-151637-1.png?resize=940%2C529&amp;ssl=1 940w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/Screenshot-2024-10-13-151637-1.png?resize=580%2C326&amp;ssl=1 580w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/Screenshot-2024-10-13-151637-1.png?resize=768%2C432&amp;ssl=1 768w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/Screenshot-2024-10-13-151637-1.png?resize=1536%2C864&amp;ssl=1 1536w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/Screenshot-2024-10-13-151637-1.png?resize=2048%2C1152&amp;ssl=1 2048w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/Screenshot-2024-10-13-151637-1.png?resize=800%2C450&amp;ssl=1 800w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/Screenshot-2024-10-13-151637-1.png?w=2400&amp;ssl=1 2400w" sizes="(max-width: 940px) 100vw, 940px"></a></figure></div>


<p><strong>Pitch</strong>: The head of the pin has a ball joint that connects it to its support structure, and allows it to pivot a bit to accommodate angular error in pitch and yaw. Pitch is the direction of travel as the booster flies in towards the launch mounts and the most dynamic. But it appears that the booster flies to a point a bit above the launch pins, and then lowers itself vertically down. So the dynamic movements should largely be done prior to catch. Any error in the pitch direction would potentially cause the booster to “swing” back and forth towards the tower once it landed, but I don’t see anything inherently wrong with this from a structural perspective. From measuring the booster on the chopsticks, there is likely at least 15 degrees of swing towards the tower that could be accommodated without hitting it.</p>


<div>
<figure><a href="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-1.png?ssl=1"><img data-recalc-dims="1" loading="lazy" decoding="async" width="940" height="528" src="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-1.png?resize=940%2C528&amp;ssl=1" alt="" srcset="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-1.png?resize=940%2C528&amp;ssl=1 940w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-1.png?resize=580%2C326&amp;ssl=1 580w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-1.png?resize=768%2C431&amp;ssl=1 768w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-1.png?resize=1536%2C863&amp;ssl=1 1536w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-1.png?resize=2048%2C1150&amp;ssl=1 2048w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-1.png?resize=800%2C450&amp;ssl=1 800w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-1.png?w=2400&amp;ssl=1 2400w" sizes="(max-width: 940px) 100vw, 940px"></a></figure></div>

<div>
<figure><a href="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-2.png?ssl=1"><img data-recalc-dims="1" loading="lazy" decoding="async" width="530" height="593" src="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-2.png?resize=530%2C593&amp;ssl=1" alt=""></a></figure></div>


<p><strong>Yaw</strong>: I think this is likely the angle with the tightest tolerance. Over a roughly 10 meter pin-to-pin distance, every 1 degree of yaw error will result in a 17 cm vertical offset with one pin hitting a catch arm before the other. The catch rails are mounted on pistons which can move vertically by about 85 cm. These could be meant for shock absorption, or they could be meant to differentially lower during a landing event with high yaw. With 85 cm of travel distance, these could offset exactly 5 deg of yaw error – a suspiciously nice number if I put my design requirements hat on!</p>



<p>During the first catch attempt the booster was almost perfectly vertical, and we saw hardly any compression of these rails on first touch. They lowered evenly about half a second after the booster had landed and settled on the rail. So there doesn’t seem to be any “baseline” shock absorption happening. Either this is because this was a clean landing and the shock absorption was not needed, or these are primarily for yaw error compensation. If you didn’t compensate for yaw, then you would impart large twisting forces on the catch arm structure with one pin hitting before the other. This can certainly be designed for, but this adds a lot of further mass needed for structural rigidity and my guess is that the SpaceX engineers would much prefer that the arms be evenly loaded.</p>



<p>The other factor helping SpaceX mitigate yaw errors is that the two arms squeezing the booster will tend to push it towards vertical in that axis. You can somewhat see this in the landing video.</p>



<h5>Attitude / Orientation Summary</h5>



<p>I expect that the Super Heavy booster has attitude knowledge and attitude control both well under 1 degree in all axes. I believe the catch requirements are roughly ± 10 deg in roll, ± 15 deg in pitch, and ± 5 deg in yaw. So under a nominal landing without any hardware failures, SpaceX will have a factor of at least 10x performance margin to requirements. </p>



<p>This margin is large enough that I expect attitude on landing is robust to failure of 1 of the 3 landing engines. With one engine out, the other two would only need to gimbal (very roughly) 2 deg to point through the center of mass, and the booster would come in tilted by that angle in pitch / yaw. But this can be handled with still plenty of angular margin. Whether only two engines have enough thrust to set the rocket down softly is another question.</p>


<div>
<figure><a href="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-6.png?ssl=1"><img data-recalc-dims="1" loading="lazy" decoding="async" width="626" height="732" src="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-6.png?resize=626%2C732&amp;ssl=1" alt="" srcset="https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-6.png?w=626&amp;ssl=1 626w, https://i0.wp.com/theshamblog.com/wp-content/uploads/2024/10/image-6.png?resize=580%2C678&amp;ssl=1 580w" sizes="(max-width: 626px) 100vw, 626px"></a></figure></div>


<h2>Controlling Velocity &amp; Angular Rate</h2>



<h5>Velocity Accuracy</h5>



<p>GPS is much more accurate at measuring velocity than position. Consumer GPS can be accurate to within 1-2 cm/s by measuring the doppler shift of the GPS signal frequency. Perhaps surprisingly, this is about as good as it gets – even going to RTK doesn’t appreciably improve the accuracy of velocity measurements. During a rocket landing, the vibrations and heavy accelerations will likely degrade this a bit. Using the same IMU as above with 0.01 milli-g acceleration drift over 420 seconds, the dead reckoning integrates up to an error of 4.1 cm/s. The numbers here are close enough that likely neither sensor dominates, and a fused approach is used. For example, if the booster can get a GPS reading of velocity at 1 cm/s just before landing burn ignition, the IMU alone could keep the error at about that level for the final 20 seconds of flight.</p>



<h5>Angular Rate Accuracy</h5>



<p>If we dig into the spec sheet for a cheap consumer IMU like the <a data-type="link" data-id="https://invensense.tdk.com/products/motion-tracking/6-axis/icm-42688-p/" href="https://invensense.tdk.com/products/motion-tracking/6-axis/icm-42688-p/">ICM-42688-P</a>, we see ±15.625 deg/s gyro accuracy at 16 bits of precision, for a resolution of 0.0005 deg/s. Scale factors, gryo bias, and temperature effects will worsen the true accuracy, but these can be largely corrected for. Vehicle modes and heavy vibrations will also inject error into the signal, but this can be filtered out. The point is that this is already 100x the accuracy you might need. Since the gyro measures angular rate directly, the error here is bounded and does not integrate up higher. SpaceX will also undoubtedly be using a nicer gyroscope than this.</p>



<h5>Capability &amp; Requirements</h5>



<p>It’s unclear how accurately SpaceX needs to control velocity and angular rate. This will be driven by the strength of the tower and catch arms to accommodate the force of stopping a moving rocket flying into it. True capability here is also hard to estimate since it is such an incredibly dynamic event. Falcon 9 shows us that SpaceX is very good at performing the hoverslam maneuver for a soft touchdown on a hard surface, and for the same reasons as before the Super Heavy booster should be easier to control. But simulations are again needed to answer this question rigorously, and no one outside SpaceX has all the data needed to do those.</p>



<h2>So What’s the Hard Part?</h2>



<p>SpaceX isn’t using magic to control their rockets. While the size of the booster gives the impression of impossible GNC precision needed to land back in the chopstick arms, it should not come as a surprise that SpaceX actually developed a system with large safety margins using sound engineering principles. They would not have attempted a catch landing without confidence that it could work.</p>



<p>But there’s a large difference between something that could work in theory, and actually work in reality. Spaceflight is such a difficult field of engineering because there is such low margin for failure – a rocket is made up of millions of individual parts, most of which have zero redundancy. The margins in landing position and orientation I look at here only matter if you can get back to the launch pad in the first place. Countless little things could go wrong, and any one of them will end with your rocket blowing up. Gravity is a relentless opponent to fight.</p>



<p>So what do I personally find most impressive about this launch?</p>



<ul>
<li>The real-time solver that generates new reference trajectories to land the booster under hard fuel constraints (SpaceX has a lot of experience with this for Falcon 9, but they are still the only ones who can do this and I remain impressed every time – shoutout to <a href="http://www.larsblackmore.com/index.htm">Lars Blackmore</a>).</li>



<li>The real-time link that seems to exist between the tower and the vehicle during the catch event, which is a new system and would be hard to test.</li>



<li>The speed at which SpaceX is iterating, building, and testing Starship, which blows the rest of the industry out of the water.</li>



<li>The sheer guts it took to risk blowing up the pad during a catch attempt after only one booster touchdown test, in a maneuver that no one has ever done before.</li>



<li>The fact that everything worked first try! There were no unknown unknowns that derailed the catch, no mismatches in configuration tracking with the upgrades rolling into every single vehicle, no engineering assumptions that were too loose, no tricky differences between test and flight, no machining/build errors that would break parts.</li>
</ul>



<p>SpaceX has demonstrated once again that they are performing at the pinnacle of engineering and operational excellence. It seems to me that the question is no longer <em>if </em>Starship will be successful in revolutionizing access to space by slashing launch costs, but <em>when</em>. Hats off to the team for reaching this milestone in such a spectacular fashion, and for sharing the journey so publicly. The videos never get old.</p>



<figure><p><iframe title="SpaceX Starship Flight 5 | Launch &amp; Booster Catch Tracking" width="1200" height="675" src="https://www.youtube.com/embed/jFpyZHueLWY?start=422&amp;feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p></figure>





</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Intelsat 33e breaks up in geostationary orbit (154 pts)]]></title>
            <link>https://spacenews.com/intelsat-33e-loses-power-in-geostationary-orbit/</link>
            <guid>41904346</guid>
            <pubDate>Mon, 21 Oct 2024 14:09:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spacenews.com/intelsat-33e-loses-power-in-geostationary-orbit/">https://spacenews.com/intelsat-33e-loses-power-in-geostationary-orbit/</a>, See on <a href="https://news.ycombinator.com/item?id=41904346">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="content">
		<main id="main">
													

									<div>

				
				
			<figure>

				<img width="879" height="485" src="https://i0.wp.com/spacenews.com/wp-content/uploads/2016/04/Epic-satellite-Boeing.jpg?fit=879%2C485&amp;ssl=1" alt="" data-hero-candidate="1" fetchpriority="high" decoding="async" srcset="https://i0.wp.com/spacenews.com/wp-content/uploads/2016/04/Epic-satellite-Boeing.jpg?w=879&amp;ssl=1 879w, https://i0.wp.com/spacenews.com/wp-content/uploads/2016/04/Epic-satellite-Boeing.jpg?resize=300%2C166&amp;ssl=1 300w, https://i0.wp.com/spacenews.com/wp-content/uploads/2016/04/Epic-satellite-Boeing.jpg?resize=768%2C424&amp;ssl=1 768w, https://i0.wp.com/spacenews.com/wp-content/uploads/2016/04/Epic-satellite-Boeing.jpg?resize=400%2C221&amp;ssl=1 400w, https://i0.wp.com/spacenews.com/wp-content/uploads/2016/04/Epic-satellite-Boeing.jpg?fit=879%2C485&amp;ssl=1&amp;w=370 370w" sizes="(max-width: 879px) 100vw, 879px">			<figcaption><span>An artist's concept of a Boeing-built Intelsat EpicNG satellite.  <span><span>Credit:</span> Boeing</span></span></figcaption>
			
			</figure><!-- .post-thumbnail -->

		

<article id="post-507397">
	<div>

		
		<p>TAMPA, Fla. —&nbsp;The Intelsat 33e satellite has broken up in geostationary orbit (GEO) and lost power, ceasing communications services for customers across Europe, Africa and parts of Asia Pacific.

</p><p>Intelsat said in an Oct. 19 news release it is working with satellite maker Boeing to address an anomaly that emerged earlier that day, but “believe it is unlikely that the satellite will be recoverable.” An Intelsat spokesperson said the satellite was not insured at the time of the issue.

</p><p>The U.S. Space Force reported Oct. 19 it is tracking around 20 pieces of debris associated with the spacecraft.

</p><p>“U.S. Space Forces-Space (S4S) has confirmed the breakup of Intelsat 33E (#41748, 2016-053B) in GEO on October 19, 2024, at approximately 0430 UTC,” <a href="https://www.space-track.org/">states an alert posted on SpaceTrack</a>, the U.S. Department of Defense’s space-tracking platform.&nbsp;

</p><p>“Currently tracking around 20 associated pieces – analysis ongoing. S4S has observed no immediate threats and is continuing to conduct routine conjunction assessments to support the safety and sustainability of the space domain.”

</p><p>Douglas Hendrix, CEO of ExoAnalytic Solutions, said the U.S.-based space-tracking company identified 57 pieces of debris Oct. 21 associated with the breakup.

</p>

<p>“We are warning operators of any spacecraft that we think are at risk of collision,” Hendrix said via email.

</p><figure><img data-recalc-dims="1" decoding="async" width="780" height="585" src="https://i0.wp.com/spacenews.com/wp-content/uploads/2024/10/1729433737373-1.jpeg?resize=780%2C585&amp;ssl=1" alt="" srcset="https://i0.wp.com/spacenews.com/wp-content/uploads/2024/10/1729433737373-1.jpeg?resize=1024%2C768&amp;ssl=1 1024w, https://i0.wp.com/spacenews.com/wp-content/uploads/2024/10/1729433737373-1.jpeg?resize=300%2C225&amp;ssl=1 300w, https://i0.wp.com/spacenews.com/wp-content/uploads/2024/10/1729433737373-1.jpeg?resize=768%2C576&amp;ssl=1 768w, https://i0.wp.com/spacenews.com/wp-content/uploads/2024/10/1729433737373-1.jpeg?resize=1536%2C1152&amp;ssl=1 1536w, https://i0.wp.com/spacenews.com/wp-content/uploads/2024/10/1729433737373-1.jpeg?resize=1200%2C900&amp;ssl=1 1200w, https://i0.wp.com/spacenews.com/wp-content/uploads/2024/10/1729433737373-1.jpeg?resize=800%2C600&amp;ssl=1 800w, https://i0.wp.com/spacenews.com/wp-content/uploads/2024/10/1729433737373-1.jpeg?resize=600%2C450&amp;ssl=1 600w, https://i0.wp.com/spacenews.com/wp-content/uploads/2024/10/1729433737373-1.jpeg?resize=400%2C300&amp;ssl=1 400w, https://i0.wp.com/spacenews.com/wp-content/uploads/2024/10/1729433737373-1.jpeg?resize=200%2C150&amp;ssl=1 200w, https://i0.wp.com/spacenews.com/wp-content/uploads/2024/10/1729433737373-1.jpeg?resize=1568%2C1176&amp;ssl=1 1568w, https://i0.wp.com/spacenews.com/wp-content/uploads/2024/10/1729433737373-1.jpeg?resize=706%2C530&amp;ssl=1 706w, https://i0.wp.com/spacenews.com/wp-content/uploads/2024/10/1729433737373-1.jpeg?w=2048&amp;ssl=1 2048w, https://i0.wp.com/spacenews.com/wp-content/uploads/2024/10/1729433737373-1-1024x768.jpeg?w=370&amp;ssl=1 370w" sizes="(max-width: 780px) 100vw, 780px"><figcaption><br>A snapshot of Intelsat 33e’s break-up taken Oct. 19 by U.K.-based Spaceflux. 44071 and 58698 are the WGS 10 (USA 291) and Ovzon-3 satellites, respectively, which Spaceflux said are unlikely in danger of being hit by the debris. “The problem is that there is a lot of uncertainty regarding the orbits of these fragments at the moment,” Spaceflux spokesperson Viktoria Urban said Oct. 21. “They can be potentially dangerous for other satellites but we do not know that yet.” Credit: Spaceflux.</figcaption></figure><p>Intelsat said it is working to move customers to other satellites in Intelsat’s fleet or spacecraft operated by third parties.

</p><p>Intelsat 33e launched in August 2016 and entered service in January 2017 at 60 degrees East, about three months later than planned following an issue with its primary thruster.

</p><p>A second propulsion issue that emerged during in-orbit tests helped <a href="https://spacenews.com/intelsat-33e-propulsion-problems-to-cut-service-life-by-3-5-years/">knock off around 3.5 years</a> from the satellite’s initially estimated 15-year lifespan.

</p><p>Intelsat 33e is the second in Intelsat’s EpicNG (next-generation) series of high-throughput satellites.

</p><p>The first, Intelsat-29e, was <a href="https://spacenews.com/intelsat-29e-declared-a-total-loss/">declared a total loss in 2019</a> after just three years in orbit. That failure was pinned on either a meteoroid impact or a wiring flaw that led to an electrostatic discharge following heightened solar weather activity.

</p><p><em>This article was updated Oct. 21 with more details about the incident.</em>


</p>	</div><!-- .entry-content -->

	<!-- .entry-footer -->

	<div>
									<p><a href="https://spacenews.com/author/jason-rainbow/" rel="author">
						<img width="80" height="80" src="https://spacenews.com/wp-content/uploads/2023/01/Jason-Rainbow-150x150.jpg" alt="" srcset="https://i0.wp.com/spacenews.com/wp-content/uploads/2023/01/Jason-Rainbow.jpg?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/spacenews.com/wp-content/uploads/2023/01/Jason-Rainbow.jpg?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/spacenews.com/wp-content/uploads/2023/01/Jason-Rainbow.jpg?w=400&amp;ssl=1 400w, https://i0.wp.com/spacenews.com/wp-content/uploads/2023/01/Jason-Rainbow.jpg?resize=200%2C200&amp;ssl=1 200w, https://i0.wp.com/spacenews.com/wp-content/uploads/2023/01/Jason-Rainbow-150x150.jpg?w=370&amp;ssl=1 370w">					</a></p><div>
					<!-- .author-bio-header -->

											<p>
							Jason Rainbow writes about satellite telecom, space finance and commercial markets for SpaceNews. He has spent more than a decade covering the global space industry as a business journalist. Previously, he was Group Editor-in-Chief for Finance Information...							<a href="https://spacenews.com/author/jason-rainbow/" rel="author">
							More by Jason Rainbow							</a>
						</p>
					
				</div><!-- .author-bio-text -->

			</div><!-- .author-bios -->

</article><!-- #post-${ID} -->



				</div><!-- .main-content -->

			
<!-- #secondary -->

		</main><!-- #main -->
	</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Epublifier – scrape pages (books, manuals) for offline reading (271 pts)]]></title>
            <link>https://github.com/maoserr/epublifier</link>
            <guid>41903864</guid>
            <pubDate>Mon, 21 Oct 2024 13:18:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/maoserr/epublifier">https://github.com/maoserr/epublifier</a>, See on <a href="https://news.ycombinator.com/item?id=41903864">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Epublifier</h2><a id="user-content-epublifier" aria-label="Permalink: Epublifier" href="#epublifier"></a></p>
<p dir="auto">Converts websites into epub.</p>
<p dir="auto">A tool that allows you to extract a list of html pages from a website and compile them into an ePub book to be imported into your eReader of choice.</p>
<p dir="auto">For advanced users who can write javascript, you can add additional parser definition to customize parsing of any site.</p>
<p dir="auto">Check out the <a href="https://github.com/maoserr/epublifier/wiki">wiki</a> for usage.</p>
<p dir="auto">Currently supporting following sites:</p>
<ol dir="auto">
<li>Novel Update</li>
<li>Wuxia World</li>
<li>Most sites from <a href="https://github.com/readthedocs-examples/awesome-read-the-docs">awesome-read-the-docs</a></li>
<li>Custom sites with UL/OL elements as table of content, or regex on Link text, or use query selector</li>
<li>Custom web app with predefined Title (header) element and Next button (clickable)</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<ul dir="auto">
<li>Firefox: <a href="https://addons.mozilla.org/en-US/firefox/addon/epublifier/" rel="nofollow">https://addons.mozilla.org/en-US/firefox/addon/epublifier/</a></li>
<li>Chrome: <a href="https://chrome.google.com/webstore/detail/epublifier/eopjnahefjhnhfanplcjpbbdkpbagikk" rel="nofollow">https://chrome.google.com/webstore/detail/epublifier/eopjnahefjhnhfanplcjpbbdkpbagikk</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Example Usage</h2><a id="user-content-example-usage" aria-label="Permalink: Example Usage" href="#example-usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Extracting list of pages</h3><a id="user-content-extracting-list-of-pages" aria-label="Permalink: Extracting list of pages" href="#extracting-list-of-pages"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/maoserr/epublifier/blob/develop/docs/nu.gif?raw=true"><img src="https://github.com/maoserr/epublifier/raw/develop/docs/nu.gif?raw=true" alt="Novel Update" title="List of Pages" data-animated-image=""></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Tranversing Webapp through next button</h3><a id="user-content-tranversing-webapp-through-next-button" aria-label="Permalink: Tranversing Webapp through next button" href="#tranversing-webapp-through-next-button"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/maoserr/epublifier/blob/develop/docs/wuxiaworld.gif?raw=true"><img src="https://github.com/maoserr/epublifier/raw/develop/docs/wuxiaworld.gif?raw=true" alt="Wuxia World" title="Next button" data-animated-image=""></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Extracting other documentation</h3><a id="user-content-extracting-other-documentation" aria-label="Permalink: Extracting other documentation" href="#extracting-other-documentation"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/maoserr/epublifier/blob/develop/docs/fastapi.gif?raw=true"><img src="https://github.com/maoserr/epublifier/raw/develop/docs/fastapi.gif?raw=true" alt="FastAPI" title="Documentations" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building</h2><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto">Build Environment</p>
<ul dir="auto">
<li>Windows 10</li>
<li>NPM version 8.1.2</li>
</ul>
<p dir="auto">Build Steps</p>
<ul dir="auto">
<li>Install NPM</li>
<li>Run <code>npm install</code> in base directory</li>
<li>Run <code>npm run build_ff</code> for Firefox</li>
<li>Run <code>npm run build</code> for Chrome</li>
</ul>
<p dir="auto">CI/CD</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<ol dir="auto">
<li><a href="https://lelinhtinh.github.io/jEpub/" rel="nofollow">jEpub</a></li>
</ol>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Egypt declared malaria-free after 100-year effort (560 pts)]]></title>
            <link>https://www.bbc.com/news/articles/cm2yl8pjgn2o</link>
            <guid>41903616</guid>
            <pubDate>Mon, 21 Oct 2024 12:51:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/cm2yl8pjgn2o">https://www.bbc.com/news/articles/cm2yl8pjgn2o</a>, See on <a href="https://news.ycombinator.com/item?id=41903616">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="text-block"><p>Egypt has been certified malaria-free by the World Health Organization (WHO) - an achievement hailed by the UN public health agency as "truly historic".<!-- --></p><p>“Malaria is as old as Egyptian civilization itself, but the disease that plagued pharaohs now belongs to its history," said WHO chief Tedros Adhanom Ghebreyesus.<!-- --></p><p>Egyptian authorities launched their first efforts to stamp out the deadly mosquito-borne infectious disease nearly 100 years.<!-- --></p><p>Certification is granted when a country proves that the transmission chain is interrupted for at least the previous three consecutive years. Malaria kills at least 600,000 people every year, nearly all of them in Africa.<!-- --></p></div><div data-component="text-block"><p>In a statement on Sunday, the WHO praised "the Egyptian government and people" for their efforts to "end a disease that has been present in the country since ancient times".<!-- --></p><p>It said Egypt was the third country to be certified in the WHO's Eastern Mediterranean Region, following the United Arab Emirates and Morocco.<!-- --></p><p>Globally, 44 countries and one territory have reached this milestone.<!-- --></p><p>But the WHO said the certification was only "the beginning of a new phase", urging Egypt to be on the alert to preserve its malaria-free status.<!-- --></p><p>To get the WHO certification, a country must demonstrate the capacity to prevent the re-establishment of transmission.<!-- --></p><p>The UN public health agency said first efforts to limit human-mosquito contact in Egypt began in the 1920s when it banned rice cultivation and agricultural crops near homes.<!-- --></p><p>Malaria is caused by a complex parasite which is spread by mosquito bites.<!-- --></p><p>Vaccines are now being used in some places - but monitoring the disease and avoiding mosquito bites are the most effective ways to prevent malaria.<!-- --></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[IOCCC Flight Simulator (2010) (153 pts)]]></title>
            <link>https://blog.aerojockey.com/iocccsim/</link>
            <guid>41903399</guid>
            <pubDate>Mon, 21 Oct 2024 12:24:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.aerojockey.com/iocccsim/">https://blog.aerojockey.com/iocccsim/</a>, See on <a href="https://news.ycombinator.com/item?id=41903399">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-3974" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting">






<div itemprop="text">
	
	
	
<blockquote><p>
Be sure to visit our <a href="http://www.facebook.com/pages/Ioccc-Flight-Simulator/165566243463997">Facebook Page</a>.</p>
</blockquote>



<p>The IOCCC Flight Simulator was the winning entry in the 1998 International Obfuscated C Code Contest. It is a flight simulator in under 2 kilobytes of code, complete with relatively accurate 6-degree-of-freedom dynamics, loadable wireframe scenery, and a small instrument panel.</p>



<p>IOCCC Flight Simulator runs on Unix-like systems with X Windows. As per contest rules, it is in the public domain.</p>



<h3>Documentation</h3>



<h4>Introduction</h4>



<p>You have just stepped out of the real world and into the virtual. You are now sitting in the cockpit of a Piper Cherokee airplane, heading north, flying 1000 feet above ground level.</p>



<p>Use the keyboard to fly the airplane. The arrow keys represent the control stick. Press the Up Arrow key to push the stick forward. Press the left arrow key to move the stick left, and so on. Press Enter to re-center the stick. Use Page Up and Page Down increase and decrease the throttle, respectively. (The rudder is automatically coordinated with the turn rate, so rudder pedals are not represented.)</p>



<p>On your display, you will see on the bottom left corner three instruments. The first is the airspeed indicator; it tells you how fast you’re going in knots. The second is the heading indicator, or compass. 0 is north, 90 is east, 180 is south, 270 is west. The third instrument is the altimeter, which measures your height above ground level in feet.</p>



<h4>Usage</h4>



<p>To use, type:</p>



<pre><code>
cat horizon.sc pittsburgh.sc | ./banks
</code>
</pre>



<p>banks is the name of the program (a quirk of IOCCC rules, and no pun intended). horizon.sc and pittsburgh.sc are scenery files.</p>



<h4>Features</h4>



<ul>
<li>Simulator models a Piper Cherokee, which is a light, single-engine, propeller-driven airplane.</li>
<li>The airplane is modeled as a six degree-of-freedom rigid body, accurately reflecting its dynamics (for normal flight conditions, at least).</li>
<li>Fly through a virtual 3-D world, while sitting at your X console.</li>
<li>Loadable scenery files.</li>
<li>Head-up display contains three instruments: a true airspeed indicator, a heading indicator (compass), and an altimeter.</li>
<li>Flight controls may be mapped to any keys at compile time by redefining the macros in the build file. Nice if your keyboard doesn’t have arrow keys.</li>
<li>Time step size can be set at compile time. This is useful to reduce flicker on network X connections. (But be careful: step sizes longer than about 0.03 seconds tend to have numerical stability problems.)</li>
<li>Airplane never stalls!</li>
<li>Airplane never runs out of fuel!</li>
</ul>



<h4>Scenery</h4>



<p>Each of the <cite>*.sc</cite> files is a scenery file. The simulator program reads in the scenery from standard input on startup. You may input more than one scenery file, as long as there are less than 1000 total lines of input.</p>



<p>Here is a brief description of the scenery files:</p>



<ul>
<li><strong>horizon.sc</strong> – A horizon, nothing more. You will probably always want to input this piece of scenery.</li>
<li><strong>mountains.sc</strong> – An alternate horizon; a little more mountainous.</li>
<li><strong>pittsburgh.sc</strong> – Scenery of downtown Pittsburgh. The downtown area is initially located to your right.</li>
<li><strong>bb.sc</strong> – Simple obstacle course. Try to fly over the buildings and under the bridges.</li>
<li><strong>pyramids.sc</strong> – Fly over the tombs of the ancient Pharaohs in this (fictitious) Egyptian landscape.</li>
<li><strong>river.sc</strong> – Follow a flowing river from the sky.</li>
</ul>



<p>A few examples of how to input scenery:</p>



<pre><code>
cat horizon.sc pittsburgh.sc | ./banks
cat mountains.sc bb.sc | ./banks
cat mountains.sc river.sc pyramids.sc | ./banks
</code>
</pre>



<p>You can simulate flying through a cloud bank as well:</p>



<pre><code>
./banks &lt; /dev/null
</code>
</pre>



<p>You will usually want at least a horizon, though.</p>



<p>The format of scenery files is simple, by the way. They’re just a list of 3-D coordinates, and the simulator simply draws line segments from point to point as listed in the scenery file. 0 0 0 is used to end a series of consecutive line segments. Note that in the coordinate system used, the third coordinate represents altitude in a negative sense: negative numbers are positive altitudes.</p>



<p>I’m sure you’ll be making your own scenery files very soon!!!</p>



<h4>Alternate Build Instructions</h4>



<p>Several options must be passed to the compiler to make the build work. The provided build file has the appropriate options set to default values. Use this section if you want to compile with different options.</p>



<p>To map a key to a control, you must pass an option to the compiler in the format “-Dcontrol=key”. The possible controls you can map are described in the table below:</p>



<pre><code>
Control  Description           Default Key
-------  --------------------- -----------
IT       Open throttle         XK_Page_Up
DT       Close throttle        XK_Page_Down
FD       Move stick forward    XK_Up
BK       Move stick back       XK_Down
LT       Move stick left       XK_Left
RT       Move stick right      XK_Right
CS       Center stick          XK_Enter
</code>
</pre>



<p>Values for the possible keys can be found in the X Windows header file &lt;X11/keysym.h&gt;. This file is most likely a cross-reference to another header, &lt;X11/keysymdef.h&gt;.</p>



<p>You must map all seven controls to keys at compile time, or the compilation will fail.</p>



<p>For example, to map Center Stick to the space-bar, the compile option would be “-DCS=XK_space”.</p>



<p>To set the time step size, you must pass the following option to the compiler: “-Ddt=duration”, where dt is literal, and where duration is the time in seconds you want the time step to be.</p>



<p>Two things to keep in mind when selecting a time step. Time steps that are too large (more than about 0.03) will cause numerical stability problems and should be avoided. Setting the time step to be smaller than your clock resolution will slow down the simulator, because the system pauses for more time than the simulator expects.</p>



<p>The best advice is to set time step size to your system timer resolution. Try a
longer period if you’re getting too much flicker.</p>



<h3>Screen Shots</h3>



<p>Here we are flying towards Downtown Pittsburgh. We can see the Point, several buildings including the USX tower, and several bridges including the Smithfield Street bridge. We see three instruments near the bottom.</p>



<figure><img fetchpriority="high" decoding="async" width="415" height="440" src="https://blog.aerojockey.com/wp-content/uploads/ioccc1.gif" alt=""></figure>



<h3>About the IOCCC Entry</h3>



<p>IOCCC stands for “International Obfuscated C Code Contest.” It is an quasi-annual contest to see who can write the most unreadable, unintelligible, unmanagable, but legal C program.</p>



<p>In the 1998 IOCCC, My flight simulator won the “Best of Show” prize. Here is the source code to the program:</p>



<pre><code>
#include                                     &lt;math.h&gt;
#include                                   &lt;sys/time.h&gt;
#include                                   &lt;X11/Xlib.h&gt;
#include                                  &lt;X11/keysym.h&gt;
                                          double L ,o ,P
                                         ,_=dt,T,Z,D=1,d,
                                         s[999],E,h= 8,I,
                                         J,K,w[999],M,m,O
                                        ,n[999],j=33e-3,i=
                                        1E3,r,t, u,v ,W,S=
                                        74.5,l=221,X=7.26,
                                        a,B,A=32.2,c, F,H;
                                        int N,q, C, y,p,U;
                                       Window z; char f[52]
                                    ; GC k; main(){ Display*e=
 XOpenDisplay( 0); z=RootWindow(e,0); for (XSetForeground(e,k=XCreateGC (e,z,0,0),BlackPixel(e,0))
; scanf("%lf%lf%lf",y +n,w+y, y+s)+1; y ++); XSelectInput(e,z= XCreateSimpleWindow(e,z,0,0,400,400,
0,0,WhitePixel(e,0) ),KeyPressMask); for(XMapWindow(e,z); ; T=sin(O)){ struct timeval G={ 0,dt*1e6}
; K= cos(j); N=1e4; M+= H*_; Z=D*K; F+=_*P; r=E*K; W=cos( O); m=K*W; H=K*T; O+=D*_*F/ K+d/K*E*_; B=
sin(j); a=B*T*D-E*W; XClearWindow(e,z); t=T*E+ D*B*W; j+=d*_*D-_*F*E; P=W*E*B-T*D; for (o+=(I=D*W+E
*T*B,E*d/K *B+v+B/K*F*D)*_; p&lt;y; ){ T=p[s]+i; E=c-p[w]; D=n[p]-L; K=D*m-B*T-H*E; if(p [n]+w[ p]+p[s
]== 0|K &lt;fabs(W=T*r-I*E +D*P) |fabs(D=t *D+Z *T-a *E)&gt; K)N=1e4; else{ q=W/K *4E2+2e2; C= 2E2+4e2/ K
 *D; N-1E4&amp;&amp; XDrawLine(e ,z,k,N ,U,q,C); N=q; U=C; } ++p; } L+=_* (X*t +P*M+m*l); T=X*X+ l*l+M *M;
  XDrawString(e,z,k ,20,380,f,17); D=v/l*15; i+=(B *l-M*r -X*Z)*_; for(; XPending(e); u *=CS!=N){
                                   XEvent z; XNextEvent(e ,&amp;z);
                                       ++*((N=XLookupKeysym
                                         (&amp;z.xkey,0))-IT?
                                         N-LT? UP-N?&amp; E:&amp;
                                         J:&amp; u: &amp;h); --*(
                                         DN -N? N-DT ?N==
                                         RT?&amp;u: &amp; W:&amp;h:&amp;J
                                          ); } m=15*F/l;
                                          c+=(I=M/ l,l*H
                                          +I*M+a*X)*_; H
                                          =A*r+v*X-F*l+(
                                          E=.1+X*4.9/l,t
                                          =T*m/32-I*T/24
                                           )/S; K=F*M+(
                                           h* 1e4/l-(T+
                                           E*5*T*E)/3e2
                                           )/S-X*d-B*A;
                                           a=2.63 /l*d;
                                           X+=( d*l-T/S
                                            *(.19*E +a
                                            *.64+J/1e3
                                            )-M* v +A*
                                            Z)*_; l +=
                                            K *_; W=d;
                                            sprintf(f,
                                            "%5d  %3d"
                                            "%7d",p =l
                                           /1.7,(C=9E3+
                              O*57.3)%0550,(int)i); d+=T*(.45-14/l*
                             X-a*130-J* .14)*_/125e2+F*_*v; P=(T*(47
                             *I-m* 52+E*94 *D-t*.38+u*.21*E) /1e2+W*
                             179*v)/2312; select(p=0,0,0,0,&amp;G); v-=(
                              W*F-T*(.63*m-I*.086+m*E*19-D*25-.11*u
                               )/107e2)*_; D=cos(o); E=sin(o); } }
</code>
</pre>



<p>Note that this program will not compile out-of-the-box. It requires certain compile-time parameters. The folloing script builds it on my Linux system:</p>



<pre><code>
#! /bin/sh
cc banks.c -o banks -DIT=XK_Page_Up -DDT=XK_Page_Down \
        -DUP=XK_Up -DDN=XK_Down -DLT=XK_Left -DRT=XK_Right \
        -DCS=XK_Return -Ddt=0.02 -lm -lX11 -L/usr/X11R6/lib
</code>
</pre>



<p>If you want to try this program, I suggest you download the <a href="http://www0.us.ioccc.org/years.html#1998">1998 IOCCC Winners Distribution</a>.</p>



<p>One of the rules of the contest was that the program could not be longer than 1536 bytes (excluding spaces, tabs, newlines, semicolons, and braces). Needless to say, cramming a flight simulator into such a small file was fairly difficult. I will say that if it weren’t for the wonderful property of orthogonal matrices, this flight simulator would not have been possible.</p>



<h3>Sightings</h3>



<ul>
<li>The IOCCC Simulator appeared in a book, <a href="http://www.amazon.com/exec/obidos/tg/detail/-/0521009626/103-3242838-6683018?vi=glance">Calculated Bets</a> by Steve Skiena.</li>
<li>Wikipedia has a listing of IOCCC Simluator in its <a href="http://en.wikipedia.org/wiki/International_Obfuscated_C_Code_Contest">IOCCC Entry</a>.</li>
</ul>



<h3>Links</h3>



<ul>
<li>The official <a href="http://www0.us.ioccc.org/">International Obfuscated C Code Contest</a> website</li>
<li>IOCCC Flight Simulator’s <a href="http://www.facebook.com/pages/Ioccc-Flight-Simulator/165566243463997">Facebook page</a>.  (I made this page because random people around the world would send me friend requests and it was creeping me out.)</li>
</ul>



<h3>Downloads</h3>



<p>I do not distribute this program myself. If you want it, you can download the <a href="http://www0.us.ioccc.org/years.html#1998">1998 IOCCC Winners Distribution</a>. The distribution comes with a dozen or so other winning entries, all quite interesting programs.</p>



<p>Note that this is a source distribution, and you will have compile it to run it. I’ve tested it on some versions of Linux, AIX, Irix, and Sun.</p>



<p>IOCCC Flight Simulator source code is in the public domain; there are no copyright restrictions on it whatsoever. However, the winners distribution has been copyrighted by the IOCCC judges. See the hint files in the distribution for details.</p>

	
	
	
	</div>




</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Erin – Open-source and self-hosted TikTok feed for your own videos (113 pts)]]></title>
            <link>https://github.com/will-moss/erin</link>
            <guid>41902407</guid>
            <pubDate>Mon, 21 Oct 2024 09:51:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/will-moss/erin">https://github.com/will-moss/erin</a>, See on <a href="https://news.ycombinator.com/item?id=41902407">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Erin</h2><a id="user-content-erin" aria-label="Permalink: Erin" href="#erin"></a></p>
    <p dir="auto">
      Self-hostable TikTok feed for your clips
      <br>
      Make a TikTok feed with your own videos
   </p>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/will-moss/erin/blob/master/screenshots/SCREENSHOT-1.png"><img width="1604" src="https://github.com/will-moss/erin/raw/master/screenshots/SCREENSHOT-1.png"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/will-moss/erin/blob/master/screenshots/SCREENSHOT-2.png"><img width="1604" src="https://github.com/will-moss/erin/raw/master/screenshots/SCREENSHOT-2.png"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/will-moss/erin/blob/master/screenshots/SCREENSHOT-3.png"><img width="1604" src="https://github.com/will-moss/erin/raw/master/screenshots/SCREENSHOT-3.png"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto">Erin is a simple and self-hostable service that enables you to view your own clips using TikTok's well-known vertical swipe feed.
<a href="https://www.reddit.com/r/selfhosted/comments/1dogl9d/selfhost_a_site_for_short_videos_like_tiktok/" rel="nofollow">A request was made on Reddit</a>
for a self-hostable app that can show filtered videos using TikTok's interface, so I made it.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<p dir="auto">Erin has all these features implemented :</p>
<ul dir="auto">
<li>Display your own videos using TikTok's swipe feed</li>
<li>Mask the videos you don't want to see in your feed*</li>
<li>Choose which feed you want to play**</li>
<li>Autoplay your feed without even swiping</li>
<li>Seek forward and backward using your keyboard, or using double taps</li>
<li>Show video metadata using TikTok's UI***</li>
<li>Simple lazy-loading mechanism for your videos</li>
<li>Automatic clip naming based on file name</li>
<li>Simple and optional security using a master password</li>
<li>Support for HTTP and HTTPS</li>
<li>Support for Docker / proxy deployment</li>
</ul>
<p dir="auto">On top of these, please note that Erin is only a React app powered entirely by <a href="https://github.com/caddyserver/caddy">Caddy</a>.
Caddy takes care of authentication, serving static files, and serving the React app all at once.</p>
<blockquote>
<p dir="auto">*: You can mask videos to hide them from your feed. Should you want to see which videos were masked, and even unmask them, you can long-press the <code>Mask</code> button, and the manager will open.</p>
</blockquote>
<blockquote>
<p dir="auto">**: By default, Erin will create a random feed from all the videos in your folder and its subdirectories. However, if you would like to create custom feeds, you can set your URL to any of the subdirectories path. For example: <code>https://my-server.tld/directory-a</code> will create a feed from the videos located in the <code>/directory-a</code> directory, and it works with any path (so, nested folders are supported).</p>
</blockquote>
<blockquote>
<p dir="auto">***: You can show a channel (with an avatar and name), a caption and a link for all your videos using a metadata file. The metadata file can be located anywhere inside your videos folder, and it must match its associated video's filename, while replacing the extension with JSON. For example: <code>my-video.mp4</code> can have its metadata in <code>my-video.json</code>. The metadata format <a href="https://github.com/will-moss/erin/blob/master/examples/video-metadata.json">is shown here</a>, and note that you can use raw HTML in the caption for custom styling and effects.</p>
</blockquote>
<p dir="auto">For more information, read about <a href="#configuration">Configuration</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Deployment and Examples</h2><a id="user-content-deployment-and-examples" aria-label="Permalink: Deployment and Examples" href="#deployment-and-examples"></a></p>
<p dir="auto">Before proceeding, regardless of Docker, Docker Compose, or a standalone deployment, please make sure
that you have created a <code>videos</code> directory containing all your video files. Later on, this directory will
be made available to your instance of Erin (by binding a volume to your Docker container, or putting the directory
next to your Caddyfile).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Deploy with Docker</h3><a id="user-content-deploy-with-docker" aria-label="Permalink: Deploy with Docker" href="#deploy-with-docker"></a></p>
<p dir="auto">You can run Erin with Docker on the command line very quickly.</p>
<p dir="auto">You can use the following commands :</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Create a .env file
touch .env

# Edit .env file ...

# Option 1 : Run Erin attached to the terminal (useful for debugging)
docker run --env-file .env -p <YOUR-PORT-MAPPING> -v ./videos:/srv/videos:ro mosswill/erin

# Option 2 : Run Erin as a daemon
docker run -d --env-file .env -p <YOUR-PORT-MAPPING> -v ./videos:/srv/videos:ro mosswill/erin"><pre><span><span>#</span> Create a .env file</span>
touch .env

<span><span>#</span> Edit .env file ...</span>

<span><span>#</span> Option 1 : Run Erin attached to the terminal (useful for debugging)</span>
docker run --env-file .env -p <span>&lt;</span>YOUR-PORT-MAPPING<span>&gt;</span> -v ./videos:/srv/videos:ro mosswill/erin

<span><span>#</span> Option 2 : Run Erin as a daemon</span>
docker run -d --env-file .env -p <span>&lt;</span>YOUR-PORT-MAPPING<span>&gt;</span> -v ./videos:/srv/videos:ro mosswill/erin</pre></div>
<blockquote>
<p dir="auto"><strong>Note :</strong> A <code>sample.env</code> file is located at the root of the repository to help you get started</p>
</blockquote>
<blockquote>
<p dir="auto"><strong>Note :</strong> When using <code>docker run --env-file</code>, make sure to remove the quotes around <code>AUTH_ENABLED</code> and <code>AUTH_SECRET</code>, or else
your container might crash due to unexpected interpolation and type conversions operated by Docker behind the scenes.</p>
</blockquote>
<p dir="auto"><h3 tabindex="-1" dir="auto">Deploy with Docker Compose</h3><a id="user-content-deploy-with-docker-compose" aria-label="Permalink: Deploy with Docker Compose" href="#deploy-with-docker-compose"></a></p>
<p dir="auto">To help you get started quickly, a few example <code>docker-compose</code> files are located in the <a href="https://github.com/will-moss/erin/blob/master/examples">"examples/"</a> directory.</p>
<p dir="auto">Here's a description of every example :</p>
<ul dir="auto">
<li>
<p dir="auto"><code>docker-compose.simple.yml</code>: Run Erin as a front-facing service on port 443, with environment variables supplied in the <code>docker-compose</code> file directly.</p>
</li>
<li>
<p dir="auto"><code>docker-compose.proxy.yml</code>: A setup with Erin running on port 80, behind a proxy listening on port 443.</p>
</li>
</ul>
<p dir="auto">When your <code>docker-compose</code> file is on point, you can use the following commands :</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run Erin in the current terminal (useful for debugging)
docker-compose up

# Run Erin in a detached terminal (most common)
docker-compose up -d

# Show the logs written by Erin (useful for debugging)
docker logs <NAME-OF-YOUR-CONTAINER>"><pre><span><span>#</span> Run Erin in the current terminal (useful for debugging)</span>
docker-compose up

<span><span>#</span> Run Erin in a detached terminal (most common)</span>
docker-compose up -d

<span><span>#</span> Show the logs written by Erin (useful for debugging)</span>
docker logs <span>&lt;</span>NAME-OF-YOUR-CONTAINER<span>&gt;</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">To run Erin, you will need to set the following environment variables in a <code>.env</code> file :</p>
<blockquote>
<p dir="auto"><strong>Note :</strong> Regular environment variables provided on the commandline work too</p>
</blockquote>
<blockquote>
<p dir="auto"><strong>Note :</strong> A <code>sample.env</code> file is located at the root of the repository to help you get started</p>
</blockquote>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>PUBLIC_URL</code></td>
<td><code>boolean</code></td>
<td>The public URL used to remotely access your instance of Erin. (Please include HTTP / HTTPS and the port if not standard 80 or 443. Do not include a trailing slash) (Read the <a href="https://caddyserver.com/docs/caddyfile/concepts#addresses" rel="nofollow">official Caddy documentation</a>)</td>
<td><a href="https://localhost/" rel="nofollow">https://localhost</a></td>
</tr>
<tr>
<td><code>AUTH_ENABLED</code></td>
<td><code>string</code></td>
<td>Whether Basic Authentication should be enabled. (This parameter is case sensitive) (Possible values : true, false)</td>
<td>true</td>
</tr>
<tr>
<td><code>AUTH_SECRET</code></td>
<td><code>string</code></td>
<td>The secure hash of the password used to protect your instance of Erin.</td>
<td>Hash of <code>secure-password</code></td>
</tr>
<tr>
<td><code>APP_TITLE</code></td>
<td><code>string</code></td>
<td>The custom title that you would like to display in the browser's tab. (Tip: You can use <code>[VIDEO_TITLE]</code> here if you want Erin to dynamically display the title of the current video.)</td>
<td>Erin - TikTok feed for your own clips</td>
</tr>
<tr>
<td><code>AUTOPLAY_ENABLED</code></td>
<td><code>boolean</code></td>
<td>Whether autoplay should be enabled. (This parameter is case sensitive) (Possible values : true, false)</td>
<td>false</td>
</tr>
<tr>
<td><code>PROGRESS_BAR_POSITION</code></td>
<td><code>string</code></td>
<td>Where the progress bar should be located on the screen. (This parameter is case sensitive) (Possible values : bottom, top)</td>
<td>bottom</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<blockquote>
<p dir="auto"><strong>Tip :</strong> To generate a secure hash for your instance, use the following command :</p>
</blockquote>
<div dir="auto" data-snippet-clipboard-copy-content="docker run caddy caddy hash-password --plaintext &quot;your-new-password&quot;"><pre>docker run caddy caddy hash-password --plaintext <span><span>"</span>your-new-password<span>"</span></span></pre></div>
<blockquote>
<p dir="auto"><strong>Note :</strong> When using <code>docker-compose.yml</code> environment variables, if your password hash contains dollar signs: double them all, or else the app will crash.
For example : <code>$ab$cd$efxyz</code> becomes <code>$$ab$$cd$$efxyz</code>. This is due to caveats with <code>docker-compose</code> string interpolation system.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshoot</h2><a id="user-content-troubleshoot" aria-label="Permalink: Troubleshoot" href="#troubleshoot"></a></p>
<p dir="auto">Should you encounter any issue running Erin, please refer to the following common problems that may occur.</p>
<blockquote>
<p dir="auto">If none of these matches your case, feel free to open an issue.</p>
</blockquote>
<p dir="auto"><h4 tabindex="-1" dir="auto">Erin is unreachable over HTTP / HTTPS</h4><a id="user-content-erin-is-unreachable-over-http--https" aria-label="Permalink: Erin is unreachable over HTTP / HTTPS" href="#erin-is-unreachable-over-http--https"></a></p>
<p dir="auto">Erin sits on top of a Caddy web server.</p>
<p dir="auto">As a result :</p>
<ul dir="auto">
<li>You may be able to better troubleshoot the issue by reading your container logs.</li>
<li>You can check the <a href="https://caddyserver.com/docs/caddyfile/concepts#addresses" rel="nofollow">official Caddy documentation regarding addresses</a>.</li>
<li>You can check the <a href="https://caddyserver.com/docs/automatic-https" rel="nofollow">official Caddy documentation regarding HTTPS</a>.</li>
</ul>
<p dir="auto">Other than that, please make sure that the following requirements are met :</p>
<ul dir="auto">
<li>
<p dir="auto">If Erin runs as a standalone application without proxy :</p>
<ul dir="auto">
<li>Make sure your server / firewall accepts incoming connections on Erin's port.</li>
<li>Make sure your DNS configuration is correct. (Usually, such record should suffice : <code>A erin XXX.XXX.XXX.XXX</code> for <code>https://erin.your-server-tld</code>)</li>
<li>Make sure your <code>.env</code> file is well configured according to the <a href="#configuration">Configuration</a> section.</li>
</ul>
</li>
<li>
<p dir="auto">If Erin runs inside Docker / behind a proxy :</p>
<ul dir="auto">
<li>Perform the previous (standalone) verifications first.</li>
<li>Make sure that <code>PUBLIC_URL</code> is well set in <code>.env</code>.</li>
<li>Check your proxy forwarding rules.</li>
<li>Check your Docker networking setup.</li>
</ul>
</li>
</ul>
<p dir="auto">In any case, the crucial part is <a href="#configuration">Configuration</a> and reading the official Caddy documentation.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Erin says that no video was found on my server</h4><a id="user-content-erin-says-that-no-video-was-found-on-my-server" aria-label="Permalink: Erin says that no video was found on my server" href="#erin-says-that-no-video-was-found-on-my-server"></a></p>
<p dir="auto">For Erin to serve your video files, those must respect the following requirements :</p>
<ul dir="auto">
<li>The file extension is one of <code>.mp4</code>, <code>.ogg</code>, <code>.webm</code>. (There are the only extensions supported by web browsers.)</li>
<li>The files are located in <code>/srv/videos</code> on your Docker container using a volume.</li>
</ul>
<p dir="auto">To make sure that your videos are inside your Docker container and in the right place, you can :</p>
<ul dir="auto">
<li>Run <code>docker exec -it &lt;NAME-OF-YOUR-CONTAINER&gt; sh</code></li>
<li>Inside the newly-opened shell, run : <code>ls /srv/videos</code></li>
<li>You should see your video files here. If not, then check your volume-binding.</li>
</ul>
<p dir="auto">If Erin is still unable to find your videos despite everything being well-configured, please open an issue
including the output of your browser's Javascript console and network tab when the request goes to <code>/media/</code>.
It may have to do with browser-caching, invalid configuration, or invalid credentials.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">How can I add new videos to my feed?</h4><a id="user-content-how-can-i-add-new-videos-to-my-feed" aria-label="Permalink: How can I add new videos to my feed?" href="#how-can-i-add-new-videos-to-my-feed"></a></p>
<p dir="auto">For now, you should just put your new video files into your videos directory that is mounted with Docker.
Erin will automatically pick up these new files, and when you refresh your browser you'll see them.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">How should I name my video files?</h4><a id="user-content-how-should-i-name-my-video-files" aria-label="Permalink: How should I name my video files?" href="#how-should-i-name-my-video-files"></a></p>
<p dir="auto">Erin will automatically translate your file name into a title to display on the interface.</p>
<p dir="auto">The conversion operated is as follows :</p>
<ul dir="auto">
<li><code>-</code> becomes <code> </code></li>
<li><code>__</code> becomes <code>-</code></li>
</ul>
<p dir="auto">Here's a few examples to help you name your files :</p>
<ul dir="auto">
<li><code>Vegas-trip__Clip-1.mp4</code> becomes <code>Vegas trip - Clip 1</code></li>
<li><code>Spanish-language__Lesson-1.mp4</code> becomes <code>Spanish language - Lesson 1</code></li>
<li><code>Spiderman-1.ogg</code> becomes <code>Spiderman 1</code></li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">In what order will my files appear in the feed?</h4><a id="user-content-in-what-order-will-my-files-appear-in-the-feed" aria-label="Permalink: In what order will my files appear in the feed?" href="#in-what-order-will-my-files-appear-in-the-feed"></a></p>
<p dir="auto">Erin randomly shuffles your video files on every browser refresh.</p>
<p dir="auto">As a result, there is no specific order for your videos to appear.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Some of my videos seem to be missing or not loaded at all</h4><a id="user-content-some-of-my-videos-seem-to-be-missing-or-not-loaded-at-all" aria-label="Permalink: Some of my videos seem to be missing or not loaded at all" href="#some-of-my-videos-seem-to-be-missing-or-not-loaded-at-all"></a></p>
<p dir="auto">For now, Erin will only attempt to retrieve the videos that have a supported extension.</p>
<p dir="auto">Supported extensions are : <code>.webm</code>, <code>.mp4</code>, and <code>.ogg</code>.</p>
<p dir="auto">However, please note that Safari doesn't seem to support <code>.ogg</code>, hence these videos will be ignored for Safari users.</p>
<p dir="auto">Should you have any advice or idea to support more extensions (especially for Safari users), please feel free to open an issue.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">My custom password doesn't work</h4><a id="user-content-my-custom-password-doesnt-work" aria-label="Permalink: My custom password doesn't work" href="#my-custom-password-doesnt-work"></a></p>
<p dir="auto">There seems to be a few caveats when using Docker / Docker Compose with Caddy-generated password hashes.</p>
<p dir="auto">These are the rules you should follow :</p>
<ul dir="auto">
<li>If you deployed Erin using the Docker CLI, via the command <code>docker run ... --env-file .env ...</code>, then your <code>AUTH_SECRET</code> should have no quote at all, and all the dollar signs should stay as they are without escape or doubling</li>
<li>If you deployed Erin using Docker Compose, via a <code>docker-compose.yml</code> file, then your <code>AUTH_SECRET</code> should have its dollar signs doubled. Example : <code>i$am$groot</code> becomes <code>i$$am$$groot</code>.</li>
</ul>
<p dir="auto">That said, remember that your password hash must be generated with the following command :</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker run caddy caddy hash-password --plaintext &quot;your-new-password&quot;"><pre>docker run caddy caddy hash-password --plaintext <span><span>"</span>your-new-password<span>"</span></span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Something else</h4><a id="user-content-something-else" aria-label="Permalink: Something else" href="#something-else"></a></p>
<p dir="auto">Please feel free to open an issue, explaining what happens, and describing your environment.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Credits</h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<p dir="auto">Hey hey ! It's always a good idea to say thank you and mention the people and projects that help us move forward.</p>
<p dir="auto">Big thanks to the individuals / teams behind these projects :</p>
<ul dir="auto">
<li><a href="https://github.com/cauemustafa/tik-tok-clone">tik-tok-clone</a> : For the base TikTok UI and smooth interaction.</li>
<li><a href="https://github.com/caddyserver/caddy">Caddy</a> : For the lightweight and powerful web server.</li>
<li>The countless others!</li>
</ul>
<p dir="auto">And don't forget to mention Erin if you like it or if it helps you in any way!</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AWS and Azure Are at Least 4x–10x More Expensive Than Hetzner (125 pts)]]></title>
            <link>https://learn.umh.app/course/aws-and-azure-are-at-least-4x-10x-more-expensive-than-hetzner/</link>
            <guid>41902103</guid>
            <pubDate>Mon, 21 Oct 2024 09:07:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://learn.umh.app/course/aws-and-azure-are-at-least-4x-10x-more-expensive-than-hetzner/">https://learn.umh.app/course/aws-and-azure-are-at-least-4x-10x-more-expensive-than-hetzner/</a>, See on <a href="https://news.ycombinator.com/item?id=41902103">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
    <div data-post-hero="">

    <div data-post-hero-content="">
          

      

        <p>Why We Recommend Hetzner to Try Out the UMH: it is at least 4x–10x less expensive than AWS and Azure for similar VM instances.</p>

        <ul data-post-hero-authors="">
            <li>
              <a href="https://learn.umh.app/instructor/jeremy/">
                  <picture>
  <source srcset="https://learn.umh.app/content/images/size/w30/format/webp/2023/02/Jeremy_huc484169f1c4136c603a7df27dcdff14f_371473_1200x0_resize_q75_box.jpg 30w, https://learn.umh.app/content/images/size/w100/format/webp/2023/02/Jeremy_huc484169f1c4136c603a7df27dcdff14f_371473_1200x0_resize_q75_box.jpg 100w" sizes="32px" type="image/webp">
  <img onload="this.classList.remove('blur')" loading="lazy" srcset="https://learn.umh.app/content/images/size/w30/format/webp/2023/02/Jeremy_huc484169f1c4136c603a7df27dcdff14f_371473_1200x0_resize_q75_box.jpg 30w, https://learn.umh.app/content/images/size/w100/format/webp/2023/02/Jeremy_huc484169f1c4136c603a7df27dcdff14f_371473_1200x0_resize_q75_box.jpg 100w" sizes="32px" src="https://learn.umh.app/content/images/size/w30/2023/02/Jeremy_huc484169f1c4136c603a7df27dcdff14f_371473_1200x0_resize_q75_box.jpg" alt="">
</picture>
                <span>Jeremy Theocharis</span>
              </a>
            </li>
        </ul>
      

        <p>Why We Recommend Hetzner to Try Out the UMH: it is at least 4x–10x less expensive than AWS and Azure for similar VM instances.</p>
    </div>

      <figure>
        <picture>
  <source srcset="https://learn.umh.app/content/images/size/w320/format/webp/2024/10/YT_Thumbnail_3_Approach-13.png 320w, https://learn.umh.app/content/images/size/w640/format/webp/2024/10/YT_Thumbnail_3_Approach-13.png 600w, https://learn.umh.app/content/images/size/w960/format/webp/2024/10/YT_Thumbnail_3_Approach-13.png 960w, https://learn.umh.app/content/images/size/w1280/format/webp/2024/10/YT_Thumbnail_3_Approach-13.png 1200w" sizes="(max-width: 600px) 480px, 720px" type="image/webp">
  <img loading="eager" srcset="https://learn.umh.app/content/images/size/w320/format/webp/2024/10/YT_Thumbnail_3_Approach-13.png 320w, https://learn.umh.app/content/images/size/w640/format/webp/2024/10/YT_Thumbnail_3_Approach-13.png 600w, https://learn.umh.app/content/images/size/w960/format/webp/2024/10/YT_Thumbnail_3_Approach-13.png 960w, https://learn.umh.app/content/images/size/w1280/format/webp/2024/10/YT_Thumbnail_3_Approach-13.png 1200w" sizes="(max-width: 600px) 480px, 720px" src="https://learn.umh.app/content/images/size/w30/2024/10/YT_Thumbnail_3_Approach-13.png" alt="AWS and Azure are At Least 4x–10x More Expensive Than Hetzner">
</picture>
        
      </figure>

  </div>
    <section data-post-content="">
      
      <article data-no-overflow="">
        <p><strong>No, you don’t have to use AWS or Azure. </strong>While AWS and Azure are industry leaders, their advantages often only materialize at massive scales. In many cases, they lead to escalating costs and vendor lock-in without delivering proportional benefits.</p><p>If your organization hasn’t yet  committed to a cloud provider or negotiated fixed contracts, it’s worth exploring alternatives like <a href="https://www.hetzner.com/cloud/?ref=learn.umh.app">Hetzner</a>.</p><p>In this article, we provide a technical comparison between Hetzner, AWS, and Azure, focusing on three key aspects:</p><ol><li>Cost Efficiency</li><li>Adequate Performance and Reliability</li><li>Industry Trends</li></ol><h2 id="cost-efficiency-hetzner-vs-aws-and-azure">Cost Efficiency: Hetzner vs. AWS and Azure</h2><p>Below is a direct comparison of equivalent instances from Hetzner, AWS, and Azure as of <em>2024-10-16</em>.</p><h3 id="base-numbers">Base Numbers</h3><table>
<thead>
<tr>
<th>Cloud Provider</th>
<th>Type</th>
<th>vCPUs</th>
<th>RAM</th>
<th>Monthly Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hetzner Cloud</td>
<td>CPX41</td>
<td>8</td>
<td>16 GB</td>
<td>$32.70 <br> (billed hourly)</td>
</tr>
<tr>
<td>AWS EC2 <br>(1-Year)</td>
<td>c6g.2xlarge</td>
<td>8</td>
<td>16 GiB</td>
<td>$180.60</td>
</tr>
<tr>
<td>AWS EC2 <br>(On-Demand)</td>
<td>c6g.2xlarge</td>
<td>8</td>
<td>16 GiB</td>
<td>$226.59</td>
</tr>
<tr>
<td>Azure VM</td>
<td>F8</td>
<td>8</td>
<td>16 GiB</td>
<td>$331.42 <br> (billed hourly)</td>
</tr>
</tbody>
</table>
<p><strong>Cost Comparison:</strong></p><ul><li>AWS On-Demand vs. Hetzner: $226.59 / $32.70 ≈ <strong>6.93 times</strong> more expensive</li><li>AWS 1-Year Reserved vs. Hetzner: $180.60 / $32.70 ≈ <strong>5.52 times</strong> more expensive</li><li>Azure vs. Hetzner: $331.42 / $32.70 ≈ <strong>10.14 times</strong> more expensive</li></ul><p>This is not an isolated case. Let’s examine more powerful instances:</p><table>
<thead>
<tr>
<th>Instance Type</th>
<th>vCPUs/CPUs</th>
<th>RAM</th>
<th>Monthly Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hetzner CPX51</td>
<td>16 vCPUs</td>
<td>32 GB</td>
<td>$71.23</td>
</tr>
<tr>
<td>Hetzner AX102 <br>(Dedicated CPU, 1-month commitment)</td>
<td>16 CPUs (Dedicated)</td>
<td>128 GB</td>
<td>$134.94</td>
</tr>
<tr>
<td>AWS c6g.4xlarge <br>(1-Year)</td>
<td>16 vCPUs</td>
<td>32 GiB</td>
<td>$361.28</td>
</tr>
<tr>
<td>AWS c6g.4xlarge <br>(On-Demand)</td>
<td>16 vCPUs</td>
<td>32 GiB</td>
<td>$457.72</td>
</tr>
<tr>
<td>AWS r6g.4xlarge</td>
<td>16 vCPUs</td>
<td>128 GiB</td>
<td>$453.18</td>
</tr>
</tbody>
</table>
<p>Not all CPUs are created equal, and performance can vary significantly between different architectures and configurations. AWS’s c6g instances use ARM-based Graviton2 processors, which offer excellent performance per dollar for certain workloads but may present compatibility issues with software not optimized for ARM architecture. When selecting an x86-based instance like AWS’s m7a.2xlarge (8 vCPUs, 32 GB RAM), which utilizes AMD EPYC processors, the cost increases to <strong>$405.47</strong> per month—making it over <strong>5 times more expensive</strong> than Hetzner’s CPX51. </p><h3 id="hidden-costs-in-aws-and-azure">Hidden Costs in AWS and Azure</h3><p>Beyond the base pricing, AWS and Azure often introduce additional expenses that can impact the total cost of ownership:</p><ul><li>Storage Costs: EBS volumes (AWS) and Managed Disks (Azure) are billed separately.</li><li>Data Transfer Fees: Charges apply for data movement between services and outbound internet traffic, which can accumulate based on usage patterns.</li><li>Complex Pricing Models: Layered pricing structures and a multitude of service offerings can make cost estimation and budgeting more challenging.</li></ul><p>In contrast, Hetzner offers straightforward, nearly all-inclusive pricing, simplifying budget management and reducing the risk of unexpected charges. Yes, there are disadvantages about managing your own infrastructure, and we will cover them in the following chapters.</p><h2 id="adequate-performance-and-reliability">Adequate Performance and Reliability</h2><p>For many manufacturing companies and system integrators, the primary requirement is reliable application hosting without the need for massive scaling or global distribution. Most workloads in the manufacturing sector continue to operate on <a href="https://learn.umh.app/lesson/introduction-into-it-ot-system-administration/">virtual machines</a>, often running on-premises Windows servers. In such contexts, the advanced features and managed services offered by AWS and Azure may be excessive and unnecessarily complex.</p><p>Many organizations have small IT teams—sometimes as few as one person—responsible for managing Industrial IoT applications and infrastructure. These teams need straightforward, cost-effective solutions that align with their existing skill sets. Starting with a couple of VMs on Hetzner allows companies to begin their journey into modern IT infrastructure without incurring high costs or requiring specialized personnel.</p><p>In contrast, adopting AWS or Azure from the outset can require a steep learning curve and may necessitate hiring additional staff with specialized expertise. We experienced this firsthand: we spent several weeks trying to perform simple administrative tasks on AWS, such as adding a billing administrator, and found the process unnecessarily complicated.</p><p><strong>Do you really need the advanced features of AWS and Azure right now? Or would a simple virtual machine at a reasonable price be sufficient?</strong> That’s the main question here.</p><p>As the creator of Ruby on Rails puts it:</p><blockquote>I’d rather make some nice software than pay 100 times more for my compute.</blockquote><p>Let's look into what the industry is saying!</p><h2 id="industry-trends">Industry Trends</h2><p>There’s a growing movement among tech companies and startups to opt for more cost-effective hosting solutions like Hetzner. The high costs associated with AWS and Azure are leading many to reconsider their choices.</p><h3 id="ahrefs">Ahrefs</h3><figure><img src="https://learn.umh.app/content/images/2024/10/image-8.png" alt="" loading="lazy" width="721" height="1089" srcset="https://learn.umh.app/content/images/size/w600/2024/10/image-8.png 600w, https://learn.umh.app/content/images/2024/10/image-8.png 721w" sizes="(min-width: 720px) 720px"></figure><p><strong>Ahrefs</strong>, a prominent SEO tool provider, detailed their experience in the article <a href="https://tech.ahrefs.com/how-ahrefs-saved-us-400m-in-3-years-by-not-going-to-the-cloud-8939dd930af8?ref=learn.umh.app">“How Ahrefs Saved US$400M in 3 Years by NOT Going to the Cloud”</a>. Their main points are:</p><p><strong>Vendor Lock-In Concerns:</strong> They emphasized the difficulty of exiting a cloud provider due to convenience and potential vendor lock-in.</p><blockquote>“It is complicated to leave a cloud once you are there… Also, abandoning a cloud infrastructure due to higher costs may not be what the engineering team wants.”</blockquote><p><strong>Performance Advantages</strong>: Owning their hardware allowed for faster and more comprehensive results, enhancing their product.</p><blockquote>“Having faster and better results means that our servers are much faster than what a cloud can provide… Our reports are also generated faster and are more comprehensive.”</blockquote><p><strong>Revenue Implications:</strong> Their total revenue over that period was around $257 million, meaning AWS costs would have exceeded their entire revenue.</p><blockquote>The company revenue wouldn’t even be close to covering the 2½-year AWS usage costs.”</blockquote><p><strong>Massive Cost Savings:</strong> Ahrefs calculated that using AWS would have cost them an additional ~$400 million over 2.5 years compared to their own infrastructure.</p><blockquote>“Ahrefs has saved ~USD 400 million by ensuring its infrastructure is NOT 100% in the IaaS cloud during the last 2½ years.”</blockquote><h3 id="insights-from-industry-leaders">Insights from Industry Leaders</h3><figure><img src="https://learn.umh.app/content/images/2024/10/Screenshot-2024-10-17-at-13.30.48.png" alt="" loading="lazy" width="1278" height="659" srcset="https://learn.umh.app/content/images/size/w600/2024/10/Screenshot-2024-10-17-at-13.30.48.png 600w, https://learn.umh.app/content/images/size/w1000/2024/10/Screenshot-2024-10-17-at-13.30.48.png 1000w, https://learn.umh.app/content/images/2024/10/Screenshot-2024-10-17-at-13.30.48.png 1278w" sizes="(min-width: 720px) 720px"><figcaption><span>David Heinemeier Hansson on the Rails World Conference 2024</span></figcaption></figure><p>David Heinemeier Hansson, the creator of Ruby on Rails, <a href="https://youtu.be/-cEn_83zRFw?t=1652&amp;ref=learn.umh.app">discussed at the Rails World Conference</a> how startups can save costs and gain more powerful machines by choosing providers like Hetzner over AWS or Azure. Key points from his keynote include:</p><p><strong>Critique of Cloud Dependency: </strong>He criticized the industry trend of making developers feel incapable of managing servers, leading to over-reliance on expensive cloud services.</p><blockquote>We’ve sort of all turned into pink elephants tied with a tiny rope of learned helplessness when it comes to deployment… The problem is that the entire industry has cultivated a fear of touching a server.”</blockquote><p><strong>Value of Self-Hosting:</strong> He pointed out that you can get substantially more computing power for less money by managing your own servers.</p><blockquote>You can buy… 40 cores, 96 threads, 256 GB of RAM for $220 a month [from Hetzner]… I’d rather make some nice software than pay 100 times more for my compute.”</blockquote><p><strong>Questioning Cloud Economics:</strong> He highlighted the significant markup on cloud services compared to the actual cost of hardware.</p><blockquote>“AWS has a 40% margin… Dell, someone who actually makes the computers, have a 5% margin. That is a failed market.”</blockquote><p>These perspectives reflect a larger industry movement: companies are reevaluating the cost-benefit equation of major cloud providers, especially when high expenses don’t translate into proportional advantages.</p><h2 id="conclusion-what-has-this-to-do-with-umh">Conclusion: What has this to do with UMH?</h2><p>To get started with the <a href="https://www.umh.app/?ref=learn.umh.app" rel="noreferrer">UMH</a>, you need to install it via the <a href="https://management.umh.app/?ref=learn.umh.app" rel="noreferrer">Management Console</a>, either locally using the experimental Docker container or in a virtual machine.</p><p>If you don’t have an on-premises VM, you don’t need to use AWS or Azure either! By choosing Hetzner, you can deploy a UMH cloud instance for approximately <strong>$32 per month</strong>, compared to several hundred dollars on AWS or Azure. This makes it a much easier sell to conservative manufacturing companies, as <strong>the initial proof of concept (PoC) with the free community edition of UMH and a Hetzner cloud instance costs just a $32 per month of software and infrastructure costs</strong>.</p><p>In conclusion, if you’re just getting started and are not yet locked into AWS or Azure, Hetzner offers a cost-effective, reliable, and straightforward alternative. It allows you to deploy applications like UMH without incurring excessive costs or dealing with unnecessary complexity.</p>
      </article>
    </section>

    

        

      <div data-related="posts">
          <h2>Read next</h2>

          
        </div>

    </div></div>]]></description>
        </item>
    </channel>
</rss>