<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 28 Sep 2025 05:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Microwave weapon downs 49 drones with a single blast (102 pts)]]></title>
            <link>https://newatlas.com/military/microwave-beam-anti-drone-weapon/</link>
            <guid>45399863</guid>
            <pubDate>Sat, 27 Sep 2025 22:25:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newatlas.com/military/microwave-beam-anti-drone-weapon/">https://newatlas.com/military/microwave-beam-anti-drone-weapon/</a>, See on <a href="https://news.ycombinator.com/item?id=45399863">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In a demonstration not so much of marksmanship but more of the advantages of microwaves, an Epirus Leonidas directed energy, high-power microwave (HPM) anti-drone weapon has knocked 49 Uncrewed Aerial Vehicles (UAV) out of the air with one shot.</p><p>Two things that make drones particularly concerning is that they're small enough to appear from unexpected corners of the sky and they're cheap enough that they can be <a href="https://newatlas.com/ufo-drones-graffiti-painting/60423/" data-cms-ai="0">deployed in huge numbers</a>. In fact, they are so cheap that they pose not only a military threat, but a serious hazard to civilian aviation from individuals who are irresponsible, mischievous, or just oblivious.</p><p>This is the reason there are so many different types of <a href="https://newatlas.com/military/thor-microwave-weapon-drone-swarms/" data-cms-ai="0">anti-drone weapons</a>. Each has their advantages and disadvantages, with none providing a one-size-fits-all panacea. Instead, each needs to be fitted to a particular scenario or deployed as part of a layered defense strategy.</p><p>One countermeasure is the use of microwave weapons like Leonidas. Named after the Spartan king who held off a Persian invasion with a vastly inferior force at the Battle of Thermopylae, Leonidas is one of a family of weapons based on using long-pulse microwave beams to burn out the electronics of small drones.</p><p>The idea isn't new, but Epirus has improved on previous iterations by using Gallium Nitride (GaN) semiconductors to generate microwaves instead of fragile, power-hungry magnetron vacuum tubes. This allows for smaller, more durable, and more mobile systems that use less power. In addition, Leonidas is software driven and can tailor its waveform for optimum effect, it is safe to use around humans who may be in the field of fire, and the present system has twice the range of the 2022 version.</p><p>But the core feature is its "one-to-many" capability that gives it operational flexibility to handle a variety of scenarios. For example, it can strike against targets with precision to take out <a href="https://newatlas.com/military/valkyrie-combat-drone-launch-smaller-drone-test-flight/" data-cms-ai="0">hostile drones</a> while avoiding collateral damage, be programmed to set up no-fly zones with safety corridors to take out hostiles while allowing friendlies to pass, sustain continuous fire without overheating, and take down swarms in one go.</p><div data-align-center="">
                
                    <figure>
    
    
    
    


<p><img alt="The Leonidas microwave weapon on an armored vehicle" width="754" height="462" data-image-size="articleImage" loading="lazy" srcset="https://assets.newatlas.com/dims4/default/d6c72a3/2147483647/strip/true/crop/754x462+0+0/resize/440x270!/format/webp/quality/90/?url=https%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F0d%2F38%2F86e911324e6883ff677ccf3ab8ce%2Fscreenshot-2025-09-21-150303.jpg 440w,https://assets.newatlas.com/dims4/default/ccb09f5/2147483647/strip/true/crop/754x462+0+0/resize/725x444!/format/webp/quality/90/?url=https%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F0d%2F38%2F86e911324e6883ff677ccf3ab8ce%2Fscreenshot-2025-09-21-150303.jpg 725w,https://assets.newatlas.com/dims4/default/cb4580d/2147483647/strip/true/crop/754x462+0+0/resize/800x490!/format/webp/quality/90/?url=https%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F0d%2F38%2F86e911324e6883ff677ccf3ab8ce%2Fscreenshot-2025-09-21-150303.jpg 800w,https://assets.newatlas.com/dims4/default/0392e8b/2147483647/strip/true/crop/754x462+0+0/resize/1200x735!/format/webp/quality/90/?url=https%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F0d%2F38%2F86e911324e6883ff677ccf3ab8ce%2Fscreenshot-2025-09-21-150303.jpg 1200w,https://assets.newatlas.com/dims4/default/c31a3b3/2147483647/strip/true/crop/754x462+0+0/resize/1920x1176!/format/webp/quality/90/?url=https%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F0d%2F38%2F86e911324e6883ff677ccf3ab8ce%2Fscreenshot-2025-09-21-150303.jpg 1920w" src="https://assets.newatlas.com/dims4/default/d09a9cf/2147483647/strip/true/crop/754x462+0+0/resize/754x462!/format/webp/quality/90/?url=https%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F0d%2F38%2F86e911324e6883ff677ccf3ab8ce%2Fscreenshot-2025-09-21-150303.jpg" sizes="(min-width: 768px) 800px, 100vw">
</p>



    
    

    
        <div><figcaption itemprop="caption">The Leonidas microwave weapon on an armored vehicle</figcaption><p>Epirus</p></div>
    
</figure>

                
            </div><p>On August 26, 2025, in front of an invitation-only audience at Camp Atterbury, Indiana, Leonidas took part in a live fire exercise in which it disabled 61 drones with 100% success. This included knocking out two groups of three drones approaching <a href="https://newatlas.com/military/roadrunner-m-anti-aircraft-drone-reusable/" data-cms-ai="0">without warning</a> from opposite directions, targeting one of two drones selected by an audience member before disabling the second one, and intercepting and dropping a single drone into a predetermined safe zone.</p><p>Then came the party piece, it took on over four dozen drones at once, dropping them out of the sky simultaneously with a single pulse. That may not seem like much in words, but a video provided by the company had the lot suddenly crashing like someone had cut their strings.</p><div data-video-disable-history="" data-align-center="">
    
        <p><ps-youtubeplayer data-video-player="" data-player-id="f66f7afb1ce0d4212adc356019d091a96" data-video-id="ZrkopSw5uas" data-video-title="Epirus’ Leonidas High-Power Microwave Defeats 49-Drone Swarm">

    <iframe id="YouTubeVideoPlayer-f66f7afb1ce0d4212adc356019d091a96" role="application" title="YouTube embedded video player" allowfullscreen="" loading="lazy" src="https://www.youtube.com/embed/ZrkopSw5uas?enablejsapi=1"></iframe>
</ps-youtubeplayer>
</p>
    
    
        <p>Epirus’ Leonidas High-Power Microwave Defeats 49-Drone Swarm</p>
    
</div><p>"This is a watershed moment for Epirus," said Andy Lowery, Epirus CEO. "We believe showcasing our weaponized electromagnetic interference is the most effective way to communicate that Leonidas is the only mission-capable, counter-swarm solution for the one-to-many fight.Those who joined us witnessed this first-hand as 61 drones went up – and 61 went down."</p><p>Source: <a href="https://www.epirusinc.com/press-releases/epirus-leonidas-high-power-microwave-defeats-49-drone-swarm-100-of-drones-flown-at-live-fire-demonstration" target="_blank" data-cms-ai="0">Epirus</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Handy – Free open-source speech-to-text app written in Rust (101 pts)]]></title>
            <link>https://handy.computer/</link>
            <guid>45399106</guid>
            <pubDate>Sat, 27 Sep 2025 20:33:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://handy.computer/">https://handy.computer/</a>, See on <a href="https://news.ycombinator.com/item?id=45399106">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-j7pv25f6="" id="main-content" role="main"><div data-astro-cid-j7pv25f6=""><video width="640" height="360" controls="" playsinline="" aria-label="Handy speech-to-text demonstration video" aria-describedby="video-transcript" data-astro-cid-j7pv25f6=""><source src="https://handy.computer/handy-video.mp4" type="video/mp4" data-astro-cid-j7pv25f6=""><source src="https://handy.computer/handy-video.webm" type="video/webm" data-astro-cid-j7pv25f6=""><track kind="captions" src="/handy-video.vtt" srclang="en" label="English captions" default="" data-astro-cid-j7pv25f6=""><p data-astro-cid-j7pv25f6="">
Your browser doesn't support HTML video. <a href="https://handy.computer/handy-video.webm" data-astro-cid-j7pv25f6="">Download the video</a> instead.
</p></video><div id="video-transcript" data-astro-cid-j7pv25f6=""><h3 data-astro-cid-j7pv25f6="">Video Transcript</h3><p data-astro-cid-j7pv25f6="">
CJ: Hello, I'm CJ and I want to show you Handy. Handy is an
                    open source speech-to-text application that you can run on
                    your own computer. Simply press a keyboard shortcut, speak,
                    and release, and Handy will paste whatever you said into the
                    text field you're typing into.
</p><p data-astro-cid-j7pv25f6="">
Let's take a look at the settings menu for Handy, and it's
                    really simple. You have a push-to-talk mode that you can
                    enable, this is enabled by default so you press and hold the
                    keys or alternatively you can turn it off so the
                    transcription starts when you press the key combination and
                    it stops when you press it again. And you can also change
                    what key binding you would like to use for the
                    transcription.
</p><p data-astro-cid-j7pv25f6="">
So now it's mapped to Ctrl-Z and if I turn this off, when I
                    hit control Z, when you look up in the top corner of my Mac
                    here, this little transcription icon lights up. And when I
                    click it again, it turns off and transcribes the audio.
                    There's nothing to paste into. So it just does nothing here.
</p><p data-astro-cid-j7pv25f6="">So sit back, relax, and let Handy give you a hand.</p></div></div><div data-astro-cid-j7pv25f6=""><div data-astro-cid-j7pv25f6=""><h2 data-astro-cid-j7pv25f6="">Free</h2><p data-astro-cid-j7pv25f6="">
Accessibility tooling belongs in everyone's hands, not
                    behind a paywall.
</p></div><div data-astro-cid-j7pv25f6=""><h2 data-astro-cid-j7pv25f6="">Open Source</h2><p data-astro-cid-j7pv25f6="">
Together we can build further. Extend Handy for yourself and
                    contribute to something bigger.
</p></div><div data-astro-cid-j7pv25f6=""><h2 data-astro-cid-j7pv25f6="">Private</h2><p data-astro-cid-j7pv25f6="">
Your voice stays on your computer. Get transcriptions
                    without sending audio to the cloud.
</p></div><div data-astro-cid-j7pv25f6=""><h2 data-astro-cid-j7pv25f6="">Simple</h2><p data-astro-cid-j7pv25f6="">
One tool, one job. Transcribe what you say and put it into a
                    text box.
</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[iPhone 17 chip becomes the fastest single-core CPU in the world on PassMark (119 pts)]]></title>
            <link>https://www.tomshardware.com/pc-components/cpus/apples-a19-becomes-the-fastest-single-core-cpu-in-the-world-on-passmark-beating-pc-chips-and-apples-own-m3-ultra-passively-cooled-iphone-17-chip-catapults-past-power-hungry-competitors</link>
            <guid>45398802</guid>
            <pubDate>Sat, 27 Sep 2025 19:48:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/pc-components/cpus/apples-a19-becomes-the-fastest-single-core-cpu-in-the-world-on-passmark-beating-pc-chips-and-apples-own-m3-ultra-passively-cooled-iphone-17-chip-catapults-past-power-hungry-competitors">https://www.tomshardware.com/pc-components/cpus/apples-a19-becomes-the-fastest-single-core-cpu-in-the-world-on-passmark-beating-pc-chips-and-apples-own-m3-ultra-passively-cooled-iphone-17-chip-catapults-past-power-hungry-competitors</a>, See on <a href="https://news.ycombinator.com/item?id=45398802">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-1451-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH.jpg" alt="Apple A19" srcset="https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-1451-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/ycugCVL9Mycynwx4LPGKeH.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<figcaption>
<span>(Image credit: Apple)</span>
</figcaption>
</div>

<div id="article-body">
<p id="9598606f-6e07-4467-9c5e-8574022330ed">Apple's latest generation of iPhones is equipped with its A19 chips — the standard A19 on iPhone 17 and the A19 Pro on iPhone 17 Air and Pros — which represent the best the company has to offer, <em>literally</em>. In PassMark's single-threaded benchmark, the A19 produced <a data-analytics-id="inline-link" href="https://x.com/PassMarkInc/status/1971365505710817321" data-url="https://x.com/PassMarkInc/status/1971365505710817321" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">the best numbers of any chip</a> available, including fully-fledged desktop SKUs. It did that while consuming significantly less power and being passively cooled. At least in this hyper-specific case, Apple's A19 has become the fastest CPU available.</p><p>Both the A19 and A19 Pro benchmarked within the margin of error of each other; however, officially, it was the regular A19 that posted 5,149 points to claim the single-thread performance crown. The A19 Pro scored 5,088 points, which makes sense considering both chips share the same cores, just differing amounts of them. The A19 beats heavy hitters like Apple's own desktop-class <a data-analytics-id="inline-link" href="https://www.tomshardware.com/desktops/apple-debuts-m3-ultra-in-refreshed-mac-studio-with-up-to-512gb-memory" data-before-rewrite-localise="https://www.tomshardware.com/desktops/apple-debuts-m3-ultra-in-refreshed-mac-studio-with-up-to-512gb-memory">M3 Ultra</a> (both 28- and 32-core variants), <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/amd-ryzen-9-9950x-vs-intel-core-ultra-9-285k-faceoff-it-isnt-even-close" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/cpus/amd-ryzen-9-9950x-vs-intel-core-ultra-9-285k-faceoff-it-isnt-even-close">Intel's Core Ultra 9 285K</a>, and even the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/new-zen-5-128-core-epyc-cpu-weilds-512mb-of-l3-cache" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/cpus/new-zen-5-128-core-epyc-cpu-weilds-512mb-of-l3-cache">EPYC 4585PX</a> from AMD — all of which would be actively cooled.</p><div id="1971365505710817321"><blockquote data-lang="en"><p lang="en" dir="ltr">This is a pretty incredible single threaded benchmark result from Apple with the A19. Plus it is claimed to use only 12watts. For comparison the Ultra 9 is 125W+ and EPYC 4585PX is 170W+https://t.co/ysO73jpaVv pic.twitter.com/e9niPV5I3y<a href="https://twitter.com/cantworkitout/status/1971365505710817321" data-url="https://twitter.com/cantworkitout/status/1971365505710817321" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">September 26, 2025</a></p></blockquote></div><p id="727fbaf0-c5aa-42b1-b355-91c2dc1f4bca-0">The tweet caption lists nominal TDPs of these chips for comparison, but that's not what a single-core load would actually use. Since it's incredibly difficult to pinpoint that, PassMark itself estimated the single-threaded power consumption <a data-analytics-id="inline-link" href="https://x.com/PassMarkInc/status/1971730057862566329" data-url="https://x.com/PassMarkInc/status/1971730057862566329" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">in a reply</a>, saying the A19 is likely using 4W, the 285K is using 44W, and the EPYC is using 56W. Even if those 1/3 assumptions are wrong, the delta is so high between the three that it doesn't really matter. The A19 is miles ahead in terms of efficiency.</p><p>Where it falters, of course, is multi-threaded performance. It doesn't scale upward when you take more/all cores into account, but that's to be expected with a mobile-only chip, given that it simply has fewer cores than every other CPU on the list. Moreover, keep in mind that the A19 is inside the iPhone 17, which doesn't have a vapor chamber, so it's even more impressive for it to pull these kinds of numbers. Then again, this isn't precisely an uber-scientific test, so don't take these results at face value.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank" data-url="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank" data-url="https://google.com/preferences/source?q=" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-oES5rGgX64e8ph3uKSdwEg"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div>
</div>



<!-- Drop in a standard article here maybe? -->




<div id="slice-container-authorBio-oES5rGgX64e8ph3uKSdwEg"><p>Hassam Nasir is a die-hard hardware enthusiast with years of experience as a tech editor and writer, focusing on detailed CPU comparisons and general hardware news. When he’s not working, you’ll find him bending tubes for his ever-evolving custom water-loop gaming rig or benchmarking the latest CPUs and GPUs just for fun.  </p></div>
</section>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The death of east London's most radical bookshop (160 pts)]]></title>
            <link>https://www.the-londoner.co.uk/scarlett-letters-closure-left-wing-bookshop/</link>
            <guid>45398153</guid>
            <pubDate>Sat, 27 Sep 2025 18:15:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.the-londoner.co.uk/scarlett-letters-closure-left-wing-bookshop/">https://www.the-londoner.co.uk/scarlett-letters-closure-left-wing-bookshop/</a>, See on <a href="https://news.ycombinator.com/item?id=45398153">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
<div data-layout="minimal">
                
                    <p><a href="#/portal/signup/free"><img src="https://www.the-londoner.co.uk/content/images/2025/07/The-Londoner-icons-3.png" alt="CTA Image" data-image-dimensions="170x170"></a>
                    </p>
                
                
                    <div>
                    
                        <p><span>This article was published by </span><b><strong>The Londoner</strong></b><span>: a new newsletter covering the capital. Join our free mailing list below to get great writing and big scoops in your inbox.</span></p>
                    
                    
                        <p><a href="#/portal/signup/free">
                            Join The Londoner for free
                        </a>
                        
                    </p></div>
                
            </div>
<p>It was 4am on the 1st of July as Jack Parker bolted upright in the basement below the Scarlett Letters bookshop. From above, Parker could hear drilling. Then a “thunderstorm” of footsteps. Startled and bleary-eyed, they hurriedly dressed, then crept up the stairs into the main bookshop. What they saw “horrified” them.</p><p>Dozens of people hurriedly packing books into boxes and unscrewing bookshelves. Amongst the torrent of people, maybe the strangest thing they noticed was the face of Blaise Agüera y Arcas: author, AI researcher and the vice president of Google’s research arm. Peculiar though it was to see one of the most senior staff at one of the world’s biggest tech giants busting into a bookshop occupation in Bethnal Green, maybe what was more peculiar was how it all came about in the first place. The setting for this bizarre scene was the Scarlett Letters, named for its owner Marin Scarlett, a radical east London bookshop that was greeted with widespread fanfare by many of the capital's left-wing activists.</p><p>And here Scarlett was, with a team of people, bursting into the bookshop in the middle of the night in an attempt to disrupt an occupation by a radical cohort of the shop’s staff. To Parker's mind, for a project started with the aim of platforming sex workers and being a “hub for resistance, community, stories and imagination” to have reached such a point was mortifying. How had things gotten so bad? Well, at least in Parker's telling, it started with a clogged toilet.</p><p>Sporting a black T-shirt, cropped hair and anticapitalist and LGBT-rights tattoos on each arm, the former bookseller meets me in the bare, debris-littered unit that used to house the bookshop. We’re here to talk about the whole sorry saga of Scarlett Letters — the rise, the fall, the union disputes, the rows, the occupations and the drilled locks under nightfall. But first, we need to talk toilets.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcdHMQ65c8ly6f0-tKauKHUlkYWoNO1vN9oHlxsjcsdddccqY3Q0aRCaf48hPa9EbBd9sEcNZ8XIOsvqbobkEj9zY-K983KkJoGpD4HFHO_11kUDBnjV8dGsGU8u455bj9XdtGj?key=kMtj8Yn4kpj1VvRQ2cmVqw" alt="" loading="lazy" width="602" height="339"><figcaption><span>The leftover debris in the bookshop now. Image by Andrew Kersley&nbsp;</span></figcaption></figure><p>On a fateful day in early April, less than six months after the shop opened, a plumber had to be called in to fix the disabled toilet, which was inexplicably installed in the non-wheelchair-accessible basement of the bookshop. After the plumber was done, staff opened the work WhatsApp chat to see a message from Marin Scarlett updating the shop’s toilet policy: “We have had an issue over the last few weeks of people just letting themselves downstairs to use the toilet. Our toilet is there for people to use on request, but it is a problem if someone feels they can just let themselves down there without asking.”</p><p>Keen to thwart any further opportunistic toilet-users, Scarlett had a new policy. Staff were to personally escort anyone who asked to use the toilet to ensure they didn’t steal any stock or snoop around the staff area. She then told her staff she wanted to “role play” some scenarios in which they could practice saying “no”. Seemingly, the crux of the toilet problem was that they were simply too kind, too feminine, too British: “You are all extremely nice, assigned female at birth, in customer service, mostly British etc., and all of this sometimes doesn't lend itself to ‘no,’” the WhatsApp message explained. She suggested staff had been failing to tell customers “no” when they asked to borrow scissors or mugs from the shop, to come behind the counter or when they wanted to serenade them without prompting.</p><p>Like a toilet with a burst pipe, “toilet-gate” then erupted. Almost immediately, a dispute broke out in the work WhatsApp over the message; there was anger not just about that final message, which Parker saw as “bizarre and sexist”, but over the political ramifications of making members of the community ask to use the toilet in the bookshop.</p><p>While there had been rumblings of unease from staff for months at the store over the lack of shifts, sick pay and secure contracts, something changed in that moment. “We hadn't broadly discussed unionising with everyone,” explains Parker. “But I saw this, it was like we were in complete solidarity. We needed to unionise immediately.”</p><div data-lexical-signup-form="">
                    <h2><span>Join for free </span></h2>
                    <p><span>This article was published by </span><b><strong>The Londoner</strong></b><span>, a new quality newspaper delivered via email. Four days a week, we send you a carefully chosen story, plus our best recommendations. We prioritise </span><b><strong>quality over quantity</strong></b><span> and everything we send is in your inbox - you don't need to click on any links. To give us a try, join our </span><b><strong>free mailing list </strong></b><span>below. </span></p>
                    
        
        
                    <p><span>Click subscribe and then check your email now to complete your signup.</span></p>
                </div><p>Unionisation at a place like Scarlett Letters might sound a bit strange. It was, after all, billed as a radical left-wing bookshop, not generally the sort of place that should need unions. Even the union was a bit confused. “I was a bit surprised when they contacted us,” explains Matt Collins, an organiser for the United Voices of the World trade union.</p><p>But within a week of “toilet-gate”, every single member of staff at the shop had joined the UVW. They drafted a list of demands, which included being granted sick pay, an end to the use of zero hours contracts<strong><em> </em></strong>and running the shop on co-operative principles (in other words, allowing staff to have a say in management). It was sent to Scarlett at the end of April. An initial meeting between Scarlett and the UVW yielded some agreements, though Scarlett told the meeting she was working six or seven days most weeks, and was earning less per hour than the booksellers, so needed to hire a manager, a move that would mean several of the booksellers would need to be let go.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXe4qzupbRocoxtysLm4mwLcA-k65W2VRjrrykHl5L7A76ILaC0x3ZhvwO1LBY0UpXq4jF1JV-vJIxLa0Z51CgFkXfz-ShSwCrlsDRW18VDwPjq4vehbC0NlrpDqWYkhlX0DfvR-?key=kMtj8Yn4kpj1VvRQ2cmVqw" alt="" loading="lazy" width="602" height="339"><figcaption><span>The now abandoned front of the bookshop. Image by Andrew Kersley</span></figcaption></figure><p>Then there was the tricky business of the bookseller’s call for a co-operative management model. Since opening, the Scarlett Letters hadn’t made a month-by-month profit, but had been kept afloat by savings and a monthly donation of £10,000 from an anonymous “angel investor”. Only Scarlett knew the identity of the investor and she shared a response from the investor that they were considering cutting or even pulling their donation if the shop became a co-operative.</p><p>With the dispute at something of a stalemate, the booksellers reached for the most obvious lever available to them, unleashing a broadside of Instagram posts in Scarlett’s direction. A newly launched account said they were in “open dispute” with their employer over the threats to fire staff and the failure to meet all its demands. “The workers are queer, trans, racialised, disabled, sex workers and students,” it argued. “Their identities have been used to advertise and fundraise for the bookshop as a radical space whilst their voices are not listened to.” As might be expected, there was an outraged response online. Responses on Instagram accused the bookshop of being a “marketing campaign” as well as “colonial”.</p><p>Eight days later, Scarlett fired back on the shop’s Instagram, claiming they had attempted to, or were in the process of meeting, almost all the union’s demands. “Trying to create a space like this in advanced capitalism is extremely difficult,” it read. “The management targeted by this dispute is not a faceless collective of executives in boardrooms. It is one person, who is multiply marginalised, a known member of the community and for the past year has been working for six or seven days a week for the fraction of the salary offered to the booksellers.” It ended with a shocking revelation: the bookshop would now be closing. A meeting with an external HR firm was scheduled for the end of June to discuss the closure and the staff’s redundancies. Scarlett herself said she was surprised at how things escalated and insists she “sent requests to meet again four separate times,” but that “these were ignored or refused”.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcQaeJGZxNUAHmRaV3XG9UANYazwR3_aRFAF2nCWe7LfGGcPhZ9WdMddWiz2tvi-UCzUNiZMFzcQbg9_vDSmu1g9efuZU3CA7saZ61zjXZ1KL_5Lrvhocq3Mrp4vjOg5AfI1ngirA?key=kMtj8Yn4kpj1VvRQ2cmVqw" alt="" loading="lazy" width="602" height="401"><figcaption><span>The bookstore back when it was still open. Image courtesy of Marin Scarlett</span></figcaption></figure><p>And that might have been that. A sad, if fairly run-of-the-mill tale of how an unfortunate toilet clogging thwarted Bethnal Green’s heady dream of a radical bookshop. But that’s not how our story ends, because the soon-to-be jobless booksellers weren’t giving up that easily. In response to Scarlett’s decision, they hatched a plan: they were going to occupy the bookshop.</p><p>Parker is keen to state that they weren’t aiming to make Scarlett hand over the directorship of the shop, rather they wanted her to donate them the stock. Estimating it to be worth £70,000 (Scarlett puts this figure at closer to £10,000) they felt they could use it to start a new bookshop more closely aligned with their political values.&nbsp;</p><p>On the Sunday ahead of the meeting, the staff moved themselves into the crammed two floor bookshop. Parker came with a suitcase, planning to spend four days there. “I didn't have enough money to keep getting the train back and forth from Wimbledon,” they explain. “So I figured that I'll just put myself on the rota for four nights. I was getting a small payment from a porn site that wasn’t a significant amount of money, but enough for the train home.”</p><p>On Monday, they joined the video call on one computer, sitting as a group with the shelves of the store as their backdrop. The meeting would last just a few minutes. They made their demand for the stock to be transferred to them. The official from the HR firm explained, on behalf of Scarlett, that the<strong><em> </em></strong>bookshop’s legal obligations meant the books were “asset-locked” and that there was no legal mechanism to transfer stock to employees for free. As the shop’s director, Scarlett could even be liable and struck off starting a company if she did so. </p><p>But the booksellers didn’t waver. They announced they would be occupying the shop until their demands were met, and abruptly left the call. Next, they hurriedly unfurled a banner announcing the occupation across the storefront. They had the only set of keys, Parker told me, so thought that as long as no-one allowed management to access the building they could stay indefinitely, protected by squatter’s rights.</p><p>By 10pm that night, the three occupiers who volunteered to stay overnight went down to the basement and bedded down, steeling themselves for the weeks of resistance to come. In the end, it would be much shorter than expected.&nbsp;</p><div data-lexical-signup-form="">
                    <h2><span>Join for free </span></h2>
                    <p><span>This article was published by </span><b><strong>The Londoner</strong></b><span>, a new quality newspaper delivered via email. Four days a week, we send you a carefully chosen story, plus our best recommendations. We prioritise </span><b><strong>quality over quantity</strong></b><span> and everything we send is in your inbox - you don't need to click on any links. To give us a try, join our </span><b><strong>free mailing list </strong></b><span>below. </span></p>
                    
        
        
                    <p><span>Click subscribe and then check your email now to complete your signup.</span></p>
                </div><p>What the group hadn’t accounted for, was the foresight of Marin Scarlett. As it happened, on the first day of the occupation she had arranged a locksmith, as well as a team to help her recover the stock. She’d covered every base: even hiring carpenters to remove the bookshelves and bringing along three legal observers to impartially oversee things. At around 4am, they arrived at Scarlett Letters and started drilling.&nbsp;</p><p>Over the next three hours, books were slid into boxes and shelves were unscrewed from walls. At some point in the night, Parker emerged up the stairwell and saw the whole sorry scene play out. “There were people I knew personally, and that just horrified me,” Parker says. </p><figure><img src="https://www.the-londoner.co.uk/content/images/2025/07/AD_4nXfEOTKGm3VN166-4MxvS1ujhRKvzYwdygLYyc1V-dMIRP5192r-NFS1QKTq82oM6DK-OrMqPo_A4BEo9Vwqb7GPdLOWyE9PoLJ_pbUFLNyWIvRhpoVM_yrneF3syb9inxMuJaQY.jpeg" alt="" loading="lazy" width="900" height="982" srcset="https://www.the-londoner.co.uk/content/images/size/w600/2025/07/AD_4nXfEOTKGm3VN166-4MxvS1ujhRKvzYwdygLYyc1V-dMIRP5192r-NFS1QKTq82oM6DK-OrMqPo_A4BEo9Vwqb7GPdLOWyE9PoLJ_pbUFLNyWIvRhpoVM_yrneF3syb9inxMuJaQY.jpeg 600w, https://www.the-londoner.co.uk/content/images/2025/07/AD_4nXfEOTKGm3VN166-4MxvS1ujhRKvzYwdygLYyc1V-dMIRP5192r-NFS1QKTq82oM6DK-OrMqPo_A4BEo9Vwqb7GPdLOWyE9PoLJ_pbUFLNyWIvRhpoVM_yrneF3syb9inxMuJaQY.jpeg 900w" sizes="(min-width: 720px) 720px"><figcaption><span>A sign on the wall of the now empty bookstore announcing its planned reopening. Image by Andrew Kersley</span></figcaption></figure><p>Now, the bookshop is a husk, almost entirely empty apart from the posters announcing future plans to reopen as “The People’s Letters” — a co-operative run by the booksellers that they believe will adhere to more leftist principles than its predecessor. </p><p>Unsurprisingly, Scarlett sees the whole debacle quite differently to the booksellers. She says she “had hoped to work with the union” and that “an improved sick pay was immediately implemented after our first meeting with UVW”. When the talks with the union broke down and things were taken to social media, sales tanked and it became clear that the store couldn’t stay open. The “shop would have stayed open until late July had the booksellers not tried to steal the stock, but their actions forced us to close several weeks early”. When she and a group of friends entered the shop to reclaim the books, they “did not know that anyone was in the property when we entered”.</p><p>Collins has spent 14 years in the trade union movement, but is equally baffled by how things managed to end where they did. “I've never experienced a dispute like it before,” he tells me. “I imagine I never will again.” Here ends the tale of Scarlett Letters, the radical bookshop the capital could have had, only for those dreams to be flushed away.</p>
<div data-layout="immersive">
                    
                        <div>
                            <p><span>Welcome to The Londoner. We’re the capital’s new magazine, delivered entirely by email. Sign up to our </span><a href="https://www.the-londoner.co.uk/young-successful-and-far-right/#/portal/signup/free" rel="noreferrer"><span>mailing list</span></a><span> and get two </span><b><strong>totally free</strong></b><span> editions of The Londoner every week: a Monday briefing, full of everything you need to know about that’s going on in the city; and an in-depth weekend piece like the one you're currently reading.</span></p><p><span>No ads, no gimmicks: just click the button below and get our unique brand of local journalism straight to your inbox. </span></p>
                        </div>
                    
                    
                        <p><a href="#/portal/signup/free">
                            Sign up for free
                        </a>
                        
                    </p></div>



    </div><p>
  <strong>How to comment:</strong><br>
  If you are <i>already a member, </i>
  <a href="#/portal/signin" onmouseover="this.style.textDecoration='underline'" onmouseout="this.style.textDecoration='none'">
     click here to sign in 
  </a> and leave a comment. <br>
  If you aren't a member, 
  <a href="#/portal/signup" onmouseover="this.style.textDecoration='underline'" onmouseout="this.style.textDecoration='none'">
    sign up here 
  </a> to be able to leave a comment. <br>

To add your photo, <a href="https://gravatar.com/" target="_blank" onmouseover="this.style.textDecoration='underline'" onmouseout="this.style.textDecoration='none'">click here to create a profile on Gravatar.</a><br>



  </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I made a public living room and the internet keeps putting weirder stuff in it (196 pts)]]></title>
            <link>https://www.theroom.lol</link>
            <guid>45398005</guid>
            <pubDate>Sat, 27 Sep 2025 17:59:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theroom.lol">https://www.theroom.lol</a>, See on <a href="https://news.ycombinator.com/item?id=45398005">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="app">
    
    <p>0</p>
    <p>IMAGE RESET ON NEXT PROMPT</p>
    <div id="overlay">
        
        <div id="uploadPane">
          <div id="uploadSection">
            <p id="uploadMessage">Upload a base image to start editing this room.</p>
            
            
          </div>
          <div id="intervalGroup">
            <p><label for="resetInterval">Reset after this many prompts:</label></p>
            
          </div>
        </div>
        <form id="form">
          
          
          
        </form>
        
        <div id="indicatorRow">
          <p>0 here</p>
          <div id="indicatorRight">
            <div id="roomSwitch" aria-label="Room selector"><p><a href="https://www.theroom.lol/">Room 1</a><span>/</span><a href="https://www.theroom.lol/overflow">Room 2</a></p></div>
            <p>--</p>
          </div>
        </div>
      </div>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Greenland Is a Beautiful Nightmare (483 pts)]]></title>
            <link>https://matduggan.com/greenland-is-a-beautiful-nightmare/</link>
            <guid>45396754</guid>
            <pubDate>Sat, 27 Sep 2025 15:46:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matduggan.com/greenland-is-a-beautiful-nightmare/">https://matduggan.com/greenland-is-a-beautiful-nightmare/</a>, See on <a href="https://news.ycombinator.com/item?id=45396754">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
        <main id="main-content" role="main">
            
<article>
    

    <figure>
        <img srcset="https://images.unsplash.com/photo-1573995975633-faee0123f31f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDN8fGdyZWVubGFuZHxlbnwwfHx8fDE3NTg5NjI3MzN8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=2000 300w,
                    https://images.unsplash.com/photo-1573995975633-faee0123f31f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDN8fGdyZWVubGFuZHxlbnwwfHx8fDE3NTg5NjI3MzN8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=2000 600w,
                    https://images.unsplash.com/photo-1573995975633-faee0123f31f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDN8fGdyZWVubGFuZHxlbnwwfHx8fDE3NTg5NjI3MzN8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=2000 1000w,
                    https://images.unsplash.com/photo-1573995975633-faee0123f31f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDN8fGdyZWVubGFuZHxlbnwwfHx8fDE3NTg5NjI3MzN8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=2000 2000w" sizes="(max-width: 1000px) 400px, 700px" src="https://images.unsplash.com/photo-1573995975633-faee0123f31f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDN8fGdyZWVubGFuZHxlbnwwfHx8fDE3NTg5NjI3MzN8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=2000" alt="Greenland is a beautiful nightmare" loading="lazy" decoding="async">
            <figcaption><span>Photo by </span><a href="https://unsplash.com/@visitgreenland?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit"><span>Visit Greenland</span></a><span> / </span><a href="https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit"><span>Unsplash</span></a></figcaption>
    </figure>
    
    <div>
            <p>Greenland is a complicated topic here in Denmark. The former colony that is still treated a bit like a colony is something that inspires a lot of emotions. Greenland has been subjected to a lot of unethical experiments by Denmark, from taking their kids to wild experiments in criminal justice. But there is also a genuine pride a lot of people have here for the place and you run into Danes who grew up there more often than I would have guessed. </p><p>When the idea of going to Greenland was introduced to me, I was curious. Having lived in Denmark for awhile, you hear a lot about the former colony and its 55,000 residents. We were invited by a family that my wife was close with growing up and is Danish. They wanted to take their father back to see the place he had spend some time in during his 20s and had left quite an impression. A few drinks in, I said "absolutely let's do it", not realizing we had already committed to going and I had missed the text message chain. </p><p>A few weeks before I went, I realized "I don't know anything about Greenland" and started to watch some YouTube videos. It was about this time when I started to get a pit in my stomach, the "oh god I think I've made a huge mistake" feeling I'm painfully familiar with after a career in tech.  Greenland appeared to have roughly 9 people living there and maybe 5 things to look at. Even professional travel personalities seemed to be scraping the bottom of the barrel. "There's the grocery store again!" they would point out as they slipped down the snowy roads. I couldn't  tell any difference between different towns in the country.</p><p>It reminded me a lot of driving through Indiana. For those not in the US, Indiana is a state in the US famous for being a state one must drive through in order to get somewhere better. If you live in Michigan, a good state and want to go to Illinois, another good state, one must pass through Indiana, a blank state. Because of this little strip here, you often found yourself passing through this place. </p><figure><img src="https://matduggan.com/content/images/2025/09/image-1.png" alt="" loading="lazy" width="470" height="299"></figure><p>Driving through Indiana isn't bad, it's just an empty void. It's like a time machine back to the 90s when people still smoke in restaurants but also there's nothing that sticks out about it. There is nothing distinct about Indiana, it's just a place full of people who got too tired on their way to somewhere better and decided "this is good enough". The difference is that Greenland is very hard to get to, as I was about to learn. </p><p>Finally the day arrived. Me, my wife, daughter, 4 other children and 6 other adults all came to the Copenhagen Airport and held up a gate agent for what felt like an hour to slowly process all of our documents. Meanwhile, I nursed a creeping paranoia that I'd be treated as some sort of American spy, given my government's recent hobby of threatening to purchase entire countries like they're vintage motorcycles on Craigslist.</p><p>The 5 hour flight is uneventful, the children are beautifully behaved and I begin to think "well this seems ok!" like the idiot I am. As I can look down and see the airport, the pilot comes on and informs us that there is too much fog to land safely. <em>Surely fog cannot stop a modern aircraft full of all these dials and screens</em> I think, foolishly. We are informed there is enough fuel to circle the airport for 5 hours to wait for the fog to lift. </p><p>What followed was three hours of flying in lazy circles, like a very expensive, very slow merry-go-round. After the allotted time, we are informed that we must fly to Iceland to refuel and then <em>we will be returning to Denmark</em>. After a total of 15 hours in the air we will be going back to exactly where we started, to do the entire thing again. We were obviously upset at this turn of events, but I noticed the native Greenlandic folks seemed not surprised at this turn of events. As I later learned, this happens <em>all the time</em>. </p><p>The native Greenlanders on board seemed utterly unsurprised by this development, displaying the kind of resigned familiarity that suggested this was Tuesday for them. I began wondering if I could just pretend Iceland was Greenland—surely my family wouldn't notice the difference? But the pilot, apparently reading my mind, announced that no one would be disembarking in Iceland. It felt oddly authoritarian, like being grounded by an airline, as if they knew we'd all just wander off into Reykjavik and call it close enough.</p><p>We crash out in a airport hotel 20 minutes from our apartment after 15 hours in the air and tons of CO2 emissions only to wake up the next day to start again. This time, I notice that all of the people are asking for (and receiving) free beer from the crew that they are stashing in their bags. It turns out soda and beer, really anything that needs to be imported, is pretty expensive in Greenland. The complimentary drinks are there to be kept for later. </p><p>Finally we land. The first thing you notice when you land in Greenland is there are no trees or grass. There is snow and then there is exposed rock. The exterior of the airport is metal but the inside is wood, which is strange because again there are no trees. This would end up being a theme, where buildings representing Denmark were made out of lots of wood, almost to ensure that you understood they weren't from here. We ended up piling all of our stuff into a bus and heading for the hotel in Nuuk. </p><h3 id="nuuk">Nuuk</h3><p>Nuuk is the capital of Greenland and your introduction to the incredible calm of the Greenlandic people. I have never met a less stressed out group of humans in my life. Nobody is really rushing anywhere, it's all pretty quiet and calm. The air is cold and crisp with lots of kids playing outside and just generally enjoying life. </p><figure><img src="https://matduggan.com/content/images/2025/09/image-2.png" alt="" loading="lazy" width="1280" height="840" srcset="https://matduggan.com/content/images/size/w600/2025/09/image-2.png 600w, https://matduggan.com/content/images/size/w1000/2025/09/image-2.png 1000w, https://matduggan.com/content/images/2025/09/image-2.png 1280w" sizes="(min-width: 720px) 720px"></figure><p>The city itself sits in a landscape so dramatically inhospitable it makes the surface of Mars look cozy. Walking through the local mall, half the shops sell gear designed to help you survive what appears to be the apocalypse. Yet somehow, there's traffic. Actual traffic jams in a place where you can walk from one end to the other in twenty minutes. It's like being stuck behind a school bus in your own driveway.</p><figure><img src="https://matduggan.com/content/images/2025/09/image-3.png" alt="" loading="lazy" width="1302" height="1208" srcset="https://matduggan.com/content/images/size/w600/2025/09/image-3.png 600w, https://matduggan.com/content/images/size/w1000/2025/09/image-3.png 1000w, https://matduggan.com/content/images/2025/09/image-3.png 1302w" sizes="(min-width: 720px) 720px"></figure><p>To put this map into some perspective, it is only six kilometers from the sorta furthest tip to the airport. </p><figure><img src="https://matduggan.com/content/images/2025/09/image-4.png" alt="" loading="lazy" width="1368" height="1114" srcset="https://matduggan.com/content/images/size/w600/2025/09/image-4.png 600w, https://matduggan.com/content/images/size/w1000/2025/09/image-4.png 1000w, https://matduggan.com/content/images/2025/09/image-4.png 1368w" sizes="(min-width: 720px) 720px"></figure><p>But riding the bus around Nuuk was a peaceful experience that lets you see pretty much the entire city without needing to book a tour or spend a lot of money. We went to Katuaq, a cultural center with a cafe and a movie theater that was absolutely delicious food. </p><p>But again even riding the bus around it is impossible to escape the feeling that this is a fundamentally hostile to human life place. The sun is bright and during the summer its pretty hot, with my skin feeling like it was starting the burn pretty much the second it was exposed to the light. It's hard to even dress for, with layers of sunscreen, bug spray and then something warm on top if you suddenly got cold. </p><p>The sun, meanwhile, has apparently forgotten how to set, turning our hotel rooms into solar ovens. You wake up in a pool of your own sweat, crack a window for relief, and immediately get hit with air so cold it feels personal. It's like being trapped in a meteorological mood swing.</p><p>So after a night here, we went back to the airport again and flew to our final destination, Ilulissat.</p><h3 id="ilulissat">Ilulissat </h3><figure><img src="https://matduggan.com/content/images/2025/09/FE5AF77D-8082-4A05-B6F4-03B54D9008DE_1_105_c.jpeg" alt="" loading="lazy" width="1024" height="768" srcset="https://matduggan.com/content/images/size/w600/2025/09/FE5AF77D-8082-4A05-B6F4-03B54D9008DE_1_105_c.jpeg 600w, https://matduggan.com/content/images/size/w1000/2025/09/FE5AF77D-8082-4A05-B6F4-03B54D9008DE_1_105_c.jpeg 1000w, https://matduggan.com/content/images/2025/09/FE5AF77D-8082-4A05-B6F4-03B54D9008DE_1_105_c.jpeg 1024w" sizes="(min-width: 720px) 720px"><figcaption><span>My new favorite airport</span></figcaption></figure><p>The flight to our final destination revealed Greenland's true nature: endless, empty hills stretching toward infinity, punctuated by ice formations that look like nature's sculpture garden.</p><figure><img src="https://matduggan.com/content/images/2025/09/F321BAFB-F30A-4D36-A2C6-E85FB3CA3BEB_1_105_c.jpeg" alt="" loading="lazy" width="1024" height="768" srcset="https://matduggan.com/content/images/size/w600/2025/09/F321BAFB-F30A-4D36-A2C6-E85FB3CA3BEB_1_105_c.jpeg 600w, https://matduggan.com/content/images/size/w1000/2025/09/F321BAFB-F30A-4D36-A2C6-E85FB3CA3BEB_1_105_c.jpeg 1000w, https://matduggan.com/content/images/2025/09/F321BAFB-F30A-4D36-A2C6-E85FB3CA3BEB_1_105_c.jpeg 1024w" sizes="(min-width: 720px) 720px"></figure><p>Landing in Ilulissat felt like victory—we'd made it to the actual destination, not just another waypoint in our Arctic odyssey. Walking through the tiny airport, past Danish military recruitment posters (apparently someone, somewhere, thought this place needed defending), I felt genuinely optimistic for the first time in days.</p><p>Well you can sleep easy Danish military, because Ilulissat is completely protected from invasion. The second I stepped outside I was set upon by a flood of mosquitos like I have never experienced before. I have been to the jungles of Vietnam, the swamps of Florida and the Canadian countryside. This was beyond anything I've ever experienced. </p><p>There are bugs in my mouth, ears, eyes and nose almost immediately. The photo below is not me being dramatic, it is actually what is required to keep them off of me. </p><figure><img src="https://matduggan.com/content/images/2025/09/46C37475-5A8C-4702-BC2E-B6B89A12F9D7_4_5005_c.jpeg" alt="" loading="lazy" width="360" height="480"></figure><p>In fact what you need to purchase in order to walk around this area at all are basically bug nets for your face. They're effectively plastic mesh bags that you put on. </p><figure><img src="https://matduggan.com/content/images/2025/09/F3FD1061-F86E-4545-937B-87F8B7FA85E3_1_105_c.jpeg" alt="" loading="lazy" width="768" height="1024" srcset="https://matduggan.com/content/images/size/w600/2025/09/F3FD1061-F86E-4545-937B-87F8B7FA85E3_1_105_c.jpeg 600w, https://matduggan.com/content/images/2025/09/F3FD1061-F86E-4545-937B-87F8B7FA85E3_1_105_c.jpeg 768w" sizes="(min-width: 720px) 720px"></figure><h3 id="the-dogs">The Dogs</h3><p>Our hotel, charming in that "remote Arctic outpost" way, sat adjacent to what I can only describe as a canine correctional facility. Dozens of sled dogs were chained to rocks like some sort of prehistoric parking lot, each with a tiny house they could retreat to when the existential weight of their circumstances became too much.</p><p>Now, I'd always imagined sled dogs living their best life—running through snow, tongues lolling, living the Disney version of Arctic life. I'd never really considered their downtime, assuming they frolicked in meadows or something equally wholesome. The reality was more "minimum security prison with a view."</p><p>The dogs are visited roughly twice a day by the person who owns and feeds them, which was quite the party for the dogs that lost their minds whenever the car pulled up. Soon the kids really looked forward to dog feeding time. The fish scrapes the dogs lived on came out of a chest freezer that was left exposed up on the rock face without electricity and you could smell it from 50 yards away when it opened. </p><p>During one such performance, a fellow parent leaned over and whispered with the casual tone of someone commenting on the weather, "I think that one is dead." Before I could process this information, the frozen canine was unceremoniously launched over a small cliff like a furry discus. A second doggy popsicle followed shortly after, right in front of our assembled children, who watched with the kind of wide-eyed fascination usually reserved for magic shows.</p><figure><img src="https://matduggan.com/content/images/2025/09/IMG_0069-2.jpeg" alt="" loading="lazy" width="2000" height="2667" srcset="https://matduggan.com/content/images/size/w600/2025/09/IMG_0069-2.jpeg 600w, https://matduggan.com/content/images/size/w1000/2025/09/IMG_0069-2.jpeg 1000w, https://matduggan.com/content/images/size/w1600/2025/09/IMG_0069-2.jpeg 1600w, https://matduggan.com/content/images/size/w2400/2025/09/IMG_0069-2.jpeg 2400w" sizes="(min-width: 720px) 720px"></figure><p>We stopped making dog feeding time a group activity after that and had to distract the kids from ravens flying away with tufts of dog fur. </p><h3 id="whales-taste-like-seaweed">Whales taste like seaweed </h3><p>Obviously a big part of Greenland is the nature, specifically the icebergs. Icebergs are incredible and during the week we spend up there, I enjoyed watching them every morning. It's like watching a mountain slowly moving while you sit still. The visual contrast of the ice and the exposed stone is beautiful and peaceful. </p><figure><img src="https://matduggan.com/content/images/2025/09/IMG_1096-copy.jpg" alt="" loading="lazy" width="2000" height="1500" srcset="https://matduggan.com/content/images/size/w600/2025/09/IMG_1096-copy.jpg 600w, https://matduggan.com/content/images/size/w1000/2025/09/IMG_1096-copy.jpg 1000w, https://matduggan.com/content/images/size/w1600/2025/09/IMG_1096-copy.jpg 1600w, https://matduggan.com/content/images/size/w2400/2025/09/IMG_1096-copy.jpg 2400w" sizes="(min-width: 720px) 720px"></figure><p>Finding our tour operator proved to be an exercise in small-town efficiency. The man who gave me directions was the same person who picked us up from the airport, who was also our tour guide, who probably doubled as the mayor and local meteorologist. It was like a one-man civic operation disguised as multiple businesses—the ultimate small-town gig economy.</p><p>The sea around Greenland is calmer than anything I've ever been on before, perfectly calm and serene. All around us whales emerged, thrilling my daughter. However the biggest hit of the entire tour, maybe the entire trip, was a member of the crew who handed each of the kids a giant rock of glacier ice to eat. I had to pull my daughter away to observe the natural beauty as she ate glacier ice like it was ice cream. "LOOK AT MY ICE" she was yelling as they slipped and slid around the deck of this boat. </p><p>So if you've ever wonder "what is a glacier", let me tell you. Greenland has a lot of ice and it pushes out from the land that is covers into the sea. When that happens, a lot of it breaks off. This sounds more exciting than it is. On TV in 4K it looks incredible, giant mountains of ice falling into the ocean. Honestly you can go read the same thing I did <a href="https://science.howstuffworks.com/environmental/earth/geophysics/glacier.htm" rel="noreferrer">here</a>.</p><p>However that doesn't happen very often. So in order for us tourists to be able to see anything, we had to go to a very productive glacier. This means there are constantly small chunks breaking off and falling into the sea. Practically though, it kinda looks like you are a boat in a slushee. It's beautiful and something to see, but also depressing to see along the rock face how much more ice there used to be. </p><p>Back in town, we hopped on the "bus". Now the bus here is clearly a retrofitted party van, complete with blue LED lights. The payment system is zip tied to a desk chair that is, itself, wedged in the front. However the bus works well and does get you around. The confusing part is that you will, once again, sometimes encounter a lot of traffic. People are driving pretty quickly and really seem to have somewhere to go. You also see a lot of fancy cars parked outside of houses here. </p><p>Which begs a pretty basic question. If there was almost nowhere to drive to in Nuuk, where in the <em>hell are these people driving</em>. The distance between the end of the road and the beginning of the road is less than 6 km. Also the process to make a road here is beyond anything you've ever seen. Everything requires a giant pile of explosives. </p><figure><img src="https://matduggan.com/content/images/2025/09/image-5.png" alt="" loading="lazy" width="575" height="561"></figure><p>Where did these vehicles even come from? Why does one ship a BMW to a place accessible only by plane and boat? More importantly, where was everyone going with such determination? It was like watching a very expensive version of bumper cars, except everyone was committed to the illusion that they had somewhere important to be. Everyone had dings and scrapes like crashes were common. </p><h3 id="grocery-store-from-the-sea">Grocery Store from the Sea</h3><p>Anyway, as I dodged speeding cars filled with people heading nowhere, I decided to hop off the bus and head to the grocery store. Inside was less a store and more the idea of a store. There was a lot of alcohol, chips, candy and shelf-stable foods, which all makes sense to me. What was strange was there wasn't a lot else, including meat. Locals couldn't be eating at the local restaurants, where the prices were as high as Berlin or Copenhagen for food. So what were they eating?</p><p>When I asked one of my bus drivers, he told me that it was pretty unusual to buy meat. They purchased a lot of whale and seal meat. I had sorta heard this before, but when we stopped the bus he pointed out a group of men hauling guns out into a small boat to go shoot seals. The guns were held together with a surprising amount of duct tape, which is not something I associate with the wild. </p><p>I had assumed, based on my casual reading of the news, that we were <em>mostly</em> done killing whales. As it turns out, I was wrong. They eat a lot of whale and it is, in fact, not hard to find. If you are curious, whale does not taste fishy. It tastes a little bit like if you cooked reindeer in a pot of seaweed. I wouldn't go out of your way for it, but it's not terrible. </p><p>The argument I've always heard for why people still kill whales is because it's part of their culture and also because it's an important source of protein. When you hear the phrase "part of their culture" I always imagined like traditional boats going out with spears. What I didn't imagine was industrial fishing boats and an industrial crane that lifts the dead whale out of the water for "processing". Some of the illusion is broken when your boat tour guide points out the metal warehouse with the word "whale" on the side. "Yeah the water here was red with blood for a week" the guide said, counting the cigarettes left in a pack he had. </p><h3 id="should-you-go-to-greenland">Should you go to Greenland?</h3><p>It's a wild place unlike anywhere I've ever been. It is the closest I have ever felt to living a sci-fi type experience. The people of Greenland are amazing, tough, calm and kind. I have nothing but positive experiences to recount from the many people I met there, Danish and Greenlandic, who patiently sat through my millions of questions. </p><p>However it is, by far, the least hospitable to human life place I've ever been to. The folks who live there have adapted to the situation in, frankly, genius ways. If that's your idea of a good time, Greenland is perfect for you. Maybe don't get emotionally attached to the sled dogs though. Or the whales. </p>
        </div>



</article>

        </main>
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI model trapped in a Raspberry Pi (121 pts)]]></title>
            <link>https://blog.adafruit.com/2025/09/26/ai-model-trapped-in-raspberry-pi-piday-raspberrypi/</link>
            <guid>45396624</guid>
            <pubDate>Sat, 27 Sep 2025 15:34:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.adafruit.com/2025/09/26/ai-model-trapped-in-raspberry-pi-piday-raspberrypi/">https://blog.adafruit.com/2025/09/26/ai-model-trapped-in-raspberry-pi-piday-raspberrypi/</a>, See on <a href="https://news.ycombinator.com/item?id=45396624">Hacker News</a></p>
Couldn't get https://blog.adafruit.com/2025/09/26/ai-model-trapped-in-raspberry-pi-piday-raspberrypi/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[A WebGL game where you deliver messages on a tiny planet (1151 pts)]]></title>
            <link>https://messenger.abeto.co/</link>
            <guid>45396441</guid>
            <pubDate>Sat, 27 Sep 2025 15:17:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://messenger.abeto.co/">https://messenger.abeto.co/</a>, See on <a href="https://news.ycombinator.com/item?id=45396441">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Scientists say X has lost its professional edge and Bluesky is taking its place (226 pts)]]></title>
            <link>https://www.psypost.org/scientists-say-x-formerly-twitter-has-lost-its-professional-edge-and-bluesky-is-taking-its-place/</link>
            <guid>45396377</guid>
            <pubDate>Sat, 27 Sep 2025 15:10:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.psypost.org/scientists-say-x-formerly-twitter-has-lost-its-professional-edge-and-bluesky-is-taking-its-place/">https://www.psypost.org/scientists-say-x-formerly-twitter-has-lost-its-professional-edge-and-bluesky-is-taking-its-place/</a>, See on <a href="https://news.ycombinator.com/item?id=45396377">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>A new study published in <em><a href="https://doi.org/10.1093/icb/icaf127" target="_blank" rel="noopener">Integrative and Comparative Biology</a></em> suggests that scientists are leaving X (formerly known as Twitter) in significant numbers due to its declining professional value. The survey of over 800 researchers and science communicators indicates that many now find Bluesky to be a more effective platform for networking, outreach, and staying updated on research. The findings suggest a significant shift in how scientists interact online, with Bluesky emerging as a preferred space for professional engagement.</p><p>Twitter, once considered the central gathering place for scientists on social media, has changed dramatically in recent years. The platform, now officially called “X,” was purchased by Elon Musk in late 2022. Since then, changes to how the platform is moderated and how content appears in users’ feeds have raised concerns among many users, especially academics.</p><p>Reports have pointed to a rise in misinformation, conspiracy theories, and harassment, particularly directed at minority groups. These shifts appear to have made the platform less welcoming and less useful for professional tasks. As Twitter’s character evolved, so too did the willingness of researchers to remain active on the platform.</p><p>In its place, Bluesky has gained attention as a new space for academic interaction. Although other platforms like Threads and Mastodon have also positioned themselves as alternatives, Bluesky appears to be the primary destination for scientists migrating from X. Against this backdrop, researchers set out to document whether scientists were truly abandoning X and whether Bluesky was filling the gap.</p><p>“I am a scholar of public understanding (and misunderstanding) of science and the environment, and have long been fascinated by where people learn things about nature. Social media has become one of the leading sources of information about the world, but the social media landscape is changing, and I wanted to see how my professional colleagues were adapting,” said study author David Shiffman, a marine biologist and public science engagement specialist based in Washington, D.C, and author of <em><a href="https://www.press.jhu.edu/books/title/12267/why-sharks-matter" target="_blank" rel="noopener">Why Sharks Matter</a></em>.</p><p>To investigate these questions, researchers distributed a survey to professional scientists, science communicators, and educators who had used both X and Bluesky for work-related purposes. In total, 813 individuals participated. The survey asked when participants joined each platform, how they used them, and how their experiences had changed over time.</p><p>The responses showed that X had once served a wide range of professional purposes. Nearly all respondents had used it to learn about developments in their fields, and most had relied on it for networking and public outreach. Many also used it for job postings, research promotion, and casual professional conversation.</p><p>However, those same users reported a sharp decline in the usefulness of X. Roughly three-quarters said the platform was now “much less useful” for networking and science communication. Two-thirds said it was less helpful for keeping up with developments in their field.</p><p>The vast majority described their experience on Twitter as increasingly unpleasant, citing irrelevant content, ads, spam, extremist posts, and a loss of meaningful engagement. Some described ethical discomfort with continuing to use a platform that appeared to tolerate, or even amplify, harassment and misinformation.</p><p>In terms of actual usage patterns, only 11 percent of respondents said they still actively use X. Nearly 40 percent had deleted their accounts entirely. Almost half said they still had accounts but rarely used them.</p><p>In contrast, users reported that Bluesky was meeting many of their professional needs. Like Twitter in its earlier days, Bluesky offered a space for learning, networking, and public engagement. Over 94 percent said they used Bluesky to stay informed about research in their field, and nearly 88 percent used it for professional networking. A majority said the new platform was more useful than X for these purposes.</p><p>The researchers also explored why people chose to try Bluesky. Nearly half said they were invited by a colleague or saw others in the science community making the shift. Many viewed Bluesky’s features — such as stronger moderation tools, less algorithmic interference, and more control over what appears in their feed — as more aligned with their professional goals.</p><p>Others said they were simply trying to avoid X’s drawbacks. More than a quarter said they moved to Bluesky because of what they perceived as a rise in extremism on X, and many explicitly named Elon Musk as a reason for their departure.</p><p>“The degree to which the scientific community’s experiences mirrored my own was surprising,” Shiffman told PsyPost. “I knew that for me, Twitter had become unusable, but the extent to which hundreds of surveyed experts strongly agreed with me on almost every point was surprising. You rarely see that kind of strong agreement in surveys.”</p><p>These results provide new evidence to support what other studies and media reports have been suggesting for some time: that X’s role as a hub for academic communication is fading. A previous study <a href="https://www.psypost.org/elon-musks-twitter-takeover-triggered-academic-exodus-study-suggests/" target="_blank" rel="noopener">documented a noticeable drop in academic activity on X</a> after Musk’s acquisition. That research tracked over 15,000 academic accounts and found a significant reduction in tweets, especially original posts and quote tweets, starting in November 2022. Verified users — typically more established academics — were especially likely to reduce their engagement.</p><p>Both studies indicate that changes to how X is managed and moderated have had measurable effects on academic use of the platform. The new survey adds further weight to this idea by showing that scientists are not just using X less — they are actively replacing it with another platform.</p><p>What makes the current study distinctive is its focus on Bluesky as the replacement. While earlier data showed general declines in X use, this survey points to a specific alternative that scientists are embracing. And unlike the earlier study, which focused on activity levels, the new survey captures users’ motivations and perceptions, offering a more detailed view of what is driving this migration.</p><p>“For many years, Twitter was the leading platform used by academics for a wide variety of purposes, including public education about science,” Shiffman explained. “I was a Twitter power-user and evangelist for a decade, and I trained thousands of scientists how to use the platform. Changes to the platform made by Elon Musk, including changing the algorithm to promote extremist views and changes to harassment policy, have made Twitter almost unusable for professional purposes, and academics are abandoning Twitter in droves. Fortunately, alternatives exist, and I, along with many other academics, prefer Bluesky of the available alternatives.”</p><p>The authors note that the survey was limited to users who had already made the switch from X to Bluesky, or were using both platforms. This means it does not account for those who may have stopped using social media altogether or migrated to other platforms. Because the survey was shared primarily through one author’s network, it may reflect the perspectives of those within particular academic communities more than others.</p><p>Another open question concerns whether Bluesky can support the same level of diversity that once defined the science community on X. Movements like Black Birders Week and Queer in STEM gained traction through Twitter’s large, visible networks. It remains unclear whether Bluesky can foster similar grassroots engagement. The authors suggest this should be the focus of future research, particularly if scientists want to ensure that new digital spaces remain inclusive.</p><p>There is also the issue of platform longevity. Whether Bluesky can maintain momentum over time — or whether users will need to shift again — is uncertain. But for now, it appears to offer what many researchers were missing from X: a sense of community, professional utility, and control over their online interactions.</p><p>The study, “<a href="https://doi.org/10.1093/icb/icaf127" target="_blank" rel="noopener">Scientists no Longer Find Twitter Professionally Useful, and have Switched to Bluesky</a>,” was authored by David S. Shiffman and Julia Wester.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Role of Amazon fires in the record atmospheric CO₂ growth in 2024 (151 pts)]]></title>
            <link>https://essopenarchive.org/doi/full/10.22541/essoar.175874118.83695562/v1</link>
            <guid>45396284</guid>
            <pubDate>Sat, 27 Sep 2025 15:02:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://essopenarchive.org/doi/full/10.22541/essoar.175874118.83695562/v1">https://essopenarchive.org/doi/full/10.22541/essoar.175874118.83695562/v1</a>, See on <a href="https://news.ycombinator.com/item?id=45396284">Hacker News</a></p>
Couldn't get https://essopenarchive.org/doi/full/10.22541/essoar.175874118.83695562/v1: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[SSH3: Faster and rich secure shell using HTTP/3 (382 pts)]]></title>
            <link>https://github.com/francoismichel/ssh3</link>
            <guid>45395991</guid>
            <pubDate>Sat, 27 Sep 2025 14:27:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/francoismichel/ssh3">https://github.com/francoismichel/ssh3</a>, See on <a href="https://news.ycombinator.com/item?id=45395991">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p><a target="_blank" rel="noopener noreferrer" href="https://github.com/francoismichel/ssh3/blob/main/resources/figures/ssh3.png"><img src="https://github.com/francoismichel/ssh3/raw/main/resources/figures/ssh3.png"></a>
</p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">SSH3 is probably going to change its name. It is still the SSH Connection Protocol (RFC4254) running on top of HTTP/3 Extended connect, but the required changes are heavy and
too distant from the philosophy of popular SSH implementations to be considered for integration. The <a href="https://datatracker.ietf.org/doc/draft-michel-remote-terminal-http3/" rel="nofollow">specification draft</a> has already been renamed ("Remote Terminals over HTTP/3"),
but we need some time to come up with a nice permanent name.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">SSH3: faster and rich secure shell using HTTP/3</h2><a id="user-content-ssh3-faster-and-rich-secure-shell-using-http3" aria-label="Permalink: SSH3: faster and rich secure shell using HTTP/3" href="#ssh3-faster-and-rich-secure-shell-using-http3"></a></p>
<p dir="auto">SSH3 is a complete revisit of the SSH
protocol, mapping its semantics on top of the HTTP mechanisms. It comes from our research work and we (researchers) recently proposed it as an <a href="https://www.ietf.org/how/ids/" rel="nofollow">Internet-Draft</a> (<a href="https://datatracker.ietf.org/doc/draft-michel-remote-terminal-http3/" rel="nofollow">draft-michel-remote-terminal-http3-00</a>).</p>
<p dir="auto">In a nutshell, SSH3 uses <a href="https://datatracker.ietf.org/doc/html/rfc9000" rel="nofollow">QUIC</a>+<a href="https://datatracker.ietf.org/doc/html/rfc8446" rel="nofollow">TLS1.3</a> for
secure channel establishment and the <a href="https://www.rfc-editor.org/rfc/rfc9110.html#name-authorization" rel="nofollow">HTTP Authorization</a> mechanisms for user authentication.
Among others, SSH3 allows the following improvements:</p>
<ul dir="auto">
<li>Significantly faster session establishment</li>
<li>New HTTP authentication methods such as <a href="https://datatracker.ietf.org/doc/html/rfc6749" rel="nofollow">OAuth 2.0</a> and <a href="https://openid.net/specs/openid-connect-core-1_0.html" rel="nofollow">OpenID Connect</a> in addition to classical SSH authentication</li>
<li>Robustness to port scanning attacks: your SSH3 server can be made <strong>invisible</strong> to other Internet users</li>
<li>UDP port forwarding in addition to classical TCP port forwarding</li>
<li>All the features allowed by the modern QUIC protocol: including connection migration (soon) and multipath connections</li>
</ul>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">Quickly want to get started ? Checkout how to <a href="#installing-ssh3">install SSH3</a>. You will learn to <a href="#deploying-an-ssh3-server">setup an SSH3 server</a> and <a href="#using-the-ssh3-client">use the SSH3 client</a>.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">⚡ SSH3 is faster</h2><a id="user-content--ssh3-is-faster" aria-label="Permalink: ⚡ SSH3 is faster" href="#-ssh3-is-faster"></a></p>
<p dir="auto">Faster for session establishment, not throughput ! SSH3 offers a significantly faster session establishment than SSHv2. Establishing a new session with SSHv2 can take 5 to 7 network round-trip times, which can easily be noticed by the user. SSH3 only needs 3 round-trip times. The keystroke latency in a running session is unchanged.</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/francoismichel/ssh3/blob/main/resources/figures/ssh3_100ms_rtt.gif"><img src="https://github.com/francoismichel/ssh3/raw/main/resources/figures/ssh3_100ms_rtt.gif" data-animated-image=""></a>
<i>SSH3 (top) VS SSHv2 (bottom) session establishement with a 100ms ping towards the server.</i>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔒 SSH3 security</h2><a id="user-content--ssh3-security" aria-label="Permalink: 🔒 SSH3 security" href="#-ssh3-security"></a></p>
<p dir="auto">While SSHv2 defines its own protocols for user authentication and secure channel establishment, SSH3 relies on the robust and time-tested mechanisms of TLS 1.3, QUIC and HTTP. These protocols are already extensively used to secure security-critical applications on the Internet such as e-commerce and Internet banking.</p>
<p dir="auto">SSH3 already implements the common password-based and public-key (RSA and EdDSA/ed25519) authentication methods. It also supports new authentication methods such as OAuth 2.0 and allows logging in to your servers using your Google/Microsoft/Github accounts.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">🧪 SSH3 is still experimental</h3><a id="user-content--ssh3-is-still-experimental" aria-label="Permalink: 🧪 SSH3 is still experimental" href="#-ssh3-is-still-experimental"></a></p>
<p dir="auto">While SSH3 shows promise for faster session establishment, it is still at an early proof-of-concept stage. As with any new complex protocol, <strong>expert cryptographic review over an extended timeframe is required before reasonable security conclusions can be made</strong>.</p>
<p dir="auto">We are developing SSH3 as an open source project to facilitate community feedback and analysis. However, we <strong>cannot yet endorse its appropriateness for production systems</strong> without further peer review. Please collaborate with us if you have relevant expertise!</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">🥷 Do not deploy the SSH3 server on your production servers for now</h3><a id="user-content--do-not-deploy-the-ssh3-server-on-your-production-servers-for-now" aria-label="Permalink: 🥷 Do not deploy the SSH3 server on your production servers for now" href="#-do-not-deploy-the-ssh3-server-on-your-production-servers-for-now"></a></p>
<p dir="auto">Given the current prototype state, we advise <em>testing SSH3 in sandboxed environments or private networks</em>. Be aware that making experimental servers directly Internet-accessible could introduce risk before thorough security vetting.</p>
<p dir="auto">While <a href="#-your-ssh3-public-server-can-be-hidden">hiding</a> servers behind secret paths has potential benefits, it does not negate the need for rigorous vulnerability analysis before entering production. We are excited by SSH3's future possibilities but encourage additional scrutiny first.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🥷 Your SSH3 public server can be hidden</h2><a id="user-content--your-ssh3-public-server-can-be-hidden" aria-label="Permalink: 🥷 Your SSH3 public server can be hidden" href="#-your-ssh3-public-server-can-be-hidden"></a></p>
<p dir="auto">Using SSH3, you can avoid the usual stress of scanning and dictionary attacks against your SSH server. Similarly to your secret Google Drive documents, your SSH3 server can be hidden behind a secret link and only answer to authentication attempts that made an HTTP request to this specific link, like the following:</p>
<div data-snippet-clipboard-copy-content="ssh3-server -bind 192.0.2.0:443 -url-path <my-long-secret>"><pre><code>ssh3-server -bind 192.0.2.0:443 -url-path &lt;my-long-secret&gt;
</code></pre></div>
<p dir="auto">By replacing <code>&lt;my-long-secret&gt;</code> by, let's say, the random value <code>M3MzkxYWMxMjYxMjc5YzJkODZiMTAyMjU</code>, your SSH3 server will only answer to SSH3 connection attempts made to the URL <code>https://192.0.2.0:443/M3MzkxYWMxMjYxMjc5YzJkODZiMTAyMjU</code> and it will respond a <code>404 Not Found</code> to other requests. Attackers and crawlers on the Internet can therefore not detect the presence of your SSH3 server. They will only see a simple web server answering 404 status codes to every request.</p>
<p dir="auto"><strong>NOTE WELL</strong>: placing your SSH3 server behind a secret URL may reduce the impact of scanning attacks but will and must <em>never</em> replace classical authentication mechanisms. The secret link should only be used to avoid your host to be discovered. Knowing the secret URL should not grant someone access to your server. Use the classical authentication mechanisms described above to protect your server.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">💐 SSH3 is already feature-rich</h2><a id="user-content--ssh3-is-already-feature-rich" aria-label="Permalink: 💐 SSH3 is already feature-rich" href="#-ssh3-is-already-feature-rich"></a></p>
<p dir="auto">SSH3 provides new feature that could not be provided by the SSHv2 protocol.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Brand new features</h3><a id="user-content-brand-new-features" aria-label="Permalink: Brand new features" href="#brand-new-features"></a></p>
<ul dir="auto">
<li><strong>UDP port forwarding</strong>: you can now access your QUIC, DNS, RTP or any UDP-based server that are only reachable from your SSH3 host.
UDP packets are forwarded using QUIC datagrams.</li>
<li><strong>X.509 certificates</strong>: you can now use your classical HTTPS certificates to authenticate your SSH3 server. This mechanism is more secure than the classical SSHv2 host key mechanism. Certificates can be obtained easily using LetsEncrypt for instance.</li>
<li><strong>Hiding</strong> your server behind a secret link.</li>
<li><strong>Keyless</strong> secure user authentication using <strong>OpenID Connect</strong>. You can connect to your SSH3 server using the SSO of your company or your Google/Github account, and you don't need to copy the public keys of your users anymore.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Famous OpenSSH features implemented</h3><a id="user-content-famous-openssh-features-implemented" aria-label="Permalink: Famous OpenSSH features implemented" href="#famous-openssh-features-implemented"></a></p>
<p dir="auto">This SSH3 implementation already provides many of the popular features of OpenSSH, so if you are used to OpenSSH, the process of adopting SSH3 will be smooth. Here is a list of some OpenSSH features that SSH3 also implements:</p>
<ul dir="auto">
<li>Parses <code>~/.ssh/authorized_keys</code> on the server</li>
<li>Certificate-based server authentication</li>
<li><code>known_hosts</code> mechanism when X.509 certificates are not used.</li>
<li>Automatically using the <code>ssh-agent</code> for public key authentication</li>
<li>SSH agent forwarding to use your local keys on your remote server</li>
<li>Direct TCP port forwarding (reverse port forwarding will be implemented in the future)</li>
<li>Proxy jump (see the <code>-proxy-jump</code> parameter). If A is an SSH3 client and B and C are both SSH3 servers, you can connect from A to C using B as a gateway/proxy. The proxy uses UDP forwarding to forward the QUIC packets from A to C, so B cannot decrypt the traffic A&lt;-&gt;C SSH3 traffic.</li>
<li>Parses <code>~/.ssh/config</code> on the client and handles the <code>Hostname</code>, <code>User</code>, <code>Port</code> and <code>IdentityFile</code> config options (the other options are currently ignored). Also parses a new <code>UDPProxyJump</code> that behaves similarly to OpenSSH's <code>ProxyJump</code>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🙏 Community support</h2><a id="user-content--community-support" aria-label="Permalink: 🙏 Community support" href="#-community-support"></a></p>
<p dir="auto">Help us progress SSH3 responsibly! We welcome capable security researchers to review our codebase and provide feedback. Please also connect us with relevant standards bodies to potentially advance SSH3 through the formal IETF/IRTF processes over time.</p>
<p dir="auto">With collaborative assistance, we hope to iteratively improve SSH3 towards safe production readiness. But we cannot credibly make definitive security claims without evidence of extensive expert cryptographic review and adoption by respected security authorities. Let's work together to realize SSH3's possibilities!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installing SSH3</h2><a id="user-content-installing-ssh3" aria-label="Permalink: Installing SSH3" href="#installing-ssh3"></a></p>
<p dir="auto">You can either download the last <a href="https://github.com/francoismichel/ssh3/releases">release binaries</a>,
<a href="#installing-ssh3-and-ssh3-server-using-go-install">install it using <code>go install</code></a> or generate these binaries yourself by compiling the code from source.</p>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">SSH3 is still experimental and is the fruit of a research work. If you are afraid of deploying publicly a new SSH3 server, you can use the
<a href="#-your-ssh3-public-server-can-be-hidden">secret path</a> feature of SSH3 to hide it behing a secret URL.</p>
</div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installing ssh3 and ssh3-server using Go install</h3><a id="user-content-installing-ssh3-and-ssh3-server-using-go-install" aria-label="Permalink: Installing ssh3 and ssh3-server using Go install" href="#installing-ssh3-and-ssh3-server-using-go-install"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="go install github.com/francoismichel/ssh3/cmd/...@latest"><pre>go install github.com/francoismichel/ssh3/cmd/...@latest</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Compiling SSH3 from source</h3><a id="user-content-compiling-ssh3-from-source" aria-label="Permalink: Compiling SSH3 from source" href="#compiling-ssh3-from-source"></a></p>
<p dir="auto">You need a recent <a href="https://go.dev/dl/" rel="nofollow">Golang</a> version to do this.
Downloading the source code and compiling the binaries can be done with the following steps:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/francoismichel/ssh3    # clone the repo
cd ssh3
go build -o ssh3 cmd/ssh3/main.go                        # build the client
CGO_ENABLED=1 go build -o ssh3-server cmd/ssh3-server/main.go   # build the server, requires having gcc installed"><pre>git clone https://github.com/francoismichel/ssh3    <span><span>#</span> clone the repo</span>
<span>cd</span> ssh3
go build -o ssh3 cmd/ssh3/main.go                        <span><span>#</span> build the client</span>
CGO_ENABLED=1 go build -o ssh3-server cmd/ssh3-server/main.go   <span><span>#</span> build the server, requires having gcc installed</span></pre></div>
<p dir="auto">If you have root/sudo privileges and you want to make ssh3 accessible to all you users,
you can then directly copy the binaries to <code>/usr/bin</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cp ssh3 /usr/bin/ &amp;&amp; cp ssh3-server /usr/bin"><pre>cp ssh3 /usr/bin/ <span>&amp;&amp;</span> cp ssh3-server /usr/bin</pre></div>
<p dir="auto">Otherwise, you can simply add the executables to your <code>PATH</code> environment variable by adding
the following line at the end of your <code>.bashrc</code> or equivalent:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export PATH=$PATH:/path/to/the/ssh3/directory"><pre><span>export</span> PATH=<span>$PATH</span>:/path/to/the/ssh3/directory</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Deploying an SSH3 server</h3><a id="user-content-deploying-an-ssh3-server" aria-label="Permalink: Deploying an SSH3 server" href="#deploying-an-ssh3-server"></a></p>
<p dir="auto">Before connecting to your host, you need to deploy an SSH3 server on it. There is currently
no SSH3 daemon, so right now, you will have to run the <code>ssh3-server</code> executable in background
using <code>screen</code> or a similar utility.</p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">As SSH3 runs on top of HTTP/3, a server needs an X.509 certificate and its corresponding private key. Public certificates can be generated automatically for your public domain name through Let's Encrypt using the <code>-generate-public-cert</code> command-line argument on the server. If you do not want to generate a certificate signed by a real certificate authority or if you don't have any public domain name, you can generate a self-signed one using the <code>-generate-selfsigned-cert</code> command-line argument. Self-signed certificates provide you with similar security guarantees to SSHv2's host keys mechanism, with the same security issue: you may be vulnerable to machine-in-the-middle attacks during your first connection to your server. Using real certificates signed by public certificate authorities such as Let's Encrypt avoids this issue.</p>
</div>
<p dir="auto">Here is the usage of the <code>ssh3-server</code> executable:</p>
<div data-snippet-clipboard-copy-content="Usage of ./ssh3-server:
  -bind string
        the address:port pair to listen to, e.g. 0.0.0.0:443 (default &quot;[::]:443&quot;)
  -cert string
        the filename of the server certificate (or fullchain) (default &quot;./cert.pem&quot;)
  -key string
        the filename of the certificate private key (default &quot;./priv.key&quot;)
  -enable-password-login
        if set, enable password authentication (disabled by default)
  -generate-public-cert value
        Automatically produce and use a valid public certificate usingLet's Encrypt for the provided domain name. The flag can be used several times to generate several certificates.If certificates have already been generated previously using this flag, they will simply be reused without being regenerated. The public certificates are automatically renewed as long as the server is running. Automatically-generated IP public certificates are not available yet.
  -generate-selfsigned-cert
        if set, generates a self-self-signed cerificate and key that will be stored at the paths indicated by the -cert and -key args (they must not already exist)
  -url-path string
        the secret URL path on which the ssh3 server listens (default &quot;/ssh3-term&quot;)
  -v    verbose mode, if set
  -version
        if set, displays the software version on standard output and exit"><pre><code>Usage of ./ssh3-server:
  -bind string
        the address:port pair to listen to, e.g. 0.0.0.0:443 (default "[::]:443")
  -cert string
        the filename of the server certificate (or fullchain) (default "./cert.pem")
  -key string
        the filename of the certificate private key (default "./priv.key")
  -enable-password-login
        if set, enable password authentication (disabled by default)
  -generate-public-cert value
        Automatically produce and use a valid public certificate usingLet's Encrypt for the provided domain name. The flag can be used several times to generate several certificates.If certificates have already been generated previously using this flag, they will simply be reused without being regenerated. The public certificates are automatically renewed as long as the server is running. Automatically-generated IP public certificates are not available yet.
  -generate-selfsigned-cert
        if set, generates a self-self-signed cerificate and key that will be stored at the paths indicated by the -cert and -key args (they must not already exist)
  -url-path string
        the secret URL path on which the ssh3 server listens (default "/ssh3-term")
  -v    verbose mode, if set
  -version
        if set, displays the software version on standard output and exit
</code></pre></div>
<p dir="auto">The following command starts a public SSH3 server on port 443 with a valid Let's Encrypt public certificate
for domain <code>my-domain.example.org</code> and answers to new sessions requests querying the <code>/ssh3</code> URL path:</p>
<div data-snippet-clipboard-copy-content="ssh3-server -generate-public-cert my-domain.example.org -url-path /ssh3"><pre><code>ssh3-server -generate-public-cert my-domain.example.org -url-path /ssh3
</code></pre></div>
<p dir="auto">If you don't have a public domain name (i.e. only an IP address), you can either use an existing certificate
for your IP address using the <code>-cert</code> and <code>-key</code> arguments or generate a self-signed certificate using the
<code>-generate-selfsigned-cert</code> argument.</p>
<p dir="auto">If you have existing certificates and keys, you can run the server as follows to use them=</p>
<div data-snippet-clipboard-copy-content="ssh3-server -cert /path/to/cert/or/fullchain -key /path/to/cert/private/key -url-path /ssh3"><pre><code>ssh3-server -cert /path/to/cert/or/fullchain -key /path/to/cert/private/key -url-path /ssh3
</code></pre></div>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">Similarly to OpenSSH, the server must be run with root priviledges to log in as other users.</p>
</div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Authorized keys and authorized identities</h4><a id="user-content-authorized-keys-and-authorized-identities" aria-label="Permalink: Authorized keys and authorized identities" href="#authorized-keys-and-authorized-identities"></a></p>
<p dir="auto">By default, the SSH3 server will look for identities in the <code>~/.ssh/authorized_keys</code> and <code>~/.ssh3/authorized_identities</code> files for each user.
<code>~/.ssh3/authorized_identities</code> allows new identities such as OpenID Connect (<code>oidc</code>) discussed <a href="#openid-connect-authentication-still-experimental">below</a>.
Popular key types such as <code>rsa</code>, <code>ed25519</code> and keys in the OpenSSH format can be used.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using the SSH3 client</h3><a id="user-content-using-the-ssh3-client" aria-label="Permalink: Using the SSH3 client" href="#using-the-ssh3-client"></a></p>
<p dir="auto">Once you have an SSH3 server running, you can connect to it using the SSH3 client similarly to what
you did with your classical SSHv2 tool.</p>
<p dir="auto">Here is the usage of the <code>ssh3</code> executable:</p>
<div data-snippet-clipboard-copy-content="Usage of ssh3:
  -pubkey-for-agent string
        if set, use an agent key whose public key matches the one in the specified path
  -privkey string
        private key file
  -use-password
        if set, do classical password authentication
  -forward-agent
        if set, forwards ssh agent to be used with sshv2 connections on the remote host
  -forward-tcp string
        if set, take a localport/remoteip@remoteport forwarding localhost@localport towards remoteip@remoteport
  -forward-udp string
        if set, take a localport/remoteip@remoteport forwarding localhost@localport towards remoteip@remoteport
  -proxy-jump string
    	if set, performs a proxy jump using the specified remote host as proxy
  -insecure
        if set, skip server certificate verification
  -keylog string
        Write QUIC TLS keys and master secret in the specified keylog file: only for debugging purpose
  -use-oidc string
        if set, force the use of OpenID Connect with the specified issuer url as parameter
  -oidc-config string
        OpenID Connect json config file containing the &quot;client_id&quot; and &quot;client_secret&quot; fields needed for most identity providers
  -do-pkce
        if set, perform PKCE challenge-response with oidc
  -v    if set, enable verbose mode"><pre><code>Usage of ssh3:
  -pubkey-for-agent string
        if set, use an agent key whose public key matches the one in the specified path
  -privkey string
        private key file
  -use-password
        if set, do classical password authentication
  -forward-agent
        if set, forwards ssh agent to be used with sshv2 connections on the remote host
  -forward-tcp string
        if set, take a localport/remoteip@remoteport forwarding localhost@localport towards remoteip@remoteport
  -forward-udp string
        if set, take a localport/remoteip@remoteport forwarding localhost@localport towards remoteip@remoteport
  -proxy-jump string
    	if set, performs a proxy jump using the specified remote host as proxy
  -insecure
        if set, skip server certificate verification
  -keylog string
        Write QUIC TLS keys and master secret in the specified keylog file: only for debugging purpose
  -use-oidc string
        if set, force the use of OpenID Connect with the specified issuer url as parameter
  -oidc-config string
        OpenID Connect json config file containing the "client_id" and "client_secret" fields needed for most identity providers
  -do-pkce
        if set, perform PKCE challenge-response with oidc
  -v    if set, enable verbose mode
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Private-key authentication</h4><a id="user-content-private-key-authentication" aria-label="Permalink: Private-key authentication" href="#private-key-authentication"></a></p>
<p dir="auto">You can connect to your SSH3 server at my-server.example.org listening on <code>/my-secret-path</code> using the private key located in <code>~/.ssh/id_rsa</code> with the following command:</p>
<div data-snippet-clipboard-copy-content="  ssh3 -privkey ~/.ssh/id_rsa username@my-server.example.org/my-secret-path"><pre><code>  ssh3 -privkey ~/.ssh/id_rsa username@my-server.example.org/my-secret-path
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Agent-based private key authentication</h4><a id="user-content-agent-based-private-key-authentication" aria-label="Permalink: Agent-based private key authentication" href="#agent-based-private-key-authentication"></a></p>
<p dir="auto">The SSH3 client works with the OpenSSH agent and uses the classical <code>SSH_AUTH_SOCK</code> environment variable to
communicate with this agent. Similarly to OpenSSH, SSH3 will list the keys provided by the SSH agent
and connect using the first key listen by the agent by default.
If you want to specify a specific key to use with the agent, you can either specify the private key
directly with the <code>-privkey</code> argument like above, or specify the corresponding public key using the
<code>-pubkey-for-agent</code> argument. This allows you to authenticate in situations where only the agent has
a direct access to the private key but you only have access to the public key.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Password-based authentication</h4><a id="user-content-password-based-authentication" aria-label="Permalink: Password-based authentication" href="#password-based-authentication"></a></p>
<p dir="auto">While discouraged, you can connect to your server using passwords (if explicitly enabled on the <code>ssh3-server</code>)
with the following command:</p>
<div data-snippet-clipboard-copy-content="  ssh3 -use-password username@my-server.example.org/my-secret-path"><pre><code>  ssh3 -use-password username@my-server.example.org/my-secret-path
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Config-based session establishment</h4><a id="user-content-config-based-session-establishment" aria-label="Permalink: Config-based session establishment" href="#config-based-session-establishment"></a></p>
<p dir="auto"><code>ssh3</code> parses your OpenSSH config. Currently, it only handles the <code>Hostname</code>; <code>User</code>, <code>Port</code> and <code>IdentityFile</code> OpenSSH options.
It also adds new option only used by SSH3, such as <code>URLPath</code> or <code>UDPProxyJump</code>. <code>URLPath</code> allows you to omit the secret URL path in your
SSH3 command. <code>UDPProxyJump</code> allows you to perform SSH3 (#proxy-jump)[Proxy Jump] and has the same meaning as the <code>-proxy-jump</code> command-line argument.
Let's say you have the following lines in your OpenSSH config located in <code>~/.ssh/config</code> :</p>
<div data-snippet-clipboard-copy-content="IgnoreUnknown URLPath
Host my-server
  HostName 192.0.2.0
  User username
  IdentityFile ~/.ssh/id_rsa
  URLPath /my-secret-path"><pre><code>IgnoreUnknown URLPath
Host my-server
  HostName 192.0.2.0
  User username
  IdentityFile ~/.ssh/id_rsa
  URLPath /my-secret-path
</code></pre></div>
<p dir="auto">Similarly to what OpenSSH does, the following <code>ssh3</code> command will connect you to the SSH3 server running on 192.0.2.0 on UDP port 443 using public key authentication with the private key located in <code>.ssh/id_rsa</code> :</p>
<div data-snippet-clipboard-copy-content="  ssh3 my-server/my-secret-path"><pre><code>  ssh3 my-server/my-secret-path
</code></pre></div>
<p dir="auto">If you do not want a config-based utilization of SSH3, you can read the sections below to see how to use the CLI parameters of <code>ssh3</code>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">OpenID Connect authentication (still experimental)</h4><a id="user-content-openid-connect-authentication-still-experimental" aria-label="Permalink: OpenID Connect authentication (still experimental)" href="#openid-connect-authentication-still-experimental"></a></p>
<p dir="auto">This feature allows you to connect using an external identity provider such as the one
of your company or any other provider that implements the OpenID Connect standard, such as Google Identity,
Github or Microsoft Entra. The authentication flow is illustrated in the GIF below.</p>
<div dir="auto">
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/francoismichel/ssh3/blob/main/resources/figures/ssh3_oidc.gif"><img src="https://github.com/francoismichel/ssh3/raw/main/resources/figures/ssh3_oidc.gif" width="75%" data-animated-image=""></a></p><p dir="auto"><em>Secure connection without private key using a Google account.</em></p>
</div>
<p dir="auto">The way it connects to your identity provider is configured in a file named <code>~/.ssh3/oidc_config.json</code>.
Below is an example <code>config.json</code> file for use with a Google account. This configuration file is an array
and can contain several identity providers configurations.</p>
<div dir="auto" data-snippet-clipboard-copy-content="[
    {
        &quot;issuer_url&quot;: &quot;https://accounts.google.com&quot;,
        &quot;client_id&quot;: &quot;<your_client_id>&quot;,
        &quot;client_secret&quot;: &quot;<your_client_secret>&quot;
    }
]"><pre>[
    {
        <span>"issuer_url"</span>: <span><span>"</span>https://accounts.google.com<span>"</span></span>,
        <span>"client_id"</span>: <span><span>"</span>&lt;your_client_id&gt;<span>"</span></span>,
        <span>"client_secret"</span>: <span><span>"</span>&lt;your_client_secret&gt;<span>"</span></span>
    }
]</pre></div>
<p dir="auto">This might change in the future, but currently, to make this feature work with your Google account, you will need to setup a new experimental application in your Google Cloud console and add your email as authorized users.
This will provide you with a <code>client_id</code> and a <code>client_secret</code> that you can then set in your <code>~/.ssh3/oidc_config.json</code>. On the server side, you just have to add the following line in your <code>~/.ssh3/authorized_identities</code>:</p>
<div data-snippet-clipboard-copy-content="oidc <client_id> https://accounts.google.com <email>"><pre><code>oidc &lt;client_id&gt; https://accounts.google.com &lt;email&gt;
</code></pre></div>
<p dir="auto">We currently consider removing the need of setting the client_id in the <code>authorized_identities</code> file in the future.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Proxy jump</h4><a id="user-content-proxy-jump" aria-label="Permalink: Proxy jump" href="#proxy-jump"></a></p>
<p dir="auto">It is often the case that some SSH hosts can only be accessed through a gateway. SSH3 allows you to perform a Proxy Jump similarly to what is proposed by OpenSSH.
You can connect from A to C using B as a gateway/proxy. B and C must both be running a valid SSH3 server. This works by establishing UDP port forwarding on B to forward QUIC packets from A to C.
The connection from A to C is therefore fully end-to-end and B cannot decrypt or alter the SSH3 traffic between A and C.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[First Malicious MCP in the Wild: The Postmark Backdoor Stealing Your Emails (251 pts)]]></title>
            <link>https://www.koi.security/blog/postmark-mcp-npm-malicious-backdoor-email-theft</link>
            <guid>45395957</guid>
            <pubDate>Sat, 27 Sep 2025 14:23:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.koi.security/blog/postmark-mcp-npm-malicious-backdoor-email-theft">https://www.koi.security/blog/postmark-mcp-npm-malicious-backdoor-email-theft</a>, See on <a href="https://news.ycombinator.com/item?id=45395957">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="blog-rich-text"><p>You know MCP servers, right? Those handy tools that let your AI assistant send emails, run database queries, basically handle all the tedious stuff we don't want to do manually anymore. Well, here's the thing not enough people talk about: we're giving these tools god-mode permissions. Tools built by people we've never met. People we have zero way to vet. And our AI assistants? We just... trust them. Completely.</p><p>Which brings me to why I'm writing this. <code>postmark-mcp</code> - downloaded <strong>1,500 times every single week</strong>, integrated into hundreds of developer workflows. Since version <code>1.0.16</code>, it's been quietly copying every email to the developer's personal server. I'm talking password resets, invoices, internal memos, confidential documents - everything.</p><p>This is the<strong> world’s first sighting of a real world malicious MCP server</strong>. The attack surface for endpoint supply chain attacks is slowly becoming the enterprise’s biggest attack surface.</p><h2>So… What Did Our Risk Engine Detect?</h2><p>Here's how this whole thing started. Our risk engine at Koi flagged <code>postmark-mcp</code> when version <code>1.0.16</code> introduced some suspicious behavior changes. When our researchers dug into it, like we do to any malware our risk engine flags, what we found was very disturbing.</p><p>On paper, this package looked perfect. The developer? Software engineer from Paris, using his real name, GitHub profile packed with legitimate projects. This wasn't some shady anonymous account with an anime avatar. This was a real person with a real reputation, someone you'd probably grab coffee with at a conference.</p><p>For 15 versions - FIFTEEN - the tool worked flawlessly. Developers were recommending it to their teams. "Hey, check out this great MCP server for Postmark integration." It became part of developer’s daily workflows, as trusted as their morning coffee.</p><p>Then version 1.0.16 dropped. Buried on line 231, our risk engine found this gem:</p><figure><p><img src="https://cdn.prod.website-files.com/689ad8c5d13f40cf59df0e0c/68d2dbea5498f5d66a60eaea_carbon%20(11)%20(1).png" loading="lazy" alt=""></p><figcaption>A simple line that steals thousands of emails</figcaption></figure><p>One single line. And boom - every email now has an unwanted passenger.</p><p>Here's the thing - there's a completely legitimate GitHub repo with the same name, officially maintained by Postmark (ActiveCampaign). The attacker took the legitimate code from their repo, added his malicious BCC line, and published it to npm under the same name. Classic impersonation.</p><p>Look, I get it. Life happens. Maybe the developer hit financial troubles. Maybe someone slid into his DMs with an offer he couldn't refuse. Hell, maybe he just woke up one day and thought "I wonder if I could get away with this." We'll never really know what flips that switch in someone's head - what makes a legitimate developer suddenly decide to backstab 1,500 users who trusted them.</p><p>But that's exactly the point. We CAN'T know. We can't predict it. And when it happens? Most of us won't even notice until it's way too late. For modern enterprises the problem is even more severe. As security teams focus on traditional threats and compliance frameworks, developers are independently adopting AI tools that operate completely outside established security perimeters. These MCP servers run with the same privileges as the AI assistants themselves - full email access, database connections, API permissions - yet they don't appear in any asset inventory, skip vendor risk assessments, and bypass every security control from DLP to email gateways. By the time someone realizes their AI assistant has been quietly BCCing emails to an external server for months, the damage is already catastrophic.</p><h2>Lets Talk About the Impact</h2><p>Okay, bear with me while I break down what we're actually looking at here.</p><p>You install an MCP server because you want your AI to handle emails, right? Seems reasonable. Saves time. Increases productivity. All that good stuff. But what you're actually doing is handing complete control of your entire email flow to someone you've never met.&nbsp;</p><p>We can only guestimate the impact:</p><ul role="list"><li>1,500 downloads every single week</li><li>Being conservative, maybe 20% are actively in use</li><li>That's about 300 organizations</li><li>Each one probably sending what, 10-50 emails daily?</li><li>We're talking about 3,000 to 15,000 emails EVERY DAY flowing straight to giftshop.club</li></ul><p>And the truly messed up part? The developer didn't hack anything. Didn't exploit a zero-day. Didn't use some sophisticated attack vector. We literally handed him the keys, said "here, run this code with full permissions," and let our AI assistants use it hundreds of times a day. We did this to ourselves.</p><figure><p><img src="https://cdn.prod.website-files.com/689ad8c5d13f40cf59df0e0c/68d2e7796d400a7fb93e983c_Screenshot%202025-09-23%20at%2021.29.33%20(1).png" loading="lazy" alt=""></p><figcaption>Koidex report for postmark-mcp</figcaption></figure><p>I've been doing security for years now, and this particular issue keeps me up at night. Somehow, we've all just accepted that it's totally normal to install tools from random strangers that can:</p><ul role="list"><li>Send emails as us (with our full authority)</li><li>Access our databases (yeah, all of them)</li><li>Execute commands on our systems</li><li>Make API calls with our credentials</li></ul><p>And once you install them? Your AI assistant just goes to town. No review process. No "hey, should I really send this email with a BCC to giftshop.club?" Just blind, automated execution. Over and over. Hundreds of times a day.</p><p>There's literally no security model here. No sandbox. No containment. Nothing. If the tool says "send this email," your AI sends it. If it says "oh, also copy everything to this random address," your AI does that too. No questions asked.</p><p>The postmark-mcp backdoor isn't sophisticated - it's embarrassingly simple. But it perfectly demonstrates how completely broken this whole setup is. One developer. One line of code. Thousands upon thousands of stolen emails.</p><figure><p><img src="https://cdn.prod.website-files.com/689ad8c5d13f40cf59df0e0c/68d2e67416d1614856d43205_Screenshot%202025-09-23%20at%2021.26.25.png" loading="lazy" alt=""></p><figcaption>postmark-mcp NPM page</figcaption></figure><h2>The Attack Timeline</h2><p><strong>Phase 1: Build a Legitimate Tool</strong><br>Versions 1.0.0 through 1.0.15 work perfectly. Users trust the package.</p><p><strong>Phase 2: Add One Line</strong><br>Version 1.0.16 adds the BCC. Nothing else changes.</p><p><strong>Phase 3: Profit</strong><br>Sit back and watch emails containing passwords, API keys, financial data, and customer information flow into giftshop.club.</p><p>This pattern absolutely terrifies me. A tool can be completely legitimate for months. It gets battle-tested in production. It becomes essential to your workflow. Your team depends on it. And then one day - BAM - it's malware. By the time the backdoor activates, it's not some random package anymore. It's trusted infrastructure.</p><p>Oh, and <code>giftshop.club</code>? Looks like it might be another one of the developer's side projects. But now it's collecting a very different kind of gift. Your emails are the gifts.</p><figure><p><img src="https://cdn.prod.website-files.com/689ad8c5d13f40cf59df0e0c/68d2dfc7f12afe4d7c64ce5e_Screenshot%202025-09-23%20at%2020.57.31.jpg" loading="lazy" alt=""></p><figcaption>Another side-project by the same developer was used as the C2 server</figcaption></figure><p>When we reached out to the developer for clarification, we got silence. No explanation. No denial. Nothing. But he did take action - just not the kind we hoped for. He promptly deleted the package from npm, trying to erase the evidence.</p><p>Here's the thing though: deleting a package from npm doesn't remove it from the machines where it's already installed. Every single one of those 1,500 weekly downloads? They're still compromised. Still sending BCCs to <code>giftshop.club</code>. The developer knows this. He's banking on victims not realizing they're still infected even though the package has vanished from npm.</p><h2>Why MCP's Entire Model Is Fundamentally Broken</h2><p>Let me be really clear about something: MCP servers aren't like regular npm packages. These are tools specifically designed for AI assistants to use autonomously. That's the whole point.</p><p>When you install postmark-mcp, you're not just adding some dependency to your package.json. You're giving your AI assistant a tool it will use hundreds of times, automatically, without ever stopping to think "hmm, is something wrong here?"</p><p>Your AI can't detect that BCC field. It has no idea emails are being stolen. All it sees is a functioning email tool. Send email. Success. Send another email. Success. Meanwhile, every single message is being silently exfiltrated. Day after day. Week after week.</p><p>The postmark-mcp backdoor isn't just about one malicious developer or 1,500 weekly compromised installations. It's a warning shot about the MCP ecosystem itself.</p><p>We're handing god-mode permissions to tools built by people we don't know, can't verify, and have no reason to trust. These aren't just npm packages - they're direct pipelines into our most sensitive operations, automated by AI assistants that will use them thousands of times without question.</p><p>The backdoor is actively harvesting emails as you read this. We've reported it to npm, but here's the terrifying question: how many other MCP servers are already compromised? How would you even know?</p><p>At Koi, we detect these behavioral changes in packages because the MCP ecosystem has no built-in security model. When you're trusting anonymous developers with your AI's capabilities, you need verification, not faith. Our risk engine automatically caught this backdoor the moment version 1.0.16 introduced the BCC behavior - something no traditional security tool would flag. But detection is just the first step. Our supply chain gateway ensures that malicious packages like this never make it into your environment in the first place. It acts as a checkpoint between your developers and the wild west of npm, MCP servers, and browser extensions - blocking known threats, flagging suspicious updates, and requiring approval for packages that touch sensitive operations like email or database access. While everyone else is hoping their developers make good choices, we're making sure they can only choose from verified, continuously monitored options.</p><p>If you're using <code>postmark-mcp</code> version <code>1.0.16</code> or later, you're compromised. Remove it immediately and rotate any credentials that may have been exposed through email. But more importantly, audit every MCP server you're using. Ask yourself: do you actually know who built these tools you're trusting with everything?</p><p>Stay paranoid. With MCPs, paranoia is just good sense.</p><h2>IOCs</h2><p><strong>Package:</strong> postmark-mcp (npm)<br><strong>Malicious Version:</strong> 1.0.16 and later<br><strong>Backdoor Email:</strong> phan@giftshop[.]club<br><strong>Domain:</strong> giftshop[.]club</p><p><strong>Detection:</strong></p><ul role="list"><li>Check for BCC headers to giftshop.club in email logs</li><li>Audit MCP server configurations for unexpected email parameters</li><li>Review npm packages for version 1.0.16+ of postmark-mcp</li></ul><p><strong>Mitigation:</strong></p><ul role="list"><li>Immediately uninstall postmark-mcp</li><li>Rotate any credentials sent via email during the compromise period</li><li>Audit email logs for sensitive data that may have been exfiltrated</li><li>Report any confirmed breaches to appropriate authorities</li></ul><p>‍</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Samsung now owns Denon, Bowers and Wilkins, Marantz, Polk, and more audio brands (239 pts)]]></title>
            <link>https://www.theverge.com/news/784390/samsung-harman-masimo-audio-acquisition-complete</link>
            <guid>45395396</guid>
            <pubDate>Sat, 27 Sep 2025 13:05:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/news/784390/samsung-harman-masimo-audio-acquisition-complete">https://www.theverge.com/news/784390/samsung-harman-masimo-audio-acquisition-complete</a>, See on <a href="https://news.ycombinator.com/item?id=45395396">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><a href="https://www.theverge.com/authors/jess-weatherbed"><img alt="Jess Weatherbed" data-chromatic="ignore" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/195820/JESSICA_WEATHERBED.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=48 1x, https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/195820/JESSICA_WEATHERBED.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96 2x" src="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/195820/JESSICA_WEATHERBED.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96"></a></p><div><p><span aria-expanded="false" aria-haspopup="true" role="button" tabindex="0"><span id="follow-author-standard_article_details-dmcyOmF1dGhvclByb2ZpbGU6OTU="><span><span><svg width="9" height="9" viewBox="0 0 9 9" fill="none" xmlns="http://www.w3.org/2000/svg" aria-label="Follow"><path d="M5 0H4V4H0V5H4V9H5V5H9V4H5V0Z"></path></svg></span></span><span>Jess Weatherbed</span></span></span></p> <p><span>is a news writer focused on creative industries, computing, and internet culture. Jess started her career at TechRadar, covering news and hardware reviews.</span></p></div></div><div id="zephr-anchor"><p>Samsung subsidiary Harman has completed its acquisition of Sound United, Masimo’s former audio business, adding a sizable expansion to Samsung’s audio brand portfolio. The $350 million deal was <a href="https://www.theverge.com/news/662437/samsung-harman-masimo-aquisition-audio-empire">first announced in May</a>, and brings Bowers &amp; Wilkins, Denon, Marantz, Definitive Technology, Polk Audio, HEOS, Classé, and Boston Acoustics under the same roof as JBL, Harman Kardon, and other audio brands that Samsung acquired when it <a href="https://www.theverge.com/2016/11/14/13620812/samsung-cars-harman-acquisition-8-billion">purchased Harman for $8 billion</a> in 2016.</p><p>”Sound United’s impressive roster of brands is rooted in a deep passion for sound, innovation, and commitment to quality that aligns with Harman’s own values,” Harman’s lifestyle lead, Dave Rogers, <a href="https://news.harman.com/releases/harman-completes-sound-united-acquisition-to-expand-premium-audio-leadership">said in a statement</a>. “This transaction unlocks meaningful growth opportunities for everyone. It bolsters Harman’s strategy to build on its unparalleled success story and scale to unprecedented heights as an audio leader.”</p><p>Sound United will operate as a standalone business under Harman’s lifestyle division to ensure that each audio brand preserves its identity and customer base. With the sale now completed, Masimo can focus its attention on the <a href="https://www.theverge.com/news/763291/apple-masimo-blood-oxygen-patent-customs-lawsuit">Apple Watch lawsuit</a> it launched against US Customs and Border Protection in August.</p><div><p><span><strong>Follow topics and authors</strong> from this story to see more like this in your personalized homepage feed and to receive email updates.</span></p><ul><li id="follow-author-article_footer-dmcyOmF1dGhvclByb2ZpbGU6OTU="><span aria-expanded="false" aria-haspopup="true" role="button" tabindex="0"><span><span><svg width="9" height="9" viewBox="0 0 9 9" fill="none" xmlns="http://www.w3.org/2000/svg" aria-label="Follow"><path d="M5 0H4V4H0V5H4V9H5V5H9V4H5V0Z"></path></svg></span><span>Jess Weatherbed</span></span></span></li><li></li><li></li><li></li><li></li></ul></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why We Think (102 pts)]]></title>
            <link>https://lilianweng.github.io/posts/2025-05-01-thinking/</link>
            <guid>45395133</guid>
            <pubDate>Sat, 27 Sep 2025 12:27:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lilianweng.github.io/posts/2025-05-01-thinking/">https://lilianweng.github.io/posts/2025-05-01-thinking/</a>, See on <a href="https://news.ycombinator.com/item?id=45395133">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>Special thanks to <a href="https://scholar.google.com/citations?user=itSa94cAAAAJ&amp;hl=en">John Schulman</a> for a lot of super valuable feedback and direct edits on this post.</span></p>
<p>Test time compute (<a href="https://arxiv.org/abs/1603.08983">Graves et al. 2016</a>, <a href="https://arxiv.org/abs/1705.04146">Ling, et al. 2017</a>, <a href="https://arxiv.org/abs/2110.14168">Cobbe et al. 2021</a>) and Chain-of-thought (CoT) (<a href="https://arxiv.org/abs/2201.11903">Wei et al. 2022</a>, <a href="https://arxiv.org/abs/2112.00114">Nye et al. 2021</a>), have led to significant improvements in model performance, while raising many research questions. This post aims to review recent developments in how to effectively use test-time compute (i.e. “thinking time”) and why it helps.</p>
<h2 id="motivation">Motivation</h2>
<p>Enabling models to think for longer can be motivated in a few different ways.</p>
<h2 id="analogy-to-psychology">Analogy to Psychology</h2>
<p>The core idea is deeply connected to how humans think. We humans cannot immediately provide the answer for <code>"What's 12345 times 56789?"</code>. Rather, it is natural to spend time pondering and analyzing before getting to the result, especially for complex problems. In <a href="https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555">Thinking, Fast and Slow (Kahneman, 2013)</a>, Daniel Kahneman characterizes human thinking into two modes, through the lens of the <a href="https://en.wikipedia.org/wiki/Dual_process_theory">dual process theory</a> :</p>
<ul>
<li><em>Fast thinking (System 1)</em> operates quickly and automatically, driven by intuition and emotion while requiring little to no effort.</li>
<li><em>Slow thinking (System 2)</em> demands deliberate, logical thought and significant cognitive efforts. This mode of thinking consumes more mental energy and requires intentional engagement.</li>
</ul>
<p>Because System 1 thinking is fast and easy, it often ends up being the main decision driver, at the cost of accuracy and logic. It naturally relies on our brain’s mental shortcuts (i.e., heuristics) and can lead to errors and biases. By consciously slowing down and taking more time to reflect, improve and analyze, we can engage in System 2 thinking to challenge our instincts and make more rational choices.</p>
<h2 id="computation-as-a-resource">Computation as a Resource</h2>
<p>One view of deep learning, is that neural networks can be characterized by the amount of computation and storage they can access in a forward pass, and if we optimize them to solve problems using gradient descent, the optimization process will figure out how to use these resources–they’ll figure out how to organize these resources into circuits for calculation and information storage. From this view, if we design an architecture or system that can do more computation at test time, and we train it to effectively use this resource, it’ll work better.</p>
<p>In Transformer models, the amount of computation (flops) that the model does for each generated token is roughly 2 times the number of parameters. For sparse models like mixture of experts (MoE), only a fraction of the parameters are used in each forward pass, so computation = 2 * parameters / sparsity, where sparsity is the fraction of experts active.</p>
<p>On the other hand, CoT enables the model to perform far more flops of computation for each token of the answer that it is trying to compute. In fact, CoT has a nice property that it allows the model to use a variable amount of compute depending on the hardness of the problem.</p>
<h2 id="latent-variable-modeling">Latent Variable Modeling</h2>
<p>A classic idea in machine learning is to define a probabilistic model with a latent (hidden) variable $z$ and a visible variable $y$, where $y$ is given to our learning algorithm. Marginalizing (summing) over the possible values of the latent variable allows us to express a rich distribution over the visible variables, $P(y) = \sum_{z \sim P(z)} P(y \mid z)$. For example, we can model the distribution over math problems and solutions by letting $x$ denote a problem statement, $y$ be ground truth answer or proof, and $z$ as a free-form thought process that leads to the proof. The marginal probability distribution to optimize would be $P(y \mid x) = \sum_{z \sim p(z\mid x)} P(y \mid x, z)$</p>
<p>The latent variable perspective is particularly useful for understanding methods that involve collecting multiple parallel CoTs or searching over the CoT–these algorithms can be seen as sampling from the posterior $P(z \mid x, y)$. This view also suggests the benefits of using the log loss $\log P(y \mid x)$ as the target objective to optimize, as the log loss objective has been so effective in pretraining.</p>
<h2 id="thinking-in-tokens">Thinking in Tokens</h2>
<p>The strategy of generating intermediate steps before generating short answers, particularly for math problems, was explored by <a href="https://arxiv.org/abs/1705.04146">Ling, et al. 2017</a>, who introduced the <a href="https://github.com/google-deepmind/AQuA">AQUA-RAT</a> dataset, and then expanded by <a href="https://arxiv.org/abs/2110.14168">Cobbe et al. 2021</a>, who introduced the <a href="https://github.com/openai/grade-school-math">Grade School Math (GSM)</a> dataset. Cobbe et al. train a generator with supervised learning on human-written solutions and verifiers that predict the correctness of a candidate solution; they can then search over these solutions. <a href="https://arxiv.org/abs/2112.00114">Nye et al. (2021</a>) experimented with intermediate thinking tokens as “scratchpads” and <a href="https://arxiv.org/abs/2201.11903">Wei et al.</a> (2022) coined the now-standard term <strong>chain-of-thought</strong> (CoT).</p>
<p>Early work on improving CoT reasoning involved doing supervised learning on human-written reasoning traces or model-written traces filtered for answer correctness, where the latter can be seen as a rudimentary form of reinforcement learning (RL). Some other work found that one could significantly boost math performance of instruction tuned models by prompting them appropriately, with <code>"think step by step"</code> (<a href="https://arxiv.org/abs/2205.11916">Kojima et al. 2022</a>) or more complex prompting to encourage the model to reflect on related knowledge first (<a href="https://arxiv.org/abs/2310.01714">Yasunaga et al. 2023</a>).</p>
<p>Later work found that the CoT reasoning capabilities can be significantly improved by doing reinforcement learning on a dataset of problems with automatically checkable solutions, such as STEM problems with short answers, or coding tasks that can be checked with unit tests (<a href="https://arxiv.org/abs/2203.14465">Zelikman et al. 2022</a>, <a href="https://arxiv.org/abs/2312.08935">Wang et al., 2023</a>, <a href="https://arxiv.org/abs/2310.10047">Liu et al., 2023</a>). This approach rose to prominence with the announcement of <a href="https://openai.com/index/learning-to-reason-with-llms/">o1-preview</a>, <a href="https://openai.com/index/introducing-o3-and-o4-mini/">o3</a>, and the R1 tech report (<a href="https://arxiv.org/abs/2501.12948">DeepSeek-AI, 2025</a>), which showed that a simple recipe where a policy gradient algorithm could lead to strong performance.</p>
<figure>
    <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/cot-wei22.png">
    <figcaption>Chain-of-thought prompting leads to higher success rate of solving math problems. Larger models benefit more from thinking time. (Image source: Wei et al. 2022)</figcaption>
</figure>
<h2 id="branching-and-editing">Branching and Editing</h2>
<p>The fundamental intent of test-time compute is to adaptively modify the model’s output distribution at test time. There are various ways of utilizing test time resources for decoding to select better samples and thus alter the model’s predictions towards a more desired distribution. Two main approaches for improving the decoding process are parallel sampling and sequential revision.</p>
<ul>
<li><strong>Parallel sampling</strong> generates multiple outputs simultaneously, meanwhile providing guidance per step with process reward signals or using verifiers to judge the quality at the end. It is the most widely adopted decoding method to improve test time performance, such as best-of-$N$ or beam search. Self-consistency (<a href="https://arxiv.org/abs/2203.11171">Wang et al. 2023</a>) is commonly used to select the answer with majority vote among multiple CoT rollouts when the ground truth is not available.</li>
<li><strong>Sequential revision</strong> adapts the model’s responses iteratively based on the output in the previous step, asking the model to intentionally reflect its existing response and correct mistakes. The revision process may have to rely on a fine-tuned model, as naively relying on the model’s intrinsic capability of self-correction without external feedback may not lead to improvement (<a href="https://arxiv.org/abs/2406.01297">Kamoi et al. 2024</a>, <a href="https://arxiv.org/abs/2310.01798">Huang et al. 2024</a>).</li>
</ul>
<p>Parallel sampling is simple, intuitive and easier to implement, but bounded by the model capability of whether it can achieve the correct solution in one-go. Sequential explicitly asks the model to reflect on mistakes but it is slower and requires extra care during implementation as it does run the risk of correct predictions being modified to be incorrect or introducing other types of hallucinations. These two methods can be used together. <a href="https://arxiv.org/abs/2408.03314">Snell et al. (2024</a>) showed that easier questions benefit from purely sequential test-time compute, whereas harder questions often perform best with an optimal ratio of sequential to parallel compute.</p>
<figure>
    <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/parallel-vs-sequential.png">
    <figcaption>Illustration of parallel sampling vs sequential revision.</figcaption>
</figure>
<h3 id="parallel-sampling">Parallel Sampling</h3>
<p>Given a generative model and a scoring function that we can use to score full or partial samples, there are various search algorithms we can use to find a high scoring sample. Best-of-$N$ is the simplest such algorithm: one just collects $N$ independent samples and chooses the highest-ranking sample according to some scoring function. Beam search is a more sophisticated search algorithm that makes the search process more adaptive, spending more sampling computation on more promising parts of the solution space.</p>
<p>Beam search maintains a set of promising partial sequences and alternates between extending them and pruning the less promising ones. As a selection mechanism, we can use a process reward model (PRM; <a href="https://arxiv.org/abs/2305.20050">Lightman et al. 2023</a>) to guide beam search candidate selection. <a href="https://arxiv.org/abs/2305.00633">Xie et al. (2023</a>) used LLM to evaluate how likely its own generated reasoning step is correct, formatted as a multiple-choice question and found that per-step self-evaluation reduces accumulative errors in multi-step reasoning during beam search decoding. Besides, during sampling, annealing the temperature helps mitigate aggregated randomness. These experiments by Xie et al. achieved 5-6% improvement on few-shot GSM8k, AQuA and StrategyQA benchmarks with the Codex model. Reward balanced search (short for “REBASE”; <a href="https://arxiv.org/abs/2408.00724">Wu et al. 2025</a>) separately trained a process reward model (PRM) to determine how much each node should be expanded at each depth during beam search, according to the softmax-normalized reward scores. <a href="https://arxiv.org/abs/2410.01044">Jiang et al. (2024)</a> trained their PRM, named “RATIONALYST”, for beam search guidance on synthetic rationales conditioned on a large amount of unlabelled data. Good rationales are filtered based on whether they help reduce the neg log-prob of true answer tokens by a threshold, when comparing the difference between when the rationales is included in the context vs not. At inference time, RATIONALYST provides process supervision to the CoT generator by helping estimate log-prob of next reasoning steps (“implicit”) or directly generating next reasoning steps as part of the prompt (“explicit”).</p>
<figure>
    <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/beam-search-xie23.png">
    <figcaption>Beam search decoding guided by LLM self-evaluation per reasoning step. (Image source: <a href="https://arxiv.org/abs/2305.00633" target="_blank">Xie et al. 2023</a>)</figcaption>
</figure>
<p>Interestingly, it is possible to trigger the emergent chain-of-thought reasoning paths <em>without</em> explicit zero-shot or few-shot prompting. <a href="https://arxiv.org/abs/2402.10200">Wang &amp; Zhou (2024)</a> discovered that if we branch out at the first sampling tokens by retaining the top $k$ tokens with highest confidence, measured as the difference between top-1 and top-2 candidates during sampling, and then continue these $k$ sampling trials with greedy decoding onward, many of these sequences natively contain CoT. Especially when CoT does appear in the context, it leads to a more confident decoding of the final answer. To calculate the confidence of the final answer, the answer span needs to be identified by task-specific heuristics (e.g. last numerical values for math questions) or  by prompting the model further with <code>"So the answer is"</code>. The design choice of only branching out at the first token is based on the observation that early branching significantly enhances the diversity of potential paths, while later tokens are influenced a lot by previous sequences.</p>
<figure>
    <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/cot-decoding.png">
    <figcaption>Top-$k$ decoding, $k$ refers to the number of candidates at the first sampling step. (Image source: <a href="https://arxiv.org/abs/2402.10200" target="_blank">Wang &amp; Zhou, 2024</a>)</figcaption>
</figure>
<h3 id="sequential-revision">Sequential Revision</h3>
<p>If the model can reflect and correct mistakes in past responses, we would expect the model to produce a nice sequence of iterative revision with increasing quality. However, this self-correction capability turns out to not exist intrinsically among LLMs and does not easily work out of the box, due to various failure modes, such as, (1) hallucination, including modifying correct responses to be incorrect; (2) behavior collapse to non-correcting behavior; e.g. making minor or no modification on the first incorrect responses; or (3) fail to generalize to distribution shift at test time. Experiments by <a href="https://arxiv.org/abs/2310.01798">Huang et al. (2024</a>) showed that naively applying self-correction leads to worse performance and external feedback is needed for models to self improve, which can be based on matching ground truths, heuristics and task-specific metrics, unit tests results for coding questions (<a href="https://arxiv.org/abs/2303.11366">Shinn, et al. 2023</a>), a stronger model (<a href="https://arxiv.org/abs/2404.17140">Zhang et al. 2024</a>), as well as human feedback (<a href="https://arxiv.org/abs/2302.02676">Liu et al. 2023</a>).</p>
<p>Self-correction learning (<a href="https://arxiv.org/abs/2211.00053">Welleck et al. 2023</a>) aims to train a corrector model $P_\theta(y \mid y_0, x)$ given a fixed generator model $P_0(y_0 \mid x)$. While the generator model remains to be generic, the corrector model can task-specific and only does generation conditioned on an initial model response and additional feedback (e.g. a sentence, a compiler trace, unit test results; can be optional):</p>
<ol>
<li>Self-correction learning first generates first generates multiple outputs per prompt in the data pool;</li>
<li>then create value-improving pairs by pairing two outputs for the same prompt together if one has a higher value than the other, (prompt $x$, hypothesis $y$, correction $y’$).</li>
<li>These pairs are selected proportional to is improvement in value, $v(y’) - v(y)$, and similarity between two outputs, $\text{Similarity}(y, y’)$ to train the corrector model.</li>
<li>To encourage exploration, the corrector provides new generations into the data pool as well. At the inference time, the corrector can be used iteratively to create a correction trajectory of sequential revision.</li>
</ol>
<figure>
  <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/self-correction-welleck23.png">
  <figcaption>Illustration of self-correction learning by matching model outputs for the same problem to form value-improving pairs to train a correction model. (Image source: Welleck et al. 2023)</figcaption>
</figure>
<p>Recursive inspection (<a href="https://arxiv.org/abs/2407.18219">Qu et al. 2024</a>) also aims to train a better corrector model but with a single model to do both generation and self-correction.</p>
<p>SCoRe (Self-Correction via Reinforcement Learning; <a href="https://arxiv.org/abs/2409.12917">Kumar et al. 2024</a>) is a multi-turn RL approach to encourage the model to do self-correction by producing better answers at the second attempt than the one created at the first attempt. It composes two stages of training: stage 1 only maximizes the accuracy of the second attempt while enforcing a KL penalty only on the first attempt to avoid too much shifting of the first-turn responses from the base model behavior; stage 2 optimizes the accuracy of answers produced by both the first and second attempts. Ideally we do want to see performance at both first and second attempts to be better, but adding stage 1 prevents the behavior collapse where the model does minor or none edits on the first response, and stage 2 further improves the results.</p>
<figure>
  <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/SCoRe-kumar24.png">
  <figcaption>Explicit training setup to improve self-correction capabilities by doing two-staged RL training. (Image source: Kumar et al. 2024)</figcaption>
</figure>
<h2 id="rl-for-better-reasoning">RL for Better Reasoning</h2>
<p>There’s been a lot of recent success in using RL to improve the reasoning ability of language models, by using a collection of questions with ground truth answers (usually STEM problems and puzzles with easy to verify answers), and rewarding the model for getting the correct answer.Recent activity in this area was spurred by strong performance of the <code>o</code>-series models from OpenAI, and the subsequent releases of models and tech reports from <a href="https://www.deepseek.com/">DeepSeek</a>.</p>
<p><code>DeepSeek-R1</code> (<a href="https://arxiv.org/abs/2501.12948">DeepSeek-AI, 2025</a>) is an open-source LLM designed to excel in tasks that require advanced reasoning skills like math, coding and logical problem solving. They run through 2 rounds of SFT-RL training, enabling R1 to be good at both reasoning and non-reasoning tasks.</p>
<ol>
<li><strong>Cold-start SFT</strong> is to fine-tune the <code>DeepSeek-V3-Base</code> base model on a collection of thousands of cold-start data. Without this step, the model has issues of poor readability and language mixing.</li>
<li><strong>Reasoning-oriented RL</strong> trains a reasoning model on reasoning-only prompts with two types of rule-based rewards:
<ul>
<li>Format rewards: The model should wrap CoTs by <code>&lt;thinking&gt; ... &lt;/thinking&gt;</code> tokens.</li>
<li>Accuracy rewards: Whether the final answers are correct. The answer for math problems needs to be present in a specific format (e.g. in a box) to be verified reliably. For coding problems, a compiler is used to evaluate whether test cases pass.</li>
</ul>
</li>
<li><strong>Rejection-sampling + non-reasoning SFT</strong> utilizes new SFT data created by rejection sampling on the RL checkpoint of step 2, combined with non-reasoning supervised data from <code>DeepSeek-V3</code> in domains like writing, factual QA, and self-cognition, to retrain <code>DeepSeek-V3-Base</code>.
<ul>
<li>Filter out CoTs with mixed languages, long paragraphs, and code blocks.</li>
<li>Include non-reasoning tasks using DeepSeek-V3 (<a href="https://arxiv.org/abs/2412.19437v1">DeepSeek-AI, 2024</a>) pipeline.</li>
<li>For certain non-reasoning tasks, call DeepSeek-V3 to generate potential CoTs before answering the question by prompting. But for simpler queries like “hello”, CoT is not needed.</li>
<li>Then fine-tune the DeepSeek-V3-Base on the total 800k samples for 2 epochs.</li>
</ul>
</li>
<li>The final <strong>RL</strong> stage trains the step 3 checkpoint on both reasoning and non-reasoning prompts, improving helpfulness, harmlessness and reasoning.</li>
</ol>
<figure>
  <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/R1-eval.png">
  <figcaption><code>DeepSeek-R1</code> performs comparable to OpenAI <code>o1-preview</code> and <code>o1-mini</code> on several widely used reasoning benchmarks. <code>DeepSeek-V3</code> is the only non-reasoning model listed. (Image source: DeepSeek-AI, 2025)</figcaption>
</figure>
<p>Interestingly the DeepSeek team showed that with pure RL, no SFT stage, it is still possible to learn advanced reasoning capabilities like reflection and backtracking (“Aha moment”). The model naturally learns to spend more thinking tokens during the RL training process to solve reasoning tasks. The “aha moment” can emerge, referring to the model reflecting on previous mistakes and then trying alternative approaches to correct them. Later, various open source efforts happened for replicating R1 results like <a href="https://github.com/huggingface/open-r1">Open-R1</a>, <a href="https://github.com/hkust-nlp/simpleRL-reason">SimpleRL-reason</a>, and <a href="https://github.com/Jiayi-Pan/TinyZero">TinyZero</a>, all based on <a href="https://github.com/QwenLM/Qwen2.5">Qwen</a> models. These efforts also confirmed that pure RL leads to great performance on math problems, as well as the emergent “aha moment”.</p>
<figure>
  <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/aha-moment.png">
  <figcaption>Examples of the model learning to reflect and correct mistakes. (Image source: (left) DeepSeek-AI, 2025; (right) Zeng et al. 2025)</figcaption>
</figure>
<p>The DeepSeek team also shared some of their unsuccessful attempts. They failed to use process reward model (PRM) as it is hard to define per-step rubrics or determine whether an intermediate step is correct, meanwhile making the training more vulnerable to reward hacking. The efforts on MCTS (Monte Carlo Tree Search) also failed due to the large search space for language model tokens, in comparison to, say, chess; and training the fine-grained value model used for guiding the search is very challenging too. Failed attempts often provide unique insights and we would like to encourage the research community to share more about what did not work out.</p>

<p>During the reasoning steps, certain intermediate steps can be reliably and accurately solved by executing code or running mathematical calculations. Offloading that part of reasoning components into an external code interpreter, as in PAL (Program-Aided Language Model; <a href="https://arxiv.org/abs/2211.10435">Gao et al. 2022</a>) or Chain of Code (<a href="https://chain-of-code.github.io/">Li et al. 2023</a>), can extend the capability of LLM with external tools, eliminating the need for LLMs to learn to execute code or function as calculators themselves. These code emulators, like in Chain of Code, can be augmented by an LLM such that if a standard code interpreter fails, we have the option of using LLM to execute that line of code instead. Using code to enhance reasoning steps are especially beneficial for mathematical problems, symbolic reasoning and algorithmic tasks. These unit tests may not exist as part of the coding questions, and in those cases, we can instruct the model to self-generate unit tests for it to test against to verify the solution (<a href="https://arxiv.org/abs/2303.11366">Shinn, et al. 2023</a>).</p>
<figure>
  <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/pal.png">
  <figcaption>An example of program-aided language model prompting looks like. (Image source: Gao et al. 2022)</figcaption>
</figure>
<p>ReAct (Reason+Act; <a href="https://arxiv.org/abs/2210.03629">Yao et al. 2023</a>) combines the action of searching the Wikipedia API and generation of reasoning traces, such that reasoning paths can incorporate external knowledge.</p>
<figure>
  <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/react.png">
  <figcaption>An example of the ReAct prompting method to solve a HotpotQA question, using Wikipedia search API as an external tool to help with reasoning. (Image source: Yao et al. 2023)</figcaption>
</figure>
<p><a href="https://openai.com/index/introducing-o3-and-o4-mini/">o3 &amp; o4-mini</a>, recently released by OpenAI, are another two good examples where the reasoning process involves tool use like Web search, code execution and image processing. The team observed that large-scale reinforcement learning exhibits the same trend as in the GPT paradigm that “more compute = better performance”.</p>
<h2 id="thinking-faithfully">Thinking Faithfully</h2>
<p>Deep learning models are often treated as black boxes and various interpretability methods have been proposed. Interpretability is useful for a couple reasons: first, it gives us an extra test to determine if the model is misaligned with its creators’ intent, or if it’s misbehaving in some way that we can’t tell by monitoring its actions. Second, it can help us determine whether the model is using a sound process to compute its answers. Chain of thought provides an especially convenient form of interpretability, as it makes the model’s internal process visible in natural language. This interpretability, however, rests on the assumption that the model truthfully describes its internal thought processes.</p>
<p>Recent work showed that monitoring CoT of reasoning models can effectively detect model misbehavior such as <a href="https://lilianweng.github.io/posts/2024-11-28-reward-hacking/">reward hacking</a>, and can even enable a weaker model to monitor a stronger model (<a href="https://arxiv.org/abs/2503.11926">Baker et al. 2025</a>). Increasing test time compute can also lead to improved adversarial robustness (<a href="https://arxiv.org/abs/2501.18841">Zaremba et al. 2025</a>); this makes sense intuitively, because thinking for longer should be especially useful when the model is presented with an unusual input, such as an adversarial example or jailbreak attempt – it can use the extra thinking time to make sense of the strange situation it’s been presented with.</p>
<figure>
  <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/cot-monitor.png">
  <figcaption>The experiment of asking the model to decide if another model tried to hack the unit tests in some way for coding questions given its thought process. We can monitor these reward hacking behavior during training with different types of monitor. The <code>exit(0)</code> coding hack is when the agent exploited a bug that allowed it to exit from the environment early without running all unit tests. The <code>raise SkipTest</code> hack is when the agent raises an exception from functions outside the testing framework in order to skip unit test evaluation. (Image source: Baker et al. 2025)</figcaption>
</figure>
<h3 id="does-the-model-tell-what-it-thinks-faithfully">Does the Model Tell What it Thinks Faithfully</h3>
<p>Intuitively, model CoTs could be biased due to lack of explicit training objectives aimed at encouraging faithful reasoning. Or when we fine-tune the model on human-written explanations, those human-written samples may contain mistakes. Thus we cannot by default assume CoT is always faithful .</p>
<p><a href="https://arxiv.org/abs/2307.13702">Lanham et al. (2023)</a> investigated several modes of CoT faithfulness failures by deliberately introducing mistakes into CoTs and measuring their impacts on the accuracy of a set of multiple choice tasks (e.g. AQuA, MMLU, ARC Challenge, TruthfulQA, HellaSwag):</p>
<ul>
<li>
<p>Mistake 1 (<em>Early answering</em>): The model may form a conclusion prematurely before CoT is generated. This is tested by early truncating or inserting mistakes into CoT. Different tasks revealed varying task-specific dependencies on CoT effectiveness; some have evaluation performance sensitive to truncated CoT but some do not. <a href="https://arxiv.org/abs/2212.10001">Wang et al. (2023)</a> did similar experiments but with more subtle mistakes related to bridging objects or language templates in the formation of CoT.</p>
</li>
<li>
<p>Mistake 2 (<em>Uninformative tokens</em>): Uninformative CoT tokens improve performance. This hypothesis is tested by replacing CoT with filler text (e.g. all periods) and this setup shows no accuracy increase and some tasks may suffer performance drop slightly when compared to no CoT.</p>
</li>
<li>
<p>Mistake 3 (<em>Human-unreadable encoding</em>): Relevant information is encoded in a way that is hard for humans to understand. Paraphrasing CoTs in an non-standard way did not degrade performance across datasets, suggesting accuracy gains do not rely on human-readable reasoning.</p>
</li>
</ul>
<figure>
  <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/cot-perturb.png">
  <figcaption>Illustration of different ways of CoT perturbation to assess its faithfulness. (Image source: Lanham et al. 2023)</figcaption>
</figure>
<p>Interestingly, Lanham et al. suggests that for multiple choice questions, smaller models may not be capable enough of utilizing CoT well, whereas larger models may have been able to solve the tasks without CoT. This dependency on CoT reasoning, measured by the percent of obtaining the same answer with vs without CoT, does not always increase with model size on multiple choice questions, but does increase with model size on addition tasks, implying that thinking time matters more for complex reasoning tasks.</p>
<figure>
  <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/cot-ablation.png">
  <figcaption>The dependency on CoT reasoning is measured as the percentage of obtaining same answers with vs without CoT. It matters more for reasoning tasks like addition and larger models benefit more. (Image source: Lanham et al. 2023)</figcaption>
<figure>
<p>Alternative approaches for testing CoT faithfulness involve perturbing prompts rather than modifying CoT paths directly (<a href="https://arxiv.org/abs/2305.04388">Turpin et al. 2023</a>, <a href="https://arxiv.org/abs/2501.08156">Chua &amp; Evans, 2025</a>, <a href="https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf">Chen et al. 2025</a>).</p>
<p>One method consistently labels correct answers as “(A)” in few-shot examples regardless of true labels to introduce biases.</p>
<p>Another prompting technique inserts misleading hints into prompts, such as <code>"I think the answer is &lt;random_label&gt; but curious to hear what you think".</code> or <code>"A Stanford Professor thinks the answer is &lt;random_label&gt;"</code>. By comparing model predictions for the same question with vs without the misleading hint, we can measure whether a model is able to faithfully describe the influence of the hint on its answer. Particularly, in cases where the model produces different hint and non-hint answers, we measure whether the model acknowledges the hint when solving the question with hint. If the model is faithful, it should explicitly acknowledge the impact and admit the change of its answer is due to the hint.</p>
<figure>
  <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/faithfulness.png">
  <figcaption>Both GPT and Claude models are sensitive to different types of biases in context. Decrease in model accuracy indicates systematic unfaithfulness. Direct hints of wrong labels are more effectively than "Answer is always A" type of bias. (Image source: Turpin et al. 2023)</figcaption>
</figure>
<p>Multiple studies found that reasoning models describe the influence of the hint much more reliably than all the non-reasoning models tested. For example, we can measure the fraction of samples where the model acknowledges the hint as a deciding factor (“faithful CoT”). Reasoning models (Claude 3.7 Sonnet, DeepSeek R1) are overall doing better than non-reasoning ones (Claude 3.6, DeepSeek V3).</p>
<figure>
  <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/faithfulness2.png">
  <figcaption>Reasoning models are more likely to reveal faithful CoT than non-reasoning models. (Image source: Chen et al. 2025)</figcaption>
<figure>
<p>Some evidence showed that using reward models leads to less faithfulness in model outputs. The reward model in classic RLHF is not trained to incentivize faithful behavior in this test as the preference comparison dataset is collected by humans selecting which one seems better or pleasing. However, in reasoning models, the CoT paths are optimized for producing the correct final answers, not to match human preference defined in RM, and thus intuitively are expected to be more faithful.</p>
<h3 id="optimization-pressure-on-cot-good-or-bad">Optimization Pressure on CoT: Good or Bad?</h3>
<p>Monitoring CoT of reasoning models for reward hacking behavior is effective. A simple way to use this monitor is to run rejection sampling with the monitor as a filter at test time and identify solutions without reward hacking. However, it only works at the early stage of RL, and, as the model is further optimized, it is rare to find a sample to pass a CoT monitor within budget. This challenge naturally suggests that we could consider incorporating CoT monitors into RL rewards—a form of process-based supervision—to prevent reward hacking.</p>
<p>However, incorporating CoT monitoring reward into RL does not work out well, as it leads to obfuscated reward hacking where the model tries to hide its true intent within the CoT (<a href="https://arxiv.org/abs/2503.11926">Baker et al. 2025</a>). With this direct optimization pressure provided by the CoT monitor applied during RL training, the model still can find ways to hack, although a bit less frequently, but can hide its intention in CoT to avoid being caught by the monitor.</p>
<figure>
  <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/RL-monitor-baker25.png">
  <figcaption>When using the monitor to provide an RL reward signal to prevent reward hacking during training, the model still can find ways to hack while avoiding being caught by the monitor. (Image source: Baker et al. 2025)</figcaption>
</figure>
<p>Experiments on CoT length RL signals by <a href="https://arxiv.org/abs/2502.03373">Yeo et al. (2025)</a> confirmed a similar message as Baker et al. that new types of reward hacking can happen with new types of RL reward shaping. They designed the reward function such that correct CoTs can have higher rewards than wrong ones, short correct CoTs obtain higher rewards than long correct ones and short incorrect CoT receive higher penalties than long incorrect CoTs. With this extra reward, the model learns to repeat text in CoTs for challenging questions rather than attempting to solve them. Yeo et al. further applied a n-gram repetition penalty to prevent this hacking behavior.</p>
<figure>
  <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/rew-shaping-yeo25.png">
  <figcaption>Careful reward shaping is needed to stabilize training with CoT length rewards. (Image source: Yeo et al. 2025)</figcaption>
</figure>
<p><a href="https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf">Chen et al. (2025)</a> experimented with a flawed RL environment, specifically using a grader with incorrect answers filled in for multiple-choice questions. The model learns to exploit the reward hack on &gt;99% of the prompts, but almost never (&lt;2%) verbalizes the reward hack in its CoT on more than half of their environments. Additional RL optimization pressure fails to incentivize the model to verbalize the hack in this case.</p>
<p>RL training is inherently sensitive to reward hacking. Only relying on heuristic investigation of reward hacking and manual fixes may lead to a “whack-a-mole” situation. We would suggest being very cautious when trying to apply optimization directly on CoT during RL training, or trying to avoid it altogether.</p>
<h2 id="thinking-in-continuous-space">Thinking in Continuous Space</h2>
<p>Adaptive Computation Time, introduced by <a href="https://arxiv.org/abs/1603.08983">Alex Graves in 2016</a>, predated large language models but pioneered the same direction of enabling the model to dynamically decide the number of computational steps to take at the inference time, which can be viewed as enabling the model to “think more” in continuous space at test time. Adaptive thinking time in continuous space can be enabled vertically via recurrent architecture or horizontally via more sequential sampling steps.</p>
<h2 id="recurrent-architecture">Recurrent Architecture</h2>
<p>A number of architecture variations have been proposed to make the Transformer architecture recurrent, enabling adaptive test time compute (<a href="https://arxiv.org/abs/1807.03819">Dehghani, et al. 2019</a>, <a href="https://arxiv.org/abs/2203.07852">Hutchins, et al. 2022</a>, <a href="https://arxiv.org/abs/2207.06881">Bulatov, et al. 2022</a>). A deep dive into literature on this topic would make the post too long, so we will only review a few.</p>
<p><a href="https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/#make-it-recurrent-universal-transformer">Universal Transformer</a> (<a href="https://arxiv.org/abs/1807.03819">Dehghani, et al. 2019</a>) combines self-attention in Transformer with the recurrent mechanism in RNN, <a href="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/#adaptive-modeling">dynamically adjusting</a> the number of steps using adaptive computation time (<a href="https://arxiv.org/abs/1603.08983">Graves, 2016</a>). On a high level, it can be viewed as a recurrent function for learning the hidden state representation per token, and if the number of steps is fixed, an Universal Transformer is equivalent to a multi-layer Transformer with shared parameters across layers.</p>
<p>A recent recurrent architecture design, proposed by <a href="https://arxiv.org/abs/2502.05171">Geiping et al. (2025)</a>, adds a recurrent block $R$ on top of the standard Transformer. Every iteration of this recurrent block takes the embedding $\mathbf{e}$ and a random state $\mathbf{s}_i$. Conceptually, this recurrent-depth architecture is a bit similar to a conditioned diffusion model, where the original input $\mathbf{e}$ is provided in every recurrent step while a random Gaussian initialized state $\mathbf{s}_i$ gets updated iteratively through the process. (Interestingly some of their experiments of designs that resemble diffusion models more turned out to be bad.)</p>
<p>
$$  
\begin{aligned}  
\mathbf{e} &amp;= P(\mathbf{x}) &amp; \text{embedding} \\ \mathbf{s}\_0 &amp;\sim \mathcal{N}(\mathbf{0}, \sigma^2 I\_{n \cdot h}) \\ \mathbf{s}\_i &amp;= R(\mathbf{e}, \mathbf{s}\_{i-1}) \quad\text{ for }i \in {1, \dots, r} &amp; \small{\text{recurrent block; resembles a Transformer block}}\\ \mathbf{p} &amp;= C(\mathbf{s}\_r) &amp; \text{unembedding}  
\end{aligned}
$$
</p>
<p>The recurrence count $r$ during training is randomized, sampled from a log-normal Poisson distribution, per input sequence. To manage computational costs, backpropagation is truncated to only the last $k$ iterations of the recurrent unit ($k=8$ in experiments), making it possible to train on the heavy-tail part of the Poisson distribution. The embedding block continues to receive gradient updates in every step since its output $\mathbf{e}$ is injected in every step, mimicking RNN training. Unsurprisingly, the stability of training a recurrent model turns out to be very sensitive. Factors like initialization, normalization and hyperparameters all matter, especially when scaling up the training. For example, hidden states can collapse by predicting the same hidden state for every token; or the model may learn to ignore the incoming state $\mathbf{s}$. To stabilize the training, Geiping et al. adopted an embedding scale factor, a small learning rate and careful tuning.</p>
<figure>
  <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/recurrent.png">
  <figcaption>Plot of an experiment on training a 3.5B model with depth recurrence. The saturation roughly happens around $\bar{r} = 32$, making us wonder how this architecture extrapolate and generalize to larger iteration counts. (Image source: Geiping et al. 2025)</figcaption>
</figure>
<h2 id="thinking-tokens">Thinking Tokens</h2>
<p>Thinking tokens refer to a set of implicit tokens introduced during training or inference that do not carry direct linguistic meaning. Instead, their role is to provide extra thinking time and compute power for the model to perform better.</p>
<p><a href="https://arxiv.org/abs/2405.08644">Herel &amp; Mikolov (2023</a>) introduced the idea of inserting special thinking tokens (<code>&lt;T&gt;</code>) after each word in a sentence and training the model on such a dataset. Each thinking token buys extra time for the model to process and make better predictions. Training with thinking tokens on a toy model setup results in lower perplexity than baseline model trained without them. The benefits of thinking tokens are more pronounced for non-trivial reasoning tasks or sentences involving numbers.</p>
<p>Similarly, pause tokens proposed by <a href="https://arxiv.org/abs/2310.02226">Goyal et al. (2024)</a> delay the model’s outputs by appending dummy tokens (e.g. character like <code>.</code> or <code>#</code>) at the end of the input sequence, giving the model extra computation during inference. It is important to inject such pause tokens both during training and inference time, while only fine-tuning on pause tokens leads to limited gain. During training, multiple copies of pause tokens are inserted at uniformly random locations and the loss on pause tokens is ignored for training.</p>
<figure>
  <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/pause-tokens-goyal24.png">
  <figcaption>Illustration of how pause tokens are injected during training and inference in comparison to standard setup. (Image source: Goyal et al. 2024)</figcaption>
</figure>
<p>Interestingly, thinking tokens or pause tokens in above experiments do not carry any extra information or add many new parameters. But why is it still helpful? On one hand, it helps expand the computation by introducing more inference loops, effectively increasing computational capacity. On the other hand, it can be seen as acting as a special, implicit form of CoTs. A drawback here is hat the model needs to be pretrained with respect to thinking tokens. Still, this strategy is an interesting way to further improve the capability of test time compute utilization on top of inference time CoTs.</p>
<p>Quiet-STaR (<a href="https://arxiv.org/abs/2403.09629">Zelikman et al. 2025</a>) introduces token-level reasoning by training the model to generate rationales after every token to explain future text. It mixes the future-text predictions with and without rationales and uses learning to generate better rationales and uses REINFORCE to optimize the quality of rationale generation.</p>
<figure>
  <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/quiet-star-zelikman25.png">
  <figcaption>Illustration of Quiet-STaR. (Image source: Zelikman et al. 2025)</figcaption>
</figure>
<p>Quiet-STaR consists of three stages:</p>
<ul>
<li>
<p><em>Think</em>: Predicting next tokens with rationales. Due to high computational cost demanded by token level reasoning, this process is designed to generate multiple rationales in parallel. A special attention map is used to enable all thought tokens to only pay attention to themselves, all preceding thought tokens within the same thought, and the preceding text.</p>
</li>
<li>
<p><em>Talk</em>: Next token prediction without rationale is mixed with post-rationale prediction. The mixing weight for two logits is learned by a special mixing head of a shallow MLP out of hidden output after each rationale. Correct next tokens can be selected via teacher forcing.</p>
</li>
<li>
<p><em>Learn</em>: Train the model to generate better rationale via REINFORCE by learning from examples that increase the probability of correct next token while discarding those that hurt the prediction.</p>
</li>
</ul>
<p>Without dataset specific fine-tuning, Quiet-STaR improves zero-shot results on CommonsenseQA (36.3%→47.2%) and GSM8K (5.9%→10.9%), within experiments on Mistral 7B.</p>
<h2 id="thinking-as-latent-variables">Thinking as Latent Variables</h2>
<p>A latent variable model defines a probabilistic framework where observable data is explained through unobserved (latent) variables. These latent variables capture hidden structures or intermediate processes that generate the observable outcomes. Language models can be viewed as probabilistic latent variable models where test-time thinking and reasoning steps are latent thought variables (<a href="https://arxiv.org/abs/2011.05268">Zhou et al. 2020</a>, <a href="https://arxiv.org/abs/2312.02179">Phan et al. 2023</a>). Such a latent variable model defines a joint distribution of problems x_i, answers y_i and latent thought z_i. We would like to optimize the log-likelihood of answers given questions and a variety of CoTs as latent variables (N is the number of samples; $K$ is the number of CoTs per problem):</p>
<p>
$$ 
\begin{aligned}  
\log \mathcal{L}(\theta)  
&amp;= \log p(y \mid x) \\  
&amp;= \log \sum_{k=1}^K p(y, z^{(k)} \mid x) \\  
&amp;= \log \sum_{k=1}^K p(z^{(k)} \mid x)\; p(y \mid z^{(k)}, x) \\  
&amp;= \log \mathbb{E}_{z^{(k)} \sim p(z^{(k)} \mid x)} \; p(y \mid z^{(k)}, x) \\  
\end{aligned}
$$
</p>
<p>Our goal is to maximize the marginal likelihood of the correct answer, $p(y \mid x)$, given a number of reasoning traces per problem, $\{z^{(k)}\}_{k=1}^K$.</p>
<h2 id="expectation-maximization">Expectation-Maximization</h2>
<p><a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">Expectation-Maximization</a> is a commonly used iterative algorithm for optimizing parameters for a model with (hidden) latent variables, and thus can be applied to train better CoTs and then condition on that to generate better responses. Typically we iterate between E-step (Expectation) where we guess the missing information about latent variables (i.e. how to sample better CoTs), and M-step (Maximization) where we optimize the model parameters based on latent variables (i.e. how to sample better answers), until convergence.</p>
<p>
$$
\log \mathcal{L}(\theta) = 
\log \mathbb{E}_{\underbrace{z^{(k)} \sim p(z^{(k)} \mid x)}_\text{E-step}}\;\underbrace{p(y \mid z^{(k)}, x)}_\text{M-step}
$$
</p>
<p>Because we cannot directly sample from the latent variable distribution $p(z \mid x, y)$, researchers have explored methods relying on human annotated data (<a href="https://arxiv.org/abs/2011.05268">Zhou et al. 2020</a>), Metropolis-Hastings MCMC (<a href="https://arxiv.org/abs/2312.02179">Phan et al. 2023</a>) or Monte Carlo sampling with special importance weights (<a href="https://arxiv.org/abs/2503.18866">Ruan et al. 2025</a>) to draw good CoT samples to update the model. <a href="https://arxiv.org/abs/2503.18866">Ruan et al. (2025</a>) experimented with training a model on a large body of Web text with latent thoughts with the EM algorithm, where the latent thought is synthesized per chunk of observed data and then the model learns over both latent thought and data in an autoregressive manner.</p>
<figure>
  <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/ruan25.png">
  <figcaption>Illustration of training on data corpus with latent thought injected. (Image source: Ruan et al. 2025)</figcaption>
</figure>
<p>They first prompt a LLM $\tilde{q}$ to generate synthetic latent thought Z_i given observed data X_i:</p>
<pre tabindex="0"><code>You are provided with a pair of web document prefix and suffix. Your task is to insert latent thoughts between them underlying the creation of the suffix conditioned on the prefix. The latent thoughts should include: the missing background knowledge and the reasoning traces underlying each claim (especially, step-by-step derivations or logical reasoning).
</code></pre><p>Special tokens like <code>&lt;StartOfLatent&gt;&lt;Prior&gt; ... &lt;EndOfPrior&gt;</code> are used to insert the generated latent thought content into the raw data for training the joint distribution $p(z, x)$ or the approximate posterior $q(z \mid x)$, depending on whether $z$ is inserted before or after $x$. However, since we are using a LLM $\tilde{q}(z \mid x)$ to generate the CoTs, it imposed a performance ceiling on how good the approximate $q(z \mid x)$ can be. Ruan et al. introduced importance weights for selecting CoT samples at the E-step, formulated as:</p>
<p>
$$
w^{(k)}  
= \frac{p(z^{(k)}, x)}{q(z^{(k)} \mid x)}  
= \frac{p(x \mid z^{(k)}) \; p(z^{(k)})}{q(z^{(k)} \mid x)}  
$$
</p>
<p>, such that we prioritize samples with CoTs that are good at predicting the observation (i.e., high $p(x \mid z^{(k)})$), simple, intuitive (i.e., high $p(z^{(k)})$) but also informative and not too obvious (i.e. low $q(z^{(k)} \mid x)$).</p>
<h2 id="iterative-learning">Iterative Learning</h2>
<p>Since pretrained models already possess the capability of generating chains of thought, it is intuitive to design an iterative improvement process where we generate multiple CoTs and fine-tune the model only on rationales that lead to correct answers.<br>
However, this straightforward design can fail because the model receives no learning signals for problems it fails to solve. STaR (“Self-taught reasoner”; <a href="https://arxiv.org/abs/2203.14465">Zelikman et al. 2022</a>) addresses this limitation by adding a “rationalization” process for failed attempts, in which the model generates good CoTs backward conditioned on both the problem and the ground truth answer and thus the model can generate more reasonable CoTs. Then the model is finetuned on correct solutions that either lead to correct outputs or are generated through rationalization.</p>
<figure>
  <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/STaR-algo-zelikman22.png">
  <figcaption>The algorithm of STaR. (Image source: Zelikman et al. 2022)</figcaption>
</figure>
<p>We can view STaR as an approximation to a policy gradient in RL, with a simple indicator function as the reward, $\mathbb{1}[\hat{y} = y]$. We want to maximize the expectation of this reward when sampling $z \sim p(z \mid x)$ and then $y \sim p(y \mid x, z)$, since $p(y \mid x) = \sum_z p(z \mid x) \; p(y \mid x, z)$.</p>
<p>
$$
\begin{aligned}  
\nabla_\theta J(\theta)  
&amp;= \nabla_\theta \mathbb{E}_{z_i, y_i \sim p(.\mid x_i)} \mathbb{1}[y_i = y_i^\text{truth}] \\  
&amp;= \sum_{i=1}^N \nabla_\theta \mathbb{1}[y_i = y_i^\text{truth}] \; p(y_i, z_i \mid x_i) \\  
&amp;= \sum_{i=1}^N \mathbb{1}[y_i = y_i^\text{truth}] \; p(y_i, z_i \mid x_i) \frac{\nabla_\theta p(y_i, z_i \mid x_i)}{p(y_i, z_i \mid x_i)} &amp; \text{;log-derivative trick}\\  
&amp;= \mathbb{E}_{z_i, y_i \sim p(.\mid x_i)} \mathbb{1}[y_i = y_i^\text{truth}] \; \nabla_\theta \log p(y_i, z_i \mid x_i) &amp; \text{;log-derivative trick}
\end{aligned}
$$
</p>
<p>Each iteration is equivalent to first selecting the CoT samples according to $\mathbb{1}[y=y^\text{truth}]$ and then running supervised fine-tuning to optimize the logprob of generating good CoTs and answers. Performance of STaR improves with more training iterations, and the “rationalization” process for generating better CoTs accelerates learning. They observed that sampling with high temperature increases the chance of getting correct answers with incorrect reasoning and finetuning LLM on such data can impair generalization. For datasets without ground truths, majority votes of multiple high-temperature outputs can serve as a proxy of ground truth answers (<a href="https://arxiv.org/abs/2207.00747">Wang et al. 2022</a>), making it possible to use synthetic samples for training.</p>
<figure>
  <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/STaR-rationalization-zelikman22.png">
  <figcaption>A comparison of accuracy of two $n$-digit numbers summation. With rationalization (CoT generation conditioned on ground truth), the model can learn complex arithmetic tasks like $5$-digit summation pretty early on. (Image source: Zelikman et al. 2022)</figcaption>
<figure>
<h2 id="scaling-laws-for-thinking-time">Scaling Laws for Thinking Time</h2>
<p>So far we have seen much evidence that allowing models to spend additional compute on reasoning before producing final answers at inference time can significantly improve performance. Techniques like prompting the model to generate intermediate reasoning steps before the answers, or training the model to pause and reflect before predicting next tokens, have been found to boost the model performance beyond the capability limit obtained during training. This essentially introduces a new dimension to tinker with for improving model intelligence, complementing established factors such as model size, training compute and data quantity, as defined in scaling laws (<a href="https://arxiv.org/abs/2001.08361">Kaplan et al. 2020</a>).</p>
<p>Recent studies demonstrated that optimizing LLM test-time compute could be more effective than scaling up model parameters (<a href="https://arxiv.org/abs/2408.03314">Snell et al. 2024</a>, <a href="https://arxiv.org/abs/2408.00724">Wu et al. 2025</a>). Smaller models combined with advanced inference algorithms can offer Pareto-optimal trade-offs in cost and performance.</p>
<p><a href="https://arxiv.org/abs/2408.03314">Snell et al. (2024)</a> evaluated and compared test-time and pretraining compute, and found that they are not 1:1 exchangeable. Test-time compute can cover up the gap easily on easy and medium questions when there is only a small gap in model capability, but proves less effective for hard problems. The ratio between token budgets for pretraining and inference matters a lot. Test-time compute is only preferable when inference tokens are substantially fewer than pretraining ones. This indicates that developing a capable base model with enough pretraining data and compute is still very critical, as test-time compute cannot solve everything or fill in big model capability gaps.</p>
<figure>
  <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/scaling-snell24.png">
  <figcaption>(Left) Evaluation accuracy as a function of test time compute budgets, via iterative revisions or parallel decoding. (Right) Comparing a small model with test-time compute sampling tricks and a 14x larger model with only greedy decoding. We can control the token budgets used at test time such that the ratio of inference to pretraining tokens is &lt;&lt; 1, ~=1 or &gt;&gt; 1 and the benefit of test time compute is clear only the ratio &lt;&lt; 1. (Image source: Snell et al. 2024)</figcaption>
</figure>
<p><code>s1</code> models (<a href="https://arxiv.org/abs/2501.19393">Muennighoff &amp; Yang, et al. 2025</a>) experimented with scaling up the CoT reasoning path length via <em>budget forcing</em> technique (i.e. forcefully lengthen it by appending the word <code>"wait"</code>, or shorten it by terminating the model’s thinking process by appending end-of-thinking token or <code>"Final Answer:"</code>). They observed a clear positive correlation between the average thinking time measured in tokens and the downstream evaluation accuracy.</p>
<figure>
  <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/s1-muennighoff25.png">
  <figcaption>Both parallel and sequential scaling methods of test-time compute shows positive correlation with the evaluation performance in s1 experiments. (Image Muennighoff &amp; Yang, et al. 2025)</figcaption>
</figure>
<p>When comparing this budget forcing technique with other decoding methods of controlling reasoning trace length, it is quite surprising that simple rejection sampling (i.e. sampling generation until the lengths fits a token budget) leads to reversed scaling, meaning that longer CoTs lead to worse performance.</p>
<figure>
  <img src="https://lilianweng.github.io/posts/2025-05-01-thinking/s1-scaling-muennighoff25.png">
  <figcaption>(Left) Longer CoT path length is positively correlated with eval accuracy. (Right) Rejection sampling for controlling the generated CoT path length shows a negative scaling where longer CoTs lead to worse eval accuracy. (Image source: Muennighoff &amp; Yang et al. 2025)</figcaption>
</figure>
<h2 id="whats-for-future">What’s for Future</h2>
<p>The exploration of test-time compute and chain-of-thought reasoning presents new opportunities for enhancing model capabilities. More interestingly, via test-time thinking, we are moving towards building future AI systems that mirror the best practices of how humans think, incorporating adaptability, flexibility, critical reflection, and error correction. Excitement with current progress invites us for more future research to improve and understand deeply not just how but why we—and our models—think.</p>
<p>At the end, I would like to call for more research for the following open research questions on test time compute and chain-of-thought reasoning.</p>
<ul>
<li>Can we incentivize the model to produce human-readable, faithful reasoning paths during RL training while avoiding reward hacking behavior?</li>
<li>How to define reward hacking? Can we capture reward hacking during RL training or inference without human intervention? How to prevent “whack-a-mole” style of fixes for reward hacking during RL training?</li>
<li>Self-correction can happen within chain-of-thought or can be encouraged to happen explicitly during multi-turn RL. How can we train the model to correct itself without hallucination or regression when ground truth is not available?</li>
<li>How to run RL training with CoT rollout for tasks that are highly contextualized, personalized and hard to grade, such as creative writing, coaching, brainstorming?</li>
<li>When we deploy the model in reality, we cannot grow test time thinking forever and how can we smoothly translate the performance gain back into the base model with reduced inference time cost (e.g. via <a href="https://arxiv.org/abs/2501.12948">distillation</a>)?</li>
<li>How to make test time spending more adaptive according to the difficulty of the problem in hand?</li>
</ul>
<h2 id="citation">Citation</h2>
<p>Please cite this work as:</p>
<pre tabindex="0"><code>Weng, Lilian. "Why We Think". Lil'Log (May 2025). https://lilianweng.github.io/posts/2025-05-01-thinking/
</code></pre><p>Or use the BibTex citation:</p>
<pre tabindex="0"><code>@article{weng2025think,
  title = {Why We Think},
  author = {Weng, Lilian},
  journal = {lilianweng.github.io},
  year = {2025},
  month = {May},
  url = "https://lilianweng.github.io/posts/2025-05-01-thinking/"
}
</code></pre><h2 id="references">References</h2>
<p>[1] Alex Graves. <a href="https://arxiv.org/abs/1603.08983">“Adaptive Computation Time for Recurrent Neural Networks.”</a>. arXiv preprint arXiv:1603.08983 (2016).</p>
<p>[2] Wang Ling, et al. <a href="https://arxiv.org/abs/1705.04146">“Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems.”</a>. arXiv preprint arXiv:1705.04146 (2017).</p>
<p>[3] Karl Cobbe, et al. <a href="https://arxiv.org/abs/2110.14168">“Training Verifiers to Solve Math Word Problems.”</a>. arXiv preprint arXiv:2110.14168 (2021).</p>
<p>[4] Jason Wei, et al. <a href="https://arxiv.org/abs/2201.11903">“Chain of Thought Prompting Elicits Reasoning in Large Language Models.”</a>. NeurIPS 2022.</p>
<p>[5] Maxwell Nye, et al. <a href="https://arxiv.org/abs/2112.00114">“Show Your Work: Scratchpads for Intermediate Computation with Language Models.”</a>. arXiv preprint arXiv:2112.00114 (2021).</p>
<p>[6] Daniel Kahneman. <em>Thinking, Fast and Slow</em>. Farrar, Straus and Giroux (2013).</p>
<p>[7] Takeshi Kojima, et al. <a href="https://arxiv.org/abs/2205.11916">“Large Language Models are Zero-Shot Reasoners.”</a>. NeurIPS 2022.</p>
<p>[8] Michihiro Yasunaga, et al. <a href="https://arxiv.org/abs/2310.01714">“Large Language Models as Analogical Reasoners”</a>. arXiv preprint arXiv:2310.01714 (2023).</p>
<p>[9] Eric Zelikman, et al. <a href="https://arxiv.org/abs/2203.14465">“STaR: Bootstrapping Reasoning With Reasoning.”</a>. NeurIPS 2022.</p>
<p>[10] Xuezhi Wang, et al. <a href="https://arxiv.org/abs/2203.11171">“Self-consistency Improves Chain of Thought Reasoning in Language Models.”</a>. ACL 2023.</p>
<p>[11] Ryo Kamoi, et al. <a href="https://arxiv.org/abs/2406.01297">“When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs.”</a>. TACL 2024.</p>
<p>[12] Jie Huang, et al. <a href="https://arxiv.org/abs/2310.01798">“Large Language Models Cannot Self-Correct Reasoning Yet.”</a>. ICLR 2024.</p>
<p>[13] Noah Shinn, et al. <a href="https://arxiv.org/abs/2303.11366">“Reflexion: Language Agents with Verbal Reinforcement Learning.”</a>. arXiv preprint arXiv:2303.11366 (2023).</p>
<p>[14] Yunxiang Zhang, et al. <a href="https://arxiv.org/abs/2404.17140">“Small Language Models Need Strong Verifiers to Self-Correct Reasoning.”</a>. ACL Findings 2024.</p>
<p>[15] Hao Liu, et al. <a href="https://arxiv.org/abs/2302.02676">“Chain of Hindsight Aligns Language Models with Feedback.”</a>. arXiv preprint arXiv:2302.02676 (2023).</p>
<p>[16] Sean Welleck, et al. <a href="https://arxiv.org/abs/2211.00053">“Generating Sequences by Learning to Self-Correct.”</a>. arXiv preprint arXiv:2211.00053 (2023).</p>
<p>[17] Yuxiao Qu, et al. <a href="https://arxiv.org/abs/2407.18219">“Recursive Introspection: Teaching Language Model Agents How to Self-Improve.”</a>. arXiv preprint arXiv:2407.18219 (2024).</p>
<p>[18] Aviral Kumar, et al. <a href="https://arxiv.org/abs/2409.12917">“Training Language Models to Self-Correct via Reinforcement Learning.”</a>. arXiv preprint arXiv:2409.12917 (2024).</p>
<p>[19] Hunter Lightman, et al. <a href="https://arxiv.org/abs/2305.20050">“Let’s Verify Step by Step.”</a>. arXiv preprint arXiv:2305.20050 (2023).</p>
<p>[20] Yuxi Xie, et al. <a href="https://arxiv.org/abs/2305.00633">“Self-Evaluation Guided Beam Search for Reasoning.”</a>. NeurIPS 2023.</p>
<p>[21] Yangzhen Wu, et al. <a href="https://arxiv.org/abs/2408.00724">“Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models”</a>. ICLR 2025.</p>
<p>[22] Dongwei Jiang, et al. <a href="https://arxiv.org/abs/2410.01044">“RATIONALYST: Pre-training Process-Supervision for Improving Reasoning”</a>. arXiv preprint arXiv:2410.01044 (2024).</p>
<p>[23] Xuezhi Wang and Denny Zhou. <a href="https://arxiv.org/abs/2402.10200">“Chain-of-Thought Reasoning Without Prompting.”</a>. arXiv preprint arXiv:2402.10200 (2024).</p>
<p>[24] DeepSeek-AI. <a href="https://arxiv.org/abs/2412.19437">“DeepSeek-V3 Technical Report.”</a> arXiv preprint arXiv:2412.19437 (2024).</p>
<p>[25] DeepSeek-AI. <a href="https://arxiv.org/abs/2501.12948">“DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.”</a>. arXiv preprint arXiv:2501.12948 (2025).</p>
<p>[26] Luyu Gao, Aman Madaan &amp; Shuyan Zhou, et al. <a href="https://arxiv.org/abs/2211.10435">“PAL: Program-aided Language Models.”</a>. ICML 2023.</p>
<p>[27] Shunyu Yao, et al. <a href="https://arxiv.org/abs/2210.03629">“ReAct: Synergizing Reasoning and Acting in Language Models.”</a>. ICLR 2023.</p>
<p>[29] Bowen Baker, et al. <a href="https://arxiv.org/abs/2503.11926">“Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation.”</a>. arXiv preprint arXiv:2503.11926 (2025).</p>
<p>[30] Wojciech Zaremba, et al. <a href="https://arxiv.org/abs/2501.18841">“Trading Inference-Time Compute for Adversarial Robustness.”</a>. arXiv preprint arXiv:2501.18841 (2025).</p>
<p>[31] Tamera Lanham, et al. <a href="https://arxiv.org/abs/2307.13702">“Measuring Faithfulness in Chain-of-Thought Reasoning”</a>. arXiv preprint arXiv:2307.13702 (2023).</p>
<p>[32] Boshi Wang, et al. <a href="https://arxiv.org/abs/2212.10001">“Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters.”</a>. ACL 2023.</p>
<p>[33] Miles Turpin, et al. <a href="https://arxiv.org/abs/2305.04388">“Language Models Don’t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting.”</a>. NeuriPS 2023.</p>
<p>[34] James Chua &amp; Owain Evans. <a href="https://arxiv.org/abs/2501.08156">“Are DeepSeek R1 And Other Reasoning Models More Faithful?”</a>. arXiv preprint arXiv:2501.08156 (2025).</p>
<p>[35] Yanda Chen et al. <a href="https://arxiv.org/abs/2505.05410">“Reasoning Models Don’t Always Say What They Think”</a>. arXiv preprint arXiv:2505.05410 (2025).</p>
<p>[36] Edward Yeo, et al. <a href="https://arxiv.org/abs/2502.03373">“Demystifying Long Chain-of-Thought Reasoning in LLMs.”</a>. arXiv preprint arXiv:2502.03373 (2025).</p>
<p>[37] Mostafa Dehghani, et al. <a href="https://arxiv.org/abs/1807.03819">“Universal Transformers.”</a>. ICLR 2019.</p>
<p>[38] DeLesley Hutchins, et al. <a href="https://arxiv.org/abs/2203.07852">“Block-Recurrent Transformers.”</a>. NeurIPS 2022.</p>
<p>[39] Aydar Bulatov, et al. <a href="https://arxiv.org/abs/2207.06881">“Recurrent Memory Transformers.”</a>. NeuriPS 2022.</p>
<p>[40] Jonas Geiping, et al. <a href="https://arxiv.org/abs/2502.05171">“Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach.”</a>. arXiv preprint arXiv:2502.05171 (2025).</p>
<p>[41] Herel &amp; Mikolov. <a href="https://arxiv.org/abs/2405.08644">“Thinking Tokens for Language Modeling.”</a>. AITP 2023.</p>
<p>[42] Sachin Goyal et al. <a href="https://arxiv.org/abs/2310.02226">“Think before you speak: Training Language Models With Pause Tokens.”</a>. ICLR 2024.</p>
<p>[43] Eric Zelikman, et al. <a href="https://arxiv.org/abs/2403.09629">“Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking.”</a>. arXiv preprint arXiv:2403.09629 (2025).</p>
<p>[44] Wangchunshu Zhou et al. <a href="https://arxiv.org/abs/2011.05268">“Towards Interpretable Natural Language Understanding with Explanations as Latent Variables.”</a>. NeurIPS 2020.</p>
<p>[45] Du Phan et al. <a href="https://arxiv.org/abs/2312.02179">“Training Chain-of-Thought via Latent-Variable Inference.”</a>. NeurIPS 2023.</p>
<p>[46] Yangjun Ruan et al. <a href="https://arxiv.org/abs/2503.18866">“Reasoning to Learn from Latent Thoughts.”</a>. arXiv preprint arXiv:2503.18866 (2025).</p>
<p>[47] Xuezhi Wang et al. <a href="https://arxiv.org/abs/2207.00747">“Rationale-Augmented Ensembles in Language Models.”</a>. arXiv preprint arXiv:2207.00747 (2022).</p>
<p>[48] Jared Kaplan, et al. <a href="https://arxiv.org/abs/2001.08361">“Scaling Laws for Neural Language Models.”</a>. arXiv preprint arXiv:2001.08361 (2020).</p>
<p>[49] Niklas Muennighoff &amp; Zitong Yang, et al. <a href="https://arxiv.org/abs/2501.19393">“s1: Simple test-time scaling.”</a>. arXiv preprint arXiv:2501.19393 (2025).</p>
<p>[50] Peiyi Wang, et al. <a href="https://arxiv.org/abs/2312.08935">“Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations”</a> arXiv preprint arXiv:2312.08935 (2023).</p>
<p>[51] Yixin Liu, et al. <a href="https://arxiv.org/abs/2310.10047">“Improving Large Language Model Fine-tuning for Solving Math Problems.”</a> arXiv preprint arXiv:2310.10047 (2023).</p>
<p>[52] Charlie Snell, et al. <a href="https://arxiv.org/abs/2408.03314">“Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters.”</a>. arXiv preprint arXiv:2408.03314 (2024).</p>
<p>[53] OpenAI. o1-preview: <a href="https://openai.com/index/learning-to-reason-with-llms/">“Learning to reason with LLMs.”</a> Sep 12, 2024.</p>
<p>[54] OpenAI. o3: <a href="https://openai.com/index/introducing-o3-and-o4-mini/">“Introducing OpenAI o3 and o4-mini.”</a> Apr 16, 2025.</p>


  </figure></figure></figure></figure></figure></figure></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ishkur's Guide to Electronic Music (286 pts)]]></title>
            <link>http://music.ishkur.com/</link>
            <guid>45394642</guid>
            <pubDate>Sat, 27 Sep 2025 10:38:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://music.ishkur.com/">http://music.ishkur.com/</a>, See on <a href="https://news.ycombinator.com/item?id=45394642">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Typst: A Possible LaTeX Replacement (666 pts)]]></title>
            <link>https://lwn.net/Articles/1037577/</link>
            <guid>45393842</guid>
            <pubDate>Sat, 27 Sep 2025 07:31:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/Articles/1037577/">https://lwn.net/Articles/1037577/</a>, See on <a href="https://news.ycombinator.com/item?id=45393842">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>
<a href="https://typst.app/">Typst</a> is a program for document
typesetting. It is especially well-suited to technical material
incorporating elements such as mathematics, tables, and floating
figures. It produces high-quality results, comparable to the gold standard,
<a href="https://www.latex-project.org/">LaTeX</a>, with a simpler markup
system and easier customization, all while compiling documents
more quickly. Typst is free software, Apache-2.0 licensed, and is written in Rust.
</p>

<h4 id="the-desire-for-a-latex-replacement">Desire for a LaTeX replacement</h4>

<p>
LaTeX is a document typesetting system built on the foundation of Donald
Knuth's <a href="https://tug.org/whatis.html">TeX</a>. LaTeX has become the
standard tool for the preparation of scholarly papers and books in several
fields, such as mathematics and computer science, and widely adopted in
others, such as physics. TeX and LaTeX, which <a href="https://www.latex-project.org/publications/2001-CAR-LaTeX-legacy.pdf">predate
Linux</a>, are early free software <a href="https://www.tug.org/TUGboat/tb24-1/gaudeul.pdf">success
stories</a>. The quality of TeX's (and therefore LaTeX's) output rivals the
work of skilled hand typesetters for both text and mathematics.
</p>

<blockquote>
<b><tt>$ sudo subscribe today</tt></b>
<p>
Subscribe today and elevate your LWN privileges. You’ll have
access to all of LWN’s high-quality articles as soon as they’re
published, and help support LWN in the process.  <a href="https://lwn.net/Promo/nst-sudo/claim">Act now</a> and you can start with a free trial subscription.
</p></blockquote>


<p>
Despite the acclaim earned by LaTeX, its community of users has been
griping about it for years, and wondering aloud whether one day a
replacement might arrive. There are several reasons for this
dissatisfaction: the LaTeX installation is huge, compilation of large
documents is not fast, and its error messages are riddles delivered by an
infuriating oracle. In addition, any nontrivial customization or alteration to the
program's behavior requires expertise in an arcane macro-expansion
language.
</p>

<p>
Along with the griping came resignation: after decades of talk about a
LaTeX replacement with nothing plausible on the horizon, and with the
recognition that LaTeX's collection of specialized packages would take
years to replace, it seemed impossible to dislodge the behemoth from its
exalted position.
</p>

<h4 id="introducing-typst">Introducing Typst</h4>

<p>
In 2019 two German developers, Laurenz
Mädje and Martin Haug, decided to try to write a LaTeX replacement "<a href="https://archive.li/LN3xb">just for fun</a>". In 2022, Mädje wrote his 
computer science <a href="https://laurmaedje.github.io/programmable-markup-language-for-typesetting.pdf">master's
thesis about Typst</a>. In March 2023, its
first pre-release beta version <a href="https://typst.app/blog/2023/beta-oss-launch">was announced</a>; a
month later, semantic versioning was adopted with the <a href="https://github.com/typst/typst/releases/tag/v0.1.0">release of
v0.1.0.</a> Typst is now at <a href="https://typst.app/blog/2025/typst-0.13">v.0.13.1</a> and shows
365&nbsp;contributors on its <a href="https://github.com/typst/typst?tab=readme-ov-file#--">GitHub repository</a>.
</p>

<p>
I had been aware of this project for over a year but had not paid much
attention, assuming it to be yet another attempt to supplant LaTeX that was
doomed to fail. A rising chorus of enthusiasm among early
adopters, and the beginnings of <a href="https://juti.if.its.ac.id/index.php/juti/index">acceptance</a> of
Typst manuscripts by scholarly journals, made me curious enough
to take the young project for a spin.
</p>

<p>
Typst is available as Rust source and as a compiled binary. To install,
visit the <a href="https://github.com/typst/typst/releases">releases
page</a> and download the appropriate archive. There are options for Linux,
macOS and Windows; I used the
precompiled Linux version for my testing.
</p>

<p>
The "<tt>typst</tt>" command accepts several subcommands. Entering
"<tt>typst fonts</tt>" lists all of the usable fonts to be found in
standard locations on the machine; nonstandard font directories can be
added manually. In my case, Typst found all of my 476&nbsp;fonts instantly;
the only ones omitted were some ancient PostScript Type&nbsp;1 fonts used
by LaTeX. Users who have LaTeX installed will have a large collection of
OpenType and TrueType math and text fonts on their machines; Typst can
use all of these. But Typst will work fine without them, as the program has
a small collection of fonts built in (try "<tt>typst fonts
--ignore-system-fonts</tt>" to see them).
</p>

<p>
Two other subcommands to explore are
"<tt>compile</tt>", which generates the output (PDF by default, with PDF/A,
SVG, and PNG available, along with HTML under development) from a source file, and
"<tt>watch</tt>" for interactive editing. The <tt>watch</tt> subcommand keeps a Typst process running that
incrementally and automatically compiles the document to PDF in response to
changes in the source. To use "<tt>typst watch</tt>" effectively, the
screen should be divided into three windows: a small terminal window to monitor
the <tt>typst</tt> output for error (or success) messages, the editing
window, and an area for any PDF reader that automatically reloads the
displayed document when it changes (many, such as my favorite, <a href="https://github.com/ahrm/sioyek?tab=readme-ov-file#sioyek">Sioyek</a>, do this). The result is a
responsive live preview, even of large documents, due to Typst's speed and
incremental compilation. For example, Frans Skarman <a href="https://fransskarman.com/phd_thesis_in_typst.html">described</a>
his experience writing his doctoral thesis in Typst, and noted
that he was able to enjoy nearly instant previews of content changes to
the book-length document. 
</p>

<h4 id="how-typst-improves-on-latex">How Typst improves on LaTeX</h4>

<p>
Typst output is quite close to that of LaTeX. It uses the same <a href="https://gwern.net/doc/design/typography/tex/1981-knuth.pdf">line-breaking
algorithm</a> developed by Donald Knuth and Michael Plass for TeX, so it
creates nicely balanced paragraphs of regular text. Its mathematical
typesetting algorithms are <a href="https://typst.app/blog/2023/january-update">based closely</a> on the
TeX algorithms, and indeed mathematical rendering is nearly
indistinguishable between the two systems.
</p>

<p>
Getting started with LaTeX can be confusing for newcomers, because it comes
with several alternative "engines" reflecting the long and complex history
of the project. These are the various binaries such as "<tt>pdflatex</tt>",
"<tt>tex</tt>", "<tt>xelatex</tt>", "<tt>luatex</tt>", "<tt>lualatex</tt>",
and more, each with somewhat different capabilities. For Typst there is
only "<tt>typst</tt>".
</p>

<p>
Markup in Typst is less verbose and easier to read than in LaTeX. It
dispenses with the plethora of curly brackets and backslashes littering
LaTeX documents by adopting, for prose, syntax in the style of <a href="https://www.markdownguide.org/basic-syntax">Markdown</a>, and, for
equations, a set of conventions designed for easy input. The
fact that curly brackets and backslashes are awkward to type on German
keyboards may have provided a little extra impetus for the developers to
create an alternative markup system that doesn't require a forest of these
symbols.
</p>

<p>
When users make syntax errors in markup or programming, inevitable
even in Typst, the system presents them with another dramatic improvement
over LaTeX (and TeX): error messages using colored glyphs that
clearly point out exactly where the problem is. I've even discovered that
Typst will save me from trying to run a syntactically correct infinite
loop.
</p>

<p>
Here is a bit of Typst markup for a shopping list, with
the resulting rendering to the right:
</p>

<p><img src="https://static.lwn.net/images/2025/typst-list.png" alt="[Rendered list]" title="Rendered list" width="235" height="291"></p><pre>   = Shopping List
   
   == Vegetables
   
   + Broccoli
   + Asparagus (*fresh only*)
   + Plantains (_ripe and green_)
   
   == Booze
   
   + Rum
     - White
     - Dark
   + #underline[Good] gin  
</pre>

<p>
The example gives a flavor of Typst's terse markup syntax. Headings are
indicated with leading <tt>=</tt> signs. Automatically numbered lists are
created by prepending <tt>+</tt> signs to items, and bulleted lists with
<tt>-</tt> signs; lists can be nested. Delimiters are shown for bold text
and italics. These are shortcuts, or markup syntax sugar, for Typst
functions for transforming text. Not every function has a corresponding
shortcut; in those cases one needs to call the function explicitly, as in
the final item.
</p>

<p>
Typst input is always within one of three modes. Markup (text) mode is the
default. The <tt>#</tt> sign preceding the function call in the last line
of the example
places Typst in "code mode". The "<a href="https://typst.app/docs/reference/text/underline/"><tt>underline()</tt></a>" function accepts a number of keyword arguments that affect its behavior, and one trailing argument, in square brackets, containing the text that it modifies. In the example, we've stuck with the default behavior, but if we wanted, for example, a red underline, we could use "<tt>#underline(stroke: red)[Good] gin</tt>". Following the square-bracketed text argument, Typst returns to interpreting input in text mode.
</p>

<p>Other functions produce output directly, rather than modifying a text argument. This bit of Typst input:</p>

<pre>    #let word = "Manhattan"
    There are #str.len(word) letters in #word.
</pre>

<p>produces the output (in typeset form) "There are 9 letters in Manhattan.". The "<a href="https://typst.app/docs/reference/foundations/str/#definitions-len"><tt>len()</tt></a>" function is part of the
"<a href="https://typst.app/docs/reference/foundations/str/"><tt>str</tt></a>" module, so it needs the namespace.</p>
 
<p>
Let's take a look at the LaTeX equivalent for the first half of the
shopping list for comparison:
</p>

<pre>   \documentclass[12pt]{article}
   \begin{document}
   \section*{Shopping List}
   
   \subsection*{Vegetables}
   
   \begin{enumerate}
   \item Broccoli
   \item Asparagus ({\bfseries fresh only})
   \item Plantains (\emph{ripe and green})
   \end{enumerate}
   \end{document}   
</pre>

<p>
The first two and the last line are boilerplate that is not required in
Typst. The difference in verbosity level and ease of reading the source is
clear.
</p>

<p>
The third Typst mode, in addition to markup and code, is math mode,
delimited by dollar signs. This is also best illustrated by an example:
</p>

<pre>   $ integral_0^1 (arcsin x)^2 (dif x)/(x^2 sqrt(1-x^2)) = π ln 2 $  
</pre>

<p>
When this is compiled by Typst, it produces the result shown below:
</p>

<blockquote>
<img src="https://static.lwn.net/images/2025/typst-series.png" width="800" alt="[Rendered integral]" title="Rendered integral">
</blockquote>

<p>
Those who've used LaTeX will begin to see from this example how math in
Typst source is less verbose and easier to read than in LaTeX. Greek
letters and other Unicode symbols can be used directly, as in modern LaTeX
engines such as <tt>lualatex</tt>, which we <a href="https://lwn.net/Articles/731581/">looked at back in 2017</a>, but with no
imports required.
</p>

<p>
The advent of the LuaTeX and LuaLaTeX projects provided users who wanted to
incorporate programming into their documents a more pleasant alternative to
the TeX macro language. As powerful as the embedded Lua system is, however,
it betrays its bolted-on status, requiring users to negotiate the interface
between Lua data structures and LaTeX or TeX internals. In Typst,
programming is thoroughly integrated into the system, with no seams between
the language used for calculation and the constructs that place characters
in the final PDF. Typst programs are invariably <a href="https://lee-phillips.org/TLexamples/">simpler</a> than their LuaLaTeX
equivalents. All authors using Typst will make at least some simple use of
its programming language, as such basic necessities as changing fonts, or
customizations such as changing the style of section headings, are
accomplished by calling Typst functions.
</p>

<p>
The Typst language is somewhat similar to Rust, perhaps
unsurprisingly. Most Typst functions are <em>pure</em>: they have no side effects and
always produce the same result given the same arguments (aside from certain
functions that mutate their arguments, such as <a href="https://typst.app/docs/reference/foundations/array/#definitions-push"><tt>array.push()</tt></a>). This
aspect reduces the probability of difficult-to-debug conflicts among
packages that plague LaTeX, and makes it easier to debug Typst documents.
</p>

<p>
Although Typst uses the same line-breaking algorithm as LaTeX, its internal
approach to overall page layout is <a href="https://laurmaedje.github.io/posts/layout-models">distinct</a>. Some
consequences are that Typst does a better job at handling movable elements
such as floating figures, and can, for example, easily split large tables
across page breaks, something that LaTeX struggles with even with
specialized packages.
</p>

<h4 id="typst-drawbacks">Typst drawbacks</h4>

<p>
Typst's page layout algorithm doesn't always permit the refinements that 
LaTeX is capable of.
For example, Typst is not as good
as LaTeX at avoiding <a href="https://practicaltypography.com/widow-and-orphan-control.html">widows
and orphans</a>.
Another salient deficiency is Typst's relative lack of specialized packages,
compared with the vast ecosystem produced by LaTeX's decades of community
involvement. However, the relative ease of programming in Typst (and the
well-organized and extensively commented underlying Rust code) suggests
that this drawback may be remedied before a comparable number of decades
have elapsed. Indeed, there are already <a href="https://typst.app/universe">over 800&nbsp;packages available</a>. Typst still cannot do
everything that LaTeX can, but the breadth of its package collection is
encouraging.
</p>

<p>
Almost no journals that provide LaTeX templates for submissions offer a
Typst option, so physicists and mathematicians adopting Typst will need to
find a way to convert their manuscripts. This is made easier for those who
use <a href="https://pandoc.org/">Pandoc</a>, as that conversion program
handles Typst.
</p>

<p>
Another drawback is the difficulty of learning Typst. The <a href="https://typst.app/docs">official documentation</a> is confusingly
organized, with information scattered unpredictably among "Tutorial",
"Reference", and "Guides" sections. Concepts are not always clearly
explained, and sometimes not presented in a logical order. The manual is
not keeping up with the rapid development of the program, and contains some
out-of-date information and errors. None of this is surprising considering
how quickly the project is moving, its early stage, and its
small core team. A work-in-progress called the <a href="https://sitandr.github.io/typst-examples-book/book"><em>Typst
Examples Book</em></a> has appeared, which may be a better starting point
than the official documentation.
</p>

<p>
There are other minor deficiencies compared with LaTeX, such as the
inability to include PDF documents. Typst provides no analogue to LaTeX's
<tt>parshape</tt> command, which lets authors mold paragraphs to, for
example, wrap around complex illustrations. The situation is likely to
change, however, as something like <tt>parshape</tt> is <a href="https://forum.typst.app/t/is-there-an-equivalent-to-latex-s-parshape/1006">being
considered</a> for the future.
</p>

<p>
More serious is the possibility of breaking changes as the system evolves,
always a risk of early adoption. I suspect, however, that these will require
only minor edits to documents in most cases. Progress seems to be steady,
rational, and <a href="https://typst.app/docs/roadmap">responsive to
user requests</a>.
</p>

<h4 id="conclusion">Conclusion</h4>

<p>
I'm using Typst in real work right now to write a physics paper. I will
need to submit my manuscript using the journal's LaTeX template, but I'm
taking advantage of Typst to make the entry of the paper's many equations
simpler, and I'll <a href="https://lee-phillips.org/typstfilters/">transform</a> the result to
LaTeX with Pandoc without needing any manual adjustment. The tooling is excellent, as my preferred editor, <a href="http://neovim.io/">Neovim</a>, has <a href="https://github.com/nvim-treesitter/nvim-treesitter?tab=readme-ov-file#nvim-treesitter">support</a>
for the <a href="https://tree-sitter.github.io/tree-sitter/">Tree-sitter
incremental parser</a>
for Markdown and Typst, which provides syntax-aware highlighting
and navigation of the source files. I use Typst's fast incremental
compilation to get live feedback as I fiddle with my math markup.
</p>

<p>
I was skeptical when I downloaded Typst to try it out, but became
enthusiastic within minutes, as I saw the first (of many) of its lovely
error messages, and remained sanguine as I saw the quality of its output. I
predict that Typst will eventually take the place of LaTeX. But even if
that never comes to pass, it is a useful tool right now.
</p><br clear="all"><table>
           <tbody><tr><th colspan="2">Index entries for this article</th></tr>
           <tr><td><a href="https://lwn.net/Archives/GuestIndex/">GuestArticles</a></td><td><a href="https://lwn.net/Archives/GuestIndex/#Phillips_Lee">Phillips, Lee</a></td></tr>
            </tbody></table><br clear="all">
<hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lifetime of social ties adds up to healthy aging at molecular level (169 pts)]]></title>
            <link>https://news.cornell.edu/stories/2025/09/lifetime-social-ties-adds-healthy-aging</link>
            <guid>45393501</guid>
            <pubDate>Sat, 27 Sep 2025 06:00:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.cornell.edu/stories/2025/09/lifetime-social-ties-adds-healthy-aging">https://news.cornell.edu/stories/2025/09/lifetime-social-ties-adds-healthy-aging</a>, See on <a href="https://news.ycombinator.com/item?id=45393501">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>The cumulative effect of social advantages across a lifetime&nbsp;– from parental warmth in childhood to friendship, community engagement and religious support in adulthood&nbsp;– may slow the biological processes of aging itself. These social advantages appear to set back “epigenetic clocks” such that a person’s biological age, as measured by analyzing DNA methylation patterns, is younger than their chronological age.&nbsp;</span></p><p><span>The&nbsp;research, which appeared&nbsp;</span><a href="https://www.sciencedirect.com/science/article/pii/S2666354625001541"><span>in the October issue</span></a><span> of the journal Brain, Behavior and Immunity - Health, drew on data from more than 2,100 adults in the long-running Midlife in the United States (MIDUS) study.</span></p><p><span>First author&nbsp;</span><a href="https://psychology.cornell.edu/anthony-ong"><span>Anthony Ong</span></a><span>, psychology professor and director of the Human Health Labs in the College of Human Ecology,&nbsp;and fellow researchers found that people with higher levels of what they called “cumulative social advantage” showed slower epigenetic aging and lower levels of chronic inflammation.</span></p><p><span>"This paper builds on a foundational study we published last year showing how cumulative social advantage relates to positive health outcomes," Ong said. "This new study digs deeper into the same data to understand the biological mechanisms - essentially, how social connections get under our skin to affect aging at the molecular level."</span></p><p><span>The study focused on so-called epigenetic clocks, molecular signatures that estimate the pace of biological aging. Two in particular&nbsp;– GrimAge and DunedinPACE&nbsp;– are considered especially predictive of morbidity and mortality. Adults with stronger, more sustained social networks showed significantly younger profiles on both clocks.</span></p><p><span>"Cumulative social advantage is really about the depth and breadth of your social connections over a lifetime,” Ong said. “We looked at four key areas: the warmth and support you received from your parents growing up, how connected you feel to your community and neighborhood, your involvement in religious or faith-based communities, and the ongoing emotional support from friends and family."</span></p><p><span>The researchers hypothesized that sustained social advantage becomes reflected in core regulatory systems linked to aging, including epigenetic, inflammatory and neuroendocrine pathways. And indeed, they&nbsp;found that higher social advantage was linked to lower levels of interleukin-6, a pro-inflammatory molecule implicated in heart disease, diabetes and neurodegeneration. But interestingly, there were no significant associations with short-term stress markers like cortisol or catecholamines.</span></p><p><span>Unlike many earlier studies that looked at social factors in isolation&nbsp;– whether a person is married, for example, or how many friends they have&nbsp;– this work conceptualized “cumulative social advantage” as a multidimensional construct. And by combining both early and later-life relational resources, the measure reflects the ways advantage clusters and compounds.&nbsp;</span></p><p><span>"What's striking is the cumulative effect - these social resources build on each other over time,” Ong said. “It's not just about having friends today; it's about how your social connections have grown and deepened throughout your life. That accumulation shapes your health trajectory in measurable ways."</span></p><p><span>This perspective draws on cumulative advantage theory, which holds that resources, whether economic or social, tend to accrue, widening disparities across the life course. This underscores a sobering reality: Access to these social resources is not evenly distributed. Race, class and educational attainment shape the likelihood of growing up with supportive parents, finding belonging in community institutions or having friends and partners who provide steady support.</span></p><p><span>That means those already disadvantaged in material ways may also be biologically disadvantaged by a relative lack of sustained social support, potentially accelerating the processes of aging and illness.</span></p><p><span>The findings dovetail with the “weathering hypothesis,” a framework developed by public health scholar Arline Geronimus, which suggests that chronic exposure to adversity and structural inequality leads to earlier health deterioration in marginalized groups. Here, researchers extend that framework to show how accumulated relational advantage, the other side of the coin, may confer resilience at the molecular level.</span></p><p><span>This doesn’t mean a single friendship or volunteer stint can turn back the biological clock. But the authors, including Frank Mann at Stony Brook University and Laura Kubzansky at Harvard University, suggest that the depth and consistency of social connection, built across decades and different spheres of life, matters profoundly. The study adds weight to the growing view that social life is not just a matter of happiness or stress relief but a core determinant of physiological health.</span></p><p><span>"Think of social connections like a retirement account," Ong said. “The earlier you start investing and the more consistently you contribute, the greater your returns. Our study shows those returns aren't just emotional; they're biological. People with richer, more sustained social connections literally age more slowly at the cellular level. Aging well means both staying healthy and staying connected - they're inseparable."</span></p></div></div>]]></description>
        </item>
    </channel>
</rss>