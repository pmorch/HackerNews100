<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 03 Jul 2023 16:00:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Self-Driving Cars Are Surveillance Cameras on Wheels (183 pts)]]></title>
            <link>https://www.schneier.com/blog/archives/2023/07/self-driving-cars-are-surveillance-cameras-on-wheels.html</link>
            <guid>36572401</guid>
            <pubDate>Mon, 03 Jul 2023 13:11:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.schneier.com/blog/archives/2023/07/self-driving-cars-are-surveillance-cameras-on-wheels.html">https://www.schneier.com/blog/archives/2023/07/self-driving-cars-are-surveillance-cameras-on-wheels.html</a>, See on <a href="https://news.ycombinator.com/item?id=36572401">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-67500">

	<div>

		
		<p>Police are <a href="https://www.bloomberg.com/news/articles/2023-06-29/self-driving-car-video-from-waymo-cruise-give-police-crime-evidence?sref=P6Q0mxvj">already using</a> self-driving car footage as video evidence:</p>
<blockquote><p>While security cameras are commonplace in American cities, self-driving cars represent a new level of access for law enforcement ­ and a new method for encroachment on privacy, advocates say. Crisscrossing the city on their routes, self-driving cars capture a wider swath of footage. And it’s easier for law enforcement to turn to one company with a large repository of videos and a dedicated response team than to reach out to all the businesses in a neighborhood with security systems.</p>
<p>“We’ve known for a long time that they are essentially surveillance cameras on wheels,” said Chris Gilliard, a fellow at the Social Science Research Council. “We’re supposed to be able to go about our business in our day-to-day lives without being surveilled unless we are suspected of a crime, and each little bit of this technology strips away that ability.”</p>
<p>[…]</p>
<p>While self-driving services like Waymo and Cruise have yet to achieve the same level of market penetration as Ring, the wide range of video they capture while completing their routes presents other opportunities. In addition to the San Francisco homicide, Bloomberg’s review of court documents shows police have sought footage from Waymo and Cruise to help solve hit-and-runs, burglaries, aggravated assaults, a fatal collision and an attempted kidnapping.</p>
<p>In all cases reviewed by Bloomberg, court records show that police collected footage from Cruise and Waymo shortly after obtaining a warrant. In several cases, Bloomberg could not determine whether the recordings had been used in the resulting prosecutions; in a few of the cases, law enforcement and attorneys said the footage had not played a part, or was only a formality. However, video evidence has become a lynchpin of criminal cases, meaning it’s likely only a matter of time.</p></blockquote>

		
			<p>
				<span>Tags: <a href="https://www.schneier.com/tag/cars/" rel="tag">cars</a>, <a href="https://www.schneier.com/tag/crime/" rel="tag">crime</a>, <a href="https://www.schneier.com/tag/law-enforcement/" rel="tag">law enforcement</a>, <a href="https://www.schneier.com/tag/privacy/" rel="tag">privacy</a>, <a href="https://www.schneier.com/tag/surveillance/" rel="tag">surveillance</a></span>			</p>

		
		
		<p>
			<a href="https://www.schneier.com/blog/archives/2023/07/self-driving-cars-are-surveillance-cameras-on-wheels.html" rel="bookmark">Posted on July 3, 2023 at 7:04 AM</a>			•
			<a href="https://www.schneier.com/blog/archives/2023/07/self-driving-cars-are-surveillance-cameras-on-wheels.html#comments">4 Comments</a>		</p>

		
	</div>

</article><p id="powered">Sidebar photo of Bruce Schneier by Joe MacInnis.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Data-Oriented Design (2018) (182 pts)]]></title>
            <link>https://www.dataorienteddesign.com/dodbook/dodmain.html</link>
            <guid>36571110</guid>
            <pubDate>Mon, 03 Jul 2023 10:41:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dataorienteddesign.com/dodbook/dodmain.html">https://www.dataorienteddesign.com/dodbook/dodmain.html</a>, See on <a href="https://news.ycombinator.com/item?id=36571110">Hacker News</a></p>
<div id="readability-page-1" class="page">


<!--End of Navigation Panel-->
<tt>Online release of Data-Oriented Design : <br>This is the free, online, reduced version. Some inessential chapters are excluded from this version, but in the spirit of this being an education resource, the essentials are present for anyone wanting to learn about data-oriented design.<br>Expect some odd formatting and some broken images and listings as this is auto generated and the Latex to html converters available are not perfect. If the source code listing is broken, you should be able to find the referenced source on <a href="https://github.com/raspofabs/dodbooksourcecode/">github</a>. If you like what you read here, consider purchasing the real paper book from <a href="https://www.amazon.com/dp/1916478700">here</a>, as not only will it look a lot better, but it will help keep this version online for those who cannot afford to buy it. Please send any feedback to <a href="mailto:support@dataorienteddesign.com">support@dataorienteddesign.com</a></tt>


<p><strong>Richard Fabian</strong>
</p>
<hr>

<br><hr>
<!--Table of Child-Links-->
<a name="CHILD_LINKS"></a>

<ul>
<li><a name="tex2html46" href="https://www.dataorienteddesign.com/dodbook/node1.html">Contents</a>
</li><li><a name="tex2html47" href="https://www.dataorienteddesign.com/dodbook/node2.html">Data-Oriented Design</a>
<ul>
<li><a name="tex2html48" href="https://www.dataorienteddesign.com/dodbook/node2.html#SECTION00210000000000000000">It's all about the data</a>
</li><li><a name="tex2html49" href="https://www.dataorienteddesign.com/dodbook/node2.html#SECTION00220000000000000000">Data is not the problem domain</a>
</li><li><a name="tex2html50" href="https://www.dataorienteddesign.com/dodbook/node2.html#SECTION00230000000000000000">Data and statistics</a>
</li><li><a name="tex2html51" href="https://www.dataorienteddesign.com/dodbook/node2.html#SECTION00240000000000000000">Data can change</a>
</li><li><a name="tex2html52" href="https://www.dataorienteddesign.com/dodbook/node2.html#SECTION00250000000000000000">How is data formed?</a>
</li><li><a name="tex2html53" href="https://www.dataorienteddesign.com/dodbook/node2.html#SECTION00260000000000000000">The framework</a>
</li><li><a name="tex2html54" href="https://www.dataorienteddesign.com/dodbook/node2.html#SECTION00270000000000000000">Conclusions and takeaways</a>
</li></ul>
<br>
</li><li><a name="tex2html55" href="https://www.dataorienteddesign.com/dodbook/node3.html">Relational Databases</a>
<ul>
<li><a name="tex2html56" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00310000000000000000">Complex state</a>
</li><li><a name="tex2html57" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00320000000000000000">The framework</a>
</li><li><a name="tex2html58" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00330000000000000000">Normalising your data</a>
</li><li><a name="tex2html59" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00340000000000000000">Normalisation</a>
<ul>
<li><a name="tex2html60" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00341000000000000000">Primary keys</a>
</li><li><a name="tex2html61" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00342000000000000000">1<sup>st</sup> Normal Form</a>
</li><li><a name="tex2html62" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00343000000000000000">2<sup>nd</sup> Normal Form</a>
</li><li><a name="tex2html63" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00344000000000000000">3<sup>rd</sup> Normal Form</a>
</li><li><a name="tex2html64" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00345000000000000000">Boyce-Codd Normal Form</a>
</li><li><a name="tex2html65" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00346000000000000000">Domain Key / Knowledge</a>
</li><li><a name="tex2html66" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00347000000000000000">Reflections</a>
</li></ul>
</li><li><a name="tex2html67" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00350000000000000000">Operations</a>
</li><li><a name="tex2html68" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00360000000000000000">Summing up</a>
</li><li><a name="tex2html69" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00370000000000000000">Stream Processing</a>
</li><li><a name="tex2html70" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00380000000000000000">Why does database technology matter?</a>
</li></ul>
<br>
</li><li><a name="tex2html71" href="https://www.dataorienteddesign.com/dodbook/node4.html">Existential Processing</a>
<ul>
<li><a name="tex2html72" href="https://www.dataorienteddesign.com/dodbook/node4.html#SECTION00410000000000000000">Complexity</a>
</li><li><a name="tex2html73" href="https://www.dataorienteddesign.com/dodbook/node4.html#SECTION00420000000000000000">Debugging</a>
</li><li><a name="tex2html74" href="https://www.dataorienteddesign.com/dodbook/node4.html#SECTION00430000000000000000">Why use an if</a>
</li><li><a name="tex2html75" href="https://www.dataorienteddesign.com/dodbook/node4.html#SECTION00440000000000000000">Types of processing</a>
</li><li><a name="tex2html76" href="https://www.dataorienteddesign.com/dodbook/node4.html#SECTION00450000000000000000">Don't use booleans</a>
</li><li><a name="tex2html77" href="https://www.dataorienteddesign.com/dodbook/node4.html#SECTION00460000000000000000">Don't use enums quite as much</a>
</li><li><a name="tex2html78" href="https://www.dataorienteddesign.com/dodbook/node4.html#SECTION00470000000000000000">Prelude to polymorphism</a>
</li><li><a name="tex2html79" href="https://www.dataorienteddesign.com/dodbook/node4.html#SECTION00480000000000000000">Dynamic runtime polymorphism</a>
</li><li><a name="tex2html80" href="https://www.dataorienteddesign.com/dodbook/node4.html#SECTION00490000000000000000">Event handling</a>
</li></ul>
<br>
</li><li><a name="tex2html81" href="https://www.dataorienteddesign.com/dodbook/node5.html">Component Based Objects</a>
<ul>
<li><a name="tex2html82" href="https://www.dataorienteddesign.com/dodbook/node5.html#SECTION00510000000000000000">Components in the wild</a>
</li><li><a name="tex2html83" href="https://www.dataorienteddesign.com/dodbook/node5.html#SECTION00520000000000000000">Away from the hierarchy</a>
</li><li><a name="tex2html84" href="https://www.dataorienteddesign.com/dodbook/node5.html#SECTION00530000000000000000">Towards managers</a>
</li><li><a name="tex2html85" href="https://www.dataorienteddesign.com/dodbook/node5.html#SECTION00540000000000000000">There is no entity</a>
</li></ul>
<br>
</li><li><a name="tex2html86" href="https://www.dataorienteddesign.com/dodbook/node6.html">Hierarchical Level of Detail</a>
<ul>
<li><a name="tex2html87" href="https://www.dataorienteddesign.com/dodbook/node6.html#SECTION00610000000000000000">Existence</a>
</li><li><a name="tex2html88" href="https://www.dataorienteddesign.com/dodbook/node6.html#SECTION00620000000000000000">Mementos</a>
</li><li><a name="tex2html89" href="https://www.dataorienteddesign.com/dodbook/node6.html#SECTION00630000000000000000">JIT mementos</a>
</li><li><a name="tex2html90" href="https://www.dataorienteddesign.com/dodbook/node6.html#SECTION00640000000000000000">Alternative axes</a>
<ul>
<li><a name="tex2html91" href="https://www.dataorienteddesign.com/dodbook/node6.html#SECTION00641000000000000000">The true measure</a>
</li><li><a name="tex2html92" href="https://www.dataorienteddesign.com/dodbook/node6.html#SECTION00642000000000000000">Beyond space</a>
</li></ul>
</li><li><a name="tex2html93" href="https://www.dataorienteddesign.com/dodbook/node6.html#SECTION00650000000000000000">Collective LOD</a>
</li></ul>
<br>
</li><li><a name="tex2html94" href="https://www.dataorienteddesign.com/dodbook/node7.html">Searching</a>
<ul>
<li><a name="tex2html95" href="https://www.dataorienteddesign.com/dodbook/node7.html#SECTION00710000000000000000">Indexes</a>
</li><li><a name="tex2html96" href="https://www.dataorienteddesign.com/dodbook/node7.html#SECTION00720000000000000000">Data-oriented Lookup</a>
</li><li><a name="tex2html97" href="https://www.dataorienteddesign.com/dodbook/node7.html#SECTION00730000000000000000">Finding low and high</a>
</li><li><a name="tex2html98" href="https://www.dataorienteddesign.com/dodbook/node7.html#SECTION00740000000000000000">Finding random</a>
</li></ul>
<br>
</li><li><a name="tex2html99" href="https://www.dataorienteddesign.com/dodbook/node8.html">Sorting</a>
<ul>
<li><a name="tex2html100" href="https://www.dataorienteddesign.com/dodbook/node8.html#SECTION00810000000000000000">Do you need to?</a>
</li><li><a name="tex2html101" href="https://www.dataorienteddesign.com/dodbook/node8.html#SECTION00820000000000000000">Maintaining</a>
</li><li><a name="tex2html102" href="https://www.dataorienteddesign.com/dodbook/node8.html#SECTION00830000000000000000">Sorting for your platform</a>
</li></ul>
<br>
</li><li><a name="tex2html103" href="https://www.dataorienteddesign.com/dodbook/node9.html">Optimisations</a>
<ul>
<li><a name="tex2html104" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00910000000000000000">When should we optimise?</a>
</li><li><a name="tex2html105" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00920000000000000000">Feedback</a>
<ul>
<li><a name="tex2html106" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00921000000000000000">Know your limits</a>
</li></ul>
</li><li><a name="tex2html107" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00930000000000000000">A strategy</a>
<ul>
<li><a name="tex2html108" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00931000000000000000">Define the problem</a>
</li><li><a name="tex2html109" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00932000000000000000">Measure</a>
</li><li><a name="tex2html110" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00933000000000000000">Analyse</a>
</li><li><a name="tex2html111" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00934000000000000000">Implement</a>
</li><li><a name="tex2html112" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00935000000000000000">Confirm</a>
</li><li><a name="tex2html113" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00936000000000000000">Summary</a>
</li></ul>
</li><li><a name="tex2html114" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00940000000000000000">Tables</a>
</li><li><a name="tex2html115" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00950000000000000000">Transforms</a>
</li><li><a name="tex2html116" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00960000000000000000">Spatial sets</a>
</li><li><a name="tex2html117" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00970000000000000000">Lazy evaluation</a>
</li><li><a name="tex2html118" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00980000000000000000">Necessity</a>
</li><li><a name="tex2html119" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00990000000000000000">Varying length sets</a>
</li><li><a name="tex2html120" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION009100000000000000000">Joins as intersections</a>
</li><li><a name="tex2html121" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION009110000000000000000">Data-driven techniques</a>
<ul>
<li><a name="tex2html122" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION009111000000000000000">SIMD</a>
</li></ul>
</li><li><a name="tex2html123" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION009120000000000000000">Structs of arrays</a>
</li></ul>
<br>
</li><li><a name="tex2html124" href="https://www.dataorienteddesign.com/dodbook/node10.html">Helping the compiler</a>
<ul>
<li><a name="tex2html125" href="https://www.dataorienteddesign.com/dodbook/node10.html#SECTION001010000000000000000">Reducing order dependence</a>
</li><li><a name="tex2html126" href="https://www.dataorienteddesign.com/dodbook/node10.html#SECTION001020000000000000000">Reducing memory dependency</a>
</li><li><a name="tex2html127" href="https://www.dataorienteddesign.com/dodbook/node10.html#SECTION001030000000000000000">Write buffer awareness</a>
</li><li><a name="tex2html128" href="https://www.dataorienteddesign.com/dodbook/node10.html#SECTION001040000000000000000">Aliasing</a>
</li><li><a name="tex2html129" href="https://www.dataorienteddesign.com/dodbook/node10.html#SECTION001050000000000000000">Return value optimisation</a>
</li><li><a name="tex2html130" href="https://www.dataorienteddesign.com/dodbook/node10.html#SECTION001060000000000000000">Cache line utilisation</a>
</li><li><a name="tex2html131" href="https://www.dataorienteddesign.com/dodbook/node10.html#SECTION001070000000000000000">False sharing</a>
</li><li><a name="tex2html132" href="https://www.dataorienteddesign.com/dodbook/node10.html#SECTION001080000000000000000">Speculative execution awareness</a>
</li><li><a name="tex2html133" href="https://www.dataorienteddesign.com/dodbook/node10.html#SECTION001090000000000000000">Branch prediction</a>
</li><li><a name="tex2html134" href="https://www.dataorienteddesign.com/dodbook/node10.html#SECTION0010100000000000000000">Don't get evicted</a>
</li><li><a name="tex2html135" href="https://www.dataorienteddesign.com/dodbook/node10.html#SECTION0010110000000000000000">Auto vectorisation</a>
</li></ul>
<br>
</li><li><a name="tex2html136" href="https://www.dataorienteddesign.com/dodbook/node11.html">Maintenance and reuse</a>
<ul>
<li><a name="tex2html137" href="https://www.dataorienteddesign.com/dodbook/node11.html#SECTION001110000000000000000">Cosmic hierarchies</a>
</li><li><a name="tex2html138" href="https://www.dataorienteddesign.com/dodbook/node11.html#SECTION001120000000000000000">Debugging</a>
<ul>
<li><a name="tex2html139" href="https://www.dataorienteddesign.com/dodbook/node11.html#SECTION001121000000000000000">Lifetimes</a>
</li><li><a name="tex2html140" href="https://www.dataorienteddesign.com/dodbook/node11.html#SECTION001122000000000000000">Avoiding pointers</a>
</li><li><a name="tex2html141" href="https://www.dataorienteddesign.com/dodbook/node11.html#SECTION001123000000000000000">Bad State</a>
</li></ul>
</li><li><a name="tex2html142" href="https://www.dataorienteddesign.com/dodbook/node11.html#SECTION001130000000000000000">Reusability</a>
</li><li><a name="tex2html143" href="https://www.dataorienteddesign.com/dodbook/node11.html#SECTION001140000000000000000">Reusable functions</a>
</li><li><a name="tex2html144" href="https://www.dataorienteddesign.com/dodbook/node11.html#SECTION001150000000000000000">Unit testing</a>
</li><li><a name="tex2html145" href="https://www.dataorienteddesign.com/dodbook/node11.html#SECTION001160000000000000000">Refactoring</a>
</li></ul>
<br>
</li><li><a name="tex2html146" href="https://www.dataorienteddesign.com/dodbook/node12.html">What's wrong?</a>
<ul>
<li><a name="tex2html147" href="https://www.dataorienteddesign.com/dodbook/node12.html#SECTION001210000000000000000">The harm</a>
</li><li><a name="tex2html148" href="https://www.dataorienteddesign.com/dodbook/node12.html#SECTION001220000000000000000">Mapping the problem</a>
</li><li><a name="tex2html149" href="https://www.dataorienteddesign.com/dodbook/node12.html#SECTION001230000000000000000">Internalised state</a>
</li><li><a name="tex2html150" href="https://www.dataorienteddesign.com/dodbook/node12.html#SECTION001240000000000000000">Instance oriented development</a>
</li><li><a name="tex2html151" href="https://www.dataorienteddesign.com/dodbook/node12.html#SECTION001250000000000000000">Hierarchical design vs change</a>
</li><li><a name="tex2html152" href="https://www.dataorienteddesign.com/dodbook/node12.html#SECTION001260000000000000000">Divisions of labour</a>
</li><li><a name="tex2html153" href="https://www.dataorienteddesign.com/dodbook/node12.html#SECTION001270000000000000000">Reusable generic code</a>
</li></ul>
<br>
</li><li><a name="tex2html154" href="https://www.dataorienteddesign.com/dodbook/node13.html">About this document ...</a>
</li></ul>
<!--End of Table of Child-Links-->
<br><hr>
<address>
Richard Fabian
2018-10-08
</address>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wikipedia switches to CC BY-SA 4.0 license (110 pts)]]></title>
            <link>https://diff.wikimedia.org/2023/06/29/stepping-into-the-future-wikimedia-projects-transition-to-creative-commons-4-0-license/</link>
            <guid>36570714</guid>
            <pubDate>Mon, 03 Jul 2023 09:35:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://diff.wikimedia.org/2023/06/29/stepping-into-the-future-wikimedia-projects-transition-to-creative-commons-4-0-license/">https://diff.wikimedia.org/2023/06/29/stepping-into-the-future-wikimedia-projects-transition-to-creative-commons-4-0-license/</a>, See on <a href="https://news.ycombinator.com/item?id=36570714">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-96580">
	<div>
			
<p><em>The Wikimedia Foundation is excited to announce the update to our license to Creative Commons Attribution-ShareAlike 4.0, one of the main changes of our recent Terms of Use update, which brings the Wikimedia projects up to the latest version of the Creative Commons licenses.</em></p>



<figure><a href="https://diff.wikimedia.org/?attachment_id=96581"><img decoding="async" src="https://diff.wikimedia.org/wp-content/uploads/2023/06/Creative-Commons-Attribution-ShareAlike-badge.png?w=1024&amp;resize=823%2C288" alt="An image of the Creative Commons Attribution-ShareAlike badge" width="823" height="288" srcset="https://diff.wikimedia.org/wp-content/uploads/2023/06/Creative-Commons-Attribution-ShareAlike-badge.png?w=2560 2560w, https://diff.wikimedia.org/wp-content/uploads/2023/06/Creative-Commons-Attribution-ShareAlike-badge.png?w=300 300w, https://diff.wikimedia.org/wp-content/uploads/2023/06/Creative-Commons-Attribution-ShareAlike-badge.png?w=768 768w, https://diff.wikimedia.org/wp-content/uploads/2023/06/Creative-Commons-Attribution-ShareAlike-badge.png?w=1024&amp;resize=823%2C288 1024w, https://diff.wikimedia.org/wp-content/uploads/2023/06/Creative-Commons-Attribution-ShareAlike-badge.png?w=1536 1536w, https://diff.wikimedia.org/wp-content/uploads/2023/06/Creative-Commons-Attribution-ShareAlike-badge.png?w=2048 2048w" sizes="(max-width: 823px) 100vw, 823px" data-recalc-dims="1"></a><figcaption>The Creative Commons Attribution-ShareAlike badge. Image by <a href="https://creativecommons.org/">Creative Commons</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.en">CC BY-SA 4.0</a>, via <a href="https://creativecommons.org/about/downloads/">Creative Commons</a>.</figcaption></figure>



<div><p>We are glad to announce an exciting change that is now effective on many Wikimedia projects as part of our recent <a href="https://meta.wikimedia.org/wiki/Wikimedia_Foundation_Legal_department/2023_ToU_updates">Terms of Use (ToU) update</a>: Wikimedia project licensing has been updated from Creative Commons Attribution-ShareAlike 3.0 (CC BY-SA 3.0) to Creative Commons Attribution-ShareAlike 4.0 (CC BY-SA 4.0).</p><p>In this blog post, we’ll discuss what this update means to users and how it’ll positively impact the millions of volunteers who contribute to, reshare, and remix Wikipedia daily.</p></div>



<h2><strong>What are Creative Commons licenses, and why are they important to free knowledge and Wikimedia projects?</strong></h2>



<div><p>Creative Commons licenses are legal tools that enable everyone contributing to a massive, decentralized project like Wikipedia to create free culture content! Content that’s contributed to the Wikimedia projects under a Creative Commons license gives others permission to reuse and remix that content. That enables the projects to make knowledge available to people around the world, free of charge.&nbsp;</p><p>Creative Commons licensing allows creators—like Wikipedia editors—to retain copyright while permitting reusers—people who use Creative Commons content in their own works to help share knowledge—to copy, distribute, and republish the work in question as long as they follow two main conditions: The “Attribution” limitation means that a reuser must give credit to the original author; the “Share Alike” limitation means that reusers must share any adaptations they make under the same or an equivalent license. In this way, Creative Commons licenses enable knowledge to be widely adapted, help keep new knowledge freely available as people develop it, and contribute to making sure that the people who receive and use the knowledge in question are informed where that knowledge comes from so they can know its source and continue to contribute to it themselves.</p></div>



<h2><strong>Why did we update our license to CC BY-SA 4.0?</strong></h2>



<p>Wikipedia’s update to the <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> license, from the previous <a href="https://creativecommons.org/licenses/by-sa/3.0/">CC BY-SA 3.0</a> license, helps to make the project more accessible, globally compatible, simplified, and readable.</p>



<p><span>1. New sources for knowledge</span></p>



<p>For years, new knowledge created under the 4.0 license could not be <em>directly</em> added to Wikipedia. Now it can! For example, United Nations (UN) bodies and <a href="https://wiki.creativecommons.org/wiki/Government_use_of_Creative_Commons">national governments</a> have licensed their publications under 4.0, and these did not match the older versions of the license. Now, publications such as those from the UN can be uploaded directly to the Wikimedia projects to provide sources, quotations, or descriptions in articles. This opens a large area of new material for the projects to work with and build on, and helps everyone move closer to the vision of making the world’s knowledge freely available for anyone, anywhere.&nbsp;</p>



<p><span>2. Internationalization</span></p>



<p>The 4.0 license has been designed to be a <em>global </em>license. With 3.0, there were multiple versions of the license, which varied depending on the country in which they were used. Now, with 4.0, the single license is applicable worldwide, <a href="https://wiki.creativecommons.org/wiki/Legal_Tools_Translation">with official translations in over 30 languages—and with many more to come!</a> This eliminates the need for different ported versions of the license, that is to say, with changed language made for use in a specific jurisdiction. It also eliminates the need for users to make translations of those ports themselves. This universality fosters increased global collaboration across projects and national borders, hence making this change crucial to achieving the 2030 Wikimedia movement goals.</p>



<p><span>3. Attribution requirements are streamlined</span></p>



<p>With version 4.0, it is easier for reusers to understand how to credit the original author of the work. For example, it clarifies that linking to a webpage with attribution information is allowable, which is helpful since doing so has already become a common method of providing attribution. It also enables people to fix attribution mistakes within a reasonable time: this is important to help address simple mistakes without the need for overly aggressive copyright enforcement demands, and makes free knowledge content safer to use for people, anywhere in the world, who may be discovering and experimenting with contributing free content for the first time.&nbsp;</p>



<p>4<span>. The license text itself is more readable</span></p>



<p>The side-by-side comparison of the licenses that we have provided below shows how the license language is now clearer to read. This is important because it helps people who contribute to projects like Wikipedia, as well as people who want to use information on Wikipedia in their own work, understand the license. Clearer, better-organized texts mean fewer attribution errors and broader use of this material for the public good.</p>



<figure><a href="https://diff.wikimedia.org/?attachment_id=96587"><img decoding="async" loading="lazy" width="1024" height="543" src="https://diff.wikimedia.org/wp-content/uploads/2023/06/Screen-Shot-2023-06-28-at-1.06.51-PM.png?resize=1024%2C543" alt="An image featuring a side-by-side comparison of some of the text from the CC BY-SA 3.0 and CC BY-SA 4.0 licenses, which shows that the new version is more concise " srcset="https://diff.wikimedia.org/wp-content/uploads/2023/06/Screen-Shot-2023-06-28-at-1.06.51-PM.png?w=1206 1206w, https://diff.wikimedia.org/wp-content/uploads/2023/06/Screen-Shot-2023-06-28-at-1.06.51-PM.png?w=300 300w, https://diff.wikimedia.org/wp-content/uploads/2023/06/Screen-Shot-2023-06-28-at-1.06.51-PM.png?w=768 768w, https://diff.wikimedia.org/wp-content/uploads/2023/06/Screen-Shot-2023-06-28-at-1.06.51-PM.png?w=1024 1024w" sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"></a><figcaption>Side-by-side comparison of some of the text from the CC BY-SA 3.0 (left) and CC BY-SA 4.0 (right) licenses, illustrating how the new version is more concise.</figcaption></figure>



<p>You can find out more about what is new in 4.0 in <a href="https://creativecommons.org/version4/">this explainer from Creative Commons</a>.</p>



<h2><strong>Conclusion</strong></h2>



<p>The update to CC BY-SA 4.0 will help the Wikimedia projects continue to thrive as open, collaborative platforms for sharing free knowledge. The update makes the projects’ content more adaptable and usable for the global community, makes large amounts of new material compatible with the Wikimedia projects, and aligns our platform with the latest standards in open licensing.&nbsp;</p>



<p>We are excited about this new chapter of more modern, flexible, and easy-to-use licensing of free knowledge. Volunteer editors have already started the process of updating the relevant policy documents on-wiki so that we can operationalize this change and better communicate it to volunteers across the various Wikimedia projects and languages. If you see a webpage with an out-of-date license version, please feel free to update it!</p>
				<div id="translate-post">
					<p><img src="https://diff.wikimedia.org/wp-content/themes/interconnection/assets/images/translate-post.jpg" alt="">
					</p>

					<div>
						<h2>Can you help us translate this article?</h2>

						<p>In order for this article to reach as many people as possible we would like your help. Can you translate this article to get the message out?</p>

													<p><a href="https://diff.wikimedia.org/wp-login.php?redirect_to=%2F2023%2F06%2F29%2Fstepping-into-the-future-wikimedia-projects-transition-to-creative-commons-4-0-license%2F%23translate-post">Start translation</a>
												</p></div>
				</div>
				
		</div>

	<!-- .entry-footer -->

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Perl 5.38 (150 pts)]]></title>
            <link>https://perldoc.perl.org/perl5380delta</link>
            <guid>36569727</guid>
            <pubDate>Mon, 03 Jul 2023 07:04:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://perldoc.perl.org/perl5380delta">https://perldoc.perl.org/perl5380delta</a>, See on <a href="https://news.ycombinator.com/item?id=36569727">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrapperlicious">
        
        <h2><a id="toc">CONTENTS</a></h2>
                  <ul>
              <li>
                <a href="#NAME">NAME</a>
              </li>
              <li>
                <a href="#DESCRIPTION">DESCRIPTION</a>
              </li>
              <li>
                <a href="#Core-Enhancements">Core Enhancements</a>
                            <ul>
              <li>
                <a href="#New-class-Feature">New class Feature</a>
              </li>
              <li>
                <a href="#Unicode-15.0-is-supported">Unicode 15.0 is supported</a>
              </li>
              <li>
                <a href="#Deprecation-warnings-now-have-specific-subcategories">Deprecation warnings now have specific subcategories</a>
              </li>
              <li>
                <a href="#%25%7B%5EHOOK%7D-API-introduced">%{^HOOK} API introduced</a>
              </li>
              <li>
                <a href="#PERL_RAND_SEED">PERL_RAND_SEED</a>
              </li>
              <li>
                <a href="#Defined-or-and-logical-or-assignment-default-expressions-in-signatures">Defined-or and logical-or assignment default expressions in signatures</a>
              </li>
              <li>
                <a href="#@INC-Hook-Enhancements-and-$INC-and-INCDIR">@INC Hook Enhancements and $INC and INCDIR</a>
              </li>
              <li>
                <a href="#Forbidden-control-flow-out-of-defer-or-finally-now-detected-at-compile-time">Forbidden control flow out of defer or finally now detected at compile-time</a>
              </li>
              <li>
                <a href="#Optimistic-Eval-in-Patterns">Optimistic Eval in Patterns</a>
              </li>
              <li>
                <a href="#REG_INF-has-been-raised-from-65,536-to-2,147,483,647">REG_INF has been raised from 65,536 to 2,147,483,647</a>
              </li>
              <li>
                <a href="#New-API-functions-optimize_optree-and-finalize_optree">New API functions optimize_optree and finalize_optree</a>
              </li>
              <li>
                <a href="#Some-gotos-are-now-permitted-in-defer-and-finally-blocks">Some gotos are now permitted in defer and finally blocks</a>
              </li>
              <li>
                <a href="#New-regexp-variable-$%7B%5ELAST_SUCCESSFUL_PATTERN%7D">New regexp variable ${^LAST_SUCCESSFUL_PATTERN}</a>
              </li>
              <li>
                <a href="#Locale-category-LC_NAME-now-supported-on-participating-platforms">Locale category LC_NAME now supported on participating platforms</a>
              </li>
          </ul>

              </li>
              <li>
                <a href="#Incompatible-Changes">Incompatible Changes</a>
                            <ul>
              <li>
                <a href="#readline()-no-longer-clears-the-stream-error-and-eof-flags">readline() no longer clears the stream error and eof flags</a>
              </li>
              <li>
                <a href="#INIT-blocks-no-longer-run-after-an-exit()-in-BEGIN">INIT blocks no longer run after an exit() in BEGIN</a>
              </li>
              <li>
                <a href="#Syntax-errors-no-longer-produce-%22phantom-error-messages%22">Syntax errors no longer produce "phantom error messages"</a>
              </li>
              <li>
                <a href="#utf8::upgrade()">utf8::upgrade()</a>
              </li>
              <li>
                <a href="#Changes-to-%22thread-safe%22-locales">Changes to "thread-safe" locales</a>
              </li>
          </ul>

              </li>
              <li>
                <a href="#Deprecations">Deprecations</a>
                            <ul>
              <li>
                <a href="#Use-of-'-as-a-package-name-separator-is-deprecated">Use of ' as a package name separator is deprecated</a>
              </li>
              <li>
                <a href="#Switch-and-Smart-Match-operator">Switch and Smart Match operator</a>
              </li>
          </ul>

              </li>
              <li>
                <a href="#Performance-Enhancements">Performance Enhancements</a>
              </li>
              <li>
                <a href="#Modules-and-Pragmata">Modules and Pragmata</a>
                            <ul>
              <li>
                <a href="#Updated-Modules-and-Pragmata">Updated Modules and Pragmata</a>
              </li>
          </ul>

              </li>
              <li>
                <a href="#Documentation">Documentation</a>
                            <ul>
              <li>
                <a href="#New-Documentation">New Documentation</a>
                            <ul>
              <li>
                <a href="#perlclass">perlclass</a>
              </li>
              <li>
                <a href="#perlclassguts">perlclassguts</a>
              </li>
          </ul>

              </li>
              <li>
                <a href="#Changes-to-Existing-Documentation">Changes to Existing Documentation</a>
                            <ul>
              <li>
                <a href="#perlapi">perlapi</a>
              </li>
              <li>
                <a href="#perldeprecation">perldeprecation</a>
              </li>
              <li>
                <a href="#perlintern">perlintern</a>
              </li>
              <li>
                <a href="#perlexperiment">perlexperiment</a>
              </li>
              <li>
                <a href="#perlfunc">perlfunc</a>
              </li>
              <li>
                <a href="#perlhacktips">perlhacktips</a>
              </li>
              <li>
                <a href="#perlop">perlop</a>
              </li>
              <li>
                <a href="#perlvar">perlvar</a>
              </li>
          </ul>

              </li>
          </ul>

              </li>
              <li>
                <a href="#Diagnostics">Diagnostics</a>
                            <ul>
              <li>
                <a href="#New-Diagnostics">New Diagnostics</a>
                            <ul>
              <li>
                <a href="#New-Errors">New Errors</a>
              </li>
              <li>
                <a href="#New-Warnings">New Warnings</a>
              </li>
          </ul>

              </li>
              <li>
                <a href="#Changes-to-Existing-Diagnostics">Changes to Existing Diagnostics</a>
              </li>
          </ul>

              </li>
              <li>
                <a href="#Configuration-and-Compilation">Configuration and Compilation</a>
              </li>
              <li>
                <a href="#Testing">Testing</a>
              </li>
              <li>
                <a href="#Platform-Support">Platform Support</a>
                            <ul>
              <li>
                <a href="#Discontinued-Platforms">Discontinued Platforms</a>
              </li>
              <li>
                <a href="#Platform-Specific-Notes">Platform-Specific Notes</a>
              </li>
          </ul>

              </li>
              <li>
                <a href="#Internal-Changes">Internal Changes</a>
              </li>
              <li>
                <a href="#Selected-Bug-Fixes">Selected Bug Fixes</a>
              </li>
              <li>
                <a href="#Acknowledgements">Acknowledgements</a>
              </li>
              <li>
                <a href="#Reporting-Bugs">Reporting Bugs</a>
              </li>
              <li>
                <a href="#Give-Thanks">Give Thanks</a>
              </li>
              <li>
                <a href="#SEE-ALSO">SEE ALSO</a>
              </li>
          </ul>

      <h2 id="NAME"><a href="#NAME">#</a>NAME</h2>

<p>perldelta - what is new for perl v5.38.0</p>

<h2 id="DESCRIPTION"><a href="#DESCRIPTION">#</a>DESCRIPTION</h2>

<p>This document describes differences between the 5.36.0 release and the 5.38.0 release.</p>

<h2 id="Core-Enhancements"><a href="#Core-Enhancements">#</a><a id="Core"></a>Core Enhancements</h2>

<h2 id="New-class-Feature"><a href="#New-class-Feature">#</a><a id="New"></a>New <code>class</code> Feature</h2>

<p>A new <b>experimental</b> syntax is now available for defining object classes, where per-instance data is stored in "field" variables that behave like lexicals.</p>

<pre><code>use feature 'class';

class Point
{
    field $x;
    field $y;

    method zero { $x = $y = 0; }
}</code></pre>

<p>This is described in more detail in <a href="https://perldoc.perl.org/perlclass">perlclass</a>. Notes on the internals of its implementation and other related details can be found in <a href="https://perldoc.perl.org/perlclassguts">perlclassguts</a>.</p>

<p>This remains a new and experimental feature, and is very much still under development. It will be the subject of much further addition, refinement and alteration in future releases. As it is experimental, it yields warnings in the <code>experimental::class</code> category. These can be silenced by a <code>no warnings</code> statement.</p>

<pre><code>use feature 'class';
no warnings 'experimental::class';</code></pre>

<h2 id="Unicode-15.0-is-supported"><a href="#Unicode-15.0-is-supported">#</a><a id="Unicode"></a>Unicode 15.0 is supported</h2>

<p>See <a href="https://www.unicode.org/versions/Unicode15.0.0/">https://www.unicode.org/versions/Unicode15.0.0/</a> for details.</p>

<h2 id="Deprecation-warnings-now-have-specific-subcategories"><a href="#Deprecation-warnings-now-have-specific-subcategories">#</a><a id="Deprecation"></a>Deprecation warnings now have specific subcategories</h2>

<p>All deprecation warnings now have their own specific deprecation category which can be disabled individually. You can see a list of all deprecated features in <a href="https://perldoc.perl.org/perldeprecation">perldeprecation</a>, and in <a href="https://perldoc.perl.org/warnings">warnings</a>. The following list is from <a href="https://perldoc.perl.org/warnings">warnings</a>:</p>

<pre><code>+- deprecated ----+
|                 |
|                 +- deprecated::apostrophe_as_package_separator
|                 |
|                 +- deprecated::delimiter_will_be_paired
|                 |
|                 +- deprecated::dot_in_inc
|                 |
|                 +- deprecated::goto_construct
|                 |
|                 +- deprecated::smartmatch
|                 |
|                 +- deprecated::unicode_property_name
|                 |
|                 +- deprecated::version_downgrade</code></pre>

<p>It is still possible to disable all deprecation warnings in a single statement with</p>

<pre><code>no warnings 'deprecated';</code></pre>

<p>but now is possible to have a finer grained control. As has historically been the case these warnings are automatically enabled with</p>

<pre><code>use warnings;</code></pre>

<h2 id="%{^HOOK}-API-introduced"><a href="#%25%7B%5EHOOK%7D-API-introduced">#</a><a id="HOOK-API-introduced"></a>%{^HOOK} API introduced</h2>

<p>For various reasons it can be difficult to create subroutine wrappers for some of perls keywords. Any keyword which has an undefined prototype simply cannot be wrapped with a subroutine, and some keywords which perl permits to be wrapped are in practice very tricky to wrap. For example <code>require</code> is tricky to wrap, it is possible but doing so changes the stack depth, and the standard methods of exporting assume that they will be exporting to a package at certain stack depth up the stack, and the wrapper will thus change where functions are exported to unless implemented with a great deal of care. This can be very awkward to deal with.</p>

<p>Accordingly we have introduced a new hash called <code>%{^HOOK}</code> which is intended to facilitate such cases. When a keyword supports any kind of special hook then the hook will live in this new hash. Hooks in this hash will be named after the function they are called by, followed by two underbars and then the phase they are executed in, currently either before or after the keyword is executed.</p>

<p>In this initial release we support two hooks <code>require__before</code> and <code>require__after</code>. These are provided to make it easier to perform tasks before and after a require statement.</p>

<p>See <a href="https://perldoc.perl.org/perlvar">perlvar</a> for more details.</p>

<h2 id="PERL_RAND_SEED"><a href="#PERL_RAND_SEED">#</a>PERL_RAND_SEED</h2>

<p>Added a new environment variable <code>PERL_RAND_SEED</code> which can be used to cause a perl program which uses <code>rand</code> without using <code>srand()</code> explicitly or which uses <code>srand()</code> with no arguments to be repeatable. See <a href="https://perldoc.perl.org/perlrun">perlrun</a>. This feature can be disabled at compile time by passing</p>

<pre><code>-Accflags=-DNO_PERL_RAND_SEED</code></pre>

<p>to <i>Configure</i> during the build process.</p>

<h2 id="Defined-or-and-logical-or-assignment-default-expressions-in-signatures"><a href="#Defined-or-and-logical-or-assignment-default-expressions-in-signatures">#</a><a id="Defined"></a>Defined-or and logical-or assignment default expressions in signatures</h2>

<p>The default expression for a subroutine signature parameter can now be assigned using the <code>//=</code> or <code>||=</code> operators, to apply the defaults whenever the caller provided an undefined or false value (respectively), rather than simply when the parameter is missing entirely. For more detail see the documentation in <a href="https://perldoc.perl.org/perlsub">perlsub</a>.</p>

<h2 id="@INC-Hook-Enhancements-and-$INC-and-INCDIR"><a href="#@INC-Hook-Enhancements-and-$INC-and-INCDIR">#</a><a id="INC-Hook-Enhancements-and-INC-and-INCDIR"></a>@INC Hook Enhancements and $INC and INCDIR</h2>

<p>The internals for <code>@INC</code> hooks have been hardened to handle various edge cases and should no longer segfault or throw assert failures when hooks modify <code>@INC</code> during a require operation. As part of this we now ensure that any given hook is executed at most once during a require call, and that any duplicate directories do not trigger additional directory probes.</p>

<p>To provide developers more control over dynamic module lookup, a new hook method <code>INCDIR</code> is now supported. An object supporting this method may be injected into the <code>@INC</code> array, and when it is encountered in the module search process it will be executed, just like how INC hooks are executed, and its return value used as a list of directories to search for the module. Returning an empty list acts as a no-op. Note that since any references returned by this hook will be stringified and used as strings, you may not return a hook to be executed later via this API.</p>

<p>When an <code>@INC</code> hook (either <code>INC</code> or <code>INCDIR</code>) is called during require, the <code>$INC</code> variable will be localized to be the value of the index of <code>@INC</code> that the hook came from. If the hook wishes to override what the "next" index in <code>@INC</code> should be it may update <code>$INC</code> to be one less than the desired index (<code>undef</code> is equivalent to <code>-1</code>). This allows an <code>@INC</code> hook to completely rewrite the <code>@INC</code> array and have perl restart its directory probes from the beginning of <code>@INC</code>.</p>

<p>Blessed CODE references in <code>@INC</code> that do not support the <code>INC</code> or <code>INCDIR</code> methods will no longer trigger an exception, and instead will be treated the same as unblessed coderefs are, and executed as though they were an <code>INC</code> hook.</p>

<h2 id="Forbidden-control-flow-out-of-defer-or-finally-now-detected-at-compile-time"><a href="#Forbidden-control-flow-out-of-defer-or-finally-now-detected-at-compile-time">#</a><a id="Forbidden"></a>Forbidden control flow out of <code>defer</code> or <code>finally</code> now detected at compile-time</h2>

<p>It is forbidden to attempt to leave a <code>defer</code> or <code>finally</code> block by means of control flow such as <code>return</code> or <code>goto</code>. Previous versions of perl could only detect this when actually attempted at runtime.</p>

<p>This version of perl adds compile-time detection for many cases that can be statically determined. This may mean that code which compiled successfully on a previous version of perl is now reported as a compile-time error with this one. This only happens in cases where it would have been an error to actually execute the code anyway; the error simply happens at an earlier time.</p>

<h2 id="Optimistic-Eval-in-Patterns"><a href="#Optimistic-Eval-in-Patterns">#</a><a id="Optimistic"></a>Optimistic Eval in Patterns</h2>

<p>The use of <code>(?{ ... })</code> and <code>(??{ ... })</code> in a pattern disables various optimisations globally in that pattern. This may or may not be desired by the programmer. This release adds the <code>(*{ ... })</code> equivalent. The only difference is that it does not and will never disable any optimisations in the regex engine. This may make it more unstable in the sense that it may be called more or less times in the future, however the number of times it executes will truly match how the regex engine functions. For example, certain types of optimisation are disabled when <code>(?{ ... })</code> is included in a pattern, so that patterns which are O(N) in normal use become O(N*N) with a <code>(?{ ... })</code> pattern in them. Switching to <code>(*{ ... })</code> means the pattern will stay O(N).</p>

<h2 id="REG_INF-has-been-raised-from-65,536-to-2,147,483,647"><a href="#REG_INF-has-been-raised-from-65,536-to-2,147,483,647">#</a><a id="REG_INF"></a><a id="REG_INF-has-been-raised-from-65-536-to-2-147-483-647"></a>REG_INF has been raised from 65,536 to 2,147,483,647</h2>

<p>Many regex quantifiers used to be limited to <code>U16_MAX</code> in the past, but are now limited to <code>I32_MAX</code>, thus it is now possible to write <code>/(?:word){1000000}/</code> for example. Note that doing so may cause the regex engine to run longer and use more memory.</p>

<h2 id="New-API-functions-optimize_optree-and-finalize_optree"><a href="#New-API-functions-optimize_optree-and-finalize_optree">#</a><a id="New1"></a>New API functions optimize_optree and finalize_optree</h2>

<p>There are two new API functions for operating on optree fragments, ensuring you can invoke the required parts of the optree-generation process that might otherwise not get invoked (e.g. when creating a custom LOGOP). To get access to these functions, you first need to set a <code>#define</code> to opt-in to using these functions.</p>

<pre><code>#define PERL_USE_VOLATILE_API</code></pre>

<p>These functions are closely tied to the internals of how the interpreter works, and could be altered or removed at any time if other internal changes make that necessary.</p>

<h2 id="Some-gotos-are-now-permitted-in-defer-and-finally-blocks"><a href="#Some-gotos-are-now-permitted-in-defer-and-finally-blocks">#</a><a id="Some"></a>Some <code>goto</code>s are now permitted in <code>defer</code> and <code>finally</code> blocks</h2>

<p>Perl version 5.36.0 added <code>defer</code> blocks and permitted the <code>finally</code> keyword to also add similar behaviour to <code>try</code>/<code>catch</code> syntax. These did not permit any <code>goto</code> expression within the body, as it could have caused control flow to jump out of the block. Now, some <code>goto</code> expressions are allowed, if they have a constant target label, and that label is found within the block.</p>

<pre><code>use feature 'defer';

defer {
  goto LABEL;
  print "This does not execute\n";
  LABEL: print "This does\n";
}</code></pre>

<h2 id="New-regexp-variable-${^LAST_SUCCESSFUL_PATTERN}"><a href="#New-regexp-variable-$%7B%5ELAST_SUCCESSFUL_PATTERN%7D">#</a><a id="New2"></a><a id="New-regexp-variable-LAST_SUCCESSFUL_PATTERN"></a>New regexp variable ${^LAST_SUCCESSFUL_PATTERN}</h2>

<p>This allows access to the last succesful pattern that matched in the current scope. Many aspects of the regex engine refer to the "last successful pattern". The empty pattern reuses it, and all of the magic regex vars relate to it. This allows access to its pattern. The following code</p>

<pre><code>if (m/foo/ || m/bar/) {
    s//PQR/;
}</code></pre>

<p>can be rewritten as follows</p>

<pre><code>if (m/foo/ || m/bar/) {
    s/${^LAST_SUCCESSFUL_PATTERN}/PQR/;
}</code></pre>

<p>and it will do the exactly same thing.</p>

<h2 id="Locale-category-LC_NAME-now-supported-on-participating-platforms"><a href="#Locale-category-LC_NAME-now-supported-on-participating-platforms">#</a><a id="Locale"></a>Locale category LC_NAME now supported on participating platforms</h2>

<p>On platforms that have the GNU extension <code>LC_NAME</code> category, you may now use it as the category parameter to <a href="https://perldoc.perl.org/POSIX#setlocale">"setlocale" in POSIX</a> to set and query its locale.</p>

<h2 id="Incompatible-Changes"><a href="#Incompatible-Changes">#</a><a id="Incompatible"></a>Incompatible Changes</h2>

<h2 id="readline()-no-longer-clears-the-stream-error-and-eof-flags"><a href="#readline()-no-longer-clears-the-stream-error-and-eof-flags">#</a><a id="readline"></a><a id="readline-no-longer-clears-the-stream-error-and-eof-flags"></a>readline() no longer clears the stream error and eof flags</h2>

<p><code>readline()</code>, also spelled <code>&lt;&gt;</code>, would clear the handle's error and eof flags after an error occurred on the stream.</p>

<p>In nearly all cases this clear is no longer done, so the error and eof flags now properly reflect the status of the stream after readline().</p>

<p>Since the error flag is no longer cleared calling close() on the stream may fail and if the stream was not explicitly closed, the implicit close of the stream may produce a warning.</p>

<p>This has resulted in two main types of problems in downstream CPAN modules, and these may also occur in your code:</p>

<ul>

<li><p>If your code reads to end of file, and then rebinds the handle to a new file descriptor, previously since the eof flag wasn't set you could continue to read from the stream. You now need to clear the eof flag yourself with <code>$handle-&gt;clearerr()</code> to continue reading.</p>

</li>
<li><p>If your code encounters an error on the stream while reading with readline() you will need to call <code>$handle-&gt;clearerr</code> to continue reading. The one case this occurred the underlying file descriptor was marked non-blocking, so the read() system call was failing with <code>EAGAIN</code>, which resulted in the error flag being set on the stream.</p>

</li>
</ul>

<p>The only case where error and eof flags continue to cleared on error is when reading from the child process for glob() in <i>miniperl</i>. This allows it to correctly report errors from the child process on close(). This is unlikely to be an issue during normal perl development.</p>

<p>[<a href="https://github.com/Perl/perl5/issues/20060">GH #20060</a>]</p>

<h2 id="INIT-blocks-no-longer-run-after-an-exit()-in-BEGIN"><a href="#INIT-blocks-no-longer-run-after-an-exit()-in-BEGIN">#</a><a id="INIT"></a><a id="INIT-blocks-no-longer-run-after-an-exit-in-BEGIN"></a><code>INIT</code> blocks no longer run after an <code>exit()</code> in <code>BEGIN</code></h2>

<p><code>INIT</code> blocks will no longer run after an <code>exit()</code> performed inside of a <code>BEGIN</code>. This means that the combination of the <code>-v</code> option and the <code>-c</code> option no longer executes a compile check as well as showing the perl version. The <code>-v</code> option executes an exit(0) after printing the version information inside of a <code>BEGIN</code> block, and the <code>-c</code> check is implemented by using <code>INIT</code> hooks, resulting in the <code>-v</code> option taking precedence.</p>

<p>[<a href="https://github.com/Perl/perl5/issues/1537">GH #1537</a>] [<a href="https://github.com/Perl/perl5/issues/20181">GH #20181</a>]</p>

<h2 id="Syntax-errors-no-longer-produce-&quot;phantom-error-messages&quot;"><a href="#Syntax-errors-no-longer-produce-%22phantom-error-messages%22">#</a><a id="Syntax"></a><a id="Syntax-errors-no-longer-produce-phantom-error-messages"></a>Syntax errors no longer produce "phantom error messages"</h2>

<p>Generally perl will continue parsing the source code even after encountering a compile error. In many cases this is helpful, for instance with misspelled variable names it is helpful to show as many examples of the error as possible. But in the case of syntax errors continuing often produces bizarre error messages and may even cause segmentation faults during the compile process. In this release the compiler will halt at the first syntax error encountered. This means that any code expecting to see the specific error messages we used to produce will be broken. The error that is emitted will be one of the diagnostics that used to be produced, but in some cases some messages that used to be produced will no longer be displayed.</p>

<p>See <a href="#Changes-to-Existing-Diagnostics">"Changes to Existing Diagnostics"</a> for more details.</p>

<h2 id="utf8::upgrade()"><a href="#utf8::upgrade()">#</a><a id="utf8"></a><a id="utf8::upgrade"></a><a href="https://perldoc.perl.org/utf8#Utility-functions"><code>utf8::upgrade()</code></a></h2>

<p>Starting in this release, if the input string is <code>undef</code>, it remains <code>undef</code>. Previously it would be changed into a defined, zero-length string.</p>

<h2 id="Changes-to-&quot;thread-safe&quot;-locales"><a href="#Changes-to-%22thread-safe%22-locales">#</a><a id="Changes"></a><a id="Changes-to-thread-safe-locales"></a>Changes to "thread-safe" locales</h2>

<p>Perl 5.28 introduced "thread-safe" locales on systems that supported them, namely modern Windows, and systems supporting POSIX 2008 locale operations. These systems accomplish this by having per-thread locales, while continuing to support the older global locale operations for code that doesn't take the steps necessary to use the newer per-thread ones.</p>

<p>It turns out that some POSIX 2008 platforms have or have had buggy implementations, which forced perl to not use them. The <code>${^SAFE_LOCALES}</code> scalar variable contains 0 or 1 to indicate whether or not the current platform is considered by perl to have a working thread-safe implementation. Some implementations have been fixed already, but FreeBSD and Cygwin have been newly discovered to be sufficiently buggy that the thread-safe operations are no longer used by perl, starting in this release. Hence, <code>${^SAFE_LOCALES}</code> is now 0 for them. Older versions of perl can be configured to avoid these buggy implementations by adding the <i>Configure</i> option <code>-DNO_POSIX_2008_LOCALE</code>.</p>

<p>And v5.38 fixes a bug in all previous perls that led to locales not being fully thread-safe. The first thread that finishes caused the main thread (named <code>thread0</code>) to revert to the global locale in effect at startup, discarding whatever the thread's locale had been previously set to. If any other thread had switched to the global locale by calling <code>switch_to_global_locale()</code> in XS code, those threads would all share the global locale, and <code>thread0</code> would not be thread-safe.</p>

<h2 id="Deprecations"><a href="#Deprecations">#</a>Deprecations</h2>

<h2 id="Use-of-'-as-a-package-name-separator-is-deprecated"><a href="#Use-of-'-as-a-package-name-separator-is-deprecated">#</a><a id="Use"></a><a id="Use-of-as-a-package-name-separator-is-deprecated"></a>Use of <code>'</code> as a package name separator is deprecated</h2>

<p>Using <code>'</code> as package separator in a variable named in a double-quoted string has warned since 5.28. It is now deprecated in both string interpolation and non-interpolated contexts, and will be removed in Perl 5.42.</p>

<h2 id="Switch-and-Smart-Match-operator"><a href="#Switch-and-Smart-Match-operator">#</a><a id="Switch"></a>Switch and Smart Match operator</h2>

<p>The "switch" feature and the smartmatch operator, <code>~~</code>, were introduced in v5.10. Their behavior was significantly changed in v5.10.1. When the "experiment" system was added in v5.18.0, switch and smartmatch were retroactively declared experimental. Over the years, proposals to fix or supplement the features have come and gone.</p>

<p>In v5.38.0, we are declaring the experiment a failure. Some future system may take the conceptual place of smartmatch, but it has not yet been designed or built.</p>

<p>These features will be entirely removed from perl in v5.42.0.</p>

<h2 id="Performance-Enhancements"><a href="#Performance-Enhancements">#</a><a id="Performance"></a>Performance Enhancements</h2>

<ul>

<li><p>Additional optree optimizations for common OP patterns. For example, multiple simple OPs replaced by a single streamlined OP, so as to be more efficient at runtime. [<a href="https://github.com/Perl/perl5/pull/19943">GH #19943</a>], [<a href="https://github.com/Perl/perl5/pull/20063">GH #20063</a>], [<a href="https://github.com/Perl/perl5/pull/20077">GH #20077</a>].</p>

</li>
<li><p>Creating an anonymous sub no longer generates an <code>srefgen</code> op, the reference generation is now done in the <code>anoncode</code> or <code>anonconst</code> op, saving runtime. [<a href="https://github.com/Perl/perl5/pull/20290">GH #20290</a>]</p>

</li>
</ul>

<h2 id="Modules-and-Pragmata"><a href="#Modules-and-Pragmata">#</a><a id="Modules"></a>Modules and Pragmata</h2>

<h2 id="Updated-Modules-and-Pragmata"><a href="#Updated-Modules-and-Pragmata">#</a><a id="Updated"></a>Updated Modules and Pragmata</h2>

<ul>

<li><p>Added the <code>is_tainted()</code> builtin function. [<a href="https://github.com/Perl/perl5/issues/19854">GH #19854</a>]</p>

</li>
<li><p>Added the <code>export_lexically()</code> builtin function as per <a href="https://github.com/Perl/PPCs/blob/main/ppcs/ppc0020-lexical-export.md">PPC 0020</a>. [<a href="https://github.com/Perl/perl5/issues/19895">GH #19895</a>]</p>

</li>
<li><p>Support for <a href="https://github.com/Perl/PPCs/blob/main/ppcs/ppc0018-module-true.md">PPC 0018</a>, <code>use feature "module_true";</code> has been added to the default feature bundle for v5.38 and later. It may also be used explicitly. When enabled inside of a module the module does not need to return true explicitly, and in fact the return will be forced to a simple true value regardless of what it originally was.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Attribute::Handlers">Attribute::Handlers</a> has been upgraded from version 1.02 to 1.03.</p>

</li>
<li><p><a href="https://perldoc.perl.org/attributes">attributes</a> has been upgraded from version 0.34 to 0.35.</p>

</li>
<li><p><a href="https://perldoc.perl.org/autodie">autodie</a> has been upgraded from version 2.34 to 2.36.</p>

</li>
<li><p><a href="https://perldoc.perl.org/B">B</a> has been upgraded from version 1.83 to 1.88.</p>

</li>
<li><p><a href="https://perldoc.perl.org/B::Concise">B::Concise</a> has been upgraded from version 1.006 to 1.007.</p>

</li>
<li><p><a href="https://perldoc.perl.org/B::Deparse">B::Deparse</a> has been upgraded from version 1.64 to 1.74.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Benchmark">Benchmark</a> has been upgraded from version 1.23 to 1.24.</p>

</li>
<li><p><a href="https://perldoc.perl.org/bignum">bignum</a> has been upgraded from version 0.65 to 0.66.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Carp">Carp</a> has been upgraded from version 1.52 to 1.54.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Class::Struct">Class::Struct</a> has been upgraded from version 0.66 to 0.68.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Compress::Raw::Bzip2">Compress::Raw::Bzip2</a> has been upgraded from version 2.103 to 2.204_001.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Compress::Raw::Zlib">Compress::Raw::Zlib</a> has been upgraded from version 2.105 to 2.204_001.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Config::Perl::V">Config::Perl::V</a> has been upgraded from version 0.33 to 0.36.</p>

</li>
<li><p><a href="https://perldoc.perl.org/CPAN">CPAN</a> has been upgraded from version 2.33 to 2.36.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Data::Dumper">Data::Dumper</a> has been upgraded from version 2.184 to 2.188.</p>

</li>
<li><p><a href="https://perldoc.perl.org/DB_File">DB_File</a> has been upgraded from version 1.857 to 1.858.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Devel::Peek">Devel::Peek</a> has been upgraded from version 1.32 to 1.33.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Devel::PPPort">Devel::PPPort</a> has been upgraded from version 3.68 to 3.71.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Digest::MD5">Digest::MD5</a> has been upgraded from version 2.58 to 2.58_01.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Digest::SHA">Digest::SHA</a> has been upgraded from version 6.02 to 6.04.</p>

</li>
<li><p><a href="https://perldoc.perl.org/DynaLoader">DynaLoader</a> has been upgraded from version 1.52 to 1.54.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Encode">Encode</a> has been upgraded from version 3.17 to 3.19.</p>

</li>
<li><p><a href="https://perldoc.perl.org/encoding::warnings">encoding::warnings</a> has been upgraded from version 0.13 to 0.14.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Env">Env</a> has been upgraded from version 1.05 to 1.06.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Errno">Errno</a> has been upgraded from version 1.36 to 1.37.</p>

</li>
<li><p><a href="https://perldoc.perl.org/experimental">experimental</a> has been upgraded from version 0.028 to 0.031.</p>

</li>
<li><p><a href="https://perldoc.perl.org/ExtUtils::CBuilder">ExtUtils::CBuilder</a> has been upgraded from version 0.280236 to 0.280238.</p>

</li>
<li><p><a href="https://perldoc.perl.org/ExtUtils::Install">ExtUtils::Install</a> has been upgraded from version 2.20 to 2.22.</p>

</li>
<li><p><a href="https://perldoc.perl.org/ExtUtils::MakeMaker">ExtUtils::MakeMaker</a> has been upgraded from version 7.64 to 7.70.</p>

</li>
<li><p><a href="https://perldoc.perl.org/ExtUtils::Miniperl">ExtUtils::Miniperl</a> has been upgraded from version 1.11 to 1.13.</p>

</li>
<li><p><a href="https://perldoc.perl.org/ExtUtils::ParseXS">ExtUtils::ParseXS</a> has been upgraded from version 3.45 to 3.51.</p>

</li>
<li><p><a href="https://perldoc.perl.org/ExtUtils::PL2Bat">ExtUtils::PL2Bat</a> has been upgraded from version 0.004 to 0.005.</p>

</li>
<li><p><a href="https://perldoc.perl.org/ExtUtils::Typemaps">ExtUtils::Typemaps</a> has been upgraded from version 3.45 to 3.51.</p>

</li>
<li><p><a href="https://perldoc.perl.org/feature">feature</a> has been upgraded from version 1.72 to 1.82.</p>

</li>
<li><p><a href="https://perldoc.perl.org/File::Basename">File::Basename</a> has been upgraded from version 2.85 to 2.86.</p>

</li>
<li><p><a href="https://perldoc.perl.org/File::Copy">File::Copy</a> has been upgraded from version 2.39 to 2.41.</p>

</li>
<li><p><a href="https://perldoc.perl.org/File::Find">File::Find</a> has been upgraded from version 1.40 to 1.43.</p>

</li>
<li><p><a href="https://perldoc.perl.org/File::Glob">File::Glob</a> has been upgraded from version 1.37 to 1.40.</p>

</li>
<li><p><a href="https://perldoc.perl.org/File::Spec">File::Spec</a> has been upgraded from version 3.84 to 3.89.</p>

</li>
<li><p><a href="https://perldoc.perl.org/File::stat">File::stat</a> has been upgraded from version 1.12 to 1.13.</p>

</li>
<li><p><a href="https://perldoc.perl.org/FileHandle">FileHandle</a> has been upgraded from version 2.03 to 2.05.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Filter::Util::Call">Filter::Util::Call</a> has been upgraded from version 1.60 to 1.64.</p>

</li>
<li><p><a href="https://perldoc.perl.org/GDBM_File">GDBM_File</a> has been upgraded from version 1.23 to 1.24.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Getopt::Long">Getopt::Long</a> has been upgraded from version 2.52 to 2.54.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Hash::Util">Hash::Util</a> has been upgraded from version 0.28 to 0.30.</p>

</li>
<li><p><a href="https://perldoc.perl.org/HTTP::Tiny">HTTP::Tiny</a> has been upgraded from version 0.080 to 0.083.</p>

</li>
<li><p><a href="https://perldoc.perl.org/I18N::Langinfo">I18N::Langinfo</a> has been upgraded from version 0.21 to 0.22.</p>

</li>
<li><p><a href="https://perldoc.perl.org/IO">IO</a> has been upgraded from version 1.50 to 1.52.</p>

</li>
<li><p>IO-Compress has been upgraded from version 2.106 to 2.204.</p>

</li>
<li><p><a href="https://perldoc.perl.org/IO::Socket::IP">IO::Socket::IP</a> has been upgraded from version 0.41 to 0.41_01.</p>

<p>On DragonflyBSD, detect setsockopt() not actually clearing <code>IPV6_V6ONLY</code> even when setsockopt() returns success. [<a href="https://rt.cpan.org/Ticket/Display.html?id=148293">cpan #148293</a>]</p>

</li>
<li><p><a href="https://perldoc.perl.org/IO::Zlib">IO::Zlib</a> has been upgraded from version 1.11 to 1.14.</p>

</li>
<li><p><a href="https://perldoc.perl.org/JSON::PP">JSON::PP</a> has been upgraded from version 4.07 to 4.16.</p>

</li>
<li><p>libnet has been upgraded from version 3.14 to 3.15.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Locale::Maketext">Locale::Maketext</a> has been upgraded from version 1.31 to 1.33.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Math::BigInt">Math::BigInt</a> has been upgraded from version 1.999830 to 1.999837.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Math::BigInt::FastCalc">Math::BigInt::FastCalc</a> has been upgraded from version 0.5012 to 0.5013.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Math::BigRat">Math::BigRat</a> has been upgraded from version 0.2621 to 0.2624.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Math::Complex">Math::Complex</a> has been upgraded from version 1.5902 to 1.62.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Memoize">Memoize</a> has been upgraded from version 1.03_01 to 1.16.</p>

</li>
<li><p><a href="https://perldoc.perl.org/MIME::Base64">MIME::Base64</a> has been upgraded from version 3.16 to 3.16_01.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Module::CoreList">Module::CoreList</a> has been upgraded from version 5.20220520 to 5.20230520.</p>

</li>
<li><p><a href="https://perldoc.perl.org/mro">mro</a> has been upgraded from version 1.26 to 1.28.</p>

</li>
<li><p><a href="https://perldoc.perl.org/NDBM_File">NDBM_File</a> has been upgraded from version 1.15 to 1.16.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Net::Ping">Net::Ping</a> has been upgraded from version 2.74 to 2.76.</p>

</li>
<li><p><a href="https://perldoc.perl.org/ODBM_File">ODBM_File</a> has been upgraded from version 1.17 to 1.18.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Opcode">Opcode</a> has been upgraded from version 1.57 to 1.64.</p>

</li>
<li><p><a href="https://perldoc.perl.org/overload">overload</a> has been upgraded from version 1.35 to 1.37.</p>

</li>
<li><p><a href="https://perldoc.perl.org/parent">parent</a> has been upgraded from version 0.238 to 0.241.</p>

</li>
<li><p><a href="https://perldoc.perl.org/PerlIO::via::QuotedPrint">PerlIO::via::QuotedPrint</a> has been upgraded from version 0.09 to 0.10.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Pod::Checker">Pod::Checker</a> has been upgraded from version 1.74 to 1.75.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Pod::Html">Pod::Html</a> has been upgraded from version 1.33 to 1.34.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Pod::Usage">Pod::Usage</a> has been upgraded from version 2.01 to 2.03.</p>

</li>
<li><p><a href="https://perldoc.perl.org/podlators">podlators</a> has been upgraded from version 4.14 to 5.01.</p>

</li>
<li><p><a href="https://perldoc.perl.org/POSIX">POSIX</a> has been upgraded from version 2.03 to 2.13.</p>

</li>
<li><p><a href="https://perldoc.perl.org/re">re</a> has been upgraded from version 0.43 to 0.44.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Safe">Safe</a> has been upgraded from version 2.43 to 2.44.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Scalar::Util">Scalar::Util</a> has been upgraded from version 1.62 to 1.63.</p>

</li>
<li><p><a href="https://perldoc.perl.org/SDBM_File">SDBM_File</a> has been upgraded from version 1.15 to 1.17.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Socket">Socket</a> has been upgraded from version 2.033 to 2.036.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Storable">Storable</a> has been upgraded from version 3.26 to 3.32.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Sys::Hostname">Sys::Hostname</a> has been upgraded from version 1.24 to 1.25.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Term::Cap">Term::Cap</a> has been upgraded from version 1.17 to 1.18.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Test::Simple">Test::Simple</a> has been upgraded from version 1.302190 to 1.302194.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Text::Balanced">Text::Balanced</a> has been upgraded from version 2.04 to 2.06.</p>

</li>
<li><p><a href="https://perldoc.perl.org/threads">threads</a> has been upgraded from version 2.27 to 2.36.</p>

</li>
<li><p><a href="https://perldoc.perl.org/threads::shared">threads::shared</a> has been upgraded from version 1.64 to 1.68.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Tie::File">Tie::File</a> has been upgraded from version 1.06 to 1.07.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Time::HiRes">Time::HiRes</a> has been upgraded from version 1.9770 to 1.9775.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Time::Piece">Time::Piece</a> has been upgraded from version 1.3401 to 1.3401_01.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Unicode::Normalize">Unicode::Normalize</a> has been upgraded from version 1.31 to 1.32.</p>

</li>
<li><p><a href="https://perldoc.perl.org/UNIVERSAL">UNIVERSAL</a> has been upgraded from version 1.14 to 1.15.</p>

</li>
<li><p><a href="https://perldoc.perl.org/User::grent">User::grent</a> has been upgraded from version 1.03 to 1.04.</p>

</li>
<li><p><a href="https://perldoc.perl.org/User::pwent">User::pwent</a> has been upgraded from version 1.01 to 1.02.</p>

</li>
<li><p><a href="https://perldoc.perl.org/utf8">utf8</a> has been upgraded from version 1.24 to 1.25.</p>

</li>
<li><p><a href="https://perldoc.perl.org/warnings">warnings</a> has been upgraded from version 1.58 to 1.65.</p>

</li>
<li><p><a href="https://perldoc.perl.org/XS::APItest">XS::APItest</a> has been upgraded from version 1.22 to 1.32.</p>

</li>
<li><p><a href="https://perldoc.perl.org/XSLoader">XSLoader</a> has been upgraded from version 0.31 to 0.32.</p>

</li>
</ul>

<h2 id="Documentation"><a href="#Documentation">#</a>Documentation</h2>

<h2 id="New-Documentation"><a href="#New-Documentation">#</a><a id="New3"></a>New Documentation</h2>

<h3 id="perlclass"><a href="#perlclass">#</a><a href="https://perldoc.perl.org/perlclass">perlclass</a></h3>

<p>Describes the new <code>class</code> feature.</p>

<h3 id="perlclassguts"><a href="#perlclassguts">#</a><a href="https://perldoc.perl.org/perlclassguts">perlclassguts</a></h3>

<p>Describes the internals of the new <code>class</code> feature.</p>

<h2 id="Changes-to-Existing-Documentation"><a href="#Changes-to-Existing-Documentation">#</a><a id="Changes1"></a>Changes to Existing Documentation</h2>

<p>We have attempted to update the documentation to reflect the changes listed in this document. If you find any we have missed, open an issue at <a href="https://github.com/Perl/perl5/issues">https://github.com/Perl/perl5/issues</a>.</p>

<p>Additionally, the following selected changes have been made:</p>

<h3 id="perlapi"><a href="#perlapi">#</a><a href="https://perldoc.perl.org/perlapi">perlapi</a></h3>

<ul>

<li><p>Documented <a href="https://perldoc.perl.org/perlapi#hv_ksplit"><code>hv_ksplit</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#hv_name_set"><code>hv_name_set</code></a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perlapi#hv_store"><code>hv_store</code></a> and <a href="https://perldoc.perl.org/perlapi#hv_stores"><code>hv_stores</code></a> documentation have been greatly improved.</p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#gv_autoload_pv"><code>gv_autoload_pv</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#gv_autoload_pvn"><code>gv_autoload_pvn</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#gv_autoload_sv"><code>gv_autoload_sv</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#gv_name_set"><code>gv_name_set</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#start_subparse"><code>start_subparse</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#SV_CHECK_THINKFIRST_COW_DROP"><code>SV_CHECK_THINKFIRST_COW_DROP</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#SV_CHECK_THINKFIRST"><code>SV_CHECK_THINKFIRST</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#SvPV_shrink_to_cur"><code>SvPV_shrink_to_cur</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#save_aelem"><code>save_aelem</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#save_aelem_flags"><code>save_aelem_flags</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#save_helem"><code>save_helem</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#save_helem_flags"><code>save_helem_flags</code></a></p>

</li>
</ul>

<h3 id="perldeprecation"><a href="#perldeprecation">#</a><a href="https://perldoc.perl.org/perldeprecation">perldeprecation</a></h3>

<ul>

<li><p>Added information about unscheduled deprecations and their categories.</p>

</li>
<li><p>Added category information for existing scheduled deprecations.</p>

</li>
<li><p>Added smartmatch and apostrophe as a package separator deprecation data.</p>

</li>
</ul>

<h3 id="perlintern"><a href="#perlintern">#</a><a href="https://perldoc.perl.org/perlintern">perlintern</a></h3>

<ul>

<li><p>Documented <a href="https://perldoc.perl.org/perlintern#save_pushptr"><code>save_pushptr</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlintern#save_scalar_at"><code>save_scalar_at</code></a></p>

</li>
<li><p>Entries have been added to <a href="https://perldoc.perl.org/perlguts">perlguts</a> for the new <code>newAV_alloc_x</code>, <code>newAV_alloc_xz</code> and <code>*_simple</code> functions.</p>

</li>
<li><p>References to the now-defunct PrePAN service have been removed from <a href="https://perldoc.perl.org/perlcommunity">perlcommunity</a> and <a href="https://perldoc.perl.org/perlmodstyle">perlmodstyle</a>.</p>

</li>
<li><p>A section on symbol naming has been added to <a href="https://perldoc.perl.org/perlhacktips">perlhacktips</a>.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perlexperiment">perlexperiment</a> has been edited to properly reference the warning categories for the defer block modifier and extra paired delimiters for quote-like operators.</p>

</li>
</ul>

<h3 id="perlexperiment"><a href="#perlexperiment">#</a><a href="https://perldoc.perl.org/perlexperiment">perlexperiment</a></h3>

<ul>

<li><p>Smartmatch has been moved from experimental status to deprecated status. Unfortunately the experiment did not work out.</p>

</li>
</ul>

<h3 id="perlfunc"><a href="#perlfunc">#</a><a href="https://perldoc.perl.org/perlfunc">perlfunc</a></h3>

<ul>

<li><p>Some wording improvements have been made for the <code>ucfirst</code>, <code>push</code>, <code>unshift</code> and <code>bless</code> functions, as well as additional examples added.</p>

</li>
</ul>

<h3 id="perlhacktips"><a href="#perlhacktips">#</a>perlhacktips</h3>

<ul>

<li><p>A new section, <a href="https://perldoc.perl.org/perlhacktips#Writing-safer-macros">"Writing safer macros" in perlhacktips</a> has been added to discuss pitfalls and solutions to using C macros in C and XS code.</p>

</li>
<li><p>A new section, <a href="https://perldoc.perl.org/perlhacktips#Choosing-good-symbol-names">"Choosing good symbol names" in perlhacktips</a>, has been added to discuss unexpected gotchas with names.</p>

</li>
</ul>

<h3 id="perlop"><a href="#perlop">#</a><a href="https://perldoc.perl.org/perlop">perlop</a></h3>

<ul>

<li><p>Document the behavior of matching the empty pattern better and specify its relationship to the new <code>${^LAST_SUCCESSFUL_PATTERN}</code> properly.</p>

</li>
</ul>

<h3 id="perlvar"><a href="#perlvar">#</a><a href="https://perldoc.perl.org/perlvar">perlvar</a></h3>

<ul>

<li><p>Added a section on "Scoping Rules of Regex Variables", and other wording improvements made throughout.</p>

</li>
<li><p>Added information on the new <code>%{^HOOK}</code> interface, and the new <code>require__before</code> and <code>require__after</code> hooks which it exposes.</p>

</li>
<li><p>Correct information on the regex variables <code>${^PREMATCH}</code>, <code>${^MATCH}</code> and <code>${^POSTMATCH}</code>, all of which were incorrectly documented due to an oversight. Specifically they only work properly after a regex operation that used the /p modifier to enable them.</p>

</li>
<li><p>Added information on the new regex variable <code>${^LAST_SUCCESSFUL_PATTERN}</code>, which represents the pattern of the last successful regex match in scope.</p>

</li>
</ul>

<h2 id="Diagnostics"><a href="#Diagnostics">#</a>Diagnostics</h2>

<p>The following additions or changes have been made to diagnostic output, including warnings and fatal error messages. For the complete list of diagnostic messages, see <a href="https://perldoc.perl.org/perldiag">perldiag</a>.</p>

<h2 id="New-Diagnostics"><a href="#New-Diagnostics">#</a><a id="New4"></a>New Diagnostics</h2>

<h3 id="New-Errors"><a href="#New-Errors">#</a><a id="New5"></a>New Errors</h3>

<ul>

<li><p>A new syntax error has been added for the error that a <code>catch</code> block does not have its required variable declaration. See <a href="https://perldoc.perl.org/perldiag#catch-block-requires-a-%28VAR%29">catch block requires a (VAR)</a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Too-many-nested-BEGIN-blocks%2C-maximum-of-%25d-allowed">Too many nested BEGIN blocks, maximum of %d allowed</a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Execution-of-%25s-aborted-due-to-compilation-errors.">Execution of %s aborted due to compilation errors.</a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Can%27t-locate-object-method-%22INC%22%2C-nor-%22INCDIR%22-nor-string-overload-via-package-%22%25s%22-%25s-in-%40INC">Can't locate object method "INC", nor "INCDIR" nor string overload via package "%s" %s in @INC</a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Attempt-to-bless-into-a-class">Attempt to bless into a class</a></p>

<p>(F) You are attempting to call <code>bless</code> with a package name that is a new-style <code>class</code>. This is not necessary, as instances created by the constructor are already in the correct class. Instances cannot be created by other means, such as <code>bless</code>.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Cannot-assign-%3Aparam%28%25s%29-to-field-%25s-because-that-name-is-already-in-use">Cannot assign :param(%s) to field %s because that name is already in use</a></p>

<p>(F) An attempt was made to apply a parameter name to a field, when the name is already being used by another field in the same class, or one of its parent classes. This would cause a name clash so is not allowed.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Cannot-create-class-%25s-as-it-already-has-a-non-empty-%40ISA">Cannot create class %s as it already has a non-empty @ISA</a></p>

<p>(F) An attempt was made to create a class out of a package that already has an <code>@ISA</code> array, and the array is not empty. This is not permitted, as it would lead to a class with inconsistent inheritance.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Cannot-invoke-a-method-of-%22%25s%22-on-an-instance-of-%22%25s%22">Cannot invoke a method of "%s" on an instance of "%s"</a></p>

<p>(F) You tried to directly call a <code>method</code> subroutine of one class by passing in a value that is an instance of a different class. This is not permitted, as the method would not have access to the correct instance fields.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Cannot-invoke-method-on-a-non-instance">Cannot invoke method on a non-instance</a></p>

<p>(F) You tried to directly call a <code>method</code> subroutine of a class by passing in a value that is not an instance of that class. This is not permitted, as the method would not then have access to its instance fields.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Cannot-%27%25s%27-outside-of-a-%27class%27">Cannot '%s' outside of a 'class'</a></p>

<p>(F) You attempted to use one of the keywords that only makes sense inside a <code>class</code> definition, at a location that is not inside such a class.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Cannot-reopen-existing-class-%22%25s%22">Cannot reopen existing class "%s"</a></p>

<p>(F) You tried to begin a <code>class</code> definition for a class that already exists. A class may only have one definition block.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Can%27t-bless-an-object-reference">Can't bless an object reference</a></p>

<p>(F) You attempted to call <code>bless</code> on a value that already refers to a real object instance.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#can%27t-convert-empty-path">can't convert empty path</a></p>

<p>(F) On Cygwin, you called a path conversion function with an empty path. Only non-empty paths are legal.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Class-already-has-a-superclass%2C-cannot-add-another">Class already has a superclass, cannot add another</a></p>

<p>(F) You attempted to specify a second superclass for a <code>class</code> by using the <code>:isa</code> attribute, when one is already specified. Unlike classes whose instances are created with <code>bless</code>, classes created via the <code>class</code> keyword cannot have more than one superclass.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Class-attribute-%25s-requires-a-value">Class attribute %s requires a value</a></p>

<p>(F) You specified an attribute for a class that would require a value to be passed in parentheses, but did not provide one. Remember that whitespace is <b>not</b> permitted between the attribute name and its value; you must write this as</p>

<pre><code>class Example::Class :attr(VALUE) ...</code></pre>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Class-%3Aisa-attribute-requires-a-class-but-%22%25s%22-is-not-one">Class :isa attribute requires a class but "%s" is not one</a></p>

<p>(F) When creating a subclass using the <code>class</code> <code>:isa</code> attribute, the named superclass must also be a real class created using the <code>class</code> keyword.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Field-already-has-a-parameter-name%2C-cannot-add-another">Field already has a parameter name, cannot add another</a></p>

<p>(F) A field may have at most one application of the <code>:param</code> attribute to assign a parameter name to it; once applied a second one is not allowed.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Field-attribute-%25s-requires-a-value">Field attribute %s requires a value</a></p>

<p>(F) You specified an attribute for a field that would require a value to be passed in parentheses, but did not provide one. Remember that whitespace is <b>not</b> permitted between the attribute name and its value; you must write this as</p>

<pre><code>field $var :attr(VALUE) ...</code></pre>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Field-%25s-is-not-accessible-outside-a-method">Field %s is not accessible outside a method</a></p>

<p>(F) An attempt was made to access a field variable of a class from code that does not appear inside the body of a <code>method</code> subroutine. This is not permitted, as only methods will have access to the fields of an instance.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Field-%25s-of-%22%25s%22-is-not-accessible-in-a-method-of-%22%25s%22">Field %s of "%s" is not accessible in a method of "%s"</a></p>

<p>(F) An attempt was made to access a field variable of a class, from a method of another class nested inside the one that actually defined it. This is not permitted, as only methods defined by a given class are permitted to access fields of that class.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Only-scalar-fields-can-take-a-%3Aparam-attribute">Only scalar fields can take a :param attribute</a></p>

<p>(F) You tried to apply the <code>:param</code> attribute to an array or hash field. Currently this is not permitted.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Required-parameter-%27%25s%27-is-missing-for-%25s-constructor">Required parameter '%s' is missing for %s constructor</a></p>

<p>(F) You called the constructor for a class that has a required named parameter, but did not pass that parameter at all.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Unexpected-characters-while-parsing-class-%3Aisa-attribute%3A-%25s">Unexpected characters while parsing class :isa attribute: %s</a></p>

<p>(F) You tried to specify something other than a single class name with an optional trailing version number as the value for a <code>class</code> <code>:isa</code> attribute. This confused the parser.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Unrecognized-class-attribute-%25s">Unrecognized class attribute %s</a></p>

<p>(F) You attempted to add a named attribute to a <code>class</code> definition, but perl does not recognise the name of the requested attribute.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Unrecognized-field-attribute-%25s">Unrecognized field attribute %s</a></p>

<p>(F) You attempted to add a named attribute to a <code>field</code> definition, but perl does not recognise the name of the requested attribute.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#%24%7B%5EHOOK%7D%7B%25s%7D-may-only-be-a-CODE-reference-or-undef">${^HOOK}{%s} may only be a CODE reference or undef</a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Attempt-to-set-unknown-hook-%27%25s%27-in-%25%7B%5EHOOK%7D">Attempt to set unknown hook '%s' in %{^HOOK}</a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Missing-or-undefined-argument-to-%25s-via-%25%7B%5EHOOK%7D%7Brequire__before%7D">Missing or undefined argument to %s via %{^HOOK}{require__before}</a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Too-many-capture-groups-%28limit-is-%25d%29-in-regex-m%2F%25s%2F">Too many capture groups (limit is %d) in regex m/%s/</a></p>

</li>
</ul>

<h3 id="New-Warnings"><a href="#New-Warnings">#</a><a id="New6"></a>New Warnings</h3>

<ul>

<li><p><a href="https://perldoc.perl.org/perldiag#Unknown-locale-category-%25d">Unknown locale category %d</a></p>

<p>This is a shortened form of an already existing diagnostic, for use when there is no new locale being switched to. The previous diagnostic was misleading in such circumstances.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Locale-%27%25s%27-is-unsupported%2C-and-may-crash-the-interpreter.">Locale '%s' is unsupported, and may crash the interpreter.</a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Treating-%25s%3A%3AINIT-block-as-BEGIN-block-as-workaround">Treating %s::INIT block as BEGIN block as workaround</a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Filehandle-STD%25s-reopened-as-%25s-only-for-input">Filehandle STD%s reopened as %s only for input</a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#%25s-on-BEGIN-block-ignored">%s on BEGIN block ignored</a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#ADJUST-is-experimental">ADJUST is experimental</a></p>

<p>(S experimental::class) This warning is emitted if you use the <code>ADJUST</code> keyword of <code>use feature 'class'</code>. This keyword is currently experimental and its behaviour may change in future releases of Perl.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#class-is-experimental">class is experimental</a></p>

<p>(S experimental::class) This warning is emitted if you use the <code>class</code> keyword of <code>use feature 'class'</code>. This keyword is currently experimental and its behaviour may change in future releases of Perl.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Method-%25s-redefined">Method %s redefined</a></p>

<p>(W redefine) You redefined a method. To suppress this warning, say</p>

<pre><code>{
   no warnings 'redefine';
   *name = method { ... };
}</code></pre>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Odd-number-of-elements-in-hash-field-initialization">Odd number of elements in hash field initialization</a></p>

<p>(W misc) You specified an odd number of elements to initialise a hash field of an object. Hashes are initialised from a list of key/value pairs so there must be a corresponding value to every key. The final missing value will be filled in with undef instead.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Old-package-separator-%22%27%22-deprecated">Old package separator "'" deprecated</a></p>

<p>(W deprecated, syntax) You used the old package separator "'" in a variable, subroutine or package name. Support for the old package separator will be removed in Perl 5.40.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#field-is-experimental">field is experimental</a></p>

<p>(S experimental::class) This warning is emitted if you use the <code>field</code> keyword of <code>use feature 'class'</code>. This keyword is currently experimental and its behaviour may change in future releases of Perl.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#method-is-experimental">method is experimental</a></p>

<p>(S experimental::class) This warning is emitted if you use the <code>method</code> keyword of <code>use feature 'class'</code>. This keyword is currently experimental and its behaviour may change in future releases of Perl.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Can%27t-call-destructor-for-0x%25p-in-global-destruction">Can't call destructor for 0x%p in global destruction</a></p>

</li>
</ul>

<h2 id="Changes-to-Existing-Diagnostics"><a href="#Changes-to-Existing-Diagnostics">#</a><a id="Changes2"></a>Changes to Existing Diagnostics</h2>

<ul>

<li><p>The compiler will now stop parsing on the first syntax error it encounters. Historically the compiler would attempt to "skip past" the error and continue parsing so that it could list multiple errors. For things like undeclared variables under strict this makes sense. For syntax errors however it has been found that continuing tends to result in a storm of unrelated or bizarre errors that mostly just obscure the true error. In extreme cases it can even lead to segfaults and other incorrect behavior.</p>

<p>Therefore we have reformed the continuation logic so that the parse will stop after the first seen syntax error. Semantic errors like undeclared variables will not stop the parse, so you may still see multiple errors when compiling code. However if there is a syntax error it will be the last error message reported by perl and all of the errors that you see will be something that actually needs to be fixed.</p>

</li>
<li><p>Error messages that output class or package names have been modified to output double quoted strings with various characters escaped so as to make the exact value clear to a reader. The exact rules on which characters are escaped may change over time but currently are that printable ASCII codepoints, with the exception of <code>"</code> and <code>\</code>, and unicode word characters whose codepoint is over 255 are output raw, and any other symbols are escaped much as Data::Dumper might escape them, using <code>\n</code> for newline and <code>\"</code> for double quotes, etc. Codepoints in the range 128-255 are always escaped as they can cause trouble on unicode terminals when output raw.</p>

<p>In older versions of perl the one liner</p>

<pre><code>$ perl -le'"thing\n"-&gt;foo()'</code></pre>

<p>would output the following error message exactly as shown here, with text spread over multiple lines because the "\n" would be emitted as a raw newline character:</p>

<pre><code>Can't locate object method "foo" via package "thing
" (perhaps you forgot to load "thing
"?) at -e line 1.</code></pre>

<p>As of this release we would output this instead (as one line):</p>

<pre><code>Can't locate object method "foo" via package "thing\n"
  (perhaps you forgot to load "thing\n"?) at -e line 1.</code></pre>

<p>Notice the newline in the package name has been quoted and escaped, and thus the error message is a single line. The text is shown here wrapped to two lines only for readability.</p>

</li>
<li><p>When package or class names in errors are very large the middle excess portion will be elided from the message. As of this release error messages will show only up to the first 128 characters and the last 128 characters in a package or class name in error messages. For example</p>

<pre><code>$ perl -le'("Foo" x 1000)-&gt;new()'</code></pre>

<p>will output the following as one line:</p>

<pre><code>Can't locate object method "new" via package "FooFooFooFooFooFooFoo
FooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFoo
FooFooFooFooFooFooFooFooFooFooFooFooFooFo"..."oFooFooFooFooFooFooFoo
FooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFoo
FooFooFooFooFooFooFooFooFooFooFooFooFoo" (perhaps you forgot to load
"FooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFoo
FooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFo"...
"oFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFoo
FooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFoo"?)
at -e line 1.</code></pre>

<p>Notice the <code> "prefix"..."suffix" </code> form of the package name in this case. In previous versions of perl the complete string would have been shown making the error message over 6k long and there was no upper limit on the length of the error message at all. If you accidentally used a 1MB string as a class name then the error message would be over 2MB long. In this perl the upper limit should be around 2k when eliding and escaping are taken into account.</p>

</li>
<li><p>Removed <code>Complex regular subexpression recursion limit (%d) exceeded</code></p>

<p>The regular expresion engine has not used recursion in some time. This warning no longer makes sense.</p>

<p>See [<a href="https://github.com/Perl/perl5/pull/19636">GH #19636</a>].</p>

</li>
<li><p>Various warnings that used to produce parenthesized hints underneath the main warning message and after its "location data" were chanaged to put the hint inline with the main message. For instance:</p>

<pre><code>Bareword found where operator expected at -e line 1, near "foo bar"
    (Do you need to predeclare foo?)</code></pre>

<p>will now look like this but as one line:</p>

<pre><code>Bareword found where operator expected (Do you need to predeclare
foo?) at -e line 1, near "foo bar"</code></pre>

<p>as a result such warnings will no longer trigger <code>$SIG{__WARN__}</code> twice, and the hint will be visible when fatal warnings is in effect.</p>

</li>
<li><p>The error message that is produced when a <code>require</code> or <code>use</code> statement fails has been changed. It used to contain the words <code>@INC contains:</code>, and it used to show the state of <code>@INC</code> *after* the require had completed and failed. The error message has been changed to say <code>@INC entries checked:</code> and to reflect the actual directories or hooks that were executed during the require statement. For example:</p>

<pre><code>perl -e'push @INC, sub {@INC=()}; eval "require Frobnitz"
    or die $@'
Can't locate Frobnitz.pm in @INC (you may need to install the
Frobnitz module) (@INC contains:) at (eval 1) line 1.</code></pre>

<p>Will change to (with some output elided for clarity):</p>

<pre><code>perl -e'push @INC, sub {@INC=()}; eval "require Frobnitz"
    or die $@'
Can't locate Frobnitz.pm in @INC (you may need to install the
Frobnitz module) (@INC entries checked:
.../site_perl/5.38.0/x86_64-linux .../site_perl/5.38.0
.../5.38.0/x86_64-linux .../5.38.0 CODE(0x562745e684b8))
at (eval 1) line 1.</code></pre>

<p>thus showing the actual directories checked. Code that checks for <code>@INC contains:</code> in error messages should be hardened against any future wording changes between the <code>@INC</code> and <code>:</code>, for instance use <code>qr/\@INC[ \w]+:/</code> instead of using <code>qr/\@INC contains:/</code> or <code>qr/\@INC entries checked:/</code> in tests as this will ensure both forward and backward compatibility.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Old-package-separator-used-in-string">Old package separator used in string</a></p>

<p>This diagnostic is now also part of the <code>deprecated</code> category.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#given-is-deprecated">given is deprecated</a> replaces <code>given is experimental</code>.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#when-is-deprecated">when is deprecated</a> replaces <code>when is experimental</code>.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Smartmatch-is-deprecated">Smartmatch is deprecated</a> replaces <code>Smartmatch is experimental</code>.</p>

</li>
</ul>

<h2 id="Configuration-and-Compilation"><a href="#Configuration-and-Compilation">#</a><a id="Configuration"></a>Configuration and Compilation</h2>

<ul>

<li><p><code>make -j6 minitest</code> could fail due to a build conflict in building <code>$(MINIPERL_EXE)</code> between the main make process and a child process. [<a href="https://github.com/Perl/perl5/issues/19829">GH #19829</a>]</p>

</li>
<li><p>Properly populate osvers on Dragonfly BSD when the hostname isn't set.</p>

</li>
<li><p>Fix typos for C99 macro name <code>PRIX64</code>.</p>

</li>
<li><p>Remove ancient and broken GCC for VMS support</p>

</li>
<li><p>Remove vestigial reference to <code>/VAXC</code> qualifier</p>

</li>
<li><p>Remove sharedperl option on VMS</p>

</li>
<li><p>VMS now has mkostemp</p>

</li>
<li><p><code>Configure</code> now properly handles quoted elements outputted by gcc. [<a href="https://github.com/Perl/perl5/issues/20606">GH #20606</a>]</p>

</li>
<li><p><code>Configure</code> probed for the return type of malloc() and free() by testing whether declarations for those functions produced a function type mismatch with the implementation. On Solaris, with a C++ compiler, this check always failed, since Solaris instead imports malloc() and free() from <code>std::</code> with <code>using</code> for C++ builds. Since the return types of malloc() and free() are well defined by the C standard, skip probing for them. <code>Configure</code> command-line arguments and hints can still override these type in the unlikely case that is needed. [<a href="https://github.com/Perl/perl5/issues/20806">GH #20806</a>]</p>

</li>
</ul>

<h2 id="Testing"><a href="#Testing">#</a>Testing</h2>

<p>Tests were added and changed to reflect the other additions and changes in this release. Furthermore, these significant changes were made:</p>

<ul>

<li><p>Unicode normalization tests have been added.</p>

</li>
<li><p>t/test.pl: Add ability to cancel an watchdog timer</p>

</li>
</ul>

<h2 id="Platform-Support"><a href="#Platform-Support">#</a><a id="Platform"></a>Platform Support</h2>

<h2 id="Discontinued-Platforms"><a href="#Discontinued-Platforms">#</a><a id="Discontinued"></a>Discontinued Platforms</h2>

<dl>

<dt id="Ultrix"><a href="#Ultrix">#</a>Ultrix</dt>
<dd>

<p>Support code for DEC Ultrix has been removed. Ultrix was the native Unix-like operating system for various Digital Equipment Corporation machines. Its final release was in 1995.</p>

</dd>
</dl>

<h2 id="Platform-Specific-Notes"><a href="#Platform-Specific-Notes">#</a><a id="Platform1"></a>Platform-Specific Notes</h2>

<dl>

<dt id="DragonflyBSD"><a href="#DragonflyBSD">#</a>DragonflyBSD</dt>
<dd>

<p>Skip tests to workaround an apparent bug in <code>setproctitle()</code>. [<a href="https://github.com/Perl/perl5/issues/19894">GH #19894</a>]</p>

</dd>
<dt id="FreeBSD"><a href="#FreeBSD">#</a>FreeBSD</dt>
<dd>

<p>FreeBSD no longer uses thread-safe locale operations, to avoid <a href="https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=265950">a bug in FreeBSD</a></p>

<p>Replace the first part of archname with <code>uname -p</code> [<a href="https://github.com/Perl/perl5/issues/19791">GH #19791</a>]</p>

</dd>
<dt id="Solaris"><a href="#Solaris">#</a>Solaris</dt>
<dd>

<p>Avoid some compiler and compilation issues on NetBSD/Solaris from regexec.c and regcomp.c.</p>

</dd>
<dt id="Synology"><a href="#Synology">#</a>Synology</dt>
<dd>

<p>Update Synology Readme for DSM 7.</p>

</dd>
<dt id="Windows"><a href="#Windows">#</a>Windows</dt>
<dd>

<p>Fix win32 memory alignment needed for gcc-12 from vmem.h.</p>

<p>utimes() on Win32 would print a message to stderr if it failed to convert a supplied <code>time_t</code> to to a <code>FILETIME</code>. [<a href="https://github.com/Perl/perl5/issues/19668">GH #19668</a>]</p>

<p>In some cases, timestamps returned by <a href="https://perldoc.perl.org/perlfunc#stat">stat()</a> and <a href="https://perldoc.perl.org/perlfunc#lstat">lstat()</a> failed to take daylight saving time into account. [<a href="https://github.com/Perl/perl5/issues/20018">GH #20018</a>] [<a href="https://github.com/Perl/perl5/issues/20061">GH #20061</a>]</p>

<p>stat() now works on <code>AF_UNIX</code> socket files. [<a href="https://github.com/Perl/perl5/issues/20204">GH #20204</a>]</p>

<p>readlink() now returns the <code>PrintName</code> from a symbolic link reparse point instead of the <code>SubstituteName</code>, which should make it better match the name the link was created with. [<a href="https://github.com/Perl/perl5/pull/20271">GH #20271</a>]</p>

<p>lstat() on Windows now returns the length of the link target as the size of the file, as it does on POSIX systems. [<a href="https://github.com/Perl/perl5/issues/20476">GH #20476</a>]</p>

<p>symlink() on Windows now replaces any <code>/</code> in the target with <code>\</code>, since Windows does not recognise <code>/</code> in symbolic links. The reverse translation is <b>not</b> done by readlink(). [<a href="https://github.com/Perl/perl5/issues/20506">GH #20506</a>]</p>

<p>symlink() where the target was an absolute path to a directory was incorrectly created as a file symbolic link. [<a href="https://github.com/Perl/perl5/issues/20533">GH #20533</a>]</p>

<p><code>POSIX::dup2</code> no longer creates broken sockets. [<a href="https://github.com/Perl/perl5/issues/20920">GH #20920</a>]</p>

<p>Closing a busy pipe could cause Perl to hang. [<a href="https://github.com/Perl/perl5/issues/19963">GH #19963</a>]</p>

</dd>
</dl>

<h2 id="Internal-Changes"><a href="#Internal-Changes">#</a><a id="Internal"></a>Internal Changes</h2>

<ul>

<li><p>Removed many deprecated C functions.</p>

<p>These have been deprecated for a long time. See <a href="https://github.com/perl/perl5/commit/7008caa915ad99e650acf2aea40612b5e48b7ba2">https://github.com/perl/perl5/commit/7008caa915ad99e650acf2aea40612b5e48b7ba2</a> for a full list.</p>

</li>
<li><p><code>get_op_descs</code>, <code>get_op_names</code>, <code>get_opargs</code>, <code>get_no_modify</code> and <code>get_ppaddr</code> have been marked deprecated.</p>

</li>
<li><p><code>hv_free_ent</code> has been marked as internal API.</p>

</li>
<li><p><code>save_pushptr</code>, <code>save_pushptrptr</code>, and <code>save_pushi32ptr</code> have been marked as internal API.</p>

</li>
<li><p>New bool related functions and macros have been added to complement the new bool type introduced in 5.36:</p>

<p>The functions are:</p>

<dl>

<dt id="newSVbool(const-bool-bool_val)"><a href="#newSVbool(const-bool-bool_val)">#</a><a id="newSVbool"></a><a id="newSVbool-const-bool-bool_val"></a><a href="https://perldoc.perl.org/perlapi#newSVbool"><code>newSVbool(const bool bool_val)</code></a></dt>
<dd>

</dd>
<dt id="newSV_true()"><a href="#newSV_true()">#</a><a id="newSV_true"></a><a href="https://perldoc.perl.org/perlapi#newSV_true"><code>newSV_true()</code></a></dt>
<dd>

</dd>
<dt id="newSV_false()"><a href="#newSV_false()">#</a><a id="newSV_false"></a><a href="https://perldoc.perl.org/perlapi#newSV_false"><code>newSV_false()</code></a></dt>
<dd>

</dd>
<dt id="sv_set_true(SV-*sv)"><a href="#sv_set_true(SV-*sv)">#</a><a id="sv_set_true"></a><a id="sv_set_true-SV-sv"></a><a href="https://perldoc.perl.org/perlapi#sv_set_true"><code>sv_set_true(SV *sv)</code></a></dt>
<dd>

</dd>
<dt id="sv_set_false(SV-*sv)"><a href="#sv_set_false(SV-*sv)">#</a><a id="sv_set_false"></a><a id="sv_set_false-SV-sv"></a><a href="https://perldoc.perl.org/perlapi#sv_set_false"><code>sv_set_false(SV *sv)</code></a></dt>
<dd>

</dd>
<dt id="sv_set_bool(SV-*sv,-const-bool-bool_val)"><a href="#sv_set_bool(SV-*sv,-const-bool-bool_val)">#</a><a id="sv_set_bool"></a><a id="sv_set_bool-SV-sv-const-bool-bool_val"></a><a href="https://perldoc.perl.org/perlapi#sv_set_bool"><code>sv_set_bool(SV *sv, const bool bool_val)</code></a></dt>
<dd>

</dd>
</dl>

<p>The macros are:</p>

<dl>

<dt id="SvIandPOK(sv)"><a href="#SvIandPOK(sv)">#</a><a id="SvIandPOK"></a><a id="SvIandPOK-sv"></a><a href="https://perldoc.perl.org/perlapi#SvIandPOK"><code>SvIandPOK(sv)</code></a></dt>
<dd>

</dd>
<dt id="SvIandPOK_off(sv)"><a href="#SvIandPOK_off(sv)">#</a><a id="SvIandPOK_off"></a><a id="SvIandPOK_off-sv"></a><a href="https://perldoc.perl.org/perlapi#SvIandPOK_off"><code>SvIandPOK_off(sv)</code></a></dt>
<dd>

</dd>
<dt id="SvIandPOK_on"><a href="#SvIandPOK_on">#</a><a href="https://perldoc.perl.org/perlapi#SvIandPOK_on"><code>SvIandPOK_on</code></a></dt>
<dd>

</dd>
</dl>

</li>
<li><p>Perl is no longer manipulating the <code>environ</code> array directly. The variable <code>PL_use_safe_putenv</code> has been removed and <code>PERL_USE_SAFE_PUTENV</code> is always defined. This means XS modules can now call <code>setenv</code> and <code>putenv</code> without causing segfaults. [<a href="https://github.com/Perl/perl5/issues/19399">perl #19399</a>]</p>

</li>
<li><p>Internal C API functions are now hidden with <code>__attribute__((hidden))</code> on the platforms that support it. This means they are no longer callable from XS modules on those platforms.</p>

<p>It should be noted that those functions have always been hidden on Windows. This change merely brings that to the other platforms. [<a href="https://github.com/Perl/perl5/pull/19655">perl #19655</a>]</p>

</li>
<li><p>New formatting symbols were added for printing values declared as <code>U32</code> or <code>I32</code>:</p>

<dl>

<dt id="I32df-Like-%d"><a href="#I32df-Like-%25d">#</a><a id="I32df"></a><a id="I32df----Like-d"></a>I32df -- Like %d</dt>
<dd>

</dd>
<dt id="U32of-Like-%o"><a href="#U32of-Like-%25o">#</a><a id="U32of"></a><a id="U32of----Like-o"></a>U32of -- Like %o</dt>
<dd>

</dd>
<dt id="U32uf-Like-%u"><a href="#U32uf-Like-%25u">#</a><a id="U32uf"></a><a id="U32uf----Like-u"></a>U32uf -- Like %u</dt>
<dd>

</dd>
<dt id="U32xf-Like-%x"><a href="#U32xf-Like-%25x">#</a><a id="U32xf"></a><a id="U32xf----Like-x"></a>U32xf -- Like %x</dt>
<dd>

</dd>
<dt id="U32Xf-Like-%X"><a href="#U32Xf-Like-%25X">#</a><a id="U32Xf"></a><a id="U32Xf----Like-X"></a>U32Xf -- Like %X</dt>
<dd>

</dd>
</dl>

<p>These are used in the same way already existing similar symbols, such as <code>IVdf</code>, are used. See <a href="https://perldoc.perl.org/perlapi#I%2FO-Formats">"I/O Formats" in perlapi</a>.</p>

</li>
<li><p>new 'HvHasAUX' macro</p>

</li>
<li><p>regexec.c: Add some branch predictions reorder conds</p>

</li>
<li><p>locale: Change macro name to be C conformant</p>

</li>
<li><p>Rename the <code>PADNAMEt_*</code> constants to <code>PADNAMEf_*</code></p>

</li>
<li><p>Changes all the API macros that retrieve a PV into a call to an inline function so as to evaluate the parameter just once.</p>

</li>
<li><p>regexec.c: multiple code refactor to make the code more readable</p>

</li>
<li><p>perl.h: Change macro name to be C conformant (remove leading _ from NOT_IN_NUMERIC macros)</p>

</li>
<li><p>regcomp.h: add new <code>BITMAP_BIT</code> macro in addition to the existing <code>BITMAP_BYTE</code> and <code>BITMAP_TEST</code> ones.</p>

</li>
<li><p>Create new regnode type ANYOFH. populate_ANYOF_from_invlist was renamed to populate_bitmap_from_invlist</p>

</li>
<li><p>regex: Refactor bitmap vs non-bitmap of qr/[]/</p>

</li>
<li><p>regcomp.c: add new functions to convert from an inversion list to a bitmap (and vice versa) <code>populate_bitmap_from_invlist</code> and <code>populate_invlist_from_bitmap</code>.</p>

</li>
<li><p>Add <code>newAVav()</code> to create an AV from an existing AV. Add <code>newAVhv()</code> to create an AV using keys and values from an existing HV.</p>

</li>
<li><p>Fix definition of <code>Perl_atof</code>.</p>

</li>
<li><p>Fix undefined behavior with overflow related <code>OPTIMIZE_INFTY</code> and delta in <i>regcomp.c</i>.</p>

</li>
<li><p>Fix regnode pointer alignment issue in <i>regcomp.h</i>.</p>

</li>
<li><p>The <code>CVf_METHOD</code> CV flag and associated <code>CvMETHOD</code> macro has been renamed to <code>CVf_NOWARN_AMBIGUOUS</code> and <code>CvNOWARN_AMBIGUOUS</code>. This closer reflects its actual behaviour (it suppresses a warning that would otherwise be generated about ambiguous names), in order to be less confusing with <code>CvIsMETHOD</code>, which indicates that a CV is a <code>method</code> subroutine relating to the <code>class</code> feature.</p>

</li>
<li><p>The <code>OPf_SPECIAL</code> flag is no longer set on the <code>OP_ENTERSUB</code> op constructed to call the <code>VERSION</code>, <code>import</code> and <code>unimport</code> methods as part of a <code>use</code> statement and attribute application, nor when assigning to an <code>:lvalue</code> subroutine.</p>

</li>
<li><p>A new CV flag <code>CVf_REFCOUNTED_ANYSV</code> has been added, which indicates that the CV is an XSUB and stores an SV pointer in the <code>CvXSUBANY.any_sv</code> union field. Perl core operations such as cloning or destroying the CV will maintain the reference count of the pointed-to SV, destroying it when required.</p>

</li>
<li><p>A new API function <a href="https://perldoc.perl.org/perlapi#Perl_localeconv">"<code>Perl_localeconv</code>" in perlapi</a> is added. This is the same as <a href="https://perldoc.perl.org/POSIX#localeconv"><code>POSIX::localeconv</code></a> (returning a hash of the <code>localeconv()</code> fields), but directly callable from XS code.</p>

</li>
<li><p>A new API function, <a href="https://perldoc.perl.org/perlapi#Perl_langinfo8">"<code>Perl_langinfo8</code>" in perlapi</a> is added. This is the same as plain <a href="https://perldoc.perl.org/perlapi#Perl_langinfo">"<code>Perl_langinfo</code>" in perlapi</a>, but with an extra parameter that allows the caller to simply and reliably know if the returned string is UTF-8.</p>

</li>
<li><p>We have introduced a limit on the number of nested <code>eval EXPR</code>/<code>BEGIN</code> blocks and <code>require</code>/<code>BEGIN</code> (and thus <code>use</code> statements as well) to prevent C stack overflows. This variable can also be used to forbid <code>BEGIN</code> blocks from executing during <code>eval EXPR</code> compilation. The limit defaults to <code>1000</code> but can be overridden by setting the <code>${^MAX_NESTED_EVAL_BEGIN_BLOCKS}</code> variable. The default itself can be changed at compile time with</p>

<pre><code>-Accflags='-DPERL_MAX_NESTED_EVAL_BEGIN_BLOCKS_DEFAULT=12345'</code></pre>

<p>Note that this value relates to the size of your C stack and if you choose an inappropriately large value Perl may segfault, be conservative about what you choose.</p>

</li>
<li><p>A new magic type <code>PERL_MAGIC_extvalue</code> has been added. This is available for use like <code>PERL_MAGIC_ext</code>, but is a value magic: upon localization the new value will not be magical.</p>

</li>
<li><p>The <code>SSNEW()</code>, <code>SSNEWt()</code>, <code>SSNEWa()</code> and <code>SSNEWat()</code> APIs now return a <code>SSize_t</code> value. The <code>SSPTR()</code> and <code>SSPTRt()</code> macros now expect a <code>SSize_t</code> parameter, and enforce that on debugging builds. [<a href="https://github.com/Perl/perl5/issues/20411">GH #20411</a>]</p>

</li>
<li><p>Filenames in cops are now refcounted under threads. Under threads we were copying the filenames into each opcode. This is because in theory opcodes created in one thread can be destroyed in another. The change adds a new struct/type <code>RCPV</code>, which is a refcounted string using shared memory. This is implemented in such a way that code that previously used a char * can continue to do so, as the refcounting data is located a specific offset before the char * pointer itself.</p>

</li>
<li><p>Added <code>HvNAMEf</code> and <code>HvNAMEf_QUOTEDPREFIX</code> special formats. They take an <code>HV *</code> as an argument and use <code>HvNAME()</code> and related macros to determine the string, its length, and whether it is utf8.</p>

</li>
<li><p>The underlying <code>Perl_dowantarray</code> function implementing the long-deprecated <a href="https://perldoc.perl.org/perlapi#GIMME"><code>GIMME</code></a> macro has been marked as deprecated, so that use of the macro emits a compile-time warning. <code>GIMME</code> has been documented as deprecated in favour of <a href="https://perldoc.perl.org/perlapi#GIMME_V"><code>GIMME_V</code></a> since Perl v5.6.0, but had not previously issued a warning.</p>

</li>
<li><p>The API function <a href="https://perldoc.perl.org/perlapi#utf8_length">"utf8_length" in perlapi</a> is now more efficient.</p>

</li>
<li><p>Added <code>SAVERCPV()</code> and <code>SAVEFREERCPV()</code> for better support for working with <code>RCPV</code> (reference counted string/pointer value) structures which currently are used in opcodes to share filename and warning bit data in a memory efficient manner.</p>

</li>
<li><p>Added <code>MORTALSVFUNC_SV()</code> and <code>MORTALDESTRUCTOR_SV()</code> macros, which make it possible to create a destructor which is fired at the end of the current statement. This uses the <code>PERL_MAGIC_destruct</code> magic to use "free" magic to trigger an action when a variable is freed. The action can be specified as a C function or as a Perl code reference.</p>

</li>
<li><p>Added the <code>%{^HOOK}</code> api and related <code>PERL_MAGIC_hook</code> and <code>PERL_MAGIC_hookelem</code> for providing ways to hook selected perl functions which for one reason or another are problematic to wrap with a customized subroutine.</p>

</li>
<li><p>Added support for <code>${^HOOK}{require__before}</code> which can be used to rewrite the filename that <code>require</code> will try to load, and also to block <code>require</code> from loading a specific module, even via fully qualified filename. The hook can also be used to perform "pre-require" and "post-require" actions.</p>

</li>
<li><p>Added support for <code>${^HOOK}{require__after}</code> which can be used to track what modules have been required after the fact.</p>

</li>
<li><p>Regular expression opcodes (regops) now use a standardized structure layout that uses unions to expose data in different format. This means it should be much easier to extend or modify regops to use more memory. This has been used to make a number of regops track how many parens they contain.</p>

</li>
</ul>

<h2 id="Selected-Bug-Fixes"><a href="#Selected-Bug-Fixes">#</a><a id="Selected"></a>Selected Bug Fixes</h2>

<ul>

<li><p>Avoid recursion and stack overflow parsing 'pack' template</p>

<p>[<a href="https://github.com/Perl/perl5/issues/16319">GH #16319</a>]</p>

</li>
<li><p>An eval() as the last statement in a regex code block could trigger an interpreter panic; e.g.</p>

<pre><code>/(?{ ...; eval {....}; })/</code></pre>

<p>[<a href="https://github.com/Perl/perl5/issues/19680">GH #19680</a>]</p>

</li>
<li><p>Disabling the <code>bareword_filehandles</code> feature no longer treats <code>print Class-&gt;method</code> as an error. [<a href="https://github.com/Perl/perl5/issues/19704">GH #19704</a>]</p>

</li>
<li><p>When a Perl subroutine tail-calls an XS subroutine using <code>goto &amp;xs_sub</code>, the XS subroutine can now correctly determine its calling context. Previously it was always reported as scalar.</p>

<p>In addition, where the Perl subroutine is freed at the same time:</p>

<pre><code>sub foo { *foo = sub {}; goto &amp;xs_sub }</code></pre>

<p>this formerly could lead to crashes if the XS subroutine tried to use the value of <code>PL_op</code>, since this was being set to NULL. This is now fixed.</p>

<p>[<a href="https://github.com/Perl/perl5/issues/19936">GH #19936</a>]</p>

</li>
<li><p>setsockopt() now uses the mechanism added in 5.36 to better distinguish between numeric and string values supplied as the <code>OPTVAL</code> parameter. [<a href="https://github.com/Perl/perl5/issues/18761">GH #18761</a>]</p>

</li>
<li><p>4-argument <code>select()</code> now rejects strings with code points above 255. Additionally, for code points 128-255, this operator will now always give the corresponding octet to the OS, regardless of how Perl stores such code points in memory. (Previously Perl leaked its internal string storage to the OS.) [<a href="https://github.com/Perl/perl5/issues/19882">GH #19882</a>]</p>

</li>
<li><p>Fix panic issue from <code>val {} inside /(?{...})/</code> [<a href="https://github.com/Perl/perl5/issues/19390">GH #19390</a>]</p>

</li>
<li><p>Fix multiple compiler warnings from <i>regexp.c</i>, <i>locale.c</i> [<a href="https://github.com/Perl/perl5/issues/19915">GH #19915</a>]</p>

</li>
<li><p>Fix a bug with querying locales on platforms that don't have <code>LC_NUMERIC</code> [<a href="https://github.com/Perl/perl5/issues/19890">GH #19890</a>]</p>

</li>
<li><p>Prevent undefined behaviour in <code>S_maybe_multideref()</code>.</p>

</li>
<li><p>Avoid signed integer overflow in <code>use integer</code> ops.</p>

</li>
<li><p>Avoid adding an offset to a NULL pointer in <code>hv_delete_common</code>.</p>

</li>
<li><p>PerlIO::get_layers will now accept IO references too</p>

<p>Previously it would only take glob references or names of globs. Now it will also accept IO references.</p>

</li>
<li><p>Fixes to memory handling for <code>PL_splitstr</code>:</p>

<ul>

<li><p>If a thread was created the allocated string would be freed twice.</p>

</li>
<li><p>If two <code>-F</code> switches were supplied the memory allocated for the first switch wouldn't be freed.</p>

</li>
</ul>

</li>
<li><p>Correctly handle <code>OP_ANONCODE</code> ops generated by CPAN modules that don't include the OPf_REF flag when propagating lvalue context. [<a href="https://github.com/Perl/perl5/pull/20532">GH #20532</a>]</p>

</li>
<li><p><a href="https://perldoc.perl.org/POSIX#strxfrm">POSIX::strxfrm</a> now uses the <code>LC_CTYPE</code> locale category to specify its collation, ignoring any differing <code>LC_COLLATE</code>. It doesn't make sense for a string to be encoded in one locale (say, ISO-8859-6, Arabic) and to collate it based on another (like ISO-8859-7, Greek). Perl assumes that the current <code>LC_CTYPE</code> locale correctly represents the encoding, and collates accordingly.</p>

<p>Also, embedded <code>NUL</code> characters are now allowed in the input.</p>

<p>If locale collation is not enabled on the platform (<code>LC_COLLATE</code>), the input is returned unchanged.</p>

</li>
<li><p>Double FETCH during stringification of tied scalars returning an overloaded object have been fixed. The FETCH method should only be called once, but prior to this release was actually called twice. [<a href="https://github.com/Perl/perl5/pull/20574">GH #20574</a>]</p>

</li>
<li><p>Writing to a magic variables associated with the selected output handle, <code>$^</code>, <code>$~</code>, <code>$=</code>, <code>$-</code> and <code>$%</code>, no longer crashes perl if the IO object has been cleared from the selected output handle. [<a href="https://github.com/Perl/perl5/issues/20733">GH #20733</a>]</p>

</li>
<li><p>Redefining a <code>use constant</code> list constant with <code>use constant</code> now properly warns. This changes the behaviour of <code>use constant</code> but is a core change, not a change to <i>constant.pm</i>. [<a href="https://github.com/Perl/perl5/issues/20742">GH #20742</a>]</p>

</li>
<li><p>Redefining a <code>use constant</code> list constant with an empty prototype constant sub would result in an assertion failure. [<a href="https://github.com/Perl/perl5/issues/20742">GH #20742</a>]</p>

</li>
<li><p>Fixed a regression where the <code>INC</code> method for objects in <code>@INC</code> would not be resolved by <code>AUTOLOAD</code>, while it was in 5.36. The <code>INCDIR</code> method for objects in <code>@INC</code> cannot be resolved by <code>AUTOLOAD</code> as <code>INC</code> would have been resolved first. [<a href="https://github.com/Perl/perl5/issues/20665">GH #20665</a>]</p>

</li>
<li><p><code>$SIG{__DIE__}</code> will now be called from eval when the code dies during compilation regardless of how it dies. This means that code expecting to be able to upgrade <code>$@</code> into an object will be called consistently. In earlier versions of perl <code>$SIG{__DIE__}</code> would not be called for certain compilation errors, for instance undeclared variables. For other errors it might be called if there were more than a certain number of errors, but not if there were less. Now you can expect that it will be called in every case.</p>

</li>
<li><p>Compilation of code with errors used to inconsistently stop depending on the count and type of errors encountered. The intent was that after 10 errors compilation would halt, but bugs in this logic meant that certain types of error would be counted, but would not trigger the threshold check to stop compilation. Other errors would. With this release after at most 10 errors compilation will terminate, regardless of what type of error they were.</p>

<p>Note that you can change the maximum count by defining <code>PERL_STOP_PARSING_AFTER_N_ERRORS</code> to be something else during the configuration process. For instance</p>

<pre><code>./Configure ... -Accflags='-DPERL_STOP_PARSING_AFTER_N_ERRORS=100'</code></pre>

<p>would allow up to 100 errors.</p>

</li>
<li><p>The API function <a href="https://perldoc.perl.org/perlapi#my_snprintf">"my_snprintf" in perlapi</a> now prints a non-dot decimal point if the perl code it ultimately is called from is in the scope of <code>use locale</code> and the locale in effect calls for that.</p>

</li>
<li><p>A number of bugs related to capture groups in quantified groups in regular expression have been fixed, especially in alternations. For example in a pattern like:</p>

<pre><code>"foobazfoobar" =~ /((foo)baz|foo(bar))+/</code></pre>

<p>the regex variable <code>$2</code> will not be "foo" as it once was, it will be undef.</p>

</li>
<li><p>Bugs with regex backreference operators that are inside of a capture group have been fixed. For instance:</p>

<pre><code>"xa=xaaa" =~ /^(xa|=?\1a){2}\z/</code></pre>

<p>will now correctly not match. [<a href="https://github.com/Perl/perl5/issues/10073">GH #10073</a>]</p>

</li>
<li><p><code>SSGROW()</code> and <code>SSCHECK()</code> have been reworked to ensure that the requested space is actually allocated. <code>SSCHECK()</code> is now an alias for <code>SSGROW()</code>.</p>

</li>
</ul>

<h2 id="Acknowledgements"><a href="#Acknowledgements">#</a>Acknowledgements</h2>

<p>Perl 5.38.0 represents approximately 12 months of development since Perl 5.36.0 and contains approximately 290,000 lines of changes across 1,500 files from 100 authors.</p>

<p>Excluding auto-generated files, documentation and release tools, there were approximately 190,000 lines of changes to 970 .pm, .t, .c and .h files.</p>

<p>Perl continues to flourish into its fourth decade thanks to a vibrant community of users and developers. The following people are known to have contributed the improvements that became Perl 5.38.0:</p>

<p>Alex, Alexander Nikolov, Alex Davies, Andreas König, Andrew Fresh, Andrew Ruthven, Andy Lester, Aristotle Pagaltzis, Arne Johannessen, A. Sinan Unur, Bartosz Jarzyna, Bart Van Assche, Benjamin Smith, Bram, Branislav Zahradník, Brian Greenfield, Bruce Gray, Chad Granum, Chris 'BinGOs' Williams, chromatic, Clemens Wasser, Craig A. Berry, Dagfinn Ilmari Mannsåker, Dan Book, danielnachun, Dan Jacobson, Dan Kogai, David Cantrell, David Golden, David Mitchell, E. Choroba, Ed J, Ed Sabol, Elvin Aslanov, Eric Herman, Felipe Gasper, Ferenc Erki, Firas Khalil Khana, Florian Weimer, Graham Knop, Håkon Hægland, Harald Jörg, H.Merijn Brand, Hugo van der Sanden, James E Keenan, James Raspass, jkahrman, Joe McMahon, Johan Vromans, Jonathan Stowe, Jon Gentle, Karen Etheridge, Karl Williamson, Kenichi Ishigaki, Kenneth Ölwing, Kurt Fitzner, Leon Timmermans, Li Linjie, Loren Merritt, Lukas Mai, Marcel Telka, Mark Jason Dominus, Mark Shelor, Matthew Horsfall, Matthew O. Persico, Mattia Barbon, Max Maischein, Mohammad S Anwar, Nathan Mills, Neil Bowers, Nicholas Clark, Nicolas Mendoza, Nicolas R, Paul Evans, Paul Marquess, Peter John Acklam, Peter Levine, Philippe Bruhat (BooK), Reini Urban, Renee Baecker, Ricardo Signes, Richard Leach, Russ Allbery, Scott Baker, Sevan Janiyan, Sidney Markowitz, Sisyphus, Steve Hay, TAKAI Kousuke, Todd Rinaldo, Tomasz Konojacki, Tom Stellard, Tony Cook, Tsuyoshi Watanabe, Unicode Consortium, vsfos, Yves Orton, Zakariyya Mughal, Zefram, 小鸡.</p>

<p>The list above is almost certainly incomplete as it is automatically generated from version control history. In particular, it does not include the names of the (very much appreciated) contributors who reported issues to the Perl bug tracker.</p>

<p>Many of the changes included in this version originated in the CPAN modules included in Perl's core. We're grateful to the entire CPAN community for helping Perl to flourish.</p>

<p>For a more complete list of all of Perl's historical contributors, please see the <i>AUTHORS</i> file in the Perl source distribution.</p>

<h2 id="Reporting-Bugs"><a href="#Reporting-Bugs">#</a><a id="Reporting"></a>Reporting Bugs</h2>

<p>If you find what you think is a bug, you might check the perl bug database at <a href="https://github.com/Perl/perl5/issues">https://github.com/Perl/perl5/issues</a>. There may also be information at <a href="http://www.perl.org/">http://www.perl.org/</a>, the Perl Home Page.</p>

<p>If you believe you have an unreported bug, please open an issue at <a href="https://github.com/Perl/perl5/issues">https://github.com/Perl/perl5/issues</a>. Be sure to trim your bug down to a tiny but sufficient test case.</p>

<p>If the bug you are reporting has security implications which make it inappropriate to send to a public issue tracker, then see <a href="https://perldoc.perl.org/perlsec#SECURITY-VULNERABILITY-CONTACT-INFORMATION">"SECURITY VULNERABILITY CONTACT INFORMATION" in perlsec</a> for details of how to report the issue.</p>

<h2 id="Give-Thanks"><a href="#Give-Thanks">#</a><a id="Give"></a>Give Thanks</h2>

<p>If you wish to thank the Perl 5 Porters for the work we had done in Perl 5, you can do so by running the <code>perlthanks</code> program:</p>

<pre><code>perlthanks</code></pre>

<p>This will send an email to the Perl 5 Porters list with your show of thanks.</p>

<h2 id="SEE-ALSO"><a href="#SEE-ALSO">#</a><a id="SEE"></a>SEE ALSO</h2>

<p>The <i>Changes</i> file for an explanation of how to view exhaustive details on what changed.</p>

<p>The <i>INSTALL</i> file for how to build Perl.</p>

<p>The <i>README</i> file for general stuff.</p>

<p>The <i>Artistic</i> and <i>Copying</i> files for copyright information.</p>


      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia’s H100: Funny L2, and Tons of Bandwidth (114 pts)]]></title>
            <link>https://chipsandcheese.com/2023/07/02/nvidias-h100-funny-l2-and-tons-of-bandwidth/</link>
            <guid>36569044</guid>
            <pubDate>Mon, 03 Jul 2023 04:55:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/2023/07/02/nvidias-h100-funny-l2-and-tons-of-bandwidth/">https://chipsandcheese.com/2023/07/02/nvidias-h100-funny-l2-and-tons-of-bandwidth/</a>, See on <a href="https://news.ycombinator.com/item?id=36569044">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>GPUs started out as devices meant purely for graphics rendering, but their highly parallel nature made them attractive for certain compute tasks too. As the GPU compute scene grew over the past couple decades, Nvidia made massive investments to capture the compute market. Part of this involved recognizing that compute tasks have different needs than graphics tasks, and diverging their GPU lineup to better target each market.</p>
<p>H100 is the latest member of Nvidia’s line of compute-oriented GPUs. It uses the Hopper architecture, and is built on a massive 814 mm<sup>2</sup> die using TSMC’s 4N process with 80 billion transistors. This giant die implements 144 Streaming Multiprocessors (SMs), 60 MB of L2 cache, and 12 512-bit HBM memory controllers. We’re testing H100’s PCIe version on Lambda Cloud, which enables 114 of those SMs, 50 MB of L2 cache, and 10 HBM2 memory controllers. The card can draw up to 350 W.</p>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sxm5.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18547" data-permalink="https://chipsandcheese.com/h100_sxm5/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sxm5.png?fit=599%2C323&amp;ssl=1" data-orig-size="599,323" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_sxm5" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sxm5.png?fit=599%2C323&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sxm5.png?fit=599%2C323&amp;ssl=1" decoding="async" width="599" height="323" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sxm5.png?resize=599%2C323&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sxm5.png?resize=599%2C323&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>Nvidia’s rendering of H100 in the SXM form factor, from the H100 whitepaper</figcaption></figure>
<p>Nvidia also offers a SXM form factor H100, which can draw up to 700W and has 132 SMs enabled. The SXM H100 also uses HBM3 memory, providing additional bandwidth to feed those extra shaders.</p>
<figure><table><tbody><tr><td></td><td>H100</td><td>A100</td></tr><tr><td>Reported Device Name</td><td>H100 PCIe</td><td>A100-SXM4-40GB</td></tr><tr><td>CPU</td><td>Intel Xeon Platinum 8480+</td><td>AMD EPYC 7J13</td></tr><tr><td>Max Core Boost Clock</td><td>1755 MHz</td><td>1410 MHz</td></tr><tr><td>Idle Core Clock</td><td>345 MHz</td><td>210 MHz</td></tr><tr><td>Memory Clock (Does not change under load)</td><td>1593 MHz</td><td>1215 MHz</td></tr></tbody></table><figcaption>I also did some limited testing on an A100 instance for comparison purposes</figcaption></figure>
<h2>Brief Note on Clock Speeds</h2>
<p>The H100 features a much higher boost clock than the A100. When microbenchmarking, the H100 occasionally dropped down to as low as 1395 MHz, or just under 80% of its maximum boost clock. Other metrics from nvidia-smi suggest we could be hitting a power limit, particularly when pulling data from L2. The H100 PCIe version has a power limit of 350W, and gets right up against that when bandwidth testing.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_clk-1.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18669" data-permalink="https://chipsandcheese.com/2023/07/02/nvidias-h100-funny-l2-and-tons-of-bandwidth/h100_clk-1/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_clk-1.png?fit=1023%2C736&amp;ssl=1" data-orig-size="1023,736" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_clk-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_clk-1.png?fit=1023%2C736&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_clk-1.png?fit=688%2C495&amp;ssl=1" decoding="async" loading="lazy" width="688" height="495" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_clk-1.png?resize=688%2C495&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_clk-1.png?w=1023&amp;ssl=1 1023w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_clk-1.png?resize=768%2C553&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_clk-1.png?w=1023&amp;ssl=1 1023w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_clk-1.png?resize=768%2C553&amp;ssl=1 768w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_clk-1.png?resize=688%2C495&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>Server cooling is able to keep the H100 at a very low temperature, even when the GPU is sucking down over 300W. Memory temperatures are a bit higher, but still well within reason.</p>
<p>A100 saw different behavior. Core clocks went to 1410 MHz under load and stays there. Power draw is also quite high, but the SXM4 version of A100 has a <a href="https://www.servethehome.com/new-nvidia-a100-pcie-add-in-card-launched/nvidia-a100-specs-sxm-and-pcie/">higher 400W power limit</a>. Probably because of that, we don’t see any clock speed drops even as power draw passes 350W.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18670" data-permalink="https://chipsandcheese.com/2023/07/02/nvidias-h100-funny-l2-and-tons-of-bandwidth/a100_clk-1/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?fit=927%2C701&amp;ssl=1" data-orig-size="927,701" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="a100_clk-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?fit=927%2C701&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?fit=688%2C520&amp;ssl=1" decoding="async" loading="lazy" width="688" height="520" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?resize=688%2C520&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?w=927&amp;ssl=1 927w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?resize=768%2C581&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?resize=200%2C150&amp;ssl=1 200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?w=927&amp;ssl=1 927w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?resize=768%2C581&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?resize=200%2C150&amp;ssl=1 200w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?resize=688%2C520&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>Like H100, A100 enjoys very low core temperatures. Passively cooled cards seem to thrive in a server chassis with plenty of airflow. A100’s memory temperatures are also lower than H100’s.</p>
<h2>Cache and Memory Setup</h2>
<p>Computers have been limited by memory speed for just about all of eternity. We’ve seen consumer GPUs counter this with increasingly sophisticated cache setups. AMD’s RX 6900 XT used a four level cache hierarchy with 128 MB of last level caching capacity, while Nvidia’s RTX 4090 extended L2 capacity to 72 MB. Nvidia’s compute GPUs have seen increased caching capacities too, but the strategy there is a bit different.</p>
<p>Streaming Multiprocessors (SMs) are Nvidia’s basic GPU building block. Nvidia has consistently emphasized SM-private caching in prior datacenter oriented GPUs. For most Nvidia architectures, a SM has a private chunk of memory that can be flexibly partitioned between L1 cache and Shared Memory (a software managed scratchpad) usage. GK210 Kepler SMs had 128 KB of memory for that compared to 64 KB on client implementations. A100 had 192 KB compared to 128 KB on client Ampere. Now, H100 brings L1/Shared Memory capacity to 256 KB.</p>
<p>We can do limited testing of L1 cache allocations by using Nvidia’s proprietary API. We usually test with OpenCL or Vulkan because many vendors support those APIs, letting tests run unmodified across a large variety of GPUs. But CUDA gives limited control over L1 and Shared Memory splits. Specifically, we can ask the GPU to prefer L1 caching capacity, prefer an equal split, or prefer Shared Memory capacity. Asking for larger L1 cache allocations doesn’t come with any latency penalty.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18442" data-permalink="https://chipsandcheese.com/h100_l1_alloc/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?fit=1372%2C601&amp;ssl=1" data-orig-size="1372,601" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_l1_alloc" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?fit=1372%2C601&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?fit=688%2C301&amp;ssl=1" decoding="async" loading="lazy" width="688" height="301" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?resize=688%2C301&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?w=1372&amp;ssl=1 1372w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?resize=768%2C336&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?resize=1200%2C526&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?resize=1320%2C578&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?w=1372&amp;ssl=1 1372w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?resize=768%2C336&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?resize=1200%2C526&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?resize=1320%2C578&amp;ssl=1 1320w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?resize=688%2C301&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>Testing memory access latency using CUDA, which lets us specify a preferred L1/Shared Memory split. </figcaption></figure></div>
<p>When we ask CUDA to prefer L1 caching capacity, we see 208 KB of L1 cache. With this setting, H100 has more first level data caching capacity than any other GPU. Even if we account for AMD’s strategy of using separate memories for caching and scratchpad purposes, H100 continues to be ahead. Adding up RDNA 3’s L0 vector cache, scalar cache, and LDS (scratchpad) capacity only gives 208 KB of storage, compared to 256 KB on Hopper.</p>
<p>Against A100, H100’s L1 is both higher capacity and lower latency. It’s a welcome improvement, and the trend of being slightly better than A100 continues further down the cache hierarchy.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18443" data-permalink="https://chipsandcheese.com/h100_latency_vs_a100/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?fit=1372%2C601&amp;ssl=1" data-orig-size="1372,601" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_latency_vs_a100" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?fit=1372%2C601&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?fit=688%2C301&amp;ssl=1" decoding="async" loading="lazy" width="688" height="301" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?resize=688%2C301&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?w=1372&amp;ssl=1 1372w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?resize=768%2C336&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?resize=1200%2C526&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?resize=1320%2C578&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?w=1372&amp;ssl=1 1372w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?resize=768%2C336&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?resize=1200%2C526&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?resize=1320%2C578&amp;ssl=1 1320w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?resize=688%2C301&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>For data that can’t fit within L1, H100 has a 50 MB L2. When A100 launched in 2020, its 40 MB L2 gave it more last level caching capacity than any Nvidia GPU at the time. H100 slightly increases cache capacity, but it’s nothing special today. Nvidia’s RTX 4090 features 72 MB of L2, while AMD’s high end RDNA 2 and RDNA 3 GPUs have 128 MB of 96 MB of last level cache respectively.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18454" data-permalink="https://chipsandcheese.com/gh100_block_diagram/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?fit=1247%2C631&amp;ssl=1" data-orig-size="1247,631" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh100_block_diagram" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?fit=1247%2C631&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?fit=688%2C348&amp;ssl=1" decoding="async" loading="lazy" width="688" height="348" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?resize=688%2C348&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?w=1247&amp;ssl=1 1247w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?resize=768%2C389&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?resize=1200%2C607&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?w=1247&amp;ssl=1 1247w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?resize=768%2C389&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?resize=1200%2C607&amp;ssl=1 1200w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?resize=688%2C348&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>H100 block diagram from Nvidia’s whitepaper, showing two L2 partitions and a link between them</figcaption></figure></div>
<p>H100 also inherits A100’s split L2 configuration. Any thread running on the GPU can access the full 50 MB of cache, but not at the same speed. Accessing the “far” partition takes nearly twice as long. It has about as much latency as VRAM on the RX 6900 XT, making it more useful for bandwidth than for getting individual warps or wavefronts to finish faster.</p>
<div>
<figure><img data-lazy-fallback="1" data-attachment-id="18464" data-permalink="https://chipsandcheese.com/h100_latency_vs_rdna2/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?fit=1372%2C601&amp;ssl=1" data-orig-size="1372,601" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_latency_vs_rdna2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?fit=1372%2C601&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?fit=688%2C301&amp;ssl=1" decoding="async" loading="lazy" width="688" height="301" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?resize=688%2C301&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?w=1372&amp;ssl=1 1372w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?resize=768%2C336&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?resize=1200%2C526&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?resize=1320%2C578&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?w=1372&amp;ssl=1 1372w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?resize=768%2C336&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?resize=1200%2C526&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?resize=1320%2C578&amp;ssl=1 1320w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?resize=688%2C301&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption>Using OpenCL to compare to AMD’s client RDNA 2 architecture</figcaption></figure></div>
<p>H100’s L2 cache feels like a two-level setup rather than a single level of cache. A thread running on H100 can access the “far” L2 cache a bit faster than on A100, so Nvidia has improved compared to the prior generation. A100 was a bit of a pioneer for Nvidia when it came to implementing large caches, and H100’s setup is a natural evolution of A100’s. But this isn’t the low latency, efficient cache setup used on modern client GPUs.</p>
<p>Out in VRAM, H100 sees slightly lower latency than A100, and is comparable to some older client GPUs. For example, the GTX 980 Ti has about 354 ns of VRAM latency.</p>
<h2>No More Constant Cache?</h2>
<p>Nvidia has long used a separate constant cache hierarchy, typically with a 2 KB constant cache backed by a 32 to 64 KB mid-level constant cache. The constant cache offers very low latency access, but is read-only and backed by a limited memory space. H100 handles constant memory differently. Nvidia can allocate up to 64 KB of constant memory (a limitation dating back to the Tesla architecture), and latency is constant throughout that range. </p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_constant.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18633" data-permalink="https://chipsandcheese.com/h100_constant/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_constant.png?fit=740%2C354&amp;ssl=1" data-orig-size="740,354" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_constant" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_constant.png?fit=740%2C354&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_constant.png?fit=688%2C329&amp;ssl=1" decoding="async" loading="lazy" width="688" height="329" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_constant.png?resize=688%2C329&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_constant.png?resize=688%2C329&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>Furthermore, latency looks nearly identical to L1 cache latency. H100 might be using the L1 data cache to hold constant data. Validating this assumption would require additional testing that I can’t currently make the time investment for, due to real life and day job demands. But whatever Nvidia did, it provides a clear improvement over A100’s constant caching, with lower latency across the board. Ada Lovelace enjoys lower latency if it can serve requests from a tiny and fast 2 KB constant cache, but also falls behind if there’s a lot of constant data.</p>
<h2>Local Memory Latency</h2>
<p>As mentioned before, H100’s SMs have a large block of private storage that can be split between L1 caching and Shared Memory use. Shared Memory is Nvidia’s term for a software managed scratchpad that offers consistently high performance. AMD’s equivalent is called the Local Data Share (LDS). On Intel GPUs, it’s called Shared Local Memory (SLM). OpenCL refers to this memory type as local memory. </p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_lds.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18566" data-permalink="https://chipsandcheese.com/h100_lds/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_lds.png?fit=480%2C288&amp;ssl=1" data-orig-size="480,288" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_lds" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_lds.png?fit=480%2C288&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_lds.png?fit=480%2C288&amp;ssl=1" decoding="async" loading="lazy" width="480" height="288" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_lds.png?resize=480%2C288&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_lds.png?resize=480%2C288&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>Even though H100 allocates it out of the same block of storage, Shared Memory is faster than L1 cache access because it doesn’t require tag comparisons and status checks to make sure there’s a hit. H100 does well compared to a wide range of GPUs, even though it can allocate more Shared Memory capacity than any other current GPU.</p>
<h2>Atomics</h2>
<p>Shared Memory (or local memory) is also useful for synchronizing threads within the same workgroup. Here, we’re testing OpenCL’s atomic_cmpxchg function, which does compare and exchange operations with a guarantee that nothing else appears to touch the memory its working with between those operations.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_localatomic.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18581" data-permalink="https://chipsandcheese.com/h100_localatomic/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_localatomic.png?fit=480%2C288&amp;ssl=1" data-orig-size="480,288" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_localatomic" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_localatomic.png?fit=480%2C288&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_localatomic.png?fit=480%2C288&amp;ssl=1" decoding="async" loading="lazy" width="480" height="288" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_localatomic.png?resize=480%2C288&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_localatomic.png?resize=480%2C288&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>H100 does reasonably well with this atomic operation, though it’s a bit behind what consumer GPUs can do. Surprisingly, that applies to older GPUs that run at lower clocks as well, like the GTX 980 Ti. H100 however does do better than the A100.</p>
<p>If we perform the same operation on global memory (i.e. memory backed by VRAM), latency is far worse. It’s slightly higher than L2 latency, so perhaps H100 is handling cross-SM synchronization at the L2 cache.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_globalatomic.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18583" data-permalink="https://chipsandcheese.com/h100_globalatomic/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_globalatomic.png?fit=480%2C288&amp;ssl=1" data-orig-size="480,288" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_globalatomic" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_globalatomic.png?fit=480%2C288&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_globalatomic.png?fit=480%2C288&amp;ssl=1" decoding="async" loading="lazy" width="480" height="288" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_globalatomic.png?resize=480%2C288&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_globalatomic.png?resize=480%2C288&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>Again, H100 slightly improves over A100, and falls short compared to consumer GPUs. But this time, the gap is much larger. The RX 6900 XT leaves both the H100 and A100 far behind. The old GTX 980 Ti also performs quite a bit better. I guess synchronizing things across a massive 814 mm<sup>2</sup> or 826 mm<sup>2</sup> die is quite challenging.</p>
<h3>Distributed Shared Memory</h3>
<p>To mitigate the cost of transferring data across the gigantic die, H100 has a feature called Distributed Shared Memory (DSMEM). Using this feature, applications can keep data within a GPC, or a cluster of SMs. This should allow for lower latency data sharing than the global atomics mentioned above, while being able to share data across more threads than would fit in a workgroup.</p>
<p>Testing this feature would involve paying $2 per hour for a H100 instance while learning a new API, and then testing with no other GPU to sanity check results against. Writing, debugging, and validating a test usually takes many hours even under favorable conditions. Nvidia claims DSMEM is <a href="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/">typically 7x faster </a>than exchanging data through global memory.</p>

<p>Latency is only part of the picture. GPUs like the H100 are designed for extremely parallel compute workloads, and probably don’t have to deal with cases where available parallelism is low. That contrasts with consumer GPUs, which occasionally face less parallel tasks like geometry processing or small draw calls. So, H100 emphasizes massive bandwidth. Starting at the L2 cache, we see over 5.5 TB/s of read bandwidth. We measured about 5.7 TB/s of read bandwidth from the RX 7900 XTX’s L2, so H100 gets almost the same amount of bandwidth with much higher caching capacity.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l2_bw.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18444" data-permalink="https://chipsandcheese.com/h100_l2_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l2_bw.png?fit=1026%2C420&amp;ssl=1" data-orig-size="1026,420" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_l2_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l2_bw.png?fit=1026%2C420&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l2_bw.png?fit=688%2C282&amp;ssl=1" decoding="async" loading="lazy" width="688" height="282" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l2_bw.png?resize=688%2C282&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l2_bw.png?w=1026&amp;ssl=1 1026w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l2_bw.png?resize=768%2C314&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l2_bw.png?w=1026&amp;ssl=1 1026w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l2_bw.png?resize=768%2C314&amp;ssl=1 768w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l2_bw.png?resize=688%2C282&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>Tested using OpenCL</figcaption></figure></div>
<p>Compared to A100, H100 enjoys a small but noticeable bandwidth boost. But that only applies to the “near” L2 partition. As noted before, A100 and H100’s L2 isn’t really a single level cache. Bandwidth is significantly worse if we exceed “near” L2 capacity. H100 also regresses compared to A100 when accessing the entire 50 MB L2, with 3.8 TB/s compared to the A100’s 4.5 TB/s. Nvidia may have determined that few workloads were L2 bandwidth bound on A100, so dropping a bit of cross-partition L2 bandwidth wasn’t a big deal.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_big_l2_bw.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18445" data-permalink="https://chipsandcheese.com/h100_big_l2_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_big_l2_bw.png?fit=1028%2C420&amp;ssl=1" data-orig-size="1028,420" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_big_l2_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_big_l2_bw.png?fit=1028%2C420&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_big_l2_bw.png?fit=688%2C281&amp;ssl=1" decoding="async" loading="lazy" width="688" height="281" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_big_l2_bw.png?resize=688%2C281&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_big_l2_bw.png?w=1028&amp;ssl=1 1028w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_big_l2_bw.png?resize=768%2C314&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_big_l2_bw.png?w=1028&amp;ssl=1 1028w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_big_l2_bw.png?resize=768%2C314&amp;ssl=1 768w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_big_l2_bw.png?resize=688%2C281&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>In an absolute sense, H100’s 50 MB L2 still offers a ton of bandwidth even when requests have to go across the cache’s partitions. For comparison, RDNA 2’s Infinity Cache offers around 2 TB/s of bandwidth, while RDNA 3’s Infinity Cache stops just short of 3 TB/s. H100 therefore offers a bit less caching capacity than the Infinity Cache on AMD’s top end client GPUs, but makes up for it with more bandwidth.</p>
<p>However, I feel like Nvidia could bring some of their client side engineering into their compute oriented GPUs. Their RTX 4090 offers about 5 TB/s of L2 bandwidth, and has a lot more L2 caching capacity. On the bright side, H100’s L2 offers much higher bandwidth than VRAM, even when requests have to cross partitions. That’s a compliment, because H100 has a ridiculous amount of VRAM bandwidth.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vram.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18446" data-permalink="https://chipsandcheese.com/h100_vram/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vram.png?fit=1024%2C420&amp;ssl=1" data-orig-size="1024,420" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_vram" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vram.png?fit=1024%2C420&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vram.png?fit=688%2C282&amp;ssl=1" decoding="async" loading="lazy" width="688" height="282" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vram.png?resize=688%2C282&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vram.png?w=1024&amp;ssl=1 1024w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vram.png?resize=768%2C315&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vram.png?w=1024&amp;ssl=1 1024w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vram.png?resize=768%2C315&amp;ssl=1 768w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vram.png?resize=688%2C282&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>With six stacks of HBM2e, the H100 can pull just short of 2 TB/s from VRAM. Thus, the H100’s VRAM bandwidth is actually quite close to RDNA 2’s Infinity Cache bandwidth. It also represents a significant improvement over A100. A100 used HBM2 and still has more VRAM bandwidth than any consumer GPU, but its lower memory clocks let H100 pull ahead.</p>
<p>H100’s VRAM bandwidth will be very useful for massive working sets without cache-friendly access patterns. Consumer GPUs have trended towards good caching instead of massive VRAM setups. The few consumer GPUs that did use HBM have turned in a mediocre performance compared to ones with a modest GDDR setup but excellent caching. That’s because caches lower latency, making it easier to keep the execution units fed even with small workloads. From how Nvidia and AMD have been building compute GPUs, it looks like compute workloads favor the opposite. A100 was already tuned for large workloads. H100 takes that further, with a lead over A100 if you can fill more than half the GPU, but falls a bit behind if you don’t.</p>
<h2>Compute Throughput</h2>
<p>A100’s SMs offered higher theoretical occupancy and FP64 performance than client Ampere, but only had half the FP32 throughput. H100 remediates that by giving each SM sub partition (SMSP) 32 FP32 units, letting it execute one warp instruction per clock.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18495" data-permalink="https://chipsandcheese.com/h100_vs_a100_sm/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?fit=1251%2C829&amp;ssl=1" data-orig-size="1251,829" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_vs_a100_sm" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?fit=1251%2C829&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?fit=688%2C456&amp;ssl=1" decoding="async" loading="lazy" width="688" height="456" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?resize=688%2C456&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?w=1251&amp;ssl=1 1251w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?resize=768%2C509&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?resize=1200%2C795&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?w=1251&amp;ssl=1 1251w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?resize=768%2C509&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?resize=1200%2C795&amp;ssl=1 1200w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?resize=688%2C456&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>Figures from the A100 and H100’s respective whitepapers</figcaption></figure></div>
<p>Alongside FP32 performance, FP64 performance doubles too. Each H100 SMSP can execute a FP64 warp instruction every two cycles, compared to once every four cycles on A100. That makes H100 a much better performer than A100 for scientific applications that need increased precision.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sm_throughput.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18499" data-permalink="https://chipsandcheese.com/h100_sm_throughput/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sm_throughput.png?fit=743%2C684&amp;ssl=1" data-orig-size="743,684" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_sm_throughput" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sm_throughput.png?fit=743%2C684&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sm_throughput.png?fit=688%2C633&amp;ssl=1" decoding="async" loading="lazy" width="688" height="633" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sm_throughput.png?resize=688%2C633&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sm_throughput.png?resize=688%2C633&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>INT32 addition throughput on A100 is definitely a measuring error. And sadly, Nvidia does not support OpenCL’s FP16 extension, so FP16 throughput couldn’t be tested</figcaption></figure></div>
<p>At the same time, H100 inherits Nvidia’s strength in integer multiplication. Specifically, INT32 multiplies execute at half rate, compared to quarter rate on AMD GPUs. On the other hand, AMD GPUs can execute 16-bit integer operations at double rate, while Nvidia GPUs can’t.</p>
<p>At the GPU level, H100 features a small SM count increase and a substantial increase in clock speeds. The result is a significant increase in compute throughput across the board. Thanks to SM-level changes, H100’s FP32 and FP64 throughput blows A100 out of the water. </p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_gpu_throughput.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18507" data-permalink="https://chipsandcheese.com/h100_gpu_throughput/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_gpu_throughput.png?fit=743%2C765&amp;ssl=1" data-orig-size="743,765" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_gpu_throughput" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_gpu_throughput.png?fit=743%2C765&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_gpu_throughput.png?fit=688%2C708&amp;ssl=1" decoding="async" loading="lazy" width="688" height="708" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_gpu_throughput.png?resize=688%2C708&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_gpu_throughput.png?resize=688%2C708&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>H100’s improvements should bring performance benefits across a very wide variety of applications, because it’s hard to think of any GPGPU program that doesn’t use FP32 or FP64. Doubling throughput for those operations along with a SM count and clock speed increase is going to make workloads finish faster.</p>
<p>Going beyond vector compute performance, H100 doubles tensor core throughput. Tensor cores specialize in doing matrix multiplication by breaking the SIMT model, and storing matrices across a warp’s registers. I don’t have a test written for tensor cores and writing one in the near future is beyond what I have time for with a free time hobby project. But, I trust Nvidia’s whitepaper on this topic.</p>
<h2>Final Words</h2>
<p>Consumer GPUs in recent years have moved towards maintaining good performance when faced with small workloads. They’re still very wide of course, but AMD and Nvidia have struck a balance between throughput and latency. RDNA 2/3 and Ada Lovelace run well over 2 GHz, meaning their clock speeds approach those of server CPUs. Alongside high clock speeds, sophisticated cache hierarchies provide latency benefits along with high bandwidth, provided access patterns are cache friendly. Meanwhile, expensive memory solutions have fallen out of favor. The few client GPUs with HBM never did well against their GDDR equipped competition, despite having more memory bandwidth and often more compute throughput to back that up.</p>
<p>But that evidently doesn’t apply to compute GPUs, because they’ve gone in the opposite direction. H100 is a massively wide GPU running at relatively low clocks, which emphasizes performance per watt over absolute performance. 1755 MHz was typical for Pascal, an architecture launched seven years ago. Cache capacity and latency are mediocre compared to recent client GPUs. Meanwhile, Nvidia has not compromised bandwidth. H100’s L2 does not fall behind client GPUs when it comes to bandwidth. After L2, VRAM bandwidth is monstrous thanks to a giant HBM configuration. H100, like A100 and AMD’s CDNA GPUs, is meant to run large, long running jobs. Based on the emphasis on VRAM bandwidth over caching capacity, these jobs probably fall into the category where if you can’t catch the access pattern with a few dozen megabytes of cache, doubling cache capacity probably won’t help.</p>
<p>H100 differentiates itself from client designs at the SM level too. More memory for L1 or Shared Memory use means carefully crafted programs can keep a lot of data very close to the execution units. Across H100’s 144 physical SMs, there’s 36.8 MB of L1 and Shared Memory capacity, which makes for a significant die area investment. Nvidia has also spent SM area to track more warps in flight in order to cope with higher L1 miss latency. H100 can track 64 warps per SM, compared to 48 on client Ampere’s and Ada Lovelace. Additional SM area gets spent to double FP32, FP64, and tensor throughput.</p>
<p>Client GPUs continue to provide reasonable compute capability, and datacenter GPUs <a href="https://www.techpowerup.com/310298/nvidia-h100-hopper-gpu-tested-for-gaming-slower-than-integrated-gpu">can be forced to render graphics</a> if you hate yourself enough. But for the foreseeable future, compute and graphics oriented architectures will likely continue to diverge. Ada Lovelace and H100 feature plenty of differences, even if they’re based off a similar base. On the AMD side, RDNA and CDNA continue to diverge too, though both can trace their ISA’s roots back to the venerable GCN architecture. This kind of divergence is natural as process node progress slows down and everyone tries to specialize to get the most out of each transistor.<br>If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our&nbsp;<a href="https://www.patreon.com/ChipsandCheese">Patreon</a>&nbsp;or our&nbsp;<a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ">PayPal</a>&nbsp;if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our&nbsp;<a href="https://discord.gg/TwVnRhxgY2">Discord</a>.</p>

<div data-post_id="10949" data-instance_id="1" data-additional_class="pp-multiple-authors-layout-boxed.multiple-authors-target-the-content" data-original_class="pp-multiple-authors-boxes-wrapper pp-multiple-authors-wrapper box-post-id-10949 box-instance-id-1">

<ul>
<li>
<p><img data-lazy-fallback="1" alt="clamchowder" src="https://secure.gravatar.com/avatar/83de286347cdfc84e1bb10146350467e?s=80&amp;d=identicon&amp;r=g" srcset="https://secure.gravatar.com/avatar/83de286347cdfc84e1bb10146350467e?s=160&amp;d=identicon&amp;r=g 2x" height="80" width="80" loading="lazy" decoding="async" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"> </p>

</li>
</ul>
</div>





</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[55 GiB/s FizzBuzz (2021) (452 pts)]]></title>
            <link>https://codegolf.stackexchange.com/questions/215216/high-throughput-fizz-buzz/236630#236630</link>
            <guid>36568192</guid>
            <pubDate>Mon, 03 Jul 2023 02:54:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://codegolf.stackexchange.com/questions/215216/high-throughput-fizz-buzz/236630#236630">https://codegolf.stackexchange.com/questions/215216/high-throughput-fizz-buzz/236630#236630</a>, See on <a href="https://news.ycombinator.com/item?id=36568192">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<h2>x86-64+AVX2 assembly language (Linux, <code>gcc</code>+<code>gas</code>)</h2>

<h3>Build and usage instructions</h3>
<p>This program is most conveniently built using <code>gcc</code>. Save it as <code>fizzbuzz.S</code> (that's a capital <code>S</code> as the extension), and build using the commands</p>
<pre><code>gcc -mavx2 -c fizzbuzz.S
ld -o fizzbuzz fizzbuzz.o
</code></pre>
<p>Run as <code>./fizzbuzz</code> piped into one command, e.g. <code>./fizzbuzz | pv &gt; /dev/null</code> (as suggested in the question), <code>./fizzbuzz | cat</code>, or <code>./fizzbuzz | less</code>. To simplify the I/O, this will not work (producing an error on startup) if you try to output to a file/terminal/device rather than a pipe. Additionally, this program may produce incorrect output if piped into two commands (e.g. <code>./fizzbuzz | pv | cat &gt; fizzbuzz.txt</code>), but only in the case where the middle command uses the <code>splice</code> system call; this is either a bug in Linux (very possible with system calls this obscure!) or a mistake in the documentation of the system calls in question (also possible). However, it should work correctly for the use case in the question, which is all that matters on CGCC.</p>
<p>This program is somewhat system-specific; it requires the operating system to be a non-ancient version of Linux, and the processor to be an x86-64 implementation that supports AVX2. (Most moderately recent processors by Intel and AMD have AVX2 support, including the Ryzen 9 mentioned in the question, and almost all use the x86-64 instruction set.) However, it avoids assumptions about the system it's running on beyond those mentioned in the header, so there's a decent chance that if you can run Linux, you can run this.</p>
<p>The program outputs a quintillion lines of FizzBuzz and then exits (going further runs into problems related to the sizes of registers). This would take tens of years to accomplish, so hopefully counts as "a very high astronomical number" (although it astonishes me that it's a small enough timespan that it might be theoretically possible to reach a number as large as a quintillion without the computer breaking).</p>
<p>As a note: this program's performance is dependent on whether it and the program it outputs to are running on sibling CPUs or not, something which will be determined arbitrarily by the kernel when you start it. If you want to compare the two possible timings, use <code>taskset</code> to force the programs onto particular CPUs:
<code>taskset 1 ./fizzbuzz | taskset 2 pv &gt; /dev/null</code> versus <code>taskset 1 ./fizzbuzz | taskset 4 pv &gt; /dev/null</code>. (The former will probably run faster, but might be slower on some CPU configurations.)</p>
<h3>Discussion</h3>
<p>I've spent months working on this program. I've long thought that "how fast can you make a FizzBuzz" would be a really interesting question for learning about high-performance programming, and when I subsequently saw this question posted on CGCC, I pretty much had to try.</p>
<p>This program aims for the maximum possible single-threaded performance. In terms of the FizzBuzz calculation itself, it is intended to sustain a performance of 64 bytes of FizzBuzz per 4 clock cycles (and is future-proofed where possible to be able to run faster if the relevant processor bottleneck – L2 cache write speed – is ever removed). This is faster than a number of standard functions. In particular, it's faster than <code>memcpy</code>, which presents interesting challenges when it comes to I/O (if you try to output using <code>write</code> then the copies in <code>write</code> will take up almost all the runtime – replacing the I/O routine here with <code>write</code> causes the performance on my CPU to drop by a factor of 5). As such, I needed to use much more obscure system calls to keep I/O-related copies to a minimum (in particular, the generated FizzBuzz text is only sent to main memory if absolutely necessary; most of the time it's stored in the processor's L2 cache and piped into the target program from there, which is why reading it from a sibling CPU can boost performance – the physical connection to the L2 cache is shorter and higher bandwidth than it would be to a more distant CPU).</p>
<p>On my computer (which has a fairly recent, but not particularly powerful, Intel processor), this program generates around 31GiB of FizzBuzz per second. I'll be interested to see how it does on the OP's computer.</p>
<p>I did experiment with multithreaded versions of the program, but was unable to gain any speed. Experiments with simpler programs show that it could be possible, but any gains may be small; the cost of communication between CPUs is sufficiently high to negate most of the gains you could get by doing work in parallel, assuming that you only have one program reading the resulting FizzBuzz (and anything that writes to memory will be limited by the write speed of main memory, which is slower than the speed with which the FizzBuzz can be generated).</p>
<h3>The program</h3>
<p>This isn't <a href="https://codegolf.stackexchange.com/questions/tagged/code-golf" title="show questions tagged 'code-golf'" rel="tag">code-golf</a>, so my explanation of the program and its algorithm are given as comments in the program itself. (I still had to lightly golf the program, and especially the explanation, to fit this post within the 65536 byte size limit.)</p>
<p>The program is written in a "literate" assembly style; it will be easiest to understand if you read it in order, from start to end. (I also added a number of otherwise useless line labels to separate the program into logical groups of instructions, in order to make the disassembly easier to read, if you're one of the people who prefers to read assembly code like that.)</p>
<pre><code>.intel_syntax prefix

// Header files.
#include &lt;asm/errno.h&gt;
#include &lt;asm/mman.h&gt;
#include &lt;asm/unistd.h&gt;
#define F_SETPIPE_SZ 1031 // not in asm headers, define it manually

// The Linux system call API (limited to 4 arguments, the most this
// program uses). 64-bit registers are unsuffixed; 32-bit have an "e"
// suffix.
#define ARG1 %rdi
#define ARG1e %edi
#define ARG2 %rsi
#define ARG2e %esi
#define ARG3 %rdx
#define ARG3e %edx
#define ARG4 %r10
#define ARG4e %r10d
#define SYSCALL_RETURN %rax
#define SYSCALL_RETURNe %eax
#define SYSCALL_NUMBER %eax

// %rax, %rcx, %rdx, %ymm0-3 are general-purpose temporaries. Every
// other register is used for just one or two defined purposes; define
// symbolic names for them for readability. (Bear in mind that some of
// these will be clobbered sometimes, e.g. OUTPUT_LIMIT is clobbered
// by `syscall` because it's %r11.)
#define OUTPUT_PTR %rbx
#define BYTECODE_IP %rbp
#define SPILL %rsi
#define BYTECODE_GEN_PTR %rdi
#define REGEN_TRIGGER %r8
#define REGEN_TRIGGERe %r8d
#define YMMS_AT_WIDTH %r9
#define YMMS_AT_WIDTHe %r9d
#define BUZZ %r10
#define BYTECODE_NEG_LEN %r10
#define FIZZ %r11
#define FIZZe %r11d
#define OUTPUT_LIMIT %r11
#define BYTECODE_END %r12
#define BYTECODE_START %r13
#define BYTECODE_STARTe %r13d
#define PIPE_SIZE %r13
#define LINENO_WIDTH %r14
#define LINENO_WIDTHe %r14d
#define GROUPS_OF_15 %r15
#define GROUPS_OF_15e %r15d
#define LINENO_LOW %ymm4
#define LINENO_MID %ymm5
#define LINENO_MIDx %xmm5
#define LINENO_TOP %ymm6
#define LINENO_TOPx %xmm6
#define LINENO_MID_TEMP %ymm7
#define ENDIAN_SHUFFLE %ymm8
#define ENDIAN_SHUFFLEx %xmm8
#define LINENO_LOW_INCR %ymm9
#define LINENO_LOW_INCRx %xmm9

// The last six vector registers are used to store constants, to avoid
// polluting the cache by loading their values from memory.
#define LINENO_LOW_INIT %ymm10
#define LINENO_MID_BASE %ymm11
#define LINENO_TOP_MAX %ymm12
#define ASCII_OFFSET %ymm13
#define ASCII_OFFSETx %xmm13
#define BIASCII_OFFSET %ymm14
#define BASCII_OFFSET %ymm15


// Global variables.
.bss
.align 4 &lt;&lt; 20
// The most important global variables are the IO buffers. There are
// two of these, each with 2MiB of memory allocated (not all of it is
// used, but putting them 2MiB apart allows us to simplify the page
// table; this gives a 30% speedup because page table contention is
// one of the main limiting factors on the performance).
io_buffers:
.zero 2 * (2 &lt;&lt; 20)
// The remaining 2MiB of memory stores everything else:
iovec_base:          // I/O config buffer for vmsplice(2) system call
.zero 16
error_write_buffer:  // I/O data buffer for write(2) system call
.zero 1
.p2align 9,0
bytecode_storage:    // the rest is a buffer for storing bytecode
.zero (2 &lt;&lt; 20) - 512


// The program starts here. It doesn't use the standard library (or
// indeed any libraries), so the start point is _start, not main.
.text
.globl _start
_start:

// This is an AVX2 program, so check for AVX2 support by running an
// AVX2 command. This is a no-op, but generates SIGILL if AVX2 isn't
// supported.
vpand %ymm0, %ymm0, %ymm0

// Initialize constant registers to their constant values.
vmovdqa LINENO_LOW_INIT, [%rip + lineno_low_init]
vmovdqa LINENO_MID_BASE, [%rip + lineno_mid_base]
vmovdqa LINENO_TOP_MAX, [%rip + lineno_top_max]
vmovdqa ASCII_OFFSET, [%rip + ascii_offset]
vmovdqa BIASCII_OFFSET, [%rip + biascii_offset]
vmovdqa BASCII_OFFSET, [%rip + bascii_offset]

// Initialize global variables to their initial values.
vmovdqa ENDIAN_SHUFFLE, [%rip + endian_shuffle_init]
vmovdqa LINENO_TOP, [%rip + lineno_top_init]

// Check the size of the L2 cache.
//
// This uses the CPUID interface. To use it safely, check what range
// of command numbers is legal; commands above the legal range have
// undefined behaviour, commands within the range might not be
// implemented but will return all-zeros rather than undefined values.
// CPUID clobbers a lot of registers, including some that are normally
// call-preserved, so this must be done first.
mov %eax, 0x80000000 // asks which CPUID extended commands exist
cpuid                // returns the highest supported command in %eax
cmp %eax, 0x80000006 // does 0x80000006 give defined results?
jb bad_cpuid_error

mov %eax, 0x80000006 // asks about the L2 cache size
cpuid                // returns size in KiB in the top half of %ecx
shr %ecx, 16
jz bad_cpuid_error   // unsupported commands return all-0s

// Calculate the desired pipe size, half the size of the L2 cache.
// This value is chosen so that the processor can hold a pipeful of
// data being output, plus a pipeful of data being calculated, without
// needing to resort to slow L3 memory operations.
shl %ecx, 10 - 1     // convert KiB to bytes, then halve
mov PIPE_SIZE, %rcx

// Ask the kernel to resize the pipe on standard output.
mov ARG1e, 1
mov ARG2e, F_SETPIPE_SZ
mov ARG3e, %ecx
mov SYSCALL_NUMBER, __NR_fcntl
syscall
cmp SYSCALL_RETURNe, -EBADF
je pipe_error
cmp SYSCALL_RETURNe, -EPERM
je pipe_perm_error
call exit_on_error
cmp SYSCALL_RETURN, PIPE_SIZE
jne pipe_size_mismatch_error

// Ask the kernel to defragment the physical memory backing the BSS
// (read-write data) segment. This simplifies the calculations needed
// to find physical memory addresses, something that both the kernel
// and processor would otherwise spend a lot of time doing, and
// speeding the program up by 30%.
lea ARG1, [%rip + io_buffers]
mov ARG2e, 3 * (2 &lt;&lt; 20)
mov ARG3e, MADV_HUGEPAGE
mov SYSCALL_NUMBER, __NR_madvise
syscall
call exit_on_error

// From now on, OUTPUT_PTR is permanently set to the memory location
// where the output is being written. This starts at the start of the
// first I/O buffer.
lea OUTPUT_PTR, [%rip + io_buffers]


///// First phase of output
//
// The FizzBuzz output is produced in three distinct phases. The first
// phase is trivial; just a hardcoded string, that's left in the
// output buffer, to be output at the end of the second phase.

first_phase:

.section .rodata
fizzbuzz_intro:
.ascii "1\n2\nFizz\n4\nBuzz\nFizz\n7\n8\nFizz\n"
.text
vmovdqu %ymm0, [%rip + fizzbuzz_intro]
vmovdqu [OUTPUT_PTR], %ymm0
add OUTPUT_PTR, 30


///// Second phase of output
//
// This is a routine implementing FizzBuzz in x86-64+AVX2 assembler in
// a fairly straightforward and efficient way. This isn't as fast as
// the third-phase algorithm, and can't handle large numbers, but will
// introduce some of the basic techniques this program uses.

second_phase_init:

// The outer loop of the whole program breaks the FizzBuzz output into
// sections where all the line numbers contain the same number of
// digits. From now on, LINENO_WIDTH tracks the number of digits in
// the line number. This is currently 2; it ranges from 2-digit
// numbers to 18-digit numbers, and then the program ends.
mov LINENO_WIDTHe, 2

// GROUPS_OF_15 is permanently set to the number of groups of 15 lines
// that exist at this line number width; it's multiplied by 10 whenever
// LINENO_WIDTH is incremented.
//
// A general note about style: often the program uses numbers that are
// statically known to fit into 32 bits, even in a register that's
// conceptually 64 bits wide (like this one). In such cases, the
// 32-bit and 64-bit versions of a command will be equivalent (as the
// 32-bit version zero-extends to 64-bits on a 64-bit processor); this
// program generally uses the 32-bit version, both because it
// sometimes encodes to fewer bytes (saving cache pressure), and
// because some processors recognise zeroing idioms only if they're 32
// bits wide.
mov GROUPS_OF_15e, 6

// Some constants used throughout the second phase, which permanently
// stay in their registers. Note that short string literals can be
// stored in normal integer registers - the processor doesn't care.
mov FIZZ, 0x0a7a7a6946  // "Fizz\n"
mov BUZZ, 0x0a7a7a7542  // "Buzz\n"

.section .rodata
.p2align 5, 0
second_phase_constants:
.byte 0, 0, 0, 0, 0, 0, 0, 0
.byte 1, 0, 0, 0, 0, 0, 0, 0
.text
vmovdqa %xmm3, [%rip + second_phase_constants]

// This program makes extensive use of a number format that I call
// "high-decimal". This is a version of decimal where the digit 0 is
// encoded as the byte 246, the digit 1 as the byte 247, ..., the
// digit 9 as the byte 255. The bytes are stored in the normal
// endianness for the processor (i.e. least significant first), and
// padded to a known length (typically 8 digits) with leading zeroes.
//
// The point of high-decimal is that it allows us to use arithmetic
// operators intended for binary on high-decimal numbers, and the
// carries will work the same way (i.e. the same digits will carry,
// although carries will be 0-based rather than 246-based); all that's
// required is to identify the digits that carried and add 246 to
// them. That means that the processor's binary ALU can be used to do
// additions directly in decimal - there's no need for loops or
// anything like that, and no need to do binary/decimal conversions.
//
// The first use for high-decimal is to store the line number during
// the second phase (it's stored differently in the third phase).
// It's stored it in the top half of %xmm1 (although it's only 64 bits
// wide, it needs to be in a vector register so that it can be
// interpreted as 8 x 8 bits when necessary; general-purpose
// registers can't do that). The bottom half of %xmm1 is unused, and
// frequently overwritten with arbitrary data.
.section .rodata
line_number_init:
#define REP8(x) x,x,x,x,x,x,x,x
.byte REP8(0)
.byte 246, 247, 246, 246, 246, 246, 246, 246
.text
vmovdqa %xmm1, [%rip + line_number_init]

// Writing line numbers is nontrivial because x86-64 is little-endian
// but FizzBuzz output is big-endian; also, leading zeroes aren't
// allowed. ENDIAN_SHUFFLE is used to fix both these problems; when
// used to control the vector shuffler, it reverses the order of a
// vector register, and rotates the elements to put the first digit
// (based on LINENO_WIDTH) into the first byte. (This method is used
// by both the second and third phases; the second phase uses only the
// bottom half, with the top half used by the third phase, but they
// are both initialized together.)
.section .rodata
endian_shuffle_init:
.byte 9, 8, 7, 6, 5, 4, 3, 2
.byte 1, 0, 255, 254, 253, 252, 251, 250
.byte 3, 2, 1, 0, 255, 254, 253, 252
.byte 251, 250, 249, 248, 247, 246, 245, 244
.text


second_phase_per_width_init:

// The second phase writing routines are macros.
//
// Fizz and Buzz are trivial. (This writes a little beyond the end of
// the string, but that's OK; the next line will overwrite them.)
#define WRITE_FIZZ   mov [OUTPUT_PTR], FIZZ; add OUTPUT_PTR, 5
#define WRITE_BUZZ   mov [OUTPUT_PTR], BUZZ; add OUTPUT_PTR, 5

// For FizzBuzz, output 32 bits of FIZZ to write "Fizz" with no
// newline, then write a "Buzz" after that.
#define WRITE_FIZZBUZZ \
  mov [OUTPUT_PTR], FIZZe; mov [OUTPUT_PTR + 4], BUZZ; \
  add OUTPUT_PTR, 9

// To write a line number, add 58 to each byte of the line number
// %xmm1, fix the endianness and width with a shuffle, and write a
// final newline.
.section .rodata
ascii_offset:
.byte REP8(58), REP8(58), REP8(58), REP8(58)
.text
#define WRITE_LINENO \
  vpaddb %xmm0, ASCII_OFFSETx, %xmm1; \
  vpshufb %xmm0, %xmm0, ENDIAN_SHUFFLEx; \
  vmovdqu [OUTPUT_PTR], %xmm0; \
  lea OUTPUT_PTR, [OUTPUT_PTR + LINENO_WIDTH + 1]; \
  mov byte ptr [OUTPUT_PTR - 1], 10  // 10 = newline

// Incrementing the line number is fairly easy: add 1 (in the usual
// binary notation, taken from %xmm3) to the high-decimal number, then
// convert any bytes that produced a carry to high-decimal 0s by
// max-ing with 246.
//
// Normally I'd use a separate constant for this, but there randomly
// happens to be an %xmm register with 246s in its top half already
// (it's intended for an entirely different purpose, but it'll do for
// this one too).
#define INC_LINENO \
  vpaddq %xmm1, %xmm3, %xmm1; vpmaxub %xmm1, LINENO_TOPx, %xmm1

// Avoid modulus tests by unrolling the FizzBuzz by 15. (Bear in mind
// that this starts at 10, not 0, so the pattern will have a different
// phase than usual.)
mov %ecx, GROUPS_OF_15e
fifteen_second_phase_fizzbuzz_lines:
WRITE_BUZZ; INC_LINENO
WRITE_LINENO; INC_LINENO
WRITE_FIZZ; INC_LINENO
WRITE_LINENO; INC_LINENO
WRITE_LINENO; INC_LINENO
WRITE_FIZZBUZZ; INC_LINENO
WRITE_LINENO; INC_LINENO
WRITE_LINENO; INC_LINENO
WRITE_FIZZ; INC_LINENO
WRITE_LINENO; INC_LINENO
WRITE_BUZZ; INC_LINENO
WRITE_FIZZ; INC_LINENO
WRITE_LINENO; INC_LINENO
WRITE_LINENO; INC_LINENO
WRITE_FIZZ; INC_LINENO
dec %ecx
jnz fifteen_second_phase_fizzbuzz_lines

second_phase_increment_width:

lea GROUPS_OF_15e, [GROUPS_OF_15 + GROUPS_OF_15 * 4]
add GROUPS_OF_15e, GROUPS_OF_15e
inc LINENO_WIDTHe

// Increment every element of the low half of ENDIAN_SHUFFLE to
// adjust it for the new width, while leaving the top half unchanged.
vpcmpeqb %xmm0, %xmm0, %xmm0
vpsubb ENDIAN_SHUFFLE, ENDIAN_SHUFFLE, %ymm0

// The second phase handles line numbers with 2 to 5 digits.
cmp LINENO_WIDTHe, 6
jne second_phase_per_width_init

///// The output routine
//
// Most FizzBuzz routines produce output with `write` or a similar
// system call, but these have the disadvantage that they need to copy
// the data being output from userspace into kernelspace. It turns out
// that when running full speed (as seen in the third phase), FizzBuzz
// actually runs faster than `memcpy` does, so `write` and friends are
// unusable when aiming for performance - this program runs five times
// faster than an equivalent that uses `write`-like system calls.
//
// To produce output without losing speed, the program therefore needs
// to avoid copies, or at least do them in parallel with calculating
// the next block of output. This can be accomplished with the
// `vmsplice` system call, which tells the kernel to place a reference
// to a buffer into a pipe (as opposed to copying the data into the
// pipe); the program at the other end of this pipe will then be able
// to read the output directly out of this program's memory, with no
// need to copy the data into kernelspace and then back into
// userspace. In fact, it will be reading out of this program's
// processor's L2 cache, without main memory being touched at all;
// this is the secret to high-performance programming, because the
// cache is much faster than main memory is.
//
// Of course, it's therefore important to avoid changing the output
// buffer until the program connected to standard output has actually
// read it all. This is why the pipe size needed to be set earlier; as
// long as the amount of output is always at least as large as the
// pipe size, successfully outputting one buffer will ensure that none
// of the other buffer is left in the pipe, and thus it's safe to
// overwrite the memory that was previously output. There is some need
// to jump through hoops later on to make sure that `swap_buffers` is
// never called with less than one pipeful of data, but it's worth it
// to get the huge performance boost.

mov %rdx, OUTPUT_PTR
and %edx, (2 &lt;&lt; 20) - 1

call swap_buffers
jmp third_phase_init

// Takes the amount of data to output in %rdx, and outputs from the
// buffer containing OUTPUT_PTR.
swap_buffers:
and OUTPUT_PTR, -(2 &lt;&lt; 20)  // rewind to the start of the buffer
mov [%rip + iovec_base], OUTPUT_PTR
mov [%rip + iovec_base + 8], %rdx
mov ARG1e, 1
lea ARG2, [%rip + iovec_base]
mov ARG3e, 1
xor ARG4e, ARG4e

// As with most output commands, vmsplice can do a short write
// sometimes, so it needs to be called in a loop in order to ensure
// that all the output is actually sent.
1: mov SYSCALL_NUMBER, __NR_vmsplice
syscall
call exit_on_error
add [ARG2], SYSCALL_RETURN
sub [ARG2 + 8], SYSCALL_RETURN
jnz 1b

xor OUTPUT_PTR, (2 &lt;&lt; 20)  // swap to the other buffer
ret


///// Third phase of output
//
// This is the heart of this program. It aims to be able to produce a
// sustained output rate of 64 bytes of FizzBuzz per four clock cycles
// in its main loop (with frequent breaks to do I/O, and rare breaks
// to do more expensive calculations).
//
// The third phase operates primarily using a bytecode interpreter; it
// generates a program in "FizzBuzz bytecode", for which each byte of
// bytecode generates one byte of output. The bytecode language is
// designed so that it can be interpreted using SIMD instructions; 32
// bytes of bytecode can be loaded from memory, interpreted, and have
// its output stored back into memory using just four machine
// instructions. This makes it possible to speed up the FizzBuzz
// calculations by hardcoding some of the calculations into the
// bytecode (this is similar to how JIT compilers can create a version
// of the program with some variables hardcoded, and throw it away on
// the rare occasions that those variables' values change).

third_phase_init:

// Reinitialize ENDIAN_SHUFFLE by copying the initializer stored in
// its high half to both halves. This works in the same way as in the
// second phase.
vpermq ENDIAN_SHUFFLE, ENDIAN_SHUFFLE, 0xEE

// Up to this point, PIPE_SIZE has held the size of the pipe. In order
// to save on registers, the pipe size is from now on encoded via the
// location in which the bytecode program is stored; the bytecode is
// started at iovec_base + PIPE_SIZE (which will be somewhere within
// bytecode_storage), so the same register can be used to find the
// bytecode and to remember the pipe size.
lea %rax, [%rip + iovec_base]
add BYTECODE_START, %rax  // BYTECODE_START is a synonym for PIPE_SIZE

// The bytecode program always holds instructions to produce exactly
// 600 lines of FizzBuzz. At width 6, those come to 3800 bytes long.
lea BYTECODE_END, [BYTECODE_START + 3800]

mov REGEN_TRIGGER, -1  // irrelevant until much later, explained there


third_phase_per_width_init:

// Calculate the amount of output at this LINENO_WIDTH. The result
// will always be divisible by 32, and thus is stored as the number of
// 32-byte units at this width; storing it in bytes would be more
// convenient, but sadly would overflow a 64-bit integer towards the
// end of the program.
lea %ecx, [LINENO_WIDTH * 8 + 47]   // bytes per 15 lines
mov YMMS_AT_WIDTH, GROUPS_OF_15
shr YMMS_AT_WIDTH, 5   // to avoid overflow, divide by 32 first
imul YMMS_AT_WIDTH, %rcx

// This program aims to output 64 bytes of output per four clock
// cycles, which it achieves via a continuous stream of 32-byte writes
// calculated by the bytecode program. One major complication here is
// that the 32-byte writes won't correspond to lines of FizzBuzz; a
// single processor instruction may end up outputting multiple
// different line numbers. So it's no longer possible to have a simple
// line number register, like it was in the second phase.
//
// Instead, the program stores an *approximation* of the line number,
// which is never allowed to differ by 100 or more from the "actual"
// line number; the bytecode program is responsible for fixing up the
// approximation to work out the correct line number to output (this
// allows the same CPU instruction to output digits from multiple
// different line numbers, because the bytecode is being interpreted
// in a SIMD way and thus different parts of the bytecode can fix the
// line number up differently within a single instruction.
//
// The line number is split over three processor registers:
// - LINENO_LOW: stores the line number modulo 200
// - LINENO_MID: stores the hundreds to billions digits
// - LINENO_TOP: stores the ten-billions and more significant digits
// (The parity of the 100s digit is duplicated between LINENO_MID and
// LINENO_LOW; this allows a faster algorithm for LINENO_MID updates.)
//
// Because there's only a need to be within 100 of the real line
// number, the algorithm for updating the line numbers doesn't need to
// run all that often (saving processor cycles); it runs once every
// 512 bytes of output, by simply adding a precalculated value
// (LINENO_LOW_INCR) to LINENO_LOW, then processing the carry to
// LINENO_MID (see later for LINENO_TOP). The amount by which the line
// number increases per 512 bytes of output is not normally going to
// be an integer; LINENO_LOW is therefore stored as a 64-bit fixpoint
// number (in which 2**64 represents "200", e.g. 2**63 would be the
// representation of "the line number is 100 mod 200"), in order to
// delay the accumulation of rounding errors as long as possible. It's
// being stored in a vector register, so there are four copies of its
// value; two of them have 50 (i.e 2**62) added, and two of them have
// 50 subtracted, in order to allow for more efficient code to handle
// the carry to LINENO_MID. Additionally, LINENO_LOW is interpreted as
// a signed number (an older version of this program was better at
// checking for signed than unsigned overflow and I had no reason to
// change).
//
// LINENO_LOW and LINENO_MID are reset every LINENO_WIDTH increase
// (this is because the program can calculate "past" the width
// increase due to not being able to break out of every instruction of
// the main loop, which may cause unwanted carries into LINENO_MID and
// force a reset).

.section .rodata
lineno_low_init:
.byte 0, 0, 0, 0, 0, 0, 0, 192
.byte 0, 0, 0, 0, 0, 0, 0, 64
.byte 0, 0, 0, 0, 0, 0, 0, 192
.byte 0, 0, 0, 0, 0, 0, 0, 64
.text
vmovdqa LINENO_LOW, LINENO_LOW_INIT

// %ecx is the number of bytes in 15 lines. That means that the number
// of 200-line units in 512 bytes is 38.4/%ecx, i.e. 384/(%ecx*10).
// Multiply by 2**64 (i.e. 384*2**64/(%ecx*10) to get LINENO_LOW_INCR.
lea %ecx, [%rcx + %rcx * 4]
add %ecx, %ecx
mov %edx, 384
xor %eax, %eax
div %rcx  // 128-bit divide, %rax = %rdx%rax / %rcx
vpxor LINENO_LOW_INCR, LINENO_LOW_INCR, LINENO_LOW_INCR
vpinsrq LINENO_LOW_INCRx, LINENO_LOW_INCRx, %rax, 0
vpermq LINENO_LOW_INCR, LINENO_LOW_INCR, 0

// LINENO_MID is almost stored in high-decimal, as four eight-digit
// numbers. However, the number represented is the closest line number
// that's 50 mod 100, stored as the two closest multiples of 100 (e.g.
// if the true line number is 235, it's approximated as 250 and then
// stored using the representations for 200 and 300), which is why
// LINENO_LOW needs the offsets of 50 and -50 to easily do a carry. A
// ymm vector holds four 64-bit numbers, two of which hold the value
// that's 0 mod 200, two which hold the value that's 100 mod 200. So
// carries on it are handled using a vector of mostly 246s, with 247s
// in the two locations which are always odd.
.section .rodata
lineno_mid_base:
.byte 246, 246, 246, 246, 246, 246, 246, 246
.byte 247, 246, 246, 246, 246, 246, 246, 246
.byte 246, 246, 246, 246, 246, 246, 246, 246
.byte 247, 246, 246, 246, 246, 246, 246, 246
.text

// This code is some fairly complex vector manipulation to initialise
// LINENO_MID to a power of 10 (handling the case where LINENO_WIDTH
// is so high that the hundreds to billions digits are all zeroes).
mov %edx, 1
mov %eax, 11
sub %eax, LINENO_WIDTHe
cmovbe %eax, %edx
shl %eax, 3
vpxor %xmm0, %xmm0, %xmm0
vpinsrq %xmm0, %xmm0, %rax, 0
vpermq %ymm0, %ymm0, 0
vpcmpeqb LINENO_MID, LINENO_MID, LINENO_MID
vpsrlq LINENO_MID, LINENO_MID, %xmm0
vpmaxub LINENO_MID, LINENO_MID_BASE, LINENO_MID
vpermq %ymm0, LINENO_MID_BASE, 0x55
vpsubb %ymm0, %ymm0, LINENO_MID_BASE
vpaddq LINENO_MID, LINENO_MID, %ymm0
vpmaxub LINENO_MID, LINENO_MID_BASE, LINENO_MID

// LINENO_TOP doesn't need to be initialized for new widths, because
// an overrun by 100 lines is possible, but by 10 billion lines isn't.
// The format consists of two 64-bit sections that hold high-decimal
// numbers (these are always the same as each other), and two that
// hold constants that are used by the bytecode generator.
.section .rodata
lineno_top_init:
.byte 198, 197, 196, 195, 194, 193, 192, 191
.byte 246, 246, 246, 246, 246, 246, 246, 246
.byte 190, 189, 188, 187, 186, 185, 184, 183
.byte 246, 246, 246, 246, 246, 246, 246, 246
.text

// When moving onto a new width, start at the start of the bytecode
// program.
mov BYTECODE_IP, BYTECODE_START


// Generating the bytecode program
//
// The bytecode format is very simple (in order to allow it to be
// interpreted in just a couple of machine instructions):
// - A negative byte represents a literal character (e.g. to produce
//   a literal 'F', you use the bytecode -'F', i.e. -70 = 0xba)
// - A byte 0..7 represents the hundreds..billions digit of the line
//   number respectively, and asserts that the hundreds digit of the
//   line number is even
// - A byte 8..15 represents the hundreds..billions digit of the line
//   number respectively, and asserts that the hundreds digit of the
//   line number is odd
//
// In other words, the bytecode program only ever needs to read from
// LINENO_MID; the information stored in LINENO_LOW and LINENO_TOP
// therefore has to be hardcoded into it. The program therefore needs
// to be able to generate 600 lines of output (as the smallest number
// that's divisible by 100 to be able to hardcode the two low digits,
// 200 to be able to get the assertions about the hundreds digits
// correct, and 3 and 5 to get the Fizzes and Buzzes in the right
// place).

generate_bytecode:

mov BYTECODE_GEN_PTR, BYTECODE_START

// FIZZ and BUZZ work just like in the second phase, except that they
// are now bytecode programs rather than ASCII.
mov FIZZ, 0xf6868697ba  // -"Fizz\n"
mov BUZZ, 0xf686868bbe  // -"Buzz\n"

// %ymm2 holds the bytecode for outputting the hundreds and more
// significant digits of a line number. The most significant digits of
// this can be obtained by converting LINENO_TOP from high-decimal to
// the corresponding bytecode, which is accomplished by subtracting
// from 198 (i.e. 256 - 10 - '0'). The constant parts of LINENO_TOP
// are 198 minus the bytecode for outputting the hundreds to billions
// digit of a number; this makes it possible for a single endian
// shuffle to deal with all 16 of the mid and high digits at once.
.section .rodata
bascii_offset:
.byte REP8(198), REP8(198), REP8(198), REP8(198)
.text
vpsubb %ymm2, BASCII_OFFSET, LINENO_TOP
vpshufb %ymm2, %ymm2, ENDIAN_SHUFFLE

#define GEN_FIZZ  mov [BYTECODE_GEN_PTR], FIZZ; add BYTECODE_GEN_PTR, 5
#define GEN_BUZZ  mov [BYTECODE_GEN_PTR], BUZZ; add BYTECODE_GEN_PTR, 5
#define GEN_FIZZBUZZ \
  mov [BYTECODE_GEN_PTR], FIZZe; \
  mov [BYTECODE_GEN_PTR + 4], BUZZ; add BYTECODE_GEN_PTR, 9
#define GEN_LINENO(units_digit) \
  vmovdqu [BYTECODE_GEN_PTR], %xmm2; \
  lea BYTECODE_GEN_PTR, [BYTECODE_GEN_PTR + LINENO_WIDTH + 1]; \
  mov [BYTECODE_GEN_PTR - 3], %al; \
  mov word ptr [BYTECODE_GEN_PTR - 2], 0xf6d0 - units_digit

// The bytecode generation loop is unrolled to depth 30, allowing the
// units digits to be hardcoded. The tens digit is stored in %al, and
// incremented every ten lines of output. The parity of the hundreds
// digit is stored in %ymm2: one half predicts the hundreds digit to
// be even, the other to be odd, and the halves are swapped every time
// the tens digit carries (ensuring the predictions are correct).
mov %eax, 0xd0
jmp 2f
inc_tens_digit:
cmp %al, 0xc7
je 1f  // jumps every 10th execution, therefore predicts perfectly
dec %eax
ret
1: mov %eax, 0xd0
vpermq %ymm2, %ymm2, 0x4e
ret

2: mov %ecx, 20
thirty_bytecode_lines:
GEN_BUZZ
GEN_LINENO(1)
GEN_FIZZ
GEN_LINENO(3)
GEN_LINENO(4)
GEN_FIZZBUZZ
GEN_LINENO(6)
GEN_LINENO(7)
GEN_FIZZ
GEN_LINENO(9)
call inc_tens_digit
GEN_BUZZ
GEN_FIZZ
GEN_LINENO(2)
GEN_LINENO(3)
GEN_FIZZ
GEN_BUZZ
GEN_LINENO(6)
GEN_FIZZ
GEN_LINENO(8)
GEN_LINENO(9)
call inc_tens_digit
GEN_FIZZBUZZ
GEN_LINENO(1)
GEN_LINENO(2)
GEN_FIZZ
GEN_LINENO(4)
GEN_BUZZ
GEN_FIZZ
GEN_LINENO(7)
GEN_LINENO(8)
GEN_FIZZ
call inc_tens_digit
dec %ecx
jnz thirty_bytecode_lines

generate_bytecode_overrun_area:

// Duplicate the first 512 bytes of the bytecode program at the end,
// so that there's no need to check to see whether BYTECODE_IP needs
// to be looped back to the start of the program any more than once
// per 512 bytes
mov %rax, BYTECODE_START
#define COPY_64_BYTECODE_BYTES(offset) \
  vmovdqa %ymm0, [%rax + offset]; \
  vmovdqa %ymm3, [%rax + (offset + 32)]; \
  vmovdqu [BYTECODE_GEN_PTR + offset], %ymm0; \
  vmovdqu [BYTECODE_GEN_PTR + (offset + 32)], %ymm3
COPY_64_BYTECODE_BYTES(0)
COPY_64_BYTECODE_BYTES(64)
COPY_64_BYTECODE_BYTES(128)
COPY_64_BYTECODE_BYTES(192)
COPY_64_BYTECODE_BYTES(256)
COPY_64_BYTECODE_BYTES(320)
COPY_64_BYTECODE_BYTES(384)
COPY_64_BYTECODE_BYTES(448)


// Preparing for the main loop
//
// Work out how long the main loop is going to iterate for.
// OUTPUT_LIMIT holds the address just beyond the end of the output
// that the main loop should produce. The aim here is to produce
// exactly one pipeful of data if possible, but to stop earlier if
// there's a change in digit width (because any output beyond that
// point will be useless: the bytecode will give it the wrong number
// of digits).
calculate_main_loop_iterations:

// Extract the pipe size from BYTECODE_START, in 32-byte units.
// During this calculation, OUTPUT_LIMIT holds the amount of output
// produced, rather than an address like normal.
mov OUTPUT_LIMIT, BYTECODE_START
lea %rdx, [%rip + iovec_base]
sub OUTPUT_LIMIT, %rdx
shr OUTPUT_LIMIT, 5

// Reduce the output limit to the end of this width, if it would be
// higher than that.
cmp OUTPUT_LIMIT, YMMS_AT_WIDTH
cmovae OUTPUT_LIMIT, YMMS_AT_WIDTH

// If there's already some output in the buffer, reduce the amount
// of additional output produced accordingly (whilst ensuring that
// a multiple of 512 bytes of output is produced).
//
// This would be buggy if the YMMS_AT_WIDTH limit were hit at the
// same time, but that never occurs as it would require two width
// changes within one pipeful of each other, and 9000000 lines of
// FizzBuzz is much more than a pipeful in size.
mov %rax, OUTPUT_PTR
and %eax, ((2 &lt;&lt; 20) - 1) &amp; -512
shr %eax, 5
sub OUTPUT_LIMIT, %rax

// The amount of output to produce is available now, and won't be
// later, so subtract it from the amount of output that needs to
// be produced now.
sub YMMS_AT_WIDTH, OUTPUT_LIMIT

// Return OUTPUT_LIMIT back to being a pointer, not an amount.
shl OUTPUT_LIMIT, 5
add OUTPUT_LIMIT, OUTPUT_PTR

prepare_main_loop_invariants:

// To save one instruction in the bytecode interpreter (which is very
// valuable, as it runs every second CPU cycle), LINENO_MID_TEMP is
// used to store a reformatted version of LINENO_MID, in which each
// byte is translated from high-decimal to ASCII, and the bytecode
// command that would access that byte is added to the result (e.g.
// the thousands digit for the hundreds-digits-odd version has 10
// added to convert from high-decimal to a pure number, '0' added to
// convert to ASCII, then 9 added because that's the bytecode command
// to access the thousands digit when the hundreds digit is odd, so
// the amount added is 10 + '0' + 9 = 57).
//
// LINENO_MID_TEMP is updated within the main loop, immediately after
// updating LINENO_MID, but because the bytecode interpreter reads
// from it it needs a valid value at the start of the loop.
.section .rodata
biascii_offset:
.byte 58, 59, 60, 61, 62, 63, 64, 65
.byte 66, 67, 68, 69, 70, 71, 72, 73
.byte 58, 59, 60, 61, 62, 63, 64, 65
.byte 66, 67, 68, 69, 70, 71, 72, 73
.text
vpaddb LINENO_MID_TEMP, BIASCII_OFFSET, LINENO_MID

// To save an instruction, precalculate minus the length of the
// bytecode. (Although the value of this is determined entirely by
// LINENO_WIDTH, the register it's stored in gets clobbered by
// system calls and thus needs to be recalculated each time.)
mov BYTECODE_NEG_LEN, BYTECODE_START
sub BYTECODE_NEG_LEN, BYTECODE_END


// The main loop

// The bytecode interpreter consists of four instructions:
// 1. Load the bytecode from memory into %ymm2;
// 2. Use it as a shuffle mask to shuffle LINENO_MID_TEMP;
// 3. Subtract the bytecode from the shuffle result;
// 4. Output the result of the subtraction.
//
// To see why this works, consider two cases. If the bytecode wants to
// output a literal character, then the shuffle will produce 0 for
// that byte (in AVX2, a shuffle with a a negative index produces an
// output of 0), and subtracting the bytecode from 0 then produces the
// character (because the bytecode encoded minus the character). If
// the bytecode instead wants to output a digit, then the shuffle will
// fetch the relevant digit from LINENO_MID_TEMP (which is the desired
// ASCII character plus the bytecode instruction that produces it),
// and subtract the bytecode instruction to just produce the character
// on its own.
//
// This produces an exactly correct line number as long as the line
// number approximation is within 100 of the true value: it will be
// correct as long as the relevant part of LINENO_MID is correct, and
// the worst case is for LINENO_MID to be storing, say, 200 and 300
// (the representation of 250) when the true line number is 400. The
// value in LINENO_MID specifically can be up to 50 away from the
// value of the line number as recorded by LINENO_MID and LINENO_LOW
// together, so as long as the line number registers are within 100,
// LINENO_MID will be within 150 (which is what is required).
//
// This doesn't update the bytecode instruction pointer or the pointer
// into the output buffer; those are updated once every 512 bytes (and
// to "advance the instruction pointer" the rest of the time, the main
// loop is unrolled, using hardcoded offsets with the pointer updates
// baked in).
//
// The bytecode instruction pointer itself is read from %rdx, not
// BYTECODE_IP, so that mid-loop arithmetic on BYTECODE_IP won't cause
// the interpreter to break.
//
// It's important to note one potential performance issue with this
// code: the read of the bytecode from memory is not only misalignable
// (`vmovdqu`); it splits a cache line 3/8 of the time. This causes L1
// split-load penalties on the 3/8 cycles where it occurs. I am not
// sure whether this actually reduces the program's performance in
// practice, or whether the split loads can be absorbed while waiting
// for writes to go through to the L2 cache. However, even if it does
// have a genuine performance cost, it seems like the least costly way
// to read the bytecode; structuring the bytecode to avoid split loads
// makes it take up substantially more memory, and the less cache that
// is used for the bytecode, the more that can be used for the output
// buffers. (In particular, increasing the bytecode to 2400 lines so
// that it's available at all four of the alignments required of it
// does not gain, because it then becomes so large that the processor
// struggles to keep it in L1 cache - it only just fits, and there
// isn't any way for it to know which parts of the cache are meant to
// stay in L1 and which are meant to leave to L2, so there's a large
// slowdown when it guesses wrong.)
#define INTERPRET_BYTECODE(bc_offset, buf_offset) \
  vmovdqu %ymm2, [%rdx + bc_offset]; \
  vpshufb %ymm0, LINENO_MID_TEMP, %ymm2; \
  vpsubb %ymm0, %ymm0, %ymm2; \
  vmovdqa [OUTPUT_PTR + buf_offset], %ymm0

// The main loop itself consists of sixteen uses of the bytecode
// interpreter, interleaved (to give the reorder buffer maximum
// flexibility) with all the other logic needed in the main loop.
// (Most modern processors can handle 4-6 instructions per clock cycle
// as long as they don't step on each others' toes; thus this loop's
// performance will be limited by the throughput of the L2 cache, with
// all the other work (bytecode interpretation, instruction decoding,
// miscellaneous other instructions, etc.) fitting into the gaps while
// the processor is waiting for the L2 cache to do its work.)

.p2align 5
main_loop:
// %rdx caches BYTECODE_IP's value at the start of the loop
mov %rdx, BYTECODE_IP
INTERPRET_BYTECODE(0, 0)

// %ymm1 caches LINENO_LOW's value at the start of the loop
vmovdqa %ymm1, LINENO_LOW
INTERPRET_BYTECODE(32, 32)

// Add LINENO_LOW_INCR to LINENO_LOW, checking for carry; it carried
// if the sign bit changed from 0 to 1. (vpandn is unintuitive; this
// is ~%ymm1 &amp; LINENO_LOW, not %ymm1 &amp; ~LINENO_LOW like the name
// suggests.)
vpaddq LINENO_LOW, LINENO_LOW_INCR, LINENO_LOW
INTERPRET_BYTECODE(64, 64)

vpandn %ymm3, %ymm1, LINENO_LOW
INTERPRET_BYTECODE(96, 96)

vpsrlq %ymm3, %ymm3, 63
INTERPRET_BYTECODE(128, 128)

// Add the carry to LINENO_MID (doubling it; LINENO_MID counts in
// units of 100 but a LINENO_LOW carry means 200).
vpaddb %ymm3, %ymm3, %ymm3
INTERPRET_BYTECODE(160, 160)

vpaddq LINENO_MID, LINENO_MID, %ymm3
INTERPRET_BYTECODE(192, 192)

vpmaxub LINENO_MID, LINENO_MID_BASE, LINENO_MID
INTERPRET_BYTECODE(224, 224)

// Update LINENO_MID_TEMP with the new value from LINENO_MID; this is
// the point at which the new value takes effect. This is done at the
// exact midpoint of the loop, in order to reduce the errors from
// updating once every 512 bytes as far as possible.
vpaddb LINENO_MID_TEMP, BIASCII_OFFSET, LINENO_MID
INTERPRET_BYTECODE(256, 256)

// Update the output and bytecode instruction pointers. The change to
// the output pointer kicks in immediately, but is cancelled out via
// the use of a negative offset until the end of the loop.
add OUTPUT_PTR, 512
INTERPRET_BYTECODE(288, -224)

add BYTECODE_IP, 512
INTERPRET_BYTECODE(320, -192)

// The change to the bytecode instruction pointer doesn't kick in
// immediately, because it might need to wrap back to the start (this
// can be done by adding BYTECODE_NEG_LEN to it); this is why the
// interpreter has a cached copy of it in %rdx.
lea %rax, [BYTECODE_IP + BYTECODE_NEG_LEN]
INTERPRET_BYTECODE(352, -160)

INTERPRET_BYTECODE(384, -128)
// Some modern processors can optimise `cmp` better if it appears
// immediately before the command that uses the comparison result, so
// a couple of commands have been moved slightly to put the `cmp` next
// to the use of its result. With modern out-of-order processors,
// there is only a marginal advantage to manually interleaving the
// instructions being used, and the `cmp` advantage outweighs that.
cmp BYTECODE_IP, BYTECODE_END

cmovae BYTECODE_IP, %rax
INTERPRET_BYTECODE(416, -96)

INTERPRET_BYTECODE(448, -64)

INTERPRET_BYTECODE(480, -32)
cmp OUTPUT_PTR, OUTPUT_LIMIT
jb main_loop

after_main_loop:
// There are two reasons the main loop might terminate: either there's
// a pipeful of output, or the line number has increased in width
// (forcing the generaion of new bytecode to put more digits in the
// numbers being printed). In the latter case, a) the output may have
// overrun slightly, and OUTPUT_PTR needs to be moved back to
// OUTPUT_LIMIT:
mov OUTPUT_PTR, OUTPUT_LIMIT
// and b) there may be less than a pipeful of output, in which case it
// wouldn't be safe to output it and the swap_buffers call needs to be
// skipped. Calculate the pipe size into %rax, the amount of output
// into %rdx (swap_buffers needs it there anyway), and compare.
lea %rax, [%rip + iovec_base]
sub %rax, BYTECODE_START
neg %eax
mov %rdx, OUTPUT_PTR
and %edx, (2 &lt;&lt; 20) - 1
cmp %edx, %eax
jb 1f
call swap_buffers

// If all the lines at this width have been exhausted, move to the
// next width.
1: test YMMS_AT_WIDTH, YMMS_AT_WIDTH
jnz check_lineno_top_carry

cmp LINENO_WIDTHe, 18  // third phase handles at most 18 digits
je fourth_phase

inc LINENO_WIDTHe
vpcmpeqb %ymm0, %ymm0, %ymm0
vpsubb ENDIAN_SHUFFLE, ENDIAN_SHUFFLE, %ymm0

lea GROUPS_OF_15, [GROUPS_OF_15 + GROUPS_OF_15 * 4]
add GROUPS_OF_15, GROUPS_OF_15

add BYTECODE_END, 320

jmp third_phase_per_width_init

// So far, the code has kept LINENO_MID and LINENO_LOW updated, but
// not LINENO_TOP. Because 10 billion lines of FizzBuzz don't normally
// have a length that's divisible by 512 (and indeed, vary in size a
// little because 10 billion isn't divisible by 15), it's possible for
// the 10-billions and higher digits to need to change in the middle
// of a main loop iteration - indeed, even in the middle of a single
// CPU instruction!
//
// It turns out that when discussing the line number registers above,
// I lied a little about the format. The bottom seven bytes of
// LINENO_MID do indeed represent the hundreds to hundred millions
// digits. However, the eighth changes in meaning over the course of
// the program. It does indeed represent the billions digit most of
// the time; but when the line number is getting close to a multiple
// of 10 billion, the billions and hundred-millions digits will always
// be the same as each other (either both 9s or both 0s). When this
// happens, the format changes: the hundred-millions digit of
// LINENO_MID represents *both* the hundred-millions and billions
// digits of the line number, and the top byte then represents the
// ten-billions digit. Because incrementing a number causes a row of
// consecutive 9s to either stay untouched, or all roll over to 0s at
// once, this effectively lets us do maths on more than 8 digits,
// meaning that the normal arithmetic code within the main loop can
// handle the ten-billions digit in addition to the digits below.
//
// Of course, the number printing code also needs to handle the new
// representation, but the number printing is done by a bytecode
// program, which can be made to output some of the digits being
// printed multiple times by repeating "print digit from LINENO_MID"
// commands within it. Those commands are generated from COUNTER_TOP
// anyway, so the program just changes the constant portion of
// COUNTER_TOP (and moves print-digit commands into the top half) in
// order to produce the appropriate bytecode changes.
//
// A similar method is used to handle carries in the hundred-billions,
// trillions, etc. digits.
//
// Incidentally, did you notice the apparent off-by-one in the
// initialisation of LINENO_MID within third_phase_per_width_init? It
// causes the "billions" digit to be initialised to 1 (not 0) when the
// line number width is 11 or higher. That's because the alternate
// representation will be in use during a line number width change (as
// higher powers of 10 are close to multiples of 10 billion), so the
// digit that's represented by that byte of LINENO_MID genuinely is a
// 1 rather than a 0.
check_lineno_top_carry:

// The condition to change line number format is:
// a) The line number is in normal format, and the hundred-millions
//    and billions digits are both 9; or
// b) The line number is in alternate format, and the hundred-millions
//    digit is 0.
// To avoid branchy code in the common case (when no format change is
// needed), REGEN_TRIGGER is used to store the specific values of the
// hundred-millions and billions digits that mean a change is needed,
// formatted as two repeats of billions, hundred-millions, 9, 9 in
// high-decimal (thus, when using normal format, REGEN_TRIGGER is
// high-decimal 99999999, i.e. -1 when interpreted as binary). The 9s
// are because vpshufd doesn't have very good resolution: the millions
// and ten-millions digits get read too, but can simply just be masked
// out. The two repeats are to ensure that both halves of LINENO_MID
// (the even-hundreds-digit and odd-hundreds-digit halves) have the
// correct value while changing (changing the format while half the
// register still ended ...98999999 would produce incorrect output).
vpshufd %xmm0, LINENO_MIDx, 0xED
vpextrq %rax, %xmm0, 0
mov %rdx, 0x0000ffff0000ffff
or %rax, %rdx
cmp %rax, REGEN_TRIGGER
jne calculate_main_loop_iterations

cmp REGEN_TRIGGER, -1
jne switch_to_normal_representation


switch_to_alternate_representation:
// Count the number of 9s at the end of LINENO_TOP. To fix an edge
// case, the top bit of LINENO_TOP is interpreted as a 0, preventing
// a 9 being recognised there (this causes 10**18-1 to increment to
// 10**17 rather than 10**18, but the program immediately exits
// before this can become a problem).
vpextrq %rdx, LINENO_TOPx, 1
mov SPILL, %rdx
shl %rdx, 1
shr %rdx, 1
not %rdx
bsf %rcx, %rdx
and %rcx, -8

// Change the format of LINENO_TOP so that the digit above the
// consecutive 9s becomes a reference to the top byte of LINENO_MID,
// and the 9s themselves references to the hundred-millions digit.
// This is done via a lookup table that specifies how to move the
// bytes around.
.section .rodata
alternate_representation_lookup_table:
.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 7, 9, 10, 11, 12, 13, 14, 15
.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 7, 9, 10, 11, 12, 13, 14, 15

.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 7, 10, 11, 12, 13, 14, 15
.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 7, 10, 11, 12, 13, 14, 15

.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 7, 11, 12, 13, 14, 15
.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 7, 11, 12, 13, 14, 15

.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 6, 7, 12, 13, 14, 15
.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 6, 7, 12, 13, 14, 15

.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 6, 6, 7, 13, 14, 15
.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 6, 6, 7, 13, 14, 15

.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 6, 6, 6, 7, 14, 15
.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 6, 6, 6, 7, 14, 15

.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 6, 6, 6, 6, 7, 15
.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 6, 6, 6, 6, 7, 15

.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 6, 6, 6, 6, 6, 7
.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 6, 6, 6, 6, 6, 7
.text

lea %rax, [%rip + alternate_representation_lookup_table]
vpshufb LINENO_TOP, LINENO_TOP, [%rax + 4 * %rcx]

// The top byte of LINENO_MID also needs the appropriate digit of
// LINENO_TOP placed there.
mov %rdx, SPILL
shr %rdx, %cl
vpinsrb LINENO_MIDx, LINENO_MIDx, %edx, 7
vpinsrb LINENO_MIDx, LINENO_MIDx, %edx, 15
vpermq LINENO_MID, LINENO_MID, 0x44

// Finally, REGEN_TRIGGER needs to store the pattern of digits that
// will prompt a shift back to the normal representation (the hundred-
// millions digit must be 0, and the value of the billions digit will
// be predictable).
inc %edx
shl %edx, 24
or %edx, 0xF6FFFF
mov REGEN_TRIGGERe, %edx
shl %rdx, 32
or REGEN_TRIGGER, %rdx
jmp generate_bytecode


switch_to_normal_representation:
// Switching back is fairly easy: LINENO_TOP can almost be converted
// back into its usual format by running the bytecode program stored
// there to remove any unusual references into LINENO_MID, then
// restoring the usual references manually. Running the program will
// unfortunately convert high-decimal to ASCII (or in this case zeroes
// because there's no need to do the subtraction), but that can be
// worked around by taking the bytewise maximum of the converted and
// original LINENO_TOP values (high-decimal is higher than bytecode
// references and much higher than zero).
vpsubb %ymm2, BASCII_OFFSET, LINENO_TOP
vpshufb %ymm0, LINENO_MID, %ymm2
vpmaxub LINENO_TOP, LINENO_TOP, %ymm0

// Manually fix the constant parts of lineno_top to contain their
// usual constant values
.section .rodata
lineno_top_max:
.byte 198, 197, 196, 195, 194, 193, 192, 191
.byte 255, 255, 255, 255, 255, 255, 255, 255
.byte 190, 189, 188, 187, 186, 185, 184, 183
.byte 255, 255, 255, 255, 255, 255, 255, 255
.text
vpminub LINENO_TOP, LINENO_TOP_MAX, LINENO_TOP

// The billions digit of LINENO_MID needs to be set back to 0 (which
// is its true value at this point: the same as the hundred-thousands
// digit, which is also 0).
vpsllq LINENO_MID, LINENO_MID, 8
vpsrlq LINENO_MID, LINENO_MID, 8
vpmaxub LINENO_MID, LINENO_MID_BASE, LINENO_MID

mov REGEN_TRIGGER, -1

jmp generate_bytecode


///// Fourth phase
//
// Ending at 999999999999999999 lines would be a little unsatisfying,
// so here's a routine to write the quintillionth line and exit.
//
// It's a "Buzz", which we can steal from the first phase's constant.

fourth_phase:

mov ARG1e, 1
lea ARG2, [%rip + fizzbuzz_intro + 11]
mov ARG3, 5
mov SYSCALL_NUMBER, __NR_write
syscall
call exit_on_error
xor ARG1e, ARG1e
jmp exit


///// Error handling code
//
// This doesn't run in a normal execution of the program, and isn't
// particularly optimised; I didn't comment it much because it isn't
// very interesting and also is fairly self-explanatory.

write_stderr:
mov ARG1e, 2
mov SYSCALL_NUMBER, __NR_write
syscall
ret

inefficiently_write_as_hex:
push %rax
push %rcx
shr %rax, %cl
and %rax, 0xF
.section .rodata
hexdigits: .ascii "0123456789ABCDEF"
.text
lea %rcx, [%rip + hexdigits]
movzx %rax, byte ptr [%rcx + %rax]
mov [%rip + error_write_buffer], %al
lea ARG2, [%rip + error_write_buffer]
mov ARG3e, 1
call write_stderr
pop %rcx
pop %rax
sub %ecx, 4
jns inefficiently_write_as_hex
ret

exit_on_error:
test SYSCALL_RETURN, SYSCALL_RETURN
js 1f
ret

.section .rodata
error_message_part_1: .ascii "Encountered OS error 0x"
error_message_part_2: .ascii " at RIP 0x"
error_message_part_3: .ascii ", exiting program.\n"
.text

1: push SYSCALL_RETURN
lea ARG2, [%rip + error_message_part_1]
mov ARG3e, 23
call write_stderr
pop SYSCALL_RETURN
neg SYSCALL_RETURN
mov %rcx, 8
call inefficiently_write_as_hex
lea ARG2, [%rip + error_message_part_2]
mov ARG3e, 10
call write_stderr
pop %rax  // find the caller's %rip from the stack
sub %rax, 5  // `call exit_on_error` compiles to 5 bytes
mov %rcx, 60
call inefficiently_write_as_hex
lea ARG2, [%rip + error_message_part_3]
mov ARG3e, 19
call write_stderr
mov ARG1e, 74
// fall through

exit:
mov SYSCALL_NUMBER, __NR_exit_group
syscall
ud2

.section .rodata
cpuid_error_message:
.ascii "Error: your CPUID command does not support command "
.ascii "0x80000006 (AMD-style L2 cache information).\n"
.text
bad_cpuid_error:
lea ARG2, [%rip + cpuid_error_message]
mov ARG3e, 96
call write_stderr
mov ARG1e, 59
jmp exit

.section .rodata
pipe_error_message:
.ascii "This program can only output to a pipe "
.ascii "(try piping into `cat`?)\n"
.text
pipe_error:
lea ARG2, [%rip + pipe_error_message]
mov ARG3e, 64
call write_stderr
mov ARG1e, 73
jmp exit

.section .rodata
pipe_perm_error_message_part_1:
.ascii "Cannot allocate a sufficiently large kernel buffer.\n"
.ascii "Try setting /proc/sys/fs/pipe-max-size to 0x"
pipe_perm_error_message_part_2: .ascii ".\n"
.text
pipe_perm_error:
lea ARG2, [%rip + pipe_perm_error_message_part_1]
mov ARG3e, 96
call write_stderr
mov %rax, PIPE_SIZE
mov %ecx, 28
call inefficiently_write_as_hex
lea ARG2, [%rip + pipe_perm_error_message_part_2]
mov ARG3e, 2
call write_stderr
mov ARG1e, 77
jmp exit

.section .rodata
pipe_size_error_message_part_1:
.ascii "Failed to resize the kernel pipe buffer.\n"
.ascii "Requested size: 0x"
pipe_size_error_message_part_2: .ascii "\nActual size: 0x"
pipe_size_error_message_part_3:
.ascii "\n(If the buffer is too large, this may cause errors;"
.ascii "\nthe program could run too far ahead and overwrite"
.ascii "\nmemory before it had been read from.)\n"
.text
pipe_size_mismatch_error:
push SYSCALL_RETURN
lea ARG2, [%rip + pipe_size_error_message_part_1]
mov ARG3e, 59
call write_stderr
mov %rax, PIPE_SIZE
mov %ecx, 28
call inefficiently_write_as_hex
lea ARG2, [%rip + pipe_size_error_message_part_2]
mov ARG3e, 16
call write_stderr
pop %rax
mov %ecx, 28
call inefficiently_write_as_hex
lea ARG2, [%rip + pipe_size_error_message_part_3]
mov ARG3e, 141
call write_stderr
mov ARG1e, 73
jmp exit
</code></pre>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Are people in tech inside an AI echo chamber? (274 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=36567918</link>
            <guid>36567918</guid>
            <pubDate>Mon, 03 Jul 2023 02:17:29 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=36567918">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="36567918">
      <td><span></span></td>      <td><center><a id="up_36567918" href="https://news.ycombinator.com/vote?id=36567918&amp;how=up&amp;goto=item%3Fid%3D36567918"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=36567918">Ask HN: Are people in tech inside an AI echo chamber?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_36567918">120 points</span> by <a href="https://news.ycombinator.com/user?id=freelanddev">freelanddev</a> <span title="2023-07-03T02:17:29"><a href="https://news.ycombinator.com/item?id=36567918">10 hours ago</a></span> <span id="unv_36567918"></span> | <a href="https://news.ycombinator.com/hide?id=36567918&amp;goto=item%3Fid%3D36567918">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Are%20people%20in%20tech%20inside%20an%20AI%20echo%20chamber%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=36567918&amp;auth=8a337da02758aaf505247c50497f3a405fe6d8b3">favorite</a> | <a href="https://news.ycombinator.com/item?id=36567918">198&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><p>I recently spoke with a friend who is not in the tech space and he hadn’t even heard of ChatGPT. He’s a millennial &amp; a white collar worker and smart. I have had conversations with non-tech people about ChatGPT/AI, but not very frequently, which led me to think, are we just in an echo chamber? Not that this would be a bad thing, as we’re all quite aware that AI will play an increasing role in our lives (in &amp; out of the office), but maybe AI mainstream adoption will take longer than we anticipate. What do you think?</p></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla on track to smash targets after producing almost a million EVs in 6 months (169 pts)]]></title>
            <link>https://thedriven.io/2023/07/03/tesla-on-track-to-smash-targets-after-producing-almost-a-million-evs-in-first-6-months/</link>
            <guid>36567725</guid>
            <pubDate>Mon, 03 Jul 2023 01:43:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thedriven.io/2023/07/03/tesla-on-track-to-smash-targets-after-producing-almost-a-million-evs-in-first-6-months/">https://thedriven.io/2023/07/03/tesla-on-track-to-smash-targets-after-producing-almost-a-million-evs-in-first-6-months/</a>, See on <a href="https://news.ycombinator.com/item?id=36567725">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">

		
		<main id="main" role="main">

			
			
				
				<article id="post-170072">
				

							
			<section>
		<figure>
			<a href="https://thedriven.io/wp-content/uploads/2023/07/Tesla-Q2-results_03.jpg">
				<img width="800" height="437" src="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-800x437.jpg?lossy=1&amp;strip=0&amp;webp=1" alt="Tesla Model 3" decoding="async" loading="lazy" data-srcset="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-90x49.jpg?lossy=1&amp;strip=0&amp;webp=1 90w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-120x66.jpg?lossy=1&amp;strip=0&amp;webp=1 120w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-150x82.jpg?lossy=1&amp;strip=0&amp;webp=1 150w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-180x98.jpg?lossy=1&amp;strip=0&amp;webp=1 180w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-240x131.jpg?lossy=1&amp;strip=0&amp;webp=1 240w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-300x164.jpg?lossy=1&amp;strip=0&amp;webp=1 300w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-320x175.jpg?lossy=1&amp;strip=0&amp;webp=1 320w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-388x212.jpg?lossy=1&amp;strip=0&amp;webp=1 388w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03.jpg?size=464x253&amp;lossy=1&amp;strip=0&amp;webp=1 464w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-560x306.jpg?lossy=1&amp;strip=0&amp;webp=1 560w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-640x350.jpg?lossy=1&amp;strip=0&amp;webp=1 640w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03.jpg?size=696x380&amp;lossy=1&amp;strip=0&amp;webp=1 696w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-800x437.jpg?lossy=1&amp;strip=0&amp;webp=1 800w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-1120x612.jpg?lossy=1&amp;strip=0&amp;webp=1 1120w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-1160x634.jpg?lossy=1&amp;strip=0&amp;webp=1 1160w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03.jpg?lossy=1&amp;strip=0&amp;webp=1 1200w" data-sizes="(max-width: 800px) 100vw, 800px" data-src="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-800x437.jpg?lossy=1&amp;strip=0&amp;webp=1">			</a>
						<figcaption>Tesla Model 3. Source: Unsplash – Vlad Tchompalov</figcaption>
					</figure>
	</section>

					<div>

										
			
							<section>

								<p>Tesla produced 479,700 electric vehicles in the second quarter of 2023 and delivered 466,140, smashing both quarterly production and sales records.</p>
<p>For the first half of the year Tesla has produced 920,508 EVs meaning the company is well placed to achieve its target of producing 2 million in 2023.</p>
<p>As expected, the Model 3 and Model Y made up 96% of Tesla’s production, with the Model S and X flagship cars accounting for just 4%.</p>
<p>Tesla’s Q2 sales were up 10% quarter-on-quarter from Q1’s previous record of 422,875. Year-on-year Q2 sales were up a staggering 83% over Q2 2022, as shown in the graph below from <a href="https://twitter.com/piloly/status/1675535815164805121">Roland Pircher.</a></p>

<figure id="attachment_170076" aria-describedby="caption-attachment-170076"><a href="https://thedriven.io/wp-content/uploads/2023/07/Tesla-Q2-results_01.jpg"><img loading="lazy" decoding="async" src="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01.jpg?lossy=1&amp;strip=0&amp;webp=1" alt="Tesla global quarterly sales" width="1200" height="675" srcset="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-90x51.jpg?lossy=1&amp;strip=0&amp;webp=1 90w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-120x68.jpg?lossy=1&amp;strip=0&amp;webp=1 120w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-150x84.jpg?lossy=1&amp;strip=0&amp;webp=1 150w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-180x101.jpg?lossy=1&amp;strip=0&amp;webp=1 180w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-240x135.jpg?lossy=1&amp;strip=0&amp;webp=1 240w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-300x169.jpg?lossy=1&amp;strip=0&amp;webp=1 300w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-320x180.jpg?lossy=1&amp;strip=0&amp;webp=1 320w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-388x218.jpg?lossy=1&amp;strip=0&amp;webp=1 388w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01.jpg?size=464x261&amp;lossy=1&amp;strip=0&amp;webp=1 464w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-560x315.jpg?lossy=1&amp;strip=0&amp;webp=1 560w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-640x360.jpg?lossy=1&amp;strip=0&amp;webp=1 640w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01.jpg?size=696x392&amp;lossy=1&amp;strip=0&amp;webp=1 696w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-800x450.jpg?lossy=1&amp;strip=0&amp;webp=1 800w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01.jpg?size=928x522&amp;lossy=1&amp;strip=0&amp;webp=1 928w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-1120x630.jpg?lossy=1&amp;strip=0&amp;webp=1 1120w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-1160x653.jpg?lossy=1&amp;strip=0&amp;webp=1 1160w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01.jpg?lossy=1&amp;strip=0&amp;webp=1 1200w" sizes="(max-width: 1160px) 100vw, 1160px" data-old-src="https://thedriven.io/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-srcset="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-90x51.jpg?lossy=1&amp;strip=0&amp;webp=1 90w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-120x68.jpg?lossy=1&amp;strip=0&amp;webp=1 120w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-150x84.jpg?lossy=1&amp;strip=0&amp;webp=1 150w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-180x101.jpg?lossy=1&amp;strip=0&amp;webp=1 180w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-240x135.jpg?lossy=1&amp;strip=0&amp;webp=1 240w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-300x169.jpg?lossy=1&amp;strip=0&amp;webp=1 300w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-320x180.jpg?lossy=1&amp;strip=0&amp;webp=1 320w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-388x218.jpg?lossy=1&amp;strip=0&amp;webp=1 388w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01.jpg?size=464x261&amp;lossy=1&amp;strip=0&amp;webp=1 464w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-560x315.jpg?lossy=1&amp;strip=0&amp;webp=1 560w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-640x360.jpg?lossy=1&amp;strip=0&amp;webp=1 640w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01.jpg?size=696x392&amp;lossy=1&amp;strip=0&amp;webp=1 696w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-800x450.jpg?lossy=1&amp;strip=0&amp;webp=1 800w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01.jpg?size=928x522&amp;lossy=1&amp;strip=0&amp;webp=1 928w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-1120x630.jpg?lossy=1&amp;strip=0&amp;webp=1 1120w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-1160x653.jpg?lossy=1&amp;strip=0&amp;webp=1 1160w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01.jpg?lossy=1&amp;strip=0&amp;webp=1 1200w" data-src="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01.jpg?lossy=1&amp;strip=0&amp;webp=1"></a><figcaption id="caption-attachment-170076">Tesla global quarterly sales. Source: <a href="https://twitter.com/piloly/status/1675535815164805121">@piloly</a></figcaption></figure>
<p>Looking at production in Q2, Tesla produced 479,700 EVs, up 13% on Q1’s figure of 440,808. If Tesla can maintain that level of quarter-on-quarter production growth for the rest of this year, it will hit around 542,000 for Q3 and 612,500 for Q4.</p>
<p>This will put total 2023 production numbers at around 2.1 million, exceeding Tesla’s stated goal of 1.8 to 2 million.</p>
<p>If Tesla does manage to produce 2.1 million EVs for 2023, it will represent an annualised growth rate of around 60%, which is also above Tesla’s long-term target of 50% annualised production growth.</p>
<p>To get a sense of Tesla’s exponential sales growth, James Stephenson graphed Q2 trailing 12-month global deliveries over the past 10 years.</p>
<p>Based the Q2 trailing 12-month figures, Tesla sales grew 47% in 2022-23, 58% in 2021-22 and 82% in 2020-21.</p>

<figure id="attachment_170078" aria-describedby="caption-attachment-170078"><a href="https://thedriven.io/wp-content/uploads/2023/07/Tesla-Q2-results_02.jpg"><img loading="lazy" decoding="async" src="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02.jpg?lossy=1&amp;strip=0&amp;webp=1" alt="Trailing 12-month global Tesla deliveries Q2 2013 to Q2 2023" width="1047" height="1408" srcset="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-90x120.jpg?lossy=1&amp;strip=0&amp;webp=1 90w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-120x160.jpg?lossy=1&amp;strip=0&amp;webp=1 120w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-150x202.jpg?lossy=1&amp;strip=0&amp;webp=1 150w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-180x242.jpg?lossy=1&amp;strip=0&amp;webp=1 180w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-240x323.jpg?lossy=1&amp;strip=0&amp;webp=1 240w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-297x400.jpg?lossy=1&amp;strip=0&amp;webp=1 297w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-300x403.jpg?lossy=1&amp;strip=0&amp;webp=1 300w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-320x430.jpg?lossy=1&amp;strip=0&amp;webp=1 320w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02.jpg?size=464x624&amp;lossy=1&amp;strip=0&amp;webp=1 464w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-560x753.jpg?lossy=1&amp;strip=0&amp;webp=1 560w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-640x861.jpg?lossy=1&amp;strip=0&amp;webp=1 640w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02.jpg?size=696x936&amp;lossy=1&amp;strip=0&amp;webp=1 696w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-800x1076.jpg?lossy=1&amp;strip=0&amp;webp=1 800w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02.jpg?size=928x1248&amp;lossy=1&amp;strip=0&amp;webp=1 928w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02.jpg?lossy=1&amp;strip=0&amp;webp=1 1047w" sizes="(max-width: 1047px) 100vw, 1047px" data-old-src="https://thedriven.io/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-srcset="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-90x120.jpg?lossy=1&amp;strip=0&amp;webp=1 90w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-120x160.jpg?lossy=1&amp;strip=0&amp;webp=1 120w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-150x202.jpg?lossy=1&amp;strip=0&amp;webp=1 150w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-180x242.jpg?lossy=1&amp;strip=0&amp;webp=1 180w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-240x323.jpg?lossy=1&amp;strip=0&amp;webp=1 240w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-297x400.jpg?lossy=1&amp;strip=0&amp;webp=1 297w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-300x403.jpg?lossy=1&amp;strip=0&amp;webp=1 300w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-320x430.jpg?lossy=1&amp;strip=0&amp;webp=1 320w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02.jpg?size=464x624&amp;lossy=1&amp;strip=0&amp;webp=1 464w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-560x753.jpg?lossy=1&amp;strip=0&amp;webp=1 560w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-640x861.jpg?lossy=1&amp;strip=0&amp;webp=1 640w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02.jpg?size=696x936&amp;lossy=1&amp;strip=0&amp;webp=1 696w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-800x1076.jpg?lossy=1&amp;strip=0&amp;webp=1 800w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02.jpg?size=928x1248&amp;lossy=1&amp;strip=0&amp;webp=1 928w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02.jpg?lossy=1&amp;strip=0&amp;webp=1 1047w" data-src="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02.jpg?lossy=1&amp;strip=0&amp;webp=1"></a><figcaption id="caption-attachment-170078">Trailing 12-month global Tesla deliveries Q2 2013 to Q2 2023. Source: <a href="https://twitter.com/ICannot_Enough/status/1675543214638465026">@ICannot_Enough</a></figcaption></figure>

<div itemtype="http://schema.org/Person" itemscope="" itemprop="author"><p><img loading="lazy" src="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442.jpeg?lossy=1&amp;strip=0&amp;webp=1" width="100" height="100" alt="Daniel Bleakley Profile Picture" itemprop="image" srcset="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442-80x80.jpeg?lossy=1&amp;strip=0&amp;webp=1 80w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442-90x90.jpeg?lossy=1&amp;strip=0&amp;webp=1 90w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442-120x120.jpeg?lossy=1&amp;strip=0&amp;webp=1 120w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442-160x160.jpeg?lossy=1&amp;strip=0&amp;webp=1 160w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442-180x180.jpeg?lossy=1&amp;strip=0&amp;webp=1 180w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442-240x240.jpeg?lossy=1&amp;strip=0&amp;webp=1 240w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442-320x320.jpeg?lossy=1&amp;strip=0&amp;webp=1 320w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442-388x388.jpeg?lossy=1&amp;strip=0&amp;webp=1 388w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442.jpeg?size=464x464&amp;lossy=1&amp;strip=0&amp;webp=1 464w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442-560x560.jpeg?lossy=1&amp;strip=0&amp;webp=1 560w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442-640x640.jpeg?lossy=1&amp;strip=0&amp;webp=1 640w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442.jpeg?size=696x696&amp;lossy=1&amp;strip=0&amp;webp=1 696w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442.jpeg?lossy=1&amp;strip=0&amp;webp=1 800w" sizes="(max-width: 800px) 100vw, 800px" data-old-src="https://thedriven.io/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-src="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442.jpeg?size=100x100&amp;lossy=1&amp;strip=0&amp;webp=1"></p><div><p>Daniel Bleakley is a clean technology researcher and advocate with a background in engineering and business. He has a strong interest in electric vehicles, renewable energy, manufacturing and public policy.</p></div></div><!-- AI CONTENT END 1 -->

								
							</section>

										
			
						</div><!-- .entry-wrap -->

					


				</article>

				
				
			
		</main>

		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lumia WOA Project – Windows 10 or Windows 11 Desktop OS for Lumia 950/XL (165 pts)]]></title>
            <link>https://woa-project.github.io/LumiaWOA/</link>
            <guid>36566999</guid>
            <pubDate>Sun, 02 Jul 2023 23:42:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://woa-project.github.io/LumiaWOA/">https://woa-project.github.io/LumiaWOA/</a>, See on <a href="https://news.ycombinator.com/item?id=36566999">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-role="main">
        <div id="hero">
            <p>
                
                <h4>Full Windows for Lumia</h4>
            </p>
        </div>
        <div>
                <h3>Caution</h3>
                <h6>Experimental firmware ahead</h6>
                <p>The firmware provided is for testing purposes only. We aren't responsible for any data loss caused by
                    the firmware images. Make backups of your data prior to installing.</p>
                <p><strong>This software has not been approved for use with emergency services. By installing this
                        software, you agree to not use it as your primary phone device due to possible disruption in
                        emergency service access.</strong></p>
            </div>
        <div>
            <r-grid columns="2">
                <r-cell span-s="row">
                    
                    <h2>The Windows You Know and Love</h2>
                    <p>This project brings the Windows 10 or Windows 11 desktop operating system to your Lumia 950 and Lumia 950 XL.
                    </p>
                    <p>It's the same edition of Windows you're used to on your traditional laptop or desktop
                        computer, but it's the version for ARM64 (armv8a) processors.</p>
                    <p>It can run ARM64, ARM, x86 and x64 applications (the last two via emulation) just fine.<sup>1</sup></p>
                </r-cell>
                <r-cell span-s="row">
                    <figure>
                        <div>
                            <picture>
                                <source srcset="https://woa-project.github.io/LumiaWOA/img/Desktop.webp" type="image/webp">
                                <img loading="lazy" src="https://woa-project.github.io/LumiaWOA/img/Desktop.png" alt="A Lumia 950">
                            </picture>
                            
                        </div>
                    </figure>
                </r-cell>
            </r-grid>
        </div>
        <div>
            <figure>
                <div>
                    <picture>
                        <source srcset="https://woa-project.github.io/LumiaWOA/img/phone_continuum.webp" type="image/webp">
                        <img loading="lazy" src="https://woa-project.github.io/LumiaWOA/img/phone_continuum.png" alt="A Lumia 950">
                    </picture>
                    
                </div>
            </figure>
            <div>
                <h2></h2>
                <h2>With Continuum<sup>2</sup></h2>
                <figure>
                    <p><img src="https://woa-project.github.io/LumiaWOA/img/desktop_continuum.png">
                        
                    </p>
                </figure>
            </div>
        </div>
        <r-grid columns="2">
            <r-cell span="1" span-s="row">
                <h2></h2>
                <h2>A Mobile Twist</h2>
                <div>
                    <p>MobileShell is a fully-featured adaptive shell aiming to mimic the appearance of Windows
                        Mobile.</p>
                    <p>MobileShell brings back the navigation bar, status bar, puts your notification toasts at the top,
                        status icons at a glance, and activates only when your phone is in tablet mode.</p>
                        
                </div>
            </r-cell>
            <r-cell span="1" span-s="row">
                <figure>
                    <div>
                        <picture>
                            <source srcset="https://woa-project.github.io/LumiaWOA/img/MobilePortrait.webp" type="image/webp">
                            <img loading="lazy" src="https://woa-project.github.io/LumiaWOA/img/MobilePortrait.png" alt="A Lumia 950">
                        </picture>
                        
                    </div>
                </figure>
            </r-cell>
            <r-cell span="1" span-s="row">
                <figure>
                    <p><img src="https://woa-project.github.io/LumiaWOA/img/MobileLandscape.png">
                        
                    </p>
                </figure>
            </r-cell>
            <r-cell span="1" span-s="row">
                <p>MobileShell also supports landscape mode, adjusting perfectly to the phone's current state. Mobile
                    Shell is made by @ADeltaX and is included by default! You can also download it from the Microsoft
                    Store:</p>
                
            </r-cell>
        </r-grid>
        <div>
            <r-grid columns="2">
                <r-cell span-s="row">
                    <h2></h2>
                    <h2>Say Hello...<sup>3</sup></h2>
                    <div>
                        <p>This project backports the cellular stack from Windows 10 Mobile to Windows desktop. On
                            supported versions of Windows, you can make calls, texts, and browse the internet using a
                            cellular connection.</p>
                        <p>Dialer (previously WOA Dialer) is our custom app that allows you to make and manage calls on your device. 
                            Dialer is bundled with the project by default, along with the classic Microsoft Phone app.</p>
                    </div>
                    
                </r-cell>
                <r-cell span-s="row">
                    <figure>
                        <div>
                            <picture>
                                <source srcset="https://woa-project.github.io/LumiaWOA/img/Dialer.webp" type="image/webp">
                                <img loading="lazy" src="https://woa-project.github.io/LumiaWOA/img/Dialer.png" alt="A Lumia 950">
                            </picture>
                            
                        </div>
                    </figure>
                </r-cell>
            </r-grid>
        </div>
        <div>
            <r-grid columns="2">
                <r-cell span-s="row">
                    <figure>
                        <div>
                            <picture>
                                <source srcset="https://woa-project.github.io/LumiaWOA/img/Chat.webp" type="image/webp">
                                <img loading="lazy" src="https://woa-project.github.io/LumiaWOA/img/Chat.png" alt="A Lumia 950">
                            </picture>
                            
                        </div>
                    </figure>
                </r-cell>
                <r-cell span-s="row">
                    <h2></h2>
                    <h2>...Or Send a Message<sup>3</sup></h2>
                    <p>With the Chat application, you can recieve and send SMS messages. MMS messages remain unsupported
                        as of now.</p>
                    
                </r-cell>
            </r-grid>
        </div>
        <r-grid columns="2">
            <r-cell span="1-2" span-s="row">
                <h2>And Much More!</h2>
            </r-cell>
            <r-cell span-s="row">
                <h4>WOA Deployer</h4>
                <p>WOA Deployer allows you to deploy with ease Windows Desktop to your device, and enabling Dual Boot
                    with 2 clicks. You can pick the windows release you want, the language you want.</p>
                <a href="https://github.com/WOA-Project/WOA-Deployer-Lumia">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>BootShim</h4>
                <p>BootShim is the UEFI bootstraper. It escalates the SoC to AArch64 and starts our UEFI.</p>
                <a href="https://github.com/imbushuo/boot-shim">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Lumia950XlPkg</h4>
                <p>Lumia950XlPkg is our EDK2 port for the Lumia 950 and Lumia 950 XL. It enables us to bootstrap Windows
                    10/11 Desktop for ARM64 processors on the Lumia.</p>
                <a href="https://github.com/WOA-Project/Lumia950XLPkg">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Lumia Drivers</h4>
                <p>Lumia Drivers is the repository hosting all driver files for Windows, and INF files which had to be
                    recreated. Some additional driver patching is also done here to make things work the way they
                    should.</p>
                <a href="https://github.com/WOA-Project/Lumia-Drivers">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Lumia USB-C</h4>
                <p>Lumia USB-C is the recreation of the USB C driver for Lumia devices. The Lumia 950 USB-C solution is
                    proprietary and personalized, thus the need for a custom driver.</p>
                <a href="https://github.com/WOA-Project/LumiaUSBC">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Color Profile</h4>
                <p>Color Profile is the stack managing the personalization of the display color tint, saturation and
                    contrast.</p>
                <a href="https://github.com/WOA-Project/ColorProfile">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Advanced Info</h4>
                <p>Advanced Info displays information about your device, within the settings app.</p>
                <a href="https://github.com/WOA-Project/AdvancedInfo">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Airwaves</h4>
                <p>Airwaves allows you to listen to FM radio, right from your phone.</p>
                <a href="https://github.com/tigerw/FM-Radio-Frontend">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>RIL Init Service</h4>
                <p>RIL Init Service allows you to have the Radio Interface Layer initialized on newer versions of
                    Windows 10/11.</p>
                <a href="https://github.com/WOA-Project/RILServiceInit">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Auto Brightness Service</h4>
                <p>The auto brightness service allows you to have automatic brightness on your device.</p>
                <a href="https://github.com/WOA-Project/AutoBrightnessSvc">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Auto Rotation Service</h4>
                <p>The auto rotation service allows you to have automatic rotation on your device.</p>
                <a href="https://github.com/WOA-Project/AutoRotate">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Vibrations</h4>
                <p>The vibration stack allows you to have haptic vibrations once you get a notification, and control the
                    intensity of the vibration via a settings application.</p>
                <a href="https://github.com/WOA-Project/VibrationSettings">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>USB Function Mode Switcher</h4>
                <p>USB Function Mode Switcher allows you to switch USB function modes.</p>
                <a href="https://github.com/WOA-Project/USBFunctionModeSwitcher">View on
                    GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Data Management Service</h4>
                <p>The data management service enables cellular data connections automatically.</p>
                <a href="https://github.com/WOA-Project/DataManagementSvc">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Power Supply Notifier</h4>
                <p>Power Supply Notifier plays a sound when your device starts charging.</p>
                <a href="https://github.com/WOA-Project/PowerSupplyNotifier">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>SynapticsTouch</h4>
                <p>The Synaptics Touch driver enables touch on your device</p>
                <a href="https://github.com/imbushuo/SynapticsTouch">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Display Dock Flyout</h4>
                <p>The Display Dock Flyout displays information about a connected Display Dock (HD-500)</p>
                <a href="https://github.com/WOA-Project/DisplayDockFlyout">View on GitHub</a>
            </r-cell>
        </r-grid>
        <div>
            <p><sup>1</sup> Applications compiled for the AMD64/x86-64 architecture are 
                supported only on build 21277+.</p>
            <p><sup>2</sup> Continuum currently only works wirelessly over Miracast.</p>
            <p><sup>3</sup> Cellular support is still unfinished and might be broken in some
                areas. Cellular calls are automatically enabled in up to Windows 10 November 2019 Update (version 19H2, build
                18363). Versions higher than this will <strong>only</strong> support cellular data. You can manually enable 
                calls on builds higher than 18363 by using <a href="https://woa-project.github.io/LumiaWOA/guides/ican0/">this guide</a>. SMS are supported up to Windows 10 November 2019 Update 
                (version 19H2, build 18363). Dual SIM devices may have issues fetching the default Carrier APN settings, a provisioning 
                package for APN may be required. The advanced settings page for Cellular in the Windows Settings app may crash. 
                Your experience will vary between carriers and devices. This software stack has not been approved for use with emergency services.
                As a consequence <strong>it should not be used as your primary way of communication</strong>. VoLTE (IMS) stack while
                present is not functional.</p>
        </div>
    </div><div>
            <p>© 2017-2021 The Lumia WOA Authors</p>
            <p>Snapdragon is a registered trademark of Qualcomm Incorporated. Microsoft, the Microsoft Corporate Logo,
                Windows, Lumia, Windows Hello, Continuum, Hyper-V, and DirectX are registered trademarks of Microsoft
                Corporation in the United States. Miracast is a registered trademark of the Wi-Fi Alliance. Other
                binaries may be copyright Qualcomm Incorporated and Microsoft Mobile.</p>
            <p>Hello from San Francisco (US), France, Italy, Germany, Spain, Hungary. Site built by @itsmichaelwest.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Xerox Smalltalk-80 GUI Was Weird (123 pts)]]></title>
            <link>https://collindonnell.com/the-xerox-smalltalk-80-gui-was-weird</link>
            <guid>36566638</guid>
            <pubDate>Sun, 02 Jul 2023 22:51:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://collindonnell.com/the-xerox-smalltalk-80-gui-was-weird">https://collindonnell.com/the-xerox-smalltalk-80-gui-was-weird</a>, See on <a href="https://news.ycombinator.com/item?id=36566638">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>As part of my ongoing interest in the origins of object-oriented programming and design patterns like MVC, I started to think the best way to fully grasp these things was to go back to the beginning. While there are modern Smalltalk’s like Squeak, Pharo, and Cuis that retain a lot of the old-school vibe, they’re also pretty different.</p>

<p>I could go into detail on how all of those options aren’t quite what I’m looking for, but since I want to write about that later, the short version for now is that I started looking into how you could emulate the original Smalltalk-80 environment on modern hardware. That ended up being incredibly easy when I found <a href="https://github.com/dbanay/Smalltalk">this</a> “by the Bluebook” implementation of it on GitHub.</p>

<p>After a couple minutes installing, I got it running and saw this:</p>

<p><img src="https://i.snap.as/ZzWRwDlc.png" alt="Smalltalk-80 first run"></p>

<p>At first glance, this looks <em>incredibly</em> similar to something like the desktop of the <a href="https://en.wikipedia.org/wiki/Apple_Lisa">Apple Lisa</a> or <a href="https://en.wikipedia.org/wiki/Classic_Mac_OS">early Mac OS</a>. It’s easy to see why people might think that Apple sort of <a href="https://www.youtube.com/watch?v=CBri-xgYvHQ">stole the graphical user interface</a> from it’s rich neighbor Xerox. It’s not true, though.</p>

<p>The first thing is that the Smalltalk environment wasn't really an operating system the same way something like Mac OS was. It’s more like an IDE that runs on bare hardware.</p>

<p>For example, there’s no traditional file system. Smalltalk environments were stored as images which contained the entire state of the system. That means all of the objects, code, data, whatever, were stored as Smalltalk objects. The creators of Smalltalk wanted to make a system where the person using it was also modifying and programming the system as they used it. That’s really cool as a programming environment, but not really how we think of normal people using computers today.</p>

<p>There’s also no desktop, icons, or pull down menus. The whole desktop metaphor isn’t really there, because that isn’t really what it was. It’s sort of pure in an appealing way. There aren’t many different categories of things there. Windows, pop-up menus, lists. Not a lot. You can collapse different windows you’re not using and organize them on your screen, but they’re all just still windows, as opposed to also having icons for files and folders and apps and other sorts of things.</p>

<p><img src="https://i.snap.as/tJ0LRLFc.png" alt="Collapsed windows"></p>

<p>If you want to do anything with a window, you might notice there’s no controls. All window operations are done modally, meaning you click, select what you want to do, and then do that thing. You can’t even move or resize a window by clicking and dragging. There is click and drag for scrollbars and text, but not for windows. Weird.</p>

<p><img src="https://i.snap.as/Z5lx819X.png" alt="To resize a window you use the “frame” button"></p>

<p>If you wanted to move this window you would select “move”, which collapses the window and ties it to your mouse location. You then move your mouse to wherever you want the window to be and click again to place it. Oh, and you can only click, because there are no key commands in this system whatsoever.</p>

<p>Looking at all of this together, yeah, the Lisa and Macintosh took a lot from Xerox. But, they also added a lot. The Smalltalk environment was a revolutionary GUI, but it was still a system you would have had to have been a <em>computer operator</em> or something to really use. It’s not a personal computer at all.</p>

<p>The fact Apple was able to see the potential and then figure out all of the metaphors and affordances which needed to be there for regular humans to use a computer like this, and that they got so many of those things right by the time of the original Macintosh is pretty incredible.</p>

<p><a href="https://collindonnell.com/tag:programming"><span>#</span><span>programming</span></a> <a href="https://collindonnell.com/tag:apple"><span>#</span><span>apple</span></a> <a href="https://collindonnell.com/tag:smalltalk"><span>#</span><span>smalltalk</span></a> <a href="https://collindonnell.com/tag:ui"><span>#</span><span>ui</span></a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Illegal Life Pro Tip: Want to ruin your competitors business? (393 pts)]]></title>
            <link>https://oppositeinvictus.com/illegal-life-pro-tip-want-to-ruin-your-competitors-business</link>
            <guid>36566634</guid>
            <pubDate>Sun, 02 Jul 2023 22:51:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://oppositeinvictus.com/illegal-life-pro-tip-want-to-ruin-your-competitors-business">https://oppositeinvictus.com/illegal-life-pro-tip-want-to-ruin-your-competitors-business</a>, See on <a href="https://news.ycombinator.com/item?id=36566634">Hacker News</a></p>
Couldn't get https://oppositeinvictus.com/illegal-life-pro-tip-want-to-ruin-your-competitors-business: Error: incorrect header check]]></description>
        </item>
        <item>
            <title><![CDATA[Ericsson to WhatsApp: The Story of Erlang (242 pts)]]></title>
            <link>https://thechipletter.substack.com/p/ericsson-to-whatsapp-the-story-of</link>
            <guid>36566167</guid>
            <pubDate>Sun, 02 Jul 2023 21:51:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thechipletter.substack.com/p/ericsson-to-whatsapp-the-story-of">https://thechipletter.substack.com/p/ericsson-to-whatsapp-the-story-of</a>, See on <a href="https://news.ycombinator.com/item?id=36566167">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84d7445f-4d93-4120-b226-de74c97b6d63_2292x1536.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84d7445f-4d93-4120-b226-de74c97b6d63_2292x1536.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84d7445f-4d93-4120-b226-de74c97b6d63_2292x1536.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84d7445f-4d93-4120-b226-de74c97b6d63_2292x1536.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84d7445f-4d93-4120-b226-de74c97b6d63_2292x1536.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84d7445f-4d93-4120-b226-de74c97b6d63_2292x1536.jpeg" width="1456" height="976" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/84d7445f-4d93-4120-b226-de74c97b6d63_2292x1536.jpeg&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:976,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:135418,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84d7445f-4d93-4120-b226-de74c97b6d63_2292x1536.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84d7445f-4d93-4120-b226-de74c97b6d63_2292x1536.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84d7445f-4d93-4120-b226-de74c97b6d63_2292x1536.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84d7445f-4d93-4120-b226-de74c97b6d63_2292x1536.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>In this post, we’re going to look at a piece of technology that was banned by the company that created it. It was kept alive by a small team of enthusiasts. Then, almost thirty years after its first development, it became the core technology underpinning one of the most important and lucrative startups of the 2010s.</p><p>Today, it plays a key role in services that are used by billions across the world.</p><p>We’re talking about the Erlang programming language.</p><p>Why is Erlang interesting? Well, one of the themes of this newsletter is that old technology can be incredibly valuable and useful. Erlang was often seen as an esoteric curiosity, until a small group of entrepreneurial engineers used it to create a start-up that they sold for billions of dollars.</p><p>Erlang’s history also has a number of lessons. The spread of general purpose hardware and software into specialist areas like telecoms. The power of individuals and small teams using the right software tools. The resilience of open source software. And more.</p><p>This is our first post on a programming language. Except it isn’t entirely about software, it’s about how software can make the most of the hardware resources available and the links between the hardware of computing and that of telephony.</p><p>So, what is Erlang, how did it come into being, and why has it had a renaissance recently?</p><p>We’re going to start at the place where Erlang was born. In Stockholm in Sweden.</p><p>If you’re old enough, you may remember Ericsson mobile phone handsets which were a popular choice in the pre-smartphone era. Today, Ericsson’s business is focused on telecommunications equipment.</p><p>Ericsson was founded by Lars Magnus Ericsson in Stockholm, Sweden in 1876. In 1878, it started making and selling telephones and switchboards.  Over the following decades, Ericsson had mixed fortunes and, after a period of financial difficulties, it was rescued by banks controlled by the Wallenberg family. By the 1960s, the Wallenbergs had full control of Ericsson.</p><p><span>We’ve previously met Ericsson in recounting the </span><a href="https://thechipletter.substack.com/p/the-first-risc-john-cocke-and-the" rel="">story</a><span> of the first RISC computer, the IBM 801. IBM wanted to to enter the telecoms equipment market and considered joining forces with Ericsson, with the development of a new computer at the center of a possible joint venture. After a series of secretive meetings at London’s Claridges Hotel, that came to nothing.</span></p><p>But telephony in general, and Ericsson in particular, needed computing.</p><p>In 1980, a small group of Ericsson engineers - Bjarne Däcker, Mike Williams, Göran Båge and Seved Torsten-dahl - proposed creating a Computer Science Laboratory (CSL) within the company. Their objectives were to create software technology for future telecoms systems and to help introduce that technology into existing Ericsson systems.</p><p><span>Their hardware mixed industry standard computers with specialised telecoms hardware, with a </span><a href="https://en.wikipedia.org/wiki/VAX-11" rel="">DEC VAX 11/750</a><span> minicomputer linked to an Ericsson telephone exchange.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fc09f58-e02f-4ec2-8730-9a3fb48b4c16_1016x1312.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fc09f58-e02f-4ec2-8730-9a3fb48b4c16_1016x1312.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fc09f58-e02f-4ec2-8730-9a3fb48b4c16_1016x1312.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fc09f58-e02f-4ec2-8730-9a3fb48b4c16_1016x1312.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fc09f58-e02f-4ec2-8730-9a3fb48b4c16_1016x1312.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fc09f58-e02f-4ec2-8730-9a3fb48b4c16_1016x1312.png" width="1016" height="1312" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9fc09f58-e02f-4ec2-8730-9a3fb48b4c16_1016x1312.png&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1312,&quot;width&quot;:1016,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:363474,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fc09f58-e02f-4ec2-8730-9a3fb48b4c16_1016x1312.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fc09f58-e02f-4ec2-8730-9a3fb48b4c16_1016x1312.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fc09f58-e02f-4ec2-8730-9a3fb48b4c16_1016x1312.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fc09f58-e02f-4ec2-8730-9a3fb48b4c16_1016x1312.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>The original setup used to develop Erlang with a VAX 11/750 minicomputer running Unix connected to an Ericsson telephone exchange</figcaption></figure></div><p>Earlier work to program telecom systems at Ericsson had used a range of languages including PL163, CHILL and modified versions of the Pascal programming language. </p><p>Now the team started to look at a range of existing programming languages, each of which brought its own approach. These languages included then mainstream languages like Ada and lesser known ones such as Concurrent Euclid, PFL, LPL, OPS4, Frames and CLU.</p><p>As the team's work progressed, it became clear that none of these individual languages by itself would meet all the team’s requirements.</p><p>In 1985, the group was joined by Joe Armstrong and Robert Virding.</p><p>Then in 1988, the Lab moved from Ericsson to Ellemtel, a joint venture between Ericsson and the Swedish national telecoms provider Televerket.</p><p>The focus now was on building software to program AXE, Ericsson’s telephone exchange, which was programmed in a language called PLEX. The motivation was to “to make something like PLEX, to run on ordinary hardware, only better.”</p><p>Carrying forward some properties of PLEX was considered essential. In particular, according to Armstrong:</p><blockquote><p>Firstly, it should be possible to change code “on the fly;” in other words, code change operations should be possible without stopping the system.</p></blockquote><p>Software systems need to be updated and maintained with new code replacing old. A telephone system can’t be taken down completely whilst a set of code updates are introduced. Customers would not be happy if they can’t make their calls!</p><p>The language also needed to support many simultaneous activities, as calls were routed between users, and it had to be able to do this efficiently. Again, according to Armstrong, the language had to take replicate this property of telecoms switching systems:</p><blockquote><p>A switching system is made from a number of individual switches. Individual switches typically handle tens to hundreds of thousands of simultaneous calls. The switching system must be capable of handling millions of calls and must tolerate the failure of individual switches, providing uninterrupted services to the user. </p></blockquote><p>To this was added the conclusions of the earlier work to test a variety of languages. In particular:</p><ul><li><p>Small languages were thought desirable.</p></li><li><p>Functional programming was liked.</p></li><li><p>Logic programming was best in terms of elegance.</p></li><li><p>Concurrency was viewed as essential for this type of problem.</p></li></ul><p>Armstrong experimented with Smalltalk, but his attention was drawn to Prolog. Experimenting with building a system in Prolog soon led to something distinct:</p><blockquote><p>What started as an experiment in “adding concurrency to Prolog” became more of a language in its own right and this language acquired a name “Erlang,” which was probably coined by Bjarne Dacker. What did the name Erlang mean? Some said it meant “Ericsson Language,” while others claimed it was named after Agner Krarup Erlang (1878 – 1929), while we deliberately encouraged this ambiguity.</p></blockquote><p>Naming Erlang after Agner Krarup Erlang the Danish Mathematician who worked in the fields of traffic engineering and queuing theory, was surely appropriate for a language used to route telephone calls!</p><p>Robert Virding was now working with Armstrong on Erlang and by the end of 1988 most of the key ideas in Erlang had been implemented. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F508ca2bc-a5fa-44d8-9e4a-d76ddec36f92_664x592.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F508ca2bc-a5fa-44d8-9e4a-d76ddec36f92_664x592.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F508ca2bc-a5fa-44d8-9e4a-d76ddec36f92_664x592.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F508ca2bc-a5fa-44d8-9e4a-d76ddec36f92_664x592.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F508ca2bc-a5fa-44d8-9e4a-d76ddec36f92_664x592.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F508ca2bc-a5fa-44d8-9e4a-d76ddec36f92_664x592.png" width="664" height="592" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/508ca2bc-a5fa-44d8-9e4a-d76ddec36f92_664x592.png&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:592,&quot;width&quot;:664,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:37564,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F508ca2bc-a5fa-44d8-9e4a-d76ddec36f92_664x592.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F508ca2bc-a5fa-44d8-9e4a-d76ddec36f92_664x592.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F508ca2bc-a5fa-44d8-9e4a-d76ddec36f92_664x592.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F508ca2bc-a5fa-44d8-9e4a-d76ddec36f92_664x592.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>So what made Erlang different, and how did it support the Ericsson team’s requirements?</p><p>At the centre of Erlang was the idea of thousands and thousands of ‘processes’. These are separate pieces of code that run largely independently of each other. Each process communicates with other processes using ‘messages’, small amounts of data that are sent and received by each process.</p><p>Each process has a ‘mailbox’ which it can check from time to time and take action depending on the contents.</p><p>Furthermore, each process is extremely ‘lightweight’ in that it uses a very small proportion of the resources of the computer. A modern server can support millions of Erlang processes running at the same time.</p><p>The team built resilience into the language, allowing processes to fail, either due to bugs or to hardware failures, without bringing down the whole system – which of course would be a disaster for a telephony switching system.</p><p>They also built in the ability to scale. Processes can run on multiple physical machines, sending messages to other processes either running on the same machine or on other machines. So, an Erlang program can easily grow by just adding more physical machines.</p><p>Word of the development of Erlang spread, and Ericsson engineers wanted to try it out:</p><blockquote><p>The team wanted to prototype a new software architecture called ACS3 designed for programming telephony services on the Ericsson MD110 PABX4 and were looking for a suitable language for the project, which is how they got to hear about Erlang. A project called ACS / Dunder was started to build the prototype.</p></blockquote><p>When the work on ACS / Dunder was evaluated, there was a striking conclusion: development in Erlang was a lot faster than the alternatives.</p><p>Erlang was revealed to the world at a conference in 1989. Work continued inside Ericsson on to develop and improve Erlang over the following years. Then in 1992 there was a seemingly small, but in fact immensely significant, step forward for Erlang. A book was published on the language.</p><p>PLEX had been proprietary to Ericsson, as it was presumed that it gave the company a commercial advantage. For Erlang, it was decided to follow the example of AT&amp;T and the C language.</p><blockquote><p>The decision to publish an Erlang book and to be fairly open about what we did was therefore to avoid isolation and follow the AT&amp;T/C path rather than the Ericsson/PLEX path. Also in 1992 we ported Erlang to MS-DOS windows, the Mac, QNX and VxWorks.</p></blockquote><p>So the Erlang team started to promote the language. Including on video. We can see some of the original designers of Erlang, showcasing the system, in ‘Erlang: The Movie’. In it Bjarne Decker, Joe Armstrong, Mike Williams and Robert Virding show off the capabilities of their new language.</p><p><strong> ‘Declarative Real Time Programming Now!’, ‘Try It, You’ll like it!’</strong></p><div id="youtube2-xrIjfIjssLE" data-attrs="{&quot;videoId&quot;:&quot;xrIjfIjssLE&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/xrIjfIjssLE?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p>In December 1995, a large project at Ellemtel, called AXE-N, collapsed. According to Armstrong:</p><blockquote><p>AXE-N was a project aimed at developing a new generation of switching products ultimately to replace the AXE10 system. The AXE-N project had developed a new hardware platform and system software that was developed in C++.</p></blockquote><p>It was decided to replace the C++ software with code written in Erlang.</p><p>Again, according to Armstrong:</p><blockquote><p>This new project was to be the largest-ever Erlang project so far, with over 60 Erlang programmers. At the start of the AXD project, the entire Erlang system was the responsibility of half a dozen people in the Lab. This number was viewed as inadequate to support the needs of a large development project and so plans were immediately enacted to build a product unit, called OTP, to officially support the Erlang system.</p></blockquote><p>OTP or the  ‘Open Telecoms Platform’ provided capabilities such as ‘hot code reloading’, databases (he ‘Mnesia’ package) and many other facilities. Despite the use of the word ‘telecom’ in the name OTP is really a general purpose set of programs and doesn’t really have anything that is peculiar to telecoms systems.</p><p>So with increasing adoption Erlang’s future within Ericsson looked bright.</p><p>Then suddenly, in 1998, Erlang was banned within the Ericsson. The reason? To avoid the costs of development of a language that was unique to Ericsson, and instead to access the shared effort being put into the development of other languages.</p><blockquote><p>The selection of an implementation language implies a more long-term commitment than the selection of a processor and OS, due to the longer life cycle of implemented products. Use of a proprietary language implies a continued effort to maintain and further develop the support and the development environment. It further implies that we cannot easily benefit from, and find synergy with, the evolution following the large-scale deployment of globally used languages.</p></blockquote><p>In other words, Erlang was almost killed by the success of open-source languages.</p><p>But all was not over for Erlang.</p><p>The language continued to be maintained within Ericsson, as it was needed to support existing products.</p><p>And the efforts of the Erlang team outside Erlang would bear fruit. According to Armstrong:</p><blockquote><p>For some time, we had been distributing Erlang to interested parties outside Ericsson, although in the form of a free evaluation system subject to a non-disclosure agreement. By 1998, about 40 evaluation systems had been distributed to external users and by now the idea of releasing Erlang subject to an open-source license was formed. Recall that at the start of the Erlang era, in 1986, “open source” was unheard of, so in 1986 everything we did was secret.</p></blockquote><p>Following the ban, the Erlang team lobbied for Erlang, no longer seen as a sensitive commercial secret for Ericsson, to be released as open source. On 2 December 1998, “Open-Source Erlang” was announced.</p><p>The original members of the Erlang team soon left Ericsson to form their own company, Bluetail, to work on Erlang. It was natural for the team to turn to internet services, as this was just at the end of the era of the internet bubble.</p><blockquote><p>Given that the Bluetail system was programmed by most of the people who had designed and implemented the Erlang and OTP systems, the project was rapidly completed and had sold its first system within six months of the formation of the company. This was one of the first products built using the OTP technology for a non-telecoms application.</p></blockquote><p>Erlang had broken away from its telecoms roots and made its way onto the internet.</p><p>Other uses for the language started to appear. It found a niche in finance, helping firms that rely on ultra-fast trading to ensure that their systems were robust. Illustrious Wall Street firm Goldman Sachs used Erlang to implement their messaging system, with its robustness and reliability again key.</p><p><span>Open-source Erlang-based projects started to appear.  One of these was </span><a href="https://en.wikipedia.org/wiki/Ejabberd" rel="">ejabberd</a><span>, originally developed by Alexey Shchepin starting in 2003, which provided high-quality software to implement a system that allowed chat messages to be sent between a large number of users.</span></p><p>Then in 2009, Jan Koum came up with an idea. What about an internet-based messaging system for the iPhone?</p><p>The iPhone was launched in 2007 and the App Store in July 2008. The first version of Android was released in September 2008.</p><p>iPhone and Android users could send text messages to each other, but doing so was costly, with charges on a per-text basis. By contrast, the amount of data used for each message was tiny, even in the context of the small data transfer allowances granted to users at the time.</p><p><span>Koum was joined by Brian Acton, who he’d met whilst working at Yahoo!</span></p><p>Koum and Acton set out to build a system where users could send each other messages using their data allowances. This would dramatically undercut the pricing of the telecoms operators. With dedicated apps on each of the new mobile platforms, they could also build in more features to make using the application more useful and fun.</p><p>The underlying software base to power the ‘back end’ of this system was already available in the form of ejabberd. Koum and Acton took the open-source messaging system and adapted it to provide the underpinnings of the product that they envisaged.</p><p><span>So, </span><a href="https://en.wikipedia.org/wiki/WhatsApp" rel="">WhatsApp</a><span> was born in January 2009.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F904669ad-127a-45f9-9eeb-e2f8d14f1b0c_1508x1338.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F904669ad-127a-45f9-9eeb-e2f8d14f1b0c_1508x1338.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F904669ad-127a-45f9-9eeb-e2f8d14f1b0c_1508x1338.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F904669ad-127a-45f9-9eeb-e2f8d14f1b0c_1508x1338.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F904669ad-127a-45f9-9eeb-e2f8d14f1b0c_1508x1338.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F904669ad-127a-45f9-9eeb-e2f8d14f1b0c_1508x1338.png" width="1456" height="1292" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/904669ad-127a-45f9-9eeb-e2f8d14f1b0c_1508x1338.png&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1292,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:467063,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F904669ad-127a-45f9-9eeb-e2f8d14f1b0c_1508x1338.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F904669ad-127a-45f9-9eeb-e2f8d14f1b0c_1508x1338.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F904669ad-127a-45f9-9eeb-e2f8d14f1b0c_1508x1338.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F904669ad-127a-45f9-9eeb-e2f8d14f1b0c_1508x1338.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>WhatsApp Logo</figcaption></figure></div><p>WhatsApp’s growth was spectacular. From 10 million users in 2010 it grew to 100 million at the end of 2012. Thanks to the efficiency of Erlang and their careful refinement of both ejabberd, the FreeBSD operating system and Erlang itself, they were able to scale quickly whilst keeping growth of both their hardware and engineering resources down.</p><p>Their ability to scale at low cost also underpinned their business strategy. They avoided showing ads to users, instead charging each user $1 each year after the first year. $1 per user may not sound much, but with their growing user base, it would soon represent significant income. It was also affordable for users even in lower income countries, and users liked the ad-free WhatsApp user experience.</p><p>By 2013, Facebook’s Mark Zuckerberg started to see WhatsApp as a threat to his own business. Facebook had its own messaging application, Facebook Messenger, but WhatsApp was winning the messaging battle.</p><p>Critically, Zuckerberg would have seen how fast WhatsApp was scaling and presumed that it would be able to continue to scale. All thanks to Erlang.</p><p>So Facebook bought WhatsApp in 2014 for $16 billion, making Koum and Acton billionaires overnight. In the previous year, WhatsApp had revenues of only $10.2 million had incurred losses of $138 million (still small for a company with 400 million users at the end of the year – and with much of this stock-based compensation rather than underlying costs). Immediately after the acquisition, Facebook dropped what had been WhatsApp’s major revenue source, by removing the $1 per year fee. At the time of the acquisition Erlang employed only 35 engineers.</p><p>Under Facebook’s ownership, WhatsApp has continued to grow, with more than 2 billion users worldwide, all supported by Erlang.</p><p>WhatsApp wasn’t the only messaging system to use Erlang. Like WhatsApp, Facebook Messenger had used Erlang and ejabberd but moved away from it after finding it difficult to recruit engineers. WeChat in China also used Erlang for a messaging system operating with billions of users.</p><p>But messaging wasn't the only area where Erlang could be built on.</p><p>Jose Valim had been a core member of the group developing the Open-Source Ruby on Rails (commonly known as ‘Rails’) web framework. Rails was started by David Heinemeier Hansson in 2003 a part of the Basecamp project management web application.</p><p>Rails provides much of the software infrastructure  that is needed to build a modern web application: the ability to create web pages, access a database with user data and so on. Rails is built using the Ruby programming language.</p><p>Valim saw the advantages of Erlang and how useful they would be to someone developing a modern web application. In particular, its scalability made it suited to applications where users need to communicate frequently with web servers.</p><p>He also saw that Erlang had some major limitations in this context, including an unfamiliar syntax. So het set out to create a language that dealt with these issues whilst retaining the core advantages of Erlang and OTP.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F080d62f2-69e6-4e78-a028-c9a5a88bfffe_1000x418.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F080d62f2-69e6-4e78-a028-c9a5a88bfffe_1000x418.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F080d62f2-69e6-4e78-a028-c9a5a88bfffe_1000x418.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F080d62f2-69e6-4e78-a028-c9a5a88bfffe_1000x418.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F080d62f2-69e6-4e78-a028-c9a5a88bfffe_1000x418.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F080d62f2-69e6-4e78-a028-c9a5a88bfffe_1000x418.png" width="1000" height="418" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/080d62f2-69e6-4e78-a028-c9a5a88bfffe_1000x418.png&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:418,&quot;width&quot;:1000,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:62250,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F080d62f2-69e6-4e78-a028-c9a5a88bfffe_1000x418.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F080d62f2-69e6-4e78-a028-c9a5a88bfffe_1000x418.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F080d62f2-69e6-4e78-a028-c9a5a88bfffe_1000x418.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F080d62f2-69e6-4e78-a028-c9a5a88bfffe_1000x418.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>So </span><a href="https://en.wikipedia.org/wiki/Elixir_(programming_language)" rel="">Elixir</a><span> was born in 2013. Elixir provides the capabilities of Erlang (in fact, the language ‘cross compiles’ to Erlang) in a syntax that is largely familiar to Ruby programmers.</span></p><p>Just as Rails on Rails built on Ruby, Elixir soon formed the base for a new ‘web framework’, developed by Chris McCord, Jose Valim and others, Phoenix, provides open-source software that is broadly equivalent to Ruby on Rails in its capabilities whilst being less resource intensive and making it more straightforward to build web applications, such as chat, that need frequent connection to a web server.</p><p>Innovation on top of Elixir and Phoenix has continued to this day. Two notable examples are LiveView, that removes the need for the user to write JavaScript running in the browser, and Nx which adds the base capabilities required for machine learning applications.</p><p>Today, many companies are using Elixir as the basis for their web applications, including large companies such as Twitch and a number of smaller start-ups. Elixir and Phoenix provide everything that is needed for even a small team to build an application that can be rapidly scaled.</p><p>Erlang continues to be developed and supported by individuals and teams both inside and outside Ericsson. In 2022, it received a notable update that speeded up the execution of Erlang programs significantly through the introduction of an updated ‘just in time’ compiler.</p><p>Elixir and Phoenix have both reached maturity. They have both attracted a small but dedicated and enthusiastic group of supporters. The ecosystem around both continues to expand, although it still can’t match that around a framework such as Ruby on Rails.</p><p>But Elixir and Phoenix haven’t taken over the world and are still relatively niche. We’ll discuss why this is and some of the lessons that can be learned from the history of Erlang, WhatsApp, and Elixir in the paid supplement to this post coming on Tuesday. This post also contains loads of links to sources and supplementary material, including the briefest introductions to programming in Erlang and Elixir.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI and the Automation of Work (192 pts)]]></title>
            <link>https://www.ben-evans.com/benedictevans/2023/7/2/working-with-ai</link>
            <guid>36565854</guid>
            <pubDate>Sun, 02 Jul 2023 21:16:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ben-evans.com/benedictevans/2023/7/2/working-with-ai">https://www.ben-evans.com/benedictevans/2023/7/2/working-with-ai</a>, See on <a href="https://news.ycombinator.com/item?id=36565854">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-nc-base="header" data-controller="AncillaryLayout">
          

          <main>
            
              <section data-content-field="main-content">
                <article id="post-6492ff3078695b7b2d072912" data-item-id="6492ff3078695b7b2d072912">

    
      
    

    <div data-layout-label="Post Body" data-type="item" data-updated-on="1687355241466" id="item-6492ff3078695b7b2d072912"><div data-block-type="2" id="block-yui_3_17_2_1_1687355242158_5623">
  <p>Pretty much everyone in tech agrees that generative AI, Large Language Models and ChatGPT are a generational change in what we can do with software, and in what we can automate with software. There isn’t much agreement on anything else about LLMs - indeed, we’re still working out what the arguments are - but everyone agrees there’s a lot more automation coming, and entirely new kinds of automation. Automation means jobs, and people. </p><p>This is also happening very fast: ChatGPT has (apparently) over 100m users after just six months, and this data from <a href="https://productiv.com/state-of-saas-trends/">Productiv</a> suggests it’s already a top-dozen ‘shadow IT’ app. So, how many jobs is this going to take, how fast, and can there be new jobs to replace them?</p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1688229104156_55683">

<p>We should start by remembering that we’ve been automating work for 200 years. Every time we go through a wave of automation, whole classes of jobs go away, but new classes of jobs get created. There is frictional pain and dislocation in that process, and sometimes the new jobs go to different people in different places, but over time the total number of jobs doesn’t go down, and we have all become more prosperous. </p>



</div><div data-block-type="2" id="block-yui_3_17_2_1_1688084473363_14341">
  <p>When this is happening to your own generation, it seems natural and intuitive to worry that this time, there aren’t going to be those new jobs. We can <em>see</em> the jobs that are going away, but we can’t predict what the new jobs will be, and often they don’t exist yet. We know (or should know), empirically, that there always have been those new jobs in the past, and that they weren’t predictable either: no-one in 1800 would have predicted that in 1900 a million Americans would work on ‘railways’ and no-one in 1900 would have predicted ‘video post-production’ or ‘software engineer’ as employment categories. But it seems insufficient to take it on faith that this will happen now just because it always has in the past. How do you know it will happen this time? Is this different?</p><p>At this point, any first-year economics student will tell us that this is answered by, amongst other things, the ‘Lump of Labour’ fallacy. </p><p>The Lump of Labour fallacy is the misconception that there is a fixed amount of work to be done, and that if some work is taken by a machine then there will be less work for people. But if it becomes cheaper to use a machine to make, say, a pair of shoes, then the shoes are cheaper, more people can buy shoes and they have more money to spend on other things besides, and we discover new things we need or want, and new jobs. The efficient gain isn’t confined to the shoe: generally, it ripples outward through the economy and creates new prosperity and new jobs. So, we don’t know what the new jobs will be, but we have a model that says, not just that there always have been new jobs, but why that is inherent in the process. Don’t worry about AI!</p><p>The most fundamental challenge to this model today, I think, is to say that no, what’s really been happening for the last 200 years of automation is that we’ve been moving up the scale of human capability. </p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1688084473363_58771">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
          
          <figcaption>
            <p>‘Barge haulers on the Volga’, Ilya Repin, 1870-73. (Note the steam boat on the horizon.)</p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1688084473363_59132">
  <p>We began with humans as beasts of burden, pulling barges up the river, and moved up: we automated legs, then arms, then fingers, and now brains. We went from farm work to blue collar work to white collar work, and now we’ll automate the white-collar work as well and there’ll be nothing left. Factories were replaced by call centres, but if we automate the call centres, what else is there? </p><p>Here, I think it’s useful to look at another piece of economic and tech history: the Jevons Paradox. </p><p>In the 19th century the British navy ran on coal. Britain had a lot of coal (it was the Saudi Arabia of the steam age) but people worried what would happen when the coal ran out. Ah, said the engineers: don’t worry, because the steam engines keep getting more efficient, so we’ll use less coal. No, said Jevons: if we make steam engines more efficient, then they will be cheaper to run, and we will use more of them and use them for new and different things, and so we will use <em>mor</em>e coal. </p><p>We’ve been applying the Jevons Paradox to white collar work for 150 years. </p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1688069358241_3998">
  <p>It’s hard to imagine jobs of the future that don’t exist yet, but it’s also hard to imagine some of the jobs of the past that have already been automated away. Gogol’s <a href="https://en.wikipedia.org/wiki/The_Overcoat">downtrodden clerks</a> in 1830s St Petersburg spent their entire adult lives copying out documents, one at a time, by hand. They were human Xeroxes. By the 1880s, typewriters produced perfectly legible text at twice the words-per-minute, and carbons gave half a dozen free copies as well. Typewriters meant a clerk could produce more than 10 times the output. A few decades later, adding machines from companies like Burroughs did the same for book-keeping and accounting: instead of adding up columns with a pen, the machine does it for you, in 20% of the time, with no mistakes. </p><p>What did that do to clerical employment? People hired far more clerks. Automation plus the Jevons Paradox meant more jobs. </p><p>If one clerk with a machine can do the work of 10, then you might have fewer clerks, but you might also you might do far more with them. If, Jevons tells us, it becomes much cheaper and more efficient to do something, you might do more of it - you might do more analysis or manage more inventory. You might build a different and more efficient business that is only possible because you can automate their administration with typewriters and adding machines. </p><p>This process keeps repeating. This is Jack Lemon as CC Baxter in <a href="https://en.wikipedia.org/wiki/The_Apartment">The Apartment</a> in 1960, using an electro-mechanical adding machine from <a href="https://en.wikipedia.org/wiki/Friden,_Inc.">Friden</a> fifty years after adding machines were new and exciting. </p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1688069358241_18109">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/50363cf324ac8e905e7df861/2d3a670a-28cf-4dd9-a07e-c37c9f9861c8/featured.png" data-image="https://images.squarespace-cdn.com/content/v1/50363cf324ac8e905e7df861/2d3a670a-28cf-4dd9-a07e-c37c9f9861c8/featured.png" data-image-dimensions="1200x894" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="649de7e7206ef75742274813" data-type="image" src="https://images.squarespace-cdn.com/content/v1/50363cf324ac8e905e7df861/2d3a670a-28cf-4dd9-a07e-c37c9f9861c8/featured.png">
                
            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1688069358241_18464">

<p>Everyone in that shot is a cell in a spreadsheet, and the whole building is a spreadsheet. Once a week someone on the top floor presses F9 and they recalculate. But they already had computers, and in 1965 or 1970 they bought a mainframe, and scrapped all the adding machines. Did white collar employment collapse? Or, as IBM advertised, did a computer give you 150 extra engineers? 25 years later, what did the PC revolution, and the accounting department in a box, do to accounting?</p>



</div><div data-block-type="2" id="block-yui_3_17_2_1_1688084473363_92506">
  <p>Dan Bricklin invented the computer spreadsheet in 1979: until then, ‘spreadsheets’ were paper (you can still buy them <a href="https://www.amazon.com/s?crid=J5T9EM16AQN4&amp;i=office-products&amp;k=spreadsheet%20paper&amp;ref=nb_sb_ss_ts-doa-p_1_11&amp;sprefix=spreadsheet%2Coffice-products%2C56">on Amazon</a>). He has <a href="https://qz.com/578661/dan-bricklin-invented-the-spreadsheet-but-dont-hold-that-against-him">some entertaining stories</a> about early use: ‘<em>People would tell me, “I was doing all this work, and coworkers thought I was amazing. But I was really goofing off because it only took an hour and then I took the the rest of the day off. People thought I was a wunderkind but I was using this tool.”’</em></p><p> So, what did Excel and the PC do to accounting employment? <a href="https://us.aicpa.org/interestareas/accountingeducation/newsandpublications/aicpa-trends-report">It went up</a>. </p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1688069358241_21167">

<p>40 years later, do spreadsheets mean you can goof off early? Not really.  </p>



</div><div data-block-json="{&quot;width&quot;:550,&quot;height&quot;:null,&quot;hSize&quot;:null,&quot;html&quot;:&quot;<blockquote class=\&quot;twitter-tweet\&quot;><p lang=\&quot;en\&quot; dir=\&quot;ltr\&quot;>Younger people may not believe this but before spreadsheets investment bankers worked really long hours. It&amp;#39;s only thanks to Excel that Goldman Sachs associates can get everything done and leave the office at 3pm on Fridays. Now LLMs mean they&amp;#39;ll only have to work one day a week!</p>&amp;mdash; Benedict Evans (@benedictevans) <a href=\&quot;https://twitter.com/benedictevans/status/1654514832765329411?ref_src=twsrc%5Etfw\&quot;>May 5, 2023</a></blockquote>\n<script async src=\&quot;https://platform.twitter.com/widgets.js\&quot; charset=\&quot;utf-8\&quot;></script>&quot;,&quot;url&quot;:&quot;https://twitter.com/benedictevans/status/1654514832765329411&quot;,&quot;resolvedBy&quot;:&quot;twitter&quot;,&quot;floatDir&quot;:null,&quot;providerName&quot;:&quot;Twitter&quot;,&quot;customThumbEnabled&quot;:false}" data-block-type="22" id="block-yui_3_17_2_1_1688219853387_28970"><blockquote><p lang="en" dir="ltr">Younger people may not believe this but before spreadsheets investment bankers worked really long hours. It's only thanks to Excel that Goldman Sachs associates can get everything done and leave the office at 3pm on Fridays. Now LLMs mean they'll only have to work one day a week!</p>— Benedict Evans (@benedictevans) <a href="https://twitter.com/benedictevans/status/1654514832765329411?ref_src=twsrc%5Etfw">May 5, 2023</a></blockquote>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1688219853387_29036">
  <p>New technology generally makes it cheaper and easier to do something, but that might mean you do the same with fewer people, or you might do much more with the same people. It also tends to mean that you change what you do. To begin with, we make the new tool fit the old way of working, but over time, we change how we work to fit the tool. When CC Baxter’s company bought a mainframe, they began by automating the way they already did things, but over time, new ways to run the business became possible. The machine lets a person do 10x the work, but you need the person. </p><p>So, all of this is to say that by default, we should expect LLMs to destroy, displace, create, accelerate and multiple jobs just as SAP, Excel, Mainframes or typewriters did. It’s just more automation.  </p><p>I think there are two counter-arguments to this. </p><p>The first is to say that yes, perhaps this is indeed just more of the same kind of change that we saw with the internet, PCs or computers, and perhaps again it will have no long-term effect on net employment, but this time it will happen much faster, and so that frictional pain will be much greater and it will be much harder to adjust. </p><p>LLMs and ChatGPT certainly are happening a lot faster than things like iPhones or the Internet, or indeed PCs. The Apple II shipped in 1977, the IBM PC in 1981 and the Mac in 1984, but it took until the early 1990s before  there were 100m PCs in use: there are 100m ChatGPT users today after just six months. You don’t need to wait for telcos to build broadband networks, or consumers to buy new devices, and generative AI sits on top of the whole existing stack of cloud, distributed computing and indeed a lot of the machine learning stack itself that was built over the last decade. To a user, it’s just a website.</p><p>However, your expectations might be different if you think about the implications of these charts, from <a href="https://productiv.com/state-of-saas-trends/">Productiv</a> again and from <a href="https://www.okta.com/uk/resources/whitepaper-businesses-at-work-2021/">Okta</a> (with a different methodology). They report that their typical customer now has hundreds of different software applications. </p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1688331732417_68266">

<p>And yet, enterprise cloud adoption is still barely a quarter of workflows.  </p>



</div><div data-block-type="2" id="block-yui_3_17_2_1_1688219853387_71837">
  <p>What does that mean for generative AI in the workplace? Whatever you think will happen, it will take years, not weeks. </p><p>First, the tools that people use for work, and the tasks that might now get a new layer of automation, are complicated and very specialised, and embody a lot of work and institutional knowledge. A lot of people are <em>experimenting </em>with ChatGPT, and seeing what it will do. If you’re reading this, you probably have too. That doesn’t mean that ChatGPT has replaced their existing workflows <em>yet</em>, and replacing or automating any of those tools and tasks is not trivial. </p><p>There’s a huge difference between an amazing demo of a transformative technology and something that a big complicated company holding other people’s business can use. You can rarely go to a law firm and sell them an API key to GCP’s translation or sentiment analysis: you need to wrap it in control, security, versioning, management, client privilege and a whole bunch of other things that only legal software companies know about (there’s a graveyard of machine learning companies that learnt this in the last decade). Companies generally can’t buy ‘technology’.  <a href="https://www.everlaw.com/">Everlaw</a> doesn’t sell translation and <a href="https://www.people.ai/">People.ai</a> doesn’t sell sentiment analysis - they sell tools and products, and often the AI is only one part of that. I don’t think a text prompt, a ‘go’ button and a black-box, general purpose text generation engine make up a product, and product takes time.  </p><p>Second, buying tools that manage big complicated things takes time even once the tool is built and has product-market fit. One of the most basic challenges in building an enterprise software startup is that startups run on an 18 month funding cycle and a lot of enterprises run on an 18 month decision cycle. SaaS itself accelerated this because you don’t need to get into the enterprise datacenter deployment schedule, but you still need purchase, integration and training, and companies with millions of customers and tens or hundreds of thousands of employees have very good reasons not to change things suddenly. The future takes a while, and the world outside Silicon Valley is complicated. </p><p>The second objection is that part of the paradigm shift of ChatGPT and LLMs is a shift in the layer of abstraction: this looks like a much more general purpose technology. Indeed, that’s why it’s exciting. It can answer <em>anything,</em> we are told. So, you could look at that chart of 473 enterprise SaaS apps and say that ChatGPT will disrupt that and collapse many those vertical apps into one prompt box. That would mean it would move faster, and also automate much more. </p><p>I think that misunderstands the problem. If a partner at a law firm wants a first draft of a paper, they want to be able to shape the parameters in completely different ways to a salesperson at an insurance company challenging a claim, probably with a different training set and certainly with a bunch of different tooling. Excel is ‘general purpose’ too, and so is SQL, but how many different kinds of ‘database’ are there? This is one reason I think the future of LLMs is to move from prompt boxes to GUIs and buttons - I think ‘prompt engineering’ and natural language’ are mutually contradictory. But either way, even if you can run everything as a thin wrapper on top of one giant foundational model (and there is very little agreement or clarity about this yet), even those wrappers will take time.</p><p>Meanwhile, while one could suggest that LLMs will subsume many apps on one axis, I think it’s equally likely that they will enable a whole new wave of unbundling on other axes, as startups peel off dozens more use cases from Word, Salesforce and SAP, and build a whole bunch more big companies by solving problems that no-one had realised were problems until LLMs let you solve them. That’s the process that explains why big companies already have 400 SaaS apps today, after all. </p><p>More fundamental, of course, there is the error rate. ChatGPT can <em>try </em>to answer ‘anything’ but the answer might be wrong. People call this hallucinations, making things up, lying or bullshitting - it’s the ‘overconfident undergraduate’ problem. I think these are all unhelpful framings: I think the best way to understand this is that when you type something into a prompt, you’re not actually asking it to answer a question at all. Rather, you’re asking it “what sort of answers would be people be likely to produce to questions that look like this?” You’re asking it to match a pattern. </p><p>Hence, if I ask ChatGPT4 to write a biography of myself, and then ask it again, it gives different answers. It suggests I went to Cambridge, Oxford or the LSE; my first job was in equity research, consulting or financial journalism. These are always the right pattern: it’s the right kind of university and the right kind of job (it never says Anglia Poly and then catering management). It is giving 100% correct answers to the question “what <strong>kinds</strong> of degrees and jobs are people <strong>like</strong> Benedict <strong>likely</strong> to have done?” It’s not doing a database lookup: it’s making a pattern. </p><p>You can see something similar in this image I got from MidJourney. The prompt was “A photograph of advertising people discussing creativity on stage in a panel on a beach at Cannes Lions.”</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1688228900463_18067">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/50363cf324ac8e905e7df861/f8965b47-137b-4dc8-9fe7-eec10e13781f/0_3.png" data-image="https://images.squarespace-cdn.com/content/v1/50363cf324ac8e905e7df861/f8965b47-137b-4dc8-9fe7-eec10e13781f/0_3.png" data-image-dimensions="1024x1024" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="64a05493b8d18000902a981a" data-type="image" src="https://images.squarespace-cdn.com/content/v1/50363cf324ac8e905e7df861/f8965b47-137b-4dc8-9fe7-eec10e13781f/0_3.png">
                
            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1688228900463_18432">
  <p>It’s matched the pattern almost perfectly - that looks like the beach at Cannes, these people are dressed like advertising people, and they even have the right haircuts. But it doesn’t <em>know</em> anything, and so it doesn’t know that people <em>never </em>have three legs, only that it’s unlikely. This isn’t ‘lying’ or ‘making things up’ - it’s matching a pattern, imperfectly. </p><p>Whatever you call it, if you don’t understand this, you can get into trouble, as happened to <a href="https://www.nytimes.com/2023/06/08/nyregion/lawyer-chatgpt-sanctions.html">this unfortunate lawyer</a>, who did not understand that when he asked for precedents, he was actually asking for things that looked like precedents. He duly got things that looked like precedents, but weren’t. It’s not a database. </p><p>If you do understand this, then you have to ask, well, where are LLMs useful? Where is it useful to have automated undergraduates, or automated interns, who can repeat a pattern, that you might have to check? The last wave of machine learning gave you infinite interns who could read anything for you, but you had to check, and now we have infinite interns that can write anything for you, but you have to check. So where is it useful to have infinite interns? Ask Dan Bricklin - we’re back to the Jevons Paradox. </p><p>This takes me, obviously, to AGI. The really fundamental objection to everything I’ve just said is to ask what would happen if we had a system that didn’t have an error rate, didn’t hallucinate, and really could do anything that people can do. If we had that, then you might not have one accountant using Excel to get the output of ten accountants: you might just have the machine. This time, it really would be be different. Where previous waves of automation meant one person could do more, now you don’t need the person.  </p><p>Like a lot of AGI questions, though, this can become circular if you’re not careful. ‘If we had a machine that could do anything people do, without any of these limitations, could it do anything people can do, without these limitations?’</p><p>Well, indeed, and if so we might have bigger problems than middle-class employment, but is that close? You can spend weeks of your life watching three hour YouTube videos of computer scientists arguing about this, and conclude only that they don’t really know either. You might also suggest that the idea this one magic piece of software will change everything, and override all the complexity of real people, real companies and the real economy, and can now be deployed in weeks instead of years, sounds like classic tech solutionism, but turned from utopia to dystopia. </p><p>As an analyst, though, I tend to prefer Hume’s empiricism over Decartes - I can only analyse what we can know. We don’t have AGI, and without that, we have only another wave of automation, and we don’t seem to have any <em>a priori</em> reason why this must be more or less painful than all the others. </p>
</div></div>

    

    

    

  </article>





  
              </section>
            
          </main>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Disabled at 22 million commits (112 pts)]]></title>
            <link>https://programming.dev/post/355447</link>
            <guid>36565842</guid>
            <pubDate>Sun, 02 Jul 2023 21:15:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://programming.dev/post/355447">https://programming.dev/post/355447</a>, See on <a href="https://news.ycombinator.com/item?id=36565842">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="postContent"><div><p>Hey folks,</p>
<p>This is an update to: <a href="https://sh.itjust.works/post/580838">https://sh.itjust.works/post/580838</a></p>
<p>On <a href="https://github.com/csm10495/commit-ment">https://github.com/csm10495/commit-ment</a>, I made it to somewhere around 22 million commits. I can’t imagine this is not a world record for commits to a single branch. In the middle of the night, I got an email from GitHub support saying:</p>
<p><img src="https://sh.itjust.works/pictrs/image/e706b3af-9157-4d97-a5c2-0b4e3725b452.png" alt=""></p>
<p>A few minutes later, I got another email like so:</p>
<p><img src="https://sh.itjust.works/pictrs/image/96f57e50-0567-4913-a8cf-e9c4773ba45f.png" alt=""></p>
<p>I’ve asked them if the two emails are related (and I guess if the first one is some sort of error since there was no personal info in that repo). I’ve also asked if they can give any information about what triggered the email and if they can give me more info about what it looks look on their side.</p>
<p>I’ve also asked if they can re-enable it so I can give one more commit to say the final results on the readme then (public) archive it.</p>
<p>We’ll see what they say.</p>
<p>Doing a pull is interesting at the moment, it shows:</p>
<pre><code>git pull origin master --no-rebase -vvv
ERROR: Access to this repository has been disabled by GitHub staff due to
excessive resource use. Please contact support via
https://support.github.com/contact to restore access to this repository.
Read about how to decrease the size of your repository:
  https://docs.github.com/articles/what-is-my-disk-quota

fatal: Could not read from remote repository.

Please make sure you have the correct access rights
and the repository exists.
</code></pre>
<p>Similar thing happens if you try to clone: <a href="https://programming.dev/cdn-cgi/l/email-protection#46212f3206212f322e33246825292b"><span data-cfemail="caada3be8aada3bea2bfa8e4a9a5a7">[email&nbsp;protected]</span></a>:csm10495/commit-ment.git</p>
<p>So yeah, I figured this would happen sooner or later. I just hope they can tell me a bit more about what it looks like on their side since managing this repo on my box is a pain, I can’t imagine what it could look like on theirs. I’m also curious how pull requests could merge at such a rate given that just doing a pull on my end could take minutes. So many questions!</p>
<p>This whole project was really just for curiosity on my end, so anything I can learn/find out is much appreciated on all ends.</p>
<p>Anyways, just figured I’d update y’all.</p>
</div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WebAuthn Is Great and It Sucks (2020) (143 pts)]]></title>
            <link>https://sec.okta.com/articles/2020/04/webauthn-great-and-it-sucks/</link>
            <guid>36565405</guid>
            <pubDate>Sun, 02 Jul 2023 20:35:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sec.okta.com/articles/2020/04/webauthn-great-and-it-sucks/">https://sec.okta.com/articles/2020/04/webauthn-great-and-it-sucks/</a>, See on <a href="https://news.ycombinator.com/item?id=36565405">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>What is WebAuthn again?</h2><p>First things first, let’s all agree that passwords suck, OK?&nbsp;</p><p>Good, glad we’re on the same page. Passwords are hard to remember, leading people to pick weak ones and reuse them over and over. Passwords are also easy to phish, with ever more subtle and believable attacks happening <a href="https://threatpost.com/clever-phishing-attack-enlists-google-translate-to-spoof-facebook-login-page/141571/">all the time</a>.&nbsp;</p><p>WebAuthn—short for Web Authentication—promises to fix passwords on the web with a strong, simple, and un-phishable standard for secure authentication. WebAuthn at its heart is a credential management API built into modern web browsers allowing web applications to strongly authenticate users, and it’s now a World Wide Web Consortium standard.</p><p>How does WebAuthn do this? Public key cryptography, which allows you to strongly authenticate without a password. Using WebAuthn, you're able to use a single authenticator (like a Yubikey, for example) on any site that supports the standard. This way, as a user, you don't need to have passwords for every site you visit, just a strong authenticator that works with WebAuthn.</p><p>In addition to offering convenience, WebAuthn provides privacy, as one site can’t figure out from the authenticator what other sites you’ve used it for. Attackers also can’t capture and successfully replay the authentication request, so malicious sites can’t use it to attack the genuine sites, eliminating <a href="https://breakdev.org/evilginx-2-next-generation-of-phishing-2fa-tokens/">man-in-the-middle attacks</a>. WebAuthn also allows you to choose your own authenticator, a device you already have (like a smartphone or computer) or an external authenticator like a USB security key.&nbsp;</p><h3>Wow, sounds great!</h3><p>Sure does!</p><h3>OK, so I looked up WebAuthn and it’s full of acronyms!</h3><p>You bet, let’s look at what they mean!</p><p><strong>FIDO</strong> is short for Fast IDentity Online. The FIDO Alliance is an open industry association with <a href="https://fidoalliance.org/members/">hundreds of member companies</a>, working to create authentication standards to help reduce the world’s over-reliance on passwords.&nbsp;</p><p><strong>FIDO2</strong> is the overarching term for the <a href="https://fidoalliance.org/specifications/download/">specifications</a> from the World Wide Web Consortium (W3C) and the FIDO Alliance. It includes both WebAuthentication (web APIs for passwordless authentication in browsers) and CTAP protocols.</p><p><strong>CTAP</strong> stands for <a href="https://fidoalliance.org/specs/fido-v2.0-ps-20190130/fido-client-to-authenticator-protocol-v2.0-ps-20190130.html">Client To Authenticator Protocol</a>. It describes how authenticators can implement second-factor and passwordless authentication. These authenticators can be built-in to devices like phones and laptops (on-device or platform authenticators), or they can be external ones (roaming authenticators or security keys) connecting over NFC, USB, and/or BLE.</p><p><strong>CTAP2</strong> is the new hotness. It enables you to use the new authenticators <strong>not only for second-factor authentication</strong>, but also for <strong><a href="https://www.w3.org/TR/webauthn/#sctn-authenticator-taxonomy">passwordless and multi-factor authentication</a></strong>.&nbsp;</p><p><strong>CTAP1</strong> is what used to be called FIDO U2F. It allows older U2F authenticators like security keys to continue to be used for second-factor authentication, i.e. as an extra step after a password.&nbsp;</p><p><img src="https://lh5.googleusercontent.com/ZO0LpBNwZXtNU7BcMFv0G97IFjEWKNGScAsD1vOBjs36YhTysbpL91AghbbSAy0QorZEHb78cLcBIvtzbuFXecY4Gaw__Re5qlWJYuORpb3KscqBv34EcVcaIX71ZD1CX_JyK7nf"><br>
Image copyright FIDO Alliance</p><p>Easy, right? Well, it could be a lot worse—just look at the <a href="https://tools.ietf.org/html/rfc6749">OAuth 2.0 specifications!</a></p><p><img src="https://lh5.googleusercontent.com/M3i3S_QJPUvtfkFRLsC17sgw2Ha_UbkgG-ULfwKu8n9vk_HDnLL4U4R0ZcPBGnBUG99uqn21abXnAEQAHBh0LofdZ7wurV2JxOpTty36qR0BYRtYFz16XDInI-YH1tu-u1CPKdHV"></p><p>If you want to see how WebAuthn works behind the scenes, watch this <a href="https://www.okta.com/resources/oktane-content/developer/#does-webauthn-signal">great video by James Fang and Payal Pan from Oktane 19</a>.</p><h2>Wait, why do I care again?</h2><p>Great question! The key promise is …. *drum roll* <strong>strong passwordless authentication</strong>!</p><p>The older FIDO U2F protocols and security keys allow for strong and phishing-resistant second-factor experiences, but now we’re talking passwordless! Just think of how smooth and seamless that will be!</p><p>These new protocols make it possible to require even stronger authentication than the user presence test of U2F protocol, where you tap the security key to authenticate. With FIDO2, <a href="https://www.w3.org/TR/webauthn/#user-verification">sites can require user verification</a> at different levels from password or PIN tied to the security key all the way to on-device biometrics, such as fingerprint readers, Face ID, or Windows Hello. This can enable single-device <a href="https://en.wikipedia.org/wiki/Multi-factor_authentication#Authentication_factors">multi-factor authentication</a>, combining the possession factor (you have the authenticator) with a knowledge factor (you know the PIN) and/or inherent factors (your biometric, like fingerprint or faceprint matches).</p><p>Now, the biggest challenge in moving past passwords is the simple fact that it has been the lowest common denominator—the easiest and cheapest thing to implement. Passwords like we know them date back 59 years (!!) going back to <a href="https://www.wired.com/2012/01/computer-password/">MIT in 1961 with the CTSS</a> operating system. Passwords are literally everywhere.&nbsp;</p><p>Every gizmo that has come since with a promise to eliminate the password has failed. There’s always been a platform, service, or system that didn’t support the latest new passwordless idea, and very few were ready to pay the cost of changing the servers, the operating systems, the applications, everything, just to get a non-compatible point solution. Passwords are cheap. Everything else is expensive.</p><p>Except now, with WebAuthn!</p><h2>Everything supports WebAuthn! Great!</h2><p>We are at a cusp of having universal support for WebAuthn!&nbsp; All major operating systems and browsers have now implemented WebAuthn.&nbsp;</p><p>As I write this in April 2020, a full <strong>83% of all the browsers in use</strong> around the world support it, as you can see in this <a href="https://caniuse.com/#search=webauthn">CanIUse</a> report.</p><p><img src="https://lh3.googleusercontent.com/2wLQQsRTDLUv1OT-jPovHPioCZpyU5XlY_-WlSa7Mb5eoDlYJ2WntAakBxKIUfSlivfmpXvp-V2IUeQS4O_8q7dIOPV4TCc5EFsPM_eiuTPYxM8S0F1ar0Eeg0icjWqCypE9NTxU"></p><p>There are dozens of different FIDO2-compatible security keys available from companies like Yubico, Feitian, Google, Kensington, and others. And developers have built support into operating systems (iOS, Android, Windows, macOS) so you can use platform authenticators like Touch ID sensors on MacBooks and facial recognition and fingerprint sensors on PCs.</p><p>Victory, hooray! Let’s go and get a FIDO2 Security Key so we can use it everywhere!</p><h2>Nothing supports WebAuthn! What?!</h2><p>Except when you go to set it up, you will find that basically <strong>no major web application</strong> supports WebAuthn the way we envisioned here as replacement for the password! D’oh!</p><p>Web applications support WebAuthn fairly well as a <strong>second factor</strong>, backward compatible to FIDO U2F, but even that support remains far from universal.</p><p>Story time! I’ve always been more than a little paranoid, an occupational hazard having worked around web security for 20+ years. Outside of work, I already had a unique, complex, and <a href="https://haveibeenpwned.com/Passwords">non-Pwned password</a> for each of the 562 websites and apps I have a login for (not counting the work apps behind Okta SSO of course). Yes, I use a <a href="https://1password.com/">password manager</a>—I’m not an animal.</p><p>Let me recount my experience when I went to set up WebAuthn on every account whose security really matters to me! Passwords begone! Here are the results!</p><h3>Let’s see the scoreboard</h3><ul><li>GMail: Yes! But alas only as a U2F security key after password.</li><li>Another email: No, but at least they support generic <a href="https://en.wikipedia.org/wiki/Time-based_One-time_Password_algorithm">TOTP second factors</a>.</li><li>Apple iCloud: Proprietary multi-factor authentication, but that’s a different story.&nbsp;</li><li>Cellular provider: LOL NOPE! Security PIN only and SMS, which they helpfully are willing to send to my kids’ phone numbers too in case they ever guess my password!</li><li>Top modern robo advisor: No, but at least they support generic TOTP.</li><li>Top online brokerage: No, but at least they support a proprietary third-party TOTP app.</li><li>Top retirement account: LOL NOPE! SMS!</li><li>Top-three credit card issuer: LOL NOPE! SMS!</li><li>Another top-three credit card issuer: LOL NOPE! No strong authentication of ANY kind.</li><li>Top-five bank: LOL NOPE! SMS! Or they’ll sell you a 90’s-style hardware TOTP token!</li><li>Local credit union: LOL NOPE! They got nothing, but the last time I tried to log in my account was locked out, so good to know that somebody’s trying to brute-force my complex, globally unique password :)</li><li>Online crypto currency wallet: Yes! But alas only as a U2F security key after password. Also, they block my FIDO2 platform authenticator and only allow USB security keys. And once you add a security key, you lose your TOTP! It’s one or the other!&nbsp;&nbsp;</li><li>My DNS / hosting provider: Yes! But alas only as a U2F security key after password.</li><li>Facebook: Yes, but just as a second factor, U2F mode.</li><li>Twitter: Yes, but just as a second factor, U2F mode, and the settings are buried deep.</li><li>Zoom: Nope. Just a well-hidden option to add a generic TOTP second factor.</li><li>Dropbox: Yes, but just as a second factor, U2F mode.</li></ul><p>So, the <strong>score is 0 (zero) out of 17 for going passwordless</strong> with WebAuthn. Sigh.</p><p>Or we could say the score is 6 out of 17, if we accept U2F mode using WebAuthn as a second factor.</p><h3>Don’t banks want better security?</h3><p>Well, they do, but they are not pushing end-user-visible and end-user-operated security tools, because today even the best ones like WebAuthn add friction in the form of inconsistencies and confusion. And as I’ll show you shortly, even with WebAuthn that friction is unfortunately real.&nbsp;</p><p>Any friction translates to confused and angry customers, which translates to millions of dollars in call-center cost and customer churn. Remember that even small banks have tens of thousands of users, large ones tens of millions! This is why banking security professionals focus so heavily on the invisible, back-end fraud detection and risk management tools. And if an attacker compromises an account and takes money, the bank can make the account holder whole again and treat it as a cost of doing business. Corporate banking portals dealing with big money transfers typically use strong authentication, as the user population is much smaller and more receptive to adopting security measures.</p><p>So, don’t look for consumer financial services to adopt passwordless WebAuthn first. That won’t happen until browsers and operating systems universally support it and <strong>not until the user experience is consistent and great</strong>.</p><h2>Why can’t we have nice things?</h2><p>So are WebAuthn and FIDO2 doomed to fail? And can it ever get us to passwordless?&nbsp;</p><p>Well, the technology doesn’t suck, the protocols work, the basic tech is kind of great, and you can and should use WebAuthn as a second-factor authenticator everywhere that matters! Security keys are one of the <a href="https://krebsonsecurity.com/2018/07/google-security-keys-neutralized-employee-phishing/">strongest practical authenticators</a> available today, and they are useful for anyone who would ever get this far into a blog post like this.</p><p>The problem is, while they technically work, the <a href="https://blog.silverorange.com/web-authn-ux-89a61ba7b555">user experience is broken</a>.&nbsp;</p><p>Every website has a different path for setting things up. What the security settings are called and where they are found are wildly inconsistent from one website to the next.&nbsp;</p><p>Every browser and operating system presents the experience in a different-looking pane or slide-over. The terminology different browsers use is inconsistent and confusing.&nbsp;</p><p>Even the very few sites that support the full passwordless WebAuthn experience have to provide other options, so you have to click on a separate link for the passwordless path. And if you make a mistake during setup, the error messages can be less than helpful.</p><p><img src="https://lh6.googleusercontent.com/b1qcKPRRquGHsuNgvomjLj1p0e9fsSTy605x88BVKynqrnKiV5bI8kU_yznvCRgHLAq_mDE20nnApEsPK9Ob5zzbah_w3l-Zcau-x8EHQ5i6i0-78NWCks5JhbcYbnU-O8Xxhmwj"></p><p>Therefore it is hard to recommend WebAuthn to the people we most want to help—our friends and family, children and parents. The way websites, browsers and OSs implement WebAuthn today does not pass my will-my-spouse-murder-me-in-my-sleep standard of deployability.&nbsp;</p><h2>Can I get passwordless WebAuthn?</h2><p>In short, probably not today. But it’s not hopeless!</p><p>Frankly, you might get passwordless first at work. Modern single sign-on providers make it possible to use WebAuthn and other modern authenticators in combination with risk scoring, device management, and other tools in the corporate IT tool box to enable end-to-end passwordless experiences. These corporate solutions, like <a href="https://www.okta.com/fastpass/">Okta FastPass</a>, combine on-device biometrics and device management solutions to get there. At work, we are <a href="https://www.okta.com/blog/2019/10/the-dogfooding-chronicles-webauthn-the-path-to-passwordless/">eating our own dogfood</a> with WebAuthn!</p><p>Outside of the workplace, across the 17 sites I looked to secure, I found none that yet supported a bona-fide WebAuthn passwordless experience.&nbsp;</p><h2>So what should I do?</h2><p>Don’t give up!</p><p>Even if we can’t have true passwordless today, <strong>adding WebAuthn as a strong second-factor authentication is absolutely worth it</strong>. I can use WebAuthn as a second factor at 6 of my 17 sites I checked. <a href="https://youtu.be/w-YDV6vC2qo">Not great, not terrible!</a> Another 4 of the 17 support adding TOTP one-time passwords. Although technically these one-time passwords are phishable, the risk is vastly reduced if you’re using a password manager app as your TOTP authenticator as well, as the password manager won’t autofill your credentials to the phishing site.&nbsp;</p><p>So using WebAuthn as a second-factor authenticator is definitely worth it and user experience is fairly seamless when combined with password managers.</p><p>But how do we ever get to the promised WebAuthn passwordless world?!</p><p>Let’s keep the pressure on the browser and operating system vendors and ask for consistency. Ask <a href="https://support.google.com/chrome/answer/95315">Chrome</a>, <a href="https://qsurvey.mozilla.com/s3/FirefoxInput/">Firefox</a>, and <a href="https://www.apple.com/feedback/safari.html">Safari</a> to standardize their naming conventions for a better user experience!</p><p>And on the other hand, let’s keep the pressure on the <a href="https://twofactorauth.org/">websites that don’t support strong authentication</a> at all and more specifically, let’s push <a href="https://www.dongleauth.info/">websites that don’t support security keys</a> to add that support.</p><p>If you’re an IT administrator and want to support WebAuthn for your employee access, it’s easy to do with modern SSO providers like <a href="https://www.okta.com/resources/whitepaper-how-webauthn-works-oad">Okta</a>.&nbsp;</p><p>And if you’re one of the people who builds these websites, consider adding WebAuthn support to it! The frameworks are there and ready for you to use, see <a href="https://developers.yubico.com/WebAuthn/WebAuthn_Developer_Guide/Overview.html">Yubico’s WebAuthn developer guide</a> and <a href="https://2018.pycon-au.org/talks/44258-webauthn-multifactor-auth-for-everyone/">this talk</a> for examples. With just a little more design and usability polish, we can all win!</p><p>Thanks for reading and stay safe!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Googling for answers costs time (2020) (118 pts)]]></title>
            <link>https://prashanth.world/seo-ruining-programming-help/</link>
            <guid>36565225</guid>
            <pubDate>Sun, 02 Jul 2023 20:16:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://prashanth.world/seo-ruining-programming-help/">https://prashanth.world/seo-ruining-programming-help/</a>, See on <a href="https://news.ycombinator.com/item?id=36565225">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>When I’m learning a new language, I tend to take a very dumb approach (and I don’t mean that in some enlightened way). I’ll usually try out a tutorial or two, but if the language is similar to another language that I know, I’ll basically just start coding without investing a lot of time learning the language features. This leads me to often googling how to do extremely basic things. Stuff like ‘python iterate over dict’. ‘javascript get object keys’. ‘elixir keyword list to map’. In the past, I’ve felt extremely adept at finding exactly what I need, but recently I’ve noticed that it’s been taking me longer and longer to really find the answers I’m looking for.</p>

<p>For example, looking at the results on Google for <a href="https://www.google.com/search?hl=en&amp;q=python%20iterate%20over%20dictionary">‘python iterate over dict’</a>, I see the following results</p>

<table>
  <thead>
    <tr>
      <th><img src="https://prashanth.world/images/google_python_dict_results.png" alt="google results"></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>Seems reasonable</em></td>
    </tr>
  </tbody>
</table>

<p>Notice that StackOverflow is the third response. I am not sure what realpython.com is, but clicking on that <a href="https://realpython.com/iterate-through-dictionary-python/">link</a> shows the following page</p>

<table>
  <thead>
    <tr>
      <th><img src="https://prashanth.world/images/real_python_image_1.png" alt="google results"></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>a whole lot of nothing</em></td>
    </tr>
  </tbody>
</table>

<p>Notice that there’s literally no content visible. Lets scroll down one page</p>

<table>
  <thead>
    <tr>
      <th><img src="https://prashanth.world/images/real_python_TOC.png" alt="google results"></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>…so…much…nonsense</em></td>
    </tr>
  </tbody>
</table>

<p>Hmm still no content. Well there’s a table of contents, but no actual content. Scrolling down to the content, it goes on and on and on and on about why dictionaries are important, what you can use dictionaries for, what are valid keys for dictionaries… The title of the post is quite literally ‘How to iterate through a dictionary in python’, but it takes NINE PAGE DOWN keystrokes just to get to the <em>section</em> called ‘the basics’.</p>

<p>How is this the number one link on Google? Specifically when looking up how to do basic things in Python, I get more and more of this filler content. I think it’s because Python attracts a lot of bootcamp and college students, people who <em>would pay</em> for training in order to learn Python. I remember one of the arguments I heard about why we were adopting Python at a previous company I worked for that was a mostly Rails (and some Elixir) shop was that Python was very easy to learn and search for on Google. I don’t think that’s true.</p>

<p>I don’t <em>know</em> if this is actually getting worse over time, but I’m noticing filler content more and more. It also seems to happen whenever searching for anything cooking related. For example: <a href="https://www.google.com/search?hl=en&amp;q=1%20cup%20flour%20to%20grams">converting 1 cup flour to grams</a>. To start, this search has a featured snippet that’s just wrong. But when you click into a few of the other links, the answer is obscured in some way (burried in a list of charts, answer is not made very apparent). Another cooking example - <a href="https://thesaltymarshmallow.com/homemade-belgian-waffle-recipe/">recipe sites</a>. It’s not a recipe unless there’s some excruciatingly long preamble, asking me to scroll at least 5 pages down before the actual recipe is available.</p>

<p>And I know the reason - ads and SEO. In order to be a good candidate for any ad revenue, you need enough ‘content’ before you can actually make any money. All this pointless content is posted just so people will spend longer on a site, which makes things more appealing for both ads and SEO. Except… well SEO should be optimising for content people actually want to see, and instead it’s optimizing for the amount of time spent on a site, regardless of whether the time on the site was actually productive.</p>

<p>But to me, the user, it feels like I’m paying for my answers with my time pointlessly scrolling around the page to find the answer I came to the site for. I can’t stand it; I feel like I am being duped, misled, being taken for a ride, just so I know how to make pancakes, or how to do a simple thing in common programming language. It feels like watching Dragon Ball Z back in the 00’s. Watching 5 episodes week after week, just for five minutes of time to elapse in the show.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Commuters prefer origin to destination transfers (154 pts)]]></title>
            <link>https://pedestrianobservations.com/2023/06/30/why-commuters-prefer-origin-to-destination-transfers/</link>
            <guid>36564608</guid>
            <pubDate>Sun, 02 Jul 2023 19:07:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pedestrianobservations.com/2023/06/30/why-commuters-prefer-origin-to-destination-transfers/">https://pedestrianobservations.com/2023/06/30/why-commuters-prefer-origin-to-destination-transfers/</a>, See on <a href="https://news.ycombinator.com/item?id=36564608">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p>It’s an empirical observation that rail riders who are faced with a transfer are much more likely to make the trip if it’s near their home than near their destination. Reinhard Clever’s since-linkrotted work gives an example from Toronto, and American commuter rail rider behavior in general; <a href="https://pedestrianobservations.com/2011/10/28/why-the-7-to-secaucus-wont-work/">I was discussing it from the earliest days of this blog</a>. He points out that American and Canadian commuter rail riders drive long distances just to get to a cheaper or faster park-and-ride stations, but are reluctant to take the train if they have any transfer at the city center end.</p>



<p>This pattern is especially relevant as, due to continued job sprawl, American rail reformers keep looking for new markets for commuter rail to serve and set their eyes on commutes to the suburbs. <a href="https://blog.bimajority.org/2018/11/12/a-further-examination-of-the-agricultural-branch-for-regional-rail/">Garrett Wollman is giving an example</a>, in the context of the Agricultural Branch, a low-usage freight line linking to the Boston-Worcester commuter line that could be used for local passenger rail service. Garrett talks about the potential ridership of the line, counting people living near it and people working near it. And inadvertently, his post makes it clear why the pattern Clever saw in Toronto is as it is.</p>



<p><strong>Residential and job sprawl</strong></p>



<p>The issue at hand is that residential sprawl and job sprawl both require riders to spend some time connecting to the train. The more typical example of residential sprawl involves isotropic single-family density in a suburban region, with commuters driving to the train station to get on a train to city center; they could be parking there or being dropped off by family, but in either case, the interface to the train for them is in their own car.</p>



<p>Job sprawl is different. Garrett points out that there are 79,000 jobs within two miles of a potential station on the Ag Branch, within the range of corporate shuttles. With current development pattern, rail service on the branch could follow the best practices there are and I doubt it would get 5% of those workers as riders, for all of the following reasons:</p>



<ul>
<li>The corporate shuttle is a bus, with all the discomfort that implies; it usually is also restricted in hours even more than traditional North American commuter rail – the frequency on the LIRR or even Caltrain is low off-peak but the trains do run all day, whereas corporate shuttles have a tendency to only run at peak. There is no own-car interface involved.</li>



<li>The traditional car-commuter train interface is to jobs in areas with traffic congestion and difficult parking. The jobs in the suburbs face neither constraint. Of note, Long Islanders working in Manhattan do transfer to the subway, because driving to the East Side to avoid the transfer from Penn Station is not a realistic option.</li>



<li>The traditional car-commuter train interface is to jobs in a city center served from all directions by commuter rail. In contrast, the jobs in the suburbs are only served by commuter rail along a single axis. There is a fair amount of reverse-peak ridership from San Francisco to Silicon Valley jobs or from New York to White Plains and Stamford jobs, even if at far lower rates than the traditional peak direction – but most people working at a suburban job center live in another suburb, own a car, and either commute in a different direction from that of the train or don’t live and work close enough to a station that the car-train-shuttle trip is faster than an all-car trip. </li>
</ul>



<p>Those features are immutable without further changes in urban design. Then there are other features that interact with the current timetables and fares. North American commuter rail has so many features designed to appeal to the type of person who drives everywhere and uses the train as a shuttle  extending their car-oriented lifestyle into the city – premium fares, heavy marketing as different from normal public transit, poor integration with said normal public transit – that interface with one’s own car is especially valuable, and interface with public transit is especially unvalued.</p>



<p>And yet, it’s clearly possible to make it work. How?</p>



<p><strong>How Europe makes it work</strong></p>



<p>Commuter trains in Europe (nobody calls them regional rail here – <a href="https://pedestrianobservations.com/2019/08/20/s-bahn-and-regionalbahn/">that term is reserved for hourly long-range trains</a>) get a lot of off-peak ridership and are not at all used exclusively by 9-to-5 commuters who drive for all other purposes. Some of this is to suburban job centers. How does this work, besides timetables and other operating practices that American reformers recognize as superior to what’s available in the US and Canada?</p>



<p>The primary answer is near-center jobs. Paris and La Défense have, between them, about 37% of the total jobs of Ile-de-France. Within the same land area, 100 km^2, both New York and Boston have a similar proportion of the jobs in their respective metro areas, about 35% each, as does San Francisco within the smaller definition of the metro area, excluding Silicon Valley. Ile-de-France’s work trip modal split is about 43%, metro New York’s is 33%, metro San Francisco’s is 17%, metro Boston’s is 12%.</p>



<p>So where Boston specifically fails is not so much office park jobs, such as those on Route 128, but near-center jobs. Its urban and suburban transit networks do a poor job of getting people to job centers like Longwood, the airport, Cambridge, and the Seaport. The same is true of San Francisco. New York’s network does a better but still mediocre job at connecting to Long Island City and Downtown Brooklyn, and a rather bad job at connecting to inner-suburban New Jersey jobs, but so many of those 35% jobs in the central 100 km^2 are in fact in the central 23 km^2 of the Manhattan core, and nearly half are in the central 4 km^2 comprising Midtown, that the poor service to the other 77 km^2 can be overlooked.</p>



<p>As far as commuter rail is concerned, the main difference in ridership between the main European networks – the Paris RER, the Berlin S-Bahn, and so on – and the American ones is how useful they are for plain urban service. Nearly all Berlin S-Bahn traffic is within the city, not the suburbs; the RER’s workhorse stations are mostly in dense inner suburbs that in most other countries would have been amalgamated into the city already.</p>



<p>To the extent that this relates to American commuter rail reforms, it’s about coverage within the city: multiple city stations, good (free, frequent) connections to local urban rail, high frequency all day to encourage urban travel (a train within the city that runs every half an hour might as well not run).</p>



<p>Suburban ridership is better here as well, but this piggybacks on very strong urban service, giving strong service from the suburbs to the city. Suburb-to-suburb commutes are done largely by car – Ile-de-France’s modal split is 43%, not 80%; there are fewer of them than in most of the US, but not fewer than in New York, Boston, or San Francisco.</p>



<p>But, well, Paris’s modal split is noticeably higher than the job share within the city – a job share that does include drivers. What gives?</p>



<p><strong>Suburban transit-oriented development</strong></p>



<p>TOD in the suburbs can create a pleasant enough rail commute that the modal split is respectable, if nothing like what is seen for jobs in Paris or Manhattan. However, for this to work, planners must eliminate the expression “corporate shuttle” from their lexicon.</p>



<p>Instead, suburban job sites must be placed right on top of the train station, or within walking distance along streets that are decently walkable. I can’t think of good Berlin examples – Berlin maintains high modal split through a strong center – but I can think of several Parisian ones: Marne-la-Vallée (including Disneyland), Noisy, Evry, Cergy. Those were often built simultaneously with greenfield suburban lines that were then connected to the RER, rather than on top of preexisting commuter lines.</p>



<p>They look nothing like American job sprawl. Here, for example, is Cergy:</p>



<figure><a href="https://pedestrianobservations.files.wordpress.com/2023/06/cergy.png"><img data-attachment-id="9549" data-permalink="https://pedestrianobservations.com/2023/06/30/why-commuters-prefer-origin-to-destination-transfers/cergy/" data-orig-file="https://pedestrianobservations.files.wordpress.com/2023/06/cergy.png" data-orig-size="1412,876" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cergy" data-image-description="" data-image-caption="" data-medium-file="https://pedestrianobservations.files.wordpress.com/2023/06/cergy.png?w=300" data-large-file="https://pedestrianobservations.files.wordpress.com/2023/06/cergy.png?w=830" src="https://pedestrianobservations.files.wordpress.com/2023/06/cergy.png?w=1024" alt="" srcset="https://pedestrianobservations.files.wordpress.com/2023/06/cergy.png?w=1024 1024w, https://pedestrianobservations.files.wordpress.com/2023/06/cergy.png?w=150 150w, https://pedestrianobservations.files.wordpress.com/2023/06/cergy.png?w=300 300w, https://pedestrianobservations.files.wordpress.com/2023/06/cergy.png?w=768 768w, https://pedestrianobservations.files.wordpress.com/2023/06/cergy.png 1412w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p>There are parking garages visible near the train stations, but also a massing of mid-rise residential and commercial buildings.</p>



<p>But speaking of residential, the issue is that employers looking for sites to locate to have no real reason to build offices on top of most suburban train stations – the likeliest highest and best usage is residential. In the case of American TOD, even the secondary-urban centers, like Worcester, probably have much more demand for residential than commercial TOD within walking distance of the train station – employers who are willing to pay near-train station premium rent might as well pay the higher premium of locating within the primary city, where the commuter shed is much larger.</p>



<p>In effect, the suburban TOD model does not counter the traditional monocentric urban layout. It instead extends it to a much larger scale. In this schema, the entirety of the city, and not just its central few square kilometers, is the monocenter, served by different lines with many stations on them. Berlin is ahead of the curve by virtue of its having multiple close-by centers as a Cold War legacy, but Paris is similar (its highest-intensity commercial TOD is in La Défense and in in-city sites like Bercy, on top of former railyards attached to Gare de Lyon).</p>



<p>At no point does this model include destination-end transfers in the suburbs. In the city, it does: a single line cannot cover all urban job sites; but the transfer is within the rapid transit system. But in the suburbs, the jobs that are serviceable by public transportation are within walking distance of the station. Shuttles may exist, but are secondary, and job sites that require them are and will always be auto-centric.</p>
			
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google search's death by a thousand cuts (259 pts)]]></title>
            <link>https://matt-rickard.com/google-searchs-death-by-a-thousand-cuts</link>
            <guid>36564042</guid>
            <pubDate>Sun, 02 Jul 2023 18:14:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matt-rickard.com/google-searchs-death-by-a-thousand-cuts">https://matt-rickard.com/google-searchs-death-by-a-thousand-cuts</a>, See on <a href="https://news.ycombinator.com/item?id=36564042">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p>Reddit communities are still private in protest of new API rules. Twitter moved beyond a login wall and is rate-limiting users. Users are frustrated but still using these sites.</p><p>But — what will happen to the Google Index? Millions of search results are effectively dead links. Users that refined Reddit search results via Google are now out of luck (Reddit’s search is inferior). Tweets in the search engine results page (SERP) now lead to a login wall for many users.</p><p>Advancements in <a href="https://matt-rickard.com/will-llms-disrupt-google" rel="noopener noreferrer nofollow">AI might disrupt Google Search</a> in a roundabout way:</p><p>Large models are trained on public data scraped via API. Content-heavy sites are most likely to be disrupted (why post on StackOverflow?) by models trained on their own data. Naturally, they want to restrict access and either (1) sell the data or (2) train their own models. This restriction prevents (or complicates) Google’ automatic scraping of the data for Search (and probably for training models, too).</p><p>Google will lose results, site by site — it will be Google Search’s death by a thousand cuts.</p><p>It’s estimated that Wikipedia shows up on the first page of 99% of searches on Google. What if Wikipedia started charging or restricting API access? It’s a dataset found in almost every large language model corpus. The Wikimedia Foundation is constantly looking for financial assistance (“please donate” banners) and has already launched an enterprise API product (Wikimedia Enterprise, 2021).</p><p>One by one, search results become dead links and are removed from the index. Users will start to rely on site-specific searches behind walled gardens. The first page of search results will not only be filled with ads but will be missing key results. Google may try to augment results with AI-generated answers, but (1) not all of these answers will be good enough, and (2) the data needed to train these answers will increasingly be found behind login or paywalls. Search might progressively get worse over the years until a new alternative arises.</p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[JavaScript Gom Jabbar (532 pts)]]></title>
            <link>https://frantic.im/javascript-gom-jabbar/</link>
            <guid>36564010</guid>
            <pubDate>Sun, 02 Jul 2023 18:12:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://frantic.im/javascript-gom-jabbar/">https://frantic.im/javascript-gom-jabbar/</a>, See on <a href="https://news.ycombinator.com/item?id=36564010">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  <header>
    
    
  </header>
  <p>You have been using JavaScript for 10 years. It’s time for your test. You are sitting in front of a computer. The test is simple: you have to open a package.json file and read it. The <code>package.json</code> is full of pain. You have to read it all.</p>
<p>You look at <code>version</code>, you haven’t reached 1.0 yet. Semver causes unpleasant memories, but you’ve learned to ignore them for so long that you don’t even notice the tickling sensation in your skull.</p>
<p>You wish you used a different <code>name</code> for your package, but some random internet person has squatted that name 7 years ago and never updated their package since. It’s only mildly discomforting. Maybe the test isn’t so bad after all?</p>
<p>Both <code>main</code> and <code>browser</code> fields are present, you sense traces of Isomorphic JavaScript. In a flash, you remember requiring <code>fs</code> module from your browser bundle. These memories are very unpleasant. The hacks you had to do to make it work were even more unpleasant.</p>
<p>The <code>type</code> is set to <code>module</code>. This has something to do with the migration from <code>requires</code> to <code>imports</code>. Why do we have to care about this, again? The extensive pain you’ve experienced trying to importing ES5 modules from ESM modules and vice versa overwhelms you again.</p>
<p>You make your way to <code>scripts</code>. What a hot, painful mess it is. You can’t look at them without your heart rate going to 150. lint, lintall, lintfast, lintdiff. Parallel runs, obscure arguments, double-escaping JSON-formatted arguments. Subcommands calling npm even through you switched to yarn and then pnpm. Thousands of variations, premutations and details make you shiver. Why do these things have to be here? Why do they need to be so complicated?</p>
<p>Some scripts still use <code>watchman</code>. Gotta remember to not use symlinks because it doesn’t support them (and the issue has been open since 2015). There’s also this gulp-based script that nobody has the guts to replace with anything else that’s considered more modern. You think that there’s actually no modern version of gulp but it feels outdated and you definitely want to get rid of it. The pains spreads from your head into your neck and shoulders.</p>
<p>The pain is barely tolerable when you reach <code>dependencies</code>. So, so many of them. There’s <code>left-pad</code>, the legendary tiny package that broke all internet, collectively causing the amount of pain and drama comparable to the destruction of Alderaan.</p>
<p>Every time you modify dependency list, some of the dependencies print out screens-worth of messages to your console, asking for donations, warning about breaking changes. You gave up trying to understand these. You only hope none of them are malicious enough to steal your secrets or ruin your computer. The threat of potential pain of that magnitute is frighting.</p>
<p>There’s also moment.js. You love that library, it has a really pleasent API. But the internet decided it’s too “mutable”, too fat, it doesn’t support treeshaking and now you have to migrate to date-fns. You haven’t started yet, but you already feel the painful refactoring in your bones.</p>
<p>Looking at every package in that list causes some amount of trauma recall. But what’s even more concerning is that the version of these packages are way behind what’s considered “current”. You know that you should upgrade them. But you also have tried that before and you know how much suffering it brings. Things will break in so many ways, big and loud ways, small and subtle ways.</p>
<p>The next thing in this damn file is <code>resolutions</code>. Yes, you remember this one. It’s a suffering you choose to avoid dealing with package upgrades.</p>
<p>You scroll down to <code>devDependencies</code>. You can’t remember the time when you only needed non-dev dependencies. Why do we have this split? Yes, right, to cause more pain.</p>
<p><code>eslint</code>. Its configuration got so strict that you can’t even write code anymore. Any small misstep and you get an angry red underline. Your CI is configured to treat any lint problem as the end of the world. It gives a false sense of security to your junior engineers on the team. You survived serveral holy wars on which rules to enable. The pain is proportional to the amount of <code>eslint-ignore</code>s you have all over your codebase. There’s a lot.</p>
<p>You also notice <code>postcss</code> hiding there. This package is a mystery to you. You don’t use it directly, it’s a requirement of a dependency of a dependency. But it’s the package that’s constantly causing you pain by throwing obscure C++ compilation errors on any new platform you try to <code>npm install</code> on. If CSS itself wasn’t painful enough.</p>
<p>Oh, dear <code>jest</code>. It started as a fast test runner. But now it’s big and fat, it depends on some babel packages while the rest of your app is transpiled by a mix of esbuild and swc. Properly configuring it with ESM and TypeScript was a PhD science project.</p>
<p>You stop to count how many tools and parsers work on your codebase: TypeScript, esbuild, swc, babel, eslint, prettier, jest, webpack, rollup, terser. You are not sure if you missed any. You are not sure if you want to know. The level of pain is so high you forget about anything else.</p>
<p><code>engines</code> prominently lists <code>node</code>. And while you hate it with the depth of your soul, you are not going to Bun or Deno because you know this will not stop the pain. This will only make the pain worse.</p>
<p>It’s the end of the file now. Final closing curly brace. You close the tab and take a breath. Look around. You are still alive, your hands and your brain intact. You survived. For now.</p>

  
  






  <div>
    <p>Hello! This text lives here to convince you to subscribe. If you are reading this, consider clicking that subscribe button for more details.</p>
    <p>I write about programming, software design and side projects <a href="https://frantic.im/subscribe/" target="_blank"><svg viewBox="0 0 800 800"><path d="M493 652H392c0-134-111-244-244-244V307c189 0 345 156 345 345zm71 0c0-228-188-416-416-416V132c285 0 520 235 520 520z"></path><circle cx="219" cy="581" r="71"></circle></svg> Subscribe</a></p>
  </div>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD CPU Use Among Linux Gamers Approaching 70% Marketshare (321 pts)]]></title>
            <link>https://www.phoronix.com/news/AMD-CPU-Linux-Gaming-67p</link>
            <guid>36563979</guid>
            <pubDate>Sun, 02 Jul 2023 18:09:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.phoronix.com/news/AMD-CPU-Linux-Gaming-67p">https://www.phoronix.com/news/AMD-CPU-Linux-Gaming-67p</a>, See on <a href="https://news.ycombinator.com/item?id=36563979">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img alt="AMD" src="https://www.phoronix.com/assets/categories/amd.webp" width="100" height="100"></p><p>
Besides being curious about the Steam Survey results for indicating the size of the Linux gaming marketshare as an overall percentage, one of the interesting metrics we are curious about each month is the AMD vs. Intel CPU marketshare for Linux gaming. AMD has been on quite an upward trajectory among Linux gamers/enthusiasts in recent years not only for their Radeon graphics cards with their popular open-source driver stack but their Ryzen CPUs have become extremely popular with Linux users. With <a href="https://www.phoronix.com/news/Steam-June-2023-Statistics">the new Steam Survey results for June</a>, AMD CPUs are found on nearly 70% of Linux gaming systems polled by Steam.
</p><p>
The June results put the AMD CPU marketshare for Linux users at 67%, a remarkable 7% increase month-over-month. In part that's due to the Steam Deck being powered by an AMD SoC but it's been a trend building for some time of AMD's increasing Ryzen CPU popularity among Linux users to their open-source driver work and continuing to build more good will with the community.
</p><p><img src="https://www.phoronix.net/image.php?id=2023&amp;image=steam_june_4"></p>
<p>In comparison, last June the AMD CPU Linux gaming marketshare <a href="https://www.phoronix.com/news/Steam-Linux-June-2022">came in at 45%</a> while Intel was at 54%. Or at the start of 2023, <a href="https://www.phoronix.com/news/Steam-Survey-January-2023">AMD CPUs were at a 55% marketshare</a> among Linux gamers. Or if going back six years, <a href="https://www.phoronix.com/news/Steam-Survey-July-2017">AMD CPU use among Linux gamers was a mere 18%</a> during the early Ryzen days.
</p><p><img src="https://www.phoronix.net/image.php?id=2023&amp;image=windows_cpu_gamers_june" alt="Windows CPU stats for June"></p>
<p>It's also the direct opposite on the Windows side. When looking at the Steam Survey results for June limited to Windows, there Intel has a 68% marketshare to AMD at 32%.
</p><p><a href="https://www.phoronix.com/image-viewer.php?id=2023&amp;image=amd_gaming_cpus_june_lrg" target="_blank"><img src="https://www.phoronix.net/image.php?id=2023&amp;image=amd_gaming_cpus_june_med" alt="AMD Ryzen boxes, cheers"></a></p>
<p>Beyond the Steam Deck, it's looking like AMD's efforts around open-source drivers, <a href="https://www.phoronix.com/news/Dell-Mario-On-AMD-Linux-Team">AMD expanding their Linux client (Ryzen) development efforts</a> over the past two years, promises around <a href="https://www.phoronix.com/search/OpenSIL">OpenSIL</a>, and other efforts commonly covered on Phoronix are paying off for AMD in wooing over their Linux gaming customer base.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Chinese Tech Terms Explained in English (169 pts)]]></title>
            <link>https://16x.engineer/2022/10/18/chinese-tech-terms.html</link>
            <guid>36563956</guid>
            <pubDate>Sun, 02 Jul 2023 18:06:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://16x.engineer/2022/10/18/chinese-tech-terms.html">https://16x.engineer/2022/10/18/chinese-tech-terms.html</a>, See on <a href="https://news.ycombinator.com/item?id=36563956">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
            <p>
              Chinese tech companies like ByteDance and Tencent are setting up
              engineering offices outside China. Local hires in these global
              offices are becoming a norm.
            </p>
            <p>
              These locally hired engineers typically have limited Chinese
              language proficiency. It is a challenge for them to understand the
              Chinese tech terms used in their daily work, and to communicate
              with their Chinese colleagues.
            </p>
            <p>
              Here are 5 common Chinese tech terms, along with explanations and
              examples of usage in a sentence.
            </p>
            <h2 id="huidu">Huidu</h2>
            <blockquote>
              <p>灰度 <span>huī dù</span></p>
            </blockquote>
            <p>
              <img src="https://16x.engineer/public/post-images/optimized/chinese-huidu.webp" alt="Huidu in English example screenshot">
            </p>
            <p>
              Huidu is a widely used Chinese technical term in companies like
              Tencent and Alibaba. Huidu means rolling out experimental new
              features to a small subset of users for testing.
            </p>
            <p>
              The implication of huidu is that the new feature may be launched
              officially (at a later date), or scrapped if it did not have the
              desired outcome.
            </p>
            <h2 id="huidu-in-english">Huidu in English</h2>
            <p>
              The literal translation of huidu is “grayscale”. Intuitively,
              huidu represents the transition state between black and white.
            </p>
            <p>
              Note that huidu can be used to describe product features,
              functionalities, or code deployment.
            </p>
            <p>
              When describing a product feature, the close equivalent would be
              <strong>beta release</strong> or
              <strong>phased rollout</strong> in English.
            </p>
            <p>
              When used for describe code deployment,
              <strong>canary release</strong> is also an accurate translation.
            </p>
            <h2 id="examples-of-huidu-in-a-sentence">
              Examples of huidu in a sentence
            </h2>
            <ul>
              <li>
                <p>
                  <em>WeChat is <strong>huidu</strong> testing a new feature that
                    allows users to register an additional WeChat account with
                    an existing phone number.</em>
                </p>
              </li>
              <li>
                <p>
                  <em>Lenovo started <strong>huiduing</strong> new version of
                    operating system for its tablets.</em>
                </p>
              </li>
            </ul>
            <h2 id="lunzi">Lunzi</h2>
            <blockquote><p>轮子 lún zi</p></blockquote>
            <p>
              <img src="https://16x.engineer/public/post-images/optimized/chinese-lunzi.webp" alt="Lunzi in English example screenshot">
            </p>
            <p>
              Lunzi describes tools, libraries or frameworks that have been
              invented or reinvented multiple times.
            </p>
            <p>
              Typically it is used for new things that are created to solve a
              specific problem, but bear resemblance to something existing.
            </p>
            <h2 id="lunzi-in-english">Lunzi in English</h2>
            <p>
              The literal translation for lunzi is “wheel”, and the origin of
              the word is likely from the English phrase “reinvent the wheel”.
            </p>
            <p>
              Despite the apparent origin from English, lunzi as a word has no
              equivalent translation in English.
            </p>
            <p>
              The literal translation “<strong>wheel</strong>” might be the best
              candidate.
            </p>
            <h2 id="example-of-lunzi-in-a-sentence">
              Example of lunzi in a sentence
            </h2>
            <ul>
              <li>
                <p>
                  <em>Engineers need create <strong>lunzi</strong> to fulfill
                    KPIs or get promoted.</em>
                </p>
              </li>
              <li>
                <p>
                  <em><strong>Lunzi</strong> created by others do not fulfil our
                    specific needs.</em>
                </p>
              </li>
            </ul>
            <h2 id="chendian">Chendian</h2>
            <blockquote><p>沉淀 chén diàn</p></blockquote>
            <p>
              <img src="https://16x.engineer/public/post-images/optimized/chinese-chendian.webp" alt="Chendian in English example screenshot">
            </p>
            <p>
              Chendian is not a technical term, but it is commonly used in
              Chinese tech companies. It means consolidating learnings from past
              experience.
            </p>
            <p>
              It also implies coming up with a systematic solution to a
              recurring problem.
            </p>
            <h2 id="chendian-in-english">Chendian in English</h2>
            <p>
              The literal translation for chendian is “chemical precipitation”,
              drawing from the intuition that things are consolidated from
              liquid form to solid state.
            </p>
            <p>
              At the beginning, things are messy and fluid. But as you learn and
              progress, they because more clear and structured.
            </p>
            <p>
              There seems to be no equivalent term in English to describe this
              concept.
            </p>
            <h2 id="example-of-chendian-in-a-sentence">
              Example of chendian in a sentence
            </h2>
            <ul>
              <li>
                <p>
                  <em>Team lead has some expectations for new interns, such as
                    critical thinking, <strong>chendian</strong> and knowledge
                    sharing.</em>
                </p>
              </li>
              <li>
                <p>
                  <em>Our team managed to <strong>chendian</strong> tools and
                    organizational capabilities from the double 11 sale.</em>
                </p>
              </li>
            </ul>
            <h2 id="dapan">Dapan</h2>
            <blockquote><p>大盘 dà pán</p></blockquote>
            <p>
              <img src="https://16x.engineer/public/post-images/optimized/chinese-dapan.webp" alt="Dapan in English example screenshot">
            </p>
            <p>
              In Chinese tech companies, dapan is used to describe dashboards
              with various charts, where you can monitor key system metrics or
              business metrics in real time.
            </p>
            <p>
              It is especially important for e-commerce companies like Alibaba
              and JD who run big sales events like double 11.
            </p>
            <h2 id="dapan-in-english">Dapan in English</h2>
            <p>
              Dapan originally refers to the big screens in stock exchanges
              showing market data.
            </p>
            <p>
              The close equivalent of dapan in English would be “<strong>monitoring dashboard</strong>”.
            </p>
            <h2 id="examples-of-dapan-in-a-sentence">
              Examples of dapan in a sentence
            </h2>
            <ul>
              <li>
                <p>
                  <em>Remember to monitor <strong>dapan</strong> closely when
                    performing production deployment.</em>
                </p>
              </li>
              <li>
                <p>
                  <em>Popular <strong>dapan</strong> products usually come with
                    WYSIWYG editors.</em>
                </p>
              </li>
            </ul>
            <h2 id="maidian">Maidian</h2>
            <blockquote>
              <p>埋点 <span>mái diǎn</span></p>
            </blockquote>
            <p>
              <img src="https://16x.engineer/public/post-images/optimized/chinese-maidian.webp" alt="Maidian in English example screenshot">
            </p>
            <p>
              In Chinese companies, maidian refers to embedding of tracking code
              to monitor and analyse user behaviour.
            </p>
            <p>
              This is typically required by product and data analytics teams,
              and the tracking code is embedded in the product by front-end
              engineers.
            </p>
            <h2 id="maidian-in-english">Maidian in English</h2>
            <p>
              As individual characters, “mai” means burying, and “dian” means
              points. The phrase “maidian” literally means “embedding points” or
              “burying seeds”.
            </p>
            <p>
              The intuition is that by “embedding” the “seed” in the product,
              the user journey can be mapped out later.
            </p>
            <p>
              It can be translated into <strong>tracking</strong>,
              <strong>tagging</strong> or
              <strong>user data analytics</strong> depending on the context.
            </p>
            <h2 id="examples-of-maidian-in-a-sentence">
              Examples of maidian in a sentence
            </h2>
            <ul>
              <li>
                <p>
                  <em>Data analytics team need to design a rigorous
                    <strong>maidian</strong> system and produce
                    <strong>maidian</strong> document to support subsequent user
                    behaviour analysis.</em>
                </p>
              </li>
              <li>
                <p>
                  <em>The cost of <strong>maidian</strong> is high because every
                    widget with user interactions needs to embed
                    <strong>maidian</strong> code.</em>
                </p>
              </li>
            </ul>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Economic inequality cannot be explained by individual bad choices, study finds (133 pts)]]></title>
            <link>https://phys.org/news/2023-06-economic-inequality-individual-bad-choices.html</link>
            <guid>36563815</guid>
            <pubDate>Sun, 02 Jul 2023 17:53:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phys.org/news/2023-06-economic-inequality-individual-bad-choices.html">https://phys.org/news/2023-06-economic-inequality-individual-bad-choices.html</a>, See on <a href="https://news.ycombinator.com/item?id=36563815">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
										
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2023/economic-inequality-ca.jpg" data-src="https://scx2.b-cdn.net/gfx/news/2023/economic-inequality-ca.jpg" data-sub-html="Correlation between ten biases within 3346 participants showed each bias was largely unique and not collinear with other biases assessed, with the exception of overplacement&nbsp;and overestimation (which rely on the presence of some biases). Credit: <i>Scientific Reports</i> (2023). DOI: 10.1038/s41598-023-36339-2">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2023/economic-inequality-ca.jpg" alt="Economic inequality cannot be explained by individual bad choices" title="Correlation between ten biases within 3346 participants showed each bias was largely unique and not collinear with other biases assessed, with the exception of overplacement&nbsp;and overestimation (which rely on the presence of some biases). Credit: Scientific Reports (2023). DOI: 10.1038/s41598-023-36339-2" width="685" height="530">
             <figcaption>
                Correlation between ten biases within 3346 participants showed each bias was largely unique and not collinear with other biases assessed, with the exception of overplacement&nbsp;and overestimation (which rely on the presence of some biases). Credit: <i>Scientific Reports</i> (2023). DOI: 10.1038/s41598-023-36339-2
            </figcaption>        </figure>
    </div>
<p>A global study led by a researcher at Columbia University Mailman School of Public Health and published in the journal <i>Scientific Reports</i> finds that economic inequality on a social level cannot be explained by bad choices among the poor nor by good decisions among the rich. Poor decisions were the same across all income groups, including for people who have overcome poverty.

										  
											        </p>
										 
										 											  
<p>While <a href="https://phys.org/tags/economic+inequality/" rel="tag">economic inequality</a> continues to rise within countries, efforts to address it have been largely ineffective, particularly those involving behavioral approaches. It has been often implied, but until now not tested, that choice patterns among low-income individuals may be a factor impeding behavioral interventions aimed at improving upward economic mobility.
</p><p>The study is based on online surveys in 22 languages with close to 5,000 participants from 27 countries in Asia, Europe, North America, and South America. Decision-making ability was measured through 10 individual biases, including (1) temporal discounting, not preferring immediate funds over larger future gains; (2) overestimation, or thinking you are better than you are at making decisions; (3) over-placement, or thinking you are better than the <a href="https://phys.org/tags/average+person/" rel="tag">average person</a> at making decisions; and (4) extremeness aversion, or taking the "middle option" simply because it seems safer than the highest or lowest.
</p><p>Taken along with <a href="https://phys.org/news/2022-07-economic-inequality-instability-impacts-long-term.html">related work</a> showing that temporal discounting is tied more to the broader societal economic environment rather than individual financial circumstances, the new findings are a major validation of arguments stating that poorer individuals are not uniquely prone to <a href="https://phys.org/tags/cognitive+biases/" rel="tag">cognitive biases</a> that alone explain protracted poverty.
</p><p>"Our research does not reject the notion that individual behavior and decision-making may directly relate to upward economic mobility. Instead, we narrowly conclude that biased decision-making does not alone explain a significant proportion of population-level economic inequality," says first author Kai Ruggeri, Ph.D., assistant professor in the Department of Health Policy and Management at Columbia Public Health.
</p><p>"Low-income individuals are not uniquely prone to cognitive biases linked to bad financial decisions. Instead, scarcity is more likely a greater driver of these decisions," Ruggeri adds.
										 																				
																				</p><div>
																						<p><strong>More information:</strong>
												Kai Ruggeri et al, The persistence of cognitive biases in financial decisions across economic groups, <i>Scientific Reports</i> (2023).  <a data-doi="1" href="https://dx.doi.org/10.1038/s41598-023-36339-2" target="_blank">DOI: 10.1038/s41598-023-36339-2</a>
																						
																						</p>
																					</div>
                               											
																					
                              										                                        
										<!-- print only -->
										<div>
											 <p><strong>Citation</strong>:
												Economic inequality cannot be explained by individual bad choices, study finds (2023, June 29)
												retrieved 2 July 2023
												from https://phys.org/news/2023-06-economic-inequality-individual-bad-choices.html
											 </p>
											 <p>
											 This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
											 part may be reproduced without the written permission. The content is provided for information purposes only.
											 </p>
										</div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMIGAlive – Play Amiga games online with people across the world (143 pts)]]></title>
            <link>https://www.amigalive.com/</link>
            <guid>36563780</guid>
            <pubDate>Sun, 02 Jul 2023 17:50:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.amigalive.com/">https://www.amigalive.com/</a>, See on <a href="https://news.ycombinator.com/item?id=36563780">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
		<main id="main" role="main">

			
				
<article id="post-65" class="page">
	
	<div>
		

		
<h2><strong>Welcome to the AmigaLive project!</strong></h2>



<h4>AmigaLive is a front-end application which utilizes the  netplay capabilities of the FS-UAE emulator. AmigaLive makes it more simple than ever before to have 2 or more people from around the world, connect and play the same game, or even use the same software, as if sharing the same Amiga computer hardware and peripherals<strong>.</strong></h4>



<div><figure><img decoding="async" width="820" height="462" src="https://www.amigalive.com/wp-content/uploads/Amiga-Facebook-2.jpg" alt="" srcset="https://www.amigalive.com/wp-content/uploads/Amiga-Facebook-2.jpg 820w, https://www.amigalive.com/wp-content/uploads/Amiga-Facebook-2-300x169.jpg 300w, https://www.amigalive.com/wp-content/uploads/Amiga-Facebook-2-768x433.jpg 768w, https://www.amigalive.com/wp-content/uploads/Amiga-Facebook-2-128x72.jpg 128w, https://www.amigalive.com/wp-content/uploads/Amiga-Facebook-2-32x18.jpg 32w" sizes="(max-width: 820px) 100vw, 820px"></figure></div>



<h4>FS-UAE  is the friendliest Amiga emulator which integrates Amiga emulation code from WinUAE and is available for all major platforms, such as Windows, Mac OS-X and Linux.</h4>



<p><strong>*This project offers an unofficial distribution of the&nbsp;FS-UAE&nbsp;software which may not be up to date, therefore please do not send any bug reports to the developer (Frode Solheim) regarding AmigaLive issues.</strong></p>



<h2><strong>Requirements:</strong></h2>



<ul><li><a href="https://www.amigalive.com/download/"><strong>Download</strong></a><b><strong> </strong>the latest version of the AmigaLive bundle</b><br><b>(Everyone needs to have the same major version)</b></li><li><strong>A modern computer running a 64-bit OS </strong><br><strong>(Linux, Mac OS-X or Windows)</strong></li><li><strong>High speed internet<br>(Wi-Fi works but a wired LAN connection is recommended)</strong></li><li><strong>Any input device:&nbsp;Joystick, GamePad, Keyboard or Mouse.&nbsp;<br>A joystick can also be emulated by using different keyboard layouts, the default keys are: cursor keys and right Ctrl as fire 1 button<br>(A digital&nbsp;2 button Joystick/GamePad is recommended)</strong></li><li><strong><strong>You need people ready to play with you, but if you don’t know many that are interested or support Amiga emulation, w<strong><strong>e welcome you to join </strong></strong></strong>us on the <a href="https://discord.gg/r8pZTyV">AmigaLive Discord</a> and give us a shout. You can notify whoever is available by using the @here command in the Discord channels 🙂</strong></li></ul>




	</div><!-- .entry-content -->
</article><!-- #post-## -->

			
		</main><!-- #main -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[First 'tooth regrowth' medicine moves toward clinical trials in Japan (851 pts)]]></title>
            <link>https://mainichi.jp/english/articles/20230609/p2a/00m/0sc/026000c</link>
            <guid>36563590</guid>
            <pubDate>Sun, 02 Jul 2023 17:33:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mainichi.jp/english/articles/20230609/p2a/00m/0sc/026000c">https://mainichi.jp/english/articles/20230609/p2a/00m/0sc/026000c</a>, See on <a href="https://news.ycombinator.com/item?id=36563590">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<!-- cxenseparse_start -->

<div>
<figure>
<div>
<a data-href="https://cdn.mainichi.jp/vol1/2023/06/09/20230609p2a00m0na021000p/9.jpg?1" data-lightbox="photos" data-title="(Getty)">
<span>
<img src="https://cdn.mainichi.jp/vol1/2023/06/09/20230609p2a00m0na021000p/6.jpg?1" alt="">

</span>

</a>
</div>
<figcaption>(Getty)</figcaption>
</figure>
</div>

<div>
<figure>
<div>
<a data-href="https://cdn.mainichi.jp/vol1/2023/06/09/20230609p2a00m0na022000p/9.jpg?1" data-lightbox="photos" data-title="Katsu Takahashi, head of the dentistry and oral surgery department at the Medical Research Institute Kitano Hospital, is seen in the city of Osaka's Kita Ward on May 16, 2023. (Mainichi/Mirai Nagira)">
<span>
<img src="https://cdn.mainichi.jp/vol1/2023/06/09/20230609p2a00m0na022000p/7.jpg?1" alt="">

</span>

</a>
</div>
<figcaption>Katsu Takahashi, head of the dentistry and oral surgery department at the Medical Research Institute Kitano Hospital, is seen in the city of Osaka's Kita Ward on May 16, 2023. (Mainichi/Mirai Nagira)</figcaption>
</figure>
</div>
<p>
    TOKYO -- A Japanese research team is making progress on the development of a groundbreaking medication that may allow people to grow new teeth, with clinical trials set to begin in July 2024.
</p>
<!-- cxenseparse_end -->

<!-- cxenseparse_start -->
<p>
    The tooth regrowth medicine is intended for people who lack a full set of adult teeth due to congenital factors. The team is aiming to have it ready for general use in 2030.
</p>
<p>
    In prior animal experiments, the medicine prompted the growth of "third-generation" teeth following baby teeth and then permanent adult teeth.
</p>
<p>
    "The idea of growing new teeth is every dentist's dream. I've been working on this since I was a graduate student. I was confident I'd be able to make it happen," said Katsu Takahashi, lead researcher and head of the dentistry and oral surgery department at the Medical Research Institute Kitano Hospital in the city of Osaka.
</p>

<div>
<figure>
<div>
<a data-href="https://cdn.mainichi.jp/vol1/2023/06/09/20230609p2a00m0na023000p/7.jpg?1" data-lightbox="photos" data-title="A new tooth is seen growing in a mouse treated with the tooth regrowth medicine. (Photo provided by Katsu Takahashi, head of the dentistry and oral surgery department at the Medical Research Institute Kitano Hospital)">
<span>
<img src="https://cdn.mainichi.jp/vol1/2023/06/09/20230609p2a00m0na023000p/6.jpg?1" alt="">

</span>

</a>
</div>
<figcaption>A new tooth is seen growing in a mouse treated with the tooth regrowth medicine. (Photo provided by Katsu Takahashi, head of the dentistry and oral surgery department at the Medical Research Institute Kitano Hospital)</figcaption>
</figure>
</div>
<p>
    Anodontia is a congenital condition that causes the growth of fewer than a full set of teeth, present in around 1% of the population. Genetic factors are thought to be the major cause for the one-tenth of anodontia patients who lack six or more teeth, a condition categorized as oligodontia. These conditions are also known as tooth agenesis. People who grow up with tooth agenesis struggle with basic abilities like chewing, swallowing and speaking from a young age, which can negatively impact their development.
</p>
<p>
    After completing a dentistry degree, Takahashi went on to graduate studies in molecular biology at Kyoto University in 1991. Afterwards, he studied in the U.S.
</p>
<p>
    Around that time, research around the world had begun to pinpoint genes that, when deleted, would cause genetically modified mice to grow fewer teeth. "The number of teeth varied through the mutation of just one gene. If we make that the target of our research, there should be a way to change the number of teeth (people have)," Takahashi said of his thoughts at the time.
</p>
<p>
    <b>Global attention</b>
</p>

<div>
<figure>
<div>
<a data-href="https://cdn.mainichi.jp/vol1/2023/06/09/20230609p2a00m0na024000p/6.jpg?1" data-lightbox="photos" data-title="The front teeth of a ferret treated with tooth regrowth medicine are seen in a photo provided by Katsu Takahashi, head of the dentistry and oral surgery department at the Medical Research Institute Kitano Hospital. The medicine induced the growth of an additional seventh tooth (center).">
<span>
<img src="https://cdn.mainichi.jp/vol1/2023/06/09/20230609p2a00m0na024000p/6.jpg?1" alt="">

</span>

</a>
</div>
<figcaption>The front teeth of a ferret treated with tooth regrowth medicine are seen in a photo provided by Katsu Takahashi, head of the dentistry and oral surgery department at the Medical Research Institute Kitano Hospital. The medicine induced the growth of an additional seventh tooth (center).</figcaption>
</figure>
</div>
<p>
    It was around 2005, when he delved further into the subject at Kyoto University after returning to Japan, that he began to see a bright path for his continued research. The researchers found that mice lacking a certain gene had an increased number of teeth. A protein called USAG-1, synthesized by the gene, was found to limit the growth of teeth. In other words, blocking the action of that protein could allow more teeth to grow.
</p>
<p>
    Takahashi's research team narrowed their focus onto USAG-1, and developed a neutralizing antibody medicine able to block the protein's function. In experiments in 2018, mice with a congenitally low number of teeth were given medicine that resulted in new teeth coming through. The research results were published in a U.S. scientific paper in 2021, and gained much attention as the beginnings of the world's first tooth regeneration medicine.
</p>
<p>
    Work is now underway to get the drug ready for human use. Once confirmed to have no ill effects on the human body, it will be aimed at treating children aged 2 to 6 who exhibit anodontia. "We hope to pave the way for the medicine's clinical use," Takahashi said.
</p>
<p>
    <b> Medicine could be game-changer</b>
</p>
<p>
    If successful, a drug to regenerate teeth may be a game-changer for the entire field of dentistry.
</p>
<p>
    Animals including sharks and some reptile species can continuously regrow teeth. It's been assumed that humans only grow two sets of teeth in their lifetime, but in fact, there is evidence that we also have the "buds" for a third set.
</p>
<p>
    Around 1% of the population exhibits the converse of anodontia: hyperdontia, a congenital condition causing a higher-than-normal number of teeth. According to research by Takahashi's team, one in three such cases manifests as the growth of a third set of teeth. Takahashi believes that in most cases, humans' ability to grow a third set was lost over time.
</p>
<p>
    When the researchers applied the drug to ferrets, they grew an additional seventh front tooth. As the new teeth grew in between the existing front teeth and were of the same shape, the medicine is thought to have induced the generation of third-set teeth in the animals.
</p>
<p>
    When treatment of teeth is no longer possible due to severe cavities or erosion of the dental sockets, known as pyorrhea, people lose them and need to rely on dental appliances such as dentures. The ability to grow third-generation teeth could change that. "In any case, we're hoping to see a time when tooth-regrowth medicine is a third choice alongside dentures and implants," Takahashi said.
</p>
<p>
    For further information or inquiries about Takahashi's research, please visit https://www.kitano-hp.or.jp/toothreg/ (in Japanese).
</p>
<p>
    (Japanese original by Mirai Nagira, Science &amp; Environment News Department)
</p>
<!-- cxenseparse_end -->

<!--| tools BGN |-->

<!--| tools END |-->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Are You Sure You Want to Use MMAP in Your Database Management System? (2022) (174 pts)]]></title>
            <link>https://db.cs.cmu.edu/mmap-cidr2022/</link>
            <guid>36563187</guid>
            <pubDate>Sun, 02 Jul 2023 16:48:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://db.cs.cmu.edu/mmap-cidr2022/">https://db.cs.cmu.edu/mmap-cidr2022/</a>, See on <a href="https://news.ycombinator.com/item?id=36563187">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>
                <i>Memory-mapped</i> (<tt>MMAP</tt>) file I/O is an OS-provided feature that
                maps the contents of a file on secondary storage into a program’s
                address space. The program then accesses pages via pointers as if
                the file resided entirely in memory. The OS transparently loads
                pages only when the program references them and automatically evicts
                pages if memory fills up.
            </p>
            <p>
                <tt>MMAP</tt>‘s perceived ease of use has seduced <i>database management system</i>
                (DBMS) developers for decades as a viable alternative to
                implementing a buffer pool. There are, however, severe correctness
                and performance issues with <tt>MMAP</tt> that are not immediately apparent.
                Such problems make it difficult, if not impossible, to use <tt>MMAP</tt>
                correctly and efficiently in a modern DBMS. In fact, several popular
                DBMSs initially used <tt>MMAP</tt> to support larger-than-memory databases
                but soon encountered these hidden perils, forcing them to switch to
                managing file I/O themselves after significant engineering costs.
            </p>
            <p>
                In this way, <tt>MMAP</tt> and DBMSs are like coffee and spicy food: an
                unfortunate combination that becomes obvious after the fact.
            </p>
            <p>
                Since developers keep trying to use <tt>MMAP</tt> in new DBMSs, we wrote this
                paper to provide a warning to others that <tt>MMAP</tt> is not a suitable
                replacement for a traditional buffer pool. We discuss the main
                shortcomings of <tt>MMAP</tt> in detail, and our experimental analysis
                demonstrates clear performance limitations. Based on these findings,
                we conclude with a prescription for when DBMS developers might
                consider using <tt>MMAP</tt> for file I/O.
            </p>
            <p><small><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16"> <path d="M14.5.5a.5.5 0 0 0-1 0V2H1a1 1 0 0 0-1 1v2h16V3a1 1 0 0 0-1-1h-.5V.5ZM2.5 4a.5.5 0 1 1 0-1 .5.5 0 0 1 0 1Zm2 0a.5.5 0 1 1 0-1 .5.5 0 0 1 0 1Zm7.5-.5a.5.5 0 1 1-1 0 .5.5 0 0 1 1 0Zm1.5.5a.5.5 0 1 1 0-1 .5.5 0 0 1 0 1Zm-7-1h3a.5.5 0 0 1 0 1h-3a.5.5 0 0 1 0-1Zm-2 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm.5-1.5a.5.5 0 1 1-1 0 .5.5 0 0 1 1 0Zm6.5 1.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm0-1a.5.5 0 1 0 0-1 .5.5 0 0 0 0 1Z"></path> <path d="M16 6H0v8a1 1 0 0 0 1 1h14a1 1 0 0 0 1-1V6ZM4.5 13a2.5 2.5 0 1 1 0-5 2.5 2.5 0 0 1 0 5Zm7 0a2.5 2.5 0 1 1 0-5 2.5 2.5 0 0 1 0 5Z"></path> </svg> Recommended Music for this Paper:<br><a href="https://youtu.be/JW8mwaw2-xc" target="_youtube" rel="noopener">Dr. Dre – High Powered (featuring RBX)</a></small></p>
          </div><div>
                <h3>Citation</h3>
                <pre>@inproceedings{crotty22-mmap💩,
  author = {Crotty, Andrew and Leis, Viktor and Pavlo, Andrew},
  title = {Are You Sure You Want to Use MMAP in Your Database Management System?},
  booktitle = {{CIDR} 2022, Conference on Innovative Data Systems Research},
  year = {2022},
}</pre>
            </div><div>
            <h3>Acknowledgments</h3>
            <p>
                This paper is the culmination of an unhealthy, years-long obsession with the idea of developers incorrectly using mmap in their DBMSs. The authors would like to thank everyone who contributed and provided helpful feedback: <a href="https://lcy.im/">Chenyao Lou</a> (PKU), <a href="https://en.wikipedia.org/wiki/File:David_Andersen_-_Professor_Street_Urchin.jpg">David “Greasy” Andersen</a> (CMU), <a href="https://www.cs.cmu.edu/~kaminsky/">Michael Kaminsky</a> (BrdgAI), <a href="https://db.in.tum.de/~neumann/">Thomas Neumann</a> (TUM), <a href="https://osg.tuhh.de/People/dietrich/">Christian Dietrich</a> (TUHH), <a href="https://www.linkedin.com/in/toddlipcon/">Todd Lipcon</a> (<a href="https://lipcon.org/">lipcon.org</a>), and <a href="https://people.ece.ubc.ca/sasha/">Sasha Fedorova</a> (UBC).
            </p>
            <p>
                This work was supported (in part) by the NSF (<a href="http://www.nsf.gov/awardsearch/showAward?AWD_ID=1846158">IIS-1846158</a>, <a href="http://www.nsf.gov/awardsearch/showAward?AWD_ID=1423210">III-1423210</a>, <a href="http://www.nsf.gov/awardsearch/showAward?AWD_ID=1252522">DGE-1252522</a>), research grants from Google and Snowflake, and the <a href="https://sloan.org/grant-detail/8638">Alfred P. Sloan Research Fellowship</a> program.
            </p>
            
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Designing the First Apple Macintosh: The Engineers’ Story (1984) (117 pts)]]></title>
            <link>https://spectrum.ieee.org/apple-macintosh</link>
            <guid>36562958</guid>
            <pubDate>Sun, 02 Jul 2023 16:25:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/apple-macintosh">https://spectrum.ieee.org/apple-macintosh</a>, See on <a href="https://news.ycombinator.com/item?id=36562958">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-elid="2661376756" data-post-url="https://spectrum.ieee.org/apple-macintosh" data-authors="Fred Guterl" data-headline="Designing the First Apple Macintosh: The Engineers’ Story" data-page-title="Designing the First Apple Macintosh: The Engineers’ Story - IEEE Spectrum"><p><strong></strong><strong>In 1979 the Macintosh</strong> personal computer existed only as the pet idea of Jef Raskin, a veteran of the Apple II team, who had proposed that <a href="https://www.apple.com/" rel="noopener noreferrer" target="_blank">Apple Computer Inc.</a> make a low-cost “appliance”-type computer that would be as easy to use as a toaster. Mr. Raskin believed the computer he envisioned, which he called Macintosh, could sell for US $1000 if it was manufactured in high volume and used a powerful microprocessor executing tightly written software.</p><p>Mr. Raskin’s proposal did not impress anyone at Apple Computer enough to bring much money from the board of directors or much respect from Apple engineers. The company had more pressing concerns at the time: <a href="https://spectrum.ieee.org/apple-lisa" target="_self">the major Lisa workstation project</a> was getting under way, and there were problems with the reliability of the Apple III, the revamped version of the highly successful Apple II.</p><div id="rebelltitem2" data-id="2" data-reload-ads="false" data-is-image="False" data-href="https://spectrum.ieee.org/apple-macintosh/particle-2" data-basename="particle-2" data-post-id="2661376756" data-published-at="1686938434" data-use-pagination="False"><p>Although the odds seemed against it in 1979, the Macintosh, designed by a handful of inexperienced engineers and programmers, is now recognized as a technical milestone in personal computing. Essentially a slimmed-down version of the Lisa workstation with many of its software features, the Macintosh sold for $2495 at its introduction in early 1984; the Lisa initially sold for $10,000. Despite criticism of the Macintosh—that it lacks networking capabilities adequate for business applications and is awkward to use for some tasks—the computer is considered by Apple to be its most important weapon in the war with IBM for survival in the personal-computer business.</p><p>From the beginning, the Macintosh project was powered by the dedicated drive of two key players on the project team. For Burrell Smith, who designed the Macintosh digital hardware, the project represented an opportunity for a relative unknown to demonstrate outstanding technical talents. For Steven Jobs, the 29-year-old chairman of Apple and the Macintosh project’s director, it offered a chance to prove himself in the corporate world after a temporary setback: although he cofounded Apple Computer, the company had declined to let him manage the Lisa project. Mr. Jobs contributed relatively little to the technical design of the Macintosh, but he had a clear vision of the product from the beginning. He challenged the project team to design the best product possible and encouraged the team by shielding them from bureaucratic pressures within the company.</p><h2>Burrell Smith and the Early Mac Design<em></em></h2><p>Mr. Smith, who was a repairman in the Apple II maintenance department in 1979, had become hooked on microprocessors several years earlier during a visit to the electronics-industry area south of San Francisco known as Silicon Valley. He dropped out of liberal-arts studies at the Junior College of Albany, New York, to pursue the possibilities of microprocessors—there isn’t anything you can’t do with those things, he thought. Mr. Smith later became a repairman in Cupertino, Calif., where he spent much time studying the cryptic logic circuitry of the Apple II, designed by company cofounder Steven Wozniak.</p><p>Mr. Smith’s dexterity in the shop impressed Bill Atkinson, one of the Lisa designers, who introduced him to Mr. Raskin as “the man who’s going to design your Macintosh.” Mr. Raskin replied noncommittally, “We’ll see about that.”</p><p>However, Mr. Smith managed to learn enough about Mr. Raskin ‘s conception of the Macintosh to whip up a makeshift prototype using a Motorola 6809 microprocessor, a television monitor, and an Apple II. He showed it to Mr. Raskin, who was impressed enough to make him the second member of the Macintosh team.</p><p>But the fledgling Macintosh project was in trouble. The Apple board of directors wanted to cancel the project in September 1980 to concentrate on more important projects, but Mr. Raskin was able to win a three-month reprieve.</p><p>Meanwhile Steve Jobs, then vice president of Apple, was having trouble with his own credibility within the company. Though he had sought to manage the Lisa computer project, the other Apple executives saw him as too inexperienced and eccentric to entrust him with such a major undertaking, and he had no formal business education. After this rejection, “he didn’t like the lack of control he had,” noted one Apple executive. “He was looking for his niche.”</p><p>Mr. Jobs became interested in the Macintosh project, and, possibly because few in the company thought the project had a future, Mr. Jobs was made its manager. Under his direction, the design team became as compact and efficient as the Macintosh was to<em></em>be—a group of engineers working at a distance from all the meetings and paper-pushing of the corporate mainstream. Mr. Jobs, in recruiting the other members of the Macintosh team, lured some from other companies with promises of potentially lucrative stock options.</p></div><p>The Macintosh project “was known in the company as ‘Steve’s folly.’”</p><div id="rebelltitem4" data-id="4" data-reload-ads="false" data-is-image="False" data-href="https://spectrum.ieee.org/apple-macintosh/particle-4" data-basename="particle-4" data-post-id="2661376756" data-published-at="1686938434" data-use-pagination="False"><p>With Mr. Jobs at the helm, the project gained some credibility among the board of directors—but not much. According to one team member, it was known in the company as “Steve’s folly.” But Mr. Jobs lobbied for a bigger budget for the project and got it. The Macintosh team grew to 20 by early 1981.</p><p>The decision on what form the Macintosh would take was left largely to the design group. At first the members had only the basic principles set forth by Mr. Raskin and Mr. Jobs to guide them, as well as the example set by the Lisa project. The new machine was to be easy to use and inexpensive to manufacture. Mr. Jobs wanted to commit enough money to build an automated factory that would produce about 300 000 computers a year. So one key challenge for the design group was to use inexpensive parts and to keep the parts count low.</p><p>Making the computer easy to use required considerable software for the user-computer interface. The model was, of course, the Lisa workstation with its graphic “windows” to display simultaneously many different programs. “Icons,” or little pictures, were used instead of cryptic computer terms to represent a selection of programs on the screen; by moving a “mouse,’’ a box the size of a pack of cigarettes, the user manipulated a cursor on the screen. The Macintosh team redesigned the software of the Lisa from scratch to make it operate more efficiently, since the Macintosh was to have far less memory than the 1 million bytes of the Lisa. But the Macintosh software was also required to operate quicker than the Lisa software, which had been criticized for being slow.</p><h2>Defining the Mac as the Project Progressed</h2><p>The lack of a precise definition for the Macintosh project was not a problem. Many of the designers preferred to define the computer as they went along. “Steve allowed us to crystallize the problem and the solution simultaneously,” recalled Mr. Smith. The method put strain on the design team, since they were continually evaluating design alternatives. “We were swamped in detail,” Mr. Smith said. But this way of working also led to a better product, the designers said, because they had the freedom to seize opportunities during the design stage to enhance the product.</p><p>Such freedom would not have been possible had the Macintosh project been structured in the conventional way at Apple, according to several of the designers. “No one tried to control us,” said one. “Some managers like to take control, and though that may be good for mundane engineers, it isn’t good if you are self-motivated.”</p></div><p><img id="95050" data-rm-shortcode-id="3321510644ca62def7dd22030ec28f21" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-table-detailing-differences-between-star-lisa-and-mac-computers.jpg?id=34133796&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/a-table-detailing-differences-between-star-lisa-and-mac-computers.jpg?id=34133796&amp;width=980" width="1240" height="502" alt="A table detailing differences between Star, Lisa, and Mac computers"></p><div id="rebelltitem6" data-id="6" data-reload-ads="false" data-is-image="False" data-href="https://spectrum.ieee.org/apple-macintosh/particle-6" data-basename="particle-6" data-post-id="2661376756" data-published-at="1686938961" data-use-pagination="False"><p>Central to the success of this method was the small, closely knit nature of the design group, with each member being responsible for a relatively large portion of the total design and free to consult other members of the team when considering alternatives. For example, Mr. Smith, who was well acquainted with the price of electronic components from his early work on reducing the cost of the Apple II, made many decisions about the economics of Macintosh hardware without time-consuming consultations with purchasing agents. Because communication among team members was good, the designers shared their areas of expertise by advising each other in the working stages, rather than waiting for a final evaluation from a group of manufacturing engineers. Housing all members of the design team in one small office made communicating easier. For example, it was simple for Mr. Smith to consult a purchasing agent about the price of parts if he needed to, because the purchasing agent worked in the same building.</p><p>Andy Hertzfeld, who transferred from the Apple II software group to design the Macintosh operating software, noted, “In lots of other projects at Apple, people argue about ideas. But sometimes bright people think a little differently. Somebody like Burrell Smith would design a computer on paper and people would say. ‘It’ll never work.’ So instead Burell builds it lightning fast and has it working before the guy can say anything.”</p></div><p>“When you have one person designing the whole computer, he knows that a little leftover gate in one part may be used in another part.”<br>—Andy Herzfeld</p><div id="rebelltitem8" data-id="8" data-reload-ads="false" data-is-image="False" data-href="https://spectrum.ieee.org/apple-macintosh/particle-8" data-basename="particle-8" data-post-id="2661376756" data-published-at="1686938961" data-use-pagination="False"><p>The closeness of the Macintosh group enabled it to make design tradeoffs that would not have been possible in a large organization, the team members contended. The interplay between hardware and software was crucial to the success of the Macintosh design, using a limited memory and few electronic parts to perform complex operations. Mr. Smith, who was in charge of the computer’s entire digital hardware design, and Mr. Herzfeld became close friends and often collaborated. “When you have one person designing the whole computer,” Mr. Hertzfeld observed, “he knows that a little leftover gate in one part may be used in another part.”</p><p>To promote interaction among the designers, one of the first things that Mr. Jobs did in taking over the Macintosh project was to arrange special office space for the team. In contrast to Apple’s corporate headquarters, identified by the company logo on a sign on its well-trimmed lawn, the team’s new quarters, behind a Texaco service station, had no sign to identify them and no listing in the company telephone directory. The office, dubbed Texaco Towers, was an upstairs, low-rent, plasterboard-walled, “tacky-carpeted” place, “the kind you’d find at a small law outfit,’’ according to Chris Espinosa, a veteran of the original Apple design team and an early Macintosh draftee. It resembled a house more than an office, having a communal area much like a living room, with smaller rooms off to the side for more privacy in working or talking. The decor was part college dormitory, part electronics repair shop: art posters, beanbag chairs, coffee machines, stereo systems, and electronic equipment of all sorts scattered about.</p></div><p>“Whenever a competitor came out with a product, we would buy and dismantle it, and it would kick around the office.”<br>—Chris Espinosa</p><div id="rebelltitem10" data-id="10" data-reload-ads="false" data-is-image="False" data-href="https://spectrum.ieee.org/apple-macintosh/particle-10" data-basename="particle-10" data-post-id="2661376756" data-published-at="1686938961" data-use-pagination="False"><p>There were no set work hours and initially not even a schedule for the development of the Macintosh. Each week, if Mr. Jobs was in town (often he was not), he would hold a meeting at which the team members would report what they had done the previous week. One of the designers’ sidelines was to dissect the products of their competitors. “Whenever a competitor came out with a product, we would buy and dismantle it, and it would kick around the office,” recalled Mr. Espinosa.</p><p>In this way, they learned what they did not want their product to be. In their competitors’ products, Mr. Smith saw a propensity for using connectors and slots for inserting printed-circuit boards—a slot for the video circuitry, a slot for the keyboard circuitry, a slot for the disk drives, and memory slots. Behind each slot were buffers to allow signals to pass onto and off the printed-circuit board properly. The buffers meant delays in the computers’ operations, since several boards shared a backplane, and the huge capacitance required for multiple PC boards slowed the backplane. The number of parts required made the competitors’ computers hard to manufacture, costly, and less reliable. The Macintosh team resolved that their PC would have but two printed-circuit boards and no slots, buffers, or backplane.</p></div><p><img id="fe15a" data-rm-shortcode-id="fa433347b51dd81951de454cbc4d9e73" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/looking-into-the-open-back-of-a-computer-with-metal-enclosures-and-wires-and-the-back-of-a-cathode-ray-tube.jpg?id=34133831&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/looking-into-the-open-back-of-a-computer-with-metal-enclosures-and-wires-and-the-back-of-a-cathode-ray-tube.jpg?id=34133831&amp;width=980" width="975" height="999" alt="Looking into the open back of a computer, with metal enclosures and wires, and the back of a cathode ray tube."><small><p>A challenge in building the Macintosh was to offer sophisticated software using the fewest and least-expensive parts.</p></small></p><div id="rebelltitem12" data-id="12" data-reload-ads="false" data-is-image="False" data-href="https://spectrum.ieee.org/apple-macintosh/particle-12" data-basename="particle-12" data-post-id="2661376756" data-published-at="1686939853" data-use-pagination="False"><p>
	To squeeze the needed components onto the board, Mr. Smith planned the Macintosh to perform specific functions rather than operate as a flexible computer that could be tailored by programmers for a wide variety of applications. By rigidly defining the configuration of the Macintosh and the functions it would perform, he eliminated much circuitry. Instead of providing slots into which the user could insert printed-circuit boards with such hardware as memory or coprocessors, the designers decided to incorporate many of the basic functions of the computer in read-only memory, which is more reliable. The computer would be expanded not by slots, but through a high-speed serial port.
</p><h2>Writing the Mac’s Software</h2><p>
	The software designers were faced in the beginning with often-unrealistic schedules. “We looked for any place where we could beg, borrow, or steal code,” Mr. Herzfeld recalled. The obvious place for them to look was the Lisa workstation. The Macintosh team wanted to borrow some of the Lisa’s software for drawing graphics on the bit-mapped display. In 1981, Bill Atkinson was refining the Lisa graphics software, called <a href="https://computerhistory.org/blog/macpaint-and-quickdraw-source-code/" target="_blank">Quickdraw</a>, and began to work part-time implementing it for the Macintosh.
</p><p>
	Quickdraw was a scheme for manipulating bit maps to enable applications programmers to construct images easily on the Macintosh bit-mapped display. The Quickdraw program allows the programmer to define and manipulate a region—a software representation of an arbitrarily shaped area of the screen. One such region is a rectangular window with rounded comers, used throughout the Macintosh software. Quickdraw also allows the programmer to keep images within defined boundaries, which make the windows in the Macintosh software appear to hold data. The programmer can unite two regions, subtract one from the other, or intersect them.
</p><p>
	In Macintosh, the Quickdraw program was to be tightly written in assembly-level code and etched permanently in ROM. It would serve as a foundation for higher-level software to make use of graphics.
</p><p>
	Quickdraw was “an amazing graphics package,” Mr. Hertzfeld noted, but it would have strained the capabilities of the 6809 microprocessor, the heart of the early Macintosh prototype. <a href="https://www.motorola.com/us/" rel="noopener noreferrer" target="_blank">Motorola Corp.</a> announced in late 1980 that the 68000 microprocessor was available, but that chip was new and unproven in the field, and at $200 apiece it was also expensive. Reasoning that the price of the chip would come down before Apple was ready to start mass-producing the Macintosh, the Macintosh designers decided to gamble on the Motorola chip.
</p><p>
	Another early design question for the Macintosh was whether to use the Lisa operating system. Since the Lisa was still in the early stages of design, considerable development would have been required to tailor its operating system for the Macintosh. Even if the Lisa had been completed, rewriting its software in assembly code would have been required for the far smaller memory of the Macintosh. In addition, the Lisa was to have a multitasking operating system, using complex circuitry and software to run more than one computer program at the same time, which would have been too expensive for the Macintosh. Thus the decision was made to write a Macintosh operating system from scratch, working from the basic concepts of the Lisa. Simplifying the Macintosh operating system posed the delicate problem of restricting the computer’s memory capacity enough to keep it inexpensive but not so much as to make it inflexible.
</p><p>
	The Macintosh would have no multitasking capability but would execute only one applications program at a time. Generally, a multitasking operating system tracks the progress of each of the programs it is running and then stores the entire state of each program—the values of its variables, the location of the program counter, and so on. This complex operation requires more memory and hardware than the Macintosh designers could afford. However, the illusion of multitasking was created by small programs built into the Macintosh system software. Since these small programs—such as one that creates the images of a calculator on the screen and does simple arithmetic—operate in areas of memory separate from applications, they can run simultaneously with applications programs.
</p></div><p><img id="636d6" data-rm-shortcode-id="205c2e1d605fa7662334de56f1f600c0" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-flow-chart-with-a-screen-shot-of-windows-on-a-computer.jpg?id=34133849&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/a-flow-chart-with-a-screen-shot-of-windows-on-a-computer.jpg?id=34133849&amp;width=980" width="2163" height="2948" alt="A flow chart with a screen shot of windows on a computer"><small><p>Embedding Macintosh software in 64 kilobytes of read-only memory increased the reliability of the computer and simplified the hardware [A]. About one third of the ROM software is the operating system. One third is taken up by Quickdraw, a program for representing shapes and images for the bit-mapped display. The remaining third is devoted to the user ­interface toolbox, which handles the display of windows, text editing, menus, and the like. The user interface of the Macintosh includes pull-down menus, which appear only when the cursor is placed over the menu name and a button on the mouse is pressed. Above, a user examining the ‘file’ menu selects the open command, which causes the computer to load the file (indicated by darkened icon) from disk into internal memory. The Macintosh software was designed to make the toolbox routines optional for programmers; the applications program offers the choice of whether or not to handle an event [B].</p></small></p><div id="rebelltitem14" data-id="14" data-reload-ads="false" data-is-image="False" data-href="https://spectrum.ieee.org/apple-macintosh/particle-14" data-basename="particle-14" data-post-id="2661376756" data-published-at="1686939853" data-use-pagination="False"><p>Since the Macintosh used a memory-mapped scheme, the 68000 microprocessor required no memory management, simplifying both the hardware and the software. For example, the 68000 has two modes of operation: a user mode, which is restricted so that a programmer cannot inadvertently upset the memory-management scheme; and a supervisor mode, which allows unrestricted access to all of the 68000’s commands. Each mode uses its own stack of pointers to blocks of memory. The 68000 was rigged to run only in the supervisor mode, eliminating the need for the additional stack. Although seven levels of interrupts were available for the 68000, only three were used.</p><p>Another simplification was made in the Macintosh’s file structure, exploiting the small disk space with only one or two floppy disk drives. In the Lisa and most other operating systems, two indexes access a program on floppy disk, using up precious random-access memory and increasing the delay in fetching programs from a disk. The designers decided to use only one index for the Macintosh—a block map, located in RAM, to indicate the location of a program on a disk. Each block map represented one volume of disk space.</p><p>This scheme ran into unexpected difficulties and may be modified in future versions of the Macintosh, Mr. Hertzfeld said. Initially, the Macintosh was not intended for business users, but as the design progressed and it became apparent that the Macintosh would cost more than expected, Apple shifted its marketing plan to target business users. Many of them add hard disk drives to the Macintosh, making the block-map scheme unwieldy.</p><p>By January 1982, Mr. Hertzfeld began working on software for the Macintosh, perhaps the computer’s most distinctive feature, which he called the user-interface toolbox.</p><p>The toolbox was envisioned as a set of software routines for constructing the windows, pull-down menus, scroll bars, icons, and other graphic objects in the Macintosh operating system. Since RAM space would be scarce on the Macintosh (it initially was to have only 64 kilobytes), the toolbox routines were to be a part of the Macintosh’s operating software; they would use the Quickdraw routines and operate in ROM.</p><p>It was important however, not to handicap applications programmers—who could boost sales of the Macintosh by writing programs for it—by restricting them to only a few toolbox routines in ROM. So the toolbox code was designed to fetch definition functions—routines that use Quickdraw to create a graphic image such as a window—from either the systems disk or an applications disk. In this way, an applications programmer could add definition functions for a program, which Apple could incorporate in later versions the Macintosh by modifying the system disk. “We were nervous about putting (the toolbox) in ROM,” recalled Mr. Hertzfeld, “We knew that after the Macintosh was out, programmers would want to add to the toolbox routines.”</p><p>Although the user could operate only one applications program at a time, he could transfer text or graphics from one applications program to another with a toolbox routine called scrapbook. Since the scrapbook and the rest of the toolbox routines were located in ROM, they could run along with applications programs, giving the illusion of multitasking. The user would cut text from one program into the scrapbook, close the program, open another, and paste the text from the scrapbook. Other routines in the toolbox, such as the calculator, could also operate simultaneously with applications programs.</p><p>Late in the design of the Macintosh software, the designers realized that, to market the Macintosh in non-English-speaking countries, an easy way of translating text in programs into foreign languages was needed. Thus computer code and data were separated in the software to allow translation without unraveling a complex computer program, by scanning the data portion of a program. No programmer would be needed for translation.</p><h2>Placing an Early Bet on the 68000 Chip</h2><p><a href="https://spectrum.ieee.org/chip-hall-of-fame-motorola-mc68000-microprocessor" target="_self">The 68000</a>, with a 16-bit data bus and 32-bit internal registers and a 7.83-megahertz clock, could grab data in relatively large chunks. Mr. Smith dispensed with separate controllers for the mouse, the disk drives, and other peripheral functions. “We were able to leverage off slave devices,” Mr. Smith explained, “and we had enough throughput to deal with those devices in a way that appeared concurrent to the user.”</p><p>When Mr. Smith suggested implementing the mouse without a separate controller, several members of the design team argued that if the main microprocessor was interrupted each time the mouse was moved, the movement of the cursor on the screen would always lag. Only when Mr. Smith got the prototype up and running were they convinced it would work.</p></div><p><img id="30420" data-rm-shortcode-id="f28ef5693cbdbda7a8717d5ff74cdc5a" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-table-listing-macintosh-prototypes-and-their-features.jpg?id=34133860&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/a-table-listing-macintosh-prototypes-and-their-features.jpg?id=34133860&amp;width=980" width="1240" height="931" alt="A table listing Macintosh prototypes and their features"></p><div id="rebelltitem16" data-id="16" data-reload-ads="false" data-is-image="False" data-href="https://spectrum.ieee.org/apple-macintosh/particle-16" data-basename="particle-16" data-post-id="2661376756" data-published-at="1686939853" data-use-pagination="False"><p>Likewise, in the second prototype, the disk drives were controlled by the main microprocessor. “In other computers,” Mr. Smith noted, “the disk controller is a brick wall between the disk and the CPU, and you end up with a poor-performance, expensive disk that you can lose control of. It’s like buying a brand-new car complete with a chauffeur who insists on driving everywhere.</p><p>The 68000 was assigned many duties of the disk controller and was linked with a disk-controller circuit built by Mr. Wozniak for the Apple II. “Instead of a wimpy little 8-bit microprocessor out there, we have this incredible 68000—it’s the world’s best disk controller,” Mr. Smith said.</p><p>Direct-memory-access circuitry was designed to allow the video screen to share RAM with the 68000. Thus the 68000 would have access to RAM at half speed during the live portion of the horizontal line of the video screen and at full speed during the horizontal and vertical retrace. [See diagram, below.]</p></div><p><img id="3ed14" data-rm-shortcode-id="9e1db1aa3f5f379aa73cc5af021c87c5" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-block-diagram-ending-in-line-drawings-of-a-speaker-and-a-cathode-ray-tube.jpg?id=34133917&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/a-block-diagram-ending-in-line-drawings-of-a-speaker-and-a-cathode-ray-tube.jpg?id=34133917&amp;width=980" width="2155" height="2676" alt="A block diagram ending in line drawings of a speaker and a cathode ray tube"><small><p>The 68000 microprocessor, which has exclusive access to the read-only memory of the Macintosh, fetches commands from ROM at full speed—.83 megahertz. The 68000 shares the random-access memory with the video and sound circuitry, having access to RAM only part of the time [A]; it fetches instructions from RAM at an average speed of about 6 megahertz. The video and sound instructions are loaded directly into the video-shift register or the sound-counter, respectively. Much of the “glue” circuitry of the Macintosh is contained in eight programmable-array-logic chips. The Macintosh’s ability to play four independent voices was added relatively late in the design, when it was realized that most of the circuitry needed already existed in the video circuitry [B]. The four voices are added in software and the digital samples stored in memory. During the video retrace, sound data is fed into the sound buffer.</p></small></p><div id="rebelltitem18" data-id="18" data-reload-ads="false" data-is-image="False" data-href="https://spectrum.ieee.org/apple-macintosh/particle-18" data-basename="particle-18" data-post-id="2661376756" data-published-at="1686939853" data-use-pagination="False"><p>While building the next prototype, Mr. Smith saw several ways to save on digital circuitry and increase the execution speed of the Macintosh. The 68000 instruction set allowed Mr. Smith to embed subroutines in ROM. Since the 68000 has exclusive use of the address and data buses of the ROM, it has access to the ROM routines at up to the full clock speed. The ROM serves somewhat as a high-speed cache memory. While building the next prototype, Mr. Smith saw several ways to save on digital circuitry and increase the execution speed of the Macintosh. The 68000 instruction set allowed Mr. Smith to embed subroutines in ROM. Since the 68000 has exclusive use of the address and data buses of the ROM, it has access to the ROM routines at up to the full clock speed. The ROM serves somewhat as a high-speed cache memory.</p><p>The next major revision in the original concept of the Macintosh was made in the computer’s display. Mr. Raskin had proposed a computer that could be hooked up to a standard television set. However, it became clear early on that the resolution of television display was too coarse for the Macintosh. After a bit of research, the designers found they could increase the display resolution from 256 by 256 dots to 384 by 256 dots by including a display with the computer. This added to the estimated price of the Macintosh, but the designers considered it a reasonable tradeoff.</p><p>To keep the parts count low, the two input/output ports of the Macintosh were to be serial. The decision to go with this was a serious one, since the future usefulness of the computer depended largely on its efficiency when hooked up to printers, local-area networks, and other peripherals. In the early stages of development, the Macintosh was not intended to be a business product, which would have made networking a high priority.</p></div><p>“We had an image problem. We wore T-shirts and blue jeans with holes in the knees, and we had a maniacal conviction that we were right about the Macintosh, and that put some people off.”<br>—Chris Espinosa</p><div id="rebelltitem20" data-id="20" data-reload-ads="false" data-is-image="False" data-href="https://spectrum.ieee.org/apple-macintosh/particle-20" data-basename="particle-20" data-post-id="2661376756" data-published-at="1686939853" data-use-pagination="False"><p>The key factor in the decision to use one high-speed serial port was the introduction in the spring of 1981 of the Zilog Corp.’s 85530 serial-communications controller, a single chip to replace two less expensive conventional parts—” vanilla” chips—in the Macintosh. The risks in using the Zilog chip were that it had not been proven in the field and it was expensive, almost $9 apiece. In addition, Apple had a hard time convincing Zilog that it seriously intended to order the part in high volumes for the Macintosh.</p><p>“We had an image problem,” explained Mr. Espinosa. “We wore T-shirts and blue jeans with holes in the knees, and we had a maniacal conviction that we were right about the Macintosh, and that put some people off. Also, Apple hadn’t yet sold a million Apple IIs. How were we to convince them that we would sell a million Macs?”</p><p>In the end, Apple got a commitment from Zilog to supply the part, which Mr. Espinosa attributes to the negotiating talents of Mr. Jobs. The serial input/output ports “gave us essentially the same bandwidth that a memory-mapped parallel port would,” Mr. Smith said. Peripherals were connected to serial ports in a daisy-chain configuration with the Apple bus network.</p><h2>Designing the Mac’s Factory Without the Product</h2><p>In the fall of 1981, as Mr. Smith worked on the fourth Macintosh prototype, the design for the Macintosh factory was getting under way. Mr. Jobs hired <a href="https://spectrum.ieee.org/steve-jobs-realworld-leading-ladies-gather" target="_self">Debi Coleman</a>, who was then working as financial manager at <a href="https://www.hp.com/us-en/home.html" target="_blank">Hewlett-Packard Co.</a> in Cupertino, Calif., to handle the finances of the Macintosh project. A graduate of Stanford University with a master’s degree in business ad­ministration, Ms. Coleman was a member of a task force at HP that was studying factories, quality management, and inventory management. This was good training for Apple, for Mr. Jobs was intent on using such concepts to build a highly automated manufacturing plant for the Macintosh in the United States.</p><p>Briefly he considered building the plant in Texas, but since the designers were to work closely with the manufacturing team in the later stages of the Macintosh design, he decided to locate the plant at Fremont, Calif., less than a half-hour’s drive from Apple’s Cupertino headquarters.</p><p>Mr. Jobs and other members of the Macintosh team made frequent tours of automated plants in various industries, particularly in Japan. At long meetings held after the visits, the manufacturing group discussed whether to borrow certain methods they had observed.</p></div><p><img id="32edd" data-rm-shortcode-id="b1e99327cd193c63c5d6c7a5f5a830aa" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/hands-hold-a-round-cone-like-object-up-to-the-front-of-a-macintosh-computer-display.jpg?id=34133930&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/hands-hold-a-round-cone-like-object-up-to-the-front-of-a-macintosh-computer-display.jpg?id=34133930&amp;width=980" width="1619" height="1301" alt="Hands hold a round cone-like object up to the front of a Macintosh computer display"><small><p>The Macintosh factory borrowed assembly ideas from other computer plants and other industries. A method of testing the brightness of cathode-ray tubes was borrowed from television manufacturers.</p></small></p><div id="rebelltitem23" data-id="23" data-reload-ads="false" data-is-image="False" data-href="https://spectrum.ieee.org/apple-macintosh/particle-23" data-basename="particle-23" data-post-id="2661376756" data-published-at="1686939853" data-use-pagination="False"><p>The Macintosh factory design was based on two major concepts. The first was “just-in-time” inventory, calling for vendors to deliver parts for the Macintosh frequently, in small lots, to avoid excessive handling of components at the factory and reduce damage and storage costs. The second concept was zero-defect parts, with any defect on the manufacturing line immediately traced to its source and rectified to prevent recurrence of the error.</p><p>The factory, which was to churn out about a half million Macintosh computers a year (the number kept increasing), was designed to be built in three stages: first, equipped with stations for workers to insert some Macintosh components, delivered to them by simple robots; second, with robots to insert components instead of workers; and third, many years in the future, with “integrated” automation, requiring virtually no human operators. In building the factory, “Steve was willing to chuck all the traditional ideas about manufacturing and the relationship between design and manufacturing,” Ms. Coleman noted. “He was willing to spend whatever it cost to experiment in this factory. We planned to have a major revision every two years.”</p><p>By late 1982, before Mr. Smith had designed the final Macintosh prototype, the designs of most of the factory’s major subassemblies were frozen, and the assembly stations could be designed. About 85 percent of the components on the digital-logic printed-circuit board were to be inserted automatically, and the remaining 15 percent were to be surface-mounted devices inserted manually at first and by robots in the second stage of the factory. The production lines for automatic insertion were laid out to be flexible; the number of stations was not defined until trial runs were made. The materials-delivery system, designed with the help of engineers recruited from Texas Instruments in Dallas, Texas, divided small and large parts between receiving doors at the materials distribution center. The finished Macintoshes coming down the conveyor belt were to be wrapped in plastic and stuffed into boxes using equipment adapted from machines used in the wine industry for packaging bottles.</p></div><p><img id="3b9b2" data-rm-shortcode-id="001bc2cbe2fca207d4d093a4fed3b94a" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-closeup-of-gears-and-rollers-with-strips-of-electronic-components-feeding-through-them.jpg?id=34133981&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/a-closeup-of-gears-and-rollers-with-strips-of-electronic-components-feeding-through-them.jpg?id=34133981&amp;width=980" width="979" height="1001" alt="A closeup of gears and rollers with strips of electronic components feeding through them"><small><p>Most of the discrete components in the Macintosh are inserted automatically into the printed-circuit boards.</p></small></p><div id="rebelltitem22" data-id="22" data-reload-ads="false" data-is-image="False" data-href="https://spectrum.ieee.org/apple-macintosh/particle-22" data-basename="particle-22" data-post-id="2661376756" data-published-at="1686939853" data-use-pagination="False"><p>As factory construction progressed, pressure built on the Macintosh design team to deliver a final prototype. The designers had been working long hours but with no deadline set for the computer’s introduction. That changed in the middle of 1981, after Mr. Jobs imposed a tough and sometimes unrealistic schedule, reminding the team repeatedly that “real artists ship” a finished product. In late 1981, when IBM announced its personal computer, the Macintosh marketing staff began to refer to a “window of opportunity” that made it urgent to get the Macintosh to customers.</p><p>“We had been saying, ‘We’re going to finish in six months’ for two years,” Mr. Hertzfeld recalled.</p><p>The new urgency led to a series of design problems that seemed to threaten the Macintosh dream.</p><h2>The Mac Team Faces Impossible Deadlines</h2><p>The computer’s circuit density was one bottleneck. Mr. Smith had trouble paring enough circuitry off his first two prototypes to squeeze them onto one logic board. In addition, he needed faster circuitry for the Macintosh display. The horizontal resolution was only 384 dots—not enough room for the 80 characters of text needed for the Macintosh to compete as a word processor. One suggested solution was to use the word-processing software to allow an 80-character line to be seen by horizontal scrolling. However, most standard computer displays were capable of holding 80 characters, and the portable computers with less capability were very inconvenient to use.</p><p>Another problem with the Macintosh display was its limited dot density. Although the analog circuitry, which was being designed by Apple engineer George Crow, accommodated 512 dots on the horizontal axis, Mr. Smith’s digital circuitry—which consisted of bipolar logic arrays—did not operate fast enough to generate the dots. Faster bipolar circuitry was considered but rejected because of its high-power dissipation and its cost. Mr. Smith could think of but one alternative: combine the video and other miscellaneous circuitry on a single custom n-channel MOS chip.</p><p>Mr. Smith began designing such a chip in February 1982. During the next six months the size of the hypothetical chip kept growing. Mr. Jobs set a shipping target of May 1983 for the Macintosh but, with a backlog of other design problems, Burrell Smith still had not finished designing the custom chip, which was named after him: the IBM (Integrated Burrell Machine) chip.</p></div><div id="rebelltitem27" data-id="27" data-reload-ads="false" data-is-image="False" data-href="https://spectrum.ieee.org/apple-macintosh/particle-27" data-basename="particle-27" data-post-id="2661376756" data-published-at="1686942238" data-use-pagination="False"><p>Meanwhile, the Macintosh offices were moved from Texaco Towers to more spacious quarters at the Apple headquarters, since the Macintosh staff had swelled to about 40. One of the new employees was Robert Belleville, whose previous employer was <a href="https://spectrum.ieee.org/xerox-parc" target="_self">the Xerox Palo Alto Research Corp</a>. At Xerox he had designed the hardware for the Star workstation—which, with its windows, icons. and mouse, might be considered an early prototype of the Macintosh. When Mr. Jobs offered him a spot on the Macintosh team, Mr. Belleville was impatiently waiting for authorization from Xerox to proceed on a project he had proposed that was similar to the Macintosh—a low-cost version of the Star.</p><p>As<em></em>the new head of the Macintosh engineering, Mr. Belleville faced the task of directing Mr. Smith, who was proceeding on what looked more and more like a dead-end course. Despite the looming deadlines, Mr. Belleville tried a soft-sell approach.</p><p>“I asked Burrell if he really needed the custom chip,” Mr. Belleville recalled. “He said yes. I told him to think about trying something else.”</p><p>After thinking about the problem for three months, Mr. Smith concluded in July 1982 that “the difference in size between this chip and the state of Rhode Island is not very great.” He then set out to design the circuitry with higher-speed programmable-array logic—as he had started to do six months earlier. He had assumed that higher resolution in the horizontal video required a faster clock speed. But he realized that he could achieve the same effect with clever use of faster bipolar-logic chips that had become available only a few months earlier. By adding several high­-speed logic circuits and a few ordinary circuits, he pushed the resolution up to 512 dots.</p><p>Another advantage was that the PALs were a mature technology and their electrical parameters could tolerate large variations from the specified values, making the Macintosh more stable and more reliable—important characteristics for a so-called appliance product. Since the electrical characteristics of each integrated circuit may vary from those of other ICs made in different batches, the sum of the variances of 50<em></em>or so components in a computer may be large enough to threaten the system’s integrity.</p></div><p>“It became an intense and almost religious argument about the purity of the system’s design versus the user’s freedom to configure the system as he liked. We had weeks of argument over whether to add a few pennies to the cost of the machine.”<br>—Chris Espinosa</p><div id="rebelltitem29" data-id="29" data-reload-ads="false" data-is-image="False" data-href="https://spectrum.ieee.org/apple-macintosh/particle-29" data-basename="particle-29" data-post-id="2661376756" data-published-at="1686942238" data-use-pagination="False"><p>Even as late as the summer of 1982, with one deadline after another blown, the Macintosh designers were finding ways of adding features to the computer. After the team disagreed over the choice of a white background for the video with black characters or the more typical white-on-black, it was suggested that both options be made available to the user through a switch on the back of the Macintosh. But this compromise led to debates about other questions.</p><p>“It became an intense and almost religious argument,” recalled Mr. Espinosa, “about the purity of the system’s design versus the user’s freedom to configure the system as he liked. We had weeks of argument over whether to add a few pennies to the cost of the machine.”</p><p>The designers, being committed to the Macintosh, often worked long hours to refine the system. A programmer might spend many night hours to reduce the time needed to format a disk from three minutes to one. The reasoning was that expenditure of a Macintosh programmer’s time amounted to little in comparison with a reduction of two minutes in the formatting time. “If you take two extra minutes per user, times a million people, times 50 disks to format, that’s a lot of the world’s time,” Mr. Espinosa explained.</p><p>But if the group’s commitment to refinements often kept them from meeting deadlines, it paid off in tangible design improvements. “There was a lot of competition for doing something very bright and creative and amazing,” said Mr. Espinosa. “People were so bright that it became a contest to astonish them.”</p><p>The Macintosh team’s approach to working—“like a Chautauqua, with daylong affairs where people would sit and talk about how they were going to do this or that’”—sparked creative thinking about the Macintosh’s capabilities. When a programmer and a hardware designer started to discuss how to implement the sound generator, for instance, they were joined by one of several nontechnical members of the team—marketing staff, finance specialists, secretaries—who remarked how much fun it would be if the Macintosh could sound four distinct voices at once so the user could program it to play music. That possibility excited the programmer and the hardware engineer enough to spend extra hours in designing a sound generator with four voices.</p><p>The payoff of such discussions with nontechnical team members, Mr. Espinosa said, “was coming up with all those glaringly evident things that only somebody completely ignorant could come up with. If you immerse yourself in a group that doesn’t know the technical limitations, then you get a group mania to try and deny those limitations. You start trying to do the impossible—and once in a while succeeding.”</p></div><p>Nobody had even considered designing a four-voice [sound] generator—that is, not until “group mania” set in.</p><div id="rebelltitem31" data-id="31" data-reload-ads="false" data-is-image="False" data-href="https://spectrum.ieee.org/apple-macintosh/particle-31" data-basename="particle-31" data-post-id="2661376756" data-published-at="1686942238" data-use-pagination="False"><p>The sound generator in the original Macintosh was quite simple—a one-bit register connected to a speaker. To vibrate the speaker, the programmer wrote a software loop that changed the value of the register from one to zero repeatedly. Nobody had even considered designing a four-voice generator—that is, not until “group mania” set in.</p><p>Mr. Smith was pondering this problem when he noticed that the video circuitry was very similar to the sound-generator circuitry. Since the video was bit-mapped, a bit of memory represented one dot on the video screen. The bits that made up a complete video image were held in a block of RAM and fetched by a scanning circuit to generate the image. Sound circuitry required similar scanning, with data in memory corresponding to the amplitude and frequency of the sound emanating from the speaker. Mr. Smith reasoned that by adding a pulse-width-modulator circuit, the video circuitry could be used to generate sound during the last microsecond of the horizontal retrace—the time it took the electron beam in the cathode-ray tube of the display to move from the last dot on each line to the first dot of the next line. During the retrace the video-scanning circuitry jumped to a block of memory earmarked for the amplitude value of the sound wave, fetched bytes, deposited them in a buffer that fed the sound generator, and then jumped back to the video memory in time for the next trace. The sound generator was simply a digital-to-analog converter connected to a linear amplifier.</p><p>To enable the sound generator to produce four distinct voices, software routines were written and embedded in ROM to accept values representing four separate sound waves and convert them into one complex wave. Thus a programmer writing applications programs for the Macintosh could specify separately each voice without being concerned about the nature of the complex wave.</p><h2>Gearing up to Build Macs</h2><p>In the fall of 1982, as the factory was being built and the design of the Macintosh was approaching its final form, Mr. Jobs began to play a greater role in the day-to-day activities of the designers. Although the hardware for the sound generator had been designed, the software to enable the computer to make sounds had not yet been written by Mr. Hertzfeld, who considered other parts of the Macintosh software more urgent. Mr. Jobs had been told that the sound generator would be impressive, with the analog circuitry and the speaker having been upgraded to accommodate four voices. But since this was an additional hardware expense, with no audible results at that point, one Friday Mr. Jobs issued an ultimatum: “If I don’t hear sound out of this thing by Monday morning, we’re ripping out the amplifier.”</p><p>That motivation sent Mr. Hertzfeld to the office during the weekend to write the software. By Sunday afternoon only three voices were working. He telephoned his colleague Mr. Smith and asked him to stop by and help optimize the software.</p></div><p>“Do you mean to tell me you’re using subroutines!” Burrell Smith exclaimed after examining the problem. “No wonder you can’t get four voices. Subroutines are much too slow.”</p><div id="rebelltitem33" data-id="33" data-reload-ads="false" data-is-image="False" data-href="https://spectrum.ieee.org/apple-macintosh/particle-33" data-basename="particle-33" data-post-id="2661376756" data-published-at="1686942238" data-use-pagination="False"><p>“Do you mean to tell me you’re using subroutines!” Mr. Smith exclaimed after examining the problem. “No wonder you can’t get four voices. Subroutines are much too slow.”</p><p>By Monday morning, the pair had written the microcode programs to produce results that satisfied Mr. Jobs.</p><p>Although Mr. Jobs’s input was sometimes hard to define, his instinct for defining the Macintosh as a product was important to its success, according to the designers. “He would say, ‘This isn’t what I want. I don’t know what I want, but this isn’t it.’” Mr. Smith said.</p><p>“He knows what great products are,” noted Mr. Hertzfeld. “He intuitively knows what people want.’’</p><p>One example was the design of the Macintosh casing, when clay models were made to demonstrate various possibilities. “I could hardly tell the difference between two models,” Mr. Hertzfeld said. “Steve would walk in and say, ‘This one stinks and this one is great.’ And he was usually right.”</p><p>Because Mr. Jobs placed great emphasis on packaging the Macintosh to occupy little space on a desk, a vertical design was used, with the disk drive placed underneath the CRT.</p><p>Mr. Jobs also decreed that the Macintosh contain no fans, which he had tried to eliminate from the original Apple computer. A vent was added to the Macintosh casing to allow cool air to enter and absorb heat from the vertical power supply, with hot air escaping at the top. The logic board was horizontally positioned.</p></div><p>[Steve] Jobs at times gave unworkable orders. When he demanded that the designers reposition the RAM chips on an early printed-circuit board because they were too close together, “most people chortled.”</p><div id="rebelltitem35" data-id="35" data-reload-ads="false" data-is-image="False" data-href="https://spectrum.ieee.org/apple-macintosh/particle-35" data-basename="particle-35" data-post-id="2661376756" data-published-at="1686942238" data-use-pagination="False"><p>Mr. Jobs, however, at times gave unworkable orders. When he demanded that the designers reposition the RAM chips on an early printed-circuit board because they were too close together, “most people chortled,” one designer said. The board was redesigned with the chips farther apart, but it did not work because the signals from the chips took too long to propagate over the increased distance. The board was redesigned again to move the chips back to their original position.</p><h2>Stopping the Radiation Leaks</h2><p>When the design group started to concentrate on manufacturing, the most imposing task was preventing radiation from leaking from the Macintosh’s plastic casing. At one time the fate of the Apple II had hung in the balance as its designers tried unsuccessfully to meet the emissions standards of the Federal Communications Commission. “I quickly saw the number of Apple II components double when several inductors and about 50 capacitors were added to the printed-circuit boards,” Mr. Smith recalled. With the Macintosh, however, he continued, “we eliminated all of the discrete electronics by going to a connector-less and solder-less design; we had had our noses rubbed in the FCC regulations, and we knew how important that was.’’ The high­speed serial I/O ports caused little interference because they were easy to shield.</p><p>Another question that arose toward the end of the design was the means of testing the Macintosh. In line with the zero-defect concept, the Macintosh team devised software for factory workers to use in debugging faults in the printed-circuit boards, as well as self-testing routines for the Macintosh itself.</p><p>The disk controller is tested with the video circuits. Video signals sent into the disk controller are read by the microprocessor. “We can display on the screen the pattern we were supposed to receive and the pattern we did receive when reading off the disk,” Mr. Smith explained, “and other kinds of prepared information about errors and where they occurred on the disk.’’</p><p>To test the printed-circuit boards in the factory, the Macintosh engineers designed software for a custom bed-of-nails tester that checks each computer in only a few seconds, faster than off-the-­shelf testers. If a board fails when a factory worker places it on the tester, the board is handed to another worker who runs a diagnostic test on it. A third worker repairs the board and returns it to the production line.</p></div><p><img id="3679b" data-rm-shortcode-id="56bbeced2e6f5852f115e5e65e31f2b8" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/rows-of-macintosh-computers-on-racks.jpg?id=34133989&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/rows-of-macintosh-computers-on-racks.jpg?id=34133989&amp;width=980" width="1143" height="869" alt="Rows of Macintosh computers on racks"><small><p>Each Macintosh is burned in—that is, turned on and heated—to detect the potential for early failures before shipping, thus increasing the reliability of the computers that are in fact shipped.</p></small></p><div id="rebelltitem36" data-id="36" data-reload-ads="false" data-is-image="False" data-href="https://spectrum.ieee.org/apple-macintosh/particle-36" data-basename="particle-36" data-post-id="2661376756" data-published-at="1686942238" data-use-pagination="False"><p>When Apple completed building the Macintosh factory, at an investment of $20 million, the design team spent most of its time there, helping the manufacturing engineers get the production lines moving. Problems with the disk drives in the middle of 1983 required Mr. Smith to redesign his final prototype twice.</p><p>Some of the plans for the factory proved troublesome, according to Ms. Coleman. The automatic insertion scheme for discrete components was unexpectedly difficult to implement. Many of the precise specifications for the geometric and electrical properties of the parts had to be reworked several times. Machines proved to be needed to align many of the parts before they were inserted. Although the machines, at $2000 apiece, were not expensive, they were a last-minute requirement.</p><p>The factory had few major difficulties with its first experimental run in December 1983, although the project had slipped from its May 1983 deadline. Often the factory would stop completely while engineers busily traced the faults to the sources—part of the zero-defect approach. Mr. Smith and the other design engineers virtually lived in the factory that December.</p><p>In January 1984 the first salable Macintosh computer rolled off the line. Although the production rate was erratic at first, it has since settled at one Macintosh every 27 seconds—about a half million a year.</p><h2>An Unheard of $30 Million Marketing Budget</h2><p>The marketing of the Macintosh shaped up much like the marketing of a new shampoo or soft drink, according to Mike Murray, who was hired in 1982 as the third member of the Macintosh marketing staff. “If Pepsi has two times more shelf space than Coke,” he explained, “you will sell more Pepsi. We want to create shelf space in your mind for the Macintosh.’’</p><p>To create that space on a shelf already crowded by IBM, Tandy, and other computer companies, Apple launched an aggressive advertising campaign—its most expensive ever.</p><p>Mr. Murray proposed the first formal marketing budget for the Macintosh in late 1983: he asked for $40 million. “People literally laughed at me,” he recalled. “They said, ‘What kind of a yo-yo is this guy?’ “He didn’t get his $40 million budget, but he got close to it—$30 million.</p></div><p>“We’ve established a beachhead with the Macintosh. If IBM knew in their heart of hearts how aggressive and driven we are, they would push us off the beach right now.”<br>—Mike Murray</p><div id="rebelltitem38" data-id="38" data-reload-ads="false" data-is-image="False" data-href="https://spectrum.ieee.org/apple-macintosh/particle-38" data-basename="particle-38" data-post-id="2661376756" data-published-at="1686944251" data-use-pagination="False"><p>
	The marketing campaign started before the Macintosh was introduced. Television viewers watching the Super Bowl football game in January 1984 saw <a href="https://www.youtube.com/watch?v=VtvjbmoDx-I" rel="noopener noreferrer" target="_blank">a commercial</a> with the Macintosh overcoming Orwell’s nightmare vision of 1984.
</p><p>
	Other television advertisements, as well as magazine and billboard ads, depicted the Macintosh as being easy to learn to use. In some ads, the Mac was positioned directly alongside IBM’s personal computer. Elaborate color foldouts in major magazines pictured the Macintosh and members of the design team.</p><p>
	“The interesting thing about this business,” mused Mr. Murray, “is that there is no history. The best way is to come in really smart, really understand the fundamentals of the technology and how the software dealers work, and then run as fast as you can.’’
</p><h2>The Mac Team Disperses</h2><p>
	“We’ve established a beachhead with the Macintosh,” explained Mr. Murray. “We’re on the beach. If IBM knew in their heart of hearts how aggressive and driven we are, they would push us off the beach right now, and I think they’re trying. The next 18 to 24 months is do-or-die time for us.”
</p><p>
	With sales of the Lisa workstation disappointing, Apple is counting on the Macintosh to survive. The ability to bring out a successful family of products is seen as a key to that goal, and the company is working on a series of Macintosh peripherals—printers, local-area networks, and the like. This, too, is proving both a technical and organizational challenge.
</p><p>
	“Once you go from a stand-alone system to a networked one, the complexity increases enormously,” noted Mr. Murray. “We cannot throw it all out into the market and let people tell us what is wrong with it. We have to walk before we can run.”
</p><p>
	Only two software programs were written by Apple for the Macintosh—Macpaint, which allows users to draw pictures with the mouse, and Macwrite, a word-processing program. Apple is counting on independent software vendors to write and market applications programs for the Macintosh that will make it a more attractive product for potential customers. The company is also modifying some Lisa software for use on Macintosh and making versions of the Macintosh software to run on the Lisa.
</p><p>
	Meanwhile the small, coherent Macintosh design team is no longer. “Nowadays we’re a large company,” Mr. Smith remarked.
</p><p><span></span>“The pendulum of the project swings,” explained Mr. Hertzfeld, who has taken a leave of absence from Apple. “Now the company is a more mainstream organization, with managers who have managers working for them. That’s why I’m not there, because I got spoiled” working on the Macintosh design team.<span></span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Do We Owe Our Teams? (112 pts)]]></title>
            <link>https://www.mironov.com/owe/</link>
            <guid>36562868</guid>
            <pubDate>Sun, 02 Jul 2023 16:15:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mironov.com/owe/">https://www.mironov.com/owe/</a>, See on <a href="https://news.ycombinator.com/item?id=36562868">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
          <div>
            
            <p>Many of my discussions with product leaders (CPOs, VPs and others who manage teams of product folks) are about the substance of product management: portfolios, competing stakeholders, pricing &amp; packaging, tarot cards as a revenue forecasting model. &nbsp;Last week, though, in my <a href="https://maven.com/richmironov/pmleaders/?ref=mironov.com">product leadership workshop</a>, we had an extended discussion about the <strong>core people-and-organizational obligations we have toward those who work for us</strong>.<br>(If you’re running some other department, these should sound familiar.)</p><h2 id="1-be-umbrellas-not-funnels">1. &nbsp;Be Umbrellas, Not Funnels</h2><p>Companies, especially executive teams, can generate a lot of chaos: “<a href="https://www.mironov.com/cake/">small</a>” interrupts, sudden shifts, cross-functional blame, budget jousting. &nbsp;In the colorful MBA vernacular, we’re either <a href="https://www.urbandictionary.com/define.php?term=Shit+Umbrella&amp;ref=mironov.com">poop umbrellas</a> or poop funnels: buffering our teams from the noise and confusion as best we can, or letting it all fall on their heads.</p><p>Umbrella-wielding is a skill: anticipating politics; de-escalating &nbsp;drama; being ready with in-the-moment analysis and insights about whether a problem is actually important; articulating which department is best equipped to handle today’s small crisis; practicing organizational <em>jiu jitsu</em>.</p><p>Examples:</p><ul><li>A Fortune 50 financial company light-years away from our core manufacturing market expresses interest. &nbsp;Sales (inevitably) wants to expand our European SMB tech manufacturing <a href="https://www.portent.com/blog/cro/ideal-customer-profiles-beginners-guide.htm?ref=mironov.com">ICP</a> to include US-based global banks. Product fit will probably be catastrophic. &nbsp;This requires a quick-but-urgent review of the opportunity with Product and Engineering before it’s committed to the revenue pipeline.</li><li>Our CEO thinks that a new product manager is doing a poor job because her product has falling revenue and poor customer feedback. &nbsp;But she’s newly assigned to a decrepit old widget that’s in a shrinking segment and hasn’t been maintained for years. &nbsp;In your opinion, she’s very talented and just inherited a mess. &nbsp;Best to walk back the CEO’s misimpression quickly, before the discussion turns to firings.</li><li>A competitor announces ChatGPT-based retirement investment advice. &nbsp;<em>(Probably doesn't work yet, or gives generically bad advice 100x faster than humans.) </em> With generative AI at the very top of its hype cycle, the Board wants a 90-day meet-the-competition development plan, and says they are willing to sacrifice everything on the roadmap to staff it. &nbsp;You’ll need to spend some political capital to slow this down just a little – then sell the idea of a prototype, so we can find out whether it makes any sense.</li></ul><p>Also, when there’s a hot issue with unclear ownership, &nbsp;we don’t throw our subordinates under the bus. &nbsp;So <a href="https://www.mironov.com/pronouns/">pronouns</a> matter: “<em><strong>I </strong></em>think this tax calculation issue crosses products and departments: give <em><strong>me</strong></em> a day to chase down which group can fix it” is much better than “Sandeep screwed up again – let me pull <em><strong>him</strong></em> into the meeting” or “those losers in BizOps were supposed to keep the pricing database 100% accurate.”</p><h2 id="2-merchandize-good-work">2. &nbsp;Merchandize Good Work</h2><p>People who are invisible – or whose work is invisible – miss out on raises and promotions and invitations to do cool stuff. &nbsp;And most of the good things our product (and engineering and design and research and documentation and test automation and support) teams do isn’t very visible. &nbsp;So it’s incumbent on us as leaders to gently – but relentlessly – <a href="https://www.mironov.com/merchandizing/">merchandize</a> our teams’ good work and accomplishments and business-relevant wins.</p><ul><li>If we’ve fixed some poor UX in our sign-up funnel, consider a 90-second video where the product manager introduces the problem, the designer who fixed it <a href="https://www.mironov.com/show-tell/">shows</a> a brief before-and-after, and the product manager recaps with outcome/impact. <em> (“1.1% higher top-of-funnel conversion means $3-4M more per quarter in top-line revenue.”)</em></li><li>Shortening onboarding time/effort lets us add more customers faster, at lower cost. &nbsp;Maybe in our monthly All-Hands we call out the Customer Success person who suggested the fix, the content creator whose “onboard yourself faster” checklist sped up onboarding by 11%, and the product manager overseeing new customer tools.</li><li>I love having my teams identify unappreciated heroes in other departments, so I can <a href="https://www.mironov.com/ty/">send their VPs a thank-you note</a> on behalf of Product. &nbsp;Recognizing good work can be habit-forming.</li></ul><p>BTW, there’s a reverse halo effect here. &nbsp;If my extended team is seen as successful and hardworking and smart, I get some of the reflected glow. &nbsp;Other leaders give us the benefit of the doubt, and talented folks want to move into our group.</p><h2 id="3-provide-honest-career-pathing">3. Provide Honest Career Pathing</h2><p>At scale, product managers might be 2-3% of a company’s employees. &nbsp;The role is vaguely defined, challenging, and demands an unusual mix of skills. People come in all shapes, sizes, styles, and talents – that mostly fit <em><strong>other</strong></em> roles. So we owe our team gentle-but-clear-and-frequent communication about how they are doing and where we see them going next. &nbsp;(When I’ve parachuted into companies as interim CPO, my first order of business has been to quickly evaluate the team I’ve inherited.) &nbsp;Examples:</p><ul><li>Someone<strong> green but smart, humble, curious, and coachable</strong>... we might map out 4-6 areas for intense mentoring through the year. &nbsp;Talk through the theory of <a href="https://www.mironov.com/moneystories/">thumbnail business cases</a>, work the first 3 or 4 together with lots of coaching, then move down the skills list.</li><li>A <strong>deeply opinionated subject expert </strong>with visible disdain for average users will focus on rarely used expert features and tend to ignore opinions from the bottom 95% of customers. &nbsp;Maybe an evangelist or implementation architect role is a better match: worth a frank talk about fit and attitudes and where to make the biggest contributions to the company.</li><li>A<strong> product veteran</strong> who is effortlessly juggling a maker team, represents product management on a strategy task force, and coaches the newbies may need a bigger challenge. &nbsp;Are there open slots for directors? &nbsp;Can we create one? Is there a juicy assignment coming up? &nbsp;A win for the company even if they move out of my team.</li></ul><h2 id="4-play-the-long-game">4. Play the Long Game</h2><p>The half-life of any particular job is short, and most companies don’t last. &nbsp;So the great people we nurture will eventually be somewhere else. &nbsp;In the long game, we’re investing in the larger product community and relationships that span years and organizations. &nbsp;Ideally, you have a portfolio of trusted peers<em> (for moral support, shared challenges)</em> and a few mentors <em>(for career advice, realistic POV)</em> and up-and-comers <em>(<a href="https://www.mironov.com/pif/">paying it forward</a>)</em>. &nbsp;As we move up through organizations, our informal external networks get more and more valuable. &nbsp;</p><p>Which raises a “what do we owe them” quandary: what if your best player signals that she wants to leave? &nbsp;I see competing obligations to my company <em>(keep the best talent)</em> and myself <em>(run a strong department)</em> and to her <em>(support new opportunities and challenges)</em>. &nbsp;It might go this way:</p><ul><li>First, an honest closed-door conversation about her real concerns or goals. &nbsp;Is she unhappy with the company? &nbsp;Frustrated by my management style? Needing flexibility or some recharge/refresh time? &nbsp;Bored and ready to move onto greener pastures? &nbsp;Different issues, different answers. &nbsp;(As product folks, we should never offer solutions until we’re clear on the underlying problem.)</li><li>If there are some reasonable solutions or accommodations, explore and sell them – but don’t oversell or promise something you can’t deliver. &nbsp;Product folks have pretty good BS detectors. &nbsp;I’d rather have her be my peer or lead a peer organization in my company than lose her.</li><li>But if not, I usually feel the obligation to help her reach/find her next adventure. &nbsp;<strong>To play the long game.</strong> &nbsp;To continue being a coach/mentor outside the company. &nbsp;To value the person, not just the work. &nbsp;That might include <a href="https://www.mironov.com/exiting/">talking about outside opportunities, exit timelines and career implications</a>; being a (quiet, careful) job reference; voicing honest opinions about trends or industries; sharing contacts at a few non-competing companies; offering to be a sounding board for her inevitable next challenges.</li></ul><p>If we cherish our people as much as our products and end users, we should be supporting their goals and dreams as much as our own.</p><h2 id="sound-byte">Sound Byte</h2><p>Leading an organization and managing people carries a substantial responsibility to those who work for us. &nbsp; We need to support and protect them so that they can do their best work; build recognition for them; and help them look ahead to what’s next. &nbsp;Not that different from building a product or <a href="https://www.mironov.com/parenting_and_pm/">raising a child</a>.</p>
          </div>
        </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Make Your Renders Unnecessarily Complicated by Modeling a Film Camera in Blender [video] (241 pts)]]></title>
            <link>https://www.youtube.com/watch?v=YE9rEQAGpLw</link>
            <guid>36562757</guid>
            <pubDate>Sun, 02 Jul 2023 16:03:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=YE9rEQAGpLw">https://www.youtube.com/watch?v=YE9rEQAGpLw</a>, See on <a href="https://news.ycombinator.com/item?id=36562757">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Aspartame: Once More Unto the Breach (166 pts)]]></title>
            <link>https://dynomight.net/aspartame/</link>
            <guid>36562739</guid>
            <pubDate>Sun, 02 Jul 2023 16:02:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dynomight.net/aspartame/">https://dynomight.net/aspartame/</a>, See on <a href="https://news.ycombinator.com/item?id=36562739">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <p>Look, I get it. Diet Coke tastes sweet because it has aspartame in it. Aspartame is a weird synthetic molecule that’s 200 times sweeter than sucrose. Half of the world’s aspartame is made by Ajinomoto of Tokyo—the same company that first brought us MSG back in 1909.</p>

<p><img src="https://dynomight.net/img/aspartame/ajinomoto-headquarters.jpg" alt="ajinomoto headquarters"></p>

<p>If you look on <a href="https://en.wikipedia.org/wiki/Aspartame">Wikipedia</a>, you’ll see that aspartame is a <em>methyl ester of the aspartic acid phenylalanine dipeptide</em>, which isn’t, like, comforting.</p>

<p>It’s normal to have a prior that aspartame might be bad for you. Certainly, that was <em>my</em> prior. Without looking at any evidence, any reasonable person would think like this:</p>

<table>
  <thead>
    <tr>
      <th>aspartame is…</th>
      <th>odds</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>…good for you</td>
      <td>very unlikely</td>
    </tr>
    <tr>
      <td>…harmless</td>
      <td>plausible</td>
    </tr>
    <tr>
      <td>…bad for you</td>
      <td>plausible</td>
    </tr>
  </tbody>
</table>

<p>This makes the decision theory pretty simple: Consuming aspartame has little upside, but substantial downside.</p>

<p>The thing is, we do have evidence. We have a lot of evidence. The FDA calls aspartame “one of the most exhaustively studied substances in the human food supply”.</p>

<p>The other thing is, the alternative to aspartame often isn’t <em>no aspartame</em> but rather <em>sugar</em> or <a href="https://dynomight.net/cola/#mexican-coke"><em>corn syrup</em></a> or even perhaps even <a href="https://dynomight.net/alcohol/"><em>alcohol</em></a>.</p>

<p>I don’t want to convince anyone to consume aspartame. But if we’re choosing between aspartame and other risky things, we should evaluate the relative risks.</p>

<h2 id="what-happens-to-aspartame-after-it-goes-into-your-body">What happens to aspartame after it goes into your body</h2>

<p>Let’s forget about safety for a second, and just look at the causal chain. Say you drink a Diet Coke. What happens next?</p>

<h3 id="fact-1-aspartame-is-quickly-broken-down-in-the-gut">Fact 1: Aspartame is quickly broken down in the gut.</h3>

<p>After you drink a Diet Coke, the aspartame goes to your guts. After that, it’s very quickly broken down into 50% phenylalanine, 40% aspartic acid, and 10% methanol. For example, a can of Diet Coke contains 184 mg of aspartame. This becomes:</p>

<ul>
  <li>92 mg of phenylalanine</li>
  <li>73.6 mg aspartic acid</li>
  <li>18.4 mg methanol</li>
</ul>

<details>
  <summary>This happens quickly and completely. No aspartame ever enters your bloodstream. The rest of your body only ever sees these three other chemicals. (<u>click here or on any paragraph with a triangle for more details</u>)</summary>

  <p>The European Food Safety Authority Report (EFSA) report: <a href="https://www.efsa.europa.eu/en/efsajournal/pub/3496">Scientific Opinion on the re-evaluation of aspartame as a food additive</a> gives this figure (slightly modified):</p>

  <p><img src="https://dynomight.net/img/aspartame/metabolism-bigger2.svg" alt="metabolism"></p>

  <p>The same report gives this discussion:</p>

  <p><img src="https://dynomight.net/img/aspartame/EFSA-breakdown.svg" alt="EFSA breakdown"></p>

</details>

<h3 id="fact-2-phenylalanine-is-a-standard-amino-acid-you-consume-all-the-time">Fact 2: Phenylalanine is a standard amino acid you consume all the time.</h3>

<p>We recently talked about <a href="https://dynomight.net/diet-coke-nootropic/">phenylalanine</a>. It is an essential amino acid. If you didn’t consume any of it then when your body tried to make certain proteins those proteins would get truncated, and then they wouldn’t do what they are supposed to do, and then you would die.</p>

<p>Fortunately, that’s almost impossible. From 2% to 5% of all protein in food is phenylalanine. The recommended dietary allowance for a 70 kg (154 lb) person is at least 2130 mg. Meat-eating men in the UK average 3500 mg per day, while vegetarians and vegans get slightly less.</p>

<p>Here are the amounts of phenylalanine in a few foods:</p>

<table>
  <tbody>
    <tr>
      <td>potato</td>
      <td>170 mg</td>
    </tr>
    <tr>
      <td>large egg</td>
      <td>340 mg</td>
    </tr>
    <tr>
      <td>8 oz (235 ml) glass of milk</td>
      <td>430 mg</td>
    </tr>
    <tr>
      <td>400g box of tofu</td>
      <td>3300 mg</td>
    </tr>
  </tbody>
</table>

<details>
  <summary>The 92 mg of phenylalanine you get from a Diet Coke is much less than what virtually everyone already gets from other sources.</summary>

  <p><a href="https://doi.org/10.1186/s13023-020-01391-y">MacDonald et al. (2020)</a>:</p>

  <p><img src="https://dynomight.net/img/aspartame/macdonald.svg" alt="macdonald"></p>

  <p>RDA guidelines are <a href="https://globalrph.com/rda-and-ear-recommendations-for-essential-amino-acids/">here</a>. For adults, the recommendation is at least 33 m/kg of phenylalanine (or tyrosine, a metabolite of phenylalanine). For a 70 kg (154 lb) person, that would be 2130 mg.</p>

  <p><a href="https://doi.org/10.1038/ejcn.2015.144">Schmidt et al. (2015)</a>:</p>

  <p><img src="https://dynomight.net/img/aspartame/schmidt.svg" alt="schmidt"></p>

</details>

<p>Around 1 in 12,000 babies is born with <a href="https://www.nichd.nih.gov/health/topics/factsheets/pku">phenylketonuria</a>, a serious genetic disorder that results in low levels of <a href="https://en.wikipedia.org/wiki/Phenylalanine_hydroxylase">phenylalanine hydroxylase</a>, making it difficult to metabolize phenylalanine. People with phenylketonuria need to carefully monitor their consumption of phenylalanine (from all sources). This is why there’s this scary <strong>ALL-BOLD WARNING</strong>.</p>

<!-- ![phenylketonurics warning](/img/aspartame/phenylketonurics.jpg) -->

<p><img alt="phenylketonurics warning" src="https://dynomight.net/img/aspartame/phenylketonurics.jpg"></p>

<p>If you had phenylketonuria you would know it already.</p>

<h3 id="fact-3-aspartic-acid-is-a-standard-amino-acid-you-consume-all-the-time">Fact 3: Aspartic acid is a standard amino acid you consume all the time.</h3>

<p>Here’s <a href="https://commons.wikimedia.org/wiki/File:ProteinogenicAminoAcids.svg">a chart from Wikimedia</a> with our friends circled:</p>

<!-- ![amino-acids](/img/aspartame/amino-acids.png) -->

<p><img alt="amino acids" src="https://dynomight.net/img/aspartame/amino-acids.png"></p>

<p>Aspartic acid is not essential in humans, meaning that if you don’t eat it, your body can make it (usually from <a href="https://en.wikipedia.org/wiki/Oxaloacetic_acid">oxaloacetic acid</a>). But that’s not likely, since almost everything with protein has aspartic acid including meat, grains, dairy, vegetables, and eggs. Men in the UK average 6600 mg of aspartic acid per day.</p>

<details>
  <summary>The 74 mg of aspartic acid you get from a Diet Coke is two orders of magnitude less than what most people get already.</summary>

  <p>Here’s <a href="https://doi.org/10.1038/ejcn.2015.144">Schmidt et al. (2015)</a> again:</p>

  <p><img src="https://dynomight.net/img/aspartame/schmidt.svg" alt="schmidt"></p>

</details>

<h3 id="fact-4-methanol-is-a-simple-alcohol-you-consume-all-the-time">Fact 4: Methanol is a simple alcohol you consume all the time.</h3>

<p>Methanol (CH₃OH) is the simplest alcohol molecule. It’s in lots of food. Here are some foods with larger average amounts.</p>

<table>
  <thead>
    <tr>
      <th>food</th>
      <th>mg/kg methanol</th>
      <th>typical serving</th>
      <th>methanol</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>wine</td>
      <td>115.0</td>
      <td>150 ml glass</td>
      <td>17 mg</td>
    </tr>
    <tr>
      <td>tomatoes</td>
      <td>281.4</td>
      <td>medium 125g tomato</td>
      <td>35 mg</td>
    </tr>
    <tr>
      <td>citrus fruit</td>
      <td>106.5</td>
      <td>medium 140g orange</td>
      <td>15 mg</td>
    </tr>
  </tbody>
</table>

<p>This vastly underestimates how much methanol you get. In land plants, the primary component of cells walls is <a href="https://en.wikipedia.org/wiki/Pectin">pectin</a>. Once in the body, pectin degrades into methanol. Here are some estimates of the <em>indirect</em> increase in methanol various fruits and vegetables cause in this way.</p>

<table>
  <thead>
    <tr>
      <th>food</th>
      <th>mg/kg methanol</th>
      <th>typical serving</th>
      <th>methanol</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>root vegetables</td>
      <td>774</td>
      <td>medium 200 g potato</td>
      <td>155 mg</td>
    </tr>
    <tr>
      <td>apples</td>
      <td>508.5</td>
      <td>medium 170 g apple</td>
      <td>132 mg</td>
    </tr>
    <tr>
      <td>oranges</td>
      <td>531</td>
      <td>medium 140g orange</td>
      <td>74 mg</td>
    </tr>
    <tr>
      <td>bananas</td>
      <td>657</td>
      <td>120g without skin</td>
      <td>79 mg</td>
    </tr>
    <tr>
      <td>avocados</td>
      <td>486</td>
      <td>100g (flesh only)</td>
      <td>59 mg</td>
    </tr>
  </tbody>
</table>

<p>You get the idea. We eat things that contain methanol or metabolize into methanol all the time. It’s estimated that most people get between 130 and 1030 mg of methanol from food per day, much more than the 18 mg in a Diet Coke.</p>

<p>Now isn’t methanol toxic? Sure, if you consume enough of it. The <a href="https://en.wikipedia.org/wiki/Median_lethal_dose">LD₅₀</a> in rats is around 5600 mg/kg, as compared to 7300 mg/kg for <a href="https://dynomight.net/alcohol-trial/">good-old ethanol</a>.</p>

<details>
  <summary>One "conspiracy theory" you hear about aspartame is that it becomes formaldehyde once it's in the body. This is <em>absolutely true</em>: When metabolizing methanol, formaldehyde is created. But small amounts of formaldehyde are <em>completely normal</em>. The half-life of formaldehyde in human blood is around 1 minute, meaning it disappears almost immediately. You get more formaldehyde (via methanol) by eating an apple than by drinking a Diet Coke. Formaldehyde itself is also present in lots of foods, like meat, seafood, fruits, vegetables, and coffee.</summary>

  <p>The EFSA report:</p>

  <p><img src="https://dynomight.net/img/aspartame/EFSA-methanol.svg" alt="ESFA methanol"></p>

  <p>Also the ESFA report:</p>

  <p><img src="https://dynomight.net/img/aspartame/EFSA-methanol-2.svg" alt="ESFA methanol"></p>

  <p><a href="https://doi.org/10.1002/jps.21319">Dhareshwar and Stella (2008)</a></p>

  <p><img src="https://dynomight.net/img/aspartame/dhareshwar-1.svg" alt="dhareshwar 1"></p>

  <p>Also <a href="https://doi.org/10.1002/jps.21319">Dhareshwar and Stella (2008)</a>:</p>

  <p><img src="https://dynomight.net/img/aspartame/dhareshwar-2.svg" alt="dhareshwar 1"></p>

  <p>The EFSA report again:</p>

  <p><img src="https://dynomight.net/img/aspartame/EFSA-formaldehyde.svg" alt="ESFA formaldehyde"></p>

</details>

<h3 id="fact-5-this-doesnt-prove-aspartame-is-safe">Fact 5: This doesn’t prove aspartame is safe.</h3>

<p>To summarize the above:</p>

<ol>
  <li>Aspartame is quickly broken down in the gut into phenylalanine, aspartic acid, and methanol. Aspartame itself never enters your bloodstream or touches any other part of your body.</li>
  <li>Phenylalanine is normal.</li>
  <li>Aspartic acid is normal.</li>
  <li>Methanol is normal.</li>
</ol>

<p>(Incidentally, this same logic does not apply to other artificial sweeteners which mostly aren’t broken down at all.)</p>

<p>While <em>informative</em>, this does not <em>prove</em> aspartame is safe. Biology is crazy. But it should inform our priors. Speaking for myself, my previous model was that consuming aspartame would result in a crazy unknown synthetic chemical circulating around my body and doing god-knows-what. My updated model is that consuming aspartame results in slightly larger amounts of some totally normal chemicals.</p>

<p>This is reassuring. But even if they’re normal, could these chemicals still cause harm? Sure. Fortunately for us, aspartame was invented a long time ago, so we have lots of evidence.</p>

<h2 id="the-scientific-consensus">The scientific consensus</h2>

<h3 id="how-to-think-about-this-situation">How to think about this situation</h3>

<p>Aspartame was first made in 1965 and was approved by the FDA in 1981. In the decades since, there have been hundreds of studies.</p>

<p>Given so many studies, focusing on individual papers is a mistake. With enough <strike>monkeys pounding away at enough typewriters</strike> scientists pounding away at enough science, lots of weirdness is expected.</p>

<p>The right strategy is to look at the entire pool of evidence. Some tiny number of people have the time and expertise to comb through the entire literature and synthesize everything. For the rest of us, the only sane thing is to read other people who have done that synthesis.</p>

<h3 id="the-us-food-and-drug-administration-fda">The US Food and Drug Administration (FDA)</h3>

<p>In typical US government fashion, the FDA doesn’t go to great lengths to explain its reasoning to the public. The best you can find is this <a href="https://www.fda.gov/food/food-additives-petitions/additional-information-about-high-intensity-sweeteners-permitted-use-food-united-states">rather lame page</a>:</p>

<p><img src="https://dynomight.net/img/aspartame/FDA.svg" alt="FDA info"></p>

<p>The history of aspartame and the FDA is contentious and sort of infuriating. For the scientific question of “is aspartame safe?” the main thing to know is that the FDA approved it a long time ago, and continues to stand by those decisions.</p>

<details>
  <summary>
But it must be said that the history and public communication of the FDA on this issue is kind of a train wreck, and if I wanted to optimize it to serve as conspiracy theory fuel, I could scarcely do any better. The FDA says it continues to monitor new studies and remains confident aspartame is safe. So why doesn't it explain its reasoning to a skeptical public? The newest document the FDA can point people to is from 26 years ago. When a concerned citizen writes in, the FDA does things like respond 12 years later, while acting like that's perfectly normal.
</summary>

  <p>The FDA first approved aspartame for dry foods in 1974. However, there was a lot of controversy about the studies performed by <a href="https://en.wikipedia.org/wiki/G.D._Searle,_LLC">G.D. Searle</a>, the company that discovered aspartame in 1965 (and that Donald Rumsfeld would become CEO of in 1977). The FDA commissioner agreed with these criticisms and placed a stay on aspartame’s approval.</p>

  <p>After more debate, the FDA finally approved aspartame in 1981 in a <a href="https://www.fda.gov/media/89219/download">26-page report</a> with this summary:</p>

  <p><img src="https://dynomight.net/img/aspartame/FDA-1981.svg" alt="FDA 1981 report"></p>

  <p>In 1983 the FDA added approval for carbonated beverages. <a href="https://www.fda.gov/media/89189/download">The report</a> is mostly boring but this part is fun:</p>

  <p><img src="https://dynomight.net/img/aspartame/FDA-1983.svg" alt="FDA 1983 report"></p>

  <p>There was controversy about US Attorney General <a href="https://en.wikipedia.org/wiki/Samuel_K._Skinner">Samuel Skinner</a> who was involved in the case and then went on to take a job at a law firm that Searle used. This led <a href="https://en.wikipedia.org/wiki/Howard_Metzenbaum">Senator Metzenbaum</a> to request an investigation by the Government Accountability Office (GAO). They reported in 1987 that the process had been followed correctly. They also got responses from 69 researchers, 43 of whom worked in aspartame research.</p>

  <p><img src="https://dynomight.net/img/aspartame/image-20220614162208248.png" alt="image-20220614162208248"></p>

  <p>Finally in 1996, the FDA <a href="http://www.gpo.gov/fdsys/pkg/FR-1996-06-28/pdf/96-16522.pdf">approved</a> aspartame as a “general sweetener”.</p>

  <p><img src="https://dynomight.net/img/aspartame/FDA-1996.svg" alt="FDA 1996 report"></p>

  <p>Also in 1996, Roger Walton, a psychiatrist at Northeastern Ohio Universities College of Medicine, wrote a <a href="https://www.lightenyourtoxicload.com/wp-content/uploads/2014/07/Dr-Walton-survey-of-aspartame-studies.pdf">survey</a> that claimed that 74 out of 74 industry-funded studies confirmed aspartame’s safety whereas 84 out of 91 independent studies identified health concerns. Walton went appeared in a <a href="https://www.youtube.com/watch?v=BK_u7DG9DY8"><em>60 Minutes</em> special</a> on aspartame. It later turned out that Walton had missed at least 50 peer-reviewed studies  and most of the independent “studies” he did cite were really just letters to the editor or similar, many weren’t negative, and some didn’t involve aspartame at all.</p>

  <p>For example, Walton cites <a href="https://doi.org/10.1016/S0140-6736(85)90920-1">Wurtman (1985)</a>. This appeared in the Lancet, which is a very good journal. But it’s just a 5-paragraph letter to the editor that points out that aspartame creates phenylalanine without other amino acids, which might do something in the brain—an idea we’ve <a href="https://dynomight.net/diet-coke-nootropic">looked at recently</a>—and gives some anecdotes about people getting seizures. Real studies do not confirm these anecdotes. <a href="https://doi.org/10.1093/ajcn/68.3.531">Wurtman himself</a> did a real study in 1998 that found “Large daily doses of aspartame had no effect on neuropsychologic, neurophysiologic, or behavioral functioning in healthy young adults”. But the Walton report remains discussed to this day. Just a few months ago, Walton published a book called “Double Blind”. I’ll let you judge the credibility for yourself:</p>

  <p><img src="https://dynomight.net/img/aspartame/double-blind.png" alt="Double Blind"></p>

  <p>Since 1996, the FDA doesn’t appear to have published any systematic argument that the current science supports the idea that aspartame is safe. If you look <em>really</em> hard, you can find a few places where the FDA has recently been forced to justify the idea that aspartame is safe. For example, a concerned citizen wrote to the FDA in 2002 and sent this <a href="https://www.regulations.gov/document/FDA-2002-P-0247-0023">response</a> in 2014:</p>

  <p><img src="https://dynomight.net/img/aspartame/FDA-2014-response.svg" alt="FDA 2014 response"></p>

  

</details>



<p>Fortunately, other countries exist. The EFSA has a nice <a href="https://www.efsa.europa.eu/en/topics/topic/aspartame">page</a> that summarizes things this way:</p>

<p><img src="https://dynomight.net/img/aspartame/EFSA-webpage.svg" alt="EFSA webpage"></p>

<p>The bottom of the page has a timeline that clearly presents all past activities, and clearly links to all ongoing studies and reports. (The fact that I’m impressed by such basic things drives home just how low the FDA sets the bar.)</p>

<p>There’s also an <em>extremely</em> nice-263 page <a href="https://www.efsa.europa.eu/en/press/news/131210">risk assessment</a>. This document is the single best (most clear, credible, thorough, and up-to-date) source of information I could find on the safety of aspartame. Here are some quotes from the summary:</p>

<blockquote>
  <p>there was no epidemiological evidence for possible associations of aspartame with various cancers in the human population.</p>
</blockquote>

<blockquote>
  <p>The Panel considered the database on the genotoxicity of methanol and concluded that the data set was limited but that the available reliable <em>in vitro</em> and <em>in vivo</em> data did not indicate a genotoxic potential for methanol.</p>
</blockquote>

<blockquote>
  <p>the calculated [No Observed Adverse Effect Level]s for methanol by oral exposure are 140 and 515-fold higher than the maximum amount of methanol that could be released when aspartame is consumed at the [Acceptable Daily Intake].</p>
</blockquote>

<blockquote>
  <p>the data on reproductive and developmental toxicity did not suggest that there was a risk from methanol derived from aspartame at the current exposure estimates or at the [Acceptable Daily Intake] of 40 mg/kg bw/day.</p>
</blockquote>

<blockquote>
  <p>based on recent measurements of basal levels of formaldehyde in blood and on the modelling of its biological turnover and steady state concentration in cells, formaldehyde formed from aspartame-derived methanol was not of safety concern at the current exposure estimates or at the ADI of 40 mg/kg bw/day.</p>
</blockquote>

<p>It ends this way:</p>

<p><img src="https://dynomight.net/img/aspartame/EFSA-report.svg" alt="EFSA report"></p>

<h3 id="health-canada">Health Canada</h3>

<p>Canada is one of the <a href="https://www.canada.ca/en/health-canada/services/food-nutrition/food-safety/food-additives/sugar-substitutes/aspartame-artificial-sweeteners.html">clearer communicators</a>:</p>

<p><img src="https://dynomight.net/img/aspartame/canada.svg" alt="canada"></p>

<p>Here are their conclusions in a handy table:</p>

<table>
  <thead>
    <tr>
      <th>Allegation</th>
      <th>Conclusion</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>The methanol in aspartame is toxic and is linked to numerous health  problems including lupus and blindness, and also mimics multiple  sclerosis</td>
      <td>Not supported</td>
    </tr>
    <tr>
      <td>Aspartame is especially dangerous for person with diabetes</td>
      <td>Not supported</td>
    </tr>
    <tr>
      <td>Aspartame causes cancer and brain tumours</td>
      <td>Not supported</td>
    </tr>
    <tr>
      <td>Aspartame causes seizures</td>
      <td>Not supported</td>
    </tr>
    <tr>
      <td>Aspartame causes allergic reactions</td>
      <td>Not supported</td>
    </tr>
  </tbody>
</table>

<h3 id="new-zealand-food-safety-authority">New Zealand Food Safety Authority</h3>

<p>From an <a href="https://web.archive.org/web/20081216093929/http://www.nzfsa.govt.nz/consumers/chemicals-nutrients-additives-and-toxins/aspartame/">archived page</a>:</p>

<p><img src="https://dynomight.net/img/aspartame/nz.svg" alt="new zealand"></p>

<h3 id="joint-expert-committee-on-food-additives-jecfa">Joint Expert Committee on Food Additives (JECFA)</h3>

<p>The UN and WHO “jointly” run the JECFA. The only public reports I can find from them are really old, like <a href="https://inchem.org/documents/jecfa/jecmono/v15je03.htm">1980</a>. For what it’s worth, this was the conclusion:</p>

<p><img src="https://dynomight.net/img/aspartame/JECFA.svg" alt="JECFA"></p>

<p>As far as I can tell, this <a href="https://inchem.org/documents/jecfa/jecmono/v16je01.htm">1981</a> update is the last word on aspartame from the JECFA. However, the JECFA currently has aspartame on the <a href="https://www.fao.org/fao-who-codexalimentarius/sh-proxy/fr/?lnk=1&amp;url=https%253A%252F%252Fworkspace.fao.org%252Fsites%252Fcodex%252FCircular%252520Letters%252FCL%2525202021-81%252Fcl21_81e.pdf">priority list of substances proposed for evaluation</a>, as requested by Columbia, Costa Rica, and—somehow—the United States of America. (<em>Update</em>: This report is due to finally come out July 14, 2023. I’ll write about it when it does.)</p>

<h3 id="the-academy-of-nutrition-and-dietetics">The Academy of Nutrition and Dietetics</h3>

<p>This is a <a href="https://en.wikipedia.org/wiki/Academy_of_Nutrition_and_Dietetics">seemingly respectable</a> organization of more than 100 thousand food and nutrition professionals, albeit one funded by food industry lobbying groups. Their <a href="https://doi.org/10.1016/j.jand.2012.03.009">position report</a> states:</p>

<p><img src="https://dynomight.net/img/aspartame/academy.svg" alt="academy"></p>

<h2 id="literature-reviews">Literature reviews</h2>

<p>Again, I don’t think it’s productive to look at individual papers. But we can look at <em>reviews</em> of all the evidence.</p>

<p>Here, some judgment is required. If I type “review aspartame safety” into Google scholar, many of the papers that come up have several of the following features:</p>

<ol>
  <li>Paper is published in an obscure journal</li>
  <li>Authors are from obscure institutions</li>
  <li>Paper only engages with a tiny slice of the literature</li>
  <li>Paper just “doesn’t look right” (e.g. has a serious grammatical error in the title)</li>
</ol>

<p>For full transparency, here are the papers I found less credible for the above reasons, given as hopefully-offense-minimizing numbers: <a href="https://doi.org/10.1007/s10616-013-9681-0">1</a> <a href="https://doi.org/10.3923/pjbs.2018.127.134">2</a> <a href="https://doi.org/10.4103%2F0976-500X.85936">3</a> <a href="https://doi.org/10.1080/1028415X.2017.1288340">4</a> <a href="https://pubmed.ncbi.nlm.nih.gov/29038387/">5</a> <a href="https://www.researchgate.net/profile/Ab-Naik/publication/360642412_Aspartame_Effects_and_Awareness/links/62949ea76886635d5cae8511/Aspartame-Effects-and-Awareness.pdf">6</a> <a href="https://doi.org/10.3390/nu13061957">7</a> <a href="https://doi.org/10.1093/nutrit/nux035">8</a>. Note that many of these <em>do</em> suggest health concerns with aspartame, but I am discounting their conclusions because I don’t think they are credible. If <em>you</em> don’t trust <em>me</em>, you should look at those papers yourself. I have linked (here or below) everything I found that was published after the year 2000 and claimed to be a review of aspartame.</p>

<p><strong>Butchko et al. <a href="https://doi.org/10.1006/rtph.2002.1542"><em>Aspartame: Review of Safety</em></a>. Regulatory Toxicology and Pharmacology, 2002.</strong></p>

<p>Our first is a 93-page monster from a team of 24 scientists (three of whom, note, are employed by The NutraSweet Company). As I write this, it’s been cited 344 times. In their summary, you can feel the frustration:</p>

<p><img src="https://dynomight.net/img/aspartame/butchko-1.svg" alt="butchko 1"></p>

<p>Here’s a table from this paper:</p>

<p><img src="https://dynomight.net/img/aspartame/butchko-2.svg" alt="butchko 2"></p>

<p>This is helpful to put things in perspective. If you drink a Diet Coke, you get 2.6 mg/kg of aspartame, assuming you weight 70 kg. That’s around 20 times less than the government recommended limits. It’s also around <em>1000 times</em> less than doses that do not show harms in animals.</p>

<p><strong>Magnuson et al. <a href="https://doi.org/10.1080/10408440701516184"><em>Aspartame: A Safety Evaluation Based on Current Use Levels, Regulations, and Toxicological and Epidemiological Studies</em></a>. Critical Reviews in Toxicology, 2007.</strong></p>

<p>This is a 99-page paper that’s been cited 430 times so far. The conclusions are kind of feisty.</p>

<p><img src="https://dynomight.net/img/aspartame/magnuson.svg" alt="magnuson"></p>

<details>
  <summary>Note that this study was funded by Ajinomoto via some kind of blind-trust arrangement.</summary>

  <p>The paper describes it this way:</p>

  <p><img src="https://dynomight.net/img/aspartame/magnuson-sponsor.svg" alt="magnuson sponsor"></p>

  <p>I still mostly trust this paper, but <a href="https://dynomight.net/alcohol-trial/">we all know how these things can go</a>.</p>

</details>

<p><strong>Rogers et al. <a href="https://doi.org/10.1038/ijo.2015.177"><em>Does low-energy  sweetener consumption affect energy intake and body weight? A systematic review, including meta-analyses, of the evidence from human and animal studies</em></a>. International Journal of Obesity, 2016.</strong></p>

<p>This paper has 320 citations and 11 authors (four of whom—sigh—have gotten grants from or worked at sweetener companies). It focuses on metabolism and weight. Here is their conclusion, where LES = Low-Energy Sweeteners, EI = Energy Intake, and BW = Body Weight.</p>

<p><img src="https://dynomight.net/img/aspartame/rogers-1.svg" alt="rogers 1"></p>

<p><img src="https://dynomight.net/img/aspartame/rogers-2.svg" alt="rogers 2"></p>

<h3 id="other-reviews">Other reviews</h3>

<p>There are many other reviews that seem scientifically solid, but aren’t as comprehensive as those above. Here’s a representative snippet from each:</p>

<table>
  <thead>
    <tr>
      <th>paper</th>
      <th>snippet</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://doi.org/10.1111/j.1541-4337.2006.tb00081.x">Kroger et al. (2006)</a></td>
      <td>“within the range of variation caused by day-to-day differences in food intake and are clearly not harmful”</td>
    </tr>
    <tr>
      <td><a href="https://doi.org/10.1016/j.fct.2013.07.040">Marinovich et al. (2009)</a></td>
      <td>“not consistently related to vascular events and preterm deliveries.”</td>
    </tr>
    <tr>
      <td><a href="https://doi.org/10.1080/19338244.2013.828674">Mallikarjun and Sieburth (2013)</a></td>
      <td>“suggest that [aspartame] consumption has no significant carcinogenic effect in rodents.”</td>
    </tr>
    <tr>
      <td><a href="https://doi.org/10.1080/10408398.2017.1304358">Santos et al. (2018)</a></td>
      <td>“no found deleterious effects associated with aspartame consumption on variables studied”</td>
    </tr>
    <tr>
      <td><a href="https://doi.org/10.1016/j.yrtph.2018.01.009">Haighton et al. (2018)</a></td>
      <td>“a conclusion that aspartame is not carcinogenic is supported”</td>
    </tr>
    <tr>
      <td><a href="https://doi.org/10.1016/j.yrtph.2019.01.033">Haighton et al. (2019)</a></td>
      <td>“do not support that [low calories sweeteners and] aspartame, are associated with an increased risk of cancer in humans.”</td>
    </tr>
    <tr>
      <td><a href="https://doi.org/10.1001/jamanetworkopen.2022.2092">McGlynn et al. (2022)</a></td>
      <td>“small improvements in body weight and cardiometabolic risk factors without evidence of harm”</td>
    </tr>
  </tbody>
</table>

<h2 id="why-write-this">Why write this?</h2>

<p>I write all this with some trepidation, as my <a href="https://dynomight.net/cola/#ps">previous</a> <a href="https://dynomight.net/thanks/">mentions</a> of aspartame caused a surprising amount of rancor.</p>

<p>But whatever, I’ll die on this hill: After aspartame is consumed, it immediately breaks down into three naturally occurring chemicals. Even large amounts of aspartame cause smaller fluctuations in those chemicals than normal food. The current science says that the health impact of aspartame is essentially zero. Every credible body that has studied this question has reached the same conclusion.</p>

<p>Is it possible that some harms have been missed? Of course! That’s how science works: Evidence accumulates slowly. It never becomes a <em>sure thing</em>, we just eventually decide it’s sure <em>enough</em> and move on with our lives.</p>

<p>So why are we still talking about aspartame? Why worry about it rather than, say, <em>sugar</em> or <em>alcohol</em>? I know many people who avoid diet soda but drink sugar-sweetened soda or large amounts of alcohol. That’s choosing a known harm over something that appears harmless.</p>

<p>Or, why not worry about:</p>

<ul>
  <li><a href="https://www.weather.gov/safety/lightning-odds">Lightning</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Cotton_swab#Medical_risks">Q-tips</a></li>
  <li><a href="https://dynomight.net/air/#things-that-create-particles-indoors">Fireplaces</a></li>
  <li><a href="https://doi.org/10.1097/PAF.0b013e31828d68c7">Bathtubs</a></li>
  <li><a href="https://www.cdc.gov/pictureofamerica/pdfs/Picture_of_America_Poisoning.pdf">Generators</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Lawn_mower#Safety_issues">Lawn mowers</a></li>
  <li><a href="https://www.bbc.com/news/world-us-canada-48360832">Umbrellas</a></li>
  <li><a href="https://wwwnc.cdc.gov/travel/yellowbook/2020/travel-by-air-land-sea/cruise-ship-travel">Cruise ships</a></li>
  <li><a href="https://dynomight.net/air/#particles-while-commuting">Air in subways</a></li>
  <li><a href="https://doi.org/10.1371/journal.pmed.0040290">Blood clots in planes</a></li>
  <li><a href="https://www.cdc.gov/nceh/radiation/air_travel.html">Cosmic rays in planes</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Heterocyclic_amine_formation_in_meat">Charred meat</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Anticholinergic#Side_effects">Anticholinergics</a></li>
  <li><a href="https://doi.org/10.1093/fqsafe/fyy025">Cocoa</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Perfluorooctanoic_acid#Health_concerns">Perfluorooctanoic acid</a></li>
  <li><a href="https://doi.org/10.1128/CMR.00011-10">Helicobacter pylori</a></li>
</ul>

<p>All of these present some real danger but mostly no one cares.</p>

<p>To me, most skepticism of aspartame looks like an <a href="https://slatestarcodex.com/2014/08/14/beware-isolated-demands-for-rigor/">isolated demand for rigor</a>—an impossibly high standard of evidence that isn’t applied to other things. We all have finite bandwidth for things to worry about, and the evidence places aspartame very low on the list of sensible worries.</p>

<p>Many of the same people who claim aspartame is risky exhort to Follow the Science in other domains. I wonder: What evidence would be convincing proof of aspartame’s safety, but doesn’t already exist? Is it even possible? If not, then OK! But if you only follow the science when the conclusions are intuitive, it’s not science that chooses your destination.</p>

  


  

  

  
  
  
  

</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[It's 2023 and memory overwrite bugs are not just a thing theyre still number one (109 pts)]]></title>
            <link>https://www.theregister.com/2023/06/29/cwe_top_25_2023/</link>
            <guid>36562727</guid>
            <pubDate>Sun, 02 Jul 2023 16:01:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2023/06/29/cwe_top_25_2023/">https://www.theregister.com/2023/06/29/cwe_top_25_2023/</a>, See on <a href="https://news.ycombinator.com/item?id=36562727">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>The most dangerous type of software bug is the out-of-bounds write, according to MITRE this week. This type of flaw is responsible for 70 CVE-tagged holes in the US government's list of known vulnerabilities that are under active attack and need to be patched, we note.</p>
<p>Out-of-bounds write, sometimes labeled <a target="_blank" rel="nofollow" href="https://cwe.mitre.org/data/definitions/787.html">CWE-787</a>, also took the top spot in 2022, showing a distinct lack of improvement.</p>
<p>An out-of-bounds write happens when software (and sometimes hardware) alters memory it's not supposed to, such as by writing data to a memory buffer and overshooting the end of that buffer, causing it to unexpectedly change other variables and information and/or just crash. That kind of bug can be triggered accidentally through normal operation, or it can be triggered deliberately by exploit code.</p>

    

<p>Typically, exploit code will induce an out-of-bounds write to alter data structures so that the flow of execution is hijacked and diverted in a way the attacker chooses, allowing them to take control of the software, be it an application, a remote service, or part of an operating system. Ideally, software should be written to prevent this kind of overwrite, and using memory-safe languages like Rust <a target="_blank" href="https://www.theregister.com/2022/11/11/nsa_urges_orgs_to_use/">can help here</a>.</p>

        


        

<p>Number two on MITRE's list is the less complex but still annoying cross-site scripting bug (<a target="_blank" rel="nofollow" href="https://cwe.mitre.org/data/definitions/79.html">CWE-79</a>), which was key in four CVEs in the known exploited vulnerabilities <a target="_blank" rel="nofollow" href="https://www.cisa.gov/known-exploited-vulnerabilities-catalog">catalog</a> maintained by Uncle Sam's CISA. This bug type is a fancy form of a failure to sanitize user input.</p>
<p>Number three — SQL injection flaws (<a target="_blank" rel="nofollow" href="https://cwe.mitre.org/data/definitions/89.html">CWE-89</a>) — account for four known exploited bugs in the CISA catalog. Again, another form of input sanitization failure. Clean and neutralize your inputs, people. You can't assume all your users are nice.</p>

        

<p>MITRE compiles the annual <a target="_blank" rel="nofollow" href="https://cwe.mitre.org/top25/archive/2023/2023_top25_list.html">CWE Top 25 list</a> by analyzing public vulnerability data in America's <a target="_blank" rel="nofollow" href="https://nvd.nist.gov/">National Vulnerability Database</a>. This year's list is based on 43,996 CVE records for vulnerabilities in 2021 and 2022, and was issued in hand with US Homeland Security and CISA.</p>
<p>"These weaknesses lead to serious vulnerabilities in software," the cybersecurity agency <a target="_blank" rel="nofollow" href="https://www.cisa.gov/news-events/alerts/2023/06/29/2023-cwe-top-25-most-dangerous-software-weaknesses">warned</a> today. "An attacker can often exploit these vulnerabilities to take control of an affected system, steal data, or prevent applications from working."&nbsp;</p>
<p>In fact, the top three most dangerous software weaknesses for 2023 were also the most dangerous, and in the same order, in the 2022 list. Progress is slow, it seems.</p>
<div>
<h3>Time to get patching</h3>

<p>Also today, CISA added eight more flaws to its <a target="_blank" rel="nofollow" href="https://www.cisa.gov/known-exploited-vulnerabilities-catalog">Known Exploited Vulnerabilities Catalog</a>. These affect D-Link and Samsung devices and they are tracked as:</p>

<ul>
<li>CVSS 9.8 — <a target="_blank" rel="nofollow" href="https://nvd.nist.gov/vuln/detail/CVE-2023-25717">CVE-2019-17621</a> D-Link DIR-859 router contains a command execution vulnerability.</li>

<li>CVSS 7.8 — <a target="_blank" rel="nofollow" href="https://nvd.nist.gov/vuln/detail/CVE-2019-20500">CVE-2019-20500</a> D-Link DWL-2600AP access points are vulnerable to command injection attacks.</li>

<li>CVSS 7.8 — <a target="_blank" rel="nofollow" href="https://nvd.nist.gov/vuln/detail/CVE-2021-25487">CVE-2021-25487</a> Samsung mobile devices are vulnerable to out-of-bounds read.&nbsp;</li>

<li>CVSS 5.5 — <a target="_blank" rel="nofollow" href="https://nvd.nist.gov/vuln/detail/CVE-2021-25489">CVE-2021-25489</a> Samsung mobile devices contain an improper input validation flaw.</li>

<li>CVSS 6.4 — <a target="_blank" rel="nofollow" href="https://nvd.nist.gov/vuln/detail/CVE-2021-25394">CVE-2021-25394</a> Samsung mobile devices are susceptible to a race condition vulnerability.</li>

<li>CVSS 9.0 — <a target="_blank" rel="nofollow" href="https://nvd.nist.gov/vuln/detail/CVE-2016-3427">CVE-2021-25395</a> another race condition bug in Samsung mobile devices, but this one's critical.&nbsp;</li>

<li>CVSS 6.7 — <a target="_blank" rel="nofollow" href="https://nvd.nist.gov/vuln/detail/CVE-2021-25371">CVE-2021-25371</a> an unspecified flaw in Samsung mobile devices.</li>

<li>CVSS 6.7 — <a target="_blank" rel="nofollow" href="https://nvd.nist.gov/vuln/detail/CVE-2021-25372">CVE-2021-25372</a> Samsung mobile devices contain an improper boundary check vulnerability.</li>
</ul>
</div>
<p>Number four, however, was one of the "biggest movers" on the list, jumping from the seventh spot last year to the fourth-ranked most dangerous issue this year. It's <a target="_blank" rel="nofollow" href="https://cwe.mitre.org/data/definitions/416.html">CWE-416</a>, or use-after-free. This type of exploitable bug is when a program, remote service, or operating system component releases memory that's no longer needed, and then continues to use it anyway. At that point, it's relying on memory that could be, say, manipulated by some other code, and can lead to crashes or hijacking of execution.</p>
<p>Again, memory-safe languages are useful here as they abstract away this fiddly memory management, or ensure insecure memory use is blocked.</p>

        

<p>Some of the other biggest movers up the list, according to MITRE, include <a target="_blank" rel="nofollow" href="https://cwe.mitre.org/data/definitions/862.html">CWE-862</a>, which covers missing authorization bugs. This weakness jumped from sixteenth position last year to number 11 in 2023.&nbsp;&nbsp;</p>
<p>Additionally, <a target="_blank" rel="nofollow" href="https://cwe.mitre.org/data/definitions/269.html">CWE-269</a> (improper privilege management) moved up seven places to 22 on the list, and <a target="_blank" rel="nofollow" href="https://cwe.mitre.org/data/definitions/863.html">CWE-863</a> (incorrect authorization) went from rose four ranks to number 24.</p>
<ul>

<li><a href="https://www.theregister.com/2023/06/21/vmware_bug_under_exploit/">A (cautionary) tale of two patched bugs, both exploited in the wild</a></li>

<li><a href="https://www.theregister.com/2023/06/21/apple_patches_triangledb_spyware/">Apple squashes kernel bug used by TriangleDB spyware</a></li>

<li><a href="https://www.theregister.com/2023/06/16/third_moveit_bug_fixed/">Third MOVEit bug fixed a day after PoC exploit made public</a></li>

<li><a href="https://www.theregister.com/2023/06/13/june_patch_tuesday_vmware_vuln/">June Patch Tuesday: VMware vuln under attack by Chinese spies, Microsoft kinda meh</a></li>
</ul>
<p>There's also a couple new entries to this year's list: <a target="_blank" rel="nofollow" href="https://cwe.mitre.org/data/definitions/269.html">CWE-269</a> (improper privilege management), in 22nd place, and <a target="_blank" rel="nofollow" href="https://cwe.mitre.org/data/definitions/863.html">CWE-863</a> (incorrect authorization) as a newcomer in 24th.</p>
<p>"CWEs are becoming more and more prevalent in vulnerability exposure conversations as the community looks to avoid the root causes that can become vulnerabilities," according to <a target="_blank" rel="nofollow" href="https://cwe.mitre.org/top25/index.html">MITRE</a>.&nbsp;</p>
<p>To this end, the nonprofit will publish a series of reports over the next few months that aim to help organizations "more effectively" use the Top 25 list. These will cover a range of topics including weaknesses that didn't quite make the Top 25 —&nbsp;but orgs should still be aware of them.&nbsp;</p>
<p>It will also publish a report on trends in CWEs over the last four years, and a report on actively exploited weaknesses based on CISA's catalog.&nbsp; ®</p>                                


                    </div></div>]]></description>
        </item>
    </channel>
</rss>