<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 02 Aug 2023 16:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[How I discovered the underground world of credit card network exploitation (106 pts)]]></title>
            <link>https://chargebackstop.com/blog/card-networks-exploitation/</link>
            <guid>36971888</guid>
            <pubDate>Wed, 02 Aug 2023 15:05:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chargebackstop.com/blog/card-networks-exploitation/">https://chargebackstop.com/blog/card-networks-exploitation/</a>, See on <a href="https://news.ycombinator.com/item?id=36971888">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
        <p><i>Originally posted by Piotr Mierzejewski at&nbsp;<a target="_blank" href="https://piotrmierzejewski.com/p/card-networks-exploitation">https://piotrmierzejewski.com/p/card-networks-exploitation</a>. Piotr is a founding engineer on the ChargebackStop.com team. Together we're helping SaaS companies detect &amp; prevent chargebacks before they happen.</i></p>
            <figure>
              <img loading="lazy" src="https://unicorn-s3.b-cdn.net/CleanShot-2023-08-02-at-15.38.55@2x-g2b1v.png" srcset="https://unicorn-s3.b-cdn.net/CleanShot-2023-08-02-at-15.38.55@2x-g2b1v.png?width=290&amp;height=163 290w,https://unicorn-s3.b-cdn.net/CleanShot-2023-08-02-at-15.38.55@2x-g2b1v.png?width=345&amp;height=194 345w,https://unicorn-s3.b-cdn.net/CleanShot-2023-08-02-at-15.38.55@2x-g2b1v.png?width=395&amp;height=222 395w,https://unicorn-s3.b-cdn.net/CleanShot-2023-08-02-at-15.38.55@2x-g2b1v.png?width=738&amp;height=414 738w,https://unicorn-s3.b-cdn.net/CleanShot-2023-08-02-at-15.38.55@2x-g2b1v.png?width=1060&amp;height=594 1060w,https://unicorn-s3.b-cdn.net/CleanShot-2023-08-02-at-15.38.55@2x-g2b1v.png?width=580&amp;height=326 580w,https://unicorn-s3.b-cdn.net/CleanShot-2023-08-02-at-15.38.55@2x-g2b1v.png?width=690&amp;height=388 690w,https://unicorn-s3.b-cdn.net/CleanShot-2023-08-02-at-15.38.55@2x-g2b1v.png?width=790&amp;height=444 790w,https://unicorn-s3.b-cdn.net/CleanShot-2023-08-02-at-15.38.55@2x-g2b1v.png?width=1476&amp;height=828 1476w,https://unicorn-s3.b-cdn.net/CleanShot-2023-08-02-at-15.38.55@2x-g2b1v.png?width=2120&amp;height=1188 2120w,https://unicorn-s3.b-cdn.net/CleanShot-2023-08-02-at-15.38.55@2x-g2b1v.png?width=870&amp;height=489 870w,https://unicorn-s3.b-cdn.net/CleanShot-2023-08-02-at-15.38.55@2x-g2b1v.png?width=1035&amp;height=582 1035w,https://unicorn-s3.b-cdn.net/CleanShot-2023-08-02-at-15.38.55@2x-g2b1v.png?width=1185&amp;height=666 1185w,https://unicorn-s3.b-cdn.net/CleanShot-2023-08-02-at-15.38.55@2x-g2b1v.png?width=2214&amp;height=1242 2214w,https://unicorn-s3.b-cdn.net/CleanShot-2023-08-02-at-15.38.55@2x-g2b1v.png?width=3180&amp;height=1782 3180w," sizes="(max-width: 320px) 290px,(max-width: 375px) 345px,(max-width: 425px) 395px,(max-width: 768px) 738px,1060px" alt="CleanShot-2023-08-02-at-15.38.55@2x-g2b1v">
              
            </figure>
          <p>A couple of weeks back at work, we were alerted that suddenly our card decline rates are much higher than usual.</p><p>A quick glance at our Stripe dashboard revealed a lot of failed charges by users with a very auto-generated-sounding names and rather odd email domains<a target="_blank" href="https://piotrmierzejewski.com/p/card-networks-exploitation#user-content-fn-1">1</a>. We quickly concluded that we were hit with a classic card testing attack. We enabled&nbsp;<a target="_blank" href="https://stripe.com/radar" rel="nofollow">Stripe Radar</a>, tossed implementing captcha in our checkout into our backlog, and moved on.</p><p>In the meantime, I noticed a couple of tweets describing similar issues from Pieter Levels and Danny Postma. I shared them with my team, we looked for infamous Jake Smith in our Stripe account (he wasn’t there), and moved on.</p><blockquote><div><p>Had to refund and cancel 240 customers called Jake Smith from Philippines with $7,000 in card testing payments</p><p>No idea why Stripe doesn’t catch these</p><p>They even passed my Cloudflare CAPTCHA?!</p><p>Had to block 🇵🇭 PH payments now sorry</p></div><cite>— @levelsio (@levelsio) June 5, 2023</cite></blockquote><p><a target="_blank" href="https://twitter.com/levelsio/status/1665625055777398784?ref_src=twsrc%5Etfw"><i>Source</i></a></p><blockquote><p>Seems like everyone I know on Twitter is getting attacked by "jack smith" with fraudulent charges.<br>Hope&nbsp;@stripe&nbsp;comes with a solution FAST, because this is going to kill some businesses.</p><cite>— Danny Postma (@dannypostmaa) June 5, 2023</cite></blockquote><p><a target="_blank" href="https://twitter.com/dannypostmaa/status/1665693420680097793?ref_src=twsrc%5Etfw"><i>Source</i></a></p><p>Fast forward couple weeks, we get high decline ratio alert again. This time I started implementing Stripe Radar rules ad-hoc (such as blocking transactions after specific number of failures within given timeframe). Looking more closely at the traffic however, it turned out that attackers tested up to four cards a minute. Most of the time the traffic was much less intense and much less consistent.</p><p>After reading some&nbsp;<a target="_blank" href="https://stripe.com/docs/disputes/prevention/card-testing#optimize-integration" rel="nofollow">materials from Stripe on card testing</a>, I realised that our implementation should be pretty well guarded against this kind of attack. We require users to log in before we allow them to open the checkout, and use Payment Element with some of the signals. According to the page linked above, our protection against card testing should be close to ‘excellent’.</p><p>After some more digging and consulting my colleagues, we arrived at a conclusion that&nbsp;the traffic we were experiencing was most likely manual, or at most very lightly automated.</p><p>My belief was being reinforced as I noticed that almost all of the cards that attackers used:</p><ul><li>Were all issued by the same bank</li><li>Had the same funding source [2]</li><li>Came from the same country: USA</li></ul><p>How was it possible that attackers had a list of cards with such similar parameters? I always imagined that compromised credit cards would would be much more diverse. Were they actually leaked from the bank itself?</p><p>One of my sub-hypotheses was that the goal of the attackers is to pay for VEED account with a stolen card and resell it for profit. I started pulling this thread and I found a Telegram channel that recommended VEED as a great tool for adding subtitles to your videos.</p><p>I also found in that channel messages with credit cards’ BINs [3], CVC, expiration dates and links to tools that generate valid credit card numbers based on these inputs.</p><p>It turns out that there’s whole underground(-ish, all of of was publicly available on the internet to me) [4]&nbsp;world of people who share credit card parameters with biggest likelihood creating a card that’s accepted on a specific website (usually some SaaS)</p>
            <figure>
              <img loading="lazy" src="https://unicorn-s3.b-cdn.net/image-894yu.png" srcset="https://unicorn-s3.b-cdn.net/image-894yu.png?width=290&amp;height=256 290w,https://unicorn-s3.b-cdn.net/image-894yu.png?width=345&amp;height=305 345w,https://unicorn-s3.b-cdn.net/image-894yu.png?width=395&amp;height=349 395w,https://unicorn-s3.b-cdn.net/image-894yu.png?width=449&amp;height=396 449w,https://unicorn-s3.b-cdn.net/image-894yu.png?width=449&amp;height=396 449w,https://unicorn-s3.b-cdn.net/image-894yu.png?width=449&amp;height=396 449w,https://unicorn-s3.b-cdn.net/image-894yu.png?width=449&amp;height=396 449w," sizes="(max-width: 320px) 290px,(max-width: 375px) 345px,(max-width: 425px) 395px,(max-width: 768px) 449px,449px" alt="image-894yu">
              
            </figure>
          <p>A telegram channel message with instructions on getting Spotify Premium for free (illegally)</p><p>We were most likely a victim of such manual attack, initiated from some private Discord server or Telegram channel. I never managed to find a specific source, but I strongly suspect it given that all of the cards had the same parameters that were all determined by a BIN. In the public channels I managed to find they quite often sent nagging messages about going private in the couple following days. I suspect most of this activity is not accessible to me.</p>
            <figure>
              <img loading="lazy" src="https://unicorn-s3.b-cdn.net/image-wyerc.png" srcset="https://unicorn-s3.b-cdn.net/image-wyerc.png?width=290&amp;height=207 290w,https://unicorn-s3.b-cdn.net/image-wyerc.png?width=345&amp;height=246 345w,https://unicorn-s3.b-cdn.net/image-wyerc.png?width=395&amp;height=282 395w,https://unicorn-s3.b-cdn.net/image-wyerc.png?width=456&amp;height=325 456w,https://unicorn-s3.b-cdn.net/image-wyerc.png?width=456&amp;height=325 456w,https://unicorn-s3.b-cdn.net/image-wyerc.png?width=456&amp;height=325 456w,https://unicorn-s3.b-cdn.net/image-wyerc.png?width=456&amp;height=325 456w," sizes="(max-width: 320px) 290px,(max-width: 375px) 345px,(max-width: 425px) 395px,(max-width: 768px) 456px,456px" alt="image-wyerc">
              
            </figure>
          <p>A telegram channel message with instructions on getting YouTube Premium for free (illegally)</p><p>On top of that, there’s a plethora of online tools that will take a list of autogenerated cards and run it through any Stripe Checkout session automatically. This was a bit surprising as I expected Stripe Checkout to be the least prone to any sort of automation, since Stripe owns this integration end-to-end.</p>
            <figure>
              <img loading="lazy" src="https://unicorn-s3.b-cdn.net/image-p1kdj.png" srcset="https://unicorn-s3.b-cdn.net/image-p1kdj.png?width=290&amp;height=202 290w,https://unicorn-s3.b-cdn.net/image-p1kdj.png?width=345&amp;height=240 345w,https://unicorn-s3.b-cdn.net/image-p1kdj.png?width=395&amp;height=275 395w,https://unicorn-s3.b-cdn.net/image-p1kdj.png?width=738&amp;height=514 738w,https://unicorn-s3.b-cdn.net/image-p1kdj.png?width=749&amp;height=521 749w,https://unicorn-s3.b-cdn.net/image-p1kdj.png?width=580&amp;height=404 580w,https://unicorn-s3.b-cdn.net/image-p1kdj.png?width=690&amp;height=480 690w,https://unicorn-s3.b-cdn.net/image-p1kdj.png?width=749&amp;height=521 749w,https://unicorn-s3.b-cdn.net/image-p1kdj.png?width=749&amp;height=521 749w," sizes="(max-width: 320px) 290px,(max-width: 375px) 345px,(max-width: 425px) 395px,(max-width: 768px) 738px,749px" alt="image-p1kdj">
              
            </figure>
          <p>Part of automatic Stripe Checkout cracking tool source code, it generates random (invalid) email address on Gmail</p>
            <figure>
              <img loading="lazy" src="https://unicorn-s3.b-cdn.net/image-0acwm.png" srcset="https://unicorn-s3.b-cdn.net/image-0acwm.png?width=290&amp;height=349 290w,https://unicorn-s3.b-cdn.net/image-0acwm.png?width=339&amp;height=407 339w,https://unicorn-s3.b-cdn.net/image-0acwm.png?width=339&amp;height=407 339w,https://unicorn-s3.b-cdn.net/image-0acwm.png?width=339&amp;height=407 339w,https://unicorn-s3.b-cdn.net/image-0acwm.png?width=339&amp;height=407 339w,https://unicorn-s3.b-cdn.net/image-0acwm.png?width=339&amp;height=407 339w,https://unicorn-s3.b-cdn.net/image-0acwm.png?width=339&amp;height=407 339w," sizes="(max-width: 320px) 290px,(max-width: 375px) 339px,(max-width: 425px) 339px,(max-width: 768px) 339px,339px" alt="image-0acwm">
              
            </figure>
          <p>Unhappy Telegram chat message regarding Pieter Levels' tweet quoted above</p><h2>The clean-up</h2><p>The painful aftermath of this attack for us was that some of the attackers succeeded and managed to pay for our product. At that stage we still weren’t sure of the scale of it though, so it was time to crunch some data.</p><p>We queried our data storage for customers with more than 5 failed charges since May 1st. Then I got ChatGPT to quickly create a Python script which found these customers’ email domains. Once we had a list of domains, we queried our database to get a list of all successful charges made by customers with emails in these domains.</p><p>With a list of charges I had another script written by ChatGPT to find which charges were being already disputed. We learnt that 15% of the successful fraudulent charges resulted in chargebacks. We decided to accept all of the disputes and swallow&nbsp;<a target="_blank" href="https://stripe.com/en-gb/pricing#faqs" rel="nofollow">Stripe’s £20 fee</a>&nbsp;for each. It’s a cost of running an online business, it seems. I created a restricted key in Stripe with lowest possible permissions, and prompted ChatGPT to create a script to accept the chargebacks.</p><p>The next Python script to come out of ChatGPT fetched all of the charges from the list, refunded all the non-disputed charges, and cancelled any active subscriptions for customers who created these charges. For these charges, we were at loss on&nbsp;<a target="_blank" href="https://support.stripe.com/questions/understanding-fees-for-refunded-payments" rel="nofollow">Stripe’s and network fees both for the successful charge and – potentially – for the refund</a>. It’s much less than £20 for a chargeback, but still a loss.</p><p>The final Python script I used wasn’t strictly necessary, but I wanted to ensure that all of the charges in the list are either disputed and accepted, or refunded. Additionally, I wanted to make sure that none of these customers had any active subscriptions anymore. I got ChatGPT to spit it out for me one more time, changed my Stripe restricted key to not have write access anymore, and ran the script. I was able to confirm the problem was solved (or at least part of the problem that we were able to uncover)!</p><p>As a sidenote: ChatGPT advised a lot of caution when running its scripts, suggested to test it well on a small sample, etc. Well appreciated! I reviewed all of the scripts carefully, and also never shared any customer data, IDs, or API keys. I think I saved at least a couple hours compared to hand-rolling these tools manually!</p><h2>On American banks</h2><p>The world of online payments is well-known to be unfair to businesses. You are unlikely to win a dispute, need to&nbsp;<a target="_blank" href="https://stripe.com/docs/disputes/measuring#excessive-dispute-activity" rel="nofollow">maintain dispute activity below 0.75%</a>, and pay&nbsp;<a target="_blank" href="https://stripe.com/docs/disputes/how-disputes-work#dispute-fees" rel="nofollow">£20 for every dispute, no matter if the business looses or wins</a>.</p><p>At the same time, banks (usually American ones) will happily accept transactions that have:</p><ul><li>Incorrect full name</li><li>Invalid&nbsp;CVV&nbsp;/&nbsp;CVC</li><li>Wrong expiration date</li><li>Only partial billing address provided, with incorrect ZIP code</li></ul><p>All of the above is still not enough to trigger a 3D secure authorisation. I’m not entirely sure why businesses should be held accountable for charges in which literally only the card number is correct.</p><p>I suspect that possibility of the above checks is somewhat limited in prepaid cards, but I’m sure that there’s room for improvement in this area.</p><h2>Prevention methods</h2><p>How can we prevent this from happening again?</p><p>As I mentioned before, at the beginning of the discovery process our gut reaction was enabling&nbsp;<a target="_blank" href="https://stripe.com/radar" rel="nofollow">Stripe Radar</a>. It’s a machine-learning-based solution that’s supposed to score every payment and block is automatically is some of the metrics do not add up.</p><p>As we learnt later on, it wasn’t very helpful with our case. The risk score for most of these transactions was between 0 and 5 (low risk). At the same time, I noticed a couple of legitimate customers being blocked after they failed to solve the 3D secure challenge twice. This is very anecdotal, but left me a little concerned about leaving the fate of the customer in hands of machine learning.</p><p>Thankfully, Stripe Radar also has&nbsp;<a target="_blank" href="https://stripe.com/docs/radar/rules" rel="nofollow">the ability to write custom rules</a>&nbsp;for when to request 3D secure challenge, send payment to manual review, or block it completely. It looks a little like pseudocode, but can be used to express surprisingly complex logic to narrow down malicious payment attempts.</p><p>This was the best weapon available to us. I put reasonable limits on the number of possible failed payment attempts within an hour, day, and week for a customer. Here’s a couple of rules from aforementioned Danny Postma that you can use as inspiration.</p><blockquote><p>Here's my custom Stripe Radar block list for anyone interested.&nbsp;pic.twitter.com/jPm1BAA5yK</p><cite>— Danny Postma (@dannypostmaa)&nbsp;May 2, 2023</cite></blockquote><p><a target="_blank" href="https://twitter.com/dannypostmaa/status/1653214594452832256?ref_src=twsrc%5Etfw"><i>Source</i></a></p><h2>Conclusions</h2><p>The cost of this fraudulent activity (starting with payment processor fees, chargeback penalties, engineering cost or even the risk of getting deplatformed) is being paid by businesses around the world. Stripe charging businesses £20 chargeback fee is another example of unfair treatment. All of these costs are eventually offloaded onto customers as higher prices.</p><p>The biggest learning to me is that the payment network that we all rely on a daily basis is quite exploitable. The ultimate decision whether the card can be charged is at issuing bank’s discretion.</p><p>Until the banks are willing to take more responsibility for their authorisations, this will keep on going. I’m not sure if there’s any incentive for them to fight it (are they at risk of getting deplatformed by the payment processors too?), but without their involvement best we can do is to keep a close eye on our charge failure rates, add captchas, and share our favourite Stripe Radar rules on Twitter.</p><h2>Footnotes</h2><ol><li>Nothing against self-hosting email, but the sad reality is that email address domains these days are either a couple of the most popular email providers, or company domains.&nbsp;</li><li>Funding source says if the card is a debit, credit, or prepaid card.&nbsp;</li><li>Bank Identification Number, the first six up to eight digits of any credit card.&nbsp;</li><li>I think the usual way of operating is starting out as a public channel, and then going private once they get the audience.&nbsp;</li></ol>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Does Cloudflare’s 1.1.1.1 DNS Block Archive.is? (2019) (125 pts)]]></title>
            <link>https://jarv.is/notes/cloudflare-dns-archive-is-blocked/</link>
            <guid>36970702</guid>
            <pubDate>Wed, 02 Aug 2023 13:36:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jarv.is/notes/cloudflare-dns-archive-is-blocked/">https://jarv.is/notes/cloudflare-dns-archive-is-blocked/</a>, See on <a href="https://news.ycombinator.com/item?id=36970702">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><strong>tl;dr:</strong> No. Quite the opposite, actually — <a target="_blank" rel="noopener noreferrer" href="https://archive.is/">Archive.is</a>’s owner is intentionally blocking 1.1.1.1 users.</p>
<p><img alt="Archive.today screenshot" fetchpriority="high" width="865" height="180" decoding="async" data-nimg="1" srcset="https://jarv.is/_next/image/?url=%2Fstatic%2Fimages%2Fnotes%2Fcloudflare-dns-archive-is-blocked%2Farchive-is.png&amp;w=1080&amp;q=60 1x, https://jarv.is/_next/image/?url=%2Fstatic%2Fimages%2Fnotes%2Fcloudflare-dns-archive-is-blocked%2Farchive-is.png&amp;w=1920&amp;q=60 2x" src="https://jarv.is/_next/image/?url=%2Fstatic%2Fimages%2Fnotes%2Fcloudflare-dns-archive-is-blocked%2Farchive-is.png&amp;w=1920&amp;q=60"></p>
<p>A <a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=19828317">recent post on Hacker News</a> pointed out something I’ve noticed myself over the past year — the <a target="_blank" rel="noopener noreferrer" href="https://archive.is/">Archive.is</a> website archiving tool (aka <a target="_blank" rel="noopener noreferrer" href="https://archive.today/">Archive.today</a> and a few other TLDs) appears unresponsive when I’m on my home network, where I use Cloudflare’s fantastic public DNS service, <a target="_blank" rel="noopener noreferrer" href="https://1.1.1.1/">1.1.1.1</a>. I didn’t connect the two variables until I read this post, where somebody noticed that the Archive.is domain resolves for <a target="_blank" rel="noopener noreferrer" href="https://developers.google.com/speed/public-dns/">Google’s 8.8.8.8</a> DNS, but not 1.1.1.1. An interesting and timeless debate on <a target="_blank" rel="noopener noreferrer" href="https://www.adweek.com/digital/why-consumers-are-increasingly-willing-to-trade-privacy-for-convenience/">privacy versus convenience</a> ensued.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://twitter.com/eastdakota">Matthew Prince</a>, the CEO and co-founder of <a target="_blank" rel="noopener noreferrer" href="https://www.cloudflare.com/">Cloudflare</a> (who’s also <a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/user?id=eastdakota">very active</a> on Hacker News), responded to the observation <a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=19828702">with a detailed explanation</a> of what’s happening behind the scenes, revealing that Archive.is’s owner is actively refusing to resolve their own website for 1.1.1.1 users because Cloudflare’s DNS offers <strong><em>too much</em></strong> privacy. Excerpt below, emphasis mine:</p>
<blockquote>
<p>We don’t block archive.is or any other domain via 1.1.1.1. [...] Archive.is’s authoritative DNS servers <strong>return bad results to 1.1.1.1 when we query them</strong>. I’ve proposed we just fix it on our end but our team, quite rightly, said that too would violate the integrity of DNS and the privacy and security promises we made to our users when we launched the service. [...] The archive.is owner has explained that <strong>he returns bad results to us because we don’t pass along the EDNS subnet information</strong>. This information leaks information about a requester’s IP and, in turn, sacrifices the privacy of users. <a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=19828702">Read more »</a></p>
</blockquote>
<p>In other words, Archive.is’s nameservers throw a hissy fit and return a bogus IP when Cloudflare <strong>doesn’t</strong> leak your geolocation info to them via the optional <a target="_blank" rel="noopener noreferrer" href="https://tools.ietf.org/html/rfc7871">EDNS client subnet feature</a>. The owner of Archive.is has plainly admitted this with <a target="_blank" rel="noopener noreferrer" href="https://twitter.com/archiveis/status/1018691421182791680">a questionable claim</a> (in my opinion) about the lack of EDNS information causing him “so many troubles.”</p>

<p>He’s even gone as far as <a target="_blank" rel="noopener noreferrer" href="https://community.cloudflare.com/t/archive-is-error-1001/18227/7">replying to support requests</a> by telling people to switch to Google’s DNS, which — surprise! — offers your location to nameservers <a target="_blank" rel="noopener noreferrer" href="https://developers.google.com/speed/public-dns/docs/ecs">with pleasure</a>.</p>
<p>I wrote the <a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=19828898">following reply</a> to Matthew, praising his team’s focus on the big picture:</p>
<blockquote>
<p>Honestly, Cloudflare choosing <em>not</em> to hastily slap a band-aid on a problem like this just makes me feel more compelled to continue using 1.1.1.1.</p>
<p>I hesitate to compare this to Apple calling themselves “courageous” when removing the headphone jack, but in this case, I think the word is appropriate. I’ll happily stand behind you guys if you take some PR hits while forcing the rest of the industry to make DNS safer — since it is understandable, admittedly, for users to conclude that “Cloudflare is blocking websites, sound the alarms!” at first glance.</p>
</blockquote>
<p>Sure, it’s annoying that I’ll need to use a VPN or change my DNS resolvers to use a pretty slick (and otherwise convenient) website archiver. But I’m more happy to see that Cloudflare is playing the privacy long-game, even at the risk of their users concluding that they’re blocking websites accessible to everyone else on the internet.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Edge Detection Doesn’t Explain Line Drawing (140 pts)]]></title>
            <link>https://aaronhertzmann.com/2020/04/19/lines-as-edges.html</link>
            <guid>36969473</guid>
            <pubDate>Wed, 02 Aug 2023 11:35:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aaronhertzmann.com/2020/04/19/lines-as-edges.html">https://aaronhertzmann.com/2020/04/19/lines-as-edges.html</a>, See on <a href="https://news.ycombinator.com/item?id=36969473">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article>

  

  <div>
    <p><strong><em>Note: a revised and updated version of this post has been</em> <a href="https://journals.sagepub.com/doi/10.1177/0301006621994407"><em>published in</em> Perception</a>, <em>or see the preprint: <a href="http://arxiv.org/abs/2101.09376">http://arxiv.org/abs/2101.09376</a>.</em></strong></p>

<p>Why do line drawings work? Why is it that we can immediately recognize objects in line drawings, even though they are not a phenomenon from our natural world. Numerous studies show that people who have never seen a line drawing before can understand a line drawing; line drawing is not something you have to learn to understand.</p>

<p>A classic answer to this question is what I will call the <strong>Lines-As-Edges hypothesis</strong>. It says that drawings simulate natural images because line features activate edge receptors in the human visual system. From what I can tell, this belief is widely shared among vision researchers; many people bring it up whenever I discuss line drawing perception, as did <a href="https://twitter.com/hardmaru/status/1250979159779635200?s=20">many commenters on a recent Twitter thread</a>.  A generalization of this is that lines correspond to <em>some</em> internal representation, such as neurons that fire on object contours, which I’ll call <strong>Line-As-Internal-Representation</strong> and also discuss here.</p>

<p>The purpose of this blog post is to explain why I think you should be skeptical of the <strong>Lines-As-Edges hypothesis</strong>. Many vision researchers and tweeters state some version of the hypothesis with uncritical certainty, as if the problem is solved, and I seem to have a hard time convincing them otherwise.  It has the feeling of unquestioned, received wisdom: everyone knows that this is true, and sees no need to question it.</p>

<p>I don’t claim that <strong>Lines-As-Edges</strong> is necessarily false, but I do argue that it is unsatisfyingly incomplete. In a <a href="https://journals.sagepub.com/doi/abs/10.1177/0301006620908207?journalCode=peca">recent paper</a> (<a href="https://arxiv.org/abs/2002.06260">preprint version</a>), I proposed a totally different explanation for line drawing. My explanation is also incomplete, but I think it provides some potential benefits. It is also compatible with <strong>Lines-As-Edges</strong>, so they could both be accurate.</p>

<h2 id="what-is-the-hypothesis">What is the Hypothesis?</h2>

<p>In order to discuss the hypothesis, we need a clear statement of what it actually says. Yet, for being so widely accepted, it’s very hard to find such a statement.  Although the idea has been “in the air” for decades, the only real statement of <em>Lines-As-Edges</em> that I’ve found is by <a href="https://www.frontiersin.org/articles/10.3389/fnhum.2011.00118/full">Sayim and Cavanagh (2011)</a>, and even they point out big gaps in the theory.</p>

<p>The idea arises from two compelling observations. First, since <a href="https://www.youtube.com/watch?v=IOHayh06LJ4">the pioneering experiments of Hubel and Wiesel in 50’s and 60’s</a>, we know that the visual cortex includes cells that are responsive the edge patterns. In short, one of the first things that happens to the signal from our retinas is edge detection.  Second, if you run an edge detector on a real image, and threshold the responses, then you often get something that looks like a line drawing. Here is one example, a Difference-of-Gaussians from the <a href="https://www.kyprianidis.com/p/cag2012/">XDoG paper</a>:</p>

<p><img src="https://aaronhertzmann.com/images/howtodraw/DoG.jpg" alt="Difference-of-Gaussian filter"></p>

<p>The most basic statement of the hypothesis is: the lines in a line drawing are drawn at natural image edges, where an edge receptor would fire. These lines activate the same edge receptor cells that the natural image would. Hence, the line drawing produces a cortical response that is very similar to that of some natural image, and thus you perceive the drawing and the photograph in roughly the same way.</p>

<h2 id="problem-1-what-about-all-the-other-features">Problem #1: What about all the other features?</h2>

<p>The most basic statement of the problem with Lines-As-Edges is that the human visual system isn’t just an edge detector. You can see colors, you can see absolute intensities. You can tell the difference between a thin black line and the silhouette of a dark object against a light background; we have both kinds of receptors in the primary visual cortex, as well as others.  Yet Lines-As-Edge supposes that the vision system discards all of this other information present in an image, for just this one special case. Why?</p>

<p>Discarding some features and not others seems arbitrary. Surely we could discard some other random collection of features and get other plausible image interpretations out of them.</p>

<p>I’ve never heard a plausible answer to this question, or even seen a concrete attempt to answer it.</p>

<h2 id="problem-2-we-cant-see-internal-representations">Problem #2: We can’t see internal representations</h2>

<p>A slightly different statement of the hypothesis is what I’ll call <strong>Lines-As-Internal-Representations</strong>.  The idea is that we have neurons that activate for object contours and similar, and that line drawings directly activate these neurons.  Lines-As-Edges is a special case of this hypothesis.</p>

<p>I don’t understand this claim at all. You can’t just show some visualization of some arbitrary neuronal activation to the brain and expect that those neurons will fire.</p>

<p>Suppose you have an algorithm that computes <code>y=f(x)</code>, and the computation involves an intermediate variable: <code>w=g(x); y=h(w)</code> so that <code>f(x)=h(g(x))</code>. You cannot expect to get the same result if you run <code>w=g(x); y=f(w)</code>. The types might not even match.  In terms of neural networks, this theory seems to be proposing that you can take filter output <code>i</code> from network layer <code>j</code> and directly feed it as input to the network. How would this even work?</p>

<h2 id="problem-3-what-is-the-benefit">Problem #3: What is the benefit?</h2>

<p>To really understand vision, we have to reason about more than just the visual cortex. If human vision was just edge detection, then we would have already figured out how vision works a long time ago.  Understanding vision by looking only at neurons is like trying to understand how a computer program works by looking only at the compiled machine instructions without reasoning about what the program is for.</p>

<p>The human vision system is extraordinarily robust at what it does: help us navigate and survive the world by providing us high-quality inferences about what we see. This process is extremely robust to all sorts of misleading and noisy sources of information.  Any explanation for how it can be fooled into inaccurate perception requires a compelling explanation in terms of the <em>goals</em> of the vision system. For example, <a href="https://en.wikipedia.org/wiki/Checker_shadow_illusion">Adelson’s checkerboard illusion</a> indicates that the visual system is much more interested in inferring actual reflectance rather than incoming irradiance.  Most visual illusions exploit inductive biases of the vision system: biases that are useful in normal situations, even though they create unexpected outcomes in cases that didn’t matter that much to our Pleistocene ancestors. Our Pleistocene ancestors probably didn’t need to consciously reason about relative irradiances.</p>

<p>Lines-as-edges posits a massive “failure” of visual inference—hallucinating shape where there is none—without providing any corresponding beneficial inductive bias.  We don’t actually believe the shape is there due to <a href="https://www.frontiersin.org/articles/10.3389/fnhum.2015.00295/full">the dichotomous nature of images</a>, but the inference of shape is still wildly erroneous under this hypothesis.</p>

<h2 id="problem-4-visual-art-isnt-just-line-drawings">Problem #4: Visual art isn’t just line drawings</h2>

<p>Suppose we add color to a line drawing:</p>

<p>
	<img src="https://aaronhertzmann.com/images/ipad_paintings/apple-watercolor2.jpg">
</p>

<p>Now you get a sense of the color of the object, and not just its outlines.  How would one generalize Lines-As-Edges to account for these different types of depiction? The visual system is no longer ignoring everything aside some gradients; it’s now paying attention to some colors (and not others).</p>

<p>Or suppose we add hatching:</p>

<p>
	<img src="https://aaronhertzmann.com/images/ipad_paintings/apple-sketchy2.jpg">
</p>

<p>How does Lines-As-Edges now explain our perception of this style?</p>

<p>Artists depict objects in a seemingly-infinite combination of outlines, colors, hatching, stippling, painting, and so much else. To account for this, the Lines-As-Edge hypothesis would need to argue that somehow the visual system recognizes each style and identifies which features to ignore and which to keep.  Again, this may be true in some sense, but it should be clear that Lines-As-Edges is far from being the end of the story.</p>

<p>Alternatively, it could be that untrained observers cannot understand these drawings; I am not aware of any studies on this topic, but I think it unlikely, since studies have shown that untrained observers can understand line drawings and they can understand photographs.</p>

<h2 id="problem-5-edge-detection-is-not-a-line-drawing-algorithm">Problem #5: Edge detection is not a line drawing algorithm</h2>

<p>Lines-As-Edges starts from the observation that edge detection can produce line drawings. But it often doesn’t.  Here are two examples from <a href="https://www.frontiersin.org/articles/10.3389/fnhum.2011.00118/full">Sayim and Cavanagh</a>:</p>

<p><img src="https://aaronhertzmann.com/images/howtodraw/sayim.jpg" alt="Lines vs. Edges"></p>

<p>That said, I think that Lines-As-Edges could be modified to account for this, combining ideas from <a href="http://people.csail.mit.edu/tjudd/apparentridges.html">Judd et al. (2007)</a> and <a href="https://journals.sagepub.com/doi/abs/10.1177/0301006620908207?journalCode=peca">my paper</a>.  The modified hypothesis would be: the visual system interprets line drawings as if they were edge images of a matte white object under headlight illumination, or averaged over a range of similar illumination. To my knowledge, this modified hypothesis is novel; except that Judd et al. make very similar assertions that come extremely close to this. However, this modification does not fix the other problems listed above.</p>

<h2 id="alternatives">Alternatives</h2>

<p>I believe <a href="https://journals.sagepub.com/doi/abs/10.1177/0301006620908207?journalCode=peca">my hypothesis</a> proposes a different way to think about this, without the problems above.  This hypothesis is compatible with Lines-As-Edges, while answering many of these questions.  My hypothesis has its own gaps, but I think it nonetheless it provides a promising way forward on these questions.</p>

<p>See <a href="https://hertzmann.github.io/2020/04/21/ar-vs-cs.html">the end of this follow-up post</a> for one possibility about how my hypothesis could incorporate edge information.</p>

  </div>

</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[South America is living one of the climate extreme events (127 pts)]]></title>
            <link>https://twitter.com/extremetemps/status/1686485331539820545</link>
            <guid>36969144</guid>
            <pubDate>Wed, 02 Aug 2023 10:43:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/extremetemps/status/1686485331539820545">https://twitter.com/extremetemps/status/1686485331539820545</a>, See on <a href="https://news.ycombinator.com/item?id=36969144">Hacker News</a></p>
Couldn't get https://twitter.com/extremetemps/status/1686485331539820545: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Win11Debloat is a simple script that removes pre-installed windows bloatware (112 pts)]]></title>
            <link>https://github.com/Raphire/Win11Debloat</link>
            <guid>36969022</guid>
            <pubDate>Wed, 02 Aug 2023 10:24:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Raphire/Win11Debloat">https://github.com/Raphire/Win11Debloat</a>, See on <a href="https://news.ycombinator.com/item?id=36969022">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Win11Debloat</h2>
<p dir="auto">Win11Debloat is a simple and lightweight powershell script that removes pre-installed windows bloatware apps, disables telemetry and declutters the experience by disabling or removing intrusive interface elements, ads and context menu items. No need to painstakingly go through all the settings yourself, or remove apps one by one. Win11Debloat makes the process quick and simple!</p>
<p dir="auto">You can pick and choose exactly which modifications you want the script to make, or use the default settings for your specific windows version. If you are unhappy with any of the changes you can easily revert them by using the registry files that are included in the 'Regfiles' folder, all of the apps that are removed can be reinstalled from the Microsoft store.</p>
<h4 tabindex="-1" dir="auto">Did this script help you? Please consider buying me a cup of coffee to support my work</h4>
<p dir="auto"><a href="https://ko-fi.com/M4M5C6UPC" rel="nofollow"><img src="https://camo.githubusercontent.com/cd07f1a5d90e454e7bbf69d22ebe4cdbd3a0b3dcf56ba0b6c2495a8e99c776be/68747470733a2f2f6b6f2d66692e636f6d2f696d672f676974687562627574746f6e5f736d2e737667" alt="ko-fi" data-canonical-src="https://ko-fi.com/img/githubbutton_sm.svg"></a></p>
<h2 tabindex="-1" dir="auto">The windows 10 default settings will</h2>
<ul dir="auto">
<li>Remove all bloatware apps from <a href="#click-for-list-of-bloat-that-is-removed">this list</a>.</li>
<li>Disable telemetry, diagnostic data &amp; targeted ads.</li>
<li>Disable bing search &amp; cortana in windows search.</li>
<li>Disable tips &amp; tricks on the lockscreen. (This may change your lockscreen wallpaper to the windows default)</li>
<li>Disable tips, tricks and suggestions in the startmenu and settings, and sync provider ads in windows explorer.</li>
<li>Disable the widget service &amp; hide the widget (news and interests) icon on the taskbar.</li>
<li>Hide the Chat (meet now) icon from the taskbar.</li>
<li>Hide the 3D objects folder under 'This pc' in windows explorer.</li>
<li>Hide the 'Include in library', 'Give access to' and 'Share' options in the context menu.</li>
</ul>
<h2 tabindex="-1" dir="auto">The windows 11 default settings will</h2>
<ul dir="auto">
<li>Remove all bloatware apps from <a href="#click-for-list-of-bloat-that-is-removed">this list</a>.</li>
<li>Disable telemetry, diagnostic data &amp; targeted ads.</li>
<li>Disable bing search, bing AI &amp; cortana in windows search.</li>
<li>Disable tips &amp; tricks on the lockscreen. (This may change your lockscreen wallpaper to the windows default)</li>
<li>Disable tips, tricks and suggestions in the startmenu and settings, and sync provider ads in windows explorer.</li>
<li>Disable the widget service &amp; hide the widget icon on the taskbar.</li>
<li>Hide the Chat icon from the taskbar.</li>
</ul>
<h2 tabindex="-1" dir="auto">The 'Custom' option allows you to customize the script to your exact needs</h2>
<p dir="auto">A full list of what changes this script can make can be found <a href="#improve-your-windows-experience">here</a>.</p>
<h2 tabindex="-1" dir="auto">Usage</h2>
<p dir="auto">Disclaimer: I believe this script to be completely safe to run, but use this script at your own risk!</p>
<h3 tabindex="-1" dir="auto">Easy method</h3>
<ol dir="auto">
<li><a href="https://github.com/Raphire/Win11Debloat/archive/master.zip">Download the latest version of the script</a>, and extract the .ZIP file to your desired location.</li>
<li>Navigate to the Win11Debloat folder</li>
<li>Double click the 'Run.bat' file to start the script. Note: If the console window immediately closes and nothing happens, try the advanced method below.</li>
<li>Accept the windows UAC prompt to run the script as administrator, this is required for the script to function.</li>
<li>A new powershell window will now open, showing the Win11Debloat menu. Select either the default or custom setup to continue.</li>
</ol>
<h3 tabindex="-1" dir="auto">Advanced method</h3>
<p dir="auto">This method gives you the option to run the script with certain parameters to tailor the behaviour of the script to your needs without requiring any user input during runtime, making it quicker and easier to deploy on a large number of systems.</p>
<ol dir="auto">
<li><a href="https://github.com/Raphire/Win11Debloat/archive/master.zip">Download the latest version of the script</a>, and extract the .ZIP file to your desired location.</li>
<li>Open powershell as an administrator.</li>
<li>Enable powershell execution by entering the following command: <code>Set-ExecutionPolicy Unrestricted -Force</code></li>
<li>In powershell, navigate to the directory where the files were extracted. Example: <code>cd c:\\Win11Debloat</code></li>
<li>Enter this into powershell to run the script: <code>.\Win11Debloat.ps1</code></li>
<li>A menu will now open. Select either the default or custom setup to continue.</li>
</ol>
<p dir="auto">To run the script without any user input, simply add parameters at the end, example: <code>.\Win11Debloat.ps1 -RemoveApps -DisableBing</code></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>-RunDefaults</td>
<td>Run the script with windows 10 default settings.</td>
</tr>
<tr>
<td>-RunWin11Defaults</td>
<td>Run the script with windows 11 default settings.</td>
</tr>
<tr>
<td>-RemoveApps</td>
<td>Remove all bloatware apps from <a href="#click-for-list-of-bloat-that-is-removed">this list</a>.</td>
</tr>
<tr>
<td>-DisableTelemetry</td>
<td>Disable telemetry, diagnostic data &amp; targeted ads.</td>
</tr>
<tr>
<td>-DisableBing</td>
<td>Disable bing search, bing AI &amp; cortana in windows search.</td>
</tr>
<tr>
<td>-DisableLockscreenTips</td>
<td>Disable tips &amp; tricks on the lockscreen.</td>
</tr>
<tr>
<td>-DisableSuggestions</td>
<td>Disable tips, tricks and suggestions in the startmenu and settings, and sync provider ads in windows explorer.</td>
</tr>
<tr>
<td>-TaskbarAlignLeft</td>
<td>Align taskbar icons to the left. (Windows 11 only)</td>
</tr>
<tr>
<td>-DisableWidgets</td>
<td>Disable the widget service &amp; hide the widget (news and interests) icon on the taskbar.</td>
</tr>
<tr>
<td>-HideChat</td>
<td>Hide the chat (meet now) icon on the taskbar.</td>
</tr>
<tr>
<td>-HideOnedrive</td>
<td>Hide the onedrive folder in the windows explorer sidepanel.</td>
</tr>
<tr>
<td>-Hide3dObjects</td>
<td>Hide the 3D objects folder under 'This pc' in windows explorer.</td>
</tr>
<tr>
<td>-HideMusic</td>
<td>Hide the music folder under 'This pc' in windows explorer.</td>
</tr>
<tr>
<td>-HideIncludeInLibrary</td>
<td>Hide the 'Include in library' option in the context menu.</td>
</tr>
<tr>
<td>-HideGiveAccessTo</td>
<td>Hide the 'Give access to' option in the context menu.</td>
</tr>
<tr>
<td>-HideShare</td>
<td>Hide the 'Share' option in the context menu.</td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" dir="auto">Debloat Windows</h2>
<p dir="auto">By default, this script removes a large selection preinstalled bloatware, while preserving actually useful apps like the calculator, mail, mediaplayer and photos. If you do end up needing any of the removed apps in the future you can easily reinstall them through the Microsoft store. A full list of what is and isn't removed can be found below, but if you're unhappy with the default selection you can customize exactly which apps are removed by the script by editing the apps list found in the <a href="https://github.com/Raphire/Win11Debloat/blob/master/Win11Debloat.ps1">'Win11Debloat.ps1'</a> file.</p>
<details>
  <summary><h4 tabindex="-1" dir="auto">Click for list of bloat that is removed</h4></summary>
  <blockquote>
<div data-snippet-clipboard-copy-content="Microsoft bloat:
- Microsoft.3DBuilder  
- Microsoft.549981C3F5F10 (Cortana app)
- Microsoft.Asphalt8Airborne  
- Microsoft.BingFinance  
- Microsoft.BingFoodAndDrink 
- Microsoft.BingHealthAndFitness
- Microsoft.BingNews  
- Microsoft.BingSports  
- Microsoft.BingTranslator  
- Microsoft.BingTravel   
- Microsoft.BingWeather  
- Microsoft.GetHelp  
- Microsoft.Getstarted (Cannot be uninstalled in Windows 11)
- Microsoft.Messaging  
- Microsoft.Microsoft3DViewer  
- Microsoft.MicrosoftOfficeHub  
- Microsoft.MicrosoftPowerBIForWindows  
- Microsoft.MicrosoftSolitaireCollection  
- Microsoft.MicrosoftStickyNotes  
- Microsoft.MixedReality.Portal  
- Microsoft.NetworkSpeedTest  
- Microsoft.News  
- Microsoft.Office.OneNote  
- Microsoft.Office.Sway  
- Microsoft.OneConnect  
- Microsoft.Print3D  
- Microsoft.RemoteDesktop  
- Microsoft.SkypeApp  
- Microsoft.Todos  
- Microsoft.WindowsAlarms  
- Microsoft.WindowsFeedbackHub  
- Microsoft.WindowsMaps  
- Microsoft.WindowsSoundRecorder  
- Microsoft.XboxApp (Old Xbox Console Companion App, no longer supported)
- Microsoft.ZuneMusic  
- Microsoft.ZuneVideo  
- MicrosoftTeams

Third party bloat:
- ACGMediaPlayer  
- ActiproSoftwareLLC  
- AdobeSystemsIncorporated.AdobePhotoshopExpress  
- Amazon.com.Amazon  
- Asphalt8Airborne   
- AutodeskSketchBook  
- CaesarsSlotsFreeCasino  
- Clipchamp.Clipchamp  
- COOKINGFEVER  
- CyberLinkMediaSuiteEssentials  
- DisneyMagicKingdoms  
- Dolby  
- DrawboardPDF  
- Duolingo-LearnLanguagesforFree  
- EclipseManager  
- Facebook  
- FarmVille2CountryEscape  
- fitbit  
- Flipboard  
- HiddenCity  
- HULULLC.HULUPLUS  
- iHeartRadio  
- king.com.BubbleWitch3Saga  
- king.com.CandyCrushSaga  
- king.com.CandyCrushSodaSaga  
- LinkedInforWindows  
- MarchofEmpires  
- Netflix  
- NYTCrossword  
- OneCalendar  
- PandoraMediaInc  
- PhototasticCollage  
- PicsArt-PhotoStudio  
- Plex  
- PolarrPhotoEditorAcademicEdition  
- Royal Revolt  
- Shazam  
- Sidia.LiveWallpaper  
- SlingTV  
- Speed Test  
- Spotify  
- TuneInRadio  
- Twitter  
- Viber  
- WinZipUniversal  
- Wunderlist  
- XING"><pre><code>Microsoft bloat:
- Microsoft.3DBuilder  
- Microsoft.549981C3F5F10 (Cortana app)
- Microsoft.Asphalt8Airborne  
- Microsoft.BingFinance  
- Microsoft.BingFoodAndDrink 
- Microsoft.BingHealthAndFitness
- Microsoft.BingNews  
- Microsoft.BingSports  
- Microsoft.BingTranslator  
- Microsoft.BingTravel   
- Microsoft.BingWeather  
- Microsoft.GetHelp  
- Microsoft.Getstarted (Cannot be uninstalled in Windows 11)
- Microsoft.Messaging  
- Microsoft.Microsoft3DViewer  
- Microsoft.MicrosoftOfficeHub  
- Microsoft.MicrosoftPowerBIForWindows  
- Microsoft.MicrosoftSolitaireCollection  
- Microsoft.MicrosoftStickyNotes  
- Microsoft.MixedReality.Portal  
- Microsoft.NetworkSpeedTest  
- Microsoft.News  
- Microsoft.Office.OneNote  
- Microsoft.Office.Sway  
- Microsoft.OneConnect  
- Microsoft.Print3D  
- Microsoft.RemoteDesktop  
- Microsoft.SkypeApp  
- Microsoft.Todos  
- Microsoft.WindowsAlarms  
- Microsoft.WindowsFeedbackHub  
- Microsoft.WindowsMaps  
- Microsoft.WindowsSoundRecorder  
- Microsoft.XboxApp (Old Xbox Console Companion App, no longer supported)
- Microsoft.ZuneMusic  
- Microsoft.ZuneVideo  
- MicrosoftTeams

Third party bloat:
- ACGMediaPlayer  
- ActiproSoftwareLLC  
- AdobeSystemsIncorporated.AdobePhotoshopExpress  
- Amazon.com.Amazon  
- Asphalt8Airborne   
- AutodeskSketchBook  
- CaesarsSlotsFreeCasino  
- Clipchamp.Clipchamp  
- COOKINGFEVER  
- CyberLinkMediaSuiteEssentials  
- DisneyMagicKingdoms  
- Dolby  
- DrawboardPDF  
- Duolingo-LearnLanguagesforFree  
- EclipseManager  
- Facebook  
- FarmVille2CountryEscape  
- fitbit  
- Flipboard  
- HiddenCity  
- HULULLC.HULUPLUS  
- iHeartRadio  
- king.com.BubbleWitch3Saga  
- king.com.CandyCrushSaga  
- king.com.CandyCrushSodaSaga  
- LinkedInforWindows  
- MarchofEmpires  
- Netflix  
- NYTCrossword  
- OneCalendar  
- PandoraMediaInc  
- PhototasticCollage  
- PicsArt-PhotoStudio  
- Plex  
- PolarrPhotoEditorAcademicEdition  
- Royal Revolt  
- Shazam  
- Sidia.LiveWallpaper  
- SlingTV  
- Speed Test  
- Spotify  
- TuneInRadio  
- Twitter  
- Viber  
- WinZipUniversal  
- Wunderlist  
- XING
</code></pre></div>
  </blockquote>
</details>
<details>
  <summary><h4 tabindex="-1" dir="auto">Click for list of what is NOT removed</h4></summary>
  <blockquote>
<div data-snippet-clipboard-copy-content="Required or useful apps for regular desktop usage:
- Microsoft.MSPaint (Paint 3D)
- Microsoft.People (Required with Mail &amp; Calendar)
- Microsoft.ScreenSketch (Snipping Tool)
- Microsoft.Whiteboard (Only preinstalled on devices with touchscreen and/or pen support)
- Microsoft.Windows.Photos
- Microsoft.WindowsCalculator
- Microsoft.WindowsCamera
- Microsoft.windowscommunicationsapps (Mail &amp; Calendar)
- Microsoft.WindowsStore (Microsoft Store, NOTE: This app cannot be reinstalled!)
- Microsoft.WindowsTerminal (New default terminal app in windows 11)
- Microsoft.YourPhone (Phone Link)
- Microsoft.ZuneMusic (Modern Media Player)

Required or useful apps for Microsoft store games:
- Microsoft.GamingApp (Modern Xbox Gaming App, required for installing some games)
- Microsoft.Xbox.TCUI (UI framework, removing this may break microsoft store, photos and certain games)
- Microsoft.XboxGameCallableUI (UI framework, required for some games)
- Microsoft.XboxGameOverlay (Game overlay, required for some games)
- Microsoft.XboxGamingOverlay (Game overlay, required for some games)
- Microsoft.XboxIdentityProvider (Xbox sign-in framework, required for some games)
- Microsoft.XboxSpeechToTextOverlay (Might be required for some games, NOTE: This app cannot be reinstalled!)"><pre><code>Required or useful apps for regular desktop usage:
- Microsoft.MSPaint (Paint 3D)
- Microsoft.People (Required with Mail &amp; Calendar)
- Microsoft.ScreenSketch (Snipping Tool)
- Microsoft.Whiteboard (Only preinstalled on devices with touchscreen and/or pen support)
- Microsoft.Windows.Photos
- Microsoft.WindowsCalculator
- Microsoft.WindowsCamera
- Microsoft.windowscommunicationsapps (Mail &amp; Calendar)
- Microsoft.WindowsStore (Microsoft Store, NOTE: This app cannot be reinstalled!)
- Microsoft.WindowsTerminal (New default terminal app in windows 11)
- Microsoft.YourPhone (Phone Link)
- Microsoft.ZuneMusic (Modern Media Player)

Required or useful apps for Microsoft store games:
- Microsoft.GamingApp (Modern Xbox Gaming App, required for installing some games)
- Microsoft.Xbox.TCUI (UI framework, removing this may break microsoft store, photos and certain games)
- Microsoft.XboxGameCallableUI (UI framework, required for some games)
- Microsoft.XboxGameOverlay (Game overlay, required for some games)
- Microsoft.XboxGamingOverlay (Game overlay, required for some games)
- Microsoft.XboxIdentityProvider (Xbox sign-in framework, required for some games)
- Microsoft.XboxSpeechToTextOverlay (Might be required for some games, NOTE: This app cannot be reinstalled!)
</code></pre></div>
  </blockquote>
</details>
<h2 tabindex="-1" dir="auto">Improve your Windows experience</h2>
<p dir="auto">This script can also make various changes to declutter &amp; improve your overall windows experience, and protect your privacy. Such as:</p>
<ul dir="auto">
<li>Disable telemetry, diagnostic data &amp; targeted ads.</li>
<li>Disable bing search, bing AI &amp; cortana in windows search.</li>
<li>Disable tips &amp; tricks on the lockscreen. (This changes your lockscreen wallpaper to the windows default)</li>
<li>Disable tips, tricks and suggestions in the startmenu and settings, and sync provider ads in windows explorer.</li>
<li>Align taskbar icons to the left. (Windows 11 only)</li>
<li>Disable the widget service &amp; hide the widget (news and interests) icon on the taskbar.</li>
<li>Hide the chat (meet now) icon on the taskbar.</li>
<li>Hide the onedrive folder in the windows explorer sidepanel. (Windows 10 only)</li>
<li>Hide the 3D objects and/or music folders under 'This pc' in windows explorer. (Windows 10 only)</li>
<li>Hide the 'Include in library', 'Give access to' and 'Share' options in the context menu. (Windows 10 only)</li>
</ul>
<p dir="auto">All of these changes can be individually reverted with the registry files that are included in the 'Regfiles' folder.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Harvard professor Francesca Gino was accused of faking data (108 pts)]]></title>
            <link>https://www.businessinsider.com/harvard-francesca-gino-fake-data-fraud-allegations-scholar-2023-7</link>
            <guid>36968670</guid>
            <pubDate>Wed, 02 Aug 2023 09:10:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.businessinsider.com/harvard-francesca-gino-fake-data-fraud-allegations-scholar-2023-7">https://www.businessinsider.com/harvard-francesca-gino-fake-data-fraud-allegations-scholar-2023-7</a>, See on <a href="https://news.ycombinator.com/item?id=36968670">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component-type="content-lock" data-load-strategy="exclude">
                                  <p>To be a true superstar in behavioral science, you need to achieve a few things.</p><p>A TED Talk, obviously. Best-selling books with bright covers filled with pop-science buzzwords like "predictable irrationality" or "expecting better." Thousands upon thousands of followers on Twitter and LinkedIn. Tenure, ideally at a top business school such as Harvard or Wharton.</p><p>It's not enough to just teach anymore. These professors want to build "an empire," Syon Bhanot, an associate professor of economics at Swarthmore College, said.</p><p>Francesca Gino ranks among the elite few who tick all the boxes. From 2019 to 2020, Gino raked in more than $1 million as a professor at Harvard Business School, studying trendy topics such as political correctness and why people lie. One of her lectures was even repackaged as Alaska Airlines in-flight entertainment.&nbsp;</p><p>But now, Gino's empire is crumbling.</p><p>In mid-June, news broke that Harvard had placed Gino on administrative leave after an <a href="https://www.chronicle.com/article/a-weird-research-misconduct-scandal-about-dishonesty-just-got-weirder" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">internal investigation</a> into allegations that she falsified research data. The next day, Data Colada — a blog run by three professors and known for exposing shoddy research — <a href="https://datacolada.org/109" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">published its first of four posts</a> saying it had found evidence of fraud in Gino's work. Data Colada reported that Harvard's still unreleased internal report on Gino was roughly 1,200 pages.</p><p>"We believe that many more Gino-authored papers contain fake data," the Data Colada professors wrote. "Perhaps dozens."</p><p>The allegations could destroy years of research and damage dozens of careers, as Gino has coauthored papers with more than 100 people. Even as Gino's peers condemn her, many say her actions aren't all that surprising in an environment where professors are pitted against one another in a mad dash to publish the next "Freakonomics."&nbsp;</p><p>It's possible "these people don't care and never cared about the science," Michael Sanders, a professor of public policy at King's College London, said. "They talk about science, and they talk about experiments, and they sort of wrap themselves in the fabric of the scientific method — just as a way of selling stuff."</p><p>Gino did not respond to requests for comment. In her last LinkedIn post before going dark in late June, <a href="https://affiliate.insider.com/?h=b06e573befd915ae3654e13fa4cc11c6979e6e1222b0d7f45121d9d339248fdf&amp;postID=64c12b3e75976ff3ff5034ae&amp;site=bi&amp;u=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn%3Ali%3Aactivity%3A7078154339113914368%2F&amp;amazonTrackingID=null&amp;platform=browser&amp;sc=false&amp;disabled=false" target="_blank" rel="noopener" data-analytics-product-module="body_link">she wrote that</a> reports about her research "will be addressed."</p><hr><p>Gino grew up in a tiny Italian mountain town of 3,000 called Tione di Trento. She attended college at the nearby University of Trento before earning her master's and Ph.D. at the Sant'anna School of Advanced Studies in Pisa. While completing her Ph.D. in 2002, Gino went to Harvard as a visiting fellow. She was supposed to stay for six to nine months, but instead, <a href="https://affiliate.insider.com/?h=4e8176ed85bc97d7063c525baec1f6c0a56551f7e2a2dec36a0e41380495cdbf&amp;postID=64c12b3e75976ff3ff5034ae&amp;site=bi&amp;u=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Ffrancescagino_i-grew-up-in-a-very-small-town-in-the-mountains-activity-7044281975578218496-WOLa%2F&amp;amazonTrackingID=null&amp;platform=browser&amp;sc=false&amp;disabled=false" target="_blank" rel="noopener" data-analytics-product-module="body_link">as she wrote earlier this year on LinkedIn</a>, "I never left."</p><p>Gino conducted her postdoctoral research at Harvard and then went on to teaching gigs at Carnegie Mellon University and the University of North Carolina at Chapel Hill. Soon, she was receiving job offers from top universities across the US, including Harvard Business School. By the time she was 32, she was back at HBS as an associate professor and "already this superstar," said a former collaborator who spoke on condition of anonymity to avoid professional repercussions.</p><p>Gino's students at Harvard loved the motorcycle-obsessed <a href="https://hbr.org/2020/06/lessons-from-a-working-mom-on-doing-it-all" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">mother of four</a>. She was <a href="https://poetsandquants.com/2015/04/15/37059/" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">fun</a>, a professor who made pasta from scratch and took improv classes with her husband, Greg Burd, an engineer. (She wrote about the classes in her book "Rebel Talent" as an example of embracing the unexpected.) Gino racked up honors including the HBS faculty award in 2015, a place on Poets&amp;Quants' list of the top 40 business professors under 40 the same year, and a spot on the Thinkers50 list — also known as the "<a href="https://www.forbes.com/sites/ruthgotian/2023/01/24/thinkers50-announces-the-new-radar-list-of-top-emerging-management-thinkers/?sh=3c8359905f6d" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">Oscars of management thinking</a>" — in 2017, 2019, and 2021. People who knew Gino were baffled by how she managed to fit everything into her schedule.</p><p>"She was a bit bionic," her former collaborator said. "There's no way I could have a research conversation if I haven't slept the night. But she could."</p><p>Gino is prolific. She specializes in intuitive but intriguing findings: <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2443674" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">Shaking hands makes people more likely to agree on deals</a>; networking makes people feel so dirty that <a href="https://www.wsj.com/articles/if-networking-makes-you-feel-dirty-youre-doing-it-wrong-11631883600" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">they develop a sudden interest in cleaning products</a>. A 2012 study she coauthored that found signing honesty pledges at the top of documents reduced cheating earned write-ups in <a href="https://www.wired.com/2012/08/signature-honesty/" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">Wired</a> and <a href="https://www.forbes.com/sites/hbsworkingknowledge/2013/04/15/an-easy-trick-to-mitigate-tax-form-cheating/?sh=3fa40db6132f" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">Forbes</a> and persuaded governments <a href="https://www.buzzfeednews.com/article/stephaniemlee/dan-ariely-honesty-study-retraction" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">from the US to Guatemala</a> to update financial forms.</p><blockquote><q>I didn't think it was fraud, but I thought it was bullshit.</q></blockquote><p>Gino easily translates her research for a nonacademic audience, offering personal stories alongside data and historical studies. A single chapter in "Rebel Talent" rattles off anecdotes from Napoleonic battles, a study of a tomato-processing company, and Gino's decision to pair red Converse with a Hugo Boss suit because it <a href="https://www.scientificamerican.com/article/gaining-status-with-red-sneakers/" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">made her students think she was more important</a>. Her work spans disciplines, with Gino coauthoring research on subjects such as <a href="https://www.hbs.edu/faculty/Pages/profile.aspx?facId=271812&amp;view=publications" target="_blank" rel="noopener" data-analytics-product-module="body_link">the dynamics of surgical teams</a> and <a href="https://francescagino.com/s/Sezer-et-al-2018-Humblebragging.pdf" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">humblebragging</a>. Gino has published more than 135 studies, and her citations exceed 32,000, according to Google Scholar. She is so productive, Swarthmore College's Bhanot said, it seems as if everyone in the field personally knows 20 people who've written something with Gino.</p><p>Her coauthors were full of praise. "She was a bright, eager, hardworking collaborator," Maurice Schweitzer, a behavioral-economics professor at Wharton who published eight studies with Gino, said.</p><p>But some felt Gino prioritized speed over substance. "I didn't think it was fraud, but I thought it was bullshit," said Simine Vazire, a professor of psychology, ethics, and well-being at the University of Melbourne. Sanders of King's College London said Gino was highly respected but her findings tended to be "quite cute." "They're sort of designed to be turned into pop books or TED Talks," he added.</p><p>"Cute" studies paid off for Gino. In the 2019-20 academic year, she was one of the highest-compensated employees at Harvard, <a href="https://projects.propublica.org/nonprofits/display_990/42103580/05_2021_prefixes_01-04%2F042103580_202006_990_2021052018155042" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">earning more than $1 million</a>. (Harvard professors on average <a href="https://www.aaup.org/report/annual-report-economic-status-profession-2022-23" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">make $274,900</a>. The minimum salary <a href="https://hr.harvard.edu/salary-ranges" target="_blank" rel="noopener" data-analytics-product-module="body_link">for a research assistant</a> at the university is $48,382, according to the school.) She developed a lucrative side gig giving speeches and leading corporate trainings for companies including Disney and Goldman Sachs. Gino's speaking fees were in the tens of thousands of dollars, with clients paying for her to travel <a href="https://thelavinagency.com/speakers/francesca-gino/" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">across 40 states and 30 countries</a>, according to the Lavin Agency. In "Rebel Talent," Gino recalls flying from Boston to New York at 6:36 a.m. to teach a class of 40 executives about decision-making — then returning on the 5 p.m. flight.</p><p>"We can look back and say, 'Oh, I should have known,'" Schweitzer said. "I don't think that there were obvious signs at the time."</p><p>Schweitzer added: "She has, like, 150 collaborators. I think people were working in good faith, assuming things were right."</p><hr><p>The first cracks appeared in 2020.</p><p>Eight years after publishing her 2012 paper about honesty pledges, Gino and her coauthors published a follow-up saying they'd failed to replicate the results and <a href="https://blogs.scientificamerican.com/observations/when-were-wrong-its-our-responsibility-as-scientists-to-say-so/" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">no longer stood by the original study</a>.</p><p>As part of the follow-up paper, the coauthors posted their 2012 data for the first time, and a group of researchers began digging through the spreadsheets. In August 2021, Data Colada reported that the researchers found "beyond any shadow of a doubt" that the 2012 data had been fabricated. Data Colada's founders — the professors Uri Simonsohn, Joe Simmons, and Leif Nelson — published <a href="https://datacolada.org/98" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">an analysis</a>, based on the researchers' work, focusing on one of the study's three experiments: odometer readings from an auto-insurance company. Data Colada found that the raw data showed clear anomalies, such as a distribution infinitely more likely to be produced by a random-number generator than actual people.</p><p>Only one of the five coauthors had handled the odometer data: Dan Ariely, a Duke University professor who's so well known that <a href="https://variety.com/2022/tv/news/nbc-the-irrational-series-la-brea-writer-arika-lisanne-mittman-ep-alongside-mark-goffman-samuel-baum-1235473150/" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">a coming NBC drama is partly inspired by his career</a>. Ariely acknowledged the data was fake but denied fabricating it himself, implying in a <a href="https://www.buzzfeednews.com/article/stephaniemlee/dan-ariely-honesty-study-retraction" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">2021 interview with BuzzFeed News</a> that the insurance company might have been responsible — <a href="https://www.npr.org/2023/07/27/1190568472/dan-ariely-francesca-gino-harvard-dishonesty-fabricated-data" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">an accusation the insurance company has vigorously denied</a>. The other four coauthors said they played no part in collecting the data. All five, Ariely included, requested the paper be retracted in light of Data Colada's analysis.</p><figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
                          
                          
                          
                            <p><img src="data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg xmlns='http://www.w3.org/2000/svg' width='1' height='1'/%3E" data-content-type="image/jpeg" data-srcs="{&quot;https://i.insider.com/64c12c6d1f17bb00196e7a54&quot;:{&quot;contentType&quot;:&quot;image/jpeg&quot;,&quot;aspectRatioW&quot;:2666,&quot;aspectRatioH&quot;:2000}}" alt="Dan Ariely, a professor of behavioral economics at Duke University, speaks during the 19th Annual Sohn Investment Conference in 2009" itemprop="contentUrl">
                        </p>
                          
                          <span>
                                <figcaption data-e2e-name="image-caption">
                                  Dan Ariely, a Duke University professor who collaborated with Francesca Gino, has become so famous that a coming NBC drama is loosely based on his career.
                                </figcaption>
                                
                          <span data-e2e-name="image-source" itemprop="creditText">
                          
                          Chris Goodney/Bloomberg via Getty Images
                          
                          </span>
                              </span>
                          </figure><p>Privately, the Data Colada professors had further concerns. A few months after dropping the Ariely bombshell, they reached out to Harvard. Gino, they said, appeared to have fabricated data in the 2012 paper, as well as in three additional studies. Harvard launched an internal investigation, and rumors about Gino began to spread. Anonymous posters on the forum Economics Job Market Rumors — <a href="https://www.businessinsider.com/harvard-yale-toxic-posts-ejmr-study-meltdown-2023-7" data-analytics-product-module="body_link" rel="">a sort of 4chan for economists</a> — began speculating about her experiments. "Lots of output, lots of it questionable," one person wrote in 2022. "Anyone notice she never shares her data?" another wrote. "Wtf is going on in b-schools?" a third commented.</p><p>Harvard placed Gino on leave in June this year, The Chronicle of Higher Education <a href="https://www.chronicle.com/article/a-dishonesty-expert-stands-accused-of-fraud-scholars-who-worked-with-her-are-scrambling" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">first reported</a>. (The university declined to comment on Gino, who remained on academic leave as of late July.) Soon after, Data Colada ran an article alleging that Gino tampered with data in at least one of her honesty-pledge experiments. Eight data points appear to be out of order on Gino's spreadsheet, Data Colada said, adding that their placement could be explained only by someone editing the data to make the results appear significant. It said: "Two different people independently faked data for two different studies in a paper about dishonesty."</p><p>Over the next two weeks, Data Colada released an analysis of three additional studies. It said Gino appeared to have falsified data as recently as 2020. For people like Nick Brown, a British data vigilante of sorts, the revelations were exciting. It helped that Brown knew the exposé was overdue, he said — two separate people had reached out to him with concerns about Gino in the past year.</p><p>"I just kind of got out a big box of popcorn," Brown said.</p><p>But for many, the allegations against Gino were upsetting. "It feels like a violation of a lot of the things we all hold dear," Jeff Lees, one of Gino's former advisees at HBS, said.</p><p>Worse, Lees said, "it's sad because it's not hard to imagine why someone does it."</p><p>To get tenure — a lifetime appointment at a university, which is the ultimate goal — you need as much research as you can get published in A-level journals. Journals want studies that break ground and earn breathless media coverage. So imagine the stress when a researcher has committed a year or two of work and spent tens or hundreds of thousands of dollars on a project only for the results to come back as insignificant.</p><p>It's easy, said Lees, to picture someone like Gino saying, "Let me just hit a few buttons and make it look true and we can all be happy."</p><p>Gino conducted her honesty-pledge study at the University of North Carolina at Chapel Hill in 2010. At the time, she was up for a job at HBS.</p><p>Throughout her career, Gino set a bar for productivity. Many researchers publish two to three papers a year; Gino published 16 articles in 2015 alone. Scholars battled to ascend to what Lees called the "1% of academia": tenure, the best-selling books, the corporate talks. Meanwhile, Gino barely seemed to break a sweat.</p><p>"Who doesn't want to get paid to just give a speech? Who doesn't want the public to see their work?" Lees said. "When you think about ascending the prestige hierarchy, that is one of the incentives to potentially commit fraud."</p><hr><p>Gino is far from the first behavioral-science professor to be accused of less-than-ethical data practices. There's Ariely, who shrugged off the 2020 scandal (<a href="https://www.timesofisrael.com/behavioral-researcher-says-he-undoubtedly-made-a-mistake-in-false-data-scandal/" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">among others</a>) and continued teaching at Duke. Brian Wansink's food-psychology studies into obesity and weight loss were so influential that he developed the US Department of Agriculture's 2010 Dietary Guidelines for Americans; in 2018, he was fired from Cornell University after <a href="https://statmodeling.stat.columbia.edu/2017/01/25/statistical-heartburn-attempt-digest-four-pizza-publications-cornell-food-brand-lab/" target="_blank" rel="noopener" data-analytics-product-module="body_link">dozens of inconsistencies</a> were found in his studies. HBS's Amy Cuddy's power-pose paper persuaded millions of women to adopt a Wonder Woman-esque stance before stressful meetings; <a href="https://www.nytimes.com/2017/10/18/magazine/when-the-revolution-came-for-amy-cuddy.html" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">Cuddy left academia</a> in 2017 after other researchers, including Data Colada, accused her of massaging her data.</p><figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
                          
                          
                          
                            <p><img src="data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg xmlns='http://www.w3.org/2000/svg' width='1' height='1'/%3E" data-content-type="image/jpeg" data-srcs="{&quot;https://i.insider.com/64c12d36671ed600191581b7&quot;:{&quot;contentType&quot;:&quot;image/jpeg&quot;,&quot;aspectRatioW&quot;:2000,&quot;aspectRatioH&quot;:1503}}" alt="Allison Williams and Amy Cuddy attend Marie Claire's Power Women Lunch in 2013" itemprop="contentUrl">
                        </p>
                          
                          <span>
                                <figcaption data-e2e-name="image-caption">
                                  Amy Cuddy's research became so well known that she spoke at Marie Claire's "Power Women Lunch" in 2013, where attendees included the actor Allison Williams.
                                </figcaption>
                                
                          <span data-e2e-name="image-source" itemprop="creditText">
                          
                          Astrid Stawiarz/Getty Images for Marie Claire
                          
                          </span>
                              </span>
                          </figure><p>Simonsohn, Simmons, and Nelson founded Data Colada in 2012 out of their shared conviction that common research techniques allowed for journals to publish countless false-positive studies. It was accepted at the time that academics tinkered with their data, adjusting their controls or cherry-picking outliers. Top professors published papers that skeptics saw as immaterial or downright absurd, <a href="https://slate.com/health-and-science/2017/06/daryl-bem-proved-esp-is-real-showed-science-is-broken.html" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">including a study that supposedly proved extrasensory perception was real</a>. For data detectives, the only way to bring objectivity back into science is to call out shoddy papers and demand more data transparency.&nbsp;</p><p>"There are no police, and there's no prosecutors, so we are kind of the vigilantes," Brown said.</p><p>Data vigilantes' efforts have brought a new level of rigor to data collection and analysis. Despite their contributions to the field, however, some remain skeptical. A post on Data Colada or a tweet from Brown is like a bomb going off in the behavioral-science world. When Data Colada published its criticism of Cuddy, "people were sending me emails like I was dying of cancer," <a href="https://www.nytimes.com/2017/10/18/magazine/when-the-revolution-came-for-amy-cuddy.html" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">she told The New York Times in 2017</a>. Many researchers are privately terrified of being falsely accused by the "data cops," one scientist said. But no one wants to criticize them because no one wants a target on their own back — including this individual, who was granted anonymity for this reason.</p><p>"Batman is a vigilante," the scientist said. "So is the Joker."</p><p>The terror arises, in part, because there are no clear guidelines about what, exactly, merits a takedown. Discussing the allegations that Gino falsified data is seen almost unanimously as fair game, especially in light of Harvard putting her on leave. People expressed more mixed feelings toward disgraced researchers such as Cuddy, who failed to replicate her findings but did not fake her data. Some see the mockery of Cuddy, in particular, as representative of a bigger problem: Women and people of color in academia are held to a different standard than white men. Cuddy, who continues to work with corporate clients, is writing a book about bullies based on her experience.</p><p>Even Data Colada's biggest supporters don't want a blog to be the primary stopgap for bad data. Vazire, the University of Melbourne professor, said that data detectives' efforts should be compensated and backed by institutional support. Academics have called for journals to dedicate more space to replicating studies or to make it mandatory for researchers to include their data.</p><p>Some changes are in the works. Journals have adopted <a href="https://journals.sagepub.com/doi/abs/10.1177/0198742920936619?journalCode=bhda#:~:text=In%20results%2Dblind%20peer%20review,or%20magnitude%20of%20the%20findings." target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">policies such as result-blind peer reviews</a>, allowing for more null results to be published. It's becoming more common for researchers to share their datasets. There's no way that the falsified data from the 2012 honesty-pledge study would have been exposed if it hadn't been logged publicly for the first time when the 2020 follow-up was published.</p><p>Even with new safeguards, the pressure to publish clicky, media-friendly studies remains.</p><p>"The honest researchers just get kind of pushed out because they're not willing to manipulate or do studies on a specific subtopic that hits," Swarthmore College's Bhanot said. "To get a lot of Twitter followers, to get a public-facing book that sells a lot, to get a bunch of TED Talks — you have to have buzzy insights that distill nicely into a 60-second sound bite."</p><hr><p>Perhaps the best way to understand why someone would fake data is to turn to Gino's own research. While people outside academia have relished the irony of a "dishonesty expert" being accused of fraud, Gino's peers tend to think her field of study makes perfect sense. When discussing Gino with Insider, multiple people brought up <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8270443/" target="_blank" rel="noopener" data-analytics-product-module="body_link">the idea of "me-search"</a> — that researchers gravitate to topics that are of personal interest to them. "We're our own therapists, in a sense," Gordon Pennycook, a behavioral-science professor, said.</p><p><a href="https://journals.sagepub.com/doi/abs/10.1111/j.1467-9280.2009.02306.x" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">In a study</a> about "<a href="https://www.fuqua.duke.edu/duke-fuqua-insights/ariely-dishonesty" target="_blank" rel="noopener" data-analytics-product-module="body_link">contagious dishonesty</a>" that Gino coauthored with Ariely, the researchers found that students were more likely to cheat if they saw someone they believed to also attend their university cheating. <a href="http://francescagino.com/s/Gino-Pierce-OBHDP-2009.pdf" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">A 2008 paper</a> found people cheated more in the presence of abundant wealth, provoked by feelings of envy. <a href="https://www.jstor.org/stable/24543538" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">A 2014 study found that</a> dishonesty led to greater creativity, saying liars felt unconstrained by rules. (Harvard asked the journal to redact the study in June, citing discrepancies between the published datasets and the earliest-known versions of the data. Gino's counsel told the journal that the retraction was necessary but that "there is no original data available.")</p><p>"You might think that somebody who researches dishonesty is obsessed with the truth," Pennycook added. "But in this case, maybe it's the other way."</p><blockquote><q>You change a couple rows of a spreadsheet, and then for the next 10 years, people are chasing ghosts.</q></blockquote><p>The allegations against Gino have already cast a long shadow over the field. An effort is underway to audit each of Gino's papers, called the "Many Co-authors" project, which hopes to identify a standardized process to deal with any studies found to be fraudulent. Since June, journals have announced plans to retract three of Gino's papers, in addition to the honesty-pledge paper. Sanders said the only thing left to discover was how many times she forged data — just the four times exposed by Data Colada or in "20, 50, 100 cases."</p><p>For collaborators, it's a stressful and infuriating time. When a paper is retracted, the research is erased, and all related citations are lost. For junior scholars, this can mean a significant chunk of their professional achievements evaporating overnight. The anger extends to those whose studies are left unscathed. The collaborator who spoke on condition of anonymity said she was frustrated for years trying to keep up with Gino. Then, when she heard that Gino had been accused of "cheating," she was furious. "You're like, excuse me?"</p><p>Others who attempted to build on Gino's studies are grappling with having wasted time, money, and energy. Both Sanders and Bhanot attempted to apply the honesty-pledge study on a much larger scale: Bhanot surveying <a href="https://works.swarthmore.edu/fac-economics/424/" target="_blank" rel="noopener" data-analytics-product-module="body_link">people borrowing money online</a> and Sanders studying Guatemalan taxpayers. Neither found any influence in having participants sign the pledge. In Sanders' case, the Guatemalan government spent a quarter of a million dollars conducting the study in an experiment that <a href="https://pubmed.ncbi.nlm.nih.gov/28452941/" target="_blank" rel="noopener" data-analytics-product-module="body_link">spanned 627,242 people</a>.</p><p>"You change a couple rows of a spreadsheet, and then for the next 10 years, people are chasing ghosts," Bhanot said.&nbsp;</p><p>Some researchers see Harvard putting Gino on leave as evidence of science prevailing: Researchers raised concerns based on public data, and the university took action. Others see her story as a message of how broken the system is. Gino thrived for well over a decade. But her mistakes were too easy to spot, and her profile was too high for her to go unpunished. Few think she'll be the last one to go down. Sanders said it could be like clockwork waiting to see whom the data detectives brought in next.</p><p>"It's hard to know how big the iceberg is underneath the surface of water," Bhanot said. "You just see the few cases of the sloppiest folks, or the biggest names, or the most famous people fall the hardest."</p><p>"But how many people," he added, simply "hide their tracks better?"</p>
                      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VkFFT: Vulkan/CUDA/Hip/OpenCL/Level Zero/Metal Fast Fourier Transform Library (122 pts)]]></title>
            <link>https://github.com/DTolm/VkFFT</link>
            <guid>36968273</guid>
            <pubDate>Wed, 02 Aug 2023 08:06:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/DTolm/VkFFT">https://github.com/DTolm/VkFFT</a>, See on <a href="https://news.ycombinator.com/item?id=36968273">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">VkFFT - Vulkan/CUDA/HIP/OpenCL/Level Zero/Metal Fast Fourier Transform library</h2>
<p dir="auto">VkFFT is an efficient GPU-accelerated multidimensional Fast Fourier Transform library for Vulkan/CUDA/HIP/OpenCL/Level Zero/Metal projects. VkFFT aims to provide the community with an open-source alternative to Nvidia's cuFFT library while achieving better performance. VkFFT is written in C language and supports Vulkan, CUDA, HIP, OpenCL, Level Zero and Metal as backends.</p>
<h2 tabindex="-1" dir="auto">The white paper of VkFFT is out - if you use VkFFT, you can cite it: <a href="https://ieeexplore.ieee.org/document/10036080" rel="nofollow">https://ieeexplore.ieee.org/document/10036080</a></h2>
<h2 tabindex="-1" dir="auto">Currently supported features:</h2>
<ul dir="auto">
<li>1D/2D/3D/ND systems - specify VKFFT_MAX_FFT_DIMENSIONS for arbitrary number of dimensions.</li>
<li>Forward and inverse directions of FFT.</li>
<li>Support for big FFT dimension sizes. Current limits: C2C or even C2R/R2C - (2^32, 2^32, 2^32).  Odd C2R/R2C - (2^12, 2^32, 2^32). R2R - (2^12, 2^12, 2^12). Depends on the amount of shared memory on the device. (will be increased later).</li>
<li>Radix-2/3/4/5/7/8/11/13 FFT. Sequences using radix 3, 5, 7, 11 and 13 have comparable performance to that of powers of 2.</li>
<li>Rader's FFT algorithm for primes from 17 up to max shared memory length (~10000). Inlined and done without additional memory transfers.</li>
<li>Bluestein's FFT algorithm for all other sequences. Full coverage of C2C range, single upload (2^12, 2^12, 2^12) for R2C/C2R/R2R. Optimized to have as few memory transfers as possible by using zero padding and merged convolution support of VkFFT.</li>
<li>Single, double and half precision support. Double precision uses CPU-generated LUT tables. Half precision still does all computations in single and only uses half precision to store data.</li>
<li>All transformations are performed in-place with no performance loss. Out-of-place transforms are supported by selecting different input/output buffers.</li>
<li>No additional transposition uploads. Note: Data can be reshuffled after the Four Step FFT algorithm with an additional buffer (for big sequences). Doesn't matter for convolutions - they return to the input ordering (saves memory).</li>
<li>Complex to complex (C2C), real to complex (R2C), complex to real (C2R) transformations and real to real (R2R) Discrete Cosine Transformations of types I, II, III and IV. R2R, R2C and C2R are optimized to run up to 2x times faster than C2C and take 2x less memory.</li>
<li>1x1, 2x2, 3x3 convolutions with symmetric or nonsymmetric kernel (no register overutilization).</li>
<li>Native zero padding to model open systems (up to 2x faster than simply padding input array with zeros). Can specify the range of sequences filled with zeros and the direction where zero padding is applied (read or write stage).</li>
<li>WHD+CN layout - data is stored in the following order (sorted by increase in strides): the width, the height, the depth, other dimensions, the coordinate (the number of feature maps), the batch number.</li>
<li>Multiple feature/batch convolutions - one input, multiple kernels.</li>
<li>Multiple input/output/temporary buffer split. Allows using data split between different memory allocations and mitigates 4GB single allocation limit.</li>
<li>Works on Nvidia, AMD, Intel and Apple GPUs. And Raspberry Pi 4 GPU.</li>
<li>Works on Windows, Linux and macOS.</li>
<li>VkFFT supports Vulkan, CUDA, HIP, OpenCL, Level Zero and Metal as backend to cover wide range of APIs.</li>
<li>Header-only library, which allows appending VkFFT directly to user's command buffer. Kernels are compiled at run-time.</li>
</ul>
<h2 tabindex="-1" dir="auto">Future release plan</h2>
<ul dir="auto">
<li>
<h5 tabindex="-1" dir="auto">Ambitious</h5>
<ul dir="auto">
<li>Multiple GPU job splitting</li>
</ul>
</li>
</ul>
<h2 tabindex="-1" dir="auto">Installation</h2>
<p dir="auto">Vulkan version:
Include the vkFFT.h file and glslang compiler. Provide the library with correctly chosen VKFFT_BACKEND definition (VKFFT_BACKEND=0 for Vulkan). Sample CMakeLists.txt file configures project based on Vulkan_FFT.cpp file, which contains examples on how to use VkFFT to perform FFT, iFFT and convolution calculations, use zero padding, multiple feature/batch convolutions, C2C FFTs of big systems, R2C/C2R transforms, R2R DCT-I, II, III and IV, double precision FFTs, half precision FFTs.<br>
For single and double precision, Vulkan 1.0 is required. For half precision, Vulkan 1.1 is required.</p>
<p dir="auto">CUDA/HIP:
Include the vkFFT.h file and make sure your system has NVRTC/HIPRTC built. Provide the library with correctly chosen VKFFT_BACKEND definition. Only single/double precision for now.<br>
To build CUDA/HIP version of the benchmark, replace VKFFT_BACKEND in CMakeLists (line 5) with the correct one and optionally enable FFTW. VKFFT_BACKEND=1 for CUDA, VKFFT_BACKEND=2 for HIP.</p>
<p dir="auto">OpenCL:
Include the vkFFT.h file. Provide the library with correctly chosen VKFFT_BACKEND definition. Only single/double precision for now.<br>
To build OpenCL version of the benchmark, replace VKFFT_BACKEND in CMakeLists (line 5) with the value 3 and optionally enable FFTW.</p>
<p dir="auto">Level Zero:
Include the vkFFT.h file. Provide the library with correctly chosen VKFFT_BACKEND definition. Clang and llvm-spirv must be valid system calls. Only single/double precision for now.<br>
To build Level Zero version of the benchmark, replace VKFFT_BACKEND in CMakeLists (line 5) with the value 4 and optionally enable FFTW.</p>
<p dir="auto">Metal:
Include the vkFFT.h file. Provide the library with correctly chosen VKFFT_BACKEND definition. VkFFT uses metal-cpp as a C++ bindings to Apple's libraries - Foundation.hpp, QuartzCore.hpp and Metal.hpp. Only single precision.<br>
To build Metal version of the benchmark, replace VKFFT_BACKEND in CMakeLists (line 5) with the value 5 and optionally enable FFTW.</p>
<h2 tabindex="-1" dir="auto">Command-line interface</h2>
<p dir="auto">VkFFT has a command-line interface with the following set of commands:<br>
-h: print help<br>
-devices: print the list of available GPU devices<br>
-d X: select GPU device (default 0)<br>
-o NAME: specify output file path<br>
-vkfft X: launch VkFFT sample X (0-17, 100, 101, 200, 201, 1000-1003) (if FFTW is enabled in CMakeLists.txt)<br>
-cufft X: launch cuFFT sample X (0-4, 1000-1003) (if enabled in CMakeLists.txt)<br>
-rocfft X: launch rocFFT sample X (0-4, 1000-1003) (if enabled in CMakeLists.txt)<br>
-test: (or no other keys) launch all VkFFT and cuFFT benchmarks<br>
So, the command to launch single precision benchmark of VkFFT and cuFFT and save log to output.txt file on device 0 will look like this on Windows:<br>
.\VkFFT_TestSuite.exe -d 0 -o output.txt -vkfft 0 -cufft 0<br>
For double precision benchmark, replace -vkfft 0 -cufft 0 with -vkfft 1 -cufft 1. For half precision benchmark, replace -vkfft 0 -cufft 0 with -vkfft 2 -cufft 2.</p>
<h2 tabindex="-1" dir="auto">How to use VkFFT</h2>
<p dir="auto">VkFFT.h is a library that can append FFT, iFFT or convolution calculation to the user-defined command buffer. It operates on storage buffers allocated by the user and doesn't require any additional memory by itself (except for LUT, if they are enabled). All computations are fully based on Vulkan compute shaders with no CPU usage except for FFT planning. VkFFT creates and optimizes memory layout by itself and performs FFT with the best-chosen parameters. For an example application, see VkFFT_TestSuite.cpp file, which has comments explaining the VkFFT configuration process.<br>
VkFFT achieves striding by grouping nearby FFTs instead of transpositions. <br>
Explicit VkFFT documentation can be found in the documentation folder.</p>
<h2 tabindex="-1" dir="auto">Benchmark results in comparison to cuFFT</h2>
<p dir="auto">The test configuration below takes multiple 1D FFTs of all lengths from the range of 2 to 4096, batch them together so the full system takes from 500MB to 1GB of data and perform multiple consecutive FFTs/iFFTs (-vkfft 1001 key). After that time per a single FFT is obtained by averaging the result.   Total system size will be divided by the time taken by a single transform upload+download, resulting in the estimation of an achieved global bandwidth. The GPUs used in this comparison are Nvidia A100 and AMD MI250. The performance was compared against Nvidia cuFFT (CUDA 11.7 version) and AMD rocFFT (ROCm 5.2 version) libraries in double precision:
<a target="_blank" rel="noopener noreferrer" href="https://github.com/DTolm/VkFFT/blob/master/benchmark_plot/fp64_cuda_a100.png?raw=true"><img src="https://github.com/DTolm/VkFFT/raw/master/benchmark_plot/fp64_cuda_a100.png?raw=true" alt="alt text"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/DTolm/VkFFT/blob/master/benchmark_plot/fp64_hip_mi250.png?raw=true"><img src="https://github.com/DTolm/VkFFT/raw/master/benchmark_plot/fp64_hip_mi250.png?raw=true" alt="alt text"></a></p>
<h2 tabindex="-1" dir="auto">Precision comparison of cuFFT/VkFFT/FFTW</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/DTolm/VkFFT/blob/master/precision_results/FP64_precision.png?raw=true"><img src="https://github.com/DTolm/VkFFT/raw/master/precision_results/FP64_precision.png?raw=true" alt="alt text"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/DTolm/VkFFT/blob/master/precision_results/FP32_precision.png?raw=true"><img src="https://github.com/DTolm/VkFFT/raw/master/precision_results/FP32_precision.png?raw=true" alt="alt text"></a></p>
<p dir="auto">Above, VkFFT precision is verified by comparing its results with FP128 version of FFTW. We test all FFT lengths from the [2, 100000] range. We perform tests in single and double precision on random input data from [-1;1] range.</p>
<p dir="auto">For both precisions, all tested libraries exhibit logarithmic error scaling. The main source of error is imprecise twiddle factor computation – sines and cosines used by FFT algorithms. For FP64 they are calculated on the CPU either in FP128 or in FP64 and stored in the lookup tables. With FP128 precomputation (left) VkFFT is more precise than cuFFT and rocFFT.</p>
<p dir="auto">For FP32, twiddle factors can be calculated on-the-fly in FP32 or precomputed in FP64/FP32. With FP32 twiddle factors (right) VkFFT is slightly less precise in Bluestein’s and Rader’s algorithms. If needed, this can be solved with FP64 precomputation.</p>
<h2 tabindex="-1" dir="auto">VkFFT - a story of Vulkan Compute GPU HPC library development: <a href="https://youtu.be/FQuJJ0m-my0" rel="nofollow">https://youtu.be/FQuJJ0m-my0</a></h2>
<h2 tabindex="-1" dir="auto">VkFFT and beyond – a platform for runtime GPU code generation: <a href="https://youtu.be/lHlFPqlOezo" rel="nofollow">https://youtu.be/lHlFPqlOezo</a></h2>
<h2 tabindex="-1" dir="auto">Check out my poster at SC22: <a href="https://sc22.supercomputing.org/presentation/?id=rpost143&amp;sess=sess273" rel="nofollow">https://sc22.supercomputing.org/presentation/?id=rpost143&amp;sess=sess273</a></h2>
<h2 tabindex="-1" dir="auto">Check out my panel at Nvidia's GTC 2021 in Higher Education and Research category: <a href="https://gtc21.event.nvidia.com/" rel="nofollow">https://gtc21.event.nvidia.com/</a></h2>
<h2 tabindex="-1" dir="auto">Python interface to VkFFT can be found here: <a href="https://github.com/vincefn/pyvkfft">https://github.com/vincefn/pyvkfft</a></h2>
<h2 tabindex="-1" dir="auto">Rust bindings to VkFFT can be found here: <a href="https://github.com/semio-ai/vkfft-rs">https://github.com/semio-ai/vkfft-rs</a></h2>
<h2 tabindex="-1" dir="auto">Benchmark results of VkFFT can be found here: <a href="https://openbenchmarking.org/test/pts/vkfft" rel="nofollow">https://openbenchmarking.org/test/pts/vkfft</a></h2>
<h2 tabindex="-1" dir="auto">Contact information</h2>
<p dir="auto">The initial version of VkFFT is developed by Tolmachev Dmitrii<br>
E-mail 1: <a href="mailto:dtolm96@gmail.com">dtolm96@gmail.com</a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Beetle That Heads for the ‘Back Door’ When Eaten by a Frog (172 pts)]]></title>
            <link>https://www.smithsonianmag.com/smart-news/when-beetle-gets-eaten-frog-it-forces-its-way-out-back-door-180975484/</link>
            <guid>36967829</guid>
            <pubDate>Wed, 02 Aug 2023 06:52:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.smithsonianmag.com/smart-news/when-beetle-gets-eaten-frog-it-forces-its-way-out-back-door-180975484/">https://www.smithsonianmag.com/smart-news/when-beetle-gets-eaten-frog-it-forces-its-way-out-back-door-180975484/</a>, See on <a href="https://news.ycombinator.com/item?id=36967829">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-article-body="">
        
          <figure>
            <img src="https://th-thumbnailer.cdn-si-edu.com/Ez9h4ct19tiX8a3s3crCpKHeDDM=/1000x750/filters:no_upscale()/https://tf-cmsv2-smithsonianmag-media.s3.amazonaws.com/filer/5f/b2/5fb2bcfd-8435-44dc-ac2f-549ee0dd98ba/escape_plan.jpg" alt="A diagram with a hypothetical escape route of the water beetle, shown traveling through the frog's internal organs" itemprop="image">
            <figcaption>
              
                A hypothetical escape route for R. attenuata
              <span>Shinji Sugiura, Kobe University</span>
            </figcaption>
          </figure>
        

        

        <p>In a <a href="https://www.youtube.com/watch?v=qbefo_vUzog">video</a> taken by ecologist Shinji Sugiura, a tiny aquatic beetle known as <em>Regimbartia attenuata</em> pulls off a death-defying feat to rival <a href="https://en.wikipedia.org/wiki/Harry_Houdini">Houdini</a>.</p>

<p>First, a frog snags the beetle and gulps it down whole. For a tense 115 minutes, nothing happens. Then, the great reveal: the same shiny insect wiggles its way out of the amphibian’s anus, leaving both frog and beetle alive and seemingly no worse for the wear.</p>

<p>Sugiura, a researcher with Kobe University, tells <em><a href="https://www.wired.com/story/frog-eats-beetle-beetle-crawls-through-guts/">Wired</a></em>’s Matt Simon that he had been planning to study the predator-prey relationship between <em>R. attenuata</em> and the frog because they share a habitat in Japan’s rice paddy fields.</p>

<p>“However, I did not predict that <em>R. attenuata</em> can escape from the frog vent,” Sugiura tells <em>Wired</em>. “I simply provided the beetle to the frogs, expecting that the frogs spat them out in response to the beetles’ behavior or something.”</p>



    
        
  
    <div>
            
              
                
                  <div>
                    <figure>
                      <img src="https://th-thumbnailer.cdn-si-edu.com/obejCX499l5_v3JyreuC7bYmnIM=/fit-in/1072x0/https://tf-cmsv2-smithsonianmag-media.s3.amazonaws.com/filer/c2/50/c250e5e0-e7a0-4196-94dd-52657c0c4554/escape.jpg" alt="">

                      <div>
                        <p><span></span> / <span></span></p>

                        
                      </div>
                      
                      <figcaption>
                        A R. attenuata beetle, top left, is able to escape from the bowels of P. nigromaculatus, top right. Below, a series of screen grabs show the beetle emerging from the frog's rear end.
                        <span>Shinji Sugiura, Kobe University</span>
                      </figcaption>
                      
                    </figure>
                  </div>
                
              
                
                  <div>
                    <figure>
                      <img src="https://th-thumbnailer.cdn-si-edu.com/EwrXZvd8ZOSbcsg5J2EehtkHhgA=/fit-in/1072x0/https://tf-cmsv2-smithsonianmag-media.s3.amazonaws.com/filer/66/a1/66a1ff02-2a2e-443e-ba4a-286942e189f5/success_rate.jpg" alt="">

                      <div>
                        <p><span></span> / <span></span></p>

                        
                      </div>
                      
                      <figcaption>
                        A graph depicts the success rates of R. attenuata escaping various frogs' intestines.
                        <span>Shinji Sugiura, Kobe University</span>
                      </figcaption>
                      
                    </figure>
                  </div>
                
              
            
          </div>
  

    

<p>According to a Kobe University <a href="https://www.kobe-u.ac.jp/research_at_kobe_en/NEWS/news/2020_08_04_01.html">statement</a>, this study marks the first time that researchers have witnessed prey quickly and actively escape the body of its predator after being eaten. Sugiura published his findings Monday in the journal <em><a href="https://www.cell.com/current-biology/fulltext/S0960-9822(20)30842-3?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0960982220308423%3Fshowall%3Dtrue">Current Biology</a></em>.</p>

<p>Sugiura first tested <em>R. attenuata</em>’s escape techniques with the frog <em>Pelophylax nigromaculatus</em> and found that a whopping 93.3 percent of the beetles were able to escape via the frog’s “vent,” or anus. He found that the beetles had similarly high success rates with four other frog species.</p>

<p>As Katherine J. Wu reports for the <em><a href="https://www.nytimes.com/2020/08/03/science/beetle-frog-poop.html">New York Times</a>, </em>the small beetles—iridescent black insects no longer than four or five millimeters across—were able to make the trip in a minimum time of six minutes. (The longest journey took about four hours, per <em>Wired</em>.) The beetles emerged out the other end covered in feces, but otherwise active and seemingly healthy.</p>

<p>Sugiura hypothesizes that the beetle may evolved this capability as an anti-frog defense tactic. When swallowed, other similar water beetles were killed and digested by the frog, reports Kristen Rogers for <a href="https://www.cnn.com/2020/08/03/world/water-beetle-escape-from-predator-scn/index.html">CNN</a>.</p>

<p>The beetle has to make its way through several inches of inner organs, including an esophagus, stomach, small intestine and large intestine, per CNN. The digestive juices make for a deadly environment, so speed is imperative.</p>

<p>Because some beetles were able to complete the harrowing trip in six minutes, Sugiura concluded that the beetle was actively moving through the frog’s insides, rather than being passively transported. He tested this theory by immobilizing some of the water beetles’ legs, which are used to swim, with a sticky wax. None of the immobilized beetles survived, but rather were digested and excreted the usual way, per the <em>New York Times</em>.</p>
<figure>
    <p data-type="video">
        

  
  <iframe width="854" src="https://www.youtube.com/embed/qbefo_vUzog" frameborder="0" allowfullscreen=""></iframe>


    </p>
    
</figure>
<p>“That was smoking gun evidence that they are using their legs,” Nora Moskowitz, an ecologist at Stanford University who studies frog digestion and who was not involved in the study, tells the <em>Times</em>.</p>

<p>Sugiura suspects that the beetles also use their legs to stimulate the frog’s cloacal sphincter, causing it to defecate. However, he’ll need to run more tests to be sure, reports <em>Wired</em>.</p>

<p>Sugiura has seen some other gnarly beetle escapes in his time: In 2018, he recorded <a href="https://www.smithsonianmag.com/smart-news/how-exploding-beetles-cause-japanese-toads-vomit-180968095/">bombardier beetles spraying a toxic chemical cocktail while inside a toad</a>, which forced the amphibian to vomit the beetle back out alive.</p>

<p>“Frogs are voracious predators, forming an irreplaceable role in food webs and most ecosystems,” biologist Jodi Rowley, who was not involved in the research, tells <em>Wired</em>. “It would be interesting to see if the frogs avoided eating these beetles in the wild, or if they continue to consume them, with the occasional beetle that fails to pull off their escape making it all worthwhile.”</p>

<p>The frog doesn’t seem to be bothered by the little beetle’s journey through its insides, Sugiura points out. “However, I do not want to eat this beetle if I’m a frog,” he tells the <em>Times</em>.</p>


        

        

        
          
  <div>
      <p>Get the latest stories in your inbox every weekday.</p>
      
    </div>


        

        

        
          


  
    
      
    
  

  


        

         
        

        <section>
          <p>Recommended Videos</p>
          
          
          
        </section>

        
          
            <section>
              <nav>Filed Under: 
                
                  
                    <a href="https://www.smithsonianmag.com/tag/animals/">Animals</a>, 
                  
                
                  
                    <a href="https://www.smithsonianmag.com/tag/baby-animals/">Baby Animals</a>, 
                  
                
                  
                    <a href="https://www.smithsonianmag.com/tag/cool-finds/">Cool Finds</a>, 
                  
                
                  
                    <a href="https://www.smithsonianmag.com/tag/frogs/">Frogs</a>, 
                  
                
                  
                    <a href="https://www.smithsonianmag.com/tag/insects/">Insects</a>, 
                  
                
                  
                    <a href="https://www.smithsonianmag.com/tag/weird-animals/">Weird Animals</a>
                  
                
              </nav>
            </section>
          
        

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stopping at 90% (184 pts)]]></title>
            <link>https://austinhenley.com/blog/90percent.html</link>
            <guid>36967594</guid>
            <pubDate>Wed, 02 Aug 2023 06:16:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://austinhenley.com/blog/90percent.html">https://austinhenley.com/blog/90percent.html</a>, See on <a href="https://news.ycombinator.com/item?id=36967594">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <div>
				  <h2>Austin Z. Henley</h2>
				  <p>I work on AI + dev tools.</p>
				</div>

	  <hr>
	  
    
	  <hr>

	
	<small>7/31/2023</small><p>

    <img src="https://austinhenley.com/blog/images/stoppedfinishline.png" alt="Stick figure relaxing with their legs propped up on a desk in front of a finish line."></p><p>I just finished a project.</p>

<p>We spent 4 months building a software system, running experiments, writing an academic paper, and submitting it to a journal for publication. So now we are done. The end. Right?</p>

<p>No! This is what I call <b>stopping at 90%</b>.</p>

<p>The core project might be complete, but there's still plenty to do. If no one knows about it or won't give it a chance, then it's as if it never happened. It's a <b>false finish line</b>.</p>

<p>But it isn't specific to research papers. It can happen to any project: the iOS app you released, the personal repo that you put on GitHub, that report your boss asked you to do, your side business of painting cute puppies, etc.</p>

<p>I see this problem <i>everywhere</i> and no one seems immune.</p>

<p>It is common to stop at 90% since the core project has concrete deliverables that can be easily measured (e.g., did we improve the performance of the system? Did we submit the paper?) where as the remaining 10% is far more difficult to track and often has no clear stopping point.</p>

<p>What are some common activities to go from 90% to 100%?</p>

<ul>
    <li>Present the work to other teams.</li>
    <li>Broadcast an email with the takeaways so that the rest of your organization knows about it.</li>
    <li>Put the code somewhere that your coworkers can make use of later.</li>
    <li>Write a blog post about it. Post it on Twitter, HN, and Reddit.</li>
    <li>Sketch out a next-steps document, even if you have no plans to continue, that explains what you would do next and why.</li>
    <li>Look for adjacent projects that could benefit.</li>
    <li>Find someone that can poke holes in your work, then go address them.</li>
</ul> 

<p>Evangelism, documentation, and polish are often just as important as the core project.</p>

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reading SEC filings using LLMs (103 pts)]]></title>
            <link>https://www.beatandraise.com/</link>
            <guid>36967205</guid>
            <pubDate>Wed, 02 Aug 2023 05:17:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.beatandraise.com/">https://www.beatandraise.com/</a>, See on <a href="https://news.ycombinator.com/item?id=36967205">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Stripe is no longer a suitable payment processor (191 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=36967159</link>
            <guid>36967159</guid>
            <pubDate>Wed, 02 Aug 2023 05:06:18 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=36967159">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="36967316"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36967316" href="https://news.ycombinator.com/vote?id=36967316&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><br><div>
                  <p><span>&gt; This is enshittification: Surpluses are first directed to users; then, once they're locked in, surpluses go to suppliers; then once they're locked in, the surplus is handed to shareholders and the platform becomes a useless pile of shit</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36967649"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36967649" href="https://news.ycombinator.com/vote?id=36967649&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><br><div>
                  <p><span>Its getting a bit boring to hear the same one word dismissal for everything over the last year. When HN learns a new word you really have to be patient.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36967689"><td></td></tr>
                <tr id="36967779"><td></td></tr>
                  <tr id="36967807"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36967807" href="https://news.ycombinator.com/vote?id=36967807&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><p><span>&gt; When HN learns a new word you really have to be patient.<p>The "new word" has been coined because there's just not so much cheap money around, and there's been a whole lot of companies that have been pressured to monetize in ways that may not be good for everyone's long term.</p><p>The word's in the zeitgeist because the phenomenon it is describing is running rampant.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="36967603"><td></td></tr>
                <tr id="36967775"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36967775" href="https://news.ycombinator.com/vote?id=36967775&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><br><div>
                  <p><span>I would not want to deal with any company where the only working method of resolving problems is talking to the CEO.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36967522"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36967522" href="https://news.ycombinator.com/vote?id=36967522&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><p><span>Do you actuality have a particular issue to raise though? Why was your account shut down?<p>&gt; when you make a decent amount of sales they will shut down your account</p><p>I mean they do give you some explanation, right? It might not be super clear (it sometimes can't due to laws) but it's not "you are now processing much volume, goodbye". That's just not in Stripe's interest at all and assuming that high volume alone is the reason is not helpful.</p><p>What happens is likely that only after a certain amount of volume review processes get kicked off. So it's perceived as some form of injustice when often the issue is fundamentally with the business or failure to comply with requirements, and other PSPs might not even let you use them in the first place.</p><p>Personally I think there should maybe be earlier vetting to avoid this (as it's bad UX for those users) but it does come at the cost of having bad initial get-started UX for the vast majority of users that can be supported.</p><p>Those won't go and complain anywhere. So really, just stating "Stripe shut down our account, bad Stripe" isn't helping anyone and it isn't helping you either because nobody will be able to give you advice.</p><p>Their support getting worse I understand. I do think there are cases that could be turned around without having to resort to social media.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36967600"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36967600" href="https://news.ycombinator.com/vote?id=36967600&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><p><span><i>&gt; I mean they do give you some explanation, right?</i><p>What makes you think that?</p><p>Financial institutions are pretty notorious for refusing to explain or justify their 'fraud' detection and suchlike.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36967721"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36967721" href="https://news.ycombinator.com/vote?id=36967721&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><br><div>
                  <p><span>Not sure if it made into the final text. But some drafts of the GDPR have provisions for rights for explaination</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36967690"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36967690" href="https://news.ycombinator.com/vote?id=36967690&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><br><div>
                  <p><span>To avoid the detection rules being circumvented by users, they won't give you any explanation, only a generalized reason. It's almost impossible for you to find out what the problem is.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36967552"><td></td></tr>
            <tr id="36967466"><td></td></tr>
                <tr id="36967710"><td></td></tr>
                <tr id="36967742"><td></td></tr>
                  <tr id="36967747"><td></td></tr>
                  <tr id="36967793"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36967793" href="https://news.ycombinator.com/vote?id=36967793&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><br><div>
                  <p><span>One thing that surprised me the most and made Stripe unusable was that it doesn't handle VAT liabilities. This means you have to manage the VAT for multiple countries yourself (and individual US states).</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36967825"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36967825" href="https://news.ycombinator.com/vote?id=36967825&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><br><div>
                  <p><span>Is there any payment processor that does handle VAT? How can they do that when they do not have information required?</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36967818"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36967818" href="https://news.ycombinator.com/vote?id=36967818&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><br><div>
                  <p><span>As a side comment, it’s unfortunate to see the hatred on this site towards those building decentralized permissionless payment networks. I don’t think some company should be able to decide whether Americans can or cannot transact.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36967259"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36967259" href="https://news.ycombinator.com/vote?id=36967259&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><br><div>
                  <p><span>What's the best alternative? Currently working on an MVP and the horror stories have really put me off Stripe.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36967626"><td></td></tr>
                <tr id="36967711"><td></td></tr>
            <tr id="36967646"><td></td></tr>
                <tr id="36967654"><td></td></tr>
                        <tr id="36967696"><td></td></tr>
            <tr id="36967527"><td></td></tr>
            <tr id="36967488"><td></td></tr>
                <tr id="36967725"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36967725" href="https://news.ycombinator.com/vote?id=36967725&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><p><span>I second LemonSqueezy. They're a bit new, but are going in the right direction.<p>Stripe isn't available in my country anyway.</p><p>PayPal's dev experience was quite bad, but trying to do business with them was virtually impossible. I wasted a lot of time from both the dev and business angles.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="36967245"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36967245" href="https://news.ycombinator.com/vote?id=36967245&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><p><span>Amen. Been saying this for years!<p>Currently moving one large and one small business off Stripe for these exact reasons.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36967624"><td></td></tr>
                  <tr id="36967634"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36967634" href="https://news.ycombinator.com/vote?id=36967634&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><br><div>
                  <p><span>Interesting was thinking to use stripe with some startup ideas, perhaps not then.  Will certainly need to give this one more research.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36967664"><td></td></tr>
            <tr id="36967686"><td></td></tr>
                <tr id="36967791"><td></td></tr>
                <tr id="36967830"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36967830" href="https://news.ycombinator.com/vote?id=36967830&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><br><div>
                  <p><span>There are <i>plenty</i> of horror stories of Paypal holding funds.  They've been doing it for far longer than most these other companies, since they've been around longer than most of them.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="36967332"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36967332" href="https://news.ycombinator.com/vote?id=36967332&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><br><div>
                  <p><span>I've been using it for 2 years (small, France based businesses) and never had any issue – though I've also heard some horror stories.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36967225"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36967225" href="https://news.ycombinator.com/vote?id=36967225&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><p><span>This.<p>It is only going to get worse and it will be no better than PayPal. Just noticed a rise in complaints about Stripe on this site.</p><p>We’ll see how far this Stripe customer support and complaints center goes on HN.</p><p>Seems like the priority is clearly greed over its own users in Stripe.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36967496"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36967496" href="https://news.ycombinator.com/vote?id=36967496&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><br><div>
                  <p><span>I can't even imagine running a physical item delivery if for some reason Stripe decides to just refund the customer if they claim chargeback for something which hasn't even been delivered</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36967745"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36967745" href="https://news.ycombinator.com/vote?id=36967745&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><p><span>That's not a Stripe thing, it a Credit Card law thing. All card processors are the same.<p>If someone pays with a card, then disputes that they did not receive the service or product, the burden is on the vendor to prove they did. It's not the card processor who makes the final call either, it's the customers card company - it's out of Stripes hands.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36967797"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36967797" href="https://news.ycombinator.com/vote?id=36967797&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><br><div>
                  <p><span>I believe the burden of proof under US law is on the credit card issuer, and they abuse their dominant market position.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="36967457"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36967457" href="https://news.ycombinator.com/vote?id=36967457&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><p><span>“do things that don’t scale” is just a pleasant way of describing enshittification.<p>ie that things that make ur product good won’t scale
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36967256"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36967256" href="https://news.ycombinator.com/vote?id=36967256&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><p><span>What is the alternative to these large corporations? I will try to say it again… Web3. Easy to integrate as an option, start accepting payments worldwid, and you can never get deplatformed and your funds won’t be frozen. Why not?<p>Over and over I see HN complain about enshittification of the latest large platform, but also somehow many people knee-jerk downvote anything about the decentralized alternatives because they don’t want them to catch on. So then… enjoy the enshittification I guess?</p><p>“Something something we have to vote for our government to do something”. I think we can just build open source decentralized alternatives to all this stuff.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36967293"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36967293" href="https://news.ycombinator.com/vote?id=36967293&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><br><div>
                  <p><span>My parents, which represent an average person nowadays, do know how to put card data into a site to pay something. What they don't know is how to pay with crypto, even more- for them cryptocoins= automatic scam. Web3 can be a nieche solution for some nieche ppl, but I don't see how it can become mainstream now, as a business you anyway need a classic payment processor bc too few ppl know what is crypto and how to use it</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36967730"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36967730" href="https://news.ycombinator.com/vote?id=36967730&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><br><div>
                  <p><span>My parents, which represent an average person nowadays, do know how to ride a horse to get somewhere. What they don't know is how to drive a automobile, even more- for them gasoline=danger. Cars can be a nieche solution for some nieche ppl, but I don't see how it can become mainstream now, as a business you anyway need a classic horse barn bc too few ppl know what is a automobile and how to use it</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36967283"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36967283" href="https://news.ycombinator.com/vote?id=36967283&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><br><div>
                  <p><span>I think the issue would be that you need to offer payment methods that your customers trust and will use. Does Web3 satisfy that need?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36967354"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36967354" href="https://news.ycombinator.com/vote?id=36967354&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><br><div>
                  <p><span>USDT, USDC, Ethereum, Bitcoin are trusted round the world. It’s not like the latest memecoin garbage</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36967450"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36967450" href="https://news.ycombinator.com/vote?id=36967450&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><br><div>
                  <p><span>None of these are actually trusted by the general population. The average person has never even heard of USDT and USDC and barely knows what the other 2 are.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36967517"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_36967517" href="https://news.ycombinator.com/vote?id=36967517&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><br><div>
                  <p><span>So excluding crypto/web3 for the above reasons, does anyone have recommendations for non stripe mostly us friendly solutions?</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36967442"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36967442" href="https://news.ycombinator.com/vote?id=36967442&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><br><div>
                  <p><span>Most people outside of circles like this one have never heard of Ethereum, let alone USDT or USDC, and when they hear Bitcoin, they think "risky" or "scam".  If your product or service is specifically aimed at tech enthusiasts and you take pains to make it easy, maybe you could get away with accepting payments only in crypto (though I doubt it), but otherwise there's no way.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36967796"><td></td></tr>
            <tr id="36967692"><td></td></tr>
                        <tr id="36967451"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36967451" href="https://news.ycombinator.com/vote?id=36967451&amp;how=up&amp;goto=item%3Fid%3D36967159"></a></center>    </td><td><p><span>&gt; Why not?<p>- Transaction fees</p><p>- Transaction times</p><p>- KYC</p><p>- refunds</p><p>Just off the top of my head
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I'm Betting on HTML (769 pts)]]></title>
            <link>https://catskull.net/html.html</link>
            <guid>36966653</guid>
            <pubDate>Wed, 02 Aug 2023 03:31:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://catskull.net/html.html">https://catskull.net/html.html</a>, See on <a href="https://news.ycombinator.com/item?id=36966653">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
      
<header>
  <nav>
    <a href="https://catskull.net/">back</a>
  </nav>
</header>

<main>
  <article>
    

    <section>
      <blockquote>
  <p>AI Use Disclaimer:
	<br>I wrote this post and then GPT-4 fixed my <span>grammer</span> and spelling</p>
</blockquote>

<p>With the advent of large language model-based artificial intelligence, semantic HTML is more important now than ever. At its core, the internet is used to transmit data that helps humans interact with the world as they perceive it. The freedom that HTML/CSS/JS provide is a double-edged sword because access to data has become limited. Instead of open and accessible data formats and APIs, we’re kept within the walled gardens of major technology companies that operate mass social media sites. Because of this, interoperability between these platforms is effectively impossible, further complicated by these companies’ hesitance to allow easy data porting. After all, that’s their entire product, and without it, they can’t make money. The recent instability of our social media sites has renewed interest in decentralized platforms like the “fediverse”. Both Meta’s Threads and Jack Dorsey’s (Twitter founder) Bluesky claim interoperability with the larger fediverse. This is great!</p>

<p>But guess what? The general population doesn’t care!</p>

<p>What I mean is that people are not typically motivated to adopt new social media platforms for reasons that may not be entirely clear. I’m not condemning these efforts - I believe there’s a future there, and I’m watching as the development progresses. However, I believe we’re already sitting on a tried and tested solution: HTML.</p>

<p>Historically, heavy use of CSS was needed to prevent HTML content from looking terrible when rendered in a browser. Luckily, it’s not 1997 anymore! There are many new HTML elements that I wasn’t aware of until recently. I believe we now have virtually a complete set of all UI elements needed to build any modern web application. I don’t foresee corporate designers giving up their fancy JavaScript date picker that, <em>in a shock to nobody</em> actually breaks the entire site on mobile, anytime soon. But we’re on the fringe, and we can do whatever we want. In fact, recently I’ve become acutely aware of reader mode. All time spent on styling will be obliterated by reader mode, and that’s a great thing!</p>

<p>Moreover, proper tagging is extremely descriptive in a machine-readable format. This is likely a more compelling reason for adopting modern HTML than saving design time. The shift from primary data interfaces to secondary interfaces is already underway. RSS refuses to die. ChatGPT-like interfaces are likely the future of human data access. We’re going back to the beginning. Advertisers may be scared, but I’m not! Let’s start the revolution and set the world on fire with modern HTML.</p>

<p>Below are a few examples of UI elements I found novel or useful. No style has been applied to these beyond the browser’s built-in style. Note that because of this, these examples may look vastly different (or be completely unsupported) in various browsers. It’s well worth your time browsing the full list on MDN: <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element">https://developer.mozilla.org/en-US/docs/Web/HTML/Element</a></p>

<h4><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/abbr"><code>&lt;abbr&gt;</code>: The Abbreviation element</a></h4>
<p>Wrap any abbreviation in this! You can apply a style to highlight them. Mostly useful for machine reading.</p>

<h4><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/datalist"><code>&lt;datalist&gt;</code>: The HTML Data List element</a></h4>
<p>Is that a typeahead I see? 🧐 Doesn’t seem to have built in validation, but the UI is there at least. Note that Safari requires <code>option</code> tags to be closed, or it just gives up. 😮‍💨</p>

<p><label for="album-choice">Choose a King Gizzard album:</label>
</p>
<datalist id="albums">
  <option value="PetroDragonic Apocalypse"></option>
  <option value="Changes"></option>
  <option value="Laminated Denim"></option>
  <option value="Ice, Death, Planets, Lungs, Mushrooms and Lava"></option>
  <option value="Omnium Gatherum"></option>
  <option value="Made in Timeland"></option>
  <option value="Butterfly 3000"></option>
  <option value="L.W."></option>
  <option value="K.G."></option>
  <option value="Infest the Rats' Nest"></option>
  <option value="Fishing for Fishies"></option>
  <option value="Gumboot Soup"></option>
  <option value="Polygondwanaland"></option>
  <option value="Sketches of Brunswick East"></option>
  <option value="Murder of the Universe"></option>
  <option value="Flying Microtonal Banana"></option>
  <option value="Nonagon Infinity"></option>
  <option value="Paper Mâché Dream Balloon"></option>
  <option value="Quarters!"></option>
  <option value="I'm in Your Mind Fuzz"></option>
  <option value="Oddments"></option>
  <option value="Float Along – Fill Your Lungs"></option>
  <option value="Eyes Like the Sky"></option>
  <option value="12 Bar Bruise"></option>
</datalist>

<h4><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/details"><code>&lt;details&gt;</code>: The Details disclosure element</a></h4>
<p>A little dropdown thing for disclosoures and stuff. Can by styled quite aggressively.</p>

<details>
    <summary>PRIVACY DISCLOSURE</summary>
    You are being watched.
</details>

<h4><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/dialog"><code>&lt;dialog&gt;</code>: The Dialog element</a></h4>
<dialog open="">
  <p>Say the magic word and make me disappear!</p>
  
</dialog>
<p>This isn’t exactly a modal, but it is a built-in element that can be opened and closed with buttons, forms, attributes, and JavaScript. If you’re building a modal, you should use this as a base. Apparently, it renders on top of the next element.</p>

<h4><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/em#i_vs._em"><code>&lt;i&gt;</code> vs. <code>&lt;em&gt;</code></a></h4>
<p>Know the difference!</p>

<h4><a href="https://www.youtube.com/watch?v=Htc-XeJwHyk"><code>&lt;iframe&gt;</code>: The Inline Frame element</a></h4>
<p>Just kidding.</p>

<h4><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/input"><code>&lt;input&gt;</code>: The Input (Form Input) element</a></h4>
<p>Please, please, please, please, please label and use inputs appropriately! This is <em>essential</em> for mobile users as the OS will open variations of the keyboard depending on context. I have an old post on that. There is a plethora of time inputs. No more datepickers please! Check out these inputs:</p>

<p>color: 
<br>
range: 
<br>
datetime-local: </p>

<h4><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/mark"><code>&lt;mark&gt;</code>: The Mark Text element</a></h4>
<p>Pretty much you can <mark>highlight text</mark>. By default Safari shows a yellow highlight. <mark>I like it!</mark></p>

<h4><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/meter"><code>&lt;meter&gt;</code>: The HTML Meter element</a></h4>
<p>Now what exactly is this cute little guy for? Safari will show it as red/yellow/green depending on parameters that can be set. You can even get fancy and set the “optimum” value. Could be very useful. Let’s get a JS demo going to make a music visualizer at 60fps!</p>
<meter min="0" max="100" low="33" high="66" optimum="80" value="90"></meter>
<meter min="0" max="100" low="33" high="66" optimum="80" value="50"></meter>
<meter min="0" max="100" low="33" high="66" optimum="80" value="20"></meter>

<h4><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/progress"><code>&lt;progress&gt;</code>: The Progress Indicator element</a></h4>
<p>Here’s a native HTML progress bar! If that’s not progress, I don’t know what is. Can be given a fixed value or indeterminate. On Safari, it’s blue when the window is active and grey when it’s not. By default it will follow the system’s accent color, or can be set with the <code>accent-color</code> CSS property.</p>

<progress id="progress-bar" aria-label="Content loading…"></progress>

<progress id="progress-bar" value="70" max="100">70 %</progress>

<p><a href="https://news.ycombinator.com/item?id=36966653">Discuss on Hacker News</a></p>

    </section>
</article>
</main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Enu – 3D live coding, implemented in Nim (121 pts)]]></title>
            <link>https://github.com/dsrw/enu</link>
            <guid>36966116</guid>
            <pubDate>Wed, 02 Aug 2023 02:12:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/dsrw/enu">https://github.com/dsrw/enu</a>, See on <a href="https://news.ycombinator.com/item?id=36966116">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Enu</h2>
<p dir="auto">3D live coding, implemented in Nim.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/dsrw/enu/blob/main/media/screenshot_2.png"><img src="https://github.com/dsrw/enu/raw/main/media/screenshot_2.png" alt="Enu Screenshot"></a></p>
<p dir="auto">Enu lets you build and explore worlds using a familiar block-building interface and a Logo inspired API.
It aspires to make 3D development more accessible, and will eventually be usable to create standalone games.</p>
<p dir="auto"><em>Note: The docs for Enu 0.2 are a work in progress. Most of the core ideas are here, but a fair bit of revision
is required. The 0.2 docs will be targeted towards new programmers, with 'Note for Nimians` sections aimed at more
experienced folks to explain what's really going on. However, things are all over the place right now, with the
intended audience changing paragraph by paragraph.</em></p>
<p dir="auto"><em>Notes for Nimians: Enu tries to simplify some Nim concepts, mainly to defer explaining unfamiliar terms. In particular,
Enu tries to hide most things related to types, calls procs 'actions', and avoids immutable variables. I believe this is
the right approach for new programmers, but I expect that more sophisticated developers will use a style closer to
traditional Nim.</em></p>
<h2 tabindex="-1" dir="auto">Goals</h2>
<p dir="auto">Enu is meant for anyone who wants to explore, experiment, or make games, but particular care has been taken to make
it usable by younger people who may struggle with reading or typing. However, rather than bypassing the keyboard with
a Scratch-like visual programming language, Enu attempts to reduce and simplify the keystrokes required for a text-based
language, while (hopefully) preserving most of the flexibility text-based code offers.</p>
<p dir="auto">With this in mind, Enu tries to:</p>
<ul dir="auto">
<li>
<p dir="auto">Reduce nesting. Indentation can be tricky for new programmers.</p>
</li>
<li>
<p dir="auto">Reduce the use of the shift key. Lower case is used almost everywhere. Commands are written to
avoid underscores and parenthesis. By default (for now at least), a <code>;</code> keypress is interpreted as <code>:</code>, as colons are
required frequently in Nim (and require shift, at least on US English keyboards) while semi-colons are not.</p>
</li>
<li>
<p dir="auto">Omit or shorten identifier names. <code>me</code> instead of <code>self/this</code>. <code>-</code> instead of <code>proc</code>. <code>5.times:</code> or <code>5.x:</code> instead of
<code>for i in 0..5:</code>. Single letter shortcuts for many common commands.</p>
</li>
<li>
<p dir="auto">Pretends to avoid types. Enu code is Nim code and is statically typed, but a fair amount of effort has been spent
hiding this fact. Types are great, but are confusing for new programmers.</p>
</li>
<li>
<p dir="auto">Spatial organization. No files. Code is text, but it's accessed through an object in the virtual world.</p>
</li>
<li>
<p dir="auto">Avoids events. Tries to make all flow based on loops and conditionals.</p>
</li>
</ul>
<h2 tabindex="-1" dir="auto">Demo</h2>
<p dir="auto"><a href="https://youtu.be/9e9sLsmsu_o" rel="nofollow">Inky: Isolation. A 90 minute game built with Enu and Nim</a></p>
<p dir="auto"><a href="https://fosdem.org/2022/schedule/event/nim_potatozombies/" rel="nofollow">Potato Zombies: Helping a 6 year old build a 3D game with Enu and Nim</a></p>
<p dir="auto"><a href="https://youtu.be/ECJsq7BeZ8w" rel="nofollow">Building 3D Games with Enu 0.2 - NimConf 2021</a></p>
<p dir="auto"><a href="https://youtu.be/upg77dMBGDE" rel="nofollow">Enu 0.1 demo video</a>:</p>
<p dir="auto"><a href="https://youtu.be/upg77dMBGDE" rel="nofollow"><img src="https://github.com/dsrw/enu/raw/main/media/screenshot_1.png" alt="Enu 0.1 Demo"></a></p>
<h3 tabindex="-1" dir="auto">Outdated Demos</h3>
<ul dir="auto">
<li><a href="https://youtu.be/AW0PT9j976s" rel="nofollow">Enu 0.01 Intro and Demo</a></li>
<li><a href="https://youtu.be/3l6tsKM1cY8" rel="nofollow">Introducing Enu - NimConf 2020</a></li>
</ul>
<h2 tabindex="-1" dir="auto">Programming Enu</h2>
<h2 tabindex="-1" dir="auto">Units</h2>
<p dir="auto">Entities/objects in Enu are referred to as units, and have a base type of <code>Unit</code>. Currently there are <code>Build</code> units
(voxel objects) and <code>Bot</code> units (the robot). There will be more in the future.</p>
<p dir="auto"><code>me</code> is the unit that owns a script, and is equivalent to <code>self</code>/<code>this</code> in other environments. <code>me</code> was selected
because it's easier for a child to type. <code>me.</code> can be auto-inserted when accessing properties of the unit. For example,
<code>me.speed = 10</code> would commonly be written as <code>speed = 10</code>. There are probably bugs with this behavior.  Please report
them.</p>
<h2 tabindex="-1" dir="auto">Prototypes</h2>
<p dir="auto">Enu uses a prototype based object system for units. To allow a unit to be a prototype, you give it a name:</p>
<p dir="auto"><code>name ghost</code></p>
<p dir="auto">Then create a new instance in a different script with <code>.new</code>:</p>
<p dir="auto"><code>var ghost2 = ghost.new</code></p>
<p dir="auto">You can also provide parameters, which can be overridden when creating a new instance:</p>
<p dir="auto"><code>name ghost(color = white, speed = 5, spookiness = 10)</code></p>
<p dir="auto">These become properties of the unit (ie <code>me.spookiness = 5</code>), but can be treated like variables in the unit's script
due to auto <code>me</code> insertion (<code>spookiness = 200</code>).</p>
<p dir="auto">To create a new instance with specific property values:</p>
<p dir="auto"><code>var ghost2 = ghost.new(spookiness = 11)</code></p>
<p dir="auto">Parameters can have a default value (<code>spookiness = 10</code>), which makes them optional when creating a new instance. If they
should be required, or there's no reasonable default value to use, specify a type (<code>spookiness: int</code>) instead, or omit
both the value and the type, which will make the type <code>auto</code>. Because <code>auto</code> can be implicit, <code>name ghost(a, b: int)</code>
treats parameters differently than <code>proc ghost(a, b: int)</code> would. With the proc, <code>a</code> and <code>b</code> are both <code>int</code>, whereas
the <code>name</code> version would make <code>a</code> <code>auto</code> and <code>b</code> <code>int</code>.</p>
<p dir="auto"><code>speed</code>, <code>color</code>, <code>global</code> can always be passed to a new instance, even if the prototype name doesn't include them.</p>
<h2 tabindex="-1" dir="auto">Random numbers</h2>
<p dir="auto">Generally, if an Enu command takes a number, it will be a <code>float</code>. However, <code>int</code> will auto-convert to <code>float</code>, and
when a numeric <code>Range</code> is passed to something expecting a number, a random value within the range will be selected. So, even
though <code>forward</code> expects a <code>float</code>, the following are all valid:</p>
<div dir="auto" data-snippet-clipboard-copy-content="forward 1.0
forward 1
forward 1.0..5.0 # Convert to a random float between 1.0 and 5.0
forward 1..5 # Convert to a random int between 1 and 5, then convert the int to a float"><pre><span>forward</span> <span>1.0</span>
<span>forward</span> <span>1</span>
<span>forward</span> <span>1.0</span><span>..</span><span>5.0</span> <span><span>#</span> Convert to a random float between 1.0 and 5.0</span>
<span>forward</span> <span>1</span><span>..</span><span>5</span> <span><span>#</span> Convert to a random int between 1 and 5, then convert the int to a float</span></pre></div>
<p dir="auto">The <code>in</code> operator can be used between two numbers to test for random chance. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="if 1 in 2:
  echo &quot;I should be hit 50% of the time&quot;
if 1 in 100:
  echo &quot;I should be hit 1% of the time&quot;"><pre><span>if</span> <span>1</span> <span>in</span> <span>2</span>:
  <span>echo</span> <span><span>"</span>I should be hit 50% of the time<span>"</span></span>
<span>if</span> <span>1</span> <span>in</span> <span>100</span>:
  <span>echo</span> <span><span>"</span>I should be hit 1% of the time<span>"</span></span></pre></div>
<p dir="auto">By default random numbers in Enu are based partially on the time and will be different each time a script is executed.
However, sometimes you want randomness to create variety, but want the same values to be chosen each time a script is
run. This is especially important when using randomness in a <code>Build</code> that you plan to manually edit later. To ensure the
same values are selected each time a script is run, set the unit's <code>seed</code> property to some integer of your choosing,
ie <code>seed = 12345</code> or <code>me.seed = 54321</code>.</p>
<p dir="auto">Any child units instanced by a unit with a seed value will get the same seed by default. However, it will still get
a unique random number generator, so changing the script for a child object won't impact the random numbers selected by
the parent.</p>
<h2 tabindex="-1" dir="auto">Commands</h2>
<h2 tabindex="-1" dir="auto"><code>move/build</code></h2>
<p dir="auto">When dealing with a <code>Build</code> unit, commands can do different things depending on whether the unit is in <code>build</code>
mode or <code>move</code> mode. <code>move</code> mode moves the unit around, while <code>build</code> creates new blocks. By default a <code>Build</code> is in
<code>build</code> mode. Often you'll pass the <code>me</code> unit to <code>move/build</code>, but it's also possible to pass other units. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="build me # generally not required, as it's the default

# make a shape:
back 5
right 3

# move it into position:
move me
up 3

# create another unit and add some blocks
var enemy = ghost.new
build enemy
down 5

# move it into position
move enemy
up 5"><pre><span>build</span> me <span><span>#</span> generally not required, as it's the default</span>

<span><span>#</span> make a shape:</span>
<span>back</span> <span>5</span>
<span>right</span> <span>3</span>

<span><span>#</span> move it into position:</span>
<span>move</span> me
<span>up</span> <span>3</span>

<span><span>#</span> create another unit and add some blocks</span>
<span>var</span> <span>enemy</span> <span>=</span> ghost.new
<span>build</span> enemy
<span>down</span> <span>5</span>

<span><span>#</span> move it into position</span>
<span>move</span> enemy
<span>up</span> <span>5</span></pre></div>
<p dir="auto">It's also possible to call commands directly against a unit instance, but they will always use <code>move</code> mode, regardless
of which mode is in use:</p>
<div dir="auto" data-snippet-clipboard-copy-content="build enemy
up 5 # build 5 blocks up
enemy.up 5 # move up 5"><pre><span>build</span> enemy
<span>up</span> <span>5</span> <span><span>#</span> build 5 blocks up</span>
enemy.<span>up</span> <span>5</span> <span><span>#</span> move up 5</span></pre></div>
<h2 tabindex="-1" dir="auto"><code>forward/back/up/down/left/right</code></h2>
<p dir="auto">Move or build x number of blocks in the specified direction. Defaults to 1 block.</p>

<h2 tabindex="-1" dir="auto"><code>turn</code></h2>
<p dir="auto">Turn a unit. Can be passed:</p>
<ul dir="auto">
<li>a number in degrees. Positive for clockwise, negative for counter-clockwise. Ex. <code>turn 180</code></li>
<li>a direction (<code>forward/back/up/down/left/right</code>) which will turn in that direction. 90 degrees by default.
Ex. <code>turn left</code>, or <code>turn up, 180</code></li>
<li>a unit to turn towards. Ex. <code>turn player</code></li>
<li>a negative unit to turn away from. Ex. <code>turn -player</code></li>
</ul>
<h2 tabindex="-1" dir="auto"><code>near(less_than = 5.0) / far(greater_than = 100.0)</code></h2>
<p dir="auto">Returns true or false if a unit is nearer/farther than the specified distance. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="if player.near:
  echo &quot;the player is 5m or closer&quot;

if player.far:
  echo &quot;the player is 100m or farther&quot;

if player.near(10):
  echo &quot;the player is 10m or closer&quot;

if player.far(25):
  echo &quot;the player is 25m or farther&quot;"><pre><span>if</span> player.near:
  <span>echo</span> <span><span>"</span>the player is 5m or closer<span>"</span></span>

<span>if</span> player.far:
  <span>echo</span> <span><span>"</span>the player is 100m or farther<span>"</span></span>

<span>if</span> player.<span>near</span>(<span>10</span>):
  <span>echo</span> <span><span>"</span>the player is 10m or closer<span>"</span></span>

<span>if</span> player.<span>far</span>(<span>25</span>):
  <span>echo</span> <span><span>"</span>the player is 25m or farther<span>"</span></span></pre></div>
<h2 tabindex="-1" dir="auto"><code>hit</code></h2>
<p dir="auto">If a unit is touching another unit, return the vector of the contact. Defaults to testing against <code>me</code>. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="if player.hit:
  echo &quot;I'm touching the player&quot;

if player.hit == UP:
  echo &quot;The player is on top of me&quot;

if player.hit(enemy1):
  echo &quot;The player hit enemy1&quot;"><pre><span>if</span> player.hit:
  <span>echo</span> <span><span>"</span>I'm touching the player<span>"</span></span>

<span>if</span> player.hit <span>==</span> <span>UP</span>:
  <span>echo</span> <span><span>"</span>The player is on top of me<span>"</span></span>

<span>if</span> player.<span>hit</span>(enemy1):
  <span>echo</span> <span><span>"</span>The player hit enemy1<span>"</span></span></pre></div>
<h2 tabindex="-1" dir="auto"><code>position/postion=</code></h2>
<p dir="auto">Gets or set the position of a unit as a Vector3. <code>me</code> by default.</p>
<div dir="auto" data-snippet-clipboard-copy-content="if player.hit(enemy):
  # if the player hits `enemy`, reset the player position to the center of the world.
  player.position = vec3(0, 0, 0)"><pre><span>if</span> player.<span>hit</span>(enemy):
  <span><span>#</span> if the player hits `enemy`, reset the player position to the center of the world.</span>
  player.position <span>=</span> <span>vec3</span>(<span>0</span>, <span>0</span>, <span>0</span>)</pre></div>
<h2 tabindex="-1" dir="auto"><code>start_position</code></h2>
<p dir="auto">The starting position of a unit. Missing currently, but will be in in 0.2.</p>
<h2 tabindex="-1" dir="auto"><code>speed/speed=</code></h2>
<p dir="auto">Gets or sets the speed of a unit. <code>me</code> by default.</p>
<p dir="auto">While building, speed refers to the number of blocks placed per frame. In the future this will be normalized to 60fps,
but currently the speed is tied to the framerate. Setting speed to 0 will build everything at once.</p>
<p dir="auto">While moving, this is the movement speed in meters per second.</p>
<p dir="auto">Switching between build and move mode doesn't impact the speed, except in the case of switching to move mode from build
mode with a speed of 0. <code>speed = 0</code> is extremely common for build mode, but makes things appear broken in move mode,
as nothing will actually move, so switching to move mode with a speed of 0 will automatically reset the speed to 1.</p>
<h2 tabindex="-1" dir="auto"><code>scale/scale=</code></h2>
<p dir="auto">Sets the scale/size of a unit. <code>me</code> by default.</p>
<h2 tabindex="-1" dir="auto"><code>glow/glow=</code></h2>
<p dir="auto">Specifies the glow/brightness of a unit. <code>me</code> by default. Currently does nothing for bots, but will in the future.</p>
<h2 tabindex="-1" dir="auto"><code>global/global=</code></h2>
<p dir="auto">Specifies if a unit is in global space, or the space of its parent. If <code>global = true</code> and the parent unit moves,
child units are unaffected. If <code>global = false</code>, the child will move with its parent. Does nothing for top level units,
as they're always global.</p>
<p dir="auto">By default, new <code>Build</code> units are <code>global = false</code> and new <code>Bot</code> units are <code>global = true</code>.</p>
<h2 tabindex="-1" dir="auto"><code>rotation</code></h2>
<p dir="auto">Gets the rotation of a unit as a Vector3.</p>
<h2 tabindex="-1" dir="auto"><code>velocity/velocity=</code></h2>
<p dir="auto">Gets or sets the velocity of a unit, as a Vector3. Currently buggy.</p>
<h2 tabindex="-1" dir="auto"><code>color/color=</code></h2>
<p dir="auto">Gets or sets a units color. <code>me</code> by default. For <code>Build</code> units, this only impacts blocks placed after the property
is set. For <code>Bot</code> units this does nothing, but in the future it will change their color.</p>
<h2 tabindex="-1" dir="auto"><code>bounce</code></h2>
<p dir="auto">Bounces a unit in the air. Currently only works for the player.</p>
<h2 tabindex="-1" dir="auto"><code>save/restore</code></h2>
<p dir="auto"><code>Build</code> units only. <code>save</code> the position, direction, drawing state, and color of the draw point, to <code>restore</code> it later.
Can optionally take a name string to enable saving/restoring multiple points.</p>
<h2 tabindex="-1" dir="auto"><code>reset</code></h2>
<p dir="auto">Instantly return unit to start position and resets rotation and scale.</p>
<h2 tabindex="-1" dir="auto"><code>home</code></h2>
<p dir="auto">Moves a unit to its start position via a <code>forward</code>, <code>left</code>, <code>down</code> sequence with appropriate values. Can fail if there
are obstructions along the way. Compare <code>position</code> to <code>start_position</code> after running to test for success.</p>
<h2 tabindex="-1" dir="auto"><code>sleep(seconds = -1.0)</code></h2>
<p dir="auto">Do nothing for the specified number of seconds. If no argument is provided, or the argument is &lt; 0, this will wait for
0.5 seconds or until unit is interrupted, which will end the <code>sleep</code> prematurely. This allows the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="forever:
  sleep()
  if player.hit:
    echo &quot;ouch!&quot;"><pre>forever:
  <span>sleep</span>()
  <span>if</span> player.hit:
    <span>echo</span> <span><span>"</span>ouch!<span>"</span></span></pre></div>
<p dir="auto">Currently, any collision will trigger an interrupt. This will be expanded in the future.</p>
<h2 tabindex="-1" dir="auto"><code>forever</code></h2>
<p dir="auto">Alias for <code>while true</code></p>
<h2 tabindex="-1" dir="auto"><code>cycle</code></h2>
<p dir="auto">Alternate between a list of values, returning the next element each time the cycle is called.</p>
<div dir="auto" data-snippet-clipboard-copy-content="forever:
  sleep 1
  echo cycle(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)"><pre>forever:
  <span>sleep</span> <span>1</span>
  <span>echo</span> <span>cycle</span>(<span><span>"</span>one<span>"</span></span>, <span><span>"</span>two<span>"</span></span>, <span><span>"</span>three<span>"</span></span>)</pre></div>
<h2 tabindex="-1" dir="auto">Shorthand Commands</h2>
<p dir="auto">Many Enu command also have a 1 letter alias. These are harder to read, but can reduce friction for folks new to
typing.</p>
<p dir="auto">The aliases are:</p>
<ul dir="auto">
<li><code>f</code> - <code>forward</code></li>
<li><code>b</code> - <code>back</code></li>
<li><code>l</code> - <code>left</code></li>
<li><code>r</code> - <code>right</code></li>
<li><code>u</code> - <code>up</code></li>
<li><code>d</code> - <code>down</code></li>
<li><code>t</code> - <code>turn</code>. Can be combined with shorthand directions, so <code>turn right</code> can be expressed as <code>t r</code></li>
<li><code>o</code> - <code>while true:</code> (o was selected because its shape is a loop)</li>
<li><code>x</code> - <code>times</code>. <code>5.x:</code> will run a code block 5 times.</li>
</ul>
<p dir="auto">In action:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# draw a cube (with no top)
10.x:
  4.x:
    f 10
    t r
  u 1"><pre><span><span>#</span> draw a cube (with no top)</span>
<span>10</span>.x:
  <span>4</span>.x:
    <span>f</span> <span>10</span>
    <span>t</span> r
  <span>u</span> <span>1</span></pre></div>
<h2 tabindex="-1" dir="auto">Actions</h2>
<p dir="auto">Procedures/functions in Enu are referred to as actions, mainly to avoid explaining the term procedure, subroutine, or
function, and to tie them to <a href="#action-loops">Action Loops</a> defined below. Their syntax resembles markdown lists, and
have the same parameter rules as <a href="#prototypes">prototype</a> names. That is, mostly the same
as Nim procs, but types can be omitted, making the parameter implicitly <code>auto</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="- hello(name):
  echo &quot;hello &quot;, name

- goodbye(name = &quot;Vin&quot;):
  echo &quot;goodbye &quot;, name

hello &quot;Claire&quot;
goodbye &quot;Cal&quot;"><pre><span>-</span> <span>hello</span>(name):
  <span>echo</span> <span><span>"</span>hello <span>"</span></span>, name

<span>-</span> <span>goodbye</span>(<span>name</span> <span>=</span> <span><span>"</span>Vin<span>"</span></span>):
  <span>echo</span> <span><span>"</span>goodbye <span>"</span></span>, name

<span>hello</span> <span><span>"</span>Claire<span>"</span></span>
<span>goodbye</span> <span><span>"</span>Cal<span>"</span></span></pre></div>
<p dir="auto">Action parameters are automatically shadowed by a variable with the same name and value, making them mutable within the
action. Enu tries to avoid the concept of immutable values.</p>
<p dir="auto">It's also possible to specify a return type between the closing bracket of the parameter list and the colon:</p>
<div dir="auto" data-snippet-clipboard-copy-content="- hello(name) string:
  &quot;hello &quot; &amp; $name

echo hello(&quot;Scott&quot;)"><pre><span>-</span> <span>hello</span>(name) <span>string</span>:
  <span><span>"</span>hello <span>"</span></span> <span>&amp;</span> <span>$</span>name

<span>echo</span> <span>hello</span>(<span><span>"</span>Scott<span>"</span></span>)</pre></div>
<p dir="auto">However, at this point it's probably better to use a <code>proc</code>.</p>
<h2 tabindex="-1" dir="auto">Action Loops</h2>
<p dir="auto"><em>Note for Nimians: Action Loops are state machines, and any proc can be a state. If the proc has a return value it will
be discarded.</em></p>
<p dir="auto">Action Loops can help control the complexity of the logic for your units. They allow you to run complicated lists of
actions and switch between them easily when situations change.</p>
<p dir="auto">You can create your own <a href="#actions">actions</a>, or you can call any of the built-in Enu <a href="#commands">commands</a> like
<code>forward</code>, <code>back</code>, <code>turn</code>, <code>sleep</code>, etc.</p>
<h2 tabindex="-1" dir="auto"><code>loop</code></h2>
<p dir="auto">An Action Loop always has one and only one current action, which it will call repeatedly until you switch to some
other action. The default action is <code>nil</code>. The first thing a loop must do is switch from <code>nil</code> to another action, using
the little switch arrow <code>-&gt;</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="loop:
  nil -> forward
  # I'll go forward forever!"><pre>loop:
  <span>nil</span> <span>-&gt;</span> forward
  <span><span>#</span> I'll go forward forever!</span></pre></div>
<p dir="auto">The little switch arrow (<code>-&gt;</code>) will switch from the action on the left to the action on the right if it's encountered
and the left action has just completed. If the loop goes through and no switches match, the current action will be
run again.</p>
<p dir="auto">We can control which switches get run by putting them in <code>if</code> statements.</p>
<div dir="auto" data-snippet-clipboard-copy-content="loop:
  nil -> forward
  if player.far:
    forward -> back
  if player.near:
    back -> forward"><pre>loop:
  <span>nil</span> <span>-&gt;</span> forward
  <span>if</span> player.far:
    forward <span>-&gt;</span> back
  <span>if</span> player.near:
    back <span>-&gt;</span> forward</pre></div>
<p dir="auto">In the above example, the loop immediately switches to <code>forward</code>, and will go forward indefinitely until one of the
conditions is met and the action is switched to something else. If the player gets too far away (more than 100m) and
the action is <code>forward</code>, the action will be switched to <code>back</code>. If the player is near (5m) and the action is <code>back</code>,
it will switch to <code>forward</code>. However, if the player is near and the action is <code>forward</code>, nothing will change. The
<code>if player.near</code> statement will be true, but <code>back -&gt; forward</code> is ignored, since the current action isn't <code>back</code>.</p>
<p dir="auto">If you want your loop to end at some point, you can switch back to <code>nil</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="loop:
  nil -> forward(10) # Some actions can take additional parameters.
  forward -> nil
  # I'll run `forward(10)` a single time, then stop and end the loop."><pre>loop:
  <span>nil</span> <span>-&gt;</span> <span>forward</span>(<span>10</span>) <span><span>#</span> Some actions can take additional parameters.</span>
  forward <span>-&gt;</span> <span>nil</span>
  <span><span>#</span> I'll run `forward(10)` a single time, then stop and end the loop.</span></pre></div>
<p dir="auto">Let's look at something more complicated, and introduce the big switch arrow (<code>==&gt;</code>) and change actions.</p>
<div dir="auto" data-snippet-clipboard-copy-content="- wander:
  speed = 1
  forward 5..10
  turn -45..45

- charge:
  speed = 5
  turn player
  forward 1

- flee:
  speed = 3
  turn -player
  forward 1

- attack:
  player.bounce 5
  turn 360

var health = 3

loop:
  nil -> wander
  if 1 in 50:
    # when each `wander` action finishes, there's a 1 in 50 (2%) chance of our unit getting a sudden
    # burst of energy and switching to the `charge` action. Otherwise we just keep wandering.
    wander -> charge

  if health == 0:
    # we died. Exit the loop. We want this to happen immediately, not after the action finishes, so we use
    # the big switch arrow. We use the special `any` action to say that this should happen regardless
    # of the running action.
    any ==> nil

  if player.hit:
    # if the player touches us while we're wandering, we flee. We want it to happen the instant the player touches us,
    # not when our current `wander` is done, so we use the big switch arrow
    wander ==> flee:
      # this is a change action. If the action switches here, the change action will also run once.
      health -= 1
    # if the player touches us while we're charging, we attack immediately.
    charge ==> attack

  if player.far:
    # if we're fleeing the player, we go back to wandering when they get far away
    flee -> wander

  # Switch to wander when our attack is done. We always want this to happen, so it isn't in a
  # conditional. It only does anything if the current action is `attack`, and we only do it when
  # the attack is done becuase we're using the little switch arrow
  attack -> wander
"><pre><span>-</span> wander:
  speed <span>=</span> <span>1</span>
  <span>forward</span> <span>5</span><span>..</span><span>10</span>
  <span>turn</span> <span>-</span><span>45</span><span>..</span><span>45</span>

<span>-</span> charge:
  speed <span>=</span> <span>5</span>
  <span>turn</span> player
  <span>forward</span> <span>1</span>

<span>-</span> flee:
  speed <span>=</span> <span>3</span>
  <span>turn</span> <span>-</span>player
  <span>forward</span> <span>1</span>

<span>-</span> attack:
  player.<span>bounce</span> <span>5</span>
  <span>turn</span> <span>360</span>

<span>var</span> <span>health</span> <span>=</span> <span>3</span>

loop:
  <span>nil</span> <span>-&gt;</span> wander
  <span>if</span> <span>1</span> <span>in</span> <span>50</span>:
    <span><span>#</span> when each `wander` action finishes, there's a 1 in 50 (2%) chance of our unit getting a sudden</span>
    <span><span>#</span> burst of energy and switching to the `charge` action. Otherwise we just keep wandering.</span>
    wander <span>-&gt;</span> charge

  <span>if</span> health <span>==</span> <span>0</span>:
    <span><span>#</span> we died. Exit the loop. We want this to happen immediately, not after the action finishes, so we use</span>
    <span><span>#</span> the big switch arrow. We use the special `any` action to say that this should happen regardless</span>
    <span><span>#</span> of the running action.</span>
    <span>any</span> <span>==&gt;</span> <span>nil</span>

  <span>if</span> player.hit:
    <span><span>#</span> if the player touches us while we're wandering, we flee. We want it to happen the instant the player touches us,</span>
    <span><span>#</span> not when our current `wander` is done, so we use the big switch arrow</span>
    wander <span>==&gt;</span> flee:
      <span><span>#</span> this is a change action. If the action switches here, the change action will also run once.</span>
      health <span>-=</span> <span>1</span>
    <span><span>#</span> if the player touches us while we're charging, we attack immediately.</span>
    charge <span>==&gt;</span> attack

  <span>if</span> player.far:
    <span><span>#</span> if we're fleeing the player, we go back to wandering when they get far away</span>
    flee <span>-&gt;</span> wander

  <span><span>#</span> Switch to wander when our attack is done. We always want this to happen, so it isn't in a</span>
  <span><span>#</span> conditional. It only does anything if the current action is `attack`, and we only do it when</span>
  <span><span>#</span> the attack is done becuase we're using the little switch arrow</span>
  attack <span>-&gt;</span> wander
</pre></div>
<h2 tabindex="-1" dir="auto">Child Loops</h2>
<p dir="auto">Actions are generally just a simple lists of commands. It's fine to put logic in them, but anything complicated
will quickly get unwieldy. Imagine we have a unit that performs two complicated actions, <code>find_treasure</code> and
<code>fight_monster</code>. <code>find_treasure</code> might need to <code>navigate</code> an area, <code>locate</code> items of interest, <code>interact</code> with them,
then return to <code>home_base</code> to deposit them. <code>fight_monster</code> could require actions like <code>evade</code>, <code>attack</code>, <code>hide</code>, and
<code>flee</code>.</p>
<p dir="auto">Combining all of this in a single action loop would give us a lot of actions, many of which are mostly unrelated,
and managing our switches could get very complicated. However, making them entirely separate isn't ideal either, as
there's probably some common functionality between them (die if our health gets too low, respawn if we get stuck). We
also need to switch between the two actions.</p>
<p dir="auto">A good way to manage this is by making <code>find_treasure</code> and <code>fight_monster</code> child loops rather than regular actions.
We can treat them like regular actions in our main <code>loop</code>, but when we switch to them they'll be able to perform
more sophisticated logic than a normal action could. In addition, our main loop will continue to run along side
the the child loop, so we can quickly switch out of the child loop with a big switch arrow <code>==&gt;</code> in response to
certain conditions, without either loop needing to worry about higher level concerns.</p>
<p dir="auto">Our main loop could look something like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="loop find_treasure:
  # our treasure logic goes here. This loop doesn't have an exit condition.
  if treasure_found:
    # We found it. Start looking again. This will switch from any action apart
    # from `look_for_chest`
    others ==> look_for_chest

loop fight_monster:
  # fight logic here. This loop should exit when the monster dies
  if monster.dead:
    any ==> nil

loop:
  # We want our unit to find_treasure indefinately. `find_treasure` doesn't exit (switch to nil)
  nil -> find_treasure
  if monster.near:
    # find_treasure doesn't need to know anything about monsters. We can break out of it
    # with a big switch arrow if we encounter one.
    find_treasure ==> fight_monster

  # fight_monster does have an exit condition (the monster dies), so we can wait for it to finish
  # using the little switch arrow, then go back to finding treasure.
  fight_monster -> find_treasure

  if health == 0:
    # if our health drops to 0, it doesn't matter what else we're doing. Die immediately. Break out
    # of any action (except die) with a big switch arrow.
    (any, -die) ==> die

    # this would also work:
    # others ==> die
  if stuck:
    # respawn if we're stuck, but only from our two child loops. We don't want to respawn if we're
    # already respawning, or we're dead. Implementation of `respawn` not shown.
    (find_treasure, fight_monster) ==> respawn

  # we're done respawning. Treasure time!
  respawn -> find_treasure"><pre><span>loop</span> find_treasure:
  <span><span>#</span> our treasure logic goes here. This loop doesn't have an exit condition.</span>
  <span>if</span> treasure_found:
    <span><span>#</span> We found it. Start looking again. This will switch from any action apart</span>
    <span><span>#</span> from `look_for_chest`</span>
    others <span>==&gt;</span> look_for_chest

<span>loop</span> fight_monster:
  <span><span>#</span> fight logic here. This loop should exit when the monster dies</span>
  <span>if</span> monster.dead:
    <span>any</span> <span>==&gt;</span> <span>nil</span>

loop:
  <span><span>#</span> We want our unit to find_treasure indefinately. `find_treasure` doesn't exit (switch to nil)</span>
  <span>nil</span> <span>-&gt;</span> find_treasure
  <span>if</span> monster.near:
    <span><span>#</span> find_treasure doesn't need to know anything about monsters. We can break out of it</span>
    <span><span>#</span> with a big switch arrow if we encounter one.</span>
    find_treasure <span>==&gt;</span> fight_monster

  <span><span>#</span> fight_monster does have an exit condition (the monster dies), so we can wait for it to finish</span>
  <span><span>#</span> using the little switch arrow, then go back to finding treasure.</span>
  fight_monster <span>-&gt;</span> find_treasure

  <span>if</span> health <span>==</span> <span>0</span>:
    <span><span>#</span> if our health drops to 0, it doesn't matter what else we're doing. Die immediately. Break out</span>
    <span><span>#</span> of any action (except die) with a big switch arrow.</span>
    (<span>any</span>, <span>-</span>die) <span>==&gt;</span> die

    <span><span>#</span> this would also work:</span>
    <span><span>#</span> others ==&gt; die</span>
  <span>if</span> stuck:
    <span><span>#</span> respawn if we're stuck, but only from our two child loops. We don't want to respawn if we're</span>
    <span><span>#</span> already respawning, or we're dead. Implementation of `respawn` not shown.</span>
    (find_treasure, fight_monster) <span>==&gt;</span> respawn

  <span><span>#</span> we're done respawning. Treasure time!</span>
  respawn <span>-&gt;</span> find_treasure</pre></div>
<p dir="auto">Child loops can also call other child loops, in which case both the parent and grandparent loops can use <code>==&gt;</code> to break
out of the top level loop. There's no set limit to nesting depth.</p>
<h2 tabindex="-1" dir="auto"><code>-&gt;</code> Little Switch Arrow</h2>
<p dir="auto">Switches from one action to another, after the first action has finished running.</p>

<h2 tabindex="-1" dir="auto"><code>==&gt;</code> Big Switch Arrow</h2>
<p dir="auto">Switches from one action to another immediately. Will interrupt the running action.</p>
<div dir="auto" data-snippet-clipboard-copy-content="if player.near:
  sleep ==> offer_quest"><pre><span>if</span> player.near:
  sleep <span>==&gt;</span> offer_quest</pre></div>
<h2 tabindex="-1" dir="auto">More about Action Loops</h2>
<h3 tabindex="-1" dir="auto"><code>as</code></h3>
<p dir="auto">Actions are just procedures, and they can take parameters. Sometimes you want to run the same action
in different situations with a different action name. You could do this by creating a new action that
calls the first one, but you can also use <code>as</code> to give an action a different name.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# explore action and health var not shown.
- flee(distance = 100):
  turn -player
  forward distance

loop:
  nil -> explore
  if player.near and health > 2:
    explore -> flee
  elif player.near and health <= 2:
    explore ==> flee(200) as really_flee
  if player.far:
    flee -> explore
  if player.far(150):
    really_flee -> explore"><pre><span><span>#</span> explore action and health var not shown.</span>
<span>-</span> <span>flee</span>(<span>distance</span> <span>=</span> <span>100</span>):
  <span>turn</span> <span>-</span>player
  <span>forward</span> distance

loop:
  <span>nil</span> <span>-&gt;</span> explore
  <span>if</span> player.near <span>and</span> health <span>&gt;</span> <span>2</span>:
    explore <span>-&gt;</span> flee
  <span>elif</span> player.near <span>and</span> health <span>&lt;=</span> <span>2</span>:
    explore <span>==&gt;</span> <span>flee</span>(<span>200</span>) <span>as</span> really_flee
  <span>if</span> player.far:
    flee <span>-&gt;</span> explore
  <span>if</span> player.<span>far</span>(<span>150</span>):
    really_flee <span>-&gt;</span> explore</pre></div>
<h3 tabindex="-1" dir="auto">Special from actions</h3>
<p dir="auto">Often loops will switch from a single action to another. However, sometimes you want to allow switching from
a variety of actions.</p>
<ul dir="auto">
<li><code>any -&gt; some_action</code> - switch from any action to the target action. This will switch (and run any change action)
even if we're already running the target action. In this example we used a little switch arrow, so it still won't
happen until the current action actually completes.</li>
<li><code>others -&gt; some_action</code> - Same as <code>any</code>, but it excludes the target action.</li>
<li><code>(action1, action2)</code> - Multiple from actions can be supported by putting them in a tuple.</li>
<li><code>(any, -action2, -action3)</code> - Switch from any action except <code>action2</code> and <code>action3</code>.</li>
</ul>
<h3 tabindex="-1" dir="auto">When do action loops run?</h3>
<p dir="auto">An action loops will run whenever its executing action finishes. In addition, action loops will run
every 0.5 seconds, and when something triggers an interrupt. Currently only the start and end of a collision with the
player trigger an interrupt, but this will be expanded.</p>
<p dir="auto">When using child loops, the top level loop runs first, then walks down the stack of loops until the currently executing
loop is reached.</p>
<h2 tabindex="-1" dir="auto">Examples</h2>
<p dir="auto"><em>TODO: Include examples of new 0.2 functionality</em></p>
<p dir="auto">Draw a square:</p>
<div dir="auto" data-snippet-clipboard-copy-content="forward 10
right 10
back 10
left 10"><pre><span>forward</span> <span>10</span>
<span>right</span> <span>10</span>
<span>back</span> <span>10</span>
<span>left</span> <span>10</span></pre></div>
<p dir="auto">or:</p>
<div dir="auto" data-snippet-clipboard-copy-content="4.times:
  forward 10
  turn_right()"><pre><span>4</span>.times:
  <span>forward</span> <span>10</span>
  <span>turn_right</span>()</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/dsrw/enu/blob/main/media/square_example.png"><img src="https://github.com/dsrw/enu/raw/main/media/square_example.png" alt="Square Example"></a>
Create a twisting tower:</p>
<div dir="auto" data-snippet-clipboard-copy-content="var
  length = 20
  height = 50

height.times:
  left length / 2
  back length / 2
  4.times:
    forward length
    turn right
  forward length / 2
  right length / 2
  turn 5
  up 1"><pre><span>var</span>
  <span>length</span> <span>=</span> <span>20</span>
  <span>height</span> <span>=</span> <span>50</span>

height.times:
  <span>left</span> length <span>/</span> <span>2</span>
  <span>back</span> length <span>/</span> <span>2</span>
  <span>4</span>.times:
    <span>forward</span> length
    <span>turn</span> right
  <span>forward</span> length <span>/</span> <span>2</span>
  <span>right</span> length <span>/</span> <span>2</span>
  <span>turn</span> <span>5</span>
  <span>up</span> <span>1</span></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/dsrw/enu/blob/main/media/tower_example.png"><img src="https://github.com/dsrw/enu/raw/main/media/tower_example.png" alt="Tower Example"></a></p>
<p dir="auto">Draw randomly:</p>
<div dir="auto" data-snippet-clipboard-copy-content="up 10
forward 10
(50..100).times:
  forward 2..5
  turn -180..180
  up 0..2"><pre><span>up</span> <span>10</span>
<span>forward</span> <span>10</span>
(<span>50</span><span>..</span><span>100</span>).times:
  <span>forward</span> <span>2</span><span>..</span><span>5</span>
  <span>turn</span> <span>-</span><span>180</span><span>..</span><span>180</span>
  <span>up</span> <span>0</span><span>..</span><span>2</span></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/dsrw/enu/blob/main/media/random_example.png"><img src="https://github.com/dsrw/enu/raw/main/media/random_example.png" alt="Random Example"></a></p>
<p dir="auto">Set the color to blue randomly with a 1 in 50 chance. Otherwise set it to white:</p>
<div dir="auto" data-snippet-clipboard-copy-content="if 1 in 50:
  color = blue
else:
  color = white"><pre><span>if</span> <span>1</span> <span>in</span> <span>50</span>:
  color <span>=</span> blue
<span>else</span>:
  color <span>=</span> white</pre></div>
<p dir="auto">or as a one-liner:</p>
<div dir="auto" data-snippet-clipboard-copy-content="color = if 1 in 50: blue else: white"><pre>color <span>=</span> <span>if</span> <span>1</span> <span>in</span> <span>50</span>: blue <span>else</span>: white</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/dsrw/enu/blob/main/media/random_color_example.png"><img src="https://github.com/dsrw/enu/raw/main/media/random_color_example.png" alt="Random Color Example"></a></p>
<p dir="auto">Move forward 10 times, cycling through colors:</p>
<div dir="auto" data-snippet-clipboard-copy-content="10.times:
  color = cycle(red, black, blue)
  forward 1"><pre><span>10</span>.times:
  color <span>=</span> <span>cycle</span>(red, black, blue)
  <span>forward</span> <span>1</span></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/dsrw/enu/blob/main/media/cycle_example.png"><img src="https://github.com/dsrw/enu/raw/main/media/cycle_example.png" alt="Color Cycle Example"></a></p>
<h2 tabindex="-1" dir="auto">Install</h2>
<p dir="auto">Download from <a href="https://github.com/dsrw/enu/releases">https://github.com/dsrw/enu/releases</a>. The Windows version isn't signed, and
UAC will warn that it's untrusted. This will be fixed in a future release.</p>
<p dir="auto">The Linux version hasn't been tested particularly well, but it works for me under Ubuntu 20.04. Please report any issues.</p>
<p dir="auto">The world format will change in a future release. Worlds created in 0.1 won't be supported in future versions.</p>
<h2 tabindex="-1" dir="auto">Build and Run</h2>
<div dir="auto" data-snippet-clipboard-copy-content="$ nimble prereqs
$ nimble build
$ nimble import_assets
$ nimble start"><pre>$ <span>nimble prereqs</span>
$ <span>nimble build</span>
$ <span>nimble import_assets</span>
$ <span>nimble start</span></pre></div>
<h2 tabindex="-1" dir="auto">Notes</h2>
<p dir="auto">Enu requires a custom Godot version, which lives in <code>vendor/godot</code>. This will be fetched
and built as part of <code>nimble prereqs</code>.</p>
<p dir="auto">See <a href="https://docs.godotengine.org/en/3.2/development/compiling/index.html" rel="nofollow">https://docs.godotengine.org/en/3.2/development/compiling/index.html</a></p>
<h2 tabindex="-1" dir="auto">Usage</h2>
<h2 tabindex="-1" dir="auto">Keyboard/Mouse</h2>
<ul dir="auto">
<li><code>ESC</code> - toggle mouse capture and to dismiss editor windows. Reloads script changes.</li>
<li><code>W, A, S, D</code> - move around when mouse is captured.</li>
<li><code>Space</code> - jump. Double jump to toggle flying. Hold to go up while flying.</li>
<li><code>Shift</code> - run.</li>
<li><code>C</code> - go down while flying.</li>
<li><code>~</code> - toggle the console.</li>
<li><code>F</code> - toggle fullscreen.</li>
<li><code>1</code> - enter edit mode.</li>
<li><code>2 - 9</code> - change active action.</li>
<li><code>Mouse Wheel Up/Down</code> - change active action.</li>
<li><code>Alt</code> - reload script changes. Hold to temporarily capture the mouse and move, so you can
change your view without having to switch away from what you're doing.</li>
<li><code>Cmd+P / Ctrl+P</code> - Pause scripts.</li>
<li><code>Cmd+Shift+S / Ctrl+Shift+S</code> - Save and reload all scripts, then pause. If you have a script that makes a unit
inaccessible (ex. moves the unit below the ground) this is a way to get things back to their start positions so they
can be edited.</li>
<li><code>Left Click</code> - Place a block/object or open the code for the currently selected object.</li>
<li><code>Right Click</code> - Remove a block/object.</li>
</ul>
<h2 tabindex="-1" dir="auto">XBox / Playstation Controller</h2>
<ul dir="auto">
<li><code>Left Stick</code> - move.</li>
<li><code>Right Stick</code> - change view.</li>
<li><code>A / X</code> - jump. Double jump to toggle flying. Hold to go up while flying.</li>
<li><code>B / ◯</code> - go down while flying. Dismiss code editor.</li>
<li><code>Y / △</code> - toggle edit mode.</li>
<li><code>L1 / R1</code> - change active action.</li>
<li><code>L2</code> - place a block/object or open the code for the currently selected object.</li>
<li><code>R2</code> - remove a block/object.</li>
<li><code>L3</code> - run.</li>
</ul>
<p dir="auto">Enu currently includes 6 block types/colors, and 1 object model (a robot). This will be greatly
expanded in the future.</p>
<h2 tabindex="-1" dir="auto">Building</h2>
<p dir="auto">Drop a block or robot with the left mouse button/controller trigger, remove it with the right. Adjoining blocks will be
combined into a single structure. With the mouse captured, building works more or less like MineCraft. Release the mouse
by pressing ESC to draw blocks using the mouse cursor.</p>
<p dir="auto">Code by switching to the code tool by left clicking/triggering on an object or structure. Changes are applied when the
code window is closed (ESC key) or CTRL is pressed. Holding CTRL will also temprarly grab the mouse and allow you to
change your position.</p>
<h2 tabindex="-1" dir="auto">Config</h2>
<p dir="auto">The Enu data directory lives in <code>~/Library/Application Support/enu</code> on Mac, <code>%AppData%\enu</code> on Windows, and
<code>~/.local/share/enu</code> on Linux. <code>config.json</code> has a few configurable options:</p>
<p dir="auto"><code>mega_pixels</code>: The render resolution, in mega pixels. Increase for more detail. Decrease for better performance.</p>
<p dir="auto"><code>font_size</code>: The font size. DPI is currently ignored, so hidpi screens will require a higher number.</p>
<p dir="auto"><code>dock_icon_size</code>: Size of the icons in the dock. DPI is currently ignored, so hidpi screens will require a higher number.</p>
<p dir="auto"><code>world</code>: The world/project to load. Change this to create a new world.</p>
<p dir="auto"><code>show_stats</code>: Show FPS and other stats.</p>
<p dir="auto"><code>start_full_screen</code>: Whether to start Enu full screen, or in a window.</p>
<p dir="auto"><code>semicolon_as_colon</code>: Both <code>;</code> and <code>:</code> will be interpreted as <code>:</code>, allowing <code>:</code> to be typed without shift. Sometimes useful for new typists.</p>
<h2 tabindex="-1" dir="auto">TODO for 0.2</h2>
<h3 tabindex="-1" dir="auto">Pivot point</h3>
<p dir="auto">Currently it isn't possible to change the pivot point for a unit, and the default point isn't properly centered for
most builds, making it difficult to rotate builds nicely. Enu 0.2 will use the draw point for the pivot point, allowing
it to be moved, and will shift everything over 0.5m, allowing most builds to rotate in a balanced way. There will
also be a command to move the draw point (and thus the pivot point) in the exact center of a build.</p>
<h3 tabindex="-1" dir="auto">REPL?</h3>
<p dir="auto">We need a way to switch worlds without editing a config file. Adding a REPL may be the easiest way to accomplish
this, and is something I wanted to add anyway.</p>
<h3 tabindex="-1" dir="auto">Testing and bug fixes</h3>
<p dir="auto">Enu has been under heavy development for a year without a great deal of testing, so there are undoubtedly bugs. I
believe these will be minor, but there are probably a fair number of them.</p>
<h3 tabindex="-1" dir="auto">v0.2.x - v0.3</h3>
<ul>
<li> iOS support.</li>
<li> Move script execution off the main thread.</li>
<li> Inventory</li>
<li> Settings UI</li>
<li> Allow the editor pane and action bar to be resized from within Enu.</li>
<li> Better collision support</li>
<li> Blocks of any color</li>
<li> In game help</li>
<li> Easy way to switch worlds in-game</li>
<li> Support loading worlds from anywhere, not just the Enu data directory</li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Patterns for building LLM-based systems and products (361 pts)]]></title>
            <link>https://eugeneyan.com/writing/llm-patterns/</link>
            <guid>36965993</guid>
            <pubDate>Wed, 02 Aug 2023 01:54:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eugeneyan.com/writing/llm-patterns/">https://eugeneyan.com/writing/llm-patterns/</a>, See on <a href="https://news.ycombinator.com/item?id=36965993">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


<p>“There is a large class of problems that are easy to imagine and build demos for, but extremely hard to make products out of. For example, self-driving: It’s easy to demo a car self-driving around a block, but making it into a product takes a decade.” - <a href="https://twitter.com/eugeneyan/status/1672692174704766976" target="_blank">Karpathy</a></p>
<p>This post is about practical patterns for integrating large language models (LLMs) into systems and products. We’ll draw from academic research, industry resources, and practitioner know-how, and try to distill them into key ideas and practices.</p>
<p>There are seven key patterns. I’ve also organized them along the spectrum of improving performance vs. reducing cost/risk, and closer to the data vs. closer to the user.</p>
<ul>
<li><a href="#evals-to-measure-performance">Evals</a>: To measure performance</li>
<li><a href="#retrieval-augmented-generation-to-add-knowledge">RAG</a>: To add recent, external knowledge</li>
<li><a href="#fine-tuning-to-get-better-at-specific-tasks">Fine-tuning</a>: To get better at specific tasks</li>
<li><a href="#caching-to-reduce-latency-and-cost">Caching</a>: To reduce latency &amp; cost</li>
<li><a href="#guardrails-to-ensure-output-quality">Guardrails</a>: To ensure output quality</li>
<li><a href="#defensive-ux-to-anticipate--handle-errors-gracefully">Defensive UX</a>: To anticipate &amp; manage errors gracefully</li>
<li><a href="#collect-user-feedback-to-build-our-data-flywheel">Collect user feedback</a>: To build our data flywheel</li>
</ul>
<p><img src="https://eugeneyan.com/assets/llm-patterns-og.png" loading="lazy" title="Image" alt="Image"></p>
<p>LLM patterns: From data to user, from defensive to offensive (<a href="#conclusion">see connections between patterns</a>)</p>
<h2 id="evals-to-measure-performance">Evals: To measure performance</h2>
<p>Evaluations are a set of measurements used to assess a model’s performance on a task. They include benchmark data and metrics. From a HackerNews comment:</p>
<blockquote>
<p>How important evals are to the team is a major differentiator between folks shipping hot garbage and those seriously building products in the space.</p>
</blockquote>
<h3 id="why-evals">Why evals?</h3>
<p>Evals enable us to measure how well our system or product is doing and detect any regressions. (A system or product can be made up of multiple components such as LLMs, prompt templates, retrieved context, and parameters like temperature.) A representative set of evals takes us a step towards measuring system changes at scale. Without evals, we would be flying blind, or would have to visually inspect LLM outputs with each change.</p>
<h3 id="more-about-evals">More about evals</h3>
<p><strong>There are many benchmarks in the field of language modeling</strong>. Some notable ones are:</p>
<ul>
<li><strong><a href="https://arxiv.org/abs/2009.03300" target="_blank">MMLU</a></strong>: A set of 57 tasks that span elementary math, US history, computer science, law, and more. To perform well, models must possess extensive world knowledge and problem-solving ability.</li>
<li><strong><a href="https://github.com/EleutherAI/lm-evaluation-harness" target="_blank">EleutherAI Eval</a></strong>: Unified framework to test models via zero/few-shot settings on 200 tasks. Incorporates a large number of evals including BigBench, MMLU, etc.</li>
<li><strong><a href="https://arxiv.org/abs/2211.09110" target="_blank">HELM</a></strong>: Instead of specific tasks and metrics, HELM offers a comprehensive assessment of LLMs by evaluating them across domains. Metrics include accuracy, calibration, robustness, fairness, bias, toxicity, etc. Tasks include Q&amp;A, information retrieval, summarization, text classification, etc.</li>
<li><strong><a href="https://github.com/tatsu-lab/alpaca_eval" target="_blank">AlpacaEval</a></strong>: Automated evaluation framework which measures how often a strong LLM (e.g., GPT-4) prefers the output of one model over a reference model. Metrics include win rate, bias, latency, price, variance, etc. Validated to have high agreement with 20k human annotations.</li>
</ul>
<p>We can group metrics into two categories: context-dependent or context-free.</p>
<ul>
<li><strong>Context-dependent</strong>: These take context into account. They’re often proposed for a specific task; repurposing them for other tasks will require some adjustment.</li>
<li><strong>Context-free</strong>: These aren’t tied to the context when evaluating generated output; they only compare the output with the provided gold references. As they’re task agnostic, they’re easier to apply to a wide variety of tasks.</li>
</ul>
<p>To get a better sense of these metrics (and their potential shortfalls), we’ll explore a few of the commonly used metrics such as BLEU, ROUGE, BERTScore, and MoverScore.</p>
<p><strong><a href="https://dl.acm.org/doi/10.3115/1073083.1073135" target="_blank">BLEU</a> (Bilingual Evaluation Understudy)</strong> is a precision-based metric: It counts the number of n-grams in the generated output that also show up in the reference, and then divides it by the total number of words in the output. It’s predominantly used in machine translation and remains a popular metric due to its cost-effectiveness.</p>
<p>First, precision for various values of \(n\) is computed:</p><p>
\[\text{precision}_n = \frac{\sum_{p \in \text{output}} \sum_{\text{n-gram} \in p} \text{Count}_{\text{clip}} (\text{n-gram})}{\sum_{p \in \text{output}} \sum_{\text{n-gram} \in p} \text{Count}(\text{n-gram})}\]
</p><p>\(Count_{clip}(\text{n-gram})\) is clipped by the maximum number of times an n-gram appears in any corresponding reference sentence.</p><p>
\[\text{Count}_{\text{clip}}(n\text{-gram}) = \min \left(\text{matched } n\text{-gram count}, \max_{r \in R} \left(n\text{-gram count in } r\right)\right)\]
</p><p>Once we’ve computed precision at various \(n\), a final BLEU-N score is computed as the geometric mean of all the \(precision_n\) scores.</p>
<p>However, since precision relies solely on n-grams and doesn’t consider the length of the generated output, an output containing just one unigram of a common word (like a stop word) would achieve perfect precision. This can be misleading and encourage outputs that contain fewer words to increase BLEU scores. To counter this, a brevity penalty is added to penalize excessively short sentences.</p><p>
\[BP =
\begin{cases}
1 &amp; \text{if } |p| &gt; |r| \\
e^{1-\frac{|r|}{|p|}} &amp; \text{otherwise}
\end{cases}\]
</p><p>Thus, the final formula is:</p><p>
\[\text{BLEU-N} = BP \cdot \exp\left(\sum_{n=1}^{N} W_n \log(\text{precision}_n)\right)\]
</p><p><strong><a href="https://aclanthology.org/W04-1013/" target="_blank">ROUGE</a> (Recall-Oriented Understudy for Gisting Evaluation)</strong>: In contrast to BLEU, ROUGE is recall-oriented. It counts the number of words in the reference that also occur in the output. It’s typically used to assess automatic summarization tasks.</p>
<p>There are several ROUGE variants. ROUGE-N is most similar to BLEU in that it also counts the number of n-gram matches between the output and the reference.</p><p>
\[\text{ROUGE-N} = \frac{\sum_{s_r \in \text{references}} \sum_{n\text{-gram} \in s_r} \text{Count}_{\text{match}} (n\text{-gram})}{\sum_{s_r \in \text{references}} \sum_{n\text{-gram} \in s_r} \text{Count} (n\text{-gram})}\]
</p><p>Other variants include:</p>
<ul>
<li>ROUGE-L: This measures the longest common subsequence (LCS) between the output and the reference. It considers sentence-level structure similarity and zeros in on the longest series of co-occurring in-sequence n-grams.</li>
<li>ROUGE-S: This measures the skip-bigram between the output and reference. Skip-bigrams are pairs of words that maintain their sentence order regardless of the words that might be sandwiched between them.</li>
</ul>
<p><strong><a href="https://arxiv.org/abs/1904.09675" target="_blank">BERTScore</a></strong> is an embedding-based metric that uses cosine similarity to compare each token or n-gram in the generated output with the reference sentence. There are three components to BERTScore:</p>
<ul>
<li>Recall: Average cosine similarity between each token in the reference and its closest match in the generated output.</li>
<li>Precision: Average cosine similarity between each token in the generated output and its nearest match in the reference.</li>
<li>F1: Harmonic mean of recall and precision</li>
</ul><p>
\[Recall_{\text{BERT}} = \frac{1}{|r|} \sum_{i \in r} \max_{j \in p} \vec{i}^T \vec{j}, \quad Precision_{\text{BERT}} = \frac{1}{|p|} \sum_{j \in p} \max_{i \in r} \vec{i}^T \vec{j}\]
\[\text{BERTscore} = F_{\text{BERT}} = \frac{2 \cdot P_{\text{BERT}} \cdot R_{\text{BERT}}}{P_{\text{BERT}} + R_{\text{BERT}}}\]
</p><p>BERTScore is useful because it can account for synonyms and paraphrasing. Simpler metrics like BLEU and ROUGE can’t do this due to their reliance on exact matches. BERTScore has been shown to have better correlation for tasks such as image captioning and machine translation.</p>
<p><strong><a href="https://arxiv.org/abs/1909.02622" target="_blank">MoverScore</a></strong> also uses contextualized embeddings to compute the distance between tokens in the generated output and reference. But unlike BERTScore, which is based on one-to-one matching (or “high alignment”) of tokens, MoverScore allows for many-to-one matching (or “soft alignment”).</p>
<p><img src="https://eugeneyan.com/assets/mover-score.jpg" loading="lazy" title="BERTScore (left) vs. MoverScore (right)" alt="BERTScore (left) vs. MoverScore (right)"></p>
<p>BERTScore (left) vs. MoverScore (right; <a href="https://arxiv.org/abs/1909.02622" target="_blank">source</a>)</p>
<p>MoverScore enables the mapping of semantically related words in one sequence to their counterparts in another sequence. It does this by solving a constrained optimization problem that finds the minimum effort to transform one text into another. The idea is to measure the distance that words would have to move to convert one sequence to another.</p>
<p>However, there are several pitfalls to using these conventional benchmarks and metrics.</p>
<p>First, there’s <strong>poor correlation between these metrics and human judgments.</strong> BLEU, ROUGE, METEOR, and others have sometimes shown <a href="https://arxiv.org/abs/2008.12009" target="_blank">negative correlation with how humans evaluate fluency</a>. In particular, BLEU and ROUGE have <a href="https://arxiv.org/abs/2303.16634" target="_blank">low correlation with tasks that require creativity and diversity</a>. Furthermore, even for the same metric, there’s <a href="https://arxiv.org/abs/2008.12009" target="_blank">high variance reported across different studies</a>. This is possibly due to methodology differences in collecting human judgments or different metric parameter settings.</p>
<p>Second, these metrics often have <strong>poor adaptability to a wider variety of tasks</strong>. Adopting a metric proposed for one task to another is not always prudent. For example, exact match metrics such as BLEU and ROUGE are a poor fit for tasks like abstractive summarization or dialogue. Since they’re based on n-gram overlap between output and reference, they don’t make sense for a dialogue task where a wide variety of responses are possible. An output can have zero n-gram overlap with the reference but yet be a good response.</p>
<p>Third, even with recent benchmarks such as MMLU, <strong>the same model can get significantly different scores based on the eval implementation</strong>. <a href="https://huggingface.co/blog/evaluating-mmlu-leaderboard">Huggingface compared the original MMLU implementation</a> with the HELM and EleutherAI implementations and found that the same example could have different prompts across various implementations.</p>
<p><img src="https://eugeneyan.com/assets/mmlu-prompt.jpg" loading="lazy" title="Different prompts for the same question across MMLU implementations" alt="Different prompts for the same question across MMLU implementations"></p>
<p>Different prompts for the same question across MMLU implementations (<a href="https://huggingface.co/blog/evaluating-mmlu-leaderboard" target="_blank">source</a>)</p>
<p>Furthermore, the evaluation approach differed across all three benchmarks:</p>
<ul>
<li>Original MMLU: Compares predicted probabilities on the answers only (A, B, C, D)</li>
<li>HELM: Uses the next token probabilities from the model and picks the token with the highest probability, even if it’s <em>not</em> one of the options.</li>
<li>EleutherAI: Computes probability of the full answer sequence (i.e., a letter followed by the answer text) for each answer. Then, pick answer with highest probability.</li>
</ul>
<p><img src="https://eugeneyan.com/assets/mmlu-eval.jpg" loading="lazy" title="Different eval for the same question across MMLU implementations" alt="Different eval for the same question across MMLU implementations"></p>
<p>Different eval for the same question across MMLU implementations (<a href="https://huggingface.co/blog/evaluating-mmlu-leaderboard" target="_blank">source</a>)</p>
<p>As a result, even for the same eval, both absolute scores and model ranking can fluctuate widely depending on eval implementation. This means that model metrics aren’t truly comparable—even for the same eval—unless the eval’s implementation is identical down to minute details like prompts and tokenization.</p>
<p>Beyond the traditional evals above, <strong>an emerging trend is to use a strong LLM as a reference-free metric</strong> for generations from other LLMs. This means we may not need human judgments or gold references for evaluation.</p>
<p><strong><a href="https://arxiv.org/abs/2303.16634" target="_blank">G-Eval</a> is a framework that applies LLMs</strong> with Chain-of-Though (CoT) and a form-filling paradigm to <strong>evaluate LLM outputs</strong>. First, they provide a task introduction and evaluation criteria to an LLM and ask it to generate a CoT of evaluation steps. Then, to evaluate coherence in news summarization, they concatenate the prompt, CoT, news article, and summary and ask the LLM to output a score between 1 to 5. Finally, they use the probabilities of the output tokens from the LLM to normalize the score and take their weighted summation as the final result.</p>
<p><img src="https://eugeneyan.com/assets/geval.jpg" loading="lazy" title="Overview of G-Eval" alt="Overview of G-Eval"></p>
<p>Overview of G-Eval (<a href="https://arxiv.org/abs/2303.16634" target="_blank">source</a>)</p>
<p>They found that GPT-4 as an evaluator had a high Spearman correlation with human judgments (0.514), outperforming all previous methods. It also outperformed traditional metrics on aspects such as coherence, consistency, fluency, and relevance. On topical chat, it did better than traditional metrics such as ROUGE-L, BLEU-4, and BERTScore across several criteria such as naturalness, coherence, engagingness, and groundedness.</p>
<p><strong>The <a href="https://arxiv.org/abs/2306.05685" target="_blank">Vicuna</a> paper adopted a similar approach.</strong> They start by defining eight categories (writing, roleplay, extraction, reasoning, math, coding, STEM, and humanities/social science) before developing 10 questions for each category. Next, they generated answers from five chatbots: LLaMA, Alpaca, ChatGPT, Bard, and Vicuna. Finally, they asked GPT-4 to rate the quality of the answers based on helpfulness, relevance, accuracy, and detail.</p>
<p>Overall, they found that GPT-4 not only provided consistent scores but could also give detailed explanations for those scores. Under the single answer grading paradigm, GPT-4 had higher agreement with humans (85%) than the humans had amongst themselves (81%). This suggests that GPT-4’s judgment aligns close with the human evaluators.</p>
<p><strong><a href="https://arxiv.org/abs/2305.14314" target="_blank">QLoRA</a> also used an LLM to evaluate another LLM’s output.</strong> They asked GPT-4 to rate the performance of various models against gpt-3.5-turbo on the Vicuna benchmark. Given the responses from gpt-3.5-turbo and another model, GPT-4 was prompted to score both out of 10 and explain its ratings. They also measured performance via direct comparisons between models, simplifying the task to a three-class rating scheme that included ties.</p>
<p>To validate the automated evaluation, they collected human judgments on the Vicuna benchmark. Using Mechanical Turk, they enlisted two annotators for comparisons to gpt-3.5-turbo, and three annotators for pairwise comparisons. They found that human and GPT-4 ranking of models were largely in agreement, with a Spearman rank correlation of 0.55 at the model level. This provides an additional data point suggesting that LLM-based automated evals could be a cost-effective and reasonable alternative to human evals.</p>
<h3 id="how-to-apply-evals">How to apply evals?</h3>
<p><strong>Building solid evals should be the starting point</strong> for any LLM-based system or product (as well as conventional machine learning systems).</p>
<p>Unfortunately, classical metrics such as BLEU and ROUGE don’t make sense for more complex tasks such as abstractive summarization or dialogue. Furthermore, we’ve seen that benchmarks like MMLU are sensitive to how they’re implemented and measured. And to be candid, unless your LLM system is studying for a school exam, using MMLU as an eval doesn’t quite make sense.</p>
<p>Thus, instead of using off-the-shelf benchmarks, we can <strong>start by collecting a set of task-specific evals</strong> (i.e., prompt, context, expected outputs as references). These evals will then guide prompt engineering, model selection, fine-tuning, and so on. And as we tweak the system, we can run these evals to quickly measure improvements or regressions. Think of it as Eval Driven Development (EDD).</p>
<p>In addition to the evaluation dataset, we <strong>also need useful metrics</strong>. They help us distill performance changes into a single number that’s comparable across eval runs. And if we can simplify the problem, we can choose metrics that are easier to compute and interpret.</p>
<p>The simplest task is probably classification: If we’re using an LLM for classification-like tasks (e.g., toxicity detection, document categorization) or extractive QA without dialogue, we can rely on standard classification metrics such as recall, precision, PRAUC, etc. And if our task has no correct answer but we have references (e.g., machine translation, extractive summarization), we might have to rely on lossier reference metrics based on matching (BLEU, ROUGE) or semantic similarity (BERTScore, MoverScore).</p>
<p>However, these metrics may not work for more open-ended tasks such as abstractive summarization, dialogue, and others. Also, collecting human judgments can be slow and expensive. Thus, we may opt to lean on <strong>automated evaluations via a strong LLM</strong>. Relative to human judgments which are typically noisy (due to differing biases among annotators), LLM judgments tend to be less noisy (as the bias is more systematic) but more biased. But if we’re aware of these LLM biases, we can mitigate them accordingly:</p>
<ul>
<li>Position bias: LLMs like GPT-4 tend to favor the response in the first position. To mitigate this, we can evaluate the same pair of responses twice while swapping their order. If the same response is preferred in both orders, we mark it as a win. Otherwise, it’s a tie.</li>
<li>Verbosity bias: LLMs tend to favor longer, wordier responses over more concise ones, even if the latter is clearer and of higher quality. A possible solution is to ensure that comparison responses are similar in length.</li>
<li>Self-enhancement bias: LLMs have a slight bias towards their own answers. GPT-4 favors itself with a 10% higher win rate while Claude-v1 favors itself with a 25% higher win rate. To counter this, don’t use the same LLM for evaluation tasks.</li>
</ul>
<p>Another tip: Rather than asking an LLM for a direct evaluation (via giving a score), try giving it a reference and asking for a comparison. This helps with reducing noise.</p>
<p>Finally, sometimes the best eval is human eval aka vibe check. (Not to be confused with the poorly named code evaluation benchmark <a href="https://arxiv.org/abs/2107.03374" target="_blank">HumanEval</a>.) As mentioned in the <a href="https://www.latent.space/p/mosaic-mpt-7b#details" target="_blank">Latent Space podcast with MosaicML</a> (34th minute):</p>
<blockquote>
<p>The vibe-based eval cannot be underrated. … One of our evals was just having a bunch of prompts and watching the answers as the models trained and see if they change. Honestly, I don’t really believe that any of these eval metrics capture what we care about. One of our prompts was “suggest games for a 3-year-old and a 7-year-old to play” and that was a lot more valuable to see how the answer changed during the course of training. — Jonathan Frankle</p>
</blockquote>
<h2 id="retrieval-augmented-generation-to-add-knowledge">Retrieval-Augmented Generation: To add knowledge</h2>
<p>Retrieval-Augmented Generation (RAG) fetches relevant data from outside the foundation model and enhances the input with this data, providing richer context to improve output.</p>
<h3 id="why-rag">Why RAG?</h3>
<p>RAG helps reduce hallucination by grounding the model on the retrieved context, thus increasing factuality. In addition, it’s cheaper to keep retrieval indices up-to-date than to continuously pre-train an LLM. This cost efficiency makes it easier to provide LLMs with access to recent data via RAG. Finally, if we need to update or remove data such as biased or toxic documents, it’s more straightforward to update the retrieval index.</p>
<p>In short, RAG applies mature and simpler ideas from the field of information retrieval to support LLM generation. In a <a href="https://www.sequoiacap.com/article/llm-stack-perspective/" target="_blank">recent Sequoia survey</a>, 88% of respondents believe that retrieval will be a key component of their stack.</p>
<h3 id="more-about-rag">More about RAG</h3>
<p>Before diving into RAG, it helps to have a basic understanding of text embeddings. (Feel free to skip this section if you’re familiar with the subject.)</p>
<p>A text embedding is a <strong>compressed, abstract representation of text data</strong> where text of arbitrary length can be represented as a vector of numbers. It’s usually learned from a corpus of text such as Wikipedia. Think of them as a universal encoding for text, where <strong>similar items are close to each other while dissimilar items are farther apart</strong>.</p>
<p>A good embedding is one that does well on a downstream task, such as retrieving similar items. Huggingface’s <a href="https://huggingface.co/spaces/mteb/leaderboard" target="_blank">Massive Text Embedding Benchmark (MTEB)</a> scores various models on diverse tasks such as classification, clustering, retrieval, summarization, etc.</p>
<p>Quick note: While we mainly discuss text embeddings here, embeddings can take many modalities. For example, <a href="https://arxiv.org/abs/2103.00020" target="_blank">CLIP</a> is multimodal and embeds images and text in the same space, allowing us to find images most similar to an input text. We can also <a href="https://eugeneyan.com/writing/search-query-matching/#supervised-techniques-improves-modeling-of-our-desired-event" target="_blank">embed products based on user behavior</a> (e.g., clicks, purchases) or <a href="https://eugeneyan.com/writing/search-query-matching/#self-supervised-techniques-no-need-for-labels" target="_blank">graph relationships</a>.</p>
<p><strong>RAG has its roots in open-domain Q&amp;A.</strong> An early <a href="https://arxiv.org/abs/2005.04611" target="_blank">Meta paper</a> showed that retrieving relevant documents via TF-IDF and providing them as context to a language model (BERT) improved performance on an open-domain QA task. They converted each task into a cloze statement and queried the language model for the missing token.</p>
<p>Following that, <strong><a href="https://arxiv.org/abs/2004.04906" target="_blank">Dense Passage Retrieval</a> (DPR)</strong> showed that using dense embeddings (instead of a sparse vector space such as TF-IDF) for document retrieval can outperform strong baselines like Lucene BM25 (65.2% vs. 42.9% for top-5 accuracy.) They also showed that higher retrieval precision translates to higher end-to-end QA accuracy, highlighting the importance of upstream retrieval.</p>
<p>To learn the DPR embedding, they fine-tuned two independent BERT-based encoders on existing question-answer pairs. The passage encoder (\(E_p\)) embeds text passages into vectors while the query encoder (\(E_q\)) embeds questions into vectors. The query embedding is then used to retrieve \(k\) passages that are closest to the question.</p>
<p>They trained the encoders so that the dot-product similarity makes a good ranking function, and optimized the loss function as the negative log-likelihood of the positive passage. The DPR embeddings are optimized for maximum inner product between the question and relevant passage vectors. The goal is to learn a vector space such that pairs of questions and their relevant passages are close together.</p>
<p>For inference, they embed all passages (via \(E_p\)) and index them in FAISS offline. Then, given a question at query time, they compute the question embedding (via \(E_q\)), retrieve the top \(k\) passages via approximate nearest neighbors, and provide it to the language model (BERT) that outputs the answer to the question.</p>
<p><strong><a href="https://arxiv.org/abs/2005.11401" target="_blank">Retrieval Augmented Generation (RAG)</a></strong>, from which this pattern gets its name, highlighted the downsides of pre-trained LLMs. These include not being able to expand or revise memory, not providing insights into generated output, and hallucinations.</p>
<p>To address these downsides, they introduced RAG (aka semi-parametric models). Dense vector retrieval serves as the non-parametric component while a pre-trained LLM acts as the parametric component. They reused the DPR encoders to initialize the retriever and build the document index. For the LLM, they used BART, a 400M parameter seq2seq model.</p>
<p><img src="https://eugeneyan.com/assets/rag.jpg" loading="lazy" title="Overview of Retrieval Augmented Generation" alt="Overview of Retrieval Augmented Generation"></p>
<p>Overview of Retrieval Augmented Generation (<a href="https://arxiv.org/abs/2005.11401" target="_blank">source</a>)</p>
<p>During inference, they concatenate the input with the retrieved document. Then, the LLM generates \(\text{token}_i\) based on the original input, the retrieved document, and the previous \(i-1\) tokens. For generation, they proposed two approaches that vary in how the retrieved passages are used to generate output.</p>
<p>In the first approach, RAG-Sequence, the model uses the same document to generate the complete sequence. Thus, for \(k\) retrieved documents, the generator produces an output for each document. The probability of each output sequence is then marginalized (sum the probability of each output sequence in \(k\) and weigh it by the probability of each document being retrieved). Finally, the output sequence with the highest probability is selected.</p>
<p>On the other hand, RAG-Token can generate each token based on a <em>different</em> document. Given \(k\) retrieved documents, the generator produces a distribution for the next output token for each document before marginalizing (aggregating all the individual token distributions.). The process is then repeated for the next token. This means that, for each token generation, it can retrieve a different set of \(k\) relevant documents based on the original input <em>and</em> previously generated tokens. Thus, documents can have different retrieval probabilities and contribute differently to the next generated token.</p>
<p><a href="https://arxiv.org/abs/2007.01282" target="_blank"><strong>Fusion-in-Decoder (FiD)</strong></a> also uses retrieval with generative models for open-domain QA. It supports two methods for retrieval, BM25 (Lucene with default parameters) and DPR. FiD is named for how it performs fusion on the retrieved documents in the decoder only.</p>
<p><img src="https://eugeneyan.com/assets/fid.jpg" loading="lazy" title="Overview of Fusion-in-Decoder" alt="Overview of Fusion-in-Decoder"></p>
<p>Overview of Fusion-in-Decoder (<a href="https://arxiv.org/abs/2007.01282" target="_blank">source</a>)</p>
<p>For each retrieved passage, the title and passage are concatenated with the question. These pairs are processed independently in the encoder. They also add special tokens such as <code>question:</code>, <code>title:</code>, and <code>context:</code> before their corresponding sections. The decoder attends over the concatenation of these retrieved passages.</p>
<p>Because it processes passages independently in the encoder, it can scale to a large number of passages as it only needs to do self-attention over one context at a time. Thus, compute grows linearly (instead of quadratically) with the number of retrieved passages, making it more scalable than alternatives such as RAG-Token. Then, during decoding, the decoder processes the encoded passages jointly, allowing it to better aggregate context across multiple retrieved passages.</p>
<p><a href="https://arxiv.org/abs/2112.04426" target="_blank"><strong>Retrieval-Enhanced Transformer (RETRO)</strong></a> adopts a similar pattern where it combines a frozen BERT retriever, a differentiable encoder, and chunked cross-attention to generate output. What’s different is that RETRO does retrieval throughout the entire pre-training stage, and not just during inference. Furthermore, they fetch relevant documents based on chunks of the input. This allows for finer-grained, repeated retrieval during generation instead of only retrieving once per query.</p>
<p>For each input chunk (\(C_u\)), the \(k\) retrieved chunks \(RET(C_u)\) are fed into an encoder. The output is the encoded neighbors \(E^{j}_{u}\) where \(E^{j}_{u} = \text{Encoder}(\text{RET}(C_{u})^{j}, H_{u}) \in \mathbb{R}^{r \times d_{0}}\). Here, each chunk encoding is conditioned on \(H_u\) (the intermediate activations) and the activations of chunk \(C_u\) through cross-attention layers. In short, the encoding of the retrieved chunks depends on the attended activation of the input chunk. \(E^{j}_{u}\) is then used to condition the generation of the next chunk.</p>
<p><img src="https://eugeneyan.com/assets/retro.jpg" loading="lazy" title="Overview of RETRO" alt="Overview of RETRO"></p>
<p>Overview of RETRO (<a href="https://arxiv.org/abs/2112.04426" target="_blank">source</a>)</p>
<p>During retrieval, RETRO splits the input sequence into chunks of 64 tokens. Then, it finds text similar to the <em>previous</em> chunk to provide context to the <em>current</em> chunk. The retrieval index consists of two contiguous chunks of tokens, \(N\) and \(F\). The former is the neighbor chunk (64 tokens) which is used to compute the key while the latter is the continuation chunk (64 tokens) in the original document.</p>
<p>Retrieval is based on approximate \(k\)-nearest neighbors via \(L_2\) distance (euclidean) on BERT embeddings. (Interesting departure from the usual cosine or dot product similarity.) The retrieval index, built on SCaNN, can query a 2T token database in 10ms.</p>
<p>They also demonstrated how to RETRO-fit existing baseline models. By freezing the pre-trained weights and only training the chunked cross-attention and neighbor encoder parameters (&lt; 10% of weights for a 7B model), they can enhance transformers with retrieval while only requiring 6M training sequences (3% of pre-training sequences). RETRO-fitted models were able to surpass the performance of baseline models and achieved performance close to that of RETRO trained from scratch.</p>
<p><img src="https://eugeneyan.com/assets/retrofit.jpg" loading="lazy" title="Performance from RETRO-fitting a pre-trained model" alt="Performance from RETRO-fitting a pre-trained model"></p>
<p>Performance from RETRO-fitting a pre-trained model (<a href="https://arxiv.org/abs/2112.04426" target="_blank">source</a>)</p>
<p><strong><a href="https://arxiv.org/abs/2203.05115" target="_blank">Internet-augmented LMs</a></strong> proposes using a humble “off-the-shelf” search engine to augment LLMs. First, they retrieve a set of relevant documents via Google Search. Since these retrieved documents tend to be long (average length 2,056 words), they chunk them into paragraphs of six sentences each. Finally, they embed the question and paragraphs via TF-IDF and applied cosine similarity to rank the most relevant paragraphs for each query.</p>
<p><img src="https://eugeneyan.com/assets/internet-llm.jpg" loading="lazy" title="Overview of internet-augmented LLMs" alt="Overview of internet-augmented LLMs"></p>
<p>Overview of internet-augmented LLMs (<a href="https://arxiv.org/abs/2203.05115" target="_blank">source</a>)</p>
<p>The retrieved paragraphs are used to condition the LLM via few-shot prompting. They adopt the conventional \(k\)-shot prompting (\(k=15\)) for closed-book QA (only questions and answers) and extend it with an evidence paragraph, such that each context is evidence, question, and answer.</p>
<p>For the generator, they used Gopher, a 280B parameter model trained on 300B tokens. For each question, they generated four candidate answers based on each of the 50 retrieved paragraphs. Finally, they select the best answer by estimating the answer probability via several methods including direct inference, RAG, noisy channel inference, and Product-of-Experts (PoE). PoE consistently performed the best.</p>
<p>RAG has also been <strong>applied to non-QA tasks such as code generation</strong>. While <strong><a href="https://arxiv.org/abs/2305.07922" target="_blank">CodeT5+</a></strong> can be used as a standalone generator, when combined with RAG, it significantly outperforms similar models in code generation.</p>
<p>To assess the impact of RAG on code generation, they evaluate the model in three settings:</p>
<ul>
<li>Retrieval-based: Fetch the top-1 code sample as the prediction</li>
<li>Generative-only: Output code based on the decoder only</li>
<li>Retrieval-augmented: Append top-1 code sample to encoder input before code generation via the decoder.</li>
</ul>
<p><img src="https://eugeneyan.com/assets/codet5.jpg" loading="lazy" title=">Overview of RAG for CodeT5+" alt=">Overview of RAG for CodeT5+"></p>
<p>Overview of RAG for CodeT5+ (<a href="https://arxiv.org/abs/2305.07922" target="_blank">source</a>)</p>
<p>As a qualitative example, they showed that retrieved code provides crucial context (e.g., use <code>urllib3</code> for an HTTP request) and guides the generative process towards more correct predictions. In contrast, the generative-only approach returns incorrect output that only captures the concepts of “download” and “compress”.</p>
<p><strong>What if we don’t have relevance judgments for query-passage pairs?</strong> Without them, we would not be able to train the bi-encoders that embed the queries and documents in the same embedding space, where relevance is represented by the inner product. <strong><a href="https://arxiv.org/abs/2212.10496" target="_blank">Hypothetical document embeddings (HyDE)</a></strong> suggests a solution.</p>
<p><img src="https://eugeneyan.com/assets/hyde.jpg" loading="lazy" title="Overview of HyDE" alt="Overview of HyDE"></p>
<p>Overview of HyDE (<a href="https://arxiv.org/abs/2212.10496" target="_blank">source</a>)</p>
<p>Given a query, HyDE first prompts an LLM, such as InstructGPT, to generate a hypothetical document. Then, an unsupervised encoder, such as Contriver, encodes the document into an embedding vector. Finally, the inner product is computed between the <em>hypothetical</em> document and the corpus, and the most similar <em>real</em> documents are retrieved.</p>
<p>The expectation is that the encoder’s dense bottleneck serves as a lossy compressor and the extraneous, non-factual details are excluded via the embedding. This reframes the relevance modeling problem from a representation learning task to a generation task.</p>
<h3 id="how-to-apply-rag">How to apply RAG</h3>
<p>From <a href="https://eugeneyan.com/writing/obsidian-copilot/" target="_blank">personal experience</a>, I’ve found that hybrid retrieval (traditional search index + embedding-based search) works better than either alone.</p>
<p>Why not embedding-based search only? While it’s great in many instances, there are situations where it falls short, such as:</p>
<ul>
<li>Searching for a person or object’s name (e.g., Eugene, Kaptir 2.0)</li>
<li>Searching for an acronym or phrase (e.g., RAG, RLHF)</li>
<li>Searching for an ID (e.g., <code>gpt-3.5-turbo</code>, <code>titan-xlarge-v1.01</code>)</li>
</ul>
<p>But keyword search has its limitations too. It only models simple word frequencies and doesn’t capture semantic or correlation information. Thus, it doesn’t deal well with synonyms or hypernyms (i.e., words that represent a generalization). This is where combining it with semantic search is complementary.</p>
<p>In addition, with a conventional search index, we can use metadata to refine results. For example, we can use date filters to prioritize newer documents or narrow our search to a specific time period. And if the search is related to e-commerce, filters on average rating or categories are helpful. Having metadata also comes in handy for downstream ranking, such as prioritizing documents that are cited more, or boosting products by their sales volume.</p>
<p><strong>With regard to embeddings</strong>, the seemingly popular approach is to use <a href="https://openai.com/blog/new-and-improved-embedding-model" target="_blank"><code>text-embedding-ada-002</code></a>. Its benefits include ease of use via an API and not having to maintain our own embedding infra or self-host embedding models. Nonetheless, experience and anecdotes suggest it’s not as good for retrieval.</p>
<p>The OG embedding approaches include Word2vec and <a href="https://fasttext.cc/" target="_blank">fastText</a>. FastText is an open-source, lightweight library that enables users to leverage pre-trained embeddings or train new embedding models. It comes with pre-trained embeddings for 157 languages and is extremely fast, even without a GPU. It’s my go-to for early-stage proof of concepts.</p>
<p>Another good baseline is <a href="https://github.com/UKPLab/sentence-transformers" target="_blank">sentence-transformers</a>. It makes it simple to compute embeddings for sentences, paragraphs, and even images. It’s based on workhorse transformers such as BERT and RoBERTa and is available in more than 100 languages.</p>
<p>More recently, instructor models have shown SOTA performance. During training, these models prepend the task description to the text. Then, when embedding new text, we simply have to describe the task to get task-specific embeddings. (Not that different from instruction tuning for embedding models IMHO.)</p>
<p>Take the <a href="https://arxiv.org/abs/2212.03533" target="_blank">E5</a> family of models, for instance. For open QA and information retrieval, we simply prepend documents in the index with <code>passage:</code>, and prepend queries with <code>query:</code>. If the task is symmetric (e.g., semantic similarity, paraphrase retrieval) or if we want to use embeddings as features (e.g., classification, clustering), we just use the <code>query:</code> prefix.</p>
<p>The <a href="https://arxiv.org/abs/2212.09741" target="_blank">Instructor</a> model takes it a step further, allowing users to customize the prepended prompt: “Represent the <code>domain</code> <code>task_type</code> for the <code>task_objective</code>:” For example, “Represent the Wikipedia document for retrieval:”. (The domain and task objective are optional). This brings the concept of prompt tuning into the field of text embedding.</p>
<p>Finally, as of Aug 1st, the top embedding model on the <a href="https://huggingface.co/spaces/mteb/leaderboard" target="_blank">MTEB Leaderboard</a> is the <a href="https://huggingface.co/thenlper/gte-large" target="_blank">GTE</a> family of models by Alibaba DAMO Academy. The top performing model’s size is half of the next best model <code>e5-large-v2</code> (0.67GB vs 1.34GB). In 2nd position is <code>gte-base</code> with a model size of only 0.22GB and embedding dimension of 768. (H/T <a href="https://twitter.com/NirantK" target="_blank">Nirant</a>.)</p>
<p>To retrieve documents with low latency at scale, we use approximate nearest neighbors (ANN). It optimizes for retrieval speed and returns the approximate (instead of exact) top \(k\) most similar neighbors, trading off a little accuracy loss for a large speed up.</p>
<p>ANN embedding indices are data structures that let us do ANN searches efficiently. At a high level, they build partitions over the embedding space so we can quickly home in on the specific space where the query vector is. Some popular techniques include:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing" target="_blank">Locality Sensitive Hashing</a> (LSH): The core idea is to create hash functions such that similar items are likely to end up in the same hash bucket. By only needing to check the relevant buckets, we can perform ANN queries efficiently.</li>
<li><a href="https://github.com/facebookresearch/faiss" target="_blank">Facebook AI Similarity Search</a> (FAISS): It uses a combination of quantization and indexing for efficient retrieval, supports both CPU and GPU, and can handle billions of vectors due to its efficient use of memory.</li>
<li><a href="https://github.com/nmslib/hnswlib" target="_blank">Hierarchical Navigable Small Worlds</a> (HNSW): Inspired by “six degrees of separation”, it builds a hierarchical graph structure that embodies the small world phenomenon. Here, most nodes can be reached from any other node via a minimum number of hops. This structure allows HNSW to initiate queries from broader, coarser approximations and progressively narrow the search at lower levels.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/scann" target="_blank">Scalable Nearest Neighbors</a> (ScaNN): ANN is done via a two-step process. First, coarse quantization reduces the search space. Then, fine-grained search is done within the reduced set. Best recall/latency trade-off I’ve seen.</li>
</ul>
<p>When evaluating an ANN index, some factors to consider include:</p>
<ul>
<li>Recall: How does it fare against exact nearest neighbors?</li>
<li>Latency/throughput: How many queries can it handle per second?</li>
<li>Memory footprint: How much RAM is required to serve an index?</li>
<li>Ease of adding new items: Can new items be added without having to reindex all documents (LSH) or does the index need to be rebuilt (ScaNN)?</li>
</ul>
<p>No single framework is better than all others in every aspect. Thus, start by defining your functional and non-functional requirements before benchmarking. Personally, I’ve found ScaNN to be outstanding in the recall-latency trade-off (see benchmark graph <a href="https://eugeneyan.com/writing/real-time-recommendations/#how-to-design-and-implement-an-mvp" target="_blank">here</a>).</p>
<h2 id="fine-tuning-to-get-better-at-specific-tasks">Fine-tuning: To get better at specific tasks</h2>
<p>Fine-tuning refers to the process of taking a pre-trained model (that has already been trained with a vast amount of data) and further refining it on a specific task. The intent is to harness the knowledge that the model has already acquired during its pre-training and apply it to a specific task, usually involving a smaller, task-specific, dataset.</p>
<p>The term “fine-tuning” is quite broad and can refer to several concepts such as:</p>
<ul>
<li><strong>Continued pre-training</strong>: With domain-specific data, apply the same pre-training regime (next token prediction, masked language modeling) on the base model.</li>
<li><strong>Instruction fine-tuning</strong>: The pre-trained (base) model is fine-tuned on examples of instruction-output pairs to follow instructions, answer questions, be waifu, etc.</li>
<li><strong>Single-task fine-tuning</strong>: The pre-trained model is honed for a narrow and specific task such as toxicity detection or summarization, similar to BERT and T5.</li>
<li><strong>Reinforcement learning with human feedback (RLHF)</strong>: This combines instruction fine-tuning with reinforcement learning. It requires collecting human preferences (e.g., pairwise comparisons) which are then used to train a reward model. The reward model is then used to further fine-tune the instructed LLM via RL techniques such as proximal policy optimization (PPO).</li>
</ul>
<p>We’ll mainly focus on single-task and instruction fine-tuning here.</p>
<h3 id="why-fine-tuning">Why fine-tuning?</h3>
<p>Fine-tuning an open LLM is becoming an increasingly viable alternative to using a 3rd-party, cloud-based LLM for several reasons.</p>
<p><strong>Performance &amp; control:</strong> Fine-tuning can improve the performance of an off-the-shelf base model, and may even surpass a 3rd-party LLM. It also provides greater control over LLM behavior, resulting in more a robust system or product. Overall, fine-tuning enables us to build products that are differentiated from simply using 3rd-party or open LLMs.</p>
<p><strong>Modularization:</strong> Single-task fine-tuning lets us to use an army of smaller models that each specialize on their own tasks. Via this setup, a system can be modularized into individual models for tasks like content moderation, extraction, summarization, etc. Also, given that each model only has to focus on a narrow set of tasks, there’s reduced concern about the alignment tax, where fine-tuning a model on one task reduces performance on other tasks.</p>
<p><strong>Reduced dependencies:</strong> By fine-tuning and hosting our own models, we can reduce legal concerns about proprietary data (e.g., PII, internal documents and code) being exposed to external APIs. It also gets around constraints that come with 3rd-party LLMs such as rate-limiting, high costs, or overly restrictive safety filters. By fine-tuning and hosting our own LLMs, we can ensure data doesn’t leave our network, and can scale throughput as needed.</p>
<h3 id="more-about-fine-tuning">More about fine-tuning</h3>
<p>Why do we need to fine-tune a <em>base</em> model? At the risk of oversimplifying, base models are primarily optimized to predict the next word based on the corpus they’re trained on. Hence, they aren’t naturally adept at following instructions or answering questions. When posed a question, they tend to respond with further questions. Thus, we perform instruction fine-tuning so they learn to respond appropriately.</p>
<p>However, fine-tuning isn’t without its challenges. First, we <strong>need a significant volume of demonstration data</strong>. For instance, in the <a href="https://arxiv.org/abs/2203.02155" target="_blank">InstructGPT paper</a>, they used 13k instruction-output samples for supervised fine-tuning, 33k output comparisons for reward modeling, and 31k prompts without human labels as input for RLHF.</p>
<p>Furthermore, fine-tuning comes with an alignment tax—the process can lead to <strong>lower performance on certain critical tasks</strong>. (There’s no free lunch after all.) The same InstructGPT paper found that RLHF led to performance regressions (relative to the GPT-3 base model) on public NLP tasks like SQuAD, HellaSwag, and WMT 2015 French to English. (A workaround is to have several smaller, specialized models that excel at narrow tasks.)</p>
<p>Fine-tuning is similar to the concept of transfer learning. As defined in Wikipedia: “Transfer learning is a technique in machine learning in which knowledge learned from a task is re-used to boost performance on a related task.” Several years ago, transfer learning made it easy for me to apply ResNet models trained on ImageNet to <a href="https://eugeneyan.com/writing/image-categorization-is-now-live/" target="_blank">classify fashion products</a> and <a href="https://eugeneyan.com/writing/image-search-is-now-live/" target="_blank">build image search</a>.</p>
<p><strong><a href="https://arxiv.org/abs/1801.06146" target="_blank">ULMFit</a></strong> is one of the earlier papers to apply transfer learning to text. They established the protocol of self-supervised pre-training (on unlabeled data) followed by fine-tuning (on labeled data). They used AWS-LSTM, an LSTM variant with dropout at various gates.</p>
<p><img src="https://eugeneyan.com/assets/ulmfit.jpg" loading="lazy" title="Overview of ULMFit" alt="Overview of ULMFit"></p>
<p>Overview of ULMFit (<a href="https://arxiv.org/abs/1801.06146" target="_blank">source</a>)</p>
<p>During pre-training (next word prediction), the model is trained on wikitext-103 which contains 28.6 Wikipedia articles and 103M words. Then, during target task fine-tuning, the LM is fine-tuned with data from the domain of the specific task. Finally, during classifier fine-tuning, the model is augmented with two additional linear blocks and fine-tuned on the target classification tasks which includes sentiment analysis, question classification, and topic classification.</p>
<p>Since then, the pre-training followed by fine-tuning paradigm has driven much progress in language modeling. <strong><a href="https://arxiv.org/abs/1810.04805" target="_blank">Bidirectional Encoder Representations from Transformers (BERT; encoder only)</a></strong> was pre-trained via masked language modeling and next sentence prediction on English Wikipedia and BooksCorpus. It was then fine-tuned on task-specific inputs and labels for single-sentence classification, sentence pair classification, single-sentence tagging, and Q&amp;A.</p>
<p><img src="https://eugeneyan.com/assets/bert.jpg" loading="lazy" title="Overview of BERT" alt="Overview of BERT"></p>
<p>Overview of BERT (<a href="https://arxiv.org/abs/1810.04805" target="_blank">source</a>)</p>
<p><strong><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank">Generative Pre-trained Transformers (GPT; decoder only)</a></strong> was first pre-trained on BooksCorpus via next token prediction. This was followed by single-task fine-tuning for tasks such as text classification, textual entailment, similarity, and Q&amp;A. Interestingly, they found that including language modeling as an auxiliary objective helped the model generalize and converge faster during training.</p>
<p><img src="https://eugeneyan.com/assets/gpt.jpg" loading="lazy" title="Overview of GPT" alt="Overview of GPT"></p>
<p>Overview of GPT (<a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pd" target="_blank">source</a>)</p>
<p><strong><a href="https://arxiv.org/abs/1910.10683" target="_blank">Text-to-text Transfer Transformer (T5; encoder-decoder)</a></strong> was pre-trained on the Colossal Clean Crawled Corpus (C4), a cleaned version of the Common Crawl from April 2019. It employed the same denoising objective as BERT, namely masked language modeling. It was then fine-tuned on tasks such as text classification, abstractive summarization, Q&amp;A, and machine translation.</p>
<p><img src="https://eugeneyan.com/assets/t5.jpg" loading="lazy" title="Overview of T5" alt="Overview of T5"></p>
<p>Overview of T5 (<a href="https://arxiv.org/abs/1910.10683" target="_blank">source</a>)</p>
<p>But unlike ULMFIt, BERT, and GPT which used different classifier heads for downstream tasks, T5 represented downstream tasks as text-to-text only. For example, a translation task would have input text starting with <code>Translation English to German:</code>, while a summarization task might start with <code>Summarize:</code> or <code>TL;DR:</code>. The prefix essentially became a hyperparameter (first instance of prompt engineering?) This design choice allowed them to use a single fine-tuned model across a variety of downstream tasks.</p>
<p><strong><a href="https://arxiv.org/abs/2203.02155" target="_blank">InstructGPT</a></strong> expanded this idea of single-task fine-tuning to instruction fine-tuning. The base model was GPT-3, pre-trained on internet data including Common Crawl, WebText, Books, and Wikipedia. It then applied supervised fine-tuning on demonstrations of desired behavior (instruction and output). Next, it trained a reward model on the dataset of comparisons. Finally, it optimized the instructed model against the reward model via PPO, with this last stage focusing more on alignment than specific task performance.</p>
<p><img src="https://eugeneyan.com/assets/instructgpt.jpg" loading="lazy" title="Overview of fine-tuning steps in InstructGPT" alt="Overview of fine-tuning steps in InstructGPT"></p>
<p>Overview of fine-tuning steps in InstructGPT (<a href="https://arxiv.org/abs/2203.02155" target="_blank">source</a>)</p>
<p>Next, let’s move from fine-tuned models to fine-tuning techniques.</p>
<p><strong><a href="https://arxiv.org/abs/2104.08691" target="_blank">Soft prompt tuning</a></strong> prepends a trainable tensor to the model’s input embeddings, essentially creating a soft prompt. Unlike discrete text prompts, soft prompts can be learned via backpropagation, meaning they can be fine-tuned to incorporate signals from any number of labeled examples.</p>
<p>Next, there’s <strong><a href="https://arxiv.org/abs/2101.00190" target="_blank">prefix tuning</a></strong>. Instead of adding a soft prompt to the model input, it prepends trainable parameters to the hidden states of all transformer blocks. During fine-tuning, the LM’s original parameters are kept frozen while the prefix parameters are updated.</p>
<p><img src="https://eugeneyan.com/assets/prefix.jpg" loading="lazy" title="Overview of prefix-tuning" alt="Overview of prefix-tuning"></p>
<p>Overview of prefix-tuning (<a href="https://arxiv.org/abs/2101.00190" target="_blank">source</a>)</p>
<p>The paper showed that this achieved performance comparable to full fine-tuning despite requiring updates on just 0.1% of parameters. Moreover, in settings with limited data and extrapolation to new topics, it outperformed full fine-tuning. One hypothesis is that the fewer parameters involved helped reduce overfitting on smaller target datasets.</p>
<p>There’s also the <strong><a href="https://arxiv.org/abs/1902.00751" target="_blank">adapter</a></strong> technique. This method adds fully connected network layers twice to each transformer block, after the attention layer and after the feed-forward network layer. On GLUE, it’s able to achieve within 0.4% of the performance of full fine-tuning by just adding 3.6% parameters per task.</p>
<p><img src="https://eugeneyan.com/assets/adapter.jpg" loading="lazy" title="Overview of adapters" alt="Overview of adapters"></p>
<p>Overview of adapters (<a href="https://arxiv.org/abs/1902.00751" target="_blank">source</a>)</p>
<p><strong><a href="https://arxiv.org/abs/2106.09685" target="_blank">Low-Rank Adaptation (LoRA)</a></strong> is a technique where adapters are designed to be the product of two low-rank matrices. It was inspired by <a href="https://arxiv.org/abs/2012.13255" target="_blank">Aghajanyan et al.</a> which showed that, when adapting to a specific task, pre-trained language models have a low intrinsic dimension and can still learn efficiently despite a random projection into a smaller subspace. Thus, they hypothesize that weight updates during adaption also have low intrinsic rank.</p>
<p><img src="https://eugeneyan.com/assets/lora.jpg" loading="lazy" title="Overview of LoRA" alt="Overview of LoRA"></p>
<p>Overview of LoRA (<a href="https://arxiv.org/abs/2106.09685" target="_blank">source</a>)</p>
<p>Similar to prefix tuning, they found that LoRA outperformed several baselines including full fine-tuning. Again, the hypothesis is that LoRA, thanks to its reduced rank, provides implicit regularization. In contrast, full fine-tuning, which updates all weights, could be prone to overfitting.</p>
<p><strong><a href="https://arxiv.org/abs/2305.14314" target="_blank">QLoRA</a></strong> builds on the idea of LoRA. But instead of using the full 16-bit model during fine-tuning, it applies a 4-bit quantized model. It introduced several innovations such as 4-bit NormalFloat (to quantize models), double quantization (for additional memory savings), and paged optimizers (that prevent OOM errors by transferring data to CPU RAM when the GPU runs out of memory).</p>
<p><img src="https://eugeneyan.com/assets/qlora.jpg" loading="lazy" title="Overview of QLoRA" alt="Overview of QLoRA"></p>
<p>Overview of QLoRA (<a href="https://arxiv.org/abs/2305.14314" target="_blank">source</a>)</p>
<p>As a result, QLoRA reduces the average memory requirements for fine-tuning a 65B model from &gt; 780GB memory to a more manageable 48B without degrading runtime or predictive performance compared to a 16-bit fully fine-tuned baseline.</p>
<p>(Fun fact: During a meetup with Tim Dettmers, he quipped that double quantization was “a bit of a silly idea but works perfectly.” Hey, if it works, it works.)</p>
<h3 id="how-to-apply-fine-tuning">How to apply fine-tuning?</h3>
<p>The first step is to <strong>collect demonstration data/labels</strong>. These could be for straightforward tasks, such as document classification, entity extraction, or summarization, or they could be more complex such as Q&amp;A or dialogue. Some ways to collect this data include:</p>
<ul>
<li><strong>Via experts or crowd-sourced human annotators</strong>: While this is expensive and slow, it usually leads to higher-quality data with <a href="https://eugeneyan.com/writing/llm-patterns/(/writing/labeling-guidelines/)%7B:target=%22_blank%22%7D">good guidelines</a>.</li>
<li><strong>Via user feedback</strong>: This can be as simple as asking users to select attributes that describe a product, rating model responses with thumbs up or down (e.g., ChatGPT), or logging which images users choose to download (e.g., Midjourney).</li>
<li><strong>Query larger open models with permissive licenses</strong>: With prompt engineering, we might be able to elicit reasonable demonstration data from a larger model (Falcon 40B Instruct) that can be used to fine-tune a smaller model.</li>
<li><strong>Reuse open-source data</strong>: If your task can be framed as a natural language inference (NLI) task, we could fine-tune a model to perform NLI using <a href="https://cims.nyu.edu/~sbowman/multinli/">MNLI data</a>. Then, we can continue fine-tuning the model on internal data to classify inputs as entailment, neutral, or contradiction.</li>
</ul>
<p>Note: Some LLM terms prevent users from using their output to train other models.</p>
<ul>
<li><a href="https://openai.com/policies/terms-of-use" target="_blank">OpenAI Terms of Use</a> (Section 2c, iii): You may not use output from the Services to develop models that compete with OpenAI.</li>
<li><a href="https://ai.meta.com/llama/license/" target="_blank">LLaMA 2 Community License Agreement</a> (Section 1b-v): You will not use the Llama Materials or any output or results of the Llama Materials to improve any other large language model (excluding Llama 2 or derivative works thereof).</li>
</ul>
<p>The next step is to <strong>define evaluation metrics</strong>. We’ve discussed this in a <a href="#evals-to-measure-performance-scalably">previous section</a>.
make
Then, <strong>select a pre-trained model.</strong> There are <a href="https://github.com/eugeneyan/open-llms" target="_blank">several open-source pre-trained models with permissive licenses</a> to choose from. Excluding Llama 2 (since it isn’t fully commercial use), Falcon-40B is known to be the best-performing model. Nonetheless, I’ve found it unwieldy to fine-tune and serve in production given how heavy it is.</p>
<p>Instead, I’m inclined to use smaller models like the Falcon-7B. And if we can simplify and frame the task more narrowly, BERT, RoBERTA, and BART are solid picks for classification and natural language inference tasks. Beyond that, Flan-T5 is a reliable baseline for translation, summarization, headline generation, etc.</p>
<p>We may also need to <strong>update the model architecture</strong>. This is needed when the pre-trained model’s architecture doesn’t align with the task. For example, we might need to update the classification heads on BERT or T5 to match our task. Tip: If the task is a simple binary classification task, NLI models can work out of the box. Entailment is mapped to positive, contradiction is mapped to negative, while the neural label can indicate uncertainty.</p>
<p><strong>Then, pick a fine-tuning approach.</strong> LoRA and QLoRA are good places to start. But if your fine-tuning is more intensive, such as continued pre-training on new domain knowledge, you may find full fine-tuning necessary.</p>
<p><strong>Finally, basic hyperparameter tuning.</strong> Generally, most papers focus on learning rate, batch size, and number of epochs (see LoRA, QLoRA). And if we’re using LoRA, we might want to tune the rank parameter (though the QLoRA paper found that different rank and alpha led to similar results.) Other hyperparameters include input sequence length, loss type (contrastive loss vs. token match), and data ratios (like the mix of pre-training or demonstration data, or the ratio of positive to negative examples, among others).</p>
<h2 id="caching-to-reduce-latency-and-cost">Caching: To reduce latency and cost</h2>
<p>Caching is a technique to store data that has been previously retrieved or computed. This way, future requests for the same data can be served faster. In the space of serving LLM generations, the popularized approach is to cache the LLM response keyed on the embedding of the input request. Then, for each new request, if a semantically similar request is received, we can serve the cached response.</p>
<p>For some practitioners, this sounds like “<a href="https://twitter.com/HanchungLee/status/1681146845186363392" target="_blank">a disaster waiting to happen.</a>” I’m inclined to agree. Thus, I think the key to adopting this pattern is figuring out how to cache safely, instead of solely depending on semantic similarity.</p>
<h3 id="why-caching">Why caching?</h3>
<p>Caching can significantly reduce latency for responses that have been served before. In addition, by eliminating the need to compute a response for the same input again and again, we can reduce the number of LLM requests and thus save cost. Also, there are certain use cases that do not support latency on the order of seconds. Thus, pre-computing and caching may be the only way to serve those use cases.</p>
<h3 id="more-about-caching">More about caching</h3>
<p>A cache is a high-speed storage layer that stores a subset of data that’s accessed more frequently. This lets us to serve these requests faster via the cache instead of the data’s primary storage (e.g., search index, relational database). Overall, caching enables efficient reuse of previously fetched or computed data. (More about <a href="https://aws.amazon.com/caching/" target="_blank">caching</a> and <a href="https://aws.amazon.com/caching/best-practices/" target="_blank">best practices</a>.)</p>
<p>An example of caching for LLMs is <a href="https://github.com/zilliztech/GPTCache" target="_blank">GPTCache</a>.</p>
<p><img src="https://eugeneyan.com/assets/gptcache.jpg" loading="lazy" title="Overview of GPTCache" alt="Overview of GPTCache"></p>
<p>Overview of GPTCache (<a href="https://github.com/zilliztech/GPTCache" target="_blank">source</a>)</p>
<p>When a new request is received:</p>
<ul>
<li>Embedding generator: This embeds the request via various models such as OpenAI’s <code>text-embedding-ada-002</code>, FastText, Sentence Transformers, and more.</li>
<li>Similarity evaluator: This computes the similarity of the request via the vector store and then provides a distance metric. The vector store can either be local (FAISS, Hnswlib) or cloud-based. It can also compute similarity via a model.</li>
<li>Cache storage: If the request is similar, the cached response is fetched and served.</li>
<li>LLM: If the request isn’t similar enough, it gets passed to the LLM which then generates the result. Finally, the response is served and cached for future use.</li>
</ul>
<p>Redis also shared a <a href="https://www.youtube.com/live/9VgpXcfJYvw?feature=share&amp;t=1517" target="_blank">similar example</a>. Some teams go as far as precomputing all the queries they anticipate receiving. Then, they set a similarity threshold on which queries are similar enough to warrant a cached response.</p>
<h3 id="how-to-apply-caching">How to apply caching?</h3>
<p><strong>We should start with having a good understanding of user request patterns</strong>. This allows us to design the cache thoughtfully so it can be applied reliably.</p>
<p>First, let’s consider a non-LLM example. Imagine we’re caching product prices for an e-commerce site. During checkout, is it safe to display the (possibly outdated) cached price? Probably not, since the price the customer sees during checkout should be the same as the final amount they’re charged. Caching isn’t appropriate here as we need to ensure consistency for the customer.</p>
<p>Now, bringing it back to LLM responses. Imagine we get a request for a summary of “Mission Impossible 2” that’s semantically similar enough to “Mission Impossible 3”. If we’re looking up cache based on semantic similarity, we could serve the wrong response.</p>
<p>We also need to consider if caching is effective for the usage pattern. One way to quantify this is via the cache hit rate (percentage of requests served directly from the cache). If the usage pattern is uniformly random, the cache would need frequent updates. Thus, the effort to keep the cache up-to-date could negate any benefit a cache has to offer. On the other hand, if the usage follows a power law where a small proportion of unique requests account for the majority of traffic (e.g., search queries, product views), then caching could be an effective strategy.</p>
<p>Beyond semantic similarity, we could also explore caching based on:</p>
<ul>
<li>Item IDs: This applies when we pre-compute <a href="https://www.cnbc.com/2023/06/12/amazon-is-using-generative-ai-to-summarize-product-reviews.html" target="_blank">summaries of product reviews</a> or generate a summary for an entire movie trilogy.</li>
<li>Pairs of Item IDs: Such as when we generate comparisons between two movies. While this appears to be \(O(N^2)\), in practice, a small number of combinations drive the bulk of traffic, such as comparison between movies in a trilogy.</li>
<li>Constrained input: Such as variables like movie genre, director, or lead actor. For example, if a user seeks movie recommendations from a specific director, we could execute a structured query and run it through an LLM to frame the response more eloquently. Another example is <a href="https://cheatlayer.com/" target="_blank">generating code based on drop-down options</a>—if the code has been verified to work, we can cache it for reliable reuse.</li>
</ul>
<p>Also, caching doesn’t only have to occur on-the-fly. As Redis shared, we can pre-compute LLM generations offline or asynchronously before serving them. By serving from a cache, we shift the latency from generation (typically seconds) to cache lookup (milliseconds). Pre-computing in batch can also help reduce cost relative to serving in real-time.</p>
<p>While the approaches listed here may not be as flexible as semantically caching on natural language inputs, I think it provides a good balance between efficiency and reliability.</p>
<h2 id="guardrails-to-ensure-output-quality">Guardrails: To ensure output quality</h2>
<p>In the context of LLMs, guardrails validate the output of LLMs, ensuring that the output doesn’t just sound good but is also syntactically correct, factual, and free from harmful content. It also includes guarding against adversarial input.</p>
<h3 id="why-guardrails">Why guardrails?</h3>
<p>They ensure that model outputs are reliable and consistent enough to use in production. For example, we may require output to be in a specific JSON schema so that it’s machine-readable, or we need code generated to be executable. Guardrails can help with such syntactic validation.</p>
<p>They also provide an additional layer of safety, and maintain quality control over an LLM’s output. For example, to verify if the content generated is appropriate for serving, we may want to check that the output isn’t harmful, verify it for factual accuracy, or ensure coherence with the context provided.</p>
<h3 id="more-about-guardrails">More about guardrails</h3>
<p><strong>One approach is to control the model’s responses via prompts.</strong> For example, Anthropic shared about prompts designed to guide the model toward generating responses that are <a href="https://arxiv.org/abs/2204.05862" target="_blank">helpful, harmless, and honest</a> (HHH). They found that Python fine-tuning with the HHH prompt led to better performance compared to fine-tuning with RLHF.</p>
<p><img src="https://eugeneyan.com/assets/hhh.jpg" loading="lazy" title="Example of HHH prompt" alt="Example of HHH prompt"></p>
<p>Example of HHH prompt (<a href="https://arxiv.org/abs/2204.05862" target="_blank">source</a>)</p>
<p><strong>A more common approach is to validate the output.</strong> An example is the <a href="https://github.com/ShreyaR/guardrails" target="_blank">Guardrails package</a>. It allows users to add structural, type, and quality requirements on LLM outputs via Pydantic-style validation. And if the check fails, it can trigger corrective action such as filtering on the offending output or regenerating another response.</p>
<p>Most of the validation logic is in <a href="https://github.com/ShreyaR/guardrails/blob/main/guardrails/validators.py" target="_blank"><code>validators.py</code></a>. It’s interesting to see how they’re implemented. Broadly speaking, the guardrails fall into the following categories:</p>
<ul>
<li>Single output value validation: This includes ensuring that the output (i) is one of the predefined choices, (ii) has a length within a certain range, (iii) if numeric, falls within an expected range, and (iv) is a complete sentence.</li>
<li>Syntactic checks: This includes ensuring that generated URLs are valid and reachable, and that Python and SQL code is bug-free.</li>
<li>Semantic checks: This verifies that the output is aligned with the reference document, or that the extractive summary closely matches the source document. These checks can be done via cosine similarity or fuzzy matching techniques.</li>
<li>Safety checks: This ensures that the generated output is free of inappropriate language or that the quality of translated text is high.</li>
</ul>
<p>Nvidia’s <a href="https://github.com/NVIDIA/NeMo-Guardrails" target="_blank">NeMo-Guardrails</a> follows a similar principle but is designed to guide LLM-based conversational systems. Rather than focusing on syntactic guardrails, it emphasizes semantic ones. This includes ensuring that the assistant steers clear of politically charged topics, provides factually correct information, and can detect jailbreaking attempts.</p>
<p>Thus, NeMo’s approach is somewhat different: Instead of using more deterministic checks like verifying if a value exists in a list or inspecting code for bugs, NeMo leans heavily on using another LLM to validate outputs (inspired by <a href="https://arxiv.org/abs/2303.08896" target="_blank">SelfCheckGPT</a>).</p>
<p>In their example for fact-checking and hallucination prevention, they ask the LLM itself to check whether the most recent output is consistent with the given context. To fact-check, the LLM is queried if the response is true based on the documents retrieved from the knowledge base. To prevent hallucinations, since there isn’t a knowledge base available, they get the LLM to generate multiple alternative completions which serve as the context. The underlying assumption is that if the LLM produces multiple completions that disagree with one another, the original completion is likely a hallucination.</p>
<p>The moderation example follows a simple approach: The response is screened for harmful and unethical content via an LLM. Given the nuance of ethics and harmful content, heuristics and conventional machine learning techniques fall short. Thus, an LLM is required for a deeper understanding of the intent and structure of dialogue.</p>
<p>Apart from using guardrails to verify the output of LLMs, we can also <strong>directly steer the output to adhere to a specific grammar.</strong> An example of this is Microsoft’s <a href="https://github.com/microsoft/guidance" target="_blank">Guidance</a>. Unlike Guardrails which <a href="https://github.com/ShreyaR/guardrails/blob/main/guardrails/constants.xml#L14" target="_blank">imposes JSON schema via a prompt</a>, Guidance enforces the schema by injecting tokens that make up the structure.</p>
<p>We can think of Guidance as a domain-specific language for LLM interactions and output. It draws inspiration from <a href="https://handlebarsjs.com/" target="_blank">Handlebars</a>, a popular templating language used in web applications that empowers users to perform variable interpolation and logical control.</p>
<p>However, Guidance sets itself apart from regular templating languages by executing linearly. This means it maintains the order of tokens generated. Thus, by inserting tokens that are part of the structure—instead of relying on the LLM to generate them correctly—Guidance can dictate the specific output format. In their examples, they show how to <a href="https://github.com/microsoft/guidance#guaranteeing-valid-syntax-json-example-notebook" target="_blank">generate JSON that’s always valid</a>, <a href="https://github.com/microsoft/guidance#rich-output-structure-example-notebook" target="_blank">generate complex output formats</a> with multiple keys, ensure that LLMs <a href="https://github.com/microsoft/guidance#role-based-chat-model-example-notebook" target="_blank">play the right roles</a>, and have <a href="https://github.com/microsoft/guidance#agents-notebook" target="_blank">agents interact with each other</a>.</p>
<p>They also introduced a concept called <a href="https://github.com/microsoft/guidance#token-healing-notebook" target="_blank">token healing</a>, a useful feature that helps us avoid subtle bugs that occur due to tokenization. In simple terms, it rewinds the generation by one token before the prompt’s end and then restricts the first generated token to have a prefix matching the last token in the prompt. This eliminates the need to fret about token boundaries when crafting prompts.</p>
<h3 id="how-to-apply-guardrails">How to apply guardrails?</h3>
<p>Though the concept of guardrails in industry is still nascent, there are a handful of immediately useful and practical strategies we can consider.</p>
<p><strong>Structural guidance:</strong> Apply guidance whenever possible. It provides direct control over outputs and offers a more precise method to ensure that output conforms to a specific structure or format.</p>
<p><strong>Syntactic guardrails:</strong> These include checking if categorical output is within a set of acceptable choices, or if numeric output is within an expected range. Also, if we generate SQL, these can verify its correctness and also ensure that all columns in the query match the schema. Ditto for Python code.</p>
<p><strong>Content safety guardrails:</strong> These verify that the output has no harmful or inappropriate content. It can be as simple as checking against the <a href="https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words" target="_blank">List of Dirty, Naughty, Obscene, and Otherwise Bad Words</a> or using <a href="https://pypi.org/project/profanity-check/" target="_blank">profanity detection</a> models. (It’s <a href="https://twitter.com/goodside/status/1685023251532320768" target="_blank">common to run moderation classifiers on output</a>.) More complex and nuanced output can rely on an LLM evaluator.</p>
<p><strong>Semantic/factuality guardrails:</strong> These confirm that the output is semantically relevant to the input. Say we’re generating a two-sentence summary of a movie based on its synopsis. We can validate if the produced summary is semantically similar to the output, or have (another) LLM ascertain if the summary accurately represents the provided synopsis.</p>
<p><strong>Input guardrails:</strong> These limit the types of input the model will respond to, helping to mitigate the risk of the model responding to inappropriate or adversarial prompts which would lead to generating harmful content. For example, you’ll get an error if you ask Midjourney to generate NSFW content. This can be as straightforward as comparing against a list of strings or using a moderation classifier.</p>
<p><img src="https://eugeneyan.com/assets/input-guardrail.jpg" loading="lazy" title="An example of an input guardrail on Midjourney" alt="An example of an input guardrail on Midjourney"></p>
<p>An example of an input guardrail on Midjourney</p>
<h2 id="defensive-ux-to-anticipate--handle-errors-gracefully">Defensive UX: To anticipate &amp; handle errors gracefully</h2>
<p>Defensive UX is a design strategy that acknowledges that bad things, such as inaccuracies or hallucinations, can happen during user interactions with machine learning or LLM-based products. Thus, the intent is to anticipate and manage these in advance, primarily by guiding user behavior, averting misuse, and handling errors gracefully.</p>
<h3 id="why-defensive-ux">Why defensive UX?</h3>
<p>Machine learning and LLMs aren’t perfect—they can produce inaccurate output. Also, they respond differently to the same input over time, such as search engines displaying varying results due to personalization, or LLMs generating diverse output on more creative settings. This can violate the principle of consistency which advocates for a consistent UI and predictable behaviors.</p>
<p>Defensive UX can help mitigate the above by providing:</p>
<ul>
<li><strong>Increased accessibility</strong>: By helping users understand how ML/LLM features work and their limitations, defensive UX makes it more accessible and user-friendly.</li>
<li><strong>Increased trust</strong>: When users see that the feature can handle difficult scenarios gracefully and doesn’t produce harmful output, they’re likely to trust it more.</li>
<li><strong>Better UX</strong>: By designing the model and UX to handle ambiguous situations and errors, defensive UX paves the way for a smoother, more enjoyable user experience.</li>
</ul>
<h3 id="more-about-defensive-ux">More about defensive UX</h3>
<p>To learn more about defensive UX, we can look at Human-AI guidelines from Microsoft, Google, and Apple.</p>
<p><strong>Microsoft’s <a href="https://www.microsoft.com/en-us/research/publication/guidelines-for-human-ai-interaction/" target="_blank">Guidelines for Human-AI Interaction</a></strong> is based on a survey of 168 potential guidelines, collected from internal and external industry sources, academic literature, and public articles. After clustering guidelines that were similar, filtering guidelines that were too vague or too specific or not AI-specific, and a round of heuristic evaluation, they narrowed it down to 18 guidelines.</p>
<p>These guidelines follow a certain style: Each one is a succinct action rule of 3 - 10 words, beginning with a verb. Each rule is accompanied by a one-liner that addresses potential ambiguities. They are organized based on their likely application during user interaction:</p>
<ul>
<li>Initially: Make clear what the system can do (G1), make clear how well the system can do what it can do (G2)</li>
<li>During interaction: Time services based on context (G3), mitigate social biases (G6)</li>
<li>When wrong: Support efficient dismissal (G8), support efficient correction (G9)</li>
<li>Over time: Learn from user behavior (G13), provide global controls (G17)</li>
</ul>
<p><strong>Google’s <a href="https://pair.withgoogle.com/guidebook/" target="_blank">People + AI Guidebook</a></strong> is rooted in data and insights drawn from Google’s product team and academic research. In contrast to Microsoft’s guidelines which are organized around the user, Google organizes its guidelines into concepts that a developer needs to keep in mind.</p>
<p>There are 23 patterns grouped around common questions that come up during the product development process, including:</p>
<ul>
<li>How do I get started with human-centered AI: Determine if the AI adds value, invest early in good data practices (e.g., evals)</li>
<li>How do I onboard users to new AI features: Make it safe to explore, anchor on familiarity, automate in phases</li>
<li>How do I help users build trust in my product: Set the right expectations, be transparent, automate more when the risk is low.</li>
</ul>
<p><strong>Apple’s <a href="https://developer.apple.com/design/human-interface-guidelines/machine-learning" target="_blank">Human Interface Guidelines for Machine Learning</a></strong> differs from the bottom-up approach of academic literature and user studies. Instead, its primary source is practitioner knowledge and experience. Thus, it doesn’t include many references or data points, but instead focuses on Apple’s longstanding design principles. This results in a unique perspective that distinguishes it from the other two guidelines.</p>
<p>The document focuses on how Apple’s design principles can be applied to ML-infused products, emphasizing aspects of UI rather than model functionality. It starts by asking developers to consider the role of ML in their app and work backwards from the user experience. This includes questions such as whether ML is:</p>
<ul>
<li>Critical or complementary: For example, Face ID cannot work without ML but the keyboard can still work without QuickType.</li>
<li>Proactive or reactive: Siri Suggestions are proactive while autocorrect is reactive.</li>
<li>Dynamic or static: Recommendations are dynamic while object detection in Photos only improves with each iOS release.</li>
</ul>
<p>It then delves into several patterns, split into inputs and outputs of a system. Inputs focus on explicit feedback, implicit feedback, calibration, and corrections. This section guides the design for how AI products request and process user data and interactions. Outputs focus on mistakes, multiple options, confidence, attribution, and limitations. The intent is to ensure the model’s output is presented in a comprehensible and useful manner.</p>
<p>The differences between the three guidelines are insightful. Google places more emphasis on considerations for training data and model development, likely influenced by its engineering-driven culture. Microsoft has more focus on mental models, likely an artifact of the HCI academic study. Lastly, Apple’s approach centers around providing a seamless UX, a focus likely due to its cultural values and principles.</p>
<h3 id="how-to-apply-defensive-ux">How to apply defensive UX?</h3>
<p>Here are some patterns based on the guidelines above. (Disclaimer: I’m not a designer.)</p>
<p><strong>Set the right expectations.</strong> This principle is consistent across all three guidelines:</p>
<ul>
<li>Microsoft: Make clear how well the system can do what it can do (help the user understand how often the AI system may make mistakes)</li>
<li>Google: Set the right expectations (be transparent with your users about what your AI-powered product can and cannot do)</li>
<li>Apple: Help people establish realistic expectations (describe the limitation in marketing material or within the feature’s context)</li>
</ul>
<p>This can be as simple as adding a brief disclaimer above AI-generated results, like those of Bard, or highlighting our app’s limitations on its landing page, like how ChatGPT does it.</p>
<p><img src="https://eugeneyan.com/assets/bard-disclaimer.png" loading="lazy" title="Example of a disclaimer on Google Bard results (Note: The code provided will not work.)" alt="Example of a disclaimer on Google Bard results (Note: The code provided will not work.)"></p>
<p>Example of a disclaimer on Google Bard results (Note: <code>nrows</code> is not a valid argument.)</p>
<p>By being transparent about our product’s capabilities and limitations, we help users calibrate their expectations about its functionality and output. While this may cause the user to trust it less in the short run, it helps foster trust in the long run—users are less likely to overestimate our product and subsequently face disappointment.</p>
<p><strong>Enable efficient dismissal.</strong> This is explicitly mentioned as Microsoft’s Guideline 8: Support efficient dismissal (make it easy to dismiss or ignore undesired AI system services).</p>
<p>For example, if a user is navigating our site and a chatbot pops up asking if they need help, it should be easy for the user to dismiss the chatbot. This ensures the chatbot doesn’t get in the way, especially on devices with smaller screens. Similarly, GitHub Copilot allows users to conveniently ignore its code suggestions by simply continuing to type. While this may reduce usage of the AI feature in the short term, it prevents it from becoming a nuisance and potentially reducing customer satisfaction in the long term.</p>
<p><strong>Provide attribution.</strong> This is listed in all three guidelines:</p>
<ul>
<li>Microsoft: Make clear why the system did what it did (enable the user to access an explanation of why the AI system behaved as it did)</li>
<li>Google: Add context from human sources (help users appraise your recommendations with input from 3rd-party sources)</li>
<li>Apple: Consider using attributions to help people distinguish among results</li>
</ul>
<p>Citations are becoming an increasingly popular design element. Take BingChat for instance. When you make a query, it includes citations, usually from reputable sources, in its responses. This not only shows where the information came from, but also allows users to assess the quality of the sources. Similarly, imagine we’re using an LLM to explain why a user might like a product. Alongside the LLM-generated explanation, we could also include a quote from an actual review or mention the product rating.</p>
<p>Context from experts and the community also enhances user trust. For example, if a user is seeking recommendations for a hiking trail, mentioning that a suggested trail comes highly recommended by the relevant community can go a long way. It not only adds value to the recommendation but also helps users calibrate trust through the human connection.</p>
<p><img src="https://eugeneyan.com/assets/social-proof.jpg" loading="lazy" title="Example of attribution via social proof" alt="Example of attribution via social proof"></p>
<p>Example of attribution via social proof (<a href="https://pair.withgoogle.com/guidebook/patterns" target="_blank">source</a>)</p>
<p>Finally, Apple’s guidelines include popular attributions such as “Because you’ve read non-fiction”, “New books by authors you’ve read”. These descriptors not only personalize the experience but also provide context, enhancing user understanding and trust.</p>
<p><strong>Anchor on familiarity.</strong> When introducing users to a new AI product or feature, it helps to guide them with familiar UX patterns and features. This makes it easier for users to focus on the main task and start to build trust in our new product. Resist the temptation to showcase new and “magical” features via exotic UI elements.</p>
<p>Along a similar vein, chat-based features are becoming more common, largely due to the popularity of ChatGPT. For example, chat with your docs, chat to query your data, chat to buy groceries. However, I <a href="https://eugeneyan.com/writing/llm-ux/" target="_blank">question whether chat is the right UX for most user experiences</a>—it just takes too much effort relative to the familiar UX of clicking on text and images.</p>
<p>Increasing user effort leads to higher expectations that are harder to meet. Netflix shared that users have <a href="https://slideslive.com/38934788/a-human-perspective-on-algorithmic-similarity?ref=folder-59726" target="_blank">higher expectations for recommendations</a> that result from explicit actions such as search. In general, the more effort a user puts in (e.g., chat, search), the higher the expectations they have. Contrast this with lower-effort interactions such as scrolling over recommendations slates or clicking on a product.</p>
<p>Thus, while chat offers more flexibility, it also demands more user effort. Moreover, using a chat box is less intuitive as it lacks signifiers on how users can adjust the output. Overall, I think that sticking with a familiar and constrained UI makes it easier for users to navigate our product; chat should only be considered as a secondary or tertiary option.</p>
<h2 id="collect-user-feedback-to-build-our-data-flywheel">Collect user feedback: To build our data flywheel</h2>
<p>Feedback from users helps us to learn their preferences. Specific to LLM products, user feedback contributes to building evals, fine-tuning, and guardrails. If we think about it, data—corpus for pre-training, expert-crafted demonstrations, human preferences for reward modeling—is one of the few moats for LLM products. Thus, we want to be deliberately thinking about collecting user feedback when designing our UX.</p>
<p>Feedback can be explicit or implicit. Explicit feedback is information users provide in response to a request by our product; implicit feedback is information we learn from user interactions without needing users to deliberately provide feedback.</p>
<h3 id="why-collect-user-feedback">Why collect user feedback</h3>
<p>User feedback <strong>helps our models improve</strong>. By learning what users like, dislike, or complain about, we can improve our models and services to better meet their needs.</p>
<p>User feedback also allows us to <strong>adapt to individual preferences</strong>. Recommendation systems are an example of this. As users interact with items, we can learn what they like and dislike and better cater to their tastes over time.</p>
<p>Finally, the feedback loop helps us <strong>evaluate our system’s overall performance</strong>. While evals can help us measure model/system performance, user feedback offers a concrete measure of user satisfaction and product effectiveness.</p>
<h3 id="how-to-collect-user-feedback">How to collect user feedback</h3>
<p><strong>Make it easy for users to provide feedback.</strong> This is echoed across all three guidelines:</p>
<ul>
<li>Microsoft: Encourage granular feedback (enable the user to provide feedback indicating their preferences during regular interaction with the AI system)</li>
<li>Google: Let users give feedback (give users the opportunity for real-time teaching, feedback, and error correction)</li>
<li>Apple: Provide actionable information your app can use to improve the content and experience it presents to people</li>
</ul>
<p>ChatGPT is one such example. Users can indicate thumbs up/down on responses, or choose to regenerate a response if it’s really bad or not helpful. This is useful feedback on human preferences which can then be used to fine-tune LLMs.</p>
<p>Midjourney is another good example. After images are generated, users can generate a new set of images (negative feedback), tweak an image by asking for a variation (positive feedback), or upscale and download the image (strong positive feedback). This enables Midjourney to gather rich comparison data on the outputs generated.</p>
<p><img src="https://eugeneyan.com/assets/midjourney.jpg" loading="lazy" title="Example of collecting user feedback as part of the UX" alt=">Example of collecting user feedback as part of the UX"></p>
<p>Example of collecting user feedback as part of the UX</p>
<p><strong>Consider implicit feedback too.</strong> Implicit feedback is information that arises as users interact with our product. Unlike the specific responses we get from explicit feedback, implicit feedback can provide a wide range of data on user behavior and preferences.</p>
<p>Copilot-like assistants are a prime example. Users indicate whether a suggestion was useful by either wholly accepting it (strong positive feedback), accepting and making minor tweaks (positive feedback), or ignoring it (neutral/negative feedback). Alternatively, they may update the comment that lead to the generated code, suggesting that the initial code generation didn’t meet their needs.</p>
<p>Chatbots, such as ChatGPT and BingChat, are another example. How has daily usage changed over time? If the product is sticky, it suggests that users like it. Also, how long is the average conversation? This can be tricky to interpret: Is a longer conversation better because the conversation was engaging and fruitful? Or is it worse because it took the user longer to get what they needed?</p>
<h2 id="other-patterns-common-in-machine-learning">Other patterns common in machine learning</h2>
<p>Apart from the seven patterns we’ve explored, there are several other patterns in machine learning that are also relevant to LLM systems and products. They include:</p>
<ul>
<li><a href="https://eugeneyan.com/writing/more-patterns/#data-flywheel-to-continuously-improve--build-a-moat" target="_blank">Data flywheel</a>: Continuous data collection improves the model and leads to a better user experience. This, in turn, promotes more usage which provides more data to further evaluate and fine-tune models, creating a virtuous cycle.</li>
<li><a href="https://eugeneyan.com/writing/more-patterns/#cascade-to-split-a-problem-into-smaller-problems" target="_blank">Cascade</a>: Rather than assigning a single, complex task to the LLM, we can simplify and break it down so it only has to handle tasks it excels at, such as reasoning or communicating eloquently. RAG is an example of this. Instead of relying on the LLM to retrieve and rank items based on its internal knowledge, we can augment LLMs with external knowledge and focus on applying the LLM’s reasoning abilities.</li>
<li><a href="https://eugeneyan.com/writing/practical-guide-to-maintaining-machine-learning/#monitor-models-for-misbehaviour-when-retraining" target="_blank">Monitoring</a>: This helps demonstrate the value added by the AI system, or the lack of it. Someone shared an anecdote of running an LLM-based customer support solution in prod for two weeks before discontinuing it—an A/B test showed that losses were 12x more when using an LLM as a substitute for their support team!</li>
</ul>
<p>(Read about more design patterns for <a href="https://eugeneyan.com/writing/design-patterns/" target="_blank">machine learning code</a> and <a href="https://eugeneyan.com/writing/more-patterns/" target="_blank">systems</a>.)</p>
<p>Also, here’s what others said:</p>
<blockquote>
<p>Separation of concerns/task decomposition- having distinct prompts for distinct subtasks and chaining them together helps w attention and reliability (hurts latency). We were having trouble specifying a rigid output structure AND variable response content so we split up the tasks — <a href="https://twitter.com/generick_ez/status/1681153738822516736" target="_blank">Erick Enriquez</a></p>
</blockquote>
<blockquote>
<p>A few others that will be needed:
role based access control: who can access what;
security: if I’m using a DB with an LLM, how do I ensure that I have the right security guards — <a href="https://twitter.com/ntkris/status/16812092400299991050" target="_blank">Krishna</a></p>
</blockquote>
<blockquote>
<p>Consistent output format: setting outputs to a standardized format such as JSON;
Tool augmentation: offload tasks to more specialised, proven, reliable models — <a href="https://twitter.com/ptuls/status/1681284873741561857" target="_blank">Paul Tune</a></p>
</blockquote>
<blockquote>
<p>Security: mitigate cache poisoning, input validation, mitigate prompt injection, training data provenance, output with non-vulnerable code, mitigate malicious input aimed at influencing requests used by tools (AI Agent), mitigate denial of service (stress test llm), to name a few :) — <a href="https://www.linkedin.com/feed/update/urn:li:activity:7087089908229558272?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7087089908229558272%2C7087224131292684288%29" target="_blank">Anderson Darario</a></p>
</blockquote>
<blockquote>
<p>Another ux/ui related: incentivize users to provide feedback on generated answers (implicit or explicit). Implicit could be sth like copilot’s ghost text style, if accepted with TAB, meaning positive feedback etc. — <a href="https://www.linkedin.com/feed/update/urn:li:activity:7087089908229558272?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7087089908229558272%2C7087149792660750336%29" target="_blank">Wen Yang</a></p>
</blockquote>
<blockquote>
<p>Great list. I would add consistency checks like self-consistency sampling, chaining and decomposition of tasks, and the emsembling of multiple model outputs. Applying each of these almost daily. <a href="https://www.threads.net/@dwhitena/post/Cu3BBaJtoyj/?igshid=OGQ5ZDc2ODk2ZA==" target="_blank">Dan White</a></p>
</blockquote>
<blockquote>
<p>Guardrails is super relevant for building analytics tools where llm is a translator from natural to programming language — <a href="https://www.threads.net/@m_voitko/post/Cu1b4liNwCS/?igshid=OGQ5ZDc2ODk2ZA==" target="_blank">m_voitko</a></p>
</blockquote>
<h2 id="conclusion">Conclusion</h2>
<p>This is the longest post I’ve written by far. If you’re still with me, thank you! I hope you found reading about these patterns helpful, and that the 2x2 below makes sense.</p>
<p><img src="https://eugeneyan.com/assets/llm-patterns.png" loading="lazy" title="LLM patterns across the axis of data to user, and defensive to offensive." alt="LLM patterns across the axis of data to user, and defensive to offensive."></p>
<p>LLM patterns across the axis of data to user, and defensive to offensive.</p>
<p>We’re still so early on the journey towards building LLM-based systems and products. Are there any key patterns or resources I’ve missed? What have you found useful or not useful? I’d love to hear your experience. <strong>Please <a href="https://twitter.com/eugeneyan" target="_blank">reach out!</a></strong></p>
<h2 id="references">References</h2>
<p>Hendrycks, Dan, et al. <a href="https://arxiv.org/abs/2009.03300" target="_blank">“Measuring massive multitask language understanding.”</a> arXiv preprint arXiv:2009.03300 (2020).</p>
<p>Gao, Leo, et al. <a href="https://github.com/EleutherAI/lm-evaluation-harness" target="_blank">“A Framework for Few-Shot Language Model Evaluation.”</a> v0.0.1, Zenodo, (2021), doi:10.5281/zenodo.5371628.</p>
<p>Liang, Percy, et al. <a href="https://arxiv.org/abs/2211.09110" target="_blank">“Holistic evaluation of language models.”</a> arXiv preprint arXiv:2211.09110 (2022).</p>
<p>Dubois, Yann, et al. <a href="https://github.com/tatsu-lab/alpaca_eval" target="_blank">“AlpacaFarm: A Simulation Framework for Methods That Learn from Human Feedback.”</a> (2023)</p>
<p>Papineni, Kishore, et al. <a href="https://dl.acm.org/doi/10.3115/1073083.1073135" target="_blank">“Bleu: a method for automatic evaluation of machine translation.”</a> Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 2002.</p>
<p>Lin, Chin-Yew. <a href="https://aclanthology.org/W04-1013/" target="_blank">“Rouge: A package for automatic evaluation of summaries.”</a> Text summarization branches out. 2004.</p>
<p>Zhang, Tianyi, et al. <a href="https://arxiv.org/abs/1904.09675" target="_blank">“Bertscore: Evaluating text generation with bert.”</a> arXiv preprint arXiv:1904.09675 (2019).</p>
<p>Zhao, Wei, et al. <a href="https://arxiv.org/abs/1909.02622" target="_blank">“MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance.”</a> arXiv preprint arXiv:1909.02622 (2019).</p>
<p>Sai, Ananya B., Akash Kumar Mohankumar, and Mitesh M. Khapra. <a href="https://arxiv.org/abs/2008.12009" target="_blank">“A survey of evaluation metrics used for NLG systems.”</a> ACM Computing Surveys (CSUR) 55.2 (2022): 1-39.</p>
<p>Liu, Yang, et al. <a href="https://arxiv.org/abs/2303.16634" target="_blank">“Gpteval: Nlg evaluation using gpt-4 with better human alignment.”</a> arXiv preprint arXiv:2303.16634 (2023).</p>
<p>Fourrier, Clémentine, et al. <a href="https://huggingface.co/blog/evaluating-mmlu-leaderboard#whats-going-on-with-the-open-llm-leaderboard" target="_blank">“What’s going on with the Open LLM Leaderboard?”</a> (2023).</p>
<p>Zheng, Lianmin, et al. <a href="https://arxiv.org/abs/2306.05685" target="_blank">“Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.”</a> arXiv preprint arXiv:2306.05685 (2023).</p>
<p>Dettmers, Tim, et al. <a href="https://arxiv.org/abs/2305.14314" target="_blank">“Qlora: Efficient finetuning of quantized llms.”</a> arXiv preprint arXiv:2305.14314 (2023).</p>
<p>Swyx et al. <a href="https://www.latent.space/p/mosaic-mpt-7b#details" target="_blank">MPT-7B and The Beginning of Context=Infinity</a> (2023).</p>
<p>Fradin, Michelle, Reeder, Lauren <a href="https://www.sequoiacap.com/article/llm-stack-perspective/" target="_blank">“The New Language Model Stack”</a> (2023).</p>
<p>Radford, Alec, et al. <a href="https://arxiv.org/abs/2103.00020" target="_blank">“Learning transferable visual models from natural language supervision.”</a> International conference on machine learning. PMLR, 2021.</p>
<p>Yan, Ziyou. <a href="https://eugeneyan.com/writing/search-query-matching/" target="_blank">“Search: Query Matching via Lexical, Graph, and Embedding Methods.”</a> eugeneyan.com, (2021).</p>
<p>Petroni, Fabio, et al. <a href="https://arxiv.org/abs/2005.04611" target="_blank">“How context affects language models’ factual predictions.”</a> arXiv preprint arXiv:2005.04611 (2020).</p>
<p>Karpukhin, Vladimir, et al. <a href="https://arxiv.org/abs/2004.04906" target="_blank">“Dense passage retrieval for open-domain question answering.”</a> arXiv preprint arXiv:2004.04906 (2020).</p>
<p>Lewis, Patrick, et al. <a href="https://arxiv.org/abs/2005.11401" target="_blank">“Retrieval-augmented generation for knowledge-intensive nlp tasks.”</a> Advances in Neural Information Processing Systems 33 (2020): 9459-9474.</p>
<p>Izacard, Gautier, and Edouard Grave. <a href="https://arxiv.org/abs/2007.01282" target="_blank">“Leveraging passage retrieval with generative models for open domain question answering.”</a> arXiv preprint arXiv:2007.01282 (2020).</p>
<p>Borgeaud, Sebastian, et al. <a href="https://arxiv.org/abs/2112.04426" target="_blank">“Improving language models by retrieving from trillions of tokens.”</a> International conference on machine learning. PMLR, (2022).</p>
<p>Lazaridou, Angeliki, et al. <a href="https://arxiv.org/abs/2203.05115" target="_blank">“Internet-augmented language models through few-shot prompting for open-domain question answering.”</a> arXiv preprint arXiv:2203.05115 (2022).</p>
<p>Wang, Yue, et al. <a href="https://arxiv.org/abs/2305.07922" target="_blank">“Codet5+: Open code large language models for code understanding and generation.”</a> arXiv preprint arXiv:2305.07922 (2023).</p>
<p>Gao, Luyu, et al. <a href="https://arxiv.org/abs/2212.10496" target="_blank">“Precise zero-shot dense retrieval without relevance labels.”</a> arXiv preprint arXiv:2212.10496 (2022).</p>
<p>Yan, Ziyou. <a href="https://eugeneyan.com/writing/obsidian-copilot/" target="_blank">“Obsidian-Copilot: An Assistant for Writing &amp; Reflecting.”</a> eugeneyan.com, (2023).</p>
<p>Bojanowski, Piotr, et al. <a href="https://arxiv.org/abs/1607.04606" target="_blank">“Enriching word vectors with subword information.”</a> Transactions of the association for computational linguistics 5 (2017): 135-146.</p>
<p>Reimers, Nils, and Iryna Gurevych. <a href="https://arxiv.org/abs/2004.09813" target="_blank">“Making Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation.”</a> Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, (2020).</p>
<p>Wang, Liang, et al. <a href="https://arxiv.org/abs/2212.03533" target="_blank">“Text embeddings by weakly-supervised contrastive pre-training.”</a> arXiv preprint arXiv:2212.03533 (2022).</p>
<p>Su, Hongjin, et al. <a href="https://arxiv.org/abs/2212.09741" target="_blank">“One embedder, any task: Instruction-finetuned text embeddings.”</a> arXiv preprint arXiv:2212.09741 (2022).</p>
<p>Johnson, Jeff, et al. <a href="https://arxiv.org/abs/1702.08734" target="_blank">“Billion-Scale Similarity Search with GPUs.”</a> IEEE Transactions on Big Data, vol. 7, no. 3, IEEE, 2019, pp. 535–47.</p>
<p>Malkov, Yu A., and Dmitry A. Yashunin. <a href="https://arxiv.org/abs/1603.09320" target="_blank">“Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs.”</a> IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 42, no. 4, IEEE, 2018, pp. 824–36.</p>
<p>Guo, Ruiqi, et al. <a href="https://arxiv.org/abs/1908.10396." target="_blank">“Accelerating Large-Scale Inference with Anisotropic Vector Quantization.”</a> International Conference on Machine Learning, (2020)</p>
<p>Ouyang, Long, et al. <a href="https://arxiv.org/abs/2203.02155" target="_blank">“Training language models to follow instructions with human feedback.”</a> Advances in Neural Information Processing Systems 35 (2022): 27730-27744.</p>
<p>Howard, Jeremy, and Sebastian Ruder. <a href="https://arxiv.org/abs/1801.06146" target="_blank">“Universal language model fine-tuning for text classification.”</a> arXiv preprint arXiv:1801.06146 (2018).</p>
<p>Devlin, Jacob, et al. <a href="https://arxiv.org/abs/1810.04805" target="_blank">“Bert: Pre-training of deep bidirectional transformers for language understanding.”</a> arXiv preprint arXiv:1810.04805 (2018).</p>
<p>Radford, Alec, et al. <a href="https://openai.com/research/language-unsupervised" target="_blank">“Improving language understanding with unsupervised learning.”</a> (2018).</p>
<p>Raffel, Colin, et al. <a href="https://arxiv.org/abs/1910.10683" target="_blank">“Exploring the limits of transfer learning with a unified text-to-text transformer.”</a> The Journal of Machine Learning Research 21.1 (2020): 5485-5551.</p>
<p>Lester, Brian, Rami Al-Rfou, and Noah Constant. <a href="https://arxiv.org/abs/2104.08691" target="_blank">“The power of scale for parameter-efficient prompt tuning.”</a> arXiv preprint arXiv:2104.08691 (2021).</p>
<p>Li, Xiang Lisa, and Percy Liang. <a href="https://arxiv.org/abs/2101.00190" target="_blank">“Prefix-tuning: Optimizing continuous prompts for generation.”</a> arXiv preprint arXiv:2101.00190 (2021).</p>
<p>Houlsby, Neil, et al. <a href="https://arxiv.org/abs/1902.00751" target="_blank">“Parameter-efficient transfer learning for NLP.”</a> International Conference on Machine Learning. PMLR, 2019.</p>
<p>Hu, Edward J., et al. <a href="https://arxiv.org/abs/2106.09685" target="_blank">“Lora: Low-rank adaptation of large language models.”</a> arXiv preprint arXiv:2106.09685 (2021).</p>
<p>Dettmers, Tim, et al. <a href="https://arxiv.org/abs/2305.14314" target="_blank">“Qlora: Efficient finetuning of quantized llms.”</a> arXiv preprint arXiv:2305.14314 (2023).</p>
<p>Williams, Adina, et al. <a href="https://cims.nyu.edu/~sbowman/multinli/" target="_blank">“A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference.”</a> Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), Association for Computational Linguistics, (2018).</p>
<p><a href="https://github.com/zilliztech/GPTCache" target="_blank">GPTCache</a> (2023).</p>
<p>Bai, Yuntao, et al. <a href="https://arxiv.org/abs/2204.05862" target="_blank">“Training a helpful and harmless assistant with reinforcement learning from human feedback.”</a> arXiv preprint arXiv:2204.05862 (2022).</p>
<p><a href="https://github.com/ShreyaR/guardrails" target="_blank">Guardrails</a> (2023)</p>
<p><a href="https://github.com/NVIDIA/NeMo-Guardrails" target="_blank">NeMo-Guardrails</a> (2023)</p>
<p>Manakul, Potsawee, Adian Liusie, and Mark JF Gales. <a href="https://arxiv.org/abs/2303.08896" target="_blank">“Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models.”</a> arXiv preprint arXiv:2303.08896 (2023).</p>
<p><a href="https://github.com/microsoft/guidance" target="_blank">Guidance</a> (2023).</p>
<p>Amershi, Saleema, et al. <a href="https://www.microsoft.com/en-us/research/publication/guidelines-for-human-ai-interaction/" target="_blank">“Guidelines for human-AI interaction.”</a> Proceedings of the 2019 chi conference on human factors in computing systems. 2019.</p>
<p><a href="https://pair.withgoogle.com/guidebook/" target="_blank">People + AI Guidebook</a> (2023).</p>
<p><a href="https://developer.apple.com/design/human-interface-guidelines/machine-learning" target="_blank">Human Interface Guidelines for Machine Learning</a> (2023).</p>
<p>Schendel, Zachary A., Faraz Farzin, and Siddhi Sundar. <a href="https://slideslive.com/38934788/a-human-perspective-on-algorithmic-similarity?ref=folder-59726" target="_blank">“A Human Perspective on Algorithmic Similarity.”</a> Proceedings of the 14th ACM Conference on Recommender Systems. 2020.</p>

<p>If you found this useful, please cite this article as:</p>
<blockquote>
<p>Yan, Ziyou. (Jul 2023). Patterns for Building LLM-based Systems &amp; Products. eugeneyan.com.
https://eugeneyan.com/writing/llm-patterns/.</p>
</blockquote>
<p>or</p>
<div><pre><code>@article{yan2023llm-patterns,
  title   = {Patterns for Building LLM-based Systems &amp; Products},
  author  = {Yan, Ziyou},
  journal = {eugeneyan.com},
  year    = {2023},
  month   = {Jul},
  url     = {https://eugeneyan.com/writing/llm-patterns/}
}</code></pre>
</div>


<p><span>Share on: </span></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Electronic Structure of LK-99 (477 pts)]]></title>
            <link>https://arxiv.org/abs/2308.00676</link>
            <guid>36965545</guid>
            <pubDate>Wed, 02 Aug 2023 00:57:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2308.00676">https://arxiv.org/abs/2308.00676</a>, See on <a href="https://news.ycombinator.com/item?id=36965545">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
    
    
      
    
  
  
  
    <p><a aria-describedby="download-button-info" href="https://arxiv.org/pdf/2308.00676">Download PDF</a></p><blockquote>
            <span>Abstract:</span>  A recent paper [Lee {\em et al.}, J. Korean Cryt. Growth Cryst. Techn. {\bf
33}, 61 (2023)] provides some experimental indications that
Pb$_{10-x}$Cu$_x$(PO$_4$)$_6$O with $x\approx 1$, coined LK-99, might be a
room-temperature superconductor at ambient pressure. Our density-functional
theory calculations show lattice parameters and a volume contraction with $x$
-- very similar to experiment. The DFT electronic structure shows Cu$^{2+}$ in
a $3d^9$ configuration with two extremely flat Cu bands crossing the Fermi
energy. This puts Pb$_{9}$Cu(PO$_4$)$_6$O in an ultra-correlated regime and
suggests that, without doping, it is a Mott or charge transfer insulator. If
doped such an electronic structure might support flat-band superconductivity or
an correlation-enhanced electron-phonon mechanism, whereas a diamagnet without
superconductivity appears to be rather at odds with our results.

    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Liang Si [<a href="https://arxiv.org/show-email/1140e50a/2308.00676">view email</a>]
      <br>
    <strong>[v1]</strong>
    
        Tue, 1 Aug 2023 17:21:48 UTC (1,572 KB)<br>
    </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Help I accidently enabled HSTS on localhost (122 pts)]]></title>
            <link>https://bartwullems.blogspot.com/2023/07/help-i-accidently-enabled-hstson.html</link>
            <guid>36964491</guid>
            <pubDate>Tue, 01 Aug 2023 23:04:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bartwullems.blogspot.com/2023/07/help-i-accidently-enabled-hstson.html">https://bartwullems.blogspot.com/2023/07/help-i-accidently-enabled-hstson.html</a>, See on <a href="https://news.ycombinator.com/item?id=36964491">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<main id="main" role="main" tabindex="-1">
<div id="page_body" name="Page Body">
<div data-version="2" id="Blog1">
<article>
<div>
<div>
<h3>
Help! I accidently enabled HSTS–on localhost
</h3>
</div>

<div id="post-body-595724242402749244">
<p>I ran into an issue after accidently enabling HSTS for a website on localhost. This was not an issue for the original website that was running in IIS and had a certificate configured. But when I tried to run an Angular app a little bit later on <code>http://localhost:4200</code> the browser redirected me immediately to <code>https://localhost</code>.</p>  <p>Whoops! That was not what I wanted in this case. </p>  <p>To fix it, you need to go the network settings of your browser, there are available at:</p>  <ul>   <li>chrome://net-internals/#hsts </li>    <li>edge://net-internals/#hsts </li>    <li>brave://net-internals/#hsts</li> </ul>  <p>Enter ‘localhost’ in the domain textbox under the <strong>Delete domain security policies</strong> section and hit <strong>Delete</strong>.</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj-QwQj2HOqm_w1HeOU0c1_4HoEmcGGGp1fCXX2RfA8Q7QHZpgZgIkzaXJgsrLwPC3Y0sRcTU1VNF3b8G5E-k_FjYMHgBcJt8p6MkMI_Y577G9uhvfTmegm8013QjSGm7hWfq0Uf0Pc6rfTi-fewkKlOf9QThgdQDDFe3nTCSXl6ZSosUidvmG3DQAAoULz/s2632/HSTS.PNG" imageanchor="1"><img data-original-height="869" data-original-width="2632" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj-QwQj2HOqm_w1HeOU0c1_4HoEmcGGGp1fCXX2RfA8Q7QHZpgZgIkzaXJgsrLwPC3Y0sRcTU1VNF3b8G5E-k_FjYMHgBcJt8p6MkMI_Y577G9uhvfTmegm8013QjSGm7hWfq0Uf0Pc6rfTi-fewkKlOf9QThgdQDDFe3nTCSXl6ZSosUidvmG3DQAAoULz/s16000/HSTS.PNG"></a></p><p>That should do the trick…</p>
</div>

</div>

</article>
</div><div data-version="2" id="PopularPosts1">
<h3>
Popular posts from this blog
</h3>
<div role="feed">
<article role="article">
<div id="post-snippet-3928160368773424779">
<p>
A colleague asked me to take a look at the following code inside a test project:    My first guess would be that this code checks that the specified condition(the contains) is true for every element in the list.&nbsp;   This turns out not to be the case. The Assert.Collection expects a list of element inspectors, one for every item in the list. The first inspector is used to check the first item, the second inspector the second item and so on. The number of inspectors should match the number of elements in the list.   An example:     The behavior I expected could be achieved using the Assert.All method:   
</p>
</div>
</article>
<article role="article">
<div id="post-snippet-2565361908360523400">
<p>
I’m really bad at remembering emoji’s. So here is cheat sheet with all emoji’s that can be used in tools that support the github emoji markdown markup:   All credits go to rcaviers  who created this list.   
</p>
</div>
</article>
<article role="article">
<div id="post-snippet-4094301063444646176">
<p>
In the world of software engineering, the Lehman's Laws of Software Evolution are widely recognized as a fundamental framework for understanding how software systems evolve over time. These laws were introduced by British computer scientist, Meir Lehman, in the 1980s(!) in his paper “Programs, Life Cycles, and Laws of Software Evolution”, and they remain relevant to this day.   Lehman's Laws describe how software systems evolve and how they can be managed over their lifespan. In this blog post, we will explore these laws in detail and their significance in software development.   From the paper :     The first law of software evolution states that  software systems must evolve or they will become progressively less useful . This law recognizes that software is not static and unchanging, but instead must adapt and grow to meet changing user needs and technological advancements. Failure to evolve software systems can result in decreased usefulness or even obsolescence. This means
</p>
</div>
</article>
</div>
</div></div>
</main>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bored? How about trying a Linux speedrun? (2020) (196 pts)]]></title>
            <link>https://rachelbythebay.com/w/2020/04/11/pengrun/</link>
            <guid>36964300</guid>
            <pubDate>Tue, 01 Aug 2023 22:46:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rachelbythebay.com/w/2020/04/11/pengrun/">https://rachelbythebay.com/w/2020/04/11/pengrun/</a>, See on <a href="https://news.ycombinator.com/item?id=36964300">Hacker News</a></p>
Couldn't get https://rachelbythebay.com/w/2020/04/11/pengrun/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Unconfirmed video showing potential LK-99 sample exhibiting the Meissner effect (283 pts)]]></title>
            <link>https://twitter.com/zebulgar/status/1686498517227814912</link>
            <guid>36964107</guid>
            <pubDate>Tue, 01 Aug 2023 22:28:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/zebulgar/status/1686498517227814912">https://twitter.com/zebulgar/status/1686498517227814912</a>, See on <a href="https://news.ycombinator.com/item?id=36964107">Hacker News</a></p>
Couldn't get https://twitter.com/zebulgar/status/1686498517227814912: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Transform your Android device into a Linux desktop (213 pts)]]></title>
            <link>https://mrs-t.medium.com/transform-your-android-device-into-a-linux-desktop-110a3d084ac6</link>
            <guid>36963200</guid>
            <pubDate>Tue, 01 Aug 2023 21:24:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mrs-t.medium.com/transform-your-android-device-into-a-linux-desktop-110a3d084ac6">https://mrs-t.medium.com/transform-your-android-device-into-a-linux-desktop-110a3d084ac6</a>, See on <a href="https://news.ycombinator.com/item?id=36963200">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><h2 id="825e">A Step-by-Step Guide on Setting up Termux with a Desktop Environment</h2><div><a rel="noopener follow" href="https://mrs-t.medium.com/?source=post_page-----110a3d084ac6--------------------------------"><div aria-hidden="false"><p><img alt="Mrs. T" src="https://miro.medium.com/v2/resize:fill:88:88/1*bQa88IpP8bN8e5NyMBzpxg.jpeg" width="44" height="44" loading="lazy"></p></div></a></div></div><figure><figcaption>Running Termux with XFCE on my Phone</figcaption></figure><p id="9c35">Have you ever wanted to do more with your phone, like setting up a Webserver or a Node.js server and running a web app directly on your phone? Or doing some coding on the go? Yes, I have too. With Termux, you can run a full Linux Desktop on your Android device, and here’s how.</p><h2 id="40c2">What is Termux?</h2><p id="470b">Termux is an Android App that emulates a&nbsp;Linux&nbsp;shell. It does not require root privileges and is entirely free. While it runs out of the box, setting up more advanced features — such as installing a Desktop Environment — requires some configuration. This article provides a step-by-step guide on how to set up Termux on your Android device.</p><figure><figcaption>Termux — Terminal Emulator for Android</figcaption></figure><h2 id="3ac8">Prerequisites</h2><p id="ed17">We need at least Android 7 or Fire OS 6 to run the newest version&nbsp;of&nbsp;Termux.</p><h2 id="aa84">Installation</h2><p id="8bd8">Unfortunately, the Play Store only offers an outdated version of the app from 2020, and the developer intends to remove it from the Play Store&nbsp;entirely. Instead, Termux has moved to F-Droid. For those unfamiliar, F-Droid is an app store focused on free and open-source software.</p><ul><li id="d891">To get started, we download the F-Droid APK from <a href="https://f-droid.org/" rel="noopener ugc nofollow" target="_blank">this link</a>. If you’ve never sideloaded APKs before, you’ll need to grant permission to download an APK from a source other than the Play Store.</li><li id="ab89">Next, we install F-Droid, and again, if you’re new to this process, you’ll need to grant permission to install an APK outside the Play Store.</li><li id="4763">Once we have installed F-Droid, we open it and search for <em>“Termux, Terminal emulator with packages”</em>. We Install Termux from there and grant permission to F-Droid to install APKs when prompted again.</li><li id="2113">After successfully installing Termux, we could technically uninstall F-Droid, but keep in mind that we’ll need to use F-Droid to keep Termux up-to-date.</li></ul><h2 id="c74a">Basic Configuration</h2><p id="d4e3">We can launch Termux and start using it. However, it is recommended to set up a few things first.</p><h2 id="048f">Updating the Package Manager</h2><pre><span id="5fbe">pkg upgrade</span></pre><p id="9b77">As with every fresh Linux installation, we should update the package manager. We use <em>pkg upgrade</em> to update the package information and installed packages to the newest version.</p><h2 id="0795">Setting up storage</h2><pre><span id="caf8">termux-setup-storage<br></span></pre><p id="46e2">To access the device storage and SD card, we’ll need to run the appropriate setup command. This will create a storage folder in our home directory.</p><pre><span id="35f2">&lt;home&gt;<br>  |- storage<br>     |- dcim      -&gt; /storage/emulated/0/DCIM<br>     |- downloads -&gt; /storage/emulated/0/Download<br>     |- movies    -&gt; /storage/emulated/0/Movies<br>     |- music     -&gt; /storage/emulated/0/Music<br>     |- pictures  -&gt; /storage/emulated/0/Pictures<br>     |- shared    -&gt; /storage/emulated/0/<br></span></pre><p id="2f87">Inside are symlinks (shortcuts) to dcim, downloads, movies, music, pictures and shared. Shared is our internal storage under <em>/storage/emulated/0/.</em> <br>The Linux system files can be found under <em>/data/data/com.termux/files/usr/,</em> or <em>~/../usr/</em> relative to our home directory.</p><h2 id="d428">Configure extra keys</h2><p id="4be3">You might have noticed that Termux provides a convenient key bar with extra keys, which you’ll likely use frequently. In my opinion, the default layout of this extra key bar is already great, but if we want to customise it, we can do so by editing the Termux config file.</p><pre><span id="ca04">nano ~/.termux/termix.properties<br></span></pre><p id="d3aa">We simply scroll down to the “extra-Keys” section, where we’ll find various examples already present in the file. To experiment with different layouts, we can comment in one of the examples and make adjustments as needed. After editing, we save the file and reload the configuration with the following command:</p><pre><span id="795b">termux-reload-settings<br></span></pre><p id="79cc">For further details, you can refer to the <a href="https://wiki.termux.com/wiki/Touch_Keyboard" rel="noopener ugc nofollow" target="_blank">Termux Wiki</a>.</p><h2 id="b80a">Using the Package Manager</h2><p id="642d">Although Termux uses dpkg and apt, it is advised to utilise the pkg tool for installing and removing packages. Pkg acts as a layer on top of apt and provides convenient utilities, including automatic update checks before package installation. Here are the most frequently used pkg commands:</p><pre><span id="c2eb">pkg search &lt;query&gt;<br></span></pre><p id="a211">The search query can be either an exact or partial name or even a term found in the package’s description.</p><pre><span id="1e13">pkg install &lt;package-name&gt;<br></span></pre><p id="7d5d">Installs the package &lt;package-name&gt;.</p><pre><span id="2048">pkg upgrade</span></pre><p id="ffc9">Updates package information and installs the newest version of all installed packages. It is recommended to do an upgrade before installing new packages.</p><pre><span id="0451">pkg uninstall &lt;package-name&gt;<br></span></pre><p id="80e6">Uninstalls the package &lt;package-name&gt;.</p><blockquote><p id="38c7"><strong>Warning:</strong> even though Termux uses&nbsp;dpkg&nbsp;and&nbsp;apt, you cannot use&nbsp;.deb packages from Debian-based distributions Like Ubuntu, Mint and so on, because Termux does not conform to the standard Linux Files System Hierarchy (FSH). This is one of the workarounds Termux uses to recumbent the need for root access that other similar apps require. Termux packages are specially tailored to work with the non-Standard FSH.</p></blockquote><h2 id="3dee">If we don’t need a Desktop Environment</h2><p id="485b">We may not need a desktop environment, depending on what we want to do. If a shell is all we need, then we’re all set. We can proceed to install our toolchain, framework, web server, or any other applications we intend to use Termux for. From this point onward, we can use Termux similarly to any other Linux distribution and follow most Linux tutorials.</p><p id="4abf">However, if we need a desktop environment,&nbsp;the&nbsp;next&nbsp;section will&nbsp;explain&nbsp;how&nbsp;to&nbsp;set&nbsp;it&nbsp;up.</p><h2 id="e216">Setting up a Desktop Environment</h2><p id="0cc2">We’ll use VNC, a free remote desktop software, for this. This method has proven to be the most effective for me. However, if you prefer to set up an XServer, you can find instructions on the <a href="https://wiki.termux.com/wiki/GraphicalEnvironment" rel="noopener ugc nofollow" target="_blank">Termux Wiki</a>.</p><p id="1ec9">First, we enable the x11-repository</p><pre><span id="ae56">pkg install x11-repository</span></pre><p id="b6f6">Next, we install a VNC server</p><pre><span id="21fc">pkg install tigervnc<br></span></pre><p id="a67c">Now, we start the VNC server on localhost.</p><pre><span id="4f75">vncserver -localhost<br></span></pre><p id="c7ab">We’ll be prompted to set a password.</p><p id="5460">If the server starts successfully, we’ll see the following message.</p><pre><span id="4c15">New 'localhost:1 ()' desktop is localhost:1<p>Starting applications specified in /data/data/com.termux/files/home/.vnc/xstartup<br>Log file is /data/data/com.termux/files/home/.vnc/localhost:1.log</p></span></pre><p id="d9ef">This is the address where we can connect with our client.</p><p id="5ec7">Now we have to tell Termux which display is used for graphical output by setting the environment variable DISPLAY to the appropriate address and port. We can type <em>export DISPLAY=":1"</em> directly into Termux, and it will work, but that way, we would have to do this each time we start Termux. A better way to do this is to put it into our <em>.bashrc</em> file. All commands inside <em>.bashrc</em> are automatically executed when we start a new terminal session.</p><p id="5f26">We open&nbsp;the file with nano and add export DISPLAY=":1".</p><pre><span id="b367">nano ~/.bashrc</span></pre><p id="eef7">We can exit nano with ctrl+x. Then we’ll be prompted to save the file.<br>Since <em>.bashrc</em> is only automatically executed on start-up, we’ll have to do this manually.</p><pre><span id="8883">. ~/.bashrc<br></span></pre><blockquote><p id="264e"><strong>Side note:</strong> ":1" is a short form for "localhost:1"</p></blockquote></div><div><p id="a62a">Alright, it’s time to install a desktop environment. Termux supports three options: XFCE, LXQt, and MATE. If you’re not familiar with Linux, I suggest avoiding MATE for the time being, as it currently lacks a meta package. This means we would need to manually install all the required packages, which can be more complex. Instead, we want to stick to XFCE or LXQt, as they have meta-packages that simplify the installation process.</p><p id="dd5c">For this guide, we’ll use XFCE.</p><pre><span id="b79a">pkg install xfce4<br></span></pre><p id="c8d8">This installs all required XFCE packages — it may take a while.</p><p id="acd8">After the installation, we must tell the VNC server to start the XFCE desktop when a client connects.</p><p id="c205">We open&nbsp;the&nbsp;configuration&nbsp;file.</p><pre><span id="1812">nano .vnc/.xstartup<br></span></pre><p id="1272">We can either delete or comment out everything that is inside the file. It should only contain these two lines:</p><pre><span id="a65c">#!/data/data/com.termux/files/usr/bin/sh<br>xfce4-session &amp;<br></span></pre><p id="5897">A browser and terminal are not included in the meta package and must be installed separately. I recommend the packages: <em>firefox</em> and <em>xfce4-terminal</em>.</p><p id="ffa3">The server side is done. Now we go to the Play Store and download a VNC Viewer. We’ll use <a href="https://play.google.com/store/apps/details?id=com.realvnc.viewer.android" rel="noopener ugc nofollow" target="_blank">RealVNC Viewer</a> in this tutorial.</p><p id="dc6b">We open RealVNC and add a new connection with the plus button. Remember the address we saw when we started the VNC server? We’ll need to set it here, but we must add 5900 to the port. So for “localhost:1", we need to set it to localhost:5901. Some VNC Viewers have problems with localhost, so to be sure, we type the numerical version “127.0.0.1:5901”.</p><figure><figcaption>Adding a new connection to RVNC</figcaption></figure><p id="adfe">We save it and are done. Now, we can connect. We’ll be prompted for the password we set earlier, and we can choose to remember the password. Now we should see the XFCE desktop.</p><p id="3e3f">If you’re familiar with Linux, this might be obvious, but if not: Earlier, we started the VNC server manually with <em>vncserver -localhost</em>. This means we have to start it again if we restart Termux. I would not advise starting the VNC server through <em>.bashrc</em> because we would start another VNC server session each time we open a new terminal window.</p><h2 id="75e0">Conclusion</h2><p id="91e3">As you can see, installing and setting up Termux is very straightforward, and Termux can be used in many ways.</p><p id="7653">I’m using it for:</p><ul><li id="135c">Software development with C/C++, python and node.js.</li><li id="2345">To edit files with Vim.</li><li id="11e5">As a git client — which I find better than any Android git client available.</li><li id="9b09">As a Webserver to quickly host something.</li><li id="0056">And for various shell tools like wget, curl, unzip, ssh, and more.</li></ul><p id="183a">Termux is the most flexible tool you can have on your Android phone, tablet or even your Kindle tablet. Try it out for yourself!</p><h2 id="19e5">Sources</h2><p id="c083"><a href="https://wiki.termux.com/wiki/Main_Page" rel="noopener ugc nofollow" target="_blank">https://wiki.termux.com/wiki/Main_Page</a></p><p id="26ed"><a href="https://wiki.termux.com/wiki/Package_Management" rel="noopener ugc nofollow" target="_blank">https://wiki.termux.com/wiki/Package_Management</a></p><p id="f72d"><a href="https://wiki.termux.com/wiki/Touch_Keyboard" rel="noopener ugc nofollow" target="_blank">https://wiki.termux.com/wiki/Touch_Keyboard</a></p><p id="9a8d"><a href="https://wiki.termux.com/wiki/Graphical_Environment" rel="noopener ugc nofollow" target="_blank">https://wiki.termux.com/wiki/Graphical_Environment</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dear Websites, Stop Asking for Ransom Sign-Ups (300 pts)]]></title>
            <link>https://iamvishnu.com/posts/randsom-signup</link>
            <guid>36962502</guid>
            <pubDate>Tue, 01 Aug 2023 20:41:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://iamvishnu.com/posts/randsom-signup">https://iamvishnu.com/posts/randsom-signup</a>, See on <a href="https://news.ycombinator.com/item?id=36962502">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>It’s a dark pattern found in many websites to make its visitors enter some data, and when submitting, it asks to sign up or sign in to see the results. If the users are not interested in signing up, then the time they spent on entering the data is wasted.</p>

<p>I recent experience made me write this post.</p>

<p>A couple of days back I found a website in ProductHunt that offered its users to generate blog post ideas based on the topic that we give. I opened the website, and on the landing page itself, there was a text input field asking for the topic. I thought for a while and typed in a topic, and clicked on the “submit” button, eagerly looking for the blog ideas to be generated. As soon as I submitted it, it asked for me to sign up or sign in to proceed. But I didn’t want to sign up on that website.</p>

<p><strong>It feels like stealing my data and asking for a ransom</strong> — the results are returned only if I sign up.</p>

<p>Had it been made very clear that I need to sign up to enjoy the service in the beginning itself, it would have appeared more authentic to me. What if it asks for payment after the sign-up process is completed? What if I have to agree to some weird terms and conditions only after giving my data? Are there more hidden ransom surprises like this?</p>

<p>Another popular example is image converters. I opened a website that offered to convert PNGs to SVGs. I browsed and selected the files. Uploaded everything. It showed all the “processing” animations for a couple of seconds. While I was waiting for the SVGs, it opened a new page asking me to sign up to download the generated SVG files. Why did you waste my time, why did you take my PNG files to your server and not giving me the results? Why didn’t you tell me that I have to sign up in the first place?</p>

<p>Dear websites, if you want to be truthful to your users, tell them beforehand what are the conditions to use the service. Don’t collect their data first and hold it for ransom signups.</p>

<ul>
  <li>If you want me to sign up for your service, that’s totally fine, but tell me beforehand.</li>
  <li>If you need payment, that’s okay, but tell me beforehand, not after collecting my data and holding the results.</li>
  <li>Or, if you really want to get me to sign up, you could give me a limited version of the service that doesn’t require me to sign up, so that I can evaluate the service and eventually sign up if I find it worthy.</li>
</ul>

<p>Dear websites, please stop asking for ransom signups.</p>

  </div>

</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[eGPU: A 750 MHz Class Soft GPGPU for FPGA (126 pts)]]></title>
            <link>https://arxiv.org/abs/2307.08378</link>
            <guid>36961996</guid>
            <pubDate>Tue, 01 Aug 2023 20:11:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2307.08378">https://arxiv.org/abs/2307.08378</a>, See on <a href="https://news.ycombinator.com/item?id=36961996">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
    
    
      
    
  
  
  
    <p><a aria-describedby="download-button-info" href="https://arxiv.org/pdf/2307.08378">Download PDF</a></p><blockquote>
            <span>Abstract:</span>  This paper introduces the eGPU, a SIMT soft processor designed for FPGAs.
Soft processors typically achieve modest operating frequencies, a fraction of
the headline performance claimed by modern FPGA families, and obtain
correspondingly modest performance results. We propose a GPGPU architecture
structured specifically to take advantage of both the soft logic and embedded
features of the FPGA. We also consider the physical location of the embedded
memories and DSP Blocks relative to the location and number of soft logic
elements in order to have a design with balanced resources. Our goal is to
create a high performance soft processor able to implement complex portions of
FPGA system designs, such as the linear solvers commonly used in wireless
systems, through push-button compilation from software. The eGPU architecture
is a streaming multiprocessor (SM) machine with 512 threads. Each SM contains
16 scalar processors (SP). Both IEEE754 FP32 and INT32 integer arithmetic are
supported. We demonstrate a single SM eGPU in an Intel Agilex device, requiring
5600 ALMs and 24 DSP Blocks, which closes timing at over 770 MHz from a
completely unconstrained compile. Multiple eGPUs can also be tightly packed
together into a single Agilex FPGA logic region, with minimal speed penalty.

    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Martin Langhammer [<a href="https://arxiv.org/show-email/6e042255/2307.08378">view email</a>]
      <br>
    <strong>[v1]</strong>
    
        Mon, 17 Jul 2023 10:32:51 UTC (62 KB)<br>
    </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hacker mods an M1 Mac mini to receive power over Ethernet instead of AC (398 pts)]]></title>
            <link>https://www.inferse.com/660551/hardware-hacker-mods-an-m1-mac-mini-to-receive-power-over-ethernet-instead-of-the-ac-input/</link>
            <guid>36961734</guid>
            <pubDate>Tue, 01 Aug 2023 19:54:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.inferse.com/660551/hardware-hacker-mods-an-m1-mac-mini-to-receive-power-over-ethernet-instead-of-the-ac-input/">https://www.inferse.com/660551/hardware-hacker-mods-an-m1-mac-mini-to-receive-power-over-ethernet-instead-of-the-ac-input/</a>, See on <a href="https://news.ycombinator.com/item?id=36961734">Hacker News</a></p>
Couldn't get https://www.inferse.com/660551/hardware-hacker-mods-an-m1-mac-mini-to-receive-power-over-ethernet-instead-of-the-ac-input/: Error: Request failed with status code 500]]></description>
        </item>
        <item>
            <title><![CDATA[Evading JavaScript anti-debugging techniques (183 pts)]]></title>
            <link>https://www.nullpt.rs/evading-anti-debugging-techniques</link>
            <guid>36961445</guid>
            <pubDate>Tue, 01 Aug 2023 19:35:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nullpt.rs/evading-anti-debugging-techniques">https://www.nullpt.rs/evading-anti-debugging-techniques</a>, See on <a href="https://news.ycombinator.com/item?id=36961445">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Debuggers serve as invaluable tools that empower developers to halt code execution and thoroughly
analyze its behavior at any given moment. By utilizing a debugger, developers can efficiently
identify and resolve issues within their code, making it an indispensable part of their toolkit.</p>
<figure><img alt="Chromium's dev tools debugger" sizes="100vw" srcset="https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fchromium_debugger.webp&amp;w=640&amp;q=75 640w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fchromium_debugger.webp&amp;w=750&amp;q=75 750w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fchromium_debugger.webp&amp;w=828&amp;q=75 828w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fchromium_debugger.webp&amp;w=1080&amp;q=75 1080w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fchromium_debugger.webp&amp;w=1200&amp;q=75 1200w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fchromium_debugger.webp&amp;w=1920&amp;q=75 1920w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fchromium_debugger.webp&amp;w=2048&amp;q=75 2048w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fchromium_debugger.webp&amp;w=3840&amp;q=75 3840w" src="https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fchromium_debugger.webp&amp;w=3840&amp;q=75" width="100" height="100" decoding="async" data-nimg="1" loading="lazy"><figcaption>Chromium's dev tools debugger</figcaption></figure>
<p>Debuggers are equally valuable tools for reverse-engineers, especially when dealing with obfuscated
code that frequently employs name-mangling techniques for variables and functions. By utilizing
debuggers, reverse-engineers can gain crucial insights into the functionality of obscured functions.
Companies that employ client-side protection are aware of this fact and thus devise strategies to thwart
attempts at debugging protected code.</p>
<p>Once upon a time, whenever you tried to open your devtools on Supreme's website, you found yourself trapped
in a pesky debugger loop. This made it incredibly annoying to reverse engineer their anti-bot scripts.</p>
<h2>The Obvious Approach</h2>
<p>In many browsers, there's an option to disable all breakpoints from triggering. While this approach will prevent
getting stuck in a loop, it comes with a trade-off – the debugger's functionality for further analysis becomes
unavailable.</p>
<figure><img alt="Deactivating breakpoints in Chromium" sizes="100vw" srcset="https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fdeactivate_breakpoints.webp&amp;w=640&amp;q=75 640w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fdeactivate_breakpoints.webp&amp;w=750&amp;q=75 750w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fdeactivate_breakpoints.webp&amp;w=828&amp;q=75 828w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fdeactivate_breakpoints.webp&amp;w=1080&amp;q=75 1080w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fdeactivate_breakpoints.webp&amp;w=1200&amp;q=75 1200w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fdeactivate_breakpoints.webp&amp;w=1920&amp;q=75 1920w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fdeactivate_breakpoints.webp&amp;w=2048&amp;q=75 2048w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fdeactivate_breakpoints.webp&amp;w=3840&amp;q=75 3840w" src="https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fdeactivate_breakpoints.webp&amp;w=3840&amp;q=75" width="100" height="100" decoding="async" data-nimg="1" loading="lazy"><figcaption>Deactivating breakpoints in Chromium</figcaption></figure>
<p>For complex scripts like the anti-bot employed by Supreme, this approach was simply not an option.</p>
<h2>Trying an Extension</h2>
<p>Greasyfork scripts such as Anti Anti-debugger attempt to bypass these debugger traps by overriding the caller's
function body, removing the debugger statements, and <code>eval</code> the new function. While this approach seemed promising,
it unfortunately does not succeed with JScrambler-protected scripts due to the evaluation of debugger traps in a distinct
context.</p>
<p>They also utilize integrity checks<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup> and hide debugger calls inside of further obfuscated eval functions<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="true" aria-describedby="footnote-label">2</a></sup>.</p>
<figure><img alt="JScrambler's debugger trap being called from an eval context" sizes="100vw" srcset="https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fjscrambler_debug_trap.webp&amp;w=640&amp;q=75 640w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fjscrambler_debug_trap.webp&amp;w=750&amp;q=75 750w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fjscrambler_debug_trap.webp&amp;w=828&amp;q=75 828w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fjscrambler_debug_trap.webp&amp;w=1080&amp;q=75 1080w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fjscrambler_debug_trap.webp&amp;w=1200&amp;q=75 1200w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fjscrambler_debug_trap.webp&amp;w=1920&amp;q=75 1920w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fjscrambler_debug_trap.webp&amp;w=2048&amp;q=75 2048w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fjscrambler_debug_trap.webp&amp;w=3840&amp;q=75 3840w" src="https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fjscrambler_debug_trap.webp&amp;w=3840&amp;q=75" width="100" height="100" decoding="async" data-nimg="1" loading="lazy"><figcaption>JScrambler's debugger trap being called from an eval context</figcaption></figure>
<h2>The Final Approach</h2>
<p>The approach my friend Jordin and I ultimately adopted involved renaming the debugger keyword entirely.
By renaming it to something like "banana," the debugger would no longer trigger on occurrences of the <code>debugger</code>
keyword. To achieve this, we built customized version of Firefox. If you're interested in trying
this out, here's the patch we came up with.</p>
<pre><code><span><span>--- a/js/src/frontend/ReservedWords.h</span>
</span><span><span>+++ b/js/src/frontend/ReservedWords.h</span>
</span><span><span>@@ -20,7 +20,7 @@</span>
</span><span><span><span> </span><span>  MACRO(catch, catch_, TokenKind::Catch)                \
</span></span></span><span><span><span></span><span> </span><span>  MACRO(const, const_, TokenKind::Const)                \
</span></span></span><span><span><span></span><span> </span><span>  MACRO(continue, continue_, TokenKind::Continue)       \
</span></span></span><span><span><span></span></span><span><span>-</span><span>  MACRO(debugger, debugger, TokenKind::Debugger)        \
</span></span></span><span><span><span></span></span><span><span>+</span><span>  MACRO(ticket_debugger, debugger, TokenKind::Debugger) \
</span></span></span><span><span><span></span></span><span><span> </span><span>  MACRO(default, default_, TokenKind::Default)          \
</span></span></span><span><span><span></span><span> </span><span>  MACRO(delete, delete_, TokenKind::Delete)             \
</span></span></span><span><span><span></span><span> </span><span>  MACRO(do, do_, TokenKind::Do)                         \
</span></span></span><span><span><span></span></span><span>--- a/js/src/vm/CommonPropertyNames.h</span>
</span><span><span>+++ b/js/src/vm/CommonPropertyNames.h</span>
</span><span><span>@@ -107,7 +107,7 @@</span>
</span><span><span><span> </span><span>  MACRO_(currencySign, currencySign, "currencySign")  \
</span></span></span><span><span><span></span><span> </span><span>  MACRO_(day, day, "day")                             \
</span></span></span><span><span><span></span><span> </span><span>  MACRO_(dayPeriod, dayPeriod, "dayPeriod")           \
</span></span></span><span><span><span></span></span><span><span>-</span><span>  MACRO_(debugger, debugger, "debugger")              \
</span></span></span><span><span><span></span></span><span><span>+</span><span>  MACRO_(ticket_debugger, debugger, "ticket_debugger")\
</span></span></span><span><span><span></span></span><span><span> </span><span>  MACRO_(decimal, decimal, "decimal")                 \
</span></span></span></code></pre>
<p>Finding this was daunting as reading through a browser's codebase is never a fun time. A simple
grep for a keyword like "debugger" could produce thousands of results, making the search
even more challenging.</p>
<p>Many builds later, we tested our patch by loading the anti-bot script <code>ticket.js</code> and hooking
<code>Array.prototype.slice</code><sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="true" aria-describedby="footnote-label">3</a></sup> to call our new <code>ticket_debugger</code> keyword.</p>
<figure><img alt="Our new ticket_debugger keyword triggers a breakpoint" sizes="100vw" srcset="https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fticket_debugger.webp&amp;w=640&amp;q=75 640w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fticket_debugger.webp&amp;w=750&amp;q=75 750w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fticket_debugger.webp&amp;w=828&amp;q=75 828w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fticket_debugger.webp&amp;w=1080&amp;q=75 1080w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fticket_debugger.webp&amp;w=1200&amp;q=75 1200w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fticket_debugger.webp&amp;w=1920&amp;q=75 1920w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fticket_debugger.webp&amp;w=2048&amp;q=75 2048w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fticket_debugger.webp&amp;w=3840&amp;q=75 3840w" src="https://www.nullpt.rs/_next/image?url=%2Fposts%2Fevading-anti-debugging-techniques%2Fticket_debugger.webp&amp;w=3840&amp;q=75" width="100" height="100" decoding="async" data-nimg="1" loading="lazy"><figcaption>Our new ticket_debugger keyword triggers a breakpoint</figcaption></figure>
<h2>It works!</h2>
<p>We introduced a new <code>ticket_debugger</code> keyword, which not only triggers a breakpoint but also resolves
the issue of the previous debugger infinite loop. We also extended this custom browser to
automatically retrieve the anti-bot's obfuscated encryption round keys and convert them back to their
original form, but I'll cover that in a later post :)</p>
<section data-footnotes="true">
<ol>
<li id="user-content-fn-1">
<p><a href="https://docs.jscrambler.com/code-integrity/documentation/transformations/anti-tampering">https://docs.jscrambler.com/code-integrity/documentation/transformations/anti-tampering</a> <a href="#user-content-fnref-1" data-footnote-backref="true" aria-label="Back to content">↩</a></p>
</li>
<li id="user-content-fn-2">
<p><a href="https://docs.jscrambler.com/code-integrity/documentation/transformations/self-defending">https://docs.jscrambler.com/code-integrity/documentation/transformations/self-defending</a> <a href="#user-content-fnref-2" data-footnote-backref="true" aria-label="Back to content">↩</a></p>
</li>
<li id="user-content-fn-3">
<p>Supreme's anti-bot system relied on the AES library <a href="https://github.com/ricmoo/aes-js">aes-js</a>, which utilizes <code>Array.prototype.slice</code> internally for managing encryption keys used on crucial cookies <a href="#user-content-fnref-3" data-footnote-backref="true" aria-label="Back to content">↩</a></p>
</li>
</ol>
</section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Permission.site (2016) (119 pts)]]></title>
            <link>https://permission.site/</link>
            <guid>36961050</guid>
            <pubDate>Tue, 01 Aug 2023 19:14:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://permission.site/">https://permission.site/</a>, See on <a href="https://news.ycombinator.com/item?id=36961050">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>





<hr>

<hr>

<hr>

<p>Async Clipboard API</p>

<p>Notes:</p>
<table>
<tbody><tr>
  <td>Augmented Reality (AR)</td>
  <td>Implemented behind the experimental flag <code>chrome://flags/#enable-experimental-web-platform-features</code>.</td>
</tr>
<tr><td>Encrypted Media (EME)</td>
  <td>May succeed without permission depending on the implementation.<br>
    Attempts to use known key systems. (See the source for the list of supported key systems.)
    <!-- Clear Key is not supported because it is not expected to require permissions. -->
  </td>
</tr>
<tr>
  <td>Async Clipboard</td>
  <td>
    These buttons test the new <a href="https://w3c.github.io/clipboard-apis/">Async Clipboard API</a>.
    <br>
    Note that these tests are different from the "Copy" button on this page, which uses the old
    (synchronous) <code>execCommand</code> method to write to the clipboard.
    <br>
    This feature is implemented behind the experimental flag <code>chrome://flags/#enable-experimental-web-platform-features</code>.
    <br>
    To enable the Content Settings UX for setting clipboard permission, you'll also need to enable
    <code>chrome://flags/#clipboard-content-setting</code>
    <br>
    Note: Only available for secure connections (https).
  </td>
</tr>
<tr>
  <td>WebAuthn Attestation</td>
  <td>
    After pressing the button, you may need to touch your security key before you can see an attestation prompt.
  </td>
</tr>
<tr>
  <td>Device Orientation/Motion</td>
  <td>
    May succeed without permission request depending on the implementation.
  </td>
</tr>
</tbody></table>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[“Web Environment Integrity”: Locking Down the Web (172 pts)]]></title>
            <link>https://brave.com/web-standards-at-brave/9-web-environment-integrity/</link>
            <guid>36960882</guid>
            <pubDate>Tue, 01 Aug 2023 19:03:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brave.com/web-standards-at-brave/9-web-environment-integrity/">https://brave.com/web-standards-at-brave/9-web-environment-integrity/</a>, See on <a href="https://news.ycombinator.com/item?id=36960882">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <section>
        
            <p><a href="https://brave.com/web-standards-at-brave/">WebStandards@Brave</a></p>
        
        
        
        
            <p>By the Brave Web Standards Team</p>
        
    </section>

    <section><p><em>by Peter Snyder, VP of Privacy Engineering and Senior Privacy Researcher</em></p>
<p>Brave strongly opposes Google’s “<a href="https://github.com/RupertBenWiser/Web-Environment-Integrity/blob/main/explainer.md" target="_blank" rel="noopener">Web Environment Integrity</a>” (WEI) proposal. As with many of Google’s recent changes and proposals regarding the Web, “Web Environment Integrity” would move power away from users, and toward large websites, including the websites Google itself runs. Though Brave uses Chromium, Brave browsers do not (and will not) include WEI.<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> Further, some browsers have introduced other features similar to, though more limited than, WEI (e.g., certain parts of WebAuthn and Privacy Keys); Brave is considering how to best restrict these features without breaking benign uses.</p>
<p>Google’s WEI proposal is frustrating, but it’s not surprising. WEI is simply the latest in Google’s ongoing efforts to prevent browser users from being in control of how they read, interact with, and use the Web. Google’s <a href="https://brave.com/web-standards-at-brave/3-web-bundles/">WebBundles proposal</a> makes it more difficult for users to block or filter out unwanted page content, <a href="https://brave.com/web-standards-at-brave/8-first-party-sets/">Google’s First Party Sets feature</a> makes it more difficult for users to make decisions around which sites can track users, and <a href="https://www.eff.org/deeplinks/2021/12/chrome-users-beware-manifest-v3-deceitful-and-threatening" target="_blank" rel="noopener">Google’s weakening of browser extensions</a> straightforwardly makes it harder for users to be in control of their Web experience by crippling top ad-and-tracker-blocking extensions such as <a href="https://ublockorigin.com/" target="_blank" rel="noopener">uBlock Origin</a>.<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> This is unfortunately far from a complete list of recent, similar user-harming Web proposals from Google. Again, Brave <a href="https://github.com/brave/brave-browser/wiki/Deviations-from-Chromium-%28features-we-disable-or-remove%29" target="_blank" rel="noopener">disables or modifies</a> all of these features in Brave’s browsers.</p>
<p>The Web is the world’s most popular, and therefore most important, open system for sharing information and distributing applications. It is critical that users stay in control of how they interact with the Web, and for the Web not to be reduced to a series of take-it-or-leave-it black-boxes that users can’t inspect, can’t understand, and can’t modify. Google’s WEI proposal (like many other Google proposals) intentionally shifts power away from users, and towards large websites and advertisers.</p>
<p>WEI is the latest step in a <a href="https://brave.com/privacy-updates/18-de-amp/">terrible</a> <a href="https://brave.com/web-standards-at-brave/6-privacy-sandbox-concerns/">direction</a> Google is pushing for the Web. Web users deserve a browser that doesn’t treat them as enemies that need to be restricted and controlled.</p>

</section>
    
        
<section>
    <h2>Related articles</h2>
    <div>
        <article>
            
            
        </article>
        <article>
            
            
        </article>
        <article>
            <div>
                <h2><a href="https://brave.com/web-standards-at-brave/6-privacy-sandbox-concerns/">Privacy And Competition Concerns with Google’s Privacy Sandbox</a></h2>
                <p>The UK CMA (along with other regulators and web activists) are largely evaluating Google’s Privacy Sandbox as an isolated, independent set of features. Evaluations that fail to consider how Privacy Sandbox will interact with other upcoming Google proposals will miss how radical and harmful Privacy Sandbox will be to the Web in practice. This piece presents how Privacy Sandbox, when considered with other upcoming Chrome features, will harm user choice, privacy, and competition on the Web.</p>
                <p><a href="https://brave.com/web-standards-at-brave/6-privacy-sandbox-concerns/">Read this article →</a>
            </p></div>
            
        </article>
        
        
    </div>
</section>

    
    

    
    
    
    
    <div>
            <h2>Ready to Brave the new internet?</h2>
            <p>Brave is built by a team of privacy focused, performance oriented pioneers of the web. Help us fix browsing together.</p>

            <p><a data-show-platform="true" data-build-channel="release" data-allows-dynamic-ref-code="true" href="https://laptop-updates.brave.com/download/BRV001">Download Brave</a>
        </p></div>

    
    
    
    <i></i>
</div><section id="interstitial">
        <div>
            <p><a href="#" aria-label="close modal"><img src="https://brave.com/static-assets/icons/close-icon.svg" alt="close"></a>
            </p>
            <p><a href="#" aria-label="close modal"><img src="https://brave.com/static-assets/icons/close-icon.svg" alt="close"></a>
            </p>
        </div>
        <div>
            
            
            <h2>Almost there…</h2>
            <h5>You’re just 60 seconds away from the best privacy online</h5>
            <p>
                If your download didn’t start automatically,
                <span><a data-show-platform="false" data-build-channel="release" data-allows-dynamic-ref-code="true" href="https://laptop-updates.brave.com/download/BRV001">click here</a>.</span>
            </p>
            <ol>
                
                <li>
                    <h3>Download Brave</h3>
                    
                        <p>Click “Save” in the window that pops up, and wait for the download to complete.</p>
                        <p>Wait for the download to complete (you may need to click “Save” in a window that pops up).</p>
                    
                </li>
                
                <li>
                    <h3>Run the installer</h3>
                    
                        <p>Click the downloaded file at the bottom left of your screen, and follow the instructions to install Brave.</p>
                        <p>Click the downloaded file at the top right of your screen, and follow the instructions to install Brave.</p>
                        <p>Click the downloaded file, and follow the instructions to install Brave.</p>
                    
                </li>
                
                <li>
                    <h3>Import settings</h3>
                    
                        <p>During setup, import bookmarks, extensions, &amp; passwords from your old browser.</p>
                    
                </li>
                
            </ol>

            <p><a href="https://support.brave.com/hc/en-us/articles/360025390311-How-do-I-download-and-install-Brave-" target="_blank" rel="noopener">Need help?</a></p>
            
            
        </div>
        <p><span>
                <img src="https://brave.com/static-assets/images/brave-logo-no-shadow.png" alt="Brave logo">
                <span>Click this file to install Brave</span>
                <img src="https://brave.com/static-assets/icons/arrow-down.svg" alt="">
            </span>
        </p>
        <p><span>
                <img src="https://brave.com/static-assets/icons/arrow-down.svg" alt="">
            </span>
            <span>Click this file to install Brave</span>
            <img src="https://brave.com/static-assets/images/brave-logo-no-shadow.png" alt="Brave logo">
        </p>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Miyazaki’s Magical Food: An ode to anime’s best cooking scenes (122 pts)]]></title>
            <link>https://www.seriouseats.com/studio-ghibli-anime-best-food-scenes</link>
            <guid>36960831</guid>
            <pubDate>Tue, 01 Aug 2023 19:01:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.seriouseats.com/studio-ghibli-anime-best-food-scenes">https://www.seriouseats.com/studio-ghibli-anime-best-food-scenes</a>, See on <a href="https://news.ycombinator.com/item?id=36960831">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mntl-sc-page_1-0" data-sc-sticky-offset="100" data-sc-ad-label-height="11" data-sc-ad-track-spacing="100" data-sc-min-track-height="250" data-sc-max-track-height="600" data-sc-breakpoint="50em" data-sc-load-immediate="5" data-sc-content-positions="[1, 1, 1, 1250, 1, 1, 1, 1]" data-bind-scroll-on-start="true">
<p id="mntl-sc-block_1-0">
Last year, I got a small tattoo on my arm of a shiny red bento box filled with white rice, divided in half by a small, whole grilled fish. On one side, the rice is topped with sakura denbu, a cherry blossom-pink condiment made from pulverized cod, along with a small pile of glistening soy beans; on the other, the rice is unadorned save for a lone umeboshi, or pickled plum. The bento is an exact replica of the one that appears in the 1988 Hayao Miyazaki film <em>My Neighbor Totoro</em>. I saw the film for the first time when I was seven, and I haven’t been able to stop thinking about that scene since.
</p>

<p id="mntl-sc-block_1-0-2">
The movie is considered by many to be a classic of Japanese animation, which is more commonly known as anime, a term used to refer to animated movies and television series produced in Japan or inspired by Japanese animation styles. Since the 1960s, when hit shows like <em>Astro Boy</em> and <em>Speed Racer</em> were adapted for and exported to foreign markets, anime’s popularity has continued to grow worldwide, which in turn has created a kind of subculture of anime fans across the globe. But within that community there’s a sub-subculture of people, like me, who love anime food.
</p>

<p id="mntl-sc-block_1-0-4">
“Anime food is an escape for a lot of people. It has a very soothing effect." says Christina Song, who created a popular anime food fan Instagram account (<a href="https://www.instagram.com/anime__food/" data-component="link" data-source="inlineLink" data-type="externalLink" data-ordinal="1">@anime__food</a>) in 2017. “It’s like a moment in time that is perfectly frozen.”
</p>

<p id="mntl-sc-block_1-0-6">
Food appears as its best, exaggerated self in anime. The medium takes the most attractive and appetizing aspects of food and enhances them: Every soft pudding has an irresistible luster, and each heaping bowl of noodles is wreathed in just the right amount of steam. While examples of perfectly presented food may be found in many anime, within the anime-food loving sub-subculture, some of the most popular scenes are from movies made by Studio Ghibli. Co-founded by Miyazaki in 1985, Ghibli is seen as one of the most influential animation studios in the world, a reputation built on the critical acclaim and box office success of movies like <em>My Neighbor Totoro</em>, <em>Princess Mononoke</em>, and <em>Spirited Away</em>, which won the Oscar for Best Animated Film in 2003.
</p>

<p id="mntl-sc-block_1-0-8">
“I've loved Studio Ghibli movies since I was a kid, and I knew I wanted to eat the food from the movie one day,” says <a href="https://www.instagram.com/en93kitchen/" data-component="link" data-source="inlineLink" data-type="externalLink" data-ordinal="1">@en93kitchen</a>, a woman (she prefers to remain anonymous) who runs a cooking school in Kanagawa, Japan, that became popular in the US after she recreated food from <a href="https://www.boredpanda.com/anime-food-recreated-real-life-en93kitchen/" data-component="link" data-source="inlineLink" data-type="externalLink" data-ordinal="2">the most iconic Studio Ghibli moments</a>.
</p>

<p id="mntl-sc-block_1-0-10">
The food scenes in Studio Ghibli productions, particularly the most famous ones from Miyazaki’s movies, are distinct from other anime in that the narratives slow down to accommodate the cooking, eating, and sharing of food. These scenes are often tightly intertwined with characters’ narratives or the overarching storyline, such as the one in which two parents transform into pigs while gorging themselves at the beginning of <em>Spirited Away</em>, creating the conflict that the protagonist of the film, Chihiro, has to ultimately resolve. From the tiniest details of a cooking sequence, like an eight-year-old chopping vegetables in a way that suggests she’s had to do it for years because of an absent parent, to the presentation of beautiful dishes, like a steaming fish pie with a golden brown fish carved into its crust, the focus on preparing and enjoying food seems to make these animated worlds come to life.
</p>

<p id="mntl-sc-block_1-0-12">
“Food and eating is so closely associated with family and emotion. It’s a really important part of the actual storytelling and scenesetting,” says Dave Jesteadt, the president of GKIDS, a company that produces animation and has handled the North American distribution of Studio Ghibli films since 2010. “Is it made with love and care because it’s an intimate family scene and this is something that shows how close the family is, or is it something that shows how fantastic the setting is?”
</p>

<p id="mntl-sc-block_1-0-14">
To celebrate the culinary excellence of Studio Ghibli and Miyazaki’s films, here are some of my favorite food scenes.
</p>

<h2 id="mntl-sc-block_1-0-16"> <span> Ponyo: Haaaaaaaam! </span> </h2>
<figure id="mntl-sc-block_1-0-17">

<figcaption id="mntl-figure-caption_1-0">
<span>From <em>Ponyo</em> © 2008 Studio Ghibli - NDHDMT .</span>
</figcaption>
</figure>
<p id="mntl-sc-block_1-0-18">
<em>Ponyo</em> explores the underwater world of magic through the eyes of its eponymous hero, a fish who transforms into a little girl and becomes friends with a human boy named Sosuke. One of the most memorable moments in the film has to do with ham. “Ponyo’s love of ham is basically a meme at this point,” says Jesteadt with a chuckle.
</p>

<p id="mntl-sc-block_1-0-20">
In this famous food scene, Sosuke’s mother makes instant ramen for the kids in two bright blue and coral lidded bowls. Sosuke gives the package of ramen a hard shake until the dry noodles drop into the ceramic bowl with a thud; Ponyo, still unfamiliar with the human world, tries to mimic him, but she grips it too hard and broken noodles scatter into her bowl. Sosuke’s mother then pours boiling water over the noodles and covers them, and the kids have to wait for, as Sosuke says, “three minutes.” When Sosuke’s mother returns, she tells them to cover their eyes (“No peeking!”) as she adds the toppings. When they open their eyes, a perfect bowl of ramen greets them: half a hard boiled egg, chopped scallions, and two thick slices of ham sit on top of a clear broth with quivering globules of fat on the surface and the noodles visible beneath. Ponyo squeals in delight and immediately grabs a hot piece of ham with her fingers. Then the scene cuts to a small noodle sticking out of Ponyo’s mouth as she falls asleep, satisfied, at the table.
</p>

<p id="mntl-sc-block_1-0-22">
I’ve come to think of ham as a way for Ponyo to relate to her human friends even though she’s from another world. In the beginning, when she’s still in her fish-like state, it’s almost grotesque to see her devour ham with such relish, but as the plot develops, her ham obsession makes her feel like a real little girl. She yells about it, giggles when she eats it, and becomes overwhelmed with excitement whenever she sees it—just like a true child with anything they love. When she eats the ham-topped ramen with Sosuke, she gets to have a fundamentally human experience: sharing a lovingly prepared meal with someone you care about.
</p>

<h2 id="mntl-sc-block_1-0-24"> <span> My Neighbor Totoro: Bento </span> </h2>
<figure id="mntl-sc-block_1-0-25">

<figcaption id="mntl-figure-caption_1-0-1">
<span>From <em>My Neighbor Totoro</em> © 1988 Studio Ghibli .</span>
</figcaption>
</figure>
<p id="mntl-sc-block_1-0-26">
<em>My Neighbor Totoro</em> is the most iconic film from the Studio Ghibli collection, so much so that Totoro himself is the studio’s logo. The 1988 film follows two sisters and a father as they settle into a new home in the countryside of Tokorozawa City in Saitama Prefecture, north of Tokyo. Throughout the movie, however, the mother is sick in the hospital. Though understated, her absence shapes how the kids cope with their new family unit, and gives context to their self-guided adventures.
</p>

<p id="mntl-sc-block_1-0-28">
In the bento scene, the older sister Satsuki wakes up early to make lunch for her father and Mei, her younger sister, before running off to school. There are several pots bubbling and steaming away at once, yet Satsuki manages all of the cooking with an ease that hints that she’s done it a million times before. Over three rice-filled bento boxes, she spoons on sakura denbu, edamame, one umeboshi, and a <a href="https://www.seriouseats.com/deep-fried-shishamo-recipe" data-component="link" data-source="inlineLink" data-type="internalLink" data-ordinal="1">shishamo</a>. Satsuki hands her lunch to Mei, who admires it. The care with which she makes the lunch demonstrates Satsuki’s love for her family—but also the responsibilities she’s been forced to take on due to her mother’s illness and the father’s busy work schedule. And while the fact that they’re often alone results in them having magical adventures with Totoro, a magical forest spirit, it can also make them feel lost when they enter back into the reality of their everyday lives.
</p>

<h2 id="mntl-sc-block_1-0-30"> <span> Howl’s Moving Castle: Calcifer’s Breakfast </span> </h2>
<figure id="mntl-sc-block_1-0-31">

<figcaption id="mntl-figure-caption_1-0-2">
<span>From <em>Howl's Moving Castle</em> © 2004 Studio Ghibli - NDDMT .</span>
</figcaption>
</figure>
<p id="mntl-sc-block_1-0-32">
Based on a book by Diana Wynne Jones, <em>Howl’s Moving Castle</em> is a fantastical adventure featuring demons, wizards, and witches in a medieval European-inspired world. In the beginning, a young woman named Sophie gets cursed by a witch and transforms into an old lady. Her only chance to break the spell is to befriend a heartless wizard named Howl.
</p>

<p id="mntl-sc-block_1-0-34">
This scene begins by showing a bounty of food on an old, dusty table. At first glance, it looks like a feast, but upon further inspection, you can see the details of neglect: the potatoes are sprouting and the onions have fully grown green tops. Sophie has just snuck into Howl’s moving castle, but she confidently grabs a glistening plate of thick bacon and a basket of eggs like she’s been there before. A fire demon named Calcifer lives in the castle and controls its movement; he also happens to control the fire used for cooking. After some banter, Calcifer spitefully gives into Sophie’s request to cook bacon. She places a thick skillet on top of the red-hot demon, reducing him to a small flame. Sunny-side-up eggs and fatty slices of bacon bubble away. “May all your bacon burn,” Calcifer angrily says. Shortly after, Howl shows up and finishes making bacon for everyone. Sophie is at first apprehensive with Howl’s seemingly kind act, but she eventually gives in and eats breakfast.
</p>

<p id="mntl-sc-block_1-0-36">
The entire meal showcases the notable attributes of each character: strong-willed Sophie, rebellious-but-kind Calcifer, and uneasy Howl.
</p>

<h2 id="mntl-sc-block_1-0-38"> <span> Kiki’s Delivery Service: Herring and Pumpkin Pie </span> </h2>
<figure id="mntl-sc-block_1-0-39">

<figcaption id="mntl-figure-caption_1-0-3">
<span>From <em>Kiki's Delivery Service</em> © 1989 Eiko Kadono - Studio Ghibli - NDDMT .</span>
</figcaption>
</figure>
<p id="mntl-sc-block_1-0-40">
<em>Kiki’s Delivery Service</em> is about a young witch named Kiki who leaves home for the first time to try and make it on her own in a big city. Along her journey for independence, she meets several characters who are dealing with a loneliness and isolation that mirrors her own experience. There’s a woman who has retreated into the woods to pursue her art; there’s a boy who’s often treated like an outcast among popular kids in the town; there’s a couple, who ultimately take Kiki in, that struggles with the changes that come with having their first child. While they all individually grapple with their own form of isolation, it draws them to each other as well.
</p>

<p id="mntl-sc-block_1-0-42">
In the food scene, Kiki meets an elderly woman and helps her around the house. Many areas of her home are dusty—small tasks have been left undone, indicating that her family has forgotten her. Out of empathy and kindness, Kiki decides to take on the additional task of helping the woman bake a herring and pumpkin pie for her granddaughter’s birthday party. It is covered with pie crust and a neatly decorated fish in the center accents the olives at the edges. Kiki struggles to build a fire from scratch for baking. After some time and care, the pie is done with seconds to spare. She rushes off in the rain to deliver it. When she arrives, she is greeted by an ungrateful young girl who looks at the pie in disgust before closing the door in Kiki’s face. It’s a rare moment in Studio Ghibli where food is used to signify rejection.
</p>

<p id="mntl-sc-block_1-0-44">
“It really highlights the idea of loneliness and isolation that the whole film deals with,” says Jesteadt.
</p>

<h2 id="mntl-sc-block_1-0-46"> <span> Spirited Away: Onigiri </span> </h2>
<figure id="mntl-sc-block_1-0-47">

<figcaption id="mntl-figure-caption_1-0-4">
<span>From <em>Spirited Away</em> © 2001 Studio Ghibli - NDDTM .</span>
</figcaption>
</figure>
<p id="mntl-sc-block_1-0-48">
<em>Spirited Away</em> is the most food-centric of all the Studio Ghibli films. Set in an onsen, or bathhouse, patronized almost exclusively by spirits, almost every scene contains food of some kind, whether it's the sumptuous feasts served to the clientele or the relatively simple fare eaten by the bathhouse employees.
</p>

<p id="mntl-sc-block_1-0-50">
The entire story is built on a food scene: A family is moving to their new home, but before they even arrive, they take a detour to visit an abandoned amusement park. As they wander, they come upon rows on rows of empty stalls with mountains of food. Piles of shimmering sausages, rice cakes, and various kinds of roasted meats are left unattended and have an aura of sinister temptation around them. Chihiro, the main character, looks upon the food uneasily and urges her parents to leave, but they don’t listen and begin to gorge themselves on someone else’s feast. Soon they are transformed into pigs, and Chihiro spends the rest of the film trying to find a way to rescue them.
</p>

<p id="mntl-sc-block_1-0-52">
But one of my favorite scenes from the movie is focused on a simpler food: After Chihiro is shown where her pig-parents are being held by a boy named Haku, she sits down to try to make sense of this entirely familiar and forbidding world. Haku offers her some <a href="https://www.seriouseats.com/yaki-onigiri-grilled-rice-asian-recipe" data-component="link" data-source="inlineLink" data-type="internalLink" data-ordinal="1">onigiri</a>, to help comfort her, and she takes a bite and she immediately begins to cry. This moment speaks to Chihiro’s vulnerability; it is the first time she allows herself to break down. It speaks to me of how intimate it can be to share even a commonplace dish with someone who cares.
</p>

<p id="mntl-sc-block_1-0-54">
My obsession with anime food has only grown over time and my love of Miyazaki films has grown with it. Whenever I look down at my arm and see the shiny red bento tattoo from <em>My Neighbor Totoro</em>, it reminds me to take a deep breath and relax. It allows me to imagine a more peaceful world, and creates a brief, gentle escape, if only for a moment.
</p>

<p id="mntl-sc-block_1-0-56" data-tracking-id="mntl-sc-block-callout" data-tracking-container="true">
<h3 id="mntl-sc-block-callout-heading_1-0">
August 2020</h3>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pixar, Adobe, Apple, Autodesk, and Nvidia form alliance for OpenUSD (602 pts)]]></title>
            <link>https://www.apple.com/newsroom/2023/08/pixar-adobe-apple-autodesk-and-nvidia-form-alliance-for-openusd/</link>
            <guid>36960625</guid>
            <pubDate>Tue, 01 Aug 2023 18:51:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.apple.com/newsroom/2023/08/pixar-adobe-apple-autodesk-and-nvidia-form-alliance-for-openusd/">https://www.apple.com/newsroom/2023/08/pixar-adobe-apple-autodesk-and-nvidia-form-alliance-for-openusd/</a>, See on <a href="https://news.ycombinator.com/item?id=36960625">Hacker News</a></p>
<div id="readability-page-1" class="page">


	
    







 
<nav id="ac-localnav" lang="en-US" role="navigation" aria-label="Newsroom" data-analytics-region="local nav" data-sticky="">
	
    
    
        




    
    

</nav>



<main id="main" role="main"> 



<span id="opens-in-new-window">opens in new window</span>

	

<section>
<article data-analytics-activitymap-region-id="article">






    
    
    







<div>
    
    <div>
                
                
                    <span>PRESS RELEASE</span>
                
                
                    <span>August 1, 2023</span>
                
                
            </div>

    <div>
            
            
            
                <h2>
                    
    
        Pixar, Adobe, Apple, Autodesk, and NVIDIA form Alliance for OpenUSD to drive open standards for 3D content
    

                </h2>
            
        </div>

    <div>
            
            
                Alliance to foster global collaboration for Universal Scene Description (USD)
            
        </div>

    
        
    
    
    
    
    

    

</div>




    
    
    


    
        
        
        
        
            <figure aria-label="Media, The Alliance for OpenUSD logo.">
                <div>
                        
                         
                            
                        
                        
                        
                        <a href="https://www.apple.com/newsroom/images/2023/08/apple-alliance-for-openusd/article/Apple-Alliance-for-OpenUSD-AOUSD-logo.zip" download="" data-analytics-title="Download image" aria-label="Download media, The Alliance for OpenUSD logo."></a>
                    </div>
            </figure>
        
    










    
    
    


     
     
    
    
        <div>
             
                 <div><span>SAN FRANCISCO</span>&nbsp;<a href="https://www.pixar.com/" target="_blank" rel="nofollow" data-analytics-exit-link="">Pixar</a>, <a href="https://www.adobe.com/" target="_blank" rel="nofollow" data-analytics-exit-link="">Adobe</a>, <a href="https://www.apple.com/" target="_blank">Apple</a>, <a href="https://www.autodesk.com/" target="_blank" rel="nofollow" data-analytics-exit-link="">Autodesk</a>, and <a href="https://www.nvidia.com/en-us/" target="_blank" rel="nofollow" data-analytics-exit-link="">NVIDIA</a>, together with the<a href="https://jointdevelopment.org/" target="_blank" rel="nofollow" data-analytics-exit-link=""> Joint Development Foundation</a> (JDF), an affiliate of the Linux Foundation, today announced the Alliance for OpenUSD (AOUSD) to promote the standardization, development, evolution, and growth of Pixar’s Universal Scene Description technology.
</div>
                 
             
                 <div>The alliance seeks to standardize the 3D ecosystem by advancing the capabilities of Open Universal Scene Description (OpenUSD). By promoting greater interoperability of 3D tools and data, the alliance will enable developers and content creators to describe, compose, and simulate large-scale 3D projects and build an ever-widening range of 3D-enabled products and services.
</div>
                 
             
                 <div>Created by Pixar Animation Studios, OpenUSD is a high-performance 3D scene description technology that offers robust interoperability across tools, data, and workflows. Already known for its ability to collaboratively capture artistic expression and streamline cinematic content production, OpenUSD’s power and flexibility make it an ideal content platform to embrace the needs of new industries and applications.
</div>
                 
             
                 <div>The alliance will develop written specifications detailing the features of OpenUSD. This will enable greater compatibility and wider adoption, integration, and implementation, and allows inclusion by other standards bodies into their specifications. The Linux Foundation’s JDF was chosen to house the project, as it will enable open, efficient, and effective development of OpenUSD specifications, while providing a path to recognition through the International Organization for Standardization (ISO).
</div>
                 
             
                 <div>AOUSD will also provide the primary forum for the collaborative definition of enhancements to the technology by the greater industry. The alliance invites a broad range of companies and organizations to join and participate in shaping the future of OpenUSD.
</div>
                 
             
                 <div>“Universal Scene Description was invented at Pixar and is the technological foundation of our state-of-the-art animation pipeline,” said Steve May, Chief Technology Officer at Pixar and Chairperson of AOUSD. “OpenUSD is based on years of research and application in Pixar filmmaking. We open-sourced the project in 2016, and the influence of OpenUSD now expands beyond film, visual effects, and animation and into other industries that increasingly rely on 3D data for media interchange. With the announcement of AOUSD, we signal the exciting next step: the continued evolution of OpenUSD as a technology and its position as an international standard.”
</div>
                 
             
                 <div>“At Adobe, we believe in providing artists a set of flexible and powerful solutions running on a variety of devices,” said Guido Quaroni, Senior Director of Engineering, 3D&amp;I at Adobe. “Leveraging a common 3D data representation during the creative process multiplies the value brought by each package and device. OpenUSD was created to be one of these ‘multipliers’ and we are excited to see a diverse group of companies joining together to support this innovative and open technology.”
</div>
                 
             
                 <div>“OpenUSD will help accelerate the next generation of AR experiences, from artistic creation to content delivery, and produce an ever-widening array of spatial computing applications,” said Mike Rockwell, Apple’s vice president of the Vision Products Group. “Apple has been an active contributor to the development of USD, and it is an essential technology for the groundbreaking visionOS platform, as well as the new Reality Composer Pro developer tool. We look forward to fostering its growth into a broadly adopted standard.”
</div>
                 
             
                 <div>“Whether you’re building CG worlds or digital twins or looking ahead to the 3D web, content creators need a cohesive way to collaborate and share data across tools, services, and platforms,” said Gordon Bradley, Fellow, Media &amp; Entertainment, Autodesk. “Autodesk is excited to support the Alliance for OpenUSD as it drives 3D interoperability for visual effects, animation, and beyond, and supports our vision to help customers design and make a better world.”&nbsp;&nbsp;
</div>
                 
             
                 <div>“OpenUSD gives 3D developers, artists, and designers the complete foundation to tackle large-scale industrial, digital content creation, and simulation workloads with broad multi-app interoperability,” said Guy Martin, Director of Open Source and Standards at NVIDIA. “This alliance is a unique opportunity to accelerate OpenUSD collaboration globally by building formal standards across industries and initiatives to realize 3D worlds and industrial digitalization.”
</div>
                 
             
                 <div>AOUSD steering committee members will be speaking at both the Academy Software Foundation’s Open Source Days on Aug. 6 and at the SIGGRAPH conference at the <a href="https://cvent.autodesk.com/event/47ffb79d-e467-4cd8-be6a-a634bb641cd8/summary" target="_blank" rel="nofollow" data-analytics-exit-link="">Autodesk Vision Series</a> on Aug. 8 at 1 p.m. PT in Room 404A.
</div>
                 
             
                 <div>To learn more about AOUSD and how to get involved, visit <a href="http://www.aousd.org/" target="_blank" rel="nofollow" data-analytics-exit-link="">www.aousd.org</a>. To tune into the Academy Software Foundation panel on USD on Aug. 6, 2023, visit <a href="https://opensourcedays2023.sched.com/event/1O8hA" target="_blank" rel="nofollow" data-analytics-exit-link="">the website</a>.
</div>
                 
             
         </div>
 

    
    
    




    


    
    
    



    
    
    




    




    
    
    






    
















	
	
		







	
	

</article>



</section>
</main>


	

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Emacs is my new window manager (258 pts)]]></title>
            <link>https://howardism.org/Technical/Emacs/new-window-manager.html</link>
            <guid>36960309</guid>
            <pubDate>Tue, 01 Aug 2023 18:31:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://howardism.org/Technical/Emacs/new-window-manager.html">https://howardism.org/Technical/Emacs/new-window-manager.html</a>, See on <a href="https://news.ycombinator.com/item?id=36960309">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<header>

</header><p>
Most companies that employ me, hand me a “work laptop” as I enter the
building. Of course, I do not install personal software and keep a
clear division between my “work like” and my “real life.”
</p>

<p>
However, I also don’t like to carry two computers just to jot down
personal notes. My remedy is to install a virtualization system and
create a “personal” virtual machine. (Building cloud software as my
day job means I usually have a few VMs running all the time.)
</p>

<p>
Since I want this VM to have minimal impact on my work, I base it on a
“Server” version of Ubuntu. however, I like some graphical features,
so my most minimal <i>after market</i> installation approach is:<sup><a id="fnr.1" href="#fn.1" role="doc-backlink">1</a></sup>
</p>

<div>
<pre>sudo apt-get install -y xinit
</pre>
</div>

<p>
Since most of what I do is <code>org-mode</code> work, the next step is:
</p>

<div>
<pre>sudo apt-get install -y emacs
</pre>
</div>

<p>
I have played with a lot of window managers, and while some claim to
be unobtrusive and minimal<sup><a id="fnr.2" href="#fn.2" role="doc-backlink">2</a></sup>, I really just want Emacs in full-screen
mode (utilizing all screen estate possible).<sup><a id="fnr.3" href="#fn.3" role="doc-backlink">3</a></sup>
</p>

<p>
To accomplish this, I create an <code>.xinitrc</code> file that contains only:
</p>

<pre id="org338a66d">exec emacs
</pre>

<p>
That’s right, folks, <b>Emacs is my window manager</b>. I add the
following to my Emacs <code>init.el</code> script:
</p>

<div>
<pre><span>(</span>set-frame-parameter nil 'fullscreen 'fullboth<span>)</span>
</pre>
</div>

<p>
And now I can split the screen into windows, launch programs– even
edit files– all without fondling the mouse, and since Emacs is in its
graphical mode, I can use my favorite fonts, decorate the fringe, etc.
</p>


<figure id="orgc3216e5">
<img src="https://howardism.org/Technical/Emacs/new-window-manager-1.png" alt="new-window-manager-1.png">

</figure>

<p>
If I need a program that doesn’t run well <i>within</i> Emacs, I can
call <code>xterm</code> (or any other application) with <code>M-! xterm</code>.
</p>

<div id="outline-container-orgf5a58ba">
<h2 id="orgf5a58ba">Web Browsing</h2>
<div id="text-orgf5a58ba">
<p>
Unlike the previous century, this century has been defined by web
applications. Most of my web efforts are looking up code
documentation and other technical resources, and this is good using
a text-oriented browser like <a href="http://emacs-w3m.namazu.org/">w3m</a> or <a href="https://www.gnu.org/software/emacs/manual/html_node/eww/index.html#Top">eww</a> embedded within Emacs.
</p>

<p>
Whenever a page doesn’t render well (can you say JavaScript), I can
hit the <code>&amp;</code> key to bring up a web browser.<sup><a id="fnr.4" href="#fn.4" role="doc-backlink">4</a></sup>. You can kick off a
graphical browser to a specific URL by binding a function that calls
this:
</p>

<div>
<pre><span>(</span>start-process <span>""</span> nil <span>"xdg-open"</span> <span>"http://mail.google.com"</span><span>)</span>
</pre>
</div>

<p>
Keep in mind, running X window applications without a <i>real</i> window
manager will overlay Emacs, which is only an option if the
window is temporary.  When closed, you are back to Emacs:
</p>


<figure id="org42b82b7">
<img src="https://howardism.org/Technical/Emacs/new-window-manager-3.png" alt="new-window-manager-3.png">

</figure>

<p>
<b>Update:</b> Since I wrote this section, a number of people have
mentioned some light-weight window managers. If you need to use a
browser, but still want Emacs to utilize every possible pixel,
the best seems to be Ratpoison<sup><a id="fnr.5" href="#fn.5" role="doc-backlink">5</a></sup> (with DWM a close second).
</p>
</div>
</div>

<div id="outline-container-org83c7a79">
<h2 id="org83c7a79">Other Applications</h2>
<div id="text-org83c7a79">
<p>
I realize that <a href="https://howardism.org/Technical/Emacs/eshell-fun.html">running shells inside Emacs</a> isn’t for everyone, but
currently using Emacs as my entire Linux desktop works pretty well.
Especially since I can split the frame into a series of windows
running:
</p>

<ul>
<li><a href="http://www.emacswiki.org/emacs/InternetRelayChat">IRC</a></li>
<li><a href="http://www.emacswiki.org/emacs/TwitteringMode">Twitter</a></li>
<li><a href="http://nullprogram.com/blog/2013/09/04/">RSS Feed Reader</a></li>
<li><a href="https://github.com/vermiculus/sx.el/">Stack Exchange</a></li>
<li><a href="http://emacs-jabber.sourceforge.net/">Google Talk/Jabber</a></li>
</ul>

<p>
I use a function to start my favorite time-sinks…er,
applications, at one time:
</p>

<div>
<pre><span>(</span><span>defun</span> <span>setup-windows</span> <span>()</span>
  <span>"Organize a series of windows for ultimate distraction."</span>
  <span>(</span><span>interactive</span><span>)</span>
  <span>(</span>delete-other-windows<span>)</span>

  <span>;; </span><span>Start with the Stack Overflow interface</span>
  <span>(</span>sx-tab-frontpage t nil<span>)</span>

  <span>;; </span><span>Put IRC on the other side</span>
  <span>(</span>split-window-horizontally<span>)</span>
  <span>(</span>other-window 1<span>)</span>
  <span>(</span>circe-connect-all<span>)</span>

  <span>;; </span><span>My RSS Feed goes on top:</span>
  <span>(</span>split-window-vertically<span>)</span>
  <span>(</span>elfeed<span>)</span>

  <span>;; </span><span>And start up the Twitter interface above that:</span>
  <span>(</span>other-window 2<span>)</span>
  <span>(</span>split-window-vertically<span>)</span>
  <span>(</span>twit<span>)</span>

  <span>(</span>window-configuration-to-register ?w<span>))</span>
</pre>
</div>

<p>
The last line insert this “current” configuration in a register, so
after stomping and stirring my windows, I return to this
organization with <code>C-x r j w</code>.
</p>

<p>
Perhaps another screenshot of these results are in order:
</p>


<figure id="org2b78531">
<img src="https://howardism.org/Technical/Emacs/new-window-manager-2.png" alt="new-window-manager-2.png">

</figure>
</div>
</div>
</div><div id="postamble">
<p>Date: 2015 Jan 12</p>
<p>Created: 2023-01-11 Wed 21:34</p>
<p><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: imessage-exporter, a CLI app and library (141 pts)]]></title>
            <link>https://github.com/ReagentX/imessage-exporter</link>
            <guid>36960244</guid>
            <pubDate>Tue, 01 Aug 2023 18:27:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ReagentX/imessage-exporter">https://github.com/ReagentX/imessage-exporter</a>, See on <a href="https://news.ycombinator.com/item?id=36960244">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">imessage-exporter</h2>
<p dir="auto">This crate provides both a library to interact with iMessage data as well as a binary that can perform some useful read-only operations using that data.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ReagentX/imessage-exporter/blob/develop/docs/hero.png"><img src="https://github.com/ReagentX/imessage-exporter/raw/develop/docs/hero.png" alt="HTML Export Sample"></a></p>
<h2 tabindex="-1" dir="auto">Binary</h2>
<p dir="auto">The <code>imessage-exporter</code> binary exports iMessage data to <code>txt</code> or <code>html</code> formats. It can also run diagnostics to find problems with the iMessage database.</p>
<p dir="auto">Installation instructions for the binary are located <a href="https://github.com/ReagentX/imessage-exporter/blob/develop/imessage-exporter/README.md">here</a>.</p>
<h2 tabindex="-1" dir="auto">Library</h2>
<p dir="auto">The <code>imessage_database</code> library provides models that allow us to access iMessage information as native data structures.</p>
<p dir="auto">Documentation for the library is located <a href="https://github.com/ReagentX/imessage-exporter/blob/develop/imessage-database/README.md">here</a>.</p>
<h3 tabindex="-1" dir="auto">Supported Features</h3>
<p dir="auto">This crate supports every iMessage feature as of MacOS 13.5 (22G74) and iOS 16.6 (20G75):</p>
<ul dir="auto">
<li>Multi-part messages</li>
<li>Replies/Threads</li>
<li>Attachments</li>
<li>Expressives</li>
<li>Reactions</li>
<li>Stickers</li>
<li>Apple Pay</li>
<li>URL Previews</li>
<li>App Integrations</li>
<li>Edited messages</li>
</ul>
<p dir="auto">See more detail about supported features <a href="https://github.com/ReagentX/imessage-exporter/blob/develop/docs/features.md">here</a>.</p>
<h2 tabindex="-1" dir="auto">Frequently Asked Questions</h2>
<p dir="auto">The FAQ document is located <a href="https://github.com/ReagentX/imessage-exporter/blob/develop/docs/faq.md">here</a>.</p>
<h2 tabindex="-1" dir="auto">Special Thanks</h2>
<ul dir="auto">
<li>All of my friends, for putting up with me sending them random messages to test things</li>
<li><a href="https://www.sqliteflow.com/" rel="nofollow">SQLiteFlow</a>, the SQL viewer I used to explore and reverse engineer the iMessage database</li>
<li><a href="https://github.com/ic005k/Xplist">Xplist</a>, an invaluable tool for reverse engineering the <code>payload_data</code> plist format</li>
<li><a href="https://www.compart.com/en/unicode/" rel="nofollow">Compart</a>, an amazing resource for looking up esoteric unicode details</li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sunak’s family firm signed deal with BP before opening new North Sea licences (138 pts)]]></title>
            <link>https://www.thelondoneconomic.com/politics/sunaks-family-firm-signed-a-billion-dollar-deal-with-bp-before-pm-opened-new-north-sea-licences-353690/</link>
            <guid>36960185</guid>
            <pubDate>Tue, 01 Aug 2023 18:23:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thelondoneconomic.com/politics/sunaks-family-firm-signed-a-billion-dollar-deal-with-bp-before-pm-opened-new-north-sea-licences-353690/">https://www.thelondoneconomic.com/politics/sunaks-family-firm-signed-a-billion-dollar-deal-with-bp-before-pm-opened-new-north-sea-licences-353690/</a>, See on <a href="https://news.ycombinator.com/item?id=36960185">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div><p><img width="750" height="576" src="https://cdn.thelondoneconomic.com/wp-content/uploads/2022/04/5a16415b-featureimage-750x576.jpg" alt="" decoding="async" srcset="https://cdn.thelondoneconomic.com/wp-content/uploads/2022/04/5a16415b-featureimage-750x576.jpg 750w, https://cdn.thelondoneconomic.com/wp-content/uploads/2022/04/5a16415b-featureimage-1200x922.jpg 1200w, https://cdn.thelondoneconomic.com/wp-content/uploads/2022/04/5a16415b-featureimage-2048x1574.jpg 2048w, https://cdn.thelondoneconomic.com/wp-content/uploads/2022/04/5a16415b-featureimage-768x590.jpg 768w, https://cdn.thelondoneconomic.com/wp-content/uploads/2022/04/5a16415b-featureimage-1536x1180.jpg 1536w, https://cdn.thelondoneconomic.com/wp-content/uploads/2022/04/5a16415b-featureimage-1500x1153.jpg 1500w" sizes="(max-width: 750px) 100vw, 750px" data-image-size="jnews-featured-750" data-stateless-media-bucket="cdn.thelondoneconomic.com" data-stateless-media-name="wp-content/uploads/2022/04/5a16415b-featureimage-scaled.jpg"></p></div>


<div>
<p>A firm founded by Rishi Sunak’s father-in-law signed a billion-dollar deal with BP two months before the prime minister opened <a href="https://www.thelondoneconomic.com/news/hundreds-of-new-oil-and-gas-licenses-to-be-granted-to-boost-british-energy-independence-353605/" target="_blank" rel="noreferrer noopener">hundreds of new licences </a>for oil and gas extraction in the North Sea.</p>
<p>In May, the <em>Times of India </em><a href="https://timesofindia.indiatimes.com/business/india-business/infosys-wins-1-5-billion-deal-from-bp/articleshow/100289149.cms?from=mdr" target="_blank" rel="noreferrer noopener">reported</a> that Infosys bagged a huge deal from the global energy company which is thought to be the second-largest in the history of the firm. </p>
<p>The Indian IT company is owned by the prime minister’s&nbsp;wife’s family although Sunak has insisted the matter is of “no legitimate public interest”.</p>
<p>It has since come to light that the IT giant has been involved in £172 million worth of public sector contracts in the UK, and even the most innocent bystanders would admit that the current drive to increase oil and gas exploration in the North Sea is more than convenient. </p>
<p>What’s more, it is made even more convenient by the fact that one of Infosys’ other major clients is Shell, whose CEO joined Rishi Sunak’s new business council two weeks ago and promised a “candid collaboration” with his government.</p>
<figure></figure>
<p>Sunak has insisted granting new oil and gas licences for the UK was “entirely consistent” with the UK commitment to reach net zero emissions by 2050.</p>
<p>The PM said even then the UK would still need oil and gas for 25 per cent of its energy needs, with the PM saying he was seeking to “power Britain from Britain” rather than the UK “relying on foreign dictators” for its energy supplies.</p>
<p>Speaking about the need for oil and gas, the Prime Minister said: “If we’re going to need it, far better to have it here at home rather than shipping it here from half way around the world with two, three, four times, the amount of carbon emissions versus the oil and gas we have here at home.</p>
<p>“So, it is entirely consistent with our plans to get to net zero.”</p>
<p>But doubts have even <a href="https://www.thelondoneconomic.com/politics/sunaks-claims-that-he-has-boosted-british-energy-independence-put-to-bed-in-viral-video-353678/" target="_blank" rel="noreferrer noopener">been raised</a> about those claims which are expertly set out by Ciaran Jenkins&nbsp;here: </p>
<figure></figure>
<p><strong>Related: <a href="https://www.thelondoneconomic.com/politics/tory-mp-savages-sunaks-decision-to-green-light-oil-and-gas-drilling-in-north-sea-353675/" target="_blank" rel="noreferrer noopener">Tory MP savages Sunak’s decision to green-light oil and gas drilling in North Sea</a></strong></p>
<p><a href="https://www.dmca.com/Protection/Status.aspx?ID=f8f26b2e-5e56-4c62-98c6-5c5e9f9cf8a0" title="Content Protection by DMCA.com"><img src="https://images.dmca.com/Badges/dmca-badge-w100-2x1-03.png?ID=https://www.dmca.com/Protection/Status.aspx?ID=f8f26b2e-5e56-4c62-98c6-5c5e9f9cf8a0" alt="Content Protection by DMCA.com" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%200%200'%3E%3C/svg%3E" data-lazy-src="https://images.dmca.com/Badges/dmca-badge-w100-2x1-03.png?ID=https://www.dmca.com/Protection/Status.aspx?ID=f8f26b2e-5e56-4c62-98c6-5c5e9f9cf8a0"></a> </p>
</div>

 </div><div>
<h4>Since you are here </h4>
<p>Since you are here, we wanted to ask for your help.</p>
<p>Journalism in Britain is under threat. The government is becoming increasingly authoritarian and our media is run by a handful of billionaires, most of whom reside overseas and all of them have strong political allegiances and financial motivations.</p>
<p>Our mission is to hold the powerful to account. It is vital that free media is allowed to exist to expose hypocrisy, corruption, wrongdoing and abuse of power. But we can't do it without you.</p>
<p>If you can afford to contribute a small donation to the site it will help us to continue our work in the best interests of the public. We only ask you to donate what you can afford, with an option to cancel your subscription at any point.</p>
<p>To donate or subscribe to <i>The London Economic</i>, <a href="https://www.thelondoneconomic.com/support-free-journalism/" rel="nofollow">click here</a>.</p>
<p>The TLE shop is also now open, with all profits going to supporting our work. </p>
<p>The shop can be found <a href="https://moteefe.com/store/tle" rel="nofollow" target="blank">here</a>.</p>
<p>You can also <a href="https://www.thelondoneconomic.com/newsletter/" target="blank">SUBSCRIBE TO OUR NEWSLETTER </a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Oregon decriminalized hard drugs – early results aren’t encouraging (425 pts)]]></title>
            <link>https://www.theatlantic.com/politics/archive/2023/07/oregon-drug-decriminalization-results-overdoses/674733/</link>
            <guid>36959267</guid>
            <pubDate>Tue, 01 Aug 2023 17:27:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theatlantic.com/politics/archive/2023/07/oregon-drug-decriminalization-results-overdoses/674733/">https://www.theatlantic.com/politics/archive/2023/07/oregon-drug-decriminalization-results-overdoses/674733/</a>, See on <a href="https://news.ycombinator.com/item?id=36959267">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header data-event-module="hero"><div><div><p>A bold reform effort hasn’t gone as planned.</p></div><div><figure><div><picture><img alt="A black and white scene of tents underneath an overpass" sizes="(min-width: 976px) 976px, 100vw" srcset="https://cdn.theatlantic.com/thumbor/J3-V_5bKwEnrdqOCW_0_UWfWqmw=/0x0:4800x2700/750x422/media/img/mt/2023/07/oregon_drug_lead/original.jpg 750w, https://cdn.theatlantic.com/thumbor/EgiEOm4Jqw7pCqu0tbXVT_TAv5A=/0x0:4800x2700/828x466/media/img/mt/2023/07/oregon_drug_lead/original.jpg 828w, https://cdn.theatlantic.com/thumbor/xFRcXBXjBU0Us1PqKHb1JhaHEeU=/0x0:4800x2700/960x540/media/img/mt/2023/07/oregon_drug_lead/original.jpg 960w, https://cdn.theatlantic.com/thumbor/8GzeaHElReej2H039grPfW5Xf-I=/0x0:4800x2700/976x549/media/img/mt/2023/07/oregon_drug_lead/original.jpg 976w, https://cdn.theatlantic.com/thumbor/1NNraWxdETHEFo0Vo9bNm0aux3Y=/0x0:4800x2700/1952x1098/media/img/mt/2023/07/oregon_drug_lead/original.jpg 1952w" src="https://cdn.theatlantic.com/thumbor/xFRcXBXjBU0Us1PqKHb1JhaHEeU=/0x0:4800x2700/960x540/media/img/mt/2023/07/oregon_drug_lead/original.jpg" width="960" height="540"></picture></div><figcaption><span>Portland, Oregon, on March 24</span>&nbsp;(<!-- -->Jordan Gale<!-- -->)</figcaption></figure></div></div><gpt-ad format="injector" sizes-at-0="mobile-wide" targeting-pos="injector-article-start" sizes-at-976="desktop-wide"></gpt-ad></header><section data-event-module="article body"><p><small><i data-stringify-type="italic">This article was featured in One Story to Read Today, a newsletter in which our editors recommend a single must-read from </i>The Atlantic<i data-stringify-type="italic">, Monday through Friday. </i><i data-stringify-type="italic"><a data-event-element="inline link" data-sk="tooltip_parent" data-stringify-link="https://www.theatlantic.com/newsletters/sign-up/one-story-to-read-today/" delay="150" href="https://www.theatlantic.com/newsletters/sign-up/one-story-to-read-today/" rel="noopener noreferrer" target="_blank">Sign up for it here.</a></i></small></p><p><em><small>Updated at 11:25 a.m. ET on July 20, 2023</small></em></p><p>Three years ago, while the nation’s attention was on the 2020 presidential election, voters in Oregon took a dramatic step back from America’s long-running War on Drugs. By a 17-point margin, Oregonians approved Ballot Measure 110, which eliminated criminal penalties for possessing small amounts of any drug, including cocaine, heroin, and methamphetamine. When the policy went into effect early the next year, it lifted the fear of prosecution for the state’s drug users and launched Oregon on an experiment to determine whether a long-sought goal of the drug-policy reform movement—decriminalization—could help solve America’s drug problems.</p><p>Early results of this reform effort, the first of its kind in any state, are now coming into view, and so far, they are not encouraging. State leaders have acknowledged faults with the policy’s implementation and enforcement measures. And Oregon’s drug problems have not improved. Last year, the state experienced one of the <a data-event-element="inline link" href="http://www.cdc.gov/nchs/nvss/vsrr/drug-overdose-data.htm">sharpest rises</a> in overdose deaths in the nation and had one of the <a data-event-element="inline link" href="https://static1.squarespace.com/static/579bd717c534a564c72ea7bf/t/643988bad6f8d913a4f61afc/1681492164612/2022+Annual+Report+-+FINAL+2.pdf">highest percentages</a> of adults with a substance-use disorder. During one two-week period last month, three children under the age of 4 <a data-event-element="inline link" href="https://www.portlandoregon.gov/police/news/read.cfm?id=492958">overdosed</a> in Portland after ingesting fentanyl.</p><p>For decades, drug policy in America centered on using law enforcement to target people who sold, possessed, or used drugs—an approach long supported by both Democratic and Republican politicians. Only in recent years, amid an epidemic of opioid overdoses and a national reconsideration of racial inequities in the criminal-justice system, has the drug-policy status quo begun to break down, as a coalition of health workers, criminal-justice-reform advocates, and drug-user activists have lobbied for a more compassionate and nuanced response. The new approach emphasizes reducing overdoses, stopping the spread of infectious disease, and providing drug users with the resources they need—counseling, housing, transportation—to stabilize their lives and gain control over their drug use.</p><p>Oregon’s Measure 110 was viewed as an opportunity to prove that activists’ most groundbreaking idea—sharply reducing the role of law enforcement in the government’s response to drugs—could work. The measure also earmarked hundreds of millions of dollars in cannabis tax revenue for building a statewide treatment network that advocates promised would do what police and prosecutors couldn’t: help drug users stop or reduce their drug use and become healthy, engaged members of their communities. The day after the measure passed, Kassandra Frederique, executive director of the Drug Policy Alliance, one of the nation’s most prominent drug-policy reform organizations, issued a <a data-event-element="inline link" href="https://web.archive.org/web/20230225033031/https:/www.drugpolicy.org/blog/2020elections">statement</a> calling the vote a “historic, paradigm-shifting win” and predicting that Oregon would become “a model and starting point for states across the country to decriminalize drug use.”</p><p id="injected-recirculation-link-0" data-view-action="view link - injected link - item 1" data-event-element="injected link" data-event-position="1"><a href="https://www.theatlantic.com/ideas/archive/2023/06/harm-reduction-decriminalization-fentanyl-meth/674214/">Sam Quinones: America’s approach to addiction has gone off the rails</a></p><p>But three years later, with rising overdoses and delays in treatment funding, even some of the measure’s supporters now believe that the policy needs to be changed. In a nonpartisan statewide <a data-event-element="inline link" href="https://www.dhmresearch.com/wp-content/uploads/2023/05/DHM-Panel-Oregon_Measure110_May-2023.pdf">poll</a> earlier this year, more than 60 percent of respondents blamed Measure 110 for making drug addiction, homelessness, and crime worse. A majority, including a majority of Democrats, said they supported bringing back criminal penalties for drug possession. This year’s legislative session, which ended in late June, saw at least a dozen Measure 110–related proposals from Democrats and Republicans alike, ranging from technical fixes to full restoration of criminal penalties for drug possession. Two significant changes—tighter restrictions on fentanyl and more state oversight of how Measure 110 funding is distributed—passed with bipartisan support.</p><p>Few people consider Measure 110 “a success out of the gate,” Tony Morse, the policy and advocacy director for Oregon Recovers, told me. The organization, which promotes policy solutions to the state’s addiction crisis, initially opposed Measure 110; now it supports funding the policy, though it also wants more state money for in-patient treatment and detox services. As Morse put it, “If you take away the criminal-justice system as a pathway that gets people into treatment, you need to think about what is going to replace it.”</p><p>Many advocates say the new policy simply needs more time to prove itself, even if they also acknowledge that parts of the ballot measure had flaws; advocates worked closely with lawmakers on the oversight bill that passed last month. “We’re building the plane as we fly it,” Haven Wheelock, a program supervisor at a homeless-services provider in Portland who helped put Measure 110 on the ballot, told me. “We tried the War on Drugs for 50 years, and it didn’t work … It hurts my heart every time someone says we need to repeal this before we even give it a chance.”</p><div><figure><picture><img alt="Workers from the organization Central City Concern hand out Narcan in Portland, Oregon, on April 5." loading="lazy" sizes="(min-width: 729px) 655px, (min-width: 576px) calc(100vw - 48px), 100vw" srcset="https://cdn.theatlantic.com/thumbor/oFnNMYa1dWWMhRqjiZByrVZtyK0=/0x0:4800x2700/655x368/media/img/posts/2023/07/B20F13B9_FB9E_4311_8CB9_0ACE9AEC5CA2/original.jpg 655w, https://cdn.theatlantic.com/thumbor/_-YZv1UpyypXHxmzmKAgoCajYfM=/0x0:4800x2700/750x421/media/img/posts/2023/07/B20F13B9_FB9E_4311_8CB9_0ACE9AEC5CA2/original.jpg 750w, https://cdn.theatlantic.com/thumbor/9NqimZZKVYTXjnw1W02Fxs36OV8=/0x0:4800x2700/850x478/media/img/posts/2023/07/B20F13B9_FB9E_4311_8CB9_0ACE9AEC5CA2/original.jpg 850w, https://cdn.theatlantic.com/thumbor/332T6MsBYniDp-0FSlwA9URDFCY=/0x0:4800x2700/928x521/media/img/posts/2023/07/B20F13B9_FB9E_4311_8CB9_0ACE9AEC5CA2/original.jpg 928w, https://cdn.theatlantic.com/thumbor/qYtJMMAWrkrXyOHrb8KCpv-6yUc=/0x0:4800x2700/1310x736/media/img/posts/2023/07/B20F13B9_FB9E_4311_8CB9_0ACE9AEC5CA2/original.jpg 1310w" src="https://cdn.theatlantic.com/thumbor/oFnNMYa1dWWMhRqjiZByrVZtyK0=/0x0:4800x2700/655x368/media/img/posts/2023/07/B20F13B9_FB9E_4311_8CB9_0ACE9AEC5CA2/original.jpg" width="655" height="368"></picture><figcaption>Workers from the organization Central City Concern hand out Narcan in Portland, Oregon, on April 5. (Jordan Gale)</figcaption></figure></div><p>Measure 110 went into effect at a time of dramatic change in U.S. drug policy. Departing from precedent, the Biden administration has <a data-event-element="inline link" href="https://www.whitehouse.gov/wp-content/uploads/2022/04/National-Drug-Control-2022Strategy.pdf">endorsed</a> and increased federal funding for a public-health strategy called harm reduction; rather than pushing for abstinence, harm reduction emphasizes keeping drug users safe—for instance, through the distribution of clean syringes and overdose-reversal medications. The term <em>harm reduction</em> appeared five times in the ballot <a data-event-element="inline link" href="https://sos.oregon.gov/admin/Documents/irr/2020/044text.pdf">text</a> of Measure 110, which forbids funding recipients from “mandating abstinence.”</p><p>Matt Sutton, the director of external relations for the Drug Policy Alliance, which helped write Measure 110 and spent <a data-event-element="inline link" href="https://secure.sos.state.or.us/orestar/gotoPublicTransactionSearchResults.do?cneSearchButtonName=prev&amp;cneSearchFilerCommitteeId=20089&amp;cneSearchContributorTxtSearchType=C&amp;cneSearchFilerCommitteeTxtSearchType=C&amp;srtOrder=asc&amp;by=NAME&amp;cneSearchPageIdx=">more than $5 million</a> to pass it, told me that reform advocates viewed the measure as the start of a nationwide decriminalization push. The effort started in Oregon because the state had been an early adopter of marijuana legalization and is considered a drug-policy-reform leader. Success would mean showing the rest of the country that “people did think we should invest in a public-health approach instead of criminalization,” Sutton said.</p><p>To achieve this goal, Measure 110 enacted two major changes to Oregon’s drug laws. First, minor drug possession was downgraded from a misdemeanor to a violation, similar to a traffic ticket. Under the new law, users caught with up to 1 gram of heroin or methamphetamine, or up to 40 oxycodone pills, are charged a $100 fine, which can be waived if they call a treatment-referral hotline. (Selling, trafficking, and possessing large amounts of drugs remain criminal offenses in Oregon.) Second, the law set aside a portion of state cannabis tax revenue every two years to fund a statewide network of harm-reduction and other services. A grant-making panel was created to oversee the funding process. At least six members of the panel were required to be directly involved in providing services to drug users; at least two had to be active or former drug users themselves; and three were to be “members of communities that have been disproportionately impacted” by drug criminalization, according to the ballot measure.</p><p>Backers of Measure 110 said the law was modeled on drug policies in Portugal, where personal drug possession was <a data-event-element="inline link" href="https://www.emcdda.europa.eu/drugs-library/portugals-national-drug-strategy-1999-english-version_en">decriminalized</a> two decades ago. But Oregon’s enforcement-and-treatment-referral system differs from Portugal’s. Users caught with drugs in Portugal are referred to a civil commission that evaluates their drug use and recommends treatment if needed, with civil sanctions for noncompliance. Portugal’s state-run health system also funds a nationwide network of treatment services, many of which focus on sobriety. Sutton said drafters of Measure 110 wanted to avoid anything that might resemble a criminal tribunal or coercing drug users into treatment. “People respond best when they’re ready to access those services in a voluntary way,” he said.</p><p>Almost immediately after taking effect, Measure 110 encountered problems. A state <a data-event-element="inline link" href="https://sos.oregon.gov/audits/Documents/2023-03.pdf">audit</a> published this year found that the new law was “vague” about how state officials should oversee the awarding of money to new treatment programs, and set “unrealistic timelines” for evaluating and funding treatment proposals. As a result, the funding process was left largely to the grant-making panel, most of whose members “lacked experience in designing, evaluating and administrating a governmental-grant-application process,” according to the audit. Last year, supporters of Measure 110<a data-event-element="inline link" href="https://www.thelundreport.org/content/people-are-dying-while-state-bureaucracy-holds-treatment-dollars-say-measure-110-proponents"> accused</a> state health officials, preoccupied with the coronavirus pandemic, of giving the panel insufficient direction and resources to handle a flood of grant applications. The state health authority <a data-event-element="inline link" href="https://olis.oregonlegislature.gov/liz/2021I1/Downloads/CommitteeMeetingDocument/255650">acknowledged missteps</a> in the grant-making process.</p><p>The audit described a chaotic process, with more than a dozen canceled meetings, potential conflicts of interest in the selection of funding recipients, and lines of applicant evaluations left blank. Full distribution of the first biennial payout of cannabis tax revenue—$302 million for harm reduction, housing, and other services—did not occur until late 2022, almost two years after Measure 110 passed. <a data-event-element="inline link" href="https://app.powerbigov.us/view?r=eyJrIjoiODU1NDNlNzUtMDBkNy00NTM1LWE4NzgtNGEyNzQxYWY0NTU2IiwidCI6IjY1OGU2M2U4LThkMzktNDk5Yy04ZjQ4LTEzYWRjOTQ1MmY0YyJ9&amp;utm_medium=email&amp;utm_source=govdelivery">Figures</a> released by the state last month show that, in the second half of 2022, recipients of Measure 110 funding provided some form of service to roughly 50,000 “clients,” though <a data-event-element="inline link" href="https://www.thelundreport.org/content/more-people-accessing-help-under-measure-110-gaps-services-data-remain">the Oregon Health Authority has said</a> that a single individual could be counted multiple times in that total. (A <a data-event-element="inline link" href="https://www.thelundreport.org/sites/default/files/OHSU%20-%20Oregon%20Gap%20Analysis%20and%20Inventory%20Report.pdf">study</a> released last year by public-health researchers in Oregon found that, as of 2020, more than 650,000 Oregonians required, but were not receiving, treatment for a substance-use disorder.)</p><p id="injected-recirculation-link-1" data-view-action="view link - injected link - item 2" data-event-element="injected link" data-event-position="2"><a href="https://www.theatlantic.com/magazine/archive/2020/05/nikki-king-opioid-treatment-program/609085/">From the May 2020 issue: America’s other epidemic</a></p><p>Meanwhile, the new law’s enforcement provisions have proved ineffectual. Of 5,299 drug-possession cases filed in Oregon circuit courts since Measure 110 went into effect, 3,381 resulted in a recipient failing to pay the fine or appear in court and facing no further penalties, <a data-event-element="inline link" href="https://www.courts.oregon.gov/about/Documents/BM110Statistics.pdf">according to</a> the Oregon Judicial Department; about 1,300 tickets were dismissed or are pending. The state audit found that, during its first 15 months in operation, the treatment-referral hotline received just 119 calls, at a cost to the state of $7,000 per call. A<a data-event-element="inline link" href="https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=1114&amp;context=ccj_fac"> survey</a> of law-enforcement officers conducted by researchers at Portland State University found that, as of July 2022, officers were issuing an average of just 300 drug-possession tickets a month statewide, compared with 600 drug-possession arrests a month before Measure 110 took effect and close to 1,200 monthly arrests prior to the outbreak of COVID-19.</p><p>“Focusing on these tickets even though they’ll be ineffective—it’s not a great use of your resources,” Sheriff Nate Sickler of Jackson County, in the rural southern part of the state, told me of his department’s approach.</p><p>Advocates have celebrated a<a data-event-element="inline link" href="https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=1114&amp;context=ccj_fac"> plunge in arrests</a>. “For reducing arrests of people of color, it’s been an overwhelming success,” says Mike Marshall, the director of Oregon Recovers. But critics say that sidelining law enforcement has made it harder to persuade some drug users to stop using. Sickler cited the example of drug-court programs, <a data-event-element="inline link" href="https://crimesolutions.ojp.gov/ratedprograms/504">which</a><a data-event-element="inline link" href="https://crimesolutions.ojp.gov/ratedprograms/125"> multiple</a><a data-event-element="inline link" href="https://crimesolutions.ojp.gov/ratedprograms/93"> studies</a><a data-event-element="inline link" href="https://crimesolutions.ojp.gov/ratedprograms/128"> have</a><a data-event-element="inline link" href="https://crimesolutions.ojp.gov/ratedprograms/478"> shown</a> to be<a data-event-element="inline link" href="https://crimesolutions.ojp.gov/ratedprograms/597"> highly</a><a data-event-element="inline link" href="https://crimesolutions.ojp.gov/ratedprograms/69"> effective</a>, including in <a data-event-element="inline link" href="https://crimesolutions.ojp.gov/ratedprograms/115">Jackson County</a>. Use of such programs in the county has declined in the absence of criminal prosecution, Sickler said: “Without accountability or the ability to drive a better choice, these individuals are left to their own demise.”</p><p>The consequences of Measure 110’s shortcomings have fallen most heavily on Oregon’s drug users. In the two years after the law took effect, the number of annual overdoses in the state rose by 61 percent, compared with a 13 percent increase nationwide, <a data-event-element="inline link" href="http://www.cdc.gov/nchs/nvss/vsrr/drug-overdose-data.htm">according to</a> the Centers for Disease Control and Prevention. In neighboring Idaho and California, where drug possession remains subject to prosecution, the rate of increase was significantly lower than Oregon’s. (The spike in Washington State was similar to Oregon’s, but that comparison is more complicated because Washington’s drug policy has fluctuated since 2021.) Other states once notorious for drug deaths, including West Virginia, Indiana, and Arkansas, are now experiencing declines in overdose rates.</p><p>In downtown Portland this spring, police cleared out what <a data-event-element="inline link" href="https://www.oregonlive.com/crime/2023/04/portland-police-clear-out-washington-center-open-air-drug-market-in-early-morning-mission.html"><em>The Oregonian </em>called</a> an “open-air drug market” in a former retail center. Prominent businesses in the area, including the outdoor-gear retailer REI, have announced closures in recent months, <a data-event-element="inline link" href="https://oregonbusiness.com/rei-announces-closure-of-portland-store/">in part citing</a> a rise in shoplifting and violence. Earlier this year, Portland business owners<a data-event-element="inline link" href="https://www.wweek.com/news/2023/03/01/a-familiar-fight-between-businesses-and-county-officials-over-homeless-services-plays-out-in-portlands-hotel-district/"> appeared</a> before the Multnomah County Commission to ask for help with crime, drug-dealing, and other problems stemming from a behavioral-health resource center operated by a harm-reduction nonprofit that was awarded more than $4 million in Measure 110 funding. In April, the center abruptly closed following employee complaints that clients were covering walls with graffiti and overdosing on-site. A subsequent<a data-event-element="inline link" href="https://www.multco.us/multnomah-county/news/behavioral-health-resource-center-reopens-after-undergoing-security-building"> investigation</a> by the nonprofit found that a security contractor had been using cocaine on the job. The center reopened two weeks later with beefed-up security measures.</p><p>Portland’s Democratic mayor, Ted Wheeler, went so far as to attempt an end run around Measure 110 in his city. Last month, Wheeler unveiled a proposal to criminalize public drug consumption in Portland, similar to existing bans on open-air drinking, saying in a <a data-event-element="inline link" href="https://www.portland.gov/wheeler/documents/full-statement-ordinance/download">statement</a> that Measure 110 “is not working as it was intended to.” He added, “Portland’s substance-abuse problems have exploded to deadly and disastrous proportions.” Wheeler withdrew the proposal days later after learning that an older state law prohibits local jurisdictions from banning public drug use.</p><p>Despite shifting public opinion on Measure 110, many Oregon leaders are not ready to give up on the policy. Earlier this month, Oregon Governor Tina Kotek signed <a data-event-element="inline link" href="https://olis.oregonlegislature.gov/liz/2023R1/Measures/Overview/HB2513">legislation</a> that strengthens state oversight of Measure 110 and requires an audit, due no later than December 2025, of about two dozen aspects of the measure’s performance, including whether it is reducing overdoses. Other bills passed by the legislature’s Democratic majority strengthened criminal penalties for possession of large quantities of fentanyl and mandated that school drug-prevention programs instruct students about the risks of synthetic opioids. Republican proposals to repeal Measure 110 outright or claw back tens of millions of dollars in harm-reduction funding were not enacted.</p><p>The fallout from Measure 110 has received some critical coverage from media outlets on the right. “It is predictable,” a scholar from the Hudson Institute <a data-event-element="inline link" href="https://www.foxnews.com/us/portland-drug-decriminalization-effort-tragedy">told Fox News</a>. “It is a tragedy and a self-inflicted wound.” (Meanwhile, in Portugal, the model for Oregon, <a data-event-element="inline link" href="https://www.washingtonpost.com/world/2023/07/07/portugal-drugs-decriminalization-heroin-crack/">some residents are raising questions</a> about their own nation’s decriminalization policy.) But so far Oregon’s experience doesn’t appear to have stopped efforts to bring decriminalization to other parts of the United States. “We’ll see more ballot initiatives,” Sutton, of the Drug Policy Alliance, said, adding that advocates are currently working with city leaders to decriminalize drugs in Washington, D.C.</p><p id="injected-recirculation-link-2" data-view-action="view link - injected link - item 3" data-event-element="injected link" data-event-position="3"><a href="https://www.theatlantic.com/health/archive/2022/01/naloxone-stronger-form-opioid-overdose/621254/">Read: An anti-overdose drug is getting stronger. Maybe that’s a bad thing?</a></p><p>Supporters of Measure 110 are now seeking to draw attention to what they say are the policy’s overlooked positive effects. This summer, the Health Justice Recovery Alliance, a Measure 110 advocacy organization, is leading an effort to spotlight expanded treatment services and boost community awareness of the treatment-referral hotline. Advocates are also coordinating with law-enforcement agencies to ensure that officers know about local resources for drug users. “People are hiring for their programs; outreach programs are expanding, offering more services,” Devon Downeysmith, the communications director for the group, told me.</p><p>An array of services around the state have been expanded through the policy: housing for pregnant women awaiting drug treatment; culturally specific programs for Black, Latino, and Indigenous drug users; and even distribution of bicycle helmets to people unable to drive to treatment meetings. “People often forget how much time it takes to spend a bunch of money and build services,” said Wheelock, the homeless-services worker, whose organization received more than $2 million in funding from Measure 110.</p><p>Still, even some recipients of Measure 110 funding wonder whether one of the law’s pillars—the citation system that was supposed to help route drug users into treatment—needs to be rethought. “Perhaps some consequences might be a helpful thing,” says Julia Pinsky, a co-founder of Max’s Mission, a harm-reduction nonprofit in southern Oregon. Max’s Mission has received $1.5 million from Measure 110, enabling the organization to hire new staff, open new offices, and serve more people. Pinsky told me she is proud of her organization’s work and remains committed to the idea that “you shouldn’t have to go to prison to be treated for substance use.” She said that she doesn’t want drug use to “become a felony,” but that some people aren’t capable of stopping drug use on their own. “They need additional help.”</p><p>Brandi Fogle, a regional manager for Max’s Mission, says her own story illustrates the complex trade-offs involved in reforming drug policy. Three and a half years ago, she was a homeless drug user, addicted to heroin and drifting around Jackson and Josephine Counties. Although she tried to stop numerous times, including one six-month period during which she was prescribed the drug-replacement medication methadone, she told me that a 2020 arrest for drug possession was what finally turned her life around. She asked to be enrolled in a 19-month drug-court program that included residential treatment, mandatory 12-step meetings, and a community-service project, and ultimately was hired by Pinsky.</p><p>Since Measure 110 went into effect, Fogle said, she has gotten pushback from members of the community for the work Max’s Mission does. She said that both the old system of criminal justice and the new system of harm reduction can benefit drug users, but that her hope now is to make the latter approach more successful. “Everyone is different,” Fogle said. “Drug court worked for me because I chose it, and I wouldn’t have needed drug court in the first place if I had received the kind of services Max’s Mission provides. I want to offer people that chance.”</p><hr><p><small><em>This article originally suggested that REI’</em><em>s store in Portland had closed; it is scheduled to close early next year.</em></small></p></section><gpt-ad format="injector" sizes-at-0="mobile-wide,native,house" targeting-pos="injector-most-popular" sizes-at-976="desktop-wide,native,house"></gpt-ad></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[City officials attempt to doxx Wikipedians (167 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-08-01/News_and_notes</link>
            <guid>36958958</guid>
            <pubDate>Tue, 01 Aug 2023 17:10:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-08-01/News_and_notes">https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-08-01/News_and_notes</a>, See on <a href="https://news.ycombinator.com/item?id=36958958">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
<h3><span id="Amidst_city_council_infighting.2C_Durham.2C_North_Carolina_city_officials_attempt_to_doxx_Wikipedians"></span><span id="Amidst_city_council_infighting,_Durham,_North_Carolina_city_officials_attempt_to_doxx_Wikipedians" data-mw-thread-id="h-Amidst_city_council_infighting,_Durham,_North_Carolina_city_officials_attempt_to-signpost-article-title"><span data-mw-comment-start="" id="h-Amidst_city_council_infighting,_Durham,_North_Carolina_city_officials_attempt_to-signpost-article-title"></span>Amidst city council infighting, Durham, North Carolina city officials attempt to doxx Wikipedians<span data-mw-comment-end="h-Amidst_city_council_infighting,_Durham,_North_Carolina_city_officials_attempt_to-signpost-article-title"></span></span></h3>
</p><div>
<p>The city attorney of <a href="https://en.wikipedia.org/wiki/Durham,_North_Carolina" title="Durham, North Carolina">Durham, North Carolina</a> attempted to coax the Wikimedia Foundation (WMF) to reveal the identities of three editors and to prohibit the placement of certain verifiable and truthful content on Wikipedia pages of city officials, <a rel="nofollow" href="https://indyweek.com/news/durham-officials-directed-city-attorney-to-try-to-unmask-anonymous-wikipedia-editors/"><i>Indy Week</i></a> and <a rel="nofollow" href="https://www.newsobserver.com/news/local/counties/durham-county/article277624088.html"><i>The News and Observer</i></a> report.
</p><p>A letter, dated June 29, outlined three complaints about content on Wikipedia. Two of the complaints pertained to coverage of a council member's alleged attempted extortion of a developer, while the third related to an image depicting the signature of mayor of Durham. The letter requested that the Wikimedia foundation remove the image and bar users from uploading it on Wikimedia projects, and requested the names and identities of the various editors who added the text and/or image content to Wikipedia.
</p>
</div><div>
<p>Recent months have been a tumultuous time for Durham's seven-member city council. In March, <a href="https://en.wikipedia.org/wiki/Elaine_M._O%27Neal" title="Elaine M. O'Neal">Elaine M. O'Neal</a>, the mayor of Durham, publicly read an <a rel="nofollow" href="https://www.newsobserver.com/news/politics-government/article273909285.html">allegation</a> that a Durham city council member (subsequently identified as <a href="https://en.wikipedia.org/wiki/Monique_Holsey-Hyman" title="Monique Holsey-Hyman">Monique Holsey-Hyman</a>) had extorted a developer for campaign contributions. The aftermath of the meeting was <a rel="nofollow" href="https://www.wral.com/full-raw-video-profanity-laced-outburst-follows-criminal-allegations-at-durham-city-council-meeting/20777708/">testy</a>, with the public able to hear shouting between officials, despite them being out of public view. An eyewitness <a rel="nofollow" href="https://indyweek.com/news/durham/durham-city-council-member-allegedly-punched-two-fellow-council-members-including-the-mayor-following-testy-work-session/">interviewed</a> by <i>Indy Week</i> alleged that Durham council member <a href="https://en.wikipedia.org/wiki/DeDreana_Freeman" title="DeDreana Freeman">DeDreana Freeman</a> had attempted to strike Durham Mayor-pro tempore and council member Mark-Anthony Middleton during the shouting session, but instead struck O'Neal once and punched the head of fellow Durham council member Leonardo Williams twice before Williams subdued her. In the aftermath of these incidents, O'Neal <a rel="nofollow" href="https://www.newsobserver.com/news/local/counties/durham-county/article276653196.html">announced</a> that she would not seek re-election as Mayor and a <a rel="nofollow" href="https://www.newsobserver.com/news/local/counties/durham-county/article273656055.html">state investigation</a> was opened into the extortion allegation (Holsey-Hyman <a rel="nofollow" href="https://www.newsobserver.com/news/politics-government/election/voter-guide/article276872203.html">denies</a> the alleged extortion attempt and a separate <a rel="nofollow" href="https://www.newsobserver.com/news/local/counties/durham-county/article273507350.html">allegation</a> that she ordered city employees to perform campaign work on her behalf).
</p><p>For making edits to the Wikipedia entries about certain figures implicated in this scandal, the letter requested the identities of <span><a href="https://en.wikipedia.org/wiki/User:Mako001">Mako001</a></span> and <span><a href="https://en.wikipedia.org/wiki/User:Willthacheerleader18">Willthacheerleader18</a></span>. The entries contained unflattering information about the public officials at the time of the letter's sending, but the entries were well-sourced; <i>Indy Week</i> reports that the entries' descriptions of the scandal were written "without any apparent factual error and with links to news articles as references".
</p><p>Several figures have publicly expressed concerns about the sending of the letter. Barry Saunders, a member of the editorial board of <i>The News and Observer</i>, <a rel="nofollow" href="https://www.newsobserver.com/opinion/article277773328.html">wrote</a> that, "[u]nless the Wikipedia posts were egregiously wrong—and there’s no evidence that they were—the three Durham officials should have taken a page, when it came to criticism, from the title of the 1970s hit by the band Bachman-Turner Overdrive: let it ride... Few voters, though, will forgive attempts to silence critics". 
</p><p>Duke University law professor Stuart Benjamin was taken aback by the letter. He told <i>The News and Observer</i>, "I understand why public officials do not want unflattering information published about them, but it is deeply troubling that any public official tried to unmask someone who posted this accurate information."
</p><p>David Larson, opinion editor of <i>The Carolina Journal</i>, concurs. "[T]his attempt to intimidate anonymous people online for daring to discuss real but unflattering details of your political service is the stuff of dysfunctional regimes", he <a rel="nofollow" href="https://www.carolinajournal.com/opinion/durhams-attempt-to-intimidate-wikipedia-over-unflattering-pages-backfires/">wrote</a>.
</p><p>The WMF, for its part, told <i>Indy Week</i> that it is "strongly committed to protecting the privacy of editors and users on Wikimedia projects".
</p><p>The letter, signed by city attorney Kimberly Rehberg, also states that she had removed the image of the signature from the Wikipedia article about <a href="https://en.wikipedia.org/wiki/Elaine_M._O%27Neal" title="Elaine M. O'Neal">Elaine M. O'Neal</a> on June 28. This checks out; that article <i>was</i> <a href="https://en.wikipedia.org/wiki/Special:Diff/1162379330" title="Special:Diff/1162379330">edited on that day</a> by a user named <span><a href="https://en.wikipedia.org/wiki/User:Kimlynn69">Kimlynn69</a></span>, and Kimlynn69 wrote a message to <span><a href="https://en.wikipedia.org/wiki/User:Johnson524">Johnson524</a></span> that <a href="https://en.wikipedia.org/wiki/Special:PermanentLink/1167184345#Signature_of_Elaine_M._O'Neal,_Mayor,_City_of_Durham,_NC" title="Special:PermanentLink/1167184345">identified herself</a> as "Kimberly M. Rehberg" and as the city attorney of Durham. Like it did for the editors who touched content relating to the scandal of the March 23 meeting, the letter had also requested Johnson524's name and identity as well.
</p><p>In response to Rehberg's message, Johnson524 explained that he had obtained the signature from <a href="https://en.wikipedia.org/wiki/Durham_Performing_Arts_Center" title="Durham Performing Arts Center">Durham Performing Arts Center</a> playbills. <i>Indy Week</i> reports that, following Johnson's reply to the message, Rehberg said in an email "there is little legal basis to demand that Wiki reveal the identity of the User or prohibit the upload of a photo of the signature to the Mayor’s Wiki page".
</p><p>The mayor, per an email obtained by <i>Indy Week</i>, was unsatisfied with Rehberg's reply. O'Neal told Rehberg that her request to send the letter to the WMF "still stands"; Rehberg said in an email sent later that day that the letter had been sent. Despite this, the letter may have never actually arrived at its intended destination. The WMF told <i>Indy Week</i> that they had not received the letter and that the letter that had been made public contained an incorrect postal address for the WMF's headquarters. Rehberg, meanwhile, told <i>Indy Week</i> that the letter had only been sent by physical mail.
</p><p><i>The Signpost</i> reached out to Johnson524 following the publication of Rehberg's letter. "I was so happy to see an outpouring of support from the Wikipedia community from editors who have been around longer than I have," he wrote: "I have always valued that Wikimedia has also never succumbed to external powers—and has continued to fight for a world of free information: whether that be not to take down/severely censor their project in Russia, to campaign for those jailed editors in Saudi Arabia, or even just go against unjust decisions by local governments here in the U.S.".
</p><p>He remained, however, displeased with the mayor's handling of the situation. "I would have even put it past the mayor Elaine O'Neal if she went back on her statement after I explained how I got the signature publicly, but since she doubled down on her attempt to try to 'unmask' me and two other editors after without really any prior contact, I am glad she will not be running for mayor again, because I don't think how this situation was handled was right at all", he wrote. – <span><a href="https://en.wikipedia.org/wiki/User:Red-tailed_hawk" title="User:Red-tailed hawk">R</a></span>
</p><p><i>(For further coverage of this story see this issue's <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-08-01/In_the_media" title="Wikipedia:Wikipedia Signpost/2023-08-01/In the media">In the media</a>.)</i>
</p>
<h3><span id="Ruwiki_founder_banned_from_editing_Wikimedia_sites" data-mw-thread-id="h-Ruwiki_founder_banned_from_editing_Wikimedia_sites-signpost-article-title"><span data-mw-comment-start="" id="h-Ruwiki_founder_banned_from_editing_Wikimedia_sites-signpost-article-title"></span>Ruwiki founder banned from editing Wikimedia sites<span data-mw-comment-end="h-Ruwiki_founder_banned_from_editing_Wikimedia_sites-signpost-article-title"></span></span></h3>
</div><div>
<p>Vladimir Medeyko (<a href="https://en.wikipedia.org/wiki/User:Drbug" title="User:Drbug">User:Drbug</a>), the former head of <a href="https://meta.wikimedia.org/wiki/Wikimedia_Russia" title="meta:Wikimedia Russia">Wikimedia Russia</a> and founder of the Russian government-approved <a href="https://en.wikipedia.org/wiki/Ruwiki_(website)" title="Ruwiki (website)">Ruwiki fork</a>, has been <a href="https://en.wikipedia.org/wiki/User:Drbug" title="User:Drbug">"banned indefinitely by the Wikimedia Foundation from editing all Wikimedia sites"</a>. Medeyko had <a href="https://meta.wikimedia.org/wiki/Special:CentralAuth/Drbug" title="meta:Special:CentralAuth/Drbug">previously been blocked indefinitely</a> on the Russian Wikipedia, following a <a href="https://ru.wikipedia.org/wiki/%D0%92%D0%B8%D0%BA%D0%B8%D0%BF%D0%B5%D0%B4%D0%B8%D1%8F:%D0%9F%D1%80%D0%BE%D1%81%D1%8C%D0%B1%D0%B0_%D0%BF%D1%80%D0%BE%D0%BA%D0%BE%D0%BC%D0%BC%D0%B5%D0%BD%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D1%82%D1%8C/%D0%A0%D1%83%D0%B2%D0%B8%D0%BA%D0%B8_%D0%BE%D1%82_%D0%92%D0%BB%D0%B0%D0%B4%D0%B8%D0%BC%D0%B8%D1%80%D0%B0_%D0%9C%D0%B5%D0%B4%D0%B5%D0%B9%D0%BA%D0%BE" title="ru:Википедия:Просьба прокомментировать/Рувики от Владимира Медейко">discussion</a> at the Russian Wikipedia's Administrators' Noticeboard, as well as on Commons, where the <a href="https://commons.wikimedia.org/w/index.php?title=Special:Log/block&amp;page=User%3ADrbug">reason given</a> was –
</p>
<blockquote><p>Long-term abuse: creating a Wikipedia fork which includes stolen content from Commons as well
</p></blockquote>
<p>See also previous <i>Signpost</i> coverage <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-07-17/In_the_media" title="Wikipedia:Wikipedia Signpost/2023-07-17/In the media">here</a> and <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-06-05/News_and_notes" title="Wikipedia:Wikipedia Signpost/2023-06-05/News and notes">here</a>. – <span><a href="https://en.wikipedia.org/wiki/User:Jayen466" title="User:Jayen466">AK</a></span>
</p>
<h3><span id="EU_policy_report_.E2.80.93_trouble_brewing_in_France_and_Italy"></span><span id="EU_policy_report_–_trouble_brewing_in_France_and_Italy" data-mw-thread-id="h-EU_policy_report_–_trouble_brewing_in_France_and_Italy-signpost-article-title"><span data-mw-comment-start="" id="h-EU_policy_report_–_trouble_brewing_in_France_and_Italy-signpost-article-title"></span>EU policy report – trouble brewing in France and Italy<span data-mw-comment-end="h-EU_policy_report_–_trouble_brewing_in_France_and_Italy-signpost-article-title"></span></span></h3>
</div><div>
<p><a href="https://meta.wikimedia.org/wiki/Wikimedia_Europe" title="m:Wikimedia Europe">Wikimedia Europe</a> has published its <a href="https://lists.wikimedia.org/hyperkitty/list/publicpolicy@lists.wikimedia.org/thread/VH2KRMZN2STSRYC7LAENN2YYTXCSRFXE/">European Policy Monitoring Report for July 2023</a>. Among other current legal developments, it highlights that –
</p>
<blockquote><p>France is working on a tech bill to regulate the entire online environment [...,] the <a rel="nofollow" href="https://www.senat.fr/dossier-legislatif/pjl22-593.html">projet de loi visant à sécuriser et réguler l’espace numérique (SREN)</a>. There are several problematic articles and aspects in the proposal that would change how content moderation on [Wikimedia] projects works. Such examples are provisions aiming to keep links to 'banned' media off websites (think <a href="https://en.wikipedia.org/wiki/Russia_Today" title="Russia Today">Russia Today</a>) or an obligation to not allow banned users from re-registering (which would require some sort of background check on all new registrations).
</p></blockquote>
<p>The report also calls attention to "Italy['s] Crusade Against the Public Domain", referring the country's efforts "to restrict and get paid for re-use of public domain material" such as Leonardo da Vinci's <a href="https://en.wikipedia.org/wiki/Vitruvian_Man" title="Vitruvian Man">Vitruvian Man</a>. – <span><a href="https://en.wikipedia.org/wiki/User:HaeB" title="User:HaeB">H</a></span>
</p>
<h3><span id="Foundation_launches_its_own_Mastodon_server" data-mw-thread-id="h-Foundation_launches_its_own_Mastodon_server-signpost-article-title"><span data-mw-comment-start="" id="h-Foundation_launches_its_own_Mastodon_server-signpost-article-title"></span>Foundation launches its own Mastodon server<span data-mw-comment-end="h-Foundation_launches_its_own_Mastodon_server-signpost-article-title"></span></span></h3>
<p>The Wikimedia Foundation has launched an instance on <a href="https://en.wikipedia.org/wiki/Mastodon_(social_network)" title="Mastodon (social network)">the federated social network Mastodon</a>, at <a rel="nofollow" href="https://wikimedia.social/">https://wikimedia.social/</a> (for <a href="https://phabricator.wikimedia.org/T337586">technical reasons</a>, it was not possible to use a wikimedia.org domain). According to a July 17 <a href="https://lists.wikimedia.org/hyperkitty/list/wikimedia-l@lists.wikimedia.org/message/A2STMXTA3BKPO2VYWX6SEUWEJQOB477Y/">announcement on Wikimedia-l</a>,
</p>
<blockquote><p>At the moment, sign-up is open for Wikimedia Foundation staff as we examine moderation and other areas. Product and technology staff will use it primarily for developer engagement. The goal is to create a space for people to connect and talk tech.
</p></blockquote>
<p>At the time of writing (July 30), the server lists 72 active users, although its <a rel="nofollow" href="https://wikimedia.social/directory">directory</a> of recently active local users only shows five who have posted. The Foundation's <a rel="nofollow" href="https://wikimedia.social/@wikimediafoundation">own @wikimediafoundation account</a> leads, with 14 posts, and has already gained over 5000 followers – undoubtedly helped by a <a href="https://en.wikipedia.org/wiki/Hacker_News" title="Hacker News">Hacker News</a> <a rel="nofollow" href="https://news.ycombinator.com/item?id=36763357">post</a> that made it <a rel="nofollow" href="https://web.archive.org/web/20230717205153/https://news.ycombinator.com/news">(near) the top</a> of that site's front page.
</p><p>The announcement comes amid continuing concerns about <a href="https://en.wikipedia.org/wiki/Twitter" title="Twitter">Twitter</a> (where the corresponding <a rel="nofollow" href="https://twitter.com/Wikimedia">@wikimedia account</a> remains active, although viewing a list of its recent tweets currently requires registration, due to recent changes by X née Twitter). In late 2022, suggestions that the Foundation should mirror the official Wikipedia Twitter account (run by its Communications department) on Mastodon had fallen flat. This later motivated the creation of a community-run Wikipedia account on the <a href="https://meta.wikimedia.org/wiki/Wikis_World" title="m:Wikis World">Wikis World</a> Mastodon server in April 2023 (see our coverage: <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-04-26/News_and_notes#Wikipedia_gains_an_official_presence_on_Mastodon_..._without_the_Wikimedia_Foundation's_involvement" title="Wikipedia:Wikipedia Signpost/2023-04-26/News and notes">"Wikipedia gains an official presence on Mastodon ... without the Wikimedia Foundation's involvement"</a> and <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-05-08/News_and_notes#Who_speaks_for_Wikipedia?_Mastodon_accreditation_reverted." title="Wikipedia:Wikipedia Signpost/2023-05-08/News and notes">"Who speaks for Wikipedia? Mastodon accreditation reverted"</a>). At the time of writing, it <a rel="nofollow" href="https://wikis.world/@wikipedia">continues to be active</a>, with 16K followers and a verified checkmark, while <a href="https://meta.wikimedia.org/wiki/Talk:@Wikipedia#Hi!_Available_to_chat?" title="m:Talk:@Wikipedia">requests by WMF staff</a> "to change the name of the account [from @wikipedia] to 'Wikipedia movement', 'Wikipedia volunteers', 'Wikipedia worldwide', or something similar" remain unheeded. – <span><a href="https://en.wikipedia.org/wiki/User:HaeB" title="User:HaeB">H</a></span>
</p>
<h3><span id="Brief_notes" data-mw-thread-id="h-Brief_notes-signpost-article-title"><span data-mw-comment-start="" id="h-Brief_notes-signpost-article-title"></span>Brief notes<span data-mw-comment-end="h-Brief_notes-signpost-article-title"></span></span></h3>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Magic Loops – Combine LLMs and code to create simple automations (154 pts)]]></title>
            <link>https://magicloops.dev</link>
            <guid>36958731</guid>
            <pubDate>Tue, 01 Aug 2023 16:57:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://magicloops.dev">https://magicloops.dev</a>, See on <a href="https://news.ycombinator.com/item?id=36958731">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><form></form><div><p><a href="https://www.loom.com/share/dc8ad18eed514f3783abb6ee3314076c?sid=8a7b137b-3393-44c3-b608-d19c9c3181bd">Watch a demo here</a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: In which areas have you compared 3+ tools and formed strong preferences? (101 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=36958516</link>
            <guid>36958516</guid>
            <pubDate>Tue, 01 Aug 2023 16:43:16 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=36958516">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="36958516">
      <td><span></span></td>      <td><center><a id="up_36958516" href="https://news.ycombinator.com/vote?id=36958516&amp;how=up&amp;goto=item%3Fid%3D36958516"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=36958516">Ask HN: In which areas have you compared 3+ tools and formed strong preferences?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_36958516">101 points</span> by <a href="https://news.ycombinator.com/user?id=tikkun">tikkun</a> <span title="2023-08-01T16:43:16"><a href="https://news.ycombinator.com/item?id=36958516">13 hours ago</a></span> <span id="unv_36958516"></span> | <a href="https://news.ycombinator.com/hide?id=36958516&amp;goto=item%3Fid%3D36958516">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20In%20which%20areas%20have%20you%20compared%203%2B%20tools%20and%20formed%20strong%20preferences%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=36958516&amp;auth=db5e09dc08790f808697d2fd623bc9f983fd5f1c">favorite</a> | <a href="https://news.ycombinator.com/item?id=36958516">158&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><div><p>For example:</p><p>I've used 3+ code editors on MacOS and prefer sublime text over VScode, coteditor, xcode.</p><p>I've used Chrome, Firefox and Safari all within the past year, and I prefer Chrome and Safari.</p><p>I've used 3+ voice transcription apps, the ones I prefer are 'just press record' and Otter.ai, I can't remember the names of the others I used - but I downloaded a bunch of Whisper based ones and non-whisper based ones on the iOS app store.</p><p>I've used 3+ messaging apps, I like iMessage and Telegram, I prefer those over Signal and WhatsApp.</p><p>I've used 3+ interfaces for GPT, and I prefer the OpenAI playground over ChatGPT, chatbot-ui, typingmind, openplayground, Poe, and a few others I can't remember.</p><p>I think it's important to note that for me these are all preferences. I'm not saying one of these tools are objectively better. I am saying that for me they are better, and I prefer them.</p><p>What are areas of products, tools, developer tools, APIs, anything, where you've personally used 3+ tools, and what did you prefer, and what did you not prefer, if you can still remember?</p></div></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Senate Votes to Let People Who’ve Used Marijuana Work at Intelligence Agencies (281 pts)]]></title>
            <link>https://www.marijuanamoment.net/senate-votes-to-let-people-whove-used-marijuana-work-at-intelligence-agencies-like-cia-and-nsa-as-part-of-defense-bill/</link>
            <guid>36958424</guid>
            <pubDate>Tue, 01 Aug 2023 16:38:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.marijuanamoment.net/senate-votes-to-let-people-whove-used-marijuana-work-at-intelligence-agencies-like-cia-and-nsa-as-part-of-defense-bill/">https://www.marijuanamoment.net/senate-votes-to-let-people-whove-used-marijuana-work-at-intelligence-agencies-like-cia-and-nsa-as-part-of-defense-bill/</a>, See on <a href="https://news.ycombinator.com/item?id=36958424">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mvp-content-main">
<p>The U.S. Senate has approved a large-scale defense bill that includes provisions to bar intelligence agencies like the CIA and NSA from denying security clearances to applicants solely due to their past marijuana use.</p>
<p>Senators adopted a number of amendments to the National Defense Authorization Act (NDAA) on Thursday before approving the overall legislation. That included attaching the full text of the separate Intelligence Authorization Act, which was itself previously <a href="https://www.marijuanamoment.net/senate-panel-votes-to-let-people-whove-used-marijuana-work-at-intelligence-agencies-like-cia-and-nsa/" target="_blank" rel="noopener">amended in committee last month to include the cannabis provision</a> from Sen. Ron Wyden (D-OR).</p>
<p>Previously, the senator filed a broader amendment to last year’s version of the authorization legislation that would have prevented employment discrimination based on prior or present cannabis use at any federal department, not just those dealing with intelligence.</p>
<p>But the provision was&nbsp;<a href="https://www.marijuanamoment.net/key-senate-committee-chairman-scaled-back-marijuana-amendment-on-security-clearance-denials-to-secure-passing-vote/" target="_blank" rel="noopener">scaled back under a second-degree amendment</a> from the committee chairman before being adopted by the panel. And then the reform was ultimately quashed altogether <a href="https://www.marijuanamoment.net/gop-senators-block-marijuana-security-clearance-protections-in-defense-bill-amendment-prompting-sponsor-to-file-broader-measures/" target="_blank" rel="noopener">when two GOP senators objected to attaching</a> the intelligence bill to the NDAA on the floor if it included the marijuana language.</p>
<p>But that level of pushback did not happen this year, and now the full Senate has <a href="https://www.congress.gov/amendment/118th-congress/senate-amendment/1087" target="_blank" rel="noopener">signed off</a> on protecting people from losing security clearances because of prior marijuana use.</p>
<p>“Notwithstanding any other provision of law, the head of an element of the intelligence community may not make a determination to deny eligibility for access to classified information to an individual based solely on the use of cannabis by the individual prior to the submission of the application for a security clearance by the individual,” the newly approved provision says.</p>
<p>Sen. Michael Bennet (D-CO), who cosponsored the reform in committee along with Wyden and Sen. Martin Heinrich (D-NM), said in a <a href="https://www.bennet.senate.gov/public/index.cfm/press-releases?id=B07CA7B8-AA42-43F4-899E-D6A077E78B10" target="_blank" rel="noopener">press release</a> that it will “modernize workforce recruitment by prohibiting intelligence community agencies from denying a security clearance to individuals based solely on past use of cannabis.”</p>
<p>A newly published Senate Intelligence Committee report on the larger legislation shows that the panel <a href="https://www.intelligence.senate.gov/publications/intelligence-authorization-act-fiscal-year-2024" target="_blank" rel="noopener">approved</a> the marijuana provision by a party-line vote of 10 to 7 last month.</p>
<p>“As more states legalize cannabis, it becomes less and less tenable to deny security clearances to those who have used it,” Wyden said in remarks inserted into the report. “The amendment…will help the Intelligence Community recruit the qualified personnel needed to protect the country.”</p>
<p>The senator had also filed a separate broader amendment in committee that would have “prohibited the head of any U.S. Government agency from denying an individual’s eligibility for access to classified information based solely on the individual’s cannabis use prior to submitting a security clearance application,” but ultimately withdrew it without a vote, the report says.</p>
<p>The White House on Thursday issued a statement of administration policy that <a href="https://www.whitehouse.gov/wp-content/uploads/2023/07/S2226-NDAA-SAP-Followon.pdf" target="_blank" rel="noopener">expresses</a> concerns with several provisions of the NDAA legislation, but is silent on the cannabis language.</p>
<p>House intelligence legislation, meanwhile, has cleared committee in that chamber but has not yet come up for floor consideration—and at this point contains no cannabis provisions.</p>
<p>The Senate-approved discretionary policy change was <a href="https://www.marijuanamoment.net/democratic-congressman-hopes-for-very-bipartisan-support-on-marijuana-security-clearance-and-military-reform-amendments/" target="_blank" rel="noopener">less far reaching than a related amendment</a> that Rep. Robert Garcia (D-CA) tried to attach to the House version of the NDAA that would have prevented the denial of security clearances for federal jobs based solely on prior cannabis use—but was ultimately not made in order for floor consideration by the Rules Committee, nor were <a href="https://www.marijuanamoment.net/gop-controlled-house-committee-blocks-every-marijuana-and-psychedelics-amendment-to-defense-bill-from-getting-floor-vote/" target="_blank" rel="noopener">more than a dozen other drug policy reform proposals</a>.</p>
<p>It’s not yet clear if efforts to attach similar provisions will be made, or whether they will succeed, when the separate intelligence bill comes before the House Rules Committee.</p>
<p>There were <a href="https://www.marijuanamoment.net/senators-seek-to-legalize-medical-marijuana-for-veterans-and-enact-other-cannabis-reforms-through-defense-bill/" target="_blank" rel="noopener">other marijuana-related amendments</a> proposed for the Senate NDAA this round, including a proposal to legalize medical cannabis for military veterans, but they also were not ultimately considered.</p>
<p>Wyden also sought to revise the defense bill with a separate amendment to make it so prior use of marijuana “may be relevant, but not determinative, to adjudications of the eligibility of the individual for access to classified information or the eligibility of the individual to hold a sensitive position.” But that did not advance either.</p>
<p>Meanwhile, on the House side, bipartisan lawmakers filed a standalone bill on Thursday to <a href="https://www.marijuanamoment.net/federal-agencies-couldnt-refuse-to-hire-people-because-of-marijuana-use-under-new-congressional-bill/" target="_blank" rel="noopener">protect people from being denied federal employment or security clearances due to marijuana use</a>—and to provide relief for people who lost opportunities due to cannabis in the past.</p>
<p><strong>—<br>
<strong>Marijuana Moment is <a href="https://www.marijuanamoment.net/bills/" target="_blank" rel="noopener">tracking more than 1,000 cannabis, psychedelics and drug policy bills</a> in state legislatures and Congress this year. <a href="https://www.patreon.com/marijuanamoment" target="_blank" rel="noopener">Patreon supporters</a> pledging at least $25/month get access to our interactive maps, charts and hearing calendar so they don’t miss any developments.</strong><br>
<a href="https://www.marijuanamoment.net/bills/" target="_blank" rel="noopener"><img data-lazy-fallback="1" decoding="async" loading="lazy" src="https://www.marijuanamoment.net/wp-content/uploads/2018/12/MM_Bill_Tracker_V5_blank.jpg" alt="" width="300" height="300" data-lazy-src="https://www.marijuanamoment.net/wp-content/uploads/2018/12/MM_Bill_Tracker_V5_blank.jpg?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><br>
<strong>Learn more about our <a href="https://www.marijuanamoment.net/bills/" target="_blank" rel="noopener">marijuana bill tracker</a> and become a <a href="https://www.patreon.com/marijuanamoment/" target="_blank" rel="noopener">supporter on Patreon</a> to get access.<br>
—</strong></strong></p>
<p>The Director of National Intelligence (DNI) issued a memo in 2021 saying that federal employers&nbsp;<a href="https://www.marijuanamoment.net/top-federal-intelligence-official-loosens-marijuana-restrictions-for-workers-and-addresses-cannabis-stocks/" target="_blank" rel="noopener">shouldn’t outright reject security clearance applicants</a>&nbsp;over past use and should also use discretion when it comes to those with cannabis investments in their stock portfolios.</p>
<p>Meanwhile, the U.S. Secret Service (USSS)&nbsp;<a href="https://www.marijuanamoment.net/u-s-secret-service-loosens-rules-on-past-marijuana-use-by-prospective-agents/" target="_blank" rel="noopener">recently updated its employment policy to be more accommodating</a>&nbsp;to applicants who’ve previously used marijuana, making it so candidates of any age become eligible one year after they last consumed cannabis. Previously, there were stricter age-based restrictions.</p>
<p>The federal Bureau of Alcohol, Tobacco, Firearms and Explosives (ATF) has also&nbsp;<a href="https://www.marijuanamoment.net/federal-law-enforcement-agency-says-state-legal-marijuana-activity-will-no-longer-automatically-disqualify-job-applicants/" target="_blank" rel="noopener">revised its cannabis rules for job applicants</a>. Applicants who’ve grown, manufactured or sold marijuana in compliance with state laws while serving in a “position of public responsibility” will no longer be automatically disqualified.</p>
<p>FBI updated its hiring policies in 2020 to make it so candidates are only automatically disqualified from joining the agency if they&nbsp;<a href="https://www.marijuanamoment.net/fbi-loosens-marijuana-employment-policy-for-would-be-agents/" target="_blank" rel="noopener">admit to having used marijuana within one year</a>&nbsp;of applying. Previously, prospective employees of the agency could not have used cannabis within the past three years.</p>

<p>Late last year, draft documents obtained by Marijuana Moment showed that the federal Office of Personnel Management (OPM) was proposing to replace a series of job application forms for prospective workers in a way that would&nbsp;<a href="https://www.marijuanamoment.net/new-federal-job-applications-wont-ask-about-most-marijuana-use-unless-it-was-within-the-past-90-days/" target="_blank" rel="noopener">treat past cannabis use much more leniently</a>&nbsp;than under current policy.</p>
<p>The Biden administration instituted a policy in 2021 authorizing&nbsp;<a href="https://www.marijuanamoment.net/marijuana-use-wont-automatically-block-people-from-federal-jobs-biden-administration-memo-says/" target="_blank" rel="noopener">waivers to be granted to certain workers</a>&nbsp;who admit to prior marijuana use, but certain lawmakers have pushed for additional reform.</p>
<p>A recent survey found that 30 percent of those between the ages of 18 and 30&nbsp;<a href="https://www.marijuanamoment.net/one-in-four-young-adults-say-federal-marijuana-employment-policies-prevent-them-from-applying-for-government-jobs-survey-finds/" target="_blank" rel="noopener">have either declined to apply or withdrawn applications</a>&nbsp;for federal jobs because of strict marijuana policies required for security clearances.</p>
<blockquote data-secret="llZrv76o5Z"><p><a href="https://www.marijuanamoment.net/minnesota-republicans-push-special-session-to-address-glaring-issues-in-marijuana-legalization-law-taking-effect-on-tuesday/">Minnesota Republicans Push Special Session To Address ‘Glaring Issues’ In Marijuana Legalization Law Taking Effect On Tuesday</a></p></blockquote>

<div><p>Marijuana Moment is made possible with support from readers. If you rely on our cannabis advocacy journalism to stay informed, please consider a monthly Patreon pledge.</p><p><a rel="nofollow" target="_blank" href="https://www.patreon.com/marijuanamoment?utm_content=post_button&amp;utm_medium=patron_button_and_widgets_plugin&amp;utm_campaign=749657&amp;utm_term=&amp;utm_source=https://www.marijuanamoment.net/senate-votes-to-let-people-whove-used-marijuana-work-at-intelligence-agencies-like-cia-and-nsa-as-part-of-defense-bill/" aria-label="Click to become a patron at Patreon!"><img data-lazy-fallback="1" src="https://www.marijuanamoment.net/wp-content/plugins/patron-plugin-pro/plugin/lib/patron-button-and-widgets-by-codebard/images/become_a_patron_button.png" alt="Become a patron at Patreon!" data-lazy-src="https://www.marijuanamoment.net/wp-content/plugins/patron-plugin-pro/plugin/lib/patron-button-and-widgets-by-codebard/images/become_a_patron_button.png?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></p></div> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Could the world go PFAS-free? Proposal to ban ‘forever chemicals’ fuels debate (291 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-023-02444-5</link>
            <guid>36958352</guid>
            <pubDate>Tue, 01 Aug 2023 16:33:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-023-02444-5">https://www.nature.com/articles/d41586-023-02444-5</a>, See on <a href="https://news.ycombinator.com/item?id=36958352">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>This February, the European Chemicals Agency (ECHA) in Helsinki published a <a href="https://echa.europa.eu/-/echa-publishes-pfas-restriction-proposal" data-track="click" data-label="https://echa.europa.eu/-/echa-publishes-pfas-restriction-proposal" data-track-category="body text link">proposal that could lead to the world’s largest-ever clampdown on chemicals production</a>. The plan, put forward by environmental agencies in five countries — Denmark, Germany, the Netherlands, Norway and Sweden — would heavily restrict the manufacture of more than 12,000 substances, collectively known as forever chemicals.</p><p>These chemicals, per- and poly-fluoroalkyl substances (PFASs), are all around us. They coat non-stick cookware, smartphone screens, weatherproof clothing and stain-resistant textiles. They are also used in microchips, jet engines, cars, batteries, medical devices and refrigeration systems (see ‘‘Forever chemicals’ in Europe’).</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-02444-5/d41586-023-02444-5_25868266.png?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-02444-5/d41586-023-02444-5_25868266.png?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="'Forever chemicals' in Europe: graphic that shows the amount of PFAS chemicals used in industry in the EU in 2020." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-02444-5/d41586-023-02444-5_25868266.png">
  <figcaption>
   <p><span>Source: ECHA</span></p>
  </figcaption>
 </picture>
</figure><p>PFASs are extraordinarily useful. Their fluorine-swaddled carbon chains let grease and water slide off textiles, and they protect industrial equipment from corrosion and heat damage. But their strong carbon–fluorine bonds cannot be broken apart by natural processes. So after PFASs escape from factories, homes and vehicles into the environment<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup>, they add to a <a href="https://www.nature.com/articles/d41586-019-00441-1" data-track="click" data-label="https://www.nature.com/articles/d41586-019-00441-1" data-track-category="body text link">forever-growing pollution problem</a>. The February proposal estimates that tens of thousands of tonnes of these chemicals escape annually in Europe alone.</p><p>Several PFASs are now known to be toxic. They have been linked to cancers and damage to immune systems, and are now banned under national and international laws. Most PFASs, however, have not yet undergone toxicology assessments or been linked to health harms. But officials at the agencies that submitted the plan to the ECHA say their persistence means they will inevitably build up until as-yet unknown safe thresholds are crossed.</p><p>“We see that there is an unacceptable risk now,” says Richard Luit, a policy adviser at the Dutch National Institute for Public Health and the Environment in Bilthoven.</p><p>There’s no prospect of an instant ban. The ECHA is consulting on the idea before it takes a position. European legislators are unlikely to have a plan to vote on before 2025, and even the current proposal offers grace periods — of more than a decade in some cases — to allow manufacturers to develop alternative materials or systems. Several permanent exemptions are also offered (including for fluorinated drugs, such as Prozac, and for materials used to calibrate scientific instruments).</p><p>But taken as a whole, the idea is to shrink PFAS use to a minimum. “We are asking society to make quite a shift,” says Luit. “We are asking to reverse all of it, go back to the drawing table and invent alternative solutions.”</p><p>Change is already under way for consumer use of PFASs. The notoriety of the toxic examples has pushed more than 100 companies and brands, including Apple, to pledge to phase out PFASs, even before it’s clear whether other materials can do the same job.</p><p>For industrial users, however, the idea of life without PFASs is a more shocking prospect. So February’s proposal has ignited debate about which uses of fluorinated chemicals the world could leave behind — and which must stay.</p><h2><b>Three forms of forever</b></h2><p>A peculiarity with fluorinated compounds, researchers say, is that some kill, whereas others are safe enough for use in medical products. “Fluorine compounds are really, really, incredibly strange in this regard,” says Mark McLinden, a chemical engineer at the US National Institute of Standards and Technology in Boulder, Colorado. “Certain fluorine compounds are incredibly toxic. And then you have things like [the gas] R134a, which is benign enough that you’re shooting it directly into your lungs in asthma inhalers”.</p><p>Forever chemicals come in three distinct forms (see ‘Fluorinated world’). The notoriously toxic kinds are fluorosurfactants. These molecules resemble those in soap, made of two parts: carbon chains with fluorine atoms wrapped around them, that repel everything, and a water-loving portion at one end of the chains that allows the molecules to dissolve in water.</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-02444-5/d41586-023-02444-5_25873988.png?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-02444-5/d41586-023-02444-5_25873988.png?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Fluorinated world: graphic that shows the structures of some of the 12,000 fluorinated PFAS structures used worldwide." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-02444-5/d41586-023-02444-5_25873988.png">
  <figcaption>
   
  </figcaption>
 </picture>
</figure><p>After some of these molecules were linked to serious health harms and widespread water pollution, individual substances were banned or severely restricted internationally: first PFOS (perfluorooctanesulfonic acid) in 2009, then PFOA (perfluorooctanoic acid) in 2019, and, last year, PFHxS (perfluorohexanesulfonic acid). Manufacturers have moved on to other fluorosurfactants, many of which lack toxicity studies.</p><p>The February proposal suggests phasing out all the fluorosurfactants at once to avoid “regrettable” substitutions, says Jona Schulze, a staff scientist at the German Environment Agency in Dessau-Roßlau.</p><p>But the proposal goes further than that. The five agencies behind it have adopted the Organisation for Economic Co-operation and Development’s definition of PFASs: any molecule with a carbon atom in a chain that’s bonded to two fluorine atoms (or, if at the end of the chain, three). Restrictions under this expansive definition cover the other two kinds of forever chemicals.</p><p>There are the fluoropolymers, the plastic-like form that most consumers encounter. The most famous example is Teflon, or polytetrafluoroethylene (PTFE), long carbon chains wrapped in fluorine atoms. A Teflon-based coating makes frying pans non-stick; in medical products, it helps catheters to glide through the body, safeguards implants from deterioration, and, coated on the inside of bottles and blister packs, prevents drugs from interacting with their glass or foil containers. Stain-resistant textiles use a variant of this structure, in which fluorine-wrapped side chains hang off a main carbon chain.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-023-00822-7" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-02444-5/d41586-023-02444-5_25857600.jpg"><p>How the US will remove ‘forever chemicals’ from its drinking water</p></a>
 </article><p>The third category of PFASs is made up of small, light fluorocarbon molecules that generally exist as gases or liquids. R134a, the asthma-inhaler propellant, is also a common refrigerant in refrigerators and mobile air-conditioning systems, for instance. Sensitive equipment that is prone to overheating, such as servers in a data centre, can be submerged in fluorocarbon fluids that cool the apparatus without shorting its circuits or running the risk of fire.</p><p>Although fluoropolymers and fluorocarbons haven’t been shown to harm consumers directly, the problems come when they’re produced and when their useful lives end. Fluoropolymers are created using toxic fluorosurfactants, which pollute water and soil around fluoropolymer plants worldwide. Some researchers also suspect that fluoropolymers might, during their long lifetimes, shed fragments small enough to be ingested, as is known to happen with microplastics (<a href="https://www.nature.com/articles/d41586-021-01143-3" data-track="click" data-label="https://www.nature.com/articles/d41586-021-01143-3" data-track-category="body text link"><i>Nature</i> <b>593</b>, 22–25; 2021</a>). As for the fluorocarbons, some are powerful greenhouse gases, and others break up into a small-molecule PFAS that is now accumulating in water.</p><p>“If no action is taken, at some point the societal costs due to continued use are likely to exceed the costs which are now associated with their restriction,” says Schulze.</p><h2><b>The electric-car conundrum</b></h2><p>To see all three forms of PFAS in one product, look no further than cars. Their air-conditioning systems use a fluorocarbon refrigerant, the hydraulic fluids usually contain fluorosurfactant additives that prevent corrosion, the painted chassis probably has a weatherproof fluoropolymer coating, and the seats are usually covered in a stain-resistant fluorinated textile.</p><p>Electric vehicles are even more reliant on fluoromaterials because of their lithium-ion batteries. These batteries get their high energy density, and therefore range, by operating at relatively high voltages, explains Gao Liu, a chemist at Lawrence Berkeley National Laboratory in Berkeley, California. The metallic content in their cathodes is usually a powder that must be bound together with a material that can withstand the high voltage. In the 1990s, that was PTFE; today, battery makers use a cheaper fluoropolymer called polyvinylidene fluoride (PVDF), containing half the fluorine.</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-02444-5/d41586-023-02444-5_25853630.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-02444-5/d41586-023-02444-5_25853630.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Employees work at a lithium-ion battery manufacturing plant in Huaibei, Anhui Province of China." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-02444-5/d41586-023-02444-5_25853630.jpg">
  <figcaption>
   <p><span>A lithium-battery manufacturing plant in Huaibei, China.</span><span>Credit: Li Xin/VCG via Getty</span></p>
  </figcaption>
 </picture>
</figure><p>Smaller fluorinated molecules have become crucial, too. Adding them to battery electrolytes allows a protective layer of lithium fluoride to form on the electrodes, improving performance and extending lifetime by preventing cracks, says Cheng Zhang, a chemist at the University of Queensland in Brisbane, Australia. This area has become a battleground for battery manufacturers, who are developing cocktails of fluorinated additives.</p><p>Liu has developed a fluorine-free binder, but it works only for a lower-voltage battery such as one based on lithium iron phosphate. These batteries do have advantages: they last longer and don’t use critical minerals such as cobalt, nickel or manganese, important factors to consider as battery production ramps up in the fight against climate change, Liu says. But even though lithium iron phosphate batteries would work for stationary storage and already power half of Chinese electric vehicles, they might not be cost-effective for long-range vehicles.</p><p>“The whole field needs to look into better chemistries,” says Liu. “The reason we switch to batteries is to protect the environment. It doesn’t make sense to invent something that’s dirtier than before.”</p><h2><b>The hydrogen economy</b></h2><p>The push for clean energy involves fluoromaterials on another front: building the hydrogen economy. Central to this effort are electrolysers that generate ‘green’ hydrogen by splitting water, powered by renewable electricity.</p><p>The fluctuations of wind and sun favour a type of electrolyser that uses a proton-exchange membrane system (PEM). Such systems can ramp up and down quickly, unlike an older, well-established electrolyser for splitting water. As the name suggests, PEMs involve membranes that control the movement of protons (that is, positively charged hydrogen ions) between electrodes. Fluorinated materials are favoured for the membrane because they can tolerate the acidic operating conditions.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-022-02247-0" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-02444-5/d41586-023-02444-5_23399136.png"><p>How to destroy ‘forever chemicals’: cheap method breaks down PFAS</p></a>
 </article><p>Seeking to enter green hydrogen production, the fluorochemicals manufacturer Chemours this January announced a US$200-million expansion in France to produce more of its fluorinated Nafion membrane. (Nafion is currently used for the valuable chlor-alkali process, which splits brine into chlorine and sodium hydroxide, products that in turn are used in half of all industrial chemical processes.)</p><p>But PFASs aren’t necessary for green hydrogen: an emerging alternative to PEMs involves systems that instead move negatively charged hydroxide ions across membranes in an alkaline environment, says Benjamin Britton, a chemist who co-founded the start-up Ionomr Innovations in Vancouver, Canada. Ionomr is among firms creating non-fluorinated membranes for such anion-exchange systems<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup>.</p><p>It could prove harder to replace Nafion in the chlor-alkali process, however: there, fluorinated membranes are better than other materials at withstanding corrosive chlorine attack. Still, some researchers are studying whether this process can work without membranes at all.</p><h2><b>The refrigeration battle</b></h2><p>By far the largest source of PFAS emissions comprises the light fluorocarbon gases. Their main application is as refrigerants. Although ammonia, an early refrigerant, is still used for industrial applications, it was fluorinated compounds, specifically chlorofluorocarbons (CFCs), that brought air conditioning and refrigeration to the masses. That’s because, unlike ammonia, they are not irritants and they are non-flammable, says McLinden.</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-02444-5/d41586-023-02444-5_25853626.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-02444-5/d41586-023-02444-5_25853626.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Congested air conditioning units on a building in Mumbai, India." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-02444-5/d41586-023-02444-5_25853626.jpg">
  <figcaption>
   <p><span>Air conditioning units in Mumbai, India.</span><span>Credit: Kuni Takahashi/Getty</span></p>
  </figcaption>
 </picture>
</figure><p>CFCs were phased out because they deplete atmospheric ozone, and were replaced by hydrofluorocarbons such as R134a. But these are greenhouse gases — and so there is an ongoing switch to hydrofluoroolefins (HFOs)<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup>. These contain a double bond between two carbon atoms, a link that’s susceptible to attack by atmospheric compounds, which helps these molecules to break apart in weeks.</p><p>Problem solved? Not exactly. Environmental scientists and officials are now advocating the phasing out of HFOs because those molecules break up in the atmosphere to form a PFAS called trifluoroacetic acid or TFA. Karsten Nödler, an analytical chemist at the German Water Centre in Karlsruhe, says that although TFA has not been linked to any health issues, its accumulation warrants concern because it is extraordinarily difficult to remove from water. Should the time come when a clean-up is required, the only option will be reverse osmosis, an expensive technique of last resort.</p><p>Other than ammonia, the fluorine-free refrigerant options are hydrocarbons, which are flammable, or carbon dioxide, which suffers efficiency losses, especially in hot weather when cooling is needed most, McLinden says. European refrigerators already use hydrocarbons, but these substances might pose too great a fire risk in large air-conditioning systems, for example. Air conditioners for small residences have become safe enough for hydrocarbons, argues Audun Heggelund, a senior adviser to the Norwegian Environmental Agency in Oslo. The February proposal gives the air-conditioning industry 12 years to switch to hydrocarbons, but it grants a permanent exemption where safety codes prohibit the use of flammable refrigerants.</p><p>McLinden suggests that a common-sense approach is to crack down on leaks. Refrigerants operate in a closed loop — in that if they leak, the device doesn’t work. So if manufacturers could assure no leaks, any refrigerant would be fine, he argues.</p><h2><b>Heavy industries</b></h2><p>The simplest but most pervasive uses of PFASs in machinery — from engines to chemical reactors — are at the interfaces between parts. Fluoropolymer greases lubricate moving surfaces, and fluoroelastomer O-rings, gaskets and seals join parts together. (Elastomers are polymers that regain their shape after being deformed.) Fluoromaterials are the only flexible ones that can resist aggressive chemical corrosion, very high temperatures and, in some applications, ultraviolet radiation, says Michael Eason, a materials engineer at James Walker, a company headquartered in Woking, UK, that manufactures high-performance sealing products. Fluoroelastomer seals are also usefully non-stick when equipment is disassembled for maintenance.</p><p>Fluoromaterials’ resistance to heat alone sets them apart from other soft materials: PTFE, for instance, can withstand a constant temperature of 260 °C for 10 years while losing only 1% of its mass, says Barbara Henry, a materials scientist at W. L. Gore, a materials-science company based in Newark, Delaware. This allows seals to last the lifetime of their equipment, for instance in an oil-well head, minimizing maintenance and therefore worker exposure to occupational hazards. It also allows machinery such as jet engines to operate at higher temperatures, and therefore more efficiently. “Because fluorinated polymers exist, every piece of equipment that’s followed a capitalist process, trying to get faster, quicker, more efficient, has adopted fluorinated materials,” says Eason.</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-02444-5/d41586-023-02444-5_25853632.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-02444-5/d41586-023-02444-5_25853632.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="An aerospace propulsion technician with hardhat and torch is inspecting the seals inside the barrel on an aircraft engine" loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-02444-5/d41586-023-02444-5_25853632.jpg">
  <figcaption>
   <p><span>A technician inspecting seals on an aircraft engine, which use PFASs.</span><span>Credit: Operation 2021/Alamy </span></p>
  </figcaption>
 </picture>
</figure><p>PTFE also protects workers in heavy industries. A thin internal layer of PTFE in multilayered textiles allows garments to remain light and breathable while providing enough heat resistance to withstand arc flashes, the explosive electrical discharges that can melt textiles on to skin. Gore has developed fluorine-free weatherproof outerwear for consumers (using expanded polyethylene), but high-performance gear still demands PTFE, says Henry.</p><p>Aware of the push to ban PFASs, however, Eason and Chaoying Wan, a materials scientist at the University of Warwick, UK, are starting a collaboration to find alternatives. A replacement that has all the properties of PTFE would be “almost impossible” to find, Eason says. But substitutes could emerge for applications where just one or two properties of PTFE are needed, although this would complicate supply chains. Eason expects that the outcome might be dozens of specialized products, whereas now a handful of fluoropolymers meet the needs of industries ranging from aerospace to pharmaceuticals to semiconductors.</p><h2><b>Computer chips</b></h2><p>Fluorochemical producers are also buoyed by the world’s race for semiconductor dominance. Last September, Chemours announced an expansion at its North Carolina facility to support domestic semiconductor production. And this year, Asahi Glass Company, a chemicals and glass manufacturer in Tokyo, also cited strong demand from the semiconductor industry when it announced a ¥35-billion ($250-million) expansion in fluorochemicals production.</p><p>PFASs are used in many ways to make computer chips. In one crucial step, manufacturers coat a silicon wafer’s surface with a ‘photoresist’ material containing PFASs: when the photoresist is illuminated, those PFASs generate strong acids that eat away at portions of the material, leaving a carefully patterned gap. In a second step, the exposed parts of the wafer are etched away — and in ‘dry etching’, a mixture of gases is used, usually containing some fluorocarbons. (Fluoropolymers are also used in a variety of microchip coatings.)</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-02444-5/d41586-023-02444-5_25853628.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-02444-5/d41586-023-02444-5_25853628.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Rows of integrated circuit boards pictured at the Smart Pioneer Electronics factory in Suzhou, China." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-02444-5/d41586-023-02444-5_25853628.jpg">
  <figcaption>
   <p><span>PFASs are used to help manufacture electronic components on microchips.</span><span>Credit: Qilai Shen/Bloomberg via Getty</span></p>
  </figcaption>
 </picture>
</figure><p>It is not easy to find alternatives to the strong acids or the etching gases. Fluorine atoms impart the necessary acidity, and fluorocarbon gases are prized for their precision in etching. The Semiconductor Research Corporation, a consortium based in Durham, North Carolina, is promoting research into ways to limit PFAS emissions and to find alternatives in the microchip industry.</p><p>In one case, companies have managed to ditch a small use of fluorosurfactants in ‘wet etching’ — processes that involve chemicals in solution. Here, fluorosurfactants helped the solutions to spread over the surfaces to be etched, says Christopher Christuk, president of electronic chemicals supplier Transene in Danvers, Massachusetts. Transene is now using fluorine-free surfactants that were identified by researchers at the University of Massachusetts Lowell (UML)<sup><a href="#ref-CR4" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">4</a></sup>. Key support for this switch came from the Massachusetts Toxics Use Reduction Institute, a state agency funded by fees levied on businesses that use toxic chemicals, which set up the partnership between Transgene and UML and funded the research project, Christuk says.</p><h2><b>The magic of fluorine: myth or fact?</b></h2><p>Industries that have known nothing but fluorine chemistry need to break away from believing in its magic, says Martin Scheringer, an environmental scientist at the Swiss Federal Institute of Technology in Zurich (ETHZ). “PFASs are a block to innovation,” he says, pointing to the example of firefighting foams. Despite making foams from PFOS for decades, the multinational technology company 3M managed to create fluorine-free firefighting foam in 2002, but only after PFOS became a high-profile pollutant. Many other industries now need to make similar breakthroughs. “We need lots of materials that have not been invented that are fluorine-free,” Scheringer says.</p><p>In December, 3M announced it would stop making all its fluorochemical products — including fluoropolymers and fluorocarbon gases and liquids — by 2025, but did not say what would take their place. This June, it reached a $10-billion settlement to pay to clean fluorosurfactants from drinking water in parts of the United States, although it faces other unresolved lawsuits.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-019-00441-1" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-02444-5/d41586-023-02444-5_19120822.jpg"><p>Tainted water: the scientists tracing thousands of fluorinated chemicals in our environment</p></a>
 </article><p>For the moment, most of the funding granted to PFAS topics relates to cleaning up pollution, and neither of the huge government-funded European Union or US programmes to boost clean energy or the manufacture of semiconductor chips specify the need to find alternatives to PFASs. “We should channel more of the funding to the research that will find new solutions,” says Jonatan Kleimark, an adviser at ChemSec, a non-profit organization based in Gothenburg, Sweden, that advocates for safer chemicals.</p><p>Eason and Wan are trying to find ways to manufacture fluoropolymers without using toxic fluorosurfactants. If that can be achieved, Eason argues, it should be fine to continue using fluoropolymers where they cannot be substituted, provided that recycling at the end of their life is also resolved. But Eason recognizes the problem of persistence with fluoropolymers. “The ECHA proposal has made everyone realize they have to do something different,” he says. “In my view, a responsible company should be looking to minimize the use of fluorinated materials.”</p><p>The officials who proposed the ban say that they welcome proposals from manufacturers to extend producer responsibility and develop closed-loop systems for recycling fluorochemicals. “They have to provide the information and step forward,” says Heggelund. But he is highly sceptical, noting the low rates of plastic recycling. And if fluoropolymers could be made without toxic surfactants, then manufacturers should have done it from the start instead of reacting to regulation, he says.</p><p>The ECHA is collecting feedback on the proposal until the end of September. After that, it will revise the plan and carry out a techno-economic assessment to evaluate the costs and benefits for society.</p><p>The agency is the only one in the world contemplating such comprehensive PFAS restrictions. But enacting a ban would send a signal to the rest of the world about the acceptability of the chemicals. Zhanyun Wang, an environmental scientist at ETHZ, thinks that the proposal will spur innovative research for applications that don’t have obvious alternatives to fluorinated chemicals. And for those that do, Wang hopes the proposal and market changes that follow could act as a “lighthouse”, as he puts it: showing industries around the world how to ditch forever chemicals for good.</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Speed isn’t everything, and slowness may in fact be more beneficial (2017) (101 pts)]]></title>
            <link>https://www.bps.org.uk/psychologist/slowness-essence-knowledge</link>
            <guid>36958315</guid>
            <pubDate>Tue, 01 Aug 2023 16:31:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bps.org.uk/psychologist/slowness-essence-knowledge">https://www.bps.org.uk/psychologist/slowness-essence-knowledge</a>, See on <a href="https://news.ycombinator.com/item?id=36958315">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Generally we associate speed with positive outcomes. Being ‘fast’ is perceived as good; being ‘slow’ is perceived as bad. Quiz shows demonstrate the benefits of fast thinking – speedy responses win prizes, while hesitation costs points. In most careers, including academia, speed is valued. But speed isn’t everything, and slowness may in fact be more beneficial to us in many circumstances. In our age of snap judgements and instant opinions, slowness and deliberative contemplation may be more important than we realise.</p><p>In his 2011 book <em>Thinking Fast and Slow</em>, Daniel Kahneman suggested that the human mind consists of two competing systems. His central hypothesis is a dichotomy between two modes of thought: ‘System 1’ is fast, instinctive and emotional. ‘System 2’ is slower, effortful, more deliberative, conscious, and more logical. To illustrate the two, imagine you see an angry face in a crowd – you will instantly focus on that individual because your brain perceives a threat and works quickly to identify it in order to keep you safe. This is an example of the ‘fast’ system. Our brains are hardwired to respond quickly to certain cues in the environment, and this helps us to survive.</p><p>In the battle of the popular science books, Malcolm Gladwell’s 2007 <em>Blink</em> exalts the virtue of ‘thinking without thinking’. His central idea is that spontaneous decisions are often as good as, or superior to, carefully planned and considered ones. Gladwell’s acclamation for snap judgements and first impressions has many parallels with Kahneman’s concept of ‘fast’ thinking. Yet there are many situations when our brains take longer to complete tasks – using Kahneman’s ‘slow’ system. For instance, complete this sum: 498 + 813. I bet you really had to think – that is your ‘slow’ system which allows you to process more tricky situations. And as I will demonstrate here, slowness is vital to many situations, and may also provide a hallmark of a healthy brain and mind.</p><p><strong>Slowness and food</strong><br>In 1986 there was uproar when a McDonald’s fast food restaurant was scheduled to open in the picturesque Piazza di Spagna in Rome. This was led by a man called Carlo Petrini, who subsequently inaugurated the ‘slow food’ movement. In contrast to fast food, diners were encouraged to source food from sustainable sources, but also to increase gastronomic pleasure by eating in a slow, relaxed way. Taste and pleasure of food are linguistically linked to slowness, such as in the phrase ‘to savour’ what you are eating. In keeping with the mantra of the ‘slow food’ movement, there is evidence that slower eating leads to greater satiety, and greater pleasure (Andrade et al., 2008). In contrast, fast eating can lead to obesity, and healthier individuals tend to engage in slower eating. Given the worldwide obesity epidemic, such findings may be significant.</p><p><strong>Slow brain processes – cognitive reappraisal</strong> <br>There are many brain processes that could be labelled ‘slow’ processes, but one that stands out is cognitive reappraisal. Reappraisal is defined as the regulation of inner states, primarily emotions, through modification of the original reaction. Emotions prompt rapid responses within us. Just like ‘fast’ thinking, emotions involve changes to multiple response systems: behavioural, experiential and physiological. An emotion generally has an identifiable impetus or trigger, either in the external environment or internally, such as a thought. The stimulus that prompts the response may have intrinsic affective properties, such as an aversive shock, or may have a learned emotional value.</p><p>If emotions prompt rapid responses, cognitive reappraisal is the ability to more slowly re-evaluate our initial reaction. This slower process allows us to regulate our own emotions and respond more appropriately to situations. For instance, imagine we are walking down the street and we pass a friend. We lift our arm and wave to them and say hello, but they simply carry on walking as if they don’t know us. Our first, fast, reaction may be to suppose that they deliberately ignored us, perhaps prompting emotions such as anger or sadness, which can spiral into a negative mood. However, if we are able to reappraise the situation more slowly, and come to a more balanced view, we may be able to avoid the negative emotional consequences. Perhaps he just didn’t see us? Perhaps he was having a bad day and didn’t feel like talking? Reappraisal allows us to focus on the facts, considering more balanced opinions and thereby regulating our emotions.</p><p>There is good evidence that the slow process of cognitive reappraisal can be good for us. Successful reappraisal lowers measures of negative emotions and is linked with adaptive long-term improvements in everyday functioning (Dillon &amp; Labar, 2005). Cognitive reappraisal may also be a crucial factor within cognitive behavioural therapy, or CBT. Its importance is demonstrated by the finding that reappraisal alone has been found to mediate the effects of individual CBT for social anxiety (Goldin et al., 2012). Crucially, individual differences in the ability to regulate one’s emotions using processes such as cognitive reappraisal might be related to both normal and pathological variations in wellbeing (Ochsner &amp; Gross, 2005). The role of cognitive reappraisal variations in mental illness is an interesting route still to follow.</p><p><strong>Slowness and mental illness</strong> <br>The distinction between fast and slow thinking has yet to be applied explicitly to the realm of psychiatry, yet many forms of psychopathology revolve around repeated failures to adaptively regulate our emotional responses. A deficit in ‘slow’ thinking processes may underlie these difficulties, and slowness may even be a hallmark of the ‘healthy’ brain (Kringelbach et al., 2015).</p><p>In psychosis, a disorder where people lose touch with reality, recent research has also pointed towards slowness and ‘slow’ thinking as a marker of recovery. In a 2015 study Philippa Garety and colleagues found that helping people with persecutory delusions to slow down their thinking, and be aware of ‘fast’ thoughts, reduced their levels of paranoia. Their patients stopped instantly jumping to conclusions in keeping with their persecutory beliefs, and were able to challenge them.</p><p>Slowness may here too be a route to recovery and could provide a cognitive index to help clinicians to work out when patients are getting better. Garety and colleagues also suggest that slowness should be a prime target for cognitive mechanisms of change in delusions. New therapies may greatly benefit from inducing ‘slow’ thinking in patients. As many psychiatric illnesses and substance-use disorders involve impulsivity and compulsivity, interventions that aim to induce slowness may become more widespread.</p><p><strong>Slow science</strong><br>At the heart of the slow science movement is a strong opposition to performance targets, and an emphasis instead upon slow, methodical processes and quality-driven research. Proponents such as Uta Frith argue that the current academic environment encourages scientists to strive for fame, promotions and tenure by propelling their results and reviews into print. However, they argue that the emphasis upon productivity is too aggressive, leading to mistakes being made and a lack of quality.</p><p>The pressure to publish facing many academics is said to drive down the quality of research. Daniel Sarewitz argues that large bodies of published scientific research are unreliable or of poor quality, citing a ‘compulsion’ to publish as a causal factor. Indeed, the mantra ‘publish or perish’ appears to have become a widespread marker of the academic lifestyle. There is even statistical evidence that many low-powered studies yield more statistically significant results, suggesting that the most ‘productive’ researchers may in fact be the least reliable (Lakens &amp; Evers, 2014).</p><p><strong>Slowness and the human brain</strong> <br>The virtues of slowness have also been explored within neuroscience, where slowness of thought has been found to be an important property of brain function. This work directly translates Kahneman’s fast and slow systems into tractable brain mechanisms and dynamics. For instance, neuroimaging studies have consistently demonstrated that the correlations in activity between brain regions evolve over time (Chang &amp; Glover, 2010). Time is therefore an important property of the brain’s dynamics; spatial patterns are formed, dissolved, and reformed over time. Both rapidity and slowness are intrinsic properties of the human brain. Research has further shown that the global phase synchrony of the time-series of brain activity evolves over a characteristic ultra-slow timescale (&lt; 0.01 Hz). The brain at rest has a steady temporal variation in the formation and dissolution of multiple communities of harmonised brain regions.</p><p>The neuroscience of slowness has been studied indirectly with regard to cognitive reappraisal, a typically slow cognitive process that occurs often as a response to initial ‘fast’ emotional reactivity. One meta-analysis of neuroimaging studies of cognitive reappraisal combined 48 fMRI studies involving the downregulation of negative emotion in a slow manner (Buhle et al., 2014). They found that reappraisal appeared to occur by prefrontal and parietal regions exerting changes in lateral temporal areas associated with semantic and perceptual representations. By actively altering the mnemonic representation of the event, the individual is able to alter the emotional significance of the event. For example, someone who has seen the horrific aftermath of a motor vehicle accident may be able to tell themselves ‘that’s not blood, it’s just ketchup’, therefore changing the valence of the triggering mental image. Looking back at the results of Garety’s team, with reference to patients experiencing delusions, it would be interesting to study the neural dynamics underlying their initial, fast, paranoid thoughts, and their later, slow, reasoning.</p><p><strong>The integration of slow and fast thinking</strong><br>My own research into parenting has benefited greatly from considering how slow and fast thinking operate together to achieve optimality in both brain and behaviour.</p><p>We know that baby cues, such as a cute baby face or a piercing distress cry, operate to attract adults’ attention rapidly (Kringelbach et al., 2008). This is a key example of the ‘fast’ system, where an environmental stimulus related to a helpless infant prompts a rapid orienting to the baby. This fast response is even substantiated in the brain, with neural activity at 140 ms, too fast for conscious appraisal, in the reward-related region of the orbitofrontal cortex. Interestingly, we find that if the typical ‘cute’ infant face is disturbed with a craniofacial deformity such as cleft lip, this burst of orbitofrontal activity is absent (Parsons et al., 2013).</p><p>Whereas in cognitive reappraisal, the initial ‘fast’ response may need altering, in the context of infants our fast response is beautifully timed to coordinate the following slowly mediated caregiving behaviour. The fast burst of activity in response to a cute infant face or a distress cry may bias the adult’s attentional resources to prompt action immediately, thereby securing the survival of the baby by making it the prime focus of the caregiver. At this point, however, slow processes take over and are fundamental to the flourishing parent–infant relationship. These slow processes involve mentalisation – the ability to treat an infant as an independent psychological agent and guess their needs and desires – and emotional scaffolding – the appropriate regulation of infant emotions. It is these appraisal behaviours that require slower processing but provide the much-needed developmental support for the infant. Such processes are also substantiated in the brain, spanning a network of regions involving capacities such as emotion, pleasure and social interaction. Becoming a parent can at first be daunting, but the interplay between slow and fast processes in parenting demonstrates how we are well equipped for the role (Kringelbach et al., 2016).</p><p><strong>Conclusion</strong><br>As the ‘slow movement’ advocates a cultural shift towards slowing down life’s pace, so does the evidence. Slow processes, be it eating, cognitive reappraisal or slow thinking in the context of psychiatric disorder, are beneficial to us. Slowness may even be an index of recovery in mental health.</p><p>Speed is evidently important in many contexts. Quick reactions and instinctive responsiveness aid survival. But we also have a subsequent ‘slow’ response, which is conscious and deliberative, and may be beneficial for more complex social interactions and moral emotions. Perhaps ‘fast’ and ‘slow’ thinking are really two sides of the same coin – intrinsically related, but with their own independent virtues. In our fast-moving society that frequently prioritises speed, the importance of slowness should not be forgotten.</p><div><p>How can we apply slowness to our own lives? Well, the ‘slow science’ movement encourages scientists to halt multitasking in favour of slow, steady methodical processes. It calls for increased time to think and muse about the scientific questions we pursue. The manifesto says that society should give scientists the time they need, but more importantly, scientists must take their time.</p><p><strong>References &nbsp; &nbsp;&nbsp;</strong></p></div><p>Andrade, A.M., Greene, G.W. &amp; Melanson, K.J. (2008). Eating slowly led to decreases in energy intake within meals in healthy women. Journal of the Academy of Nutrition and Dietetics, 108(7), 1186–1191.</p><p>Buhle, J.T., Silvers, J.A., Wager, T.D. et al. (2014). Cognitive reappraisal of emotion. Cerebral Cortex, 24(11), 2981–2990.</p><p>Chang, C. &amp; Glover, G.H. (2010). Time-frequency dynamics of resting-state brain connectivity measured with fMRI. Neuroimage, 50, 81–98.</p><p>Dillon, D.G. &amp; Labar, K.S. (2005). Startle-modulation during conscious emotion regulation is arousal-dependent. Behavioural Neuroscience, 119, 1118–1124.</p><p>Garety, P.,&nbsp;Waller, H.,&nbsp;Emsley, R.&nbsp;et al. (2015). Cognitive mechanisms of change in delusions.&nbsp;Schizophrenia Bulletin. 41, 400–410.</p><p>Goldin, P., Ziv, M., Jazaieri, H. et al. (2012). Cognitive reappraisal self-efficacy mediates the effects of individual cognitive-behavioral therapy for social anxiety disorder. Journal of Consulting and Clinical Psychology, 80(6), 1034–1040.</p><p>Kahneman, D. (2011).&nbsp;Thinking, fast and slow. Macmillan.</p><p>Kringelbach, M.L., Lehtonen, A., Squire, S. et al. (2008). A specific and rapid neural signature for parental instinct. PLoS ONE 3(2), e1664. doi:10.1371/journal.pone.0001664</p><p>Kringelbach, M.L., McIntosh, A.R., Ritter, P. et al. (2015). The rediscovery of slowness: Exploring the timing of cognition. Trends in Cognitive Sciences, 19(10): 616–628.</p><p>Kringelbach, M.L., Stark, E.A., Alexander, C. et al. (2016). On cuteness: Unlocking the parental brain and beyond. Trends in Cognitive Sciences, 20(7): 545–558.</p><p>Lakens, D. &amp; Evers, E.R. (2014). Sailing from the seas of chaos into the corridor of stability: Practical recommendations to increase the informational value of studies. Perspectives on Psychological Science, 9(3): 278–292.</p><p>Ochsner, K.N. &amp; Gross, J.J. (2005). The cognitive control of emotion. Trends in Cognitive Sciences, 9, 242–249.</p><p>Parsons, C.E., Young, K.S., Mohseni, H. et al. (2013). Minor structural abnormalities in the infant face disrupt neural processing: A unique window into early caregiving responses. Social Neuroscience, 8, 268–274.</p><p>Sarewitz, D. (2016). The pressure to publish pushes down quality. Nature, 533, 147.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: PromptTools – open-source tools for evaluating LLMs and vector DBs (180 pts)]]></title>
            <link>https://github.com/hegelai/prompttools</link>
            <guid>36958175</guid>
            <pubDate>Tue, 01 Aug 2023 16:23:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/hegelai/prompttools">https://github.com/hegelai/prompttools</a>, See on <a href="https://news.ycombinator.com/item?id=36958175">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto">
  <a href="https://hegel-ai.com/" rel="nofollow"><img src="https://camo.githubusercontent.com/9dc94ea8ed74836421b7a68dc3cc5c2018a3813e4959ac674464af2dbc3ebfe4/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f352f35312f4f776c5f6f665f4d696e657276612e737667" width="75" height="75" data-canonical-src="https://upload.wikimedia.org/wikipedia/commons/5/51/Owl_of_Minerva.svg"></a>
</p>
<h2 tabindex="-1" dir="auto">
 PromptTools
</h2>
<p dir="auto">
<g-emoji alias="wrench" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f527.png">🔧</g-emoji> Test and experiment with prompts, LLMs, and vector databases. <g-emoji alias="hammer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f528.png">🔨</g-emoji>
</p><p dir="auto">
  <a href="http://prompttools.readthedocs.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/e2c8bc70e24645b11a871385f3cc0c8d9e7390cab4af72ad58b8b5dac3a3e0d8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f56696577253230446f63756d656e746174696f6e2d446f63732d79656c6c6f77" data-canonical-src="https://img.shields.io/badge/View%20Documentation-Docs-yellow"></a>
  <a href="https://discord.gg/7KeRPNHGdJ" rel="nofollow"><img src="https://camo.githubusercontent.com/187c113eab90c77afead6d322dde08290442239c20e4cbe16c75a8ba9e98819a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4a6f696e2532306f7572253230636f6d6d756e6974792d446973636f72642d626c7565" data-canonical-src="https://img.shields.io/badge/Join%20our%20community-Discord-blue"></a>
  <a href="https://pepy.tech/project/prompttools" rel="nofollow"><img src="https://camo.githubusercontent.com/297e58b7179777ab9ccf0704f88d49f13e84b6e806780214b9a510555ea0f7a9/68747470733a2f2f706570792e746563682f62616467652f70726f6d7074746f6f6c73" alt="Total Downloads" data-canonical-src="https://pepy.tech/badge/prompttools"></a>
  <a href="https://github.com/hegelai/prompttools">
      <img src="https://camo.githubusercontent.com/669d6fbed56f268f157513ab63dd8d4e78f1a3ec4862083441e69171c8274d80/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f686567656c61692f70726f6d7074746f6f6c73" data-canonical-src="https://img.shields.io/github/stars/hegelai/prompttools">
  </a>
  <a href="https://twitter.com/hegel_ai" rel="nofollow"><img src="https://camo.githubusercontent.com/49ce7954806af1dc959aab9fec9a44be9c20ef64ba5644f7e1280410587f7d0a/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f486567656c5f41493f7374796c653d736f6369616c" data-canonical-src="https://img.shields.io/twitter/follow/Hegel_AI?style=social"></a>
</p>
<p dir="auto">Welcome to <code>prompttools</code> created by <a href="https://hegel-ai.com/" rel="nofollow">Hegel AI</a>! This repo offers a set of free, open-source tools for testing and experimenting with prompts. The core idea is to enable developers to evaluate prompts using familiar interfaces like <em>code</em> and <em>notebooks</em>.</p>
<p dir="auto">In just a few lines of codes, you can test your prompts and parameters across different models (whether you are using
OpenAI, Anthropic, or LLaMA models). You can even evaluate the retrieval accuracy of vector databases.</p>
<div dir="auto" data-snippet-clipboard-copy-content="prompts = [&quot;Tell me a joke.&quot;, &quot;Is 17077 a prime number?&quot;]
models = [&quot;gpt-3.5-turbo&quot;, &quot;gpt-4&quot;]
temperatures = [0.0]
openai_experiment = OpenAIChatExperiment(models, prompts, temperature=temperatures)
openai_experiment.run()
openai_experiment.visualize()"><pre><span>prompts</span> <span>=</span> [<span>"Tell me a joke."</span>, <span>"Is 17077 a prime number?"</span>]
<span>models</span> <span>=</span> [<span>"gpt-3.5-turbo"</span>, <span>"gpt-4"</span>]
<span>temperatures</span> <span>=</span> [<span>0.0</span>]
<span>openai_experiment</span> <span>=</span> <span>OpenAIChatExperiment</span>(<span>models</span>, <span>prompts</span>, <span>temperature</span><span>=</span><span>temperatures</span>)
<span>openai_experiment</span>.<span>run</span>()
<span>openai_experiment</span>.<span>visualize</span>()</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/hegelai/prompttools/blob/main/img/demo.gif"><img src="https://github.com/hegelai/prompttools/raw/main/img/demo.gif" alt="image" data-animated-image=""></a></p>
<p dir="auto">To stay in touch with us about issues and future updates, join the <a href="https://discord.gg/7KeRPNHGdJ" rel="nofollow">Discord</a>.</p>
<h2 tabindex="-1" dir="auto">Quickstart</h2>
<p dir="auto">To install <code>prompttools</code>, you can use <code>pip</code>:</p>

<p dir="auto">You can run a simple example of a <code>prompttools</code> locally with the following</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/hegelai/prompttools.git
cd prompttools &amp;&amp; jupyter notebook examples/notebooks/OpenAIChatExperiment.ipynb"><pre><code>git clone https://github.com/hegelai/prompttools.git
cd prompttools &amp;&amp; jupyter notebook examples/notebooks/OpenAIChatExperiment.ipynb
</code></pre></div>
<p dir="auto">You can also run the notebook in <a href="https://colab.research.google.com/drive/1YVcpBew8EqbhXFN8P5NaFrOIqc1FKWeS?usp=sharing" rel="nofollow">Google Colab</a></p>
<h2 tabindex="-1" dir="auto">Playground</h2>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/hegelai/prompttools/blob/main/img/playground.gif"><img src="https://github.com/hegelai/prompttools/raw/main/img/playground.gif" width="1000" height="500" data-animated-image=""></a>
</p>
<p dir="auto">If you want to interact with <code>prompttools</code> using our playground interface, you can launch it with the following commands.</p>
<p dir="auto">First, install prompttools:</p>

<p dir="auto">Then, clone the git repo and launch the streamlit app:</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/hegelai/prompttools.git
cd prompttools &amp;&amp; streamlit run prompttools/playground/playground.py"><pre><code>git clone https://github.com/hegelai/prompttools.git
cd prompttools &amp;&amp; streamlit run prompttools/playground/playground.py
</code></pre></div>
<h2 tabindex="-1" dir="auto">Documentation</h2>
<p dir="auto">Our <a href="https://prompttools.readthedocs.io/en/latest/index.html" rel="nofollow">documentation website</a> contains the full API reference
and more description of individual components. Check it out!</p>
<h2 tabindex="-1" dir="auto">Supported Integrations</h2>
<p dir="auto">Here is a list of APIs that we support with our experiments:</p>
<p dir="auto">LLMs</p>
<ul dir="auto">
<li>OpenAI (Completion, ChatCompletion) - <strong>Supported</strong></li>
<li>LLaMA.Cpp (LLaMA 1, LLaMA 2) - <strong>Supported</strong></li>
<li>HuggingFace (Hub API, Inference Endpoints) - <strong>Supported</strong></li>
<li>Anthropic - <strong>Supported</strong></li>
<li>Google PaLM - <strong>Supported</strong></li>
<li>LangChain - Exploratory</li>
</ul>
<p dir="auto">Vector Databases and Data Utility</p>
<ul dir="auto">
<li>Chroma - <strong>Supported</strong></li>
<li>Weaviate - <strong>Supported</strong></li>
<li>MindsDB - <strong>Supported</strong></li>
<li>Milvus - Exploratory</li>
<li>Pinecone - Exploratory</li>
<li>LanceDB - Exploratory</li>
<li>LlamaIndex - Exploratory</li>
</ul>
<p dir="auto">If you have any API that you'd like to see being supported soon, please open an issue or
a PR to add it. Feel free to discuss in our Discord channel as well.</p>
<h2 tabindex="-1" dir="auto">Frequently Asked Questions (FAQs)</h2>
<ol dir="auto">
<li>
<p dir="auto">Will this library forward my LLM calls to a server before sending it to OpenAI, Anthropic, and etc.?</p>
<ul dir="auto">
<li>No, the source code will be executed on your machine. Any call to LLM APIs will be directly executed from your machine without any forwarding.</li>
</ul>
</li>
<li>
<p dir="auto">Does <code>prompttools</code> store my API keys or LLM inputs and outputs to a server?</p>
<ul dir="auto">
<li>No, all data stay on your local machine. No metrics, telemetry, or usage data are collected. As a result,
we would love to hear direct feedback from you. Please open an issue or join our Discord.</li>
</ul>
</li>
<li>
<p dir="auto">How do I persist my results?</p>
<ul dir="auto">
<li>To persist the results of your tests and experiments, you can export your <code>Experiment</code> with the methods <code>to_csv</code>,
<code>to_json</code>, <code>to_lora_json</code>, or <code>to_mongo_db</code>. We are building more persistence features and we will be happy to further discuss your use cases, pain points, and what export
options may be useful for you.</li>
</ul>
</li>
</ol>
<h2 tabindex="-1" dir="auto">Contributing</h2>
<p dir="auto">We welcome PRs and suggestions! Don't hesitate to open a PR/issue or to reach out to us <a href="mailto:team@hegel-ai.com">via email</a>.
Please have a look at our <a href="https://github.com/hegelai/prompttools/blob/main/CONTRIBUTING.md">contribution guide</a> and
<a href="https://github.com/hegelai/prompttools/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22">"Help Wanted" issues</a> to get started!</p>
<h2 tabindex="-1" dir="auto">Usage and Feedback</h2>
<p dir="auto">We will be delighted to work with early adopters to shape our designs. Please reach out to us <a href="mailto:team@hegel-ai.com">via email</a> if you're
interested in using this tooling for your project or have any feedback.</p>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto">We will be gradually releasing more components to the open-source community. The current license can be found in the  <a href="https://github.com/hegelai/prompttools/blob/main/LICENSE">LICENSE</a> file. If there is any concern, please <a href="mailto:eam@hegel-ai.com">contact us</a> and we will be happy to work with you.</p>
</article>
          </div></div>]]></description>
        </item>
    </channel>
</rss>