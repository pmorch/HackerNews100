<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 27 Aug 2024 16:30:14 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[New 0-Day Attacks Linked to China's 'Volt Typhoon' (105 pts)]]></title>
            <link>https://krebsonsecurity.com/2024/08/new-0-day-attacks-linked-to-chinas-volt-typhoon/</link>
            <guid>41367964</guid>
            <pubDate>Tue, 27 Aug 2024 14:31:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://krebsonsecurity.com/2024/08/new-0-day-attacks-linked-to-chinas-volt-typhoon/">https://krebsonsecurity.com/2024/08/new-0-day-attacks-linked-to-chinas-volt-typhoon/</a>, See on <a href="https://news.ycombinator.com/item?id=41367964">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
												<p>Malicious hackers are exploiting a zero-day vulnerability in <strong>Versa Director</strong>, a software product used by many Internet and IT service providers. Researchers believe the activity is linked to <strong>Volt Typhoon</strong>, a Chinese cyber espionage group focused on infiltrating critical U.S. networks and laying the groundwork for the ability to disrupt communications between the United States and Asia during any future armed conflict with China.</p>
<div id="attachment_68507"><p><img aria-describedby="caption-attachment-68507" decoding="async" src="https://krebsonsecurity.com/wp-content/uploads/2024/08/ss-dissolvingsphere.png" alt="" width="840" height="508" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/08/ss-dissolvingsphere.png 840w, https://krebsonsecurity.com/wp-content/uploads/2024/08/ss-dissolvingsphere-768x464.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/08/ss-dissolvingsphere-782x473.png 782w" sizes="(max-width: 840px) 100vw, 840px"></p><p id="caption-attachment-68507">Image: Shutterstock.com</p></div>
<p>Versa Director systems are primarily used by Internet service providers (ISPs), as well as managed service providers (MSPs) that cater to the IT needs of many small to mid-sized businesses simultaneously. In <a href="https://versa-networks.com/blog/versa-security-bulletin-update-on-cve-2024-39717-versa-director-dangerous-file-type-upload-vulnerability/" target="_blank" rel="noopener">a security advisory</a> published Aug. 26, Versa urged customers to deploy a patch for the vulnerability (<a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2024-39717" target="_blank" rel="noopener">CVE-2024-39717</a>), which the company said is fixed in <em>Versa Director 22.1.4</em> or later.</p>
<p>Versa said the weakness allows attackers to upload a file of their choosing to vulnerable systems. The advisory placed much of the blame on Versa customers who “failed to implement system hardening and firewall guidelines…leaving a management port exposed on the internet that provided the threat actors with initial access.”</p>
<p>Versa’s advisory doesn’t say how it learned of the zero-day flaw, but its vulnerability listing at mitre.org acknowledges “there are reports of others based on backbone telemetry observations of a 3rd party provider, however these are unconfirmed to date.”</p>
<p>Those third-party reports came in late June 2024 from <strong>Michael Horka</strong>, senior lead information security engineer at <strong>Black Lotus Labs</strong>, the security research arm of <strong>Lumen Technologies</strong>, which operates one of the global Internet’s largest backbones.</p>
<p>In an interview with KrebsOnSecurity, Horka said Black Lotus Labs identified a web-based backdoor on Versa Director systems belonging to four U.S. victims and one non-U.S. victim in the ISP and MSP sectors, with the earliest known exploit activity occurring at a U.S. ISP on June 12, 2024.</p>
<p>“This makes Versa Director a lucrative target for advanced persistent threat (APT) actors who would want to view or control network infrastructure at scale, or pivot into additional (or downstream) networks of interest,” Horka <a href="https://blog.lumen.com/taking-the-crossroads-the-versa-director-zero-day-exploitation/" target="_blank" rel="noopener">wrote</a> in a blog post published today.</p>
<p>Black Lotus Labs said it assessed with “medium” confidence that Volt Typhoon was responsible for the compromises, noting the intrusions bear the hallmarks of the Chinese state-sponsored espionage group — including zero-day attacks targeting IT infrastructure providers, and Java-based backdoors that run in memory only.</p>
<p>In May 2023, the <strong>National Security Agency</strong> (NSA), the <strong>Federal Bureau of Investigation</strong> (FBI), and the <strong>Cybersecurity Infrastructure Security Agency</strong> (CISA) issued <a href="https://media.defense.gov/2023/May/24/2003229517/-1/-1/0/CSA_Living_off_the_Land.PDF" target="_blank" rel="noopener">a joint warning</a> (PDF) about Volt Typhoon, also known as “<strong>Bronze Silhouette</strong>” and “<strong>Insidious Taurus</strong>,” which described how the group uses small office/home office (SOHO) network devices to hide their activity.</p>
<p>In early December 2023, Black Lotus Labs <a href="https://blog.lumen.com/routers-roasting-on-an-open-firewall-the-kv-botnet-investigation/" target="_blank" rel="noopener">published its findings</a> on “<strong>KV-botnet</strong>,” thousands of compromised SOHO routers that were chained together to form a covert data transfer network supporting various Chinese state-sponsored hacking groups, including Volt Typhoon.</p>
<p>In January 2024, the <strong>U.S. Department of Justice</strong> disclosed the FBI had <a href="https://www.justice.gov/opa/pr/us-government-disrupts-botnet-peoples-republic-china-used-conceal-hacking-critical" target="_blank" rel="noopener">executed a court-authorized takedown</a> of the KV-botnet shortly before Black Lotus Labs released its December report.<span id="more-68493"></span></p>
<p>In February 2024, CISA again joined the FBI and NSA in <a href="https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-038a" target="_blank" rel="noopener">warning</a> Volt Typhoon had compromised the IT environments of multiple critical infrastructure organizations — primarily in communications, energy, transportation systems, and water and wastewater sectors — in the continental and non-continental United States and its territories, including Guam.</p>
<p>“Volt Typhoon’s choice of targets and pattern of behavior is not consistent with traditional cyber espionage or intelligence gathering operations, and the U.S. authoring agencies assess with high confidence that Volt Typhoon actors are pre-positioning themselves on IT networks to enable lateral movement to OT [operational technology] assets to disrupt functions,” that alert warned.</p>
<p>In a speech at Vanderbilt University in April, FBI Director <strong>Christopher Wray</strong> <a href="https://www.reuters.com/technology/cybersecurity/fbi-says-chinese-hackers-preparing-attack-us-infrastructure-2024-04-18/" target="_blank" rel="noopener">said</a> China is developing the “ability to physically wreak havoc on our critical infrastructure at a time of its choosing,” and that China’s plan is to “land blows against civilian infrastructure to try to induce panic.”</p>
<p><strong>Ryan English</strong>, an information security engineer at Lumen, said it’s disappointing his employer didn’t at least garner an honorable mention in Versa’s security advisory. But he said he’s glad there are now a lot fewer Versa systems exposed to this attack.</p>
<p>“Lumen has for the last nine weeks been very intimate with their leadership with the goal in mind of helping them mitigate this,” English said. “We’ve given them everything we could along the way, so it kind of sucks being referenced just as a third party.”</p>
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zuckerberg claims regret on caving to White House pressure on content (134 pts)]]></title>
            <link>https://www.politico.com/news/2024/08/26/zuckerberg-meta-white-house-pressure-00176399</link>
            <guid>41365868</guid>
            <pubDate>Tue, 27 Aug 2024 09:50:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.politico.com/news/2024/08/26/zuckerberg-meta-white-house-pressure-00176399">https://www.politico.com/news/2024/08/26/zuckerberg-meta-white-house-pressure-00176399</a>, See on <a href="https://news.ycombinator.com/item?id=41365868">Hacker News</a></p>
Couldn't get https://www.politico.com/news/2024/08/26/zuckerberg-meta-white-house-pressure-00176399: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Tinybox of tinygrad by George Hotz is finally entering production (142 pts)]]></title>
            <link>https://twitter.com/realgeorgehotz/status/1828197925874463166</link>
            <guid>41365637</guid>
            <pubDate>Tue, 27 Aug 2024 08:54:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/realgeorgehotz/status/1828197925874463166">https://twitter.com/realgeorgehotz/status/1828197925874463166</a>, See on <a href="https://news.ycombinator.com/item?id=41365637">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Notes on Buttondown.com (116 pts)]]></title>
            <link>https://jmduke.com/posts/microblog/buttondown-dot-com/</link>
            <guid>41364783</guid>
            <pubDate>Tue, 27 Aug 2024 05:25:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jmduke.com/posts/microblog/buttondown-dot-com/">https://jmduke.com/posts/microblog/buttondown-dot-com/</a>, See on <a href="https://news.ycombinator.com/item?id=41364783">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <main>
                <p>We spent $85,000 for <code>buttondown.com</code> in April; this was the biggest capital expenditure I've ever made, and though it was coming from cash flow generated by Buttondown rather than my own checking account it was by rough estimation the largest non-house purchase I've ever made.</p>
<p>As of August, we're officially migrated over from <code>buttondown.email</code> to <code>buttondown.com</code>. I'm sure I'll do a more corporate blog post on the transition in the future, but for now I want to jot down some process notes:</p>
<ul>
<li>The entire process was made much more painful due to Buttondown's architecture, which is a hybrid of Vercel/Next (for the marketing site and docs site) and Django/Heroku (for the core app) managed by a HAProxy load balancer to route requests. We ended up using <a href="https://hurl.dev/">hurl</a> as a test harness around HAProxy, something we probably should have done three years ago.</li>
<li>I went in expecting SEO traffic to be hit as Google renegotiates legions of canonical URLs; it hasn't, at least thus far. Instead, everything seems to have just <em>bumped</em> fairly healthily.</li>
<li>I expected <em>more</em> production issues to come up than actually did. I credit this to a fairly clear scope: the goal was "to migrate all web traffic to .com", which meant that a) we didn't need to re-map any paths and b) we didn't need to worry about mapping SMTP traffic (which still runs through <code>buttondown.email</code>).</li>
<li>The hardest part of the process was the stuff you can't grep for. URLs on other sites, OAuth redirect URLs, that sort of thing.</li>
<li>Starting with isolated domains (the documentation site, the demo site) that weren't tied to the aforementioned HAProxy load balancer gave me some good early confidence that the migration would be smooth.</li>
</ul>
<p>Overall: very happy with how it turned out. I would describe the project roughly as "three months of fretting/planning, one week of grepping, and one week of fallout."</p>
<p>Was it worth it? Yes, I think so. Most theoretical capital expenditures Buttondown can make right now have a non-trivial ongoing cost associated with them (buy another newsletter company or content vertical and now you have to run it on a day-to-day basis; do a big marketing build-out and you have to manage it; etc.) — this was a sharp but fixed cost, and it's something that I knew I wanted to do in the fullness of time. (And, most importantly, people stop referring to Buttondown as "Buttondown Email", a personal pet peeve of mine.)</p>

            </main>
        </div><p>
  © 2024 Justin Duke · All rights reserved · have a nice day.
</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sovereign Tech Fund to Invest €686k in FreeBSD Infrastructure Modernization (154 pts)]]></title>
            <link>https://freebsdfoundation.org/blog/sovereign-tech-fund-to-invest-e686400-in-freebsd-infrastructure-modernization/</link>
            <guid>41364776</guid>
            <pubDate>Tue, 27 Aug 2024 05:23:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://freebsdfoundation.org/blog/sovereign-tech-fund-to-invest-e686400-in-freebsd-infrastructure-modernization/">https://freebsdfoundation.org/blog/sovereign-tech-fund-to-invest-e686400-in-freebsd-infrastructure-modernization/</a>, See on <a href="https://news.ycombinator.com/item?id=41364776">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
              
<article id="post-16252">
  <!-- .post-header -->

  
  <div>
    <h5>August 26, 2024</h5><section>
<p><strong>Investment to accelerate zero trust builds, SBOM, security tooling, and developer experience</strong></p>
</section>

<section>
<p><strong>Boulder, CO – August 26, 2024—</strong>The FreeBSD Foundation, dedicated to advancing the open source FreeBSD operating system and supporting the community, announced that Germany’s Sovereign Tech Fund (STF) has agreed to invest €686,400 in the FreeBSD project to drive improvements in infrastructure, security, regulatory compliance, and developer experience.</p>
</section>

<section>
<p>The work, organized and managed by the FreeBSD Foundation, will begin in August 2024 and continue through 2025. It will focus on five key projects:</p>
</section>

<section>
<ul><section>
<li><strong>Zero Trust Builds</strong>: Enhance tooling and processes</li>
</section>

<section>
<li><strong>CI/CD Automation</strong>: Streamline software delivery and operations</li>
</section>

<section>
<li><strong>Reduce Technical Debt</strong>: Implement tools and processes to keep technical debt low</li>
</section>

<section>
<li><strong>Security Controls</strong>: Modernize and extend security artifacts, including the FreeBSD Ports and Package Collection, to assist with regulatory compliance</li>
</section>

<section>
<li><strong>SBOM Improvements</strong>: Enhance and implement new tooling and processes for FreeBSD SBOM</li>
</section></ul>
</section>

<section>
<p>Developers are the lifeblood of every open source project. The Sovereign Tech Fund’s investment in FreeBSD infrastructure will ensure a world-class developer experience while preserving and extending the security and digital sovereignty for which FreeBSD is renowned.&nbsp;</p>
</section>

<section>
<p>The work commissioned by STF also aligns closely with the recent <a href="https://www.whitehouse.gov/oncd/briefing-room/2024/08/09/fact-sheet-biden-harris-administration-releases-end-of-year-report-on-open-source-software-security-initiative-2/">August 9, 2024 summary report</a> released by the <a href="https://www.whitehouse.gov/oncd/">U.S. Office of the National Cyber Director</a> (ONCD), consolidating feedback from the 2023 request for information on key priorities for securing the open source software ecosystem. By enhancing security controls and SBOM tooling, the FreeBSD Foundation is helping to keep FreeBSD at the forefront of improved vulnerability disclosure mechanisms and secure software foundations.&nbsp;</p>
</section>

<section>
<p>“The Sovereign Tech Fund is pleased to support the FreeBSD project,” said Fiona Krakenbürger, co-founder of STF. “This investment in critical digital infrastructure will accelerate modernization of FreeBSD, enhance security hygiene, and improve developer experiences. The widespread prevalence of FreeBSD means that these improvements will have a far-reaching impact on the global public sector and the research sector, as well as commercial users. We are excited to contribute to its continued modernization in a way that best serves the public interest as well as the FreeBSD community.”</p>
</section>

<section>
<p>“We are deeply grateful for this significant investment from the Sovereign Tech Fund, which will further enhance security and infrastructure for FreeBSD developers and users,” said Deb Goodkin, Executive Director of the FreeBSD Foundation. “As it has for thirty years, the FreeBSD project is again positioning itself at the vanguard of open source security, resilience, and reliability. The world’s governments recognize the key role open source projects like FreeBSD play in our shared digital infrastructure. This STF-commissioned work will provide the necessary visibility, auditability, and trust for commercial FreeBSD users facing new regulations as well as public sector, academic, and individual users.”&nbsp;</p>
</section>

<section>
<p>The Sovereign Tech Fund (<a href="https://www.sovereigntechfund.de/">https://www.sovereigntechfund.de</a>) supports the development, improvement, and maintenance of open digital infrastructure in the public interest. Its goal is to strengthen the open source ecosystem sustainably, focusing on security, resilience, technological diversity, and the people behind the code. STF is funded by the German Federal Ministry for Economic Affairs and Climate Action (BMWK) and hosted at and supported by the German Federal Agency for Disruptive Innovation GmbH (SPRIND).</p>
</section>

<section>
<p><br><strong>How to Get Involved:&nbsp;</strong></p>
</section>

<section>
<p>The FreeBSD Foundation is committed to transparent and collaborative communication. All announcements and updates will be made through established public channels. For questions or interest in participating in potential Advisory Committees to provide feedback and guidance on STF-funded work, please contact partnerships@freebsdfoundation.org.</p>
</section>

<section>
<p><strong>About The FreeBSD Foundation</strong></p>
</section>

<section>
<p>The FreeBSD Foundation is a 501(c)(3) non-profit organization supporting the FreeBSD Project and community. Accepting donations from individuals and businesses, the Foundation uses funds to develop features, employ software engineers, improve build and test infrastructure, advocate for FreeBSD through in-person and online events, and provide training and educational material. Representing the FreeBSD Project in legal affairs, the Foundation is the recognized entity for contracts, licenses, and other legal arrangements and is entirely donation supported. Learn more at <a href="https://freebsdfoundation.org/">freebsdfoundation.org</a></p>
</section><p><span><a href="https://freebsdfoundation.org/blog/freebsd-ports-and-packages-what-you-need-to-know/" rel="prev">Previous</a></span><span></span>  </p></div><!-- .post-content -->

</article><!-- #post-16252 -->
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Anthropic publishes the 'system prompts' that make Claude tick (146 pts)]]></title>
            <link>https://techcrunch.com/2024/08/26/anthropic-publishes-the-system-prompt-that-makes-claude-tick/</link>
            <guid>41364637</guid>
            <pubDate>Tue, 27 Aug 2024 04:45:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/08/26/anthropic-publishes-the-system-prompt-that-makes-claude-tick/">https://techcrunch.com/2024/08/26/anthropic-publishes-the-system-prompt-that-makes-claude-tick/</a>, See on <a href="https://news.ycombinator.com/item?id=41364637">Hacker News</a></p>
Couldn't get https://techcrunch.com/2024/08/26/anthropic-publishes-the-system-prompt-that-makes-claude-tick/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Box64 and RISC-V in 2024: What It Takes to Run the Witcher 3 on RISC-V (288 pts)]]></title>
            <link>https://box86.org/2024/08/box64-and-risc-v-in-2024/</link>
            <guid>41364549</guid>
            <pubDate>Tue, 27 Aug 2024 04:23:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://box86.org/2024/08/box64-and-risc-v-in-2024/">https://box86.org/2024/08/box64-and-risc-v-in-2024/</a>, See on <a href="https://news.ycombinator.com/item?id=41364549">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

			
<p>It’s been over a year since <a href="https://box86.org/2023/05/box64-and-risc-v/">our last update</a> on the state of the RISC-V backend, and we recently successfully ran The Witcher 3 on an RISC-V PC, which I believe is the first AAA game ever to run on an RISC-V machine. So I thought this would be a perfect time to write an update, and here it comes.</p>



<figure><p>
<iframe title="The Witcher 3 running On RiSC-V" width="580" height="326" src="https://www.youtube.com/embed/5UMUEM0gd34" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p><figcaption>The Witcher 3 Running on RISC-V via Box64, Wine, and DXVK.</figcaption></figure>



<h2>The Story</h2>



<p>A year ago, RV64 DynaRec could only run some relatively “easy-to-run” native Linux games, such as Stardew Valley, World of Goo, etc.</p>



<p>On the one hand, this was because after a large number of new x86_64 instructions were implemented quickly in RISC-V, there were many bugs left in the DynaRec. Things won’t work if you don’t implement the x86_64 ISA correctly. But the most important factor is that we had no RISC-V device could be plugged into an AMD graphics card at the time, and the IMG integrated graphics cards on VisionFive 2 and LicheePi 4A did not support OpenGL, only OpenGL ES.</p>



<p>We can get a certain level of OpenGL support using gl4es, which allows games like Stardew Valley to run, but it is not enough for other more serious Linux games, as well as all Windows games in general.</p>



<p>So this became a hard barrier for us to test more x86 programs in the wider world, until both ptitSeb and I received the Milk-V Pioneer from Sophgo, which is a 64-core RISC-V PC, and of course, it also has a PCIe slot for a graphics card. Many thanks to Sophgo!</p>



<p>In addition, another core contributor xctan also found a way to “plug” an AMD graphics card into VisionFive 2 via the M.2 interface. With that, we were exposed to the wider world and we’ve since fixed a ton of RV64 DynaRec bugs and also added a ton of new x86 instructions. Changing in quantity leads to changes in quality, more and more games were working, and finally, we tried running The Witcher 3 for the first time, and it just worked!</p>



<p>That’s the story of running The Witcher 3 on RISC-V.</p>



<h2>What is the Current Status of RISC-V DynaRec?</h2>



<p>The x86 instruction set is very very big. According to rough statistics, the ARM64 backend implements more than 1,600 x86 instructions in total, while the RV64 backend implements about 1,000 instructions. Among them, more than 300 of these instructions are newly supported AVX ones that we haven’t implemented at all in RISC-V. Anyway, still need some catching up.</p>



<p>Also, for SSE instructions, we use scalar instructions for implementation, while AArch64 uses the Neon extension and LoongArch64 uses the LSX extension. So the performance is quite poor compared to the other two backends.</p>



<p>However, things are not set in stone. RISC-V has a vector extension called the Vector extension. Yeah I know, so I will call it RVV from now on.</p>



<p>There are already some devices that support RVV on the market, such as the Milk-V Pioneer mentioned above, which supports the xtheadvector extension, which is a variant of RVV version 0.7.1 (things are a bit complicated). In addition, the SpacemiT K1/M1 SoC released not long ago supports the ratified version of RVV 1.0. Currently, the Banana Pi F3 and Milk-V Jupiter equipped with this SoC are already available for purchase.</p>



<p>With these devices available, recently we have added basic RVV support to box64 and implemented several common SSE instructions. However, this work is still very early, so it will not help the performance for now. But the future is promising, right?</p>



<p>Next, let’s talk about the two dark clouds hanging over the RISC-V backend. These are the stuff where I feel RISC-V is most lacking in x86 emulation over the past year.</p>



<h2>The Most Wanted Instructions for x86 Emulation</h2>



<p>At least in the context of x86 emulation, among all 3 architectures we support, RISC-V is the least expressive one. Compared with AArch64 and LoongArch64, RISC-V lacks many convenient instructions, which means that we have to use more instructions to emulate the same behavior, so the translation efficiency will be lower.</p>



<p>Among them, two instructions are the most critical ones — the ability to pick a range of bits from one register into another; and the ability to insert some bits from one register into a range of another register.</p>



<p>Both LoongArch64 and AArch64 have equivalent instructions, but the RISC-V world has no counterparts for these two instructions, whether official or vendor extensions. It’s not some complex instructions that break the RISC philosophy, so it’s a shame they do not exist on RISC-V.</p>



<p>But why it’s so important for x86 emulation? Because the x86 ISA tends to preserve the unchanged bits.</p>



<p>For example, for an <code>ADD AH, BL</code> instruction, box64 needs to extract the lowest byte from RBX, added to the second lowest byte of RAX, and then insert it back into the second lowest byte of RAX <strong>while keeping all other bytes in RAX unchanged</strong>.</p>



<p>On LoongArch64, we have <code>BSTRPICK.D</code> to pick the bits, and <code>BSTRINS.D</code> to insert the bits, so the implementation would be:</p>



<pre><code>BSTRPICK.D scratch1, xRAX, 15, 8
BSTRPICK.D scratch2, xRBX, 7, 0
ADD scratch1, scratch1, scratch2
BSTRINS.D xRAX, scratch1, 15, 8</code></pre>



<p>Simple and intuitive, right? And it would be as simple on ARM64, with <code>UBFX</code> and <code>BFI</code> opcodes. On RISC-V, however, we have to do this:</p>



<pre><code># extract the second lowest byte of RAX
SRLI scratch1, xRAX, 8
ANDI scratch1, scratch1, 0xFF
# extract the lowest byte of RBX
ANDI scratch2, xRBX, 0xFF
# do the addition
ADD scratch1, scratch1, scratch2
# fill scratch3 with mask 0xFFFF_FFFF_FFFF_00FF
LUI	scratch3, 0xFFFF0
ADDIW   scratch3, scratch3, 0xFF
# insert it back
AND xRAX, xRAX, scratch3
ANDI scratch1, scratch1, 0xFF
SLLI scratch1, scratch1, 8
OR xRAX, xRAX, scratch1</code></pre>



<p>So a whole of 10 instructions for a simple byte add and this is by no means an isolated case! There are many similar instructions in x86, and their implementation on RISC-V is more cumbersome.</p>



<h2>The Frustration of 16-byte Atomic Instructions</h2>



<p>x86 has LOCK prefixed instructions for lock-free atomic operations, and box64 mainly uses LR/SC sequence to emulate these. LR/SC is short for Load-Reserved / Store-Conditionally.</p>



<p>For example, for <code>LOCK ADD [RAX], RCX</code>, we generate the following code:</p>



<pre><code>MARKLOCK:
LR.D scratch1, (xRAX)
ADD scratch2, scratch1, xRCX
SC.D scratch3, scratch2, (xRAX)
BNEZ scratch3, MARKLOCK</code></pre>



<p>If the address in RAX is unaligned, things become a bit more complex, but in general, this works really well.</p>



<p>Except for the <code>LOCK CMPXCHG16B</code> instruction, which compares <code>RDX:RAX</code> with 16 bytes of memory and exchanges <code>RCX:RBX</code> to the memory address. While some 16-byte atomic instructions in AArch64 and LoongArch64 can be used to implement this, again, there are no counterparts in RISC-V whatsoever, unfortunately.</p>



<p>Therefore, we cannot implement this instruction as perfectly as other architectures, and even more unfortunately, many programs use this instruction, such as Unity games.</p>



<h2>The End</h2>



<p>In the end, and despite all those short-comming, The Witcher 3 actually runs, at up to 15 fps in-game and full speed on the main menu! So not that bad for a machine never designed to run AAA games!</p>



<figure><img decoding="async" width="1024" height="576" src="https://box86.org/wp-content/uploads/2024/08/witcher3menu_riscv-1024x576.png" alt="The Witcher 3 Menu with DXVK_HUD running on RiSC-V" srcset="https://box86.org/wp-content/uploads/2024/08/witcher3menu_riscv-1024x576.png 1024w, https://box86.org/wp-content/uploads/2024/08/witcher3menu_riscv-300x169.png 300w, https://box86.org/wp-content/uploads/2024/08/witcher3menu_riscv-768x432.png 768w, https://box86.org/wp-content/uploads/2024/08/witcher3menu_riscv-1536x864.png 1536w, https://box86.org/wp-content/uploads/2024/08/witcher3menu_riscv-1200x675.png 1200w, https://box86.org/wp-content/uploads/2024/08/witcher3menu_riscv.png 1920w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Harvard and MIT's $800M Mistake: The Triple Failure of 2U, EdX, and Axim (197 pts)]]></title>
            <link>https://www.classcentral.com/report/2u-edx-bankruptcy/</link>
            <guid>41363549</guid>
            <pubDate>Tue, 27 Aug 2024 00:34:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.classcentral.com/report/2u-edx-bankruptcy/">https://www.classcentral.com/report/2u-edx-bankruptcy/</a>, See on <a href="https://news.ycombinator.com/item?id=41363549">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p><img loading="lazy" src="https://www.classcentral.com/report/wp-content/uploads/2024/08/2u-bankruptcy-banner-triple-failure.png" alt="" width="1536" height="768" srcset="https://www.classcentral.com/report/wp-content/uploads/2024/08/2u-bankruptcy-banner-triple-failure.png 1536w, https://www.classcentral.com/report/wp-content/uploads/2024/08/2u-bankruptcy-banner-triple-failure-300x150.png 300w, https://www.classcentral.com/report/wp-content/uploads/2024/08/2u-bankruptcy-banner-triple-failure-1024x512.png 1024w, https://www.classcentral.com/report/wp-content/uploads/2024/08/2u-bankruptcy-banner-triple-failure-768x384.png 768w" sizes="(max-width: 1536px) 100vw, 1536px"></p>
<p>In 2021, the unprofitable 2U bought edX, an unprofitable non-profit, for a staggering $800 million. Even to my untrained eye, it seemed like a questionable decision.</p>
<p>In my <a href="https://www.classcentral.com/report/2u-edx-acquisition-analysis/">analysis of the acquisition</a>, I had highlighted the financial strain this acquisition would place on 2U, noting the additional $42 million of annual interest expense due to the loan taken to finance the purchase.</p>
<p>Last month, <a href="https://www.wsj.com/business/2u-ed-tech-company-files-chapter-11-bankruptcy-24ca1017">2U filed for bankruptcy</a>, primarily due to the significant debt it took on, particularly to finance the acquisition of edX.</p>
<p>Fast forward to 2024, and we’re faced with a harsh reality: 2U is bankrupt, edX seems to have stagnated, and Axim Collaborative (the&nbsp; non-profit from the sale that retained the $800 million) has barely made a peep.</p>
<p>As one of the few voices who expressed skepticism from the outset (in a business sense), I feel compelled to break down this triple failure and the unfulfilled promises that accompanied it.</p>
<h2>From Ivy League to Bankruptcy: A Timeline</h2>
<p>2021:</p>
<ul>
<li aria-level="1">June/July: 2U announces <a href="https://www.classcentral.com/report/2u-edx-acquisition-analysis/">acquisition of edX for $800 million</a></li>
<li aria-level="1">November: Acquisition finalized</li>
</ul>
<p>2022:</p>
<ul>
<li aria-level="1">February: <a href="https://www.classcentral.com/report/2u-stock-drop/">2U stock drops 50</a>% after Q1 earnings report, market cap falls below edX purchase price</li>
<li aria-level="1">July/August: 2U announces strategy shift centered around edX, including <a href="https://www.classcentral.com/report/2u-layoffs/">layoffs and cost reductions</a></li>
<li aria-level="1">2U implements “platform strategy” to unify operations under edX brand</li>
</ul>
<p>2023:</p>
<ul>
<li aria-level="1">March: 2U investor day outlines <a href="https://www.classcentral.com/report/edx-catalog-subscriptions/">plans for edX</a> including new subscription models and “funnel builders”</li>
<li aria-level="1"><a href="https://www.classcentral.com/report/axim-collaborative/">Axim Collaborative announced</a> with a new CEO</li>
<li aria-level="1">Q3:
<ul>
<li aria-level="2">2U announces <a href="https://www.classcentral.com/report/2u-edx-disastrous-quarter/">disastrous Q3 results</a></li>
<li aria-level="2">Market cap falls to less than $80 million</li>
<li aria-level="2">USC partnership terminated (costing USC $40 million “break fee”)</li>
<li aria-level="2"><a href="https://www.classcentral.com/report/2u-layoffs-2023/">Additional layoffs implemented</a></li>
<li aria-level="2"><a href="https://2u.com/newsroom/2u-inc-announces-leadership-transition/">CEO Chip Paucek steps down</a>, replaced by CFO Paul Lalljie</li>
</ul>
</li>
<li aria-level="1">Q4: 2U takes over programs from <a href="https://www.classcentral.com/report/2u-edx-disastrous-quarter/">Pearson’s OPM business</a></li>
</ul>
<p>2024:</p>
<ul>
<li aria-level="1">January: Another wave of <a href="https://www.classcentral.com/report/2u-layoffs-2024/">layoffs at 2U/edX</a></li>
<li aria-level="1">July: <a href="https://www.wsj.com/business/2u-ed-tech-company-files-chapter-11-bankruptcy-24ca1017">2U files for Bankruptcy</a></li>
</ul>
<h2>2U’s Costly Gamble</h2>
<blockquote><p>As we make all of these changes, I’m proud to say that by year’s end we’ll have answered the question from our IPO eight years ago. Yes, you can build a sustainable education business in higher ed at scale.</p></blockquote>
<p>This quote, made almost a year ago by 2U CEO Chip Paucek, came as he announced a major layoff and the consolidation of different 2U brands under edX. Three months later, he stepped down as 2U CEO and board member.</p>
<p>This type of hubris is something I’ve observed since I began following 2U after their acquisition of edX. 2U envisions a reality that doesn’t exist but acts as if it does.</p>
<p>Here’s an example. In response to a <a href="https://www.wsj.com/articles/that-fancy-university-course-it-might-actually-come-from-an-education-company-11657126489">Wall Street Journal article about 2U’s practices</a> (one of many such articles), Paucek wrote a blog post titled “<a href="https://2u.com/newsroom/dont-let-the-skeptic-win/">Don’t Let the Skeptic Win</a>.” One line caught my attention: “We’ve helped our partners dramatically lower the cost of their degrees”.</p>
<table>
<caption>Cost summary of 2U Degrees</caption>
<thead>
<tr>
<td></td>
<td>Masters</td>
<td>Doctorates</td>
</tr>
</thead>
<tbody>
<tr>
<td>Total</td>
<td>99</td>
<td>12</td>
</tr>
<tr>
<td>Median Price</td>
<td>$66.5K</td>
<td>$106K</td>
</tr>
<tr>
<td>Min Price</td>
<td>$34K</td>
<td>$56K</td>
</tr>
<tr>
<td>Max Price</td>
<td>$126K</td>
<td>$201K</td>
</tr>
</tbody>
</table>
<p>This claim surprised me because a few months later, I investigated the <a href="https://www.classcentral.com/report/2u-online-degrees-cost/">cost of 2U-powered degrees</a>. I found that 20% of these degrees would cost the learner over $100,000, with the most expensive exceeding $200,000.</p>
<p>I would argue the opposite—2U’s business model required degrees to be as expensive as on-campus degrees. Acquiring students is a significant cost for online degrees, and as a company tries to grow to satisfy investors, it spends more to acquire students.</p>
<p>In 2021, 2U spent $456M on sales and marketing, accounting for almost 40% of its expenses. This is why it acquired edX: to optimize these costs and reduce their reliance on paid advertising.</p>
<figure id="attachment_75860" aria-describedby="caption-attachment-75860"><a href="https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-marketing-engine.png"><img loading="lazy" src="https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-marketing-engine.png" alt="2U’s plan: Combining edX marketplace with 2U’s marketing engine" width="1324" height="748" srcset="https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-marketing-engine.png 1324w, https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-marketing-engine-300x169.png 300w, https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-marketing-engine-1024x579.png 1024w, https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-marketing-engine-768x434.png 768w" sizes="(max-width: 1324px) 100vw, 1324px"></a><figcaption id="caption-attachment-75860">2U’s plan: Combining edX marketplace with 2U’s marketing engine</figcaption></figure>
<p>Essentially, edX’s role after the acquisition was to help reduce the cost of acquiring learners for 2U’s various programs: GetSmarter Executive/Professional Education, Trilogy Bootcamps, and 2U-powered online degrees.</p>
<p>Before acquiring edX, 2U’s cost per enrollment was approximately $3,900. They anticipated reducing this cost by 10% by marketing their programs to edX users and would lead to $40–$60M in annual savings.</p>
<p>In my analysis of <a href="https://www.classcentral.com/report/2u-edx-acquisition-analysis/">2U’s acquisition of edX</a>, I had talked about why this model might not work. I highlighted two key issues: the mismatch between 2U’s high-cost programs and edX’s largely international user base, and edX’s own poor track record in converting learners to their more affordable online degrees</p>
<figure id="attachment_87874" aria-describedby="caption-attachment-87874"><a href="https://www.classcentral.com/report/wp-content/uploads/2023/10/edx-fce-2023.jpeg"><img loading="lazy" src="https://www.classcentral.com/report/wp-content/uploads/2023/10/edx-fce-2023.jpeg" alt="" width="2330" height="1340" srcset="https://www.classcentral.com/report/wp-content/uploads/2023/10/edx-fce-2023.jpeg 2330w, https://www.classcentral.com/report/wp-content/uploads/2023/10/edx-fce-2023-300x173.jpeg 300w, https://www.classcentral.com/report/wp-content/uploads/2023/10/edx-fce-2023-1024x589.jpeg 1024w, https://www.classcentral.com/report/wp-content/uploads/2023/10/edx-fce-2023-768x442.jpeg 768w, https://www.classcentral.com/report/wp-content/uploads/2023/10/edx-fce-2023-1536x883.jpeg 1536w, https://www.classcentral.com/report/wp-content/uploads/2023/10/edx-fce-2023-2048x1178.jpeg 2048w" sizes="(max-width: 2330px) 100vw, 2330px"></a><figcaption id="caption-attachment-87874">Degree enrollments drop as edX embraces new marketing strategy (<a href="https://s26.q4cdn.com/441000616/files/doc_financials/2023/q2/2023-08-08-FINAL-2Q-23_Earnings_Presentation.pdf">Source</a>)</figcaption></figure>
<p>These anticipated savings never materialized. In fact, 2U’s enrollments continued to decline after they implemented the “new marketing framework.”</p>
<p>While they were able to reduce their marketing and sales expenses, this resulted in fewer enrollments. Furthermore, any savings were offset by the annual interest payments on the debt incurred to purchase edX.</p>
<p>This financial burden was so significant that it led to three rounds of layoffs. To generate more cash at the end of last year, 2U resorted to the drastic measure of “Portfolio Management,” terminating certain degree partnerships in exchange for breakup fees. Agreements totaling approximately $150 million were finalized by the end of 2023.</p>
<p>When 2U acquired edX, it painted a rosy picture of reduced student acquisition costs and a “platform strategy” that would revolutionize online education. Reality could not have been more different.</p>
<p>The bankruptcy filing in 2024 was the final nail in the coffin, proving that the edX acquisition was not just a misstep, but a catastrophic error that sank the entire company.</p>
<h2>EdX’s Stagnation</h2>
<p>“2U’s people, technology, and scale will expand edX’s ability to deliver on our mission and enable all learners to unlock their potential. #freetodegree”<span>&nbsp;</span></p>
<ul>
<li aria-level="1"><em>Anant Agarwal (@agarwaledu)<a href="https://twitter.com/agarwaledu/status/1409840807155224576?ref_src=twsrc%5Etfw"> June 29, 2021</a></em></li>
</ul>
<p>As misguided as it was, 2U has been very specific about how edX fits into its plans: reducing their user acquisition costs. That’s why 2U spent $800 million in cash to buy edX. EdX is a marketing channel for 2U.</p>
<p>EdX’s reason, on the other hand, was a bit vague. 2U has some marketing expertise that would somehow help edX catch up to Coursera.</p>
<p>Even before the pandemic, Coursera was increasing the gap between itself and edX. The gap only widened after the pandemic. C<a href="https://www.classcentral.com/report/the-second-year-of-the-mooc/">oursera gained almost as many learners in 2020</a> alone as edX did since its launch nine years ago.</p>
<figure id="attachment_75863" aria-describedby="caption-attachment-75863"><a href="https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-broad-strokes-e1625475120473.png"><img loading="lazy" src="https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-broad-strokes-e1625475120473.png" alt="Broad strokes of the 2U + edX future plans" width="1100" height="611" srcset="https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-broad-strokes-e1625475120473.png 1100w, https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-broad-strokes-e1625475120473-300x167.png 300w, https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-broad-strokes-e1625475120473-1024x569.png 1024w, https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-broad-strokes-e1625475120473-768x427.png 768w" sizes="(max-width: 1100px) 100vw, 1100px"></a><figcaption id="caption-attachment-75863">Broad strokes of the 2U + edX future plans</figcaption></figure>
<p>I had argued that edX’s rationale for the acquisition was flawed. 2U’s “marketing engine” is built around higher-priced programs involving high-touch activities like sales. This approach is unlikely to benefit the majority of edX’s lower-priced course offerings or help it catch up to Coursera.</p>
<p>Competing with Coursera would require significant investment – potentially hundreds of millions of dollars – which 2U may struggle to provide given its new debt from the acquisition. Even if 2U had the money, it’s unclear why they would take that risk.</p>
<p>2U acquired edX primarily to leverage its marketplace (the website itself) and popular brand to market expensive programs like boot camps and degrees. This is exactly what happened.</p>
<figure id="attachment_92482" aria-describedby="caption-attachment-92482"><img loading="lazy" src="https://www.classcentral.com/report/wp-content/uploads/2024/08/2U-changes-to-edx-seo-leads.png" alt="The image shows two slides from 2U's Investor Day in 2023, focusing on edX's strategy to grow its user base and the impact on organic prospect volume. The left slide lists URLs for various learning pages on edX's website, such as sustainability, marketing, artificial intelligence, public health, and others, emphasizing the strategy of publishing new pages to attract high-intent audiences. The right slide features a line chart titled &quot;Bootcamp Prospect Volume from edX SEO,&quot; displaying a black line indicating the volume of edX organic prospects over time, with the note &quot;edX Adding 30% Incremental Organic Vol&quot; showing a steady increase from December 2021 to February 2023. The chart highlights a rise in domestic U.S. prospect volume, demonstrating edX's efforts to enhance its SEO performance and attract more users." width="2048" height="1024" srcset="https://www.classcentral.com/report/wp-content/uploads/2024/08/2U-changes-to-edx-seo-leads.png 2048w, https://www.classcentral.com/report/wp-content/uploads/2024/08/2U-changes-to-edx-seo-leads-300x150.png 300w, https://www.classcentral.com/report/wp-content/uploads/2024/08/2U-changes-to-edx-seo-leads-1024x512.png 1024w, https://www.classcentral.com/report/wp-content/uploads/2024/08/2U-changes-to-edx-seo-leads-768x384.png 768w, https://www.classcentral.com/report/wp-content/uploads/2024/08/2U-changes-to-edx-seo-leads-1536x768.png 1536w" sizes="(max-width: 2048px) 100vw, 2048px"><figcaption id="caption-attachment-92482"><em>2U’s 2023 Investor Day slides reveal edX’s SEO strategy and bootcamp lead growth.</em></figcaption></figure>
<p>Most programs now featured on edX’s homepage were previously 2U offerings. In fact, 2U rebranded their bootcamps as “<a href="https://press.edx.org/edx-boot-camps">edX bootcamps</a>.”</p>
<p>The rebranding of Trilogy Education’s (acquired for $750M) boot camps as “edX bootcamps” is now backfiring, as <a href="https://feweek.co.uk/ofsted-slates-us-firm-with-5m-dfe-bootcamps-contract/">recent Ofsted reports</a> criticized the program’s management and outcomes, leading to negative publicity. These shortcomings have led to low course completion rates and few learners securing jobs.</p>
<p>This mismanagement reflects poorly on the edX brand, potentially eroding its reputation for quality education. The association of the edX name with these problematic bootcamps amplifies the negative impact.</p>
<p>From an outside perspective, it appeared that a significant portion of edX’s efforts went into promoting 2U programs. Unfortunately for both 2U and edX, <a href="https://www.classcentral.com/report/2u-stock-drop/">edX’s declining SEO performance</a> made it increasingly difficult for 2U to monetize its $800 million acquisition effectively.</p>
<figure id="attachment_92494" aria-describedby="caption-attachment-92494"><img loading="lazy" src="https://www.classcentral.com/report/wp-content/uploads/2024/08/anant-linkedin-bio-2u.png" alt="" width="1024" height="512" srcset="https://www.classcentral.com/report/wp-content/uploads/2024/08/anant-linkedin-bio-2u.png 1024w, https://www.classcentral.com/report/wp-content/uploads/2024/08/anant-linkedin-bio-2u-300x150.png 300w, https://www.classcentral.com/report/wp-content/uploads/2024/08/anant-linkedin-bio-2u-768x384.png 768w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption id="caption-attachment-92494">Anant Agarwal LinkedIn bio.</figcaption></figure>
<p>With each round of layoffs and reorganization, edX’s CEO, Anant Agarwal, received a new title at 2U, progressing from Chief Open Education Officer to Chief Platform Officer and finally to Chief Academic Officer. It’s unclear how many of the original edX crew remain. I know that many of them have left or been laid off.</p>
<p>While trying to locate the post about the acquisition on edX’s blog, I realized that the entire edX blog has been removed. However, you can still read the post on the <a href="https://web.archive.org/web/20221207072316/https://blog.edx.org/our-shared-mission-a-strong-foundation-for-an-exciting-future">Internet Web Archive</a>.</p>
<p>The very qualities that made edX unique—its non-profit status—were eroded, leaving it a mere shadow of its former self. Its future now rests in the hands of 2U’s creditors.</p>
<h2>Axim Collaborative’s Hollow Legacy</h2>
<figure id="attachment_92492" aria-describedby="caption-attachment-92492"><img loading="lazy" src="https://www.classcentral.com/report/wp-content/uploads/2024/08/axim-homepage.png" alt="" width="1024" height="512" srcset="https://www.classcentral.com/report/wp-content/uploads/2024/08/axim-homepage.png 1024w, https://www.classcentral.com/report/wp-content/uploads/2024/08/axim-homepage-300x150.png 300w, https://www.classcentral.com/report/wp-content/uploads/2024/08/axim-homepage-768x384.png 768w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption id="caption-attachment-92492">Axim Collaborative homepage.</figcaption></figure>
<p><a href="https://www.axim.org/">Axim Collaborative</a> is the non-profit organization that retained the ~$800 million from the 2U acquisition. It’s the only cash-rich entity in this story, yet three years later, it has little to show for its wealth.</p>
<p>To clarify the somewhat confusing sequence of events:</p>
<ol>
<li aria-level="1">The original non-profit edX sold its brand and most assets to 2U.</li>
<li aria-level="1">The remaining non-profit entity was temporarily renamed “The Center for Reimagining Learning.”</li>
<li aria-level="1">Last year, this organization was officially named Axim Collaborative and appointed a new CEO.</li>
</ol>
<p>Organizationally, Axim Collaborative is essentially the continuation of the original edX non-profit, minus the assets sold to 2U. It kept the sale proceeds and the Open edX platform, which wasn’t part of the acquisition.</p>
<p>Notably, some of the same MIT and Harvard leaders who made the decision to sell edX now sit on Axim’s board. These are the same individuals responsible for the disastrous sale.</p>
<p>During the original acquisition announcement, the non-profit was <a href="https://news.mit.edu/2021/mit-harvard-transfer-edx-2u-0629">positioned ambitiously</a>:</p>
<p>Backed by these substantial resources, the nonprofit will focus on overcoming persistent inequities in online learning, in part through exploring how to apply artificial intelligence to enable personalized learning that responds and adapts to the style and needs of the individual learner.<span>&nbsp;</span></p>
<p>However, <a href="https://projects.propublica.org/nonprofits/organizations/460807740">recent tax returns</a> suggest another reality. According to its 2023 Tax Return (fiscal year ending in June 2023), Axim is sitting on $735 million. In FY 2022, it made $15 million from investment income and had expenses of $9 million.</p>
<p>The promised focus on artificial intelligence and personalized learning seems to have evaporated, especially ironic given the recent AI boom.</p>
<p>Axim appears to have become primarily a grant-giving organization. Besides supporting Open edX, there’s little evidence of using its “substantial resources” for innovation as initially promised.</p>
<p>For perspective, Axim’s current assets exceed the total amount edX spent during its entire non-profit phase.</p>
<p>Instead of being an innovator, Axim Collaborative seems to be a non-entity in the edtech space, its promises of innovation and equity advancement largely unfulfilled.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Predicting the Future of Distributed Systems (147 pts)]]></title>
            <link>https://blog.colinbreck.com/predicting-the-future-of-distributed-systems/</link>
            <guid>41363499</guid>
            <pubDate>Tue, 27 Aug 2024 00:26:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.colinbreck.com/predicting-the-future-of-distributed-systems/">https://blog.colinbreck.com/predicting-the-future-of-distributed-systems/</a>, See on <a href="https://news.ycombinator.com/item?id=41363499">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        



<main id="site-main">
<article>

    <header>


        


        

            <figure>
                <img srcset="https://blog.colinbreck.com/content/images/size/w300/2024/08/tuscan-doors-1.jpg 300w,
                            https://blog.colinbreck.com/content/images/size/w600/2024/08/tuscan-doors-1.jpg 600w,
                            https://blog.colinbreck.com/content/images/size/w1000/2024/08/tuscan-doors-1.jpg 1000w,
                            https://blog.colinbreck.com/content/images/size/w2000/2024/08/tuscan-doors-1.jpg 2000w" sizes="(min-width: 1400px) 1400px, 92vw" src="https://blog.colinbreck.com/content/images/size/w2000/2024/08/tuscan-doors-1.jpg" alt="Predicting the Future of Distributed Systems">
            </figure>

    </header>

    <section>
        <p>There are significant changes happening in distributed systems. Object storage is becoming the database, tools for transactional processing and analytical processing are becoming one in the same, and there are new programming models that promise some combination of superior security, portability, management of application state, or simplification. These changes will influence how systems are operated in addition to how they are programmed. While I want to embrace many of these innovations, it can be hard to pick a path forward.</p>
<p>If a new technology only provides incremental value, most people will question the investment. Even when a technology promises a step change in value, it can be difficult to adopt if there is no migration path, and risky if it will be difficult to change if it ends up being the wrong investment. Many transactional and analytical systems are starting to use object storage because there is a clear step-change in value, and optionality is mitigating many of the risks. However, while I appreciate the promise of new programming models, the path forward is a lot harder to grasp. If people can’t rationalize the investment, most will keep doing what they already know. As the saying goes, nobody gets fired for buying IBM.</p>
<p>I have been anticipating changes in transactional and analytical systems for a few years, especially around object storage and programming models, not because I’m smarter or better at predicting the future, but because I have been lucky enough to be exposed to some of them. While I cannot predict the future, I will share how I’m thinking about it.</p>
<h2 id="one-way-door-and-two-way-door-decisions">One-Way-Door and Two-Way-Door Decisions</h2>
<p>On <a href="https://www.youtube.com/watch?v=DcWqzZ3I2cY&amp;t=3565s">Lex Fridman’s podcast</a>,<sup><a href="#fn1" id="fnref1">[1]</a></sup> Jeff Bezos described how he manages risk from the perspective of one-way-door decisions and two-way-door decisions. A one-way-door decision is final, or takes significant time and effort to change. For these decisions, it is important to slow down, ensure you are involving the right people, and gather as much information as possible. These decisions should be guided by executive leadership.<sup><a href="#fn2" id="fnref2">[2]</a></sup></p>
<blockquote>
<p>Some decisions are so consequential and so important and so hard to reverse that they really are one-way-door decisions. You go in that door, you’re not coming back. And those decisions have to be made very deliberately, very carefully. If you can think of yet another way to analyze the decision, you should slow down and do that.<br>
—Jeff Bezos</p>
</blockquote>
<p>A two-way-door decision is less consequential. If you make the wrong decision, you can always come back and choose another door. These decisions should be made quickly, often by individuals or small teams. Executives should not waste time on two-way-door decisions. In fact, doing so destroys agency.<sup><a href="#fn3" id="fnref3">[3]</a></sup></p>
<blockquote>
<p>What can happen is that you have a one-size-fits-all decision-making process where you end up using the heavyweight process on all decisions, including the lightweight ones—the two-way-door decisions.<br>
—Jeff Bezos</p>
</blockquote>
<p>It is critical that organizations correctly identify one-way-door and two-way-door decisions. It is costly to apply a heavyweight decision-making process to two-way-door decisions, but even more costly is a person or a small team making what they think is a two-way-door decision when it is really a one-way-door decision now imposed on the whole organization for years to come. Many technology choices are one-way-door decisions because they require significant investments and are costly and time-consuming to change.<sup><a href="#fn4" id="fnref4">[4]</a></sup></p>
<h2 id="object-storage">Object Storage</h2>
<p>Cloud object storage is almost two decades old. While it is very mature and incredibly reliable and durable, it continues to see a lot of innovation.<sup><a href="#fn5" id="fnref5">[5]</a></sup> With the amount of attention to backwards compatibility, systems integration, and interoperability, almost every investment in object storage feels like a two-way-door decision, and this will continue to accelerate adoption, investment, and innovation.<sup><a href="#fn6" id="fnref6">[6]</a></sup></p>
<p>Over a decade ago, I built a durable message queue for sharing industrial data between enterprises using Azure Blob Storage page blobs. Because the object storage provided object leases, atomic writes, durability, and replication, it allowed us to build a simple and reliable system without having to worry about broker leadership election, broker quorums, data replication, data synchronization, or other challenges. The read side was independent of the write side, stateless, and could be scaled completely independently. While the company I was working for couldn’t figure out how to leverage this infrastructure to its fullest extent, I knew that relying on object storage was an architecture that had many advantages and I expected to encounter it again.<sup><a href="#fn7" id="fnref7">[7]</a></sup></p>
<p>Fast forward to today and there are many systems—everything from relational databases, time-series databases, message queues, data warehouses, and services for application metrics—using object storage as a core part of their architecture, including transactional workloads and not just analytical workloads, archival storage, or batch processing.<sup><a href="#fn8" id="fnref8">[8]</a></sup> In addition, object storage features have expanded to include cross-region replication, immutability, object versioning, tiered storage, backup, read-after-write consistency, conditional writes,<sup><a href="#fn9" id="fnref9">[9]</a></sup> encryption, metadata, authorization, and more. These features can be used to address industry regulation, compliance, cost optimization, data lifecycle management, disaster recovery, and much more, and in a standard way across services, without having to build it directly into each application. So not only is object storage attractive from an architectural perspective, it is a win for simplicity and consistency.</p>
<p>While I can’t predict exactly how object storage will evolve, I expect the popularity of object storage to increase, especially for transactional and analytical systems. For example, storing data in Parquet files in Amazon S3 feels like a pretty safe bet. I expect read performance will continue to improve through reduced latency, increased bandwidth, improved caching, or better indexing, because it is something that will benefit the huge numbers of applications using S3.<sup><a href="#fn10" id="fnref10">[10]</a></sup> If another storage format becomes more attractive than Parquet, I trust I can use an open table format, like Apache Iceberg or Delta Lake, to manage this evolution if I don’t want to reprocess the historical data. If I do want to reprocess the data, I can rely on the elasticity of cloud infrastructure to reprocesses files when they are accessed, or as a one-time batch job. I’m not worried about choosing an open table format, because they all seem excellent, they are converging on a similar set of features, and they will undoubtedly support interoperability and migration. Similarly, if I rely on an embedded library for query optimization and processing, like DuckDB or Apache DataFusion,<sup><a href="#fn11" id="fnref11">[11]</a></sup> I expect them to continue to improve and share similar features.<sup><a href="#fn12" id="fnref12">[12]</a></sup> In other situations, I might rely on Amazon Athena, Trino, Apache Spark, Pandas, or Polars for data processing. Tools will continue to improve for importing data from, or exporting data to, relational databases, data warehouses, and time-series databases. If I want to run the same services using another cloud provider, or in my own datacenter, there are other object storage services that have S3-compatible APIs.<sup><a href="#fn13" id="fnref13">[13]</a></sup> In other words, lots and lots of two-way doors. Actually, it is an embarrassment of riches.</p>
<p>Object storage is also a very simple storage abstraction. Embedded data processing libraries, like DuckDB and Apache DataFusion, can use the local file system interchangeably with object storage. This opens up the opportunity to move workloads from distributed cloud computing infrastructure and embed them directly in a single server, or move them client-side, embedded in a web browser, or even embedded into IoT devices or industrial equipment controlling critical infrastructure.<sup><a href="#fn14" id="fnref14">[14]</a></sup> The ability to move workloads around to meet changing requirements for availability, scalability, cost, locality, durability, latency, privacy, and security opens up even more two-way doors. With object storage, it’s two-way doors all the way down.</p>
<h2 id="programming-models">Programming Models</h2>
<p>The most disruptive change in the next decade may be how we program systems—a fundamental change in how software is developed and operated—and even what we view as software and what we view as infrastructure—that most people have yet to grasp.<sup><a href="#fn15" id="fnref15">[15]</a></sup><sup><a href="#fn16" id="fnref16">[16]</a></sup> Many fail to see the value, and almost everyone is skeptical of how we get from here to there. While I believe the eventual outcomes are clear, the path forward is anything but. The fact that everything seems like a one-way door is hindering adoption.</p>
<p>I have been anticipating a shift in programming models for many years, not through any great insight of my own, but through my experiences building systems with Akka, a toolkit for distributed computing, including actor-model programming and stream processing. I saw how these primitives solved the challenges I had been working on for fifteen years in industrial computing—flow control, bounded resource constraints, state management, concurrency, distribution, scaling, and resiliency—and not just in logical ways, but from first principles. For example, actors can provide a means of modelling entities, like IoT devices, and managing state, but leave the execution and distribution of those entities up to the run-time, and in a thread-safe way. Reactive Streams provides a way to interface and interoperate systems, expressing the logic of the program, while letting the run-time handle the system dynamics in a reliable way. I could see how these models would logically extend to stateful functions and beyond, as I described in my keynote talk <a href="https://www.youtube.com/watch?v=Ifaqjop1gzU">From Fast-Data to a Key Operational Technology for the Enterprise</a> in 2018.</p>
<p>Today, there are many systems trying to solve these challenges from one perspective or another. If you squint, they break down into roughly three categories. The first category are systems that abstract the most difficult parts of distributed systems, like managing state, workflows, and partial failures. These are systems like Kalix, Dapr, Temporal, Restate, and a few others. These systems generally involve adopting the platform APIs in your programming language of choice. In the second category, in addition to abstracting some of the difficult parts of distributed systems, the platform will execute arbitrary code in the form of a binary, a container, or WebAssembly. Included in this category are wasmCloud, NATS Execution Engine,<sup><a href="#fn17" id="fnref17">[17]</a></sup> Spin, AWS Fargate, and others. The final category are the somewhat uncategorizable because they are so unique, like Golem, which, if I understand correctly, uses the stack-based WebAssembly virtual machine to execute programs durably,<sup><a href="#fn18" id="fnref18">[18]</a></sup> and Unison, which is an entirely new programing language and run-time environment.</p>
<p>However attractive or well engineered these solutions are, ten years from now, not all of these technologies, or the companies developing them, will exist. Even with the promise of solving important problems and accelerating organizations, it is nearly impossible to pick a technology because of this huge investment risk. Furthermore, so much of what matters is the quality and maturity of the tools for building, deploying, static analysis, debugging, performance analysis and all the rest, and most engineers are uncomfortable giving up control over the whole stack.<sup><a href="#fn19" id="fnref19">[19]</a></sup> Adding to the skepticism are questions about how AWS, Azure, Cloudflare, and the other cloud service providers will enter this market with their own integrated and potentially ubiquitous solutions. At the moment, it seems like one-way door after one-way door.</p>
<p>As I see it, the biggest opportunity for a new programming model is extracting the majority of the code from an application and moving it into the infrastructure instead. The second biggest opportunity is for the remaining code—what people refer to as the business logic, the essence of the program—to be portable and secure. A concrete example will help demonstrate how I’m thinking about the future.</p>
<p>In addition to the business logic, embedded in almost all modern programs are HTTP or gRPC servers for client requests, libraries for logging and metrics, clients for interfacing with databases, object storage, message queues, and lots more. Depending on when each application was last updated, built, and deployed, there will be many versions of this auxiliary code running in production. To patch a critical security vulnerability, just finding the affected services can be an enormous undertaking.<sup><a href="#fn20" id="fnref20">[20]</a></sup> Most organizations do not have mature software inventories, but even if they do, the inventory only helps with identifying the services, they still need to be updated, built, tested, and redeployed. Instead of embedding HTTP servers and logging libraries and database clients and all the rest into an application binary, if this code can move down into the infrastructure, then these resources can be isolated, secured, monitored, scaled, inventoried, and patched independently from application code, very similar to how monitoring, upgrading, securing, and patching servers underneath a Kubernetes cluster is transparent to the application developer today.<sup><a href="#fn21" id="fnref21">[21]</a></sup> If the business logic can be described and executed like this, then it also becomes possible to move code between environments, like between the cloud and the IoT edge, or between service providers.<sup><a href="#fn22" id="fnref22">[22]</a></sup></p>
<p>To encourage adoption, new programming models must find ways to transform the one-way-door decisions into two-way-door decisions. WebAssembly may help with this. WebAssembly offers a secure way to run portable code, and the WebAssembly Component Model could be the basis of a standard set of interfaces that more than one platform can provide.<sup><a href="#fn23" id="fnref23">[23]</a></sup> There may be other ways these platforms can encourage adoption by lowering risk, but the two most important things to me are: 1) not having to rewrite every application—in other words, some kind of migration path, rather than only greenfield adoption<sup><a href="#fn24" id="fnref24">[24]</a></sup> and 2) not being locked into a single provider should I want to move to a different platform, or move workloads from the cloud to my own datacenter, or into embedded IoT.</p>
<h2 id="what-is-the-future">What is the Future?</h2>
<p>There are major shifts happening in the software industry. In the future, distributed systems will look different. The decomposition of databases, transactional systems, and operational technology to incorporate object storage is well underway thanks to many two-way doors. New programming models could be very disruptive, but with so many one-way doors, the challenge of picking the technology winners and losers has never been harder. It is easier to keep doing what we already know.</p>
<blockquote>
<p>In a distributed system, there is no such thing as a perfect failure detector.<br>
We can’t hide the complexity...our abstractions are going to leak.<br>
—<a href="https://www.youtube.com/watch?v=A-frep1y80s&amp;t=286s">Peter Alvaro</a></p>
</blockquote>
<p>Programming a distributed system is hard because of the challenge of partial failures. Arguably, the success of object storage is partly due to abstractions that don’t hide all of the complexity. It remains to be seen how well new programming models can deal with partial failures without contorting the programming model itself. But these new systems are promising because they are getting back to basics, just with the lines of abstraction drawn in different places. This should result in systems that are simpler, more modular, with better separation of concerns, that are much easier to build, operate, maintain, secure, and scale. Perhaps the biggest question is, will the early adopters out-compete the others? Or will the rest of the industry catch up quickly once the new programming and operational models become clear? How safe is it to just keep doing what we already know?</p>
<blockquote>
<p>Abstractions are going to leak, so make the abstractions fluid.<br>
—<a href="https://bravenewgeek.com/abstraction-considered-harmful/">Peter Alvaro</a></p>
</blockquote>
<p>It is impossible to predict the future and I’m not going to pretend I can foresee it better than anyone else. However, I am confident in the macro trends of continued investment in object storage and, some day, the widespread adoption of new programming models that move more code down into the infrastructure. It will be fun to look back in a few years.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>This is an excellent podcast on the leadership of complex organizations and products. Another highlight for me is Jeff <a href="https://www.youtube.com/watch?v=DcWqzZ3I2cY&amp;t=7171s">describing his meeting culture</a>. <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>The <a href="https://s2.q4cdn.com/299287126/files/doc_financials/annual/2015-Letter-to-Shareholders.PDF">2015 Amazon Letter to Shareholders</a> also described the concept of one-way and two-way doors. Thanks to my friend John Mayerhofer for suggesting I incorporate this idea in this essay. <a href="#fnref2">↩︎</a></p>
</li>
<li id="fn3"><p>For more on agency, see my essay <a href="https://blog.colinbreck.com/the-importance-of-agency/">The Importance of Agency</a>. <a href="#fnref3">↩︎</a></p>
</li>
<li id="fn4"><p>I’ve been through many platform and service migrations. Even when they are the right or necessary, they can take years. In his book <em>An Elegant Puzzle</em>, Will Larson notes that growing organizations will always be in the middle of a migration—“growth makes migrations a way of life”—and it is important to be good at them: “If you don’t get effective at software and systems migrations, you’ll end up languishing in technical debt.” <a href="#fnref4">↩︎</a></p>
</li>
<li id="fn5"><p>For a deep dive into the history and incredible engineering of Amazon S3, see Andy Warfield’s article and talk: <a href="https://www.allthingsdistributed.com/2023/07/building-and-operating-a-pretty-big-storage-system.html">Building and operating a pretty big storage system called S3</a>. Marc Olson also published a deep dive into block storage: <a href="https://www.allthingsdistributed.com/2024/08/continuous-reinvention-a-brief-history-of-block-storage-at-aws.html">Continuous reinvention: A brief history of block storage at AWS</a>. <a href="#fnref5">↩︎</a></p>
</li>
<li id="fn6"><p>Because of the volume of data stored in cloud object storage, it is difficult and expensive to migrate to alternative solutions. In a very positive way, this forces vendors to pay a lot of attention to migration paths and backwards compatibility. <a href="#fnref6">↩︎</a></p>
</li>
<li id="fn7"><p>For more information on this architecture, see <a href="https://blog.colinbreck.com/shared-nothing-architectures-for-server-replication-and-synchronization/">Shared-Nothing Architectures for Server Replication and Synchronization</a>. <a href="#fnref7">↩︎</a></p>
</li>
<li id="fn8"><p>Examples include Amazon Aurora, InfluxDB, WarpStream, Snowflake, and Grafana Mimir. See Chris Riccomini’s article <a href="https://materializedview.io/p/databases-are-falling-apart">Databases Are Falling Apart: Database Disassembly and Its Implications</a> for a comprehensive exploration of this topic. <a href="#fnref8">↩︎</a></p>
</li>
<li id="fn9"><p>Azure Blob Storage has supported conditional writes for many years. <a href="https://aws.amazon.com/about-aws/whats-new/2024/08/amazon-s3-conditional-writes/">Amazon S3 just added this feature</a>. <a href="#fnref9">↩︎</a></p>
</li>
<li id="fn10"><p>For example, Amazon recently introduced S3 Express One Zone which offers millisecond latency, the AWS Common Runtime (CRT) libraries which can saturate network bandwidth for S3 file transfer, and Mountpoint for Amazon S3 for mounting S3 buckets on the local file system. It is also possible to rely on the cloud provider’s infrastructure and optimizations, many of which are only possible at scale, rather than solving these problems yourself. See <a href="https://www.youtube.com/watch?v=sc3J4McebHE&amp;t=1333s">this example</a> from Andy Warfield. <a href="#fnref10">↩︎</a></p>
</li>
<li id="fn11"><p>DuckDB and Apache DataFusion are both incredibly high-quality open-source projects and both are lead by incredibly good engineers. These libraries are blurring what can be done inside a database versus outside a database. Databases and object stores are starting to meet in the middle. See the talks <a href="https://www.youtube.com/watch?v=bZOvAKGkzpQ">DuckDB Internals</a> by <a href="https://mytherin.github.io/">Mark Raasveldt</a> and <a href="https://www.youtube.com/watch?v=I-Z7kFGsYRI">Building InfluxDB 3.0 with Apache Arrow, DataFusion, Flight and Parquet</a> by <a href="http://andrew.nerdnetworks.org/">Andrew Lamb</a>. <a href="#fnref11">↩︎</a></p>
</li>
<li id="fn12"><p>There can be too much focus on performance, rather than reliability, security, ease of use, and other important factors. Databases and query optimizers tend to converge on performance, because as soon as one discovers a performance optimization, the others also implement it. In addition, most innovations in databases and query optimizers outside of SQL tend to eventually be implemented in SQL. For more on these topics, see the article <a href="https://motherduck.com/blog/perf-is-not-enough/">Perf Is Not Enough</a> by Jordan Tigani and the talk <a href="https://www.youtube.com/watch?v=-wCzn9gKoUk">A Short Summary of the Last Decades of Data Management</a> by Hannes Mühleisen. <a href="#fnref12">↩︎</a></p>
</li>
<li id="fn13"><p>For example, Cloudflare R2 and MinIO. <a href="#fnref13">↩︎</a></p>
</li>
<li id="fn14"><p>I expand on this topic in my position paper <a href="https://blog.colinbreck.com/object-storage-and-in-process-databases-are-changing-distributed-systems/">Object Storage and In-Process Databases are Changing Distributed Systems</a>. <a href="#fnref14">↩︎</a></p>
</li>
<li id="fn15"><p>Software developers are often accused of wanting to try the latest fads and tinker with their code until it is perfect. In an industry that changes rapidly, it is important to evaluate trends, but for the vast majority of developers I have worked with, they are all focused on shipping—satisfaction only comes when their work is in the hands of customers and providing value. Perhaps because I’ve always worked on industrial software, the engineers I have worked with have also preferred simple and practical solutions that maximize reliability. If I reflect on the small number of people I have seen struggle—people who could be accused of tinkering or preferring complexity—it is because either the objectives of the work were not clear, or the individuals lacked engineering skills to complement their programming skills. <a href="#fnref15">↩︎</a></p>
</li>
<li id="fn16"><p>One of the last big changes in infrastructure was the emergence of Kubernetes over Mesos, YARN, Ansible, Chef, and other technologies for managing the infrastructure itself. See the <a href="https://x.com/jboner/status/783727538652614656">Deployment Coolness Specturm</a> from Jay Kreps. Kubernetes has accelerated many organizations by decoupling the management of infrastructure from the development of software, especially when managing infrastructure across multiple environments or teams. <a href="#fnref16">↩︎</a></p>
</li>
<li id="fn17"><p>My impression is NATS included containers not to be the next container run-time, but because people are not yet ready for the leap to WebAssembly. <a href="#fnref17">↩︎</a></p>
</li>
<li id="fn18"><p>Golem can make imperative code resilient because Golem itself uses event sourcing for the deterministic WebAssembly instructions. It remains to be seen how Golem will adapt to multi-threaded WebAssembly. See the talk <a href="https://www.youtube.com/watch?v=fHPYetd3q2g">Building Durable Microservices with WebAssembly by John A. De Goes</a> for more information. <a href="#fnref18">↩︎</a></p>
</li>
<li id="fn19"><p>In considering these new programming models, we would be well served not to forget Joel Spolsky’s essay <a href="https://www.joelonsoftware.com/2002/02/13/the-iceberg-secret-revealed/">The Iceberg Secret, Revealed</a>. <a href="#fnref19">↩︎</a></p>
</li>
<li id="fn20"><p>This is what organizations experienced as they scrambled to patch the Log4J <a href="https://en.wikipedia.org/wiki/Log4Shell">Log4Shell</a> remote code execution vulnerability. <a href="#fnref20">↩︎</a></p>
</li>
<li id="fn21"><p>This point is better illustrated visually: <a href="https://www.youtube.com/watch?v=oRuSX-FYybU&amp;t=336s">see this part of my talk from S4</a>. <a href="#fnref21">↩︎</a></p>
</li>
<li id="fn22"><p>It also becomes possible to experiment with techniques for continuous improvement to optimize performance or cost. This is a common technique in the process industries that I wrote about in <a href="https://blog.colinbreck.com/observations-on-observability/">Observations on Observability</a>. <a href="#fnref22">↩︎</a></p>
</li>
<li id="fn23"><p>The interfaces could include object storage, relational databases, key-value stores, message queues, application logs, service metrics, authentication, and authorization. <a href="#fnref23">↩︎</a></p>
</li>
<li id="fn24"><p>I manged to make it this far into an essay about the future of programming distributed systems without mentioning generative artificial intelligence (AI) or large language models (LLMs). Perhaps they have a place in rewriting or porting code as recently described by <a href="https://x.com/ajassy/status/1826608791741493281">Andy Jassy</a>. <a href="#fnref24">↩︎</a></p>
</li>
</ol>
</section>

    </section>


</article>
</main>





    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Erasure Coding for Distributed Systems (258 pts)]]></title>
            <link>https://transactional.blog/blog/2024-erasure-coding</link>
            <guid>41361281</guid>
            <pubDate>Mon, 26 Aug 2024 20:03:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://transactional.blog/blog/2024-erasure-coding">https://transactional.blog/blog/2024-erasure-coding</a>, See on <a href="https://news.ycombinator.com/item?id=41361281">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <hr>
    
    
    
    <div id="toc">

<ul>
<li><a href="#_erasure_coding_basics">Erasure Coding Basics</a></li>
<li><a href="#_applications_in_distributed_systems">Applications in Distributed Systems</a>
<ul>
<li><a href="#_space_and_tail_latency_improvements">Space and Tail Latency Improvements</a></li>
<li><a href="#_quorum_systems">Quorum Systems</a></li>
</ul>
</li>
<li><a href="#_usage_basics">Usage Basics</a></li>
<li><a href="#_usage_not_so_basics">Usage Not So Basics</a>
<ul>
<li><a href="#_decoding_cost_variability">Decoding Cost Variability</a></li>
<li><a href="#_library_differences">Library Differences</a></li>
</ul>
</li>
<li><a href="#_implementing_erasure_codes">Implementing Erasure Codes</a>
<ul>
<li><a href="#_algorithmic_efficiency">Algorithmic Efficiency</a></li>
<li><a href="#_implementation_efficiency">Implementation Efficiency</a></li>
</ul>
</li>
<li><a href="#_references">References</a></li>
</ul>
</div>
<div id="preamble">
  <p>Suppose one has \(N\) servers across which to store a file.  One extreme is to give each of the \(N\) servers a full copy of the file.  Any server can supply a full copy of the file, so even if \(N-1\) servers are destroyed, then the file hasn’t been lost.  This provides the best durability and fault tolerance but is the most expensive in terms of storage space used.  The other extreme is to carve the data up into \(N\) equal-sized chunks, and give each server one chunk.  Reading the file will require reading all \(N\) chunks and reassembling the file.  This will provide the best cost efficiency, as each server can contribute to the file read request while using the minimum amount of storage space.</p>
<p>Erasure codes are the way to more generally describe the space of trade-offs between storage efficiency and fault tolerance.  One can say "I’d like this file carved into \(N\) chunks, such that it can still be reconstructed with any \(M\) chunks destroyed", and there’s an erasure code with those parameters which will provide the minimum-sized chunks necessary to meet that goal.</p>
<p>The simplest intuition for there being middle points in this tradeoff is to consider a file replicated across three servers such that reading from any two should be able to yield the whole contents.  We can divide the file into two pieces, the first half of the file forms the first chunk (\(A\)) and the second half of the file forms the second chunk (\(B\)).  We can then produce a third equal-sized chunk (\(C\)) that’s the exclusive or of the first two (\(A \oplus B = C\)).  By reading any two of the three chunks, we can reconstruct the whole file:</p>
<table>
<colgroup>
<col>
<col>
</colgroup>
<tbody>
<tr>
<th><p>Chunks Read</p></th>
<th><p>Reconstruct Via</p></th>
</tr>
<tr>
<td><p>\(\{A, B\}\)</p></td>
<td><p>\(A :: B\)</p></td>
</tr>
<tr>
<td><p>\(\{A, C\}\)</p></td>
<td><p>\(A :: A \oplus C =&gt; (A \oplus (A \oplus B)) =&gt; A :: B\)</p></td>
</tr>
<tr>
<td><p>\(\{B, C\}\)</p></td>
<td><p>\(B \oplus C :: B =&gt; (B \oplus (A \oplus B)) :: B =&gt; A :: B\)</p></td>
</tr>
</tbody>
</table>
<p>And all erasure codes follow this same pattern of having separate data and parity chunks.</p>
</div>
<h2 id="_erasure_coding_basics">
Erasure Coding Basics
</h2> 
<p>Configuring an erasure code revolves around one formula:</p>

<div>
<table>
<tbody><tr>
<td>
\(k\)
</td>
<td>
<p>The number of pieces the data is split into.  One must read at least this many chunks in total to be able to reconstruct the value.  Each chunk in the resulting erasure code will be \(1/k\) of the size of the original file.</p>
</td>
</tr>
<tr>
<td>
\(m\)
</td>
<td>
<p>The number of parity chunks to generate.  This is the fault tolerance of the code, or the number of reads which can fail to complete.</p>
</td>
</tr>
<tr>
<td>
\(n\)
</td>
<td>
<p>The total number of chunks that are generated.</p>
</td>
</tr>
</tbody></table>
</div>
<p>Erasure codes are frequently referred to by their \(k+m\) tuple.  It is important to note that the variable names are not consistent across all literature.  The only constant is that an erasure code written as \(x+y\) means \(x\) data chunks and \(y\) parity chunks.</p>
<p>Please enjoy a little calculator to show the effects of different \(k\) and \(m\) settings:</p>
<div x-data="{k: 3, m: 2}">
<p>
Each chunk is \(1/k = \)<kbd x-text="(100/k).toFixed(2)"></kbd>% of the size of the original data.  There are \(k + m =\)<kbd x-text="k+m"></kbd> chunks total, and together they are equivalent to \((m + k) / k =\)<kbd x-text="((m+k)/k).toFixed(2)"></kbd> full copies of the data.
</p></div>
<p>Erasure codes are incredibly attractive to storage providers, as they offer a way to fault tolerance at minimal storage overhead.
Backblaze B2 runs with \(17+3\), allowing it to tolerate 3 failures using 1.18x the storage space.  OVH Cloud uses an \(8+4\) code, allowing it to tolerate 4 failures using 1.5x the storage space.  Scaleway uses a \(6+3\) code, tolerating three failures using 1.5x the storage space.  "Cloud storage reliability for Big Data applications"<a id="_sideref_1"></a><sup>[1]</sup> pays significant attention to the subject of erasure coding due to the fundamental role it plays in increasing durability for storage providers at a minimal cost of additional storage space.
<span><a id="_sidedef_1"></a>[1]: Rekha Nachiappan, Bahman Javadi, Rodrigo N. Calheiros, and Kenan M. Matawie. 2017. Cloud storage reliability for Big Data applications. <em>J. Netw. Comput. Appl.</em> 97, C (November 2017), 35–47. <a href="https://scholar.google.com/scholar?cluster=12723199345811969350">[scholar]</a></span></p>
<p>The main trade-off in erasure coding is a reduction in storage space used at the cost of an increase in requests issued to read data.  Rather than issuing one request to read a file-sized chunk from one disk, requests are issued to \(k+m\) disks.  Storage systems meant for infrequently accessed data, form ideal targets for erasure coding.  Infrequent access means issuing more IO operations per second won’t be a problematic tax, and the storage savings are significant when compared to storing multiple full copies of every file.</p>
<p>"Erasure coding" describes a general class of algorithms and not any one algorithm in particular.  In general, Reed-Solomon codes can be used to implement any \(k+m\) configuration of erasure codes.  Due to the prevalence of <a href="https://en.wikipedia.org/wiki/Standard_RAID_levels">RAID</a>, special attention in erasure coding research has been paid to developing more efficient algorithms specialized for implementing these specific subsets of erasure coding. RAID-0 is \(k+0\) erasure coding.  RAID-1 is \(1+m\) erasure coding.  RAID-4 and RAID-5 are slightly different variations of \(k+1\) erasure coding.  RAID-6 is \(k+2\) erasure coding.  Algorithms specifically designed for these cases are mentioned in the implementation section below, but it’s also perfectly fine to not be aware of what exact algorithm is being used to implement the choice of a specific \(k+m\) configuration.</p>
<p>Everything described in this post is about <em>Minimum Distance Separable</em> (MDS) erasure codes, which are only one of many erasure code families.  MDS codes provide the quorum-like property that any \(m\) chunks can be used to reconstruct the full value.  Other erasure codes take other tradeoffs, where some combinations of less than \(m\) chunks can be used to reconstruct the full value, but other combinations require more than \(m\) chunks.  "Erasure Coding in Windows Azure Storage"<a id="_sideref_2"></a><sup>[2]</sup> nicely explains the motivation of why Azure devised Local Reconstruction Codes for their deployment.  "SD Codes: Erasure Codes Designed for How Storage Systems Really Fail"<a id="_sideref_3"></a><sup>[3]</sup> pitches specializing an erasure code towards recovering from sector failures, as the most common failure type.  Overall, if one has knowledge about the expected pattern of failures, then a coding scheme that allow recovering from expected failures with less than \(m\) chunks, and unexpected failures with more than \(m\) chunks would have a positive expected value.
<span><a id="_sidedef_2"></a>[2]: Cheng Huang, Huseyin Simitci, Yikang Xu, Aaron Ogus, Brad Calder, Parikshit Gopalan, Jin Li, and Sergey Yekhanin. 2012. Erasure Coding in Windows Azure Storage. In <em>2012 USENIX Annual Technical Conference (USENIX ATC 12)</em>, USENIX Association, Boston, MA, 15–26. <a href="https://scholar.google.com/scholar?cluster=7930684733311413322">[scholar]</a><br>
         <a id="_sidedef_3"></a>[3]: James S. Plank, Mario Blaum, and James L. Hafner. 2013. SD Codes: Erasure Codes Designed for How Storage Systems Really Fail. In <em>11th USENIX Conference on File and Storage Technologies (FAST 13)</em>, USENIX Association, San Jose, CA, 95–104. <a href="https://scholar.google.com/scholar?cluster=6762112190773483176">[scholar]</a></span></p>
<h2 id="_applications_in_distributed_systems">
Applications in Distributed Systems
</h2> 
<h3 id="_space_and_tail_latency_improvements">
Space and Tail Latency Improvements
</h3> 
<p>The most direct application is in reducing the storage cost and increasing the durability of data in systems with a known, fixed set of replicas.
Think of blob/object storage or NFS storage.  A metadata service maps a file path to a server that stores the file.  Instead of having 3 replicas storing the full file each, have 15 replicas store the chunks of the (10+5) erasure coded file.  Such a coding yields half the total amount of data to store, and more than double the fault tolerance.</p>
<p>More generally, this pattern translates to "instead of storing data across \(X\) servers, consider storing it across \(X+m\) replicas with an \(X+m\) erasure code".  Over on Marc Brooker’s blog, this is illustrated <a href="https://brooker.co.za/blog/2023/01/06/erasure.html">using a caching system</a>.  Instead of using consistent hashing to identify one of \(k\) cache servers to query, one can use a \(k+m\) erasure code with \(k+m\) cache servers and not have to wait for the \(m\) slowest responses.  This provides both a storage space and tail latency improvement.</p>
<p>Again, the space and latency savings do come at a cost, which is an increase in IOPS/QPS, or effectively CPU.  In both cases, we’re betting that the limiting resource which determines how many machines or disks we need to buy is storage capacity, and that we can increase our CPU usage to decrease the amount of data that needs to be stored.  If the system is already pushing its CPU limits, then erasure coding might not be a cost-saving idea.</p>
<h3 id="_quorum_systems">
Quorum Systems
</h3> 
<p>Consider a quorum system with 5 replicas, where one must read from and write to at least 3 of them, a simple majority.  Erasure codes are well matched on the read side, where a \(3+2\) erasure code equally represents that a read may be completed using the results from any 3 of the 5 replicas.  Unfortunately, the rule is that writes are allowed to complete as long as they’re received by any 3 replicas, so one could only use a \(1+2\) code, which is exactly the same as writing three copies of the file.  Thus, there are no trivial savings to be had by applying erasure coding.</p>
<p>RS-Paxos<a id="_sideref_4"></a><sup>[4]</sup> examined the applicability of erasure codes to Paxos, and similarly concluded that the only advantage is when there’s an overlap between two quorums of more than one replica.  A quorum system of 7 replicas, where one must read and write to at least 5 of them would have the same 2 replica fault tolerance, but would be able to apply a \(3+2\) erasure code.  In general, with \(N\) replicas and a desired fault tolerance of \(f\), the best one can do with a fixed erasure coding scheme is \((N-2f)+f\).
<span><a id="_sidedef_4"></a>[4]: Shuai Mu, Kang Chen, Yongwei Wu, and Weimin Zheng. 2014. When paxos meets erasure code: reduce network and storage cost in state machine replication. In <em>Proceedings of the 23rd International Symposium on High-Performance Parallel and Distributed Computing</em> (HPDC '14), Association for Computing Machinery, New York, NY, USA, 61–72. <a href="https://scholar.google.com/scholar?cluster=16520033292975033789">[scholar]</a></span></p>
<p>HRaft<a id="_sideref_5"></a><sup>[5]</sup> explores that there is a way to get the desired improvement from a simple majority quorum, but adapting the coding to match the number of available replicas.  When all 5 replicas are available then we may use a \(3+2\) encoding, when 4 are available then use a \(2+2\) encoding, and when only 3 are available then use a \(1+2\) encoding<a id="_sideref_6"></a><sup>[6]</sup>.  Adapting the erasure code to the current replica availability yields our optimal improvement, but comes with a number of drawbacks.  Each write is optimistic in guessing the number of replicas that are currently available, and writes must be re-coded and resent to all replicas if one replica unexpectedly doesn’t acknowledge the write.  Additionally, one must still provision the system such that a replica storing the full value of every write is possible, so that after two failures, the system running in a \(1+2\) configuration won’t cause unavailability due to lacking disk space or throughput.  However, if failures are expected to be rare and will be recovered from quickly, then HRaft’s adaptive encoding scheme will yield significant improvements.
<span><a id="_sidedef_5"></a>[5]: Yulei Jia, Guangping Xu, Chi Wan Sung, Salwa Mostafa, and Yulei Wu. 2022. HRaft: Adaptive Erasure Coded Data Maintenance for Consensus in Distributed Networks. In <em>2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</em>, 1316–1326. <a href="https://scholar.google.com/scholar?cluster=15724086733201598850">[scholar]</a></span>
<span><a id="_sidedef_6"></a>[6]: And just to emphasize again, a \(1+2\) erasure encoding is just 3 full copies of the data.  It’s the same as not applying any erasure encoding.  The only difference is that it’s promised that only three full copies of the data are generated and sent to replicas.</span></p>
<h2 id="_usage_basics">
Usage Basics
</h2> 
<p>For computing erasure codings, there is a mature and standard <a href="https://jerasure.org/">Jerasure</a>.  If on a modern Intel processor, the Intel <a href="https://www.intel.com/content/www/us/en/developer/tools/isa-l/overview.html">Intelligent Storage Acceleration Library</a> is a SIMD-optimized library consistently towards the top of the benchmarks.</p>
<p>As an example, we can use <a href="https://pypi.org/project/pyeclib/">pyeclib</a> as a way to get easy access to an erasure coding implementation from python, and apply it to specifically to HRaft’s proposed adaptive erasure coding scheme:</p>
<details>
<summary>Python source code</summary>
<div>
<pre><code data-lang="python"><span>#!/usr/bin/env python
# Usage: ./ec.py &lt;K&gt; &lt;M&gt;
</span><span>import</span> <span>sys</span>
<span>K</span> <span>=</span> <span>int</span><span>(</span><span>sys</span><span>.</span><span>argv</span><span>[</span><span>1</span><span>])</span>
<span>M</span> <span>=</span> <span>int</span><span>(</span><span>sys</span><span>.</span><span>argv</span><span>[</span><span>2</span><span>])</span>

<span># Requires running the following to install dependencies:
# $ pip install --user pyeclib
# $ sudo dnf install liberasurecode-devel
</span><span>import</span> <span>pyeclib.ec_iface</span> <span>as</span> <span>ec</span>

<span># liberasurecode_rs_vand is built into liberasurecode, so this
# shouldn't have any other dependencies.
</span><span>driver</span> <span>=</span> <span>ec</span><span>.</span><span>ECDriver</span><span>(</span><span>ec_type</span><span>=</span><span>'liberasurecode_rs_vand'</span><span>,</span>
                     <span>k</span><span>=</span><span>K</span><span>,</span> <span>m</span><span>=</span><span>M</span><span>,</span> <span>chksum_type</span><span>=</span><span>'none'</span><span>)</span>
<span>data</span> <span>=</span> <span>bytes</span><span>([</span><span>i</span> <span>%</span> <span>100</span> <span>+</span> <span>32</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>10000</span><span>)])</span>
<span>print</span><span>(</span><span>f</span><span>"Erasure Code(K data chunks = </span><span>{</span><span>K</span><span>}</span><span>, M parity chunks = </span><span>{</span><span>M</span><span>}</span><span>)"</span>
      <span>f</span><span>" of </span><span>{</span><span>len</span><span>(</span><span>data</span><span>)</span><span>}</span><span> bytes"</span><span>)</span>

<span># Produce the coded chunks.
</span><span>chunks</span> <span>=</span> <span>driver</span><span>.</span><span>encode</span><span>(</span><span>data</span><span>)</span>

<span># There's some metdata that's prefixed onto each chunk to identify
# its position.  This isn't technically required, but there isn't
# an easy way to disable it.  There's also some additional bytes
# which I can't account for.
</span><span>metadata_size</span> <span>=</span> <span>len</span><span>(</span><span>driver</span><span>.</span><span>get_metadata</span><span>(</span><span>chunks</span><span>[</span><span>0</span><span>]))</span>
<span>chunk_size</span> <span>=</span> <span>len</span><span>(</span><span>chunks</span><span>[</span><span>0</span><span>])</span> <span>-</span> <span>metadata_size</span>
<span>print</span><span>(</span><span>f</span><span>"Encoded into </span><span>{</span><span>len</span><span>(</span><span>chunks</span><span>)</span><span>}</span><span> chunks of </span><span>{</span><span>chunk_size</span><span>}</span><span> bytes"</span><span>)</span>
<span>print</span><span>(</span><span>""</span><span>)</span>

<span># This replication scheme is X% less efficient than writing 1 copy
</span><span>no_ec_size</span> <span>=</span> <span>(</span><span>K</span><span>+</span><span>M</span><span>)</span> <span>*</span> <span>len</span><span>(</span><span>data</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>"No EC: </span><span>{</span><span>(</span><span>M</span><span>+</span><span>K</span><span>)</span><span>*</span><span>len</span><span>(</span><span>data</span><span>)</span><span>}</span><span> bytes, </span><span>{</span><span>1</span><span>/</span><span>(</span><span>K</span><span>+</span><span>M</span><span>)</span> <span>*</span> <span>100</span><span>}</span><span>% efficiency"</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>"Expected: </span><span>{</span><span>(</span><span>M</span><span>+</span><span>K</span><span>)</span><span>/</span><span>K</span> <span>*</span> <span>len</span><span>(</span><span>data</span><span>)</span><span>}</span><span> bytes,"</span>
      <span>f</span><span>" </span><span>{</span><span>1</span><span>/</span> <span>(</span><span>1</span><span>/</span><span>K</span> <span>*</span> <span>(</span><span>K</span><span>+</span><span>M</span><span>))</span> <span>*</span> <span>100</span><span>}</span><span>% efficiency"</span><span>)</span>
<span>total_ec_size</span> <span>=</span> <span>chunk_size</span> <span>*</span> <span>len</span><span>(</span><span>chunks</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>"Actual: </span><span>{</span><span>total_ec_size</span><span>}</span><span> bytes,"</span>
      <span>f</span><span>" </span><span>{</span><span>len</span><span>(</span><span>data</span><span>)</span> <span>/</span> <span>total_ec_size</span> <span>*</span> <span>100</span><span>}</span><span>% efficiency"</span><span>)</span>

<span># Validate that our encoded data decodes using minimal chunks
</span><span>import</span> <span>random</span>
<span>indexes</span> <span>=</span> <span>random</span><span>.</span><span>sample</span><span>(</span><span>range</span><span>(</span><span>K</span><span>+</span><span>M</span><span>),</span> <span>K</span><span>)</span>
<span># Prepended metadata is used to determine the chunk part number
# from the data itself.  Other libraries require this to be
# passed in as part of the decode call.
</span><span>decoded_data</span> <span>=</span> <span>driver</span><span>.</span><span>decode</span><span>([</span><span>chunks</span><span>[</span><span>idx</span><span>]</span> <span>for</span> <span>idx</span> <span>in</span> <span>indexes</span><span>])</span>
<span>assert</span> <span>decoded_data</span> <span>==</span> <span>data</span></code></pre>
</div>
</details>
<p>When there are 5/5 replicas available, HRaft would use a \(3+2\) erasure code:</p>
<div>
<pre>$ ./ec.py 3 2
Erasure Code(K data chunks = 3, M parity chunks = 2) of 10000 bytes
Encoded into 5 chunks of 3355 bytes

No EC: 50000 bytes, 20% efficiency
Expected: 16666.666666666668 bytes, 60.00000000000001% efficiency
Actual: 16775 bytes, 59.61251862891207% efficiency</pre>
</div>
<p>When there are 4/5 replicas available, HRaft would use a \(2+2\) erasure code:</p>
<div>
<pre>$ ./ec.py 2 2
Erasure Code(K data chunks = 2, M parity chunks = 2) of 10000 bytes
Encoded into 4 chunks of 5021 bytes

No EC: 40000 bytes, 25% efficiency
Expected: 20000.0 bytes, 50% efficiency
Actual: 20084 bytes, 49.790878311093406% efficiency</pre>
</div>
<p>When there are 3/5 replicas available, HRaft would use a \(1+2\) erasure code:</p>
<div>
<pre>$ ./ec.py 1 2
Erasure Code(K data chunks = 1, M parity chunks = 2) of 10000 bytes
Encoded into 3 chunks of 10021 bytes

No EC: 30000 bytes, 33.33333333333333% efficiency
Expected: 30000.0 bytes, 33.33333333333333% efficiency
Actual: 30063 bytes, 33.263480025280245% efficiency</pre>
</div>
<h2 id="_usage_not_so_basics">
Usage Not So Basics
</h2> 
<p>As always, things aren’t quite perfectly simple.</p>
<h3 id="_decoding_cost_variability">
Decoding Cost Variability
</h3> 
<p>Decoding performance varies with the number of data chunks that need to be recovered.  Decoding a \(3+2\) code from the three data chunks is computationally trivial.  Decoding the same file from two data chunks and one parity chunk involves solving a system of linear equations via Gaussian elimination, and the computational increases as the number of required parity chunks involved increases.  Thus, if using an erasure code as part of a quorum system, be aware that the CPU cost of decoding will vary depending on exactly which replicas reply.</p>
<p>There are a few different papers comparing different erasure code implementations and their performance across varying block size and number of data chunks to reconstruct.  I’ll suggest "Practical Performance Evaluation of Space Optimal Erasure Codes for High Speed Data Storage Systems"<a id="_sideref_7"></a><sup>[7]</sup> as the one I liked the most, from which the following figure was taken:
<span><a id="_sidedef_7"></a>[7]: Rui Chen and Lihao Xu. 2019. Practical Performance Evaluation of Space Optimal Erasure Codes for High-Speed Data Storage Systems. <em>SN Comput. Sci.</em> 1, 1 (December 2019). <a href="https://scholar.google.com/scholar?cluster=9222594581704961566">[scholar]</a></span></p>

<div>

<p><img src="https://transactional.blog/images/blog/2024-erasure-coding/decoding_performance-45d237e9.png" alt="decoding performance">
</p>
</div>
<h3 id="_library_differences">
Library Differences
</h3> 
<p>Liberasurecode abstracts over most common erasure coding implementation libraries, but be aware that does not mean that the implementations are equivalent.  Just because two erasure codes are both \(3+2\) codes doesn’t mean the same math was used to construct them.</p>
<p>Correspondingly, liberasurecode doesn’t <em>just</em> do the linear algebra work, it "helpfully" adds metadata necessary to configure which decoder to use and how, which you can’t disable or modify:</p>
<div>
<p>liberasurecode / erasurecode.h</p>
<div>
<pre><code data-lang="c"><span>struct</span> <span>__attribute__</span><span>((</span><span>__packed__</span><span>))</span>
<span>fragment_metadata</span>
<span>{</span>
    <span>uint32_t</span>    <span>idx</span><span>;</span>                <span>/* 4 */</span>
    <span>uint32_t</span>    <span>size</span><span>;</span>               <span>/* 4 */</span>
    <span>uint32_t</span>    <span>frag_backend_metadata_size</span><span>;</span>    <span>/* 4 */</span>
    <span>uint64_t</span>    <span>orig_data_size</span><span>;</span>     <span>/* 8 */</span>
    <span>uint8_t</span>     <span>chksum_type</span><span>;</span>        <span>/* 1 */</span>
    <span>uint32_t</span>    <span>chksum</span><span>[</span><span>LIBERASURECODE_MAX_CHECKSUM_LEN</span><span>];</span> <span>/* 32 */</span>
    <span>uint8_t</span>     <span>chksum_mismatch</span><span>;</span>    <span>/* 1 */</span>
    <span>uint8_t</span>     <span>backend_id</span><span>;</span>         <span>/* 1 */</span>
    <span>uint32_t</span>    <span>backend_version</span><span>;</span>    <span>/* 4 */</span>
<span>}</span> <span>fragment_metadata_t</span><span>;</span></code></pre>
</div>
</div>
<p>This is just a liberasurecode thing.  Using either Jerasure or ISA-L directly allows access to only the erasure coded data.  It <em>is</em> required as part of the APIs that each chunk must be provided along with if it was the Nth data or parity chunk, so the index must be maintained somehow as part of metadata.</p>
<p>As was noted in the <a href="https://www.youtube.com/watch?v=URAm-bbst-o">YDB talk at HydraConf</a>, Jerasure does a permutation of the output from what one would expect from just the linear algebra.  This means that it’s up to the specific implementation details of a library as to if reads must be aligned with writes — Jerasure cannot read a subset or superset of what was encoded.  ISA-L applies no permutation, so reads may decode unaligned subsets or supersets of encoded data.</p>
<p>Jerasure and ISA-L are, by far, the most popular libraries for erasure coding, but they’re not the only ones.  <a href="https://github.com/tahoe-lafs/zfec">tahoe-lafs/zfec<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a> is also a reasonably well-known implementation.  Christopher Taylor has written at least three MDS erasure coding implementations taking different tradeoffs (<a href="https://github.com/catid/cm256">catid/cm256<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a>, <a href="https://github.com/catid/longhair">catid/longhair<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a>, <a href="https://github.com/catid/leopard">catid/leopard<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a>), and a comparison and discussion of the differences can be found on <a href="https://github.com/catid/leopard/blob/master/Benchmarks.md">leopard’s benchmarking results page</a>.  If erasure coding becomes a bottleneck, a library more optimized for your specific use case can likely be found somewhere, but ISA-L is generally good enough.</p>
<h2 id="_implementing_erasure_codes">
Implementing Erasure Codes
</h2> 
<p>It is entirely acceptable and workable to treat erasure codes as a magic function that turns 1 file into \(n\) chunks and back.  You can stop reading here, and not knowing the details of what math is being performed will not hinder your ability to leverage erasure codes to great effect in distributed systems or databases.  (And if you continue, take what follows with a large grain of salt, as efficient erasure coding is a subject folk have spent years on, and the below is what I’ve collected from a couple of days of reading through papers I only half understand.)</p>
<p>The construction of the \(n\) chunks is some linear algebra generally involving a Galois Field, none of which is important to understand to be able to productively <em>use</em> erasure codes.  Backblaze published <a href="https://www.backblaze.com/blog/reed-solomon/">a very basic introduction</a>.  The best introduction to the linear algebra of erasure coding that I’ve seen is Fred Akalin’s <a href="https://www.akalin.com/intro-erasure-codes">"A Gentle Introduction to Erasure Codes"</a>.  <a href="https://tomverbeure.github.io/2022/08/07/Reed-Solomon.html">Reed-Solomon Error Correcting Codes from the Bottom Up</a> covers Reed-Solomon codes and Galois Field polynomials specifically.  There’s also a plethora of erasure coding-related questions on the Stack Overflow family of sites, so any question over the math that one might have has already likely been asked and answered there.</p>
<p>With the basics in place, there are two main dimensions to investigate: what is the exact MDS encoding and decoding algorithm to implement, and how can one implement that algorithm most efficiently?</p>
<h3 id="_algorithmic_efficiency">
Algorithmic Efficiency
</h3> 
<p>In general, most MDS codes are calculated as a matrix multiplication, where addition is replaced with XOR, and multiply is replaced with a more expensive multiplication over GF(256).  For the special cases of 1-3 parity chunks (\(m \in \{1,2,3\}\)), there are algorithms not derived from Reed-Solomon and which use only XORs:</p>
<div>
<ul>
<li>
<p>\(m=1\) is a trivial case of a single parity chunk, which is just the XOR of all data chunks.</p>
</li>
<li>
<p>\(m=2\) is also known as RAID-6, for which I would recommend Liberation codes<a id="_sideref_8"></a><sup>[8]</sup><a id="_sideref_9"></a><sup>[9]</sup> as <em>nearly</em> optimal with an implementation available as part of <a href="https://jerasure.org/">Jerasure</a>, and HDP codes<a id="_sideref_10"></a><sup>[10]</sup> and EVENODD<a id="_sideref_11"></a><sup>[11]</sup> as notable but patented.  If \(k+m+2\) is prime, then X-Codes<a id="_sideref_12"></a><sup>[12]</sup> are also optimal.</p>
</li>
<li>
<p>\(m=3\) can be done via STAR coding<a id="_sideref_13"></a><sup>[13]</sup>.</p>
</li>
</ul>
</div>

<p>Otherwise and more generally, a form of Reed-Solomon coding is used.  The encoding/decoding matrix is either a \(k \times n\) Vandermonde<a id="_sideref_14"></a><sup>[14]</sup> matrix with the upper \(k \times k\) of it Gaussian eliminated to form an identity matrix, or an \(k \times k\) identity matrix with a \(k \times m\) Cauchy<a id="_sideref_15"></a><sup>[15]</sup> matrix glued onto the bottom.  In both cases, the goal is to form a matrix where the top \(k \times k\) is an identity matrix (so that each data chunk is preserved), and any deletion of \(m\) rows yields an invertible matrix.  Encoding is multiplying by this matrix, and decoding deletes the rows corresponding to erased chunks, and then solves the matrix as a system of linear equations for the missing data.</p>
<p>Gaussian elimination, as used in ISA-L, is the simplest method of decoding, but also the slowest.  For Cauchy matrixes, this can be improved<a id="_sideref_16"></a><sup>[16]</sup>, as done in <a href="https://github.com/catid/cm256">catid/cm256<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a>.  The current fastest methods appear to be implemented in <a href="https://github.com/catid/leopard">catid/leopard<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a>, which uses Fast Fourier Transforms<a id="_sideref_17"></a><sup>[17]</sup><a id="_sideref_18"></a><sup>[18]</sup> for encoding and decoding.</p>

<h3 id="_implementation_efficiency">
Implementation Efficiency
</h3> 
<p>There are levels of implementation efficiency for erasure codes that function over any \(k+m\) configuration:</p>
<div>
<ol>
<li>
<p>Implement the algorithm in C, and rely on the compiler for auto-vectorization.</p>
<p>This provides the most straightforward and most portable implementation, at acceptable performance.  Usage of <code>restrict</code> and ensuring the appropriate architecture-specific compilation flags have been specified (e.g. <code>-march=native</code>).</p>
</li>
<li>
<p>Rely on a vectorization library or compiler intrinsics to abstract the platform specifics.</p>
<p><a href="https://github.com/google/highway">google/highway<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a> and <a href="https://github.com/xtensor-stack/xsimd">xtensor-stack/xsimd<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a> appear to be reasonably commonly used libraries that try to use the best available SIMD instructions to accomplish general tasks.  There is also the upcoming <a href="https://en.cppreference.com/w/cpp/experimental/simd/simd"><code>std::experimental::simd</code></a>.  C/C++ compilers also offer <a href="https://gcc.gnu.org/onlinedocs/gcc/Vector-Extensions.html">builtins</a> for vectorization support.</p>
<p>The core of encoding and decoding is Galois field multiply and addition.  Optimized libraries for this can be found at <a href="https://github.com/catid/gf256">catid/gf256<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a> and <a href="https://web.eecs.utk.edu/~jplank/plank/papers/CS-07-593/">James Plank’s Fast Galois Field Arithmetic Library</a>.</p>
</li>
<li>
<p>Handwrite a vectorized implementation of the core encoding and decoding functions.</p>
<p>Further discussion of fast GF(256) operations can be found in the PARPAR project: <a href="https://github.com/animetosho/ParPar/blob/master/fast-gf-multiplication.md">fast-gf-multiplication</a> and the <a href="https://github.com/animetosho/ParPar/blob/master/xor_depends/info.md">xor_depends work</a>.  The consensus appears to be that a XOR-only GF multiply should be faster than a table-driven multiply.</p>

</li>
</ol>
</div>
<p>Optimizing further involves specializing the code to one specific \(k+m\) configuration by transforming the matrix multiplication with a constant into a linear series of instructions, and then:</p>
<div>
<ol start="4">
<li>
<p>Find an optimal coding matrix and XOR schedule for the specific GF polynomial and encoding matrix.</p>

</li>
<li>
<p>Apply further operation, memory, and cache optimizations.</p>

<p>The code is publicly available at <a href="https://github.com/yuezato/xorslp_ec">yuezato/xorslp_ec<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a>.</p>
</li>
<li>
<p>Programmatically explore an optimized instruction schedule for a specific architecture.</p>

<p>The code is publicly available at <a href="https://github.com/Thesys-lab/tvm-ec">Thesys-lab/tvm-ec<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a>.</p>
</li>
</ol>
</div>
<p>For a more fully explored treatment of this topic, please see <a href="https://www.usenix.org/conference/fast19/presentation/zhou">"Fast Erasure Coding for Data Storage: A Comprehensive Study of the Acceleration Techniques"</a><a id="_sideref_19"></a><sup>[19]</sup>, which also has a video of the presenter if that’s your preferred medium.
<span><a id="_sidedef_19"></a>[19]: Tianli Zhou and Chao Tian. 2020. Fast Erasure Coding for Data Storage: A Comprehensive Study of the Acceleration Techniques. <em>ACM Trans. Storage</em> 16, 1 (March 2020). <a href="https://scholar.google.com/scholar?cluster=15189943361362749273">[scholar]</a></span></p>
<h2 id="_references">
References
</h2> 
<p><a href="https://transactional.blog/blog/2024-erasure-coding.bib">References as BibTeX</a></p>
<p>And if you’re looking to broadly dive deeper, I’d suggest starting with reviewing <a href="https://dblp.org/pid/07/3005.html">James S. Plank’s publications</a>.</p>
    <!-- TODO: consider https://utteranc.es/ for in-page comments. -->
      <hr>
      
      <p><a href="https://discu.eu/?q=https://transactional.blog/blog/2024-erasure-coding.html&amp;submit_title=Erasure%20Coding%20for%20Distributed%20Systems">See discussion of this page on Reddit, HN, and lobsters.</a></p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Snowden: The arrest of Durov is an assault on the basic human rights (129 pts)]]></title>
            <link>https://twitter.com/Snowden/status/1827695836832334169</link>
            <guid>41360808</guid>
            <pubDate>Mon, 26 Aug 2024 19:24:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/Snowden/status/1827695836832334169">https://twitter.com/Snowden/status/1827695836832334169</a>, See on <a href="https://news.ycombinator.com/item?id=41360808">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The Arrest of Pavel Durov Is a Reminder That Telegram Is Not Encrypted (142 pts)]]></title>
            <link>https://gizmodo.com/the-arrest-of-pavel-durov-is-a-reminder-that-telegram-is-not-encrypted-2000490960</link>
            <guid>41359502</guid>
            <pubDate>Mon, 26 Aug 2024 17:28:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/the-arrest-of-pavel-durov-is-a-reminder-that-telegram-is-not-encrypted-2000490960">https://gizmodo.com/the-arrest-of-pavel-durov-is-a-reminder-that-telegram-is-not-encrypted-2000490960</a>, See on <a href="https://news.ycombinator.com/item?id=41359502">Hacker News</a></p>
Couldn't get https://gizmodo.com/the-arrest-of-pavel-durov-is-a-reminder-that-telegram-is-not-encrypted-2000490960: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Intel SGX Fuse Key0, a.k.a. Root Provisioning Key Was Extracted by Researchers (211 pts)]]></title>
            <link>https://twitter.com/_markel___/status/1828112469010596347</link>
            <guid>41359152</guid>
            <pubDate>Mon, 26 Aug 2024 16:56:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/_markel___/status/1828112469010596347">https://twitter.com/_markel___/status/1828112469010596347</a>, See on <a href="https://news.ycombinator.com/item?id=41359152">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Eating the Birds of America: Audubon's Culinary Reviews of America's Birds (145 pts)]]></title>
            <link>https://usbirdhistory.com/audubon-eating-americas-birds/</link>
            <guid>41359132</guid>
            <pubDate>Mon, 26 Aug 2024 16:55:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://usbirdhistory.com/audubon-eating-americas-birds/">https://usbirdhistory.com/audubon-eating-americas-birds/</a>, See on <a href="https://news.ycombinator.com/item?id=41359132">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>On June 26, 1826, John James Audubon sat aboard the cotton schooner <em>Delos </em>off of Florida’s Gulf coast, en route from New Orleans to Liverpool, where he was hoping to find a publisher for his extensive portfolio of paintings of American birds.[1] On this particular day, the winds were still, leaving Audubon’s boat to rock with the waves, making no forward progress. Audubon sat on deck with the ship’s crew, watching a group of small black and white birds skimming above the waves. Since they had sailed beyond the mouth of the Mississippi and entered the Gulf, Audubon had observed growing numbers of these birds, watching as they searched the open seas for floating patches of vegetation. As they watched a small flock fly near their boat, the ship’s mate raised a gun and brought down four of the birds with one blast of birdshot. At Audubon’s request, he then brought them on board. This was the first time that Audubon was able to examine the birds up close, extend their wings with his hands, and run his fingers through their feathers. Audubon recorded careful notes of the details of its plumage, beak, eyes, wings, and claws. He skinned each of the birds before dissecting them and noting the contents of their stomach, finding them full of fish in various states of digestion. He then took one final measure of the bird, writing that “the flesh of this Petrel was fat, but tough, with a strong smell, and unfit for food; for, on tasting it, <em>as is my practice</em>, I found it to resemble that of the porpoises.”[2]</p>



<figure>
<figure><img loading="lazy" decoding="async" width="640" height="418" data-id="138" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-shearwater-dusky-petrel.png?resize=640%2C418&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-shearwater-dusky-petrel.png?w=893&amp;ssl=1 893w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-shearwater-dusky-petrel.png?resize=300%2C196&amp;ssl=1 300w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-shearwater-dusky-petrel.png?resize=768%2C501&amp;ssl=1 768w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-shearwater-dusky-petrel.png?resize=850%2C555&amp;ssl=1 850w" sizes="(max-width: 640px) 100vw, 640px" data-recalc-dims="1"></figure>



<figure><img loading="lazy" decoding="async" width="640" height="429" data-id="139" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-shearwater-photo.png?resize=640%2C429&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-shearwater-photo.png?w=929&amp;ssl=1 929w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-shearwater-photo.png?resize=300%2C201&amp;ssl=1 300w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-shearwater-photo.png?resize=768%2C515&amp;ssl=1 768w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-shearwater-photo.png?resize=850%2C570&amp;ssl=1 850w" sizes="(max-width: 640px) 100vw, 640px" data-recalc-dims="1"></figure>
<figcaption>Left, Audubon’s painting of his eponymous shearwater (which he called a Dusky Petrel), and right, Audubon’s Shearwater in real life (credit: <a href="https://commons.wikimedia.org/w/index.php?curid=4251231">Dominic Sherony CC BY-SA 2.0</a>). </figcaption></figure>



<p>In this passage, Audubon was describing a bird that he called the Dusky Petrel, which was not one that often made its way onto the plates of sailors or into market stalls. The bird was, in fact, not yet known to science, and if Audubon was not the first person to sample its meat, he was likely the first to describe the bird in writing. Audubon assumed that the slender, charcoal-colored bird was the same species as one that had first been described in the Pacific 40 years previously. In fact, he was taking detailed notes on a related bird that was not yet known to science. This fact was recognized in 1872, and the bird was renamed Audubon’s Shearwater in recognition of his discovery.[3]&nbsp;</p>









<p>Whether birds were large or small, familiar or obscure, palatable or nauseating, Audubon made sampling their meat a part of his extensive process of studying America’s birds for his<em> Ornithological Biography, or, An account of the habits of the birds of the United States of America</em>, a five-volume text that would accompany the 435 paintings that composed his <em>Birds of America</em>, for which he gained his fame. Alongside a description of each bird’s appearance, diet, behavior, and breeding habits, Audubon frequently included a reflection on the taste of the bird’s flesh.</p>


<div>
<figure><img loading="lazy" decoding="async" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-birds-of-america-volume-NYPL.png?resize=540%2C459&amp;ssl=1" alt="" width="540" height="459" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-birds-of-america-volume-NYPL.png?w=769&amp;ssl=1 769w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-birds-of-america-volume-NYPL.png?resize=300%2C255&amp;ssl=1 300w" sizes="(max-width: 540px) 100vw, 540px" data-recalc-dims="1"><figcaption>A first-edition copy of <em>Birds of America</em>, which featured life-sized paintings of 435 of America’s birds, hand-painted on “double elephant” size pages measuring more than two feet by three feet. This copy is held at the New York Public Library.</figcaption></figure></div>


<p>Throughout the 19th century, wild birds were a common part of both rural and urban cuisine. In 1867, Thomas De Voe included an inventory of all of the birds he had seen for sale in the markets of Boston, Philadelphia, and New York, which included some 36 species of waterfowl, 57 varieties of shorebird and upland game, and 27 kinds of songbirds, for a total of 120 species of birds.[4] Yet by sampling the majority of the 435 birds he painted for his <em>Birds of America </em>(mostly based on birds he had taxidermied and posed himself), Audubon far exceeded the variety of birds even the most accomplished gourmand might have possibly eaten. John James Audubon, without a doubt, holds the record for “most species of American birds eaten.”</p>


<div>
<figure><img loading="lazy" decoding="async" width="640" height="518" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/De-Voe-Market-Assistant-Title-Page-image-1.png?resize=640%2C518&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/De-Voe-Market-Assistant-Title-Page-image-1.png?w=661&amp;ssl=1 661w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/De-Voe-Market-Assistant-Title-Page-image-1.png?resize=300%2C243&amp;ssl=1 300w" sizes="(max-width: 640px) 100vw, 640px" data-recalc-dims="1"><figcaption>Title page from Thomas De Voe’s 1867 book <em>The Market Assistant</em>, which listed 120 species of American birds found for sale in markets.</figcaption></figure></div>


<p>Since 1918, it has been a federal crime to “pursue, hunt, take, capture, kill, attempt to take, capture, or kill, possess, offer for sale, sell, offer to barter, barter, offer to purchase, purchase” or do really anything else with the bodies of most migratory birds,[5] with the exception of a few species designated as game, meaning that Audubon’s record is definitively safe. Safe too are most birds, at least from diners with adventurous palates. That being said, Audubon did provide a fascinating eater’s guide to America’s birds, which is an informative read for all of us who will (happily!) never have the opportunity to dine on Bald Eagle, or for that matter, Passenger Pigeon. So without further ado, here is a sampling of Audubon’s reviews of the culinary merits of America’s birds.[6]</p>



<h2>Game Birds</h2>



<p>First, Audubon shared his opinions about the many birds categorized as game, which would have been commonly eaten in his day. Audubon considered the meat from <strong>Ring-necked Ducks</strong> and <strong>Common Eiders</strong> to be “excellent” ; that of the <strong>Cackling Goose</strong> “exquisite”, and the <strong>Barnacle Goose</strong> “sweet and tender, and highly esteemed for the table.” <strong>Ruddy Ducks</strong> “provide good eating when fat and young.” <strong>Wood Ducks</strong> are at their best in the Fall, when Audubon found them to be tender and juicy. Audubon was much less fond of other waterfowl, finding the <strong>Bufflehead </strong>“fishy and disagreeable,” the <strong>Common Goldeneye</strong> “unfit for being eaten” (although he found their eggs delicious), and the <strong>Surf Scoter</strong> “tough, rank, and fishy, so as to be scarcely fit for food.” The <strong>Hooded Merganser</strong> too “has a fishy taste and odour, although it is relished by some persons.”</p>



<figure>
<figure><img loading="lazy" decoding="async" width="423" height="266" data-id="145" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/common-eider.png?resize=423%2C266&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/common-eider.png?w=423&amp;ssl=1 423w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/common-eider.png?resize=300%2C189&amp;ssl=1 300w" sizes="(max-width: 423px) 100vw, 423px" data-recalc-dims="1"></figure>



<figure><img loading="lazy" decoding="async" width="378" height="249" data-id="143" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/bufflehead.png?resize=378%2C249&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/bufflehead.png?w=378&amp;ssl=1 378w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/bufflehead.png?resize=300%2C198&amp;ssl=1 300w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/bufflehead.png?resize=350%2C230&amp;ssl=1 350w" sizes="(max-width: 378px) 100vw, 378px" data-recalc-dims="1"></figure>



<figure><img loading="lazy" decoding="async" width="365" height="267" data-id="147" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/hooded-merganser.png?resize=365%2C267&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/hooded-merganser.png?w=365&amp;ssl=1 365w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/hooded-merganser.png?resize=300%2C219&amp;ssl=1 300w" sizes="(max-width: 365px) 100vw, 365px" data-recalc-dims="1"></figure>
<figcaption>The Common Eider is “excellent” (Left, <a href="https://commons.wikimedia.org/w/index.php?curid=98887404">Rhododendrites – Own work, CC BY-SA 4.0</a>), the Bufflehead is “fishy and disagreeable” (Center, <a href="https://commons.wikimedia.org/w/index.php?curid=617245">CC BY-SA 3.0</a>), and the Hooded Merganser has “a fishy taste and odour” (Right, <a href="https://commons.wikimedia.org/w/index.php?curid=99102166">Rhododendrites – Own work, CC BY-SA 4.0</a>).</figcaption></figure>



<p>After the <strong><a href="https://usbirdhistory.com/transatlantic-turkeys/">Wild Turkey</a></strong>, Audubon’s favorite upland game bird to eat was the <strong>Ruffed Grouse</strong>, which were best taken in September after spending a summer feeding on mountain Huckleberries and Whortleberries. Meat from <strong>Mourning Doves</strong> was “remarkably fine, when they are obtained young and in the proper season.” The <strong>White-crowned Pigeon</strong>, found in the Florida Keys, is the most skittish bird which Audubon encountered, which he reasoned was due to the “continued war waged against them, their flesh being juicy, well flavoured, and generally tender, even in old birds.” When they feed on grasshoppers and strawberries, <strong>Upland Sandpipers</strong> are “truly delicious.” When fed on cantharides, however, they can become quite noxious if not cleaned properly, a situation which caused several New Orleans acquaintances of Audubon to leave their dinner expeditiously, “suffer[ing] greatly”, under circumstances which, Audubon wrote, “cannot well be described here.” Audubon claimed that <strong>Woodcocks </strong>were considered such fine dishes that some who he called epicures ate these birds “with all their viscera, worms and insects to boot, the intestines in fact being considered the most savoury parts.” Audubon, for his part, never joined in this practice. </p>



<h2>Backyard Birds</h2>



<p>Audubon also tried just about every songbird and backyard bird, big or small, whether they were commonly eaten at the time or not. <strong><a href="https://usbirdhistory.com/how-robins-got-their-name/">Robins</a></strong>, which he considered “fat and juicy, and afford excellent eating,” were hunted and eaten widely throughout the south, as were <strong>Bobolinks</strong>, which have “extremely tender and juicy” flesh. <strong>Brown-headed Cowbirds</strong> were also shot in large numbers, and Audubon considered them “more delicate and better flavoured than the species with which they associate, excepting the Robin”. The <strong>Louisiana Waterthrush</strong>, which is much less common of a bird, “becomes so plump as to be a pure mass of fat” during the winter, and “furnishes extremely delicate eating.” <strong>Orchard Orioles</strong> make good eating in the early fall, and according to Audubon are a favorite of the “Creoles of Louisiana.” The meat of <strong>Purple Finches </strong>was “equal to that of any other small bird.” <strong>Vesper Sparrows</strong> have “juicy, tender and savoury” flesh, but while <strong>Swamp Sparrows</strong> are fat and tender, their flesh is “sedgy.”&nbsp;</p>



<figure>
<figure><img loading="lazy" decoding="async" width="307" height="392" data-id="142" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/bobolink.png?resize=307%2C392&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/bobolink.png?w=307&amp;ssl=1 307w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/bobolink.png?resize=235%2C300&amp;ssl=1 235w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/bobolink.png?resize=300%2C383&amp;ssl=1 300w" sizes="(max-width: 307px) 100vw, 307px" data-recalc-dims="1"></figure>



<figure><img loading="lazy" decoding="async" width="518" height="354" data-id="144" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/cedar-waxwing.png?resize=518%2C354&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/cedar-waxwing.png?w=518&amp;ssl=1 518w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/cedar-waxwing.png?resize=300%2C205&amp;ssl=1 300w" sizes="(max-width: 518px) 100vw, 518px" data-recalc-dims="1"></figure>



<figure><img loading="lazy" decoding="async" width="343" height="330" data-id="152" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/swamp-sparrow.png?resize=343%2C330&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/swamp-sparrow.png?w=343&amp;ssl=1 343w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/swamp-sparrow.png?resize=300%2C289&amp;ssl=1 300w" sizes="(max-width: 343px) 100vw, 343px" data-recalc-dims="1"></figure>
<figcaption>Bobolinks are “extremely tender and juicy” (Left: <a href="https://commons.wikimedia.org/w/index.php?curid=113783228">Kshanti Greene – Own work, CC BY-SA 4.0</a>), Cedar Waxwings are good in pies (Center: <a href="https://commons.wikimedia.org/w/index.php?curid=93030737">Alan Rice – Own work, CC BY-SA 4.0</a>), and Swamp Sparrows are tender but “sedgy” (Right: <a href="https://commons.wikimedia.org/w/index.php?curid=19418982">Cephas – Own work, CC BY-SA 3.0</a>).</figcaption></figure>



<p><strong>Common Grackles</strong> have flesh that is “little better than that of the <strong>Crow</strong>, being dry and ill-flavoured” (although he found their eggs to be “quite delicate”). Nevertheless, Audubon reports with some curiosity that grackles were “frequently used, with the addition of one or two Golden-winged Woodpeckers or Redwings, to make what is here called <em>pot pie</em>, even amidst a profusion of so many better things.” Blackbirds weren’t the only birds that ended up baked in pies. An acquaintance of Audubon’s shipped a basket of <strong>Cedar Waxwings</strong> to New Orleans as a Christmas present, only to find that “the steward of the steamer, in which they were shipped, made pies of them for the benefit of the passengers.” Audubon found the flesh of most types of woodpeckers to taste too much like ants, but <strong>Northern Flickers</strong> were nevertheless frequently eaten by hunters and occasionally sold in markets. Although the <strong>Whip-poor-will</strong> was rarely killed because it is “too small to be sought as an article of food,” Audubon still sampled its meat, which he deemed “savoury.”</p>



<h2>Seabirds and Shorebirds</h2>



<p>Audubon found the meat from <strong>Wood Storks</strong> to be too tough and oily to eat, but wrote that in Louisiana they were skinned, cooked, and eaten. Meat from <strong>Roseate Spoonbills</strong> was “oily and poor eating.” <strong>Puffins </strong>were so tough and fishy that Audubon could only recommend their meat to those facing desperate circumstances (a judgment he also passed on the <strong>American Oystercatcher</strong> and <strong>American White Pelican</strong>), and their eggs were not much better. The <strong>Magnificent Frigatebird</strong> was “unfit for any other person than one in a state of starvation.” <strong>Belted Kingfisher </strong>eggs make fine eating, but their meat is “extremely fishy, oily, and disagreeable.” <strong>Cormorants </strong>were perhaps the only bird that Audubon did not taste, and he intended to avoid doing so until such a time as “nothing better could be procured” – a situation he must have considered truly desperate.</p>



<figure>
<figure><img loading="lazy" decoding="async" width="416" height="275" data-id="154" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/image.png?resize=416%2C275&amp;ssl=1" alt="This image has an empty alt attribute; its file name is roseate-spoonbill.png" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/image.png?w=416&amp;ssl=1 416w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/image.png?resize=300%2C198&amp;ssl=1 300w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/image.png?resize=350%2C230&amp;ssl=1 350w" sizes="(max-width: 416px) 100vw, 416px" data-recalc-dims="1"></figure>



<figure><img loading="lazy" decoding="async" width="275" height="386" data-id="148" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/magnificent-frigatebird.png?resize=275%2C386&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/magnificent-frigatebird.png?w=275&amp;ssl=1 275w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/magnificent-frigatebird.png?resize=214%2C300&amp;ssl=1 214w" sizes="(max-width: 275px) 100vw, 275px" data-recalc-dims="1"></figure>



<figure><img loading="lazy" decoding="async" width="384" height="286" data-id="141" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/belted-kingfisher.png?resize=384%2C286&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/belted-kingfisher.png?w=384&amp;ssl=1 384w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/belted-kingfisher.png?resize=300%2C223&amp;ssl=1 300w" sizes="(max-width: 384px) 100vw, 384px" data-recalc-dims="1"></figure>
<figcaption>Roseate Spoonbills are “oily and poor eating (left: <a href="https://commons.wikimedia.org/w/index.php?curid=42301356">Photo Dante – Own work, CC BY-SA 4.0</a>), Magnificent Frigatebirds are “unfit for any other person than one in a state of starvation” (center: <a href="https://commons.wikimedia.org/w/index.php?curid=12334028">John Picken, CC BY 2.0</a>), and Belted Kingfishers are “extremely fishy, oily, and disagreeable (right: <a href="https://commons.wikimedia.org/w/index.php?curid=116127806">MarshBunny – Own work, CC BY-SA 4.0</a>). </figcaption></figure>



<p>Audubon found <strong>Loons </strong>to be “not very palatable,” with their meat “being tough, rank, and dark coloured.” This opinion was not shared by all, however, as Audubon had “seen it much relished by many lovers of good-living, especially at Boston, where it was not infrequently served almost raw.” <strong>Black Terns</strong> are “tolerably good.” Fishermen and eggers would preserve <strong>Herring Gulls</strong> in salt to be eaten over the winter. Audubon was indifferent to mature <strong>Killdeer</strong>, but found the young in their first Autumn to be “fat, juicy, and tender.” <strong>Dunlins </strong>were also fat and juicy, for which reason they were shot in large numbers. <strong>Spotted Sandpipers</strong> are “delicious” and fat in the fall. Audubon had reached the limits of his descriptive capabilities by the time he described the meat from <strong>Black-necked Stilts</strong>, which he labeled neither good nor bad, but “ordinary.”</p>



<h2>Birds You Will Definitely Never Get To Eat</h2>



<p>Finally, Audubon left us his thoughts on birds that may have been considered food in his day, but are now endangered, extinct, or highly protected by federal law. Meat from adult <strong>Passenger Pigeons</strong> (extinct since 1914) was dark but tolerable. That of young birds taken from their nests, however, was “much esteemed.” When <strong>Carolina Parakeets</strong> (extinct since 1918) are young, “their flesh is tolerable food.” Similarly, young <strong>Whooping Cranes</strong> (current population: 543 birds) were “tender and juicy.” Audubon pronounced older birds of that species to be “tough and unfit for the table”, although he noted that the Seminoles of Florida hunted them for food. Audubon also sampled the meat of <strong>Peregrine Falcons</strong>, which he found to be very tough. Young <strong>Bald Eagles</strong> were like “veal in taste and tenderness,” while the flesh of <strong>Barred Owls</strong> is “palatable.” Of all the birds he ate, there was one he compared to our most common poultry. Meat from <strong>Snowy Owls, </strong>Audubon wrote, is like “that of a chicken, and not indelicate eating.”</p>



<figure>
<figure><img loading="lazy" decoding="async" width="493" height="624" data-id="150" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-passenger-pigeon.png?resize=493%2C624&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-passenger-pigeon.png?w=493&amp;ssl=1 493w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-passenger-pigeon.png?resize=237%2C300&amp;ssl=1 237w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-passenger-pigeon.png?resize=300%2C380&amp;ssl=1 300w" sizes="(max-width: 493px) 100vw, 493px" data-recalc-dims="1"></figure>



<figure><img loading="lazy" decoding="async" width="410" height="594" data-id="149" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-carolina-parakeet.png?resize=410%2C594&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-carolina-parakeet.png?w=410&amp;ssl=1 410w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-carolina-parakeet.png?resize=207%2C300&amp;ssl=1 207w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-carolina-parakeet.png?resize=300%2C435&amp;ssl=1 300w" sizes="(max-width: 410px) 100vw, 410px" data-recalc-dims="1"></figure>



<figure><img loading="lazy" decoding="async" width="521" height="821" data-id="151" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-whooping-crane.png?resize=521%2C821&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-whooping-crane.png?w=521&amp;ssl=1 521w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-whooping-crane.png?resize=190%2C300&amp;ssl=1 190w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2023/08/audubon-whooping-crane.png?resize=300%2C473&amp;ssl=1 300w" sizes="(max-width: 521px) 100vw, 521px" data-recalc-dims="1"></figure>
<figcaption>Audubon’s paintings of the Passenger Pigeon (left – their meat was dark, but tolerable), Carolina Parakeet (center – also “tolerable food”), and the Whooping Crane (right – “tender and juicy”). </figcaption></figure>







<div>




<p>Enter your email to receive new posts any time they’re published.</p>
</div>



<div>




<p>Enter your email to receive new posts any time they’re published.</p>
</div>



<hr>



<p>[1] T. S. Palmer, Audubon’s Shearwater in the United States, <em>The Auk</em>, Volume 48, Issue 2, 1 April 1931, Pages 198–206, <a href="https://doi.org/10.2307/4076787">https://doi.org/10.2307/4076787</a><br>[2] Audubon,&nbsp;John James.&nbsp;Ornithological Biography, Or an Account of the Habits of the Birds of the United States of America: Accompanied by Descriptions of the Objects Represented in the Work Entitled The Birds of America, and Interspersed with Delineations of American Scenery and Manners.&nbsp;United Kingdom:&nbsp;Black,&nbsp;1835.<br>[3] Palmer, “Audubon’s Shearwater”<br>[4] De Voe,&nbsp;Thomas Farrington.&nbsp;The Market Assistant: Containing a Brief Description of Every Article of Human Food Sold in the Public Markets of the Cities of New York, Boston, Philadelphia, and Brooklyn; Including the Various Domestic and Wild Animals, Poultry, Game, Fish, Vegetables, Fruits &amp;c., &amp;c. with Many Curious Incidents and Anecdotes.&nbsp;United States:&nbsp;Hurd and Houghton,&nbsp;1867.<br>[5] Migratory Bird Treaty Act, 16 U.S.C. §703<br>[6] All quotes are from Audubon, Ornithological Biography, vol. I-V. </p>



<hr>



<div data-v="2" data-block-id="7bf3034"><div><article><h4><a href="https://usbirdhistory.com/we-used-to-have-parrots/">We Used to Have Parrots</a></h4><a href="https://usbirdhistory.com/we-used-to-have-parrots/"></a><p>At the northern tip of Kentucky there’s a mineral spring called Big Bone Lick. Some of the earliest Europeans to arrive at the spring found massive bones sticking out of the mud, left by enormous animals that had evidently gotten stuck while…</p><a href="https://usbirdhistory.com/we-used-to-have-parrots/">Continue Reading</a></article></div><div><article><h4><a href="https://usbirdhistory.com/canaries-in-coal-mines/">Canaries in Coal Mines</a></h4><a href="https://usbirdhistory.com/canaries-in-coal-mines/"><figure><img loading="lazy" decoding="async" width="441" height="414" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/07/mine-rescuers-with-a-canary.png?fit=441%2C414&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/07/mine-rescuers-with-a-canary.png?w=441&amp;ssl=1 441w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/07/mine-rescuers-with-a-canary.png?resize=300%2C282&amp;ssl=1 300w" sizes="(max-width: 441px) 100vw, 441px"></figure></a><p>Sometime between 7:00 and 7:30AM the morning of November 6, 1922, a stray spark ignited a pocket of gas deep beneath the town of Spangler, Pennsylvania. Several men working inside the Reilly coal mine were immediately killed by the blast, thrown against…</p><a href="https://usbirdhistory.com/canaries-in-coal-mines/">Continue Reading</a></article></div><div><article><h4><a href="https://usbirdhistory.com/americas-favorite-bird/">America’s Favorite Bird</a></h4><a href="https://usbirdhistory.com/americas-favorite-bird/"></a><p>From New York Public Library’s collection Aviary and Cage Birds: A Series of 50 (Player’s Cigarettes). Shrewdness. Intelligence. Capability to endure a vast amount of exposure and hardship. Skill at telling male and female birds apart. These were the most important characteristics…</p><a href="https://usbirdhistory.com/americas-favorite-bird/">Continue Reading</a></article></div><div><article><h4><a href="https://usbirdhistory.com/when-birds-meant-food/">When Birds Meant Food</a></h4><a href="https://usbirdhistory.com/when-birds-meant-food/"></a><p>Looking back at the pieces I’ve written since I started this newsletter last September, I realize that an uncomfortably large number of posts deal with killing birds, often for their meat, sometimes out of necessity, and many times just for fun. These…</p><a href="https://usbirdhistory.com/when-birds-meant-food/">Continue Reading</a></article></div><div><article><h4><a href="https://usbirdhistory.com/deadly-ceilometers/">Deadly Pillars of Light</a></h4><a href="https://usbirdhistory.com/deadly-ceilometers/"><figure><img loading="lazy" decoding="async" width="640" height="591" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/04/ceilometer-this-machine-kills-birds.png?fit=640%2C591&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/04/ceilometer-this-machine-kills-birds.png?w=641&amp;ssl=1 641w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/04/ceilometer-this-machine-kills-birds.png?resize=300%2C277&amp;ssl=1 300w" sizes="(max-width: 640px) 100vw, 640px"></figure></a><p>To make a safe landing, one of the many pieces of data that pilots need is the height of clouds above the landing strip. Because clouds block a pilot’s vision, a low cloud bank can make it difficult, even dangerous, for planes…</p><a href="https://usbirdhistory.com/deadly-ceilometers/">Continue Reading</a></article></div><div><article><h4><a href="https://usbirdhistory.com/birds-and-slavery/">Birds and Slavery</a></h4><a href="https://usbirdhistory.com/birds-and-slavery/"><figure><img loading="lazy" decoding="async" width="640" height="243" src="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/03/federal-writers-project-slave-narratives-collection-interviewees.png?fit=640%2C243&amp;ssl=1" alt="" srcset="https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/03/federal-writers-project-slave-narratives-collection-interviewees.png?w=1242&amp;ssl=1 1242w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/03/federal-writers-project-slave-narratives-collection-interviewees.png?resize=300%2C114&amp;ssl=1 300w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/03/federal-writers-project-slave-narratives-collection-interviewees.png?resize=1024%2C388&amp;ssl=1 1024w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/03/federal-writers-project-slave-narratives-collection-interviewees.png?resize=768%2C291&amp;ssl=1 768w, https://i0.wp.com/usbirdhistory.com/wp-content/uploads/2024/03/federal-writers-project-slave-narratives-collection-interviewees.png?resize=850%2C322&amp;ssl=1 850w" sizes="(max-width: 640px) 100vw, 640px"></figure></a><p>From 1936 to 1938, an army of formerly-unemployed writers hired under the Works Progress Administration traveled South to collect the life histories of aging women and men born into slavery. The Federal Writers Project was just one of many depression-era initiatives designed…</p><a href="https://usbirdhistory.com/birds-and-slavery/">Continue Reading</a></article></div></div>




			</div></div>]]></description>
        </item>
    </channel>
</rss>